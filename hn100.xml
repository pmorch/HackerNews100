<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 05 Sep 2023 11:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[ZFS for Dummies (243 pts)]]></title>
            <link>https://ikrima.dev/dev-notes/homelab/zfs-for-dummies/</link>
            <guid>37387392</guid>
            <pubDate>Tue, 05 Sep 2023 03:07:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ikrima.dev/dev-notes/homelab/zfs-for-dummies/">https://ikrima.dev/dev-notes/homelab/zfs-for-dummies/</a>, See on <a href="https://news.ycombinator.com/item?id=37387392">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="main">
              <article>
                
                  

  
    <a href="https://github.com/ikrima/gamedevguide/blob/master/docs/dev-notes/homelab/zfs-for-dummies.md" title="Edit this page">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/ikrima/gamedevguide/raw/master/docs/dev-notes/homelab/zfs-for-dummies.md" title="View source of this page">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"></path></svg>
    </a>
  



<p>As mentioned on previous posts, I have spent the past few weeks dealing with a ZFS crash on my FreeNAS install. Because of that, not only was I forced to learn how to troubleshoot ZFS, but I also had to learn how to setup new volumes and come up with new backup strategies (between a few other things).</p>
<p>This was a great opportunity for me to learn more about ZFS (because I new ‚Äònada‚Äô to start with). And I‚Äôm happy to share some of the knowledge that I gathered with you on this post.</p>
<p>Please keep in mind that I don‚Äôt consider myself an expert on ZFS (not even close), but I will try to make things simple and easy to understand for someone, who like me, is just getting started with ZFS.</p>
<h2 id="about-zfs">About ZFS<a href="#about-zfs" title="Permanent link">#</a></h2>
<h4 id="what-is-zfs-and-its-history">What is ZFS and It‚Äôs History<a href="#what-is-zfs-and-its-history" title="Permanent link">#</a></h4>
<p>ZFS is a local filesystem (i.e.: ext4, NTFS, exfat) and logical volume manager (i.e.: LVM on Linux) created by Sun Microsystems. ZFS was published under an open source license until Oracle bought Sun Microsystems and closed source the license. Because the source code was already in the open and ported to different OSs, eventually a project called ‚ÄòOpenZFS‚Äô was created, and that is the core code that is used on most Unix like systems today (Linux, FreeBSD, etc.).</p>
<h3 id="zfs-components">ZFS Components<a href="#zfs-components" title="Permanent link">#</a></h3>
<p><a href="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/zfs-components.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/zfs-components.png"></a></p>
<h4 id="vdev">vdev<a href="#vdev" title="Permanent link">#</a></h4>
<p>A vdev is composed of one or more physical drives (can also be of things other than hard drive, like files). They can be combined together in mirrors or RAIDZs.</p>
<p><a href="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/vdev.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/vdev.png"></a></p>
<p>üí° <em><strong>TIP:</strong> There are 7 different types of vdevs, and some of them (like host spare, L2ARC and ZIL) are very important.</em></p>
<h4 id="pool">Pool<a href="#pool" title="Permanent link">#</a></h4>
<p>A pool is composed of one or more vdevs and they usually contain a volume or a dataset (which you create after creating the pool). You create/define your vdevs when you create a pool (with the <code>zpool</code> command which we‚Äôll see later). This allows you to mix vdev types together to achieve other RAIDZ levels (see example below):</p>
<p><a href="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/raid10.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/raid10.png"></a></p>
<h4 id="datasets">Datasets<a href="#datasets" title="Permanent link">#</a></h4>
<p>Dataset is the filesystem part of ZFS (so far we‚Äôve seen the LVM components). Here you can define user access, quotas, compression, snapshots, etc‚Ä¶</p>
<h4 id="volume">Volume<a href="#volume" title="Permanent link">#</a></h4>
<p>Volume is the brother of datasets but in a block device representation. It provides some of the features that datasets have, but not all. Volumes can be useful to run other filesystems on top of ZFS, or to export iSCSI extents.</p>
<h3 id="raidz-types">RAIDZ Types<a href="#raidz-types" title="Permanent link">#</a></h3>
<ul>
<li>Dynamic/Simple Stripe (RAID0) - Distributes data without parity. Loosing a device means loosing all data</li>
<li>MIRROR (RAID1) - Mirrored drives. Used with 2 to 4 disks (or more)</li>
<li>RAIDZ-1 (RAID5) - Distributes parity along with the data and can lose one physical drive before a raid failure. RAIDZ requires at least 3 disks</li>
<li>RAIDZ-2 (RAID6) - Distributes parity along with the data and can lose up to 2 physical drives. RAIDZ-2 requires at least 4 disks</li>
<li>RAIDZ-3 - Distributes parity along with the data and can lose up to 3 physical drives. RAIDZ-3 requires at least 4, but should be used with no less than 5 disks</li>
</ul>
<p><a href="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/raidz-comparisson.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/raidz-comparisson.png"></a></p>
<hr>
<h2 id="commands">Commands<a href="#commands" title="Permanent link">#</a></h2>
<p>Let‚Äôs take a look at the most common commands for handling ZFS pools and filesystem. We‚Äôll use <code>/dev/sdx</code> to refer to device names, but keep in mind that using the device UUID is preferred in order to avoid boot issues due to device name changes.</p>
<h3 id="1zfs-pool-commands">1.ZFS Pool Commands<a href="#1zfs-pool-commands" title="Permanent link">#</a></h3>
<p>These are the commands related to creating vdevs and pools. We will be looking at:</p>
<ul>
<li><code>zpool create</code> - Create a pool (and vdevs)</li>
<li><code>zpool status</code> - Displays pool status</li>
<li><code>zpool list</code> - List pool and it‚Äôs details</li>
<li><code>zpool history</code> - Shows history of commands for zpool</li>
<li><code>zpool import</code>- Imports and mounts pool</li>
<li><code>zpool export</code> - Exports and unmounts pool</li>
<li><code>zpool destroy</code> - Destroy pool and all filesystems</li>
<li><code>zpool scrub</code> - Starts scrub of pool</li>
</ul>
<h4 id="11creating-a-pool-and-vdevs">1.1.Creating a Pool (and vdevs)<a href="#11creating-a-pool-and-vdevs" title="Permanent link">#</a></h4>
<p>To create a new pool we use the <code>zpool create</code> command. We specify the pool name and the device we want to use.</p>
<p>It‚Äôs basic usage is:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-0-1"><span data-linenos="1 "></span># zpool create [pool] [devices]
</span></code></pre></div>
<p>Now let‚Äôs look at different examples for this command.</p>
<h5 id="create-a-pool-on-a-single-disk"><strong>Create a pool on a single disk</strong><a href="#create-a-pool-on-a-single-disk" title="Permanent link">#</a></h5>
<p>The command below creates a pool on a single disk.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-1-1"><span data-linenos="1 "></span># zpool create tank /dev/sdb  
</span></code></pre></div>
<h5 id="create-a-dynamic-stripe-pool-on-3-disks"><strong>Create a dynamic stripe pool on 3 disks</strong><a href="#create-a-dynamic-stripe-pool-on-3-disks" title="Permanent link">#</a></h5>
<p>Remember that dynamic stripe is the same as RAID0 and that it has no parity.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-2-1"><span data-linenos="1 "></span># zpool create tank /dev/sdb /dev/sdc /dev/sdd
</span></code></pre></div>
<p>We can view the new pool with <code>zpool status</code></p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-3-1"><span data-linenos=" 1 "></span>root@ubuntu-vm:~# zpool status
</span><span id="__span-3-2"><span data-linenos=" 2 "></span>  pool: tank
</span><span id="__span-3-3"><span data-linenos=" 3 "></span> state: ONLINE
</span><span id="__span-3-4"><span data-linenos=" 4 "></span>  scan: none requested
</span><span id="__span-3-5"><span data-linenos=" 5 "></span>config:
</span><span id="__span-3-6"><span data-linenos=" 6 "></span>
</span><span id="__span-3-7"><span data-linenos=" 7 "></span>NAME        STATE     READ WRITE CKSUM
</span><span id="__span-3-8"><span data-linenos=" 8 "></span>tank        ONLINE       0     0     0
</span><span id="__span-3-9"><span data-linenos=" 9 "></span>  sdb       ONLINE       0     0     0
</span><span id="__span-3-10"><span data-linenos="10 "></span>  sdc       ONLINE       0     0     0
</span><span id="__span-3-11"><span data-linenos="11 "></span>  sdd       ONLINE       0     0     0
</span><span id="__span-3-12"><span data-linenos="12 "></span>
</span><span id="__span-3-13"><span data-linenos="13 "></span>errors: No known data errors
</span></code></pre></div>
<p>Note that the pool name is ‚Äòtank‚Äô and the vdevs are ‚Äòsdb‚Äô, ‚Äòsdc‚Äô and ‚Äòsdd‚Äô</p>
<h5 id="create-a-mirrorred-pool-on-2-disks"><strong>Create a mirrorred pool on 2 disks</strong><a href="#create-a-mirrorred-pool-on-2-disks" title="Permanent link">#</a></h5>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-4-1"><span data-linenos="1 "></span># zpool create tank mirror sdb sdc
</span></code></pre></div>
<p>Note that I can omit <code>/dev</code> and give the device name. Let‚Äôs view the result.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-5-1"><span data-linenos=" 1 "></span># zpool status
</span><span id="__span-5-2"><span data-linenos=" 2 "></span>  pool: tank
</span><span id="__span-5-3"><span data-linenos=" 3 "></span> state: ONLINE
</span><span id="__span-5-4"><span data-linenos=" 4 "></span>  scan: none requested
</span><span id="__span-5-5"><span data-linenos=" 5 "></span>config:
</span><span id="__span-5-6"><span data-linenos=" 6 "></span>
</span><span id="__span-5-7"><span data-linenos=" 7 "></span>NAME        STATE     READ WRITE CKSUM
</span><span id="__span-5-8"><span data-linenos=" 8 "></span>tank        ONLINE       0     0     0
</span><span id="__span-5-9"><span data-linenos=" 9 "></span>  mirror-0  ONLINE       0     0     0
</span><span id="__span-5-10"><span data-linenos="10 "></span>    sdb     ONLINE       0     0     0
</span><span id="__span-5-11"><span data-linenos="11 "></span>    sdc     ONLINE       0     0     0
</span><span id="__span-5-12"><span data-linenos="12 "></span>
</span><span id="__span-5-13"><span data-linenos="13 "></span>errors: No known data errors
</span></code></pre></div>
<p>Our vdev is ‚Äòmirror-0‚Äô and our pool is tank.</p>
<h5 id="create-a-raid-z-pool"><strong>Create a RAID-Z pool</strong><a href="#create-a-raid-z-pool" title="Permanent link">#</a></h5>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-6-1"><span data-linenos="1 "></span># zpool create tank raidz sdb sdc sdd
</span></code></pre></div>
<p>And the result indicating that my vdev is RAIDZ1.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-7-1"><span data-linenos=" 1 "></span>root@ubuntu-vm:~# zpool status
</span><span id="__span-7-2"><span data-linenos=" 2 "></span>  pool: tank
</span><span id="__span-7-3"><span data-linenos=" 3 "></span> state: ONLINE
</span><span id="__span-7-4"><span data-linenos=" 4 "></span>  scan: none requested
</span><span id="__span-7-5"><span data-linenos=" 5 "></span>config:
</span><span id="__span-7-6"><span data-linenos=" 6 "></span>
</span><span id="__span-7-7"><span data-linenos=" 7 "></span>NAME        STATE     READ WRITE CKSUM
</span><span id="__span-7-8"><span data-linenos=" 8 "></span>tank        ONLINE       0     0     0
</span><span id="__span-7-9"><span data-linenos=" 9 "></span>  raidz1-0  ONLINE       0     0     0
</span><span id="__span-7-10"><span data-linenos="10 "></span>    sdb     ONLINE       0     0     0
</span><span id="__span-7-11"><span data-linenos="11 "></span>    sdc     ONLINE       0     0     0
</span><span id="__span-7-12"><span data-linenos="12 "></span>    sdd     ONLINE       0     0     0
</span><span id="__span-7-13"><span data-linenos="13 "></span>
</span><span id="__span-7-14"><span data-linenos="14 "></span>errors: No known data errors
</span></code></pre></div>
<p>You can use the same command to create RAIDZ2,3 pools.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-8-1"><span data-linenos="1 "></span># zpool create [pool name] raidz[1,2,3] [devices]
</span></code></pre></div>
<h5 id="specifying-a-default-mount-point-for-the-pool"><strong>Specifying a default mount point for the pool</strong><a href="#specifying-a-default-mount-point-for-the-pool" title="Permanent link">#</a></h5>
<p>You can also specify the default mount point for the pool by using the <code>-m</code> flag as you create it.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-9-1"><span data-linenos="1 "></span># zpool create tank -m /mnt/tank mirror sdb sdc
</span></code></pre></div>
<p>We can see that our new pool was created and mounted at <code>/mnt/tank</code></p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-10-1"><span data-linenos="1 "></span># zfs list
</span><span id="__span-10-2"><span data-linenos="2 "></span>NAME   USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-10-3"><span data-linenos="3 "></span>tank    99K  4.36G       24K  /mnt/tank
</span></code></pre></div>
<p>üí° <em><strong>TIP:</strong> Also read up on the <code>zpool add</code> command.</em></p>
<h4 id="12getting-pool-status">1.2.Getting Pool Status<a href="#12getting-pool-status" title="Permanent link">#</a></h4>
<p>After we create a new pool it‚Äôs automatically imported into our system. As we have seen before, we can view details of the pool with the <code>zpool status</code> command.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-11-1"><span data-linenos=" 1 "></span># zpool status tank
</span><span id="__span-11-2"><span data-linenos=" 2 "></span>  pool: tank
</span><span id="__span-11-3"><span data-linenos=" 3 "></span> state: ONLINE
</span><span id="__span-11-4"><span data-linenos=" 4 "></span>  scan: none requested
</span><span id="__span-11-5"><span data-linenos=" 5 "></span>config:
</span><span id="__span-11-6"><span data-linenos=" 6 "></span>
</span><span id="__span-11-7"><span data-linenos=" 7 "></span>NAME        STATE     READ WRITE CKSUM
</span><span id="__span-11-8"><span data-linenos=" 8 "></span>tank        ONLINE       0     0     0
</span><span id="__span-11-9"><span data-linenos=" 9 "></span>  mirror-0  ONLINE       0     0     0
</span><span id="__span-11-10"><span data-linenos="10 "></span>    sdb     ONLINE       0     0     0
</span><span id="__span-11-11"><span data-linenos="11 "></span>    sdc     ONLINE       0     0     0
</span><span id="__span-11-12"><span data-linenos="12 "></span>
</span><span id="__span-11-13"><span data-linenos="13 "></span>errors: No known data errors
</span></code></pre></div>
<p>Some of the fields we did not look before are:</p>
<ul>
<li><code>state:</code> Indicates if pool is online or not</li>
<li><code>status:</code> Additional information about the pool</li>
<li><code>action:</code> Indicates if there are any pending actions for the pool</li>
<li><code>scan:</code> If a scrub is in progress or the last scrub run status</li>
<li><code>errors:</code> Indicates if there are any problems with the pool</li>
</ul>
<p>For example:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-12-1"><span data-linenos=" 1 "></span># zpool status tank
</span><span id="__span-12-2"><span data-linenos=" 2 "></span>  pool: tank
</span><span id="__span-12-3"><span data-linenos=" 3 "></span> state: ONLINE
</span><span id="__span-12-4"><span data-linenos=" 4 "></span>status: Some supported features are not enabled on the pool. The pool can
</span><span id="__span-12-5"><span data-linenos=" 5 "></span>        still be used, but some features are unavailable.
</span><span id="__span-12-6"><span data-linenos=" 6 "></span>action: Enable all features using 'zpool upgrade'. Once this is done,
</span><span id="__span-12-7"><span data-linenos=" 7 "></span>        the pool may no longer be accessible by software that does not support
</span><span id="__span-12-8"><span data-linenos=" 8 "></span>        the features. See zpool-features(7) for details.
</span><span id="__span-12-9"><span data-linenos=" 9 "></span>  scan: scrub repaired 0 in 0 days 03:37:12 with 0 errors on Wed Oct 28 03:37:13 2020
</span><span id="__span-12-10"><span data-linenos="10 "></span>config:
</span><span id="__span-12-11"><span data-linenos="11 "></span>
</span><span id="__span-12-12"><span data-linenos="12 "></span>NAME        STATE     READ WRITE CKSUM
</span><span id="__span-12-13"><span data-linenos="13 "></span>tank        ONLINE       0     0     0
</span><span id="__span-12-14"><span data-linenos="14 "></span>  mirror-0  ONLINE       0     0     0
</span><span id="__span-12-15"><span data-linenos="15 "></span>    sdb     ONLINE       0     0     0
</span><span id="__span-12-16"><span data-linenos="16 "></span>    sdc     ONLINE       0     0     0
</span><span id="__span-12-17"><span data-linenos="17 "></span>
</span><span id="__span-12-18"><span data-linenos="18 "></span>errors: No known data errors
</span></code></pre></div>
<p>Another example:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-13-1"><span data-linenos=" 1 "></span># zpool status -v tank
</span><span id="__span-13-2"><span data-linenos=" 2 "></span>  pool: tank
</span><span id="__span-13-3"><span data-linenos=" 3 "></span> state: ONLINE
</span><span id="__span-13-4"><span data-linenos=" 4 "></span>status: One or more devices has experienced an error resulting in data
</span><span id="__span-13-5"><span data-linenos=" 5 "></span>    corruption.  Applications may be affected.
</span><span id="__span-13-6"><span data-linenos=" 6 "></span>action: Restore the file in question if possible.  Otherwise restore the
</span><span id="__span-13-7"><span data-linenos=" 7 "></span>    entire pool from backup.
</span><span id="__span-13-8"><span data-linenos=" 8 "></span>   see: http://illumos.org/msg/ZFS-8000-8A
</span><span id="__span-13-9"><span data-linenos=" 9 "></span>  scan: scrub repaired 0 in 0 days 04:21:43 with 0 errors on Sun Feb 23 04:21:45 2020
</span><span id="__span-13-10"><span data-linenos="10 "></span>config:
</span><span id="__span-13-11"><span data-linenos="11 "></span>
</span><span id="__span-13-12"><span data-linenos="12 "></span>NAME        STATE     READ WRITE CKSUM
</span><span id="__span-13-13"><span data-linenos="13 "></span>tank        ONLINE       0     0     0
</span><span id="__span-13-14"><span data-linenos="14 "></span>  mirror-0  ONLINE       0     0     0
</span><span id="__span-13-15"><span data-linenos="15 "></span>    sdb     ONLINE       0     0     0
</span><span id="__span-13-16"><span data-linenos="16 "></span>    sdc     ONLINE       0     0     0
</span><span id="__span-13-17"><span data-linenos="17 "></span>
</span><span id="__span-13-18"><span data-linenos="18 "></span>errors: Permanent errors have been detected in the following files:
</span><span id="__span-13-19"><span data-linenos="19 "></span>
</span><span id="__span-13-20"><span data-linenos="20 "></span>        tank:&lt;0xdcca&gt;
</span></code></pre></div>
<h4 id="13listing-pools">1.3.Listing Pools<a href="#13listing-pools" title="Permanent link">#</a></h4>
<p>As we have seen before, we can view some details of the pool with the <code>zpool status</code> command. But there are other commands, like <code>zpool list</code> that can give us information about the pool.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-14-1"><span data-linenos="1 "></span># zpool list {pool name}
</span></code></pre></div>
<p>On the example below, we look at the details for our mirrored tank pool:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-15-1"><span data-linenos="1 "></span># zpool list
</span><span id="__span-15-2"><span data-linenos="2 "></span>NAME   SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
</span><span id="__span-15-3"><span data-linenos="3 "></span>tank  4.50G   117K  4.50G        -         -     0%     0%  1.00x    ONLINE  -
</span></code></pre></div>
<h4 id="14show-pool-history">1.4.Show Pool History<a href="#14show-pool-history" title="Permanent link">#</a></h4>
<p>This is another useful command that displays the history of commands that were executed against a pool from it‚Äôs creation (of course only commands that make changes to the pool‚Äôs configuration).</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-16-1"><span data-linenos="1 "></span># zpool history tank
</span><span id="__span-16-2"><span data-linenos="2 "></span>History for 'tank':
</span><span id="__span-16-3"><span data-linenos="3 "></span>2020-11-02.15:02:53 zpool create tank -m /mnt/tank mirror sdb sdc
</span><span id="__span-16-4"><span data-linenos="4 "></span>2020-11-02.15:50:43 zpool scrub tank
</span><span id="__span-16-5"><span data-linenos="5 "></span>2020-11-02.15:53:30 zfs set compression=lz4 tank
</span><span id="__span-16-6"><span data-linenos="6 "></span>2020-11-02.15:54:03 zpool scrub tank
</span></code></pre></div>
<h4 id="15importing-pools">1.5.Importing Pools<a href="#15importing-pools" title="Permanent link">#</a></h4>
<p>Usually after creating a pool it‚Äôs set to import and mount automatically, but you may encounter scenarios where you need to manually import a pool (like when troubleshooting or after re-imaging a system).</p>
<p>Note that the import command will also mount the pool.</p>
<h5 id="lists-pools-available-to-import"><strong>Lists pools available to import</strong><a href="#lists-pools-available-to-import" title="Permanent link">#</a></h5>
<p>Running the <code>zpool import</code> command without a pool name will show you a list of pools that can be imported.</p>
<p>Example of when no pools are available to be imported.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-17-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zpool import
</span><span id="__span-17-2"><span data-linenos="2 "></span>no pools available to import
</span></code></pre></div>
<p>Here we have a pool that can be imported.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-18-1"><span data-linenos=" 1 "></span>root@ubuntu-vm:~# zpool import
</span><span id="__span-18-2"><span data-linenos=" 2 "></span>   pool: tank
</span><span id="__span-18-3"><span data-linenos=" 3 "></span>     id: 2008489828128587072
</span><span id="__span-18-4"><span data-linenos=" 4 "></span>  state: ONLINE
</span><span id="__span-18-5"><span data-linenos=" 5 "></span> action: The pool can be imported using its name or numeric identifier.
</span><span id="__span-18-6"><span data-linenos=" 6 "></span> config:
</span><span id="__span-18-7"><span data-linenos=" 7 "></span>
</span><span id="__span-18-8"><span data-linenos=" 8 "></span>tank        ONLINE
</span><span id="__span-18-9"><span data-linenos=" 9 "></span>  mirror-0  ONLINE
</span><span id="__span-18-10"><span data-linenos="10 "></span>    sdb     ONLINE
</span><span id="__span-18-11"><span data-linenos="11 "></span>    sdc     ONLINE
</span></code></pre></div>
<h5 id="importing-the-pool"><strong>Importing the pool</strong><a href="#importing-the-pool" title="Permanent link">#</a></h5>
<p>Give the command a pool name and it will be imported.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-19-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zpool import tank
</span><span id="__span-19-2"><span data-linenos="2 "></span>
</span><span id="__span-19-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zpool list
</span><span id="__span-19-4"><span data-linenos="4 "></span>NAME   SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
</span><span id="__span-19-5"><span data-linenos="5 "></span>tank  4.50G   147K  4.50G        -         -     0%     0%  1.00x    ONLINE  -
</span></code></pre></div>
<p>You can also import all available pools by using the <code>-a</code> option.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-20-1"><span data-linenos="1 "></span># zpool import -a
</span></code></pre></div>
<h5 id="importing-a-pool-with-an-alternate-root-location"><strong>Importing a Pool with an Alternate Root Location</strong><a href="#importing-a-pool-with-an-alternate-root-location" title="Permanent link">#</a></h5>
<p>Use the <code>-R</code> flag to mount the pool to an alternate root location. Note that this is not the mount path for the pool, but an alternate root folder.</p>
<p><em><code>tank</code> is by default configured to be mounted at <code>/mnt/tank</code></em></p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-21-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zpool import -R /mnt/tank2 tank
</span><span id="__span-21-2"><span data-linenos="2 "></span>
</span><span id="__span-21-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zfs list
</span><span id="__span-21-4"><span data-linenos="4 "></span>NAME   USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-21-5"><span data-linenos="5 "></span>tank   117K  4.36G       24K  /mnt/tank2/mnt/tank
</span></code></pre></div>
<h4 id="16exporting-the-pool">1.6.Exporting the Pool<a href="#16exporting-the-pool" title="Permanent link">#</a></h4>
<p>As expected, this is the opposite of the import command. The export command attempts to unmount any mounted file systems within the pool before continuing.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-22-1"><span data-linenos="1 "></span># zpool export [pool name]
</span></code></pre></div>
<p>For example:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-23-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zpool export tank
</span><span id="__span-23-2"><span data-linenos="2 "></span>
</span><span id="__span-23-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zpool list
</span><span id="__span-23-4"><span data-linenos="4 "></span>no pools available
</span></code></pre></div>
<p>If any of the file systems fail to unmount you can forcefully unmount them by using the <code>-f</code> option. However, if ZFS volumes exist and are in use, even with <code>-f</code> it will fail to export.</p>
<h4 id="17destroyingdeleting-pools">1.7.Destroying/Deleting Pools<a href="#17destroyingdeleting-pools" title="Permanent link">#</a></h4>
<p>We can use the <code>zfs destroy</code> command to delete pools and all it‚Äôs child datasets and/or volumes.</p>
<p>‚ö†Ô∏è <strong>WARNING:</strong> <em>This will delete all your data, including any snapshots your may have.</em></p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-24-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zpool destroy tank
</span><span id="__span-24-2"><span data-linenos="2 "></span>
</span><span id="__span-24-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zpool list
</span><span id="__span-24-4"><span data-linenos="4 "></span>no pools available
</span><span id="__span-24-5"><span data-linenos="5 "></span>
</span><span id="__span-24-6"><span data-linenos="6 "></span>root@ubuntu-vm:~# zpool import
</span><span id="__span-24-7"><span data-linenos="7 "></span>no pools available to import
</span></code></pre></div>
<h4 id="18scrubbing-pools">1.8.Scrubbing Pools<a href="#18scrubbing-pools" title="Permanent link">#</a></h4>
<p>ZFS scrub checks every block in a pool against its known checksum to make sure that the data is valid. If you have vdevs with parity, ZFS scrub will also repair the data using healthy data from other disks. Scrubs should run on a schedule to make sure your systems stays healthy.</p>
<h5 id="initiating-a-scrub"><strong>Initiating a scrub</strong><a href="#initiating-a-scrub" title="Permanent link">#</a></h5>
<p>Initiating a scrub of a pool is as simple as running:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-25-1"><span data-linenos="1 "></span># zpool scrub [pool]
</span></code></pre></div>
<h5 id="checking-the-status-of-a-scrub"><strong>Checking the status of a scrub</strong><a href="#checking-the-status-of-a-scrub" title="Permanent link">#</a></h5>
<p>You can check the status of a scrub with <code>zpool status</code> and looking for messages in the ‚Äòscan‚Äô section.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-26-1"><span data-linenos=" 1 "></span>root@ubuntu-vm:/mnt/tank# zpool status tank
</span><span id="__span-26-2"><span data-linenos=" 2 "></span>  pool: tank
</span><span id="__span-26-3"><span data-linenos=" 3 "></span> state: ONLINE
</span><span id="__span-26-4"><span data-linenos=" 4 "></span>  scan: scrub repaired 0B in 0 days 00:00:03 with 0 errors on Tue Nov  3 16:26:23 2020
</span><span id="__span-26-5"><span data-linenos=" 5 "></span>config:
</span><span id="__span-26-6"><span data-linenos=" 6 "></span>
</span><span id="__span-26-7"><span data-linenos=" 7 "></span>NAME        STATE     READ WRITE CKSUM
</span><span id="__span-26-8"><span data-linenos=" 8 "></span>tank        ONLINE       0     0     0
</span><span id="__span-26-9"><span data-linenos=" 9 "></span>  mirror-0  ONLINE       0     0     0
</span><span id="__span-26-10"><span data-linenos="10 "></span>    sdb     ONLINE       0     0     0
</span><span id="__span-26-11"><span data-linenos="11 "></span>    sdc     ONLINE       0     0     0
</span><span id="__span-26-12"><span data-linenos="12 "></span>
</span><span id="__span-26-13"><span data-linenos="13 "></span>errors: No known data errors
</span></code></pre></div>
<h5 id="stopping-a-scrub"><strong>Stopping a scrub</strong><a href="#stopping-a-scrub" title="Permanent link">#</a></h5>
<p>Use the <code>-s</code> flag.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-27-1"><span data-linenos="1 "></span># zpool scrub -s [pool]
</span></code></pre></div>
<h3 id="2zfs-filesystem-commands">2.ZFS Filesystem Commands<a href="#2zfs-filesystem-commands" title="Permanent link">#</a></h3>
<p>Now we will look at the commands that will help us work with filesystems (datasets) and volumes. We will concentrate more on the filesystem side of things and will not cover volumes.</p>
<p>The commands we will review are:</p>
<ul>
<li><code>zfs create</code> - Creates a new volume or filesystem</li>
<li><code>zfs mount/umount</code> - Mounts the filesystem</li>
<li><code>zfs list</code> - Lists datasets and snapshots</li>
<li><code>zfs get/set</code> - Gets configuration and sets configuration for the dataset</li>
<li><code>zfs snapshot</code> - Handles snapshots</li>
<li><code>zfs diff</code> - Used to compare data between snapshot</li>
<li><code>zfs rollback</code> - Rolls back a snapshot</li>
<li><code>zfs send/recv</code> - Sends a snapshot as a stream of data</li>
<li><code>zfs destroy</code> - Deletes datasets and snapshots</li>
</ul>
<h4 id="21-creating-datasets">2.1. Creating Datasets<a href="#21-creating-datasets" title="Permanent link">#</a></h4>
<p>We can create datasets with the <code>zfs create</code> command. Here we create ‚Äòdataset1‚Äô as child of the ‚Äòtank‚Äô dataset (that was created automatically with the <code>zpool create</code> command).</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-28-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs create tank/dataset1
</span><span id="__span-28-2"><span data-linenos="2 "></span>
</span><span id="__span-28-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zfs list
</span><span id="__span-28-4"><span data-linenos="4 "></span>NAME            USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-28-5"><span data-linenos="5 "></span>tank            145K  9.36G     30.6K  /tank
</span><span id="__span-28-6"><span data-linenos="6 "></span>tank/dataset1  30.6K  9.36G     30.6K  /tank/dataset1
</span></code></pre></div>
<h5 id="creating-missing-parent-datasets"><strong>Creating missing parent datasets</strong><a href="#creating-missing-parent-datasets" title="Permanent link">#</a></h5>
<p>We can also create missing parent datasets with the <code>-p</code> flag (similar to <code>mkdir -p</code>).</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-29-1"><span data-linenos=" 1 "></span>root@ubuntu-vm:~# zfs create tank/dataset1/childset1/childset2
</span><span id="__span-29-2"><span data-linenos=" 2 "></span>cannot create 'tank/dataset1/childset1/childset2': parent does not exist
</span><span id="__span-29-3"><span data-linenos=" 3 "></span>
</span><span id="__span-29-4"><span data-linenos=" 4 "></span>root@ubuntu-vm:~# zfs create -p tank/dataset1/childset1/childset2
</span><span id="__span-29-5"><span data-linenos=" 5 "></span>
</span><span id="__span-29-6"><span data-linenos=" 6 "></span>root@ubuntu-vm:~# zfs list
</span><span id="__span-29-7"><span data-linenos=" 7 "></span>NAME                                USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-29-8"><span data-linenos=" 8 "></span>tank                                249K  9.36G     30.6K  /tank
</span><span id="__span-29-9"><span data-linenos=" 9 "></span>tank/dataset1                      30.6K  9.36G     30.6K  /tank/dataset1
</span><span id="__span-29-10"><span data-linenos="10 "></span>tank/dataset1/childset1            61.3K  9.36G     30.6K  /tank/dataset1/childset1
</span><span id="__span-29-11"><span data-linenos="11 "></span>tank/dataset1/childset1/childset2  30.6K  9.36G     30.6K  /tank/dataset1/childset1/childset2
</span></code></pre></div>
<h4 id="22-mounting-filesystems-datasets">2.2. Mounting Filesystems (Datasets)<a href="#22-mounting-filesystems-datasets" title="Permanent link">#</a></h4>
<p>We can use the <code>zfs mount/unmount</code> commands to view the current mount points as well as mounting/unmounting filesystems.</p>
<h5 id="viewing-current-mounted-filesystems"><strong>Viewing current mounted filesystems</strong><a href="#viewing-current-mounted-filesystems" title="Permanent link">#</a></h5>
<p>Without any arguments, <code>zfs mount</code> will display all mounted zfs filesystems and their respective mount points (without the child datasets).</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-30-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs mount
</span><span id="__span-30-2"><span data-linenos="2 "></span>tank                            /tank
</span></code></pre></div>
<h5 id="mounting-filesystems"><strong>Mounting filesystems</strong><a href="#mounting-filesystems" title="Permanent link">#</a></h5>
<p>Use <code>zfs mount [pool|dataset]</code> to mount filesystems. On the example below we use <code>zfs mount</code> to establish that no datasets are mounted, and then we mount the ‚Äòtank‚Äô dataset and confirm that is mounted with <code>zfs mount</code>.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-31-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs mount
</span><span id="__span-31-2"><span data-linenos="2 "></span>
</span><span id="__span-31-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zfs mount tank
</span><span id="__span-31-4"><span data-linenos="4 "></span>
</span><span id="__span-31-5"><span data-linenos="5 "></span>root@ubuntu-vm:~# zfs mount
</span><span id="__span-31-6"><span data-linenos="6 "></span>tank                            /tank
</span></code></pre></div>
<p>Use the <code>-a</code> option to mount all filesystems.</p>
<h5 id="mount-a-child-dataset"><strong>Mount a child dataset</strong><a href="#mount-a-child-dataset" title="Permanent link">#</a></h5>
<p>You can also mount a child dataset without the parent datasets. For example, here we confirm that ‚Äòtank‚Äô is not mounted, then we look at the available datasets, and we execute the command to mount the <code>tank/dataset2/childset2</code> dataset only.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-32-1"><span data-linenos=" 1 "></span>root@ubuntu-vm:~# zfs mount
</span><span id="__span-32-2"><span data-linenos=" 2 "></span>
</span><span id="__span-32-3"><span data-linenos=" 3 "></span>root@ubuntu-vm:~# zfs list
</span><span id="__span-32-4"><span data-linenos=" 4 "></span>NAME                                USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-32-5"><span data-linenos=" 5 "></span>tank                                249K  9.36G     30.6K  /tank
</span><span id="__span-32-6"><span data-linenos=" 6 "></span>tank/dataset1                      30.6K  9.36G     30.6K  /tank/dataset1
</span><span id="__span-32-7"><span data-linenos=" 7 "></span>tank/dataset2                      91.9K  9.36G     30.6K  /tank/dataset2
</span><span id="__span-32-8"><span data-linenos=" 8 "></span>tank/dataset2/childset2            61.3K  9.36G     30.6K  /tank/dataset2/childset2
</span><span id="__span-32-9"><span data-linenos=" 9 "></span>tank/dataset2/childset2/childset2  30.6K  9.36G     30.6K  /tank/dataset2/childset2/childset2
</span><span id="__span-32-10"><span data-linenos="10 "></span>
</span><span id="__span-32-11"><span data-linenos="11 "></span>root@ubuntu-vm:~# zfs mount tank/dataset2/childset2
</span><span id="__span-32-12"><span data-linenos="12 "></span>
</span><span id="__span-32-13"><span data-linenos="13 "></span>root@ubuntu-vm:~# zfs mount
</span><span id="__span-32-14"><span data-linenos="14 "></span>tank/dataset2/childset2         /tank/dataset2/childset2
</span></code></pre></div>
<p>Note that this will create the required path in the OS filesystem to mount the child dataset. If you decide to mount the parent dataset later you may run into a <code>directory is not empty</code> error because of the created directories.</p>
<h5 id="unmounting-filesystems"><strong>Unmounting filesystems</strong><a href="#unmounting-filesystems" title="Permanent link">#</a></h5>
<p>Run<code>zfs unmount</code> and specify the dataset name.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-33-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs mount
</span><span id="__span-33-2"><span data-linenos="2 "></span>tank                            /tank
</span><span id="__span-33-3"><span data-linenos="3 "></span>
</span><span id="__span-33-4"><span data-linenos="4 "></span>root@ubuntu-vm:~# zfs unmount tank
</span><span id="__span-33-5"><span data-linenos="5 "></span>
</span><span id="__span-33-6"><span data-linenos="6 "></span>root@ubuntu-vm:~# zfs mount
</span></code></pre></div>
<h4 id="23-listing-filesystems-datasets">2.3. Listing Filesystems (Datasets)<a href="#23-listing-filesystems-datasets" title="Permanent link">#</a></h4>
<p>You can list the dataset by running <code>zfs list [dataset name]</code>.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-34-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs list tank
</span><span id="__span-34-2"><span data-linenos="2 "></span>NAME   USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-34-3"><span data-linenos="3 "></span>tank   253K  9.36G     30.6K  /tank
</span></code></pre></div>
<p>And you can also specify the mount point as an argument.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-35-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs list /tank
</span><span id="__span-35-2"><span data-linenos="2 "></span>NAME   USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-35-3"><span data-linenos="3 "></span>tank   253K  9.36G     30.6K  /tank
</span></code></pre></div>
<p>If run without a dataset name, <code>zfs list</code> will show all datasets in the system recursively.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-36-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs list
</span><span id="__span-36-2"><span data-linenos="2 "></span>NAME                                USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-36-3"><span data-linenos="3 "></span>tank                                253K  9.36G     30.6K  /tank
</span><span id="__span-36-4"><span data-linenos="4 "></span>tank/dataset1                      30.6K  9.36G     30.6K  /tank/dataset1
</span><span id="__span-36-5"><span data-linenos="5 "></span>tank/dataset2                      91.9K  9.36G     30.6K  /tank/dataset2
</span><span id="__span-36-6"><span data-linenos="6 "></span>tank/dataset2/childset2            61.3K  9.36G     30.6K  /tank/dataset2/childset2
</span><span id="__span-36-7"><span data-linenos="7 "></span>tank/dataset2/childset2/childset2  30.6K  9.36G     30.6K  /tank/dataset2/childset2/childset2
</span></code></pre></div>
<p>üí° <em><strong>TIP:</strong> when specifying a dataset name you can also use the <code>-r</code> flag to display the dataset recursively.</em></p>
<h4 id="24-getting-and-setting-dataset-properties">2.4. Getting and Setting Dataset Properties<a href="#24-getting-and-setting-dataset-properties" title="Permanent link">#</a></h4>
<p>Properties control the behavior of filesystems, volumes, snapshots, and clones. ZFS properties can look similar to mount options.</p>
<h5 id="getting-a-list-of-all-the-properties-for-a-dataset"><strong>Getting a list of all the properties for a dataset</strong><a href="#getting-a-list-of-all-the-properties-for-a-dataset" title="Permanent link">#</a></h5>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-37-1"><span data-linenos="1 "></span># zfs get all [dataset]
</span></code></pre></div>
<h5 id="getting-the-current-value-for-a-specific-property"><strong>Getting the current value for a specific property</strong><a href="#getting-the-current-value-for-a-specific-property" title="Permanent link">#</a></h5>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-38-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs get compression tank
</span><span id="__span-38-2"><span data-linenos="2 "></span>NAME  PROPERTY     VALUE     SOURCE
</span><span id="__span-38-3"><span data-linenos="3 "></span>tank  compression  off       default
</span></code></pre></div>
<h5 id="setting-a-property-value"><strong>Setting a property value</strong><a href="#setting-a-property-value" title="Permanent link">#</a></h5>
<p>Use the <code>zfs set</code> command.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-39-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs set compression=lz4 tank
</span><span id="__span-39-2"><span data-linenos="2 "></span>
</span><span id="__span-39-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zfs get compression tank
</span><span id="__span-39-4"><span data-linenos="4 "></span>NAME  PROPERTY     VALUE     SOURCE
</span><span id="__span-39-5"><span data-linenos="5 "></span>tank  compression  lz4       local
</span></code></pre></div>
<h4 id="25-creating-snapshots">2.5. Creating Snapshots<a href="#25-creating-snapshots" title="Permanent link">#</a></h4>
<p>Snapshots allow you to save the state of a filesystem to a current point in time, without duplicating storage (files are not copied). It flags existing data as ¬´read-only¬ª while allowing new data to be added to the filesystem without affecting the existing data blocks that are protected by the snapshot (the whole process is a bit more complicated than this).</p>
<p>Let‚Äôs take a look at the image below as an example. You have a filesystem with existing data (Data A) and you take a snapshot (snapshot 1). Then you make some changes, add new files (Data B) and take another snapshot (snapthot 2). After that you make even more changes (Data C).</p>
<p><a href="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/snapshot.1.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/snapshot.1.png"></a></p>
<p>Snapshot 1 protects the original data (Data A), while snapshot 2 protects Data Changes (B) as well as the original data (Data A). So you can delete snapshot 1 and data (A) will still be protected.</p>
<p><a href="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/snapshot.2.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/snapshot.2.png"></a></p>
<p><em><strong>Note:</strong> The amount of data used for the snapshots is very small because we are not copying the files, but instead the filesystem top-level metadata block indicating the they belong to a snapshot.</em></p>
<p>And here are a few scenarios of what happens when you delete files and snapshots:</p>
<p><a href="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/snapshot.3.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://ikrima.dev/dev-notes/_assets/homelab/zfs-for-dummies/snapshot.3.png"></a></p>
<p>Snapshots are great for testing software development, or creating a failsafe before an upgrade. But by no means they should be considered (by itself) as a backup or DR solution.</p>
<h5 id="creating-a-snapshot"><strong>Creating a snapshot</strong><a href="#creating-a-snapshot" title="Permanent link">#</a></h5>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-40-1"><span data-linenos="1 "></span>zfs snapshot create [pool/dataset@snapshot_name]
</span></code></pre></div>
<p>For example:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-41-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs snapshot tank/dataset1@snapshot1
</span><span id="__span-41-2"><span data-linenos="2 "></span>
</span><span id="__span-41-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zfs list -t snapshot
</span><span id="__span-41-4"><span data-linenos="4 "></span>NAME                   USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-41-5"><span data-linenos="5 "></span>tank/dataset1@snapshot1  17.3K      -     3.00G  -
</span></code></pre></div>
<h5 id="creating-recursive-snapshots"><strong>Creating recursive snapshots</strong><a href="#creating-recursive-snapshots" title="Permanent link">#</a></h5>
<p>If you have multiple child datasets, you can either create one snapshot of the top-level dataset (usually the pool name), or use the <code>-r</code> flag to create snapshots recursively.</p>
<p>Snapshot of the main dataset:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-42-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs snapshot tank@snapshot-master
</span><span id="__span-42-2"><span data-linenos="2 "></span>
</span><span id="__span-42-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zfs list -t snapshot
</span><span id="__span-42-4"><span data-linenos="4 "></span>NAME                   USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-42-5"><span data-linenos="5 "></span>tank@snapshot-master     0B      -     30.6K  -
</span></code></pre></div>
<p>Recursive snapshot:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-43-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs snapshot -r tank@recursive
</span><span id="__span-43-2"><span data-linenos="2 "></span>
</span><span id="__span-43-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zfs list -t snapshot
</span><span id="__span-43-4"><span data-linenos="4 "></span>NAME                      USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-43-5"><span data-linenos="5 "></span>tank@recursive              0B      -     30.6K  -
</span><span id="__span-43-6"><span data-linenos="6 "></span>tank/dataset1@recursive     0B      -     3.00G  -
</span></code></pre></div>
<h5 id="listing-snapshots"><strong>Listing snapshots</strong><a href="#listing-snapshots" title="Permanent link">#</a></h5>
<p>Use <code>zfs list -t snapshot</code>.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-44-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs list -t snapshot
</span><span id="__span-44-2"><span data-linenos="2 "></span>NAME                      USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-44-3"><span data-linenos="3 "></span>tank@recursive              0B      -     30.6K  -
</span><span id="__span-44-4"><span data-linenos="4 "></span>tank/dataset1@recursive     0B      -     3.00G  -
</span></code></pre></div>
<h4 id="26-comparing-snapshots">2.6. Comparing Snapshots<a href="#26-comparing-snapshots" title="Permanent link">#</a></h4>
<p>You can use <code>zfs diff</code> to compare snapshots.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-45-1"><span data-linenos="1 "></span># zfs diff [older snapshot] [newer snapshot]
</span></code></pre></div>
<p>For example:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-46-1"><span data-linenos="1 "></span>root@ubuntu-vm:# zfs diff tank@initial tank@second
</span><span id="__span-46-2"><span data-linenos="2 "></span>+/mnt/tank/file-1.txt
</span><span id="__span-46-3"><span data-linenos="3 "></span>+/mnt/tank/file-2.txt
</span><span id="__span-46-4"><span data-linenos="4 "></span>+/mnt/tank/file-3.txt
</span><span id="__span-46-5"><span data-linenos="5 "></span>M/mnt/tank/
</span></code></pre></div>
<h4 id="27-restoring-a-snapshot">2.7. Restoring a Snapshot<a href="#27-restoring-a-snapshot" title="Permanent link">#</a></h4>
<p>Restore a snapshots with <code>zfs rollback</code>. Note that restoring a snapshot will delete all files that were created after the snapshot (as we saw in our example). It will also delete any newer snapshots (you will be asked to use the <code>-r</code> option to rollback and delete newer snapshots).</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-47-1"><span data-linenos="1 "></span>zfs rollback [pool/dataset@snapshot_name]
</span></code></pre></div>
<h4 id="29-sending-and-receiving-snapshots">2.9. Sending and Receiving Snapshots<a href="#29-sending-and-receiving-snapshots" title="Permanent link">#</a></h4>
<p>One of the best features of ZFS is ‚ÄòZFS send‚Äô. It allows you send snapshots as a stream of data. This is a great way replicate a snapshot and it‚Äôs dataset to a file, another pool or even to another system via SSH. Amazing no!</p>
<p>Let‚Äôs look at the example below. We have 2 pools in our system named ‚Äòtank‚Äô and ‚Äòbackup‚Äô.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-48-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zpool list
</span><span id="__span-48-2"><span data-linenos="2 "></span>NAME     SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
</span><span id="__span-48-3"><span data-linenos="3 "></span>tank       9G  1.50G  7.50G        -         -     0%    16%  1.00x    ONLINE  -
</span><span id="__span-48-4"><span data-linenos="4 "></span>backup  4.50G   104K  4.50G        -         -     0%     0%  1.00x    ONLINE  -
</span></code></pre></div>
<p>In our tank pool we have a dataset for our Movies.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-49-1"><span data-linenos="1 "></span>root@ubuntu-vm:/tank/Movies# zfs list -r tank
</span><span id="__span-49-2"><span data-linenos="2 "></span>NAME          USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-49-3"><span data-linenos="3 "></span>tank         1.50G  7.22G       24K  /tank
</span><span id="__span-49-4"><span data-linenos="4 "></span>tank/Movies  1.50G  7.22G     1.50G  /tank/Movies
</span></code></pre></div>
<p>Before we can send this data we need create a snapshot:</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-50-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs snapshot tank/Movies@$(date '+%Y-%m-%d_%H-%M')
</span><span id="__span-50-2"><span data-linenos="2 "></span>
</span><span id="__span-50-3"><span data-linenos="3 "></span>root@ubuntu-vm:~# zfs list -t snapshot
</span><span id="__span-50-4"><span data-linenos="4 "></span>NAME                           USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-50-5"><span data-linenos="5 "></span>tank/Movies@2020-11-03_15-29     0B      -     1.50G  -
</span></code></pre></div>
<p>And now we can send our snapshot to our backup pool with <code>zfs send/recv</code>.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-51-1"><span data-linenos="1 "></span>root@ubuntu-vm:~# zfs send tank/Movies@2020-11-03_15-29 | zfs recv backup/Movies
</span></code></pre></div>
<p>And let‚Äôs confirm that it worked.</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-52-1"><span data-linenos=" 1 "></span>root@ubuntu-vm:~# zfs list
</span><span id="__span-52-2"><span data-linenos=" 2 "></span>NAME            USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-52-3"><span data-linenos=" 3 "></span>backup         1.50G  2.86G       24K  /backup
</span><span id="__span-52-4"><span data-linenos=" 4 "></span>backup/Movies  1.50G  2.86G     1.50G  /backup/Movies
</span><span id="__span-52-5"><span data-linenos=" 5 "></span>tank           1.50G  7.22G       24K  /tank
</span><span id="__span-52-6"><span data-linenos=" 6 "></span>tank/Movies    1.50G  7.22G     1.50G  /tank/Movies
</span><span id="__span-52-7"><span data-linenos=" 7 "></span>
</span><span id="__span-52-8"><span data-linenos=" 8 "></span>root@ubuntu-vm:~# zfs list -t snapshot
</span><span id="__span-52-9"><span data-linenos=" 9 "></span>NAME                             USED  AVAIL     REFER  MOUNTPOINT
</span><span id="__span-52-10"><span data-linenos="10 "></span>backup/Movies@2020-11-03_15-29     0B      -     1.50G  -
</span><span id="__span-52-11"><span data-linenos="11 "></span>tank/Movies@2020-11-03_15-29       0B      -     1.50G  -
</span></code></pre></div>
<p>üí° <em><strong>TIP:</strong> It‚Äôs worth to look into all the options and use cases for ZFS send. Combined with RAIDZs and snapshots, it can help you make your filesystem almost indestructible.</em></p>
<h4 id="210-destroying-filesystems-datasets-and-snapshots">2.10. Destroying Filesystems (Datasets) and Snapshots<a href="#210-destroying-filesystems-datasets-and-snapshots" title="Permanent link">#</a></h4>
<h5 id="destroying-datasets"><strong>Destroying datasets</strong><a href="#destroying-datasets" title="Permanent link">#</a></h5>
<p>To destroy a dataset, use <code>zfs destroy</code> (the <code>-r</code> flag also works here).</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-53-1"><span data-linenos="1 "></span>zfs destroy [pool/dataset]
</span></code></pre></div>
<h5 id="destroying-snapshots"><strong>Destroying snapshots</strong><a href="#destroying-snapshots" title="Permanent link">#</a></h5>
<p>To destroy a snapshot, also use the <code>zfs destroy</code> command (and the <code>-r</code> flag also works here).</p>
<div><p><span>Text Only</span></p><pre><span></span><code><span id="__span-54-1"><span data-linenos="1 "></span>zfs destroy [pool/dataset@snapshot_name]
</span></code></pre></div>
<hr>
<h2 id="conclusion">Conclusion<a href="#conclusion" title="Permanent link">#</a></h2>
<p>While we covered a lot of different topics and commands on ZFS, in reality, we really only scratched the surface on what ZFS can do. If you want to learn more about ZFS I‚Äôve added a few links below with some great reading.</p>
<hr>
<p><strong>References and additional reading:</strong></p>
<ul>
<li>FreeBSD Mastery: ZFS - <a href="https://www.goodreads.com/book/show/25595471-freebsd-mastery">https://www.goodreads.com/book/show/25595471-freebsd-mastery</a></li>
<li>Ubuntu Wiki - <a href="https://wiki.ubuntu.com/ZFS">https://wiki.ubuntu.com/ZFS</a></li>
<li>Zpool Administration - <a href="https://pthree.org/2012/04/17/install-zfs-on-debian-gnulinux/">https://pthree.org/2012/04/17/install-zfs-on-debian-gnulinux/</a></li>
<li>ZFS Build - <a href="http://www.zfsbuild.com/">http://www.zfsbuild.com/</a></li>
<li>ZFS Features and Terminology - <a href="https://www.freebsd.org/doc/handbook/zfs-term.html">https://www.freebsd.org/doc/handbook/zfs-term.html</a></li>
<li>Klennet Storage Software - <a href="https://www.klennet.com/zfs-recovery/zfs-basics.aspx">https://www.klennet.com/zfs-recovery/zfs-basics.aspx</a></li>
</ul>

  <hr>
<p><small>
    
      Last update:
      <span>2023-09-01</span>
      
        <br>
        Created:
        <span>2023-09-01</span>
      
    
  </small>
</p>


  




                
              </article>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Get a cable modem, go to jail (1999) (383 pts)]]></title>
            <link>http://telecom.csail.mit.edu/judy-sammel.html</link>
            <guid>37386397</guid>
            <pubDate>Tue, 05 Sep 2023 00:02:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://telecom.csail.mit.edu/judy-sammel.html">http://telecom.csail.mit.edu/judy-sammel.html</a>, See on <a href="https://news.ycombinator.com/item?id=37386397">Hacker News</a></p>
<div id="readability-page-1" class="page"><i>updated....April 26, 1999</i> 
<center>
<h5> Visit Judy Sammel's Homepage: <a href="http://www.pobox.com/~sammel">www.pobox.com/~sammel</a>
</h5>

</center>I've just been through a 
truly bizarre experience with Comcast@home, and I thought the assembled 
readership might be interested in hearing about it. <br>
<hr>

<h3>Chapter 1: In Which I Learn About the Various Divisions of the District 
Court of the State of Maryland</h3><b><i>Thursday, November 19, 1998:</i></b> 
It's an ordinary day. I get home from work, picking up the mail as I walk toward 
my house.&nbsp; I notice that the return address on one of the letters is the 
District Court of the State of Maryland. As I walk up the front steps, I think, 
"I just did jury duty last year; they can't be asking me to serve again so soon, 
can they?"&nbsp; After I settle in at home, I open the letter and read, "You 
have been summoned to appear as a defendant in a trial..."&nbsp;&nbsp; My first 
thought is that someone is suing me for something, but then I see at the top of 
the page 'State of Maryland vs. Sammel, Judith Lynne', and realize that this 
isn't a<b><i> person</i></b> suing me.&nbsp; My second thought is that I've 
gotten a parking ticket that blew off of my windshield and has gone unpaid, or 
maybe a speeding ticket from the traffic cameras I've heard about.&nbsp; It's 
too late to call the Court to see what the case is about, so I have to wait 
until the next day. 
<p>The next morning,&nbsp; I call the telephone number that's printed on the 
letter, and a recording says something like "press 1 for Civil Division, 2 for 
Traffic Division, 3 for Criminal Division, 4 for information about specific 
cases," etc.&nbsp; I choose the option for information about specific cases, and 
the person who answers the phone asks me to read her the case number.&nbsp; 
After I read it, I hear her say, "Just a minute.&nbsp; Let me transfer you to 
the Criminal Division." 
</p><p>"Whaaaaaaaaat?" I ask, but it's too late.&nbsp; I'm on hold. 
</p><p>Someone else picks up the phone, and again I read the case number and ask 
what the case is about. 
</p><p>"Cable fraud, " she replies. <br>
</p><hr>

<h3>Chapter 2: In Which Comcast Cablevision's Administrative Offices in 
Baltimore are Informed that "At Home" is More Than Just "A Place They'd Rather 
Be Than At The Office"</h3>Unfortunately, I think I know what the problem 
is.&nbsp; I had signed up for Comcast@home cable modem service last March, but 
had not signed up for cable TV service.&nbsp; (Last time I checked, this was 
legal.&nbsp; In fact, it is mentioned specifically as a service in the FAQ list 
on @home's web site.)&nbsp; But, when they came to install my @home service, 
they had neglected to install the filter (they call it a video trap) to filter 
out the video signal&nbsp; from reaching my home.&nbsp; (I live in a townhouse 
community, and there is a "pedestal" containing cable connections for several 
houses--this is a few houses away from mine.)&nbsp; The installers had told me 
that they would return to install the video trap, and that I could enjoy cable 
TV service until they came back to do so--they said I should consider it an 
incentive to sign up for the service.&nbsp; Well, they never came back to 
install the video trap. 
<p>Then, in early June, my @home service stopped working.&nbsp; I made several 
calls to Comcast Cablevision and Comcast@home customer service.&nbsp; Each 
claimed that it was the responsibility of the other to determine what the 
problem was, and fix it.&nbsp; During my conversations with Comcast Cablevision 
and Comcast@home, I told them both that nobody had come back to install the 
video trap.&nbsp; Finally, Comcast@home agreed to come out and see what the 
problem was.&nbsp; I told them that they should bring a video trap with them 
when they came to repair the service.&nbsp; Because of their schedule and my 
vacation schedule, they didn't come out to repair the service until late 
June.&nbsp; The repair technician, when he arrived, said that the reason that my 
@home service stopped working was that someone had disconnected the cable in the 
cable "pedestal" on my street.&nbsp; He said that it was likely that Comcast 
Cablevision personnel had done this, not realizing that I had Comcast@home 
service. When he'd reconnected my service,&nbsp; I asked him about the 
installation of the video trap.&nbsp; He said he hadn't brought one with him, 
but would take care of it later.&nbsp; Again, nobody came back to install the 
device. 
</p><p>So, after I receive this letter from the District Court, I figure that 
Comcast Cablevision has been out to my neighborhood and has seen that I was 
connected up again (and again not realized that I had Comcast@home cable modem 
service.)&nbsp; I call the administrative offices of Comcast Cablevision here in 
Baltimore, and explain the situation.&nbsp; They say that they will have 
Comcast's attorneys call me to discuss it.&nbsp; I also make the suggestion that 
they come out and install a video trap on my line so that this won't happen 
again.&nbsp; They assure me they will send someone out that day to do it.&nbsp; 
I get home fairly late that evening and find that the video trap hasn't been 
installed.&nbsp; Since the Comcast administrative offices are closed by this 
time, I call @home customer service. I call at 10:30 PM and remain on hold until 
11:15 PM, at which time a customer service representative takes the call. When I 
ask about getting a video trap placed on the line, his first response is that 
Comcast, not @home, should take care of that, and I should call Comcast customer 
service.&nbsp; I explain to him that, based on my prior experience with Comcast 
customer service, if I call them, the following situation will likely occur: 
</p><ul>
  <li>the Comcast customer service representative will ask for my Comcast 
  account number 
  </li><li>I will reply that I have none, since I am only a Comcast@home customer, 
  not a Comcast cable TV customer 
  </li><li>the service representative will tell me they can't do anything for me, and 
  I will have to call @home customer service </li></ul>He then agrees to talk to 
his supervisor.&nbsp; He returns to the line to tell me that he will place a 
work order for a video trap to be placed on the line. <br>
<hr>

<h3>Chapter 3: In Which I Wait Not-So-Patiently for Comcast's Attorney to 
Call,&nbsp; and for the Video Trap to be Installed</h3><b><i>Saturday, November 
21, 1998:</i></b> After I place two calls to their offices, Comcast's attorney 
who is assigned to my case returns my call to assure me that the criminal 
charges will be dropped. I mention that the video trap wasn't installed the 
previous day. 
<p><b><i>Sunday, November 22, 1998:</i></b> As of&nbsp; Sunday morning, neither 
Comcast nor @home has arrived to place a video trap on the line.&nbsp; I call 
Comcast customer service to ask about the status of this, and am initially told 
that since I am not a Comcast cable TV customer, I have to call @home to resolve 
any problems.&nbsp; I explain that the situation is related to criminal charges 
that Comcast has filed against me, and ask the account executive to try to find 
a way to assist me from her location.&nbsp;&nbsp; She asks for my name and 
address.&nbsp; I give it to her, and she comes back on the line after a few 
moments and says that the only name she has listed at that address is that of 
former occupants of my home (from1989-1992).&nbsp; I explain again that I am not 
a Comcast cable TV customer, but only a Comcast@home customer, so there would 
likely not be a record of my name, unless she has records of Comcast@home 
customers as well as Comcast cable TV customers.&nbsp; She again asks me to hold 
the line.&nbsp; When she returns, she says that she has checked into the 
situation with @home, and has found out that the video trap was not placed on 
the line because the part is out of stock.&nbsp; She says that when parts are 
shipped, they will send an installer out to put it in.&nbsp; She says that in 
the meantime, I cannot be held responsible for the signal coming into my home. 
</p><p><b><i>Tuesday, November 24, 1998:</i></b> Comcast's attorney leaves a message 
on my answering machine saying: "I withdrew...did a request to withdraw those 
charging documents" .&nbsp;&nbsp;&nbsp; I will soon find out the hard way that 
the "withdrew...did a request to withdraw" wording actually means 
something--Comcast doesn't have the ability to get criminal charges dropped on 
their own, even though they're the ones who got the court to file them in the 
first place.&nbsp; Once the charges are "in the system", Comcast can only 
<b><i>request</i></b> that this be done. <br>
</p><hr>

<h3>Chapter 4: In Which A Police Officer Serves Me With&nbsp; A Criminal Summons 
in Front of an Out-of-Town Houseguest on the Eve of 
Thanksgiving</h3><b><i>Wednesday, November 25, 1998:</i></b> A uniformed police 
officer drives up to my house in his police cruiser and delivers the summons to 
me; it shows 4 counts of cable fraud, with the possibility of 6 months in jail 
on each count.&nbsp; I had always wondered why, when someone commits what 
appears to be a single crime, you hear things about being charged with "10 
counts" of something-or-other. In my case, Comcast's visit in June (when they 
cut off my service) was 2 counts--one for cable having been fraudulently hooked 
up, one for fraudulent receipt of service; and a subsequent visit they made in 
July, during which they saw that service had been reconnected, was again 2 
counts.&nbsp; I try to tell the police officer what Comcast's attorney has told 
me to tell him--that the charges are being dropped, and that he should call her 
to verify this--but he could care less about this--he just wants a signature on 
the papers and wants to leave.&nbsp; A friend of mine, who has driven up from 
Virginia to spend the holiday weekend with me, is sitting nearby in my kitchen 
watching this sorry event; it's a really humiliating experience. Anyway, by the 
end of the day, I find that the video trap has finally been installed on the 
line. <br>
<hr>

<h3>Chapter 5:&nbsp; In Which I Come to the Conclusion that Lady Justice is Not 
Just Blindfolded, But Actually Blind</h3><b><i>Monday, November 30, 
1998:</i></b>&nbsp; I telephone the State's Attorney's office to see if they 
have received and processed Comcast's request to withdraw the charges.&nbsp; 
They tell me that it has been received, and that the State's Attorney's office 
has disapproved Comcast's request.&nbsp; They tell me that no reason for the 
disapproval was supplied. I begin to feel like a character in a Kafka 
novel.&nbsp; I call Comcast's administrative offices in Baltimore and tell them 
that the State's Attorney has denied the request to withdraw charges.&nbsp; 
"They can't do that!" the employee that I speak to exclaims.&nbsp; "Well, 
obviously they can, because...they have," I reply. <br>
<hr>

<h3>Chapter 6: In Which I Determine That an Excellent Way to Pique Someone's 
Curiosity is to Ask, "Do You Have any Recommendations for a Good Criminal 
Defense Attorney?"</h3>No chapter here, but it's too good of a title to pass 
up.&nbsp; Actually, I am told by some colleagues who are attorneys that even in 
a cut-and-dried case like this, I should expect to pay $750 or $1000 for an 
attorney if I need to mount a defense in court. <br>
<hr>

<h3>Chapter 7: In Which I Find Out that the Attorney for Comcast Actually Does 
Have a Sense of Humor</h3>I speak with the Comcast attorney about my call to 
State's Attorney's office. She says she was unaware that State's Attorney has 
denied the request to drop the charges, and that she will write another letter 
to ensure that the situation is taken care of.&nbsp; She also asks about whether 
the video trap has been installed and is working correctly. I tell her that 
everything is blocked except for 2 religious channels, 1 Spanish language 
channel, and the video portion of E! TV.&nbsp; She says something along the 
lines of, "I guess the signal of the Lord manages to find its way through 
somehow." <br>
<hr>

<h3>Chapter 8:&nbsp; In Which I Learn That the Cable TV Franchising Authority is 
Willing to Handle Complaints About Cable TV Issues--But Only Up To a Point</h3>A 
friend of mine had suggested that I find out who the Franchising Authority for 
cable television in Baltimore County is, and that I ask them to assist in the 
getting the problem resolved. I find out that the Baltimore County Council is 
the local Franchising Authority. On November 30, I call the County Council's 
offices and speak to an employee who is assigned to handle cable television 
issues. I explain the situation; she says she will check into it. On December 
1,&nbsp; I speak to her again, and she says she had gotten in touch with the 
Assistant to the VP at Comcast Cablevision of Baltimore, and he will call her 
back with information.&nbsp; On December 3, when I speak to her again, she says 
that the assistant to the VP had assured her that Comcast is working on the 
problem. She is satisfied with this answer, and encourages me to "call her back 
to let her know when the problem's been resolved".&nbsp; It's nice to know that 
the employees of our local elected officials are really concerned about taking 
such a proactive role in ensuring that the problems of constituents are handled 
effectively. <br>
<hr>

<h3>&nbsp;Chapter 9:&nbsp; Boy, Do I Feel Like A Criminal</h3><b><i>Thursday, 
December 3, 1998:</i></b>&nbsp; One of the other Comcast&nbsp; attorneys calls 
me to say that the only person at the State's Attorney's office who can take 
care of the situation is an Assistant State's Attorney who is the Chief of the 
District Court Division, and that he has been out of the office all week.&nbsp; 
The attorney gives me the phone number, and tells me I can try to reach him 
myself.&nbsp; I call the number a few times during the day, and am alternately 
told that he is out of the office, or that "you're not allowed to call the 
State's Attorney, you're a defendant!".&nbsp; I leave a message for him, but it 
is not returned. 
<p>Also,&nbsp; around this time, I have a bizarre discussion with the 
Comcast&nbsp; attorney to try to find out why the State's Attorney disapproved 
the request to withdraw the charges.&nbsp; She explains that procedurally, they 
cannot withdraw the charges once the trial date has been set, and the summons 
has been served.&nbsp; She tells me that I should have "avoided being 
served".&nbsp; I never thought I'd ever get myself into a situation where I had 
to make a point of avoiding process servers. Anyway, it's true that the trial 
date had already been set (it was set on the same day Comcast applied for the 
Statement of Charges, November 17, 1998).&nbsp; But, the summons was not served 
to me until Wednesday, November 25.&nbsp; The State's Attorney disapproved the 
request for withdrawal of the charges on Tuesday, November 24, the day before I 
was served.&nbsp; So, either the State's Attorney's office doesn't know its own 
procedures, doesn't bother to follow them, or perhaps there's something more 
nefarious going on; I'm still not sure. <br>
</p><hr>

<h3>Chapter 10: In Which I Get To Speak With Sandra O'Connor's 
Second-In-Command</h3><b><i>Friday, December 4, 1998:</i></b> No, not 
<b><i>that</i></b> Sandra O'Connor.&nbsp; Baltimore County has its own, local, 
Sandra O'Connor.&nbsp; She's the State's Attorney (an elected official) and I've 
actually voted for her.&nbsp; I speak with her Deputy for Operations. Since he's 
responsible for the overseeing the District Court Division, he's the one that 
the Chief of the District Court Division reports to. I explain that Comcast said 
that the Chief of the District Court Division was the only one who could take 
care of the situation, but that he's been unavailable for several days.&nbsp; I 
ask the Deputy for Operations if he can take care of the situation in his 
employee's absence. He tells me no--that I should wait until his employee 
returns on the following Monday. He tells me that there is nothing in my file to 
show that Comcast has done anything related to the case since November 24, and 
that, if Comcast has promised to get the charges dismissed,&nbsp; I should be 
"on them" to take care of it. <br>
<hr>

<h3>Chapter 11: In Which I Get to Sit and Stew About This for Another Weekend 
Because the Only Person on Earth Who Can Apparently Take Care of this Situation 
is Either in Training, on Vacation, or Simply Out of the Office, Depending Upon 
Whom You Talk To</h3><b><i>December 5-6, 1998:</i></b> My demeanor begins to 
degrade into "don't get mad, get even" mode. I do some research on what the 
elements of a case of malicious prosecution are. (And, of course, I can do all 
of my research over the Internet--at cable modem speeds.)&nbsp; Paraphrasing one 
article that I read, the elements are: <br>1.&nbsp; The people that you sue for 
malicious prosecution have to be the one that got charges filed against you in 
the first place. <br>2.&nbsp; The case has to have been decided in your favor, 
in one way or another. <br>3.&nbsp; There has to have been a lack of probable 
cause. <br>4.&nbsp; There has to have been malice. <br>5.&nbsp; There have to be 
damages. 
<p>Although "malice" in this situation might be hard to prove, one article 
published on the subject asserts that malice can be implied from a lack of 
probable cause, and from inadequate investigation and research.&nbsp; "Lack of 
probable cause" might also be difficult to prove here, but I believe that a good 
lawyer could argue that the fact that "cables have been connected and Comcast 
Cablevision was not the one who connected them" is no longer acceptable as 
probable cause, now that Comcast Cablevision has given @home (or its local 
affiliate, Comcast Online Communications) the authority to connect them. <br>
</p><hr>

<h3>Chapter 12: Things Finally Start Falling Into Place</h3><b><i>December 7, 
1998:</i></b>&nbsp; I leave a voice mail message for the General Counsel at 
Comcast's Corporate Headquarters in Philadelphia telling him I'm unhappy with 
the way the situation is being handled, and that I'm considering filing a case 
in civil court for malicious prosecution. I receive a conciliatory call back 
from him, and also from one of the local Comcast attorneys telling me that the 
situation is being taken care of promptly.&nbsp; I finally get a call from the 
Chief of the District Court Division of the State's Attorney's office saying 
they will "nol pros" the case.&nbsp; It's great; I'm getting to learn some Latin 
while I'm at it.&nbsp; "Nol pros" is short for "nolle prosequi", which is Latin 
for "I will not prosecute."&nbsp;&nbsp; I can't believe it's taken over two 
weeks just to get a verbal agreement from the State's Attorney not to prosecute 
a case that everyone involved clearly agrees was based on a Comcast error. 
<p><b><i>December 12, 1998:</i></b> I receive a courtesy copy of a letter from 
the State's Attorney to the Judge, requesting that the trial date be moved up, 
and saying that they intend to nol pros. 
</p><p><b><i>December 22, 1998:</i></b>&nbsp; I call the District Court, and find 
out that the Judge has moved the trial date up to January 12, 1999, instead of 
the original date, which was in March 1999. <br>
</p><hr>

<h3>Chapter 13:&nbsp; In Which I Get an Unsolicited Letter From a Local Criminal 
Defense Attorney</h3><b><i>December 30, 1998:</i></b>&nbsp; I come home from 
work to find another interesting letter in my mailbox; this time it's from a 
local criminal defense attorney.&nbsp;&nbsp; It reads, "Dear Ms. Sammel,&nbsp; 
When you are charged with a criminal offense, not only can your freedom and 
liberty be taken away, a criminal record can affect you the rest of your 
life."&nbsp; The attorney then proceeds to offer his services, and even provides 
me with a handy little reminder card with my trial date stamped on it.&nbsp; 
Great.&nbsp; I guess that since the court's records are considered public 
documents, anyone can come in and get copies of them.&nbsp; I'm afraid to think 
about what kinds of mailing lists I could end up on as a result of this.&nbsp; 
Will I start receiving notices asking me if I want to subscribe to <b><i>Prison 
Life</i></b> magazine? <br>
<hr>

<h3>Chapter 14:&nbsp; In Which I Go to the Trial "Just In Case"</h3>Although 
I've been told by Comcast that I don't need to show up in court on the trial 
date, I've never received notice from the District Court about this.&nbsp; And, 
considering that the last paperwork I received from the court said "a warrant 
for your arrest may be issued" if you don't show up at your trial, I decide it 
would be best to show up anyway. 
<p><b><i>January 12, 1999: </i></b>Just when I'm starting to think that 
Comcast's motto has been morphed&nbsp; into "Comcast--Everything You Convict 
With", the court appearance is pretty uneventful.&nbsp; I show up at 8:30 along 
with about a dozen other people who are involved in other cases in the same 
courtroom that morning. According to the list outside the courtroom, it looks 
like there are a couple of theft cases and a couple of drug cases on the 
schedule.&nbsp; I'm actually starting to look forward to an interesting morning 
in court, hearing about all of this stuff, but the State's Attorney calls my 
case first.&nbsp; I start walking up to the Defendant's table, and hear the 
State's Attorney say that he's entering a nol pros.&nbsp;&nbsp; I've just 
reached the table when the judge says,&nbsp; "your case is dismissed, you're 
free to go," and so I turn back around and leave the courtroom. <br>
</p><hr>

<h3>Chapter 15: In Which The Assembled Readership Gets To Find Out That I Never 
Got Sent To Jail, But Just Got Threatened With The Possibility Of&nbsp; 
It</h3>Sorry to disappoint those of you who were looking forward to hearing 
about that part. 
<p>I gave the "malicious prosecution" scenario a thought, but I'm not really a 
lawsuit kind of person.&nbsp; Unless I find out that Comcast has been involved 
in a pattern of cases like these without regard to the consequences, I don't 
intend to follow up on filing a malicious prosecution lawsuit.&nbsp; Besides, if 
I want to get my criminal record expunged, it looks like I will need to sign a 
waiver absolving them of liability for this incident.&nbsp; I am hoping, 
however, that Comcast will reimburse me for the $30 fee involved in getting my 
record expunged. <br>
</p><hr>

<h3>Epilogue:</h3>Although everyone I dealt with seemed apologetic, and willing 
to help get this fairly outrageous situation taken care of, I believe that there 
are still some problems that Comcast and @Home management need to address.&nbsp; 
For that reason, I decided to write to the Presidents of both companies, and ask 
them to respond to several issues.&nbsp; A copy of the letter is below: <br>
<hr>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
[address deleted] 
<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
January 13, 1999 
</p><p>Brian L. Roberts <br>President, Comcast Corporation <br>1500 Market Street 
<br>Philadelphia PA 19102 
</p><p>Thomas A. Jermoluk <br>President,&nbsp; @Home Network <br>425 Broadway Street 
<br>Redwood City, CA 94063 
</p><p>Dear Messrs. Roberts and Jermoluk: 
</p><p>&nbsp;I am a Comcast@home cable modem subscriber (sammel@home.com) who is not 
concurrently a subscriber of Comcast cable television.&nbsp; Because of what I 
believe to be insufficient action by each of your companies, and insufficient 
coordination between them, I was recently criminally charged with cable fraud in 
the District Court of Maryland for Baltimore County ("State of Maryland vs. 
Sammel, Judith Lynne", case number 5C00097816).&nbsp; I was charged because: 
</p><ul>
  <li>@Home installation and repair technicians failed to install protective 
  measures to block out the cable television signal when both installing and 
  repairing my cable modem service, and failed to mark the connection inside the 
  cable "pedestal" that serves my home to indicate that it was a valid @Home 
  connection; and, 
  </li><li>Comcast Cablevision did not bother to check the records of&nbsp; the @Home 
  Network to see if I was a subscriber of cable modem service before filing an 
  application for a criminal summons, after they saw that the cable wires 
  serving my home were connected. </li></ul>&nbsp;Although, after a review of the 
facts of the situation, Comcast Cablevision convinced the State's Attorney to 
enter a "nolle prosequi" (a decision not to prosecute) in this case, I am 
requesting that both of your companies review their procedures for handling 
Comcast@home subscribers who are not concurrently Comcast cable television 
subscribers, to ensure that this situation does not happen to anyone else.&nbsp; 
I would appreciate if you would both respond to me concerning the following 
issues: 
<blockquote>
  <li>What are the procedures used to mark the cable connections of 
  non-cable-TV-subscriber customers of Comcast@home to ensure that Comcast 
  technicians are aware that the connection is a valid Comcast@home connection? 
  How do you assure that these procedures are being followed? 
  </li><li>Who is responsible for installing protective measures to block out the 
  cable television signal on the lines of non-cable-TV-subscriber customers of 
  Comcast@home, and how do you assure that these protective measures are being 
  properly installed? 
  </li><li>I would like request that procedures be put in place, so that prior to 
  submitting a request to any court of law to file criminal charges for cable 
  fraud,&nbsp; Comcast Cablevision will check the records of the @Home Network 
  to see if the individual to be charged is a valid customer of 
  Comcast@home.&nbsp; I am also asking the Administrative Commissioner of the 
  District Court of Maryland for Baltimore County (who will be receiving a 
  courtesy copy of this letter) to ensure that these procedures are followed 
  prior to the issuing of any summons for cable fraud in our county.&nbsp; 
  Perhaps the Court could require some type of affirmation that this check of 
  @Home's customer records has been performed prior to determining that there is 
  "probable cause" for a summons to be issued in any cable fraud case.&nbsp; I 
  firmly believe that if Comcast Cablevision has given @Home technicians the 
  authority to connect/modify cable connections, they also need to accept some 
  responsibility for having done so, and not automatically assume that any lines 
  that Comcast Cablevision has not connected themselves have been fraudulently 
  connected by the homeowner. 
  </li><li>I would like to request that procedures be put in place by Comcast 
  Cablevision customer support so that the records of non-cable-TV-subscriber 
  customers of Comcast@home are available to them, and that they may be able to 
  respond to customer service calls related&nbsp; to the physical cable 
  connections of Comcast@home customers (instead of insisting that only @Home 
  customer support can handle these requests.) 
  </li><li>I would like to request that procedures be put in place by both Comcast 
  Cablevision and Comcast@home customer support, so that they are each fully 
  aware of what their respective company's responsibilities are with regard to 
  non-cable-TV-subscriber customers of Comcast@home. </li></blockquote>I would 
like to note that every Comcast Cablevision and Comcast@home customer service 
representative I have spoken with has been extremely courteous, has seemed 
knowledgeable, and has conscientiously attempted to assist me within the bounds 
of what their positions allowed them to do.&nbsp; This leads me to believe that 
the problem is more of a "systemic" one, that can only be addressed from a 
management level; this is why I am asking both of you to ensure that this never 
happens to anyone again.&nbsp; In addition, I am requesting an apology for the 
unfortunate situation which has occurred. 
<p>Please contact me at your earliest convenience to inform me about how each of 
the issues listed above is being addressed.&nbsp; My mailing address and e-mail 
address are listed above, and my telephone numbers are [deleted]. 
</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Sincerely, <br>&nbsp; 
</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Judith L. Sammel <br>&nbsp; 
</p><p>cc: 
</p><ul>&nbsp; 
  <li>Joel Snyder, Administrative Commissioner, District Court of Maryland for 
  Baltimore County 
  </li><li>The Honorable S.G. Samuel Moxley, Councilmember, Baltimore County Council 
  (Franchising Authority for cable television in Baltimore County) 
  </li><li>Consumer Protection Division, Office of the Attorney General, State of 
  Maryland 
  </li><li>Federal Communications Commission, Cable Services Bureau 
  </li><li>C. Lynne Silverman, Esquire, Ned S. Kodeck, Chartered (attorney for 
  Comcast Cablevision of Maryland, LP) 
  </li><li>USENET newsgroups: athome.discussion-athomesvc, athome.discussion-general, 
  athome.users-general, balt.general, rec.video.cable-tv, misc.legal </li></ul>
<hr>

<h3>Acknowledgments:</h3>Thanks very much to attorneys S.A., S.B. (and her 
colleague H.), C.E., R.G. and B.H., who provided me with assorted legal advice. 
(Although, I imagine that "pro bono cable fraud work" won't really add much 
substance to your r√©sum√©s.)&nbsp; I'd also like to offer thanks to various 
friends and colleagues who assisted me with such useful comments as, "we promise 
we'll visit you in jail," and to my 'unindicted coconspirator' S.R. for 
convincing me to get cable modem service in the first place. <br>
<hr>
<br><span color="#ff0000"><b><span size="+2">Update</span></b>:</span> 
<p><b><i>25 January 1999:</i></b>&nbsp; I received a nice letter from Comcast's 
Baltimore Metro area Vice President, and the local General Manager of Comcast 
Online <br>apologizing for these events, and outlining the steps they've taken 
to make sure this doesn't happen again.&nbsp; They now have a procedure to note 
@Home subscribers without cable TV service in their cable service billing 
system, and have added some checks and balances to ensure that their cable fraud 
investigators will be able to identify this type of situation in the 
future.&nbsp; This is a welcome change from the initial conversations I had with 
Comcast's attorney about this issue, where she insisted that no change in 
procedures was necessary, other than to ensure that the Comcast Online 
installers installed video traps in all installations where it's warranted. They 
graciously offered to pay the Court processing fees to get my record expunged, 
but I haven't gotten around to filling out the paperwork yet.&nbsp; Since the 
court told me that it can take up to 120 days for the paperwork to go through 
one it's filed, I'm not in such a great hurry anyway. 
</p><p><b><i>31 January 1999: </i></b>The <b><i>Multichannel News</i></b> felt that 
this story was worth publishing in the February 1, 1999 edition of their 
magazine (on the front page, even!)&nbsp; They snarfed my title, too.&nbsp; They 
gave me permission to repost the articles on my web site: '<a href="http://members.home.net/sammel/mcnarticle1.htm">Get a Cable Modem...Go to 
Jail'</a>&nbsp; and <a href="http://members.home.net/sammel/mcnarticle2.htm">Sammel's Tale of @Jail</a> 
. 
</p><p><b><i>2 February 1999:</i></b>&nbsp; I got home from work and found two notes 
attached to my door.&nbsp; One was another letter of apology from the Baltimore 
Metro Area VP and the local Comcast Online General Manager, saying that they 
plan to extend my @home service for a year at no charge, along with a $50.00 
gift certificate from Bibelot (my favorite bookstore!) The second was a 
handwritten note--they had apparently come by to deliver this in person, but I 
wasn't home.&nbsp; This is a very nice gesture; I appreciate it very much. 
</p><p>Does it make me feel bad for having contacted <b><i>Multichannel 
News</i></b>?&nbsp; No; not at all--for two reasons: 
</p><ul>
  <li>Although Comcast made every effort to get this taken care of after I 
  contacted them about the summons back in November--the bottom line is that 
  <i>this never should have happened</i>.&nbsp; There were people at Comcast who 
  knew last June (at least) that the lack of communication between the cable TV 
  folks and the cable modem folks was causing problems.&nbsp; It just never made 
  it high enough on anyone's radar screen to do anything about it. 
  </li><li>I went to Multichannel News rather than a general-audience publication 
  because I wanted to try to reach the folks at Shaw, Cox, TCI, Rogers, and Road 
  Runner (and its associated cable partners) to get them thinking about whether 
  the same problems could exist at their operations. </li></ul><b><i>4 February 
1999: </i></b>I got a humorous surprise when I got home from work.&nbsp; I had 
asked Multichannel News if they could send me a copy of the print edition of 
their publication, and it arrived in today's mail.&nbsp; I found out that not 
only was my saga featured on the front page and the op-ed section, but there was 
a wonderful <a href="http://members.home.net/ccb/cartoon.htm">cartoon</a> to go 
with it. 
<p><b><i>7 February 1999:</i></b>&nbsp; Since <b><i>Multichannel News</i></b> 
updated their on-line edition,&nbsp; I changed&nbsp; the references for the 
articles they published about this case (see 31 January 1999 entry) to point to 
my reposts of their articles, instead.&nbsp; Also, since the print version of 
Mike Farrell's article in last week's edition had some text missing, they've 
reprinted the article in the February 8, 1999 print edition. All of the articles 
are searchable in Multichannel's archives available from their <a href="http://www.multichannel.com/">home page</a>. 
</p><p><b><i>16 February 1999:</i></b>&nbsp; It appears that "Broadband Bob" picked 
up my story and published a brief version to their mailing list, and on their <a href="http://www.catv.org/bbb-report/">web page</a>.&nbsp; Also, a Brazilian has 
written an <a href="http://www.geocities.com/WallStreet/Exchange/8822/Noticias/1999/news0199.html">article</a> 
in Portugese based on Mike Farrell's article in Multichannel News. 
</p><p>I had a problem last Friday (February 12) with my cable modem service.&nbsp; 
Everything was proceeding at a snail's pace--mail, news, web-browsing.&nbsp; It 
continued for several hours, so I called to see if there was a problem in our 
area, or if they knew what might be wrong.&nbsp; They tried some troubleshooting 
(Tier 1 and Tier 2), and they said there was an RF problem with my connection, 
and they'd need to send someone out to look at it. The first time they had 
available was Monday (yesterday).&nbsp; They said "9:00 AM with a 4 hour window" 
for their arrival time.&nbsp; At 1:40 PM, a technician showed up.&nbsp; He did 
some testing at the cable modem (found a very weak signal), then at the 
connection where the cable enters my house (again, a weak signal), and then at 
the "pedestal" out on the street (again a weak signal--he said it was likely 
weak because there were <b><i>two</i></b> video traps on my line.&nbsp; I don't 
know this for sure, but I strongly suspect that the Comcast@home folks still had 
the "order on the books" for the video trap to be placed (see Chapter 3 above), 
and did not get the word that one had already been installed by Comcast 
Cablevision; so they came by to install another one.&nbsp; (So, not only don't 
the two organizations talk to each other, but they don't even recognize each 
other's handiwork?)&nbsp; So, he removed one of the video traps, and the signal 
started coming through strongly, and my service resumed immediately.&nbsp; I 
told him I wanted to make sure he didn't remove both video traps.&nbsp; He 
seemed curious why I was concerned about that, so I showed him the court 
documents, and told him I wanted to prevent a recurrence of the bizarre 
situation I found myself in last November.&nbsp; He said he couldn't believe 
that something like that had actually happened.&nbsp; (Join the crowd!) 
</p><p>Keep the cards and letters (well...the e-mails) coming.&nbsp; One person has 
suggested that I post the titles of the books I purchased with the gift 
certificate Comcast gave me, in the hopes that I bought some books that were 
apropos to the issue at hand.&nbsp; Honestly, I hadn't thought of that when I 
was book shopping, so I didn't specifically look for books that might be 
pertinent (Is there a "Malicious Prosecution for Dummies" book out?)&nbsp; The 
books I got were:&nbsp; <i>The Pillars of Hercules</i>, by Paul Theroux (a 
travelogue of his trip through the Mediterranean); <i>Getting A Life</i>, by 
Blix/Heitmiller (financial and personal strategies for voluntary simplicity); 
<i>The Pocket Idiot's Guide to Home Repair, </i>by David Tenenbaum; and, <i>The 
Argument Culture: Stopping America's War of Words</i>, by Deborah Tannen (of 
<i>You Just Don't Understand</i> fame).&nbsp; Fairly boring--sorry to disappoint 
you. 
</p><p>I mailed all the paperwork to the District Court to get my criminal record 
expunged.&nbsp; It included the "General Waiver and Release" which says that I 
release and discharge Comcast and the Baltimore County Police "and any and all 
other persons" for any claims I may have for wrongful conduct related to this 
incident.&nbsp; So, Comcast can rest easy about the legal aspect of all of this. 

</p><p><b><i>9 April 1999:&nbsp; </i></b>I got an e-mail from Brian McWilliams at 
<b><i><a href="http://www.internetnews.com/">InternetNews</a></i></b>, asking me 
to give an interview for InternetNews Radio.&nbsp; I called and spoke to him, 
and voila--a couple of hours later, there's a segment on my story in Real Audio 
format on <a href="http://members.home.net/sammel/inr.ram">InternetNews 
Radio</a>. (¬© 1999 Internet.com&nbsp; Linked to with permission of Internet.com) 

</p><p><b><i>10 April 1999:&nbsp; </i></b>There's a blurb about my saga&nbsp; in the 
current issue of&nbsp; <b><i><a href="http://www.netsurf.com/nsd/nsd.05.11.html">Netsurfer Digest</a></i></b> 
(Volume 05, Issue 11). 
</p><p><b><i>13 April 1999:&nbsp; </i></b>My story is a "Tasty Bit of the Day" from 
<b><i><a href="http://tbtf.com/">Tasty Bits from the Technology 
Front</a></i></b>.&nbsp; Lisa Ronthal at <b><i>WorldNetDaily</i></b> has a 
paragraph and a link to my site in her <b><i><a href="http://www.worldnetdaily.com/bluesky_ronthal/19990413_xcint_for_codecr.shtml">Interscope</a></i></b> 
column. 
</p><p><b><i>16 April 1999:&nbsp;</i></b> The Macintosh world has discovered the 
story, and links to it appear on <b><i><a href="http://www.macaddict.com/">Macaddict</a></i></b>, <b><i><a href="http://www.macresource.com/">Macresource</a></i></b>, and <b><i><a href="http://www.macintouch.com/">MacInTouch</a></i></b>. 
</p><p><b><i>25 April 1999:&nbsp;</i></b> I received a copy of the expungement 
order, dated 14 April 1999.&nbsp; This means that the court has 30 days to serve 
the order on the Baltimore County Detention Center, The Baltimore County Police, 
the Msp-Cjis Repository (?) and the State's Attorney.&nbsp; They each then have 
30 days to expunge their records, and provide a Certificate of Compliance back 
to the court, and to me.&nbsp; Now, if we can only expunge all records of my 
"criminal activities" from the Internet.&nbsp; :-)&nbsp; Someone has submitted 
my URL for the current issue (Vol. 20, Issue 33) of the <b><i><a href="http://catless.ncl.ac.uk/Risks/20.33.html">Risks Digest</a></i></b> which 
is available on the web, or on USENET as <b><i>comp.risks </i></b>.&nbsp; Yahoo 
has also listed my site in their section on <b><i><a href="http://headlines.yahoo.com/Full_Coverage/Tech/Bandwidth_News/">Bandwidth 
News</a></i></b>. 
</p><p><b><i>26 April 1999:&nbsp;</i></b> I've been "<a href="http://slashdot.org/article.pl?sid=99/04/26/1229227">slashdotted</a>". 
<br>&nbsp; 
</p><hr>

<center> Return to <a href="http://telecom-digest.org/" <="" a=""> Telecom Archives </a></center><a href="http://telecom-digest.org/" <="" a="">
</a>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oscilloscope watch ships after 10 years on Kickstarter (254 pts)]]></title>
            <link>https://www.tomshardware.com/news/oscilloscope-watch-ships-after-10-years</link>
            <guid>37385811</guid>
            <pubDate>Mon, 04 Sep 2023 22:38:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/news/oscilloscope-watch-ships-after-10-years">https://www.tomshardware.com/news/oscilloscope-watch-ships-after-10-years</a>, See on <a href="https://news.ycombinator.com/item?id=37385811">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p>Ten years ago on Kickstarter, Gabriel Anzziani unveiled plans to produce an <a href="https://www.kickstarter.com/projects/920064946/oscilloscope-watch" data-url="https://www.kickstarter.com/projects/920064946/oscilloscope-watch"><u>oscilloscope watch</u></a>. The project caught on in popularity before fading into the ether for about a decade. After nearly forgetting about the project, <a href="https://twitter.com/BitBangingBytes/status/1695192177310150993" data-url="https://twitter.com/BitBangingBytes/status/1695192177310150993"><u>early backers</u></a> were surprised this month to receive a package containing the oscilloscope watch.</p><p>The project page received an update on July 30th of this year from Anzziani confirming that backers are now officially starting to receive their watches. According to the <a href="https://www.kickstarter.com/projects/920064946/oscilloscope-watch/posts/3872064" data-url="https://www.kickstarter.com/projects/920064946/oscilloscope-watch/posts/3872064"><u>post</u></a>, Anzziani is sending out between 10 and 20 rewards per week. The goal is to have all early rewards shipped by the end of 2023.</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">10 years ago I saw an oscilloscope watch on #Kickstarter Today it arrived! pic.twitter.com/svowwpAqQx<a href="https://twitter.com/BitBangingBytes/status/1695192177310150993" data-url="https://twitter.com/BitBangingBytes/status/1695192177310150993">August 25, 2023</a></p></blockquote><p><span role="button" tabindex="0" aria-label="See more">See more</span></p></div><p>The oscilloscope watch has two modes. As the name suggests, it functions as both a watch and an oscilloscope. The watch mode has several useful features including formatting options for 24 vs 12 hour layouts and even an alarm. Of course, it also has an oscilloscope mode that works when the probes are inserted.</p><div aria-hidden="false" data-swipeable="true" data-hydrate="true" id="slice-container-UirZBzrCVkFBWDErQZZYPn-imageGallery-5"><figure data-bordeaux-image-check="false"><div data-hydrate="true"><p><img src="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" alt="Oscilloscope"></p></div><figcaption><span itemprop="copyrightHolder">(Image credit: Gabriel Anzziani)</span></figcaption></figure></div><p>The watch is powered by an 8-bit Xmega microcontroller with an internal PDI. It can be programmed to use custom mods using C. It has 8 buttons that can be programmed, as well, that surround the watch face on the outer edge. According to Anzziani, one goal of the project was to enable users to create their own apps for the watch.</p><p>The screen is a 1.28-inch, low-power E Ink display. Anzziani explains the expected battery life varies depending on whether or not the oscilloscope is in use. Without using the oscilloscope the battery can last around 30 days on a single charge. Using the oscilloscope cuts that down to about 12 hours.</p><p>If you want to get a closer look at this project, you can find details about it over at <a href="https://www.kickstarter.com/projects/920064946/oscilloscope-watch" data-url="https://www.kickstarter.com/projects/920064946/oscilloscope-watch"><u>Kickstarter</u></a>. &nbsp;Rewards are no longer available but you can still read more about how the project goes together. If this project seems up your alley, you might also want to check out this project that uses a Raspberry Pi Pico to power an <a href="https://www.tomshardware.com/news/raspberry-pi-pico-oscilloscope"><u>oscilloscope</u></a> with a smartphone UI.</p>
</div><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent"><section><p>Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news ‚Äî and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox.</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Emacs Bedrock: A minimal Emacs starter kit (175 pts)]]></title>
            <link>https://sr.ht/~ashton314/emacs-bedrock/</link>
            <guid>37385716</guid>
            <pubDate>Mon, 04 Sep 2023 22:26:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sr.ht/~ashton314/emacs-bedrock/">https://sr.ht/~ashton314/emacs-bedrock/</a>, See on <a href="https://news.ycombinator.com/item?id=37385716">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="bedrock"><a href="#bedrock" rel="nofollow noopener">#</a>Bedrock</h2>
<p>Stepping stones to a better Emacs experience</p>
<h3 id="synopsis"><a href="#synopsis" rel="nofollow noopener">#</a>Synopsis</h3>
<p>An <em>extremely</em> minimal Emacs starter kit uses just one external package by default, and only GNU-ELPA packages on an opt-in basis. Intended to be copied once and then modified as the user grows in knowledge and power.</p>
<ul>
<li><a href="https://sr.ht/~ashton314/emacs-bedrock/" rel="nofollow noopener">Project homepage</a></li>
<li><a href="https://todo.sr.ht/~ashton314/emacs-bedrock" rel="nofollow noopener">Issue Tracker</a></li>
<li>Mirrors:
<ul>
<li><a href="https://github.com/ashton314/emacs-bedrock" rel="nofollow noopener">GitHub</a> (just a place holder)</li>
</ul>
</li>
</ul>
<p><strong>NOTICE:</strong> Requires Emacs 29.1 or better.</p>
<h3 id="description"><a href="#description" rel="nofollow noopener">#</a>Description</h3>
<p>This is a minimal Emacs starter kit. Like, <em>really</em> minimal. Here's the short of the philosophy:</p>
<ul>
<li>
<p>Focus on using default, built-in Emacs behavior</p>
<p>Yes, we all love our fancy third-party packages. This starter kit focuses on what is built-in to Emacs. Why? Because there are too many good packages and picking and choosing the best is a joy we leave to the user.</p>
</li>
<li>
<p>Explain every customization and encourage modification</p>
<p>The goal of this starter kit is to encourage end-user adaptation and growth. All of the <code>.el</code> files should be legible and, more importantly, justify in plain English the rationale for adding the configuration they do.</p>
</li>
<li>
<p>No magic</p>
<p>We keep things <em>crushingly</em> simple here. That means no fancy loadable modules or whatnot. Everything is as straight-forward as can be.</p>
</li>
</ul>
<p>There are two files of interest: <code>early-init.el</code> and <code>init.el</code>.</p>
<h4 id="codeearly-initelcode"><a href="#codeearly-initelcode" rel="nofollow noopener">#</a><code>early-init.el</code></h4>
<p>The early init file uses <em>strictly</em> built-in Emacs features to do the following:</p>
<ul>
<li>Improve startup time</li>
<li>Set up initial frame behavior</li>
</ul>
<h4 id="codeinitelcode"><a href="#codeinitelcode" rel="nofollow noopener">#</a><code>init.el</code></h4>
<p>This is where the meat of all configuration goes. This file:</p>
<ul>
<li>Add minor UI niceties (e.g. clock in the tab-bar, full-screen by default, etc.)</li>
<li>Set the default theme (<code>modus-vivendi</code>)</li>
<li>Turn on discovery aids (e.g. <code>help-quick</code>, <a href="https://github.com/justbur/emacs-which-key" rel="nofollow noopener">which-key</a>, etc.)</li>
</ul>
<h4 id="trying-this-out-without-committing-too-hard"><a href="#trying-this-out-without-committing-too-hard" rel="nofollow noopener">#</a>Trying this out without committing too hard</h4>
<p>Emacs 29.1 added the handy <code>--init-directory</code> flag. This means that you can run <code>emacs --init-directory path/to/emacs-bedrock/</code> and all the customizations and package installations will be isolated to the project directory. Emacs should only add files that are already in the <code>.gitignore</code>.</p>
<p>Once you're happy, you should just copy <code>init.el</code> and <code>early-init.el</code> to <code>~/.emacs.d/</code>.</p>
<h3 id="mixins"><a href="#mixins" rel="nofollow noopener">#</a>Mixins</h3>
<p>For those who'd like a little more help in tailoring Emacs for specific purposes, the <code>mixins/</code> folder contains a few files that can be included via <code>(load-file "mixin/mixin-name.el")</code> from the <code>init.el</code> file, or copied wholesale or in part into <code>init.el</code> directly.</p>
<p><strong>NOTE:</strong> If you copy the <code>mixin/</code> directory to <code>~/.emacs.d/</code> or wherever you're setting <code>user-emacs-directory</code>, then simply incrementing the appropriate lines in the <code>init.el</code> file should work.</p>
<p>Mixins:</p>
<ul>
<li>Base UI Enhancements</li>
<li>Development tools</li>
<li>Org-mode</li>
<li>Vim refugee</li>
<li>Email (TODO: mu4e, EBDB)</li>
<li>Researcher (TODO: denote)</li>
</ul>
<h5 id="codemixinsbaseelcode"><a href="#codemixinsbaseelcode" rel="nofollow noopener">#</a><code>mixins/base.el</code></h5>
<p>Packages this mixin adds:</p>
<ul>
<li><a href="https://github.com/abo-abo/avy" rel="nofollow noopener">Avy</a></li>
<li><a href="https://github.com/oantolin/embark" rel="nofollow noopener">Embark</a></li>
<li><a href="https://github.com/minad/vertico" rel="nofollow noopener">Vertico</a></li>
<li><a href="https://github.com/minad/marginalia/" rel="nofollow noopener">Marginalia</a></li>
<li><a href="https://github.com/minad/corfu" rel="nofollow noopener">Corfu</a></li>
<li><a href="https://github.com/minad/consult" rel="nofollow noopener">Consult</a></li>
<li><a href="https://github.com/oantolin/orderless" rel="nofollow noopener">Orderless</a></li>
</ul>
<p>Along with a few ancillary packages that enhance the above.</p>
<p>These are some of the best UI enhancements that Emacs has to offer. Vertico and Consult make common operations like searching files, switching buffers, etc. a breeze. Corfu enhances the "completion at point" (aka "tab-to-complete") to show a little popup window like what you'd be used to in e.g. VS Code.</p>
<p>Avy is the fastest way to move around in a buffer, and it can do a <em>lot</em>.<a href="https://karthinks.com/software/avy-can-do-anything/" rel="nofollow noopener">^1</a> Embark is kind of like a right-click context menu, but entirely keyboard driven.</p>
<h5 id="codemixinsdevelcode"><a href="#codemixinsdevelcode" rel="nofollow noopener">#</a><code>mixins/dev.el</code></h5>
<p>Packages this mixin adds:</p>
<ul>
<li><a href="https://magit.vc/" rel="nofollow noopener">magit</a></li>
<li>Markdown, YAML, and JSON modes</li>
</ul>
<p>Magit is the best Git interface in the known universe. Some people use Emacs just so they can use Magit. It's that good. Entry point is bound to <code>C-c g</code> by default.</p>
<p>Built-in packages that this mixin configures:</p>
<ul>
<li><a href="https://github.com/joaotavora/eglot" rel="nofollow noopener">Eglot</a> (<a href="https://microsoft.github.io/language-server-protocol/" rel="nofollow noopener">Language Server Protocol (LSP) client</a>)</li>
<li>Treesit (<a href="https://github.com/tree-sitter" rel="nofollow noopener">Tree-Sitter</a> support)</li>
</ul>
<p>Both of these packages are new in Emacs 29. Be sure to run <code>M-x treesit-install-language-grammar</code> to install the language grammar you'll need before editing a file the respective language for the first time.</p>
<h5 id="codemixinsvim-likeelcode"><a href="#codemixinsvim-likeelcode" rel="nofollow noopener">#</a><code>mixins/vim-like.el</code></h5>
<p>Packages this mixin adds:</p>
<ul>
<li><a href="https://github.com/emacs-evil/evil" rel="nofollow noopener">Evil</a></li>
</ul>
<p>If you like Vim keybindings, then this is the mixin for you. It configures <code>evil-mode</code> and enables it, so you get Vim-like keybindings all throughout Emacs. I understand that this is the best Vim emulation outside of Vim itself. I use <code>evil-mode</code> in all my work.</p>
<p>Other packages that I use personally, but are not on GNU or non-GNU ELPA and so left out of the config include:</p>
<ul>
<li><a href="https://github.com/emacs-evil/evil-collection" rel="nofollow noopener">Evil-Collection</a> Add Evil-friendly keybindings to lots of corners of Emacs</li>
<li><a href="https://github.com/cofi/evil-leader" rel="nofollow noopener">Evil-Leader</a> Setting a prefix (i.e. "leader") key</li>
<li><a href="https://github.com/gregsexton/origami.el" rel="nofollow noopener">Origami</a> Code folding</li>
</ul>
<h5 id="codemixinsorgelcode"><a href="#codemixinsorgelcode" rel="nofollow noopener">#</a><code>mixins/org.el</code></h5>
<p>This mixin configures <code>org-mode</code>. There is a <em>lot</em> that Bedrock cannot configure out of the box‚Äîyou will need to modify all variables to fit your file system and needs, as explained in comments in the file.</p>
<h5 id="codemixinsemailelcode"><a href="#codemixinsemailelcode" rel="nofollow noopener">#</a><code>mixins/email.el</code></h5>
<p>TODO</p>
<h3 id="using"><a href="#using" rel="nofollow noopener">#</a>Using</h3>
<p>Clone this repository wherever. Then you should copy <code>early-init.el</code>, <code>init.el</code>, and (optionally, recommended) <code>mixins/</code> into your <code>~/.emacs.d/</code> repository:</p>
<div><pre><span></span>git clone https://git.sr.ht/~ashton314/emacs-bedrock
mkdir -p ~/.emacs.d/
cp emacs-bedrock/early-init.el ~/.emacs.d/
cp emacs-bedrock/init.el ~/.emacs.d/
cp -r emacs-bedrock/mixins ~/.emacs.d/
</pre></div>
<p>Fire up Emacs and you're good to go!</p>
<h4 id="philosophy"><a href="#philosophy" rel="nofollow noopener">#</a>Philosophy</h4>
<p>Many people are looking for a good set of defaults and some easy-to-use switches that let Emacs get out of the way and let them work on what they want to. This is fine. This is not what Bedrock tries to do.</p>
<p>Emacs is the most customizable piece of software in existence. (No citation needed.) My goal with Bedrock is to make Emacs a little nicer by enabling some things that I personally think should be enabled by default. Bedrock goes a little further by suggesting a few well-built packages that go on to enhance the experience.</p>
<p>Bedrock encourages inspection and modification. I don't plan on making some core that periodically gets updated. You can think of this as just some guy's config that you wanted to adopt.</p>
<p>As an example of a deliberate choice, the <code>help-quick</code> buffer pops open on startup. Once a user has gotten used to this, they can just go into their <code>early-init.el</code> file and modify it themselves to remove that hook if they don't like it. It's a simple one-line change, and only users who are ready for it will do it.</p>
<p>When I started learning Emacs, my dad gave me his <code>.emacs</code> file. (That's what we used back in ye olden days instead of <code>.emacs.d/init.el</code> and stuff.) I used it without modification for many years. Eventually I learned how to write my own functions and customizations. This package aims to give other users a similar experience. When someone comes to me and expresses their desire to learn Emacs, I can point them at this to help them get over the initial hump, but not coddle them so much that they're afraid or unable to change things to their liking.</p>
<h3 id="requirements"><a href="#requirements" rel="nofollow noopener">#</a>Requirements</h3>
<p>Emacs 29.1 or later.</p>
<p>Emacs 29.1 is, as of 2023-09-04, the latest stable release. The specific features from Emacs 29.1 that Bedrock relies on are:</p>
<ul>
<li>The <code>use-package</code> macro for configuration</li>
<li>Enhancements to the built-in completion help (<code>completions-auto-select</code>, <code>completion-auto-help</code>, etc.)</li>
<li>Built-in tree-sitter support</li>
<li>Built-in LSP client (Eglot)</li>
</ul>
<h3 id="development"><a href="#development" rel="nofollow noopener">#</a>Development</h3>
<p>This is version <code>1.0.0</code>. No new <code>use-package</code> declarations will be added to <code>init.el</code>. No promises on the mixins. :)</p>
<p>This is a hobby project. Please be patient with development.</p>
<p>I welcome any feedback you may have. You can <a href="https://todo.sr.ht/~ashton314/emacs-bedrock" rel="nofollow noopener">open issues</a> or <a href="https://lambdaland.org/#contact" rel="nofollow noopener">drop me a line</a> directly with any comments or suggestions.</p>
<h4 id="roadmap"><a href="#roadmap" rel="nofollow noopener">#</a>Roadmap</h4>
<p>See the <a href="https://todo.sr.ht/~ashton314/emacs-bedrock" rel="nofollow noopener">issue tracker</a> on SourceHut.</p>
<h3 id="changelog"><a href="#changelog" rel="nofollow noopener">#</a>Changelog</h3>
<ul>
<li>
<p>1.0.0</p>
<p>2023-09-04</p>
<p>First "stable" release! Line number width improved, fix default load paths, expand Eglot and Vertico config, fix Corfu load.</p>
</li>
<li>
<p>0.2.1</p>
<p>2023-06-20</p>
<p>Minor bug fixes; add Embark package.</p>
</li>
<li>
<p>0.2.0</p>
<p>2023-03-14</p>
<p>Flesh out the <code>mixin/vim-like.el</code> so that there's <em>some</em> Vim configuration.</p>
</li>
<li>
<p>0.1.0</p>
<p>2023-01-17</p>
<p>Begin work on <code>mixin/org.el</code>, turn on windmove-mode.</p>
</li>
<li>
<p>0.0.2</p>
<p>2023-01-03</p>
<p>Reorganize to slim down <code>early-init.el</code> and add the first mixin files.</p>
</li>
<li>
<p>0.0.1</p>
<p>2023-01-03</p>
<p>Initial "release".</p>
</li>
</ul>

<ul>
<li>Ashton Wiersdorf <a href="https://lambdaland.org/" rel="nofollow noopener">https://lambdaland.org</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLM Python/CLI tool adds support for embeddings (135 pts)]]></title>
            <link>https://simonwillison.net/2023/Sep/4/llm-embeddings/</link>
            <guid>37384797</guid>
            <pubDate>Mon, 04 Sep 2023 20:37:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2023/Sep/4/llm-embeddings/">https://simonwillison.net/2023/Sep/4/llm-embeddings/</a>, See on <a href="https://news.ycombinator.com/item?id=37384797">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p>4th September 2023</p>

<p><a href="https://llm.datasette.io/">LLM</a> is my Python library and command-line tool for working with language models. I just released <a href="https://llm.datasette.io/en/stable/changelog.html#v0-9">LLM 0.9</a> with a new set of features that extend LLM to provide tools for working with <em>embeddings</em>.</p>
<p>This is a long post with a lot of theory and background. If you already know what embeddings are, here‚Äôs a TLDR you can try out straight away:</p>
<div><pre><span><span>#</span> Install LLM</span>
pip install llm

<span><span>#</span> If you already installed via Homebrew/pipx you can upgrade like this:</span>
llm install -U llm

<span><span>#</span> Install the llm-sentence-transformers plugin</span>
llm install llm-sentence-transformers

<span><span>#</span> Install the all-MiniLM-L6-v2 embedding model</span>
llm sentence-transformers register all-MiniLM-L6-v2

<span><span>#</span> Generate and store embeddings for every README.md in your home directory, recursively</span>
llm embed-multi readmes \
  --model sentence-transformers/all-MiniLM-L6-v2 \
  --files <span>~</span>/ <span><span>'</span>**/README.md<span>'</span></span>
  <span><span>#</span> Add --store to store the text content as well</span>

<span><span>#</span> Run a similarity search for "sqlite" against those embeddings</span>
llm similar readmes -c sqlite</pre></div>
<p>For everyone else, read on and the above example should hopefully all make sense.</p>
<h4>Embeddings</h4>
<p>Embeddings are a fascinating concept within the larger world of language models.</p>
<p>I explained embeddings in my recent talk, <a href="https://simonwillison.net/2023/Aug/27/wordcamp-llms/">Making Large Language Models work for you</a>. The relevant section of the slides and transcript <a href="https://simonwillison.net/2023/Aug/27/wordcamp-llms/#embeddings">is here</a>, or you can <a href="https://www.youtube.com/watch?v=aC7UQcZN6y8&amp;t=2189s">jump to that section on YouTube</a>.</p>
<p>An embedding model lets you take a string of text‚Äîa word, sentence, paragraph or even a whole document‚Äîand turn that into an array of floating point numbers called an <em>embedding vector</em>.</p>
<p><img src="https://static.simonwillison.net/static/2023/wordcamp-llms/llm-work-for-you.055.jpeg" alt="On the left is a text post from one of my sites: Storing and serving related documents with openai-to-sqlite and embeddings. An arrow points to a huge JSON array on the right, with the label 1536 floating point numbers."></p>
<p>A model will always produce the same length of array‚Äî1,536 numbers for the <a href="https://platform.openai.com/docs/guides/embeddings">OpenAI embedding model</a>, 384 for <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">all-MiniLM-L6-v2</a>‚Äîbut the array itself is inscrutable. What are you meant to do with it?</p>
<p>The answer is that you can compare them. I like to think of an embedding vector as a location in 1,536-dimensional space. The distance between two vectors is a measure of how semantically similar they are in meaning, at least according to the model that produced them.</p>
<p><img src="https://static.simonwillison.net/static/2023/wordcamp-llms/llm-work-for-you.056.jpeg" alt="A location in 1,536 dimension space  There's a 3D plot with 400 red dots arranged randomly across 3 axis."></p>
<p>‚ÄúOne happy dog‚Äù and ‚ÄúA playful hound‚Äù will end up close together, even though they don‚Äôt share any keywords. The embedding vector represents the language model‚Äôs interpretation of the meaning of the text.</p>
<p>Things you can do with embeddings include:</p>
<ol>
<li>Find <strong>related items</strong>. I use this on <a href="https://til.simonwillison.net/">my TIL site</a> to display related articles, as described in <a href="https://til.simonwillison.net/llms/openai-embeddings-related-content">Storing and serving related documents with openai-to-sqlite and embeddings</a>.</li>
<li>Build <strong>semantic search</strong>. As shown above, an embeddings-based search engine can find content relevant to the user‚Äôs search term even if none of the keywords match.</li>
<li>Implement <strong>retrieval augmented generation</strong>‚Äîthe trick where you take a user‚Äôs question, find relevant documentation in your own corpus and use that to get an LLM to spit out an answer. More on that <a href="https://simonwillison.net/2023/Aug/27/wordcamp-llms/#retrieval-augmented-generation">here</a>.</li>
<li>
<strong>Clustering</strong>: you can find clusters of nearby items and identify patterns in a corpus of documents.</li>
<li><strong>Classification</strong>: calculate the embedding of a piece of text and compare it to pre-calculated ‚Äúaverage‚Äù embeddings for different categories.</li>
</ol>
<h4>LLM‚Äôs new embedding features</h4>
<p>My goal with LLM is to provide a plugin-driven abstraction around a growing collection of language models. I want to make installing, using and comparing these models as easy as possible.</p>
<p>The new release adds several command-line tools for working with embeddings, plus a new Python API for working with embeddings in your own code.</p>
<p>It also adds support for installing additional embedding models via plugins. I‚Äôve released one plugin for this so far: <a href="https://github.com/simonw/llm-sentence-transformers">llm-sentence-transformers</a>, which adds support for new models based on the <a href="https://www.sbert.net/">sentence-transformers</a> library.</p>
<p>The example above shows how to use <code>sentence-transformers</code>. LLM also supports API-driven access to the OpenAI <code>ada-002</code> model.</p>
<p>Here‚Äôs how to embed some text using <code>ada-002</code>, assuming you have <a href="https://llm.datasette.io/en/stable/setup.html">installed LLM already</a>:</p>
<div><pre><span><span>#</span> Set your OpenAI API key</span>
llm keys <span>set</span> openai
<span><span>#</span> &lt;paste key here&gt;</span>

<span><span>#</span> Embed some text</span>
llm embed -m ada-002 -c <span><span>"</span>Hello world<span>"</span></span></pre></div>
<p>This will output a huge JSON list of floating point numbers to your terminal. You can add <code>-f base64</code> (or <code>-f hex</code>) to get that back in a different format, though none of these outputs are instantly useful.</p>
<p>Embeddings are much more interesting when you store them.</p>
<p>LLM already uses SQLite to <a href="https://llm.datasette.io/en/stable/logging.html">store prompts and responses</a>. It was a natural fit to use SQLite to store embeddings as well.</p>
<h4>Embedding collections</h4>
<p>LLM 0.9 introduces the concept of a <strong>collection</strong> of embeddings. A collection has a name‚Äîlike <code>readmes</code>‚Äîand contains a set of embeddings, each of which has an ID and an embedding vector.</p>
<p>All of the embeddings in a collection are generated by the same model, to ensure they can be compared with each others.</p>
<p>The <code>llm embed</code> command can store the vector in the database instead of returning it to the console. Pass it the name of an existing (or to-be-created) collection and the ID to use to store the embedding.</p>
<p>Here we‚Äôll store the embedding for the phrase ‚ÄúHello world‚Äù in a collection called <code>phrases</code> with the ID <code>hello</code>, using that <code>ada-002</code> embedding model:</p>
<div><pre>llm embed phrases hello -m ada-002 -c <span><span>"</span>Hello world<span>"</span></span></pre></div>
<p>Future phrases can be added without needing to specify the model again, since it is remembered by the collection:</p>
<div><pre>llm embed phrases goodbye -c <span><span>"</span>Goodbye world<span>"</span></span></pre></div>
<p>The <code>llm embed-db collections</code> shows a list of collections:</p>
<div><pre>phrases: ada-002
  2 embeddings
readmes: sentence-transformers/all-MiniLM-L6-v2
  16796 embeddings</pre></div>
<p>The data is stored in a SQLite <code>embeddings</code> table with the following schema:</p>
<div><pre>CREATE TABLE [collections] (
   [id] <span>INTEGER</span> <span>PRIMARY KEY</span>,
   [name] <span>TEXT</span>,
   [model] <span>TEXT</span>
);
<span>CREATE</span> <span>TABLE</span> "<span>embeddings</span>" (
   [collection_id] <span>INTEGER</span> <span>REFERENCES</span> [collections]([id]),
   [id] <span>TEXT</span>,
   [embedding] BLOB,
   [content] <span>TEXT</span>,
   [content_hash] BLOB,
   [metadata] <span>TEXT</span>,
   [updated] <span>INTEGER</span>,
   <span>PRIMARY KEY</span> ([collection_id], [id])
);

CREATE UNIQUE INDEX [idx_collections_name]
    <span>ON</span> [collections] ([name]);
CREATE INDEX [idx_embeddings_content_hash]
    <span>ON</span> [embeddings] ([content_hash]);</pre></div>
<p>By default this is the SQLite database at the location revealed by <a href="">llm embed-db path</a>, but you can pass <code>--database my-embeddings.db</code> to various LLM commands to use a different database.</p>
<p>Each embedding vector is stored as a binary BLOB in the <code>embedding</code> column, consisting of those floating point numbers packed together as 32 bit floats.</p>
<p>The <code>content_hash</code> column contains a MD5 hash of the content. This helps avoid re-calculating the embedding (which can cost actual money for API-based embedding models like <code>ada-002</code>) unless the content has changed.</p>
<p>The <code>content</code> column is usually <code>null</code>, but can contain a copy of the original text content if you pass the <code>--store</code> option to the <code>llm embed</code> command.</p>
<p><code>metadata</code> can contain a JSON object with metadata, if you pass <code>--metadata '{"json": "goes here"}</code>.</p>
<p>You don‚Äôt have to pass content using <code>-c</code>‚Äîyou can instead pass a file path using the <code>-i/--input</code> option:</p>
<div><pre>llm embed docs llm-setup -m ada-002 -i llm/docs/setup.md</pre></div>
<p>Or pipe things to standard input like this:</p>
<div><pre>cat llm/docs/setup.md <span>|</span> llm embed docs llm-setup -m ada-002 -i -</pre></div>
<h4>Embedding similarity search</h4>
<p>Once you‚Äôve built a collection, you can search for similar embeddings using the <code>llm similar</code> command.</p>
<p>The <code>-c "term"</code> option will embed the text you pass in using the embedding model for the collection and use that as the comparison vector:</p>
<div><pre>llm similar readmes -c sqlite</pre></div>
<p>You can also pass the ID of an object in that collection to use that embedding instead. This gets you related documents, for example:</p>
<div><pre>llm similar readmes sqlite-utils/README.md</pre></div>
<p>The output from this command is currently newline-delimited JSON.</p>
<h4>Embedding in bulk</h4>
<p>The <code>llm embed</code> command embeds a single string at a time. <code>llm embed-multi</code> is much more powerful: you can feed a CSV or JSON file, a SQLite database or even have it read from a directory of files in order to embed multiple items at once.</p>
<p>Many embeddings models are optimized for batch operations, so embedding multiple items at a time can provide a significant speed boost.</p>
<p>The <code>embed-multi</code> command is described <a href="https://llm.datasette.io/en/stable/embeddings/cli.html#llm-embed-multi">in detail in the documentation</a>. Here are a couple of fun things you can do with it.</p>
<p>First, I‚Äôm going to create embeddings for every single one of my Apple Notes.</p>
<p>My <a href="https://datasette.io/tools/apple-notes-to-sqlite">apple-notes-to-sqlite</a> tool can export Apple Notes to a SQLite database. I‚Äôll run that first:</p>
<div><pre>apple-notes-to-sqlite notes.db</pre></div>
<p>This took quite a while to run on my machine and generated a 828M SQLite database containing 6,462 records!</p>
<p>Next, I‚Äôm going to embed the content of all of those notes using the <code>sentence-transformers/all-MiniLM-L6-v2</code> model:</p>
<div><pre>llm embed-multi notes \
  -d notes.db \
  --sql <span><span>'</span>select id, title, body from notes<span>'</span></span> \
  -m sentence-transformers/all-MiniLM-L6-v2</pre></div>
<p>This took around 15 minutes to run, and increased the size of my database by 13MB.</p>
<p>The <code>--sql</code> option here specifies a SQL query. The first column must be an <code>id</code>, then any subsequent columns will be concatenated together to form the content to embed.</p>
<p>In this case the embeddings are written back to the same <code>notes.db</code> database that the content came from.</p>
<p>And now I can run embedding similarity operations against all of my Apple notes!</p>
<div><pre>llm similar notes -d notes.db -c <span><span>'</span>ideas for blog posts<span>'</span></span></pre></div>
<h4>Embedding files in a directory</h4>
<p>Let‚Äôs revisit the example from the top of this post. In this case, I‚Äôm using the <code>--files</code> option to search for files on disk and embed each of them:</p>
<div><pre>llm embed-multi readmes \
  --model sentence-transformers/all-MiniLM-L6-v2 \
  --files <span>~</span>/ <span><span>'</span>**/README.md<span>'</span></span></pre></div>
<p>The <code>--files</code> option takes two arguments: a path to a directory and a pattern to match against filenames. In this case I‚Äôm searching my home directory recursively for any files named <code>README.md</code>.</p>
<p>Running this command gives me embeddings for all of my README.md files, which I can then search against like this:</p>
<div><pre>llm similar readmes -c sqlite</pre></div>
<h4>Embeddings in Python</h4>
<p>So far I‚Äôve only covered the command-line tools. LLM 0.9 also introduces a new Python API for working with embeddings.</p>
<p>There are two aspects to this. If you just want to embed content and handle the resulting vectors yourself, you can use <code>llm.get_embedding_model()</code>:</p>
<pre><span>import</span> <span>llm</span>

<span># This takes model IDs and aliases defined by plugins:</span>
<span>model</span> <span>=</span> <span>llm</span>.<span>get_embedding_model</span>(<span>"sentence-transformers/all-MiniLM-L6-v2"</span>)
<span>vector</span> <span>=</span> <span>model</span>.<span>embed</span>(<span>"This is text to embed"</span>)</pre>
<p><code>vector</code> will then be a Python list of floating point numbers.</p>
<p>You can serialize that to the same binary format that LLM uses like this:</p>
<pre><span>binary_vector</span> <span>=</span> <span>llm</span>.<span>encode</span>(<span>vector</span>)
<span># And to deserialize:</span>
<span>vector</span> <span>=</span> <span>llm</span>.<span>decode</span>(<span>binary_vector</span>)</pre>
<p>The second aspect of the Python API is the <code>llm.Collection</code> class, for working with collections of embeddings. This example code is quoted <a href="https://llm.datasette.io/en/stable/embeddings/python-api.html#working-with-collections">from the documentation</a>:</p>
<pre><span>import</span> <span>sqlite_utils</span>
<span>import</span> <span>llm</span>

<span># This collection will use an in-memory database that will be</span>
<span># discarded when the Python process exits</span>
<span>collection</span> <span>=</span> <span>llm</span>.<span>Collection</span>(<span>"entries"</span>, <span>model_id</span><span>=</span><span>"ada-002"</span>)

<span># Or you can persist the database to disk like this:</span>
<span>db</span> <span>=</span> <span>sqlite_utils</span>.<span>Database</span>(<span>"my-embeddings.db"</span>)
<span>collection</span> <span>=</span> <span>llm</span>.<span>Collection</span>(<span>"entries"</span>, <span>db</span>, <span>model_id</span><span>=</span><span>"ada-002"</span>)

<span># You can pass a model directly using model= instead of model_id=</span>
<span>embedding_model</span> <span>=</span> <span>llm</span>.<span>get_embedding_model</span>(<span>"ada-002"</span>)
<span>collection</span> <span>=</span> <span>llm</span>.<span>Collection</span>(<span>"entries"</span>, <span>db</span>, <span>model</span><span>=</span><span>embedding_model</span>)

<span># Store a string in the collection with an ID:</span>
<span>collection</span>.<span>embed</span>(<span>"hound"</span>, <span>"my happy hound"</span>)

<span># Or to store content and extra metadata:</span>
<span>collection</span>.<span>embed</span>(
    <span>"hound"</span>,
    <span>"my happy hound"</span>,
    <span>metadata</span><span>=</span>{<span>"name"</span>: <span>"Hound"</span>},
    <span>store</span><span>=</span><span>True</span>
)

<span># Or embed things in bulk:</span>
<span>collection</span>.<span>embed_multi</span>(
    [
        (<span>"hound"</span>, <span>"my happy hound"</span>),
        (<span>"cat"</span>, <span>"my dissatisfied cat"</span>),
    ],
    <span># Add this to store the strings in the content column:</span>
    <span>store</span><span>=</span><span>True</span>,
)</pre>
<p>As with everything else in LLM, the goal is that anything you can do with the CLI can be done with the Python API, and vice-versa.</p>
<h4 id="llm-cluster">Clustering with llm-cluster</h4>
<p>Another interesting application of embeddings is that you can use them to cluster content‚Äîidentifying patterns in a corpus of documents.</p>
<p>I‚Äôve started exploring this area with a new plugin, called <strong><a href="https://github.com/simonw/llm-cluster">llm-cluster</a>.</strong></p>
<p>You can install it like this:</p>

<p>Let‚Äôs create a new collection using data pulled from GitHub. I‚Äôm going to import all of the <a href="https://github.com/simonw/llm/issues">LLM issues</a> from the GitHub API, using my <a href="https://github.com/simonw/paginate-json">paginate-json</a> tool:</p>
<div><pre>paginate-json <span><span>'</span>https://api.github.com/repos/simonw/llm/issues?state=all&amp;filter=all<span>'</span></span> \
  <span>|</span> jq <span><span>'</span>[.[] | {id: .id, title: .title}]<span>'</span></span> \
  <span>|</span> llm embed-multi llm-issues - \
    --database issues.db \
    --model sentence-transformers/all-MiniLM-L6-v2 \
    --store</pre></div>
<p>Running this gives me a <code>issues.db</code> SQLite database with 218 embeddings contained in a collection called <code>llm-issues</code>.</p>
<p>Now let‚Äôs try out the <code>llm-cluster</code> command, requesting ten clusters from that collection:</p>
<div><pre>llm cluster llm-issues --database issues.db 10</pre></div>
<p>The output from this command, truncated, looks like this:</p>
<div><pre>[
  {
    <span>"id"</span>: <span><span>"</span>0<span>"</span></span>,
    <span>"items"</span>: [
      {
        <span>"id"</span>: <span><span>"</span>1784149135<span>"</span></span>,
        <span>"content"</span>: <span><span>"</span>Tests fail with pydantic 2<span>"</span></span>
      },
      {
        <span>"id"</span>: <span><span>"</span>1837084995<span>"</span></span>,
        <span>"content"</span>: <span><span>"</span>Allow for use of Pydantic v1 as well as v2.<span>"</span></span>
      },
      {
        <span>"id"</span>: <span><span>"</span>1857942721<span>"</span></span>,
        <span>"content"</span>: <span><span>"</span>Get tests passing against Pydantic 1<span>"</span></span>
      }
    ]
  },
  {
    <span>"id"</span>: <span><span>"</span>1<span>"</span></span>,
    <span>"items"</span>: [
      {
        <span>"id"</span>: <span><span>"</span>1724577618<span>"</span></span>,
        <span>"content"</span>: <span><span>"</span>Better ways of storing and accessing API keys<span>"</span></span>
      },
      {
        <span>"id"</span>: <span><span>"</span>1772024726<span>"</span></span>,
        <span>"content"</span>: <span><span>"</span>Support for `-o key value` options such as `temperature`<span>"</span></span>
      },
      {
        <span>"id"</span>: <span><span>"</span>1784111239<span>"</span></span>,
        <span>"content"</span>: <span><span>"</span>`--key` should be used in place of the environment variable<span>"</span></span>
      }
    ]
  },
  {
    <span>"id"</span>: <span><span>"</span>8<span>"</span></span>,
    <span>"items"</span>: [
      {
        <span>"id"</span>: <span><span>"</span>1835739724<span>"</span></span>,
        <span>"content"</span>: <span><span>"</span>Bump the python-packages group with 1 update<span>"</span></span>
      },
      {
        <span>"id"</span>: <span><span>"</span>1848143453<span>"</span></span>,
        <span>"content"</span>: <span><span>"</span>Python library support for adding aliases<span>"</span></span>
      },
      {
        <span>"id"</span>: <span><span>"</span>1857268563<span>"</span></span>,
        <span>"content"</span>: <span><span>"</span>Bump the python-packages group with 1 update<span>"</span></span>
      }
    ]
  }
]</pre></div>
<p>These look pretty good! But wouldn‚Äôt it be neat if we had a snappy title for each one?</p>
<p>The <code>--summary</code> option can provide exactly that, by piping the members of each cluster through a call to another LLM in order to generate a useful summary.</p>
<div><pre>llm cluster llm-issues --database issues.db 10 --summary</pre></div>
<p>This uses <code>gpt-3.5-turbo</code> to generate a summary for each cluster, with this default prompt:</p>
<blockquote>
<p>Short, concise title for this cluster of related documents.</p>
</blockquote>
<p>The results I got back are pretty good, including:</p>
<ul>
<li>Template Storage and Management Improvements</li>
<li>Package and Dependency Updates and Improvements</li>
<li>Adding Conversation Mechanism and Tools</li>
</ul>
<p>I tried the same thing using a Llama 2 model <a href="https://simonwillison.net/2023/Aug/1/llama-2-mac/">running on my own laptop</a>, with a custom prompt:</p>
<pre><code>llm cluster llm-issues --database issues.db 10 \
  --summary --model mlc-chat-Llama-2-13b-chat-hf-q4f16_1 \
  --prompt 'Concise title for this cluster of related documents, just return the title'
</code></pre>
<p>I didn‚Äôt quite get what I wanted! Llama 2 is proving a lot harder to prompt, so each cluster came back with something that looked like this:</p>
<blockquote>
<p>Sure! Here‚Äôs a concise title for this cluster of related documents:</p>
<p>‚ÄúDesign Improvements for the Neat Prompt System‚Äù</p>
<p>This title captures the main theme of the documents, which is to improve the design of the Neat prompt system. It also highlights the focus on improving the system‚Äôs functionality and usability</p>
</blockquote>
<p><a href="https://github.com/simonw/llm-cluster">llm-cluster</a> only took a few hours to throw together, which I‚Äôm seeing as a positive indicator that the LLM library is developing in the right direction.</p>
<h4>Future plans</h4>
<p>The two future features I‚Äôm most excited about are indexing and chunking.</p>
<h5>Indexing</h5>
<p>The <a href="https://llm.datasette.io/en/stable/embeddings/cli.html#llm-similar">llm similar</a> command and <a href="https://llm.datasette.io/en/stable/embeddings/python-api.html#retrieving-similar-items">collection.similar()</a> Python method currently use effectively the slowest brute force approach possible: calculate a cosine difference between input vector and every other embedding in the collection, then sort the results.</p>
<p>This works fine for collections with a few hundred items, but will start to suffer for collections of 100,000 or more.</p>
<p>There are plenty of potential ways of speeding this up: you can run a vector index like <a href="https://github.com/facebookresearch/faiss">FAISS</a> or <a href="https://github.com/nmslib/hnswlib">hnswlib</a>, use a database extension like <a href="https://github.com/asg017/sqlite-vss">sqlite-vss</a> or <a href="https://github.com/pgvector/pgvector">pgvector</a>, or turn to a hosted vector database like <a href="https://www.pinecone.io/">Pinecone</a> or <a href="https://milvus.io/">Milvus</a>.</p>
<p>With this many potential solutions, the obvious answer for LLM is to address this with plugins.</p>
<p>I‚Äôm still thinking through the details, but the core idea is that users should be able to define an index against one or more collections, and LLM will then coordinate updates to that index. These may not happen in real-time‚Äîsome indexes can be expensive to rebuild, so there are benefits to applying updates in batches.</p>
<p>I experimented with FAISS earlier this year in <a href="https://datasette.io/plugins/datasette-faiss">datasette-faiss</a>. That‚Äôs likely to be the base for my first implementation.</p>
<p>The <a href="https://llm.datasette.io/en/stable/embeddings/python-api.html#sql-schema">embeddings table</a> has an <code>updated</code> timestamp column to support this use-case‚Äîso indexers can run against just the items that have changed since the last indexing run.</p>
<p>Follow <a href="https://github.com/simonw/llm/issues/216">issue #216</a> for updates on this feature.</p>
<h5>Chunking</h5>
<p>When building an embeddings-based search engine, the hardest challenge is deciding how best to ‚Äúchunk‚Äù the documents.</p>
<p>Users will type in short phrases or questions. The embedding for a four word question might not necessarily map closely to the embedding of a thousand word article, even if the article itself should be a good match for that query.</p>
<p>To maximize the chance of returning the most relevant content, we need to be smarter about what we embed.</p>
<p>I‚Äôm still trying to get a good feeling for the strategies that make sense here. Some that I‚Äôve seen include:</p>
<ul>
<li>Split a document up into fixed length shorter segments.</li>
<li>Split into segments but including a ~10% overlap with the previous and next segments, to reduce problems caused by words and sentences being split in a way that disrupts their semantic meaning.</li>
<li>Splitting by sentence, using NLP techniques.</li>
<li>Splitting into higher level sections, based on things like document headings.</li>
</ul>
<p>Then there are more exciting, LLM-driven approaches:</p>
<ul>
<li>Generate an LLM summary of a document and embed that.</li>
<li>Ask an LLM ‚ÄúWhat questions are answered by the following text?‚Äù and then embed each of the resulting questions!</li>
</ul>
<p>It‚Äôs possible to try out these different techniques using LLM already: write code that does the splitting, then feed the results to <a href="https://llm.datasette.io/en/stable/embeddings/python-api.html#storing-embeddings-in-bulk">Collection.embed_multi()</a> or <a href="https://llm.datasette.io/en/stable/embeddings/cli.html#llm-embed-multi">llm embed-multi</a>.</p>
<p>But... it would be really cool if LLM could split documents for you‚Äîwith the splitting techniques themselves defined by plugins, to make it easy to try out new approaches.</p>
<h4>Get involved</h4>
<p>It should be clear by now that the potential scope of the LLM project is enormous. I‚Äôm trying to use plugins to tie together an enormous and rapidly growing ecosystem of models and techniques into something that‚Äôs as easy for people to work with and build on as possible.</p>
<p>There are plenty of ways you can help!</p>
<ul>
<li>
<a href="https://datasette.io/discord-llm">Join the #llm Discord</a> to talk about the project.</li>
<li>Try out plugins and run different models with them. There are <a href="https://llm.datasette.io/en/stable/plugins/directory.html">12 plugins already</a>, and several of those can be used to run dozens if not hundreds of models (<a href="https://github.com/simonw/llm-mlc">llm-mlc</a>, <a href="https://github.com/simonw/llm-gpt4all">llm-gpt4all</a> and <a href="https://github.com/simonw/llm-llama-cpp">llm-llama-cpp</a> in particular). I‚Äôve hardly scratched the surface of these myself, and I‚Äôm testing exclusively on Apple Silicon. I‚Äôm really keen to learn more about which models work well, which models don‚Äôt and which perform the best on different hardware.</li>
<li>Try <a href="https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html">building a plugin</a> for a new model. My dream here is that every significant Large Language Model will have an LLM plugin that makes it easy to install and use.</li>
<li>Build stuff using LLM and let me know what you‚Äôve built. Nothing fuels an open source project more than stories of cool things people have built with it.</li>
</ul>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When tech says ‚Äòno‚Äô (122 pts)]]></title>
            <link>https://www.ben-evans.com/benedictevans/2023/8/24/when-tech-says-no</link>
            <guid>37384517</guid>
            <pubDate>Mon, 04 Sep 2023 20:14:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ben-evans.com/benedictevans/2023/8/24/when-tech-says-no">https://www.ben-evans.com/benedictevans/2023/8/24/when-tech-says-no</a>, See on <a href="https://news.ycombinator.com/item?id=37384517">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="item-64e740480448971f24d33be6" data-layout-label="Post Body" data-type="item" data-updated-on="1692877202218">
  <p>This week I‚Äôve been reading the latest drafts of the UK‚Äôs <a href="https://www.gov.uk/government/consultations/revised-investigatory-powers-act-notices-regimes-consultation/consultation-on-revised-notices-regimes-in-the-investigatory-powers-act-2016-accessible-version">Investigatory Powers Act</a>, which attempts to give UK law enforcement the power to ban security fixes, amongst other things (<a href="https://www.justsecurity.org/87615/changes-to-uk-surveillance-regime-may-violate-international-law/">here‚Äôs a summary</a>), and thinking about other famous regulatory disasters, and in turn thinking about what it means when people in an industry say ‚Äòno!‚Äô </p><p>Whenever anyone proposes new rules or regulations, the people affected always have reasons why this is a terrible idea that will cause huge damage.  This applies to bankers, doctors, farmers, lawyers, academics‚Ä¶ and indeed software engineers. They always say ‚Äòno‚Äô and policy-makers can‚Äôt take that at face value: they discount it by some percentage, as a form of bargaining. But when people say ‚Äòno‚Äô, they might actually mean one of three different things, and it‚Äôs important to understand the difference. </p><p>First, and this is the default, they‚Äôre saying no because they just don‚Äôt like it. They have their own opinion of how this should be done and don‚Äôt want outsiders making them change it. Quite possibly they already considered your plan and decided against it. The new policy is probably awkward, annoying and inconvenient, and will cost money (even if it‚Äôs not explicitly aimed at profits). It‚Äôs a pain in the arse. However, it is also <em>possible</em>, and not actually a big deal, and in the end it won‚Äôt really damage the product or the company. They <em>can</em> do it - they just don‚Äôt like it.  </p><p>A good current example might be the EU DSA‚Äôs requirement that if you run a marketplace and can ban people from using it, you need to have some due process and right of appeal: if Airbnb kicks someone off and that affects their income, that can‚Äôt be entirely arbitrary. Airbnb or Uber might think this is unnecessarily bureaucratic and that their existing processes are fine, but life will go on. Social networks will have to offer chronological feeds, though the theory of harm behind this rule is at best poorly-evidenced and most normal users don‚Äôt actually like them. It‚Äôs annoying to people at Meta or Tiktok, in the abstract, but it doesn‚Äôt matter much either way. And the next iPhone will probably switch to USB-C because of a new EU rule that has little to no real engineering or environmental benefit. In the end, it doesn‚Äôt actually matter much, and life goes on. ‚ÄòNo‚Äô just means ‚Äòthat‚Äôs annoying‚Äô. </p><p>Second, though, the tech industry (or the doctors, or the farmers) might be saying no because this really will have very serious negative consequences that you haven‚Äôt understood. </p><p>My favourite example is California‚Äôs <a href="https://en.wikipedia.org/wiki/California_Assembly_Bill_5_(2019)">2019 ‚ÄòAB5‚Äô law</a>. This aimed to classify ‚Äògig economy‚Äô workers, especially (and deliberately) Uber drivers, as employees, with access to healthcare. This might or might not be a good policy objective (reasonable people can debate this), but the law itself was drafted so ineptly that it effectively made anyone who did any freelance work an employee, and hence in turn effectively banned freelance work. There followed a desperate scramble to exempt over 100 professions, from doctors to truck drivers to hairdressers, before the whole thing had to be abandoned. A lot of people told the politicians about the problem, but the politicians just said ‚Äúeveryone always says every law will be a disaster‚Äù and ignored them. Oops.</p><p>Tech has a lot of examples of this kind of thing. The Canadian government told Google and Meta that if a link to a newspaper story ever appears in search, or if a journalist ever posts a link to a story on Facebook, then they have to pay the newspaper for sending business to the newspaper. Most of the Canadian tech and indeed media industries pointed out how stupid this was, and Google and Meta said that given the choice, they‚Äôd stop letting news appear rather than pay a fee they could not control and that had no economic basis. The government thought this was the first kind of ‚Äòno‚Äô and a bluff, but actually, it was the second kind. Oops. </p><p>To give another EU example (because that‚Äôs where most of the laws are coming from right now) the initial drafts of the DMA required anyone running a messaging app to let ‚Äòany‚Äô third party interconnect and interoperate, and to give any such third party ‚Äòall‚Äô the same access to internal data as internal teams. That sounds sensible‚Ä¶ until you realise that hundreds of groups are trying to connect to WhatsApp or iMessage to spam their users, and you‚Äôve just told Meta and Apple to let them do that, and that dozens of intelligence agencies would love to have ‚Äòall the data your internal teams have‚Äô. Fortunately, in this case, when the entire tech industry said ‚Äòyou‚Äôre out of your mind‚Äô the EU did actually listen. </p><p>If the second kind of ‚Äòno‚Äô is ‚Äòthat‚Äôs a really bad idea‚Äô, the third kind is ‚Äòwe actually can‚Äôt do that‚Äô. </p><p>The perennial example here, of course, is encryption. For the last 25 years, engineers have said ‚Äòwe can make it secure, or we can let law enforcement have access, but that means the Chinese can get in too‚Äù and politicians reply ‚Äúno, make secure but not for people we like‚Äù. </p><p>My old boss Marc Andreessen, back when he was on the internet, liked to call this the ‚Äònerd harder‚Äô argument. The engineer says not ‚ÄúI don‚Äôt want to‚Äù nor ‚Äúthat‚Äôs a bad idea‚Äù but ‚ÄúI genuinely have no idea how to do that even if I wanted to‚Äù and the policy-maker replies ‚Äúyou‚Äôre an engineer - work it out!‚Äù ‚ÄúWork it out‚Äù is generally a demand to invent new mathematics, but sadly, mathematics doesn‚Äôt work like that. Your MPs‚Äô WhatsApp group can be secure, or it can readable by law enforcement and the Chinese, but you cannot have encryption that can be broken only by our spies and not their spies. Pick one. </p><p>I think the structural problem here, across all three kinds of ‚Äòno‚Äô, is that this is pretty new to most of us. I often compare regulation of tech to regulation of cars - we do regulate cars, but it‚Äôs complicated and there are many different kinds of question. ‚ÄòShould cars have different emissions requirements?‚Äô is a different kind of question to ‚Äòdoes the tax code favour too much low-density development?‚Äô and both questions are complicated. It‚Äôs a lot easier to want less congestion in cities than to achieve it, and it‚Äôs a lot easier to worry about toxic content on social media than to solve it, or even agree what ‚Äòsolve‚Äô would mean. </p><p>But we all grew up with cars. We have a pretty good idea of how roads work, and what gearboxes are, even if we‚Äôve never seen one, and if someone proposed that cars should not come with seats or headlights because that‚Äôs unfair competition for third-party suppliers, we could all see the problem. When policy-makers ask for secure encryption with a back door, we do not always see that this would like be telling Ford and GM to stop their cars from crashing, and to make them run on gasoline that doesn‚Äôt burn. Well yes, that would be nice, but how? They say ‚Äòno‚Äô? Easy - just threaten them with a fine of 25% of global revenue and they‚Äôll build it! </p><p>A Californian optimist would say that we‚Äôll age out of this. The policy class that got their staff to print their emails will age out and be replaced by the generation that grew up sending emojis, and understands that tech policy is just as nuanced, complex and full of trade-offs as healthcare, transport or housing policy. A European would ask how well California handles healthcare, transport or housing. </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing a C compiler in 500 lines of Python (407 pts)]]></title>
            <link>https://vgel.me/posts/c500/</link>
            <guid>37383913</guid>
            <pubDate>Mon, 04 Sep 2023 19:17:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vgel.me/posts/c500/">https://vgel.me/posts/c500/</a>, See on <a href="https://news.ycombinator.com/item?id=37383913">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    
    <p>A few months ago, I set myself the challenge of writing a C compiler in 500 lines of Python<sup><a href="#lines">1</a></sup>, after writing my <a href="https://vgel.me/posts/donut/">SDF donut</a> post.
How hard could it be?
The answer was, pretty hard, even when dropping quite a few features.
But it was also pretty interesting, and the result is surprisingly functional and not too hard to understand!</p>
<p>There's too much code for me to comprehensively cover in a single blog post<sup><a href="#yak">2</a></sup>, so I'll just give an overview of the decisions I made, things I had to cut, and the general architecture of the compiler, touching on a representative piece of each part.
Hopefully after reading this post, <a href="https://github.com/vgel/c500/blob/main/compiler.py">the code</a> is more approachable!</p>
<span id="continue-reading"></span><h2 id="Decisions,_decisions"><a href="#Decisions,_decisions">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Decisions,_decisions">
</a>Decisions, decisions</h2>
<p>The first, and most critical decision, was that this would be a <em>single-pass</em> compiler.
500 lines is too spare to be defining and transforming an abstract syntax tree!
What does that mean?</p>
<h3 id="Most_compilers:_faffing_around_with_syntax_trees"><a href="#Most_compilers:_faffing_around_with_syntax_trees">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Most_compilers:_faffing_around_with_syntax_trees">
</a>Most compilers: faffing around with syntax trees</h3>
<p>Well, most compiler's internals look something like this:</p>
<center>
<img src="https://vgel.me/posts/c500/parsenon.png" alt="the codepoints walk down the yellow brick road, get lexed into tokens, then worship at the world's largest chomsky to become syntax trees, then are torn to pieces by the codegen hydra to produce machine instructions">
</center>
<p>The tokens get lexed, then a <em>parser</em> runs over them and builds pretty little syntax trees:</p>
<pre data-lang="python"><code data-lang="python"><span># hypothetical code, not from anywhere
</span><span>def </span><span>parse_statement</span><span>(</span><span>lexer</span><span>) -&gt; PrettyLittleSyntaxTree:
</span><span>    </span><span>...
</span><span>    </span><span>if </span><span>type </span><span>:= </span><span>lexer.try_next(TYPE_NAME):
</span><span>        variable_name </span><span>= </span><span>lexer.next(IDENTIFIER)
</span><span>
</span><span>        </span><span>if </span><span>lexer.try_next(</span><span>"="</span><span>):
</span><span>            initializer </span><span>= </span><span>parse_initializer(lexer)
</span><span>        </span><span>else</span><span>:
</span><span>            initializer </span><span>= </span><span>None
</span><span>
</span><span>        lexer.next(SEMICOLON)
</span><span>
</span><span>        </span><span>return </span><span>VariableDeclarationNode(
</span><span>            </span><span>type </span><span>= </span><span>type</span><span>,
</span><span>            </span><span>name </span><span>= </span><span>variable_name,
</span><span>            </span><span>initializer </span><span>= </span><span>initializer,
</span><span>        )
</span><span>    </span><span>...
</span><span>
</span><span># much later...
</span><span>def </span><span>emit_code_for</span><span>(</span><span>node</span><span>: PrettyLittleSyntaxTree) -&gt; DisgustingMachineCode:
</span><span>    </span><span>...
</span><span>    </span><span>if </span><span>isinstance</span><span>(node, VariableDeclarationNode):
</span><span>        slot </span><span>= </span><span>reserve_stack_space(node.type.sizeof())
</span><span>        add_to_environment(node.name, slot)
</span><span>        </span><span>if </span><span>node.initializer </span><span>is not </span><span>None</span><span>:
</span><span>            register </span><span>= </span><span>emit_code_for(node.initializer)
</span><span>            emit(</span><span>f</span><span>"mov </span><span>{register}</span><span>, [</span><span>{slot}</span><span>]"</span><span>)
</span><span>    </span><span>...
</span></code></pre>
<p>The important thing here is that there's <em>two passes</em>, first the parsing builds up a syntax tree, then a second pass chews that tree up and turns it into machine code.
That's really useful for most compilers!
It keeps the parsing and codegen separate, so each can evolve independently.
It also means that you can transform the syntax tree before using it to generate code‚Äîfor example, by applying optimizations to it.
In fact, most compilers have <em>multiple</em> levels of "intermediate representations" between the syntax tree and codegen!</p>
<p>This is really great, good engineering, best practices, recommended by experts, etc.
But‚Ä¶ it takes too much code, so we can't do it.</p>
<p>Instead, we'll be <em>single-pass</em>: code generation happens <em>during parsing</em>.
We parse a bit, emit some code, parse a bit more, emit a bit more code.
So for example, here's some real code from the <code>c500</code> compiler for parsing the prefix <code>~</code> op:</p>
<pre data-lang="python"><code data-lang="python"><span># lexer.try_next() checks if the next token is ~, and if so, consumes
</span><span># and returns it (truthy)
</span><span>elif </span><span>lexer.try_next(</span><span>"~"</span><span>):
</span><span>    </span><span># prefix() parses and generates code for the expression after the ~,
</span><span>    </span><span># and load_result emits code to load it, if needed
</span><span>    meta </span><span>= </span><span>load_result(prefix())
</span><span>    </span><span># immediately start yeeting out the negation code!
</span><span>    emit(</span><span>"i32.const 0xffffffff"</span><span>)
</span><span>    emit(</span><span>"i32.xor"</span><span>)
</span><span>    </span><span># webassembly only supports 32bit types, so if this is a smaller type,
</span><span>    </span><span># mask it down
</span><span>    mask_to_sizeof(meta.type)
</span><span>    </span><span># return type information
</span><span>    </span><span>return </span><span>meta
</span></code></pre>
<p>Notice there's no syntax trees, no <code>PrefixNegateOp</code> nodes.
We see some tokens and immediately spit out the corresponding instructions.</p>
<p>You may have noticed those instructions are <em>WebAssembly</em>, which leads us into the next section...</p>
<h3 id="Using_WebAssembly,_for_some_reason?"><a href="#Using_WebAssembly,_for_some_reason?">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Using_WebAssembly,_for_some_reason?">
</a>Using WebAssembly, for some reason?</h3>
<p>So I decided to make the compiler target WebAssembly.
I honestly don't know why I did this, it really didn't make it easier‚ÄîI guess I was just curious?
WebAssembly is a really weird target, especially for C.
Besides the somewhat-external issues like spending a lot of time confused before I realized WebAssembly v2 is pretty different than WebAssembly v1, the instruction set itself is <em>weird</em>.</p>
<p>For one, there's <em>no goto</em>.
Instead, you have blocks‚Äîstructured assembly, imagine that!‚Äîand "break" instructions that jump to either the beginning or end of a specific nesting-level of block.
This was basically inconsequential for <code>if</code> and <code>while</code>, but made implementing <code>for</code> <em>extremely</em> cursed, which we'll go over later.</p>
<p>Additionally, WebAssembly doesn't have registers, it has a stack, and is a stack machine.
At first you might think that's awesome, right?
C needs a stack!
We can just use the WebAssembly stack as our C stack!
Nope, because you can't take references to the WebAssembly stack.
So instead, we need to maintain our own in-memory stack <em>anyways</em>, and then shuffle it on and off of the WASM parameter stack.</p>
<p>So in the end, I think I ended up with slightly <em>more</em> code than I would have needed to target a more normal ISA like x86 or ARM.
But it was interesting!
And theoretically, you could run code compiled with <code>c500</code> in a browser, although I haven't tried (I just use the <code>wasmer</code> CLI).</p>
<h3 id="Error_handling"><a href="#Error_handling">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Error_handling">
</a>Error handling</h3>
<p>It basically doesn't.
There's a function <code>die</code>, which is called when anything weird happens and dumps a compiler stack trace‚Äîif you're lucky, you get a line number and a somewhat-vague error message.</p>
<pre><code><span>------------------------------
</span><span>
</span><span>  File "...compiler.py", line 835, in &lt;module&gt;
</span><span>    compile("".join(fi))  # todo: make this line-at-a-time?
</span><span>  File "...compiler.py", line 823, in compile
</span><span>    global_declaration(global_frame, lexer)
</span><span>  &lt;snip&gt;
</span><span>  File "...compiler.py", line 417, in value
</span><span>    var, offset = frame.get_var_and_offset(varname)
</span><span>  File "...compiler.py", line 334, in get_var_and_offset
</span><span>    return self.parent.get_var_and_offset(name)
</span><span>  File "...compiler.py", line 336, in get_var_and_offset
</span><span>    die(f"unknown variable {n}", None if isinstance(name, str) else name.line)
</span><span>  File "...compiler.py", line 14, in die
</span><span>    traceback.print_stack()
</span><span>
</span><span>------------------------------
</span><span>
</span><span>error on line 9: unknown variable c
</span></code></pre>
<p>The Rust compiler, this is not :-)</p>
<h3 id="What_to_drop"><a href="#What_to_drop">
  <img src="https://vgel.me/permalink.svg" alt="permalink for What_to_drop">
</a>What to drop</h3>
<p>Finally, I had to decide what <em>not</em> to support, since it just wasn't feasible to get <em>all</em> of C into 500 lines. (sorry!)
I decided I wanted a really decent sampling of features that tested what the general implementation approach was capable of‚Äîfor example, if I had skipped pointers, I could have just gotten away with the WASM parameter stack and shed a lot of complexity, but that would have felt like cheating.</p>
<p>I ended up implementing the following features:</p>
<ul>
<li>arithmetic operations and binary operators, with proper precedence</li>
<li><code>int</code>, <code>short</code>, and <code>char</code> types</li>
<li>string constants (with escapes)</li>
<li>pointers (of however many levels), including correct pointer arithmetic (incrementing an <code>int*</code> adds 4)</li>
<li>arrays (only single-level, not <code>int[][]</code>)</li>
<li>functions</li>
<li>typedefs (and the lexer hack!)</li>
</ul>
<p>Notably, it doesn't support:</p>
<ul>
<li>structs :-( would be possible with more code, the fundamentals were there, I just couldn't squeeze it in</li>
<li>enums / unions</li>
<li>preprocessor directives (this would probably be 500 lines by itself...)</li>
<li>floating point. would also be possible, the <code>wasm_type</code> stuff is in, again just couldn't squeeze it in</li>
<li>8 byte types (<code>long</code>/<code>long long</code> or <code>double</code>)</li>
<li>some other small things like pre/post cremements, in-place initialization, etc., which just didn't quite fit</li>
<li>any sort of standard library or i/o that isn't returning an integer from <code>main()</code></li>
<li>casting expressions</li>
</ul>
<p>The compiler passes 34/220 test cases in the <a href="https://github.com/c-testsuite/c-testsuite">c-testsuite</a>.
More importantly to me, it can compile and run the following program successfully:</p>
<pre data-lang="c"><code data-lang="c"><span>int </span><span>swap</span><span>(</span><span>int</span><span>* </span><span>a</span><span>, </span><span>int</span><span>* </span><span>b</span><span>) {
</span><span>  </span><span>int</span><span> t;
</span><span>  t </span><span>= *</span><span>a; </span><span>*</span><span>a </span><span>= *</span><span>b; </span><span>*</span><span>b </span><span>=</span><span> t;
</span><span>  </span><span>return</span><span> t;
</span><span>}
</span><span>
</span><span>int </span><span>fib</span><span>(</span><span>int </span><span>n</span><span>) {
</span><span>  </span><span>int</span><span> a, b;
</span><span>  </span><span>for </span><span>(a </span><span>=</span><span> b </span><span>= </span><span>1</span><span>; n </span><span>&gt; </span><span>2</span><span>; n </span><span>=</span><span> n </span><span>- </span><span>1</span><span>) {
</span><span>    swap(</span><span>&amp;</span><span>a, </span><span>&amp;</span><span>b);
</span><span>    b </span><span>=</span><span> b </span><span>+</span><span> a;
</span><span>  }
</span><span>  </span><span>return</span><span> b;
</span><span>}
</span><span>
</span><span>int </span><span>main</span><span>() {
</span><span>  </span><span>return </span><span>fib(</span><span>10</span><span>); </span><span>// 55
</span><span>}
</span></code></pre>
<p>OK, enough about deciding things, let's get into the code!</p>
<h2 id="Helper_types"><a href="#Helper_types">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Helper_types">
</a>Helper types</h2>
<p>There's a small collection of helper types and classes that the compiler uses.
None of them are particularly strange, so I'll pass over them fairly quickly.</p>
<h3 id="Emitter_(compiler.py:21)"><a href="#Emitter_(compiler.py:21)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Emitter_(compiler.py:21)">
</a><code>Emitter</code> <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L21">compiler.py:21</a>)</small></h3>
<p>This is a singleton helper to emit nicely-formatted WebAssembly code.</p>
<p>WebAssembly, at least the textual format, is formatted as s-expressions, but individual instructions don't need to be parenthesized:</p>
<pre data-lang="clojure"><code data-lang="clojure"><span>(module
</span><span>  </span><span>;; &lt;snip...&gt;
</span><span>  (func $swap
</span><span>    (param $a i32)
</span><span>    (param $b i32)
</span><span>    (result i32)
</span><span>    global.get $__stack_pointer </span><span>;; prelude -- adjust stack pointer
</span><span>    i32.const </span><span>12
</span><span>    i32.sub
</span><span>    </span><span>;; &lt;snip...&gt;
</span><span>  )
</span><span>)
</span></code></pre>
<p><code>Emitter</code> just helps with emitting code with nice indentation so it's easier to read.
It also has a <code>no_emit</code> method, which will be used for an ugly hack later‚Äîstay tuned!</p>
<h3 id="StringPool_(compiler.py:53)"><a href="#StringPool_(compiler.py:53)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for StringPool_(compiler.py:53)">
</a>StringPool <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L53">compiler.py:53</a>)</small></h3>
<p><code>StringPool</code> holds all the string constants so they can be arranged in a contiguous region of memory, and hands out addresses into that for the codegen to use.
When you write <code>char *s = "abc"</code> in <code>c500</code>, what really happens is:</p>
<ol>
<li><code>StringPool</code> appends a null terminator</li>
<li><code>StringPool</code> checks if it's already stored <code>"abc"</code>, and if so, just hands that address back</li>
<li>Otherwise, <code>StringPool</code> adds it to a dictionary along with the base address + the total byte length stored so far‚Äîthe address of this new string in the pool</li>
<li><code>StringPool</code> hands <em>that</em> address back</li>
<li>When all the code is finished compiling, we create an <code>rodata</code> section with the giant concatenated string produced by <code>StringPool</code>, stored at the string pool base address (retroactively making all the addresses <code>StringPool</code> handed out valid)</li>
</ol>
<h3 id="Lexer_(compiler.py:98)"><a href="#Lexer_(compiler.py:98)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Lexer_(compiler.py:98)">
</a><code>Lexer</code> <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L98">compiler.py:98</a>)</small></h3>
<p>The <code>Lexer</code> class is complex, because lexing C is complex <small>(<code>(\\([\\abfnrtv'"?]|[0-7]{1,3}|x[A-Fa-f0-9]{1,2}))</code> is a real regex in that code for character escapes)</small>, but conceptually simple: the lexer marches along identifying what the token at the current position is.
The caller can peek that token, or it can use <code>next</code> to tell the lexer to advance, "consuming" that token.
It can also use <code>try_next</code> to conditionally advance only if the next token is a certain kind‚Äîbasically, <code>try_next</code> is a shortcut for <code>if self.peek().kind == token: return self.next()</code>.</p>
<p>There's some additionally complexity because of something called the <a href="https://en.wikipedia.org/wiki/Lexer_hack">"lexer hack"</a>.
Essentially, when parsing C you want to know if something is a type name or variable name (because that context matters for compiling certain expressions), but there's no syntactic distinction between them: <code>int int_t = 0;</code> is perfectly valid C, as is <code>typedef int int_t; int_t x = 0;</code>.</p>
<p>To know if an arbitrary token <code>int_t</code> is a type name or a variable name, we need to feed type information from the parsing/codegen stage back into the lexer.
This is a giant pain for regular compilers that want to keep their lexer, parser, and codegen modules pure and plantonically separate, but it's actually not very hard for us!
I'll explain it more when we get to the <code>typedef</code> section, but basically we just keep <code>types: set[str]</code> in <code>Lexer</code>, and when lexing, check if a token is in that set before giving it a token kind:</p>
<pre data-lang="python"><code data-lang="python"><span>if </span><span>m </span><span>:= </span><span>re.match(</span><span>r</span><span>"</span><span>^</span><span>[a-zA-Z_][a-zA-Z0-9_]</span><span>*</span><span>"</span><span>, self.src[self.loc :]):
</span><span>    tok </span><span>= </span><span>m.group(</span><span>0</span><span>)
</span><span>    </span><span>...
</span><span>    </span><span># lexer hack
</span><span>    </span><span>return </span><span>Token(TOK_TYPE </span><span>if </span><span>tok </span><span>in </span><span>self.types </span><span>else </span><span>TOK_NAME, tok, self.line)
</span></code></pre>
<h3 id="CType_(compiler.py:201)"><a href="#CType_(compiler.py:201)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for CType_(compiler.py:201)">
</a><code>CType</code> <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L201">compiler.py:201</a>)</small></h3>
<p>This is just a dataclass for representing information about a C type, like you'd write in <code>int **t</code> or <code>short t[5]</code> or <code>char **t[17]</code>, minus the <code>t</code>.</p>
<p>It contains:</p>
<ul>
<li>the type's name (with any typedefs resolved), such as <code>int</code> or <code>short</code></li>
<li>what level of pointer is is (<code>0</code> = not a pointer, <code>1</code> = <code>int *t</code>, <code>2</code> = <code>int **t</code>, and so on)</li>
<li>what the array size is (<code>None</code> = not an array, <code>0</code> = <code>int t[0]</code>, <code>1</code> = <code>int t[1]</code>, and so on)</li>
</ul>
<p>Notably, as mentioned before, this type only supports single-level arrays, and not nested arrays like <code>int t[5][6]</code>.</p>
<h3 id="FrameVar_and_StackFrame_(compiler.py:314)"><a href="#FrameVar_and_StackFrame_(compiler.py:314)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for FrameVar_and_StackFrame_(compiler.py:314)">
</a><code>FrameVar</code> and <code>StackFrame</code> <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L314">compiler.py:314</a>)</small></h3>
<p>These classes handle our C stack frames.</p>
<p>As I mentioned before, because you can't take references to the WASM stack, we have to manually handle the C stack, we can't use the WASM one.</p>
<p>To set up the C stack, the prelude emitted in <code>__main__</code> sets up a global <code>__stack_pointer</code> variable, and then every function call decrements that by however much space the function needs for its parameters and local variables‚Äîcalculated by that function's <code>StackFrame</code> instance.</p>
<p>I'll go over how that calculation works in more detail when we get to parsing functions, but essentially, each parameter and local variable gets a slot in that stack space, and increases <code>StackFrame.frame_size</code> (and thus the offset of the <em>next</em> variable) depending on its size.
The offset, type information, and other data for each parameter and local variable are stored in a <code>FrameVar</code> instance, in <code>StackFrame.variables</code>, in order of declaration.</p>
<h3 id="ExprMeta_(compiler.py:344)"><a href="#ExprMeta_(compiler.py:344)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for ExprMeta_(compiler.py:344)">
</a><code>ExprMeta</code> <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L344">compiler.py:344</a>)</small></h3>
<p>This final dataclass is used to track whether the result of an expression is a <em>value</em> or a <em>place</em>.
We need to keep track of this distinction in order to handle certain expressions differently based on how they're used.</p>
<p>For example, if you have a variable <code>x</code> of type <code>int</code>, it can be used in two ways:</p>
<ol>
<li><code>x + 1</code> wants the <em>value</em> of <code>x</code>, say <code>1</code>, to operate on</li>
<li><code>&amp;x</code> wants the <em>address</em> of <code>x</code>, say <code>0xcafedead</code></li>
</ol>
<p>When we parse the <code>x</code> expression, we can easily fetch the address from the stack frame:</p>
<pre data-lang="python"><code data-lang="python"><span># look the variable up in the `StackFrame`
</span><span>var, offset </span><span>= </span><span>frame.get_var_and_offset(varname)
</span><span># put the base address of the C stack on top of the WASM stack
</span><span>emit(</span><span>f</span><span>"global.get $__stack_pointer"</span><span>)
</span><span># add the offset (in the C stack)
</span><span>emit(</span><span>f</span><span>"i32.const </span><span>{offset}</span><span>"</span><span>)
</span><span>emit(</span><span>"i32.add"</span><span>)
</span><span># the address of the variable is now on top of the WASM stack
</span></code></pre>
<p>But now what?
If we <code>i32.load</code> this address to get the value, then <code>&amp;x</code> will have no way to get the address.
But if we don't load it, then <code>x + 1</code> will try to add one to the address, resulting in <code>0xcafedeae</code> instead of <code>2</code>!</p>
<p>That's where <code>ExprMeta</code> comes in: we leave the address on the stack, and return an <code>ExprMeta</code> indicating this is a <em>place</em>:</p>
<pre data-lang="python"><code data-lang="python"><span>return </span><span>ExprMeta(</span><span>True</span><span>, var.type)
</span></code></pre>
<p>Then, for operations like <code>+</code> that always want to operate on values instead of places, there's a function <code>load_result</code> that turns any places into values:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>load_result</span><span>(</span><span>em</span><span>: ExprMeta) -&gt; ExprMeta:
</span><span>    </span><span>"""Load a place `ExprMeta`, turning it into a value
</span><span>    `ExprMeta` of the same type"""
</span><span>    </span><span>if </span><span>em.is_place:
</span><span>        </span><span># emit i32.load, i32.load16_s, etc., based on the type
</span><span>        emit(em.type.load_ins())
</span><span>    </span><span>return </span><span>ExprMeta(</span><span>False</span><span>, em.type)
</span><span>
</span><span>...
</span><span># in the code for parsing `+`
</span><span>lhs_meta </span><span>= </span><span>load_result(parse_lhs())
</span><span>...
</span></code></pre>
<p>Meanwhile, an operation like <code>&amp;</code> just doesn't load the result, and instead leaves the address on the stack: in an important sense, <code>&amp;</code> is a no-op in our compiler, since it doesn't emit any code!</p>
<pre data-lang="python"><code data-lang="python"><span>if </span><span>lexer.try_next(</span><span>"&amp;"</span><span>):
</span><span>    meta </span><span>= </span><span>prefix()
</span><span>    </span><span>if not </span><span>meta.is_place:
</span><span>        die(</span><span>"cannot take reference to value"</span><span>, lexer.line)
</span><span>    </span><span># type of &amp;x is int* when x is int, hence more_ptr
</span><span>    </span><span>return </span><span>ExprMeta(</span><span>False</span><span>, meta.type.more_ptr())
</span></code></pre>
<p>Note also that, despite being an <em>address</em>, the result of <code>&amp;</code> <em>isn't</em> a place! <small>(The code returns an <code>ExprMeta</code> with <code>is_place=False</code>.)</small>
The result of <code>&amp;</code> should be treated like a value, since <code>&amp;x + 1</code> <em>should</em> add <code>1</code> (or rather, <code>sizeof(x)</code>) to the address.
That's why we need the place/value distinction, since just "being an address" isn't enough to know whether the result of an expression should be loaded.</p>
<p>OK, enough about helper classes.
Let's move on to the meat of codegen!</p>
<h2 id="Parsing_and_code_generation"><a href="#Parsing_and_code_generation">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Parsing_and_code_generation">
</a>Parsing and code generation</h2>
<p>The general control flow of the compiler goes like this:</p>
<center>
<img src="https://vgel.me/posts/c500/compiler-flow.drawio.svg">
</center>
<p>The blue rectangles represent the main functions of the compiler‚Äî<code>__main__</code>, <code>compile()</code>, <code>global_declaration()</code>, <code>statement()</code>, and <code>expression()</code>.
The long chain of squares at the bottom shows the operator precedence‚Äîmost of those functions are automatically generated by a higher-order function, however!</p>
<p>I'll go through the blue squares one-by-one and explain anything interesting in each.</p>
<h3 id="__main___(compiler.py:827)"><a href="#__main___(compiler.py:827)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for __main___(compiler.py:827)">
</a><code>__main__</code> <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L827">compiler.py:827</a>)</small></h3>
<p>This one is pretty short and dull.
Here it is in full:</p>
<pre data-lang="python"><code data-lang="python"><span>if </span><span>__name__ </span><span>== </span><span>"__main__"</span><span>:
</span><span>    </span><span>import </span><span>fileinput
</span><span>
</span><span>    </span><span>with </span><span>fileinput.input(</span><span>encoding</span><span>=</span><span>"utf-8"</span><span>) </span><span>as </span><span>fi:
</span><span>        </span><span>compile</span><span>(</span><span>""</span><span>.join(fi))  </span><span># todo: make this line-at-a-time?
</span></code></pre>
<p>Clearly I never finished that TODO!
The only really interesting thing here is the <code>fileinput</code> module, which you may not have heard of.
From the module docs,</p>
<blockquote>
<p>Typical use is:</p>
<pre data-lang="python"><code data-lang="python"><span>import </span><span>fileinput
</span><span>for </span><span>line </span><span>in </span><span>fileinput.input(</span><span>encoding</span><span>=</span><span>"utf-8"</span><span>):
</span><span>    process(line)
</span></code></pre>
<p>This iterates over the lines of all files listed in sys.argv[1:],
defaulting to sys.stdin if the list is empty.  If a filename is '-' it
is also replaced by sys.stdin and the optional arguments mode and
openhook are ignored.  To specify an alternative list of filenames,
pass it as the argument to input().  A single file name is also allowed.</p>
</blockquote>
<p>This means, technically, <code>c500</code> supports multiple files!
<small>(If you don't mind them all being concatenated and having messed-up line numbers :-) <code>fileinput</code> is actually fairly sophisticated and has a <code>filelineno()</code> method, I just didn't use it for space reasons.)</small></p>
<h3 id="compile()_(compiler.py:805)"><a href="#compile()_(compiler.py:805)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for compile()_(compiler.py:805)">
</a><code>compile()</code> <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L805">compiler.py:805</a>)</small></h3>
<p><code>compile()</code> is the first interesting function here, and is short enough to also include verbatim:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>compile</span><span>(</span><span>src</span><span>: </span><span>str</span><span>) -&gt; </span><span>None</span><span>:
</span><span>    </span><span># compile an entire file
</span><span>
</span><span>    </span><span>with </span><span>emit.block(</span><span>"(module"</span><span>, </span><span>")"</span><span>):
</span><span>        emit(</span><span>"(memory 3)"</span><span>)
</span><span>        emit(</span><span>f</span><span>"(global $__stack_pointer (mut i32) (i32.const </span><span>{PAGE_SIZE </span><span>* </span><span>3</span><span>}</span><span>))"</span><span>)
</span><span>
</span><span>        emit(</span><span>"(func $__dup_i32 (param i32) (result i32 i32)"</span><span>)
</span><span>        emit(</span><span>"  (local.get 0) (local.get 0))"</span><span>)
</span><span>        emit(</span><span>"(func $__swap_i32 (param i32) (param i32) (result i32 i32)"</span><span>)
</span><span>        emit(</span><span>"  (local.get 1) (local.get 0))"</span><span>)
</span><span>
</span><span>        global_frame </span><span>= </span><span>StackFrame()
</span><span>        lexer </span><span>= </span><span>Lexer(src, </span><span>set</span><span>([</span><span>"int"</span><span>, </span><span>"char"</span><span>, </span><span>"short"</span><span>, </span><span>"long"</span><span>, </span><span>"float"</span><span>, </span><span>"double"</span><span>]))
</span><span>        </span><span>while </span><span>lexer.peek().kind </span><span>!= </span><span>TOK_EOF:
</span><span>            global_declaration(global_frame, lexer)
</span><span>
</span><span>        emit(</span><span>'(export "main" (func $main))'</span><span>)
</span><span>
</span><span>        </span><span># emit str_pool data section
</span><span>        emit(</span><span>f</span><span>'(data $.rodata (i32.const </span><span>{str_pool.base}</span><span>) "</span><span>{str_pool.pooled()}</span><span>")'</span><span>)
</span></code></pre>
<p>This function handles emitting the module level prelude.</p>
<p>First, we emit a pragma for the WASM VM to reserve 3 pages of memory (<code>(memory 3)</code>), and we set the stack pointer to start at the end of that reserved region (it will grow downwards).</p>
<p>Then, we define two stack manipulation helpers <code>__dup_i32</code> and <code>__swap_i32</code>.
These should be familiar if you've ever used Forth: <code>dup</code> duplicates the item on top of the WASM stack <small>(<code>a -- a a</code>)</small>, and <code>swap</code> swaps the position of the top two items on the WASM stack <small>(<code>a b -- b a</code>)</small>.</p>
<p>Next, we initialize a stack frame to hold the global variables, initialize the lexer with the built-in typenames for the lexer hack, and chew up global declarations until we run out!</p>
<p>Finally, we export <code>main</code> and dump the string pool.</p>
<h3 id="global_declaration()_(compiler.py:743)"><a href="#global_declaration()_(compiler.py:743)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for global_declaration()_(compiler.py:743)">
</a><code>global_declaration()</code> <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L743">compiler.py:743</a>)</small></h3>
<p>This function is too long to inline the whole thing, but the signature looks like this:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>global_declaration</span><span>(</span><span>global_frame</span><span>: StackFrame, </span><span>lexer</span><span>: Lexer) -&gt; </span><span>None</span><span>:
</span><span>    </span><span># parse a global declaration -- typedef, global variable, or function.
</span><span>    </span><span>...
</span></code></pre>
<p>It handles typedefs, global variables, and functions.</p>
<p>Typedefs are cool, since this is where the lexer hack happens!</p>
<pre data-lang="python"><code data-lang="python"><span>if </span><span>lexer.try_next(</span><span>"typedef"</span><span>):
</span><span>    </span><span># yes, `typedef int x[24];` is valid (but weird) c
</span><span>    </span><span>type</span><span>, name </span><span>= </span><span>parse_type_and_name(lexer)
</span><span>    </span><span># lexer hack!
</span><span>    lexer.types.add(name.content)
</span><span>    typedefs[name.content] </span><span>= </span><span>type
</span><span>
</span><span>    lexer.next(</span><span>";"</span><span>)
</span><span>    </span><span>return
</span></code></pre>
<p>We reuse a general type-name parsing tool since typedefs inherit all of C's weird "declaration reflects usage" rules, which is convenient for us. (and less so for the perplexed newbie!)
Then we inform the lexer we've discovered a new type name, so that in the future that token will be lexed as a type name instead of a variable name.</p>
<p>Finally for typedefs, we store the type in the global typedef registry, consume the trailing semicolon, and return back to <code>compile()</code> for the next global declaration.
Importantly, the type we store is a <em>whole parsed type</em>, since if you do <code>typedef int* int_p;</code> and then later write <code>int_p *x</code>, <code>x</code> should get a resulting type of <code>int**</code>‚Äîthe pointer level is additive!
That means we can't just store the base C typename, and instead need to store an entire <code>CType</code>.</p>
<p>If the declaration <em>wasn't</em> a typedef, we parse a variable type and name.
If we find a <code>;</code> token we know it's a global variable declaration (since we don't support global initializers).
In that case, we add the global variable to the global stack frame and bail.</p>
<pre data-lang="python"><code data-lang="python"><span>if </span><span>lexer.try_next(</span><span>";"</span><span>):
</span><span>    global_frame.add_var(name.content, decl_type, </span><span>False</span><span>)
</span><span>    </span><span>return
</span></code></pre>
<p>If there's no semicolon, however, we're definitely dealing with a function.
To generate code for a function, we need to:</p>
<ol>
<li>Make a new <code>StackFrame</code> for the function, named <code>frame</code></li>
<li>Then, parse all the parameters and store them in the frame with <code>frame.add_var(varname.content, type, is_parameter=True)</code></li>
<li>After that, parse all the variable declarations with <code>variable_declaration(lexer, frame)</code>, which adds them to <code>frame</code></li>
<li>Now we know how large the function's stack frame needs to be (<code>frame.frame_size</code>), so we can start emitting the prelude!</li>
<li>First, for all the parameters in the stack frame (added with <code>is_parameter=True</code>), we generate WASM <code>param</code> declarations so the function can be called with the WASM calling convention (passing the parameters on the WASM stack):</li>
</ol>
<pre data-lang="python"><code data-lang="python"><span>for </span><span>v </span><span>in </span><span>frame.variables.values():
</span><span>    </span><span>if </span><span>v.is_parameter:
</span><span>        emit(</span><span>f</span><span>"(param $</span><span>{v.name} {v.type.wasmtype}</span><span>)"</span><span>)
</span></code></pre>
<ol start="5">
<li>Then, we can emit a <code>result</code> annotation for the return type, and adjust the C stack pointer to make space for the function's parameters and variables:</li>
</ol>
<pre data-lang="python"><code data-lang="python"><span>emit(</span><span>f</span><span>"(result </span><span>{decl_type.wasmtype}</span><span>)"</span><span>)
</span><span>emit(</span><span>"global.get $__stack_pointer"</span><span>)
</span><span># grow the stack downwards
</span><span>emit(</span><span>f</span><span>"i32.const </span><span>{frame.frame_offset </span><span>+ </span><span>frame.frame_size}</span><span>"</span><span>)
</span><span>emit(</span><span>"i32.sub"</span><span>)
</span><span>emit(</span><span>"global.set $__stack_pointer"</span><span>)
</span></code></pre>
<ol start="6">
<li>For each parameter (in reverse order, because stacks), copy it from the WASM stack to our stack:</li>
</ol>
<pre data-lang="python"><code data-lang="python"><span>for </span><span>v </span><span>in </span><span>reversed</span><span>(frame.variables.values()):
</span><span>    </span><span>if </span><span>v.is_parameter:
</span><span>        emit(</span><span>"global.get $__stack_pointer"</span><span>)
</span><span>        emit(</span><span>f</span><span>"i32.const </span><span>{frame.get_var_and_offset(v.name)[</span><span>1</span><span>]}</span><span>"</span><span>)
</span><span>        emit(</span><span>"i32.add"</span><span>)
</span><span>        </span><span># fetch the variable from the WASM stack
</span><span>        emit(</span><span>f</span><span>"local.get $</span><span>{v.name}</span><span>"</span><span>)
</span><span>        </span><span># and store it at the calculated address in the C stack
</span><span>        emit(v.type.store_ins())
</span></code></pre>
<ol start="7">
<li>Finally, we can call <code>statement(lexer, frame)</code> in a loop to codegen all the statements in the function, until we hit the closing bracket:</li>
</ol>
<pre data-lang="python"><code data-lang="python"><span>while not </span><span>lexer.try_next(</span><span>"}"</span><span>):
</span><span>    statement(lexer, frame)
</span></code></pre>
<ol start="8">
<li>Bonus step: we assume the function will always have a <code>return</code>, so we <code>emit("unreachable")</code> so the WASM analyzer doesn't freak out.</li>
</ol>
<p>Whoof!
That was a lot.
But that's all for functions, and thus for <code>global_declaration()</code>, so let's move on to <code>statement()</code>.</p>
<h3 id="statement()_(compiler.py:565)"><a href="#statement()_(compiler.py:565)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for statement()_(compiler.py:565)">
</a><code>statement()</code> <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L565">compiler.py:565</a>)</small></h3>
<p>There's a lot of code in <code>statement()</code>.
However, most of it is fairly repetitive, so I'll just explain <code>while</code> and <code>for</code>, which should give a good overview.</p>
<p>Remember how WASM doesn't have jumps, and instead has structured control flow?
That's relevant now.</p>
<p>First, let's see how it works with <code>while</code>, where it's not too much trouble.
A while loop in WASM looks like this:</p>
<pre data-lang="clojure"><code data-lang="clojure"><span>block
</span><span>  loop
</span><span>    </span><span>;; &lt;test&gt;
</span><span>    i32.eqz
</span><span>    br_if </span><span>1
</span><span>    </span><span>;; &lt;loop body&gt;
</span><span>    br </span><span>0
</span><span>  end
</span><span>end
</span></code></pre>
<p>As you can see, there are two types of blocks‚Äî<code>block</code> and <code>loop</code> (there's also an <code>if</code> block type, which I didn't use).
Each encloses some number of statements and then ends with <code>end</code>.
Inside a block, you can break with <code>br</code>, or conditionally based on the top of the WASM stack with <code>br_if</code> (there's also <code>br_table</code>, which I didn't use).</p>
<p>The <code>br</code> family takes a <em>labelidx</em> parameter, here either <code>1</code> or <code>0</code>, which is what level of block the operation applies to.
So in our while loop, the <code>br_if 1</code> applies to the outer block‚Äîindex 1, while the <code>br 0</code> applies to the inner block‚Äîindex 0. <small>(indices are always relative to the instruction in question‚Äî0 is the innermost block <em>to that instruction</em>.)</small></p>
<p>Finally, the last rule to know is that a <code>br</code> in a <code>block</code> jumps <em>forwards</em>, to the end of the <code>block</code>, whereas a <code>br</code> in a <code>loop</code> jumps <em>backwards</em>, to the beginning of the <code>loop</code>.</p>
<p>So hopefully the while loop code makes sense now!
Looking at it again,</p>
<pre data-lang="clojure"><code data-lang="clojure"><span>block
</span><span>  loop
</span><span>    </span><span>;; &lt;test&gt;
</span><span>    i32.eqz
</span><span>
</span><span>    </span><span>;; if test == 0, jump forwards (1 = labelidx of the `block`),
</span><span>    </span><span>;; out of the loop
</span><span>    br_if </span><span>1
</span><span>
</span><span>    </span><span>;; &lt;loop body&gt;
</span><span>
</span><span>    </span><span>;; unconditionally jump backwards (0 = labelidx of the `loop`).
</span><span>    </span><span>;; to the beginning of the loop
</span><span>    br </span><span>0
</span><span>  end
</span><span>end
</span></code></pre>
<p>In more normal assembly, this would correspond to:</p>
<pre data-lang="nasm"><code data-lang="nasm"><span>.loop_start
</span><span>  ;; &lt;test&gt;
</span><span>  </span><span>jz </span><span>.block_end
</span><span>  ;; &lt;loop body&gt;
</span><span>  </span><span>jmp </span><span>.loop_start
</span><span>.block_end
</span></code></pre>
<p>But with jumps, you can express things that you can't (easily) in WASM‚Äîfor example, you could jump into the middle of a block.</p>
<p><small>(This mainly is an issue for compiling C's <code>goto</code>, which I didn't even attempt‚Äîthere's an algorithm that can transform any code using <code>goto</code> into an equivalent program using structured control flow, but it's complicated and I don't think it would work with our single-pass approach.)</small></p>
<p>But for while loops, this isn't too bad.
All we have to do is:</p>
<pre data-lang="python"><code data-lang="python"><span># `emit.block` is a context manager to emit the first parameter ("block" here),
</span><span># and then the second ("end") on exit
</span><span>with </span><span>emit.block(</span><span>"block"</span><span>, </span><span>"end"</span><span>):
</span><span>    </span><span>with </span><span>emit.block(</span><span>"loop"</span><span>, </span><span>"end"</span><span>):
</span><span>        </span><span># emit code for the test, ending with `i32.eqz`
</span><span>        parenthesized_test()
</span><span>        </span><span># emit code to exit the loop if the `i32.eqz` was true
</span><span>        emit(</span><span>"br_if 1"</span><span>)
</span><span>        </span><span># emit code for the body
</span><span>        bracketed_block_or_single_statement(lexer, frame)
</span><span>        </span><span># emit code to jump back to the beginning
</span><span>        emit(</span><span>"br 0"</span><span>)
</span></code></pre>
<p>With for loops though, it gets nasty.
Consider a for loop like this:</p>
<pre data-lang="c"><code data-lang="c"><span>for </span><span>(i </span><span>= </span><span>0</span><span>; i </span><span>&lt; </span><span>5</span><span>; i </span><span>=</span><span> i </span><span>+ </span><span>1</span><span>) {
</span><span>    j </span><span>=</span><span> j </span><span>* </span><span>2 </span><span>+</span><span> i;
</span><span>}
</span></code></pre>
<p>The order the parts of the for loop will be seen by the lexer/code generator is:</p>
<ol>
<li><code>i = 0</code></li>
<li><code>i &lt; 5</code></li>
<li><code>i = i + 1</code></li>
<li><code>j = j * 2 + i</code></li>
</ol>
<p>But the order we need to put them in the code, to work with WASM's structured control flow, is:</p>
<pre><code><span>block
</span><span>  ;; &lt; code for `i = 0` (1) &gt;
</span><span>  loop
</span><span>    ;; &lt; code for `i &lt; 5` (2) &gt;
</span><span>    br_if 1
</span><span>    ;; &lt; code for `j = j * 2 + i` (4!) &gt;
</span><span>    ;; &lt; code for `i = i + 1` (3!) &gt;
</span><span>    br 0
</span><span>  end
</span><span>end
</span></code></pre>
<p>Notice that 3 and 4 are inverted in the generated code, making the order 1, 2, 4, 3.
This is a problem for a single pass compiler!
Unlike a normal compiler, we can't store the advancement statement for later.
Or‚Ä¶ can we?</p>
<p>How I ended up handling this is by making the lexer <em>cloneable</em>, and re-parsing the advancement statement <em>after</em> parsing the body.
Essentially, the code looks like:</p>
<pre data-lang="python"><code data-lang="python"><span>elif </span><span>lexer.try_next(</span><span>"for"</span><span>):
</span><span>    lexer.next(</span><span>"("</span><span>)
</span><span>    </span><span>with </span><span>emit.block(</span><span>"block"</span><span>, </span><span>"end"</span><span>):
</span><span>        </span><span># parse initializer (i = 0)
</span><span>        </span><span># (outside of loop since it only happens once)
</span><span>        </span><span>if </span><span>lexer.peek().kind </span><span>!= </span><span>";"</span><span>:
</span><span>            expression(lexer, frame)
</span><span>            emit(</span><span>"drop"</span><span>) </span><span># discard result of initializer
</span><span>        lexer.next(</span><span>";"</span><span>)
</span><span>
</span><span>        </span><span>with </span><span>emit.block(</span><span>"loop"</span><span>, </span><span>"end"</span><span>):
</span><span>            </span><span># parse test (i &lt; 5), if present
</span><span>            </span><span>if </span><span>lexer.peek().kind </span><span>!= </span><span>";"</span><span>:
</span><span>                load_result(expression(lexer, frame))
</span><span>                emit(</span><span>"i32.eqz ;; for test"</span><span>)
</span><span>                emit(</span><span>"br_if 1 ;; exit loop"</span><span>)
</span><span>            lexer.next(</span><span>";"</span><span>)
</span><span>
</span><span>            </span><span># handle first pass of advancement statement, if present
</span><span>            saved_lexer </span><span>= </span><span>None
</span><span>            </span><span>if </span><span>lexer.peek().kind </span><span>!= </span><span>")"</span><span>:
</span><span>                saved_lexer </span><span>= </span><span>lexer.clone()
</span><span>                </span><span># emit.no_emit() disables code output inside of it,
</span><span>                </span><span># so we can skip over the advancement statement for now
</span><span>                </span><span># to get to the for loop body
</span><span>                </span><span>with </span><span>emit.no_emit():
</span><span>                    expression(lexer, frame)
</span><span>            lexer.next(</span><span>")"</span><span>)
</span><span>
</span><span>            </span><span># parse body
</span><span>            bracketed_block_or_single_statement(lexer, frame)
</span><span>
</span><span>            </span><span># now that we parsed the body, go back and re-parse
</span><span>            </span><span># the advancement statement using the saved lexer
</span><span>            </span><span>if </span><span>saved_lexer </span><span>!= </span><span>None</span><span>:
</span><span>                expression(saved_lexer, frame)
</span><span>
</span><span>            </span><span># jump back to beginning of loop
</span><span>            emit(</span><span>"br 0"</span><span>)
</span></code></pre>
<p>As you can see, the hack is to save the lexer, then use <em>that</em> to go back and handle the advancement statement later, instead of saving the syntax tree like a normal compiler would.
Not very elegant‚Äîcompiling for loops is probably the gnarliest code in the compiler‚Äîbut it works well enough!</p>
<p>The other parts of <code>statement()</code> are mostly similar, so I'll skip over them to get to the last main part of the compiler‚Äî<code>expression()</code>.</p>
<h3 id="expression()_(compiler.py:375)"><a href="#expression()_(compiler.py:375)">
  <img src="https://vgel.me/permalink.svg" alt="permalink for expression()_(compiler.py:375)">
</a><code>expression()</code> <small>(<a href="https://github.com/vgel/c500/blob/e10f78891f925c2501611b848c10086ecc16ca4f/compiler.py#L375">compiler.py:375</a>)</small></h3>
<p><code>expression()</code> is the last big method in the compiler, and it handles parsing expressions, as you might expect.
It contains many inner methods, one for each precedence level, each returning the <code>ExprMeta</code> struct described earlier (which handle the "place vs value" distinction and can be turned into a value using <code>load_result</code>).</p>
<p>The bottom of the precedence stack is <code>value()</code> (somewhat confusingly named, since it can return <code>ExprMeta(is_place=True, ...)</code>).
It handles constants, parenthesized expressions, function calls, and variable names.</p>
<p>Above that, the basic pattern for a precedence level is a function like this:</p>
<pre data-lang="python"><code data-lang="python"><span> </span><span>def </span><span>muldiv</span><span>() -&gt; ExprMeta:
</span><span>    </span><span># lhs is the higher precedence operation (prefix operators, in this case)
</span><span>    lhs_meta </span><span>= </span><span>prefix()
</span><span>    </span><span># check if we can parse an operation
</span><span>    </span><span>if </span><span>lexer.peek().kind </span><span>in </span><span>(</span><span>"*"</span><span>, </span><span>"/"</span><span>, </span><span>"%"</span><span>):
</span><span>        </span><span># if so, load in the left hand side
</span><span>        lhs_meta </span><span>= </span><span>load_result(lhs_meta)
</span><span>        </span><span># grab the specific operator
</span><span>        op_token </span><span>= </span><span>lexer.next()
</span><span>        </span><span># the right hand side should use this function, for e.g. `x * y * z`
</span><span>        load_result(muldiv())
</span><span>        </span><span># emit an opcode to do the operation
</span><span>        </span><span>if </span><span>op_token </span><span>== </span><span>"*"</span><span>:
</span><span>            emit(</span><span>f</span><span>"i32.mul"</span><span>)
</span><span>        </span><span>elif </span><span>op_token </span><span>== </span><span>"/"</span><span>:
</span><span>            emit(</span><span>f</span><span>"i32.div_s"</span><span>)
</span><span>        </span><span>else</span><span>: </span><span># %
</span><span>            emit(</span><span>f</span><span>"i32.rem_s"</span><span>)
</span><span>        </span><span># mask down the result if this is a less-than-32bit type
</span><span>        mask_to_sizeof(lhs_meta.type)
</span><span>        </span><span># we produced a value (is_place=False)
</span><span>        </span><span>return </span><span>ExprMeta(</span><span>False</span><span>, lhs_meta.type)
</span><span>    </span><span># if we didn't find a token, just return the left hand side unchanged
</span><span>    </span><span>return </span><span>lhs_meta
</span></code></pre>
<p>In fact, this pattern is so consistent that most operations, including <code>muldiv</code>, aren't written out, but instead defined by a higher-order function <code>makeop</code>:</p>
<pre data-lang="python"><code data-lang="python"><span># function for generating simple operator precedence levels from declarative
</span><span># dictionaries of { token: instruction_to_emit }
</span><span>def </span><span>makeop</span><span>(
</span><span>    </span><span>higher</span><span>: Callable[[], ExprMeta], </span><span>ops</span><span>: </span><span>dict</span><span>[</span><span>str</span><span>, </span><span>str</span><span>], </span><span>rtype</span><span>: CType </span><span>| </span><span>None </span><span>= </span><span>None
</span><span>) -&gt; Callable[[], ExprMeta]:
</span><span>    </span><span>def </span><span>op</span><span>() -&gt; ExprMeta:
</span><span>        lhs_meta </span><span>= </span><span>higher()
</span><span>        </span><span>if </span><span>lexer.peek().kind </span><span>in </span><span>ops.keys():
</span><span>            lhs_meta </span><span>= </span><span>load_result(lhs_meta)
</span><span>            op_token </span><span>= </span><span>lexer.next()
</span><span>            load_result(op())
</span><span>            </span><span># TODO: type checking?
</span><span>            emit(</span><span>f</span><span>"</span><span>{ops[op_token.kind]}</span><span>"</span><span>)
</span><span>            mask_to_sizeof(rtype </span><span>or </span><span>lhs_meta.type)
</span><span>            </span><span>return </span><span>ExprMeta(</span><span>False</span><span>, lhs_meta.type)
</span><span>        </span><span>return </span><span>lhs_meta
</span><span>
</span><span>    </span><span>return </span><span>op
</span><span>
</span><span>muldiv </span><span>= </span><span>makeop(prefix, {</span><span>"*"</span><span>: </span><span>"i32.mul"</span><span>, </span><span>"/"</span><span>: </span><span>"i32.div_s"</span><span>, </span><span>"%"</span><span>: </span><span>"i32.rem_s"</span><span>})
</span><span>...
</span><span>shlr </span><span>= </span><span>makeop(plusminus, {</span><span>"&lt;&lt;"</span><span>: </span><span>"i32.shl"</span><span>, </span><span>"&gt;&gt;"</span><span>: </span><span>"i32.shr_s"</span><span>})
</span><span>cmplg </span><span>= </span><span>makeop(
</span><span>    shlr,
</span><span>    {</span><span>"&lt;"</span><span>: </span><span>"i32.lt_s"</span><span>, </span><span>"&gt;"</span><span>: </span><span>"i32.gt_s"</span><span>, </span><span>"&lt;="</span><span>: </span><span>"i32.le_s"</span><span>, </span><span>"&gt;="</span><span>: </span><span>"i32.ge_s"</span><span>},
</span><span>    CType(</span><span>"int"</span><span>),
</span><span>)
</span><span>cmpe </span><span>= </span><span>makeop(cmplg, {</span><span>"=="</span><span>: </span><span>"i32.eq"</span><span>, </span><span>"!="</span><span>: </span><span>"i32.ne"</span><span>}, CType(</span><span>"int"</span><span>))
</span><span>bitand </span><span>= </span><span>makeop(cmpe, {</span><span>"&amp;"</span><span>: </span><span>"i32.and"</span><span>})
</span><span>bitor </span><span>= </span><span>makeop(bitand, {</span><span>"|"</span><span>: </span><span>"i32.or"</span><span>})
</span><span>xor </span><span>= </span><span>makeop(bitor, {</span><span>"^"</span><span>: </span><span>"i32.xor"</span><span>})
</span><span>...
</span></code></pre>
<p>Only a few operations with special behavior need to be defined explicitly, like <code>plusminus</code> which needs to handle the nuances of C pointer math.</p>
<p>And that's it!
That's the last main piece of the compiler.</p>
<h2 id="Wrapping_up..."><a href="#Wrapping_up...">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Wrapping_up...">
</a>Wrapping up...</h2>
<p>That's been our tour of the <a href="https://github.com/vgel/c500/">C compiler in 500 lines of Python</a>!
Compilers have a reputation for being complex‚ÄîGCC and Clang are massive, and even TCC, the <em>Tiny</em> C Compiler, is tens of thousands of lines of code‚Äîbut if you're willing to sacrifice code quality and do everything in a single pass, they can be surprisingly compact!</p>
<p>I'd be interested to hear if you write your own single-pass compiler‚Äîmaybe for a custom language?
I think this kind of compiler could potentially be a great stage0 for a self-hosted language, since it's so simple.</p>
<p>Next time, this blog will be back to regularly-scheduled LLM posting with a post about making a small transformer by hand!</p>
<pre data-lang="python"><code data-lang="python"><span>MODEL </span><span>= </span><span>{
</span><span>    </span><span># EMBEDDING USAGE
</span><span>    </span><span>#  P = Position embeddings (one-hot)
</span><span>    </span><span>#  T = Token embeddings (one-hot, first is `a`, second is `b`)
</span><span>    </span><span>#  V = Prediction scratch space
</span><span>    </span><span>#
</span><span>    </span><span>#       [P, P, P, P, P, T, T, V]
</span><span>    </span><span>"wte"</span><span>: np.array(
</span><span>        </span><span># one-hot token embeddings
</span><span>        [
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># token `a` (id 0)
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>],  </span><span># token `b` (id 1)
</span><span>        ]
</span><span>    ),
</span><span>    </span><span>"wpe"</span><span>: np.array(
</span><span>        </span><span># one-hot position embeddings
</span><span>        [
</span><span>            [</span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 0
</span><span>            [</span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 1
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 2
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 3
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 4
</span><span>        ]
</span><span>    ),
</span><span>    </span><span>...</span><span>: </span><span>...
</span><span>}
</span></code></pre>
<p>If that sounds interesting, or you want to see more posts like this, consider <a href="https://twitter.com/voooooogel/">following me on Twitter</a> or subscribing to my mailing list to get updates on new posts!</p>

<p>If you have thoughts about this post, please feel free to <a href="https://vgel.me/contact">get in touch</a>!
<small>(Even if you just want to say "that was cool" or want to ask a clarifying question‚Äîdon't feel like it needs to be capital-I-Important!)</small></p>
<p>And if you're still around, you must really like the blog, so here's some more stuff to check out :-)</p>
<ul>
<li><a href="https://vgel.me/posts">My other blog posts</a>, such as:
<ul>
<li><a href="https://vgel.me/posts/donut">Signed distance functions in 46 lines of Python</a></li>
<li><a href="https://vgel.me/posts/tools-not-needed/">GPT-3 will ignore tools when it disagrees with them</a></li>
<li><a href="https://vgel.me/posts/mmap-arena-alloc">mmap(1Tb): A Rust arena allocator (ab)using Linux overcommit</a></li>
<li><a href="https://vgel.me/posts/gpt4-javascript">Does GPT-4 think better in Javascript?</a></li>
</ul>
</li>
<li><a href="https://vgel.me/">My other projects</a>, including <a href="https://vgel.me/fiction">my short fiction</a></li>
<li>My <a href="https://twitter.com/voooooogel/">Twitter</a></li>
</ul>
<hr>

<!---->


    <ul>
      
        <li><strong>Previous entry:</strong> <a href="https://vgel.me/posts/adversarial-training-data/">I'm worried about adversarial training data</a></li>
      
      
    </ul>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spectrolite ‚Äì Mac app to make colorful risograph prints and zines more easily (137 pts)]]></title>
            <link>https://spectrolite.app</link>
            <guid>37383749</guid>
            <pubDate>Mon, 04 Sep 2023 19:00:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrolite.app">https://spectrolite.app</a>, See on <a href="https://news.ycombinator.com/item?id=37383749">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Home insurers cut natural disasters from policies as climate risks grow (112 pts)]]></title>
            <link>https://www.washingtonpost.com/business/2023/09/03/natural-disaster-climate-insurance/</link>
            <guid>37383548</guid>
            <pubDate>Mon, 04 Sep 2023 18:44:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/business/2023/09/03/natural-disaster-climate-insurance/">https://www.washingtonpost.com/business/2023/09/03/natural-disaster-climate-insurance/</a>, See on <a href="https://news.ycombinator.com/item?id=37383548">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/business/2023/09/03/natural-disaster-climate-insurance/: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Chinchilla‚Äôs death (156 pts)]]></title>
            <link>https://espadrine.github.io/blog/posts/chinchilla-s-death.html</link>
            <guid>37383413</guid>
            <pubDate>Mon, 04 Sep 2023 18:31:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://espadrine.github.io/blog/posts/chinchilla-s-death.html">https://espadrine.github.io/blog/posts/chinchilla-s-death.html</a>, See on <a href="https://news.ycombinator.com/item?id=37383413">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <article>

<blockquote>
<p>‚ÄúWith more careful calculations, one can win; with less, one cannot‚Äù
‚Äî Sun Tzu, <em>The Art of War</em>.</p>
</blockquote>
<p>Making extrapolations is crucial to avoid wasting our computing power on slow
convergence. After all, if you had to walk to the Everest,
you wouldn‚Äôt eyeball it: you would use a GPS.</p>
<p>Sometimes you have to look away from the GPS and onto the road, though.
Sometimes things don‚Äôt extrapolate through simple formulae.
It was true for XIXth-century physicists with the <a href="https://en.wikipedia.org/wiki/Ultraviolet_catastrophe">ultraviolet catastrophe</a>;
it is true for LLMs too.
What we estimate to be true near the center can deviate widely in the far lands‚Ä¶</p>
<p><img src="https://i.imgur.com/Mf85NuW.png" alt="Image of Minecraft far lands: a terrain that suddenly becomes distorted and overlaps itself cliffly"></p>
<h2 id="What_s_this_Chinchilla_thing_anyway_">What‚Äôs this Chinchilla thing anyway? </h2>
<p>Smaller models have fewer multiplications.
Thus they run faster. Thus they train faster.
However, the theory goes, they eventually reach the limit of their capacity for
knowledge, and their learning slows, while that of a larger model,
with a larger capacity, will overtake them and reach better performance
past a given amount of training time.</p>
<p>While estimating how to get the best bang for the buck during training,
both <a href="https://arxiv.org/abs/2001.08361">OpenAI</a> and <a href="https://arxiv.org/abs/2203.15556">DeepMind</a> attempted to draw the Pareto
frontier. They don‚Äôt state explicitly that they use that theory to draw it;
the closest quote that hints at this hidden assumption is from OpenAI:</p>
<blockquote>
<p>We expect that larger models should always perform better than smaller models.
[‚Ä¶]
A model with fixed size will be capacity-limited.</p>
</blockquote>
<p>This presumption is the bedrock of how they compute the Pareto frontier.
In the Chinchilla work, figure 2 shows the training loss of a large number of
training runs for models with varying size.
At a first glance, those curves follow the theory:
the smaller models initially have a lower loss (good),
but eventually it slows down,
and gets overtaken by the curve from a larger model (bad).</p>
<p><img src="https://espadrine.github.io/blog/assets/chinchilla-s-death/chinchilla.png" alt="Chinchilla graph comparing the loss curves for many different model sizes"></p>
<p>In that chart, they drew grey dots every time they pinpointed the smaller model
starting to lose out to a larger model.
The grey line, the Pareto frontier, is how they computed their scaling laws.</p>
<p>The problem with this assumption is that
we have no idea what would happen if we let the smaller model train for longer,
since they stopped its training as soon as it was overtaken.</p>
<p>Enter the LLaMA paper.</p>
<h2 id="Can_Chinchillas_picture_a_Llama_s_sights_">Can Chinchillas picture a Llama‚Äôs sights? </h2>
<p>Earlier this year, Meta trained four models with varying sizes.
Unlike other works, they trained each of them for a very large amount of time;
even the smaller ones.</p>
<p>They published the training run curves:</p>
<p><img src="https://espadrine.github.io/blog/assets/chinchilla-s-death/llama1-training.png" alt="Training loss curves for the four LLaMA model sizes"></p>
<ol>
<li>Each curve first plummets in a <strong>power law</strong>,</li>
<li>and then seemingly enters a <strong>nearly-linear</strong> decrease in loss
(corresponding to a fairly constant rate of knowledge acquisition).</li>
<li>At the very tip of the curve, they all break this line by <strong>flattening</strong>
slightly.</li>
</ol>
<p>Right off the bat, I want to tackle a subtle misconception that people can have
related to the end-of-curve flattening.
They are all trained with gradient descent using a variable learning rate
(which is, roughly,
a hyperparameter for how much to go in the direction of the gradient).
To get a good training, they had to constantly decrease the learning rate,
so that it can detect ever-subtler patterns in the source material.
The formula they use for that decrease is the most widely used:
the cosine schedule.</p>
<p><img src="https://espadrine.github.io/blog/assets/chinchilla-s-death/warmup_cosine_schedule.png" alt="Learning rate as a function of training steps under a cosine schedule with
warmup: it first increases linearly, then slopes down with increasing speed,
before reaching an inflection point halfway and slowing down ever slower. Image from Huggingface documentation"></p>
<p>As you can see from the graph, towards the end of the training run,
the cosine schedule stops decreasing the learning rate at the speed which
yielded such a good, near-linear training loss curve.
The slowdown in learning is an artefact of that.
The model does not necessarily cease to have
the capacity to learn at the same near-linear rate!
In fact, if we had more text to give it,
we would have stretched the cosine schedule,
so its learning rate would have continued to go down at the same rate.</p>
<p>The model‚Äôs fitness landscape does not depend on the amount of data
we can feed its training; so the change in learning rate decrease
is not well-justified.</p>
<p>That is not the main point of this article, though.</p>
<p>The training loss curve can be misleading in another way.
Sure, they are all trained on the same data;
but they don‚Äôt go through that data at the same speed.
What we want to know is <strong>not</strong> how sample-efficient the model is
(on this front, the larger model clearly learns more from what it saw).
Let‚Äôs picture instead a race:
all those models start at the same time,
and we want to know which one crosses the finish line first.
In other words, when throwing a fixed amount of compute at the training,
who learns the most in that time?</p>
<p>Thankfully, we can combine the loss curves with another piece of data that Meta
provided: the amount of time that each model took to train.</p>
<table>
 <tbody><tr><th>   Model   </th><th> GPU-hours </th><th> Tokens/second </th>
 </tr><tr><td> LLaMA1-7B  </td><td>   82432  </td><td>    3384.3    </td>
 </tr><tr><td> LLaMA1-13B </td><td>  135168  </td><td>    2063.9    </td>
 </tr><tr><td> LLaMA1-33B </td><td>  530432  </td><td>     730.5    </td>
 </tr><tr><td> LLaMA1-65B </td><td> 1022362  </td><td>     379.0    </td>
</tr></tbody></table>
<p><img src="https://espadrine.github.io/blog/assets/chinchilla-s-death/llama1-training-speed.svg" alt="LLaMA 1 training loss vs GPU-hours spent"></p>
<p><a href="https://github.com/espadrine/espadrine.github.com/blob/master/blog/assets/chinchilla-s-death/llama-data.py"><em>(Code for generating the graph here.)</em></a></p>
<p>Let‚Äôs first mention that the whole Chinchilla graph that we saw,
covers only a small sliver on the left of this graph.
In that sliver, we see the same behaviour that Chinchilla documents.
Look at the 7B, for instance (which in the Chinchilla graph would actually be
among the top two curves in terms of size):
it initially drops its loss much faster than the bigger models, then slows down,
and the 13B model overtakes it and reaches 1.9 first.</p>
<p>But then, comes a far-lands, unexpected twist: the 7B enters a near-linear
regime, with a steep downward trend, and seems on its way to maybe overpass the
13B again? It is hard to tell on that graph what would happen if the 7B was
trained for longer.</p>
<p>However, the same behaviour seemed to be true between the 13B and the 33B,
where the initial Chinchilla slowdown also gives way to a near-linear regime,
at which point the 13B goes down fast! It is only surpassed by the 33B unfairly,
by granting the latter more than double the compute time.</p>
<p>And the same slowdown-then-speedup occurs between the 33B and the 65B,
to such an extent that the 33B never actually gets overtaken by the 65B.
What the graph shows breaks OpenAI‚Äôs and Chinchilla‚Äôs assumption:
<strong>the bigger model hasn‚Äôt won</strong> (yet).
The slowdown they detected is not actually caused by reaching some capacity limit!</p>
<p>Still, that 7B line is a bit unsatisfactory.
If only Meta had trained it for longer‚Ä¶</p>
<p>Suspense over: they did! They released LLaMA 2 this week!</p>
<h2 id="Time_to_confirm_our_suspicions">Time to confirm our suspicions </h2>
<p><img src="https://espadrine.github.io/blog/assets/chinchilla-s-death/llama2-training.png" alt="Training loss curves for the four LLaMA 2 model sizes"></p>
<p>We also, again, got the training times:</p>
<table>
 <tbody><tr><th>   Model   </th><th> GPU-hours </th><th> Tokens/second </th>
 </tr><tr><td> LLaMA2-7B  </td><td>  184320  </td><td>    3031.9    </td>
 </tr><tr><td> LLaMA2-13B </td><td>  368640  </td><td>    1515.9    </td>
 </tr><tr><td> LLaMA2-34B </td><td> 1038336  </td><td>     533.7    </td>
 </tr><tr><td> LLaMA2-70B </td><td> 1720320  </td><td>     322.1    </td>
</tr></tbody></table>
<p><img src="https://espadrine.github.io/blog/assets/chinchilla-s-death/llama2-training-speed.svg" alt="LLaMA 2 training loss vs GPU-hours spent"></p>
<p>Immediately, at a glance, we notice that the training curves don‚Äôt match those
of LLaMA 1, even when the models are identical.
As it turns out, LLaMA 2 was trained on double the context size,
and a longer cosine schedule, which unfortunately
has negatively impacted all model sizes.
However, smaller models have been impacted worse than larger ones.
As a result, the 34B model, which in LLaMA 1 remained always better than the 65B
model at any training time spent, now dips slightly above the 70B model,
before overtaking it:</p>
<p><img src="https://espadrine.github.io/blog/assets/chinchilla-s-death/llama-training-speed-comparison.webp" alt="LLaMA 1 vs 2 training loss vs GPU-hours spent"></p>
<p>More importantly, comparing the training speeds strongly confirms our suspicions
from LLaMA 1:</p>
<ol>
<li>First, they are faster than bigger models,</li>
<li>Then, they slow down, and are overtaken by larger models (as per
Chinchilla),</li>
<li>BUT THEN, they enter the near-linear regime, in which smaller models have a
steeper descent into superior knowledge, and they overtake larger models
yet again!</li>
</ol>
<p>A fascinating consequence ties into making the right choices
when starting a training run:
contrary to popular belief, <strong>larger models yield worse results</strong>.
If you had to pick a parameter size and dataset, you might be better off opting
for a 7B model and training for 7 epochs on trillions of tokens.</p>
<p>Look at the near-linear regime of the 7B model, and extrapolate its line to when
the 70B model stopped:
had the 70B computation been spent on the 7B instead,
it would potentially have reached a lower perplexity!</p>
<p>Another thing we notice from LLaMA 2 is that the learning slowdown at the end of
the LLaMA 1 curves was indeed an artefact of the cosine schedule.
That slowdown is completely absent from the LLaMA 2 training run at the
corresponding mark of 1 trillion tokens read.</p>
<p>In fact, maybe the reason that, at that same mark, the LLaMA 2 7B model has a
worse quality than the LLaMA 1 7B model had,
may be because <em>its cosine schedule is stretched</em>!</p>
<p>Let‚Äôs go back to the Chinchilla paper to argue that point.
In appendix A, figure A1, they show an ablation study for various cosine
schedule parameters (phrased another way:
various ways to stretch the learning rate curve).</p>
<p><img src="https://espadrine.github.io/blog/assets/chinchilla-s-death/chinchilla-cosine-ablation-study.png" alt="Chinchilla cosine schedule ablation study"></p>
<p>They make the point that the lowest loss is achieved when the curve is not
stretched. That is supported by the graphs, but we notice something off.
After reading 6 million tokens, the training loss at the top is below 2.8;
meanwhile, at the same mark, the training loss of the bottom model is above.
Yet the only difference between the models is the cosine schedule!
Because the bottom model was slated to go through more training data,
the ‚Äúunstretched‚Äù cosine schedule was computed for a bigger number of steps,
which effectively stretches it.
If the learning rate had instead followed
the schedule assigned to fewer training steps,
it would have had a better loss for the same amount of training time.</p>
<p>More broadly, that raises a question that I leave open:
if the cosine schedule is not optimal,
how should the shape of its tail be instead?</p>

    
  </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tidal energy is not renewable (359 pts)]]></title>
            <link>https://cs.stanford.edu/people/zjl/tide.html</link>
            <guid>37383283</guid>
            <pubDate>Mon, 04 Sep 2023 18:17:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cs.stanford.edu/people/zjl/tide.html">https://cs.stanford.edu/people/zjl/tide.html</a>, See on <a href="https://news.ycombinator.com/item?id=37383283">Hacker News</a></p>
<div id="readability-page-1" class="page">
	
	
	
	
	<div><p><a href="https://cs.stanford.edu/people/zjl/menu.html">List of Papers</a></p></div>
    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Geo Guesser identifies the location and seat number from an aerial shot (224 pts)]]></title>
            <link>https://twitter.com/georainbolt/status/1698553197684777069</link>
            <guid>37383245</guid>
            <pubDate>Mon, 04 Sep 2023 18:13:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/georainbolt/status/1698553197684777069">https://twitter.com/georainbolt/status/1698553197684777069</a>, See on <a href="https://news.ycombinator.com/item?id=37383245">Hacker News</a></p>
Couldn't get https://twitter.com/georainbolt/status/1698553197684777069: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Medical-evidence giant Cochrane battles funding cuts and closures (108 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-02741-z</link>
            <guid>37382232</guid>
            <pubDate>Mon, 04 Sep 2023 16:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-02741-z">https://www.nature.com/articles/d41586-023-02741-z</a>, See on <a href="https://news.ycombinator.com/item?id=37382232">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02741-z/d41586-023-02741-z_25990172.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02741-z/d41586-023-02741-z_25990172.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="A 77-year-old patient with signs of stroke undergoes an MRI." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02741-z/d41586-023-02741-z_25990172.jpg">
  <figcaption>
   <p><span>Specialist stroke units in hospitals were adopted worldwide after a seminal review on their efficacy, which became one of Cochrane‚Äôs famed systematic reviews.</span><span>Credit: BSIP/Education Images/Universal Images Group via Getty</span></p>
  </figcaption>
 </picture>
</figure><p>This month, more than 1,000 people will gather in London for a meeting of Cochrane, the group known for its gold-standard reviews of evidence in medicine. The conference marks the 30th anniversary of an organization that helped to spark a worldwide movement to base health care on research.</p><p>But in the hallways, some attendees will be discussing whether and how the group can survive. In March, 19 of 52 groups that produce Cochrane‚Äôs systematic reviews closed after the UK National Institute for Health and Care Research (NIHR) stopped funding them. And in July, Cochrane UK in Oxford ‚Äî where the group was founded ‚Äî revealed that it will close next March, after the NIHR ceased its support.</p><p>The closures come amid a major reorganization that will change how Cochrane produces and publishes systematic reviews, instigated partly in anticipation of funding difficulties. Some researchers are concerned that Cochrane will not be able to maintain its output of reviews ‚Äî which shape the clinical guidelines used by doctors worldwide ‚Äî or meet the growing demand for more complex, timely evidence syntheses. ‚ÄúI don‚Äôt see a very clear or bright future for Cochrane,‚Äù says Gabriel Rada, a specialist in evidence-based health care at the Pontifical Catholic University of Chile, Santiago, who previously directed Cochrane Chile.</p><p>‚ÄúWhat‚Äôs happened to groups in the UK is very sad,‚Äù says Karla Soares-Weiser, editor-in-chief of the organization‚Äôs database of evidence, the Cochrane Library in London. ‚ÄúWe want Cochrane to be here for the next generation. And there‚Äôs adjustments that we have to make to guarantee that this organization is sustainable for the future.‚Äù</p><p>Cochrane says that the lost NIHR funding ‚Äî around ¬£4.2 million (US$5.3 million) ‚Äî does not affect its core income, which was ¬£8.9 million in 2022. However, a more existential threat looms. Around ¬£6.8 million of that core income came from subscriptions to the Cochrane Library, but Cochrane aims to make all its reviews open access by 2025, putting the revenue at risk. Catherine Spencer, Cochrane‚Äôs chief executive in London, says that the group is seriously examining how to make that move ‚Äúin a way that would ensure that Cochrane is viable into the future‚Äù.</p><h2>Radical reviews</h2><p>The Cochrane UK centre slated for closure is symbolically important. The Cochrane Collaboration was launched there at a 1993 meeting, convened by physician and researcher Iain Chalmers, that attendees still speak of with zeal.</p><p>At the time, decisions in Western medicine tended to be based on conventional wisdom and the opinion of the most senior physician in the room. Chalmers and others at the meeting had the then-radical idea that medical practice should be based on systematic reviews of rigorous research evidence ‚Äî such as randomized controlled trials ‚Äî showing whether a treatment is effective. The group was named after physician and epidemiologist Archie Cochrane, who had championed evidence from randomized trials in previous decades.</p><p>The group started as a grass-roots organization with a decentralized structure powered by passionate academics who worked for free, and became central in the rise of evidence-based medicine. It established a series of mostly autonomous groups around the world, responsible for producing systematic reviews in areas such as stroke, movement disorders and infectious diseases.</p><p>To produce reviews, researchers follow standardized methods to find and analyse all the rigorous evidence on a question such as whether a therapy helps or harms. Systematic reviews are valued for their ability to draw conclusions from multiple, conflicting studies, like extracting a signal from noise. Cochrane developed a reputation for particularly rigorous methods and reviews.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02741-z/d41586-023-02741-z_25990388.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02741-z/d41586-023-02741-z_25990388.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Registration stand at the 2016 Cochrane Colloquium in Seoul, South Korea." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02741-z/d41586-023-02741-z_25990388.jpg">
  <figcaption>
   <p><span>Cochrane has more than 11,000 members worldwide who are involved in synthesizing or disseminating clinical evidence.</span><span>Credit: The Cochrane Collaboration</span></p>
  </figcaption>
 </picture>
</figure><p>The UK National Health Service was an early funder of Cochrane and its UK-based groups. Later, the NIHR provided support, mainly paying for support staff to help produce reviews.</p><p>So when the NIHR confirmed in August 2021 that it would stop funding all the UK-based review groups, the news came as a shock, says Peter Langhorne, a stroke researcher at the University of Glasgow, UK, who was a coordinating editor for Cochrane‚Äôs Stroke Group until 2020. In 1993, Langhorne and his colleagues independently published a seminal systematic review<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>, which was later regularly updated as part of Cochrane, that showed the effectiveness of specialist stroke units. This led to their widespread adoption, saving tens of thousands of lives.</p><h2>Long shadow</h2><p>The Stroke Group‚Äôs closure meant that three people lost their jobs, says Langhorne, who is concerned that some important systematic reviews won‚Äôt now be done. ‚ÄúI think it‚Äôs a real danger that the priorities of patients could be lost,‚Äù he says. Around one-third of Cochrane‚Äôs reviews in 2022 came from UK groups that have now closed.</p><p>But the writing had long been on the wall. A <a href="https://www.journalslibrary.nihr.ac.uk/downloads/other-nihr-research/evaluation-of-NIHR-investment-in-cochrane/NIHR_Cochrane_Report_Feb_17.pdf" data-track="click" data-label="https://www.journalslibrary.nihr.ac.uk/downloads/other-nihr-research/evaluation-of-NIHR-investment-in-cochrane/NIHR_Cochrane_Report_Feb_17.pdf" data-track-category="body text link">2017 review of the NIHR‚Äôs investment in Cochrane</a> found considerable differences in productivity and review quality between groups. It also noted that reviews were slow to produce, and that many published reviews were out of date or did not address priority topics. What‚Äôs more, the same specialist group that helped authors to produce a review would decide whether it was fit to publish, raising concerns within and outside Cochrane about editorial standards. (Cochrane has acknowledged many of these concerns in reports that highlight the need for reform.)</p><p>The wider research community also criticized the NIHR for putting all its money for research synthesis into Cochrane, says ≈Ωarko Alfireviƒá, a specialist in fetal and maternal medicine at the University of Liverpool, UK, who was coordinating editor for Cochrane‚Äôs now-closed Pregnancy and Childbirth Group. That made sense when ‚ÄúCochrane was the only show in town‚Äù, he says, but now ‚Äúthe whole industry of research synthesis is massive‚Äù.</p><p>The NIHR said in a statement that it remains committed to supporting evidence-informed practice in health care. In May, it announced that it had awarded ¬£22.5 million over five years to nine other groups as part of a new evidence-synthesis programme.</p><h2>Growing pains</h2><p>Cochrane now has more than 11,000 members involved in synthesizing or disseminating evidence worldwide. It has published more than 16,000 reviews and has been central in stimulating the now-copious production of systematic reviews and other evidence syntheses. ‚ÄúIt shifted the ground,‚Äù says Paul Garner, professor emeritus in public health at the Liverpool School of Tropical Medicine and former head of Cochrane‚Äôs Infectious Diseases Group. ‚ÄúIt was a tremendous example of rapid diffusion of a technology.‚Äù A 2021 study<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> found that more than 80 medical systematic reviews were published every day in 2019; around 7% were Cochrane reviews.</p><p>The group has been no stranger to criticism and controversy. Some members dislike how the grass-roots community has morphed into a more business-like, centralized organization. ‚ÄúThat‚Äôs not the Cochrane that we knew,‚Äù says Nancy Santesso, a health researcher at McMaster University in Hamilton, Canada, and deputy director of Cochrane Canada.</p><p>Yet she and others acknowledge that reform was necessary as the organization grew ‚Äî and that such changes are always difficult. ‚ÄúThe problem is that you inevitably become corporate,‚Äù says Alfireviƒá, ‚Äúand academics, by definition, hate being told what to do.‚Äù</p><h2>Broad shake-up</h2><p>Cochrane‚Äôs reorganization aims to address many criticisms ‚Äî for example, it is centralizing all editorial processes and separating them from review development to ensure that reviews are of consistent quality. It is trying to make it easier and quicker to produce reviews by creating evidence-synthesis units, as well as externally funded ‚Äòthematic groups‚Äô that represent broad areas, such as health equity and global ageing. Soares-Weiser says Cochrane‚Äôs preliminary data suggest that the throughput of reviews could be maintained despite the UK cuts. She adds that the organization is developing a scientific strategy to focus on high-value reviews in areas aligned with the United Nations Sustainable Development Goals.</p><p>The upcoming London meeting, from 4‚Äì6 September, is seen as particularly significant because of the upheaval ‚Äî and because it‚Äôs the first in-person Cochrane colloquium in five years, owing partly to the COVID-19 pandemic. Santesso has attended every colloquium since 2002, but this year, ‚ÄúIf I go and there isn‚Äôt that scientific strength there, then I‚Äôm not sure I would go again,‚Äù she says.</p><p>Even those critical of Cochrane say it‚Äôs important that the group survives. Philippe Ravaud, an epidemiologist at Paris City University who led Cochrane France until 2019, argues that improving evidence syntheses requires major reforms, including working with researchers to improve the planning and quality of the clinical trials that will be synthesized. ‚ÄúThere is no organization aside from Cochrane that can do that,‚Äù he says.</p><p>Chalmers, who left the organization in 2003, says he has no sentimentality about the Cochrane collaboration, but says that its function remains as important as ever. ‚ÄúThere is no argument about trying to get better, more valid, up-to-date information in the hands of patients and clinicians,‚Äù he says. ‚ÄúIf the organization didn‚Äôt exist, something like it would need to be invented.‚Äù</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can we talk to whales? (147 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2023/09/11/can-we-talk-to-whales</link>
            <guid>37382117</guid>
            <pubDate>Mon, 04 Sep 2023 16:37:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2023/09/11/can-we-talk-to-whales">https://www.newyorker.com/magazine/2023/09/11/can-we-talk-to-whales</a>, See on <a href="https://news.ycombinator.com/item?id=37382117">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><figure data-testid="IframeEmbed"></figure><blockquote><div><p>Ah, the world! Oh, the world!</p><inline-embed name="align-right" attrs="[object Object]" childtypes="" contenttype="callout:align-right"><p><em>‚Äî‚ÄúMoby-Dick.‚Äù</em></p></inline-embed></div></blockquote><p>David Gruber began his almost impossibly varied career studying bluestriped grunt fish off the coast of Belize. He was an undergraduate, and his job was to track the fish at night. He navigated by the stars and slept in a tent on the beach. ‚ÄúIt was a dream,‚Äù he recalled recently. ‚ÄúI didn‚Äôt know what I was doing, but I was performing what I thought a marine biologist would do.‚Äù</p><p>Gruber went on to work in Guyana, mapping forest plots, and in Florida, calculating how much water it would take to restore the Everglades. He wrote a Ph.D. thesis on carbon cycling in the oceans and became a professor of biology at the City University of New York. Along the way, he got interested in green fluorescent proteins, which are naturally synthesized by jellyfish but, with a little gene editing, can be produced by almost any living thing, including humans.</p><p>While working in the Solomon Islands, northeast of Australia, Gruber discovered dozens of species of fluorescent fish, including a fluorescent shark, which opened up new questions. What would a fluorescent shark look like to another fluorescent shark? Gruber enlisted researchers in optics to help him construct a special ‚Äúshark‚Äôs eye‚Äù camera. (Sharks see only in blue and green; fluorescence, it turns out, shows up to them as greater contrast.) Meanwhile, he was also studying creatures known as comb jellies at the Mystic Aquarium, in Connecticut, trying to determine how, exactly, they manufacture the molecules that make them glow. This led him to wonder about the way that jellyfish experience the world. Gruber enlisted another set of collaborators to develop robots that could handle jellyfish with jellyfish-like delicacy.</p><p>‚ÄúI wanted to know: Is there a way where robots and people can be brought together that builds empathy?‚Äù he told me.</p><p>In 2017, Gruber received a fellowship to spend a year at the Radcliffe Institute for Advanced Study, in Cambridge, Massachusetts. While there, he came across a book by a free diver who had taken a plunge with some sperm whales. This piqued Gruber‚Äôs curiosity, so he started reading up on the animals.</p><p>The world‚Äôs largest predators, sperm whales spend most of their lives hunting. To find their prey‚Äîgenerally squid‚Äîin the darkness of the depths, they rely on echolocation. By means of a specialized organ in their heads, they generate streams of clicks that bounce off any solid (or semi-solid) object. Sperm whales also produce quick bursts of clicks, known as codas, which they exchange with one another. The exchanges seem to have the structure of conversation.</p><p>One day, Gruber was sitting in his office at the Radcliffe Institute, listening to a tape of sperm whales chatting, when another fellow at the institute, Shafi Goldwasser, happened by. Goldwasser, a Turing Award-winning computer scientist, was intrigued. At the time, she was organizing a seminar on machine learning, which was advancing in ways that would eventually lead to ChatGPT. Perhaps, Goldwasser mused, machine learning could be used to discover the meaning of the whales‚Äô exchanges.</p><p>‚ÄúIt was not exactly a joke, but almost like a pipe dream,‚Äù Goldwasser recollected. ‚ÄúBut David really got into it.‚Äù</p><p>Gruber and Goldwasser took the idea of decoding the codas to a third Radcliffe fellow, Michael Bronstein. Bronstein, also a computer scientist, is now the DeepMind Professor of A.I. at Oxford.</p><p>‚ÄúThis sounded like probably the most crazy project that I had ever heard about,‚Äù Bronstein told me. ‚ÄúBut David has this kind of power, this ability to convince and drag people along. I thought that it would be nice to try.‚Äù</p><p>Gruber kept pushing the idea. Among the experts who found it loopy and, at the same time, irresistible were Robert Wood, a roboticist at Harvard, and Daniela Rus, who runs M.I.T.‚Äôs Computer Science and Artificial Intelligence Laboratory. Thus was born the Cetacean Translation Initiative‚ÄîProject <em>CETI</em> for short. (The acronym is pronounced ‚Äúsetty,‚Äù and purposefully recalls <em>SETI</em>, the Search for Extraterrestrial Intelligence.) <em>CETI</em> represents the most ambitious, the most technologically sophisticated, and the most well-funded effort ever made to communicate with another species.</p><p>‚ÄúI think it‚Äôs something that people get really excited about: Can we go from science fiction to science?‚Äù Rus told me. ‚ÄúI mean, can we talk to whales?‚Äù</p><p>Sperm whales are nomads. It is estimated that, in the course of a year, an individual whale swims at least twenty thousand miles. But scattered around the tropics, for reasons that are probably squid-related, there are a few places the whales tend to favor. One of these is a stretch of water off Dominica, a volcanic island in the Lesser Antilles.</p><p><em>CETI</em> has its unofficial headquarters in a rental house above Roseau, the island‚Äôs capital. The group‚Äôs plan is to turn Dominica‚Äôs west coast into a giant whale-recording studio. This involves installing a network of underwater microphones to capture the codas of passing whales. It also involves planting recording devices on the whales themselves‚Äîcetacean bugs, as it were. The data thus collected can then be used to ‚Äútrain‚Äù machine-learning algorithms.</p><figure><p><span><p>The scientist David Gruber explains the mission of Project CETI, and what his team has learned about how whales communicate.</p>
</span></p></figure><p>In July, I went down to Dominica to watch the <em>CETI</em> team go sperm-whale bugging. My first morning on the island, I met up with Gruber just outside Roseau, on a dive-shop dock. Gruber, who is fifty, is a slight man with dark curly hair and a cheerfully anxious manner. He was carrying a waterproof case and wearing a <em>CETI</em> T-shirt. Soon, several more members of the team showed up, also carrying waterproof cases and wearing <em>CETI</em> T-shirts. We climbed aboard an oversized Zodiac called <em>CETI</em> 2 and set off.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The night before, a tropical storm had raked the region with gusty winds and heavy rain, and Dominica‚Äôs volcanic peaks were still wreathed in clouds. The sea was a series of white-fringed swells. <em>CETI</em> 2 sped along, thumping up and down, up and down. Occasionally, flying fish zipped by; these remained aloft for such a long time that I was convinced for a while they were birds.</p><p>About two miles offshore, the captain, Kevin George, killed the engines. A graduate student named Yaly Mevorach put on a set of headphones and lowered an underwater mike‚Äîa hydrophone‚Äîinto the waves. She listened for a bit and then, smiling, handed the headphones to me.</p><p>The most famous whale calls are the long, melancholy ‚Äúsongs‚Äù issued by humpbacks. Sperm-whale codas are neither mournful nor musical. Some people compare them to the sound of bacon frying, others to popcorn popping. That morning, as I listened through the headphones, I thought of horses clomping over cobbled streets. Then I changed my mind. The clatter was more mechanical, as if somewhere deep beneath the waves someone was pecking out a memo on a manual typewriter.</p><p>Mevorach unplugged the headphones from the mike, then plugged them into a contraption that looked like a car speaker riding a broom handle. The contraption, which I later learned had been jury-rigged out of, among other elements, a metal salad bowl, was designed to locate clicking whales. After twisting it around in the water for a while, Mevorach decided that the clicks were coming from the southwest. We thumped in that direction, and soon George called out, ‚ÄúBlow!‚Äù</p><p>A few hundred yards in front of us was a gray ridge that looked like a misshapen log. (When whales are resting at the surface, only a fraction of their enormous bulk is visible.) The whale blew again, and a geyser-like spray erupted from the ridge‚Äôs left side.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a27506&quot;}" href="https://www.newyorker.com/cartoon/a27506" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>‚ÄúThis is the exact moment I asked you to water my plant.‚Äù</span></p><p><span>Cartoon by Jonathan Rosen</span></p></div></span></p></figure><p>As we were closing in, the whale blew yet again; then it raised its elegantly curved flukes into the air and dove. It was unlikely to resurface, I was told, for nearly an hour.</p><p>We thumped off in search of its kin. The farther south we travelled, the higher the swells. At one point, I felt my stomach lurch and went to the side of the boat to heave.</p><p>‚ÄúI like to just throw up and get back to work,‚Äù Mevorach told me.</p><p>Trying to attach a recording device to a sperm whale is a bit like trying to joust while racing on a Jet Ski. The exercise entails using a thirty-foot pole to stick the device onto the animal‚Äôs back, which in turn entails getting within thirty feet of a creature the size of a school bus. That day, several more whales were spotted. But, for all of our thumping around, <em>CETI</em> 2 never got close enough to one to unhitch the tagging pole.</p><p>The next day, the sea was calmer. Once again, we spotted whales, and several times the boat‚Äôs designated pole-handler, Odel Harve, attempted to tag one. All his efforts went for naught. Either the whale dove at the last minute or the recording device slipped off the whale‚Äôs back and had to be fished out of the water. (The device, which was about a foot long and shaped like a surfboard, was supposed to adhere via suction cups.) With each new sighting, the mood on <em>CETI</em> 2 lifted; with each new failure, it sank.</p><p>On my third day in Dominica, I joined a slightly different subset of the team on a different boat to try out a new approach. Instead of a long pole, this boat‚Äîa forty-foot catamaran called <em>CETI</em> 1‚Äîwas carrying an experimental drone. The drone had been specially designed at Harvard and was fitted out with a video camera and a plastic claw.</p><p>Because sperm whales are always on the move, there‚Äôs no guarantee of finding any; weeks can go by without a single sighting off Dominica. Once again, though, we got lucky, and a whale was soon spotted. Stefano Pagani, an undergraduate who had been brought along for his piloting skills, pulled on what looked like a V.R. headset, which was linked to the drone‚Äôs video camera. In this way, he could look down at the whale from the drone‚Äôs perspective and, it was hoped, plant a recording device, which had been loaded into the claw, on the whale‚Äôs back.</p><p>The drone took off and zipped toward the whale. It hovered for a few seconds, then dropped vertiginously. For the suction cups to adhere, the drone had to strike the whale at just the right angle, with just the right amount of force. Post impact, Pagani piloted the craft back to the boat with trembling hands. ‚ÄúThe nerves get to you,‚Äù he said.</p><p>‚ÄúNo pressure,‚Äù Gruber joked. ‚ÄúIt‚Äôs not like there‚Äôs a <em>New Yorker</em> reporter watching or anything.‚Äù Someone asked for a round of applause. A cheer went up from the boat. The whale, for its part, seemed oblivious. It lolled around with the recording device, which was painted bright orange, stuck to its dark-gray skin. Then it dove.</p><p>Sperm whales are among the world‚Äôs deepest divers. They routinely descend two thousand feet and sometimes more than a mile. (The deepest a human has ever gone with scuba gear is just shy of eleven hundred feet.) If the device stayed on, it would record any sounds the whale made on its travels. It would also log the whale‚Äôs route, its heartbeat, and its orientation in the water. The suction was supposed to last around eight hours; after that‚Äîassuming all went according to plan‚Äîthe device would come loose, bob to the surface, and transmit a radio signal that would allow it to be retrieved.</p><p>I said it was too bad we couldn‚Äôt yet understand what the whales were saying, because perhaps this one, before she dove, had clicked out where she was headed.</p><p>‚ÄúCome back in two years,‚Äù Gruber said.</p><p>Every sperm whale‚Äôs tail is unique. On some, the flukes are divided by a deep notch. On others, they meet almost in a straight line. Some flukes end in points; some are more rounded. Many are missing distinctive chunks, owing, presumably, to orca attacks. To I.D. a whale in the field, researchers usually rely on a photographic database called Flukebook. One of the very few scientists who can do it simply by sight is <em>CETI</em>‚Äôs lead field biologist, Shane Gero.</p><p>Gero, who is forty-three, is tall and broad, with an eager smile and a pronounced Canadian accent. A scientist-in-residence at Ottawa‚Äôs Carleton University, he has been studying the whales off Dominica since 2005. By now, he knows them so well that he can relate their triumphs and travails, as well as who gave birth to whom and when. A decade ago, as Gero started having children of his own, he began referring to his ‚Äúhuman family‚Äù and his ‚Äúwhale family.‚Äù (His human family lives in Ontario.) Another marine biologist once described Gero as sounding ‚Äúlike Captain Ahab after twenty years of psychotherapy.‚Äù</p><p>When Gruber approached Gero about joining Project <em>CETI</em>, he was, initially, suspicious. ‚ÄúI get a lot of e-mails like ‚ÄòHey, I think whales have crystals in their heads,‚Äô and ‚ÄòMaybe we can use them to cure malaria,‚Äô&nbsp;‚Äù Gero told me. ‚ÄúThe first e-mail David sent me was, like, ‚ÄòHi, I think we could find some funding to translate whale.‚Äô And I was, like, ‚ÄòOh, boy.‚Äô&nbsp;‚Äù</p><p>A few months later, the two men met in person, in Washington, D.C., and hit it off. Two years after that, Gruber did find some funding. <em>CETI</em> received thirty-three million dollars from the Audacious Project, a philanthropic collaborative whose backers include Richard Branson and Ray Dalio. (The grant, which was divided into five annual payments, will run out in 2025.)</p><p>The whole time I was in Dominica, Gero was there as well, supervising graduate students and helping with the tagging effort. From him, I learned that the first whale I had seen was named Rita and that the whales that had subsequently been spotted included Raucous, Roger, and Rita‚Äôs daughter, Rema. All belonged to a group called Unit R, which Gero characterized as ‚Äútightly and actively social.‚Äù Apparently, Unit R is also warmhearted. Several years ago, when a group called Unit S got whittled down to just two members‚ÄîSally and TBB‚Äîthe Rs adopted them.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Sperm whales have the biggest brains on the planet‚Äîsix times the size of humans‚Äô. Their social lives are rich, complicated, and, some would say, ideal. The adult members of a unit, which may consist of anywhere from a few to a few dozen individuals, are all female. Male offspring are permitted to travel with the group until they‚Äôre around fifteen years old; then, as Gero put it, they are ‚Äúsocially ostracized.‚Äù Some continue to hang around their mothers and sisters, clicking away for months unanswered. Eventually, though, they get the message. Fully grown males are solitary creatures. They approach a band of females‚Äîpresumably not their immediate relatives‚Äîonly in order to mate. To signal their arrival, they issue deep, booming sounds known as clangs. No one knows exactly what makes a courting sperm whale attractive to a potential mate; Gero told me that he had seen some clanging males greeted with great commotion and others with the cetacean equivalent of a shrug.</p><p>Female sperm whales, meanwhile, are exceptionally close. The adults in a unit not only travel and hunt together; they also appear to confer on major decisions. If there‚Äôs a new mother in the group, the other members mind the calf while she dives for food. In some units, though not in Unit R, sperm whales even suckle one another‚Äôs young. When a family is threatened, the adults cluster together to protect their offspring, and when things are calm the calves fool around.</p><p>‚ÄúIt‚Äôs like my kids and their cousins,‚Äù Gero said.</p><p>The day after I watched the successful drone flight, I went out with Gero to try to recover the recording device. More than twenty-four hours had passed, and it still hadn‚Äôt been located. Gero decided to drive out along a peninsula called Scotts Head, at the southwestern tip of Dominica, where he thought he might be able to pick up the radio signal. As we wound around on the island‚Äôs treacherously narrow roads, he described to me an idea he had for a children‚Äôs book that, read in one direction, would recount a story about a human family that lives on a boat and looks down at the water and, read from the other direction, would be about a whale family that lives deep beneath the boat and looks up at the waves.</p><p>‚ÄúFor me, the most rewarding part about spending a lot of time in the culture of whales is finding these fundamental similarities, these fundamental patterns,‚Äù he said. ‚ÄúAnd, you know, sure, they won‚Äôt have a word for ‚Äòtree.‚Äô And there‚Äôs some part of the sperm-whale experience that our primate brain just won‚Äôt understand. But those things that we share must be fundamentally important to why we‚Äôre here.‚Äù</p><p>After a while, we reached, quite literally, the end of the road. Beyond that was a hill that had to be climbed on foot. Gero was carrying a portable antenna, which he unfolded when we got to the top. If the recording unit had surfaced anywhere within twenty miles, Gero calculated, we should be able to detect the signal. It occurred to me that we were now trying to listen for a listening device. Gero held the antenna aloft and put his ear to some kind of receiver. He didn‚Äôt hear anything, so, after admiring the view for a bit, we headed back down. Gero was hopeful that the device would eventually be recovered. But, as far as I know, it is still out there somewhere, adrift in the Caribbean.</p><p>The first scientific, or semi-scientific, study of sperm whales was a pamphlet published in 1835 by a Scottish ship doctor named Thomas Beale. Called ‚ÄúThe Natural History of the Sperm Whale,‚Äù it proved so popular that Beale expanded the pamphlet into a book, which was issued under the same title four years later.</p><p>At the time, sperm-whale hunting was a major industry, both in Britain and in the United States. The animals were particularly prized for their spermaceti, the waxy oil that fills their gigantic heads. Spermaceti is an excellent lubricant, and, burned in a lamp, produces a clean, bright light; in Beale‚Äôs day, it could sell for five times as much as ordinary whale oil. (It is the resemblance between semen and spermaceti that accounts for the species‚Äô embarrassing name.)</p><p>Beale believed sperm whales to be silent. ‚ÄúIt is well known among the most experienced whalers that they never produce any nasal or vocal sounds whatever, except a trifling hissing at the time of the expiration of the spout,‚Äù he wrote. The whales, he said, were also gentle‚Äî‚Äúa most timid and inoffensive animal.‚Äù Melville relied heavily on Beale in composing ‚ÄúMoby-Dick.‚Äù (His personal copy of ‚ÄúThe Natural History of the Sperm Whale‚Äù is now housed in Harvard‚Äôs Houghton Library.) He attributed to sperm whales a ‚Äúpyramidical silence.‚Äù</p><p>‚ÄúThe whale has no voice,‚Äù Melville wrote. ‚ÄúBut then again,‚Äù he went on, ‚Äúwhat has the whale to say? Seldom have I known any profound being that had anything to say to this world, unless forced to stammer out something by way of getting a living.‚Äù</p><p>The silence of the sperm whales went unchallenged until 1957. That year, two researchers from the Woods Hole Oceanographic Institution picked up sounds from a group they‚Äôd encountered off the coast of North Carolina. They detected strings of ‚Äúsharp clicks,‚Äù and speculated that these were made for the purpose of echolocation. Twenty years elapsed before one of the researchers, along with a different colleague from Woods Hole, determined that some sperm-whale clicks were issued in distinctive, often repeated patterns, which the pair dubbed ‚Äúcodas.‚Äù Codas seemed to be exchanged between whales and so, they reasoned, must serve some communicative function.</p><p>Since then, cetologists have spent thousands of hours listening to codas, trying to figure out what that function might be. Gero, who wrote his Ph.D. thesis on vocal communication between sperm whales, told me that one of the ‚Äúuniversal truths‚Äù about codas is their timing. There are always four seconds between the start of one coda and the beginning of the next. Roughly two of those seconds are given over to clicks; the rest is silence. Only after the pause, which may or may not be analogous to the pause a human speaker would put between words, does the clicking resume.</p><p>Codas are clearly learned or, to use the term of art, socially transmitted. Whales in the eastern Pacific exchange one set of codas, those in the eastern Caribbean another, and those in the South Atlantic yet another. Baby sperm whales pick up the codas exchanged by their relatives, and before they can click them out proficiently they ‚Äúbabble.‚Äù</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The whales around Dominica have a repertoire of around twenty-five codas. These codas differ from one another in the number of their clicks and also in their rhythms. The coda known as three regular, or 3R, for example, consists of three clicks issued at equal intervals. The coda 7R consists of seven evenly spaced clicks. In seven increasing, or 7I, by contrast, the interval between the clicks grows longer; it‚Äôs about five-hundredths of a second between the first two clicks, and between the last two it‚Äôs twice that long. In four decreasing, or 4D, there‚Äôs a fifth of a second between the first two clicks and only a tenth of a second between the last two. Then, there are syncopated codas. The coda most frequently issued by members of Unit R, which has been dubbed 1+1+3, has a cha-cha-esque rhythm and might be rendered in English as click&nbsp;.&nbsp;.&nbsp;. click&nbsp;.&nbsp;.&nbsp;. click-click-click.</p><p>If codas are in any way comparable to words, a repertoire of twenty-five represents a pretty limited vocabulary. But, just as no one can yet say what, if anything, codas mean to sperm whales, no one can say exactly what features are significant to them. It may be that there are nuances in, say, pacing or pitch that have so far escaped human detection. Already, <em>CETI</em> team members have identified a new kind of signal‚Äîa single click‚Äîthat may serve as some kind of punctuation mark.</p><p>When whales are resting near the surface, their exchanges can last an hour or more. Even by human standards, sperm-whale chatter is insistent and repetitive. ‚ÄúThey‚Äôre talking on top of each other all the time,‚Äù Gero told me.</p><p>A snatch of dialogue recorded between two members of Unit R runs as follows. (Both Roger and Rita are adult females.)</p><blockquote><p>Roger: 1+1+3<br>Rita: 1+1+3, 1+1+3<br>Roger: 9I<br>Rita: 1+1+3<br>Roger: 10I<br>Rita: 1+1+3, 1+1+3<br>Roger: 11I<br>Rita: 1+1+3<br>Roger: 10I, 11I, 1+1+3<br>Rita: 1+1+3</p></blockquote><p>The ‚Äúconversation‚Äù continues along much these same lines, until Rita finally changes her tune:</p><blockquote><p>Rita: 1+1+3<br>Roger: 12R, 10I, 10I, 9I, 9I<br>Rita: 9I, 8I</p></blockquote><p>Not long ago, suffering from writer‚Äôs block, I asked ChatGPT if it could rewrite ‚ÄúMoby-Dick‚Äù from the whale‚Äôs perspective. The chatbot began modestly. ‚ÄúAs an A.I. language model, I can certainly attempt to rewrite a small excerpt,‚Äù it told me. Then it launched into what it titled ‚ÄúMoby-Dick‚ÄîThe White Leviathan‚Äôs Tale.‚Äù</p><p>In Moby-Dick‚Äôs ‚ÄúMoby-Dick,‚Äù the plot turned on an unfortunate case of mistaken identity. Some other whale had gnawed off Ahab‚Äôs leg; the white whale was as innocent as the driven snow.</p><p>‚ÄúMy heart was one of peace, my spirit one with the rhythm of the currents,‚Äù ChatGPT wrote:</p><blockquote><p>I knew not of the vendetta forming against me, for I was but a creature of instinct, a guardian of the seas. But Captain Ahab‚Äôs thirst for vengeance loomed large, and I became the object of his undying rage. The scar he bore from a previous encounter with my kind had branded me as his ultimate adversary. In his eyes, I represented a force of nature that defied his understanding, a living enigma of the ocean‚Äôs might.</p></blockquote><p>In paragraph seven, I was surprised‚Äîand excited‚Äîto see the whale/bot allude to the problem of interspecies communication:</p><blockquote><p>Deep within the abyss of the sea, I pondered the futility of the captain‚Äôs quest. I longed to communicate with him, to show him that my kind, too, had emotions and families to protect. But the language barrier between our worlds remained an insurmountable chasm.</p></blockquote><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a27086&quot;}" href="https://www.newyorker.com/cartoon/a27086" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Zoe Si</span></p></div></span></p></figure><p>As anyone who has been conscious for the past ten months knows, ChatGPT is capable of amazing feats. It can write essays, compose sonnets, explain scientific concepts, and produce jokes (though these last are not necessarily funny). If you ask ChatGPT how it was created, it will tell you that first it was trained on a ‚Äúmassive corpus‚Äù of data from the Internet. This phase consisted of what‚Äôs called ‚Äúunsupervised machine learning,‚Äù which was performed by an intricate array of processing nodes known as a neural network. Basically, the ‚Äúlearning‚Äù involved filling in the blanks; according to ChatGPT, the exercise entailed ‚Äúpredicting the next word in a sentence given the context of the previous words.‚Äù By digesting millions of Web pages‚Äîand calculating and recalculating the odds‚ÄîChatGPT got so good at this guessing game that, without ever understanding English, it mastered the language. (Other languages it is ‚Äúfluent‚Äù in include Chinese, Spanish, and French.)</p><p>In theory at least, what goes for English (and Chinese and French) also goes for sperm whale. Provided that a computer model can be trained on enough data, it should be able to master coda prediction. It could then‚Äîonce again in theory‚Äîgenerate sequences of codas that a sperm whale would find convincing. The model wouldn‚Äôt understand sperm whale-ese, but it could, in a manner of speaking, speak it. Call it ClickGPT.</p><p>Currently, the largest collection of sperm-whale codas is an archive assembled by Gero in his years on and off Dominica. The codas contain roughly a hundred thousand clicks. In a paper published last year, members of the <em>CETI</em> team estimated that, to fulfill its goals, the project would need to assemble some four billion clicks, which is to say, a collection roughly forty thousand times larger than Gero‚Äôs.</p><p>‚ÄúOne of the key challenges toward the analysis of sperm whale (and more broadly, animal) communication using modern deep learning techniques is the need for sizable datasets,‚Äù the team wrote.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In addition to bugging individual whales, <em>CETI</em> is planning to tether a series of three ‚Äúlistening stations‚Äù to the floor of the Caribbean Sea. The stations should be able to capture the codas of whales chatting up to twelve miles from shore. (Though inaudible above the waves, sperm-whale clicks can register up to two hundred and thirty decibels, which is louder than a gunshot or a rock concert.) The information gathered by the stations will be less detailed than what the tags can provide, but it should be much more plentiful.</p><p>One afternoon, I drove with Gruber and <em>CETI</em>‚Äôs station manager, Yaniv Aluma, a former Israeli Navy <em>SEAL</em>, to the port in Roseau, where pieces of the listening stations were being stored. The pieces were shaped like giant sink plugs and painted bright yellow. Gruber explained that the yellow plugs were buoys, and that the listening equipment‚Äîessentially, large collections of hydrophones‚Äîwould dangle from the bottom of the buoys, on cables. The cables would be weighed down with old train wheels, which would anchor them to the seabed. A stack of wheels, rusted orange, stood nearby. Gruber suddenly turned to Aluma and, pointing to the pile, said, ‚ÄúYou know, we‚Äôre going to need more of these.‚Äù Aluma nodded glumly.</p><p>The listening stations have been the source of nearly a year‚Äôs worth of delays for <em>CETI</em>. The first was installed last summer, in water six thousand feet deep. Fish were attracted to the buoy, so the spot soon became popular among fishermen. After about a month, the fishermen noticed that the buoy was gone. Members of <em>CETI</em>‚Äôs Dominica-based staff set out in the middle of the night on <em>CETI</em> 1 to try to retrieve it. By the time they reached the buoy, it had drifted almost thirty miles offshore. Meanwhile, the hydrophone array, attached to the rusty train wheels, had dropped to the bottom of the sea.</p><p>The trouble was soon traced to the cable, which had been manufactured in Texas by a company that specializes in offshore oil-rig equipment. ‚ÄúThey deal with infrastructure that‚Äôs very solid,‚Äù Aluma explained. ‚ÄúBut a buoy has its own life. And they didn‚Äôt calculate so well the torque or load on different motions‚Äîtwisting and moving sideways.‚Äù The company spent months figuring out why the cable had failed and finally thought it had solved the problem. In June, Aluma flew to Houston to watch a new cable go through stress tests. In the middle of the tests, the new design failed. To avoid further delays, the <em>CETI</em> team reconfigured the stations. One of the reconfigured units was installed late last month. If it doesn‚Äôt float off, or in some other way malfunction, the plan is to get the two others in the water sometime this fall.</p><p>A sperm whale‚Äôs head takes up nearly a third of its body; its narrow lower jaw seems borrowed from a different animal entirely; and its flippers are so small as to be almost dainty. (The formal name for the species is <em>Physeter macrocephalus</em>, which translates roughly as ‚Äúbig-headed blowhole.‚Äù) ‚ÄúFrom just about any angle,‚Äù Hal Whitehead, one of the world‚Äôs leading sperm-whale experts (and Gero‚Äôs thesis adviser), has written, sperm whales appear ‚Äúvery strange.‚Äù I wanted to see more of these strange-looking creatures than was visible from a catamaran, and so, on my last day in Dominica, I considered going on a commercial tour that offered customers a chance to swim with whales, assuming that any could be located. In the end‚Äîpartly because I sensed that Gruber disapproved of the practice‚ÄîI dropped the idea.</p><p>Instead, I joined the crew on <em>CETI</em> 1 for what was supposed to be another round of drone tagging. After we‚Äôd been under way for about two hours, codas were picked up, to the northeast. We headed in that direction and soon came upon an extraordinary sight. There were at least ten whales right off the boat‚Äôs starboard. They were all facing the same direction, and they were bunched tightly together, in rows. Gero identified them as members of Unit A. The members of Unit A were originally named for characters in Margaret Atwood novels, and they include Lady Oracle, Aurora, and Rounder, Lady Oracle‚Äôs daughter.</p><p>Earlier that day, the crew on <em>CETI</em> 2 had spotted pilot whales, or blackfish, which are known to harass sperm whales. ‚ÄúThis looks very defensive,‚Äù Gero said, referring to the formation.</p><p>Suddenly, someone yelled out, ‚ÄúRed!‚Äù A burst of scarlet spread through the water, like a great banner unfurling. No one knew what was going on. Had the pilot whales stealthily attacked? Was one of the whales in the group injured? The crowding increased until the whales were practically on top of one another.</p><p>Then a new head appeared among them. ‚ÄúHoly fucking shit!‚Äù Gruber exclaimed.</p><p>‚ÄúOh, my God!‚Äù Gero cried. He ran to the front of the boat, clutching his hair in amazement. ‚ÄúOh, my God! Oh, my God!‚Äù The head belonged to a newborn calf, which was about twelve feet long and weighed maybe a ton. In all his years of studying sperm whales, Gero had never watched one being born. He wasn‚Äôt sure anyone ever had.</p><p>As one, the whales made a turn toward the catamaran. They were so close I got a view of their huge, eerily faceless heads and pink lower jaws. They seemed oblivious of the boat, which was now in their way. One knocked into the hull, and the foredeck shuddered.</p><p>The adults kept pushing the calf around. Its mother and her relatives pressed in so close that the baby was almost lifted out of the water. Gero began to wonder whether something had gone wrong. By now, everyone, including the captain, had gathered on the bow. Pagani and another undergraduate, Aidan Kenny, had launched two drones and were filming the action from the air. Mevorach, meanwhile, was recording the whales through a hydrophone.</p><p>To everyone‚Äôs relief, the baby began to swim on its own. Then the pilot whales showed up‚Äîdozens of them.</p><p>‚ÄúI don‚Äôt like the way they‚Äôre moving,‚Äù Gruber said.</p><p>‚ÄúThey‚Äôre going to attack for sure,‚Äù Gero said. The pilot whales‚Äô distinctive, wave-shaped fins slipped in and out of the water.</p><p>What followed was something out of a marine-mammal ‚ÄúLord of the Rings.‚Äù Several of the pilot whales stole in among the sperm whales. All that could be seen from the boat was a great deal of thrashing around. Out of nowhere, more than forty Fraser‚Äôs dolphins arrived on the scene. Had they come to participate in the melee or just to rubberneck? It was impossible to tell. They were smaller and thinner than the pilot whales (which, their name notwithstanding, are also technically dolphins).</p><p>‚ÄúI have no prior knowledge upon which to predict what happens next,‚Äù Gero announced. After several minutes, the pilot whales retreated. The dolphins curled through the waves. The whales remained bunched together. Calm reigned. Then the pilot whales made another run at the sperm whales. The water bubbled and churned.</p><p>‚ÄúThe pilot whales are just being pilot whales,‚Äù Gero observed. Clearly, though, in the great ‚Äústruggle for existence,‚Äù everyone on board <em>CETI</em> 1 was on the side of the baby.</p><p>The skirmishing continued. The pilot whales retreated, then closed in again. The drones began to run out of power. Pagani and Kenny piloted them back to the catamaran to exchange the batteries. These were so hot they had to be put in the boat‚Äôs refrigerator. At one point, Gero thought that he spied the new calf, still alive and well. (He would later, from the drone footage, identify the baby‚Äôs mother as Rounder.) ‚ÄúSo that‚Äôs good news,‚Äù he called out.</p><p>The pilot whales hung around for more than two hours. Then, all at once, they were gone. The dolphins, too, swam off.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>‚ÄúThere will never be a day like this again,‚Äù Gero said as <em>CETI</em> 1 headed back to shore.</p><p>That evening, everyone who‚Äôd been on board <em>CETI</em> 1 and <em>CETI</em> 2 gathered at a dockside restaurant for a dinner in honor of the new calf. Gruber made a toast. He thanked the team for all its hard work. ‚ÄúLet‚Äôs hope we can learn the language with that baby whale,‚Äù he said.</p><p>I was sitting with Gruber and Gero at the end of a long table. In between drinks, Gruber suggested that what we had witnessed might not have been an attack. The scene, he proposed, had been more like the last act of ‚ÄúThe Lion King,‚Äù when the beasts of the jungle gather to welcome the new cub.</p><p>‚ÄúThree different marine mammals came together to celebrate and protect the birth of an animal with a sixteen-month gestation period,‚Äù he said. Perhaps, he hypothesized, this was a survival tactic that had evolved to protect mammalian young against sharks, which would have been attracted by so much blood and which, he pointed out, would have been much more numerous before humans began killing them off.</p><p>‚ÄúYou mean the baby whale was being protected by the pilot whales from the sharks that aren‚Äôt here?‚Äù Gero asked. He said he didn‚Äôt even know what it would mean to test such a theory. Gruber said they could look at the drone footage and see if the sperm whales had ever let the pilot whales near the newborn and, if so, how the pilot whales had responded. I couldn‚Äôt tell whether he was kidding or not.</p><p>‚ÄúThat‚Äôs a nice story,‚Äù Mevorach interjected.</p><p>‚ÄúI just like to throw ideas out there,‚Äù Gruber said.</p><blockquote><div><p>‚ÄúMy! You don‚Äôt say so!‚Äù said the Doctor. ‚ÄúYou never talked that way to me before.‚Äù</p><p>‚ÄúWhat would have been the good?‚Äù said Polynesia, dusting some cracker crumbs off her left wing. ‚ÄúYou wouldn‚Äôt have understood me if I had.‚Äù</p><inline-embed name="align-right" attrs="[object Object]" childtypes="" contenttype="callout:align-right"><p><em>‚Äî‚ÄúThe Story of Doctor Dolittle.‚Äù</em></p></inline-embed></div></blockquote><p>The Computer Science and Artificial Intelligence Laboratory (<em>CSAIL</em>), at M.I.T., occupies a Frank Gehry-designed building that appears perpetually on the verge of collapse. Some wings tilt at odd angles; others seem about to split in two. In the lobby of the building, there‚Äôs a vending machine that sells electrical cords and another that dispenses caffeinated beverages from around the world. There‚Äôs also a yellow sign of the sort you might see in front of an elementary school. It shows a figure wearing a backpack and carrying a briefcase and says ‚Äú<em>NERD XING</em>.‚Äù</p><p>Daniela Rus, who runs <em>CSAIL</em> (pronounced ‚Äúsee-sale‚Äù), is a roboticist. ‚ÄúThere‚Äôs such a crazy conversation these days about machines,‚Äù she told me. We were sitting in her office, which is dominated by a robot, named Domo, who sits in a glass case. Domo has a metal torso and oversized, goggly eyes. ‚ÄúIt‚Äôs either machines are going to take us down or machines are going to solve all of our problems. And neither is correct.‚Äù</p><p>Along with several other researchers at <em>CSAIL</em>, Rus has been thinking about how <em>CETI</em> might eventually push beyond coda prediction to something approaching coda comprehension. This is a formidable challenge. Whales in a unit often chatter before they dive. But what are they chattering about? How deep to go, or who should mind the calves, or something that has no analogue in human experience?</p><p>‚ÄúWe are trying to correlate behavior with vocalization,‚Äù Rus told me. ‚ÄúThen we can begin to get evidence for the meaning of some of the vocalizations they make.‚Äù</p><p>She took me down to her lab, where several graduate students were tinkering in a thicket of electronic equipment. In one corner was a transparent plastic tube loaded with circuitry, attached to two white plastic flippers. The setup, Rus explained, was the skeleton of a robotic turtle. Lying on the ground was the turtle‚Äôs plastic shell. One of the students hit a switch and the flippers made a paddling motion. Another student brought out a two-foot-long robotic fish. Both the fish and the turtle could be configured to carry all sorts of sensors, including underwater cameras.</p><p>‚ÄúWe need new methods for collecting data,‚Äù Rus said. ‚ÄúWe need ways to get close to the whales, and so we‚Äôve been talking a lot about putting the sea turtle or the fish in water next to the whales, so that we can image what we cannot see.‚Äù</p><p><em>CSAIL</em> is an enormous operation, with more than fifteen hundred staff members and students. ‚ÄúPeople here are kind of audacious,‚Äù Rus said. ‚ÄúThey really love the wild and crazy ideas that make a difference.‚Äù She told me about a diver she had met who had swum with the sperm whales off Dominica and, by his account at least, had befriended one. The whale seemed to like to imitate the diver; for example, when he hung in the water vertically, it did, too.</p><p>‚ÄúThe question I‚Äôve been asking myself is: Suppose that we set up experiments where we engage the whales in physical mimicry,‚Äù Rus said. ‚ÄúCan we then get them to vocalize while doing a motion? So, can we get them to say, ‚ÄòI‚Äôm going up‚Äô? Or can we get them to say, ‚ÄòI‚Äôm hovering‚Äô? I think that, if we were to find a few snippets of vocalizations that we could associate with some meaning, that would help us get deeper into their conversational structure.‚Äù</p><p>While we were talking, another <em>CSAIL</em> professor and <em>CETI</em> collaborator, Jacob Andreas, showed up. Andreas, a computer scientist who works on language processing, said that he had been introduced to the whale project at a faculty retreat. ‚ÄúI gave a talk about understanding neural networks as a weird translation problem,‚Äù he recalled. ‚ÄúAnd Daniela came up to me afterwards and she said, ‚ÄòOh, you like weird translation problems? Here‚Äôs a weird translation problem.‚Äô&nbsp;‚Äù</p><p>Andreas told me that <em>CETI</em> had already made significant strides, just by reanalyzing Gero‚Äôs archive. Not only had the team uncovered the new kind of signal but also it had found that codas have much more internal structure than had previously been recognized. ‚ÄúThe amount of information that this system can carry is much bigger,‚Äù he said.</p><p>‚ÄúThe holy grail here‚Äîthe thing that separates human language from all other animal communication systems‚Äîis what‚Äôs called ‚Äòduality of patterning,‚Äô&nbsp;‚Äù Andreas went on. ‚ÄúDuality of patterning‚Äù refers to the way that meaningless units‚Äîin English, sounds like ‚Äúsp‚Äù or ‚Äúot‚Äù‚Äîcan be combined to form meaningful units, like ‚Äúspot.‚Äù If, as is suspected, clicks are empty of significance but codas refer to something, then sperm whales, too, would have arrived at duality of patterning. ‚ÄúBased on what we know about how the coda inventory works, I‚Äôm optimistic‚Äîthough still not sure‚Äîthat this is going to be something that we find in sperm whales,‚Äù Andreas said.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a25122&quot;}" href="https://www.newyorker.com/cartoon/a25122" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>‚ÄúBut with traffic I rarely make it past twenty miles per hour.‚Äù</span></p><p><span>Cartoon by Sofia Warren</span></p></div></span></p></figure><p>The question of whether any species possesses a ‚Äúcommunication system‚Äù comparable to that of humans is an open and much debated one. In the nineteen-fifties, the behaviorist B. F. Skinner argued that children learn language through positive reinforcement; therefore, other animals should be able to do the same. The linguist Noam Chomsky had a different view. He dismissed the notion that kids acquire language via conditioning, and also the possibility that language was available to other species.</p><p>In the early nineteen-seventies, a student of Skinner‚Äôs, Herbert Terrace, set out to confirm his mentor‚Äôs theory. Terrace, at that point a professor of psychology at Columbia, adopted a chimpanzee, whom he named, tauntingly, Nim Chimpsky. From the age of two weeks, Nim was raised by people and taught American Sign Language. Nim‚Äôs interactions with his caregivers were videotaped, so that Terrace would have an objective record of the chimp‚Äôs progress. By the time Nim was three years old, he had a repertoire of eighty signs and, significantly, often produced them in sequences, such as ‚Äúbanana me eat banana‚Äù or ‚Äútickle me Nim play.‚Äù Terrace set out to write a book about how Nim had crossed the language barrier and, in so doing, made a monkey of his namesake. But then Terrace double-checked some details of his account against the tapes. When he looked carefully at the videos, he was appalled. Nim hadn‚Äôt really learned A.S.L.; he had just learned to imitate the last signs his teachers had made to him.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>‚ÄúThe very tapes I planned to use to document Nim‚Äôs ability to sign provided decisive evidence that I had vastly overestimated his linguistic competence,‚Äù Terrace wrote.</p><p>Since Nim, many further efforts have been made to prove that different species‚Äîorangutans, bonobos, parrots, dolphins‚Äîhave a capacity for language. Several of the animals who were the focus of these efforts‚ÄîKoko the gorilla, Alex the gray parrot‚Äîbecame international celebrities. But most linguists still believe that the only species that possesses language is our own.</p><p>Language is ‚Äúa uniquely human faculty‚Äù that is ‚Äúpart of the biological nature of our species,‚Äù Stephen R. Anderson, a professor emeritus at Yale and a former president of the Linguistic Society of America, writes in his book ‚ÄúDoctor Dolittle‚Äôs Delusion.‚Äù</p><p>Whether sperm-whale codas could challenge this belief is an issue that just about everyone I talked to on the <em>CETI</em> team said they‚Äôd rather not talk about.</p><p>‚ÄúLinguists like Chomsky are very opinionated,‚Äù Michael Bronstein, the Oxford professor, told me. ‚ÄúFor a computer scientist, usually a language is some formal system, and often we talk about artificial languages.‚Äù Sperm-whale codas ‚Äúmight not be as expressive as human language,‚Äù he continued. ‚ÄúBut I think whether to call it ‚Äòlanguage‚Äô or not is more of a formal question.‚Äù</p><p>‚ÄúIronically, it‚Äôs a semantic debate about the meaning of language,‚Äù Gero observed.</p><p>Of course, the advent of ChatGPT further complicates the debate. Once a set of algorithms can rewrite a novel, what counts as ‚Äúlinguistic competence‚Äù? And who‚Äîor what‚Äîgets to decide?</p><p>‚ÄúWhen we say that we‚Äôre going to succeed in translating whale communication, what do we mean?‚Äù Shafi Goldwasser, the Radcliffe Institute fellow who first proposed the idea that led to <em>CETI</em>, asked.</p><p>‚ÄúEverybody‚Äôs talking these days about these generative A.I. models like ChatGPT,‚Äù Goldwasser, who now directs the Simons Institute for the Theory of Computing, at the University of California, Berkeley, went on. ‚ÄúWhat are they doing? You are giving them questions or prompts, and then they give you answers, and the way that they do that is by predicting how to complete sentences or what the next word would be. So you could say that‚Äôs a goal for <em>CETI</em>‚Äîthat you don‚Äôt necessarily understand what the whales are saying, but that you could predict it with good success. And, therefore, you could maybe generate a conversation that would be understood by a whale, but maybe you don‚Äôt understand it. So that‚Äôs kind of a weird success.‚Äù</p><p>Prediction, Goldwasser said, would mean ‚Äúwe‚Äôve realized what the pattern of their speech is. It‚Äôs not satisfactory, but it‚Äôs something.</p><p>‚ÄúWhat about the goal of understanding?‚Äù she added. ‚ÄúEven on that, I am not a pessimist.‚Äù</p><p>There are now an estimated eight hundred and fifty thousand sperm whales diving the world‚Äôs oceans. This is down from an estimated two million in the days before the species was commercially hunted. It‚Äôs often suggested that the darkest period for <em>P. macrocephalus</em> was the middle of the nineteenth century, when Melville shipped out of New Bedford on the Acushnet. In fact, the bulk of the slaughter took place in the middle of the twentieth century, when sperm whales were pursued by diesel-powered ships the size of factories. In the eighteen-forties, at the height of open-boat whaling, some five thousand sperm whales were killed each year; in the nineteen-sixties, the number was six times as high. Sperm whales were boiled down to make margarine, cattle feed, and glue. As recently as the nineteen-seventies, General Motors used spermaceti in its transmission fluid.</p><p>Near the peak of industrial whaling, a biologist named Roger Payne heard a radio report that changed his life and, with it, the lives of the world‚Äôs remaining cetaceans. The report noted that a whale had washed up on a beach not far from where Payne was working, at Tufts University. Payne, who‚Äôd been researching moths, drove out to see it. He was so moved by the dead animal that he switched the focus of his research. His investigations led him to a naval engineer who, while listening for Soviet submarines, had recorded eerie underwater sounds that he attributed to humpback whales. Payne spent years studying the recordings; the sounds, he decided, were so beautiful and so intricately constructed that they deserved to be called ‚Äúsongs.‚Äù In 1970, he arranged to have ‚ÄúSongs of the Humpback Whale‚Äù released as an LP.</p><p>‚ÄúI just thought: the world has to hear this,‚Äù he would later recall. The album sold briskly, was sampled by popular musicians like Judy Collins, and helped launch the ‚ÄúSave the Whales‚Äù movement. In 1979, <em>National Geographic</em> issued a ‚Äúflexi disc‚Äù version of the songs, which it distributed as an insert in more than ten million copies of the magazine. Three years later, the International Whaling Commission declared a ‚Äúmoratorium‚Äù on commercial hunts which remains in effect today. The move is credited with having rescued several species, including humpbacks and fin whales, from extinction.</p><p>Payne, who died in June at the age of eighty-eight, was an early and ardent member of the <em>CETI</em> team. (This was the case, Gruber told me, even though he was disappointed that the project was focussing on sperm whales, rather than on humpbacks, which, he maintained, were more intelligent.) Just a few days before his death, Payne published an op-ed piece explaining why he thought <em>CETI</em> was so important.</p><p>Whales, along with just about every other creature on Earth, are now facing grave new threats, he observed, among them climate change. How to motivate ‚Äúourselves and our fellow humans‚Äù to combat these threats?</p><p>‚ÄúInspiration is the key,‚Äù Payne wrote. ‚ÄúIf we could communicate with animals, ask them questions and receive answers‚Äîno matter how simple those questions and answers might turn out to be‚Äîthe world might soon be moved enough to at least start the process of halting our runaway destruction of life.‚Äù</p><p>Several other <em>CETI</em> team members made a similar point. ‚ÄúOne important thing that I hope will be an outcome of this project has to do with how we see life on land and in the oceans,‚Äù Bronstein said. ‚ÄúIf we understand‚Äîor we have evidence, and very clear evidence in the form of language-like communication‚Äîthat intelligent creatures are living there and that we are destroying them, that could change the way that we approach our Earth.‚Äù</p><p>‚ÄúI always look to Roger‚Äôs work as a guiding star,‚Äù Gruber told me. ‚ÄúThe way that he promoted the songs and did the science led to an environmental movement that saved whale species from extinction. And he thought that <em>CETI</em> could be much more impactful. If we could understand what they‚Äôre saying, instead of ‚Äòsave the whales‚Äô it will be ‚Äòsaved by the whales.‚Äô</p><p>‚ÄúThis project is kind of an offering,‚Äù he went on. ‚ÄúCan technology draw us closer to nature? Can we use all this amazing tech we‚Äôve invented for positive purposes?‚Äù</p><p>ChatGPT shares this hope. Or at least the A.I.-powered language model is shrewd enough to articulate it. In the version of ‚ÄúMoby-Dick‚Äù written by algorithms in the voice of a whale, the story ends with a somewhat ponderous but not unaffecting plea for mutuality:</p><blockquote><p>I, the White Leviathan, could only wonder if there would ever come a day when man and whale would understand each other, finding harmony in the vastness of the ocean‚Äôs embrace.&nbsp;‚ô¶</p></blockquote></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An effort to ban caste discrimination in California has touched a nerve (152 pts)]]></title>
            <link>https://www.politico.com/news/2023/09/04/ban-caste-discrimination-california-bill-00113817</link>
            <guid>37381979</guid>
            <pubDate>Mon, 04 Sep 2023 16:25:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/2023/09/04/ban-caste-discrimination-california-bill-00113817">https://www.politico.com/news/2023/09/04/ban-caste-discrimination-california-bill-00113817</a>, See on <a href="https://news.ycombinator.com/item?id=37381979">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div>
<p>
California state Sen. Aisha Wahab (center) gathers with supporters after a press conference introducing a bill that would outlaw caste discrimination in the state. | Jos√© Luis Villegas/AP Photo
</p>

</div>
<p>SACRAMENTO, Calif. ‚Äî Caste discrimination wasn‚Äôt on the radar of many lawmakers in California. Then it showed up on their doorstep.</p>
<p>Hundreds of people mobilized outside the state Capitol in recent months, protesting a bill from first-term state Sen. Aisha Wahab to add caste to the list of protected groups in California ‚Äî a proposal that many felt was unnecessary and unfairly tarnished the image of the South Asian community. Hearings on the bill got heated.</p>

<p>‚ÄúClearly we hit a nerve,‚Äù Wahab, who got death threats and is being targeted with a recall for her efforts, said at one hearing.</p>
</div><div>
<p>If the bill passes as expected and Gov. Gavin Newsom signs it into law, California would become the first state to explicitly outlaw caste-based discrimination, though Seattle has done so and other cities are considering it. Caste, a social hierarchy in which one‚Äôs group is inherited, is historically associated with South Asia and Hindus, and opponents argue such a ban stigmatizes the religious group.</p>
<p>The affair has had repercussions for Wahab in her heavily South Asian district. It‚Äôs become a bitter lesson in the pitfalls of wading into nuanced cultural issues in an ever-more diverse nation.</p>
<p>Wahab, a progressive tapped by Newsom to highlight his signature gun control effort, appeared to be caught off guard by the vitriolic response to what she views as a straightforward issue.</p>
<p>‚ÄúThis is a civil rights bill,‚Äù she said in an interview. ‚ÄúIt‚Äôs very simple. We‚Äôre trying to protect people.‚Äù</p>
<p>For her, it began as she campaigned in her San Francisco Bay Area district, hearing about an issue that has emerged in some employment discrimination cases in Silicon Valley as well as a divisive <a href="http://clerk.seattle.gov/~archives/Ordinances/Ord_126767.pdf" target="_blank" data-tracking="mpos=&amp;mid=&amp;lindex=&amp;lcol=">measure in Seattle</a> and elsewhere. But a bill to explicitly ban caste discrimination hadn‚Äôt been introduced in the California Legislature, even from the two members of South Asian descent.</p>
<p>The fact that this subject came up in the first place perhaps isn‚Äôt surprising. Indians represent the second-largest U.S. immigrant group after Mexicans, and Wahab‚Äôs district has one of the largest populations of Indian Americans. More broadly, South Asians have become more visible in American politics, with Nikki Haley and Vivek Ramaswamy running in the Republican presidential primary.</p>
<p>Wahab‚Äôs legislation, <span>Senate Bill 403</span>, is a floor vote away from reaching the governor‚Äôs desk, but not before a fractious legislative process in which she received pushback even from fellow progressive Democrats. Newsom‚Äôs office would not say whether he supports the bill.</p>
<p>Committee hearings were packed, with lines for public comment stretching out the door. Social media has been ablaze from both sides, and lawmakers have received tens of thousands of calls and emails. When the city council in Cupertino <a href="https://twitter.com/CityofCupertino/status/1683510888933003267" target="_blank" data-tracking="mpos=&amp;mid=&amp;lindex=&amp;lcol=">passed a resolution</a> opposing the bill, city officials said it was the most-attended public meeting they‚Äôve ever seen in the majority-Asian suburb.</p>
<h3>A progressive split</h3>
<p>Backlash from constituents and local officials prompted two Democratic state lawmakers whose districts overlap with Wahab‚Äôs, Assemblymembers Evan Low (D-Campbell) and Alex Lee (D-San Jose), to take the unusual step of openly disagreeing with their progressive colleague, suggesting amendments that ultimately watered down the legislation. All three are also in the Legislature‚Äôs Asian American and Pacific Islander caucus.</p>
<p>‚ÄúIt‚Äôs not politically expedient, but it‚Äôs the right thing to do,‚Äù Low said in an interview. ‚ÄúIt‚Äôs my genuine interest, because it breaks my heart to see members of our AAPI community being split.‚Äù</p>
<p>Lee‚Äôs office, which typically logs about 10 constituents providing a stance on a bill, received over 600 messages on SB 403. Just 26 were in support, according to a spokesperson. Low said that the ratio of opposition to support was ‚Äú99 to 1.‚Äù</p>
<p>The pair met with Wahab to share their concerns. Eventually, Wahab agreed to place caste under ‚Äúancestry‚Äù rather than list it as a standalone category such as race, gender identity and age, ensuring that the word remained in the bill, but less prominently.</p>
<p>Low did not take a vote on the proposal. But the amendments won over Lee, who gave a floor speech explaining why he was supporting the bill ‚Äî and noting that he tried to ensure the ban ‚Äúdoesn‚Äôt unfairly single out anyone.‚Äù</p>
<p>Low and another Bay Area legislator, state Sen. Josh Becker (D-Menlo Park), said caste hadn‚Äôt come up as an issue in decades of being around Silicon Valley tech circles, where there have been accusations of caste discrimination. Activists on both sides of the debate have focused on educating lawmakers about caste.</p>
<p>‚ÄúA lot of staff asked, ‚ÄòWhat is caste?‚Äô‚Äù Wahab said of the reaction when she first considered introducing the bill. ‚ÄúThey had to Google it.‚Äù</p>

<p>Suhag Shukla, executive director of the Hindu American Foundation ‚Äî one of the groups opposing the bill ‚Äî said that the term ‚Äúcaste‚Äù is unlike the state‚Äôs other protected categories.</p>
<p>‚ÄúEveryone has a race. Everyone has an ancestry. Everyone has a gender. Everyone has an age,‚Äù Shukla said. ‚ÄúNot everyone has a caste.‚Äù</p>
<p>Shukla believes the bill has sailed through the Legislature because nobody wants to be seen as being against an anti-discrimination bill.</p>
<p>The issue hits home for Assemblymember Ash Kalra (D-San Jose), the first Indian American elected to the state Legislature and one of two South Asian lawmakers serving in either house. Kalra voted for the proposal but said it was an emotional issue for him. He lamented during a committee hearing about seeing his community ‚Äútear each other apart on social media,‚Äù and hoped that both sides would make a ‚Äúcommitment to healing.‚Äù</p>
<h3>Heart of the movement</h3>
<p>Silicon Valley, home to a large South Asian population and some of the world‚Äôs largest tech companies, has been at the heart of a movement to combat caste-based discrimination.</p>
<p>A 2020 lawsuit by the California Civil Rights Department ‚Äî believed to be the first in the state to be filed on the grounds of caste-based discrimination ‚Äî accused two Cisco supervisors of discriminating against and harassing an employee who identified as Dalit, the lowest class in the caste hierarchy. The case against Cisco is ongoing, though complaints against the two supervisors were dropped earlier this year.</p>
<p>The bill‚Äôs supporters see the lawsuit as a milestone that has enabled more caste-oppressed people to come forward.</p>
<p>‚ÄúRight now, it‚Äôs such a gray area,‚Äù said Tanuja Gupta, who in 2021 quit her job as a senior manager at Google News in a <a href="https://www.washingtonpost.com/technology/2022/06/02/google-caste-equality-labs-tanuja-gupta/" target="_blank" data-tracking="mpos=&amp;mid=&amp;lindex=&amp;lcol=">highly publicized exit</a> after an event she had organized about caste issues was postponed.</p>
<p>Gupta is now in law school in New York. She said that one of the most frustrating parts of advocating for SB 403 has been the argument that caste discrimination isn‚Äôt occurring because there have been so few documented cases, calling it a ‚Äúchicken and egg argument.‚Äù</p>
<p>Using a different surname to protect against discrimination is not uncommon, said Prem Pariyar, a delegate for the National Association of Social Workers and Cal State East Bay alum who helped lead a successful push last year for the CSU school system to include caste in its <a href="https://www.calstate.edu/csu-system/faculty-staff/academic-senate/resolutions/2021-2022/3527.pdf" target="_blank" data-tracking="mpos=&amp;mid=&amp;lindex=&amp;lcol=">anti-discrimination policy</a>.</p>
<p>Pariyar was born into a Dalit family in Nepal and came to California in 2015 to escape caste discrimination. Friends told him that the state was progressive, friendly to immigrants and accepting of different cultures. Instead, he recalled being alienated by his Nepalese coworkers, who refused to room in shared housing with him because of his caste. Pariyar said he was forced to live out of a van for a month, an experience he called depressing and scary.</p>
<p>‚ÄúI thought they would not repeat those kinds of practices here,‚Äù Pariyar said.</p>
<h3>Talking points</h3>
<p>In mid-July, about 250 people gathered at an events center in Fremont, an East Bay suburb in Wahab‚Äôs district, for ‚ÄúCaste Con,‚Äù a full day of programming against the bill. Several Fremont city officials attended, as well as Palo Alto Mayor Lydia Kou. Fremont Mayor Lily Mei, who lost to Wahab in last year‚Äôs race for the local state Senate seat, was given a standing ovation when she was introduced.</p>
<p>The event was moderated by Satish Sharma, chair of the Global Hindu Federation based in the United Kingdom. Copies of Sharma‚Äôs book ‚ÄúCaste, Conversion: A Colonial Conspiracy‚Äù were available for free in the lobby.</p>
<p>At one point, Sharma asked ChatGPT to define ‚Äúcaste,‚Äù and then pointed out the number of times that the word ‚ÄúHindu‚Äù appeared in the computer‚Äôs response. ‚ÄúThat‚Äôs not an accident,‚Äù Sharma later said in an interview. ‚ÄúIt‚Äôs been seeded for such a long time. The word is a hate brand.‚Äù</p>
<p>Later, attendees heard talking points on how to defend their stance in the state Capitol. Salvatore Babones, a sociologist and associate professor at the University of Sydney, said people have to ‚Äúaccept the debate‚Äù over caste, noting that simple arguments such as ‚ÄúI‚Äôm not a Nazi‚Äù and ‚ÄúI‚Äôm not a white supremacist‚Äù do not work in the United States.</p>
<p>‚ÄúYou have to fight it on American terms,‚Äù Babones said. ‚ÄúIf you don‚Äôt fight it on American terms, you‚Äôre going to lose.‚Äù</p>
<p><i>Sejal Govindarao contributed to this report.</i></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Refact Code LLM: 1.6B LLM for code that reaches 32% HumanEval (177 pts)]]></title>
            <link>https://refact.ai/blog/2023/introducing-refact-code-llm/</link>
            <guid>37381862</guid>
            <pubDate>Mon, 04 Sep 2023 16:13:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://refact.ai/blog/2023/introducing-refact-code-llm/">https://refact.ai/blog/2023/introducing-refact-code-llm/</a>, See on <a href="https://news.ycombinator.com/item?id=37381862">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
                
                <p>Today we‚Äôre introducing Refact LLM: 1.6B code model with infill real-time code completion (including fill-in-the-middle(FIM) capability) and chat.
Refact LLM achieves the state-of-the-art performance among the code LLMs, coming closer to  HumanEval as Starcoder, being 10x smaller in size, and it beats other code models such as StableCode, CodeGen and ReplitCode on HumanEval metric.</p>
<p><strong>Summary</strong>:</p>
<ul><li>1.6b parameters</li><li>20 programming languages</li><li>4096 tokens context</li><li>code completion and chat capabilities</li><li>SoTA on HumanEval benchmark among similar code models</li><li>pre-trained on permissive licensed code and available for commercial use</li></ul>
<div><table><thead><tr><th>Model</th><th>Model Size</th><th>HumanEval pass@1</th></tr></thead><tbody><tr><td>DeciCoder-1b</td><td>1b</td><td>19.1%</td></tr><tr><td>Refact-1.6-fim</td><td>1.6b</td><td>32.0%</td></tr><tr><td>StableCode</td><td>3b</td><td>20.2%</td></tr><tr><td>ReplitCode v1</td><td>3b</td><td>21.9%</td></tr><tr><td>CodeGen2.5-multi</td><td>7b</td><td>28.4%</td></tr><tr><td>CodeLlama</td><td>7b</td><td>33.5%</td></tr><tr><td>StarCoder</td><td>15b</td><td>33.6%</td></tr></tbody></table></div>
<p>The base model was trained on our own set of code with permissive licenses only and open text datasets (the text to code ratio was 50:50). In total, we trained our base model on 1.2T tokens of code on our cluster.</p>
<p>The model was then fine-tuned with open code instruction-following datasets filtered for quality and a synthetic dataset based on <a href="https://huggingface.co/datasets/bigcode/the-stack-dedup">The Stack dedup v1.1</a> to improve FIM and boosting the base model performance.</p>
<p>You can read more about the architecture decisions that we made in the <a href="https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/">blog post</a>.</p>
<p>We aim for the model to be accessible to everyone, we‚Äôre releasing the model for commercial use under BigScience OpenRAIL-M license and making the weight available on <a href="https://huggingface.co/smallcloudai/Refact-1_6B-fim">HuggingFace</a>.</p>
<p>While the trend recently was for the model sizes to get bigger, we wanted to lower barriers to entry and make it a versatile tool for developers with varying hardware setups. With the smaller size, running the model is much faster and affordable than ever: the model can be served on most of all modern GPUs requiring just 3Gb RAM and works great for real-time code completion tasks.</p>
<p>Refact LLM can be easily integrated into existing developers workflows with <a href="https://github.com/smallcloudai/refact/">an open-source docker container</a> and <a href="https://marketplace.visualstudio.com/items?itemName=smallcloud.codify">VS Code</a> and <a href="https://plugins.jetbrains.com/plugin/20647-codify">JetBrains</a> plugins. With Refact‚Äôs intuitive user interface, developers can utilize the model easily for a variety of coding tasks. Finetune is available in the self-hosting (docker) and Enterprise versions, making suggestions more relevant for your private codebase.</p>
<p><img src="https://refact.ai/images/blog/introducing-refact-code-llm/palindrome.gif"></p>
<p>Refact 1.6B LLM is the third model in the family of our code models, with <a href="https://huggingface.co/smallcloudai/codify_3b_multi">CodeContrast 3b</a> and <a href="https://huggingface.co/smallcloudai/codify_medium_multi">CodeContrast 0.3b</a> released previously. We aim to continue with our research and future updates to improve the LLM‚Äôs performance and capabilities. We would love to get community contributions and feedback to enhance the model further. For any questions and ideas, please visit our <a href="https://smallcloud.ai/discord">Discord</a>.</p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Portal 64 ‚Äì A demake of Portal for the Nintendo 64 (326 pts)]]></title>
            <link>https://github.com/lambertjamesd/portal64</link>
            <guid>37381407</guid>
            <pubDate>Mon, 04 Sep 2023 15:27:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lambertjamesd/portal64">https://github.com/lambertjamesd/portal64</a>, See on <a href="https://news.ycombinator.com/item?id=37381407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Portal64</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/lambertjamesd/portal64/blob/master/assets/images/portal64_readme_logo.gif"><img src="https://github.com/lambertjamesd/portal64/raw/master/assets/images/portal64_readme_logo.gif" alt="" data-animated-image=""></a></p>
<p dir="auto">A demake of Portal for the Nintendo 64.</p>
<h2 tabindex="-1" dir="auto">How to build</h2>
<p dir="auto">First, you will need to setup <a href="https://crashoveride95.github.io/n64hbrew/modernsdk/startoff.html" rel="nofollow">Modern SDK</a>.</p>
<p dir="auto">After installing modern sdk you will want to also install</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install libnustd"><pre>sudo apt install libnustd</pre></div>
<p dir="auto">Next, you will need to download Blender 3.0 or higher. Then set the environment variable <code>BLENDER_3_0</code> to be the absolute path where the Blender executable is located on your system.</p>

<p dir="auto">e.g. add this to your ~/.bashrc</p>
<div dir="auto" data-snippet-clipboard-copy-content="export BLENDER_3_0=&quot;/usr/bin/blender&quot;"><pre><span>export</span> BLENDER_3_0=<span><span>"</span>/usr/bin/blender<span>"</span></span></pre></div>

<p dir="auto">You will need to install Python <code>vpk</code>.</p>


<p dir="auto">Install <code>vtf2png</code>, <code>sfz2n64</code>, and setup <code>skeletool64</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="echo &quot;deb [trusted=yes] https://lambertjamesd.github.io/apt/ ./&quot; \
    | sudo tee /etc/apt/sources.list.d/lambertjamesd.list
sudo apt update
sudo apt install vtf2png sfz2n64 mpg123 sox imagemagick unzip"><pre><span>echo</span> <span><span>"</span>deb [trusted=yes] https://lambertjamesd.github.io/apt/ ./<span>"</span></span> \
    <span>|</span> sudo tee /etc/apt/sources.list.d/lambertjamesd.list
sudo apt update
sudo apt install vtf2png sfz2n64 mpg123 sox imagemagick unzip</pre></div>

<p dir="auto">Install lua5.4 (remove other perhaps installed versions first, skelatool64 needs to be build with luac 5.4!)</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install lua5.4 liblua5.4-dev liblua5.4-0"><pre>sudo apt install lua5.4 liblua5.4-dev liblua5.4-0</pre></div>

<p dir="auto">Setup and build skelatool64 (the version included in this portal64 repo!)</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd skelatool64
./setup_dependencies.sh
make"><pre><span>cd</span> skelatool64
./setup_dependencies.sh
make</pre></div>

<p dir="auto">You will need to install nodejs. You can use apt for this</p>


<p dir="auto">You then need to add the following files from where Portal is installed to the folder <code>vpk</code>. (see vpk/add_vpk_here.md  for more details!)</p>
<div data-snippet-clipboard-copy-content="portal/portal_pak_000.vpk  
portal/portal_pak_001.vpk  
portal/portal_pak_002.vpk  
portal/portal_pak_003.vpk  
portal/portal_pak_004.vpk  
portal/portal_pak_005.vpk  
portal/portal_pak_dir.vpk

hl2/hl2_sound_misc_000.vpk
hl2/hl2_sound_misc_001.vpk
hl2/hl2_sound_misc_002.vpk
hl2/hl2_sound_misc_dir.vpk"><pre><code>portal/portal_pak_000.vpk  
portal/portal_pak_001.vpk  
portal/portal_pak_002.vpk  
portal/portal_pak_003.vpk  
portal/portal_pak_004.vpk  
portal/portal_pak_005.vpk  
portal/portal_pak_dir.vpk

hl2/hl2_sound_misc_000.vpk
hl2/hl2_sound_misc_001.vpk
hl2/hl2_sound_misc_002.vpk
hl2/hl2_sound_misc_dir.vpk
</code></pre></div>
<p dir="auto">Finally, run <code>make</code> to build the project.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clean out any previous build files
make clean

# Build
make

# In case you have any trouble with ROM running on hardware try
# wine install required to run properly
sudo apt install wine
make fix"><pre><span><span>#</span> Clean out any previous build files</span>
make clean

<span><span>#</span> Build</span>
make

<span><span>#</span> In case you have any trouble with ROM running on hardware try</span>
<span><span>#</span> wine install required to run properly</span>
sudo apt install wine
make fix</pre></div>
<br>
<h2 tabindex="-1" dir="auto">Build with Docker</h2>
<p dir="auto">Using the docker image the only setup step you need is to populating the vpk folder. After that you can build the docker image using</p>
<p dir="auto">Build the Docker image.</p>
<div dir="auto" data-snippet-clipboard-copy-content="make -f Makefile.docker image"><pre>make -f Makefile.docker image</pre></div>

<p dir="auto">Then build the rom using</p>

<p dir="auto">That will generate the rom at <code>/build/portal64.z64</code></p>
<br>
<h2 tabindex="-1" dir="auto">Current Level Checklist</h2>
<ul>
<li> Add indicator lights for signals</li>
<li> Apply rotation to static content</li>
<li> Remove overlap underneath doors</li>
</ul>
<h2 tabindex="-1" dir="auto">Current New Feature TODO List</h2>
<ul>
<li> rumble pak support?</li>
<li> Change default controls</li>
<li> Add puzzle element connections and additional signs</li>
<li> Use a much nearer clipping plane when rendering the portal gun</li>
<li> Investigate crash after falling into death water on test chamber 8</li>
<li> Add particle effects (shooting portal gun, energy pellet)</li>
<li> Add auto save checkpoints</li>
<li> Correct elevator timing</li>
<li> Adding loading notice between levels #45</li>
<li> ball velocity in test chamber 11</li>
<li> test chamber 04 has seams in a corner</li>
<li> pausing while glados is speaking can end her speech early</li>
<li> don't count boxes on buttons until it is released and stable</li>
<li> Portal not rendering recursively sometimes #138</li>
<li> disable portal surfaces manually on some surfaces #135</li>
<li> test chamber 02 needs more light in the first room</li>
<li> Presort portal gun polygon order #102</li>
</ul>
<h2 tabindex="-1" dir="auto">Current New Sounds TODO List</h2>
<ul>
<li> Box collision sounds</li>
<li> Unstationary scaffolding moving sound</li>
<li> Ambient background loop</li>
</ul>
<h2 tabindex="-1" dir="auto">Current Bug TODO List (Hardware Verified) (High-&gt;Low priority)</h2>
<p dir="auto">----------------------- v8</p>
<ul>
<li> player can clip through back of elevator by jumping and strafeing at the back corners while inside.</li>
<li> Player can trap themselves in chamber 5 by following instructions issue #75</li>
<li> Two wall portals next to eachother can be used to clip any object out of any level by pushing it into corner, then dropping.</li>
<li> Passing into a ceiling portal can sometimes mess with the player rotation</li>
<li> various visual glitches when running NTSC on PAL console #65</li>
<li> various visual glitches when running PAL on NTSC console #65</li>
<li> Can shoot portals, and walk through signage</li>
<li> Can place portals on ground after final fizzler on all levels</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VSCodium ‚Äì Open-source binaries of VSCode (456 pts)]]></title>
            <link>https://vscodium.com</link>
            <guid>37381335</guid>
            <pubDate>Mon, 04 Sep 2023 15:20:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vscodium.com">https://vscodium.com</a>, See on <a href="https://news.ycombinator.com/item?id=37381335">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="main">
<nav><ul>
<li><a href="#intro">home</a></li>
<li><a href="#why">Why</a></li>
<li><a href="#install">Install</a></li>
<li><a href="#moreinfo">More Info</a></li>
</ul></nav>
<div id="intro">
<p><img src="https://vscodium.com/img/codium_cnl.svg" alt="VSCodium logo" width="200"></p>
<h2>VSCodium</h2>
<h3>Free/Libre Open Source Software Binaries of VS Code</h3>
<p><a href="https://github.com/vscodium/vscodium/releases"><img src="https://img.shields.io/github/release/vscodium/vscodium.svg" alt="current release"></a>
<a href="https://github.com/VSCodium/vscodium/blob/master/LICENSE"><img src="https://img.shields.io/github/license/VSCodium/vscodium.svg" alt="license"></a>
<a href="https://gitter.im/VSCodium/Lobby"><img src="https://img.shields.io/gitter/room/vscodium/vscodium.svg" alt="Gitter"></a></p>
<p>VSCodium is a community-driven, freely-licensed binary distribution of Microsoft‚Äôs editor VS Code.</p>
<p><img src="https://vscodium.com/img/vscodium.png" alt="Screenshot"></p>
</div>
<div id="why">
<h2>Why Does This Exist</h2>
<p>Microsoft‚Äôs <code>vscode</code> source code is open source (MIT-licensed), but the product available for download (Visual Studio Code) is licensed under <a href="https://code.visualstudio.com/license">this not-FLOSS license</a> and contains telemetry/tracking. According to <a href="https://github.com/Microsoft/vscode/issues/60#issuecomment-161792005">this comment</a> from a Visual Studio Code maintainer:</p>
<blockquote>
<p>When we [Microsoft] build Visual Studio Code, we do exactly this. We clone the vscode repository, we lay down a customized product.json that has Microsoft specific functionality (telemetry, gallery, logo, etc.), and then produce a build that we release under our license.</p>
<p>When you clone and build from the vscode repo, none of these endpoints are configured in the default product.json. Therefore, you generate a ‚Äúclean‚Äù build, without the Microsoft customizations, which is by default licensed under the MIT license</p>
</blockquote>
<p>The VSCodium project exists so that you don‚Äôt have to download+build from source. This project includes special build scripts that clone Microsoft‚Äôs vscode repo, run the build commands, and upload the resulting binaries for you to <a href="https://github.com/VSCodium/vscodium/releases">GitHub releases</a>. <strong>These binaries are licensed under the MIT license. Telemetry is disabled.</strong></p>
<p>If you want to build from source yourself, head over to <a href="https://github.com/Microsoft/vscode">Microsoft‚Äôs vscode repo</a> and follow their <a href="https://github.com/Microsoft/vscode/wiki/How-to-Contribute#build-and-run">instructions</a>. VSCodium exists to make it easier to get the latest version of MIT-licensed VS Code.</p>
</div>
<div id="install">

<hr>
<h2>Use a Package Manager (providing VSCodium in their repository)</h2>
<p>The following package managers use their own repository, in case of any installation issues report to their related repository.</p>
<hr>

<h3>Install with Brew (Mac)</h3>
<p>If you are on a Mac and have <a href="https://brew.sh/">Homebrew</a> installed:</p>
<div><pre><code>brew <span>install</span> <span>--cask</span> vscodium
</code></pre></div>
<p><em>Note for Mac OS X Mojave users: if you see ‚ÄúApp can‚Äôt be opened because Apple cannot check it for malicious software‚Äù when opening VSCodium the first time, you can right-click the application and choose Open. This should only be required the first time opening on Mojave.</em></p>
<hr>

<h3>Install with Windows Package Manager (WinGet)</h3>
<p>If you use Windows and have <a href="https://github.com/microsoft/winget-cli">Windows Package Manager</a> installed:</p>
<pre><code>winget install vscodium
</code></pre>
<hr>

<h3>Install with Chocolatey (Windows)</h3>
<p>If you use Windows and have <a href="https://chocolatey.org/">Chocolatey</a> installed (thanks to <a href="https://github.com/Thilas">@Thilas</a>):</p>
<pre><code>choco install vscodium
</code></pre>
<hr>

<h3>Install with Scoop (Windows)</h3>
<p>If you use Windows and have <a href="https://scoop.sh/">Scoop</a> installed:</p>
<pre><code>scoop bucket add extras
</code></pre>
<pre><code>scoop install vscodium
</code></pre>
<hr>

<h3>Install with snap (Linux)</h3>
<p>VSCodium is available in the <a href="https://snapcraft.io/">Snap Store</a> as <a href="https://snapcraft.io/codium">Codium</a>, currently maintained by the VSCodium project.
If your GNU/Linux distribution has support for <a href="https://snapcraft.io/docs/installing-snapd">snaps</a>:</p>
<div><pre><code>snap <span>install </span>codium <span>--classic</span>
</code></pre></div>
<hr>

<h3>Install on Parrot OS:</h3>
<p>VSCodium is pre-installed in Parrot OS.</p>
<p>In case you don‚Äôt find it by default, you can retrieve it from the official Parrot repo</p>
<div><pre><code><span>sudo </span>apt update <span>&amp;&amp;</span> <span>sudo </span>apt <span>install </span>codium
</code></pre></div>
<hr>

<h3>Install on Nix(OS)</h3>
<p>VSCodium is available in Nixpkgs. You can install it by adding <code>vscodium</code> to <code>environment.systemPackages</code> in <code>configuration.nix</code>, or locally:</p>
<div><pre><code>nix-env <span>-iA</span> nixpkgs.vscodium
</code></pre></div>
<hr>

<h3>Install on Arch Linux</h3>
<p>VSCodium is available on the <a href="https://aur.archlinux.org/packages/vscodium-bin/">AUR (Arch User Repository)</a>, and can be installed with an AUR Helper.</p>
<p>Examples:</p>
<ul>
<li><strong>Aura</strong>:
<div><pre><code>sudo aura -A vscodium-bin
</code></pre></div>
</li>
<li><strong>Yay</strong>:

</li>
</ul>
<h2>Use a Package Manager (deb/rpm, provided by VSCodium related repository)</h2>
<p><a href="https://github.com/paulcarroty">@paulcarroty</a> has set up a <a href="https://gitlab.com/paulcarroty/vscodium-deb-rpm-repo">repository</a> for VSCodium. The instructions below are adapted from there with <a href="https://download.vscodium.com/">CDN</a> mirror. Any issues installing VSCodium using your package manager should be directed to that repository‚Äôs issue tracker.</p>
<hr>

<h3>Install on Debian / Ubuntu (deb package):</h3>
<p>Add the GPG key of the repository:</p>
<div><pre><code>wget <span>-qO</span> - https://gitlab.com/paulcarroty/vscodium-deb-rpm-repo/raw/master/pub.gpg <span>\</span>
    | gpg <span>--dearmor</span> <span>\</span>
    | <span>sudo dd </span><span>of</span><span>=</span>/usr/share/keyrings/vscodium-archive-keyring.gpg
</code></pre></div>
<p>Add the repository:</p>
<div><pre><code><span>echo</span> <span>'deb [ signed-by=/usr/share/keyrings/vscodium-archive-keyring.gpg ] https://download.vscodium.com/debs vscodium main'</span> <span>\</span>
    | <span>sudo tee</span> /etc/apt/sources.list.d/vscodium.list
</code></pre></div>
<p>Update then install vscodium (if you want vscodium-insiders, then replace <code>codium</code> by <code>codium-insiders</code>):</p>
<div><pre><code><span>sudo </span>apt update <span>&amp;&amp;</span> <span>sudo </span>apt <span>install </span>codium
</code></pre></div>
<hr>

<h3>Install on Fedora / RHEL / CentOS / RockyLinux / OpenSUSE (rpm package):</h3>
<p>Add the GPG key of the repository:</p>
<div><pre><code><span>sudo </span>rpmkeys <span>--import</span> https://gitlab.com/paulcarroty/vscodium-deb-rpm-repo/-/raw/master/pub.gpg
</code></pre></div>
<p>Add the repository:</p>
<ul>
<li><strong>Fedora/RHEL/CentOS/Rocky Linux</strong>:
<div><pre><code><span>printf</span> <span>"[gitlab.com_paulcarroty_vscodium_repo]</span><span>\n</span><span>name=download.vscodium.com</span><span>\n</span><span>baseurl=https://download.vscodium.com/rpms/</span><span>\n</span><span>enabled=1</span><span>\n</span><span>gpgcheck=1</span><span>\n</span><span>repo_gpgcheck=1</span><span>\n</span><span>gpgkey=https://gitlab.com/paulcarroty/vscodium-deb-rpm-repo/-/raw/master/pub.gpg</span><span>\n</span><span>metadata_expire=1h"</span> | <span>sudo tee</span> <span>-a</span> /etc/yum.repos.d/vscodium.repo
</code></pre></div>
</li>
<li><strong>OpenSUSE/SUSE</strong>:
<div><pre><code><span>printf</span> <span>"[gitlab.com_paulcarroty_vscodium_repo]</span><span>\n</span><span>name=gitlab.com_paulcarroty_vscodium_repo</span><span>\n</span><span>baseurl=https://download.vscodium.com/rpms/</span><span>\n</span><span>enabled=1</span><span>\n</span><span>gpgcheck=1</span><span>\n</span><span>repo_gpgcheck=1</span><span>\n</span><span>gpgkey=https://gitlab.com/paulcarroty/vscodium-deb-rpm-repo/-/raw/master/pub.gpg</span><span>\n</span><span>metadata_expire=1h"</span> | <span>sudo tee</span> <span>-a</span> /etc/zypp/repos.d/vscodium.repo
</code></pre></div>
</li>
</ul>
<p>Install the software:
(if you want vscodium-insiders, then replace <code>codium</code> by <code>codium-insiders</code>)</p>
<ul>
<li><strong>Fedora/RHEL/CentOS/Rocky Linux</strong>:

</li>
<li><strong>OpenSUSE/SUSE</strong>:

<hr>
</li>
</ul>

<h3>Install on Gentoo / Funtoo Linux (ebuild):</h3>
<ul>
<li><strong>Funtoo</strong>:</li>
</ul>
<div><pre><code><span>sudo </span>emerge <span>-av</span> vscodium-bin
</code></pre></div>
<ul>
<li><strong>Gentoo</strong>:</li>
</ul>

<hr>

<h2>Flatpak Option (Linux)</h2>
<p>VSCodium is (unofficially) available as a <a href="https://flathub.org/apps/details/com.vscodium.codium">Flatpak app</a> and here‚Äôs the <a href="https://github.com/flathub/com.vscodium.codium">build repo</a>. If your distribution has support for <a href="https://flathub.org/">flatpak</a>, and you have enabled the <a href="https://flatpak.org/setup/">flathub repo</a>, you can install VSCodium via the command line:</p>
<div><pre><code>flatpak <span>install </span>flathub com.vscodium.codium
</code></pre></div>
<p>‚Ä¶or by opening the <a href="https://dl.flathub.org/repo/appstream/com.vscodium.codium.flatpakref">flatpakref</a> file from <a href="https://flathub.org/apps/details/com.vscodium.codium">Flathub</a>. VSCodium can also be found in GNOME Software if you have <code>gnome-software-plugin-flatpak</code> installed (as recommended in the Flathub setup instructions).</p>
</div>
<div id="moreinfo">
<h2>More Info</h2>
<p>The most up-to-date information on migrating from Visual Studio Code and other quirks you might encounter are <a href="https://github.com/VSCodium/vscodium/blob/master/DOCS.md">documented</a>.</p>
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Keep ‚Äì GitHub Actions for your monitoring tools (130 pts)]]></title>
            <link>https://github.com/keephq/keep</link>
            <guid>37381268</guid>
            <pubDate>Mon, 04 Sep 2023 15:15:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/keephq/keep">https://github.com/keephq/keep</a>, See on <a href="https://news.ycombinator.com/item?id=37381268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/assets/keep.png?raw=true"><img src="https://github.com/keephq/keep/raw/main/assets/keep.png?raw=true" width="86"></a>
</p>
<h2 tabindex="-1" dir="auto">The open-source alerts management and automation platform</h2>

<p><a href="https://github.com/keephq/keep/blob/main/LICENSE">
        <img src="https://camo.githubusercontent.com/ac033cba59891cb8c4a65b0ca7df802488f4fc380c640bfd591396855e799f9a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6b65657068712f6b656570" data-canonical-src="https://img.shields.io/github/license/keephq/keep">
    </a>
    <a href="https://keephq.dev/slack" rel="nofollow">
        <img src="https://camo.githubusercontent.com/517ab7911d889dc940c47033b7dea1a89d31757c9dc5a57a0efba5935ad05a22/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436861742d6f6e253230536c61636b2d626c756576696f6c6574" alt="Slack community channel" data-canonical-src="https://img.shields.io/badge/Chat-on%20Slack-blueviolet">
    </a>
    <a href="https://codecov.io/gh/keephq/keep" rel="nofollow">
        <img src="https://camo.githubusercontent.com/e947440180aa861f089c54c91ebc1c0ab7e65f021c349127674fc5a1692290ef/68747470733a2f2f636f6465636f762e696f2f67682f6b65657068712f6b6565702f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d3256543658594d524753" data-canonical-src="https://codecov.io/gh/keephq/keep/branch/main/graph/badge.svg?token=2VT6XYMRGS">
    </a>
</p>
<p dir="auto">
    <a href="#why-keep">Why Keep?</a>
    ¬∑
    <a href="#getting-started">Getting started</a>
    ¬∑
    <a href="#supported-providers">Supported tools and integrations</a>
    ¬∑
    <a href="https://docs.keephq.dev/" rel="nofollow">Docs</a>
    ¬∑
    <a href="https://platform.keephq.dev/" rel="nofollow">Try it out</a>
    ¬∑
    <a href="https://keephq.dev/" rel="nofollow">Website</a>
    ¬∑
    <a href="https://github.com/keephq/keep/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.md&amp;title=">Report Bug</a>
    ¬∑
    <a href="https://keephq.dev/slack" rel="nofollow">Slack Community</a>
</p>
<h3 tabindex="-1" dir="auto">
Keep makes it easy to consolidate all your alerts into a single pane of glass and to orchestrate workflows to automate your end-to-end processes. <p> Enrich any tool with <a href="https://docs.datadoghq.com/service_management/workflows/" rel="nofollow">Datadog Workflow Automation</a> like capabilities.
</p></h3>
<h2 tabindex="-1" dir="auto">How does it work?</h2>
<ol dir="auto">
<li><strong>Connect your tools</strong>: Connect everything from monitoring platforms to databases and ticketing systems.</li>
</ol>

<ol start="2" dir="auto">
<li><strong>Set up Workflows</strong>: Initiate automated workflows in response to alerts or based on custom intervals.</li>
</ol>
<div dir="auto">
<table>
<thead>
<tr>
<th>Create and upload workflows</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/assets/upload_workflow.gif"><img src="https://github.com/keephq/keep/raw/main/assets/upload_workflow.gif" data-animated-image=""></a></td>
</tr>
</tbody>
</table>
</div>
<ol start="3" dir="auto">
<li><strong>Operational efficiency</strong>: Automate your alert handling to focus your team's efforts on what really matters.</li>
</ol>
<h2 tabindex="-1" dir="auto">Why Keep?</h2>
<ol dir="auto">
<li><strong>Centralized dashboard</strong>: Manage all your alerts across different platforms in a single interface.</li>
<li><strong>Noise reduction</strong>: Deduplicate and correlate alerts to reduce alert fatigue.</li>
<li><strong>Automation</strong>: Trigger workflows for alert enrichment and response.</li>
<li><strong>Developer-first</strong>: Keep is API-first and lets you manage your workflows as code.</li>
<li><strong>Works with every tool</strong>: Plenty of <a href="#supported-providers">supported providers</a> and more to come.</li>
</ol>
<h2 tabindex="-1" dir="auto">Workflows</h2>
<p dir="auto">The easiest way of thinking about Workflow in Keep is GitHub Actions. At its core, a Workflow in Keep is a declarative YAML file, composed of triggers, steps, and actions and serves to manage, enrich, and automate responses to alerts:</p>
<div dir="auto" data-snippet-clipboard-copy-content="workflow:
  id: most-basic-keep-workflow
  description: send a slack message when a cloudwatch alarm is triggered
  # workflow triggers - supports alerts, interval, and manual triggers
  triggers:
    - type: alert
      filters:
        - key: source
          value: cloudwatch
    - type: manual
  # list of steps that can add context to your alert
  steps:
    - name: enrich-alert-with-more-data-from-a-database
      provider:
        type: bigquery
        config: &quot;{{ providers.bigquery-prod }}&quot;
        with:
          query: &quot;SELECT customer_id, customer_type as date FROM `customers_prod` LIMIT 1&quot;
  # list of actions that can automate response and do things with your alert
  actions:
    - name: trigger-slack
      provider:
        type: slack
        config: &quot; {{ providers.slack-prod }} &quot;
        with:
          message: &quot;Got alarm from aws cloudwatch! {{ alert.name }}&quot;"><pre><span>workflow</span>:
  <span>id</span>: <span>most-basic-keep-workflow</span>
  <span>description</span>: <span>send a slack message when a cloudwatch alarm is triggered</span>
  <span><span>#</span> workflow triggers - supports alerts, interval, and manual triggers</span>
  <span>triggers</span>:
    - <span>type</span>: <span>alert</span>
      <span>filters</span>:
        - <span>key</span>: <span>source</span>
          <span>value</span>: <span>cloudwatch</span>
    - <span>type</span>: <span>manual</span>
  <span><span>#</span> list of steps that can add context to your alert</span>
  <span>steps</span>:
    - <span>name</span>: <span>enrich-alert-with-more-data-from-a-database</span>
      <span>provider</span>:
        <span>type</span>: <span>bigquery</span>
        <span>config</span>: <span><span>"</span>{{ providers.bigquery-prod }}<span>"</span></span>
        <span>with</span>:
          <span>query</span>: <span><span>"</span>SELECT customer_id, customer_type as date FROM `customers_prod` LIMIT 1<span>"</span></span>
  <span><span>#</span> list of actions that can automate response and do things with your alert</span>
  <span>actions</span>:
    - <span>name</span>: <span>trigger-slack</span>
      <span>provider</span>:
        <span>type</span>: <span>slack</span>
        <span>config</span>: <span><span>"</span> {{ providers.slack-prod }} <span>"</span></span>
        <span>with</span>:
          <span>message</span>: <span><span>"</span>Got alarm from aws cloudwatch! {{ alert.name }}<span>"</span></span></pre></div>
<p dir="auto">Workflow triggers can either be executed manually when an alert is activated or run at predefined intervals. More examples can be found <a href="https://github.com/keephq/keep/tree/main/examples/workflows">here</a>.</p>
<h2 tabindex="-1" dir="auto">Supported Providers</h2>
<blockquote>
<p dir="auto">Missing any? Just submit a <a href="https://github.com/keephq/keep/issues/new?assignees=&amp;labels=&amp;projects=&amp;template=new_provider.md&amp;title=">new provider issue</a> and we will add it in the blink of an eye.</p>
</blockquote>
<h3 tabindex="-1" dir="auto">Observability tools</h3>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/newrelic-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/newrelic-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/datadog-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/datadog-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/cloudwatch-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/cloudwatch-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/elastic-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/elastic-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/grafana-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/grafana-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/prometheus-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/prometheus-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/zabbix-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/zabbix-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/sentry-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/sentry-icon.png?raw=true"></a>
</p>
<h3 tabindex="-1" dir="auto">Databases and data warehouses</h3>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/bigquery-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/bigquery-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/mysql-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/mysql-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/postgres-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/postgres-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/snowflake-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/snowflake-icon.png?raw=true"></a>
</p>
<h3 tabindex="-1" dir="auto">Communication platforms</h3>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/slack-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/slack-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/teams-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/teams-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/telegram-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/telegram-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/pushover-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/pushover-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/resend-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/resend-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/discord-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/discord-icon.png?raw=true"></a>
</p>
<h3 tabindex="-1" dir="auto">Incident Management tools</h3>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/pagerduty-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/pagerduty-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/opsgenie-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/opsgenie-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/zenduty-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/zenduty-icon.png?raw=true"></a>
</p>
<h3 tabindex="-1" dir="auto">Ticketing tools</h3>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/jira-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/jira-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/trello-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/trello-icon.png?raw=true"></a>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/keephq/keep/blob/main/keep-ui/public/icons/github-icon.png?raw=true"><img width="32" height="32" src="https://github.com/keephq/keep/raw/main/keep-ui/public/icons/github-icon.png?raw=true"></a>
</p>
<h2 tabindex="-1" dir="auto">Getting Started</h2>
<h3 tabindex="-1" dir="auto">Overview</h3>
<p dir="auto">Keep composed of three main components:</p>
<ol dir="auto">
<li><a href="https://github.com/keephq/keep/tree/main/keep-ui">Keep UI</a> - A NextJS app to connect your providers, centralize alerts and create the workflows.</li>
<li><a href="https://github.com/keephq/keep/tree/main/keep">Keep Backend</a> - A FastAPI server that implements the business logic behind Keep, including integrating with the tools, working with alerts and scheduling and running the workflows.</li>
<li><a href="https://github.com/keephq/keep/blob/main/keep/cli/cli.py">Keep CLI</a> - A CLI that lets you control and manage Keep via CLI.</li>
</ol>
<blockquote>
<p dir="auto"><strong>Disclaimer</strong>: we use <a href="https://posthog.com/faq" rel="nofollow">PostHog</a> to collect anonymous telemetries to better learn how users use Keep (masked screen recordings for CLI commands)
To turn PostHog off, set the <code>DISABLE_POSTHOG</code> environment variable.</p>
</blockquote>
<h3 tabindex="-1" dir="auto">Quickstart</h3>
<h4 tabindex="-1" dir="auto">Spinning up Keep with docker-compose</h4>
<p dir="auto">The easiest way to start with Keep is to run it via docker-compose:</p>
<div dir="auto" data-snippet-clipboard-copy-content="wget -O docker-compose.yml https://raw.githubusercontent.com/keephq/keep/main/docker-compose.yml
docker-compose -f docker-compose.yml up"><pre>wget -O docker-compose.yml https://raw.githubusercontent.com/keephq/keep/main/docker-compose.yml
docker-compose -f docker-compose.yml up</pre></div>
<p dir="auto">The UI is now available at <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a> and the backend is available at <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a>.</p>
<h4 tabindex="-1" dir="auto">Local development</h4>
<p dir="auto">You can also start Keep within your favorite IDE, e.g. <a href="https://docs.keephq.dev/development/getting-started#vscode" rel="nofollow">VSCode</a></p>
<h4 tabindex="-1" dir="auto">Wanna get Keep up and running in production? Go through our detailed <a href="https://docs.keephq.dev/development" rel="nofollow">development guide</a></h4>
<h2 tabindex="-1" dir="auto">ü´µ Keepers</h2>
<p dir="auto">Thank you for contributing and continuously making <b>Keep</b> better, <b>you're awesome</b> ü´∂</p>
<a href="https://github.com/keephq/keep/graphs/contributors">
  <img src="https://camo.githubusercontent.com/9aaf66e2a9e49fe5d0f06401a3b0a637e19daecdc36424f934051eae43ec0db8/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6b65657068712f6b656570" data-canonical-src="https://contrib.rocks/image?repo=keephq/keep">
</a>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Svelte is surprisingly easy to learn (169 pts)]]></title>
            <link>https://kaviisuri.com/you-dont-need-to-learn-svelte</link>
            <guid>37380980</guid>
            <pubDate>Mon, 04 Sep 2023 14:47:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kaviisuri.com/you-dont-need-to-learn-svelte">https://kaviisuri.com/you-dont-need-to-learn-svelte</a>, See on <a href="https://news.ycombinator.com/item?id=37380980">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><h2 id="heading-introduction">Introduction</h2>
<p>What if I told you, You don't need to learn svelte! Not because svelte is bad, or not worth learning. It's quite the opposite. You don't need to learn it because you already know it! <em>Dan Dan Daaaannn...</em></p>
<p>You've been working with JavaScript, crafting web marvels and wrangling code like a seasoned pro. You've tamed the wild async functions, navigated the treacherous waters of callbacks, and even had a dance-off with the quirky <code>this</code> keyword. So, <strong>why on Earth would you need to learn something new</strong>?</p>
<h3 id="heading-what-is-this">What is this?</h3>
<p>In this journey <strong>we uncover a superpower, hidden deep inside you</strong>, buried under mounds of react wrappers and redux sagas. Today, you'll come to realize that Svelte is not some arcane magic potion you need to memorize incantations or syntax for. <strong>It's JavaScript, with a sleek makeover and a secret identity.</strong></p>
<p>By the end of this article, <strong>you'll find that you don't need to</strong> learn <strong>Svelte‚Äîit's practically d√©j√† vu with a futuristic twist!</strong></p>
<p>Svelte is suprisingly easy to learn, and it's still pretty awesome! So, Let's get to it!</p>
<h2 id="heading-what-is-svelte">What is Svelte?</h2>
<p>Svelte is a UI framework (we don't shy away from that word here, unlike some other "libraries", ahem ahem...), that's compiled, compact and complete!</p>
<p>It's extremely fast, easy to use and also <a target="_blank" href="https://survey.stackoverflow.co/2023/#section-admired-and-desired-web-frameworks-and-technologies">the most admired JS Web framework according to Stack Overflow Survey</a>.</p>
<h2 id="heading-why-svelte-you-ask">Why Svelte, You Ask?</h2>
<h2 id="heading-compiled">Compiled</h2>
<p>With Svelte, there's no runtime needed. No need to send a few kilobytes of "SvelteJS" runtime on every website you ever built/will build. It's just a compiler with targets to "document.getElementById" instead of machine code.</p>
<h2 id="heading-concise">Concise</h2>
<p>Svelte lets you write beautiful syntax, you know it already! It's HTML CSS and JS (it's like the Avengers of web languages, teaming up to form something magnificent). Svelte compiles it down to extremely fast and optimized JS.</p>
<p>It's like having your cake and eating it too, except it's code, and you're making it run faster than a caffeine-infused squirrel.</p>
<h2 id="heading-complete">Complete</h2>
<p>Svelte comes with a state management solution (one of the best I've seen till now), and component-scoped styling (no styled components needed). It even has motion primitives for those jazzy animations that make users go "oooh" and "ah."</p>
<p>So, while you were busy <code>npm install</code>ing the bare essentials for your projects, Svelte was chilling in the corner, whispering, "I've got this, fam."</p>
<h2 id="heading-its-all-javascript">It's all Javascript</h2>
<p>Love it or hate it, Javascript is a very useful technology, and all the libraries, frameworks, and "meta" frameworks, are nothing but abstractions over JS (mostly leaky ones).</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1693294080856/65301be7-5198-4b58-bf0b-a64e85edf472.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>That's where Svelte shines, even though it's another "web framework", It doesn't throw a bunch of alien syntax at you, expecting you to decipher it like an ancient scroll. No, Svelte takes the familiar, battle-tested primitives, syntax, and concepts of JavaScript and weaves them into its fabric.</p>
<p>So, while other frameworks might introduce themselves with an air of mystique, Svelte strides in with a friendly nod, saying, "Hey, it's all JavaScript here. Let's create something amazing together."</p>
<h2 id="heading-but-why-though">But why though...</h2>
<p>Okay, Okay, I see you react lovers (I'm one too!), you might not be convinced yet. Here are a few examples to tip you to the right side...</p>
<h2 id="heading-effectsreactivity">Effects/Reactivity</h2>
<p>With react, here's what's needed to make something run every time a dependency changes</p>
<pre><code><span><span>function</span> <span>Component</span>(<span></span>) </span>{
    useEffect(<span>() =&gt;</span> {
        <span>console</span>.log(<span>"Count Changed!"</span>, count)
    }, [count])

    <span>return</span> <span>/** jsx **/</span>
}
</code></pre>
<p>Not so bad, is it? Now see what it translates to in svelte</p>
<pre><code><span>&lt;<span>script</span>&gt;</span><span>
    <span>$</span>: {
        <span>console</span>.log(<span>"Count Changed!"</span>, count)
    }
</span><span>&lt;/<span>script</span>&gt;</span>

<span>&lt;!-- html --&gt;</span>
</code></pre>
<p>That's it! Svelte automatically tracks dependencies, and you only need to label a code block as reactive (<code>$:</code>). It'll run the code any time those dependencies change.</p>
<h2 id="heading-storesstate-management">Stores/State Management</h2>
<p>While in react, you might go with complex setups like zustand, react-query or redux. Svelte gives you an in-built solution. A fascinating, cutting-edge idea called "pub-sub". Yupp, it's that dumb.</p>
<pre><code><span>/** store.ts **/</span>
<span>export</span> <span>const</span> counts = writable({
    <span>a</span>: <span>0</span>,
    <span>b</span>: <span>0</span>,
})
</code></pre>
<pre><code><span>&lt;<span>script</span>&gt;</span><span>
    <span>import</span> counts <span>from</span> <span>'./store.ts'</span>;
    <span>import</span> {onMount} <span>from</span> <span>'svelte'</span>;
</span><span>&lt;/<span>script</span>&gt;</span>

<span>&lt;<span>button</span> <span>on:click</span>=<span>{()</span> =&gt;</span> $counts.a += 1}&gt;
    Increment A ({$counts.a})
<span>&lt;/<span>button</span>&gt;</span>
</code></pre>
<p>Here, the component only "subscribes" to <code>count.a</code>, unline what react context allows you to do.</p>
<blockquote>
<p>The <code>$</code> just before the variable name makes the component handle subscribing and unsubscribing to the store for you!</p>
</blockquote>
<h2 id="heading-simple-lifecycle">Simple Lifecycle</h2>
<p>While React uses some implicit behaviors of <code>useEffect</code> to do component lifecycles, Svelte is extremely explicit and simple</p>
<pre><code><span>&lt;<span>script</span>&gt;</span><span>
    <span>// ...imports</span>
    onMount(<span>() =&gt;</span> {
        <span>// ... on component mount stuff</span>
        <span>return</span> <span>() =&gt;</span> { <span>// cleanup stuff 1 }</span>
    })
</span><span>&lt;/<span>script</span>&gt;</span>
<span>&lt;!-- Component Markup --&gt;</span>
<span>&lt;<span>button</span>&gt;</span>click me!<span>&lt;/<span>button</span>&gt;</span>
</code></pre>
<p>If you wanna do something only once, just write it in the script tag! Unlike React Components, the bodies of the script tag of a Svelte Component are only executed when it's created.</p>
<h2 id="heading-what-if-i-want-redux-actions">What if I want Redux actions?</h2>
<p>To restrict changes to the store, all you need to do is this:</p>
<pre><code><span>const</span> _counts = writeable({
    <span>a</span>: <span>0</span>,
    <span>b</span>: <span>0</span>,
});

<span>export</span> <span>const</span> counts = {
    <span>subscribe</span>: _counts.subscribe,
    <span>incrementA</span>: <span>() =&gt;</span> _counts.update(<span>(<span>counts</span>) =&gt;</span> {...counts, <span>a</span>: counts.a + <span>1</span>}),
    ...
};
</code></pre>
<p>A store is an object with a <code>subscribe</code> method. You're free to mold it as you please!</p>
<h2 id="heading-scoped-styles">Scoped Styles</h2>
<pre><code><span>&lt;<span>script</span>&gt;</span><span>
<span>// ...</span>
</span><span>&lt;/<span>script</span>&gt;</span>

<span>&lt;<span>div</span> <span>class</span>=<span>"container"</span>&gt;</span>
    <span>&lt;<span>div</span> <span>class</span>=<span>"card"</span>&gt;</span>
        <span>&lt;<span>h3</span>&gt;</span>Name<span>&lt;/<span>h3</span>&gt;</span>
        <span>&lt;<span>p</span>&gt;</span>Description<span>&lt;/<span>p</span>&gt;</span>
    <span>&lt;/<span>div</span>&gt;</span>
<span>&lt;/<span>div</span>&gt;</span>

<span>&lt;<span>style</span>&gt;</span><span>
    <span>.container</span> {
        <span>/** .. **/</span>
    }
    <span>.card</span> {
        <span>/** .. **/</span>
    }
</span><span>&lt;/<span>style</span>&gt;</span>
</code></pre>
<p>All of these classes are scoped to the component by default, "container" means nothing outside of this component.</p>
<p>You get all this, with 0 dependency and no runtime cost.</p>
<h2 id="heading-but-what-about-the-ecosystem">But what about the "ecosystem"?</h2>
<p>That's the best part, the svelte ecosystem is bigger than React! because it's all of the JS ecosystem.</p>
<p>You see, in the realm of Svelte, there's no need for those wrappers that you might have wrestled with in the past. Say goodbye to deciphering cryptic syntax or quirky APIs just to connect a wrapper to the actual library you want to use. Nope, with Svelte, it's as straightforward as it gets.</p>
<blockquote>
<p>It's like a direct line to JavaScript paradise ‚Äì no middlemen required! üöÄ</p>
</blockquote>
<h2 id="heading-lets-look-at-an-example">Let's Look at an example</h2>
<p>Imagine you're a developer aiming to add some stunning charts to your web app using the renowned "Chart.js" library.</p>
<h3 id="heading-react-a-web-of-wrappers">React: A Web of Wrappers</h3>
<h4 id="heading-step-1-install-the-wrapper">Step 1: Install the Wrapper</h4>
<p>First, you'd need to find a React wrapper for Chart.js, install it, and cross your fingers that it behaves as expected. Also remember to use the <code>2</code> version.</p>
<pre><code><span>// Terminal</span>
npm install react-chartjs<span>-2</span> chart.js
</code></pre>
<h4 id="heading-step-2-import-the-wrapper">Step 2: Import the Wrapper</h4>
<p>Next, you import the wrapper, wrap your component around it, and pray there are no compatibility issues.</p>
<pre><code><span>// Component.jsx</span>
<span>import</span> React <span>from</span> <span>'react'</span>;
<span>import</span> { Bar } <span>from</span> <span>'react-chartjs-2'</span>;

<span>const</span> ChartComponent = <span>() =&gt;</span> {
  <span>const</span> data = {<span>/* Chart data here */</span>};

  <span>return</span> <span><span>&lt;<span>Bar</span> <span>data</span>=<span>{data}</span> /&gt;</span></span>;
};

<span>export</span> <span>default</span> ChartComponent;
</code></pre>
<h4 id="heading-step-3-deal-with-wrapper-limitations">Step 3: Deal with Wrapper Limitations</h4>
<p>You realize that the wrapper doesn't quite support the latest Chart.js features, so you resort to direct manipulation to achieve your goals.</p>
<blockquote>
<p>(I deleted the code here because it got too long ü§ï)</p>
</blockquote>
<h3 id="heading-svelte-the-direct-approach">Svelte: The Direct Approach</h3>
<h4 id="heading-step-1-add-the-library">Step 1: Add the Library</h4>
<p>In the world of Svelte, you don't need a special wrapper. You just add the Chart.js library directly.</p>
<pre><code><span>&lt;!-- Component.svelte --&gt;</span>
<span>&lt;<span>script</span>&gt;</span><span>
  <span>import</span> { onMount } <span>from</span> <span>'svelte'</span>;
  <span>import</span> Chart <span>from</span> <span>'chart.js'</span>;

  <span>let</span> chartInstance;
  <span>let</span> canvas
  <span>$</span>: {
    <span>const</span> ctx = canvas.getContext(<span>'2d'</span>);
    chartInstance = <span>new</span> Chart(ctx, {<span>/* Chart config here */</span>});
  }
</span><span>&lt;/<span>script</span>&gt;</span>

<span>&lt;<span>canvas</span> <span>bind:this</span>=<span>{canvas}</span>&gt;</span><span>&lt;/<span>canvas</span>&gt;</span>
</code></pre>
<h3 id="heading-and-done">And... Done!</h3>
<p>No wrappers, no compatibility struggles ‚Äì just plain JavaScript, building upon what you already know. Svelte's direct approach feels like a breath of fresh air, reminding us that sometimes, less is truly more.</p>
<h2 id="heading-sold-sold-where-do-i-start">Sold! Sold! Where do I start?</h2>
<p>Ah, the enthusiasm! You're ready to jump into the Svelte universe and flex those JavaScript muscles. Here we go!</p>
<h2 id="heading-step-1-embrace-the-basics">Step 1: Embrace The Basics</h2>
<p>Good Documentation is a godsend in software, and <a target="_blank" href="https://learn.svelte.dev/tutorial/welcome-to-svelte">Svelte's Intro Tutorial</a> is especially good. It's like the "Welcome to Svelte" mat laid out for you.</p>
<p>This is where Svelte gives you the first taste of its most-priced quality, its simplicity.</p>
<h2 id="heading-step-2-craft-a-mini-masterpiece">Step 2: Craft a Mini-Masterpiece</h2>
<p>Once you've completed the tutorial and seen how seamlessly Svelte meshes with your JavaScript prowess, it's playtime. Go make a project you've been thinking of building. Don't have one? Here are a few cool ones you can start with:</p>
<ol>
<li><p><strong>Virtual Recipe Box -</strong> So you never lose your favourite recipes</p>
</li>
<li><p><strong>Snippets Manager</strong> - So you don't ever loose that bash command you keep safely in your bash history (I know you do...)</p>
</li>
<li><p><strong>Project Idea Repository</strong> - So you don't need this list again!</p>
</li>
</ol>
<p>It's your playground, play around, and you'll realize, how beautiful it is to just use JS and the web platform.</p>
<h2 id="heading-conclusion">Conclusion</h2>
<p>Svelte is like a breath of fresh air among the syntax-heavy, complexity-mongering tools that exist in the JS ecosystem. You don't need to learn svelte, because it has a learning curve so small, that you won't even realize it when you learn it.</p>
<p>You of course won't become an expert in a day, but Svelte enables you to focus on building stuff rather than learning weird quirks of the framework.</p>
<p>So, whether you're a battle-hardened developer or a curious newcomer, consider giving Svelte a spin. It might just inspire you to view web development through a cybernetically enhanced lens.</p>
<blockquote>
<p>Alright, let's cut to the chase, my friend! You've just uncovered a goldmine of knowledge, and the fun doesn't stop here.</p>
<p>So, why wait? Dive into the excitement, join our newsletter community, and let's embark on this journey together. Your inbox is about to become the ultimate source of inspiration. Sign up now and stay ahead of the curve!</p>
</blockquote>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Antioxidants found to spur cancer growth and metastasis in mice (141 pts)]]></title>
            <link>https://news.ki.se/antioxidants-stimulate-blood-flow-in-tumours</link>
            <guid>37380863</guid>
            <pubDate>Mon, 04 Sep 2023 14:35:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.ki.se/antioxidants-stimulate-blood-flow-in-tumours">https://news.ki.se/antioxidants-stimulate-blood-flow-in-tumours</a>, See on <a href="https://news.ycombinator.com/item?id=37380863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          <p><img src="https://news.ki.se/sites/default/files/styles/article_full_width/public/qbank/TingWang_fotoStefanZimmerman_custom20230831170719.jpg" alt="Ting Wang"></p><div>
            <p>
            Ting Wang. Photo: Stefan Zimmerman
          </p></div>
        </div><div>
        
            <p>‚ÄúMany clinical trials have evaluated the efficacy of angiogenesis inhibitors, but the results have not been as successful as anticipated,‚Äù says <a href="https://staff.ki.se/people/ting-wang">Ting Wang</a>, doctoral student in Professor Berg√∂‚Äôs group at Karolinska Institutet. ‚ÄúOur study opens the door to more effective ways of preventing angiogenesis in tumours; for example, patients whose tumours exhibit high levels of BACH1 might benefit more from anti-angiogensis therapy than patients with low BACH1 levels.‚Äù</p>

<p>The researchers used a range of cell-biological methods and concentrated most of their work on lung cancer tumours by studying organoids ‚Äì small cultivated microtumours from patients. But they also studied mice and samples of human breast and kidney tumours. Tumours in which BACH1 was activated, either via ingested antioxidants or by overexpression of the BACH1 gene, produced more new blood vessels and were highly sensitive to angiogenesis inhibitors.</p>

<p>‚ÄúThe next step is to examine in detail how levels of oxygen and free radicals can regulate the BACH1 protein, and we will continue to determine the clinical relevance of our results,‚Äù says Ting Wang. ‚ÄúWe‚Äôll also be doing similar studies in other cancer forms such as breast, kidney and skin cancer.‚Äù</p>

<p>The study was conducted in close collaboration with KI researchers Susanne Schlisio, Staffan Str√∂mblad and Eckardt Treuter and researchers at the First Affiliated Hospital of Zhengzhou University. The research was financed primarily by grants from the Swedish Cancer Society, the Swedish Research Council, the Sj√∂berg Foundation, the Knut and Alice Wallenberg Foundation, the Centre for Innovative Medicine (CIMED) and Karolinska Institutet. There are no reported conflicts of interest.</p>

<h2><strong>Publication</strong></h2>

<p><a href="https://www.jci.org/articles/view/169671">‚ÄùAntioxidants stimulate BACH1-dependent tumor angiogenesis‚Äù</a>, Ting Wang, Yongqiang Dong, Zhiqiang Huang, Guoqing Zhang, Ying Zhao, Haidong Yao, Jianjiang Hu, Elin T√ºksammel, Huan Cai, Ning Liang, Xiufeng Xu, Xijie Yang, Sarah Schmidt, Xi Qiao, Susanne Schlisio, Staffan Str√∂mblad, Hong Qian, Changtao Jiang, Eckardt Treuter, Martin O. Berg√∂,&nbsp;<em>Journal of Clinical Investigation</em>, online 31 August 2023, doi: 10.1172/JCI169671.</p>
      
      </div></div>]]></description>
        </item>
    </channel>
</rss>