<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 16 Dec 2023 16:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Andy Matuschak – Self-Teaching, Spaced Repetition, Why Books Don't Work (109 pts)]]></title>
            <link>https://www.dwarkeshpatel.com/p/andy-matuschak</link>
            <guid>38663733</guid>
            <pubDate>Sat, 16 Dec 2023 12:24:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dwarkeshpatel.com/p/andy-matuschak">https://www.dwarkeshpatel.com/p/andy-matuschak</a>, See on <a href="https://news.ycombinator.com/item?id=38663733">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>A few weeks ago, I sat beside Andy Matuschak to record how he reads a textbook.</p><p>Even though my own job is to learn things, I was shocked with how much more intense, painstaking, and effective his learning process was.</p><p>So I asked if we could record a conversation about how he learns and a bunch of other topics:</p><ul><li><p>How he identifies and interrogates his confusion (much harder than it seems, and requires an extremely effortful and slow pace)</p></li><li><p>Why memorization is essential to understanding and decision-making</p></li><li><p>How come some people (like Tyler Cowen) can integrate so much information without an explicit note taking or spaced repetition system.</p></li><li><p>How LLMs and video games will change education</p></li><li><p>How independent researchers and writers can make money</p></li><li><p>The balance of freedom and discipline in education</p></li><li><p>Why we produce fewer von Neumann-like prodigies nowadays</p></li><li><p>How multi-trillion dollar companies like Apple (where he was previously responsible for bedrock iOS features) manage to coordinate millions of different considerations (from the cost of different components to the needs of users, etc) into new products designed by 10s of 1000s of people.</p></li></ul><p><span>Watch on </span><a href="https://youtu.be/dmeRQN9z504" rel="">YouTube</a><span>. Listen on </span><a href="https://apple.co/44jwwCn" rel="">Apple Podcasts</a><span>, </span><a href="https://spoti.fi/3PTkCL2" rel="">Spotify</a><span>, or any other podcast platform. Read the full transcript </span><a href="https://www.dwarkeshpatel.com/andy-matuschak" rel="">here</a><span>. Follow </span><a href="https://twitter.com/dwarkesh_sp" rel="">me on Twitter</a><span> for updates on future episodes.</span></p><div id="youtube2-dmeRQN9z504" data-attrs="{&quot;videoId&quot;:&quot;dmeRQN9z504&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/dmeRQN9z504?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><span>To see Andy’s process in action, check out the </span><a href="https://youtu.be/OFuu4pesKf0" rel="">video</a><span> where we record him studying a quantum physics textbook, talking aloud about his thought process, and using his memory system prototype to internalize the material.</span></p><div id="youtube2-OFuu4pesKf0" data-attrs="{&quot;videoId&quot;:&quot;OFuu4pesKf0&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/OFuu4pesKf0?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><span>You can check out his </span><a href="https://andymatuschak.org/" rel="">website</a><span> and </span><a href="https://notes.andymatuschak.org/" rel="">personal notes</a><span>, and </span><a href="https://twitter.com/andy_matuschak" rel="">follow him on Twitter</a><span>.</span></p><p><span>Visit </span><a href="https://cometeer.com/lunar" rel="">cometeer.com/lunar</a><span> for $20 off your first order on the best coffee of your life!</span></p><p><span>If you want to sponsor an episode, contact me at </span><a href="mailto:dwarkesh.sanjay.patel@gmail.com" rel="">dwarkesh.sanjay.patel@gmail.com</a><span>.</span></p><p>(00:00:52) - Skillful reading</p><p>(00:02:30) - Do people care about understanding?</p><p>(00:06:52) - Structuring effective self-teaching</p><p>(00:16:37) - Memory and forgetting</p><p>(00:33:10) - Andy’s memory practice</p><p>(00:40:07) - Intellectual stamina</p><p>(00:44:27) - New media for learning (video, games, streaming)</p><p>(00:58:51) - Schools are designed for the median student</p><p>(01:05:12) - Is learning inherently miserable?</p><p>(01:11:57) - How Andy would structure his kids’ education</p><p>(01:30:00) - The usefulness of hypertext</p><p>(01:41:22) - How computer tools enable iteration</p><p>(01:50:44) - Monetizing public work</p><p>(02:08:36) - Spaced repetition</p><p>(02:10:16) - Andy’s personal website and notes</p><p>(02:12:44) - Working at Apple</p><p>(02:19:25) - Spaced repetition 2</p><p><strong>Dwarkesh Patel</strong><span> 00:00:52</span></p><p><span>Today I have the pleasure of speaking with </span><strong>Andy Matuschak</strong><span>, who is a researcher, engineer, and designer working on tools for thought. In addition to this podcast we did an interesting collaboration on Andy's YouTube channel which I encourage you all to check out, where I just watched Andy try to learn some new material.&nbsp;</span></p><p>It was just an intro chapter of quantum mechanics. Honestly I was expecting to see some cool techniques or be impressed but I was way more surprised than I expected to be by the deliberateness and the effortfulness of the practice.&nbsp;</p><p><span>It was 15 minutes a page in this textbook. And for every small thing that Andy thought, ”</span><em>I don't fully understand this, the author's trying to say something here, he's trying to draw an analogy or relationship, I'm not sure I totally comprehend the relationship between classical mechanics equation and the quantum mechanics equation, the author thinks is analogous,</em><span>” just really delving deep in that.&nbsp;</span></p><p>I thought that it was really interesting that this is a way to approach new material. So in this conversation I'm looking forward to talking with Andy about not only that experience, but a whole bunch of his other research and the other tools he's built.&nbsp;</p><p>Let me ask you this. That experience made me think that this is somebody who actually cares about understanding the material. Do you think people in general care about actually integrating and understanding the material they're consuming in books and textbooks? Don't you think they'd make more effort to actually assimilate that information if they cared to?</p><p><strong>Andy Matuschak</strong><span> 00:02:30</span></p><p>I think the statement is just a little too general to comment on. I think it's certainly the case that most students don't actually want to do this because they're learning stuff that they don't actually care about learning or even if they do care about learning it, often there isn't a clear connection between whatever reading or activity they're doing in the moment and the thing that originally inspired them for the subject and what they actually want to do. So there's always something tenuous going on. On the other hand, it's amazing to look at subreddits and to look at the level of nerdy and fascination that will be brought to bear on gardening equipment or knots, for instance. People are competing to tie some very obscure 18th century knot or whatever, and they're flipping through almanacs from the period. So when people are interested and it connects to something that's truly meaningful for them, they really do want to absorb and we see that in their behavior.&nbsp;</p><p><span>There is a second thing that I think is relevant. To explain this, I will reference </span><a href="https://www.amazon.com/How-Read-Book-Classic-Intelligent/dp/0671212095/" rel="">Mortimer Adler and Van Doren's How to Read a Book</a><span>, which is a great guide on serious reading. They consider the case of people who often have difficult or demanding books on their bedside table. So these are kind of aspirational, like, “</span><em>Oh, I wish I could read King Lear. I want to be the kind of person who reads King Lear.”</em><span> You put it on your bedside table and people will read it before bed. They'll find that they fall asleep while they're reading it, they're not really absorbing or understanding this book. It's not just an issue of memory, they simply are not apprehending the words on the page. The authors of How to Read a Book make the case that the issue with these people who are falling asleep reading King Lear is not that they don't want to stay awake and to really deal with that text, in many cases, it's that they actually don't know how. They butt their heads up against this very difficult wall of material. It's almost like a rock climber who's not very experienced going up against a wall that only has these really subtle notches. To an experienced rock climber, those subtle notches are like a ladder and they can get right in there and start making some progress and seeing what's up with this wall. But if you're an inexperienced rock climber, it just looks like a solid wall. The claim, maybe this is an optimistic claim, you can take me to task, is that there is such a thing as being a more skillful reader and being a more skillful reader will actually, in practice, in many cases, when the reading is aligned with your actual interests, produce a more serious, more understanding, forward kind of reading.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 00:05:18</span></p><p><span>Right. So there's two models of why people might fail to retain the material they're consuming. One is they got it at some point, but they forgot it. And the other is they never understood it in the first place and they just never noticed that they never understood it. What I found really interesting was you going paragraph by paragraph, sentence by sentence, and asking “</span><em>Have I got this?”</em><span> This was material that I had tried to go through the week before. And there were things when you dwelled on something, I'm like, “</span><em>Actually, I don't understand that either.”</em><span> and I didn't notice I didn't understand that. How are you able to notice your confusion while you are going through it?&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 00:05:56</span></p><p>This is a kind of habit. It's a skill that can be built. Adler and Van Doren suggest that the first and most important rule of skillful reading, active reading, is asking questions and trying to answer them. If you just dwell on that, what kinds of questions should I be asking and how should I go about asking them? How should I go about answering them when the author isn't present? And so on and so forth. [Unclear] They also say conversely, and this is meant as a criticism, an undemanding reader asks no questions and gets no answers. I certainly have read many, many books that way, particularly before I developed this habit and I often found myself falling into that second category. The issue was not that I failed to remember things, but rather that my eyes just skidded across paragraphs without even realizing.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:06:47</span></p><p>You're halfway through a chapter and you're thinking, what is the chapter about?&nbsp;</p><p>Ok, the broader question is — now that we have all these online resources, some of which you’ve helped develop like Khan Academy, it seems that the value of conscientiousness as a trait has dramatically increased. If you can motivate yourself to learn these things, the world is out there for you to absorb. What are the sort of design or UI or even content modifications that can be made to give you a conscientiousness boost? In the past you had a professor, you had peers, you had in-person deadlines to motivate you. Is there something equivalent to a pen and paper and how that boosts your mathematical IQ for conscientiousness?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 00:07:33</span></p><p><span>Right. One enduring result in education psychology is that when you're doing a lot of cognition, metacognition is difficult. What I mean by that is when you're thinking really hard about the stuff on the page, it's very difficult for you to plan, to regulate yourself, to figure out what the best next action to do is, to reflect and evaluate on whether you're understanding things. All that gets harder as the material gets harder and as it gets less familiar. So one common thread, at least in learning science stuff, has been outsourced metacognition. Some of the ways we outsource that are actually very familiar, they're things like somebody gives you a syllabus and tells you what to read when and you reference that. That </span><em>is</em><span> a user interface, that is a design practice. If you're a self-motivated student, one thing you can do and that I've done, is just go appropriate a syllabus from some graduate level course that corresponds to the text that you're reading as that might be a good guide to what's most important and how to approach this.</span></p><p><span>There are also lots of things that one can build directly into the interfaces. Just as one example, in </span><a href="https://quantum.country/" rel="">Quantum Country</a><span>, which was a textbook that </span><a href="https://michaelnielsen.org/" rel="">Michael Nielsen</a><span> and I developed to explore some ideas around augmented reading experiences, we embedded a bunch of review questions every 1500 words or so in this text on quantum computation. Our primary intention in doing this was to help people remember what they read. We had this theory that part of what makes it hard to learn a complex subject is that there's all these new definitions and notation and terms and things being thrown at you at once and you're being asked to combine these things, which are still unfamiliar. And so you're constantly having to retrieve these elements and struggling to do it, either it's taking a while or your success rate is low.&nbsp;</span></p><p><span>That was our motivation but it had this other metacognitive benefit that was really important. When you’re asked these questions after reading 1500 words it is an opportunity for you to notice that you did not in fact absorb what was in that thing. Not that you don't remember but that there's a word in the question that is apparently important that you simply didn't even notice. And so not only does that give you feedback, it tells you that maybe you need to reread that specific section, but it may also change your behavior towards future sections. In interviews, readers told us that after they reached the first set of questions or a particularly difficult set of questions, they found themselves slowing down and reading more attentively or actually realizing that their reading practices were ineffective in general. In the way that you were mentioning towards the start of the conversation. There's been a bunch of research on adjunct questions, questions that go along with a text, and they have all kinds of effects. The adjunct questions have the kind of effects on forward material I was just describing and they also have the effect of making you reflect on what you've just learned. And in addition to the questions being asked, you might find yourself pondering, “</span><em>Well, I'm being asked about this. But why does this matter?</em><span>”</span></p><p><strong>Dwarkesh Patel</strong><span> 00:10:58</span></p><p><span>Yeah, on the point of adopting a syllabus from somebody else. One problem you might have as a self learner is you have some goal, a reason for learning, and then you start thinking, “</span><em>Well, do I really need this chapter? Do I really need this content?”</em><span> At this point, you're doing the metacognition that you were trying to use a syllabus to avoid.</span></p><p><strong>Andy Matuschak</strong><span> 00:11:16</span></p><p>Yeah.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:11:20</span></p><p><span>If you are trying to self learn and there is a resource that is a close approximation of the syllabus you want. Should you just think “</span><em>Hey, I don't know why I need this chapter. I'm just going to go through it.</em><span>” or should you use your own judgment there?&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 00:11:33</span></p><p>This is a pretty classic issue for learning in general. You have this problem where to bootstrap yourself in a domain you have to outsource the question of what is necessary to know. You might know, for instance, that you really want to build a model that can generate images given descriptions, like Midjourney, but you don't even know what you need to study to do that. So you pick up some textbooks on machine learning. You're outsourcing the answer to this question to the author. What is necessary to know to build things? Maybe you can find a book that's actually labeled “What you need to know to make an image generating model” But even then, you're outsourcing the answer to the author.&nbsp;</p><p><span>You can take that answer as a start and treat it as tentative and revise it iteratively. And as you become more skilled you can lean less on it. And you probably should. I think a very common mistake that people make is to feel that they need to do the thing the right way and that is exhaustive and completionist. If they fail because they find themselves bored or unmotivated because the material doesn't actually seem to relate to what they want to know, but they're just going on faith that, “</span><em>Well, if I follow what the author says, everything will be good.”</em><span> Anyway, they find themselves having trouble for that reason, and then they just stop. This is bad. They would be better off just skipping around according to their interest and continuing.&nbsp;</span></p><p>One other thing I'll say about this is that the role that these syllabi play is as a scaffold. This is a term of art from learning science, but it relates to the thing we're familiar with. If you want to get higher up a building, you may not be able to climb it yourself, but you can build some scaffolding around it and then suddenly you can reach that top shelf or the top of that building. The scaffolding is ubiquitous in education. We give you simpler versions of questions first, that's a kind of scaffolding. We partially work the answer first, that's a kind of scaffolding. We give you worked examples first, where we might ask you to predict the next step of the work example. That's also a kind of scaffolding. Where the metaphor breaks down is that once you become more capable, we try to remove the scaffolding. It's called fading. The idea is that once you have solved a lot of calculus problems, you don't need half of it worked out and you're just filling in one of the blanks anymore. And in fact, doing that would not be as effective a learning experience.&nbsp;</p><p><span>If I'm studying something in computer science, which is a domain that I know really well, I don't need those syllabi, not in the same way for most subjects, and I think that's mostly just because the amount of cognitive demand that's placed on me by the subject is just much lower than it is for other subjects. So much of it is familiar already that I can deploy my own planning more effectively as I go. But it's also the case that because I know so many things about the subject, I can do a better job from the get go of making a plan. Because making a plan requires modeling a path or predicting a path or saying, “</span><em>Well, I guess I'd need to see how this connects to that or something like this.”</em><span> And if your destination and your starting point are very far away, then you can't necessarily see all the things in between or how to draw those lines. But if those things are only a couple hops away, you can maybe infer pretty accurately.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 00:15:22</span></p><p><span>Right. I guess this maybe implies that if you do want to learn about a subject, it might just be helpful to just do an </span><em>Intro to X subject</em><span> course or textbook, not necessarily because it is instrumentally valuable to whatever problem you're interested in but because it'll give you the context by which to proceed on, the actual learning.&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 00:15:45</span></p><p>That's true. It's also the case that you don't even know all the stuff there is. This is another key problem and this is another reason why we outsource stuff. There's a fundamental tension in unschooling, for instance. Just let the kids pursue what they're interested in. That's cool. There's a lot of good things about that. But say that a kid's true passion turns out to be ocean geology or something and they're in a landlocked country and there's just no one around them that talks about ocean geology, then they're missing out on some great opportunity. But if the school had a program where they are bringing in guest speakers and then there's a special lecture on ocean geology from this person and it lights up the kid's world, even if they wouldn't have chosen that lecture, that's a good thing.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:16:33</span></p><p>Yeah. Unschooling is actually an interesting subject to talk to you about.&nbsp;</p><p><span>But before that, I want to ask you about this excerpt from a Paul Graham blog post titled </span><a href="http://www.paulgraham.com/know.html" rel="">How You Know</a><span> and it says, “</span><em>Reading and experience train your model of the world. And even if you forget the experience or what you read, its effect on your model of the world persists. Your mind is like a compiled program you've lost the source of. It works, but you don't know why.”</em><span> So it's a compiled program, you don't need the source code. Is it okay that we're forgetting so much of what we're reading?</span></p><p><strong>Andy Matuschak</strong><span> 00:17:05</span></p><p>What he's saying is true, to some extent, whether or not that extent is sufficient is going to depend a great deal on the situation and on what you need. If your aspiration actually depends on having a deep, detailed understanding of the material, then the imprint on your worldview or on your automatic responses made by the book may not be sufficient. On the other hand, if what you want is to absorb a lot of different ways of looking at the world, knowing the details of these isn't necessarily important. Maybe you just want to know that Confucius emphasizes community and society as a moral patient in contrast to the individualism of a bunch of humanist philosophers. And if that's kind of the level that you feel like you need to make decisions in that domain then I think that's fine.&nbsp;</p><p><span>Very practically speaking, it's funny that he uses the word compile, because one of the prominent theories of cognition, that is how we come to know and learn things, is this theory called </span><a href="https://en.wikipedia.org/wiki/ACT-R" rel="">ACT-R</a><span> by John Anderson. A key part of it is this process that he calls knowledge compilation. This is the process by which we take individual facts and turn them into higher level patterns that we can generalize and apply in more contexts. And I think that's what Paul is gesturing at. By reading a book which contains a story or a case study you learn to generalize to some extent and you apply it in other contexts when it seems relevant.&nbsp;</span></p><p>The reason why I bring up Anderson's theory is just that he has a bunch of specific claims about what's necessary for knowledge compilation to happen and what you'll be able to do as a consequence of certain degrees of knowledge compilation. I think he'd probably respond to this by saying that — actually, in order to effectively compile things that you've learned into schemas that will match feature scenarios effectively, you need to be exposed repeatedly to those things, you need to use them, you need to do a variety of things that will basically show your brain that is relevant to apply these things in combination. And simply reading probably won't do that. But if you read and you have a lot of conversations and you're in a context where it's demanding and it's drawing on what you read, then you may naturally do that kind of compilation step.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:19:41</span></p><p><span>I've actually been thinking about this in preparation of talking with you. I've had the pleasure of talking to a lot of interesting people across a lot of different fields. When I look back on some of my old conversations, I notice that I actually had a lot more context at the time I interviewed them and had done all the prep than I can remember now. Sometimes I'll listen back to a conversation and I won't even remember the content in the conversation. And I remember thinking after the conversation, I knew so much more about this field than was compressed into this one hour interview, right? I had to prep other things that might come up. And afterwards I'm like, “</span><em>I don't even remember the things that were in this one hour.”</em><span> But then the other part of me thinks, “</span><em>Well, I'm getting better at doing the podcast”</em><span>, that might imply that I've picked up something. But it is a shame that I didn't have some sort of rigorous practice throughout the time of retaining the material that I was keeping.</span></p><p><strong>Andy Matuschak</strong><span> 00:20:31</span></p><p>Well, yeah, I expect the main [unclear] in which you're getting better, is actually not really about any of the details of those materials. I think it's about your practices as an interviewer, the way that you generate questions, you probably have a bunch of patterns, whether you know it or not. You read a thing that a person has written in hopes of generating good questions about it. And even though you maybe don't have this habit for textbooks yet, of constantly demanding things of the textbook, you have started to develop this for essays or blog posts that interesting people you're interviewing have read. And to point to this Anderson theory, in the course of repeatedly doing that, you've made parts of it automatic, so that you don't need to do it consciously, you can focus more on the material, you can probably take on more difficult material, or actually understand material at a higher level than you could have before, because less of yourself is engaged in this question of how do I make the questions from the material?</p><p><strong>Dwarkesh Patel</strong><span> 00:21:36</span></p><p>Yeah, I certainly hope so. Otherwise, there's a question to be asked of what I've been doing all these years.&nbsp;</p><p>Having interviewed some of these people who are infovores and have consumed and continuously consume a lot of content, they don’t have a note-taking practice.</p><p>This is something you also noticed and pointed out in your notes. Tyler Cowen, for example, I don't think he has any sort of note-taking practice. He just devours information. What is your theory of how these people are integrating things that they're reading?</p><p><strong>Andy Matuschak</strong><span> 00:22:07</span></p><p><span>Tyler's a good example. I think he's actually a little easier than some others we might discuss. So, let's talk about Tyler for a second. One of the other things that's so interesting about Tyler is his writing obligations. This is a man who's blogged every day since 2007 or something and has a weekly Bloomberg column, something like 1500 words, and also has published something like a book a year for a decade or more, and occasionally publishes some academic articles, plus like a bunch of other collateral. That </span><em>is</em><span> notes. And I think it's also important to note that like the way that Tyler writes these blog posts and the way that Tyler does these columns and even the books is very different from the way that many other book authors work. Tyler’s blog posts often have this a real first draft mentality to them. He's just thinking out loud and he's got decades of practice thinking out loud and like writing down a decent take the first time. And so he gets something pretty good, the first time, much of the time. And that works for him. So that is a note, right? Your initial thoughts on the subject is what you would write in a note.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 00:23:24</span></p><p><span>Yeah, one of my former guests, </span><a href="https://www.dwarkeshpatel.com/p/scott-young" rel="">Scott Young</a><span>, was comparing Bryan Kaplan’s books and Tyler Cowen's books and he said, when you read a Brian Kaplan book it's like a chess game. If you try to move a pawn up on this case for education, I've got this rook that I can move here. With Tyler, it's more like he’s shooting the shit on a subject.&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 00:23:43</span></p><p><a href="https://twitter.com/cauchyfriend/status/1641577765060349954?s=20" rel="">Bangladeshi train stations</a></p><p><strong>Dwarkesh Patel</strong><span> 00:23:44</span></p><p>Yeah, right, right. On a separate question, do LLMs make memorization more or less valuable? There's a case you can make for both. But on net, is it more important to have more Anki cards in your deck now that GPT-4 is out?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 00:23:58</span></p><p><span>Maybe this is a good time to talk about what memorization is or what it's for. We could use that word to refer to the practice of learning more trivia. For instance, a thing that I and some people I know have done is, we’ve gone through a book called </span><a href="http://book.bionumbers.org/" rel="">Cell Biology by the Numbers</a><span>, which says all of these things like, how big exactly is a nucleotide? Like how much volume does it take up? It's kind of helpful occasionally to know that it's about a nanoliter. And that can help you model things. So you can just commit all of those things to memory, right? That's one kind of memorization. And we could talk about how LLMs affect that. But I just want to make the case that so much of what you do and experience day to day is memory bound, or is memory influenced in important ways. For instance, your ability to understand a difficult argument, even in the course of a text, is memory bound. Some of that's working memory. But your ability to understand an argument that has many steps in it, more steps than you can keep in your working memory, depends on your ability to think of some of those steps in terms of some stuff that you already know, so that you can kind of reduce it or abstract it.&nbsp;</span></p><p><span>Likewise in creative work, there's a bunch of studies trying to catalog case studies of how it is that people have flashes of insight. It's a little hard to talk about that but one of the things that's a pretty consistent source of insight for people is noticing a surprising connection or a surprising contradiction. It probably feels pretty familiar, right? You're reading through the newspaper and you see that people have finally figured out how to do X and you're like, “</span><em>Wait a minute, that means if I combine it with this other thing, like we'd be able to do Y!”</em><span> or something like that. Now that's only possible if the other thing is in your memory. If you have to think to look up the other thing, the newspaper wouldn't seem so salient to you.&nbsp;</span></p><p><span>Early on in my time in Khan Academy I learned a whole lot of details about the education market in a very thorough way using memory systems. This let me be in high level executive kinds of conversations where we're trying to figure out strategy stuff and somebody would propose a particular direction and I could say things like, “</span><em>Well the total budget spending for instructional materials is this and that market is growing by this percent per year and 10% of students in the US are in this place”</em><span> and so on and so forth. Basically I could evaluate ideas on the fly in a way that others couldn’t. Anyway, this and other things are just part of my rant about how people in general under-appreciate the role that memory has in our lives.&nbsp;</span></p><p>So just to come back to the question, explicit memorization or explicit making sure that you can recall the thing reliably. We can test it against these things. So for the case of the creative instinct, for instance, noticing the contradiction, noticing the connection, I imagine that we will have future notebooks that will do some of this noticing with us and that will decrease our need to be able to rely on our own sense of salience or something like that. But I guess I don't know how much. My own experience coming up with weird ideas that feel very new is that it feels very personal, it feels very [unclear]. I often haven't been able to describe, textually, the constituents of the thing very clearly. There's just kind of a feeling that something in this general direction is connected with something in that general direction, or there's a tension. That makes me a little hesitant. LLMs depend on our ability to externalize things and to make them legible. Back to the learning point about the role of memory. If what you're trying to do is to understand something pretty difficult, your ability to understand that thing is still absolutely going to be bound on your memory of the constituent material.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:28:11</span></p><p>Do you think there's pedagogical value in forgetting? Some anecdotal or unrelated evidence is in neural networks where sometimes you can improve performance by pruning some of the weights. Obviously, we forget things and we don't remember everything. When we sleep, we lose a lot of our memories. Is it possible that by not getting the details and only getting the gist, that actually helps us better generalize the insights we're getting from text and things like that? What do you think of that way of thinking?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 00:28:42</span></p><p>Yeah, it could be. Memory is very connected to attention. And we can't attend everything. So one of the roles of memory is to help guide us to the things that are important. Maybe I happen to know that the magnitude and energy of an electron volt, that's something I can draw on because of the memory system stuff, but I also don't want that to be front and center in my mind all the time. I don’t want it to be hyper salient the way that I want some very important design principles to be. So yeah, there's some role there. There's also some theories that the reason we have forgetting is that our environment or ancestral environment was very traumatic. So we would like our episodic memory in particular to maybe not be all that faithful. I actually don't know the status of those theories.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:29:30</span></p><p>Probably why you forget dreams as well, right? Dreams are pretty traumatic. If you thought of them as the same as a real life experience.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 00:29:37</span></p><p>Yeah. Another weird thing about memory is that as far as we can determine, memories aren't lost exactly, at least not completely. There's a series of interesting experiments that people have used to demonstrate that decades later, things are still there. If you can cue them right, people can bring things back, even things that they feel are lost. And of course, you can also cue people in ways that are hallucinatory so you need to be careful about that. I guess the reason why I bring that up is that it flies in the face of this view that there's a limit.&nbsp;</p><p><span>One of the things that I think is kind of weird about this memory system stuff, or like memory champions, or something like that is “</span><em>Oh, if you do these things, will you start to forget other normal human stuff?”</em><span> And what's weird is, no. I've been doing this memory system stuff for years and I just know more stuff now. This is aligned with the experimental literature, which seems to suggest that, there's probably upper bounds but we're not close to them. Some of these memory champions have memorized maybe two orders of magnitude more things than I have practiced. Certainly people who are multi-lingual have really, really absurd numbers of things memorized. So there isn't a resource management argument.</span></p><p><strong>Dwarkesh Patel</strong><span> 00:30:59</span></p><p>If there isn't, why do we forget so many things? Is there some reason the brain just forgets some of the things we’re coming across? Maybe we were training the ancestral environment to find certain things salient that don’t just map onto books?</p><p><strong>Andy Matuschak</strong><span> 00:31:16</span></p><p>It’s a good question. We're getting to a part of the cognitive science space that I'm less familiar with and also that I suspect we simply know less about. But let me just riff a little bit. One of the things that we sort of know is this idea of spreading activation. When you go to try to look something up or when you try to deal with a particular situation, there's something almost kind of like DNS exchanges or like routing on a network or something where we start from some point that is like a stimulus, and speaking very informally, we kind of expand outwards from there and there are effectively like weights on those connections. By tuning those weights effectively, we route the packets on the network effectively. Memory is encoded in these weights, at least partially. So if you didn't forget things, then you might just have this weird cacophony on the network and in particular, what's salient? What to do next? Which response seems most appropriate to this question? You might answer those kinds of things very effectively, because all this stuff is coming up for you, that is much less relevant. One of the theories about how well we remember stuff in what circumstances is actually called predictive utility theory. And it suggests that the probability of retrieval of a particular item in a given situation actually does correspond with basically a model of to what extent the brain predicts it will be useful.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:32:48</span></p><p>Right. And then the prediction but doesn't necessarily map on to…</p><p><strong>Andy Matuschak</strong><span> 00:32:54</span></p><p>Doesn’t necessarily, exactly. So when you repeatedly access something, when you practice retrieving it, the prediction of the utility of the thing goes up. And when you do it in a variety of situations, it goes up across a broader distribution.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:33:10</span></p><p>Okay, so this is interesting. When did you start your memory practice? Presumably it was after after Apple?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 00:33:14</span></p><p>Yeah.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:33:15</span></p><p>Okay. Let me ask you this. At Apple, you were in charge of a bunch of important flagship features on iOS and I'm guessing other things. Presumably you didn't have some sort of practice but since you were encountering these things day to day, that natural frequency and way in which problems came up, did you have a worse understanding of those problems then compared to now, knowing what you do and having the practices you do, you're able to comprehend now? I don't know if that question made sense.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 00:33:45</span></p><p>No, that's a great question. Here's a fun thing. I was much better at what I was doing then than I am at what I'm doing now. That's pretty funny. It was just totally different. Let's talk about this a little bit. This feels very, very juicy for me. Most of what I was doing was engineering. Some of it was very difficult engineering, but mostly engineering, and mostly on things that were fairly well understood. I wasn't trying to decide what should be done, sometimes I was from a technical perspective, but certainly rarely from a product perspective. It was rarely a relevant question for me. I was a somewhat design minded engineer and I did a bunch of engineering and design-ish things on tasks which were set out for me. By the time I joined Apple I had been programming for a really long time, 13 years maybe, and programming in Apple's ecosystem for probably two to three thirds of that time. So everything was just really familiar. It was mostly flow all the time, every day. I was just in it. I knew the stuff that I needed to know. I was very well practiced. And the space didn't change that much. Most engineers at Apple most of the time are not pushing the frontier of what is known, like trying to discover. They're doing very difficult technical work, mostly applying things that they already know and understand quite well to problems which are usually not always pretty well understood. Memory was essential to me doing that job well, but I had already built most of it by the time I got there. I'd already built just tons of stuff for Apple's platform. I had to learn a lot of stuff. I learned a ton of stuff about the internals of those systems. But because I already had such a rich understanding, both of Apple's platforms and of computer science and engineering in general, I had this really rich network for stuff to slot into. Learning stuff is easier when you have other stuff to connect to. It's a nice principle. Metacognitive load on me was lighter because others were figuring out what we should be doing. Just like by contrast, now I'm doing research, I'm trying to discover things that are not known. I'm trying to make things that didn't exist. The hard questions that I answer are mostly, what should be done or what should I do? And that question is not just a technical one of how I should implement this feature that needs to get built, but what intervention on a reader should be taken? That requires synthesizing lots of different unfamiliar literature.</p><p><strong>Dwarkesh Patel</strong><span> 00:36:30</span></p><p><span>There's two different threads I want to go on. Maybe I'll just mention the other one. This is also related to the thing we're talking about a few minutes ago with LLMs. </span><a href="https://en.wikipedia.org/wiki/Literature-based_discovery" rel="">Swanson-Linking</a><span>. Swanson was just somebody who read the medical literature and he was just familiar with a lot of esoteric results. Different things would come up and he would be able to figure out what different things are connected. For example, he noticed in one case that headaches are linked to some other symptom and that other symptom is linked to magnesium deficiency. Apparently a whole bunch of people's headaches were solved once they were given magnesium supplements and he noticed that connection. Again, this is the kind of sort of combinatorial thing that you wouldn't notice otherwise.&nbsp;</span></p><p>But on this subject itself, there's this natural way in which you're able to get up to speed in all the things that are happening at Apple. Is it possible and maybe advantageous to do similar kinds of things in other fields? For example, instead of doing an explicit space repetition system when you're trying to absorb material from books, you just read a cluster of books and hopefully things would just come up that are relevant to get in again. Or is there a value in having explicit practice of setting up cards and so on?</p><p><strong>Andy Matuschak</strong><span> 00:37:50</span></p><p><span>Yeah, again the answer is going to be it depends. Maybe the most familiar example of what you're talking about is immersion learning a new language. Immersion learning is like a great thing and it's going to be more interesting and more effective than doing space repetition practice. It's going to be integrative. It's going to be socially based. So there's a bunch of stuff about social learning that's relevant. A problem though is that say you decide you want to learn Swahili today and you go down to the local Swahili community center and you're like, “</span><em>Cool, I'm going to immerse myself”</em><span> Good luck. You can't even get started. So through this lens, explicit practice is a way to bootstrap yourself. All of the best pianists at sight reading that I knew in university played with churches. They were so good at sight reading because they had to show up every Sunday and they're playing a different thing. New hymn every Sunday. So this is immersion also. Over time, they're learning all these cadences and these things that are really common and whatever. But you can't show up and be the church pianist every Sunday in the first place if you don't already have some decent foundation. This is a bootstrapping argument. One role for explicit practice of this kind is to get yourself into a position where you can more naturalistically reinforce. But there are still going to be instances where naturalistic reinforcement isn't going to work. For example, the linking that you brought up, one issue for doctors is rare diagnosis. So if it's only going to be once every couple of years that you see a patient that's going to present with these symptoms, that's not going to be frequent enough to naturally reinforce your memory of that. You're going to need some out of band mechanism. And unfortunately, I think for many kinds of creative leaps and creative insights, that may be closer to the regime that we’re in.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 00:39:50</span></p><p>Yeah, that makes a lot of sense. Where in many fields, the things you're regularly doing is the thing you need to reinforce. It makes a lot of sense that if you're a researcher, the long tail of events that might come up is a thing, it might happen once every few months but the regularity is not a thing that matters, right? It's [unclear] on your work.&nbsp;</p><p><span>Here's a question I actually have. When we were doing the quantum mechanics textbook, it was like three hours and afterwards, I was just exhausted. I was actually surprised that you went the entire three hours without interruption. Afterwards, I was packing up and you're like, “</span><em>Hey, I'm about to actually go to my piano lesson.”</em><span> I was so confused at how you had the stamina to keep going. Is the stamina just inherent in you? Or is that something you did to develop?&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 00:40:39</span></p><p>One of the things that I think is funny about stamina is first off, there's some kind of weird grass is always greener kind of situation where, I often feel struck by other people's stamina and feel like I have very little of it. I struggle with energy. I've actually written extensively about all my struggles with energy and ways of managing energy. I spent a lot of time thinking about it, managing the energy levels and structuring my day around it. So I think there is something where one often feels maybe lower stamina than one actually is because one misapprehends other's stamina. Okay, in that particular situation, how do I explain why three hours of studying, etc. First off, social. So if I were alone and studying that book for three hours, and I weren't effectively trying to perform for you Dwarkesh, it wouldn't have been nearly as energizing for me. And I definitely would have taken breaks. I still would have been able to go for three hours, I think. Part of the reason for that is that it's simply way less hard than things I normally do.&nbsp;</p><p><span>In some sense, learning quantum mechanics should be much harder and it kind of is cognitively demanding in a lot of ways. It's much more cognitively demanding in kind of a direct way than what I actually do day to day. But it's much less demanding on what William James calls the </span><a href="https://www.jstor.org/stable/2177575" rel="">Energies of Men</a><span>, which is something like a life force that permits you to act according to your will or something like that. Maybe it's gumption, maybe it's willpower, maybe some people call it [unclear], these aren't all the same thing exactly. But sitting and staring at a page and deciding what you should do next on a research project is incredibly draining on that resource. The sitting and not knowing is the hardest thing that I do in my work. It's a wonderful vacation to be presented with, “</span><em>Oh great, somebody else is going to tell me what to do. This is great.”</em><span>&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 00:42:40</span></p><p><span>So although it might be less demanding than our usual work, it is definitely more demanding than the way in which I or most people approach textbooks or other material in the sense that, I would just read through and then once I get to the exercise, I'm like, “</span><em>let's see what I didn't understand.</em><span>” Whereas just the attention and the intensity to go through sentence by sentence and constantly being paying attention seems to be way more exhausting.</span></p><p><strong>Andy Matuschak</strong><span> 00:43:07</span></p><p><span>Yeah, I mean, so this is sort of true. It's definitely the case that I will occasionally do some of this before bed reading, where I think “</span><em>Oh, let me just do a little bit more.</em><span>” and it's basically useless. But I want to make the case that there is a kind of pocket that you can fall into. Maybe you call it flow where the demandingness that you're bringing to bear is matched to your ability, the book is not overwhelming, you feel like you can make your way through it, and this is actually more engaging. I occasionally will find myself reading as an undemanding reader and finding my attention kind of slipping because I'm just not that attached to the text emotionally, I'm kind of reading dutifully, I'm like trying to get through it. That sometimes produces an adversarial aspect where the text is in my way or it's kind of something to be accomplished. And often I will find that I need to bring more gumption to bear to power through and make myself sit there and keep flipping the pages than I need if I actually just open my curiosity and attention and really start engaging the book.</span></p><p><strong>Dwarkesh Patel</strong><span> 00:44:27</span></p><p>There are ideas that people have come up with for different pedagogical tools, which are mediums that give closer connection to the reader. One is, you have some sort of fiction account, where a concept is introduced and reinforced, or you have a video game with characters you care about. As far as I know, there isn't something that has really taken off using these sorts of new mediums. Why do you think that is? Is it just an inherent limitation of everything but text and lectures or people just haven't given it the right content and design?</p><p><strong>Andy Matuschak</strong><span> 00:45:00</span></p><p><span>Yeah, I'm fascinated by this question. Let's see, I can say a few things about it. One is that I would argue that one medium has taken off in an absolutely enormous way and that’s video. People love video. People will watch Grant Sanderson spend an hour going through some explanation of an esoteric math problem, people who would never crack a Springer graduate textbook in mathematics or something like that. The issue is that they will not walk away from that interaction with much understanding </span><em>but</em><span> they're much more engaged. So that's cool. That's suggestive and it suggests the question, is there a version of that which actually produces detailed understanding? Maybe one approach to producing that might be like a game. My favorite example of this is </span><a href="https://store.steampowered.com/app/210970/The_Witness/?snr=1_1056_4__curatorfeatureddiscount&amp;curator_clanid=33228275" rel="">The Witness by Jonathan Blow</a><span>. Have you played The Witness?&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 00:45:52</span></p><p>No.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 00:45:54</span></p><p><span>I think The Witness is an absolutely extraordinary work of art. It's a game that has no text, at least no text that's relevant to the game elements. In kind of classic </span><a href="https://en.wikipedia.org/wiki/Myst_(series)" rel="">Myst</a><span> style, you wake up on an island, and figure out what's going on. And the game proceeds to explain to you, without using words, but just by shaping your environment, a series of extremely complex mechanics of a system that exists in this world. You learn a bunch of stuff and it gets to the point where it feels like you're in conversation with the game's designers. It's like, “</span><em>Ah, he's asking me to do this here.</em><span>” No one's asking you, right? There's no text, but you can feel that you are being asked. You perform some interaction in the environment and you feel that you have answered the game's response in kind. This is very, very interesting. It's like a medium of action. Some people have tried to make educational games, games that are explicitly about arithmetic or something, Jonathan Blow's game is not about that. It's the mechanics that you learn are they're about the environment. I don't think anybody has yet really succeeded in doing this about explicit subjects. There are, for instance, things like </span><a href="https://store.steampowered.com/app/220200/Kerbal_Space_Program/" rel="">Kerbal Space Program</a><span>. Maybe people learn some things about project management or orbital mechanics from that. </span><a href="https://www.zachtronics.com/" rel="">Zachtronics</a><span> has a bunch of games that are sort of about assembly language, roughly speaking. Maybe you can learn some things about that. The issue seems to be that games are ultimately in aesthetic form.&nbsp;</span></p><p><span>The purpose of the game is to have an experience that feels a particular way. And so they're sort of serving a different purpose than Grant's videos or a text. Grant's videos are also serving a different purpose from the text. The text you might pick up because you're like, “</span><em>I want to be able to build a robot.</em><span>” So you pick up a textbook on robotics or something. And so is there something that you can pick up that's sort of like a game in so far as it's an active environment that you use in a similar situation to “</span><em>I want to learn to build a robot?</em><span>” Maybe kind of? We don't quite have those yet. We have some things that are kind of like that. I don't know if you've seen </span><a href="https://www.nand2tetris.org/" rel="">From Nand to Tetris</a><span>. This is a very interesting project that's kind of along these lines. And what characterizes it, like games, is doing. It's active. So when I was asking all those questions of the book, that was active learning, active reading, Nand to Tetris is naturally active. So this is a course in which you kind of start with basically nothing. You start with memory and you build a virtual computer and build Tetris. You build a processor and stuff. The whole thing's active. The whole time you're making the computer grow. This is doing a similar job to the question asking that I was doing, except that you don't have to regulate all of that yourself. The regulation, the choice of what activity to do, is in the course, is in the structure of the material. I think some kind of mass medium that is like that is waiting to be created, but that can be applied in many, many circumstances. We have the non-mass medium version of it already and it's apprenticeship. If you want to be a good yoga teacher, you go hang out in yoga studios. If you want to be a good surfer, you go to the beach when the other surfers are there and you participate peripherally and you talk to them and you learn about their tactics. They might give you some feedback eventually and you'll start to participate less and less peripherally over time and eventually you'll be part of the community. This isn’t a mass medium. We can't print billion copies of it like we can with a book.</span></p><p><strong>Dwarkesh Patel</strong><span> 00:49:46</span></p><p><span>What is the experience of watching </span><a href="https://geohot.com/" rel="">George Hotz</a><span> on the stream code up tiny grad? How does that compare to just being in an office with him? Because even if you're in an office with him, there would be constraints on his time and how much engagement there would be. Why isn’t video a scalable way to increase apprenticeship?&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 00:50:07</span></p><p>I'm actually incredibly excited about streaming as a medium for this. We're gesturing at a particular kind of learning that needs to happen. It's often called tacit knowledge. One of the things that you have to learn to do as an engineer is to learn to deal with 100,000 different weird situations where something is not behaving the right way. Eventually you learn pattern recognition, you learn ways of dealing with this. Much of this is not described in any book. It's not explicitly taught. You just learn it by doing it over a long period of time. By watching George do it, I think that people do absorb stuff. They can absorb some of that knowledge. That's part of how apprentices absorb that knowledge. There's a few things that are missing. You're not getting feedback. There's a whole lot of chaff there. There's a whole lot of stuff that probably isn't all that meaningful. It's also true for apprentices. I'm pretty excited about streaming videos. I've complained loudly that there aren't more designer streamers.&nbsp;</p><p>One of the things that I think is really interesting is that we have some disciplines like programming where there are a million books on courses about how to learn to program. They don't give you everything you need. There's this tacit knowledge stuff that you need to develop. If you work through these courses, if you go through the MIT OpenCourseWare for computer science, you'll be able to build some stuff and you'll be able to lift yourself up. This is not true in all domains. In particular, design, but lots of other domains that are like that, like musical composition, architecture, something like this. Nope. It's normally done in studio classes. Lots and lots of hands-on feedback. The feedback is highly contingent. It's highly contextual. We just haven't figured out how to communicate this. It's good to see lots of programmer streamers, but I really want to see the streamers in these other domains.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:52:10</span></p><p>On the point about more programming books. Ironically, the reason why there's some more resources on programming is that it's just so legible, but it already makes it easier to understand in the first place. You just have this reinforcement. Nand to Tetris is like a video game analog to learning, maybe not just programming, but how things in the internals of a computer work but programming has an element where it already feels like a video game. I have a friend who has a sort of intense sort of manic energy, and he used to be addicted to video games when he was a teenager, and now he just stays up all night in these coding binges. It's just the same part of the brain. Are you optimistic about things like video games and fiction being able to work and feel as though they're not already kind of like a video game, like programming?</p><p><strong>Andy Matuschak</strong><span> 00:52:56</span></p><p><span>I think what makes programming feel like a video game is this sense of instantaneousness, this sense of direct contact with the environment. You're learning about a conceptual world, but that world is right underneath your hands, and as you manipulate it, you're constantly getting this feedback, the red squiggly lines, you’re pressing command R regularly, and you're seeing it fail, and that feels great. There's this feeling that's very common for programmers, and it's laden with doom. The feeling is it's like 9 p.m., and you've been working on a thing all day, and it's almost working. It's almost working. And you know, if you just debug this one thing, then your project will be done, and you'll be able to go to, so you're like, “</span><em>Well, I'll just stay up and I'll debug this one last thing.</em><span>” And then you start debugging it, and you get it, and you solve it, and that feels great then immediately you run into one more thing, like, “</span><em>Oh, it's almost running all the way through, it's almost going end to end,</em><span>” and you're like, “</span><em>Well, I'll just stay up a little bit longer.</em><span>” Before you know it, it's 2 a.m. You keep going because it feels so good. You feel the sense of forward progress. You're not just staring at a wall. For the programming problems where you are at a brick wall, it doesn't feel like this. It feels bad. Can every field be transformed into something where you can feel the sense of forward progress? You can get this rapid feedback cycle. I think that's really hard. It's not clear to me that some fields can be transformed in that way. I think we can get a lot closer in most cases than we're at right now.&nbsp;</span></p><p><span>What's hard about designing a user interface is that often there's this feeling of exploring a combinatorial search space. Programming often feels like a search problem too. You have a sense that there's some right way to solve the problem. There might be some set of right ways to solve the problem, and you're looking for it. And you have some heuristics that guide you to, like, “</span><em>Oh, this might be a dynamic programming problem!</em><span>”, or this might be something that is solved well by separating concerns or something like that. Design often feels less like that. You have those heuristics, too. You have those patterns, too. Often it just feels like, “</span><em>Nope, I just need to try 300 things.</em><span>” The core action of Figma is to duplicate. You have an artboard, you tried something, and that didn't work so you select it, and you press Command D. And what you end up with, and when you look at Design Twitter, it's just all these screenshots of people's Figmas with a million artboards. They're just trying stuff. And you don't have this feeling, or at least I don't, and I think many designers don't have this feeling of progress. You're just kind of exploring parts of the search space, and you're learning that parts of the search space don't work. And eventually you stumble on one that does, but you don't have this feeling of getting closer often. Often there will be like weeks that go by without feeling like you're getting closer, because what you're doing is just kind of like narrowing the search space.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 00:56:22</span></p><p>Interesting. Although there are people who are obsessed with Design. What is the sort of loop that keeps them obsessed with a process that doesn't feel intrinsically forward feeding?</p><p><strong>Andy Matuschak</strong><span> 00:56:34</span></p><p><span>So to some extent, I think they are skillful. The people that I know who are like this, it's a combination that they’re often skillful and the nature of the problems that they're solving are highly tractable. An example of a kind of thing that designers will often rabbit hole into is designing a poster. It actually often used to be kind of a cliche that at Facebook, there were all these posters up on the wall of the office. Very, very elaborate, beautifully designed posters for a talk that someone was coming to give at Facebook. Why did somebody put all this effort into it? Well, it feels really good, because a poster is really constrained, it’s finite, it's ephemeral. You can start it and yeah, there's a search space, but you can find a decent part of the search space pretty rapidly. And once you're there, there's this beautiful and very enticing feeling of turning the crank and like making it better and polishing it and trying this or that. But when you're trying this or that, like, all of the options are kind of okay. And you're kind of trying them out of curiosity, or like maybe it can be even better. And that's very different from the kind of design where you're just like, “</span><em>I simply don't know how to do this.</em><span>” And I think it's part of why those designers loved making those posters. It's a snack. It's a treat. It's also something they get to control whereas ordinarily, they don't.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 00:58:08</span></p><p>Yeah, just don't tell the manager how many software engineering hours were used up in the poster designing at Facebook.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 00:58:19</span></p><p>Well, no software engineering. It's only designers. But for the software engineers, code golf is the equivalent, right?&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 00:58:25</span></p><p>What is code golf?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 00:58:27</span></p><p><span>You know, in golf, you try to get the lowest score. So code golf, you try to solve the problem as minimally as possible. Like, “</span><em>Ah. I don't need this. I can combine this. I can do it in three lines. If I use Haskell, I can do it in one line.</em><span>” That's a kind of thing programmers do that's like this. But just endless refactoring is another thing that's kind of like this. You have the thing working, but it could be more beautiful.</span></p><p><strong>Dwarkesh Patel</strong><span> 00:58:51</span></p><p>Right. So it seems like the tools and the ideas you're developing seem especially geared towards very intelligent and very motivated students. If they would be different, what would the tools that you would develop for a median student in the education system look like? Both in motivation and in other traits?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 00:59:14</span></p><p><span>Yeah, they'd be super different. I kind of got out of the educational space in part because I don't like the framing of this problem. For the median student, the education system mostly wants to make the student do things they don't want to do. It's not about helping them achieve their goals more easily or more effectively. For the most part, it's about achieving goals that aren't theirs. Obviously, that's not always true. But for the median student, it kind of is true. When I was at Khan Academy I was kind of thinking about this problem. At Khan Academy, we were mostly thinking about not just the median learner, but like maybe the 25th percentile learner. One of the angles that felt most relevant, maybe not from an efficacy perspective, but for me, from like a breaking out of this, getting them to follow goals that aren't their own perspective, was to focus on inquiry learning and to focus on transforming the learning experience into something that actually is related to their goals. That is, we're asking questions that are authentically interesting, that they authentically want to answer, and that they can participate in in a way that feels natural. We did a lot of experiments with dynamic media, representations of things. The idea being that, you've probably seen these like plastic blocks or things that people can play with when they're kids to get an idea of numbers and number systems. Kids will play with these things unprompted because they're fun. It's just a pleasure to handle them. It's a pleasure to manipulate them. When you have them in hand, it's very natural to suggest, “</span><em>Ah, can you make a pattern like this?</em><span> </span><em>Why can't you seem to make patterns like that? Why is that?</em><span>”&nbsp;</span></p><p><a href="https://en.wikipedia.org/wiki/Cuisenaire_rods" rel="">Cuisenaire rods</a><span> is the name for a set of 10 rods that have basically unit length 1 to 10, and they're all different colors. You can do things like take the rod that represents 8, and put 2 of the rods that represent 4 up next to it, and show that this one you can divide into 2 rods effectively. But then if you take 7 there is no other pair of rods, for the same color, you can put it next to it. So, you get these different patterns and things kind of naturally suggest themselves by experimenting with these materials and having conversations with people around these materials.&nbsp;</span></p><p>One of the things we were interested in was, are there things that are like that that are more advanced topics? Can we create something that's kind of like those rods, but that is about a more advanced topic in math or about debates in history or something like that? One of our tactics was to lean heavily on social interaction. People like talking about stuff with people, if it's a real conversation. For the same reason that I had to use less willpower to study that quantum mechanics text, because you were there with me, a student who's engaged in a real activity with a peer will need less willpower as well. They'll also learn from their peers if you structure things right. Social learning becomes interesting. But I think at a high level, I mostly have abandoned this question to others. Basically everyone in the educational space, this isn't totally true, but like 90+% of people in the educational space are focused on the bottom quartile, not even the median. And there's a good reason for this. Many people who are in education are motivated by arguments of equity and opportunity. They want everybody to have the opportunities they had. They're very motivated by the injustices that they see in the differing access and the differing support that different people have. And they're very motivated by the very real disadvantages that accrue to the bottom quartile performing students. It's also true that the marginal impact that you'll have in that student's life will be much greater, probably, than the marginal impact on say an 80th percentile performing student or so the argument goes, like that student will be fine, which is probably true.</p><p><strong>Dwarkesh Patel</strong><span> 01:03:57</span></p><p>But there's a big marginal difference between fine and supercharged.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:04:04</span></p><p>Yeah, that's true. Anyway, I say all this to say that I understand why the vast majority of people in education are focused on what they're focused on. And I think it is good. And I'm glad they're doing it. I have mostly decided to let them do that. I’d focus elsewhere.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:04:22</span></p><p>Yeah. No, I see tremendous value in focusing on the cool new shit that's coming out, where's that coming from? And what's the way to increase that? It's interesting to know that the same tools might not just work across the spectrum.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:04:38</span></p><p>Yeah. Part of the trouble here is that the cool shit is very likely to come from students who are performing at the 20th percentile in school, because they're disaffected and bored and none of this stuff matters to them, right? Part of the trouble here is that by opting out of helping these people learn, there are all kinds of interesting inventions that could probably occur that aren't occurring. So I don't quite know how to contend with that. I guess basically I'm trying to bite off a piece of a problem that feels maybe tractable.</p><p><strong>Dwarkesh Patel</strong><span> 01:05:12</span></p><p>Once all the tools are built, when you're at the end of your career, is the learning process supposed to feel fun? Or does it have to feel fun? Is there an element of even when all the tools are there, that there's just like a level of David Goggins, this is going to be miserable, but I've decided to learn this in this way and I just had to go through it.</p><p><strong>Andy Matuschak</strong><span> 01:05:33</span></p><p><span>Where does misery come from? I'm asking this honestly, not really rhetorically. Let me try to answer my own question. Let me say first off that I am, broadly speaking, very opposed to what I understand to be David Gogginsesque attitude towards almost anything. In this particular instance, I think what I think is something like, if I ask why is it miserable to learn a particular subject? The answers that come to mind are things like, first off, I don't care about this subject. And I think that's not what we're talking about. You're asking about a world in which these great tools exist and someone's using one of these tools to try to do something they really care about. So another reason why it could be miserable that I think is pretty common is that you have some idea about, you're not going fast enough, or you're failing, or you're struggling, and the misery comes from resisting that. It comes from feeling like you're doing poorly and you shouldn't be doing poorly, it's bad that you're doing poorly. And maybe you're feeling fearful that others are going to judge you or you don't have enough time or something like that. And I think that's basically like an emotional problem that needs to get healed, rather than like a practical problem with learning. In the case of something like organic chemistry, where you truly do just need to learn 200 names or something. One answer is that it can be done very cheaply using modern memory systems. Organic chemistry students suffer through this and they don't need to. But even with modern memory systems, you're probably going to spend a total of 100 or so minutes across some weeks, studying all of these formulae. That still is unpleasant so can that be resolved? And I think the answer is yes, actually. I was thinking about this in the context of the Cell Biology By The Numbers book I was telling you about where there's all of these things like the volume of the nucleotide is a nanoliter. To study the flashcard “</span><em>What's the volume of a nucleotide?</em><span>” is not terribly pleasant. I'm not sure it constitutes suffering exactly. It's fine. I'll do it while waiting in line. But I think there is a better version of that, which is like solving an interesting Fermi problem which involves that term. So something like, if I have a vial of the COVID vaccine, how many copies of the COVID RNA are likely to actually be in it if the vial is a milliliter large? That's a fun little question and I can enjoy sitting and noodling on that. And in doing so, I will need to retrieve the volume of the nucleotide to help me make that approximation. So I think there's moves like that you can use to paper over any remaining stuff that feels kind of necessarily unpleasant or rote.</span></p><p><strong>Dwarkesh Patel</strong><span> 01:08:30</span></p><p>I'm actually surprised to hear you say that because one way in which I read some of your stuff is that this is actually a way of endorsing the traditional way of thinking about education, but using new tools to get the traditional ones. To give you an example of what I'm talking about, you go back to a headmaster from the 1900s, and you say, is it important to have the taxonomy of a subject memorized? You say, of course it is, that's why you’re going to spend a year memorizing the taxonomy. And then you would say memorizing is actually important so that you have a dictionary by which to proceed on the subject. So in those ways you have new systems for doing that same kind of thing. And the reason in this particular case, I was expecting you to say, No, you have to be disciplined if you decided to learn something. I expected that in the case of the three hours of intense learning followed by an intense piano session, you were just really tired at the end and you're like, “But no, this is something I have to do this evening.” So yes, I'm actually surprised to hear you say that.</p><p><strong>Andy Matuschak</strong><span> 01:09:33</span></p><p><span>Yeah, no, I really enjoy this tension. I'm probably reacting to the Goggins reference with a bit of an over extreme overcorrection or something but this really is how I feel. And I feel this tension all the time. The histories in educational psychology that I'm most aligned with are the most robotic, authoritarian kind of histories, and also the ones that are most kind of unschooling and Montessoriesque. I really have a ton of sympathy for elements of both of these directions and there's kind of a weird synthesis of this in my head that I can't fully externalize. I guess part of what I'm saying is aspirational. It certainly is the case that I do in practice use willpower to make things happen. Just as an example of something totally contrary to everything I was saying, I use a tool called </span><a href="https://www.beeminder.com/" rel="">Beeminder</a><span>, which charges me if I don't do certain things. This sounds kind of military, but it's certainly more authoritarian than this kind of freewheeling butterflies kind of gesture I was making a moment ago. And I use it to make sure that I do my memory practice. Shouldn't my memory practice be so joyful? It's at the center of my research, right? It should be the most interesting, exciting part of my day, but often it's not. And so I use this to do it anyway. So there's some tension here.&nbsp;</span></p><p>I think I do want to say the reason why I'm willing to endorse this headmaster's view about the taxonomy has to do with the price. I did a bunch of memorization in high school, and it was very inefficient, and it was very uncertain. It was emotionally difficult because I wouldn't even feel confident that I'd learned the stuff. I didn't know what it was to learn something reliably, to be confident that I'd be able to recall it. And it was hugely time consuming because I didn't have techniques or tools. And now, part of why I respond so favorably to just learning the taxonomy is that, for me, it's just trivial. Like, yeah, sure, whatever, throw it into the deck. It'll consume a total of 15 minutes over the next few weeks and then I'll know it. It just doesn't cost anything. Other things in learning do still have real costs and those are maybe more difficult to negotiate.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:11:57</span></p><p>Actually, this is maybe a good place to ask you about unschooling and your attitude towards it. Somebody on Twitter had this question, how are you structuring the education of your kids as they're growing up?</p><p><strong>Andy Matuschak</strong><span> 01:12:10</span></p><p>Well, okay, so to be clear, I don't have kids.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:12:14</span></p><p>Right. Hypothetical kids.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:12:17</span></p><p><span>So yeah, so you're gonna hear the foolish response of a person talking about what one would do hypothetically. This is very difficult. The school, of course, has many purposes other than instructional, right? It has a social purpose, it has a societal purpose, it has a behavioral purpose, and it also has like a pragmatic purpose of basically babysitting. Those things can be unbundled. I think it's pretty interesting to consider that. If I actually did have a kid, I would probably consider that project pretty thoroughly. I think it's pretty likely that some kind of homeschooling situation would occur. It probably wouldn't be [unclear] the teacher, but it would probably be the people I would hire. I have some resources. I'm not wealthy but I have some resources, that is maybe a difference. During the pandemic, I was struck by a company called Schoolhouse, which is now defunct, started by Brian Toball. The idea was that he noticed that people were getting together in pods, right? That was the thing we did during the pandemic. And in particular, they got together in pods with their classmates from school, maybe five or six kids. And some of these pods started hiring elementary school teachers who were not working because of the pandemic. And the elementary school teachers would come to the backyard of one of these people's houses, and the five or six kids would get together with the elementary school teacher, and they do stuff all day. Buying this one teacher's time split five or six ways was actually really very tractable. Say you want to pay the person $50 an hour, maybe that seems reasonable for a teacher, this is not that hard to do, and actually costs substantially less than a private school. I think Schoolhouse costs something like a fifth or whatever, the cost of an elementary school. Once you get to older grades you may need specialists. It's actually not clear if you do. My friend, </span><a href="https://www.linkedin.com/in/alecresnick" rel="">Alec Resnick</a><span>, is working on a very interesting school called Powder House in Somerville, Massachusetts, that does something like the model I just described, where you have adults who are in more of like a coaching role, and they aren't necessarily domain specialists, but they'll connect people with domain specialists. Anyway, I would explore something like that model. I'm sorry, this is a little bit vague. If you want to ask about something specific...&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 01:14:38</span></p><p>Sure, let me ask you a more specific question. This child grows up and is now 12. At this point, you have taught arithmetic, reading and everything. Do you proceed and say you have to learn your biology, you have to learn chemistry, or do you just say, what are you interested in? Are you interested in Roman history? Oh, let's learn about the aqueducts. Or is there an actual curriculum that proceeds until they get to college?</p><p><strong>Andy Matuschak</strong><span> 01:14:59</span></p><p><span>Yeah, this is really challenging. One of the heroes of the reform school movement is this philosopher named John Dewey, and he has a lovely book called </span><a href="https://www.amazon.com/Experience-Education-John-Dewey/dp/0684838281" rel="">Experience and Education</a><span>, sort of written near the end of his time, looking back on all of his efforts to reform schooling in a kind of unschooling-ish direction. He was never as extreme as that, but broadly looking for freedom on the child's part. And he makes this wonderful argument that because these kids don’t have a fully developed prefrontal cortex, certainly don’t have a fully developed kind of sense of self, to let them do whatever it is that their whim commands them to do in any given moment is actually not freedom, but rather is chaining them to whatever that impulse is. It makes them the subject of these tides of impulse. And I think that's a pretty compelling argument. It doesn't authorize tyranny, but it also suggests that, you know, you got to be a little bit skeptical about the planning or the plans of 12-year-olds, I guess. How skeptical should one be? I don't know. I think I would probably have stronger opinions on that if I had a 12-year-old. But my instinct as a foolish non-parent would be something of a mix. I would be interested in exposing the 12-year-old to lots of topics and possibilities. I would be voluble in expressing the consequences of any particular actions. Like if they just want to compose music all day, we could talk about like, well, what does that mean? What kind of life does that look like? I would try to be non-coercive in this as much as possible. And I think to some extent, the child should be allowed to feel the consequences of their choices. This is complicated by the fact that, again, I'm not wealthy, but any child of mine would have chances. If they made some weird choice about a career path when they're 13 and so they didn't get into Harvard or whatever, that would be okay. They could be 24 and finally figure it out then, or 32 and finally figure it out then. It would probably turn out fine. This doesn't seem like reliable guidance. You should notice I'm feeling very confused about it.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 01:17:36</span></p><p>Yeah, no worries. So one question I have is historically, and maybe even to the modern day, it seems like improving education has been a very intractable problem. And you did reference this earlier when we were talking about gearing towards median student versus whatever percentile you're working with. But I don't know. Do you feel like there's been progress even in the percentile you're gearing your stuff towards? And if not, what is the explanation for the relative stasis? I mean, this is something you talked about. We have so many new tools with IT. What explains the broader sort of stagnation here?</p><p><strong>Andy Matuschak</strong><span> 01:18:09</span></p><p>The funny answer to your question is actually there's been a ton of progress. Actually, things are pretty good. I think the stat is that in 1900, 6% of teenagers graduated high school in the US. Now, that doesn't mean that 94% didn't have an education that we would regard as a high school education, but it roughly means that. Now, these people are homeschooled. It's also the case that a high school education meant something lesser back then. A substantial fraction of high schoolers now study AP courses and complete them in high school. That's at the high end. On the low end, illiteracy was a very live situation 100 years ago in the US and is emphatically not now. Now it is the case that something like 10 to 15% of adults, depending on which polls you use, maybe would struggle to perform simple kinds of number manipulation or reading or writing kind of tasks, but our bar is basically moved. It used to be like, can you read it all? These tasks are maybe a little artificial. They're maybe not relevant to their day to day, and that's actually why they're experiencing this. The number of people, the fraction of the population who graduates at 17 or something, knowing a particular amount of stuff has basically moved up monotonically. And this is mostly about the bottom portion of the population. It used to be the majority were effectively uneducated past age 10 or something other than informally and in their trade. And really the story of the 20th century has been in part one of mass education. Part of why we have a service economy, an IT economy, is that basically all of our population is educated to a particular level. If you look at the national tests of fourth, eighth and 12th grade math and language proficiency, you'll see really pretty slow movement in the 75th percentile and practically none at all in recent decades. But you'll see absolutely enormous movement in the bottom quartile. And so in some sense the story, especially the last 20, 30 years, has been closing what's often called the performance of achievement gap where certain groups, part of underfunded schools, or who might have households that are unsupportive, or difficult, were just not having anything like the educational attainment of their peers. And that story has changed.</p><p><strong>Dwarkesh Patel</strong><span> 01:21:02</span></p><p>One thing I'm curious about is that every other part of the distribution has been moved upwards. Has the ceiling been raised significantly?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:21:10</span></p><p>Depends on what we mean by the ceiling.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:21:14</span></p><p>Because you can go back like hundreds of years and the most learned people around. It's just incredible you look back on how many books Thomas Jefferson read. There's some story where Kennedy was hosting a bunch of Nobel laureates in the White House in 1963 or something. And he says, this is the greatest collection of genius and insight and wisdom that has been collected into this room ever since the time that Thomas Jefferson dined alone.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:21:48</span></p><p>Right. I think it's very hard to raise the ceiling. The ceiling has aristocratic tutors. The ceiling has whatever family dynamics and heritable propensities produce tremendous intellectual greatness. Early 20th century schools produced von Neumann. Right. And it's certainly not at all clear that they're now producing more von Neumanns or something like that.In fact, von Neumann's productions seem to have probably very little to do with any kind of mass schooling that we would recognize. As far as the very top, I think that's difficult. We're talking about an institution that was created for the masses. I guess there have always been people who have been using resources outside of those kinds of systems. So the mass system doesn't seem to help those people, I guess. That doesn't seem surprising.</p><p><strong>Dwarkesh Patel</strong><span> 01:22:45</span></p><p>By the way, on the von Neumann thing. Okay, a mass system doesn't help them. What is the production function for a von Neumann?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:22:53</span></p><p>Yeah, so lots of people have studied this. I actually am not a student of Von Neumann's history. I know that many of his peers, the 20th century greats, got something like aristocratic tutoring or came from small Eastern European incredible schools that there's stories about these things. I actually don't know them. I'm sorry.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:23:14</span></p><p>I mean, I’m sure you’ve heard about that one high school.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:23:18</span></p><p>Yes. Yes.</p><p><strong>Dwarkesh Patel</strong><span> 01:23:20</span></p><p>Okay, interesting. Are we getting worse at the von Neumann production or is it just static?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:23:26</span></p><p>Well, maybe. I don't know. So let's see. Here's the theory that seems kind of plausible. If someone was going to have aristocratic tutors in the late 19th century, would they now go to a fancy private school and would that experience now actually be less good for them? I don't know. I think it's probably more likely that they'd go to the fancy private school and also still have fancy tutors and then go to a very exclusive university where they're going to get a bunch of highly hands-on kind of interaction with professors.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:23:58</span></p><p>Although the reason that might not be the case is the opportunity costs for people who might become teachers or aristocratic tutors is much higher now, whereas the kind of person who would be, your tutor can now directly be making lots of money on Silicon Valley or Wall Street.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:24:13</span></p><p>That's interesting. Okay, so that would be an argument that maybe it's not so much about the 20th century that we've gotten worse about this, but more like over history. Maybe Aristotle was a tutor to Alexander the Great, and now Aristotle would be like a full professor and wouldn't need to take that job. That might be so. It may be the case that some tutors have been priced out of the market, but it's not clear to me that the most expensive tutors actually would be the best. There is a bunch of empirical research on tutoring, and one of the questions they ask is, what kind of experience level do the tutors need to have? And it's interesting, how far you get in tutoring efficacy when the tutor doesn't necessarily know anything. Just having another warm body there actually contributes a very large effect. I mean, things get better as you get an expert. And I also have a kind of healthy skepticism of these studies. I think part of the role of having Aristotle as a tutor is communicating a worldview. It's not something that would show up on a test or something that these studies would be measuring. And so having an extremely inspiring individual might actually be the important component, and inspiring is going to be highly correlated with expensive, I think. Not necessarily, I don't know. That feels complicated.</p><p><strong>Dwarkesh Patel</strong><span> 01:25:32</span></p><p>I mean, especially today, the material is available. What the tutor is bringing is the inspiration and the motivation. Not exclusively but one of the large parts.</p><p><strong>Andy Matuschak</strong><span> 01:25:41</span></p><p><span>That's right. They're not really responsible for instruction. I'll also say that I know lots of people who have postdoc tutors right now. These people, as graduate students, they're very pleased often to have a $60 an hour tutoring kind of commission. And that's a little sad. But, you know, the pool of available postdocs to hire as tutors is very large now compared to how it would have been 100 years ago. The pool being bigger doesn't mean that the top 1% are getting more though. So I think that's undecided. There is a question of, have teachers gotten better at their jobs over the last 50 years? And there are some ways in which maybe they have. There's been a bunch of projects of trying to disseminate certain research results, ways of instruction that are more effective in other ways. For instance it's better to interleave stuff than doing blocked units where it's like, “</span><em>Okay, like, we're going to talk about the Civil War, and then we're going to talk about women's suffrage.</em><span>” It's somewhat far apart but it's better to kind of weave these things into each other, not just in history, but in general. So that kind of dissemination has been happening more systematically in the last few decades. I'm unaware of any kind of studies or results trying to establish anything about the efficacy of teachers now versus long ago.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 01:27:10</span></p><p>Well, I'm sure you've seen the claim that one of the consequences of the very unforeseen circumstance of the mid-20th century, was that one of the very few occupations an intelligent woman could pursue was teaching. And now that other options are available, which is obviously hugely good, you know, there's other competition for the same very intelligent woman.</p><p><strong>Andy Matuschak</strong><span> 01:27:34</span></p><p>Oh, that's interesting. I haven't heard that claim. Yeah, I'd have to think about it. I guess it's not clear to me how much intelligence matters. If you want to think of that as some kind of separable quantity.</p><p><strong>Dwarkesh Patel</strong><span> 01:27:48</span></p><p>Or whatever trade is relevant to just that. You just had a population that was hostage to either housework or teaching.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:27:56</span></p><p>I guess what I'm saying is something like, if that were true, and there are like a bunch of people who are now astrophysicists or something, it's not clear to me actually that they would have been good teachers. Being a good teacher is often about empathy and effective communication and care. It's very personal. It's very intimate. You need to understand the subject but to teach a 15 year old or something, you actually don't need to understand it at a postgraduate level necessarily. It's very interesting to see that there's a bunch of studies of the impact of domain knowledge on teaching efficacy. I've read some in math, I'm sure they exist in all fields. And one of the things that comes up is like, if you aren't very familiar or comfortable in math, then you will struggle specifically to do inquiry oriented classes, classes that are more about creative ways of thinking with math, or open ended problems, as opposed to like here's how to do this algorithm. Because to conduct those kinds of classes, you have to be able to think on your feet. You pose a difficult question to which there may not be just one appropriate answer and your students will throw all kinds of stuff at you. And you have to be able to take that stuff and integrate it and show how one student's answer relates to another student's answer and show how those conceptions can be built upon in order to produce some useful understanding for what you had in mind. Anyway, this kind of improvisation requires a mathematical familiarity and ability. But I don't think it requires anything like extraordinary ability.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:29:39</span></p><p>Yeah, but more than the extraordinary have been pulled out of teaching as a consequence.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:29:43</span></p><p>Yeah, I guess I'm just wondering what the correlation is. If it's the case that actually effective teaching is mostly about empathy, then maybe it's anti-correlated. Like the people who are going to be good particle physicists are actually like they wouldn't make good teachers anyway. Maybe.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:30:00</span></p><p>Interesting. Why hasn't hypertext changed how people write more? Often I write a blog post and I actually do wonder how much different it is with the knowledge that I can add footnotes and I can link to things. I'm actually kind of a fan of how Wikipedia organizes content. It is genuinely surprising how often the best explanation of a subject is just this resource that is trying to explain every single subject. Because there's this practice of you don't need to do exposition in every single topic. You can just hide it behind links and things like that. Anyway, so why hasn't hypertext changed online writing more?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:30:34</span></p><p><span>This is a really good question. I think the reason why Wikipedia works as well as it does is that encyclopedia entries are already forced to stand on their own. And that was true before hypertext existed. In fact, encyclopedias were already hypertext-ish before there was hypertext. There are some other interesting kinds of hypertext that existed pre-computers. There is this very interesting book called </span><a href="https://en.wikipedia.org/wiki/A_Syntopicon" rel="">The Syntopicon</a><span> from Adler. If you wanted to understand what classical authors had to say about a topic like the father's responsibility to a daughter, you can look that up in The Syntopicon and you will get references across Rousseau, through the Bible, and so on and so forth. And those are kind of hyperlinks. They were printed on dead trees, but you're expected to get the books down and look up the appropriate pages. The Syntopicon wasn't that successful. I think it's in part because those concepts, unlike the Wikipedia entries, don't quite stand on their own so cleanly. You kind of need sinews, you need linkages. And actually, I want to make the case that while Wikipedia is an astounding resource, I find it rarely to be the best available introduction or explanation of a topic. I find it often to be like a good jumping off point. It'll help me know the right thing to ask about. It's good as a reference. Hypertext is a very effective navigational aid. It can help you get to a spot that you're looking for very quickly because it's about automating flipping through pages. And so for a reference, it's very effective. If what you have is a book of chemical compounds and their properties, hypertext is going to let you navigate that book very effectively. Likewise, dictionaries have been revolutionized by hypertext. Navigating around the sources by clicking on links to say like, “</span><em>Oh, shade it a little bit more like that.</em><span>” It's like a much better thesaurus. I guess I'm making the case that there are certain kinds of texts that are more amenable to hypertext, because they are more amenable to having the reader dropped in the middle of them. Encyclopedias are like that, dictionaries are like that. Most texts are not like that and most concepts are not like that. I guess most ideas are embedded in something kind of holistic or richer. They require a narrative arc. They're difficult to excerpt. Not everything, but things that are not so raw and atomically informational. There were all these dreams of hypertext novels, for instance, and some people wrote them. And one of the problems that a hypertext novel has, and it actually can be seen in a choose your own adventure book that existed before there was digital hypertext, that the author is forced to write something like a lowest common denominator story, the page that is the destination of a hyperlink, it has to work as the endpoint of all of its reference. And so it can't establish any kind of coherent or consistent arc, unless there's a kind of sameness to all of the reference. And the more that there's sameness to the reference, the less useful hypertext is. So a lot of people have been disappointed by this conclusion. I, among them. I'll say that I do find hypertext very useful in my own notes, not really for reading. I actually don't think it makes for a very good reading experience for others.</span></p><p><strong>Dwarkesh Patel</strong><span> 01:34:15</span></p><p>Having been a reader, you have a separate webpage where you have your working notes. It is a very cool UI to explore your thoughts.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:34:24</span></p><p>Thanks. It does an interesting thing for me as a writer. It lets me build stuff up over time. Today, I was reading this very old cognitive psychology paper on the topic of adjunct questions, which we discussed earlier, the effects of asking questions while you read, not on remembering the information covered in the questions, but on the general effect it has on stuff that isn't touched by the questions. I have some notes on the design decisions of the mnemonic medium, this Quantum Country thing that I was talking about earlier, interleaving the questions into the text. Those notes are kind of partial. They evolve over time. What was the impact of doing this? My notes about that, they've come from interviews with readers. They expand when I read a paper that's relevant to them. It means that when I go to design the next system, and I'm thinking about the role of questions in text, I'll have a place to look. The role of hypertext is roughly a navigational aid. It's possible to do this without hypertext. You’d just end up with what Neumann had, a giant dresser-like thing, but made of card files rather than drawers for clothing.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:35:50</span></p><p><span>This actually goes back nicely to the original conversation we had about why people like Tyler are able to integrate so much information without an explicit note-taking system. Another person who comes to mind immediately is Byrne Hobart. Again, you have an example of somebody who is extremely prolific and writes a tremendously detailed and insightful </span><a href="https://www.thediff.co/" rel="">daily financial newsletter</a><span>. It's a daily note-taking practice in some sense.&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 01:36:18</span></p><p>Nothing quite accumulates for either of them, at least not in the same way. It's very interesting. They're doing the whole thing over again every day. One thing I find interesting about Levine's newsletter is that when he's talking about a topic repeatedly, like the recent bank collapse or something, he will have to explain some concept like interest rate risk over and over again for days. Every day he has to explain it, but every day he explains it anew, and every day the explanation is colored a little bit by that day.&nbsp;</p><p>This is an argument against the kind of note-taking that I do. It's an argument for ephemerality, for recreating the thing every day, because it will change and it will become inflected by what you're thinking about and your experiences. It's pretty interesting. I find myself doing a mix these days. I have a journal that's about today, and I'll do a bunch of writing. Often I'm recapitulating stuff I've written before, and I have other things that are trying to be more durable, and be a useful reference that can stand outside of time. The combination feels useful. I don't yet have a clearer model of when one is better than the other.</p><p><strong>Dwarkesh Patel</strong><span> 01:37:37</span></p><p><span>An interesting way to tie in what you just said with the hypertext: Byrne’s newsletter doesn't give that much context. Often you'll find yourself lost about the concept being talked about if you're not familiar with the topic. I asked him at some point, have you considered doing narrations of your blog post? Scott Alexander has somebody who has a podcast where they narrate his blog posts. He said, “</span><em>I don't think it would work out as well for mine, because I heavily rely on the old blogosphere</em><span>’</span><em>s norms around hypertext, where you can add jokes and sarcasm.</em><span>” One example of this is his write up about SBF and his collapse. It has a bunch of links - if you want to learn more about margin calls, read this. And he goes, and if you want to learn more about the psychology of utilitarian bets, read this, and it's just a link to the Amazon page of Crime and Punishment. That kind of stuff is harder to do.&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 01:38:39</span></p><p>Yeah, you're right. He's leaning more on his past explanations, which is interesting, because he can't update them. That format of writing a newsletter and then linking it to past newsletters, or as you say, the former blogosphere thing to do, you have a series of six words and each word is linked to a previous post on the topic. I certainly have written stuff like that. It's kind of funny. It's approximating the durable note thing I was writing about, but without the ability to revise it over time. Maybe for many topics, you don't need that ability. I wonder now what fraction of my notes are in the state they were when I did my first major revision of them. It's probably at least a third, it might be more than half.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:39:24</span></p><p>What percentage of notes have you published?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:39:27</span></p><p>By word count? By note? I don't know. For instance, my journal notes are not published. There's one of those every day so there's a lot of them. If we're looking by note, we're excluding all of those. I also have a note about all of the people in my life and those are not public, unless they're public individuals. There's a lot of notes that are not public, but they're mostly not durable. They wouldn't be all that meaningful to others. The journals might be, but they're also intimate.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:40:01</span></p><p>Are they written in a way that would be intelligible to somebody else?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:40:05</span></p><p>It depends. Usually my journals are complete sentences, complete paragraphs. Sometimes bullets, sometimes veering and breaking and changing to new subjects suddenly. They tend to be filled with links to the things that I'm talking about, in part because I'm trying to accumulate context in those things.</p><p><strong>Dwarkesh Patel</strong><span> 01:40:27</span></p><p>How come they're not just shorthand?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:40:31</span></p><p>It's partially because past me is another person, it's kind of a cliche. I am routinely looking at journal entries from a year ago. You could view that as a failure of this note writing system. In some ideal sense, I shouldn't be looking at these journal entries, because if something's important, and it's going to be something I refer to a year later, it should be in some durable evergreen note. I don't know. You don't always want to do that. It feels like prepping. Maybe there's an amount of prepping that's good. We live in California and maybe everybody should have an earthquake kit. Maybe that's good, but maybe you don't need to hoard 300 cans of beans. There's an amount of prepping that feels like a reasonable amount to do and there's an amount that feels kind of dutiful and unpleasant.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:41:22</span></p><p><span>As a researcher, who is in the Silicon Valley circles, what is your opinion on the startup advice “</span><em>Do things fast. Fail fast. Get to users immediately with an MVP.</em><span>”? As somebody who is making products, but is also in a different mode than a typical startup, how do you think about advice like that?</span></p><p><strong>Andy Matuschak</strong><span> 01:41:42</span></p><p><span>I have complicated feelings about this. I need different advice on different days. Of course, different people need different advice on different days. When I was getting into this kind of work, what that advice led me to do is to not think all that deeply about the ideas I was exploring. An idea would come up and I’d think, “</span><em>Oh, I can try that.</em><span>” I would try that, then I'd learn something, and then I'd repeat. There wasn't this sense of building a theory of what the problem is and what it would mean to solve it. Instead, it was just a theory of action. A theory of action as opposed to a theory of change is, imagine you're in your current position, and eventually want to get to some goal state, a theory of action is you look around you and you say, “</span><em>Well, what can I do? What can I build? What do I see as possible?</em><span>”.&nbsp;</span></p><p>A theory of change is to look at the endpoint to try to work backwards. The metaphor is imperfect because in research, you don't exactly know what the endpoint is and you certainly don't know how to work backwards. I guess what I'm saying is that following that advice historically has led me to try things that were straightforward. The most powerful design work has ideas in it. What makes a nonlinear text editor, the text editors that we all know and love, so powerful is this observation that writing is a nonlinear process, but writing with a pen linearizes it. Many, many other observations like that and on the nature of what it means to have a thinking environment is how we got that particular interface. Likewise, the way that we got powerful programming environments is by people thinking very hard about what it means to specify a system and coming up with new primitives that express those ideas.&nbsp;</p><p>The most powerful interfaces are often the expression of new ideas or new primitives that capture new ways of doing, new kinds of objects that can be manipulated. In Photoshop, for instance, you can manipulate a photo by means of a construct called a layer. This is a very strange idea. It has some precedent in dark rooms where you could potentially have sheets of film. I don't mean the negatives, I mean sheets of gels that you could potentially put over lights to affect the exposure - to make there be more exposure here and less there. But in Photoshop, they're non-destructive and they're continuously manipulatable. The layer is like a new primitive that is introduced into the activity of photo editing. It utterly changed what you could do in photo editing.&nbsp;</p><p><span>What I'm saying in a very long-winded and confused way is that it's difficult to have ideas by means of building an MVP very rapidly. Now, if you have an idea that you think is interesting, it is good to test it rapidly. Part of why I'm confused in my response here is that it's good advice once you have something worth testing. For me, adopting that mindset, and I've lived in it for so long that it's very ingrained in me, it makes me not sit in stillness and in confusion and in contemplation with the ideas long enough for them to be good. Michael Nielsen and I made Quantum Country and when I was trying to think about what to do next, the most obvious or natural idea was “</span><em>What if we just tried that with lots of other things?</em><span>”.&nbsp;</span></p><p><span>That idea occurred to me and the pandemic had just struck, so I was feeling a little timid, creatively or emotionally. I wanted something that felt safe and I knew I could do that. I can build a platform that generalizes this thing that we did for this textbook. So I did. And I did it relatively quickly. I did it in a few months. And that wasn't the right thing to do. It wasn't really the right question to be asking. The idea wasn't that strong. Building this highly general version of it wasn't the right way to test it. I would have been better building more one-offs rather than a self-serve thing that anyone could use. And this comes down to the difference in aim. I'm not trying to build some kind of scalable thing for the world at this moment. I'm trying to build the idea. The prototype is an expression of the idea. Once it arrives at a good place, then maybe there can be some scalable solution. But it's not necessarily at that place. Until it's at that place, there's a lot of thinking and sketching that goes along with the building and prototyping. Part of my confusion here is that often I still need to hear this advice. Often I will just tie myself in knots in theory land. What I really need to do is to have a friend sit me down and say, “</span><em>Is there a piece of this that you can carve off and build next week?</em><span>” So you're hearing a lot of tension.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 01:47:14</span></p><p><span>Interesting. What was the consequence of shipping </span><a href="https://withorbit.com/" rel="">Orbit</a><span> out before it felt ready to scale?</span></p><p><strong>Andy Matuschak</strong><span> 01:47:25</span></p><p><span>I mean, I learned some things. It was fine. It taught me a lot about where that particular format succeeds and fails in other venues. It was just not a very effective way to find those things out. It was an MVP in the sense that it has very few features and it's very simple. But it was highly general. It's a deployed thing that has infrastructure and has accounts, it has all this stuff that you do when you're building a real thing. That's very different from, “</span><em><span>Let me work with this one author and see if I can make it work with this one other book that's very different from </span><a href="https://quantum.country/" rel="">Quantum Country</a><span> to form a specific question or specific theory about, it worked for this text, what's the next kind of text that would be good to test with?</span></em><span>”. I certainly could have done it much more rapidly.</span></p><p><strong>Dwarkesh Patel</strong><span> 01:48:16</span></p><p>Why do you think that this idea of tools of thought has nerd sniped so many people in Silicon Valley?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:48:24</span></p><p>It contains this message for technologists that they can potentially be very powerful. That's always tantalizing for people. It also feels very actionable for people in a way that's super misleading. I meet tons and tons of people who tell me that they're interested in tools for thought and 95 plus percent of them are engineers. The problem with this is that building an interesting tool for thought is basically entirely a design problem. Their design ideas are usually not very good, or troubled in a variety of ways. Yet, they can make a thing that solves a problem for them in their lives. That feels very tantalizing or encouraging. It feels like something to get their hands around.&nbsp;</p><p>We in Silicon Valley are very, very interested in thought. We are thinking people. People are very interested and engaged with anything that can potentially expand our capacity. That too is tantalizing. What if I could think better? It's also tantalizing because it's meta. There's all these cliches about people tinkering with their dot files endlessly or tinkering with their blog website, which has two posts on it, but they have to rewrite it because they want to do something else. The new one will have three posts on it before they rewrite it again. Tools for thought also scratch that itch. It's work about the work. This sounds very cynical, by the way. I don't mean for it to be, I'm just trying to earnestly answer the question.&nbsp;</p><p>Here's a more optimistic and generous response. Many of us got into computing because computers portray a sense of personal empowerment and possibility. We remember growing up, being locked in our bedrooms at midnight, fooling around. We have this very powerful tool at our disposal and it's opening up these worlds for us. For many people here, that was a formative part of their personal development. Anybody pointing to that and saying we can do more stuff like that is going to be pretty compelling.</p><p><strong>Dwarkesh Patel</strong><span> 01:50:44</span></p><p>This was an interesting question from Matt Clancy on Twitter - what are the characteristics of a good crowdfunded research project?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:50:53</span></p><p>One of the unfortunate things that I've learned in my crowdfunding experience is that there are some dynamics that seem hard to change. One of them is churn rate. Any subscription revenue business model - that's what I have - you lose subscribers every month. In my case, it's about 2%. It's not that large, but it does mean that I need a certain number of new subscribers all the time. One thing I've learned is that the churn rate is surprisingly insensitive to anything I do. I experiment with a variety of things and it hasn't meaningfully changed the churn rate. What does change things is getting more people into the top of the funnel, in other words, marketing. There are some things that have affected the fraction of those people in the top of the funnel who convert. I really hate this way of thinking about it.&nbsp;</p><p>In summary, the thing that I've discovered that's sad is that I ended up having to think about this a little bit. I realize that this crowdfunding project only even slightly works, because it's understandable and interesting to others. It's already in a place where there's some results that look promising. It's very easy to imagine other projects that are not broadly applicable. If I were doing marine geology stuff, I probably wouldn't have a big crowd of internet people, not nearly as large or excited. That's one property - this work is very general. It applies to many, many people. It applies to people who have disposable income. If I were doing a research project on writing practices of disadvantaged artists, my audience might not have as much disposable income.&nbsp;</p><p>I have already made some progress. That's important. Unfortunately it's probably very difficult to use crowdfunding in the very early days of a research project. I've already chosen a research agenda or direction, and I can express it. Crowdfunding probably applies after the first few stages of research have been completed. There's probably standard grant advice, where at some point, I'm going to be using this crowdfunding to figure out the next thing, and I won't be able to explain it to anybody. There certainly are seedlings, but you have to have something in flight. I need to be able to say something about my progress with some kind of regularity.&nbsp;</p><p>For instance, my wife is working on this study of biological markers of age in association with delirium and traumatic brain injury. To do this, she is signing up patients who show up to the hospital with traumatic brain injury. Once they agree to participate in the study, they agree with the taking of various blood samples and things like that. Recruiting enough patients to get the significance that she requires will take two years or something like this. She can report a little bit of intermediate stuff, but certainly not a monthly update. Right?</p><p><strong>Dwarkesh Patel</strong><span> 01:54:13</span></p><p>Yeah, that would be a weird Patreon post.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:54:17</span></p><p>Yeah, I can't quite report monthly updates either, but there's a cadence that's necessary.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:54:22</span></p><p>Why bother with it at all? I'm sure there's many wealthy individuals who would be happy to single-handedly fund your research. Is there a reason you chose crowdfunding?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:54:32</span></p><p>Those wealthy individuals are very welcome to reach out and offer to do so. I will say I've been fortunate to have many high net worth individuals as sponsors, but each of them is providing a sponsorship of $100 a month on my Patreon. That is what I get from these people. I'm certainly not getting wild offers for more.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:54:59</span></p><p>I think you're using the wrong tool given the wealth distribution of your audience.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:55:05</span></p><p>Maybe. There's a couple of ways to interpret your question. One question is, why crowdfund when I could appeal to high net worth individuals? Another version is why crowdfund at all, as opposed to raising grants or talking to philanthropies? Are you mostly focused on the first of those?&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:55:19</span></p><p>Yes.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:55:21</span></p><p><span>If I'm going to be honest, it's because it has worked. The history of the crowdfunding of this project, like many things in my life, is the result of goading from </span><a href="https://michaelnielsen.org/" rel="">Michael Nielsen</a><span>. Early on when we were working on this Quantum Country project, he suggested we set this up and I hemmed and hawed and said, “</span><em>Yeah, it's going to be a distraction. We don't really need this right now. Let's deal with it later when we have something to show.</em><span>” He's like, “</span><em>No, no, no, let's just get it started. It's going to take a long time to get enough subscribers.</em><span>” It turned out he was right. The process of building a subscriber base and crowdfunding a project takes a couple of years, at least in my experience. Starting earlier was better.&nbsp;</span></p><p>If that hadn't worked or if we hadn't started early, I probably would have just reached out and asked for individual help and I probably will if it fails on me. I'll say also, when there have been specific projects that I've wanted to do that require, say, hiring people, I have reached out to high net worth friends and they've helped, but in the low five figure, four figure range. And that's great and I'm very grateful. The answer may be a mix. One of the big limitations to crowdfunding is it can't sustain a team or institution. It can barely sustain me. I'm somewhere between a grad student and a junior faculty member. And that's okay. There's a variety of reasons why that's okay for me that are pretty particular to my circumstance, but it certainly wouldn't be okay for everybody. Even for me, it doesn't allow me to support others.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 01:57:06</span></p><p>Right. It's even more striking because you're pretty well known, especially amongst the audience that would be happy to fund this kind of work. If the LeBron James of independent public research is earning between a grad student and a junior faculty member, it's not a great sign.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 01:57:39</span></p><p>It's worth considering that I'm maybe not very good at this. First off, I'm not that successful as a researcher. I object to the LeBron James characterization. It's true that I'm maybe the most successful crowdfunded researcher in tech stuff, and that's kind of weird. In the last couple of years, I've figured some stuff out, but I wouldn't say I've had any spectacular hit publications.&nbsp;</p><p>One thing that is true of this is that when I have big publications, I get a lot of new subscribers. There is some kind of market force that could be higher if I were having more spectacular success with my research. It's also true that I systematically avoid marketing it. That's a self-protection thing. I am really worried about the corrosive influence of audience and marketing on honest inquiry. It is very easy to distort my work. It's almost a default to try to make it be something that people would be more likely to like rather than the thing that I actually want to investigate, or to do the boring simple version of it rather than the interesting deep version so I can publish more stuff more often.&nbsp;</p><p><span>One thing that I've chosen not to do, and it's a choice that's definitely cost me financially, is to publish what academics would call minimum viable units of paper. They have a pithier phrase than that. Minimum viable papers. It's very common to take any new marginal insight that is above a particular bar and publish that. I just haven't done that. I've written informal letters to my patrons, “</span><em>Hey, I figured this thing out this month.</em><span>” If I were an academic, I would have published that as a paper. If I were a marketing-oriented crowdfunded researcher, I would have done some glossy thing and promoted it “</span><em>Look at this thing I figured out.</em><span>” But actually, I just don't think it's that big a deal and I'd rather get on to the next thing. I have that choice of waiting to publish. That's not really what I'm worried about. Really what I'm worried about is marketing, man. Marketing. It makes it so hard to be honest with oneself, at least in my experience. Not only to be honest, with what I think is interesting and important, but even to be honest about the results. Every paper is, in some sense, a little marketing piece trying to make the case that it's significant, that its results are really exciting, really important. That is really corrosive to discovery. It's true that you need a really strong emotional connection to the work in order to do good work. Part of that emotional connection comes from a sense of excitement of maybe being hot on the tail of something really good. There's a temptation to portray what you found in the best possible light, to downplay its limitations, to take up space and to totalize. All of this is just death for discovery.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 02:01:00</span></p><p><span>It is interesting to hear that from somebody who inadvertently and without intentionally trying to do so has done a good job of spreading your material. I've known about you for a long time. I do wonder if there's an element of, if you get to a certain level of quality, trying to market your stuff, not only doesn't help, but probably hurts you. If you can try to think of somebody like Gwern trying to post YouTube shorts of his blog post, it would just be like, “</span><em>What are you doing, man?</em><span>” It's just so good that he doesn’t need to promote it.&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 02:01:33</span></p><p><a href="https://gwern.net/" rel="">Gwern</a><span> is an interesting example because there's a simpler failure mode. I still routinely run into people who will tell me, “</span><em>Oh, I've really liked your work for a while. I didn't know you had a Patreon.</em><span>” That's a simple failure of a certain kind of marketing on my part. Gwern actually has this even worse. I adore Gwern. I have learned so much from him. You can go to his </span><a href="https://www.patreon.com/gwern" rel="">Patreon page</a><span> and he actually makes public his revenue. He makes a tiny fraction of what I do on Patreon. I think this is inappropriate. Gwern is a much more impactful researcher than I am and he has a much bigger audience than I do. The fact that they aren't converting into patrons is mostly a matter of the way that he talks about it and the way that he presents it. It's not that he needs to market more people to his webpage. I expect he has plenty of traffic and a large audience. It's much larger than mine. There are a bunch of variables about the way you talk about this membership offering. None of us really want to think about them. I've ended up at a slightly more effective part of the space, but I'm pretty sure that there's much more effective ways to do whatever it is I'm doing.</span></p><p><strong>Dwarkesh Patel</strong><span> 02:02:48</span></p><p><span>Yeah, this is a really interesting problem because I have a Substack where if people choose, they can help contribute to the podcast. It's a broad enough revenue to help pay for certain episodes and traveling. However, in comparison to what I’ll be making now that I’m going to be doing ads, it's a small fraction. Some people might say “</span><em>It's unfortunate that you have to do ads</em><span>” and maybe listeners will just be finding out for the first time that there was an option on Substack. But you don't want to be in the position where you're asking listeners for money every episode, right?&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 02:03:25</span></p><p><span>Yeah, I hate asking people for money. This is a common issue for creative people. I hate it. I really hate it. I probably need to get over this. I do want to make one point, though. I had much more success with my Patreon when I recast it as, “</span><em>Oh, please subscribe to support my work</em><span>”, like the thing you were describing, to an offering to become a member. “</span><em>When you become a member, these things will happen.</em><span>” These things need not be terribly substantial necessarily. There's a difference between a tip jar and a membership in people's minds. Becoming a member means something. If you could offer something small, that feels membership-ish, you might get very different results. Gwern has the kind of tip jar vibe. These days, I have a member vibe. My instinct is that if you were to move to “</span><em>Become a member of Gwern</em><span>’</span><em>s lab</em><span>”, he would have better results.</span></p><p><strong>Dwarkesh Patel</strong><span> 02:04:30</span></p><p>He has a thing on Patreon where if you donate five bucks or eight bucks, he'll read an entire book and review it.</p><p><strong>Andy Matuschak</strong><span> 02:04:37</span></p><p>Yeah, this is crazy. I don't know if anybody's ever taken him up on this.</p><p><strong>Dwarkesh Patel</strong><span> 02:04:39</span></p><p>Yeah. That's like valuing his time at a dollar.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 02:04:45</span></p><p>Yeah, I don't quite understand this. It's also the case that you'd probably have an easier time asking for subscriptions if you had a larger audience first. You can build the audience for free and then have some bonus offering behind a wall. I feel very conflicted about this actually, maybe you can help me think about it.&nbsp;</p><p><span>I have all these patron essays. It's where most of my writing is these days, because I'm waiting until I can collect enough things for the next big public piece. I have a couple of big public pieces in various stages of flight. I'm writing a lot for patrons and probably much of my audience or people out there don't even know that it exists. One challenge of member only content is even making clear to others that it's there. Often people will try to achieve this by tweeting or sending newsletters out about this subscriber only content. I just can't bring myself to do it. It feels terrible to say, “</span><em>Oh, here's a link, but you can't view it.</em><span>” I can't do it. I don't know how you think about this, or if you think about subscriber only material for Lunar Society.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 02:05:59</span></p><p>I was actually just about to mention this to you. I'm a patron and I got a chance to read all your patron only essays, and they're great. I was thinking while I was reading them that it's really unfortunate that a person might not know they exist. If they're not familiar enough with your work to go ahead and sign up, it's just behind the Patreon. It's a shame that one of the ways to fund public work is to make some of that work less public.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 02:06:31</span></p><p>There are better ways to do this. There are design solutions. For instance, if it were the case that my work was mostly all in one place rather than separate places, and the subset of the work that's public was visually and structurally adjacent to the subset of the work that's private, it would be clear that there's additional stuff that's available. Perhaps you can see the first bit of it – Substack has this to get some sense of what it is that you'd be seeing. I've invested like zero effort into figuring out an appropriate presentation of this stuff.</p><p><strong>Dwarkesh Patel</strong><span> 02:07:06</span></p><p>Right. Another thing to consider is that a big part of the impact of your writing work is how many people actually consume it. The expected value of that is dominated by the probability it goes viral. For example, you had this really insightful post based on your experience in industry at Apple about the possibilities of Vision Pro and in what ways it's living up to and not. I think that would have just gone huge.</p><p><strong>Andy Matuschak</strong><span> 02:07:43</span></p><p>Oh, thanks. I did make it public. I put it on Twitter and it was on the front page of Hacker News. I think you're right. Usually I don't want this stuff to go viral. The primary value that most of it has for people is opening up a window into a particular very unusual kind of creative work that they don't normally get to see the behind the scenes of. And most of it is kind of context laden. It's not really freestanding. And I don't really want to write it as if it could be freestanding. I've occasionally had the experience of one of these things getting widely distributed and then getting all these comments of people being angrily confused about what I'm even talking about. That's kind of discouraging. All of this to say when I want to write something for broad public consumption, I write something for broad public consumption.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 02:08:36</span></p><p>Okay, I've got some questions from Twitter.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 02:08:43</span></p><p>Okay, bring it on Twitter.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 02:08:38</span></p><p>This is another question from Matt Clancy. Are there other examples of beneficial knowledge work practices that perhaps mostly work because they are former spaced repetition practice where the participants don't realize it?</p><p><strong>Andy Matuschak</strong><span> 02:08:52</span></p><p><span>This is embedded in our working world. For a researcher, when you need to write papers regularly and you're writing those background sections, you're repeatedly explaining the history of a particular line of research and citing the appropriate sources. That is a kind of spaced repetition. When you have students and you're mentoring them in conversation “</span><em>Oh, in this kind of situation, you really need to remember to do x</em><span>”. That is a kind of spaced repetition. All this is kind of accidental. The doctors have rounds - even when they're not seeing patients regularly, they're still exposed to other patients. There's often a structure in this where while the patient is being presented, you're supposed to be trying to think of what to ask. “</span><em>What would my differential be?</em><span>” Before you hear it, there's covert retrieval happening. It's everywhere in our world. It's spaced, and it's repeated.&nbsp;</span></p><p>The thing that differentiates the formal practice that I've been exploring is that it focuses on material that you wouldn't normally have repeated either because you're too early with it to have a consistent practice, or because it's just not firmly tethered enough in anything in your life.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 02:10:16</span></p><p><span>This is a question from Ian Vanagas. “</span><em>What is the optimal amount of effort that should go into a personal website?</em><span>” I think he might have noticed the amount of CSS that exists on </span><a href="https://andymatuschak.org/" rel="">andymatuschak.org</a><span>, which is very beautiful.&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 02:10:32</span></p><p>I don't like it. But this is what everybody says about their website, right? It's three years old, that means I want to redesign it, but I will not allow myself to because it feels like a distraction. What's the right amount of effort? There's no general answer to that question. Of course, that's going to be my answer but what can I say about it? What's the job of the website? What's it trying to do? Many people, especially engineers, do themselves a disservice by fretting over their websites unnecessarily, building vast technical infrastructure when really what they want is a place to post markdown files. They're better off getting a ghost installation. The main thing to think about is what is it that you want to put out in the world? What is the ideal form of that thing? And to try to find some way of organizing and expressing that.</p><p>We have these common patterns, like a blog or a portfolio. Often people end up forcing themselves into these patterns. People will end up using blogging software to make something that's durable. Very interesting personal websites often come from people who are thinking about that question – the shape of the thing that they want to put out into the world and making something that speaks to it. Often, once you understand the shape of making the thing, it's not that effortful. My website was not an enormous project for me, probably should have been a slightly larger one, given that my income depends on people coming through it.</p><p><strong>Dwarkesh Patel</strong><span> 02:12:04</span></p><p>The working notes with the…?</p><p><strong>Andy Matuschak</strong><span> 02:12:06</span></p><p>That was a weekend.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 02:12:07</span></p><p>Really?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 02:12:08</span></p><p><span>Yeah, I feel bad about it because it's made its way into tons of commercial projects now. People are like, “</span><em>Ah, this is the way to present network notes</em><span>”. I think it's not very good in a variety of ways. I spent like a couple days on it.</span></p><p><strong>Dwarkesh Patel</strong><span> 02:12:21</span></p><p>Wow. Because I thought this is where the question was alluding to – you must have spent months on this.</p><p><strong>Andy Matuschak</strong><span> 02:12:25</span></p><p>Nope. It is a little bit like the thing about the mechanic hitting one thing and knowing the thing. I have design intuitions that led me in a particular direction. But there's lots of things I don't like about it. I just haven't allowed myself to spend any more time on it because I just don't think it's important enough.</p><p><strong>Dwarkesh Patel</strong><span> 02:12:44</span></p><p>I have a question about your time at Apple before I ask the final Twitter question. Everybody has an iPhone and from the outside, there must be so many different tradeoffs and constraints when a thing like this is being designed. What is the supply of certain components and the cost? What do different consumers want? What features is the R&amp;D team ready to put forward? At your time at Apple, you were responsible for a lot of these cornerstone design features. How is all that information integrated – taking all of these constraints into account and deciding that this is the design? How does that happen?</p><p><strong>Andy Matuschak</strong><span> 02:13:20</span></p><p><span>It's very compartmentalized. None of what you just said was relevant to me. It was all pre-specified. At Apple, you have a little domain that's your own, and the boundaries of that domain are determined by everybody else's little domain. There's a person who's responsible for thermals. Actually, there's a team that's responsible for thermals, and they figure out things like “</span><em>What is our thermal budget? How much can we have the CPU on and during what kinds of working situations?”</em><span> I can't argue with that. Those are just my constraints.&nbsp;</span></p><p><strong>Dwarkesh Patel</strong><span> 02:14:00</span></p><p>But aren't those constraints informed by different problems?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 02:14:06</span></p><p>It is iterative. We'll run into stuff where there's a thing we really want to do, but we can't pull it off, because it drains too much power. So Hey Siri is an interesting example. To be able to activate a voice command at any time without interacting with the device is great. People prototype that just like having a thing listening in the background and watching for it. But that requires having the main CPU on all the time, processing audio buffers. You simply can't do that – it drains the battery. That attempt led to eventually having this dedicated co-processor that runs at a lower power that's very limited and restricted and it can be on when the main CPU is not on, and it can listen for that sound.</p><p><strong>Dwarkesh Patel</strong><span> 02:14:51</span></p><p><span>Is there a person whose job it is to take all things into account? “</span><em>I have decided, given the memos from everybody, that thermals, you guys need to work on this, you guys work on that?</em><span>”.</span></p><p><strong>Andy Matuschak</strong><span> 02:15:04</span></p><p><span>Not exactly. It's a little more push and pull. Some of a team’s priorities will be internally determined. The thermals team has its hobby horses, and it knows what it thinks is important. Some of them will be externally determined. There is an executive team that makes ultimate decisions about the main priorities for next year's devices. “</span><em>Ah, next year, we're going to do this face ID thing to unlock the phone and we're not going to have a home button.</em><span>” If you want to not have the home button, and you want to have the screen go edge to edge, it has all of these impacts like top to bottom on the device. That decision creates lots of necessary work for lots of teams.&nbsp;</span></p><p>Some stuff is kind of handled at a more local level. For instance, the director of iOS apps might decide, we have this problem because the apps were built at the same time as the system frameworks, we end up building our apps using this weird Frankenstein, partially internal framework, partially the public one that our developers use. The internal one is always a little bit different and it's not always maintained reliably. So we have all these problems about the skew between the two. A big priority for us is going to be to rewrite all the pieces of our apps to only use the public bits so that they could be distributed on the App Store.That's a more local decision rather than at a top level executive team.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 02:16:36</span></p><p>What I find really interesting about this is that it's possible for a $2 trillion company to integrate all this information to have a cohesive hierarchy where so many different products, so many different trade offs are being made. Does that make you think that over time, these very well functioning tech firms will get bigger and bigger, that they can actually handle the cost of having this much overhead?&nbsp;</p><p><strong>Andy Matuschak</strong><span> 02:17:02</span></p><p>Let me first just respond to this observation about the enormity of the company and then we'll talk about the other firms. The reason Apple is able to do this is because of the way they delegate. While there is a very strong command and control structure, and important decisions are made by a small group of people at the top, the individual leaders in various areas at all levels of hierarchy have an enormous amount of latitude. That's the only way that any of this can work. Individual people are given very, very strong responsibility and authority within domains to make decisions. That's how you can have all of these disparate products.&nbsp;</p><p><a href="https://www.apple.com/in/leadership/craig-federighi/" rel="">Craig Federighi</a><span> is head of software at Apple. What does that mean? How can you be head of software? How many platforms do they have? iOS, iPadOS, WatchOS, VisionOS, MacOS. There's also an operating system running in a bunch of the little chips in the cables. All of that is under Craig. What does that mean? In practice, what it means is that there is a set of software concerns that he's super concerned with and he's thinking about day to day. When I was at Apple, I had Craig Federighi in my office talking about gesture-recognizer heuristics with me, because that was something that was hyper salient to him. At the same time, he was basically completely ignoring 95% of software-related decisions. He just fully delegated those things to others.&nbsp;</span></p><p>There's a really interesting Harvard Business Review piece from a few years back about Apple's management structure and how they have different concentric rings of responsibility for any given leader. I don't exactly remember the breakdown, but say there will be 5% of things that you're responsible for – that you have your hands on at all times and you are directly manipulating, controlling. There's a ring outside of that, that's a little bit bigger. Those are the things that you're keeping an eye on. They are salient to you, you're getting reports on them, you are checking in on them, you are thinking about them, you're coming up with ideas and sending them down the chain, but you're not directly controlling them. Then there's a bunch of stuff that you've figured out how to delegate and you want to hear if there's problems. They talk about how that structure's evolved over time. It's now been eight years since I've been at Apple so I'm sure it's practically unrecognizable to me.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 02:19:25</span></p><p><span>This is a question from Bazel Halperin on Twitter. “</span><em>Is the lack of spaced repetition adoption a market failure? Or is a lack of adoption efficient?</em><span>”&nbsp;</span></p><p><strong>Andy Matuschak</strong><span> 02:19:35</span></p><p>It's probably mostly efficient. In places where spaced repetition, as it stands without substantial novel cultural knowledge that's difficult to transmit and isolate, is valuable, we see a lot of space repetition usage. Among medical students who are highly motivated and have lots of reason to study, the material is shaped in a way that's highly amenable to spaced repetition usage, there's tons of spaced repetition usage. In fact, the med student Anki subreddit is bigger than the Anki subreddit.&nbsp;</p><p><span>Likewise, among language learners, spaced repetition in various forms is extremely common. Duolingo has spaced repetition integrated into it. Spaced repetition is naturally present in the process of immersion learning. Modern spaced repetition tools between the </span><a href="https://en.wikipedia.org/wiki/Leitner_system" rel="">Leitner’s Box</a><span> and Wozniak's </span><a href="https://www.supermemo.com/en" rel="">SuperMemo</a><span>, were both originally motivated by language learning. In language learning, there's a substantial market for spaced repetition. It could be used in a variety of more creative ways. For instance, Russell Simmons has pointed out to me that studying individual vocabulary words on flashcards often misses integrative opportunities. What you really want is to study lots of sentences, or possibly to build up towards that. Duolingo does something like that. People in spaced repetition for language learning subreddits mostly don't. Some of them do, it's complicated. There's edges of the market where you need early adopters to try things that have rough edges. And the early adopters sometimes get cut and bleed a little bit. That's why people aren't rushing into it.&nbsp;</span></p><p>As to why spaced repetition isn't widely used, for instance, to learn quantum physics, it's basically correctly priced. I can use spaced repetition to learn quantum physics a bit faster. It doesn't make it a fait accompli or anything like that. It's not like learning anatomy, where basically if you study the deck, you'll be done. You need some more stuff. I'm working on some of that stuff. You also need an incredible amount of very unusual knowledge that's largely tacit at the moment, in order to use it in that way. That's part of what motivated recording this other video is to show some of that in action. The fact that the market isn't acting on this thing that it can't really act on seems pretty appropriate.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 02:21:56</span></p><p>That's a good place to tie off that collaboration and this project. This is really interesting.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 02:22:04</span></p><p>Thank you so much.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 02:22:05</span></p><p>This is many hours of just insights and lots of food for thought.&nbsp;</p><p><strong>Andy Matuschak</strong><span> 02:22:07</span></p><p>Wonderful. Thank you.&nbsp;</p><p><strong>Dwarkesh Patel</strong><span> 02:22:08</span></p><p>Yeah, thanks for coming on.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI suspends ByteDance's account after it used GPT to train its own AI model (305 pts)]]></title>
            <link>https://www.theverge.com/2023/12/15/24003542/openai-suspends-bytedances-account-after-it-used-gpt-to-train-its-own-ai-model</link>
            <guid>38662160</guid>
            <pubDate>Sat, 16 Dec 2023 06:17:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/12/15/24003542/openai-suspends-bytedances-account-after-it-used-gpt-to-train-its-own-ai-model">https://www.theverge.com/2023/12/15/24003542/openai-suspends-bytedances-account-after-it-used-gpt-to-train-its-own-ai-model</a>, See on <a href="https://news.ycombinator.com/item?id=38662160">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>OpenAI suspends ByteDance’s account after it used GPT to train its own AI model.</p><p>In <a href="https://www.theverge.com/2023/12/15/24003151/bytedance-china-openai-microsoft-competitor-llm">today’s<em> </em>issue of <em>Command Line</em></a>, I reported that ByteDance has been violating the developer license of both Microsoft and OpenAI by using GPT-generated data to train its own, competing model in China.</p><p>After my report was published, OpenAI spokesperson Niko Felix sent the following statement confirming that ByteDance’s account has been suspended:</p><blockquote><p>All API customers must adhere to our usage policies to ensure that our technology is used for good. While ByteDance’s use of our API was minimal, we have suspended their account while we further investigate. If we discover that their usage doesn’t follow these policies, we will ask them to make necessary changes or terminate their account.</p></blockquote><p>As I reported, most of ByteDance’s GPT usage has been done through Microsoft’s Azure platform, not through OpenAI directly. I’ve asked Microsoft if it will follow OpenAI and suspend ByteDance’s access as well.</p><br></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The music player you wish you had in the early 2000s (103 pts)]]></title>
            <link>https://www.crowdsupply.com/cool-tech-zone/tangara</link>
            <guid>38661735</guid>
            <pubDate>Sat, 16 Dec 2023 04:31:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.crowdsupply.com/cool-tech-zone/tangara">https://www.crowdsupply.com/cool-tech-zone/tangara</a>, See on <a href="https://news.ycombinator.com/item?id=38661735">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
  

  
    
  <nav>
    
  </nav>

  

  
  

  































    
  



  
  




    
  <section>
      <p><strong>Tangara</strong> is a portable music player. It outputs high-quality sound through a 3.5-mm headphone jack, lasts a full day on a charge, and includes a processor that’s powerful enough to support any audio format you can throw at it. It’s also 100% open hardware running open-source software, which makes it easy to customize, repair, and upgrade. Tangara plays what you want to hear, however you want to hear it.</p>
<p>Listen to music, audio books, and podcasts on a purpose-built device with a tried-and-true form factor, a familiar user interface, and no interest in your data. Or tear it apart and put it back together again. By tweaking our current firmware, you can experiment with alternative user-interface patterns, new types of content, tracker-based music production, alarm-clock applications, and much more. Or you can design a new faceplate with a different kind of display panel, more physical buttons, speakers, jacks, or…a cherry-wood enclosure? Whatever turns your clickwheel.</p>









    
  <p><a href="https://www.crowdsupply.com/img/8f86/e4847379-447d-4b33-b5ad-7f3cce6c8f86/tangara-outside.jpg">
    <img alt="" src="https://www.crowdsupply.com/img/8f86/e4847379-447d-4b33-b5ad-7f3cce6c8f86/tangara-outside_jpg_md-xl.jpg" title="" width="1600">
    </a>

  </p>

<p>Tangara is great DIY platform for non-audio applications, as well. For example, the ESP32 module at its core is popular among those who enjoy exploring and learning about (other people’s) Wi-Fi and Bluetooth connections. Unlike most such platforms, however, it also gives you a full-color display, a battery, and a one-finger touch interface to work with.</p>
<h2 id="features-specifications">Features &amp; Specifications</h2><ul>
<li>ESP32-based, with a WM8523 DAC and an INA1620 amplifier</li>
<li>3.5-mm audio output with support for 200 mW at 32 ohms</li>
<li>Current support for 16-bit audio at 44.1 or 48 kHz (DAC maxes out at 24 bits and 192-kHz )</li>
<li>Bluetooth audio support (SBC codec only)</li>
<li>Firmware supports MP3, FLAC, Opus, and Vorbis codecs</li>
<li>USB Type-C charging and firmware updates. Data transfer (via SAMD21 + tinyusb) in development</li>
<li>2200-mAh battery with a standard, 3-pin JST connector</li>
<li>4-mA standby current and ~120 mA active current, depending on the headphones and the volume</li>
<li>1.8-inch, 160x128 full-color TFT display</li>
<li>Two hardware buttons and a flexible, capacitive 'clickwheel'</li>
<li>An ERM haptic motor</li>
<li>Uses a standard, SDXC card for storage. Available up to 2 TB</li>
<li>Firmware based on ESP-IDF, written in C++17</li>
<li>A pretty cool retro transparent enclosure</li>
</ul>









    
  
  

<h2 id="open-source">Open Source</h2><p>Our software, firmware, and hardware design files are available <a href="https://sr.ht/~jacqueline/tangara/">on sourcehut</a>.</p>
<h2 id="interested-in-this-project">Interested in This Project?</h2><p>You can sign up at the top of this page to be notified when the campaign launches and to receive other updates. We provide useful information only, and you can unsubscribe at any time.</p>









    
  <p><a href="https://www.crowdsupply.com/img/0809/c8e8171c-42a7-4933-9e9a-89e0aeec0809/tangara-transparent-screenon-vert-crop.jpg">
    <img alt="" src="https://www.crowdsupply.com/img/0809/c8e8171c-42a7-4933-9e9a-89e0aeec0809/tangara-transparent-screenon-vert-crop_jpg_md-xl.jpg" title="" width="1600">
    </a>

  </p>


  </section>

    
  
    

    
    <div id="press">
        <h2>In the Press</h2>

          <div>
              <p><a href="https://www.hackster.io/news/cool-tech-zone-s-tangara-is-the-open-hardware-2000s-aesthetic-answer-to-apple-s-ipod-classic-64d71e89fd31"><img alt="" height="75" src="https://www.crowdsupply.com/img/6266/01bdce69-d9a0-4fa1-976d-9a9c03776266/hackster-logo_png_press-logo.png" title="Hackster News" width="150"></a>
              </p>
              <div>
                <p><a href="https://www.hackster.io/news/cool-tech-zone-s-tangara-is-the-open-hardware-2000s-aesthetic-answer-to-apple-s-ipod-classic-64d71e89fd31">Hackster News</a></p>
                <p>"Proudly inspired by Apple's original iPad, this pocket-sized music playback device is fully open and hackable — and crowdfunding soon."</p>
              </div>
            </div>
          <div>
              <p><a href="https://www.youtube.com/watch?v=1K8rfw9n75k&amp;t=554s"><img alt="" height="75" src="https://www.crowdsupply.com/img/46d8/f60bd65f-9ec4-4486-a20d-b94d74f946d8/electromaker-logo-at-3x_jpg_press-logo.png" title="Electromaker" width="150"></a>
              </p>
              
            </div>


      </div>




    
    <div>
        <h2 id="team">About the Team</h2>
        
  <div>
    <p><img alt="" height="140" src="https://www.crowdsupply.com/img/da7c/51d66b2a-9ca8-45e8-ba9b-53e95dc4da7c/cool-tech-zone-logo-1.svg" title="Cool Tech Zone" width="200">
    </p>
    
  </div>

        
    

      </div>

    
  
    





  
    
  <div>
      <h6>Subscribe to the Crowd Supply newsletter, highlighting the latest creators and projects</h6>
      
    </div>

    
  

  

  
    
  





</div>]]></description>
        </item>
        <item>
            <title><![CDATA[CRDT Concepts: Causal Trees (166 pts)]]></title>
            <link>https://www.farley.ai/posts/causal</link>
            <guid>38661580</guid>
            <pubDate>Sat, 16 Dec 2023 03:55:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.farley.ai/posts/causal">https://www.farley.ai/posts/causal</a>, See on <a href="https://news.ycombinator.com/item?id=38661580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I recently came across an elegant CRDT design that is useful for text collaboration applications that I just had to write about. What follows is an exploration of Causal Trees and an introduction (or refresher) to several important distributed systems concepts.</p><p>CRDTs are a family of algorithms and datastructures that provide eventual consistency guarantees by following some simple mathematical properties. As their popularity has increased over the last decade, so too has their usage. CRDTS are commonly found in collaborative applications, where concurrent updates can be frequent, but they’re also used quite extensively in local network and peer-to-peer environments, due to the algorithms not requiring a central authority to reconcile inconsistencies. That last sentence is quite important, because it means that we can get distributed eventual consistency (things eventually converge to a consistent value among nodes) without complicated processes such as consensus. Don’t fret if you’re a fan of central authority though, <a href="https://www.figma.com/">Figma</a> successfully uses CRDTs server-side to handle the collaborative aspects of their product, as well as <a href="https://github.com/soundcloud/roshi">Soundcloud</a> and many others.</p><p>The CRDT I will be describing today is the Causal Tree (roughly as <a href="https://web.archive.org/web/20190505005829/http://www.st.ewi.tudelft.nl/victor/articles/ctre.pdf">described</a> by Victor Grishchenko). Which we will look at in the context of a simple text collaboration application. But before we get too far ahead of ourselves, have a play with the example below and see if you can get an intuitive sense of how the algorithm operates. Some things to keep an eye out for are the ids associated with each node (on hover), and also what happens when individual characters are deleted.</p><h3>Example 1</h3><p>Each client in the above example has their own local causal tree, the compiled value of which you can see written to their respective text inputs. The implementation we will be discussing is known as a state-based CRDT, or CvRDT, which means that we send the whole tree over the wire to clients instead of just the individual operations as they occur. As a client types, new nodes are added to their tree and sent over to peers who are interested in seeing their updates, when a client receives another client’s tree (which may or may not have new nodes) they merge with their own tree in a way that guarantees a consistent result. One way to think of merge is like a set join, if client 1 has the values {a, b, c} and client 2 has {d}, merging the clients in either order will always result in the same value. But before we go deeper on the merge operation let’s take a step back and see how the tree is structured.</p><h2>Tree structure</h2><p>In our toy example above, each node represents an individual character. You may have noticed that each node’s parent is the letter that directly precedes that node (this isn’t always the case). This ordering is one half of the equation that allows us to position nodes consistently. Take for example an alternative structure where each character has an array index position instead.</p><p>h<span>⁰ </span>e<span>¹ </span>l<span>²	</span>l<span>³	</span>o<span>⁴	</span></p><p>Now imagine that you went on to add an exclamation mark at the end of the string, but concurrently with that operation you merged changes from another client that added the string “Welcome and “ before “hello”. You’re now left with “Welco!me and hello”, which was not your original intention. Now, you _could_ bake in some logic to your application that adjusts your old index to account for the new changes, shifting the index of each character in “hello” by the amount of characters that were inserted, but that’s not the elegant algorithm that I promised. That’s something else known as <a href="https://en.wikipedia.org/wiki/Operational_transformation">Operational Transformation (OT)</a> that folks that like to complicate simple things use (Google Docs uses OT heavily).</p><p>The much simpler approach taken in causal trees is to simply associate a node with the node preceding it. If we do that, it doesn’t matter if the underlying tree changes, it still preserves our original intention. In essence we’re trading an absolute positioning approach for a relative one.</p><p>But how can individual nodes reference each other? If you hover over any of the nodes below you will notice that they have two identifiers, one timestamp and one entity id. A timestamp is a monotonically increasing integer value that gets incremented each time a node is added to a tree. Because wall clocks in distributed systems are unreliable to be used to order operations, we use this method combined with the entity id when necessary to create what’s called a total order of the nodes in our tree. Those familiar with <a href="https://en.wikipedia.org/wiki/Lamport_timestamp">Lamport timestamps</a> will recognise this approach. If a tree is isolated from any other trees and you type a sequence of characters, you will produce a tree similar to a linked list, with each node’s timestamp being 1 higher than its parent. If we get sent another client’s tree that we notice has nodes with higher timestamps we merge those nodes into ours by attaching them to their parents, that will either exist in our tree or be introduced by theirs. We then simply update our local tree’s timestamp to be the max of our current timestamp and the tree’s timestamp that we are merging with. Intuitively, this means that when we start adding nodes after the merge, the id represents how much context, or amount of nodes we have seen before making the decision to add that node, in other words, what ‘caused’ the new node.</p><p>Hover over the nodes in the example below and see if you can figure out what the final value will be, based on the ids alone. Click “Reveal” if you get stuck.</p><h3>Example 2</h3><p>What's illustrated here is that higher timestamps represent operations that happened with more context, or at a later point in the document's history, and as such they are traversed first when building the final value.</p><p>So what did the user type and in what order to create the tree above?</p><ul><li>They first typed “Causatrees”</li><li>Realising they misspelled the word they added an "l " after "Causa"</li></ul><p>Notice how there are two branches after the 6th node? it makes sense we first traverse the branch with the higher timestamp because it was added after the misspelling had occurred. But there's a few more cases we need to be aware of.</p><h2>Traversal</h2><p>Which brings us to an important topic, tree traversal and sibling trees. To produce the correct final value we need to traverse the tree in depth-first pre-order, but with special cases for sibling branches. Those special cases are:</p><ul><li>If a node has multiple children (sibling branches) traverse the branches ordered by timestamp descending order. We saw this in the previous example.</li><li>If branches have the same timestamp, traverse the branch that has the higher entity id first. What you order by here isn’t important but it is critical that we order in a way that is consistent. Ordering by entity id is just an example of that.</li></ul><p>I said at the start of the article that causal trees can be used in collaborative, networked environments, which means that it needs to handle concurrent updates, duplicate writes, partitioning between clients and all of the stuff we love to think about when it comes to distributed system design. So how does it do that?</p><p>When some tree (a) merges with another tree (b), the merge operation is essentially a diff and patch. What that means is we find all of the nodes from tree ‘a’ that aren’t in ‘b’ and add them to ‘a’. This operation is idempotent, which means we can perform it any number of times and the result will be the same, like inserting a new value to a set. Two other properties that are required for a CvRDT are commutativity and associativity. Commutativity ensures that the order of merging does not affect the final result, for instance, merging tree 'a' with 'b' yields the same result as merging 'b' with 'a'. Associativity allows for grouping merge operations in any order, so combining 'a' with 'b', and then with 'c', gives the same result as merging 'a' with the result of merging 'b' with 'c'. When each tree (or node) in a distributed system observes the same set of changes, even if in different orders, they will eventually converge to the same consistent state due to these properties. Let’s look at an example.</p><p>In the example below we have three clients, the first of which has written “I &lt;3” and then gone offline. Concurrently, after receiving the first client’s changes, client 2 and 3 type “ Pears” and “ Apples” respectively. We can see these concurrent changes represented by the sibling trees after the heart. Instead of having both changes interleaved to produce a jumbled concoction of applepear letter-soup, client 1 (and all clients after merging) is left with text that represents both changes. The ids at the sibling branch/fork are the same, which, again, shows what context each client had available to them when they made their change.</p><h3>Example 3</h3><p>This demonstration showcases how causal trees can resolve concurrent updates in a consistent manner. Whether you would want to do that instead of showing conflicts to the user for them to resolve themselves is up to the application.</p><h2>Deletion</h2><p>You may have noticed by now that if you delete a node, the value of that node changes to a 🪦. When a node is deleted we need to keep it around for consistency reasons, but the value is ignored when we create the final value from traversing the tree. This is a common technique found in distributed database design and CRDT designs known as tombstoning. I’ll leave it as an exercise to the reader to try to understand how consistency would be impacted if we removed the node completely from the tree on deletion.</p><h2>Conclusion</h2><p>And that’s a wrap! hopefully you’ve learnt something new about CRDTs! If you’re wondering about real-world implementations you will be happy to know that the ideas we talked about today are integral to the design of such popular general purpose CRDT libraries as <a href="https://github.com/automerge/automerge">Automerge</a> and <a href="https://github.com/yjs/yjs">Yjs</a>.</p><p>If you’re curious about tree size implications, optimizations, and things like node garbage collection, then please check the further reading list below, there’s some gems in there I think you will enjoy.</p><h2>Further reading</h2><ul><li><a href="https://josephg.com/blog/crdts-go-brrr/">CRDTs go brr (optimization exploration).</a></li><li><a href="https://web.archive.org/web/20190505005829/http://www.st.ewi.tudelft.nl/victor/articles/ctre.pdf">The original Causal Tree paper.</a></li><li><a href="http://archagon.net/blog/2018/03/24/data-laced-with-history/">Data laced with history (exploration of causal trees and optimizations).</a></li><li><a href="https://github.com/sno6/causal/blob/main/simpletree/simpletree.go">Educational Go implementation that I wrote.</a></li></ul><h2>Thanks</h2><p>Thanks to the folks at <a href="https://reactflow.dev/">React Flow</a> for letting me use a free Pro account to build the interactive demos.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Advancements in machine learning for machine learning (264 pts)]]></title>
            <link>https://blog.research.google/2023/12/advancements-in-machine-learning-for.html</link>
            <guid>38661296</guid>
            <pubDate>Sat, 16 Dec 2023 02:50:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2023/12/advancements-in-machine-learning-for.html">https://blog.research.google/2023/12/advancements-in-machine-learning-for.html</a>, See on <a href="https://news.ycombinator.com/item?id=38661296">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-6088118107306075362">
<p><span>Posted by Phitchaya Mangpo Phothilimthana, Staff Research Scientist, Google DeepMind, and Bryan Perozzi, Senior Staff Research Scientist, Google Research</span>

</p><p>
With the recent and accelerated advances in machine learning (ML), machines can <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">understand natural language</a>, <a href="https://blog.google/technology/ai/lamda/">engage in conversations</a>, <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview">draw images</a>, <a href="https://arxiv.org/abs/2210.02303">create videos</a> and more. Modern ML models are programmed and trained using ML programming frameworks, such as <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://github.com/google/jax">JAX</a>, <a href="https://pytorch.org/">PyTorch</a>, among many others. These libraries provide high-level instructions to ML practitioners, such as linear algebra operations (e.g., matrix multiplication, convolution, etc.) and neural network layers (e.g., <a href="https://keras.io/api/layers/convolution_layers/convolution2d/">2D convolution layers</a>, <a href="https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/">transformer layers</a>). Importantly, practitioners need not worry about how to make their models run efficiently on hardware because an ML framework will automatically optimize the user's model through an underlying <em>compiler</em>. The efficiency of the ML workload, thus, depends on how good the compiler is. A compiler typically relies on heuristics to solve complex optimization problems, often resulting in suboptimal performance.
</p> <p>
In this blog post, we present exciting advancements in ML for ML. In particular, we show how we use ML to improve efficiency of ML workloads! Prior works, both internal and external, have shown that we can use ML to improve performance of ML programs by selecting better ML compiler decisions. Although there exist a few datasets for program performance prediction, they target small sub-programs, such as basic blocks or kernels. We introduce “<a href="https://arxiv.org/abs/2308.13490">TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs</a>” (presented at <a href="https://nips.cc/Conferences/2023">NeurIPS 2023</a>), which we recently released to fuel more research in ML for program optimization. We hosted a <a href="https://www.kaggle.com/competitions/predict-ai-model-runtime/overview">Kaggle competition</a> on the dataset, which recently completed with 792 participants on 616 teams from 66 countries. Furthermore, in “<a href="https://arxiv.org/abs/2305.12322">Learning Large Graph Property Prediction via Graph Segment Training</a>”, we cover a novel method to scale <a href="https://arxiv.org/abs/2005.03675">graph neural network</a> (GNN) training to handle large programs represented as graphs. The technique both enables training arbitrarily large graphs on a device with limited memory capacity and improves generalization of the model.
</p>





<h2>ML compilers</h2>


<p>
ML compilers are software routines that convert user-written programs (here, mathematical instructions provided by libraries such as TensorFlow) to executables (instructions to execute on the actual hardware). An ML program can be represented as a computation graph, where a node represents a tensor operation (such as <a href="https://en.wikipedia.org/wiki/Matrix_multiplication">matrix multiplication</a>), and an edge represents a tensor flowing from one node to another. ML compilers have to solve many complex optimization problems, including <em>graph-level </em>and <em>kernel-level</em> optimizations. A graph-level optimization requires the context of the entire graph to make optimal decisions and transforms the entire graph accordingly. A kernel-level optimization transforms one kernel (a fused subgraph) at a time, independently of other kernels.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s1999/image6.png"><img data-original-height="465" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s16000/image6.png"></a></td></tr><tr><td>Important optimizations in ML compilers include graph-level and kernel-level optimizations.</td></tr></tbody></table>


<p>
To provide a concrete example, imagine a <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">matrix</a> (2D tensor):
</p>





<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s1999/image4.png"><img data-original-height="290" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s16000/image4.png"></a></p>



<p>
It can be stored in computer memory as [A B C a b c] or [A a B b C c], known as <a href="https://en.wikipedia.org/wiki/Row-_and_column-major_order">row- and column-major memory layout</a>, respectively. One important ML compiler optimization is to assign memory layouts to all intermediate tensors in the program. The figure below shows two different layout configurations for the same program. Let’s assume that on the left-hand side, the assigned layouts (in red) are the most efficient option for each individual operator. However, this layout configuration requires the compiler to insert a <em>copy</em> operation to transform the memory layout between the <em>add</em> and <em>convolution</em> operations. On the other hand, the right-hand side configuration might be less efficient for each individual operator, but it doesn’t require the additional memory transformation. The layout assignment optimization has to trade off between local computation efficiency and layout transformation overhead.
</p>





<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s1999/image3.png"><img data-original-height="343" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s16000/image3.png"></a></td></tr><tr><td>A node represents a tensor operator, annotated with its output tensor shape [<em>n<sub>0</sub></em>, <em>n<sub>1</sub></em>, ...], where <em>n<sub>i </sub></em>is the size of dimension <em>i</em>. Layout {<em>d<sub>0</sub></em>, <em>d<sub>1</sub></em>, ...} represents minor-to-major ordering in memory. Applied configurations are highlighted in red, and other valid configurations are highlighted in blue. A layout configuration specifies the layouts of inputs and outputs of influential operators (i.e., convolution and reshape). A copy operator is inserted when there is a layout mismatch.</td></tr></tbody></table>



<p>
If the compiler makes optimal choices, significant speedups can be made. For example, we have seen <a href="https://ieeexplore.ieee.org/document/9563030">up to a 32% speedup</a> when choosing an optimal layout configuration over the default compiler’s configuration in the <a href="https://www.tensorflow.org/xla">XLA</a> benchmark suite.
</p>




<h2>TpuGraphs dataset</h2>


<p>
Given the above, we aim to improve ML model efficiency by improving the ML compiler. Specifically, it can be very effective to equip the compiler<strong> </strong>with a <a href="https://arxiv.org/abs/2008.01040">learned cost model</a><strong> </strong>that takes in an input program and compiler configuration and then outputs the predicted runtime of the program. 
</p>
<p>
With this motivation, we <a href="https://arxiv.org/abs/2308.13490">release TpuGraphs</a>, a dataset for learning cost models for programs running on Google’s custom <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">Tensor Processing Units</a> (TPUs). The dataset targets two XLA compiler configurations: <em>layout</em> (generalization of row- and column-major ordering, from matrices, to higher dimension tensors) and <em>tiling</em> (configurations of tile sizes). We provide download instructions and starter code on the <a href="https://github.com/google-research-datasets/tpu_graphs">TpuGraphs GitHub</a>. Each example in the dataset contains a computational graph of an ML workload, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the dataset are collected from open-source ML programs, featuring popular model architectures, e.g., <a href="https://arxiv.org/abs/1512.03385">ResNet</a>, <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a>, <a href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a>, and <a href="https://arxiv.org/abs/1706.03762">Transformer</a>. The dataset provides 25× more graphs than the largest (earlier) graph property prediction dataset (with comparable graph sizes), and graph size is 770× larger on average compared to existing performance prediction datasets on ML programs. With this greatly expanded scale, for the first time we can explore the graph-level prediction task on large graphs, which is subject to challenges such as scalability, training efficiency, and model quality.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s2868/image18.png"><img data-original-height="1546" data-original-width="2868" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s16000/image18.png"></a></td></tr><tr><td>Scale of TpuGraphs compared to other graph property prediction datasets.</td></tr></tbody></table>




<p>
We provide baseline learned cost models with our dataset (architecture shown below). Our baseline models are based on a GNN since the input program is represented as a graph. Node features, shown in blue below, consist of two parts. The first part is an <em>opcode id</em>, the most important information of a node, which indicates the type of tensor operation. Our baseline models, thus, map an opcode id to an <em>opcode embedding</em> via an embedding lookup table. The opcode embedding is then concatenated with the second part, the rest of the node features, as inputs to a GNN. We combine the node embeddings produced by the GNN to create the fixed-size embedding of the graph using a simple graph pooling reduction (i.e., sum and mean). The resulting graph embedding is then linearly transformed into the final scalar output by a feedforward layer.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s2284/image20.png" imageanchor="1"><img data-original-height="1106" data-original-width="2284" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s16000/image20.png"></a></td></tr><tr><td>Our baseline learned cost model employs a GNN since programs can be naturally represented as graphs.</td></tr></tbody></table>




<p>
Furthermore we present <a href="https://arxiv.org/abs/2305.12322">Graph Segment Training</a> (GST), a method for scaling GNN training to handle large graphs on a device with limited memory capacity in cases where the prediction task is on the entire-graph (i.e., graph-level prediction). Unlike scaling training for node- or edge-level prediction, scaling for graph-level prediction is understudied but crucial to our domain, as computation graphs can contain hundreds of thousands of nodes. In a typical GNN training (“Full Graph Training”, on the left below), a GNN model is trained using an entire graph, meaning all nodes and edges of the graph are used to compute gradients. For large graphs, this might be computationally infeasible. In GST, each large graph is partitioned into smaller segments, and a random subset of segments is selected to update the model; embeddings for the remaining segments are produced without saving their intermediate activations (to avoid consuming memory). The embeddings of all segments are then combined to generate an embedding for the original large graph, which is then used for prediction. In addition, we introduce the historical embedding table to efficiently obtain graph segments’ embeddings and segment dropout to mitigate the staleness from historical embeddings. Together, our complete method speeds up the end-to-end training time by 3×.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s790/image2.png"><img data-original-height="434" data-original-width="790" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s16000/image2.png"></a></td></tr><tr><td>Comparing Full Graph Training (typical method) vs Graph Segment Training (our proposed method).</td></tr></tbody></table>




<h2>Kaggle competition</h2>


<p>
Finally, we ran the “<a href="https://kaggle.com/competitions/predict-ai-model-runtime">Fast or Slow? Predict AI Model Runtime</a>” competition over the TpuGraph dataset. This competition ended with 792 participants on 616 teams. We had 10507 submissions from 66 countries. For 153 users (including 47 in the top 100), this was their first competition. We learned many interesting new techniques employed by the participating teams, such as:
</p>
<ul>

<li><em>Graph pruning / compression</em>: Instead of using the GST method, many teams experimented with different ways to compress large graphs (e.g., keeping only subgraphs that include the configurable nodes and their immediate neighbors).

</li><li><em>Feature padding value</em>: Some teams observed that the default padding value of 0 is problematic because 0 clashes with a valid feature value, so using a padding value of -1 can improve the model accuracy significantly.

</li><li><em>Node features</em>: Some teams observed that additional node features (such as <a href="https://www.tensorflow.org/xla/operation_semantics#dot">dot general’s contracting dimensions</a>) are important. A few teams found that different encodings of node features also matter.

</li><li><em>Cross-configuration attention</em>: A winning team designed a simple layer that allows the model to explicitly "compare" configs against each other. This technique is shown to be much better than letting the model infer for each config individually. 
</li>
</ul>
<p>
We will debrief the competition and preview the winning solutions at the competition session at the <a href="https://mlforsystems.org/">ML for Systems workshop</a> at NeurIPS on December 16, 2023. Finally, congratulations to all the winners and thank you for your contributions to advancing research in ML for systems!
</p>





<h2>NeurIPS expo</h2>


<p>
If you are interested in more research about structured data and artificial intelligence, we hosted the NeurIPS Expo panel <a href="https://nips.cc/Expo/Conferences/2023/talk%20panel/78252">Graph Learning Meets Artificial Intelligence</a> on December 9, which covered advancing learned cost models and more! 
</p>




<h2>Acknowledgements</h2>


<p>
<em>Sami Abu-el-Haija (Google Research) contributed significantly to this work and write-up.   The research in this post describes joint work with many additional collaborators including Mike Burrows, Kaidi Cao, Bahare Fatemi, Jure Leskovec, Charith Mendis, Dustin Zelle, and Yanqi Zhou.</em>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bluetooth keystroke-injection in Android, Linux, macOS and iOS (276 pts)]]></title>
            <link>https://github.com/skysafe/reblog/tree/main/cve-2023-45866</link>
            <guid>38661182</guid>
            <pubDate>Sat, 16 Dec 2023 02:26:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/skysafe/reblog/tree/main/cve-2023-45866">https://github.com/skysafe/reblog/tree/main/cve-2023-45866</a>, See on <a href="https://news.ycombinator.com/item?id=38661182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:skysafe/reblog" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="IGY1jHWvj45J7PAhwmSm3IcdAgcoSkzoDHuOMXnnxEauLpqeRxlktc30r9E6XkyKEnfEhMXshXrYQTnMOd0Ehg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="skysafe/reblog" data-current-org="skysafe" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=toL9V8ml0zAn3gyLixuwanlGPvABX29m4y6axKT6ADVEjH%2BuLJR8xyvWLnEkNA4snWBUWa6l8P%2FpF%2FqMgUQABQ%2B3BHZBAA9lqAkIEbLtsrZLoxJPFFyV6uU4%2FiDs1h9p6S7O7W5ZDMdAnZc93EOuvBZcMn9YQoJJ0FGvBofnKfScuEUcupBsn3Q%2FIlxhy%2F4uPxsPpC5fFTA%2FhNXYWoWKslM3bsJq0dXRv5eAS9Yo%2FpF30ZqlMkC3%2FXeFZ035pO5xWQlmoDd3Kpm83zRwBGqh8v%2FBJaR1lbtCSlaO1pSho2Ap9oml%2BQazM0LtupFlArIPbYFZxvGaMwTBvdwE51c6mOt232n73aZh38jQ8NpXYTEw7SChMz0gdFR%2B8gWUGbRd%2FHkNlmXRw9Gf4OkrzJRIwVGbe6IKanPl8gTJITm3S5U1dIkH8j2pRVCbqupbP0HLQa4E5rJ%2F8grpMc1iPmuRHo%2FpDk8HXdm%2BXbaYxGhFBPy%2Fmbd%2Bg2lTlJhwMTkb6zpxwcUwWhSjy%2B3q6sZSgX%2F7VOTkR6ZxV0Qjqys1aCEf3ruShkX1MnH1ZubU--mPjgXi7ZKrurNwYN--QRIQ1I18v%2BMEZ6XhBYKZHA%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Ffiles%2Fdisambiguate&amp;source=header-repo&amp;source_repo=skysafe%2Freblog" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/skysafe/reblog/tree/main/cve-2023-45866&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="2a181ee65596245633167bff86efb2eedc190dbf33fef3e419c987eda2894bcf" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/files/disambiguate;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[City council passes bill enabling tenants to report vacant apartments (101 pts)]]></title>
            <link>https://www.thecity.nyc/2023/12/06/warehousing-vacant-apartments-report-council/</link>
            <guid>38660744</guid>
            <pubDate>Sat, 16 Dec 2023 00:54:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thecity.nyc/2023/12/06/warehousing-vacant-apartments-report-council/">https://www.thecity.nyc/2023/12/06/warehousing-vacant-apartments-report-council/</a>, See on <a href="https://news.ycombinator.com/item?id=38660744">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					

<article id="post-32604">
	<div>

		
		

<p>The City Council passed a bill Wednesday that enables tenants to report vacant apartments in their buildings to the city housing agency — with sponsors hoping to spur action on tens of thousands of empty units.</p>

<p><a href="https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=5555545&amp;GUID=8CEAD9C5-DE49-4876-8B73-83DC3467FEEB&amp;Options=ID%7CText%7C&amp;Search=195">Intro 195</a> allows tenants to report maintenance code issues to the Housing Preservation and Development (HPD) department via 311, and have city officials inspect vacant units when they may pose a hazard to those in units nearby. Tenants living among empty apartments have described trash, mold, open windows, leaky gas pipes and rodents as scourges in their buildings.&nbsp;</p>

<p>“We’re feeling very hopeful. We’re not used to getting this far,” says Illapa Sairitupac, a tenant organizer with Cooper Square Committee, about the bill. “I’m looking forward to tenants having more power in making sure HPD comes to inspect these units.”</p>


<p>A spokesperson for Mayor Eric Adams did not immediately respond to an inquiry about whether the mayor intends to sign the bill, which passed 39 to 8.</p>

<p>Tenants, advocates and elected officials rallied outside City Hall on Wednesday ahead of the Council vote, championing the bill and other measures to rein in “warehousing” — in which landlords keep apartments vacant in anticipation of future rent increases.</p>


<p>Landlords <a href="https://www.thecity.nyc/2022/10/19/60000-rent-stabilized-apartments-vacant-warehousing-nyc-landlords-housing/">reported more than 60,000 vacant rent-regulated apartments</a> to the state in 2021 — a figure that <a href="https://www.thecity.nyc/2022/11/17/rent-stabilized-vacant-apartments/">dipped to about 39,000</a> the following year, roughly the same as pre-pandemic levels.</p>


<p>Landlord groups don’t dispute that thousands of rent-regulated apartments are vacant, and blame 2019 changes to state rent laws that severely limit how much they can charge new tenants for after longtime renters move out.&nbsp;</p>

<p>“This bill will do absolutely nothing to address the current problems with empty rent-stabilized units, or improve renting conditions for tenants,” said Jay Martin, executive director of the Community Housing Improvement Program, an organization representing landlords of rent stabilized properties. “If HPD issues violations or writes fines on vacant units, it will just suck more money out of struggling rent-stabilized buildings.”</p>

<p>John Leyva lives at 63 Tiffany Pl. in Brooklyn, a building where expiring subsidies will permit the landlord to rent what had been affordable apartments at much higher rents — and several apartments are vacant. “The conditions they talk about, I know them,” says Leyva, citing mold due to a water leak.&nbsp;</p>


<figure><img decoding="async" width="780" height="520" src="https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2.jpg?resize=780%2C520&amp;ssl=1" alt="John Leyva holds a sign during a City Hall rally calling on landlords to stop warehousing units." srcset="https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2-scaled.jpg?resize=1024%2C683&amp;ssl=1 1024w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2-scaled.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2-scaled.jpg?resize=768%2C512&amp;ssl=1 768w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2-scaled.jpg?resize=1536%2C1024&amp;ssl=1 1536w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2-scaled.jpg?resize=2048%2C1365&amp;ssl=1 2048w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2-scaled.jpg?resize=1200%2C800&amp;ssl=1 1200w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2-scaled.jpg?resize=1568%2C1045&amp;ssl=1 1568w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2-scaled.jpg?resize=400%2C267&amp;ssl=1 400w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2-scaled.jpg?w=2340&amp;ssl=1 2340w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/09/120623_john_leyva_vacant_unit_bill_rally_2-1024x683.jpg?w=370&amp;ssl=1 370w" sizes="(max-width: 780px) 100vw, 780px" data-recalc-dims="1"><figcaption>John Leyva, a tenant at 63 Tiffany Pl. in Brooklyn, says his landlord has kept several apartments in his building vacant for nearly five years, Dec. 6, 2023. <span><span>Credit:</span> Sam Rabiyah/THE CITY</span></figcaption></figure>

<p>“It should burn their soul that there are families right now in homeless shelters when they could have been in an apartment for the last five years,” says Leyva. “The landlords, they just do whatever they want. But who protects the tenants? Because it’s not HPD.”</p>

<p>At a June 6 Council <a href="https://www.thecity.nyc/2023/06/06/housing-apartment-vacancies-rent-stabilized/">hearing</a>, HPD officials pushed back on the bill.&nbsp;</p>

<p>Intro 195 “would divert critical resources from HPD’s enforcement,” testified Assistant Commissioner for Housing Policy Lucy Joffe, remarking that the debate on apartment warehousing “has become a bit of a distraction in the press in the past few months.”&nbsp;</p>

<p>HPD presented data showing only about 2,500 empty rent-stabilized apartments were available for rent, had a legal rent below $1,000, were in need of repairs, and had been vacant for more than one year.&nbsp;</p>

<p>“Dismissing it outright and entirely, it felt offensive,” said Councilmember Carlina Rivera, (D-Manhattan) the prime sponsor of Intro 195, to THE CITY this week. “You have dozens and dozens of tenants continuing to speak up on this issue and organizing around the city.”</p>

<p>HPD now supports the bill and has “worked closely with the Council to ensure this legislation works in a way that puts New Yorkers first,” HPD spokesperson William Fowler told THE CITY.&nbsp;</p>

<p>The Council projects the law would require HPD to spend an additional $150,000 to create an appointment system allowing landlords to sign up for inspections of unoccupied apartments.&nbsp;</p>

<p>The new bill also empowers tenants to sue landlords to force owners to open up vacant apartments for inspection.&nbsp;</p>

<p>“That’s huge and very important to tenants,” says Jodie Leidecker, an organizer with the Coalition to End Apartment Warehousing.</p>

<p>The bill’s backers see tenant reports as a step toward quantifying the scale of warehousing.&nbsp;</p>

<p>“When calling 311, not only are you memorializing something, but the data can be tracked online,” said Rivera. “Having publicly accessible data is helpful to track trends. We know there are certain property owners who are keeping a lot of units vacant.”</p>

<p>While the Council also has a <a href="https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=5641451&amp;GUID=3C246C5D-B089-448E-AD19-47279E68BBE9&amp;Options=ID%7CText%7C&amp;Search=352">bill</a> pending that would require landlords to register empty apartments and stores, it has not moved forward to a vote.</p>

<p>Landlords have potential financial incentives to intentionally keep apartments vacant.&nbsp;</p>

<p>Until recently, landlords could legally combine two adjacent vacant apartments into one larger one — a process tenant groups call <a href="https://www.thecity.nyc/2022/11/16/rent-stabilized-frankenstein-loophole-landlords-hcr/">“frankensteining”</a> — and reset the legal rent to be higher than what they’d make from the rents of the two original units. <a href="https://citylimits.org/2023/10/25/ny-quietly-finalizes-housing-regulations-cheered-by-frankenstening-critics/">New state rules</a> aim to end that practice, as do<a href="https://citylimits.org/2023/06/21/bills-to-bolster-nys-rent-stabilization-law-head-to-governors-desk/"> bills passed by the state legislature</a> that sit on governor Kathy Hochul’s desk for her signature.&nbsp;&nbsp;</p>

<p>Meanwhile, <a href="https://www.thecity.nyc/2023/08/15/coliving-outpost-bedstuy-ridgewood-bushwick/">an investigation from THE CITY</a> highlighted widespread use in some neighborhoods of a provision allowing landlords to remove apartments from regulation once they’ve undergone “substantial rehabilitation” — encouraging owners to empty and renovate all apartments in a building.</p>

<p>“Warehousing as a whole has really permeated the public consciousness,” said Leidecker, citing <a href="https://www.thecity.nyc/category/in-depth/destabilized/">THE CITY’s reporting</a>. “We are in a major, historic housing crisis … People are down here fighting for crumbs and we need every unit of housing that’s available.”</p>

	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
</article><!-- #post-${ID} -->
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Location history data in Google Maps will soon be stored on user devices (147 pts)]]></title>
            <link>https://www.businessinsider.com/google-maps-location-data-history-stored-locally-2023-12</link>
            <guid>38660646</guid>
            <pubDate>Sat, 16 Dec 2023 00:39:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/google-maps-location-data-history-stored-locally-2023-12">https://www.businessinsider.com/google-maps-location-data-history-stored-locally-2023-12</a>, See on <a href="https://news.ycombinator.com/item?id=38660646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="piano-inline-content-wrapper" data-piano-inline-content-wrapper=""> 
                    
                    
                    
                          
                          
                          <section data-offer-key="pre-churn-offer" data-component-type="inline-offer" data-place-after-element-selector=".post-content .content-lock-content > p">
                            <article>
                              <img src="https://www.businessinsider.com/public/assets/subscription/marketing/banner-overlay/top-left.svg" alt="">
                              <img src="https://www.businessinsider.com/public/assets/subscription/marketing/banner-overlay/bottom-right.svg" alt="">
                          
                                        </article>
                          </section>
                    
                    <div data-component-type="content-lock" data-load-strategy="exclude">
                                  <ul><li>Location history data in <a target="_blank" href="https://www.businessinsider.com/google-maps-best-features-tips-tricks-2019-5" data-analytics-product-module="summary_bullets" rel="">Google Maps</a> will soon be stored directly on user devices.</li><li>Google itself will no longer have access to the data.</li><li>This also means law enforcement won't be able to request it from Google anymore.</li></ul><!-- Excluded mobile ad on desktop --><div id="formContainer" data-component-type="inline-newsletter-module" data-event-label="insider_today" data-newsletter-id="1" data-newsletter-title="Insider Today" data-acq-source="techinlinesignup">
                        
                        
                          <div>
                              <p>Thanks for signing up!</p>
                              
                              <p>
                              Access your favorite topics in a personalized feed while you're on the go.
                                    </p>
                            </div>
                        
                            <div>
                                <p><img src="https://www.businessinsider.com/public/assets/rebrand/newsletter-bull.png" data-old-src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1 1'%3E%3C/svg%3E" data-src="/public/assets/rebrand/newsletter-bull.png">
                              
                              
                              
                              </p>    </div>
                        
                          
                        </div><p>Google is making some changes in Google Maps that will increase user privacy.</p><p>Data from the Timeline feature in Google Maps, which is controlled by the Location History setting and keeps a record of routes and trips users have taken, will soon be stored directly on users' devices instead of by Google.</p><p>That means Google itself will no longer have access to user location history data. And by extension, neither will law enforcement, which has often <a target="_blank" href="https://www.businessinsider.com/police-getting-help-social-media-to-prosecute-people-seeking-abortions-2023-2" data-analytics-product-module="body_link" rel="">requested user location data from Google</a> — for example, through "geofence" orders, which request data about every user who was near a specific place at a specific time.</p><p><span></span><span><p>Google has come under increasing pressure to stop collecting user location data, especially since <a target="_blank" href="https://www.businessinsider.com/supreme-court-abortion-decision-final-ruling-scotus-roe-v-wade-2022-6#:~:text=The%20Supreme%20Court%20overturned%20the,Friday%2C%20while%20Republicans%20celebrated%20it." data-analytics-product-module="body_link" rel="">Roe v. Wade was overturned</a>. Location data, along with internet search history and even <a target="_blank" href="https://www.businessinsider.com/roe-abortion-surveillance-location-data-scotus-computer-search-history-2022-6" data-analytics-product-module="body_link" rel="">messaging history can be used as criminal evidence</a> against individuals who get an abortion in states where abortion is illegal.</p><p>42 Democrats from the US House and Senate signed a letter last May addressed to Google CEO Sundar Pichai urging the company to <a target="_blank" href="https://www.businessinsider.com/democrats-demand-google-stop-collecting-location-data-abortion-rights-2022-5" data-analytics-product-module="body_link" rel="">stop collecting and retaining user location information</a>.</p><p>"Google's current practice of collecting and retaining extensive records of cell phone location data will allow it to become a tool for far-right extremists looking to crack down on people seeking reproductive health care," the letter read.</p><p>Last July, Google announced it would <a target="_blank" href="https://www.businessinsider.com/google-says-delete-location-data-of-users-visiting-abortion-clinics-2022-7" data-analytics-product-module="body_link" rel="">delete the location history data of users who visited abortion clinics</a>, drug treatment centers, domestic violence shelters, weight loss clinics, and other sensitive health-related locations. The company said that if its systems identified that a user had visited one of these sensitive locations, it would then delete the entry from that user's location history "soon after they visit."</p><p>Now this control is back in the hands of individual users.</p><p>Google told Business Insider that the update is part of a larger effort by the company to increase user privacy and give individuals more control over their data, pointing to other tools like auto-delete and Incognito Mode. It says the response to the Location History update has been positive.</p><p>The Location History setting is turned off by default in Google Maps, but here's how to find it in the app, toggle it on or off, and delete specific entry:</p><p><strong>1) Click on the icon in the top right corner of the screen.</strong></p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/png" data-srcs="{&quot;https://i.insider.com/657c875f0ec98e92f75000a0&quot;:{&quot;contentType&quot;:&quot;image/png&quot;,&quot;aspectRatioW&quot;:1179,&quot;aspectRatioH&quot;:2556}}" alt="Google Maps" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Click on your user icon.
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Kylie Kirschner
                          
                          </span>
                              </span>
                          </figure><p><strong>2) Click on "Your data in Maps."</strong></p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/png" data-srcs="{&quot;https://i.insider.com/657c89237a3c8094d5dd8cf4&quot;:{&quot;contentType&quot;:&quot;image/png&quot;,&quot;aspectRatioW&quot;:1179,&quot;aspectRatioH&quot;:2556}}" alt="Google Maps" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Find "Your data in Maps."
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Kylie Kirschner
                          
                          </span>
                              </span>
                          </figure><p><strong>3) Scroll down to "Location History."</strong></p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/png" data-srcs="{&quot;https://i.insider.com/657c88c950edbc52a864cb77&quot;:{&quot;contentType&quot;:&quot;image/png&quot;,&quot;aspectRatioW&quot;:1179,&quot;aspectRatioH&quot;:2556}}" alt="Google maps" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Find "Location History."
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Kylie Kirschner
                          
                          </span>
                              </span>
                          </figure><p>Here you can turn the Timeline feature and your location history on or off, and change your backup and auto-delete settings.</p><p>Clicking on "See &amp; delete activity" will allow you to see any location history that's already been saved in Google Maps and give you the option to delete specific entries.</p></span></p><!-- Excluded mobile ad on desktop --><!-- Excluded mobile ad on desktop -->
                          
                        <!-- Excluded mobile ad on desktop -->
                          
                        
                          
                        
                      </div>
                    
                    
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New plans for self-hosted Zulip customers (118 pts)]]></title>
            <link>https://blog.zulip.com/2023/12/15/new-plans-for-self-hosted-customers/</link>
            <guid>38659529</guid>
            <pubDate>Fri, 15 Dec 2023 22:18:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.zulip.com/2023/12/15/new-plans-for-self-hosted-customers/">https://blog.zulip.com/2023/12/15/new-plans-for-self-hosted-customers/</a>, See on <a href="https://news.ycombinator.com/item?id=38659529">Hacker News</a></p>
<div id="readability-page-1" class="page"><section> <p><a href="https://zulip.com/">Zulip</a> is a 100% open-source team chat application designed
for efficient communication. Zulip is available as a
<a href="https://zulip.com/plans/">cloud service</a> or a
<a href="https://zulip.com/self-hosting/">self-hosted solution</a>. Organizations that
self-host Zulip can do so on their own (relying on
<a href="https://zulip.readthedocs.io/en/stable/production/index.html">extensive documentation</a>
and best-effort <a href="https://zulip.com/development-community/">community</a> support),
or purchase commercial support for their installation. You can learn more about
Zulip in
<a href="https://blog.zulip.com/2023/12/15/zulip-8-0-released/">today’s blog post</a>
announcing the 8.0 release.</p>
<h2 id="what-is-changing">What is changing</h2>
<p>Starting today, new self-hosted customers will no longer get unlimited free
access to
<a href="https://zulip.readthedocs.io/en/latest/production/mobile-push-notifications.html">Zulip’s Mobile Push Notification Service</a>.
Organizations with more than 10 users will need to sign up for a plan in order
to access the service. For current users of the service, unlimited free access
will end on <strong>February 15, 2024.</strong></p>
<p>We now offer two <a href="https://zulip.com/plans/#self-hosted">new plans</a> that include
unlimited access to mobile push notifications. The <strong>Business</strong> plan also
includes expert commercial support. The <strong>Community</strong> plan is free of charge,
and is
<a href="https://zulip.com/help/self-hosted-billing#free-community-plan">available</a> to
open-source projects, research groups, and communities.</p>
<p>As always, we remain fully <a href="https://zulip.com/values/">committed</a> to Zulip’s
100% open-source model. Organizations that do not require technical support or
services from us can freely install and use a complete version of Zulip. We are
not turning Zulip into a proprietary product with an “open core” demo version —
Zulip’s open-source software <em>is the product</em>.</p>
<h2 id="why-we-are-introducing-a-new-business-plan">Why we are introducing a new Business plan</h2>
<p>Our aim is to make signing up for the new Business plan be the norm for
businesses relying on self-hosted Zulip as their mission-critical communication
platform. This will generate revenue for the company that stewards and
financially supports Zulip’s development.</p>
<p>Zulip is proudly independent, with
<a href="https://zulip.com/values/#building-a-sustainable-business-aligned-with-our-values">no venture capital funding</a>,
which means that our revenue strongly impacts the pace of Zulip’s development.
In addition to supporting the project as a whole, we expect this change will let
us dedicate more resources to product improvements requested by Zulip’s
self-hosted customers.</p>
<p>We also believe the new Business plan will establish a better framework for our
relationship with self-hosted customers. Until now, purchasing commercial
support required a conversation with our sales team. In practice, this process
felt too heavy for most small- and medium-sized businesses, and they would
instead end up self-managing their Zulip installation.</p>
<p>Self-managing might be OK for a while, but an extended team chat outage can stop
work in its tracks; Slack incidents regularly trigger a
<a href="https://www.nytimes.com/2023/07/27/technology/slack-down.html">flurry</a>
<a href="https://www.forbes.com/sites/siladityaray/2023/07/27/office-messaging-app-slack-is-down-company-says-its-investigating/">of</a>
<a href="https://www.theverge.com/2023/8/17/23835971/slack-major-issues-outage-send-messages">media</a>
<a href="https://www.independent.co.uk/tech/slack-down-not-working-status-latest-b2425211.html">coverage</a>
about the disruption. This puts a big burden on system administrators who are
responsible for securing, maintaining, and upgrading a Zulip server on their
own.</p>
<p>Commercial support will now be included when organizations buy Business plan
access to
<a href="https://zulip.readthedocs.io/en/latest/production/mobile-push-notifications.html">Zulip’s Mobile Push Notification Service</a>.
Our support team can answer questions about installation and upgrades, provide
guidance in tricky situations, and help avoid painful complications before they
happen. We can also guide organizations on how best to use dozens of Zulip
features and configuration options.</p>
<h2 id="pricing">Pricing</h2>
<p>The Business plan is priced at $6.67/user/month with annual billing (or $8/month
billed monthly), which is the same price as
<a href="https://zulip.com/plans/">Zulip Cloud Standard</a>. This model lets organizations
budget for using Zulip without having to decide whether self-hosting or Zulip
Cloud will <a href="https://zulip.com/help/zulip-cloud-or-self-hosting">better suit</a>
their needs. On our side, the resources required to provide an excellent support
experience for a wide range of self-hosted configurations more than balance
Zulip Cloud hosting costs.</p>
<p>We offer a 30-day free trial of Zulip Business to make it convenient to run a
full-scale Zulip evaluation with your team, and never have to pay if you go with
a different chat product.</p>
<p>We created Zulip to empower teams to collaborate effectively, and are confident
that the <a href="https://zulip.com/why-zulip/">increased productivity from using Zulip</a>
makes our plans well worthwhile for organizations that pay people to get work
done.</p>
<p>As an example, the annual plan will pay for itself if an employee making
$65K/year saves just <em>3 minutes per week</em> [1] by using Zulip. Put another way,
the cost of the plan is <em>0.12%</em> of that employee’s direct compensation, not even
counting overhead.</p>
<blockquote>
<p>“In fact now it seems strange to me to just fire off messages in Slack with no
subject – that’s chaos, madness. The genius of subject lines is that you can
quickly and easily catch up on the messages you missed in your off-hours… This
feature alone saves me hours a week.”</p>
<p><span>—
<a href="https://www.theregister.com/2021/07/28/zulip_open_source_chat_collaboration_software/">Zulip review in <em>The Register</em></a></span></p>
</blockquote>
<p>In addition to providing a free Community plan, we offer
<a href="https://zulip.com/help/self-hosted-billing#business-plan-discounts">generous discounts</a>
for organizations whose circumstances make the Business plan pricing hard to
afford. Please reach out to <a href="mailto:sales@zulip.com">sales@zulip.com</a> if you
have any questions.</p>
<h2 id="why-a-mobile-push-notification-service-is-needed">Why a mobile push notification service is needed</h2>
<p>As Zulip’s maintainers, we publish the
<a href="https://github.com/zulip/zulip-mobile#readme">open-source</a> Zulip mobile apps
for <a href="https://zulip.com/apps/ios">iOS</a> and
<a href="https://zulip.com/apps/android">Android</a>, and offer a
<a href="https://zulip.readthedocs.io/en/latest/production/mobile-push-notifications.html">service</a>
that allows self-hosted Zulip servers to send mobile push notifications to their
users. Mobile push notifications must be centrally managed due to iOS and
Android
<a href="https://zulip.readthedocs.io/en/latest/production/mobile-push-notifications.html#why-a-push-notification-service-is-necessary">app store policies</a>.</p>
<p>We will continue to offer the
<a href="https://zulip.readthedocs.io/en/latest/production/mobile-push-notifications.html">Zulip’s Mobile Push Notification Service</a>
free of charge for organizations with up to 10 users, and for organizations
eligible for the Community plan.</p>
<h2 id="what-these-changes-mean-for-self-hosted-customers">What these changes mean for self-hosted customers</h2>
<p>If you are a current user of the Mobile Push Notification Service, you will
continue to have unlimited free access to the service until <strong>February
15, 2024.</strong> Starting today, you also can:</p>
<ul>
<li>
<p><strong>Do nothing.</strong> We remain <a href="https://zulip.com/values/">committed</a> to Zulip’s
100% open source model, so you do not have to purchase a plan in order to use
Zulip. We are not turning Zulip into a proprietary product with an “open core”
demo version — Zulip’s open-source software <em>is the product</em>. This option
doesn’t include support, and access to the Mobile Push Notification Service
will end on February 15, 2024 for organizations with more than 10 users.</p>
</li>
<li>
<p><strong>Apply for Zulip Community.</strong> The Community plan is free of charge, and
<a href="http://(details)/">available</a> to open-source projects, research groups, and
community organizations. It includes unlimited push notifications and
community support for many Zulip features.</p>
</li>
<li>
<p><strong>Upgrade to Zulip Business or Zulip Enterprise.</strong> Both plans include
unlimited access to the Mobile Push Notification Service, and commercial
support for dozens of features and integrations that help businesses take full
advantage of their Zulip implementation. Zulip Enterprise additionally
includes real-time support during installation and upgrades, technical support
for advanced deployment options, and more. Discounts are available for
education, non-profits, customers based in the developing world, and other
situations where the standard Zulip Business pricing would be a stretch for an
organization.</p>
</li>
<li>
<p><strong>Move to Zulip Cloud</strong>. The Business plan is priced the same as
<a href="https://zulip.com/plans/">Zulip Cloud Standard</a>, which we hope will help you
<a href="https://zulip.com/help/zulip-cloud-or-self-hosting">choose</a> between
self-hosting and Zulip Cloud based on what best suits your needs. If you would
like to switch to Zulip Cloud hosting, with its professionally managed, always
up-to-date experience, our high-quality
<a href="https://zulip.com/help/export-your-organization">export</a> tools make it easy
to move your organization.</p>
</li>
</ul>
<p>We’ve put a lot of care into designing a pricing model that we believe offers
great value compared to other team chat options on the market, and allows
organizations without an IT budget to continue using Zulip unchanged, for free.
If this announcement makes you consider migrating off Zulip, please
<a href="mailto:sales@zulip.com">get in touch with us</a> before you make your decision —
we may be able to help!</p>
<h3 id="next-steps">Next steps</h3>
<p>You can <a href="https://zulip.com/plans/#self-hosted">learn more</a> about the new plans,
and <a href="https://zulip.com/help/self-hosted-billing">read</a> detailed instructions on
how to sign up. When you upgrade to the Business plan, you can start the plan
right away (if you’d like support to start immediately), or schedule a February
15 start date.</p>
<p>Upgrading your Zulip server to version 8.0
(<a href="https://blog.zulip.com/2023/12/15/zulip-8-0-released/">released today</a>!) makes
it significantly more convenient to manage your plan, but you do <em>not</em> have to
upgrade your Zulip installation in order to sign up for a plan. The same plans
are offered for all Zulip versions.</p>
<p>If you wish to apply for the Community plan or a discounted Business plan, we
strongly recommend submitting your application by January 31 in case additional
information is required before your application is approved.</p>
<h3 id="let-us-know-if-we-can-help">Let us know if we can help</h3>
<p>We’d love to work with you to make the transition as smooth as possible for your
organization. If you have any questions or concerns about the next steps, the
timeline, which plan is appropriate for you, or anything else, send us a note at
<a href="mailto:sales@zulip.com">sales@zulip.com</a>!</p>
<p>[1] $80 annual Business plan cost / ($65,000 salary per year / 2400 work minutes
in a week) = 2.95 minutes</p> </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Prompt – A terminal for a container-oriented desktop (110 pts)]]></title>
            <link>https://blogs.gnome.org/chergert/2023/12/14/prompt/</link>
            <guid>38659430</guid>
            <pubDate>Fri, 15 Dec 2023 22:09:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.gnome.org/chergert/2023/12/14/prompt/">https://blogs.gnome.org/chergert/2023/12/14/prompt/</a>, See on <a href="https://news.ycombinator.com/item?id=38659430">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main">

		
<article id="post-10831">
	
	<!-- .entry-header -->

	<div>
		<p><a href="https://gitlab.gnome.org/chergert/prompt">Prompt</a> is a terminal that marries the best of <a href="https://flathub.org/apps/org.gnome.Builder">GNOME Builder</a>’s seamless container support, the beauty of <a href="https://flathub.org/apps/org.gnome.TextEditor">GNOME Text Editor</a>, and the robustness of VTE. I like to think of it as a <b>companion terminal to Builder</b>.</p>
<p>Though it’s also useful for immutable/container-oriented desktops like <a href="https://fedoraproject.org/silverblue/">Fedora&nbsp;Silverblue</a> or <a href="https://projectbluefin.io/">Project Bluefin</a> where containers are front-and-center.</p>
<p><a href="http://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-27-04.png"><img decoding="async" src="http://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-27-04-1024x831.png" alt="A screenshot of Prompt with a menu open showing a list of available containers to spawn a new terminal shell within." width="600" srcset="https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-27-04-1024x831.png 1024w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-27-04-300x243.png 300w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-27-04-768x623.png 768w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-27-04.png 1327w" sizes="(max-width: 1024px) 100vw, 1024px"></a></p>
<p>This came out of a prototype I made for the GNOME Builder IDE nearly a decade ago. We already had all the container abstractions so why not expose them as a Terminal Workspace?</p>
<p>Prompt extracts the container portion of the code-foundry into a standalone program.</p>
<p>My prototype didn’t go anywhere in recent years because I was conflicted. I have a high performance bar for software I ship and VTE wasn’t there yet on Wayland-based compositors which I use. But if you frequent this blog you already know that I reached out to the meticulous VTE maintainers and helped them pick the right battles to nearly double performance this GNOME cycle. I also ported <code>gnome-terminal</code> to GTK 4 which provided me ample opportunity to see where and how containers would integrate from an application perspective.</p>
<p>I designed Prompt to be Flatpak-first. That has design implications if you want a robust feature-set. Typically an application is restricted to the PID and PTY namespace within the Flatpak sandbox even if you’re capable of executing processes on the host. That means using TTY API like <code>tcgetpgrp()</code> becomes utterly useless when the kernel <code>ioctl()</code> returns you a PID of 0 (as it’s in a different namespace). Notably, 0 is the one value <code>tcgetpgrp()</code> is not documented to return. How fun!</p>
<p>To give Prompt the best chance at tracking containers and foreground processes a <code>prompt-agent</code> runs from the host system. It is restricted to very old versions of GLib/GObject/GIO and JSON-GLib because we know that <code>/usr/bin/flatpak</code> will already require them. Using those libraries instead of certain GLibc API helps us in situations where GLibc is only forward-compatible and not backwards-compatible. Combined with point-to-point D-Bus serialization on top of a <code>socketpair()</code> we have a robust way to pass file-descriptors between the agent and the UI process and we’ll use that a bunch.</p>
<p>There are a lot of little tricks in here to keep things fast and avoid energy-drain. For example, process tracking is done with a combination of exponential-backoff and re-triggering based on either new content arriving or certain key-presses. It gives a very low-latency feeling to the <code>sudo</code>/<code>ssh</code> feature I love from Console, albeit with less overhead.</p>
<p>One thing I find helpful with Builder is that when I come back to it my project session is right there. So this has session support too. It will restore your tabs/containers how they were before. So if you have a similar workflow you might find that useful. If not? Just turn it off in Preferences.</p>
<p>I want to have a bit of fun because right now I’m stuck indoors caring for my young, paraplegic dog. So it’s packed full of palettes you can play with. Who doesn’t like a little color!</p>
<p><a href="http://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-28-55.png"><img fetchpriority="high" decoding="async" src="http://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-28-55-1024x520.png" alt="A screenshot of Prompt with the preferences window open allowing the selection of numerous palettes with diverse color sets. The terminal window is recolored using colors from the palette." width="660" height="335" srcset="https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-28-55-1024x520.png 1024w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-28-55-300x152.png 300w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-28-55-768x390.png 768w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-28-55-1536x780.png 1536w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-12-28-55-2048x1040.png 2048w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>There are some subtle performance choices that make for a better experience in Prompt. For example, I do like having a small amount of padding around the terminal so that rounded corners look nice and also avoids an extra framebuffer when rendering on the GPU. However, that looks odd with scrollback. So Prompt rewrites the snapshot from VTE to remove the background and extraneous clipping. We already have a background from window recoloring anyway. It’s a small detail that just feels good when using it.</p>
<p>Another subtle improvement is detecting when we are entering the tab overview. Currently, libadwaita uses a <code>GtkWidgetPaintable</code> to represent the tab contents. This works great for the likes of Epiphany where the contents are backed by GL textures. But for a terminal application we have a lot of text and we don’t want to redraw it scaled as would happen in this case. That puts a lot of pressure on the glyph cache. So instead, we create a single texture upfront and scale that texture. Much smoother.</p>
<p>For people writing terminal applications there is a little inspector you can popup to help you out. It can be difficult to know if you’re doing the right thing or getting the right behavior so this might be something we can extend going forward to make that easier for you. GTK’s inspector already does so much so this is just an extension of what you could do there.</p>
<p><a href="http://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-14-46-16.png"><img decoding="async" src="http://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-14-46-16-1024x520.png" alt="A terminal window open with a secondary &quot;inspector&quot; window open. The inspector shows what column and row the mouse is positioned as well as the cursor and what non-visible OSC hyperlink is under the pointer." width="660" height="335" srcset="https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-14-46-16-1024x520.png 1024w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-14-46-16-300x152.png 300w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-14-46-16-768x390.png 768w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-14-46-16-1536x780.png 1536w, https://blogs.gnome.org/chergert/files/2023/12/Screenshot-from-2023-12-14-14-46-16-2048x1040.png 2048w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>Creating Prompt has elevated problems we should fix.</p>
<ul>
<li>Podman creates an additional PTY which sort of breaks the whole notion of foreground processes. <a href="https://github.com/containers/podman/issues/20767">Filed an issue upstream</a> and it seems likely we can get that addressed for personal containers. That will improve what happens when you close your terminal tab with something running or if you SSH’d into another host from the container.</li>
<li>Container tracking is currently limited to Fedora hosts because <code>toolbox</code> only emits the container-tracking escape sequences when the host is Fedora. The current goal I’ve discussed with VTE maintainers is that we’ll use a new “termprop” feature in VTE that will be silently dropped on terminal applications not interested in it. That way <code>toolbox</code> and the likes can safely emit the escape sequence.</li>
<li>Currently podman will exit if you pass a <code>--user</code> or <code>--workdir</code> that does not exist in the container. That isn’t a problem with <code>toolbox</code> as it is always your user and fails gracefully for directories. So we need a good strategy to see if both of those are available to inherit when creating new tabs.</li>
<li>This does have transparency support, but it’s hidden in a GSetting for now. Once we have libadwaita with CSS variable support we can probably make this look better during transitions which is where it falls down now. We also need some work on how <code>AdwTabOverview</code> does snapshots of tabs and inserts a background.</li>
<li>I have a <a href="https://gitlab.gnome.org/chergert/prompt/-/blob/main/README.md?ref_type=heads#jhbuild"><code>.bashrc</code> snippet</a> to treat <code>jhbuild</code> as a container which is helpful for those of us GNOME developers still using it.</li>
<li>Accessibility is an extremely important piece of our infrastructure in GNOME. So part of this work will inevitably tie into making sure the a11y portion of VTE works with the soon-to-land a11y improvements in GTK. That has always been missing on GTK 4-based VTE and therefore every terminal based upon it.</li>
</ul>
<pre>$ flatpak install --user --from <a href="https://nightly.gnome.org/repo/appstream/org.gnome.Prompt.Devel.flatpakref">https://nightly.gnome.org/repo/appstream/org.gnome.Prompt.Devel.flatpakref</a>
</pre>
<p>If you like software that I write, consider donating to a pet shelter near you this holiday season. We’re so lucky to have great pet care in Oregon but not everywhere is so lucky.</p>
<p>Happy Holiday Hacking!</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article><!-- #post-10831 -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Smallest USB-C MIDI Synth (246 pts)]]></title>
            <link>https://mitxela.com/projects/smsc</link>
            <guid>38658497</guid>
            <pubDate>Fri, 15 Dec 2023 20:36:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitxela.com/projects/smsc">https://mitxela.com/projects/smsc</a>, See on <a href="https://news.ycombinator.com/item?id=38658497">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mxmain"><p><a href="https://mitxela.com/projects/hardware"><img onload="this.style.opacity=1;" src="https://mitxela.com/img/titles/mitxela_dot_com-65.png" title="Back to Hardware" alt="Back to Hardware"></a></p><p>15 Dec 2023<br><b>Progress: Complete</b></p><p>
A new entrant in my series of "smallest and worst" MIDI synthesizers. (I'm not including the <a href="https://mitxela.com/projects/flash_synth">flash synth</a> in that list, which isn't supposed to be the worst!)</p><p>

Here's a video of the creation:</p><p>

<iframe width="704" height="396" src="https://www.youtube.com/embed/bmFmsn6VZSM" allowfullscreen=""></iframe></p><h3>Story</h3><p>
The last few weeks I've been dabbling around with the CH32V003, a 32-bit RISC-V microcontroller that's unbelievably cheap.</p><p>

One of the first things that occurred to me, when I noticed it didn't have hardware USB but the processor is clocked at 48MHz, is that it would be awesome to write a software USB stack for it. I have wanted, for a long time, to dig deep and write a bit-banged USB library, just because it's the best way to learn. I greatly enjoyed writing my <a href="https://mitxela.com/projects/kiloboot">ethernet bootloader in assembly</a>. It's hard to justify writing a USB stack from scratch when one already exists, however, so when I saw the CH32V003 I thought this was the perfect time to do something both educational and <i>useful</i>.</p><p>

Picture my surprise to find that CNLohr has <a href="https://github.com/cnlohr/rv003usb">already done it</a>! I can't exactly complain, that's a fantastic achievement and makes the chip even more useful and impressive than it already was.</p><p>

The very least I can do is get some USB-MIDI working on the chip. At the time of writing, the USB-MIDI demo was unfinished, so I tried it out by soldering a dev board together. It started out a little smaller, but by the end of the experimentation my board looked like this:</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/ch32v003-usb1.jpg" alt="Messy dev board creation"></p><p>

That's a TSOP20 breakout and a Micro-USB breakout, superglued together with some other scrap circuit board. A regulator, capacitors and a few resistors complete the circuit. The two header pins are my programming header (though with the USB plugged in, we don't even need to connect the ground pin, and could get away with just one pin to program).</p><p>

On the right is a set of buttons. I configured the USB-MIDI device to play notes when these buttons are pressed.</p><p>

At the bottom is a piezo buzzer. Naturally, when MIDI data arrives at the chip, it produces a square wave. I did this with one of the hardware timers of the chip, outputting in differential mode to maximise volume.</p><p>

USB MIDI messages are four bytes, and our low-speed USB endpoint can be eight bytes, so we could (and normally would) send two MIDI messages per packet. However for this simple demo I just blocked until the next USB interrupt for each message.</p><p>

A bit of MIDI loopback on the host side, and we have a really terrible toy keyboard!</p><h3>USB dev board</h3><p>
There are a few dev boards available for the CH32V003, but it doesn't look like any of them wire up the USB pins, probably because there's no hardware USB. I doubt this is the last USB project I'll do with it, so to avoid having to repeatedly wire up that mess above, a simple dev board is in order. I tried to make it as small as possible.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/ch32v003-usb3.png" alt="KiCad screenshot of dev board"></p><p>

All the necessary pins are broken out, all pins are labelled on both sides. The 1.5K resistor can be soldered in one of two positions, either to D5, or direct to VDD if you don't need software reconnection of USB and want to use D5 for something else. On the underside, the USB data lines can be cut and series resistors can be added, if needed.</p><p>

The three pins in the top right corner are 3V3, GND and D1, which is what I've been using as a programming header. You can either connect all three (to program it with USB unplugged), or D1 and ground, or just D1 if the programmer is on the same machine that the USB is plugged into.</p><p>

The pins around the edge are .1 inch pitch, and the board is 15.2mm by 20.3mm total.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/ch32v003-usb2.jpg" alt="Clean dev board creation"></p><p>

Here's the obligatory 3D model of the board:</p><p>


<model-viewer src="/img/uploads/sw/model-viewer/ch32v003usb.wrl" poster="/img/uploads/sw/model-viewer/ch32v003usb.jpg" loading="onclick">
  Sorry, your browser doesn't support embedded 3D models.
</model-viewer></p><p>

The KiCad design files for this board are published <a href="https://github.com/mitxela/ch32v003usb-hw">here</a> and <a href="https://git.mitxela.com/ch32v003usb-hw">here</a>.</p><h3>More USB, all the USB</h3><p>
Given how cheap the part is and how few supporting components it needs (the USB stack even does the same oscillator calibration frame timing trick that V-USB does), I thought it might be fun to recreate some of my USB ATtiny projects.</p><p>

The <a href="https://mitxela.com/projects/stylocard">stylocard</a> comes to mind first. I've done a few unpublished redesigns of that board in the past, and the best improvement was getting rid of the analog input method which was a little unreliable once the thing got dirty, and switching to direct readout, which means a microcontroller with at least 22 GPIO. One day I should publish all that. Unfortunately our CH32V003 doesn't have enough pins to read the keyboard and do USB together.</p><p>

We could just go with the same as before, a bunch of resistors and make the reading analog, or we could try and do something clever, or even just add a shift register, but it then occurred to me that since the CH32V003 is <i>so</i> cheap, why not just stick two of them on the board? It would be hilarious. One of them could read half the keyboard, the other would read the rest and also do USB.</p><p>

The possibilities are not unlimitless!</p><p>

Recreating my <a href="https://mitxela.com/projects/silly_synth">smallest USB MIDI synth</a> was the next thought. Something I've wanted to do for a while is produce a thin circuit board as they do for some dongles, sliding the thin circuit board inside of the USB-A plug, essentially creating the circuit I did before but in a way that can be mass-produced and easily assembled.</p><p>

But a more interesting idea was to bring the synth forwards through time into the age of USB-C. Electronically this just means adding a couple of resistors and fitting the right connector. You can't mount electronics inside a USB-C plug so easily though. I did find some mid-mount USB-C plugs which may have worked, but after a bit more searching I settled on this vertical mount type, intended for building docks.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/vertical-usb-c.jpg" alt="Vertical mount USB-C plug"></p><p>

The part number is USB4151 although there are a few similar parts from different suppliers.</p><p>

When USB-C was introduced, a lot of engineers complained about the difficulty of fanning out the connections. It seems the designers of the connector assumed that everyone would be using high density boards with microvias. The footprint alone of this connector is technically beyond the spec of a standard 6/6mil process, and that's before we've added any traces.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/usb-footprint0.jpg" alt="Footprint for the USB-C connector from the datasheet"></p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/usb-footprint.png" alt="Footprint for the USB-C connector in KiCad"></p><p>

The plastic studs require a non-plated through hole in very close proximity of a plated hole. For this joke project I'm not going to pay for tighter tolerances, so I decided to just ignore the DRC violation and if they aren't able to manufacture the holes I can trim off the plastic studs with a knife.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/vertical-usb-c2.jpg" alt="Vertical mount USB-C plug underside"></p><p>

The difficulty of fanout on a two layer board is such that special USB-C connectors are available, that don't break out all of the pins, if all you need is USB 2.0 or just power. However, they're not available in this vertical format, and besides I eat tricky routing problems for breakfast.</p><h3>Routing</h3><p>
With the vertical-mount USB-C plug, our ambition is to make the smallest possible circuit board underneath it, that can fit within the diameter of an ordinary piezo buzzer. The buzzer has a pin pitch of 7.62mm, or 0.3 inches, and the outer diameter is 13.8mm, but we want our circuit to fit inside the depression, that meniscus of the potting compound, which means a maximum diameter of about 12mm.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/piezo.jpg" alt="Piezo buzzer"></p><p>

There's no possible orientation where the piezo's pins don't foul the USB support pins. To deal with this, I widened the footprint spacing. It should be fine to bend the pins outwards a little, but if it doesn't fit we can file them down too. As the design iterated, I reshuffled this a few times, eventually I got them down to just 8mm apart.</p><p>

We don't need to connect the USB 3 pins, that's the four superspeed pairs and the two SBU pins, but we do need to wire up CC1 and CC2, which totals 14 pins to connect. Keeping the copper annulus around each plated hole as small as possible, it's <i>just</i> possible to have all the necessary tracks escape.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/routing1.png" alt="KiCad screenshot"></p><p>

Naturally the tracks are rounded with teardrops, because I have standards to live up to.</p><p>

As the shielding pins are all connected, we could cheat and not connect the grounds together on the board, but in the end it was fine to route these all together too.</p><p>

On the underside, routing is just as tight, with the QFN part shoved off-centre to make enough room for the tracks. Thankfully it doesn't hurt if we connect unused GPIO to ground (or to other signals really), so we can conveniently route ground right through the middle of the chip instead of going around.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/routing2.png" alt="KiCad screenshot"></p><p>

The regulator is an SC-70 package, that's like the miniature version of SOT-23. You can get even smaller regulators but it didn't seem like it would be an issue. Similarly, around the periphery I've used resistors and capacitors in 0603 format, just because there's no real pressure for space once we're outside of the piezo/USB/QFN footprint mess.</p><p>

On the front side I put three test pads, for power, ground and D1 (SWIO) for programming. In reality only one pad is needed, I'm just going to plug the USB-C in with an extension cable and touch a single wire to the programming pad.</p><h3>Panelization</h3><p>
I wanted to do the panel myself for three reasons.</p><p>

When you get boards made that are this small, and you plan to stencil solder paste onto them, it's extremely fiddly to hold things if you don't have a frame. A 12mm circle would be very tedious to hold at the best of times, but here we have components on both sides so after one side is soldered it'll be almost impossible to stencil the other.</p><p>

Secondly, I specifically wanted the panel to have explicit symmetry. We use the frame and the mounting holes to align the stencil. To save on the tedium of doing this twice, I put down two copies of the design, with the second flipped over. The whole panel is symmetric, so we can stencil one side, flip the board and stencil the other.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/routing3.jpg" alt="KiCad screenshot of panel"></p><p>

The third reason to do the panel myself is that I wanted to also use the frame as a jig. There's an oval hole in the middle designed to be a tight fit around a USB-C plug. Once the board is part-soldered and broken out of the panel, it's going to be a real pain to do anything to it, so this at least should give us a basic grip on the thing. We could have made a jig ourselves out of something else, but there aren't many materials that can be laser-cut and would survive the reflow oven. In a sense, FR4 is the perfect support material.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/render2.jpg" alt="Render of panel with piezos hidden"></p><p>

I should take this moment to praise just how useful the 3D model viewer is. I can remember once having a bizarre argument with a contractor who was a little old-school and failed to see the benefit of it. He'd grown up with OrCAD in the 90s and insisted that setting up 3D models for part footprints was a waste of time, or at the very least, not his job. 3D modelling is for the mechanical engineer, he kept saying.</p><p>

But being able to look at a realistic render provides such a huge safety buffer against silly mistakes. In the old days, we used to spend hours poring over gerber files looking for common mistakes because if you made one, it could set everything back by weeks. And they happened all the time! Things like missing the soldermask aperture on a footprint, or exporting shapes onto the wrong layers. Ever since we shifted to KiCad and made full use of the 3D viewer, I don't think I've ever made one of those mistakes. I still check the gerber files religiously, but the 3D viewer is a second layer of defence against mistakes.</p><p>

In the render below, I've highlighted the piezo, and you can immediately see that the pins don't quite line up with the footprint. This is because I've intentionally altered the footprint in the hope that we can bend those pins outward a little, as mentioned above. But it's exactly the kind of thing that the 3D viewer can help you with, to get a visual on the interference and whether it looks like it's going to work. Or at the very least, it might make you go back and check the 3D model is correct.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/render1.jpg" alt="Render of panel, with piezo highlighted in green"></p><p>

Here's an interactive 3D model if you're especially keen:</p><p>

<model-viewer src="/img/uploads/sillysynth/c/sillysynth2.wrl" poster="/img/uploads/sillysynth/c/render3.jpg" loading="onclick">
  Sorry, your browser doesn't support embedded 3D models.
</model-viewer></p><h3>Assembly</h3><p>
If you get your boards made at the lowest tolerances and they're below a certain size, manufacturers will subsidise the price. It's essentially a promotional deal, they charge you almost nothing because it costs them almost nothing to chuck tiny boards into the corners and crooks of other panels. I wonder how many other people have tried to produce a board with a (nominally) USB 3.2 Gen 2 connector on a two layer, 6/6mil tolerance board.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/bareboard.jpg" alt="Bare board in hand"></p><p>

They didn't question my footprint at all, and it seems to have been produced without problems.</p><p>

The correct order of assembly is to do the tiny parts first, and the USB connector last. The through-mount aspect of the USB connector means it's not possible to stencil the other side once it's fitted.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly1.jpg" alt="One board assembled still in the panel"></p><p>

The USB connector comes with a plastic cap, to allow you to pick it up with a vacuum nozzle.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly2.jpg" alt="Close up of USB-C connector on board"></p><p>

I had planned to reflow a bunch of these, but it's so small I ended up doing all of them with the heat gun. It's possible (and not that unusual) to reflow a board with components underneath already soldered. Even if the solder melts, surface tension holds them in place. It's also possible to use two different alloys of solder with different melting points if it's a concern. But hitting it with the heatgun it's easy enough to direct the heat only to where needed.</p><p>

If I had reflowed it though, the plan was to stencil and place components on both sides, then reflow the whole thing at once. Something like the following picture:</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly3.jpg" alt="Mockup of how the panel could be used to support the board"></p><p>

The connector itself would poke through the grating that makes up the bed of the reflow oven.</p><p>

Anyway I didn't do that, I just soldered them all in place as it was way less tricky than I'd imagined.
 
<img src="https://mitxela.com/img/uploads/sillysynth/c/production-line.jpg" alt="Two boards being assembled"></p><p>

Carefully snap them out of the panel and file the rough edges a little.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly4.jpg" alt="Three assembled boards"></p><p>

After assembling them, I did wonder if perhaps I should have gone with smaller capacitors after all. They're the tallest single components, and smaller caps are easily available. Oops, too late now.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly5.jpg" alt="Close up of the capacitors, board held between fingers"></p><p>

As expected, the buzzer pins were a tight fit, but there was enough play to jam them into place and get the board flush. I then trimmed them to length and delicately soldered the stubs.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/finished1.jpg" alt="Fully assembled"></p><p>

Comically I waited until this moment to realise I didn't have enough piezo buzzers in stock. I ordered some more and the new ones are a minutely different design, which was inevitable. Never mind.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/finished2.jpg" alt="Fully assembled"></p><p>

That vertical-mount USB-C connector was designed to go inside a dock for a phone or tablet. It's supposed to poke through a moulded plastic case, which means it's a little longer than it needed to be. I did have a think about 3D printing a little plastic cover to go over the circuit board and the lowest part of the connector, but I doubt it would look very good. We wouldn't want to cover that mitxela logo anyway.</p><p>

USB-C extension cables are technically against the spec, but that doesn't mean you can't buy them and all kinds of other nonsensical cables and connectors. I have one that only works in certain orientations, which is just so distressing and the opposite of what USB-C was supposed to be, but it'll do to give us power while I poke that SWIO pin with a probe. I flashed the four different synths with different device names, which helps us differentiate them in a DAW. And by DAW I mean the 1998 edition of Cakewalk running in wine.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/foursynths1.jpg" alt="Four silly USB-C synths"></p><p>

I then went out of my way to buy a four-port USB-C hub. Surprisingly difficult to find, most of them turn USB-C into various more helpful connectors like USB-A, HDMI, SD card, and so on. </p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/foursynths2.jpg" alt="Four silly USB-C synths"></p><p>

I have a bunch of the PCBs left, maybe I should make another handful and get even more hubs?</p><p>

Interestingly the design works with the hub, and it works if I plug it into my phone, but it doesn't enumerate if I plug it straight into my laptop. But it does enumerate on the end of that noncompliant USB-C extension cable I've concocted. It's entirely possible I've wired up the USB-C port marginally wrong, or perhaps the resistors are not exactly the right value – the type of USB connection is determined by the strength of some of the pull resistors. Either way I don't think I care enough about this comedy synth to look into it much further. It's just something to keep in mind for the next USB-C device.</p><div><p>

I have put the source code for this project in the <a href="https://github.com/mitxela/smsc">usual</a> <a href="https://git.mitxela.com/smsc">places</a>.</p></div><nav>
<a href="https://mitxela.com/projects/random" title="random project">~</a>
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/">mitxela.com</a></span> » 
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects">Projects</a></span> » 
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects/hardware">Hardware</a></span> »
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects/smsc">Smallest USB-C MIDI Synth</a></span>
<p>Questions? Comments? Check out the <a href="https://mitxela.com/forum">Forum</a>
</p><p><a href="https://mitxela.com/support">Support mitxela.com</a>
</p></nav></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leave work slightly unfinished for easier flow the next day (214 pts)]]></title>
            <link>https://read.engineerscodex.com/p/simple-software-engineering-habits</link>
            <guid>38658262</guid>
            <pubDate>Fri, 15 Dec 2023 20:13:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://read.engineerscodex.com/p/simple-software-engineering-habits">https://read.engineerscodex.com/p/simple-software-engineering-habits</a>, See on <a href="https://news.ycombinator.com/item?id=38658262">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>“Your outcomes are a lagging indicator of your habits.” - James Clear</em></p><p>As I became a better software engineer, I noticed 4 key habits in my daily workflow that had made me much more productive.</p><blockquote><p><em><span>Friendly plug: </span><strong><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> is a compilation of 450+ software engineering and system design questions covering databases, authentication, caching, etc. They’ve been vetted by engineers from Google, Meta, Apple, and more.</span></strong></em></p><p><em><span>I’m a core contributor to </span><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> and it’s helped many of my peers (including myself) pass the “software trivia questions” during interviews and feel more confident at work.</span></em></p></blockquote><p data-attrs="{&quot;url&quot;:&quot;https://swequiz.com&quot;,&quot;text&quot;:&quot;Check it out&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://swequiz.com/" rel=""><span>Check it out</span></a></p><p>“Flow” is the root of productivity when programming.&nbsp;</p><p><span>Since software engineering is a </span><a href="http://www.paulgraham.com/makersschedule.html" rel="">“maker” activity</a><span> where I’m producing something, I generally perform best when I have a large block of uninterrupted “flow” time to work on a project.</span></p><p>However, it can often be really hard to get into flow if you’re stuck scrambling on what tasks your project goals entail. Ambiguity is difficult to deal with. Not even knowing where to start can make reaching that “flow state” much harder.</p><p>Each successful action snowballs into more.</p><p>There are a few techniques I use to do this:</p><ul><li><p><strong>Stop right before a “sticking point.”</strong><span> A sticking point is a task that’s part of a project where I know the steps to do to complete it, but I don’t know if there are hidden costs.&nbsp;</span></p><ul><li><p>For example, if my sticking point is deploying my ML model and HTTP server to a dev instance and verifying that it processes requests properly, then the hidden costs are deployment errors, authentication errors, resource constraints, etc.</p></li></ul></li><li><p><strong>Write down the next steps extremely clearly.</strong><span> Writing down steps makes regaining context and the state of mind from the day before easier.</span></p><ul><li><p><strong>Make them actionable and unambiguous.</strong></p></li></ul></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png" width="1456" height="572" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:572,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:65283,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>My first experience with a real “shortcut ninja” was actually not with a software engineer. It was with my investment banker friend, who sped around his Excel sheets without ever touching the mouse.&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png" width="516" height="516" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:516,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>This, except I finally did appreciate it years later. Source: </span><a href="https://workchronicles.com/keyboard-shortcuts/" rel="">Work Chronicles</a></figcaption></figure></div><p>After that, I took the time to learn keyboard shortcuts, to the point where I grab my mouse ~60% less than I used to. (Yes, I tracked this.)</p><p>Every editor and tool in my workflow has keyboard shortcuts for pretty much any action you can think of. This doesn’t just apply to your IDE, but also your version control systems, your web browser, and your docs.</p><blockquote><p>For example, my IDE has a linter/formatter/cleaner all in one shortcut, which I use often as I write code to make sure lines stay neat.</p><p>I commonly use Command/Ctrl + Shift + V to paste in text without formatting in docs and chats.</p><p><span>Pressing </span><code>.</code><span> (period) on a GitHub repo page will automatically open up the repo in a VSCode Web instance.</span></p></blockquote><p>When I do need to touch my mouse, it’s configured with shortcuts also. I’m lucky enough to have a mouse with buttons on the sides. I’ve programmed these buttons to switch between Spaces on my Mac, though you can program them for whatever feels intuitive for you. (You can even program them to be different per program.)</p><p><span>The best way to learn shortcuts? </span><strong>Introduce the most common parts of a program that you use, one a time.</strong><span> </span></p><p>For example, if you find yourself right-clicking to format your code often, that can be the first one you “practice.” Every so often, add a new shortcut to your repertoire and use it naturally as you code throughout the weeks. Over time, the shortcuts will be muscle memory.</p><p>I commonly have to run a set of common commands on my terminal.</p><p>I have certain pages that I always visit and some notes about various languages that I always come back to. For example, I simultaneously use templates too rarely and yet too often when writing C++, meaning I usually need to reference the docs when using them.</p><p><span>Instead of digging around documentation pages or constantly looking through my terminal history, I keep commands and common doc lookups in a giant doc with one word describing the command. I call it my </span><em>Big Book of Commands</em><span>, which is around ~10 pages long now. I’m easily able to find any command I need with a quick Ctrl+F. Then, a Shift + Command + ➡️ is a full line-select for an easy copy-paste.</span></p><p>I also have a few common macros programmed into my keyboard for the commands and terms, like hard-to-remember ACL groups. Sometimes, I utilize Terminal aliases.</p><p><span>My friend </span></p><p><span> wrote a great article that dives into his own workflow tips, which starts off with a great primer into aliases, keyboard shortcuts, and tools: </span><a href="https://careercutler.substack.com/p/the-top-7-software-engineering-workflow" rel="">The top 7 software engineering workflow tips I wish I knew earlier 🧰</a></p><p>This is less directly programming related, but I learned to say “no” to things. </p><blockquote><p>I said no to novel technology when boring technology would do the job. </p><p>I said no to automating something when it only needed to be done once manually. </p><p>I said no to more tasks when I knew I was already overloaded with work (even though my people-pleasing mind pleaded to take them on). </p><p>I said no to scope creep suggested by our designers. </p><p>I said no to low-impact tasks.</p></blockquote><p>Learning to say no was harder than I expected, yet one of the most valuable skills I’ve applied in both the workplace and in my personal life.</p><p>Sometimes, it’s painful to say no to things. In both my career, hobbies, and personal life, there are times I say no to things I really want to do. But I don’t because I know my time and energy is better spent on what I’m currently focusing on.</p><p>It’s a cliche to quote Steve Jobs, but I remind myself of his famous quote “focus is about saying no” often.</p><p><span>My friend </span></p><p><span> also has a fantastic article about saying no, which I highly recommend: </span><a href="https://www.thecaringtechie.com/p/software-eng-guide-to-saying-no" rel="">The Software Engineer's guide to saying "no"</a></p><p><em>When you’re ready, here's how I can help:</em></p><p><em><strong><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> is a compilation of 450+ software engineering and system design questions covering databases, authentication, caching, etc. They’ve been vetted by engineers from Google, Meta, Apple, and more.</span></strong></em></p><p><em><span>I’m a core contributor to </span><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> and it’s helped many of my peers (including myself) pass the “software trivia questions” during interviews and feel 10x more confident at work.</span></em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Suspects can refuse to provide phone passcodes to police, court rules (519 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/12/suspects-can-refuse-to-provide-phone-passcodes-to-police-court-rules/</link>
            <guid>38657577</guid>
            <pubDate>Fri, 15 Dec 2023 19:16:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/12/suspects-can-refuse-to-provide-phone-passcodes-to-police-court-rules/">https://arstechnica.com/tech-policy/2023/12/suspects-can-refuse-to-provide-phone-passcodes-to-police-court-rules/</a>, See on <a href="https://news.ycombinator.com/item?id=38657577">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/phone-passcode-800x533.jpeg" alt="A person's hand holding a smartphone while entering the screen-lock passcode.">
      <figcaption><p>Getty Images | releon8211</p></figcaption>  </figure>

  




<!-- cache miss 88:single/related:5898e440fb979d41242bf9b8a83f0ba0 --><!-- empty -->
<p>Criminal suspects can refuse to provide phone passcodes to police under the US Constitution's Fifth Amendment privilege against self-incrimination, according to a unanimous <a href="https://legacy.utcourts.gov/opinions/supopin/State%20v.%20Valdez20231214.pdf">ruling issued today</a> by Utah's state Supreme Court. The questions addressed in the ruling could eventually be taken up by the US Supreme Court, whether through review of this case or a similar one.</p>
<p>The case involves Alfonso Valdez, who was arrested for kidnapping and assaulting his ex-girlfriend. Police officers obtained a search warrant for the contents of Valdez's phone but couldn't crack his passcode.</p>
<p>Valdez refused to provide his passcode to a police detective. At his trial, the state "elicited testimony from the detective about Valdez's refusal to provide his passcode when asked," today's ruling said. "And during closing arguments, the State argued in rebuttal that Valdez's refusal and the resulting lack of evidence from his cell phone undermined the veracity of one of his defenses. The jury convicted Valdez."</p>
<p>A court of appeals reversed the conviction, agreeing "with Valdez that he had a right under the Fifth Amendment to the United States Constitution to refuse to provide his passcode, and that the State violated that right when it used his refusal against him at trial." The Utah Supreme Court affirmed the court of appeals ruling.<br>
                                            </p>
                                                        
<h2>Case possibly ripe for Supreme Court review</h2>
<p>The ruling offered some commentary on the developing legal questions about device passcodes:</p>
<blockquote><p>The prevalence of passcodes that encrypt the information on electronic devices—which are often seized by law enforcement while investigating criminal conduct—has raised important questions about how the Fifth Amendment extends to law enforcement's efforts to unlock these devices and decrypt the contents inside. These questions have proven to be especially complex where law enforcement attempts to access the contents of a seized device by means that do not require the suspect to disclose the actual passcode—like, for example, obtaining an order to compel the suspect to provide an unlocked device.</p></blockquote>
<p>The Valdez case does not involve an order to compel a suspect to unlock a device. Instead, "law enforcement asked Valdez to verbally provide his passcode," Utah justices wrote. "While these circumstances involve modern technology in a scenario that the Supreme Court has not yet addressed, we conclude that these facts present a more straightforward question that is answered by settled Fifth Amendment principles."</p>
<p>Ruling against the state, the Utah Supreme Court said it "agree[s] with the court of appeals that verbally providing a cell phone passcode is a testimonial communication under the Fifth Amendment."</p>
<p>Berkeley Law Professor Orin Kerr <a href="https://reason.com/volokh/2023/12/14/is-compelled-decryption-heading-to-the-supreme-court/">wrote today</a> that the case could head to the US Supreme Court. "One of the major issues in the law of digital evidence investigations is how the Fifth Amendment privilege against self-incrimination applies to unlocking phones," Kerr wrote.</p>
<p>So far, "the lower court case law is a total mess," according to Kerr. "No one can say what the law is. And I've been waiting for a case to come down that might be a good candidate for US Supreme Court review to clear up the mess."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[McDonald's ice cream machine hackers say they found 'smoking gun' (195 pts)]]></title>
            <link>https://www.wired.com/story/kytch-taylor-mcdonalds-ice-cream-machine-smoking-gun/</link>
            <guid>38657192</guid>
            <pubDate>Fri, 15 Dec 2023 18:47:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/kytch-taylor-mcdonalds-ice-cream-machine-smoking-gun/">https://www.wired.com/story/kytch-taylor-mcdonalds-ice-cream-machine-smoking-gun/</a>, See on <a href="https://news.ycombinator.com/item?id=38657192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>A little over three years have passed since McDonald's sent out an email to thousands of its restaurant owners around the world that abruptly cut short the future of <a href="https://www.wired.com/story/they-hacked-mcdonalds-ice-cream-makers-started-cold-war/">a three-person startup called Kytch</a>—and with it, perhaps one of McDonald's best chances for fixing its famously out-of-order ice cream machines.</p><p>Until then, Kytch had been selling McDonald's restaurant owners a popular internet-connected gadget designed to attach to their notoriously fragile and often broken soft-serve McFlurry dispensers, manufactured by McDonalds equipment partner Taylor. The Kytch device would essentially hack into the ice cream machine's internals, monitor its operations, and send diagnostic data over the internet to an owner or manager to help keep it running. But despite Kytch's efforts to solve the Golden Arches’ intractable ice cream problems, a McDonald’s email in November 2020 warned its franchisees not to use Kytch, stating that it represented a safety hazard for staff. Kytch says its sales dried up practically overnight.</p><p>Now, after years of litigation, the ice-cream-hacking entrepreneurs have unearthed evidence that they say shows that Taylor, the soft-serve machine maker, helped engineer McDonald's Kytch-killing email—kneecapping the startup not because of any safety concern, but in a coordinated effort to undermine a potential competitor. And Taylor's alleged order, as Kytch now describes it, came all the way from the top.</p><div data-testid="GenericCallout"><p><span><picture><img alt="Image may contain: Food, Creme, Dessert, and Cream" src="https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_775%2Cc_limit/web_hp_IceCream_15046_2.jpg" srcset="https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_120,c_limit/web_hp_IceCream_15046_2.jpg 120w, https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_240,c_limit/web_hp_IceCream_15046_2.jpg 240w, https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_320,c_limit/web_hp_IceCream_15046_2.jpg 320w, https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_640,c_limit/web_hp_IceCream_15046_2.jpg 640w" sizes="100vw"></picture></span></p><div><p>Secret codes. Legal threats. Betrayal. How one couple built a device to fix McDonald’s notoriously broken soft-serve machines—and how the fast-food giant froze them out.</p></div></div><p>On Wednesday, Kytch filed a newly unredacted motion for summary adjudication in its lawsuit against Taylor for alleged trade libel, tortious interference, and other claims. The new motion, which replaces a redacted version from August, refers to internal emails Taylor released in the discovery phase of the lawsuit, which were quietly unsealed over the summer. The motion focuses in particular on one email from Timothy FitzGerald, the CEO of Taylor parent company Middleby, that appears to suggest that either Middleby or McDonald's send a communication to McDonald's franchise owners to dissuade them from using Kytch's device.</p><p>“Not sure if there is anything we can do to slow up the franchise community on the other solution,” FitzGerald wrote on October 17, 2020. “Not sure what communication from either McD or Midd can or will go out.”</p><p>In their legal filing, the Kytch cofounders, of course, interpret “the other solution” to mean their product. In fact, FitzGerald's message was sent in an email thread that included Middleby's then COO, David Brewer, who had wondered earlier whether Middleby could instead acquire Kytch. Another Middleby executive responded to FitzGerald on October 17 to write that Taylor and McDonald’s had already met the previous day to discuss sending out a message to franchisees about McDonald’s lack of support for Kytch.</p><p>But Jeremy O'Sullivan, a Kytch cofounder, claims—and Kytch argues in its legal motion—that FitzGerald’s email nonetheless proves Taylor's intent to hamstring a potential rival. “It's the smoking gun,” O'Sullivan says of the email. “He's plotting our demise.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Although FitzGerald's email doesn't actually order anyone to act against Kytch, the company’s motion argues that Taylor played a key role in what happened next. It's an “ambiguous yet direct message to his underlings,” argues Melissa Nelson, Kytch's other cofounder. “It's just like a mafia boss giving coded instructions to his team to whack someone."</p><p>On November 2, 2020, a little over two weeks after FitzGerald's open-ended suggestion that perhaps a “communication” from McDonald's or Middleby to franchisees could “slow up” adoption of “the other solution,” McDonald's <a href="https://www.wired.com/story/mcdonalds-ice-cream-machine-hacking-kytch-taylor-internal-emails/">sent out its email blast</a> cautioning restaurant owners not to use Kytch's product.</p><p>The email stated that the Kytch gadget “allows complete access to all aspects of the equipment’s controller and confidential data”—meaning Taylor’s and McDonald’s data, not the restaurant owners’ data; that it “creates a potential very serious safety risk for the crew or technician attempting to clean or repair the machine"; and finally, that it could cause “serious human injury.” The email concluded with a warning in italics and bold: “McDonald’s strongly recommends that you remove the Kytch device from all machines and discontinue use.”</p><p>Kytch has long argued that McDonald’s safety warning was bogus: In its legal complaint, it noted that its devices received certification from Underwriters Laboratory, an independent product safety nonprofit, including meeting its safety standards. It also countered in the complaint any claim that a Kytch device's remote connection to an ice cream machine could result in the machine turning on while a person's hand was inside—in fact, Taylor's own manual advises unplugging the machine before servicing it, and removing the door of the machine to access its rotating barrels automatically disables its motor.</p><p>Kytch's legal motion now argues that FitzGerald's email reveals that the McDonald's warning to restaurant owners was never really about safety, so much as protecting its equipment partner from a startup that might represent competition. The CEO's email “essentially put into place their plan to defame us," Nelson says.</p><p>She and O’Sullivan also argue that the internal email directly contradicts FitzGerald’s public statements that Middleby hadn’t sought to kill Kytch. “We’re not in business to put other companies out of business,” FitzGerald <a href="https://www.nytimes.com/2022/03/12/business/mcdonald-kytch-ice-cream-lawsuit.html">told <em>The New York Times</em></a> early last year.</p><p>When WIRED reached out to Middleby, Taylor’s parent company, for comment, a spokesperson responded in a statement disputing Kytch’s interpretation of its internal emails. “McDonald’s decided to issue the November 2020 field brief on its own accord, not at Middleby or Taylor’s direction,” the statement reads. “Taylor stood, and continues to stand, by the accuracy of statements made in the field brief.” The spokesperson also notes that Taylor won an early ruling in the lawsuit against Kytch’s request for a preliminary injunction—which would have prevented Taylor from developing a device that Kytch claims was <a href="https://www.wired.com/story/mcdonalds-ice-cream-machine-hacking-kytch-taylor-internal-emails/">copied from its product</a>—and promises an upcoming filing responding to Kytch’s argument, which court documents say will happen in early 2024.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>At the time of McDonald's warning email to franchisees about Kytch, Taylor was developing its own internet-connected ice cream machine, what it referred to as Taylor Shake/Sundae Connectivity, which McDonald's recommended in the same email. But, even now, more than two years after it was promised for delivery, that device has yet to arrive in restaurants—and the <a href="https://arstechnica.com/gadgets/2023/08/mcdonalds-ice-cream-machine-teardown-shows-error-codes-dmca-keep-it-broken/">publicly documented ice cream headaches</a> at McDonald’s appear to have continued. According to the website <a data-offer-url="https://mcbroken.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://mcbroken.com/&quot;}" href="https://mcbroken.com/" rel="nofollow noopener" target="_blank">McBroken</a>, which tracks ice cream machine downtime at McDonald's restaurants across the US, between 13 percent and 17 percent of McDonald's restaurants have had broken ice cream machines at any given time just this month. That percentage has recently been as high as 35 percent in New York City and 28 percent in Washington, DC.</p><p>Taylor declined to comment on any upcoming internet-connected ice cream machine model. But that long-touted solution to the problem has still not been made available to franchisees, according to one McDonald's restaurant owner who goes by the handle McFranchisee (and previously used the handle McD Truth) on X. But McFranchisee says that Taylor has integrated those new features into its next model, which is expected to be available in four to six months. (McFranchisee has also criticized Kytch, claiming that the startup's failure was due to its own reliability problems and an increase in its prices, not a Taylor or McDonald's conspiracy against them.)</p><p>Despite the email from Middleby's CEO that Kytch claims suggests dissuading franchisees from using Kytch's product, Kytch argues that <a href="https://www.wired.com/story/mcdonalds-ice-cream-machine-hacking-kytch-taylor-internal-emails/">other documents released in the lawsuit’s discovery phase</a> show McDonald's itself was also eager to stymie Kytch from the beginning. In February 2020, Taylor president Jeremy Dobrowolski wrote in another email that “McDonald's is all hot and heavy about” Kytch's growing use in restaurants. Before the company sent out its November 2 email warning franchisees about Kytch, Taylor and McDonald’s executives had a meeting to discuss the message, and a McDonald's exec also sent a draft to Taylor for its approval. A Taylor executive wrote to others within the ice cream machine company, “I am a bit in shock they are willing to take such a strong position.”</p><p>When WIRED reached out to McDonald’s for comment on Kytch’s new argument about the “smoking gun” email from Taylor’s CEO, a spokesperson responded with a statement: “McDonald’s won’t speculate about the intent behind this email discussion that we weren't a part of. The intent of our Nov. 2020 communication was to bring awareness to potential safety concerns regarding the unapproved Kytch device.”</p><p>In addition to its lawsuit against Taylor, Kytch is still pursuing a <a href="https://www.wired.com/story/kytch-ice-cream-machine-hackers-sue-mcdonalds-900-million/">bigger lawsuit against McDonald's itself</a>, asking for $900 million in damages for what it describes in its legal complaint as McDonald’s effort to “<a data-offer-url="https://www.courtlistener.com/docket/63127533/1/kytch-inc-v-mcdonalds-corporation/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.courtlistener.com/docket/63127533/1/kytch-inc-v-mcdonalds-corporation/&quot;}" href="https://www.courtlistener.com/docket/63127533/1/kytch-inc-v-mcdonalds-corporation/" rel="nofollow noopener" target="_blank">drive Kytch out of the marketplace</a>.” That lawsuit against McDonald's, if it moves forward, may soon produce more answers explaining Kytch’s legal claims that McDonald's appears to have cooperated with Taylor in telling its customers not to use Kytch—even as many of its restaurants took a significant hit from lost ice cream sales.</p><p>In the meantime, Kytch says it plans, if necessary, to take the lawsuit against Taylor to trial, currently set to take place in May at Alameda County Superior Court in Oakland, California. “The conspiracy described in Kytch’s complaint involved folks at the highest levels of leadership, not just at Taylor but also at Middleby and at McDonald’s,” says Daniel Watkins, Kytch’s attorney. “We’re really looking forward to the opportunity to present it to an Oakland jury trial.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I bricked my Christmas lights (482 pts)]]></title>
            <link>https://www.whizzy.org/2023-12-14-bricked-xmas/</link>
            <guid>38657126</guid>
            <pubDate>Fri, 15 Dec 2023 18:41:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.whizzy.org/2023-12-14-bricked-xmas/">https://www.whizzy.org/2023-12-14-bricked-xmas/</a>, See on <a href="https://news.ycombinator.com/item?id=38657126">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      

      

      <article role="main">
        <h2 id="reverse-engineering-bluetooth-le-led-light-controllers-or-how-i-bricked-my-christmas-lights">Reverse engineering Bluetooth LE LED light controllers, or How I Bricked My Christmas Lights</h2>

<p>If a device communicates via Bluetooth LE and has an app, it deserves to be integrated into my home automation system.</p>

<p>I’ve spent a significant amount of time reverse engineering various budget-friendly LED light strips to automate them. The process is generally repetitive, but I find it enjoyable. Recently, I successfully connected the cheapest lights I’ve ever come across — a £2.38 Bluetooth LE-controlled 5M non-addressable strip — to Home Assistant in just a few hours. You can buy some <a href="https://www.aliexpress.com/item/1005005485885067.html">here</a> and the code is <a href="https://github.com/8none1/bj_led">here</a>.</p>

<p>There is also the LEDnetWF controller I did the reverse engineering for <a href="https://github.com/raulgbcr/lednetwf_ble">here</a>.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/ha.jpg" alt="AliExpress Lights"></p>

<p>I also had another set of addressable lights on my desk. While decorating my office for Christmas, I decided to invest some time in connecting them to Home Assistant using the BJ_LED code as a template. It should have been straightforward, right? Well, yes, but also no.</p>

<p>These lights consist of a 10M long string of addressable LEDs controlled by the “iDeal LED” app. The app is feature-rich and works reasonably well. The LEDs are likely WS2812 or similar. I was quite pleased with these lights, which you can <a href="https://www.aliexpress.com/item/1005004829475855.html">find on AliExpress</a>.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/aliex_lights.jpg" alt="AliExpress Lights"></p>

<p>Now, let me share a cautionary tale. While I’m omitting some details for brevity, there are no secrets here, and additional instructions are readily available online. I understand this might feel a bit like <a href="https://knowyourmeme.com/memes/how-to-draw-an-owl">drawing the rest of the owl</a> but the provided links should serve as a starting point for anyone interested in reverse engineering their own LED lights.</p>

<h2 id="step-1-the-bytes-over-the-wire">Step 1. The bytes over the wire</h2>

<p>To control devices from your own software, the first step is to examine the bytes sent over Bluetooth to the device from the app. Typically, lights use a simple protocol with a header, command bytes (for actions like turning on/off, changing color), and a footer, which might be a checksum.</p>

<p>Android makes this process easy. Enable developer mode on your Android device, install the app for your lights, and enable <code>Bluetooth HCI snoop</code> in the developer settings. This logs Bluetooth bytes to a file readable by <a href="https://www.wireshark.org/">Wireshark</a>. Perform actions in the app, such as turning the lights on and off, and use <code>adb</code> to copy the logs to your computer.</p>

<p>For example:</p>

<pre><code>adb pull sdcard/btsnoop_hci.log .
</code></pre>

<p>Open the log in Wireshark to see the exact bytes sent to the device. Look for patterns in the values, and you’ll likely identify a series of bytes for each action, with one byte alternating between two values (e.g. <code>1</code> and <code>0</code> for <code>on</code> and <code>off</code>). Here’s a useful Wireshark filter:</p>

<pre><code>bluetooth.dst == ff:ff:ff:ff:ff:ff &amp;&amp; btatt.opcode.method==0x12
</code></pre>

<p>Change MAC address to be the MAC of your lights.  <code>btatt.opcode.method==0x12</code> is a write from the Android device to the lights.</p>

<p>Congratulations, you are now a reverse engineer!</p>

<p>Pro-tip:  You can speed things up a bit by using <a href="https://tshark.dev/">tshark</a> instead of Wireshark.  What you really care about is the values being written to the LED controller.  <code>tshark -r &lt;filename&gt; -T fields -e btatt.value</code> will dump the payload to the terminal for easy interrogation.</p>

<p>Sometimes your bytes will look like this:</p>

<pre><code>69 96 02 01 01
69 96 02 01 00
69 96 02 01 01
69 96 02 01 00
69 96 02 01 01
69 96 02 01 00
69 96 02 01 01
69 96 02 01 00
</code></pre>

<p>On, off, on, off, on, off, on, off.</p>

<p>Sometimes your bytes will look like this:</p>

<pre><code>84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
79 d1 db a4 09 19 c2 46 a8 58 0a e7 d1 1b 78 84
84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
79 d1 db a4 09 19 c2 46 a8 58 0a e7 d1 1b 78 84
84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
79 d1 db a4 09 19 c2 46 a8 58 0a e7 d1 1b 78 84
84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
</code></pre>

<p>There is still a repeating pattern here.  There are two distinct sets of bytes, one for on &amp; one for off, but… what?  Why is it so noisy?
Who designs their protocol like this?
The answer is: someone who is trying to hide something.</p>

<h2 id="step-2--replay-attacks">Step 2.  Replay attacks</h2>

<p>If your goal is simply turning the lights on and off, the repeating series of bytes you observed might be sufficient for power control. Test this with <code>gatttool</code>, which lets you connect to a BLE device and send bytes. You’ll need to know the handle to send bytes to, which you can find using Wireshark.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/wireshark.jpg" alt=""></p>

<p>For more control, understanding all those bytes is essential. Let’s go to the source…</p>

<h2 id="step-3-decompile-the-android-app">Step 3. Decompile the Android app</h2>

<p>Download the app’s APK and open it in <a href="https://github.com/skylot/jadx">jadx</a>. Witness the secrets within!</p>

<p>In my case, I noticed references to AES in the source, indicating a potentially encrypted protocol. If the data is encrypted, some assumptions can be made:</p>

<ul>
  <li>The encrypted data doesn’t change every time, suggesting a consistent key.</li>
  <li>The data needs quick decryption on a low-power MCU, favouring shorter keys.</li>
  <li>The key is likely not unique to each device, making a fixed key plausible.</li>
</ul>

<p>The source code contained a compiled AES library <code>libAES.so</code>, which <code>jadx</code> can’t help me with.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/jadx.jpg" alt="">
<img src="https://www.whizzy.org/wp-content/uploads/2023/12/jadx2.jpg" alt=""></p>

<p>This is where I got stuck.  For about 5 minutes.</p>

<p>I asked <a href="https://ubuntu.social/@popey">@popey</a> and <a href="https://mastodon.social/@sil">@sil</a> for some ideas.  @sil Googled some of the decompiled app code and found <a href="https://habr-com.translate.goog/ru/articles/722412/?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&amp;_x_tr_pto=wapp&amp;_x_tr_hist=true">this</a> page.  On closer examination the code looks identical.  This chap used <a href="https://hex-rays.com/ida-free/">ida free</a> to decompile the AES library and found the key embedded in it.  Let’s try that key.</p>

<pre><code>from Crypto.Cipher import AES

key = [
    0x34, 0x52, 0x2A, 0x5B, 0x7A, 0x6E, 0x49, 0x2C,
    0x08, 0x09, 0x0A, 0x9D, 0x8D, 0x2A, 0x23, 0xF8
]

def decrypt_aes_ecb(ciphertext, key):
    cipher = AES.new(key, AES.MODE_ECB)
    plaintext = cipher.decrypt(ciphertext)
    return plaintext
</code></pre>

<p>When we try and decrypt the <code>on</code> and <code>off</code> packets we get:</p>

<pre><code>05 54 55 52 4E 01 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 00 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 01 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 00 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 01 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 00 00 00 00 00 00 00 00 00 00 00
</code></pre>

<p>Success!  This is a lot more sensible.  A fixed header, byte 5 switching between a <code>1</code> and a <code>0</code> for on and off, and a bunch of zeros.</p>

<p>We can now decrypt all the packets being sent to the device and we can encrypt our own bytes so that we can duplicate the controls from the Android app in our own code.  It’s pretty much mission accomplished at this point.</p>

<h2 id="step-4-all-the-functions">Step 4. All the functions</h2>

<p>Now, work through each app function, recording the bytes sent. Write down each action, do it multiple times, and use separators like turning the lights on and off. This helps spot patterns and correlate notes with captured bytes.</p>

<p>For example, your process might be:</p>

<pre><code>turn off, turn on - [start of function]
set to red
set to green
set to blue
set to red
set to green
set to blue
set to red
set to green
set to blue
turn off, turn on - [end of colour changing]
set brightness to 100%
set brightness to 50%
set brightness to 10%
set brightness to 50%
set brightness to 100%
turn off, turn on - [end of brightness]
</code></pre>

<p>This will help you to spot patterns in the data and see which bytes change depending on what you are doing.</p>

<h2 id="step-5--automated-e-waste-generator">Step 5.  Automated e-waste generator</h2>

<p>While exploring color changes, I observed that the app never sent a value higher than 0x1F (5 bits) for red, green, or blue. Curious, I tried sending 8-bit values, and it worked remarkably well — brighter colors!</p>

<p>Great success!</p>

<p>Excited by my discovery I got to wondering what other secrets this light controller was hiding from me.  I wonder if there are any additional effects beyond the 10 that the app uses?
A good way to try this out would be a simple loop.</p>

<pre><code>    for n in range(20):
        print(f"Setting effect {n}")
        set_effect(n)
        time.sleep(20)
</code></pre>

<p>I ran this and watched 1 to 10.  So far so good, then it ticked over to 11 and AH HA!  I have found a secret mode!
Then it ticked over to 12 and… darkness.</p>

<p>Oh well, I guess there are only 11 effects, that’s fine.  I’ll reboot it and finish off the rest of the code.</p>

<p>And that was then end of my fun.</p>

<p>The lights never came back.</p>

<p>They don’t advertise on Bluetooth any more and I can’t connect to them.  I’ve tried holding down the button when turning them on.  I’ve left them unplugged over night to see if that helps, but no.</p>

<p>They are dead.</p>

<p>I guess I overflowed some buffer and I’ve corrupted the firmware.</p>

<p>All is not lost however.  The LEDs themselves are standard addressable LEDs so I can at least hook the string up to a different microcontroller and use them.</p>

<h2 id="tell-me-how-i-can-break-my-own-lights">Tell me how I can break my own lights</h2>

<p>Despite the setback, I documented most of the protocol and created a Github project with a Home Assistant custom component. It works, but proceed at your own risk.</p>

<p><a href="https://github.com/8none1/idealLED">Github: 8none1/idealLED</a></p>

      </article>

      

      

      
        <!-- Check if any share-links are active -->








      

      
      
  
  
  

  


  

  



    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI: Prompt Engineering (258 pts)]]></title>
            <link>https://platform.openai.com/docs/guides/prompt-engineering</link>
            <guid>38657029</guid>
            <pubDate>Fri, 15 Dec 2023 18:30:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://platform.openai.com/docs/guides/prompt-engineering">https://platform.openai.com/docs/guides/prompt-engineering</a>, See on <a href="https://news.ycombinator.com/item?id=38657029">Hacker News</a></p>
Couldn't get https://platform.openai.com/docs/guides/prompt-engineering: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Fastmail Employees Form a Union (187 pts)]]></title>
            <link>https://union.place/@fastmailunited/111563614375789166</link>
            <guid>38656727</guid>
            <pubDate>Fri, 15 Dec 2023 18:01:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://union.place/@fastmailunited/111563614375789166">https://union.place/@fastmailunited/111563614375789166</a>, See on <a href="https://news.ycombinator.com/item?id=38656727">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Do large language models need all those layers? (172 pts)]]></title>
            <link>https://www.amazon.science/blog/do-large-language-models-really-need-all-those-layers</link>
            <guid>38656039</guid>
            <pubDate>Fri, 15 Dec 2023 17:00:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amazon.science/blog/do-large-language-models-really-need-all-those-layers">https://www.amazon.science/blog/do-large-language-models-really-need-all-those-layers</a>, See on <a href="https://news.ycombinator.com/item?id=38656039">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Large language models (<a href="https://www.amazon.science/tag/large-language-models" data-cms-ai="0">LLMs</a>) have been around for a while but have really captured the attention of the public this year, with the advent of ChatGPT. LLMs are typically pretrained on massive volumes of data; recent variants are additionally tuned to follow instructions and incorporate human feedback using <a href="https://www.amazon.science/tag/reinforcement-learning" data-cms-ai="0">reinforcement learning</a>.</p><p>A fascinating ability that these LLMs demonstrate is in-context learning, where a model can learn to perform a task just by following a few (or sometimes even zero) good examples provided along with a new input. Following this paradigm of learning, larger LLMs also proved more capable of performing a wide variety of tasks than smaller ones, when the amount of pretraining data was fixed.</p><p>In a <a href="https://www.amazon.science/publications/rethinking-the-role-of-scale-for-in-context-learning-an-interpretability-based-case-study-at-66-billion-scale" data-cms-ai="0">paper</a> we’re presenting at this year’s meeting of the Association for Computational Linguistics (<a href="https://www.amazon.science/conferences-and-events/acl-2023" data-cms-ai="0">ACL</a>), we investigate the importance of model scale for in-context learning, from the perspective of architectural interpretability. We specifically ask the question <i>Are all LLM components really needed to perform in-context learning?</i></p><div data-align-right-rail=""><ps-related-content>
  <div data-media="" data-content-type="blog post">
        
            <div>
                
                    <a aria-label="20B-parameter Alexa model sets new marks in few-shot learning" href="https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning" data-cms-ai="0"><picture><source type="image/webp" width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/42f31a5/2147483647/strip/true/crop/865x489+2+0/resize/400x226!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F8d%2Fbf%2Fbb7071ba45ba92c7197cdd81285c%2F20b-encoder-decoder.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4="><source width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/f27a5d6/2147483647/strip/true/crop/865x489+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F8d%2Fbf%2Fbb7071ba45ba92c7197cdd81285c%2F20b-encoder-decoder.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    
        <img data-image-size="relatedContent" alt="20B-encoder-decoder.gif" width="400" height="226" data-src="https://assets.amazon.science/dims4/default/f27a5d6/2147483647/strip/true/crop/865x489+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F8d%2Fbf%2Fbb7071ba45ba92c7197cdd81285c%2F20b-encoder-decoder.gif" data-lazy-load="true" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    </picture>
</a>
                
            </div>
        

        <div>
            
              <p>
                  Related content
              </p>
            
            
                
            

            
    <p>With an encoder-decoder architecture — rather than decoder only — the Alexa Teacher Model excels other large language models on few-shot tasks such as summarization and machine translation.</p>


          </div>
    </div>
</ps-related-content>
</div><p>We conducted our investigation as a case study of the OPT-66B model, a 66-billion-parameter LLM that was open-sourced by Meta last year to serve as an open replica of GPT-3 (and was the largest publicly available decoder-only LLM at the time of our study). We found that a significant portion of the model could be discarded without affecting performance, indicating that OPT-66B and quite likely other prominent LLMs are undertrained.</p><p>We believe our findings are useful in helping build more powerful LLMs by identifying (or more generally providing methods to identify) architectural elements that may need to be trained better.</p><h2><p>LLM building blocks</p></h2><p>Modern LLMs use the Transformer architecture, which depends on an attention mechanism: the model learns to predict which prior tokens in the sequence it should <i>attend</i> to when predicting the current token.</p><div data-align-left-rail=""><ps-related-content>
  <div data-media="" data-content-type="blog post">
        
            <div>
                
                    <a aria-label="ACL: Computational linguistics in the age of large language models" href="https://www.amazon.science/blog/acl-computational-linguistics-in-the-age-of-large-language-models" data-cms-ai="0"><picture><source type="image/webp" width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/754777b/2147483647/strip/true/crop/1117x631+2+0/resize/400x226!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fc6%2Fcd%2F7eb8d5be4213b78e95877b929f65%2Fyang.16x9.png" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4="><source width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/1035f0b/2147483647/strip/true/crop/1117x631+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fc6%2Fcd%2F7eb8d5be4213b78e95877b929f65%2Fyang.16x9.png" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    
        <img data-image-size="relatedContent" alt="Yang.16x9.png" width="400" height="226" data-src="https://assets.amazon.science/dims4/default/1035f0b/2147483647/strip/true/crop/1117x631+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fc6%2Fcd%2F7eb8d5be4213b78e95877b929f65%2Fyang.16x9.png" data-lazy-load="true" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    </picture>
</a>
                
            </div>
        

        <div>
            
              <p>
                  Related content
              </p>
            
            
                
            

            
    <p>Amazon’s Yang Liu, general chair of this year’s meeting of the Association for Computational Linguistics, on the road ahead for LLMs.</p>


          </div>
    </div>
</ps-related-content>
</div><p>Specifically, LLMs use multihead attention, meaning that they apply multiple attention mechanisms, or heads, in parallel. OPT-66B has 64 layers with 72 attention heads in each layer. The output of the multihead attention passes through a separate feed-forward network (FFN) at each layer.</p><p>Our first method for analyzing OPT-66B was to assign a score to each attention head and FFN indicating how important they were to a given task. On the basis of those scores, we then pruned the model.</p><p>We found that important attention heads are primarily clustered in the model’s intermediate layers, and important FFNs are primarily in later layers. The ability to perform zero-/few-shot in-context learning on 14 different natural-language-processing (NLP) datasets/tasks stayed nearly intact when up to 70% (~15.7B parameters in OPT-66B) of the attention heads are removed.</p><div data-align-center=""><figure>
    

    
        <p><figcaption>A heat map representing attention heads’ aggregate importance scores for five-shot in-context learning across 14 NLP tasks, at each layer of the OPT-66B model.</figcaption></p>
    
</figure></div><p>The attention heads that are important (and unimportant) for in-context learning also seemed to overlap across tasks and shots. This indicates that a common task-agnostic subset of the attention heads is responsible for in-context learning. We also found that up to 20% of the FFNs (~8.5B parameters) can be removed with minimal decline in performance on zero-/few-shot in-context learning.</p><p>Our second analytic technique was to quantify the capacity of all attention heads in OPT-66B to perform a pair of task-agnostic primitive operations associated with in-context learning. Those primitives are <i>prefix matching</i> and <i>copying</i>: explicitly searching for a prior occurrence of the current token in context and copying over the token that succeeded it (its <i>suffix</i>).</p><div data-align-center=""><figure>
    

    
        <p><figcaption>Prefix matching and copying.</figcaption></p>
    
</figure></div><p>Heads specialized for these two operations were first discovered by the machine learning research company Anthropic and termed induction heads. We found that a small set of heads in OPT-66B have nontrivial scores for both primitives. We also found that these heads overlap (to varying degrees) with the heads important for specific tasks identified earlier. This indicates that induction heads are capable of more sophisticated behaviors associated with in-context learning, such as latent concept matching, but are not the only heads with such capabilities.</p><div data-align-right-rail=""><ps-related-content>
  <div data-media="" data-content-type="blog post">
        
            <div>
                
                    <a aria-label="Responsible AI in the generative era" href="https://www.amazon.science/blog/responsible-ai-in-the-generative-era" data-cms-ai="0"><picture><source type="image/webp" width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/d247ab4/2147483647/strip/true/crop/1912x1080+4+0/resize/400x226!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fe5%2F96%2Fab6a9ad44e6fa1758b3493ada7b6%2Fllm-watermarking.AI.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4="><source width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/7337ef8/2147483647/strip/true/crop/1912x1080+4+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fe5%2F96%2Fab6a9ad44e6fa1758b3493ada7b6%2Fllm-watermarking.AI.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    
        <img data-image-size="relatedContent" alt="LLM watermarking.AI.gif" width="400" height="226" data-src="https://assets.amazon.science/dims4/default/7337ef8/2147483647/strip/true/crop/1912x1080+4+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fe5%2F96%2Fab6a9ad44e6fa1758b3493ada7b6%2Fllm-watermarking.AI.gif" data-lazy-load="true" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    </picture>
</a>
                
            </div>
        

        <div>
            
              <p>
                  Related content
              </p>
            
            
                
            

            
    <p>Generative AI raises new challenges in defining, measuring, and mitigating concerns about fairness, toxicity, and intellectual property, among other things. But work has started on the solutions.</p>


          </div>
    </div>
</ps-related-content>
</div><p>Our overarching observation that only a core nucleus of attention heads and FFNs seem to be important for in-context learning indicates that OPT-66B and quite likely other prominent LLMs are undertrained. This also reinforces recent research that questions the efficacy of keeping the amount of pretraining data fixed when scaling models up, suggesting that the amount of pretraining data seen must be scaled hand-in-hand with the models themselves to attain optimal performance. It would be interesting to see how newer variants of LLMs released since the publication of our study, such as those tuned to follow instructions, fare in such analyses.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Not even LinkedIn is that keen on Microsoft's cloud: Shift to Azure abandoned (130 pts)]]></title>
            <link>https://www.theregister.com/2023/12/14/linkedin_abandons_migration_to_microsoft/</link>
            <guid>38656038</guid>
            <pubDate>Fri, 15 Dec 2023 17:00:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/12/14/linkedin_abandons_migration_to_microsoft/">https://www.theregister.com/2023/12/14/linkedin_abandons_migration_to_microsoft/</a>, See on <a href="https://news.ycombinator.com/item?id=38656038">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>LinkedIn has abandoned its efforts to migrate its datacenter infrastructure to Microsoft Azure four years after announcing the planned move.</p>
<p>Citing sources familiar with the matter, CNBC <a target="_blank" rel="nofollow" href="https://www.cnbc.com/2023/12/14/linkedin-shelved-plan-to-migrate-to-microsoft-azure-cloud.html">reports</a> the effort, codenamed "Blueshift," had run up against numerous challenges in the years since Microsoft acquired the professional networking site in 2016 for $27 billion.</p>
<p>In a statement to <em>The Register</em>, LinkedIn confirmed its plans to invest in its own datacenters while using Azure services where appropriate.</p>

    

<p>"This includes our running 100 employee-facing applications on Azure, leveraging Azure FrontDoor and ongoing work to consolidate our datacenter locations that are currently spread across multiple buildings under a single roof," the spokesperson said. "Azure has been crucial to support and scale collaboration and productivity for our teams to deliver value to our members."</p>

        


        

<p>The decision marks a reversal of LinkedIn's plans, <a target="_blank" href="https://engineering.linkedin.com/blog/2019/building-next-infra">announced</a> in a 2019 blog post, to migrate its workloads to a public cloud. At the time Mohak Shroff, the social network's SVP of engineering, touted the move as an opportunity to better support the site's growing membership.</p>
<p>"With the incredible member and business growth we're seeing, we've decided to begin a multi-year migration of all LinkedIn workloads to the public cloud," he wrote. "Moving to Azure will give us access to a wide array of hardware and software innovations, and unprecedented global scale."</p>
<ul>

<li><a href="https://www.theregister.com/2023/12/13/aws_sees_direct_uk_government/">In just one year, UK.gov's direct spend on AWS rises 76 percent</a></li>

<li><a href="https://www.theregister.com/2023/12/13/google_gemini_duet_ai/">Like Microsoft, Google can't stop its cloud from pouring AI all over your heads</a></li>

<li><a href="https://www.theregister.com/2023/12/07/aws_says_only_microsoft_has/">AWS accuses Microsoft of clipping customers' cloud freedoms</a></li>

<li><a href="https://www.theregister.com/2023/12/11/microsoft_union_ai_partnership/">Microsoft partners with labor unions to shape and regulate AI</a></li>
</ul>
<p>Over the past few years LinkedIn has deployed some services in Azure. In early 2022, the social network <a target="_blank" rel="nofollow" href="https://engineering.linkedin.com/blog/2022/accelerating-the-linkedin-experience-with-azure-front-door">tapped up</a> Azure FrontDoor, Microsoft's content delivery network, which caches commonly accessed content across a global network of edge datacenters reducing the bandwidth and access latencies required to serve users.</p>
<p>However, by mid-2022, CNBC reports, the cracks in LinkedIn's migration strategy were beginning to show. In a memo last summer, LinkedIn CTO Raghu Hiremagalur reportedly told employees LinkedIn was moving to a hybrid-cloud model with some services running in the cloud and others in the company's dedicated datacenters.</p>

        

<p>As it turned out, while Azure's scale may have presented a tantalizing opportunity at first blush, LinkedIn was having a hard time taking advantage of the cloud provider's software. Sources told CNBC that issues arose when LinkedIn attempted to lift and shift its existing software tools to Azure rather than refactor them to run on the cloud provider's ready made tools. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Threestudio – Framework for 3D content generation (107 pts)]]></title>
            <link>https://github.com/threestudio-project/threestudio</link>
            <guid>38655536</guid>
            <pubDate>Fri, 15 Dec 2023 16:11:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/threestudio-project/threestudio">https://github.com/threestudio-project/threestudio</a>, See on <a href="https://news.ycombinator.com/item?id=38655536">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
    <themed-picture data-catalyst-inline="true"><picture>
    <img alt="threestudio" src="https://user-images.githubusercontent.com/19284678/236847132-219999d0-4ffa-4240-a262-c2c025d15d9e.png" width="50%">
    </picture></themed-picture>
</p>
<p dir="auto"><b>
threestudio is a unified framework for 3D content creation from text prompts, single images, and few-shot images, by lifting 2D text-to-image generation models.
</b></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/245695565-f48eca9f-45a7-4092-a519-6bb99f4939e4.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1NjUtZjQ4ZWNhOWYtNDVhNy00MDkyLWE1MTktNmJiOTlmNDkzOWU0LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmMGI2NDA2Y2VmZDYzZWE1MzM2YjY3YTVlODk5NTlmZTI4N2U5MDc5YmY2YWYxMjEwMGM1M2ZhODhmMWNkYzUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8JamM8fetrNTI8koVeCEwpkvtKlCINhFKAbJfHt3FIM"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/245695565-f48eca9f-45a7-4092-a519-6bb99f4939e4.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1NjUtZjQ4ZWNhOWYtNDVhNy00MDkyLWE1MTktNmJiOTlmNDkzOWU0LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmMGI2NDA2Y2VmZDYzZWE1MzM2YjY3YTVlODk5NTlmZTI4N2U5MDc5YmY2YWYxMjEwMGM1M2ZhODhmMWNkYzUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8JamM8fetrNTI8koVeCEwpkvtKlCINhFKAbJfHt3FIM" width="100%" content-type-secured-asset="image/gif" data-animated-image=""></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/245695584-01a00207-3240-4a8e-aa6f-d48436370fe7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1ODQtMDFhMDAyMDctMzI0MC00YThlLWFhNmYtZDQ4NDM2MzcwZmU3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY0OWZlMTczNGFiYmZhMDBhMDY4NTQwYmY1YzJiMjU4OGFkMmNlNDNhMjM0ZThjNTk1N2U3N2IzOGVlMWE2MWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ExiA4qTu-NdNyREC3APhNUegdwYkr-ZXtxRSuX5S3VE"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/245695584-01a00207-3240-4a8e-aa6f-d48436370fe7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1ODQtMDFhMDAyMDctMzI0MC00YThlLWFhNmYtZDQ4NDM2MzcwZmU3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY0OWZlMTczNGFiYmZhMDBhMDY4NTQwYmY1YzJiMjU4OGFkMmNlNDNhMjM0ZThjNTk1N2U3N2IzOGVlMWE2MWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ExiA4qTu-NdNyREC3APhNUegdwYkr-ZXtxRSuX5S3VE" width="100%"></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047444-1dbdebab-43d5-4830-872c-66b38d9fda92.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0NDQtMWRiZGViYWItNDNkNS00ODMwLTg3MmMtNjZiMzhkOWZkYTkyLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJmZmFkZmQ5Y2EwZDE1NmQxZWQ5M2Y1OWIyZjFkYjA2YWY5ZGIxNTUwN2ZiMTFlMjhjZDYxNWI4ZTQzZGIzNGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.YEGdWJPEFq_I3_dXZs8VrpBWQL15OTSvEJSBQx8VmLw"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047444-1dbdebab-43d5-4830-872c-66b38d9fda92.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0NDQtMWRiZGViYWItNDNkNS00ODMwLTg3MmMtNjZiMzhkOWZkYTkyLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJmZmFkZmQ5Y2EwZDE1NmQxZWQ5M2Y1OWIyZjFkYjA2YWY5ZGIxNTUwN2ZiMTFlMjhjZDYxNWI4ZTQzZGIzNGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.YEGdWJPEFq_I3_dXZs8VrpBWQL15OTSvEJSBQx8VmLw" width="60%" data-animated-image=""></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047384-437b4044-142c-4e5d-a406-4d9bad0205e1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDczODQtNDM3YjQwNDQtMTQyYy00ZTVkLWE0MDYtNGQ5YmFkMDIwNWUxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUwMWZlY2YxZGVhZTAyNWI3YmNmYWRkMDA5MTIxYmQ4ZjZjYmNlNzI5YTAwYjNkYTg1NTMyNjU2ODM2YTU5NTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.or4eWogG-TmYCRFnSpMrLxDbVjwhnPP8E4dJAgkzmpE"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047384-437b4044-142c-4e5d-a406-4d9bad0205e1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDczODQtNDM3YjQwNDQtMTQyYy00ZTVkLWE0MDYtNGQ5YmFkMDIwNWUxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUwMWZlY2YxZGVhZTAyNWI3YmNmYWRkMDA5MTIxYmQ4ZjZjYmNlNzI5YTAwYjNkYTg1NTMyNjU2ODM2YTU5NTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.or4eWogG-TmYCRFnSpMrLxDbVjwhnPP8E4dJAgkzmpE" width="60%"></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047426-4f4d62c5-2304-4e20-b632-afe6d144a203.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0MjYtNGY0ZDYyYzUtMjMwNC00ZTIwLWI2MzItYWZlNmQxNDRhMjAzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExZTkyMmI3NzFjMjlkNTEyMjc2NmMwOGFmYmU1OWNlYWNmOWYzZGU0NzlkNDAxMzYxYmJmZWM0NmY0ZDUyMTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VWK9TDoRp7g-3L3TCbGjkoTDZFD--DA8GZNOLBEe2h4"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047426-4f4d62c5-2304-4e20-b632-afe6d144a203.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0MjYtNGY0ZDYyYzUtMjMwNC00ZTIwLWI2MzItYWZlNmQxNDRhMjAzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExZTkyMmI3NzFjMjlkNTEyMjc2NmMwOGFmYmU1OWNlYWNmOWYzZGU0NzlkNDAxMzYxYmJmZWM0NmY0ZDUyMTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VWK9TDoRp7g-3L3TCbGjkoTDZFD--DA8GZNOLBEe2h4" width="68%" data-animated-image=""></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047241-2f36ddbd-e3cf-4431-b269-47a9cb3d6e6e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDcyNDEtMmYzNmRkYmQtZTNjZi00NDMxLWIyNjktNDdhOWNiM2Q2ZTZlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmODlmY2I0MmFhYWYwODk0YTgwMjNhMDZjZjQxYWM1ZWRjMmJlNjgxOTM1ZDI1YTE0ZTI3OGUyN2I4NDA2OTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.diZG2o4ut4OpZoM3INnDyk2Vkq5U0W4MVr7IMzGOyQc"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047241-2f36ddbd-e3cf-4431-b269-47a9cb3d6e6e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDcyNDEtMmYzNmRkYmQtZTNjZi00NDMxLWIyNjktNDdhOWNiM2Q2ZTZlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmODlmY2I0MmFhYWYwODk0YTgwMjNhMDZjZjQxYWM1ZWRjMmJlNjgxOTM1ZDI1YTE0ZTI3OGUyN2I4NDA2OTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.diZG2o4ut4OpZoM3INnDyk2Vkq5U0W4MVr7IMzGOyQc" width="68%"></a>
</p>
<p dir="auto"><b>
👆 Results obtained from methods implemented by threestudio 👆 <br>
| <a href="https://ml.cs.tsinghua.edu.cn/prolificdreamer/" rel="nofollow">ProlificDreamer</a> | <a href="https://dreamfusion3d.github.io/" rel="nofollow">DreamFusion</a> | <a href="https://research.nvidia.com/labs/dir/magic3d/" rel="nofollow">Magic3D</a> | <a href="https://pals.ttic.edu/p/score-jacobian-chaining" rel="nofollow">SJC</a> | <a href="https://github.com/eladrich/latent-nerf">Latent-NeRF</a> | <a href="https://fantasia3d.github.io/" rel="nofollow">Fantasia3D</a> | <a href="https://fabi92.github.io/textmesh/" rel="nofollow">TextMesh</a> |
<br>
| <a href="https://zero123.cs.columbia.edu/" rel="nofollow">Zero-1-to-3</a> | <a href="https://guochengqian.github.io/project/magic123/" rel="nofollow">Magic123</a> |
<br>
| <a href="https://instruct-nerf2nerf.github.io/" rel="nofollow">InstructNeRF2NeRF</a> | <a href="https://control4darxiv.github.io/" rel="nofollow">Control4D</a> |
</b>
</p><p dir="auto">
  <a href="https://colab.research.google.com/github/threestudio-project/threestudio/blob/main/threestudio.ipynb" rel="nofollow">
  <img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg">
  </a>
  <a href="https://huggingface.co/spaces/bennyguo/threestudio" rel="nofollow"><img src="https://camo.githubusercontent.com/d62c84d474b9ca5604efd7987fe4a377b12835d0274154b3c6addfa140ec4809/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323047726164696f25323044656d6f2d48756767696e67666163652d6f72616e6765" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Gradio%20Demo-Huggingface-orange"></a>
  <a href="http://t23-g-01.threestudio.ai/" rel="nofollow"><img src="https://camo.githubusercontent.com/95c56fd6e110f90d44c0a85ef004d02a627e0db551b8c5ff88b2bdd24ca01a5f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47726164696f25323044656d6f2d54656e63656e742d626c75653f6c6f676f3d74656e63656e747171266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Gradio%20Demo-Tencent-blue?logo=tencentqq&amp;logoColor=white"></a>
  <a href="https://discord.gg/ejer2MAB8N" rel="nofollow"><img src="https://camo.githubusercontent.com/4d4aaf8201525ce15823a9d09c37ecbd84dfa70300a9b42c247dbe0a00d78388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d3538363546323f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;logoColor=white"></a>
</p>
<p dir="auto">
    Did not find what you want? Checkout <a href="https://threestudio-project.github.io/threestudio-extensions/" rel="nofollow"><b>threestudio-extension</b></a> or submit a feature request <a href="https://github.com/threestudio-project/threestudio/discussions/46" data-hovercard-type="discussion" data-hovercard-url="/threestudio-project/threestudio/discussions/46/hovercard">here</a>!
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/24589363/287081695-ac6089a7-d88f-414c-96d6-a5e75616115a.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODE2OTUtYWM2MDg5YTctZDg4Zi00MTRjLTk2ZDYtYTVlNzU2MTYxMTVhLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc3NmI1YmJlNzhjZGE1ZmZkMDUwZjZhZWFhMTg3ZGNlYjliZTQ4OTY3N2UwZWJlODQzNThmZTk1ZDI0YjUwMjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nF3tdOu7fiGcX4EYiCyQiEbpIOw9ZFc8tbNdz34cnL4"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/24589363/287081695-ac6089a7-d88f-414c-96d6-a5e75616115a.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODE2OTUtYWM2MDg5YTctZDg4Zi00MTRjLTk2ZDYtYTVlNzU2MTYxMTVhLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc3NmI1YmJlNzhjZGE1ZmZkMDUwZjZhZWFhMTg3ZGNlYjliZTQ4OTY3N2UwZWJlODQzNThmZTk1ZDI0YjUwMjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nF3tdOu7fiGcX4EYiCyQiEbpIOw9ZFc8tbNdz34cnL4" width="68%"></a>
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/24589363/287080628-8892898f-8bd8-43dc-a4ec-dd8d078af860.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODA2MjgtODg5Mjg5OGYtOGJkOC00M2RjLWE0ZWMtZGQ4ZDA3OGFmODYwLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI2YTQ0YjNmMDQ5ZWMzNjMxYTg2Y2I5ZjgzYjY2M2Y4NzViMzA4OTM5Nzc5MDQ0NTU1YzkzYjllMmMzM2RiM2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.0HWYlYXHYbpuyoTxGxvKmeGyub_4kUOM_s9J6g7PCus"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/24589363/287080628-8892898f-8bd8-43dc-a4ec-dd8d078af860.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODA2MjgtODg5Mjg5OGYtOGJkOC00M2RjLWE0ZWMtZGQ4ZDA3OGFmODYwLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI2YTQ0YjNmMDQ5ZWMzNjMxYTg2Y2I5ZjgzYjY2M2Y4NzViMzA4OTM5Nzc5MDQ0NTU1YzkzYjllMmMzM2RiM2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.0HWYlYXHYbpuyoTxGxvKmeGyub_4kUOM_s9J6g7PCus" width="50%" data-animated-image=""></a>
</p>
<h2 tabindex="-1" dir="auto">News</h2>
<ul dir="auto">
<li>11/30/2023 Implementation of <a href="https://github.com/DSaurus/threestudio-mvdream">MVDream</a>, <a href="https://github.com/DSaurus/threestudio-3dgs">Gaussian Splatting</a> as the custom extensions. You can also use neural representation to fit a mesh by <a href="https://github.com/DSaurus/threestudio-meshfitting">Mesh-Fitting</a>.</li>
<li>11/30/2023: Implementation of <a href="https://threestudio-project.github.io/threestudio-extensions/" rel="nofollow">custom extension system</a> and you can add your extensions in <a href="https://github.com/threestudio-project/threestudio-extensions">this project</a>.</li>
<li>08/25/2023: Implementation of <a href="https://guochengqian.github.io/project/magic123/" rel="nofollow">Magic123</a>! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#magic123-">here</a> to give it a try.</li>
<li>07/06/2023: Join our <a href="https://discord.gg/ejer2MAB8N" rel="nofollow">Discord server</a> for lively discussions!</li>
<li>07/03/2023: Try text-to-3D online in <a href="https://huggingface.co/spaces/bennyguo/threestudio" rel="nofollow">HuggingFace Spaces</a> or using our <a href="http://t23-g-01.threestudio.ai/" rel="nofollow">self-hosted service</a> (GPU support from Tencent). To host the web interface locally, see <a href="https://github.com/threestudio-project/threestudio#gradio-web-interface">here</a>.</li>
<li>06/20/2023: Implementations of Instruct-NeRF2NeRF and Control4D for high-fidelity 3D editing! Follow the instructions for <a href="https://github.com/threestudio-project/threestudio#control4d-">Control4D</a> and <a href="https://github.com/threestudio-project/threestudio#instructnerf2nerf-">Instruct-NeRF2NeRF</a> to give it a try.</li>
<li>06/14/2023: Implementation of TextMesh! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#textmesh-">here</a> to give it a try.</li>
<li>06/14/2023: Implementation of <a href="https://arxiv.org/abs/2303.15413" rel="nofollow">prompt debiasing</a> and <a href="https://perp-neg.github.io/" rel="nofollow">Perp-Neg</a>! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#tips-on-improving-quality">here</a> to give it a try.</li>
<li>05/29/2023: An experimental implementation of using <a href="https://zero123.cs.columbia.edu/" rel="nofollow">Zero-1-to-3</a> for 3D generation from a single image! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#zero-1-to-3-">here</a> to give it a try.</li>
<li>05/26/2023: Implementation of <a href="https://ml.cs.tsinghua.edu.cn/prolificdreamer/" rel="nofollow">ProlificDreamer</a>! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#prolificdreamer-">here</a> to give it a try.</li>
<li>05/14/2023: You can experiment with the SDS loss on 2D images using our <a href="https://github.com/threestudio-project/threestudio/blob/main/2dplayground.ipynb">2dplayground</a>.</li>
<li>05/13/2023: You can now try threestudio on <a href="https://colab.research.google.com/github/threestudio-project/threestudio/blob/main/threestudio.ipynb" rel="nofollow">Google Colab</a>!</li>
<li>05/11/2023: We now support exporting textured meshes! See <a href="https://github.com/threestudio-project/threestudio#export-meshes">here</a> for instructions.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/237616977-ccae2820-e702-484c-a43f-81678a365427.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzc2MTY5NzctY2NhZTI4MjAtZTcwMi00ODRjLWE0M2YtODE2NzhhMzY1NDI3LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdlNWQ1YzhlMTMzNTIxMzgxMDQ4NDk3YWMzNmUwYWM4OWEzY2RhOGExZjU2ZmFkNzBjM2Q1MWI2N2U1ZTVhMTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Z5ohn3i68p0q-RmIKMwOUq-vSFejlKGL7eJd32OUXKI"><img src="https://private-user-images.githubusercontent.com/19284678/237616977-ccae2820-e702-484c-a43f-81678a365427.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzc2MTY5NzctY2NhZTI4MjAtZTcwMi00ODRjLWE0M2YtODE2NzhhMzY1NDI3LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdlNWQ1YzhlMTMzNTIxMzgxMDQ4NDk3YWMzNmUwYWM4OWEzY2RhOGExZjU2ZmFkNzBjM2Q1MWI2N2U1ZTVhMTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Z5ohn3i68p0q-RmIKMwOUq-vSFejlKGL7eJd32OUXKI" alt="export-blender"></a></p>
<h2 tabindex="-1" dir="auto">Installation</h2>
<p dir="auto">See <a href="https://github.com/threestudio-project/threestudio/blob/main/docs/installation.md">installation.md</a> for additional information, including installation via Docker.</p>
<p dir="auto">The following steps have been tested on Ubuntu20.04.</p>
<ul dir="auto">
<li>You must have an NVIDIA graphics card with at least 6GB VRAM and have <a href="https://developer.nvidia.com/cuda-downloads" rel="nofollow">CUDA</a> installed.</li>
<li>Install <code>Python &gt;= 3.8</code>.</li>
<li>(Optional, Recommended) Create a virtual environment:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m virtualenv venv
. venv/bin/activate

# Newer pip versions, e.g. pip-23.x, can be much faster than old versions, e.g. pip-20.x.
# For instance, it caches the wheels of git packages to avoid unnecessarily rebuilding them later.
python3 -m pip install --upgrade pip"><pre>python3 -m virtualenv venv
<span>.</span> venv/bin/activate

<span><span>#</span> Newer pip versions, e.g. pip-23.x, can be much faster than old versions, e.g. pip-20.x.</span>
<span><span>#</span> For instance, it caches the wheels of git packages to avoid unnecessarily rebuilding them later.</span>
python3 -m pip install --upgrade pip</pre></div>
<ul dir="auto">
<li>Install <code>PyTorch &gt;= 1.12</code>. We have tested on <code>torch1.12.1+cu113</code> and <code>torch2.0.0+cu118</code>, but other versions should also work fine.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# torch1.12.1+cu113
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113
# or torch2.0.0+cu118
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"><pre><span><span>#</span> torch1.12.1+cu113</span>
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113
<span><span>#</span> or torch2.0.0+cu118</span>
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118</pre></div>
<ul dir="auto">
<li>(Optional, Recommended) Install ninja to speed up the compilation of CUDA extensions:</li>
</ul>

<ul dir="auto">
<li>Install dependencies:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<ul dir="auto">
<li>
<p dir="auto">(Optional) <code>tiny-cuda-nn</code> installation might require downgrading pip to 23.0.1</p>
</li>
<li>
<p dir="auto">(Optional, Recommended) The best-performing models in threestudio use the newly-released T2I model <a href="https://github.com/deep-floyd/IF">DeepFloyd IF</a>, which currently requires signing a license agreement. If you would like to use these models, you need to <a href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0" rel="nofollow">accept the license on the model card of DeepFloyd IF</a>, and login into the Hugging Face hub in the terminal by <code>huggingface-cli login</code>.</p>
</li>
<li>
<p dir="auto">For contributors, see <a href="https://github.com/threestudio-project/threestudio#contributing-to-threestudio">here</a>.</p>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Quickstart</h2>
<p dir="auto">Here we show some basic usage of threestudio. First let's train a DreamFusion model to create a classic pancake bunny.</p>
<p dir="auto"><strong>If you are experiencing unstable connections with Hugging Face, we suggest you either (1) setting environment variable <code>TRANSFORMERS_OFFLINE=1 DIFFUSERS_OFFLINE=1 HF_HUB_OFFLINE=1</code> before your running command after all needed files have been fetched on the first run, to prevent from connecting to Hugging Face each time you run, or (2) downloading the guidance model you used to a local folder following <a href="https://huggingface.co/docs/huggingface_hub/v0.14.1/guides/download#download-an-entire-repository" rel="nofollow">here</a> and <a href="https://huggingface.co/docs/huggingface_hub/v0.14.1/guides/download#download-files-to-local-folder" rel="nofollow">here</a>, and set <code>pretrained_model_name_or_path</code> of the guidance and the prompt processor to the local path.</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# if you have agreed the license of DeepFloyd IF and have >20GB VRAM
# please try this configuration for higher quality
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot;
# otherwise you could try with the Stable Diffusion model, which fits in 6GB VRAM
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot;"><pre><span><span>#</span> if you have agreed the license of DeepFloyd IF and have &gt;20GB VRAM</span>
<span><span>#</span> please try this configuration for higher quality</span>
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span>
<span><span>#</span> otherwise you could try with the Stable Diffusion model, which fits in 6GB VRAM</span>
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span></pre></div>
<p dir="auto">threestudio uses <a href="https://github.com/omry/omegaconf">OmegaConf</a> for flexible configurations. You can easily change any configuration in the YAML file by specifying arguments without <code>--</code>, for example the specified prompt in the above cases. For all supported configurations, please see our <a href="https://github.com/threestudio-project/threestudio/blob/main/DOCUMENTATION.md">documentation</a>.</p>
<p dir="auto">The training lasts for 10,000 iterations. You can find visualizations of the current status in the trial directory which defaults to <code>[exp_root_dir]/[name]/[tag]@[timestamp]</code>, where <code>exp_root_dir</code> (<code>outputs/</code> by default), <code>name</code> and <code>tag</code> can be set in the configuration file. A 360-degree video will be generated after the training is completed. In training, press <code>ctrl+c</code> one time will stop training and head directly to the test stage which generates the video. Press <code>ctrl+c</code> the second time to fully quit the program.</p>
<h3 tabindex="-1" dir="auto">Multi-GPU training</h3>
<p dir="auto">Multi-GPU training is supported, but may still be <a href="https://github.com/threestudio-project/threestudio/issues/195" data-hovercard-type="issue" data-hovercard-url="/threestudio-project/threestudio/issues/195/hovercard">buggy</a>. Note that <code>data.batch_size</code> is the batch size <strong>per rank (device)</strong>. Also remember to</p>
<ul dir="auto">
<li>Set <code>data.n_val_views</code> to be a multiple of the number of GPUs.</li>
<li>Set a unique <code>tag</code> as timestamp is disabled in multi-GPU training and will not be appended after the tag. If you the same tag as previous trials, saved config files, code and visualizations will be overridden.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# this results in an effective batch size of 4 (number of GPUs) * 2 (data.batch_size) = 8
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0,1,2,3 system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot; data.batch_size=2 data.n_val_views=4"><pre><span><span>#</span> this results in an effective batch size of 4 (number of GPUs) * 2 (data.batch_size) = 8</span>
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0,1,2,3 system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span> data.batch_size=2 data.n_val_views=4</pre></div>
<p dir="auto">If you define the <code>CUDA_VISIBLE_DEVICES</code> environment variable before you call <code>launch.py</code>, you don't need to specify <code>--gpu</code> - this will use all available GPUs from <code>CUDA_VISIBLE_DEVICES</code>. For instance, the following command will automatically use GPUs 3 and 4:</p>
<p dir="auto"><code>CUDA_VISIBLE_DEVICES=3,4 python launch.py --config configs/dreamfusion-if.yaml --train system.prompt_processor.prompt="a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes"</code></p>
<p dir="auto">This is particularly useful if you run <code>launch.py</code> in a cluster using a command that automatically picks GPU(s) and exports their IDs through CUDA_VISIBLE_DEVICES, e.g. through SLURM:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd git/threestudio
. venv/bin/activate
srun --account mod3d --partition=g40 --gpus=1 --job-name=3s_bunny python launch.py --config configs/dreamfusion-if.yaml --train system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot;"><pre><span>cd</span> git/threestudio
<span>.</span> venv/bin/activate
srun --account mod3d --partition=g40 --gpus=1 --job-name=3s_bunny python launch.py --config configs/dreamfusion-if.yaml --train system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span></pre></div>
<h3 tabindex="-1" dir="auto">Resume from checkpoints</h3>
<p dir="auto">If you want to resume from a checkpoint, do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# resume training from the last checkpoint, you may replace last.ckpt with any other checkpoints
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
# if the training has completed, you can still continue training for a longer time by setting trainer.max_steps
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt trainer.max_steps=20000
# you can also perform testing using resumed checkpoints
python launch.py --config path/to/trial/dir/configs/parsed.yaml --test --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
# note that the above commands use parsed configuration files from previous trials
# which will continue using the same trial directory
# if you want to save to a new trial directory, replace parsed.yaml with raw.yaml in the command

# only load weights from saved checkpoint but dont resume training (i.e. dont load optimizer state):
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 system.weights=path/to/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> resume training from the last checkpoint, you may replace last.ckpt with any other checkpoints</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
<span><span>#</span> if the training has completed, you can still continue training for a longer time by setting trainer.max_steps</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt trainer.max_steps=20000
<span><span>#</span> you can also perform testing using resumed checkpoints</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --test --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
<span><span>#</span> note that the above commands use parsed configuration files from previous trials</span>
<span><span>#</span> which will continue using the same trial directory</span>
<span><span>#</span> if you want to save to a new trial directory, replace parsed.yaml with raw.yaml in the command</span>

<span><span>#</span> only load weights from saved checkpoint but dont resume training (i.e. dont load optimizer state):</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 system.weights=path/to/trial/dir/ckpts/last.ckpt</pre></div>
<h3 tabindex="-1" dir="auto">Export Meshes</h3>
<p dir="auto">To export the scene to texture meshes, use the <code>--export</code> option. We currently support exporting to obj+mtl, or obj with vertex colors.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# this uses default mesh-exporter configurations which exports obj+mtl
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter
# specify system.exporter.fmt=obj to get obj with vertex colors
# you may also add system.exporter.save_uv=false to accelerate the process, suitable for a quick peek of the result
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.exporter.fmt=obj
# for NeRF-based methods (DreamFusion, Magic3D coarse, Latent-NeRF, SJC)
# you may need to adjust the isosurface threshold (25 by default) to get satisfying outputs
# decrease the threshold if the extracted model is incomplete, increase if it is extruded
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_threshold=10.
# use marching cubes of higher resolutions to get more detailed models
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_method=mc-cpu system.geometry.isosurface_resolution=256"><pre><span><span>#</span> this uses default mesh-exporter configurations which exports obj+mtl</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter
<span><span>#</span> specify system.exporter.fmt=obj to get obj with vertex colors</span>
<span><span>#</span> you may also add system.exporter.save_uv=false to accelerate the process, suitable for a quick peek of the result</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.exporter.fmt=obj
<span><span>#</span> for NeRF-based methods (DreamFusion, Magic3D coarse, Latent-NeRF, SJC)</span>
<span><span>#</span> you may need to adjust the isosurface threshold (25 by default) to get satisfying outputs</span>
<span><span>#</span> decrease the threshold if the extracted model is incomplete, increase if it is extruded</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_threshold=10.
<span><span>#</span> use marching cubes of higher resolutions to get more detailed models</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_method=mc-cpu system.geometry.isosurface_resolution=256</pre></div>
<p dir="auto">For all the options you can specify when exporting, see <a href="https://github.com/threestudio-project/threestudio/blob/main/DOCUMENTATION.md#exporters">the documentation</a>.</p>
<p dir="auto">See <a href="https://github.com/threestudio-project/threestudio#supported-models">here</a> for example running commands of all our supported models. Please refer to <a href="https://github.com/threestudio-project/threestudio#tips-on-improving-quality">here</a> for tips on getting higher-quality results, and <a href="https://github.com/threestudio-project/threestudio#vram-optimization">here</a> for reducing VRAM usage.</p>
<h3 tabindex="-1" dir="auto">Gradio Web Interface</h3>
<p dir="auto">Launch the Gradio web interface by</p>
<div data-snippet-clipboard-copy-content="python gradio_app.py launch"><pre><code>python gradio_app.py launch
</code></pre></div>
<p dir="auto">Parameters:</p>
<ul dir="auto">
<li><code>--listen</code>: listens to all addresses by setting <code>server_name="0.0.0.0"</code> when launching the Gradio app.</li>
<li><code>--self-deploy</code>: enables changing arbitrary configurations directly from the web.</li>
<li><code>--save</code>: enables checkpoint saving.</li>
</ul>
<p dir="auto">For feature requests, bug reports, or discussions about technical problems, please <a href="https://github.com/threestudio-project/threestudio/issues/new">file an issue</a>. In case you want to discuss the generation quality or showcase your generation results, please feel free to participate in the <a href="https://github.com/threestudio-project/threestudio/discussions">discussion panel</a>.</p>
<h2 tabindex="-1" dir="auto">Supported Models</h2>
<h3 tabindex="-1" dir="auto">ProlificDreamer <a href="https://arxiv.org/abs/2305.16213" rel="nofollow"><img src="https://camo.githubusercontent.com/c50bc699e2a0a94d97f8594bf640ecd1f2b1732cd84367106970fcef26f3a61c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330352e31363231332d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2305.16213-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>This is an unofficial experimental implementation! Please refer to <a href="https://github.com/thu-ml/prolificdreamer">https://github.com/thu-ml/prolificdreamer</a> for official code release.</strong></p>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion, 256x256 Stage1)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description prolificdreamer.mp4">prolificdreamer.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/243357000-27b42d8f-4aa4-4b47-8ea0-0f77db90fd1e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMDAtMjdiNDJkOGYtNGFhNC00YjQ3LThlYTAtMGY3N2RiOTBmZDFlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBiYjUxMDdlZTZhMDJmNzlhYWY2ZmM4MTU3MDYwMWI3MWYxYjI1YmVjMjgxN2FiMTNmZjRhM2I4ZTAyMGExY2UmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Q-2NK5juuNSn7NG22DuLEh1tlNpOOqDFbI7rV_izums" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/243357000-27b42d8f-4aa4-4b47-8ea0-0f77db90fd1e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMDAtMjdiNDJkOGYtNGFhNC00YjQ3LThlYTAtMGY3N2RiOTBmZDFlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBiYjUxMDdlZTZhMDJmNzlhYWY2ZmM4MTU3MDYwMWI3MWYxYjI1YmVjMjgxN2FiMTNmZjRhM2I4ZTAyMGExY2UmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Q-2NK5juuNSn7NG22DuLEh1tlNpOOqDFbI7rV_izums" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    
    <span aria-label="Video description prolificdreamer-scene.mp4">prolificdreamer-scene.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/243357021-ffcbbb01-3817-4663-a2bf-5e21a076bc3d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMjEtZmZjYmJiMDEtMzgxNy00NjYzLWEyYmYtNWUyMWEwNzZiYzNkLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY2NWRhYWNmZTc4ZjAyOTg5OWQ1ODMyYTEzODJiNjc3NDgxZDkxNTNiYmU1YTgyNzViYjFmMzE1ZGM3NWQ4YzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.u1mDjx6qaWce6cvEFYtL_QiLcgJMZrJX5B11yW0FSik" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/243357021-ffcbbb01-3817-4663-a2bf-5e21a076bc3d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMjEtZmZjYmJiMDEtMzgxNy00NjYzLWEyYmYtNWUyMWEwNzZiYzNkLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY2NWRhYWNmZTc4ZjAyOTg5OWQ1ODMyYTEzODJiNjc3NDgxZDkxNTNiYmU1YTgyNzViYjFmMzE1ZGM3NWQ4YzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.u1mDjx6qaWce6cvEFYtL_QiLcgJMZrJX5B11yW0FSik" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion, 256x256 Stage1, 512x512 Stage2+3)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description prolificdreamer-full.mp4">prolificdreamer-full.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/243357051-cfab881e-18dc-45fc-8384-7476f835b36e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwNTEtY2ZhYjg4MWUtMThkYy00NWZjLTgzODQtNzQ3NmY4MzViMzZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZiZjQ4YTRlZDAzNzg2NzE5MjBjZWMxMGY0MzU5ZmNjNzZhMzFkNTQ4ZjM5ZDBiODlhMTRkYWJjZTllZWExZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.uDVbLmY5OiDwVhzA-Lv5Z7V3vdq6u38vIO5AnXxOWfI" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/243357051-cfab881e-18dc-45fc-8384-7476f835b36e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwNTEtY2ZhYjg4MWUtMThkYy00NWZjLTgzODQtNzQ3NmY4MzViMzZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZiZjQ4YTRlZDAzNzg2NzE5MjBjZWMxMGY0MzU5ZmNjNzZhMzFkNTQ4ZjM5ZDBiODlhMTRkYWJjZTllZWExZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.uDVbLmY5OiDwVhzA-Lv5Z7V3vdq6u38vIO5AnXxOWfI" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Notable differences from the paper:</p>
<ul dir="auto">
<li>ProlificDreamer adopts a two-stage sampling strategy with 64 coarse samples and 32 fine samples, while we only use 512 coarse samples.</li>
<li>In the first stage, we only render 64x64 images at the first 5000 iterations. After that, as the empty space has been effectively pruned, rendering 512x512 images wouldn't cost too much VRAM.</li>
<li>We currently don't support multiple particles.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- Stage 1 (NeRF) --------- #
# object generation with 512x512 NeRF rendering, ~30GB VRAM
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot;
# if you don't have enough VRAM, try training with 64x64 NeRF rendering, ~15GB VRAM
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; data.width=64 data.height=64 data.batch_size=1
# using the same model for pretrained and LoRA enables 64x64 training with <10GB VRAM
# but the quality is worse due to the use of an epsilon prediction model for LoRA training
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; data.width=64 data.height=64 data.batch_size=1 system.guidance.pretrained_model_name_or_path_lora=&quot;stabilityai/stable-diffusion-2-1-base&quot;
# Using patch-based renderer to reduce memory consume, 512x512 resolution, ~20GB VRAM
python launch.py --config configs/prolificdreamer-patch.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot;
# scene generation with 512x512 NeRF rendering, ~30GB VRAM
python launch.py --config configs/prolificdreamer-scene.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;Inside of a smart home, realistic detailed photo, 4k&quot;

# --------- Stage 2 (Geometry Refinement) --------- #
# refine geometry with 512x512 rasterization, Stable Diffusion SDS guidance
python launch.py --config configs/prolificdreamer-geometry.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; system.geometry_convert_from=path/to/stage1/trial/dir/ckpts/last.ckpt

# --------- Stage 3 (Texturing) --------- #
# texturing with 512x512 rasterization, Stable Difusion VSD guidance
python launch.py --config configs/prolificdreamer-texture.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; system.geometry_convert_from=path/to/stage2/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> --------- Stage 1 (NeRF) --------- #</span>
<span><span>#</span> object generation with 512x512 NeRF rendering, ~30GB VRAM</span>
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span>
<span><span>#</span> if you don't have enough VRAM, try training with 64x64 NeRF rendering, ~15GB VRAM</span>
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> data.width=64 data.height=64 data.batch_size=1
<span><span>#</span> using the same model for pretrained and LoRA enables 64x64 training with &lt;10GB VRAM</span>
<span><span>#</span> but the quality is worse due to the use of an epsilon prediction model for LoRA training</span>
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> data.width=64 data.height=64 data.batch_size=1 system.guidance.pretrained_model_name_or_path_lora=<span><span>"</span>stabilityai/stable-diffusion-2-1-base<span>"</span></span>
<span><span>#</span> Using patch-based renderer to reduce memory consume, 512x512 resolution, ~20GB VRAM</span>
python launch.py --config configs/prolificdreamer-patch.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span>
<span><span>#</span> scene generation with 512x512 NeRF rendering, ~30GB VRAM</span>
python launch.py --config configs/prolificdreamer-scene.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>Inside of a smart home, realistic detailed photo, 4k<span>"</span></span>

<span><span>#</span> --------- Stage 2 (Geometry Refinement) --------- #</span>
<span><span>#</span> refine geometry with 512x512 rasterization, Stable Diffusion SDS guidance</span>
python launch.py --config configs/prolificdreamer-geometry.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> system.geometry_convert_from=path/to/stage1/trial/dir/ckpts/last.ckpt

<span><span>#</span> --------- Stage 3 (Texturing) --------- #</span>
<span><span>#</span> texturing with 512x512 rasterization, Stable Difusion VSD guidance</span>
python launch.py --config configs/prolificdreamer-texture.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> system.geometry_convert_from=path/to/stage2/trial/dir/ckpts/last.ckpt</pre></div>
<h3 tabindex="-1" dir="auto">DreamFusion <a href="https://arxiv.org/abs/2209.14988" rel="nofollow"><img src="https://camo.githubusercontent.com/0a00143f284574687e6b04ef66124417c8da00ce27e46ee6dfc3dc8fe7e2465d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323230392e31343938382d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2209.14988-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (DeepFloyd IF, batch size 8)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description dreamfusion-if.mp4">dreamfusion-if.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694848-38ae4ea4-554b-4c9d-b4c7-fba5bee3acb3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NDgtMzhhZTRlYTQtNTU0Yi00YzlkLWI0YzctZmJhNWJlZTNhY2IzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWViNjEzM2NkOTA4NDM4OGZjZjJjY2I1YTFiZGFiMjBiOGNhMGNiMGYzMTg5NjYxMmU0MTI0YjcxMjI5Mjk0MTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.5Ifmg-F54IVJFg-Uc9oKpcXsT3h9VjGy2wFfjz2kTP4" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694848-38ae4ea4-554b-4c9d-b4c7-fba5bee3acb3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NDgtMzhhZTRlYTQtNTU0Yi00YzlkLWI0YzctZmJhNWJlZTNhY2IzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWViNjEzM2NkOTA4NDM4OGZjZjJjY2I1YTFiZGFiMjBiOGNhMGNiMGYzMTg5NjYxMmU0MTI0YjcxMjI5Mjk0MTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.5Ifmg-F54IVJFg-Uc9oKpcXsT3h9VjGy2wFfjz2kTP4" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>We use open-source T2I models (StableDiffusion, DeepFloyd IF), while the paper uses Imagen.</li>
<li>We use a guidance scale of 20 for DeepFloyd IF, while the paper uses 100 for Imagen.</li>
<li>We do not use sigmoid to normalize the albedo color but simply scale the color from <code>[-1,1]</code> to <code>[0,1]</code>, as we find this help convergence.</li>
<li>We use HashGrid encoding and uniformly sample points along rays, while the paper uses Integrated Positional Encoding and sampling strategy from MipNeRF360.</li>
<li>We adopt camera settings and density initialization strategy from Magic3D, which is slightly different from the DreamFusion paper.</li>
<li>Some hyperparameters are different, such as the weighting of loss terms.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training
# here we adopt random background augmentation to improve geometry quality
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.background.random_aug=true
# uses StableDiffusion, requires ~6GB VRAM in training
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;"><pre><span><span>#</span> uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training</span>
<span><span>#</span> here we adopt random background augmentation to improve geometry quality</span>
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.background.random_aug=true
<span><span>#</span> uses StableDiffusion, requires ~6GB VRAM in training</span>
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span></pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>DeepFloyd IF performs <strong>way better than</strong> StableDiffusion.</li>
<li>Validation shows albedo color before <code>system.material.ambient_only_steps</code> and shaded color after that.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_sparsity</code> if your scene is stuffed with floaters/becoming empty.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_orient</code> if you object is foggy/over-smoothed.</li>
<li>Try replacing the background to random colors with a probability 0.5 by setting <code>system.background.random_aug=true</code> if you find the model incorrectly treats the background as part of the object.</li>
<li>DeepFloyd IF uses T5-XXL as its text encoder, which consumes ~15GB VRAM even when using 8-bit quantization. This is currently the bottleneck for training with less VRAM. If anyone knows how to run the text encoder with less VRAM, please file an issue. We're also trying to push the text encoder to <a href="https://replicate.com/" rel="nofollow">Replicate</a> to enable extracting text embeddings via API, but are having some network connection issues. Please <a href="mailto:imbennyguo@gmail.com">contact bennyguo</a> if you would like to help out.</li>
</ul>
<h3 tabindex="-1" dir="auto">Magic3D <a href="https://arxiv.org/abs/2211.10440" rel="nofollow"><img src="https://camo.githubusercontent.com/92872accb7b8db7a3702adf6bebcf7b02c66650ec96f1d3aabf1843d39b2d171/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323231312e31303434302d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2211.10440-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (DeepFloyd IF, batch size 8; first row: coarse, second row: refine)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description magic3d-if.mp4">magic3d-if.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694858-0ed6939e-cd7a-408f-a94b-406709ae90c0.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NTgtMGVkNjkzOWUtY2Q3YS00MDhmLWE5NGItNDA2NzA5YWU5MGMwLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA5ZTAyMGU3MmYyZDNiNTY5ZTA5M2RiNTUwMDk4ODEzNTZlZjNhMDA4Y2U4MzJhZTU3MDQ4YzhlMjFhZWZmZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VCyXtsj2LL4k6vgdUHSi8ltI1yIeKuq-3lXquWeW7VE" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694858-0ed6939e-cd7a-408f-a94b-406709ae90c0.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NTgtMGVkNjkzOWUtY2Q3YS00MDhmLWE5NGItNDA2NzA5YWU5MGMwLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA5ZTAyMGU3MmYyZDNiNTY5ZTA5M2RiNTUwMDk4ODEzNTZlZjNhMDA4Y2U4MzJhZTU3MDQ4YzhlMjFhZWZmZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VCyXtsj2LL4k6vgdUHSi8ltI1yIeKuq-3lXquWeW7VE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>We use open-source T2I models (StableDiffusion, DeepFloyd IF) for the coarse stage, while the paper uses eDiff-I.</li>
<li>In the coarse stage, we use a guidance scale of 20 for DeepFloyd IF, while the paper uses 100 for eDiff-I.</li>
<li>In the coarse stage, we use analytic normal, while the paper uses predicted normal.</li>
<li>In the coarse stage, we use orientation loss as in DreamFusion, while the paper does not.</li>
<li>There are many things that are omitted from the paper such as the weighting of loss terms and the DMTet grid resolution, which could be different.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<p dir="auto">First train the coarse stage NeRF:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training
python launch.py --config configs/magic3d-coarse-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;
# uses StableDiffusion, requires ~6GB VRAM in training
python launch.py --config configs/magic3d-coarse-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;"><pre><span><span>#</span> uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training</span>
python launch.py --config configs/magic3d-coarse-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span>
<span><span>#</span> uses StableDiffusion, requires ~6GB VRAM in training</span>
python launch.py --config configs/magic3d-coarse-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span></pre></div>
<p dir="auto">Then convert the NeRF from the coarse stage to DMTet and train with differentiable rasterization:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# the refinement stage uses StableDiffusion, and requires ~5GB VRAM in training
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
# if you're unsatisfied with the surface extracted using the default threshold (25)
# you can specify a threshold value using `system.geometry_convert_override`
# decrease the value if the extracted surface is incomplete, increase if it is extruded
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10."><pre><span><span>#</span> the refinement stage uses StableDiffusion, and requires ~5GB VRAM in training</span>
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
<span><span>#</span> if you're unsatisfied with the surface extracted using the default threshold (25)</span>
<span><span>#</span> you can specify a threshold value using `system.geometry_convert_override`</span>
<span><span>#</span> decrease the value if the extracted surface is incomplete, increase if it is extruded</span>
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10.</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>For the coarse stage, DeepFloyd IF performs <strong>way better than</strong> StableDiffusion.</li>
<li>Magic3D uses a neural network to predict the surface normal, which may not resemble the true geometric normal and degrade geometry quality, so we use analytic normal instead.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_sparsity</code> if your scene is stuffed with floaters/becoming empty.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_orient</code> if you object is foggy/over-smoothed.</li>
<li>Try replacing the background with random colors with a probability 0.5 by setting <code>system.background.random_aug=true</code> if you find the model incorrectly treats the background as part of the object.</li>
</ul>
<h3 tabindex="-1" dir="auto">Score Jacobian Chaining <a href="https://arxiv.org/abs/2212.00774" rel="nofollow"><img src="https://camo.githubusercontent.com/61e153afb36cca33506ce0f5fd1560c6fe94155476c855965212182a1808e6f5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323231322e30303737342d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2212.00774-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description sjc.mp4">sjc.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694871-87a247c1-2d3d-4cbf-89df-450bfeac3aca.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzEtODdhMjQ3YzEtMmQzZC00Y2JmLTg5ZGYtNDUwYmZlYWMzYWNhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjNzlkOWExZWI5MGRjYTY2NDYzODhiMDA5ZjVmMDM5MDQxZDMwMTc1ODRjZGVlYWVjOWY2YzVkYzY1ZGNhODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.JK95IOQiZYkdb49L26wWkn-UarGBmxQDgMB2Dbh7ops" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694871-87a247c1-2d3d-4cbf-89df-450bfeac3aca.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzEtODdhMjQ3YzEtMmQzZC00Y2JmLTg5ZGYtNDUwYmZlYWMzYWNhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjNzlkOWExZWI5MGRjYTY2NDYzODhiMDA5ZjVmMDM5MDQxZDMwMTc1ODRjZGVlYWVjOWY2YzVkYzY1ZGNhODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.JK95IOQiZYkdb49L26wWkn-UarGBmxQDgMB2Dbh7ops" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Notable differences from the paper: N/A.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# train with sjc guidance in latent space
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;A high quality photo of a delicious burger&quot;
# train with sjc guidance in latent space, trump figure
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;Trump figure&quot; trainer.max_steps=30000 system.loss.lambda_emptiness=&quot;[15000,10000.0,200000.0,15001]&quot; system.optimizer.params.background.lr=0.05 seed=42"><pre><span><span>#</span> train with sjc guidance in latent space</span>
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>A high quality photo of a delicious burger<span>"</span></span>
<span><span>#</span> train with sjc guidance in latent space, trump figure</span>
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>Trump figure<span>"</span></span> trainer.max_steps=30000 system.loss.lambda_emptiness=<span><span>"</span>[15000,10000.0,200000.0,15001]<span>"</span></span> system.optimizer.params.background.lr=0.05 seed=42</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>SJC uses subpixel rendering which decodes a <code>128x128</code> latent feature map for better visualization quality. You can turn off this feature by <code>system.subpixel_rendering=false</code> to save VRAM in validation/testing.</li>
</ul>
<h3 tabindex="-1" dir="auto">Latent-NeRF <a href="https://arxiv.org/abs/2211.07600" rel="nofollow"><img src="https://camo.githubusercontent.com/2cc556c25ad825d7078271e0eb352715d1a528320b10d315bc1f1866c129a96c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323231312e30373630302d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2211.07600-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description latent-nerf.mp4">latent-nerf.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694876-5a270347-6a41-4429-8909-44c90c554e06.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzYtNWEyNzAzNDctNmE0MS00NDI5LTg5MDktNDRjOTBjNTU0ZTA2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTkyYWVjN2VlZjY1NzMwOTlhMTQ2NWE5MDc3Nzc1MGRhNmFkMThlZDFlZjMxOTdjMTNhMGJmZGU5MjkzMzZkMjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.yZzVykUB51Io13qb0I6VjWOYM91vw1uk0Kzt917FVqg" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694876-5a270347-6a41-4429-8909-44c90c554e06.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzYtNWEyNzAzNDctNmE0MS00NDI5LTg5MDktNDRjOTBjNTU0ZTA2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTkyYWVjN2VlZjY1NzMwOTlhMTQ2NWE5MDc3Nzc1MGRhNmFkMThlZDFlZjMxOTdjMTNhMGJmZGU5MjkzMzZkMjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.yZzVykUB51Io13qb0I6VjWOYM91vw1uk0Kzt917FVqg" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Notable differences from the paper: N/A.</p>
<p dir="auto">We currently only implement Latent-NeRF for text-guided and Sketch-Shape for (text,shape)-guided 3D generation. Latent-Paint is not implemented yet.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# train Latent-NeRF in Stable Diffusion latent space
python launch.py --config configs/latentnerf.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;
# refine Latent-NeRF in RGB space
python launch.py --config configs/latentnerf-refine.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt

# train Sketch-Shape in Stable Diffusion latent space
python launch.py --config configs/sketchshape.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=&quot;a teddy bear in a tuxedo&quot;
# refine Sketch-Shape in RGB space
python launch.py --config configs/sketchshape-refine.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=&quot;a teddy bear in a tuxedo&quot; system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> train Latent-NeRF in Stable Diffusion latent space</span>
python launch.py --config configs/latentnerf.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span>
<span><span>#</span> refine Latent-NeRF in RGB space</span>
python launch.py --config configs/latentnerf-refine.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt

<span><span>#</span> train Sketch-Shape in Stable Diffusion latent space</span>
python launch.py --config configs/sketchshape.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=<span><span>"</span>a teddy bear in a tuxedo<span>"</span></span>
<span><span>#</span> refine Sketch-Shape in RGB space</span>
python launch.py --config configs/sketchshape-refine.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=<span><span>"</span>a teddy bear in a tuxedo<span>"</span></span> system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt</pre></div>
<h3 tabindex="-1" dir="auto">Fantasia3D <a href="https://arxiv.org/abs/2303.13873" rel="nofollow"><img src="https://camo.githubusercontent.com/98f846baf085f2ee0b2b0e21e7522d380f3b53e7b71174e2ba574f1b00bc1858/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330332e31333837332d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2303.13873-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description fantasia-3d.mp4">fantasia-3d.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694880-33b0db21-4530-47f1-9c3b-c70357bc84b3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4ODAtMzNiMGRiMjEtNDUzMC00N2YxLTljM2ItYzcwMzU3YmM4NGIzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk1MWNkODMxZjM2NGM4ODEzZmRlMTJhYzZjNDA2YjU3NTkzNDM0ZDhhNDA1NmE2OGMzM2RiN2Y1MzU5OGQzMWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.RXvCegjzK4rbcg-xAUfin2BznxNnh9T1xJ1DMLN_okE" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694880-33b0db21-4530-47f1-9c3b-c70357bc84b3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4ODAtMzNiMGRiMjEtNDUzMC00N2YxLTljM2ItYzcwMzU3YmM4NGIzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk1MWNkODMxZjM2NGM4ODEzZmRlMTJhYzZjNDA2YjU3NTkzNDM0ZDhhNDA1NmE2OGMzM2RiN2Y1MzU5OGQzMWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.RXvCegjzK4rbcg-xAUfin2BznxNnh9T1xJ1DMLN_okE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion, mesh initialization)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description fantasia3d-mesh.mp4">fantasia3d-mesh.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/242636697-762903c1-665b-47b5-a2c2-bd7021a9e548.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzY2OTctNzYyOTAzYzEtNjY1Yi00N2I1LWEyYzItYmQ3MDIxYTllNTQ4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkZjA0NTYwNjk4ZDA3MDY3OGJjZmE3OTU2OTE2M2YyMGYwZGM4YWE1OWFlMjI4OTljZjdkZmY1OTEyMWQyZDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.G78odD1DOTH_iZiw492CGCEmIDvF-KSG1lvUVSKLSdU" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/242636697-762903c1-665b-47b5-a2c2-bd7021a9e548.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzY2OTctNzYyOTAzYzEtNjY1Yi00N2I1LWEyYzItYmQ3MDIxYTllNTQ4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkZjA0NTYwNjk4ZDA3MDY3OGJjZmE3OTU2OTE2M2YyMGYwZGM4YWE1OWFlMjI4OTljZjdkZmY1OTEyMWQyZDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.G78odD1DOTH_iZiw492CGCEmIDvF-KSG1lvUVSKLSdU" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/242637322-2d22e30f-4a32-454a-a06e-d6e6bd2a1b96.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzczMjItMmQyMmUzMGYtNGEzMi00NTRhLWEwNmUtZDZlNmJkMmExYjk2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjMDBkOWZmYTM4MjZjMWY4YTcwODdkNTVkN2RkYjEwMDg4ZmRjNmFlMmE3NDU4ZmExMDMzODE0M2YzNWQyOGEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.mIXWeTV9bsJ7NdDyI4ZhZ4S3xK1aVy8PX89j-oGne9w"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/242637322-2d22e30f-4a32-454a-a06e-d6e6bd2a1b96.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzczMjItMmQyMmUzMGYtNGEzMi00NTRhLWEwNmUtZDZlNmJkMmExYjk2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjMDBkOWZmYTM4MjZjMWY4YTcwODdkNTVkN2RkYjEwMDg4ZmRjNmFlMmE3NDU4ZmExMDMzODE0M2YzNWQyOGEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.mIXWeTV9bsJ7NdDyI4ZhZ4S3xK1aVy8PX89j-oGne9w" width="100%"></a>
</p>
<p dir="auto">Notable differences from the paper:</p>
<ul dir="auto">
<li>We enable tangent-space normal perturbation by default, which can be turned off by appending <code>system.material.use_bump=false</code>.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- Geometry --------- #
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a DSLR photo of an ice cream sundae&quot;
# Fantasia3D highly relies on the initialized SDF shape
# the default shape is a sphere with radius 0.5
# change the shape initialization to match your input prompt
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;The leaning tower of Pisa&quot; system.geometry.shape_init=ellipsoid system.geometry.shape_init_params=&quot;[0.3,0.3,0.8]&quot;
# or you can initialize from a mesh
# here shape_init_params is the scale of the shape
# also make sure to input the correct up and front axis (in +x, +y, +z, -x, -y, -z)
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;hulk&quot; system.geometry.shape_init=mesh:load/shapes/human.obj system.geometry.shape_init_params=0.9 system.geometry.shape_init_mesh_up=+y system.geometry.shape_init_mesh_front=+z
# --------- Texture --------- #
# to train PBR texture continued from a geometry checkpoint:
python launch.py --config configs/fantasia3d-texture.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a DSLR photo of an ice cream sundae&quot; system.geometry_convert_from=path/to/geometry/stage/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> --------- Geometry --------- #</span>
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a DSLR photo of an ice cream sundae<span>"</span></span>
<span><span>#</span> Fantasia3D highly relies on the initialized SDF shape</span>
<span><span>#</span> the default shape is a sphere with radius 0.5</span>
<span><span>#</span> change the shape initialization to match your input prompt</span>
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>The leaning tower of Pisa<span>"</span></span> system.geometry.shape_init=ellipsoid system.geometry.shape_init_params=<span><span>"</span>[0.3,0.3,0.8]<span>"</span></span>
<span><span>#</span> or you can initialize from a mesh</span>
<span><span>#</span> here shape_init_params is the scale of the shape</span>
<span><span>#</span> also make sure to input the correct up and front axis (in +x, +y, +z, -x, -y, -z)</span>
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>hulk<span>"</span></span> system.geometry.shape_init=mesh:load/shapes/human.obj system.geometry.shape_init_params=0.9 system.geometry.shape_init_mesh_up=+y system.geometry.shape_init_mesh_front=+z
<span><span>#</span> --------- Texture --------- #</span>
<span><span>#</span> to train PBR texture continued from a geometry checkpoint:</span>
python launch.py --config configs/fantasia3d-texture.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a DSLR photo of an ice cream sundae<span>"</span></span> system.geometry_convert_from=path/to/geometry/stage/trial/dir/ckpts/last.ckpt</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>If you find the shape easily diverge in early training stages, you may use a lower guidance scale by setting <code>system.guidance.guidance_scale=30.</code>.</li>
</ul>
<h3 tabindex="-1" dir="auto">TextMesh <a href="https://arxiv.org/abs/2304.12439" rel="nofollow"><img src="https://camo.githubusercontent.com/1ef5ad9a7578c2b914416b439572e33f7443a2746362edf2a0b232e59a085351/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330342e31323433392d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2304.12439-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (DeepFloyd IF, batch size 4)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description textmesh-if.mp4">textmesh-if.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/245696160-72217cdd-765a-475b-92d0-4ab62bf0f57a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTYxNjAtNzIyMTdjZGQtNzY1YS00NzViLTkyZDAtNGFiNjJiZjBmNTdhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE0ZjIxZTc0MDQxNGM3ODBlNzk0MWViNDdkM2EwYTdhNDQwZjdiZDYwMTU5ZTQyOTk3MTk3ZmMyZjliMTc4YWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.IxiDrL4A-nvPYk0JfPFjyb7mGPR0uuQR2SsQW5jSIo0" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/245696160-72217cdd-765a-475b-92d0-4ab62bf0f57a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTYxNjAtNzIyMTdjZGQtNzY1YS00NzViLTkyZDAtNGFiNjJiZjBmNTdhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE0ZjIxZTc0MDQxNGM3ODBlNzk0MWViNDdkM2EwYTdhNDQwZjdiZDYwMTU5ZTQyOTk3MTk3ZmMyZjliMTc4YWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.IxiDrL4A-nvPYk0JfPFjyb7mGPR0uuQR2SsQW5jSIo0" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>Most of the settings are the same as the DreamFusion model. Please refer to the notable differences of the DreamFusion model.</li>
<li>We use NeuS as the geometry representation while the original paper uses VolSDF.</li>
<li>We adopt techniques from <a href="https://arxiv.org/abs/2306.03092" rel="nofollow">Neuralangelo</a> to stablize normal computation when using hash grids.</li>
<li>We currently only implemented the coarse stage of TextMesh.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# uses DeepFloyd IF, requires ~15GB VRAM
python launch.py --config configs/textmesh-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;lib:cowboy_boots&quot;"><pre><span><span>#</span> uses DeepFloyd IF, requires ~15GB VRAM</span>
python launch.py --config configs/textmesh-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>lib:cowboy_boots<span>"</span></span></pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>TextMesh uses a surface-based geometry representation, so you don't need to manually tune the isosurface threshold when exporting meshes!</li>
</ul>
<h3 tabindex="-1" dir="auto">Control4D <a href="https://arxiv.org/abs/2305.20082" rel="nofollow"><img src="https://camo.githubusercontent.com/fd4b0abaf42e9ee37229eb60b5910b3e4148c1c96d4bb7ff7ac204e6426221d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330352e32303038322d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2305.20082-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>This is an experimental implementation of Control4D using threestudio! Control4D will release the full code including static and dynamic editing after paper acceptance.</strong></p>
<p dir="auto"><strong>Results obtained by threestudio (512x512)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description origin_1.mp4">origin_1.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/24589363/247185185-97d9aadd-32c7-488f-9543-6951b285d588.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODUxODUtOTdkOWFhZGQtMzJjNy00ODhmLTk1NDMtNjk1MWIyODVkNTg4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgyZWQ5MTM4ODVlMGNjNTcxMmJkNDkxYjcxYTYzNDVmNjZkZWU0OTZkZjMyMjA4MDA0YzIzNmZhMTY5MTE3ODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.x-8BtwUBdXv9YKKpy3Pf0KxOPYTX3DSTWgFOeETn16o" data-canonical-src="https://private-user-images.githubusercontent.com/24589363/247185185-97d9aadd-32c7-488f-9543-6951b285d588.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODUxODUtOTdkOWFhZGQtMzJjNy00ODhmLTk1NDMtNjk1MWIyODVkNTg4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgyZWQ5MTM4ODVlMGNjNTcxMmJkNDkxYjcxYTYzNDVmNjZkZWU0OTZkZjMyMjA4MDA0YzIzNmZhMTY5MTE3ODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.x-8BtwUBdXv9YKKpy3Pf0KxOPYTX3DSTWgFOeETn16o" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">We currently don't support dynamic editing.</p>
<p dir="auto">Download the data sample of control4D using this <a href="https://mailstsinghuaeducn-my.sharepoint.com/:u:/g/personal/shaorz20_mails_tsinghua_edu_cn/EcqOaEuNwH1KpR0JTzL4Ur0BO_iJr8RiY2rNAGVC7h3fng?e=Dyr2gu" rel="nofollow">link</a>.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- Control4D --------- #
# static editing with 128x128 NeRF + 512x512 GAN rendering, ~20GB VRAM
python launch.py --config configs/control4d-static.yaml --train --gpu 0 data.dataroot=&quot;YOUR_DATAROOT/twindom&quot; system.prompt_processor.prompt=&quot;Elon Musk wearing red shirt, RAW photo, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3&quot;"><pre><span><span>#</span> --------- Control4D --------- #</span>
<span><span>#</span> static editing with 128x128 NeRF + 512x512 GAN rendering, ~20GB VRAM</span>
python launch.py --config configs/control4d-static.yaml --train --gpu 0 data.dataroot=<span><span>"</span>YOUR_DATAROOT/twindom<span>"</span></span> system.prompt_processor.prompt=<span><span>"</span>Elon Musk wearing red shirt, RAW photo, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3<span>"</span></span></pre></div>
<h3 tabindex="-1" dir="auto">InstructNeRF2NeRF <a href="https://arxiv.org/abs/2303.12789" rel="nofollow"><img src="https://camo.githubusercontent.com/9187e8930897819d323fd5d972e6473d03185795beeec7cc9091f417dc0cc8d2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330332e31323738392d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2303.12789-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description in2n.mp4">in2n.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/24589363/247187180-7aa43a2d-87d7-4ef5-94b6-f778ddb041b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODcxODAtN2FhNDNhMmQtODdkNy00ZWY1LTk0YjYtZjc3OGRkYjA0MWI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEzM2YzZTgwNDY1YjY0MGU0NGJlYzkzMDJiZTQ3MDBkYTE2N2IyYzc5ZTdjNGVmMzhiODRjNDNiYzdjZjQ3ZDUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lwgDP0NkmZkjvvfzEJoM54FEZ3NgSHeWoLBNaC3Z0RQ" data-canonical-src="https://private-user-images.githubusercontent.com/24589363/247187180-7aa43a2d-87d7-4ef5-94b6-f778ddb041b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODcxODAtN2FhNDNhMmQtODdkNy00ZWY1LTk0YjYtZjc3OGRkYjA0MWI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEzM2YzZTgwNDY1YjY0MGU0NGJlYzkzMDJiZTQ3MDBkYTE2N2IyYzc5ZTdjNGVmMzhiODRjNDNiYzdjZjQ3ZDUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lwgDP0NkmZkjvvfzEJoM54FEZ3NgSHeWoLBNaC3Z0RQ" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Download the data sample of InstructNeRF2NeRF using this <a href="https://mailstsinghuaeducn-my.sharepoint.com/:u:/g/personal/shaorz20_mails_tsinghua_edu_cn/EbNazeNAYsBIvxGeXuCmOXgBiLv8KM-hfRNbNS7DtTvSvA?e=C1k4bM" rel="nofollow">link</a>.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- InstructNeRF2NeRF --------- #
# 3D editing with NeRF patch-based rendering, ~20GB VRAM
python launch.py --config configs/instructnerf2nerf.yaml --train --gpu 0 data.dataroot=&quot;YOUR_DATAROOT/face&quot; data.camera_layout=&quot;front&quot; data.camera_distance=1 data.eval_interpolation=[1,3,50] system.prompt_processor.prompt=&quot;Turn him into Albert Einstein&quot;"><pre><span><span>#</span> --------- InstructNeRF2NeRF --------- #</span>
<span><span>#</span> 3D editing with NeRF patch-based rendering, ~20GB VRAM</span>
python launch.py --config configs/instructnerf2nerf.yaml --train --gpu 0 data.dataroot=<span><span>"</span>YOUR_DATAROOT/face<span>"</span></span> data.camera_layout=<span><span>"</span>front<span>"</span></span> data.camera_distance=1 data.eval_interpolation=[1,3,50] system.prompt_processor.prompt=<span><span>"</span>Turn him into Albert Einstein<span>"</span></span></pre></div>
<h3 tabindex="-1" dir="auto">Magic123 <a href="https://arxiv.org/abs/2306.17843" rel="nofollow"><img src="https://camo.githubusercontent.com/fc721c573072ceed6e5ffadac512640054425b25449d462163de96d1e99800b8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330362e31373834332d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2306.17843-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Zero123 + Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description magic123.mp4">magic123.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/263146263-335a58a8-8fee-485b-ac27-c55a16f4a673.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMxNDYyNjMtMzM1YTU4YTgtOGZlZS00ODViLWFjMjctYzU1YTE2ZjRhNjczLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWJiNTM0NTczZWE1ZjFiNmExOTA2YWNkNjE3ZThjNTI4M2JmMmFiZjE5OTNkMDlmYWI4MjVkNjVhZDI4MTc5NjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nztBsQhofQ6SkQg_7wiJjoW5bxuAZGm9qEACjFU8d14" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/263146263-335a58a8-8fee-485b-ac27-c55a16f4a673.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMxNDYyNjMtMzM1YTU4YTgtOGZlZS00ODViLWFjMjctYzU1YTE2ZjRhNjczLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWJiNTM0NTczZWE1ZjFiNmExOTA2YWNkNjE3ZThjNTI4M2JmMmFiZjE5OTNkMDlmYWI4MjVkNjVhZDI4MTc5NjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nztBsQhofQ6SkQg_7wiJjoW5bxuAZGm9qEACjFU8d14" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>This is an unofficial re-implementation which shares the same overall idea with the <a href="https://github.com/guochengqian/Magic123">official implementation</a> but differs in some aspects like hyperparameters.</li>
<li>Textual Inversion is not supported, which means a text prompt is needed for training.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<p dir="auto">First train the coarse stage NeRF:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Zero123 + Stable Diffusion, ~12GB VRAM
# data.image_path must point to a 4-channel RGBA image
# system.prompt_proessor.prompt must be specified
python launch.py --config configs/magic123-coarse-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=&quot;a delicious hamburger&quot;"><pre><span><span>#</span> Zero123 + Stable Diffusion, ~12GB VRAM</span>
<span><span>#</span> data.image_path must point to a 4-channel RGBA image</span>
<span><span>#</span> system.prompt_proessor.prompt must be specified</span>
python launch.py --config configs/magic123-coarse-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span></pre></div>
<p dir="auto">Then convert the NeRF from the coarse stage to DMTet and train with differentiable rasterization:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Zero123 + Stable Diffusion, ~10GB VRAM
# data.image_path must point to a 4-channel RGBA image
# system.prompt_proessor.prompt must be specified
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
# if you're unsatisfied with the surface extracted using the default threshold (25)
# you can specify a threshold value using `system.geometry_convert_override`
# decrease the value if the extracted surface is incomplete, increase if it is extruded
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10."><pre><span><span>#</span> Zero123 + Stable Diffusion, ~10GB VRAM</span>
<span><span>#</span> data.image_path must point to a 4-channel RGBA image</span>
<span><span>#</span> system.prompt_proessor.prompt must be specified</span>
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
<span><span>#</span> if you're unsatisfied with the surface extracted using the default threshold (25)</span>
<span><span>#</span> you can specify a threshold value using `system.geometry_convert_override`</span>
<span><span>#</span> decrease the value if the extracted surface is incomplete, increase if it is extruded</span>
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10.</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>If the image contains non-front-facing objects, specifying the approximate elevation and azimuth angle by setting <code>data.default_elevation_deg</code> and <code>data.default_azimuth_deg</code> can be helpful. In threestudio, top is elevation +90 and bottom is elevation -90; left is azimuth -90 and right is azimuth +90.</li>
</ul>
<h3 tabindex="-1" dir="auto">Stable Zero123</h3>
<p dir="auto"><strong>Installation</strong></p>
<p dir="auto">Download pretrained Stable Zero123 checkpoint <code>stable-zero123.ckpt</code> into <code>load/zero123</code> from <a href="https://huggingface.co/stabilityai/stable-zero123" rel="nofollow">https://huggingface.co/stabilityai/stable-zero123</a></p>
<p dir="auto"><strong>Results obtained by threestudio (Stable Zero123 vs Zero123-XL)</strong>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/22424247/289674233-bf2d2213-5027-489c-a6ba-1c56c14ee8b7.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yODk2NzQyMzMtYmYyZDIyMTMtNTAyNy00ODljLWE2YmEtMWM1NmMxNGVlOGI3LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwOTY5NDY4ZTM2OTk4Mzg0YmE2YTFkMmUzNWQ0ODY4M2FlYmM0MDEwZjk2ZjJjYjQ1MzMyYjkzODI2NTg3ODQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.v6OV91vhNJ8WxaWudRqGQ8M0JDSnTzK09dXnDhhnlVg"><img src="https://private-user-images.githubusercontent.com/22424247/289674233-bf2d2213-5027-489c-a6ba-1c56c14ee8b7.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yODk2NzQyMzMtYmYyZDIyMTMtNTAyNy00ODljLWE2YmEtMWM1NmMxNGVlOGI3LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwOTY5NDY4ZTM2OTk4Mzg0YmE2YTFkMmUzNWQ0ODY4M2FlYmM0MDEwZjk2ZjJjYjQ1MzMyYjkzODI2NTg3ODQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.v6OV91vhNJ8WxaWudRqGQ8M0JDSnTzK09dXnDhhnlVg" alt="Final_video_v01" data-animated-image=""></a></p>
<p dir="auto"><strong>Example running commands</strong></p>
<ol dir="auto">
<li>Take an image of your choice, or generate it from text using your favourite AI image generator such as SDXL Turbo (<a href="https://clipdrop.co/stable-diffusion-turbo" rel="nofollow">https://clipdrop.co/stable-diffusion-turbo</a>) E.g. "A simple 3D render of a friendly dog"</li>
<li>Remove its background using Clipdrop (<a href="https://clipdrop.co/remove-background" rel="nofollow">https://clipdrop.co/remove-background</a>)</li>
<li>Save to <code>load/images/</code>, preferably with <code>_rgba.png</code> as the suffix</li>
<li>Run Zero-1-to-3 with the Stable Zero123 ckpt:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python launch.py --config configs/stable-zero123.yaml --train --gpu 0 data.image_path=./load/images/hamburger_rgba.png"><pre>python launch.py --config configs/stable-zero123.yaml --train --gpu 0 data.image_path=./load/images/hamburger_rgba.png</pre></div>
<p dir="auto"><strong>IMPORTANT NOTE: This is an experimental implementation and we're constantly improving the quality.</strong></p>
<p dir="auto"><strong>IMPORTANT NOTE: This implementation extends the Zero-1-to-3 implementation below, and is heavily inspired from the Zero-1-to-3 implementation in <a href="https://github.com/threestudio-project/threestudio/blob/main/stable-dreamfusion">https://github.com/ashawkey/stable-dreamfusion</a>! <code>extern/ldm_zero123</code> is borrowed from <code>stable-dreamfusion/ldm</code>.</strong></p>
<h3 tabindex="-1" dir="auto">Zero-1-to-3 <a href="https://arxiv.org/abs/2303.11328" rel="nofollow"><img src="https://camo.githubusercontent.com/cff3bb989636a7adcd8dfd9b30f2be23d690ed419b7e6b0a5fd65f147731cbc1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330332e31313332382d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2303.11328-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Installation</strong></p>
<p dir="auto">Download pretrained Zero123XL weights into <code>load/zero123</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd load/zero123
wget https://zero123.cs.columbia.edu/assets/zero123-xl.ckpt"><pre><span>cd</span> load/zero123
wget https://zero123.cs.columbia.edu/assets/zero123-xl.ckpt</pre></div>
<p dir="auto"><strong>Results obtained by threestudio (Zero-1-to-3)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description ezgif-3-355a192487.mp4">ezgif-3-355a192487.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/22424247/252868915-f4e7b66f-7a46-4f9f-8fcd-750300cef651.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNTI4Njg5MTUtZjRlN2I2NmYtN2E0Ni00ZjlmLThmY2QtNzUwMzAwY2VmNjUxLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk5YmI2MGRhZGY0Zjc5MWMzZDUwNTNhMjgyNDk0ODhkZWQ5NDE1ZjJmYzhkOThhN2YxNDEyNTJmMDg0NzY0NGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.NZxBwlaLEPYsqbRhn4QJwiV6iF5_0AnZaSGKlg-cF2c" data-canonical-src="https://private-user-images.githubusercontent.com/22424247/252868915-f4e7b66f-7a46-4f9f-8fcd-750300cef651.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNTI4Njg5MTUtZjRlN2I2NmYtN2E0Ni00ZjlmLThmY2QtNzUwMzAwY2VmNjUxLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk5YmI2MGRhZGY0Zjc5MWMzZDUwNTNhMjgyNDk0ODhkZWQ5NDE1ZjJmYzhkOThhN2YxNDEyNTJmMDg0NzY0NGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.NZxBwlaLEPYsqbRhn4QJwiV6iF5_0AnZaSGKlg-cF2c" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>IMPORTANT NOTE: This is an experimental implementation and we're constantly improving the quality.</strong></p>
<p dir="auto"><strong>IMPORTANT NOTE: This implementation is heavily inspired from the Zero-1-to-3 implementation in <a href="https://github.com/threestudio-project/threestudio/blob/main/stable-dreamfusion">https://github.com/ashawkey/stable-dreamfusion</a>! <code>extern/ldm_zero123</code> is borrowed from <code>stable-dreamfusion/ldm</code>.</strong></p>
<p dir="auto"><strong>Example running commands</strong></p>
<ol dir="auto">
<li>Take an image of your choice, or generate it from text using your favourite AI image generator such as Stable Diffusion XL (<a href="https://clipdrop.co/stable-diffusion" rel="nofollow">https://clipdrop.co/stable-diffusion</a>) E.g. "A simple 3D render of a friendly dog"</li>
<li>Remove its background using Clipdrop (<a href="https://clipdrop.co/remove-background" rel="nofollow">https://clipdrop.co/remove-background</a>)</li>
<li>Save to <code>load/images/</code>, preferably with <code>_rgba.png</code> as the suffix</li>
<li>Run Zero-1-to-3:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python launch.py --config configs/zero123.yaml --train --gpu 0 data.image_path=./load/images/dog1_rgba.png"><pre>python launch.py --config configs/zero123.yaml --train --gpu 0 data.image_path=./load/images/dog1_rgba.png</pre></div>
<p dir="auto">For more scripts for Zero-1-to-3, please check <code>threestudio/scripts/run_zero123.sh</code>.</p>
<p dir="auto">Previous Zero-1-to-3 weights are available at <code>https://huggingface.co/cvlab/zero123-weights/</code>. You can download them to <code>load/zero123</code> as above, and replace the path at <code>system.guidance.pretrained_model_name_or_path</code>.</p>
<p dir="auto"><strong>Guidance evaluation</strong></p>
<p dir="auto">Also includes evaluation of the guidance during training. If <code>system.freq.guidance_eval</code> is set to a value &gt; 0, this will save rendered image, noisy image (noise added mentioned at top left), 1-step-denoised image, 1-step prediction of original image, fully denoised image. For example:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/22424247/242720649-c8e7d835-4937-4852-bfb0-3e906e6b66b7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNDI3MjA2NDktYzhlN2Q4MzUtNDkzNy00ODUyLWJmYjAtM2U5MDZlNmI2NmI3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIzMzIxNzM1ZmQwODgzODJkYTgxMmVjNDE1OTUzODU5OWY2MTFmNWMwZmQzZjEwOWI5NmFmMzJhODdhMmY3OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xBAJWPzi1QbuZgb3ujJScfy8tyKgNhkQMXyRw-vp4LU"><img src="https://private-user-images.githubusercontent.com/22424247/242720649-c8e7d835-4937-4852-bfb0-3e906e6b66b7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNDI3MjA2NDktYzhlN2Q4MzUtNDkzNy00ODUyLWJmYjAtM2U5MDZlNmI2NmI3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIzMzIxNzM1ZmQwODgzODJkYTgxMmVjNDE1OTUzODU5OWY2MTFmNWMwZmQzZjEwOWI5NmFmMzJhODdhMmY3OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xBAJWPzi1QbuZgb3ujJScfy8tyKgNhkQMXyRw-vp4LU" alt="it143-train"></a></p>
<h3 tabindex="-1" dir="auto">More to come, please stay tuned.</h3>

<p dir="auto"><strong>If you would like to contribute a new method to threestudio, see <a href="https://github.com/threestudio-project/threestudio#contributing-to-threestudio">here</a>.</strong></p>
<h2 tabindex="-1" dir="auto">Prompt Library</h2>
<p dir="auto">For easier comparison, we collect the 397 preset prompts from the website of <a href="https://dreamfusion3d.github.io/gallery.html" rel="nofollow">DreamFusion</a> in <a href="https://github.com/threestudio-project/threestudio/blob/main/load/prompt_library.json">this file</a>. You can use these prompts by setting <code>system.prompt_processor.prompt=lib:keyword1_keyword2_..._keywordN</code>. Note that the prompt should starts with <code>lib:</code> and all the keywords are separated by <code>_</code>. The prompt processor will match the keywords to all the prompts in the library, and will only succeed if there's <strong>exactly one match</strong>. The used prompt will be printed to the console. Also note that you can't use this syntax to point to every prompt in the library, as there are prompts that are subset of other prompts lmao. We will enhance the use of this feature.</p>
<h2 tabindex="-1" dir="auto">Tips on Improving Quality</h2>
<p dir="auto">It's important to note that existing techniques that lift 2D T2I models to 3D cannot consistently produce satisfying results. Results from great papers like DreamFusion and Magic3D are (to some extent) cherry-pickled, so don't be frustrated if you do not get what you expected on your first trial. Here are some tips that may help you improve the generation quality:</p>
<ul dir="auto">
<li><strong>Increase batch size</strong>. Large batch sizes help convergence and improve the 3D consistency of the geometry. State-of-the-art methods claim using large batch sizes: DreamFusion uses a batch size of 4; Magic3D uses a batch size of 32; Fantasia3D uses a batch size of 24; some results shown above use a batch size of 8. You can easily change the batch size by setting <code>data.batch_size=N</code>. Increasing the batch size requires more VRAM. If you have limited VRAM but still want the benefit of large batch sizes, you may use <a href="https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#accumulate-gradients" rel="nofollow">gradient accumulation provided by PyTorch Lightning</a> by setting <code>trainer.accumulate_grad_batches=N</code>. This will accumulate the gradient of several batches and achieve a large effective batch size. Note that if you use gradient accumulation, you may need to multiply all step values by N times in your config, such as values that have the name <code>X_steps</code> and <code>trainer.val_check_interval</code>, since now N batches equal to a large batch.</li>
<li><strong>Train longer.</strong> This helps if you can already obtain reasonable results and would like to enhance the details. If the result is still a mess after several thousand steps, training for a longer time often won't help. You can set the total training iterations by <code>trainer.max_steps=N</code>.</li>
<li><strong>Try different seeds.</strong> This is a simple solution if your results have correct overall geometry but suffer from the multi-face Janus problem. You can change the seed by setting <code>seed=N</code>. Good luck!</li>
<li><strong>Tuning regularization weights.</strong> Some methods have regularization terms which can be essential to obtaining good geometry. Try tuning the weights of these regularizations by setting <code>system.loss.lambda_X=value</code>. The specific values depend on your situation, you may refer to <a href="https://github.com/threestudio-project/threestudio#supported-models">tips for each supported model</a> for more detailed instructions.</li>
<li><strong>Try debiasing methods.</strong> When conventional SDS techniques like DreamFusion, Magic3D, SJC, and others fail to produce the desired 3D results, Debiased Score Distillation Sampling (D-SDS) can be a solution. D-SDS is devised to tackle challenges such as artifacts or the Janus problem, employing two strategies: score debiasing and prompt debiasing. You can activate score debiasing by just setting <code>system.guidance.grad_clip=[0,0.5,2.0,10000]</code>, where the order is <code>start_step, start_value, end_value, end_step</code>. You can enable prompt debiasing by setting <code>system.prompt_processor.use_prompt_debiasing=true</code>. When using prompt debiasing, it's recommended to set a list of indices for words that should potentially be removed by <code>system.prompt_processor.prompt_debiasing_mask_ids=[i1,i2,...]</code>. For example, if the prompt is <code>a smiling dog</code> and you only want to remove the word <code>smiling</code> for certain views, you should set it to <code>[1]</code>. You could also manually specify the prompt for each view by setting <code>system.prompt_processor.prompt_side</code>, <code>system.prompt_processor.prompt_back</code> and <code>system.prompt_processor.prompt_overhead</code>. For a detailed explanation of these techniques, refer to <a href="https://arxiv.org/abs/2303.15413" rel="nofollow">the D-SDS paper</a> or check out <a href="https://susunghong.github.io/Debiased-Score-Distillation-Sampling/" rel="nofollow">the project page</a>.</li>
<li><strong>Try Perp-Neg.</strong> The <a href="https://perp-neg.github.io/" rel="nofollow">Perp-Neg algorithm</a> can potentially alleviate the multi-face Janus problem. We now support Perp-Neg for <code>stable-diffusion-guidance</code> and <code>deep-floyd-guidance</code> by setting <code>system.prompt_processor.use_perp_neg=true</code>.</li>
</ul>
<h2 tabindex="-1" dir="auto">VRAM Optimization</h2>
<p dir="auto">If you encounter CUDA OOM error, try the following in order (roughly sorted by recommendation) to meet your VRAM requirement.</p>
<ul dir="auto">
<li>If you only encounter OOM at validation/test time, you can set <code>system.cleanup_after_validation_step=true</code> and <code>system.cleanup_after_test_step=true</code> to free memory after each validation/test step. This will slow down validation/testing.</li>
<li>Use a smaller batch size or use gradient accumulation as demonstrated <a href="https://github.com/threestudio-project/threestudio#tips-on-improving-quality">here</a>.</li>
<li>If you are using PyTorch1.x, enable <a href="https://huggingface.co/docs/diffusers/optimization/fp16#memory-efficient-attention" rel="nofollow">memory efficient attention</a> by setting <code>system.guidance.enable_memory_efficient_attention=true</code>. PyTorch2.0 has built-in support for this optimization and is enabled by default.</li>
<li>Enable <a href="https://huggingface.co/docs/diffusers/optimization/fp16#sliced-attention-for-additional-memory-savings" rel="nofollow">attention slicing</a> by setting <code>system.guidance.enable_attention_slicing=true</code>. This will slow down training by ~20%.</li>
<li>If you are using StableDiffusionGuidance, you can use <a href="https://github.com/dbolya/tomesd">Token Merging</a> to <strong>drastically</strong> speed up computation and save memory. You can easily enable Token Merging by setting <code>system.guidance.token_merging=true</code>. You can also customize the Token Merging behavior by setting the parameters <a href="https://github.com/dbolya/tomesd/blob/main/tomesd/patch.py#L183-L213">here</a> to <code>system.guidance.token_merging_params</code>. Note that Token Merging may degrade generation quality.</li>
<li>Enable <a href="https://huggingface.co/docs/diffusers/optimization/fp16#offloading-to-cpu-with-accelerate-for-memory-savings" rel="nofollow">sequential CPU offload</a> by setting <code>system.guidance.enable_sequential_cpu_offload=true</code>. This could save a lot of VRAM but will make the training <strong>extremely slow</strong>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">threestudio use <a href="https://github.com/omry/omegaconf">OmegaConf</a> to manage configurations. You can literally change anything inside the yaml configuration file or by adding command line arguments without <code>--</code>. We list all arguments that you can change in the configuration in our <a href="https://github.com/threestudio-project/threestudio/blob/main/DOCUMENTATION.md">documentation</a>. Happy experimenting!</p>
<h2 tabindex="-1" dir="auto">wandb (Weights &amp; Biases) logging</h2>
<p dir="auto">To enable the (experimental) wandb support, set <code>system.loggers.wandb.enable=true</code>, e.g.:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python launch.py --config configs/zero123.yaml --train --gpu 0 system.loggers.wandb.enable=true`"><pre>python launch.py --config configs/zero123.yaml --train --gpu 0 system.loggers.wandb.enable=true<span><span>`</span></span></pre></div>
<p dir="auto">If you're using a corporate wandb server, you may first need to login to your wandb instance, e.g.:
<code>wandb login --host=https://COMPANY_XYZ.wandb.io --relogin</code></p>
<p dir="auto">By default the runs will have a random name, recorded in the <code>threestudio</code> project. You can override them to give a more descriptive name, e.g.:</p>
<p dir="auto"><code>python launch.py --config configs/zero123.yaml --train --gpu 0 system.loggers.wandb.enable=true system.loggers.wandb.name="zero123xl_accum;bs=4;lr=0.05"</code></p>
<h2 tabindex="-1" dir="auto">Contributing to threestudio</h2>
<ul dir="auto">
<li>Fork the repository and create your branch from <code>main</code>.</li>
<li>Install development dependencies:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements-dev.txt"><pre>pip install -r requirements-dev.txt</pre></div>
<ul dir="auto">
<li>
<p dir="auto">If you are using VSCode as the text editor: (1) Install <code>editorconfig</code> extension. (2) Set the default linter to mypy to enable static type checking. (3) Set the default formatter to black. You could either manually format the document or let the editor format the document each time it is saved by setting <code>"editor.formatOnSave": true</code>.</p>
</li>
<li>
<p dir="auto">Run <code>pre-commit install</code> to install pre-commit hooks which will automatically format the files before commit.</p>
</li>
<li>
<p dir="auto">Make changes to the code, update README and DOCUMENTATION if needed, and open a pull request.</p>
</li>
</ul>
<h3 tabindex="-1" dir="auto">Code Structure</h3>
<p dir="auto">Here we just briefly introduce the code structure of this project. We will make more detailed documentation about this in the future.</p>
<ul dir="auto">
<li>All methods are implemented as a subclass of <code>BaseSystem</code> (in <code>systems/base.py</code>). There typically are six modules inside a system: geometry, material, background, renderer, guidance, and prompt_processor. All modules are subclass of <code>BaseModule</code> (in <code>utils/base.py</code>) except for guidance, and prompt_processor, which are subclass of <code>BaseObject</code> to prevent them from being treated as model parameters and better control their behavior in multi-GPU settings.</li>
<li>All systems, modules, and data modules have their configurations in their own dataclasses.</li>
<li>Base configurations for the whole project can be found in <code>utils/config.py</code>. In the <code>ExperimentConfig</code> dataclass, <code>data</code>, <code>system</code>, and module configurations under <code>system</code> are parsed to configurations of each class mentioned above. These configurations are strictly typed, which means you can only use defined properties in the dataclass and stick to the defined type of each property. This configuration paradigm (1) naturally supports default values for properties; (2) effectively prevents wrong assignments of these properties (say typos in the yaml file) or inappropriate usage at runtime.</li>
<li>This projects use both static and runtime type checking. For more details, see <code>utils/typing.py</code>.</li>
<li>To update anything of a module at each training step, simply make it inherit to <code>Updateable</code> (see <code>utils/base.py</code>). At the beginning of each iteration, an <code>Updateable</code> will update itself, and update all its attributes that are also <code>Updateable</code>. Note that subclasses of <code>BaseSystem</code>, <code>BaseModule</code> and <code>BaseObject</code> are by default inherited to <code>Updateable</code>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Known Problems</h2>
<ul dir="auto">
<li>Gradients of Vanilla MLP parameters are empty in AMP (temporarily fixed by disabling autocast).</li>
<li>FullyFused MLP may cause NaNs in 32 precision.</li>
</ul>
<h2 tabindex="-1" dir="auto">Credits</h2>
<p dir="auto">threestudio is built on the following amazing open-source projects:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/Lightning-AI/lightning">Lightning</a></strong> Framework for creating highly organized PyTorch code.</li>
<li><strong><a href="https://github.com/omry/omegaconf">OmegaConf</a></strong> Flexible Python configuration system.</li>
<li><strong><a href="https://github.com/KAIR-BAIR/nerfacc">NerfAcc</a></strong> Plug-and-play NeRF acceleration.</li>
</ul>
<p dir="auto">The following repositories greatly inspire threestudio:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/ashawkey/stable-dreamfusion">Stable-DreamFusion</a></strong></li>
<li><strong><a href="https://github.com/eladrich/latent-nerf">Latent-NeRF</a></strong></li>
<li><strong><a href="https://github.com/pals-ttic/sjc">Score Jacobian Chaining</a></strong></li>
<li><strong><a href="https://github.com/ashawkey/fantasia3d.unofficial">Fantasia3D.unofficial</a></strong></li>
</ul>
<p dir="auto">Thanks to the maintainers of these projects for their contribution to the community!</p>
<h2 tabindex="-1" dir="auto">Citing threestudio</h2>
<p dir="auto">If you find threestudio helpful, please consider citing:</p>
<div data-snippet-clipboard-copy-content="@Misc{threestudio2023,
  author =       {Yuan-Chen Guo and Ying-Tian Liu and Ruizhi Shao and Christian Laforte and Vikram Voleti and Guan Luo and Chia-Hao Chen and Zi-Xin Zou and Chen Wang and Yan-Pei Cao and Song-Hai Zhang},
  title =        {threestudio: A unified framework for 3D content generation},
  howpublished = {\url{https://github.com/threestudio-project/threestudio}},
  year =         {2023}
}"><pre><code>@Misc{threestudio2023,
  author =       {Yuan-Chen Guo and Ying-Tian Liu and Ruizhi Shao and Christian Laforte and Vikram Voleti and Guan Luo and Chia-Hao Chen and Zi-Xin Zou and Chen Wang and Yan-Pei Cao and Song-Hai Zhang},
  title =        {threestudio: A unified framework for 3D content generation},
  howpublished = {\url{https://github.com/threestudio-project/threestudio}},
  year =         {2023}
}
</code></pre></div>
</article>
          </div></div>]]></description>
        </item>
    </channel>
</rss>