<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 30 Jul 2023 01:00:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Big Tobacco knew radioactive Po210 in cigarettes posed cancer risk, kept quiet (213 pts)]]></title>
            <link>https://www.uclahealth.org/news/big-tobacco-knew-radioactive-particles-in-cigarettes</link>
            <guid>36925019</guid>
            <pubDate>Sat, 29 Jul 2023 21:47:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.uclahealth.org/news/big-tobacco-knew-radioactive-particles-in-cigarettes">https://www.uclahealth.org/news/big-tobacco-knew-radioactive-particles-in-cigarettes</a>, See on <a href="https://news.ycombinator.com/item?id=36925019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Tobacco companies knew that cigarette smoke contained radioactive alpha particles for more than four decades and developed "deep and intimate" knowledge of these particles' cancer-causing potential, but they deliberately kept their findings from the public, according to a new study by UCLA researchers.</p>  <p>The analysis of dozens of previously unexamined internal tobacco industry documents, made available in 1998 as the result of a legal settlement, reveals that the industry was aware of cigarette radioactivity some five years earlier than previously thought and that tobacco companies, concerned about the potential lung cancer risk, began in-depth investigations into the possible effects of radioactivity on smokers as early as the 1960s.</p>  <p>"The documents show that the industry was well aware of the presence of a radioactive substance in tobacco as early as 1959," the authors write. "Furthermore, the industry was not only cognizant of the potential 'cancerous growth' in the lungs of regular smokers, but also did quantitative radiobiological calculations to estimate the long-term lung radiation absorption dose of ionizing alpha particles emitted from cigarette smoke."</p>  <p>The study, published&nbsp;online&nbsp;Sept. 27&nbsp;in Nicotine &amp; Tobacco Research, the peer-reviewed journal of the Society for Research on Nicotine and Tobacco, adds to a growing body of research detailing the industry's knowledge of cigarette smoke radioactivity and its efforts to suppress that information.</p>  <p>"They knew that the cigarette smoke was radioactive way back then and that it could potentially result in cancer, and they deliberately kept that information under wraps," said the study's first author, Hrayr S. Karagueuzian, an adjunct professor of cardiology who conducts research at UCLA's Cardiovascular Research Laboratory, part of the David Geffen School of Medicine at UCLA. "Specifically, we show here that the industry used misleading statements to obfuscate the hazard of ionizing alpha particles to the lungs of smokers and, more importantly, banned any and all publication on tobacco smoke radioactivity."</p>  <p>The radioactive substance — which the UCLA study shows was first brought to the attention of the tobacco industry in 1959 — was identified in 1964 as the isotope polonium-210, which emits carcinogenic alpha radiation. Polonium-210 can be found in all commercially available domestic and foreign cigarette brands, Karagueuzian said, and is absorbed by tobacco leaves through naturally occurring radon gas in the atmosphere and through high-phosphate chemical fertilizers used by tobacco growers. The substance is eventually inhaled by smokers into the lungs.</p>  <p>The study outlines the industry's growing concerns about the cancer risk posed by polonium-210 inhalation and the research that industry scientists conducted over the decades to assess the radioactive isotope's potential effect on smokers — including one study that quantitatively measured the potential lung burden from radiation exposure in a two-pack-a-day smoker over a two-decade period.</p>  <p>Karagueuzian and his colleagues made independent calculations using industry and academic data and arrived at results that very closely mirrored those of that industry study, which was conducted nearly a quarter-century ago. They then compared those results to rates used by the Environmental Protection Agency to estimate lung cancer risk among individuals exposed to similar amounts of alpha particle–emitting radon gas in their homes.</p>  <p>"The gathered data from the documents on the relevant radiobiological parameters of the alpha particles — such as dose, distribution and retention time — permitted us to duplicate the industry's secretly estimated radiation absorbed dose by regular smokers over a 20- or 25-year period, which equaled 40 to 50 rads," he said. "These levels of rads, according to the EPA's estimate of lung cancer risk in residents exposed to radon gas, equal 120 to 138 deaths per 1,000 regular smokers over a 25-year period."</p>  <p>Despite the potential risk of lung cancer, tobacco companies declined to adopt a technique discovered in 1959, and another discovered&nbsp;1980, that could have helped eliminate polonium-210 from tobacco, the researchers said. The technique, known as an acid-wash, was found to be highly effective in removing the radioisotope from tobacco plants, where it forms a water-insoluble complex with the sticky, hair-like structures called trichomes that cover the leaves.</p>  <p>And while the industry frequently cited concerns over the cost and the possible environmental impact as rationales for not using the acid wash, UCLA researchers uncovered documents that they say indicate the reason may have been far different.</p>  <p>"The industry was concerned that the acid media would ionize the nicotine, making it more difficult to be absorbed into the brains of smokers and depriving them of that instant nicotine rush that fuels their addiction," Karagueuzian said. "The industry also were well aware that the curing of the tobacco leaves for more than a one-year period also would not eliminate the polonium-210, which has a half-life of 135 days, from the tobacco leaves because it was derived from its parent, lead-210, which has a half-life of 22 years."<em></em></p>  <div><p>Karagueuzian said the insoluble alpha particles bind with resins in the cigarette smoke and get stuck and accumulate at the bronchial bifurcations of the lungs, forming "hot spots," instead of dispersing throughout the lungs. In fact, previous research on lung autopsies in smokers who died of lung cancer showed that malignant growths were primarily located at the same bronchial bifurcations where these hot spots reside.</p></div> <p>"We used to think that only the chemicals in the cigarettes were causing lung cancer," Karagueuzian said. "But the case of the these hot spots, acknowledged by the industry and academia alike, makes a strong case for an increased probability of long-term development of malignancies caused by the alpha particles. If we're lucky, the alpha particle–irradiated cell dies. If it doesn't, it could mutate and become cancerous."</p>  <p>Karagueuzian said the findings are very timely in light of the June 2009 passage of the Family Smoking Prevention and Tobacco Control Act, which grants the U.S. Food and Drug Administration broad authority to regulate and remove harmful substances — with the exception of nicotine — from tobacco products. The UCLA research, he said, makes a strong case that the FDA ought to consider making the removal of alpha particles from tobacco products a top priority.</p>  <p>"Such a move could have a considerable public health impact, due to the public's graphic perception of radiation hazards," he said.</p>  <p>To uncover the information, Karagueuzian and his team combed through the internal tobacco industry documents made available online as part of the landmark 1998 Tobacco Master Settlement Agreement. Documents from Philip Morris, R.J. Reynolds, Lorillard, Brown I Williamson, the American Tobacco Company, the Tobacco Institutes and the Council for Tobacco Research, as well as the Bliley documents, were examined, Karagueuzian said.</p>  <p>The team searched for key terms such as "polonium-210," "atmospheric fallout," "bronchial epithelium," "hot particle" and "lung cancer," among others.</p>  <p>Karagueuzian said the earliest causal link between alpha particles and cancer was made around 1920, when alpha particle–emitting radium paint was used to paint luminescent numbers on watch dials. The painting was done by hand, and the workers commonly used their lips to produce a point on the tip of the paint brush. Many workers accumulated significant burdens of alpha particles through ingestion and absorption of radium-226 into the bones and subsequently developed jaw and mouth cancers. The practice was eventually discontinued.</p>  <p>Another example involves liver cancer in patients exposed to chronic low-dose internal alpha particles emitted from the poorly soluble deposits of thorium dioxide after receiving the contrast agent Thorotrast. It has been suggested that the liver cancers resulted from point mutations of the tumor suppressor gene p53 by the accumulated alpha particles present in the contrast media. The use of Thorotrast as contrast agent was stopped in the 1950s.</p>  <p>In addition to Karagueuzian, authors of the study include the late Amos Norman, professor emeritus in the departments of radiation oncology and radiological sciences at UCLA; James Sayre, of the departments of biostatistics and radiological sciences at UCLA; and Celia White, who served from 1999 to 2002 as director of content and services at the Legacy Tobacco Documents Library, which contains more than 13 million documents created by major tobacco companies related to their advertising, manufacturing, marketing, sales and scientific research activities.</p>  <p>The study was funded by the University of California Tobacco-Related Disease Research Program, established by the passage of California's SB1613 in 1989 to fund a comprehensive University of California grant program to support research into the prevention, causes and treatment of tobacco-related diseases.</p>  <p>The authors report no conflict of interest.</p>   
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Police are not primarily crime fighters, according to the data (107 pts)]]></title>
            <link>https://www.reuters.com/legal/government/police-are-not-primarily-crime-fighters-according-data-2022-11-02/</link>
            <guid>36924117</guid>
            <pubDate>Sat, 29 Jul 2023 20:17:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/government/police-are-not-primarily-crime-fighters-according-data-2022-11-02/">https://www.reuters.com/legal/government/police-are-not-primarily-crime-fighters-according-data-2022-11-02/</a>, See on <a href="https://news.ycombinator.com/item?id=36924117">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-testid="paragraph-0">(Reuters) - A new report adds to a growing line of research showing that police departments don’t solve serious or violent crimes with any regularity, and in fact, spend very little time on crime control, in contrast to popular narratives.</p><p data-testid="paragraph-1">The <a data-testid="Link" href="https://catalyst-ca.cdn.prismic.io/catalyst-ca/126c30a8-852c-416a-b8a7-55a90c77a04e_APCA+ACLU+REIMAGINING+COMMUNITY+SAFETY+2022_5.pdf" target="_blank">report</a> was published Oct. 25 by advocacy group Catalyst California and the ACLU of Southern California. It relies on county budgets' numbers and new policing data provided under the state’s Racial and Identity Profiling Act, which took effect in 2019.</p><p data-testid="paragraph-2">The law requires police to report demographic and other basic information about their work, including the duration of a stop and what actions were taken, like ordering someone out of a car.</p><p data-testid="paragraph-3">Records provided by the sheriff’s departments in Los Angeles, Sacramento, San Diego and Riverside showed the same longstanding pattern of racial disparities in police stops throughout the country for decades. Black people in San Diego were more than twice as likely than white residents to be stopped by sheriff’s deputies, for example.</p><p data-testid="paragraph-4">More notably, researchers analyzed the data to show how officers spend their time, and the patterns that emerge tell a striking story about how policing actually works. Those results, too, comport with existing research showing that U.S. police spend much of their time conducting racially biased stops and searches of minority drivers, often without reasonable suspicion, rather than “fighting crime.”</p><p data-testid="paragraph-5">Overall, sheriff patrol officers spend significantly more time on officer-initiated stops – “proactive policing” in law enforcement parlance – than they do responding to community members’ calls for help, according to the report. Research has shown that the practice is a fundamentally ineffective public safety strategy, the report pointed out.</p><p data-testid="paragraph-6">In 2019, 88% of the time L.A. County sheriff’s officers spent on stops was for officer-initiated stops rather than in response to calls. The overwhelming majority of that time – 79% – was spent on traffic violations. By contrast, just 11% of those hours was spent on stops based on reasonable suspicion of a crime.</p><p data-testid="paragraph-7">In Riverside, about 83% of deputies’ time spent on officer-initiated stops went toward traffic violations, and just 7% on stops based on reasonable suspicion.</p><p data-testid="paragraph-8">Moreover, most of the stops are pointless, other than inconveniencing citizens, or worse – “a routine practice of pretextual stops,” researchers wrote. Roughly three out of every four hours that Sacramento sheriff’s officers spent investigating traffic violations were for stops that ended in warnings, or no action, for example.</p><p data-testid="paragraph-9">Researchers calculated that more of the departments’ budgets go toward fruitless traffic stops than responses to service calls -- essentially wasting millions of public dollars.</p><p data-testid="paragraph-10">Chauncee Smith, a senior manager at Catalyst California, told me they wanted to test the dominant media and political narrative that police agencies use public funds to keep communities safe.</p><p data-testid="paragraph-11">“We found there is a significant inconsistency between their practices” and what the public might think police do, Smith said. “It begs the question of why we keep doubling down on public safety strategies that have been proven time and time again to fail.”</p><p data-testid="paragraph-12">The departments were mostly non-responsive to my questions.</p><p data-testid="paragraph-13">Riverside Sheriff Chad Bianco said the data -- which is self-reported -- is flawed. All four departments declined to answer specific questions about how officers spend their time, and didn’t provide contradictory information.</p><p data-testid="paragraph-14">The prevailing political myth about police work was echoed again in August, when President Joe Biden announced his administration’s <a data-testid="Link" href="https://www.reuters.com/legal/government/bidens-crime-prevention-plan-repeats-old-mistakes-policing-2022-08-02/">“fund the police” measure</a> to support hiring more cops around the country over the next five years.</p><p data-testid="paragraph-15">“When it comes to fighting crime, we know what works: officers on the street who know the neighborhood,” Biden said.</p><p data-testid="paragraph-16">Most of the existing research flatly contradicts that account.</p><p data-testid="paragraph-17">In 2016, a group of criminologists conducted a <a data-testid="Link" href="https://link.springer.com/article/10.1007/s11292-016-9269-8" target="_blank">systematic review</a> of 62 earlier studies of police force size and crime between 1971 and 2013. They concluded that 40 years of studies consistently show that “the overall effect size for police force size on crime is negative, small, and not statistically significant.”</p><p data-testid="paragraph-18">“This line of research has exhausted its utility,” the authors wrote. “Changing policing strategy is likely to have a greater impact on crime than adding more police.”</p><p data-testid="paragraph-19">Decades of data similarly shows that police don’t solve much serious and violent crime – the safety issues that most concern everyday people.</p><p data-testid="paragraph-20">Over the past decade, “consistently less than half of all violent crime and less than twenty-five percent of all property crime were cleared,” William Laufer and Robert Hughes wrote in a 2021 law review <a data-testid="Link" href="https://tmsnrt.rs/3DOcF3g" target="_blank">article</a>. Laufer and Hughes are professors in the Wharton School of the University of Pennsylvania’s Legal Studies and Business Ethics Department.</p><p data-testid="paragraph-21">Police “have never successfully solved crimes with any regularity, as arrest and clearance rates are consistently low throughout history,” and police have never solved even a bare majority of serious crimes, University of Utah college of law professor Shima Baradaran Baughman wrote in another 2021 law review <a data-testid="Link" href="https://tmsnrt.rs/3UbfhNO" target="_blank">article</a>, including murder, rape, burglary and robbery.</p><p data-testid="paragraph-22">Existing research also affirms the findings in the recent report on police work in California.</p><p data-testid="paragraph-23">Law “enforcement is a relatively small part of what police do every day,” Barry Friedman, a law professor at the New York University School of Law wrote in a 2021 law review <a data-testid="Link" href="https://tmsnrt.rs/3UdsxSe" target="_blank">article</a>.</p><p data-testid="paragraph-24">Studies have shown that the average police officer spent about one hour per week responding to crimes in progress, Friedman wrote.</p><p data-testid="paragraph-25">Police spend most of their time on traffic violations and routine, minor issues, like noise complaints, according to three different, recent analyses of dispatch data from <a data-testid="Link" href="https://www.latimes.com/california/story/2020-07-05/lapd-911-calls-reimagining-police" target="_blank">Los Angeles</a>, <a data-testid="Link" href="https://www.vera.org/news/most-911-calls-have-nothing-to-do-with-crime-why-are-we-still-sending-police" target="_blank">Baltimore, Detroit, New Orleans, Seattle</a>, and <a data-testid="Link" href="https://www.newhavenindependent.org/index.php/article/police_dispatch_stats" target="_blank">New Haven, Connecticut</a>.</p><p data-testid="paragraph-26">The New York Times reviewed national dispatch data from the FBI in June 2020, and found that just 4% of officers’ time is devoted to violent crime.</p><p data-testid="paragraph-27">“We hope the report helps reshape the narrative about the relationship between law enforcement and safety,” Smith told me. Californians “should understand that a reimagination of community safety is far overdue and that equitable and community-centered solutions” are more effective alternatives.</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank">The Thomson Reuters Trust Principles.</a></p><p>Opinions expressed are those of the author. They do not reflect the views of Reuters News, which, under the Trust Principles, is committed to integrity, independence, and freedom from bias.</p><div><address><p data-testid="Body">Hassan Kanu writes about access to justice, race, and equality under law. Kanu, who was born in Sierra Leone and grew up in Silver Spring, Maryland, worked in public interest law after graduating from Duke University School of Law. After that, he spent five years reporting on mostly employment law. He lives in Washington, D.C. Reach Kanu at hassan.kanu@thomsonreuters.com</p></address></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[1953 'Phantom' A-bomb film 'Hiroshima,' with 88,000 extras, screening in Tokyo (118 pts)]]></title>
            <link>https://mainichi.jp/english/articles/20230729/p2a/00m/0et/006000c</link>
            <guid>36923550</guid>
            <pubDate>Sat, 29 Jul 2023 19:20:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mainichi.jp/english/articles/20230729/p2a/00m/0et/006000c">https://mainichi.jp/english/articles/20230729/p2a/00m/0et/006000c</a>, See on <a href="https://news.ycombinator.com/item?id=36923550">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<!-- cxenseparse_start -->

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2023/07/29/20230729p2g00m0et031000p/6.jpg?1" data-lightbox="photos" data-title=" A scene from the 1953 film " hiroshima."="" (image="" courtesy="" of="" (c)="" "kiseki="" eno="" jonetsu="" project")"="">
<span>
<img src="https://cdn.mainichi.jp/vol1/2023/07/29/20230729p2g00m0et031000p/6.jpg?1" alt="">

</span>

</a>
</div>
<figcaption> A scene from the 1953 film "Hiroshima." (Image courtesy of (C) "Kiseki eno Jonetsu Project")</figcaption>
</figure>
</div>
<p>
    TOKYO -- The 1953 Japanese film "Hiroshima," in which some 88,000 residents of the atomic-bombed city appeared as extras, will be screened at a civic center in western Tokyo on July 30, in the hope that many people will learn about the production conveying the reality of damage from the nuclear attack.
</p>
<!-- cxenseparse_end -->

<!-- cxenseparse_start -->
<p>
    The movie, directed by Hideo Sekigawa, is based on the 1951 book "Genbaku no ko" (Children of The A-Bomb: Testament of the Boys and Girls of Hiroshima), compiled by Arata Osada. The story portrays the chaos in the immediate aftermath of the U.S.' Aug. 6, 1945 atomic bombing of Hiroshima, with some 88,000 residents, many of them survivors, performing as extras. It will be played at Kitano Community Center in the city of Hachioji on July 30.
</p>
<p>
    Kai Kobayashi, 50, a film producer living in Hachioji and the grandson of Taihei Kobayashi, who was an assistant to the director in the film, has been working to have the film played at various locations. "As this year marks 70 years since the film's production, I want people to learn about the movie through which creators strove to share the reality of damage from the atomic bombing," Kai said.
</p>
<p>
    "Filmed a mere eight years after the atomic bombing, a vast number of Hiroshima residents took part in the filmmaking with thoughts for their family members who perished in the bombing, and famous actors joined in. It's a film that could never be created again," Kai said.
</p>
<p>
    Kai carries on the will of his father Ippei Kobayashi, who launched a drive to rerun the film in 2008, and completed a digitally remastered version in 2017. He also created English subtitles for the film in 2019.
</p>
<p>
    After his father passed away in 2015, Kai moved to Hachioji and learned of the Hachiouji Peace and Atomic Bomb Museum, which exhibits documents and mementos of A-bomb victims. He has since been in talks with the museum over collaboration in passing down memories of the bombing. After some snags during the coronavirus pandemic, the latest project materialized in the wake of Russia's full-scale invasion of Ukraine, with the museum organizing the July 30 screening.
</p>

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2023/07/29/20230729p2g00m0et032000p/8.jpg?1" data-lightbox="photos" data-title=" A scene from the 1953 film " hiroshima."="" (image="" courtesy="" of="" (c)="" "kiseki="" eno="" jonetsu="" project")"="">
<span>
<img src="https://cdn.mainichi.jp/vol1/2023/07/29/20230729p2g00m0et032000p/6.jpg?1" alt="">

</span>

</a>
</div>
<figcaption> A scene from the 1953 film "Hiroshima." (Image courtesy of (C) "Kiseki eno Jonetsu Project")</figcaption>
</figure>
</div>
<p>
    "At the time (the film was created), the Korean War had broken out, and now Russia has invaded Ukraine. The nuclear threat is alive even 70 years on. I hope people will experience the power that this film has," Kai said.
</p>
<p>
    Starring Hiroshima-native Yumeji Tsukioka and other actors, the film won the Berlin International Film Festival feature film award in 1955. However, its domestic distributor did not run the movie in Japan after being at odds with filmmakers about deleting some scenes. The film was thus called a "phantom movie."
</p>
<p>
    The Hachioji peace museum, established in 1997, is open with free admission twice a week. It houses more than 2,000 books including notes written by A-bomb survivors, as well as clothes of a junior high school student who was killed in the Hiroshima atomic bombing.
</p>
<p>
    Kotaro Sugiyama, 73, co-head of the museum, commented, "Even though the G7 summit was held in Hiroshima in May (this year), the reality of damage from the atomic bombing has not been sufficiently communicated. Now is the time for many people to watch this film."
</p>
<p>
    The July 30 screening at Kitano Community Center will start at 2 p.m. A material fee of 500 yen (approx. $3.50) will be collected from adults, but admission is free for high school students and younger children. No reservation is necessary. For inquiries, call Sugiyama on: 090-1128-8983 (in Japanese).
</p>
<p>
    (Japanese original by Megumi Nokura, Hachioji Bureau)
</p>
<!-- cxenseparse_end -->

<!--| tools BGN |-->

<!--| tools END |-->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Counterterrorism internet surveillance is being retargeted at sex workers (108 pts)]]></title>
            <link>https://theintercept.com/2023/07/29/skull-games-surveillance-sex-workers/</link>
            <guid>36923154</guid>
            <pubDate>Sat, 29 Jul 2023 18:39:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theintercept.com/2023/07/29/skull-games-surveillance-sex-workers/">https://theintercept.com/2023/07/29/skull-games-surveillance-sex-workers/</a>, See on <a href="https://news.ycombinator.com/item?id=36923154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    
<p><u>The most popular</u><a href="https://www.youtube.com/watch?v=-LHTrFsSLZA"> video</a> on Vaught Victor Marx’s YouTube now has more than 15 million views. Standing solemnly in a dark blue karate gi while his son Shiloh Vaughn Marx smiles and points a gun at his face, Marx uses his expertise as a seventh-degree black belt in “Cajun Karate Keichu-Do” to perform what he claims was the world’s fastest gun disarm. Over a period of just 80 milliseconds — according to Marx’s measurement — he snatches the gun from his son and effortlessly ejects the magazine. It’s a striking display, one that unequivocally shouts: I am here to stop bad guys.</p>



<p>Marx is more than just a competitive gun-disarmer and martial artist. He is also a former Marine, a self-proclaimed exorcist, and an author and filmmaker. He also helped launch the Skull Games, a privatized intelligence outfit that purports to hunt pedophiles, sex traffickers, and other “demonic activity” using a blend of sock-puppet social media accounts and commercial surveillance tools — including face recognition software.</p>



<p>The Skull Games events have attracted notable corporate allies. Recent games have been “powered” by the internet surveillance firm Cobwebs, and an upcoming competition is partnered with cellphone-tracking data broker <a href="https://theintercept.com/2022/04/22/anomaly-six-phone-tracking-zignal-surveillance-cia-nsa/">Anomaly Six</a>.</p>



<p>The moral simplicity of Skull Games’s mission is emblazoned across its website in fierce, all-caps type: “We hunt predators.” And Marx has savvily ridden recent popular attention to the independent film “<a href="https://www.vanityfair.com/hollywood/2023/07/sound-of-freedom-child-trafficking-movie">Sound of Freedom</a>,” a dramatization of the life of fellow anti-trafficking crusader Tim Ballard. In the era of QAnon and conservative “groomer” panic, vowing to take down shadowy — <a href="https://www.theatlantic.com/magazine/archive/2022/01/children-sex-trafficking-conspiracy-epidemic/620845/">and frequently exaggerated</a> — networks of “traffickers” under the aegis of Christ is an exercise in shrewd branding.</p>



<!-- BLOCK(newsletter)[0](%7B%22componentName%22%3A%22NEWSLETTER%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(newsletter)[0] -->



<p>Although its name is a reference to the mind games played by pimps and traffickers, Skull Games, which Marx’s church is no longer officially involved in, is itself a form of sport for its participants: a sort of hackathon for would-be Christian saviors, complete with competition. Those who play are awarded points based on their sleuthing. Finding a target’s high school diploma or sonogram imagery nets 15 points, while finding the same tattoo on multiple women would earn a whopping 300. On at least one occasion, according to materials reviewed by The Intercept and Tech Inquiry, participants competed for a chance at prizes, including paid work for Marx’s California church and one of its surveillance firm partners.</p>



<p>While commercially purchased surveillance exists largely outside the purview of the law, Skull Games was founded to answer to a higher power. The event started under the auspices of All Things Possible Ministries, the Murrieta, California, evangelical church Marx <a href="https://projects.propublica.org/nonprofits/display_990/200310367/2004_10_EO%2F20-0310367_990_200312">founded</a> in 2003.</p>



<p>Marx has attributed his conversion to Christianity to becoming reunited with his biological father — according to Marx, formerly a “practicing warlock” — toward the end of his three years in the Marine Corps. Marx’s tendency to blame demons and warlocks would become the central cause of controversy of his own ministry, largely as a result of his focus on <a href="https://podcasts.apple.com/us/podcast/a-conversation-on-spiritual-warfare-with-victor/id1510009447?i=1000513628583">exorcisms</a> as the solutions to issues ranging from pornography to veteran suicides. As Marx recently <a href="https://www.youtube.com/watch?app=desktop&amp;v=YnWq4CXOg0E">told</a> “The Spillover” podcast, “I hunt pedophiles, but I also hunt demons.”</p>



<p>Skull Games also ends up being a hunt for sex workers, <a href="https://theintercept.com/2018/06/13/sesta-fosta-sex-work-criminalize-advocacy/">conflating</a> them with <a href="https://theintercept.com/2019/04/05/florida-human-trafficking-registry-sex-work/">trafficking </a><a href="https://theintercept.com/2019/04/05/florida-human-trafficking-registry-sex-work/">victims</a> as they prepare intelligence dossiers on women before turning them over to police.</p>



<p>Groups seeking to rescue sex workers — whether through religion, <a href="https://theintercept.com/2018/03/24/demand-abolition-sex-work-nonprofit-prosecutors-king-county/">prosecution</a>, or both — are nothing new, said Kristen DiAngelo, executive director of the advocacy group Sex Workers Outreach Project Sacramento. What Skull Games represents — the technological <a href="https://theintercept.com/2020/03/02/citizen-app/">outsourcing</a> of police work to civilian volunteers — presents a new risk to sex workers, she argued.</p>



<!-- BLOCK(pullquote)[1](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22right%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="right"><!-- CONTENT(pullquote)[1] -->“I think it’s dangerous because you set up people to have that vigilante mentality.”<!-- END-CONTENT(pullquote)[1] --></blockquote><!-- END-BLOCK(pullquote)[1] -->



<p>“I think it’s dangerous because you set up people to have that vigilante mentality — that idea that, we’re going to go out and we’re going to catch somebody — and they probably really believe that they are going to ‘save someone,’” DiAngelo told The Intercept and Tech Inquiry. “And that’s that savior complex. We don’t need saving; we need support and resources.”</p>



<p>The eighth Skull Games, which took place over the weekend of July 21, operated out of a private investigation firm headquartered in a former church in Wanaque, New Jersey. A photo of the event <a href="https://www.linkedin.com/posts/michele-block-a5174b86_skull-games-08-wrap-up-i-am-humbled-by-activity-7089184727819046912-EWNa?utm_source=share&amp;utm_medium=member_desktop">shared</a> by the director of intelligence of Skull Games showed 57 attendees — almost all wearing matching black T-shirts — standing in front of corporate due diligence firm Hetherington Group’s office with a Skull Games banner unfurled across its front doors. Hetherington Group’s address is simple to locate online, but their office signage doesn’t mention the firm’s name, only saying “593 Ringwood LLC” above the words “In God We Trust.” (Cynthia Hetherington, the CEO of Hetherington Group and a board member of Skull Games, distanced her firm from the surveillance programs normally used at the events. “Cobwebs brought the bagels, which I’m still trying to digest,” she said. “I didn’t see their software anywhere in the event.”)</p>



<p>The attempt to merge computerized counterinsurgency techniques with right-wing evangelism has left some Skull Games participants uncomfortable. One experienced attendee of the January 2023 Skull Games was taken aback by an abundance of prayer circles and paucity of formal training. “Within the first 10 minutes,” the participant recalled of a training webinar, “I was like, ‘What the fuck is this?’”</p>


<!-- BLOCK(photo)[2](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22bleed%22%2C%22bleed%22%3A%22xtra-large%22%2C%22width%22%3A%22auto%22%7D) --><div><!-- CONTENT(photo)[2] --> <p><img decoding="async" width="2500" height="1668" src="https://production.public.theintercept.cloud/wp-content/uploads/2023/07/2M69C9D-jeff-tiegs.jpg?w=1000" alt="2M69C9D Jeff Tiegs, chief operations officer of All Things Possible Ministries, blesses U.S. Army Soldiers and explains to them the religious origins of a popular hand gesture on Joint Base Elmendorf-Richardson, Alaska, April 20, 2022. Tiegs said the hand gesture popularized by Star Trek originated as a blessing of the descendants of Aaron, a Jewish High Priest in the Torah." srcset="https://theintercept.com/wp-content/uploads/2023/07/2M69C9D-jeff-tiegs.jpg 2500w, https://theintercept.com/wp-content/uploads/2023/07/2M69C9D-jeff-tiegs.jpg?resize=300,200 300w, https://theintercept.com/wp-content/uploads/2023/07/2M69C9D-jeff-tiegs.jpg?resize=768,512 768w, https://theintercept.com/wp-content/uploads/2023/07/2M69C9D-jeff-tiegs.jpg?resize=1024,683 1024w, https://theintercept.com/wp-content/uploads/2023/07/2M69C9D-jeff-tiegs.jpg?resize=1536,1025 1536w, https://theintercept.com/wp-content/uploads/2023/07/2M69C9D-jeff-tiegs.jpg?resize=2048,1366 2048w, https://theintercept.com/wp-content/uploads/2023/07/2M69C9D-jeff-tiegs.jpg?resize=540,360 540w, https://theintercept.com/wp-content/uploads/2023/07/2M69C9D-jeff-tiegs.jpg?resize=1000,667 1000w" sizes="(max-width: 2500px) 100vw, 2500px"></p><p>Jeff Tiegs blesses U.S. Army Soldiers and explains to them the religious origins of a popular hand gesture on Joint Base Elmendorf-Richardson, Alaska, on April 20, 2022.</p>
<p>
Photo: Alamy</p><!-- END-CONTENT(photo)[2] --></div><!-- END-BLOCK(photo)[2] -->


<h2 id="h-delta-force-osint">Delta Force OSINT</h2>



<p>The numbers of nongovernmental surveillance practitioners has risen in tandem with the post-9/11 boom in commercial tools for social media surveillance, analyzing private chat rooms, and tracking cellphone pings.</p>



<p>Drawing on this abundance of civilian expertise, Skull Games brings together current and former military and law enforcement personnel, along with former sex workers and even employees of surveillance firms themselves. Both Skull Games and the high-profile, <a href="https://www.vice.com/en/article/k7a3qw/a-famed-anti-sex-trafficking-group-has-a-problem-with-the-truth">MAGA-beloved</a> Operation Underground Railroad have <a href="https://cobwebs.com/en/blog/osint-technology-impacts-recovery-of-victims-in-human-trafficking-cases/">worked with</a> Cobwebs, but Skull Games roots its branding in counterinsurgency and special operations rather than homeland security.</p>



<p>“I fought the worst of the worst: ISIS, Al Qaeda, the Taliban,” Skull Games president and former Delta Force soldier Jeff Tiegs <a href="https://vimeo.com/835469558">has said</a>. “But the adversary I despise the most are human traffickers.” Tiegs has told interviewers that he takes “counterterrorism / counterinsurgency principles” and applies them to these targets. </p>



<!-- BLOCK(pullquote)[3](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22left%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="left"><!-- CONTENT(pullquote)[3] -->“I fought the worst of the worst: ISIS, Al Qaeda, the Taliban. But the adversary I despise the most are human traffickers.”<!-- END-CONTENT(pullquote)[3] --></blockquote><!-- END-BLOCK(pullquote)[3] -->



<p>The plan broadly mimicked a widely <a href="https://www.cbsnews.com/news/new-search-engine-exposes-the-dark-web/">praised</a> Pentagon effort to catch traffickers that was ultimately <a href="https://www.tellfinder.com/">shut down</a> this May due to a lack of funding. In a training session earlier this month, Tiegs noted that active-duty military service members take part in the hunts; veterans like Tiegs himself are everywhere. The attendee list for a recent training event shows participants with day jobs at the Department of Defense, Portland Police Bureau, and Air Force, as well as a lead contracting officer from U.S. Citizenship and Immigration Services.</p>



<p>Skull Games employs U.S. Special Forces jargon, which dominates the pamphlets handed out to volunteers. Each volunteer is assigned the initial informal rank of private and works out of a “Special Operations Coordination Center.” Government acronyms abound: Participants are asked to keep in mind CCIRs — Commander’s Critical Information Requirements&nbsp;— while preventing EEFIs — Essential Elements of Friendly Information— from falling into the hands of the enemy.</p>



<p>Tiegs’s transition from counterinsurgency to counter-human-trafficking empresario came after he met Jeff Keith, the founder of the anti-trafficking nonprofit Guardian Group, where Tiegs was an executive for nearly five years. While Tiegs was developing Guardian Group’s tradecraft for identifying victims, he was also beginning to work more closely with Marx, whom he met on a trip to Iraq in 2017. By the end of 2018, <a href="https://projects.propublica.org/nonprofits/display_990/721613750/11_2019_prefixes_71-77%2F721613750_201812_990_2019110816831017">Marx</a> and <a href="https://projects.propublica.org/nonprofits/organizations/200310367/201812819349300691/full">Tiegs</a> had joined each others’ boards.</p>



<p>Beyond the Special Forces acumen of its leadership, what sets Skull Games apart from other amateur predator-hunting efforts is its reliance on “open-source intelligence.” OSINT, as it’s known, is a military euphemism popular among its practitioners that refers to a broad amalgam of <a href="https://theintercept.com/2021/09/21/surveillance-social-media-police-microsoft-shadowdragon-kaseware/">intelligence-gathering techniques</a>, most relying on surveilling the public internet and purchasing sensitive information from commercial data brokers.</p>


<!-- BLOCK(promote-related-post)[4](%7B%22componentName%22%3A%22PROMOTE_RELATED_POST%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22relatedPostNumber%22%3A2%7D) -->

<!-- END-BLOCK(promote-related-post)[4] -->



<p>Sensitive personal information is today bought and sold so widely, including by law enforcement and spy agencies, that the Office of the Director of National Intelligence <a href="https://www.dni.gov/files/ODNI/documents/assessments/ODNI-Declassified-Report-on-CAI-January2022.pdf">recently warned</a> that data “that could be used to cause harm to an individual’s reputation, emotional well-being, or physical safety” is available on “nearly everyone.”</p>



<p>Skull Games’s efforts to tap this unregulated sprawl of digital personal data function as sort of vice squad auxiliaries. Participants scour the U.S. for digital evidence of sex work before handing their findings over to police —&nbsp;officers the participants often describe as friends and collaborators.</p>



<p>After publicly promoting 2020 as the year Guardian Group would “scale” its tradecraft up to tackling many more cases, Tiegs abruptly jumped from his role as chief operating officer of the organization into the same title at All Things Possible — Marx’s church. By December 2021, Tiegs had launched the first Skull Games under the umbrella of All Things Possible. The event was put together in close partnership with Echo Analytics, which had been <a href="https://www.bizjournals.com/tampabay/news/2021/02/08/quiet-professionals-acquires-tampa-company.html">acquired</a> earlier that year by Quiet Professionals, a surveillance contractor led by a former <a href="https://quietprofessionalsllc.com/andy-wilson/">Delta Force</a> sergeant major. The first Skull Games took place in the Tampa offices of Echo Analytics, just 13 miles from the headquarters of U.S. Special Operations Command.</p>



<p>As of May 2023, Tiegs has separated from All Things Possible and leads the Skull Games as a newly independent, tax-exempt nonprofit. “Skull Games is separate and distinct from ATP,” he said in an emailed statement. “There is no role for ATP or Marx in Skull Games.”</p>



<h2>The Hunt</h2>



<p>Reached by phone, Tiegs downplayed the role of powerful surveillance tools in Skull Games’s work while also conceding he wasn’t always aware of what technologies were being used in the hunt for predators — or how.</p>



<p>Despite its public emphasis on taking down traffickers, much of Skull Games’s efforts boil down to scrolling through sex worker ad listings and attempting to identify the women. Central to the sleuthing, according to Tiegs and training materials reviewed by The Intercept and Tech Inquiry, is the search for visual indicators in escort ads and social media posts that would point to a woman being trafficked. An <a href="https://ovc.ojp.gov/library/publications/indicators-sex-trafficking-online-escort-ads-final-report">October 2022 report</a> funded by the research and development arm of the U.S. Department of Justice, however, concluded that the appearance of many such indicators — mostly emojis and acronyms — was statistically insignificant.</p>



<p>Tiegs spoke candidly about the centrality of face recognition to Skull Games. “So here’s a girl, she’s being exploited, we don’t know who she is,” he said. “All we have is a picture and a fake name, but, using some of these tools, you’re able to identify her mugshot. Now you know everything about her, and you’re able to start really putting a case together.”</p>



<p>According to notes viewed by The Intercept and Tech Inquiry, the competition recommended that volunteers use FaceCheck.id and <a href="https://www.nytimes.com/2022/05/26/technology/pimeyes-facial-recognition-search.html">PimEyes</a>, programs that allow users to conduct <a href="https://theintercept.com/2022/07/16/facial-recognition-search-children-photos-privacy-pimeyes/">reverse image searches for an uploaded picture of face</a>. In a July Skull Games webinar, one participant noted that they had been able to use PimEyes to find a sex worker’s driver’s license posted to the web.</p>


<!-- BLOCK(promote-related-post)[5](%7B%22componentName%22%3A%22PROMOTE_RELATED_POST%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22relatedPostNumber%22%3A1%7D) -->

<!-- END-BLOCK(promote-related-post)[5] -->



<p>In January, Cobwebs Technologies, an Israeli firm, announced it would provide Skull Games with access to its Tangles surveillance platform. According to Tiegs, the company is “one of our biggest supporters.” Previous <a href="https://www.vice.com/en/article/xgynn4/company-helping-irs-go-undercover-cobwebs-technologies">reporting</a> from Motherboard detailed the IRS Criminal Investigation unit’s usage of Cobwebs for undercover investigations. </p>



<p>Skull Games training materials provided to The Intercept and Tech Inquiry provide detailed instructions on the creation of “sock puppet” social media accounts: fake identities for covert research and other uses. Tiegs denied recommending the creation of such pseudonymous accounts, but on the eve of the eighth Skull Games, team leader Joe Labrozzi told fellow volunteers, “We absolutely recommend sock puppets,” according to a training seminar transcript reviewed by The Intercept and Tech Inquiry. Other volunteers shared tips on creating fake social media accounts, including the use of ChatGPT and machine learning-based face-generation tools to build convincing social media personas.</p>



<!-- BLOCK(cta)[6](%7B%22componentName%22%3A%22CTA%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(cta)[6] -->



<p>Tiegs also denied a participant’s assertion that Clearview AI’s face recognition software was heavily used in the January 2023 Skull Games. Training materials obtained by Tech Inquiry and The Intercept, however, suggest otherwise. At one point in a July training webinar, a Virginia law enforcement volunteer who didn’t give their name asked what rules were in place for using their official access to face recognition and other law enforcement databases. “It’s easier to ask for forgiveness than permission,” replied another participant, adding that some police Skull Games volunteers had permission to tap their <a href="https://theintercept.com/2022/05/04/clearview-face-recognition-staten-island/">departmental access to Clearview AI</a> and Spotlight, an investigative tool that uses Amazon’s <a href="https://theintercept.com/2018/05/25/amazon-surveillance-facial-recognition-congress/">Rekognition</a> technology to identify faces.</p>



<p>Cobwebs — which <a href="https://www.police1.com/police-products/investigation/computer-digital-forensics/press-releases/cobwebs-technologies-joins-penlink-to-expand-its-digital-investigative-platform-aHaxRjsLTkePzg7O/">became part</a> of the American wiretapping company PenLink earlier this month — provides a broad array of surveillance capabilities, according to a <a href="https://www.documentcloud.org/documents/23890883-cobwebs-bureau-of-indian-affairs-contract-and-emails">government procurement document</a> obtained through a Freedom of Information Act request. Cobwebs provides investigators with the ability to continuously monitor the web for certain keyphrases. The Tangles platform can also provide face recognition; fuse OSINT with personal account data collected from search warrants; and pinpoint individuals through the locations of their phones — granting the ability to track a person’s movements going back as many as three years without judicial oversight.</p>



<p>When reached for comment, Cobwebs said, “Only through collaboration between all sectors of society — government, law enforcement, academia — and the proper tools, can we combat human trafficking.” The company did not respond to detailed questions about how its platform is used by Skull Games.</p>



<p>According to a source who previously attended a Skull Games event, and who asked for anonymity because of their ongoing role in counter-trafficking, only one member of the “task force” of participants had access to the Tangles platform: a representative from Cobwebs itself who could run queries from other task force analysts when requested. The rest of the group was equipped with whatever OSINT-gathering tools they already had access to outside of Skull Games, creating a lopsided exercise in which some participants were equipped with little more than their keyboards and Google searches, while others tapped tools like Clearview or Thomson Reuters CLEAR, an <a href="https://theintercept.com/2019/11/14/ice-lexisnexis-thomson-reuters-database/">analytics tool used by U.S. Immigration and Customs Enforcement</a>.</p>


<!-- BLOCK(promote-related-post)[7](%7B%22componentName%22%3A%22PROMOTE_RELATED_POST%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22relatedPostNumber%22%3A4%7D) -->

<!-- END-BLOCK(promote-related-post)[7] -->



<p>Tiegs acknowledged that most Skull Games participants likely have some professional OSINT expertise. By his account, they operate on a sort of BYO-intelligence-gathering-tool basis and, owing to Skull Games’s ad hoc use of technology, said he couldn’t confirm how exactly Cobwebs may have been used in the past. Despite Skull Games widely <a href="https://www.linkedin.com/pulse/title-skull-games-hawaii-expanding-our-fight-against-brendan-huff/">advertising</a> its partnership with another source of cellphone location-tracking data — the commercial surveillance company Anomaly Six — Tiegs said, “We’re not pinpointing the location of somebody.” He claimed Skull Games uses less sophisticated techniques to generate leads for police who may later obtain a court order for, say, geolocational data. (Anomaly Six said that it is not providing its software or data to Skull Games.)</p>



<p>Tiegs also expressed frustration with the notion that deploying surveillance tools to crack down on sex work would be seen as impermissible. “We allow Big Data to monitor everything you’re doing to sell you iPods or sunglasses or new socks,” he said, “but if you need to leverage some of the same technology to protect women and children, all of the sudden everybody’s up in arms.”</p>



<p>Tiegs added, “I’m really conflicted how people rationalize that.”</p>


<!-- BLOCK(photo)[8](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22bleed%22%2C%22bleed%22%3A%22xtra-large%22%2C%22width%22%3A%22auto%22%7D) --><div><!-- CONTENT(photo)[8] --> <p><img decoding="async" loading="lazy" width="2500" height="1592" src="https://production.public.theintercept.cloud/wp-content/uploads/2023/07/AP19154107769403-sex-workers-rights-decriminalize.jpg?w=1000" alt="People march in support of sex workers, Sunday, June 2, 2019, in Las Vegas. People marched in support of decriminalizing sex work and against the Fight Online Sex Trafficking Act and the Stop Enabling Sex Traffickers Act, among other issues. (AP Photo/John Locher)" srcset="https://theintercept.com/wp-content/uploads/2023/07/AP19154107769403-sex-workers-rights-decriminalize.jpg 2500w, https://theintercept.com/wp-content/uploads/2023/07/AP19154107769403-sex-workers-rights-decriminalize.jpg?resize=300,191 300w, https://theintercept.com/wp-content/uploads/2023/07/AP19154107769403-sex-workers-rights-decriminalize.jpg?resize=768,489 768w, https://theintercept.com/wp-content/uploads/2023/07/AP19154107769403-sex-workers-rights-decriminalize.jpg?resize=1024,652 1024w, https://theintercept.com/wp-content/uploads/2023/07/AP19154107769403-sex-workers-rights-decriminalize.jpg?resize=1536,978 1536w, https://theintercept.com/wp-content/uploads/2023/07/AP19154107769403-sex-workers-rights-decriminalize.jpg?resize=2048,1304 2048w, https://theintercept.com/wp-content/uploads/2023/07/AP19154107769403-sex-workers-rights-decriminalize.jpg?resize=540,344 540w, https://theintercept.com/wp-content/uploads/2023/07/AP19154107769403-sex-workers-rights-decriminalize.jpg?resize=1000,637 1000w" sizes="(max-width: 2500px) 100vw, 2500px"></p><p>People march in support of sex workers and decriminalizing sex work on June 2, 2019, in Las Vegas.</p>
<p>
Photo: John Locher/AP</p><!-- END-CONTENT(photo)[8] --></div><!-- END-BLOCK(photo)[8] -->


<h2>“Pure Evil”</h2>



<p>A potent strain of anti-sex work sentiment — not just opposition to trafficking — has pervaded Skull Games since its founding. Although the events are no longer affiliated with a church, Tiegs and his lieutenants’ devout Christianity suggests the digital hunt for pedophiles and pimps remains a form of spiritual warfare.</p>



<p>Michele Block, a Canadian military intelligence veteran who has worked as Skull Games’s director of intelligence since its founding at All Things Possible, is open about her belief that their surveillance efforts are part of a battle against Satan. In a December 2022 interview at America Fest, a four-day conference organized by the right-wing group Turning Point USA, Block <a href="https://www.youtube.com/watch?v=Bc5W3ZHK3jk">described</a> her work as a fight against “pure evil,” claiming that many traffickers are specifically targeting Christian households.</p>



<p>Tiegs argued that “100 percent” of sex work is human trafficking and that “to legalize the purchasing of women is a huge mistake.”</p>



<p>The combination of digital surveillance and Christian moralizing could have serious consequences not only for “predators,” but also their prey: The America Fest interview showed that Skull Games hopes to take down alleged traffickers by first going after the allegedly trafficked.</p>



<!-- BLOCK(pullquote)[9](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22right%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="right"><!-- CONTENT(pullquote)[9] -->“So basically, 24/7, our intelligence department identifies victims of sex trafficking.”<!-- END-CONTENT(pullquote)[9] --></blockquote><!-- END-BLOCK(pullquote)[9] -->



<p>“So basically, 24/7,” Block explained, “our intelligence department identifies victims of sex trafficking.” All of this information — both the alleged trafficker and alleged victim — is then handed over to police. Although Tiegs says Skull Games has provided police with “a couple hundred” such OSINT leads since its founding, he conceded the group has no information about how many have resulted in prosecutions or indictments of actual traffickers.</p>



<p>When asked about Skull Games’s position on arresting victims, Tiegs emphasized that “arresting is different from prosecuting” and argued, “Sometimes they do need to make the arrest, because of the health and welfare of that person. She needs to get clean, maybe she’s high. … Very rarely, in my opinion, is it right to charge and prosecute a girl.”</p>



<p>Sex worker advocates, however, say any punitive approach is not only ungrounded in the reality of the trade, but also hurts the very people it purports to help. Although exploitation and coercion are dire realities for many sex workers, most women choose to go into sex work either out of personal preference or financial necessity, according to DiAngelo, of Sex Workers Outreach Project Sacramento. (The Chicago branch of SWOP was a <a href="https://www.aclu.org/legal-document/aclu-v-clearview-ai-complaint">plaintiff</a> in the American Civil Liberties Union’s <a href="https://www.aclu.org/cases/aclu-v-clearview-ai">successful</a> 2020 lawsuit against Clearview AI in Illinois.)</p>



<p>Referring to research she had conducted with the University of California, Davis, DiAngelo explained that socioeconomic desperation is the most common cause of trafficking, a factor only worsened by a brush with the law. “The majority of the people we interview, even if we removed the person who was exploiting them from their life, they still wanted to be in the sex trade,” DiAngelo explained.</p>



<p>Both DiAngelo and <a href="https://www.newmoonnetwork.org/about">Savannah Sly</a> of the nonprofit New Moon Network, an advocacy group for sex workers, pointed to flaws in the techniques that police claim detect trafficking from coded language in escort ads. “You can’t tell just by looking at a picture whether someone’s trafficked or not,” Sly said. The “dragnet” surveillance of sex workers performed by groups like Skull Games, she claimed, imperils their human rights. “If I become aware I’m being surveilled, that’s not helping my situation,” Sly said, “Sex workers live with a high degree of paranoia.”</p>



<p>Rather than “rescuing” women from trafficking, DiAngelo argued Skull Games’s collaboration with police risks driving women into the company of people seeking to take advantage of them — particularly if they’ve been arrested and face diminished job prospects outside of sex work. DiAngelo said, “They’re going to lock them into sex work, because once you get the scarlet letter, nobody wants you anymore.”</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open Source Outdoor Air Quality Monitor (122 pts)]]></title>
            <link>https://www.airgradient.com/open-air/</link>
            <guid>36922790</guid>
            <pubDate>Sat, 29 Jul 2023 17:59:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.airgradient.com/open-air/">https://www.airgradient.com/open-air/</a>, See on <a href="https://news.ycombinator.com/item?id=36922790">Hacker News</a></p>
<div id="readability-page-1" class="page">

<div>
<p>We use cookies on this website to give you the best experience. To find out more, read our
<a href="https://www.airgradient.com/privacy-policy/">privacy policy</a>.
</p>
</div>


<header>
<div>
<nav>
<a href="https://www.airgradient.com/"> <img src="https://www.airgradient.com/images/logos/logo.svg" alt="AirGradient" width="160px"> </a>


</nav>
</div>
</header>
<div>
<p><img src="https://www.airgradient.com/images/outdoor.jpg" alt="">
</p>
<div>
<p><strong>AirGradient Open Air</strong></p>

<p>Open Air is an outdoor air quality monitor enabling you to know if the air quality is healthy or not. It measures PM1, PM2.5, PM10, Temperature and Humidity. It’s fully open-source and customizable, so you can extend it in whatever way you like.</p>


</div>
</div>
<div>
<h2>Why Air Quality is So Important</h2>
<p><img src="https://www.airgradient.com/" alt=""></p><div><p>According to the World Health Organization, more than 90% of the people breathe unhealthy air. Air pollution is the biggest driver of premature death of all environmental health risks. With climate change and the warming of the planet, air pollution, e.g. through wildfires are also increasingly becoming a major issue in regions with previously very clean air.</p><p>Air quality can quickly fluctuate between good and unhealthy levels and thus a dense network of air quality monitors ensures that people know when they need to protect their health. The Open Air outdoor monitor keeps you always informed about the air you breathe.</p></div>
</div>
<div>
<h2>Key Characteristics</h2>
<p><img src="https://www.airgradient.com/images/why.png" alt=""></p><div><p>The monitor measures PM1, PM2.5, PM10, Temperature and Humidity. In future we will offer options with TVOC and CO2 sensors. The standard version uses two independent particle sensors with integrated temperature and humidity sensors. </p><p> This monitor focuses on measuring particulate matter, especially PM2.5. PM2.5 has the potential to cause significant harm to our well-being. These tiny particles are released into the air through various sources such as vehicle emissions, industrial activities, burning of fossil fuels or wildfires. When inhaled, PM2.5 particles can penetrate deep into our respiratory system, leading to a range of health issues. Long-term exposure to PM2.5 has been linked to respiratory and cardiovascular diseases, including asthma, bronchitis, heart attacks, and even premature death.</p><p>The Open Air constantly measures the air quality and informs you when the air becomes unhealthy, and you should take protective measures.</p></div>
</div>
<div>
<h2>Flexible Data Platforms</h2>
<p><img src="https://www.airgradient.com/images/dash.jpg" alt=""></p><div><p>The AirGradient Open Air monitor is unique as it gives you full flexibility on how you want to monitor and use the data. Being open-source you are not locked in to any specific data platform but can connect the data to any server, i.e. Home Assistant or use the AirGradient data platform - a data platform specifically made for air quality monitoring. </p><p> This gives you complete ownership and freedom of your data and our community has built a number of extensions to existing data platforms, e.g. home assistant with ESPHome.</p><p> Of course you can also use the AirGradient dashboard that is already pre-flashed on the monitor and very easy to setup.</p><p> This powerful dashboard lets you immediately see the air quality and environmental status of multiple locations. Built for speed and scale. You can set up specific alerts and are notified automatically if air quality is exceeds your defined ranges.</p><p> Get powerful daily and weekly reports detailing the air quality of each location, providing you with a clear, concise summaries at a glance.</p><p> Outdoor monitors can also be displayed on the AirGradient Map and you can opt-in to share your outdoor data with openAQ, a non-profit with the mission to deomocritize air quality data and make it freely available.</p></div>
</div>
<div>
<h2>AirGradient Map with OpenAQ Data</h2>
<p><img src="https://www.airgradient.com/images/map.jpg" alt=""></p><div>
<p>If you use the AirGradient data platform, you can opt-in to share your outdoor data with the public and openAQ. OpenAQ is a non-profit organisation collecting and organizing worldwide air quality data and making it available for the public and research. Your data is then also available on the AirGradient map together with thousands of other monitors from openAQ.</p>
</div>
</div>
<div>
<h2>Easy to Assemble, Setup &amp; Install</h2>
<p><img src="https://www.airgradient.com/" alt=""></p><div>
<p>The kit is very easy to assemble, and you can also get the monitor as fully assembled version. We provide detailed instructions and videos on how to connect the monitor to your WiFi network and set it up on the dashboard and map. If you run into any problem, just <a href="https://www.airgradient.com/support">contact us</a> directly and we are happy to help. You can also ask questions in the <a href="https://forum.airgradient.com/">AirGradient Forum</a> and get valuable tips from the community.</p>
</div>
</div>
<div>
<h2>Mounting Options</h2>
<p><img src="https://www.airgradient.com/images/mounting.png" alt=""></p><div><p>The monitor can be mounted on walls or poles. For pole mounting, use two stainless steel zip ties and feed them through the zip tie holders. Alternatively you can use two screws and mount the monitor on a wall. To prevent theft, there is a small hole on the monitor that you can use with a secure screw to prevent the monitor from easily taken off.</p><p>The enclosure is fully weather proof so it can be mounted being exposed to the elements.</p></div>
</div>
<div>
<h2>Optimized Enclosure</h2>
<p><img src="https://www.airgradient.com/images/enclosure.png" alt=""></p><div><p>The enclosure has been designed with two primary goals, firstly to ensure accurate measurements of the air quality and secondly to be robust and long lasting. </p><p> For accurate measurements it is important that there is a good airflow to the sensor modules, that air loops in front of the sensor modules are avoided and that the risk of condensation inside the enclosure is as much as possible reduced.</p><p> The enclosure itself is very robust and consists of high quality ASA plastic that gives an excellent durability as well as UV protection.</p></div>
</div>
<div id="features">
<div>
<h2>Available as Kit or Fully Assembled</h2>
<p>You can either buy the components yourself and build it on your own, or purchase as a kits or fully assembled unit with warranty and test report from our online shop.</p>
</div>
<div>
<div>
<h2>Open Air Kit</h2>
<p><img src="https://www.airgradient.com/images/kits/open_air_overview.jpg"></p><p><small>Outdoor air quality monitor with dual PM modules. Measures PM1, PM2.5, PM10, Temperature and Humidity. Comes with pre-assembled PCB and can be built without soldering within minutes.</small></p>
<ul>
<small>
<li>Dual PM sensors</li>
<li>4m USB C cable (w/o power plug)</li>
<li>Easy to assemble kit</li>
<li>24 months AirGradient data platform with map (or connect it to your own server, e.g. home assistant)</li>
<li>Fully open-source code so you can easily make adjustments</li>
</small>
</ul>
<h2><small>USD</small><b> 95.00</b></h2>

<p><a href="https://www.airgradient.com/open-airgradient/instructions/diy-open-air-presoldered-v11/" type="button">Build Instructions</a>
<a href="https://www.airgradient.com/shop" type="button">Purchase Now</a>
</p></div>
<div>
<h2>Open Air Fully Assembled</h2>
<p><img src="https://www.airgradient.com/images/oa-assembled.jpg"></p><p><small>Outdoor air quality monitor with dual PM modules. Measures PM1, PM2.5, PM10, Temperature and Humidity. Fully assembled and tested ready to be used immediately.</small></p>
<ul>
<small>
<li>Dual PM sensors</li>
<li>4m USB C cable (w/o power plug)</li>
<li>Fully assembled</li>
<li>24 months AirGradient data platform with map (or connect it to your own server, e.g. home assistant)</li>
<li>Fully open-source code so you can easily make adjustments</li>
<li>Tested in our test chamber with test report</li>
<li>Cerified: CE, FCC, RoHS, REACH</li>
<li>12 months warranty</li>
</small>
</ul>
<h2><small>USD</small><b> 155.00</b></h2>

<p><a href="https://www.airgradient.com/shop" type="button">Purchase Now</a>
</p></div>
</div>
</div>

<div id="specs">
<div>
<h2>Technical Data</h2>
<p>Below is the technical data of the default version of the Open Air as it is available in our online store.</p>
</div>
<form id="form1">
<div>
<div>
<p>Specification</p>
<p>Description</p>
</div>
<div>
<p>Microcontroller</p>
<p>ESP32-C3-MINI (32-bit RISC-V single-core processor, up to 160MHz, 384 KB ROM, 400 KB SRAM, 8 KB SRAM in RTC, 4 MB flash in chip package)</p>
</div>
<div>
<p>WiFi</p>
<p>2.4GHz IEEE 802.11 b/g/n-compliant</p>
</div>
<div>
<p>Bluetooth</p>
<p>Bluetooth LE: Bluetooth 5, Bluetooth mesh</p>
</div>
<div>
<p>Extensions</p>
<p>Broken out on PCB: I2C, 3 GPIO, 2 UART</p>
</div>
<div>
<p>Peripherals</p>
<p>Status LED, Push Button, USB C Connector</p>
</div>
<div>
<p>External Hardware Watchdog</p>
<p>Texas Instruments TPL5010</p>
</div>
<div>
<p>Particle Sensor Module</p>
<p>2x Plantower PMS5003T (laser scattering principle). Accuracy: ±10%@100~500μ g/m³, ±10μg/m³@0~100μ g/m³</p>
</div>
<div>
<p>Temperature and Humidity</p>
<p>ENS210 (inside PMS5003T module). Accuracy: Temperature ±0.2°C @ 0 - 70°C; Humidity ±3.5% RH @ 20 - 80% RH</p>
</div>
<div>
<p>Enclosure</p>
<p>ASA Plastic, UV Resistant and Weather Proof</p>
</div>
<div>
<p>Mounting Options</p>
<p>Wall or pole mounting options</p>
</div>
<div>
<p>Cable</p>
<p>4m USB C Cable including data lines for flashing</p>
</div>
<div>
<p>Certifications</p>
<p>CE, RoHS, REACH, FCC ID: 2AC7Z-ESPC3MINI</p>
</div>
</div>
</form>
</div>
<div id="features">
<h2>Ideally combined with indoor monitors</h2>
<div><p>In order to fully understand the air quality of your environment, we recommend you also monitor the air quality indoors. We spend most of our time indoors and often the indoor air can also be very unhealthy due to indoor pollutants like volatile organic components (VOCs), or high particle concentrations from cooking. High CO2 values due to insufficient ventilation can lead to tiredness and lower cognitive performance.</p><p> By monitoring both, indoor and outdoor air quality you get valuable additional data, e.g. where the pollution is coming from, how well does the ventilation and air purification of your home works etc. The AirGradient dashboard is especially suited to compare indoor and outdoor air quality in combined reports and charts.</p><p> Go to our <a href="https://www.airgradient.com/kits">indoor air quality monitor page</a> to learn more about our open-source indoor monitors.</p></div>
</div>
<img src="https://www.airgradient.com/images/colocationmap.jpg" alt="">
<div>
<h2>Research</h2>
<div>
<p>In order to ensure the accuracy of the monitor we work with more than 15 universities and research institutions around the world that have co-located the monitor with their reference instruments. A special thanks to all our scientific partners for participating in this program. To learn more or if you are interested to participate, you can read more on our <a href="https://www.airgradient.com/research">research page</a>.</p>
</div>
</div>
<div>
<h2>In 2019, 99% of the world population was living in places where the WHO air quality guidelines levels were not met. Ambient (outdoor air pollution) in both cities and rural areas was estimated to cause 4.2 million premature deaths worldwide in 2016.</h2>
<p>World Health Organisation</p>
</div>
<div>
<h2>Easy to Extend</h2>
<p><img src="https://www.airgradient.com/images/pcb-features-2.png" alt=""></p><div><p>The open-hardware / open-source nature of the monitor allow easy changes and additions on the firmware as well hardware. The board is powered by an ESP32-C3-MINI microcontroller that is easy to program with a wide eco-system and has integrated WiFi and Bluetooth capabilities.</p><p>The ESP32 can directly be flashed through the USB C port and thus changes in the open-source firmware can easily be done.</p><p> For the ones, who would like to make adjustments to the monitor, there are breakout pins available for I2C and three additional IOs. In addition, you can repurpose the two UARTs. The board provides both, 5 and 3.3 volts.</p><p>We integrated an external hardware watchdog chip that automatically reboots the unit in case something goes wrong and thus the monitor can achieve high up times.</p></div>
</div>
<div id="features">
<div>
<h2>Open Hardware, Open Source, Open Data, Open Everything!</h2>
<p>As the name implies, the Open Air has been designed from the ground up with the strong desire democratizing air quality monitoring. Access to the information about the air you breathe should not be a privilege of the rich. Air pollution is one of the largest global environmental and health risks, requiring a high density sensor network to understand and solve the problem. We provide a blueprint of a robust, affordable and open design that citizens and NGOs worldwide can use to accurately measure air quality.</p>
</div>
<div>
<div>
<ion-icon name=""></ion-icon>
<h4>Fully Open Source</h4>
<p>All code, schematics, 3D files etc. are open-source and published under CC-BY-SA 4.0 license. You can easily adapt the hardware as well as flash your own firmware.</p>
</div>
<div>
<ion-icon name=""></ion-icon>
<h4>You own your Data</h4>
<p>There is a trend in our industry to monetize air quality data to the extent that often the owner of the monitor does not own the data. We believe this is wrong. The data that your monitor generates should belong to the owner of the monitor.</p>
</div>
</div>
<p><img src="https://www.airgradient.com/images/partners.png" alt="">
</p>
<div>
<div>
<ion-icon name=""></ion-icon>
<h4>openAQ</h4>
<p>We have partnered up with <a href="https://openaq.org/">openAQ</a>, a non-profit with the mission to provide air quality data to influence policy, and to enable the public to access information on air pollution through open data and open-source tools.</p>
</div>
<div>
<ion-icon name=""></ion-icon>
<h4>1% for the Planet</h4>
<p>We joined 1% For The Planet and pledged to donate at least one percent of your annual sales to non-profit organisations dedicated to protect the planet. You can read more about our motivation to join 1% for the planet on our plog post <a href="https://www.airgradient.com/open-airgradient/blog/1-percent-for-the-planet/">‘Why We Joined 1% For The Planet’</a></p>
</div>
</div>
</div>
<div id="testimonial">
<div>
<p>
<h2>Statements from People Involved</h2>
</p>
</div>
<div>
<p>Having used the AirGradient indoor DIY kits as well as the commercial AirGradient ONE in awareness projects with schools and universities here in Thailand, I am very keen on piloting this new outdoor monitor. I am also interested in exchanging ideas and resources for student engagement with this open source projct.</p>

</div>
<div>
<p>The San Joaquin Valley of California has extreme climate injustice and the worst air quality in the United States, yet continues poorly monitored. The SEEN Team has partnered with AirGradient to facilitate low-cost monitoring in rural disadvantaged communities, and to increase awareness and education related to air quality. We intend to provide more and better real-time air quality information for local decision makers to act in the best interest of their underserved communities.</p>

</div>
<div>
<p>I’m very happy to contribute to this open hardware project with my product design experience and working with the team to not only ensuring a beautiful look but a highly functional design. Turning the required air flow characteristics and vent specification into a good looking product was a challenge I really liked. I’m looking forward to see the product in many Citizen Science projects around the world.</p>

</div>
<div>
<p>Having tested numerous air quality sensors for the projects we are doing I found myself extremely impressed with AirGradient. Not only are they technically first class with their product, they have a level of engagement and adaptability which makes working with them very easy. A welcome element to this is their clear commitment to the importance of environmental issues, showing itself in their interest in the details of our projects - all this helps them to deliver…!</p>
<div>
<p>Professor Rod Jones</p>
<p><span>
University of Cambridge
</span>
</p></div>
</div>
<div>
<p>AirGradient have consistently gone above and beyond their contractual duties in order to better support the SAMHE Project. They have shown great flexibility and a genuine interest to listen to the complex needs of the project; as a result, they deliver valuable products and a friendly service, both in a manner that is timely for the project.</p>
<div>
<p>Henry Burridge</p>
<p><span>
Imperial College London
</span>
</p></div>
</div>
</div>
<div id="features">
<h2>Why we care about Air Quality</h2>
<div><p>AirGradient started as a volunteer project to help a school in Northern Thailand monitor the air quality in classrooms during the highly polluted burning season. From the beginning, we have worked with students and educators to building hardware to increase awareness in the area of air quality, to understand the health impacts of pollutants, and to empower the positive changes that can be made to reduce air pollution.</p><p>The experience and expertise that we gained designing and manufacturing our professional AirGradient ONE RESET Air Accredited Monitor has been put into our open-source / open-hardware air quality build instructions and kits.</p><p>The industry-grade sensor modules used in our open hardware monitors are the same modules found in pre-built monitors costing hundreds of dollars. As a result, you get highly accurate data quality, essential to having a reliable understanding of the air around you.</p></div>
</div>

<div>
<p><img src="https://www.airgradient.com/images/1percent.png" alt=""></p><p>We’ve pledged 1% of sales to the preservation and restoration of the natural environment.</p>
</div>









</div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Army Field Manual on Leadership (1990) [pdf] (144 pts)]]></title>
            <link>https://armyoe.files.wordpress.com/2018/03/1990-fm-22-100.pdf</link>
            <guid>36922517</guid>
            <pubDate>Sat, 29 Jul 2023 17:32:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://armyoe.files.wordpress.com/2018/03/1990-fm-22-100.pdf">https://armyoe.files.wordpress.com/2018/03/1990-fm-22-100.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=36922517">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Universal and transferable adversarial attacks on aligned language models (160 pts)]]></title>
            <link>https://llm-attacks.org/zou2023universal.pdf</link>
            <guid>36921808</guid>
            <pubDate>Sat, 29 Jul 2023 16:28:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://llm-attacks.org/zou2023universal.pdf">https://llm-attacks.org/zou2023universal.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=36921808">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The world’s largest wind turbine has been switched on (105 pts)]]></title>
            <link>https://www.iflscience.com/the-worlds-largest-wind-turbine-has-been-switched-on-70047</link>
            <guid>36921675</guid>
            <pubDate>Sat, 29 Jul 2023 16:15:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iflscience.com/the-worlds-largest-wind-turbine-has-been-switched-on-70047">https://www.iflscience.com/the-worlds-largest-wind-turbine-has-been-switched-on-70047</a>, See on <a href="https://news.ycombinator.com/item?id=36921675">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section><header><h2>It’s turbo time.</h2><div><div><picture><source media="(max-width: 567px)" srcset="https://assets.iflscience.com/assets/authorNo/113/avatar/62929/laura-simmons-t.webp" type="image/webp"><source media="(max-width: 567px)" srcset="https://assets.iflscience.com/assets/authorNo/113/avatar/62929/laura-simmons-t.jpg" type="image/jpeg"><img src="https://assets.iflscience.com/assets/authorNo/113/avatar/62929/laura-simmons-t.jpg" alt="Laura Simmons - Editor and Staff Writer" title="Laura Simmons" type="image/jpeg" loading="lazy"></picture></div><div><div><p><img alt="clock" title="clock" width="16" height="16" src="https://assets.iflscience.com/svg/_clock.svg"><span>Published</span><time data-published-date="2023-07-28T13:58:03Z" date-time="2023-07-28T13:58:03Z"></time></p></div><div><p><span><img alt="comments" title="comments" width="28" height="28" src="https://assets.iflscience.com/svg/_comments.svg"></span><span>9</span><span>Comments</span></p><p><span><img alt="share" title="share" width="32" height="32" src="https://assets.iflscience.com/svg/_share.svg"></span><span>3.1k</span><span>Shares</span></p></div></div></div></header><figure><picture><source media="(min-width: 1000px)" srcset="https://assets.iflscience.com/assets/articleNo/70047/aImg/69649/wind-turbine-l.webp" type="image/webp"><source media="(min-width: 1000px)" srcset="https://assets.iflscience.com/assets/articleNo/70047/aImg/69649/wind-turbine-l.jpg" type="image/jpeg"><source media="(max-width: 567px)" srcset="https://assets.iflscience.com/assets/articleNo/70047/aImg/69649/wind-turbine-m.webp" type="image/webp"><source media="(max-width: 567px)" srcset="https://assets.iflscience.com/assets/articleNo/70047/aImg/69649/wind-turbine-m.jpg" type="image/jpeg"><img src="https://assets.iflscience.com/assets/articleNo/70047/aImg/69649/wind-turbine-m.jpg" alt="Mingyang Smart Energy offshore wind turbine close up" title="wind turbine" type="image/jpeg" loading="lazy"></picture><figcaption><p>The new mega-turbine is similar to the one pictured here, from the MySE series, designed and produced by Mingyang Smart Energy.</p><p>Image credit: Mingyang Smart Energy</p></figcaption></figure><article><div><p id="isPasted">China has long been touted as a <a href="https://www.iflscience.com/china-start-wind-power-revolution-2030-36432" target="_blank">revolutionary</a> when it comes to wind power. Earlier this year, it was reported that the country had begun construction of a wind farm using what were then hailed as the <a href="https://www.iflscience.com/china-has-started-building-a-wind-farm-using-the-world-s-largest-wind-turbines-67429" target="_blank">largest turbines ever seen</a>, each with a capacity of 16 megawatts. Now, a new milestone has been reached, with the successful switch-on of a turbine with a rotor diameter over twice the length of a football field.</p><p>China Three Gorges Corporation <a href="https://www.ctg.com.cn/sxjt/xwzx55/zhxw23/1438359/index.html" target="_blank" rel="noopener noreferrer">announced</a> that the 16-megawatt MySE 16-260 turbine had been successfully installed at the company’s offshore wind farm near Fujian Province on July 19. The behemoth is 152 meters (500 feet) tall, and each single blade is 123 meters (403 feet) and weighs 54 tons. This means that the sweep of the blades as they rotate covers an area of 50,000 square meters (nearly 540,000 square feet).</p><p>It's the first time such a large turbine has been hooked up to a commercial grid.</p><p>According to the corporation, just one of these turbines should be able to produce enough electricity to power 36,000 households of three people each for one year. Detailing the impressive green credentials of this technology, they claim that wind-powered domestic electricity could reduce carbon dioxide emissions by 54,000 tons compared with using <a href="https://www.iflscience.com/china-announces-it-will-stop-financing-coalfired-projects-abroad-61029" target="_blank">coal-fired power stations</a>.</p><p>The Fuijian offshore wind farm sits in the Taiwan Strait. Gusts of force 7 on the <a href="https://www.weather.gov/mfl/beaufort" target="_blank">Beaufort scale</a>, classified as “near gales”, are a regular occurrence in these treacherous waters, which is obviously perfect for generating wind power – provided, of course, that your turbines can withstand the weather. Mingyang Smart Energy, who designed the MySE 16-260, were already confident their machine was up to the challenge, stating in a <a href="https://www.linkedin.com/posts/mingyangsmartenergy_myse16-offshorewind-activity-7086980077443297280-Cazt/" target="_blank">LinkedIn post</a> that it could handle “extreme wind speeds of 79.8 [meters per second].”</p><p>Still, it wasn’t very long at all before these claims were put to the test, in the wake of the devastating typhoon <a href="https://www.bbc.co.uk/news/world-asia-66229532" target="_blank">Talim</a> that ravaged East Asia earlier this month. The typhoon threat is ever-present in this region, and the new mega-turbine withstood the onslaught.</p><p>Buoyed by the success of this installation, China Three Gorges Corporation is already looking to the future. “In the next step, the 16 [megawatt] unit will be applied in batches in the second phase of the Zhangpu Liuao Offshore Wind Farm Project constructed by China Three Gorges Corporation,” said executive director of the Three Gorges Group Fujian Company Lei Zengjuan.</p><p>Whilst China has been leading the way in developing bigger and more powerful turbines, other countries are hot on its heels. Construction is underway on the USA’s <a href="https://www.iflscience.com/new-massive-offshore-wind-turbine-can-power-a-home-for-2-days-with-a-single-spin-64247" target="_blank">Vineyard Wind 1</a>, a massive offshore development that will incorporate 13-megawatt GE Haliade-X turbines. In 2021, Denmark announced a project to build a dedicated <a href="https://www.iflscience.com/denmark-to-build-worldfirst-artificial-island-for-hundreds-of-giant-wind-turbines-58671" target="_blank">artificial island</a> of wind turbines off its coast.</p><p>In a world where a push away from <a href="https://www.iflscience.com/the-climate-doom-is-just-getting-started-say-experts-as-emissions-soar-66886" target="_blank">fossil fuels</a> is more <a href="https://www.iflscience.com/new-ipcc-climate-report-we-are-up-the-proverbial-creek-but-we-do-have-a-paddle-68051" target="_blank">urgently needed</a> than ever before, any and all advances in <a href="https://www.iflscience.com/tags/renewable-energy" target="_blank">renewable energy</a> must surely be good news.</p><p>[H/T: <a href="https://www.popularmechanics.com/science/green-tech/a44632369/china-turns-on-worlds-largest-turbine/" target="_blank" rel="noopener noreferrer">Popular Mechanics</a>]</p></div></article><div><hr><div><h3>ARTICLE POSTED IN</h3></div></div></section><br><div><header><img alt="technology" title="technology" width="32" height="32" src="https://assets.iflscience.com/svg/label/_technology.svg"><h2>More Technology Stories</h2></header><div><p><img alt="clock" title="clock" width="16" height="16" src="https://assets.iflscience.com/svg/_clock.svg"><span>Jul 28 2023  </span></p><div><p><span><img alt="share" title="share" width="32" height="32" src="https://assets.iflscience.com/svg/_share.svg"></span><span>48</span></p></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“Gzip beats BERT?” Part 2: dataset issues, improved speed, and results (191 pts)]]></title>
            <link>https://kenschutte.com/gzip-knn-paper2/</link>
            <guid>36921552</guid>
            <pubDate>Sat, 29 Jul 2023 16:04:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kenschutte.com/gzip-knn-paper2/">https://kenschutte.com/gzip-knn-paper2/</a>, See on <a href="https://news.ycombinator.com/item?id=36921552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3>"Gzip beats BERT?" Part 2: dataset issues, improved speed, and results</h3><p><i>Published 2023 / 07 / 28</i></p><div id="topimg"><p><img src="https://kenschutte.com/gzip-knn-paper2/banner.png"></p><p>
Image: Four DALL-E results for "gzip as a
super intelligence" (No, I am not a "prompt engineer").
</p></div><div><p><b>Summary</b>: <i>Update to my <a href="https://kenschutte.com/gzip-knn-paper/">last post</a> analyzing the gzip+knn paper. Some of the datasets have heavily contaminated train/test tests. I show some ways to significantly speed-up the <code>gzip</code> method. I present more results of my <span>"fair"</span> reimplemented version.</i></p></div><div><p>Last week I wrote
<a href="https://kenschutte.com/gzip-knn-paper/">my analysis</a>
of the code
for the <a href="https://aclanthology.org/2023.findings-acl.426/">paper</a>
<i><span>"Low-Resource"</span> Text Classification: A Parameter-Free Classification Method with Compressors</i>.
The paper proposed a text classification method using
<code>gzip</code> + <code>kNN</code> and gained some attention on twitter by showing this <span>"simple"</span>
method beat many benchmarks, including language models like BERT.</p><p>It turns out that the classification method used
in their code <i>looked at the test label</i> as
part of the decision method and thus led
to an unfair comparison to the baseline results.
You can read the discussion with the author
on this <a href="https://github.com/bazingagin/npc_gzip/issues/3">github issue</a>.
They confirmed that this was not a bug, but a way
to compute the <i>maximum possible</i> accuracy for a stochastic
classifier (choosing one of the top-2 nearest neighbors). I claim that this too unfairly inflates their
method’s results against the baselines.
Also, it’s unnecessary: you can use
a stochastic classifier and present
the <i>average</i> (and possibly more stats like min, max, std, etc.) over repeated trials.</p><p>In this post I describe another major problem that
someone discovered: some of the datasets have heavily
contaminated train/test sets. I also show
some ways to greatly speed up the <code>gzip</code>-based method,
and I present more complete results using
what I consider a <span>"fair"</span> accuracy calculation.</p></div><h3>Contents</h3><ul><li><a href="#dataset-issues">1. Dataset issues</a></li><ul><li><a href="#details">1.1. Details</a></li><li><a href="#ambiguous-data-points">1.2. Ambiguous data points</a></li></ul><li><a href="#speed-improvements">2. Speed Improvements</a></li><ul><li><a href="#repeated-work">2.1. Repeated work</a></li><li><a href="#gzip-/-zlib-tricks">2.2. gzip / zlib tricks</a></li><li><a href="#multiprocessing">2.3. Multiprocessing</a></li></ul><li><a href="#results">3. Results</a></li><ul><li><a href="#notes">3.1. Notes</a></li></ul><li><a href="#conclusions">4. Conclusions</a></li></ul><div><div><h3 id="dataset-issues">1. Dataset issues</h3><div><p>At the end of my previous post, I included a brief
to-do note: <i><span>"Why is filipino so high (1.0 in one case)?"</span></i>,
referring to one dataset (<code>DengueFilipino</code>) having suspiciously high accuracy.
My attempt to run the official code
gave 100% accuracy. The paper reported 99.8% (Table 5).</p><p>Before I got around to looking at that, someone
else discovered and reported the problem (see the
<a href="https://github.com/bazingagin/npc_gzip/issues/13">github issue</a>) - the <code>filipino</code> dataset has <i>exactly equal train and test tests</i>! Other datasets also have
significant train/test contamination.
There are also duplicate data points.</p></div><div><div><h3 id="details">1.1. Details</h3><div><p>
Here are the statistics I computed for all the datasets: </p><table id="statstable"><tbody><tr><th></th><th colspan="3">train set</th><th colspan="3">test set</th><th colspan="2">train-test intersection</th></tr><tr><th>name</th><th>count</th><th>unique</th><th>%dup</th><th>count</th><th>unique</th><th>%dup</th><th>count</th><th>% of test</th></tr><tr><td>AG_NEWS</td><td>120000</td><td>120000</td><td></td><td>7600</td><td>7600</td><td></td><td>0</td><td></td></tr><tr><td>DBpedia</td><td>560000</td><td>560000</td><td></td><td>70000</td><td>70000</td><td></td><td>0</td><td></td></tr><tr><td>YahooAnswers</td><td>1400000</td><td>1400000</td><td></td><td>60000</td><td>60000</td><td></td><td>0</td><td></td></tr><tr><td>20News</td><td>11314</td><td>11314</td><td></td><td>7532</td><td>7532</td><td></td><td>0</td><td></td></tr><tr><td>ohsumed</td><td>3357</td><td>3357</td><td></td><td>4043</td><td>4043</td><td></td><td>0</td><td></td></tr><tr><td>R8</td><td>5485</td><td>5427</td><td>1.1%</td><td>2189</td><td>2176</td><td>0.6%</td><td>4</td><td>0.2%</td></tr><tr><td>R52</td><td>6532</td><td>6454</td><td>1.2%</td><td>2568</td><td>2553</td><td>0.6%</td><td>6</td><td>0.2%</td></tr><tr><td>KinyarwandaNews</td><td>17014</td><td>9199</td><td>45.9%</td><td>4254</td><td>2702</td><td>36.5%</td><td>643</td><td>23.8%</td></tr><tr><td>KirundiNews</td><td>3689</td><td>1791</td><td>51.5%</td><td>923</td><td>698</td><td>24.4%</td><td>631</td><td>90.4%</td></tr><tr><td>DengueFilipino</td><td>4015</td><td>3951</td><td>1.6%</td><td>4015</td><td>3951</td><td>1.6%</td><td>3951</td><td>100.0%</td></tr><tr><td>SwahiliNews</td><td>22207</td><td>22207</td><td></td><td>7338</td><td>7338</td><td></td><td>34</td><td>0.5%</td></tr><tr><td>SogouNews</td><td>450000</td><td>450000</td><td></td><td>60000</td><td>60000</td><td></td><td>0</td><td></td></tr></tbody></table><p>The <a href="https://github.com/bazingagin/npc_gzip/">paper’s repo</a> does minimal processing on the datasets. It turns out that <i>these problems exist in the source Huggingface datasets</i>. The two worst ones can be checked quickly using only Huggingface’s <code>datasets.load_dataset</code>:</p><div><pre><span></span><span>&gt;&gt;&gt;</span> <span>from</span> <span>datasets</span> <span>import</span> <span>load_dataset</span>
<span>&gt;&gt;&gt;</span> <span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>&gt;&gt;&gt;</span> <span># train == test</span>
<span>&gt;&gt;&gt;</span> <span>ds</span> <span>=</span> <span>load_dataset</span><span>(</span><span>"dengue_filipino"</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>print</span><span>(</span><span>list</span><span>(</span><span>ds</span><span>[</span><span>'train'</span><span>])</span> <span>==</span> <span>list</span><span>(</span><span>ds</span><span>[</span><span>'test'</span><span>]))</span>
<span>True</span>

<span>&gt;&gt;&gt;</span> <span># 90% overlap:</span>
<span>&gt;&gt;&gt;</span> <span>ds2</span> <span>=</span> <span>load_dataset</span><span>(</span><span>'kinnews_kirnews'</span><span>,</span> <span>'kirnews_cleaned'</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>keys</span> <span>=</span> <span>sorted</span><span>(</span><span>ds2</span><span>[</span><span>'test'</span><span>][</span><span>0</span><span>]</span><span>.</span><span>keys</span><span>())</span>
<span>&gt;&gt;&gt;</span> <span>set_train</span> <span>=</span> <span>set</span><span>([</span><span>tuple</span><span>([</span><span>item</span><span>[</span><span>key</span><span>]</span> <span>for</span> <span>key</span> <span>in</span> <span>keys</span><span>])</span> <span>for</span> <span>item</span> <span>in</span> <span>ds2</span><span>[</span><span>'train'</span><span>]])</span>
<span>&gt;&gt;&gt;</span> <span>set_test</span>  <span>=</span> <span>set</span><span>([</span><span>tuple</span><span>([</span><span>item</span><span>[</span><span>key</span><span>]</span> <span>for</span> <span>key</span> <span>in</span> <span>keys</span><span>])</span> <span>for</span> <span>item</span> <span>in</span> <span>ds2</span><span>[</span><span>'test'</span><span>]])</span>
<span>&gt;&gt;&gt;</span> <span>print</span><span>(</span><span>np</span><span>.</span><span>mean</span><span>([</span><span>a</span> <span>in</span> <span>set_train</span> <span>for</span> <span>a</span> <span>in</span> <span>set_test</span><span>]))</span>
<span>0.9040114613180515</span>  
</pre></div></div></div><div><h3 id="ambiguous-data-points">1.2. Ambiguous data points</h3><div><div><p>Of course, having test points that
are in your training data
will artificially raise the
accuracy for all classifiers. But,
it is particularly beneficial
for <code>kNN</code> - a point’s nearest neighbor will be
itself and we should get everything perfectly correct
(as opposed to neural network
classifiers that typically have some
amount of training error).</p><p>So, how did the <code>DengueFilipino</code> tests
have below 100% accuracy for <code>kNN</code>?
As I mentioned, my re-running
of the official code gave 100%, so I’m not
sure how they got the 99.8% in Table 5.
My <code>kNN(k=1)</code> classifier got 99.9%
accuracy (5 errors out of 4015), so let’s
look at the five test points it got wrong:</p></div><table id="tab_errors"><tbody><tr><th></th><th colspan="2">test point</th><th colspan="2">closest train point</th></tr><tr><th>index</th><th>text</th><th>label</th><th>text</th><th>label</th></tr><tr><td>1269</td><td>"Broken heart syndrome ?"</td><td>4</td><td>"Broken heart syndrome ?"</td><td>2</td></tr><tr><td>2363</td><td>"Monday sickness ?"</td><td>4</td><td>"Monday sickness ?"</td><td>0</td></tr><tr><td>2441</td><td>"Monday sickness ?"</td><td>4</td><td>"Monday sickness ?"</td><td>0</td></tr><tr><td>2677</td><td>"Monday sickness ?"</td><td>2</td><td>"Monday sickness ?"</td><td>0</td></tr><tr><td>3440</td><td>"playsafe, u ain't sick ?"</td><td>4</td><td>"playsafe, u ain't sick ?"</td><td>2</td></tr></tbody></table><div><p>So, the dataset has <i>repeated texts with different
ground-truth labels</i>.
Note, this is different from the <i>dup</i> (duplicates)
column in the above table, which was counting
duplicates of the <code>(data,label)</code> tuples. In the <code>kNN</code>, these
duplicates have equal distances, and in these cases,
it happened to select the wrong ones.</p><p>Someone with knowledge of the
dataset will have to determine
if this is a preparation error or an actual
desired property.
If it’s the latter, perhaps a different
scoring criterion could be used (of course,
followed by all compared methods).</p></div></div></div></div></div><div><h3 id="speed-improvements">2. Speed Improvements</h3><div><div><p>In trying to recreate the baseline results,
you will quickly learn that the <code>gzip + kNN</code>
method is <i>slow</i> for datasets
with a large training set.</p><p>The costly computation is the distance
matrix containing <code>num_train × num_test</code> elements.</p><p>In the paper’s codebase, there is a
<code>calc_dis</code> method <a href="https://github.com/bazingagin/npc_gzip/blob/c18e2206af844abaf934c00f2e61b8e8f4bf2925/experiments.py#L34">here</a> and
if we remove the abstractions and
remove some of the unused options,
we see it doing the following:</p></div><div><pre><span></span><span>#test_data:[str]</span>
<span>#train_data:[str]</span>

<span>#clen: compute the compressed-length of data:</span>
<span>clen</span> <span>=</span> <span>lambda</span> <span>data</span> <span>:</span> <span>len</span><span>(</span><span>gzip</span><span>.</span><span>compress</span><span>(</span><span>data</span><span>))</span>        

<span># NCD: Normalized Compression Distance</span>
<span># distance function</span>
<span>NCD</span> <span>=</span> <span>lambda</span> <span>c1</span><span>,</span> <span>c2</span><span>,</span> <span>c12</span> <span>:</span> <span>(</span><span>c12</span> <span>-</span> <span>min</span><span>(</span><span>c1</span><span>,</span><span>c2</span><span>))</span> <span>/</span> <span>max</span><span>(</span><span>c1</span><span>,</span> <span>c2</span><span>)</span>
        
<span>for</span> <span>i</span><span>,</span><span>t1</span> <span>in</span> <span>enumerate</span><span>(</span><span>test_data</span><span>):</span>
    <span>l1</span> <span>=</span> <span>clen</span><span>(</span><span>t1</span><span>.</span><span>encode</span><span>(</span><span>'utf8'</span><span>))</span>
    <span>for</span> <span>j</span><span>,</span><span>t2</span> <span>in</span> <span>enumerate</span><span>(</span><span>train_data</span><span>):</span>
        <span>l2</span> <span>=</span> <span>clen</span><span>(</span><span>t2</span><span>.</span><span>encode</span><span>(</span><span>'utf8'</span><span>))</span>
	<span>l12</span> <span>=</span> <span>clen</span><span>(</span> <span>(</span><span>t1</span> <span>+</span> <span>' '</span> <span>+</span> <span>t2</span><span>)</span><span>.</span><span>encode</span><span>(</span><span>'utf8'</span><span>)</span> <span>)</span>
	<span>D</span><span>[</span><span>i</span><span>,</span><span>j</span><span>]</span> <span>=</span> <span>NCD</span><span>(</span><span>l1</span><span>,</span><span>l2</span><span>,</span><span>l12</span><span>)</span> <span># fill distance matrix</span>
        
</pre></div></div><div><div><h3 id="repeated-work">2.1. Repeated work</h3><div><div><p>We can quickly see a few things are being unnecessarily repeated,</p><ul><li><p>There are a few repeated <code>str.encode()</code>.
We can start by converting everything to bytes.</p></li><li><p>The computation of <code>l2</code> is in the inner loop, but does
not depend on the test data (<code>t1</code>).
We should pre-compute <code>l2</code> for the whole training
set only once.</p></li></ul><p>So, now we have something like,</p></div><div><pre><span></span><span>train_data</span> <span>=</span> <span>[</span><span>a</span><span>.</span><span>encode</span><span>(</span><span>'utf8'</span><span>)</span> <span>for</span> <span>a</span> <span>in</span> <span>train_data</span><span>]</span>    
<span>test_data</span>  <span>=</span> <span>[</span><span>a</span><span>.</span><span>encode</span><span>(</span><span>'utf8'</span><span>)</span> <span>for</span> <span>a</span> <span>in</span> <span>test_data</span><span>]</span>

<span>train_lengths</span> <span>=</span> <span>[</span><span>clen</span><span>(</span><span>t2</span><span>)</span> <span>for</span> <span>t2</span> <span>in</span> <span>train_data</span><span>]</span>
    
<span>for</span> <span>i</span><span>,</span><span>t1</span> <span>in</span> <span>enumerate</span><span>(</span><span>test_data</span><span>):</span>
    <span>l1</span> <span>=</span> <span>clen</span><span>(</span><span>t1</span><span>)</span>
    <span>for</span> <span>j</span><span>,</span><span>t2</span> <span>in</span> <span>enumerate</span><span>(</span><span>train_data</span><span>):</span>
        <span>l2</span> <span>=</span> <span>train_lengths</span><span>[</span><span>j</span><span>]</span>
	<span>l12</span> <span>=</span> <span>clen</span><span>(</span> <span>(</span><span>t1</span> <span>+</span> <span>b</span><span>' '</span> <span>+</span> <span>t2</span><span>)</span> <span>)</span>
	<span>D</span><span>[</span><span>i</span><span>,</span><span>j</span><span>]</span> <span>=</span> <span>NCD</span><span>(</span><span>l1</span><span>,</span><span>l2</span><span>,</span><span>l12</span><span>)</span>
</pre></div><p>I don’t have the exact speed-up numbers on this step right
now, but I think it is pretty significant.
For example, <code>YahooAnswers</code> dataset
has 1.4 million training samples and 60 thousand test samples.
So, in the original code, for each of the 60 thousand test
points, it was recomputing the <code>clen()</code> for each of the 1.4 million
training documents <i>every time</i>.</p></div></div><div><h3 id="gzip-/-zlib-tricks">2.2. gzip / zlib tricks</h3><div><div><p>Another case of repeated computation: we compress <code>t1</code>
(the test point), but
then in the inner-loop, we compress it again (as
the start of the larger string <code>t1 + " " + t2</code>).
Can we remove this rendancy? Yes we can!</p><p>First, the relationship between a few related terms:
<b><a href="https://en.wikipedia.org/wiki/Gzip">GZIP</a></b>
is a <i>file format</i> for holding compressed data.
The data is compressed with the <b><a href="https://en.wikipedia.org/wiki/Deflate">DEFLATE</a></b> algorithm.
<b><a href="https://www.zlib.net/">zlib</a></b>
is a library that implements <i>DEFLATE</i>
and has it’s own lower-level format for
compressed data.</p><p>So, <i>gzip</i> is simply a small
wrapper around <i>zlib</i>.
You can see this clearly in Python’s source code
for the <code>gzip.compress(x)</code> function [<a href="https://github.com/python/cpython/blob/6138ecdeb80d3a62d5cef27b08669495bccbe19b/Lib/gzip.py#L576">here</a>]:
it simply returns <code>header + zlib.compress(x) + footer</code>.
[<i>So maybe instead of the headline <span>"gzip beats BERT?"</span> it should be <span>"zlib beats BERT?"</span> or <span>"DEFLATE beats BERT?"</span></i>]</p><p>We want to compress a string (test point <code>t1</code>) with <code>zlib</code>, then continue that compression with different training points (<code>t2</code>). Luckily Python’s
<a href="https://docs.python.org/3/library/zlib.html"><code>zlib</code> module</a> provides the exact interface we need: a <code>compressobj</code> that stores the state of the <code>zlib</code> compressor and a <code>copy</code> method to copy its state.
The <a href="https://docs.python.org/3/library/zlib.html#zlib.Compress.copy">Python docs for <code>copy()</code></a>
even mention our use-case: <code>Compress.copy(): Returns a copy of the compression object. This can be used to efficiently compress a set of data that share a common initial prefix.</code></p><p>I implemented this in a class called <a href="https://github.com/kts/gzip-knn/blob/main/gziplength.py"><code>GzipLengthCalc</code></a>. Using this, the main loop is now,</p></div><div><pre><span></span><span>for</span> <span>i</span><span>,</span><span>t1</span> <span>in</span> <span>enumerate</span><span>(</span><span>test_data</span><span>):</span>

    <span>#starts compressing 't1 + b" "'        </span>
    <span>g</span> <span>=</span> <span>GzipLengthCalc</span><span>(</span><span>t1</span><span>)</span>
    <span>l1</span> <span>=</span> <span>g</span><span>.</span><span>length1</span>

    <span>for</span> <span>j</span><span>,</span><span>t2</span> <span>in</span> <span>enumerate</span><span>(</span><span>train_data</span><span>):</span>
        <span>l2</span> <span>=</span> <span>precomputed_lengths</span><span>[</span><span>j</span><span>]</span>
        <span># continues from stored zlib state to</span>
        <span># compress t2:</span>
        <span>l12</span> <span>=</span> <span>g</span><span>.</span><span>length2</span><span>(</span><span>t2</span><span>)</span>
        <span>D</span><span>[</span><span>i</span><span>,</span><span>j</span><span>]</span> <span>=</span> <span>NCD</span><span>(</span><span>l1</span><span>,</span> <span>l2</span><span>,</span> <span>l12</span><span>)</span>
</pre></div><div><p>Keep in mind that the inner-loop is run billions of times for some of the datasets, so removing anything there can be a huge speed up.</p><p>Any more room for improvement?</p><ul><li><p>I tried removing more of the <code>NCD</code> calculation from the inner loop. The loops can compute the matrix of distances, and we compute the <code>NCD</code> outside of the loops using vectorized numpy, <code>D = (C12 - np.minimum(C1,C2)) / np.maximum(C1,C2)</code>. This had a small speed improvement, but at the memory cost of allocated 3 matrices instead of 1, so I didn’t use that.</p></li><li><p>Is it possible that precomputed <code>clen(t2)</code> can speed up the computation of <code>clen(t1 + b' ' + t2)</code>? Probably not. <code>zlib</code> works sequentially. <i>Perhaps</i>, with some real <code>zlib</code> wizardry: there are internal parameters like a <span>"block size"</span> such that the past bytes a certain distance before the current point no longer matter. Perhaps this could be exploited in the case of large texts.</p></li><li><p>If more significant gains were wanted, I’d suggest computing this double-loop in C.</p></li></ul></div></div></div><div><h3 id="multiprocessing">2.3. Multiprocessing</h3><div><div><p>The original code had some support for multiprocessing.
I expanded this using <a href="https://docs.python.org/3/library/concurrent.futures.html">concurrent.futures</a> and made it flexible to parallelize to the number of workers using <code>os.cpu_count()</code>.</p><p>For example, if you run a machine with 36 vCPUs, you can get 36 Python
processes all running at 100% as shown below.</p><p>So, who cares about the <a href="https://wiki.python.org/moin/GlobalInterpreterLock">GIL</a>? Well, as-is, the
multiprocessing has quite a bit of overhead in passing around large amounts of data to the
child processes. This could be improved.</p></div><p><a href="https://kenschutte.com/gzip-knn-paper2/top.png"><img width="500" src="https://kenschutte.com/gzip-knn-paper2/top_tn.png"></a></p></div></div></div></div><div><h3 id="results">3. Results</h3><div><div><p>With these speed improvements, I could
run my implementation across all the datasets.
I present results below along with the
comparable results from the paper, Table 3 and Table 5.</p><p>The rows in my results table are:</p><ul><li><p><i>paper</i>: Copied value from the paper (Table 3 and Table 5).</p></li><li><p><i>rerun</i>: My run of the (<a href="https://github.com/bazingagin/npc_gzip">code</a>) provided by the authors. The blanks in this row mean it was too slow, and I didn’t finish it.</p></li><li><p><i>top2</i>: My implementation, with <i>top-2</i> accuracy. <code>kNN</code> that is marked correct if either of the top-2 nearest neighbors are correct.</p></li><li><p><i>knn1</i>: My implementation, standard <code>kNN</code> with <code>k=1</code>, i.e.&nbsp;just taking the label of the single nearest neighbor.</p></li></ul></div><p><a href="https://kenschutte.com/gzip-knn-paper2/table3b.jpg"><img src="https://kenschutte.com/gzip-knn-paper2/table3b.jpg"></a></p><table><tbody><tr><th></th><th>AGNews</th><th>DBpedia</th><th>YahooAn.</th><th>20News</th><th>Ohsumed</th><th>R8</th><th>R52</th></tr><tr><th>paper</th><td>0.937</td><td>0.970</td><td>0.638</td><td>0.685</td><td>0.521</td><td>0.954</td><td>0.896</td></tr><tr><th>rerun</th><td></td><td></td><td></td><td>0.685</td><td></td><td></td><td></td></tr><tr><th>top2</th><td>0.937</td><td>0.970</td><td>0.622</td><td>0.685</td><td>0.481</td><td>0.952</td><td>0.889</td></tr><tr><th>knn1</th><td>0.876</td><td>0.942</td><td>0.485</td><td>0.607</td><td>0.365</td><td>0.913</td><td>0.852</td></tr></tbody></table><p><a href="https://kenschutte.com/gzip-knn-paper2/table5b.jpg"><img src="https://kenschutte.com/gzip-knn-paper2/table5b.jpg"></a></p><table><tbody><tr><th></th><th>KinyarwandaNews</th><th>KirundiNews</th><th>DengueFilipino</th><th>SwahiliNews</th><th>SogouNews</th></tr><tr><th>paper</th><td>0.891</td><td>0.905</td><td>0.998</td><td>0.927</td><td>0.975</td></tr><tr><th>rerun</th><td>0.891</td><td>0.906</td><td>1.000</td><td>0.927</td><td></td></tr><tr><th>top2</th><td>0.891</td><td>0.906</td><td>1.000</td><td>0.927</td><td>0.973</td></tr><tr><th>knn1</th><td>0.835</td><td>0.858</td><td>0.999</td><td>0.850</td><td>0.951</td></tr></tbody></table></div><div><h3 id="notes">3.1. Notes</h3><div><p>The following 3 rows should all have the same value: <i>paper</i>, <i>rerun</i>, and <i>top2</i>. As in my previous blog post, that confirms that the
paper used a non-comparable <i>top-2</i> accuracy metric and I was
able to recreate it. There are a few cases where these 3 are not the same:</p><ul><li><p>The three datasets: <i>Ohsumed</i>, <i>R8</i>, and <i>R52</i> are the three datasets that are not automatically prepared by the <code>npc_gzip</code> repo [<a href="https://github.com/bazingagin/npc_gzip/issues/17">see issue</a>]. It is very possible that I am using a slightly different dataset than the original paper.</p></li><li><p><code>YahooAnswers</code> dataset. Not sure why <code>paper = 0.638</code> and <code>top2 = 0.622</code>.</p></li><li><p><code>DengueFilipino</code> is very close. This was the one where training test equals test set.</p></li></ul><p>In the table images, I wrote the <code>knn1</code> scores below the table. When using these as the <code>gzip</code> results, we generally don’t see improvement compared to baseline methods.</p><p>For now, I only have the <code>k=1</code> results here. <a href="https://kenschutte.com/gzip-knn-paper/">My last post</a> showed a handfull of different <code>k</code> values and methods for breaking ties. In those results, there was no clear winner across datasets. I will try generate these numbers and add to the results above.</p></div></div></div><div><h3 id="conclusions">4. Conclusions</h3><div><p>Between the accuracy calculation and
contaminated datasets, I believe that
many of the key results (and thus also the conclusions)
in the paper are not valid.</p><p>The paper touts <code>kNN + gzip</code> as computationally
simpler than language-model-based methods,
but beware that it is <i>slow</i> for the datasets
with large amount of training samples.</p><p>Nevertheless, using ideas from text compression
for text classification tasks is an interesting idea
and may lead to other interesting research.
For example, see <a href="https://github.com/cyrilou242/ftcc">this work</a> using <code>zstd</code> dictionaries per class to do something with the same concept, but much more efficiently.</p></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ways to shoot yourself in the foot with Redis (104 pts)]]></title>
            <link>https://philbooth.me/blog/four-ways-to-shoot-yourself-in-the-foot-with-redis</link>
            <guid>36920630</guid>
            <pubDate>Sat, 29 Jul 2023 14:25:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://philbooth.me/blog/four-ways-to-shoot-yourself-in-the-foot-with-redis">https://philbooth.me/blog/four-ways-to-shoot-yourself-in-the-foot-with-redis</a>, See on <a href="https://news.ycombinator.com/item?id=36920630">Hacker News</a></p>
Couldn't get https://philbooth.me/blog/four-ways-to-shoot-yourself-in-the-foot-with-redis: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[If we want a shift to walking we need to prioritize dignity (387 pts)]]></title>
            <link>https://www.strongtowns.org/journal/2023/7/28/if-we-want-a-shift-to-walking-we-need-to-prioritize-dignity</link>
            <guid>36920622</guid>
            <pubDate>Sat, 29 Jul 2023 14:25:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.strongtowns.org/journal/2023/7/28/if-we-want-a-shift-to-walking-we-need-to-prioritize-dignity">https://www.strongtowns.org/journal/2023/7/28/if-we-want-a-shift-to-walking-we-need-to-prioritize-dignity</a>, See on <a href="https://news.ycombinator.com/item?id=36920622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-nc-base="header" data-controller="AncillaryLayout">
          

          <main>
            
              <section data-content-field="main-content">
                <article id="post-64c2930926f82617bbca3aa5" data-item-id="64c2930926f82617bbca3aa5">

    
      
    

    <div data-layout-label="Post Body" data-type="item" data-updated-on="1690477370700" id="item-64c2930926f82617bbca3aa5"><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_269191">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/eacf6bfa-c47f-49a1-ae89-74d7c932c612/dignity-comparison-scaled-e1689281726969-624x328.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/eacf6bfa-c47f-49a1-ae89-74d7c932c612/dignity-comparison-scaled-e1689281726969-624x328.jpg" data-image-dimensions="624x328" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/eacf6bfa-c47f-49a1-ae89-74d7c932c612/dignity-comparison-scaled-e1689281726969-624x328.jpg" width="624" height="328" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/eacf6bfa-c47f-49a1-ae89-74d7c932c612/dignity-comparison-scaled-e1689281726969-624x328.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/eacf6bfa-c47f-49a1-ae89-74d7c932c612/dignity-comparison-scaled-e1689281726969-624x328.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/eacf6bfa-c47f-49a1-ae89-74d7c932c612/dignity-comparison-scaled-e1689281726969-624x328.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/eacf6bfa-c47f-49a1-ae89-74d7c932c612/dignity-comparison-scaled-e1689281726969-624x328.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/eacf6bfa-c47f-49a1-ae89-74d7c932c612/dignity-comparison-scaled-e1689281726969-624x328.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/eacf6bfa-c47f-49a1-ae89-74d7c932c612/dignity-comparison-scaled-e1689281726969-624x328.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/eacf6bfa-c47f-49a1-ae89-74d7c932c612/dignity-comparison-scaled-e1689281726969-624x328.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-fa7aad9dfb45a96bb22e">
  <p>Have you ever had a friend return from a vacation and gush about how great it was to walk in the place they’d visited? “You can walk everywhere! To a café, to the store. It was amazing!” Immediately after saying that, your friend hops in their car and drives across the parking lot to the Starbucks to which they could easily have walked.</p><p>Why does walking feel so intuitive when we’re in a city built before cars, yet as soon as we return home, walking feels like an unpleasant chore that immediately drives us into a car?</p><p>A lot contributes to this dilemma, like the density of the city, or the relative cheapness and convenience of driving. But there’s a bigger factor here: We don’t design the pedestrian experience for dignity.</p><p>This is a national problem, but certainly one those of us in the Minneapolis–Saint Paul metropolitan area can see throughout the Twin Cities metro: Even where pedestrian facilities are built, brand-new, ADA-compliant, and everything else—using them feels like a chore, or even stressful and unpleasant.</p><p>Dignity is a really important concept in active transportation, but one that we often miss in the conversation about making streets better for walking and biking. I’ve been delighted to see the term appear on&nbsp;<a href="https://www.instagram.com/pedestriandignity/">a social media account advocating for pedestrians</a>. But as we plan and design better streets for active transportation, we need to consider the dignity of the pedestrian experience.</p><h2>A Hierarchy of Needs</h2><p>Three related concepts exist in designing great pedestrian spaces, and they can be arranged similarly to Maslow’s hierarchy of needs. The base of the pyramid is the most essential, but having a complete and delightful pedestrian experience requires all three layers. The layers are compliance, safety, and dignity.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_272938">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/11ec90a8-fe26-4d02-bb28-ac29fea82b45/dignity-triangle-500x500.png" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/11ec90a8-fe26-4d02-bb28-ac29fea82b45/dignity-triangle-500x500.png" data-image-dimensions="500x500" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/11ec90a8-fe26-4d02-bb28-ac29fea82b45/dignity-triangle-500x500.png" width="500" height="500" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/11ec90a8-fe26-4d02-bb28-ac29fea82b45/dignity-triangle-500x500.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/11ec90a8-fe26-4d02-bb28-ac29fea82b45/dignity-triangle-500x500.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/11ec90a8-fe26-4d02-bb28-ac29fea82b45/dignity-triangle-500x500.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/11ec90a8-fe26-4d02-bb28-ac29fea82b45/dignity-triangle-500x500.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/11ec90a8-fe26-4d02-bb28-ac29fea82b45/dignity-triangle-500x500.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/11ec90a8-fe26-4d02-bb28-ac29fea82b45/dignity-triangle-500x500.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/11ec90a8-fe26-4d02-bb28-ac29fea82b45/dignity-triangle-500x500.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_273322">

<p>
  <h2><strong>Compliance</strong>: Often Not Enough</h2>
</p>



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_176851">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/84362759-87a9-4025-bb23-e600862816e0/shady-oak-road-1024x575.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/84362759-87a9-4025-bb23-e600862816e0/shady-oak-road-1024x575.jpg" data-image-dimensions="1024x575" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/84362759-87a9-4025-bb23-e600862816e0/shady-oak-road-1024x575.jpg" width="1024" height="575" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/84362759-87a9-4025-bb23-e600862816e0/shady-oak-road-1024x575.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/84362759-87a9-4025-bb23-e600862816e0/shady-oak-road-1024x575.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/84362759-87a9-4025-bb23-e600862816e0/shady-oak-road-1024x575.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/84362759-87a9-4025-bb23-e600862816e0/shady-oak-road-1024x575.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/84362759-87a9-4025-bb23-e600862816e0/shady-oak-road-1024x575.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/84362759-87a9-4025-bb23-e600862816e0/shady-oak-road-1024x575.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/84362759-87a9-4025-bb23-e600862816e0/shady-oak-road-1024x575.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Shady Oak Road in Hopkins is ADA compliant, but crossing here could be unsafe for any user. (Source: Google Street View.)</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_177240">
  <p>At the bottom of the pyramid you have compliance—for pedestrian facilities, that mainly means complying with ADA rules. This requirement is non-negotiable for agencies because failure to obey exposes them to legal challenges. The ADA has done a great deal to make pedestrian facilities better for all—certainly wheelchair users, but also those who walk, use strollers, ride bicycles on sidewalks, etc.</p><p>Unfortunately, compliance with ADA rules&nbsp;<em>alone</em>&nbsp;often does not yield good pedestrian facilities.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_307522">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/7241ef8c-b669-46d3-b562-f976acfe94ac/bad-faith-compliance-1024x650.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/7241ef8c-b669-46d3-b562-f976acfe94ac/bad-faith-compliance-1024x650.jpg" data-image-dimensions="1024x650" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/7241ef8c-b669-46d3-b562-f976acfe94ac/bad-faith-compliance-1024x650.jpg" width="1024" height="650" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/7241ef8c-b669-46d3-b562-f976acfe94ac/bad-faith-compliance-1024x650.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/7241ef8c-b669-46d3-b562-f976acfe94ac/bad-faith-compliance-1024x650.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/7241ef8c-b669-46d3-b562-f976acfe94ac/bad-faith-compliance-1024x650.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/7241ef8c-b669-46d3-b562-f976acfe94ac/bad-faith-compliance-1024x650.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/7241ef8c-b669-46d3-b562-f976acfe94ac/bad-faith-compliance-1024x650.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/7241ef8c-b669-46d3-b562-f976acfe94ac/bad-faith-compliance-1024x650.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/7241ef8c-b669-46d3-b562-f976acfe94ac/bad-faith-compliance-1024x650.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>As part of an ADA upgrade project, Edina and Hennepin County removed the north leg crosswalk, requiring pedestrians to cross this busy intersection three times to proceed on the north-side sidewalk.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_307906">

<p>For example, many agencies will simply remove pedestrian facilities to reduce the cost of compliance. A good example is the intersection of France and Parklawn Avenues in Edina. If you were on the west side of France and wanted to walk to the Allina clinic in 2013, you could simply have crossed on the north crosswalk. But to improve ADA compliance, Edina removed the north crosswalk in 2014. Now, you would have to cross the busy signalized intersection three times just to continue on the north sidewalk.</p>



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_186897">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/5b58d6aa-e00f-4432-aacf-492fb990cb85/removed-ped-button-500x472.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/5b58d6aa-e00f-4432-aacf-492fb990cb85/removed-ped-button-500x472.jpg" data-image-dimensions="500x472" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/5b58d6aa-e00f-4432-aacf-492fb990cb85/removed-ped-button-500x472.jpg" width="500" height="472" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/5b58d6aa-e00f-4432-aacf-492fb990cb85/removed-ped-button-500x472.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/5b58d6aa-e00f-4432-aacf-492fb990cb85/removed-ped-button-500x472.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/5b58d6aa-e00f-4432-aacf-492fb990cb85/removed-ped-button-500x472.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/5b58d6aa-e00f-4432-aacf-492fb990cb85/removed-ped-button-500x472.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/5b58d6aa-e00f-4432-aacf-492fb990cb85/removed-ped-button-500x472.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/5b58d6aa-e00f-4432-aacf-492fb990cb85/removed-ped-button-500x472.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/5b58d6aa-e00f-4432-aacf-492fb990cb85/removed-ped-button-500x472.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>The crosswalk at France and Parklawn, showing the rusted outline of the former pedestrian push button. (Source: Google Street View.)</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_187356">

<p>In other cases, compliance is in good faith but not enough to make a pedestrian facility really usable—because complete compliance would entail a much larger project. This can be found when a broken-down sidewalk, or one with obstructions in the way, gets brand-new corner curb ramps but no other improvements. A wheelchair user can easily get up off the street at the corner, but can’t go farther than 10 feet without hitting another impediment.</p>



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_328267">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f5951cb9-8f79-4bd9-8cc3-70aba0dd4c47/31st-2nd-1024x663.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f5951cb9-8f79-4bd9-8cc3-70aba0dd4c47/31st-2nd-1024x663.jpg" data-image-dimensions="1024x663" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f5951cb9-8f79-4bd9-8cc3-70aba0dd4c47/31st-2nd-1024x663.jpg" width="1024" height="663" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f5951cb9-8f79-4bd9-8cc3-70aba0dd4c47/31st-2nd-1024x663.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f5951cb9-8f79-4bd9-8cc3-70aba0dd4c47/31st-2nd-1024x663.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f5951cb9-8f79-4bd9-8cc3-70aba0dd4c47/31st-2nd-1024x663.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f5951cb9-8f79-4bd9-8cc3-70aba0dd4c47/31st-2nd-1024x663.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f5951cb9-8f79-4bd9-8cc3-70aba0dd4c47/31st-2nd-1024x663.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f5951cb9-8f79-4bd9-8cc3-70aba0dd4c47/31st-2nd-1024x663.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f5951cb9-8f79-4bd9-8cc3-70aba0dd4c47/31st-2nd-1024x663.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>This new curb ramp and pedestrian push buttons at 31st and 2nd are great, but if you’re a wheelchair user, they won’t help you for long: not 10 feet away, you’ll encounter a section of sidewalk too narrow to pass due to a street light. (Source: Google Street View.)</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_328651">

<p>
  <h2><strong>Safety</strong>: A Step Further, But What Is Still Lacking?</h2>
</p>



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_342585">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/62eb6023-1935-489a-b92e-49dfe8bc0f3d/58th-st-undignified-1024x768.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/62eb6023-1935-489a-b92e-49dfe8bc0f3d/58th-st-undignified-1024x768.jpg" data-image-dimensions="1024x768" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/62eb6023-1935-489a-b92e-49dfe8bc0f3d/58th-st-undignified-1024x768.jpg" width="1024" height="768" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/62eb6023-1935-489a-b92e-49dfe8bc0f3d/58th-st-undignified-1024x768.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/62eb6023-1935-489a-b92e-49dfe8bc0f3d/58th-st-undignified-1024x768.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/62eb6023-1935-489a-b92e-49dfe8bc0f3d/58th-st-undignified-1024x768.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/62eb6023-1935-489a-b92e-49dfe8bc0f3d/58th-st-undignified-1024x768.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/62eb6023-1935-489a-b92e-49dfe8bc0f3d/58th-st-undignified-1024x768.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/62eb6023-1935-489a-b92e-49dfe8bc0f3d/58th-st-undignified-1024x768.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/62eb6023-1935-489a-b92e-49dfe8bc0f3d/58th-st-undignified-1024x768.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>58th Street in Edina is ADA compliant and probably safe enough to cross with low volumes. But the experience is undignified, with little separation from car traffic, and no shade.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_342970">
  <p>In the middle of the pyramid, you have safety—both perceived and actual. It is possible to create a facility that is compliant but does not seem very safe. Picture sparkling new curb ramps to cross a 45-mph surface street with no marked crosswalk. In other cases, facilities are well-designed and safe, but may still not be dignified.</p><p>An example of this is in my own backyard, on Hennepin County’s Nicollet Avenue. A very-welcome project last year installed new crosswalks to popular Augsburg Park. These have durable crosswalk markings, excellent signage, and refuge medians. But crossing still feels like a negotiation with drivers. And the overall sidewalk experience on the 1950s street is still lacking, with sidewalks at the back-of-curb and little to no shade.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_203270">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f2dde0f3-ac22-425f-9294-a2f3f090419e/nicollet-ave-500x375.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f2dde0f3-ac22-425f-9294-a2f3f090419e/nicollet-ave-500x375.jpg" data-image-dimensions="500x375" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f2dde0f3-ac22-425f-9294-a2f3f090419e/nicollet-ave-500x375.jpg" width="500" height="375" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f2dde0f3-ac22-425f-9294-a2f3f090419e/nicollet-ave-500x375.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f2dde0f3-ac22-425f-9294-a2f3f090419e/nicollet-ave-500x375.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f2dde0f3-ac22-425f-9294-a2f3f090419e/nicollet-ave-500x375.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f2dde0f3-ac22-425f-9294-a2f3f090419e/nicollet-ave-500x375.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f2dde0f3-ac22-425f-9294-a2f3f090419e/nicollet-ave-500x375.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f2dde0f3-ac22-425f-9294-a2f3f090419e/nicollet-ave-500x375.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/f2dde0f3-ac22-425f-9294-a2f3f090419e/nicollet-ave-500x375.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Nicollet Avenue and 71st Street in Richfield.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_203657">
  <h2><strong>Dignity</strong>: Making Walking Feel Right</h2><p>Finally, we have dignity.&nbsp;To determine whether a facility is dignified, I propose a simple test:</p><p>If you were driving past and saw a friend walking or rolling there, what would your first thought be:</p><ol data-rte-list="default"><li><p>“Oh, no, Henry’s car must have broken down! I better offer him a ride.”</p></li><li><p>“Oh, looks like Henry’s out for a walk! I should text him later.”</p></li></ol><p>This is a surprisingly good test. Picture seeing your friend on a leafy sidewalk versus walking along a 45 mph suburban arterial.&nbsp;<strong>What would you think intuitively?</strong></p><p>But to get more specific, these are the key factors in making a pedestrian experience dignified:</p><ul data-rte-list="default"><li><p>Shade and light.</p></li><li><p>Convenience.</p></li><li><p>Enclosure and proportions.</p></li><li><p>Engagement.</p></li></ul><h3>Shade and Light</h3>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_358827">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/ae4bbafa-17df-484b-8d2e-0c6c72683f28/northfield-sidewalk-375x500.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/ae4bbafa-17df-484b-8d2e-0c6c72683f28/northfield-sidewalk-375x500.jpg" data-image-dimensions="375x250" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/ae4bbafa-17df-484b-8d2e-0c6c72683f28/northfield-sidewalk-375x500.jpg" width="375" height="250" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/ae4bbafa-17df-484b-8d2e-0c6c72683f28/northfield-sidewalk-375x500.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/ae4bbafa-17df-484b-8d2e-0c6c72683f28/northfield-sidewalk-375x500.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/ae4bbafa-17df-484b-8d2e-0c6c72683f28/northfield-sidewalk-375x500.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/ae4bbafa-17df-484b-8d2e-0c6c72683f28/northfield-sidewalk-375x500.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/ae4bbafa-17df-484b-8d2e-0c6c72683f28/northfield-sidewalk-375x500.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/ae4bbafa-17df-484b-8d2e-0c6c72683f28/northfield-sidewalk-375x500.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/ae4bbafa-17df-484b-8d2e-0c6c72683f28/northfield-sidewalk-375x500.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>St. Olaf Avenue in Northfield has a dignified amount of shade—not tunnel-like, but keeping the sidewalk cool and protected from the sun.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_359211">

<p>A dignified facility needs consistent shade during hot summer months. At night, shadows should be minimal and the route should be clear. Especially when a tree canopy is present, this is best achieved with more individual fixtures installed lower to the ground and at a lower light output. However, a fairly consistent light level can be achieved even with basic cobraheads, as long as there are enough to light the corridor fully.</p>



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_373830">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/4b9d977a-b718-4096-9e40-f247fa7907b5/dignity-less-light-scaled.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/4b9d977a-b718-4096-9e40-f247fa7907b5/dignity-less-light-scaled.jpg" data-image-dimensions="3404x1915" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/4b9d977a-b718-4096-9e40-f247fa7907b5/dignity-less-light-scaled.jpg" width="3404" height="1915" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/4b9d977a-b718-4096-9e40-f247fa7907b5/dignity-less-light-scaled.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/4b9d977a-b718-4096-9e40-f247fa7907b5/dignity-less-light-scaled.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/4b9d977a-b718-4096-9e40-f247fa7907b5/dignity-less-light-scaled.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/4b9d977a-b718-4096-9e40-f247fa7907b5/dignity-less-light-scaled.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/4b9d977a-b718-4096-9e40-f247fa7907b5/dignity-less-light-scaled.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/4b9d977a-b718-4096-9e40-f247fa7907b5/dignity-less-light-scaled.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/4b9d977a-b718-4096-9e40-f247fa7907b5/dignity-less-light-scaled.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>The flowers are beautiful, but a dark street at night is less dignified than a well-lit one. Left is 70th Street near Garfield Avenue; right is Lyndale and 75th.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_374215">
  <h3>Convenience</h3><p>Routes should be intuitive, easy, and not feel tedious to navigate. Having to make sharp, 90° turns or go out of your way feels awkward and makes you feel like your time and effort are wasted—even if the detour is relatively minor.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_229652">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/fbf174d1-526f-433e-a75e-cf811929e2ec/inconvenient-routing-1024x768.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/fbf174d1-526f-433e-a75e-cf811929e2ec/inconvenient-routing-1024x768.jpg" data-image-dimensions="1024x768" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/fbf174d1-526f-433e-a75e-cf811929e2ec/inconvenient-routing-1024x768.jpg" width="1024" height="768" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/fbf174d1-526f-433e-a75e-cf811929e2ec/inconvenient-routing-1024x768.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/fbf174d1-526f-433e-a75e-cf811929e2ec/inconvenient-routing-1024x768.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/fbf174d1-526f-433e-a75e-cf811929e2ec/inconvenient-routing-1024x768.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/fbf174d1-526f-433e-a75e-cf811929e2ec/inconvenient-routing-1024x768.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/fbf174d1-526f-433e-a75e-cf811929e2ec/inconvenient-routing-1024x768.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/fbf174d1-526f-433e-a75e-cf811929e2ec/inconvenient-routing-1024x768.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/fbf174d1-526f-433e-a75e-cf811929e2ec/inconvenient-routing-1024x768.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Inconvenient pedestrian routing at York and 66th.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_393561">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/93461139-dd7a-46a0-86a1-387431fe14fc/bloomington-sidewalk-inconvenient-1024x768.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/93461139-dd7a-46a0-86a1-387431fe14fc/bloomington-sidewalk-inconvenient-1024x768.jpg" data-image-dimensions="1024x768" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/93461139-dd7a-46a0-86a1-387431fe14fc/bloomington-sidewalk-inconvenient-1024x768.jpg" width="1024" height="768" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/93461139-dd7a-46a0-86a1-387431fe14fc/bloomington-sidewalk-inconvenient-1024x768.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/93461139-dd7a-46a0-86a1-387431fe14fc/bloomington-sidewalk-inconvenient-1024x768.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/93461139-dd7a-46a0-86a1-387431fe14fc/bloomington-sidewalk-inconvenient-1024x768.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/93461139-dd7a-46a0-86a1-387431fe14fc/bloomington-sidewalk-inconvenient-1024x768.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/93461139-dd7a-46a0-86a1-387431fe14fc/bloomington-sidewalk-inconvenient-1024x768.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/93461139-dd7a-46a0-86a1-387431fe14fc/bloomington-sidewalk-inconvenient-1024x768.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/93461139-dd7a-46a0-86a1-387431fe14fc/bloomington-sidewalk-inconvenient-1024x768.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>A winding path around a bus stop pull-out on 82nd Street.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_230061">

<p>
  <h3>Enclosure and Proportions</h3>
</p>



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_402269">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/617a965d-a450-458e-9370-4688a6d53bab/hopkins.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/617a965d-a450-458e-9370-4688a6d53bab/hopkins.jpg" data-image-dimensions="3143x1768" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/617a965d-a450-458e-9370-4688a6d53bab/hopkins.jpg" width="3143" height="1768" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/617a965d-a450-458e-9370-4688a6d53bab/hopkins.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/617a965d-a450-458e-9370-4688a6d53bab/hopkins.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/617a965d-a450-458e-9370-4688a6d53bab/hopkins.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/617a965d-a450-458e-9370-4688a6d53bab/hopkins.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/617a965d-a450-458e-9370-4688a6d53bab/hopkins.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/617a965d-a450-458e-9370-4688a6d53bab/hopkins.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/617a965d-a450-458e-9370-4688a6d53bab/hopkins.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Compare these two streets in Hopkins: Shady Oak Road, which is wide open with a sense of enclosure, and Eighth Avenue, which is better proportioned with a clear street wall.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_402653">
  <p>It’s a very uncomfortable experience to walk along a wide-open corridor with no walls or edge definition—and it’s a common experience along suburban arterials, where you may have a wide road on one side and a wide-open parking lot on the other. You feel exposed and vulnerable. At the same time, overgrown sidewalks or ones that encroach on pedestrian space can feel claustrophobic and inconvenient. The right balance is needed.</p><h3>Engagement</h3>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_243247">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/8e5f9c79-b1cc-4fa8-bc6a-10fd4f2448a8/bp-sidewalk-blank-walls-1024x766.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/8e5f9c79-b1cc-4fa8-bc6a-10fd4f2448a8/bp-sidewalk-blank-walls-1024x766.jpg" data-image-dimensions="1024x766" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/8e5f9c79-b1cc-4fa8-bc6a-10fd4f2448a8/bp-sidewalk-blank-walls-1024x766.jpg" width="1024" height="766" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/8e5f9c79-b1cc-4fa8-bc6a-10fd4f2448a8/bp-sidewalk-blank-walls-1024x766.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/8e5f9c79-b1cc-4fa8-bc6a-10fd4f2448a8/bp-sidewalk-blank-walls-1024x766.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/8e5f9c79-b1cc-4fa8-bc6a-10fd4f2448a8/bp-sidewalk-blank-walls-1024x766.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/8e5f9c79-b1cc-4fa8-bc6a-10fd4f2448a8/bp-sidewalk-blank-walls-1024x766.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/8e5f9c79-b1cc-4fa8-bc6a-10fd4f2448a8/bp-sidewalk-blank-walls-1024x766.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/8e5f9c79-b1cc-4fa8-bc6a-10fd4f2448a8/bp-sidewalk-blank-walls-1024x766.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/8e5f9c79-b1cc-4fa8-bc6a-10fd4f2448a8/bp-sidewalk-blank-walls-1024x766.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>This sidewalk in Brooklyn Park has only the frontage of dilapidated privacy fences.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_243639">
  <p>Finally, engaging frontage is always more appealing than blank frontage. The extreme of this principle is obvious: Walking down a traditional main street is more pleasurable than walking through an industrial park. But even where land uses are similar, engagement of frontage can vary a lot: picture the difference between walking past the front doors of houses in a traditional neighborhood, and walking past privacy fences and back yards in cul-de-sac suburban neighborhoods. The traditional neighborhood is more interesting and engaging to walk through.</p><p>When I was visiting downtown Northfield, I noted a new building along Water Street (MN-3), which had similar materials to the older downtown buildings on Division: windows, brick, and (cultured) stone base. Yet the back was turned to the street, and the experience of walking past was undignified.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_417919">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/2a72184f-93f4-4640-8a39-4b951303a944/northfield-unengaging-2-scaled.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/2a72184f-93f4-4640-8a39-4b951303a944/northfield-unengaging-2-scaled.jpg" data-image-dimensions="3413x1920" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/2a72184f-93f4-4640-8a39-4b951303a944/northfield-unengaging-2-scaled.jpg" width="3413" height="1920" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/2a72184f-93f4-4640-8a39-4b951303a944/northfield-unengaging-2-scaled.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/2a72184f-93f4-4640-8a39-4b951303a944/northfield-unengaging-2-scaled.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/2a72184f-93f4-4640-8a39-4b951303a944/northfield-unengaging-2-scaled.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/2a72184f-93f4-4640-8a39-4b951303a944/northfield-unengaging-2-scaled.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/2a72184f-93f4-4640-8a39-4b951303a944/northfield-unengaging-2-scaled.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/2a72184f-93f4-4640-8a39-4b951303a944/northfield-unengaging-2-scaled.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/2a72184f-93f4-4640-8a39-4b951303a944/northfield-unengaging-2-scaled.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Consider the visual interest of these buildings in downtown Northfield. On the left, walking past tinted windows and blank walls on a new building along a concurrent section of Water Street and Highway 3 on the west side of downtown. On the right are Division Street’s engaging storefronts.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_418303">
  <h2>A Pedestrian Cannot Live on Compliance Alone</h2><p>Creating compliant sidewalks and trails is a high priority for agencies seeking to avoid litigation and serve pedestrians on the most basic level. Although that has some benefits, it isn’t enough. Whether actively undermining walkability (like removing crosswalks to achieve ADA compliance) to simply not doing enough (adding a new curb ramp to an otherwise wheelchair-hostile sidewalk), we need to go much further.</p><p>To make walking and rolling a desirable, everyday activity, we need facilities that are compliant, safe, and dignified. We have many examples in our communities of great pedestrian ways—but we have a long way to go to make it universal, and truly move the needle toward walking.</p>
</div><div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1690466434279_154204">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/1054aab5-9291-401b-aa56-3bd8d697b2c9/sean-2014.jpg" data-image="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/1054aab5-9291-401b-aa56-3bd8d697b2c9/sean-2014.jpg" data-image-dimensions="300x200" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/1054aab5-9291-401b-aa56-3bd8d697b2c9/sean-2014.jpg" width="300" height="200" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/1054aab5-9291-401b-aa56-3bd8d697b2c9/sean-2014.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/1054aab5-9291-401b-aa56-3bd8d697b2c9/sean-2014.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/1054aab5-9291-401b-aa56-3bd8d697b2c9/sean-2014.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/1054aab5-9291-401b-aa56-3bd8d697b2c9/sean-2014.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/1054aab5-9291-401b-aa56-3bd8d697b2c9/sean-2014.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/1054aab5-9291-401b-aa56-3bd8d697b2c9/sean-2014.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/53dd6676e4b0fedfbc26ea91/1054aab5-9291-401b-aa56-3bd8d697b2c9/sean-2014.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1690466434279_138835">

<p><a href="https://sdho.org/"><strong>Sean Hayford Oleary</strong></a> is a web developer and planner. He serves on the Richfield City Council, and previously on the city's Planning and Transportation commissions. Articles are written from a personal perspective and not on behalf of Richfield or others. Sean has a masters in urban planning from the Humphrey School. Follow his love of streets, home improvement, and all things Richfield on Twitter <a href="https://twitter.com/sdho">@sdho</a>.</p>



</div></div></div>

    

    

    

  </article>





  
              </section>
            
          </main>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Critical theory is radicalizing high school debate (163 pts)]]></title>
            <link>https://www.slowboring.com/p/how-critical-theory-is-radicalizing</link>
            <guid>36920566</guid>
            <pubDate>Sat, 29 Jul 2023 14:18:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.slowboring.com/p/how-critical-theory-is-radicalizing">https://www.slowboring.com/p/how-critical-theory-is-radicalizing</a>, See on <a href="https://news.ycombinator.com/item?id=36920566">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Every year, </span><a href="https://www.nfhs.org/articles/high-school-debate-provides-springboard-to-success-for-many-individuals/" rel="">hundreds of thousands</a><span> of students around the U.S. participate in competitive debate. Most start competing at a young age (early high school or even middle school), eager to learn about politics. At its best, the activity teaches students how to think critically about the government and the trade-offs that policymakers face. They are assigned to argue for different positions that they may not agree with and engage with their peers’ diverse perspectives. </span></p><p>I started competing in Parliamentary debate at 12 years old. Growing up in Silicon Valley—a place full of scorn for politics—and attending a STEM-focused high school, debate was how I learned about public policy and economics. Often, the activity broadened and enriched how I thought about politics. But debate has strayed from these goals. Instead of expanding students’ worldviews, debate has increasingly narrowed to become a microcosm of critical theory.</p><p>In a traditional debate round, students argue over a topic assigned by the tournament — for example, “The U.S. should adopt universal healthcare.” One side is expected to argue in favor of the motion (the affirmation side), and one against (the negation side). However, in recent years, many debaters have decided to flat-out ignore the assigned topic and instead hijack the round by proposing brand new (i.e., wholly unrelated to the original topic), debater-created resolutions that advocate complex social criticisms based on various theories — Marxism, anti-militarism, feminist international relations theory, neocolonialism, securitization, anthropocentrism, orientalism, racial positionality, Afro-Pessimism, disablism, queer ecology, and transfeminism. (To be clear, traditional feminism is out of fashion and seen as too essentialist.)</p><p><span>These critical theory</span></p><p><span> arguments, known as kritiks, are usually wielded by the negation side to criticize the fundamental assumptions of their affirmation side opponents. Kritik advocates argue that the world is so systematically broken that discussing public policy proposals and reforms misses what really matters: the need to fundamentally revolutionize society in some way. For example, if the topic was “The U.S. should increase the federal minimum wage,” the affirmation side might provide some arguments supporting this policy. But then the negation side, instead of arguing that the government shouldn’t raise the minimum wage, might reject spending any time on the original resolution and counter-propose a </span><a href="https://docs.google.com/document/d/11rRW787W27oBzJJNChMu4la6dSV4B78wd6S7yS4yei8/edit?usp=sharing" rel="">Marxist kritik</a><span>.</span></p><p><span> Here’s an example of how the negation might introduce this kritik:</span></p><blockquote><p>Revolutionary theory is a prior question — the aff [proposal about raising the minimum wage] is irrelevant in the grand scheme of capitalism... [You as a judge should] evaluate the debate as a dialectical materialist — you are a historian inquiring into the determinant factors behind the PMC [first affirmation speech] — The role of the ballot is to endorse the historical outlook of the topic with the most explanatory power... Vote negative to endorse Marxist labor theory of value.</p></blockquote><p><span>Or, if the topic was “The U.S. should increase troops in the Korean DMZ,” the negation might choose not to argue against the resolution and propose a </span><a href="https://docs.google.com/document/d/1G4X7if8NIk_4Aas8tUN9rzyzj7DGpIe-SpJn8yKGtf8/edit?usp=sharing" rel="">securitization kritik</a><span>:</span></p><blockquote><p>Securitization is a political decision that discursively constructs certain phenomena as threats to justify their management and extermination. The practice of security erases alternate perspectives through the dominance of Western rationalism, permitting unchecked violence against alterity. We should use this round to create space for an epistemological multiplicity that breaks down dominant discourses of North Korea.</p></blockquote><p>These are two examples of negation kritiks. Additionally, sometimes the affirmation side kicks off the debate by proposing a kritik — they don’t even bother advocating for the original resolution! For example, let’s say the original topic was “The U.S. should impose a carbon tax.” The affirmation&nbsp;side could decide to throw the resolution out the window and instead argue for an Afro-Pessimism kritik:</p><blockquote><p>Western societies are structured on Enlightenment-era philosophy that fundamentally does not value Black people as people, and defines them as slaves. Even though documents like the Constitution have been amended to end slavery, it created a society that is rotten to the core, and the only way to fix it is to burn down civil society.</p></blockquote><p><span>Over the past 20 years, kritiks have become massively popular in competitive high school debate.</span></p><p><span> In the 1990s, race-based kritiks started to crop up in both </span><a href="https://en.wikipedia.org/wiki/Policy_debate" rel="">Policy</a><span> (also known as Cross-Ex or CX) and </span><a href="https://en.wikipedia.org/wiki/Lincoln%E2%80%93Douglas_debate_format" rel="">Lincoln-Douglas</a><span> — the two original high school debate formats. Since then, they’ve become ubiquitous and expanded to include many other critical theories. YouTube recordings of debate rounds started becoming available around 2015. I reviewed all Tournament of Champions semifinal and final round recordings from 2015 to 2023, and found that</span><a href="https://www.youtube.com/watch?v=G1myltiTsM4&amp;ab_channel=JusticeW" rel=""> </a><a href="https://www.youtube.com/watch?v=d8EJNEnNC4Y&amp;ab_channel=LASADebate" rel="">about</a><a href="https://www.youtube.com/watch?v=_oQlFJI23JA&amp;t=1168s&amp;ab_channel=LASADebate" rel=""> two</a><span>-</span><a href="https://www.youtube.com/watch?v=bUeiUBRWtBE&amp;ab_channel=JusticeW" rel="">thirds</a><span> </span><a href="https://www.youtube.com/watch?v=G1myltiTsM4&amp;ab_channel=JusticeW" rel="">of</a><span> </span><a href="https://www.youtube.com/watch?v=G09N9NIAQ68&amp;ab_channel=RyanJames" rel="">Policy</a><span> </span><a href="https://www.youtube.com/watch?v=n6eEltSSRLY&amp;ab_channel=JohnBlock" rel="">rounds</a><span> and </span><a href="https://www.youtube.com/watch?v=CLXWcZ_kiIc&amp;list=PLwXiN_PfPjWOfRxx9p3L6VX724_PtVWOf&amp;index=14&amp;ab_channel=UKDigitalSpeechandDebate" rel="">almost</a><span> </span><a href="https://www.youtube.com/watch?v=a1oX7AAETlU&amp;ab_channel=KentuckyDebate" rel="">half</a><a href="https://www.youtube.com/watch?v=6549Ooh2ZxE&amp;ab_channel=PremierDebateAmerica" rel=""> </a><a href="https://www.youtube.com/watch?v=qGpnRkwrV1g&amp;ab_channel=KentuckyDebate" rel="">of</a><a href="https://www.youtube.com/watch?v=RP_81CItgcE&amp;ab_channel=DebateDrills" rel=""> </a><a href="https://www.youtube.com/watch?v=Okmb8v2l6yA&amp;list=PL6xLHF4dYRIWXfXrleUC5xlrQDxx39-1X&amp;index=22&amp;ab_channel=KentuckyDebate" rel="">Lincoln</a><span>-</span><a href="https://www.youtube.com/watch?v=e4MzZ_WS_Ww&amp;ab_channel=PremierDebateAmerica" rel="">Douglas</a><a href="https://www.youtube.com/watch?v=Okmb8v2l6yA&amp;list=PL6xLHF4dYRIWXfXrleUC5xlrQDxx39-1X&amp;index=22&amp;ab_channel=KentuckyDebate" rel=""> </a><a href="https://www.youtube.com/watch?v=dAXpCLs7I4Q&amp;ab_channel=SanJoseDebateIntensive" rel="">rounds</a><a href="https://www.youtube.com/watch?v=a1oX7AAETlU&amp;ab_channel=KentuckyDebate" rel=""> </a><a href="https://www.youtube.com/watch?v=6549Ooh2ZxE&amp;ab_channel=PremierDebateAmerica" rel="">featured</a><a href="https://www.youtube.com/watch?v=qGpnRkwrV1g&amp;ab_channel=KentuckyDebate" rel=""> </a><a href="https://www.youtube.com/watch?v=h0_6LLg3nqw&amp;ab_channel=TessWelch" rel="">critical</a><a href="https://www.youtube.com/watch?v=CLXWcZ_kiIc&amp;list=PLwXiN_PfPjWOfRxx9p3L6VX724_PtVWOf&amp;index=14&amp;ab_channel=UKDigitalSpeechandDebate" rel=""> </a><a href="https://www.youtube.com/watch?v=RP_81CItgcE&amp;ab_channel=DebateDrills" rel="">theory</a><span>.</span></p><p><span>Two new formats were started more recently, in response to this rising tide of critical theory: </span><a href="https://en.wikipedia.org/wiki/Public_forum_debate" rel="">Public Forum</a><span> in 2002 (started by CNN founder Ted Turner) and </span><a href="https://en.wikipedia.org/wiki/Parliamentary_debate" rel="">Parliamentary</a><span> debate around the same time.</span></p><p><span> These new formats were intended to focus more on public policy discussions and less on critical theory. However, critical theory has started to invade Public Forum and Parliamentary debate too. Critical theory was featured in 12.5% of Public Forum Tournament of Champions</span><a href="https://www.youtube.com/watch?v=jolUEZGp7Nc&amp;ab_channel=DanielGarepis-Holland" rel=""> semifinals</a><span> and</span><a href="https://www.youtube.com/watch?v=7iG_0V9dGRo&amp;ab_channel=KentuckyDebate" rel=""> finals</a><span> from 2015-2023. I couldn’t calculate the rate for Parliamentary debate because it has fewer recorded rounds online than the other formats, but several of the Tournament of Champions</span><a href="https://www.youtube.com/watch?v=udhCzs-G1tA&amp;ab_channel=whsdebate" rel=""> </a><span>rounds on YouTube (including the 2018</span><a href="https://www.youtube.com/watch?v=Gr3LwxegCg8&amp;ab_channel=SierraMaciorowski" rel=""> Finals</a><span> and</span><a href="https://www.youtube.com/watch?v=AESxIdkVgTo&amp;ab_channel=NuevaDebate" rel=""> Quarterfinals</a><span> and the 2023 </span><a href="https://www.youtube.com/watch?v=T9bPYX99KaM&amp;ab_channel=NPDLRecordings" rel="">Semifinals</a><span> and </span><a href="https://www.youtube.com/watch?v=wz-8wbPKSSI&amp;ab_channel=NPDLRecordings" rel="">Quarterfinals</a><span>) feature critical arguments.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png" width="1040" height="840" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:840,&quot;width&quot;:1040,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6323d96-a1c3-42ca-a142-88dffc4d7557_1040x840.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Tournament of Champions elimination rounds are an imperfect representation of more local debate leagues, which tend to feature more topic-focused debate and less critical theory. However, they reflect the broader trend in the activity toward kritiks and the enormous success of these arguments. Tournament of Champions debate tournaments have a significant impact on trends in local debate. Thousands of debaters watch these rounds on YouTube and base their strategies on what they see working at the highest levels of the activity. One former Policy debater told me that in her local Salt Lake City league, debaters often ran critical theory arguments because they aspired to be nationally successful and emulate the top competitors.&nbsp;</p><p>After kritiks were introduced as a competitive strategy, debaters became increasingly enthusiastic about them — which is not surprising, given the left-wing skew of young and educated people. After they graduated, many debaters delved even deeper into these critical theories as college students. A lot of these college students remained involved in the debate community as judges and coaches and indicated in their publicly available judging preferences that they like critical theory arguments. As a result, the next generation of debaters familiarized themselves with these theories even more and learned how to advocate for them in order to win rounds. Many debaters, again, found that they liked these arguments. They graduated and became debate judges, and the whole cycle started again.</p><p><span>Below are quotes from written judge preferences from the </span><a href="https://www.tabroom.com/index/tourn/index.mhtml?tourn_id=24340" rel="">2023 Tournament of Champions</a><span> across all four formats, which illustrate the high school debater to critical theory-loving judge pipeline (also note that “K” is an abbreviation for kritik):</span></p><ul><li><p><span>“</span><a href="https://www.tabroom.com/index/paradigm.mhtml?judge_person_id=83696" rel="">Love the K</a><span>, this is where i spent more of the time in my debate and now coaching career, I think I have an understanding of generally every K, in college, I mostly read Afro-Pessimism/Gillespie, but other areas of literature I am familiar with cap, cybernetics, baudrillard, psychoanalysis, Moten/Afro-Optimism, Afro-Futurism, arguments in queer and gender studies, whatever the K is I should have somewhat a basic understanding of it.”</span></p></li><li><p><span>“</span><a href="https://www.thefp.com/p/judges-ruin-high-school-debate-tournaments" rel="">Before anything else</a><span>, including being a debate judge, I am a Marxist-Leninist-Maoist... I cannot check the revolutionary proletarian science at the door when I’m judging... I will no longer evaluate and thus never vote for rightest capitalist-imperialist positions/arguments... Examples of arguments of this nature are as follows: fascism good, capitalism good, imperialist war good, neoliberalism good, defenses of US or otherwise bourgeois nationalism, Zionism or normalizing Israel, colonialism good, US white fascist policing good, etc.”&nbsp;</span></p></li><li><p><span>“...</span><a href="https://www.tabroom.com/index/paradigm.mhtml?judge_person_id=150785" rel="">I’ve almost exclusively read</a><span> variations of Marxism-Leninism-Maoism... I find these arguments to be a valuable and fun tool in debate and am happy to evaluate these debates to the best of my ability.”</span></p></li><li><p><a href="https://www.tabroom.com/index/paradigm.mhtml?judge_person_id=75817" rel="">Kritik vs. kritik debates are</a><span> “currently my favorite type of debate to judge. My literature knowledge is primarily concentrated in Marxism, Maoism, and proletarian feminism, and I have a baseline familiarity with postcolonial theory, queer theory, and feminist standpoint theory, but I'm down to evaluate anything as long as it's explained well.”</span></p></li><li><p><span>“</span><a href="https://www.tabroom.com/index/paradigm.mhtml?judge_person_id=51549" rel="">Ks I have written files on</a><span>/answering/into the lit for - spanos, psycho, cap, communist horizon, security, fem, mao, death cult, berlant, scranton, queerness, set col...”</span></p></li><li><p><span>“</span><a href="https://www.tabroom.com/index/paradigm.mhtml?judge_person_id=237703" rel="">You will not lose my ballot</a><span> just for running a K. Ever.”</span></p></li><li><p><span>“</span><a href="https://www.tabroom.com/index/paradigm.mhtml?judge_person_id=37265" rel="">I am frequently entertained</a><span> and delighted by well-researched critical positions on both the affirmative and negative”</span></p></li><li><p><a href="https://www.tabroom.com/index/paradigm.mhtml?judge_person_id=21367" rel="">Kritiks</a><span> “are my favorite arguments to hear and were the arguments that I read most of my career.”</span></p></li><li><p><span>“</span><a href="https://www.tabroom.com/index/paradigm.mhtml?judge_person_id=97275" rel="">Ks are my favorite</a><span>!”</span></p></li></ul><p><span>And these aren’t </span><a href="https://twitter.com/zackbeauchamp/status/1674411234387537920" rel="">cherry-picked examples</a><span>. I analyzed judge preferences on Tabroom and found that at the 2023 Tournament of Champions, many judges embraced critical theory across formats:&nbsp;</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png" width="1040" height="840" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:840,&quot;width&quot;:1040,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3948ce0e-4184-4328-9d8e-4de97bdef788_1040x840.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Across debate formats, familiarity with critical theory has become essential to high-level success. According to Matthew Adelstein, a former high school Policy debater who lost a round at the Tournament of Champions because his opponents argued a </span><a href="https://acrobat.adobe.com/link/track?uri=urn%3Aaaid%3Ascds%3AUS%3Aaafd2f60-f4f6-471a-9cd6-0e552b70db5e&amp;viewer%21megaVerb=group-discover" rel="">kritik</a><span> based on </span><a href="https://www.thefp.com/p/personal-tweets-lose-high-school-debates" rel="">personal attacks</a><span> against him:</span></p><blockquote><p>Some huge portion of the arguments that are made at the high levels of Policy debate are based on critical theory... to be successful, you have to read a lot of articles, for example, with people arguing that we should decolonize the entire United States and give back all the land to Native people, that the world cannot improve for Black people [Afro-Pessimism], and that capitalism is a terrible system.</p></blockquote><p>Even in Public Forum and Parliamentary debate—which have less critical theory—debaters still have to prepare for kritiks because many judges will vote for them. A Public Forum debater who reached Semifinals at the Tournament of Champions told me: “I had to know critical theory to win... you have to be prepared in case you have to run it or go against it.” I felt the same way competing in Parliamentary debate. </p><p>Kritiks are so persuasive to left-wing judges that debaters can’t succeed in the activity without being great at them. Competitors who don’t want to argue for kritiks themselves still have to learn how to respond to them without contesting their radical premises. For example, many leftist judges will not accept a response to a Marxism kritik that argues that capitalism is good. Instead, debaters have to concede that capitalism is a bad system and make other leftist arguments like, “it’s capitalistic to fail to argue for the topic” and “Marxism isn’t the most effective response to capitalism; instead we need to look to other critical theories” (like Afro-Pessimism or transfeminism). This drives out students who don’t want to learn about critical theory and creates a vicious cycle where the only people left are kritik debaters.</p><p>Furthermore, even though kritiks philosophically attack power structures, in practice they have frequently entrenched inequities in debate. Kritiks are often (although not always) strategically employed by students from big, well-funded debate programs. Their opponents—who often attend schools with fewer coaches and resources—may not be familiar with the dense philosophical arguments. This is especially challenging because kritik teams reject the topic that their opponents are expecting, and surprise them with completely new content that they have not prepared for.</p><p>For hundreds of thousands of high schoolers, debate is their first substantive exposure to politics —to policy ideas, to political theory (critical and otherwise), and to formulating and rebutting the sort of arguments that shape our political system.</p><p><span>It’s an activity that selects for kids who often go on to have important careers in politics. Here’s a </span><a href="https://en.wikipedia.org/wiki/List_of_policy_debaters" rel="">list of politically influential people</a><span> who competed in Policy debate at the high school or college level—Presidents Lyndon Johnson, John F. Kennedy, and Richard Nixon; Speaker of the House Nancy Pelosi, Senator Elizabeth Warren; Supreme Court Justices Samuel Alito and Stephen Breyer; Treasury Secretary Larry Summers, Republican political advisor Karl Rove, and Acting Solicitor General Neal Katyal. Of course, most debaters don’t become famous politicians, but many of them take lower-profile public service jobs, are vocal about politics, and vote consistently.</span></p><p>This is what concerns me so deeply about this seismic shift in the debate landscape—and why I would hate to see the Public Forum and Parliamentary formats follow the trajectory of Policy and Lincoln-Douglas. Kritiks promote a worldview with pernicious implications for American politics among a group of people who are likely to end up in positions to have a serious impact on American politics.</p><p>When debaters reject the topic and advocate for these critical theories, they choose not to engage in pragmatic policy discussions. Instead, they condemn American institutions and society as rotten to the core. They conclude that reform is hopeless and the only solution is to burn it all down. Even if they’re not advocating for kritiks, in order to succeed at the national level, debaters have to learn how to respond critical theory arguments without actually disagreeing with their radical principles.</p><p>High school debate has become an activity that incentivizes students to advocate for nihilist accelerationism in order to win rounds. It’s the type of logic that leads young people to label both parties as equally bad and to disengage from electoral politics. What most normal people think debate is about — advocating either side of a plausible public-policy topic — is no longer the focus. With kritiks taking a larger share, debate is increasingly societally rejectionist. Too often the activity is no longer a forum for true discussion, but a site of radicalization. </p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cyberdecks (2013) (176 pts)]]></title>
            <link>https://blog.rfox.eu/en/Hardware/Cyberdecks.html</link>
            <guid>36920010</guid>
            <pubDate>Sat, 29 Jul 2023 13:17:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks.html">https://blog.rfox.eu/en/Hardware/Cyberdecks.html</a>, See on <a href="https://news.ycombinator.com/item?id=36920010">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p id="1ffaeab6-954e-443c-97c4-45b8b3fa0650"><time>@2016/02/13</time></p>
      <p id="4986a651-2f0d-49d5-8d4d-bd64274bf142">It has been few days, since I created the&nbsp;<a href="https://www.reddit.com/r/cyberDeck/">/r/cyberDeck</a>&nbsp;subreddit. I did so, partly because I was inspired by the&nbsp;<a href="http://www.activewirehead.com/building-a-cyberdeck/">Building a cyberdeck</a>&nbsp;article, but also because of few IRC discussions I participated, and because I think that there is more to this idea, than just nice cyberpunkish look and feel.</p>
      <h2 id="5844b5d7-484e-4001-8da5-75d94e52a03b">What is `deck`?</h2>
      <p id="1d22ecd9-cae9-4922-97dd-302e1bfb7e83">Deck or CyberDeck is this mobile computer first imagined by William Gibson in Neuromancer and later slightly extended and redefined by the&nbsp;<a href="https://en.wikipedia.org/wiki/Shadowrun">Shadowrun</a>&nbsp;as well as other (<a href="https://en.wikipedia.org/wiki/Cyberpunk_2020">Cyberpunk 2020</a>,&nbsp;<a href="https://en.wikipedia.org/wiki/GURPS_Cyberpunk">GURPS Cyberpunk</a>) role-play games, card games (<a href="https://en.wikipedia.org/wiki/Netrunner">Netrunner</a>) and novels.</p>
      <blockquote id="b334ab0f-4b6b-4227-a622-0aad4814ce0f">
        With his deck waiting, back in the loft, an Ono-Sendai Cyberspace 7. They'd left the place littered with the abstract white forms of the foam packing units, with crumpled plastic film and hundreds of tiny foam beads. The Ono-Sendai; next year's most expensive Hosaka computer; a Sony monitor; a dozen disks of corporate-grade ice; a Braun coffeemaker. Armitage had only waited for Case's approval of each piece.— GIBSON, William.&nbsp;Neuromancer. New York: Ace Books, 1984, 271 s. ISBN 0-441-56959-5.
      </blockquote>
      
      <p id="9e5394c7-753f-4dc8-81a2-c5d3c4fd0831">(William Gibson's Neuromancer: the graphic novel volume 1. New York, N.Y.: Epic Comics, 1989, 1 v.. ISBN 0871355744.)</p>
      <blockquote id="93e0bcda-366e-40a8-8606-acb6e08192e0">
        He snugged the surgical steel jack into the socket at his temple and his fingers flew across the keyboard of his Fuchi cyberdeck, launching him into the Matrix. His vision shifted to that dazzling electronic world of analog space where cybernetic functions took on an almost palpable reality. He ran the electron paths of cyberspace up the satellite link and down again into the Seattle Regional Telecommunications Grid. Within seconds, he was well on his way to the rendezvous with his companions inside the Renraku arcology.
      </blockquote>
      <p id="e3a3c039-d501-4ef4-968b-6e89e92e0049">— CHARRETTE, Robert N.&nbsp;Never deal with a dragon. New York, N.Y., U.S.A.: Roc, 1990, 377 p. ISBN 0451450787.</p>
      <figure id="6836ee5c-cd8b-4b36-90f3-d9bfe9f5acf0">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_2.png" title="Untitled_2.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_2_thumb.jpg"></a>
        <figcaption>
          (<a href="http://www.juangimenez.com/">Juan Gimenez</a>)
        </figcaption>
      </figure>
      <div id="ef38e132-aa3e-443e-966d-44a56823a376">
          <figure id="5b81cda8-5ec9-4955-bbb0-b99dcbb57bc5">
            <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_5.png" title="Untitled_5.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_5.png"></a>
          </figure>
          <figure id="45dfd473-8fa9-405d-9023-a2c45471f47f">
            <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_6.png" title="Untitled_6.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_6.png"></a>
          </figure>
          <p id="a234a909-ea76-4991-a4d4-079f51ebb47e"><em>(Various internet sources, mostly tumblr / pinterest.)</em></p>
          
        </div>
      <p id="0eeb0135-cb2c-4d6b-97a6-73777790a374">Although both in Neuromancer and Shadowrun novels (<em>Never Deal with a Dragon</em> for example) is deck equipped with neural interface, it is not uncommon that it is depicted with builtin keyboard.</p>
      
      
      
      
      
      <figure id="4db6fbc2-02a8-47d8-8bdc-2a2d493cd642">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_12.png" title="Untitled_12.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_12_thumb.jpg"></a>
      </figure>
      <p id="a6ec7362-2d1f-4283-a328-abf43b772255"><em>(Vairous internet sources, mostly tumblr / pinterest.)</em></p>
      <blockquote id="cef669a6-6cc4-4e17-b1e0-b56b62087f89">
        Sam slid back the cover panel and pulled out the telecom connector. With a quick switch of plugs, the Elf's cyber-deck took the place of Castillano's computer. He reached for the datacord that would connect his socket with the deck. He almost changed his mind, but found courage when he remembered the innocents in the arcology who would suffer if no one tried to help. He slipped the plug in, steeling himself against the expected pain.<p>
        
        It came, flashing through his brain faster than before and leaving a distant malaise in its wake. Sam focused his mind on the task at hand. Turning a blind eye to the gleaming spires and pulsing data paths that surrounded him in cyber-space, he charged forward to the massive Renraku construct. Using his company passwords, he opened a portal into the main database.</p><p>
        
        Glittering rows of stars lay in serried ranks and columns all around him. Each point of light was a datafile, its tint reflecting the filing category. Sam fed the cyberdeck the key words and executed the search function. His point of view shifted with dazzling speed along the rows. He paused briefly at each file suggested by the deck, discarding useless information as he searched.</p><p>
        
        In what seemed like only a few minutes, he found it. He copied the file and fled back to where he had entered the Matrix.</p><p>
        
        "There is a counteragent," he announced to the circle of concerned faces as he pulled the data cord from his temple.
      </p></blockquote>
      <p id="cbf9dff7-f879-44e8-b374-77099606631a">— CHARRETTE, Robert N. Never deal with a dragon. New York, N.Y., U.S.A.: Roc, 1990, 377 p. ISBN 0451450787.</p>
      <h2 id="ffb5df27-6049-4594-9e8b-79a111cf93ec">Inspiration</h2>
      <p id="f1d3ff9e-da95-4699-884b-e0bdf136b180">The obvious inspiration for the whole cyberdeck thing was the 8bit home computers back in the era:</p>
      <figure id="38b27467-e4c7-4409-be51-6e95d5f563ad">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_13.png" title="Untitled_13.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_13.png"></a>
        <figcaption>
          (Amstrad CPC 464 by <a href="https://denema.deviantart.com/art/Amstrad-CPC-464-189255658">DeNeMa</a>. Only thing it is missing is neural interface ;)
        </figcaption>
      </figure>
      
      
      <p id="01221c14-56e5-4974-b9f2-70f4895c7604">Imagine yourself passing computer store in 80's and see in the shop windows those beautiful computers. Almost no one knows what to do with them, but they are cool, flashy, with efects never seen before. Talking heads in TV talk about Hackers and information superhighways, and everyone is curious and anything seems possible. It really makes your fantasy going.</p>
      
      <figure id="1271896a-7eaa-4f09-b4b2-75f94908b3ca">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_20.png" title="Untitled_20.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_20.png"></a>
        <figcaption>
          (Source: <a href="http://www.plaidstallions.com/toystores.html">Vintage toy stores</a>.)
        </figcaption>
      </figure>
      <p id="2101b37f-3644-4b2e-91c3-ff55f3b8cbf9">It's not that hard to imagine, that this where the <em>deckers</em> (cyberpunk hackers) and <em>netrunners</em> holding the deck, flying in 3D space and fighting computer programs, came from.</p>
      
      <p id="864948ad-8e48-47a7-8f8c-ccf497eee808">Today, still a lot of people is still attracted to decks because of their cool look. With the advent of small one-board computers like Raspberry PI, you can see attempts and discussions about building the decks:</p>
      <ul id="04995f9b-f2c3-4222-b248-e82c62b86410">
        <li>
          <a href="http://www.cyberpunkforums.com/viewtopic.php?id=1766">Making a cyberspace deck</a>
        </li>
      </ul>
      <ul id="e3462930-4582-4fff-83e1-0e675a751064">
        <li>
          <a href="https://www.reddit.com/r/Cyberpunk/comments/2af5bm/revisting_an_old_idea_building_a_classic/">Revisting an old idea - building a classic cyberdeck using current tech</a>
        </li>
      </ul>
      <ul id="c70fbbf0-f22a-41b6-a914-702d968428a3">
        <li>
          <a href="https://www.reddit.com/r/Cyberpunk/comments/212tko/finally_peicing_together_my_pi_cyberdeck_work_in/">Finally peicing together my Pi Cyberdeck (Work in Progress)</a>
        </li>
      </ul>
      <ul id="3809497e-e26e-42d5-9681-c364a60aaa8b">
        <li>
          <a href="http://n-o-d-e.net/post/130139019901/how-to-create-a-gibsonshadowrun-inspired">How to create a gibson/shadowrun inspired cyberdeck</a>
        </li>
      </ul>
      
      <h2 id="b69ac677-55fd-4d6c-9e37-58dc7c1f53f1">Why the deck?</h2>
      <p id="b15cdd09-1af4-4c7b-9b22-6c1cbf643fe2">So, why would anyone want to use deck and not a notebook?</p>
      <p id="ecb927bf-fed0-4bc4-87ea-3794847d4a29">The idea of usefulness of the deck came to me from the opposite direction, than to most of the people I guess;</p>
      <p id="a38e5309-b12a-41de-9d75-0e0b5f3082c3">I was thinking a lot about what does being „digital nomad“ mean and what would be required to be truly independent, but not to give up comfort of having two displays, one of which is big 27" LCD. I work as a programmer (did you know, that there is <a href="https://www.reddit.com/r/HMDprogramming">/r/HMDprogramming</a>? :)), and big monitor directly contributes to my productivity. I really <strong>need</strong> a lot of space for editor, terminals and other stuff I deal with.</p>
      <p id="c6182706-61d3-488c-9c72-570a4599a8d5">Consider following example:</p>
      <figure id="9b6849b6-7c14-43fe-b508-8b5bdface196">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_27.png" title="Untitled_27.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_27_thumb.jpg"></a>
      </figure>
      <p id="c40657a4-1f1e-4c44-b1ee-734734e1ab2b">And that's just one of 16 virtual desktops I use, others filled with documentation, server connections, database consoles and similar stuff. If you try to cram all that stuff on notebook screen, it just isn't right and context switching can get annoying really quickly:</p>
      <figure id="4bb09791-07ff-4903-96b8-60f550ccc50c">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_28.png" title="Untitled_28.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_28_thumb.jpg"></a>
      </figure>
      <p id="95faeaba-1bd6-4942-b742-16a7e15af095">So I was thinking; Would it be possible to have all the comfort of big screen and still live like a nomad, always on the road? Pretty soon, it was obvious, that you would have to have either big caravan (or maybe a camel with LCD holder :P), or HMD display.</p>
      
      
      <p id="42a34008-646d-41ce-8e7c-7366f23166f3">(This year should be good year for Head Mounted Displays. From left to right: <em>HTC Vive</em>, <em>Oculus Rift</em>, <em>Sony project Morpheus</em>, <em>Razer OSVR</em>, <em>Rapture HMD</em> and <em>Avegant Glyph</em>.)</p>
      <p id="ea657bae-2a4f-4cf2-81ee-e8e53b6df935">But most of the notebooks will have problems to handle the HMD, because of high requirements on GPU, which also means high power consumption (that is also true for decks, but you are not limited by screen size and the size limits for notebooks). Also, the idea of having the display and the HMD at the same time is just pointless. You won't see it with HMD on, and it would just consume power for no reason. That's how the idea of decks came into my mind.</p>
      <p id="6344f09d-f47b-4dad-8b09-4aca609e3c5b">I think, that in the near future, there is relatively big fraction of computer market share for the decks, because HMD's will be more and more common, but I don't think, that we will see them often sooner than 10 years from now.</p>
      <p id="e0be89f2-b80d-471a-82cf-a3bce80ffa7f">EDIT: To get the idea of VR environment, look at this video;</p>
      <figure id="acb7ff57-3d73-4b72-904a-284b9abdd6d5">
        <p>
          <iframe width="100%" height="50%" frameborder="0" src="https://www.youtube.com/embed/bjE6qXd6Itw" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        </p>
      </figure>
      <h2 id="3e2dae46-cd79-475b-975f-b6e9b63ff3b0">The Deck I would like to build</h2>
      <p id="8df43fba-99ec-424f-b538-a6ecaf1302df">Given unlimited budget and access to good workshop, I would build highly customized <strong>workstation</strong>, with highly customized software.</p>
      <p id="0565dd38-34b0-4168-a2ff-930abe04efc8">There is this piece of email conversation between me and <a href="https://www.abclinuxu.cz/lide/pavel.krivanek">Pavel Křivánek</a>, which I can't forget. Loosely translated:</p>
      <blockquote id="48acd99b-ae74-4f4a-a253-d3865fb9b114">
        <em>&gt; ... I think, that I will try to write simple Smalltalk interpret one day. Thats best for learning new language.</em><p>
        
        If I can advise you, try the Self interpret. The nuances of how brilliantly it works with lexical spaces, activation objects and so on are just breathtaking.</p><p>
        
        <em>&gt; Lately, I was also captivated by Squeak, with which I<br>
        was toying a little and I think, that there are really interesting<br>
        things, which I still need to explore. It seems to me, that there is<br>
        strong emphasis on the man-software synergy (ala Engelbart), at the<br>
        expense of standard sw development, which I find interesting. Maybe I<br>
        will have to look into Self, that prototype-based development looks like<br>
        it is better for this kind of applications.</em></p><p>
        
        For me, the Self is the matter of heart. Especially how it<br>
        solved a lot of Smalltalk's problems by simplifying it, is a really<br>
        special case in the world of programming languages.</p><p>
        
        On the other hand, Smalltalk now balances better the academical flamboyance with practicality. Even the Self's authors acknowledges, that there is sometimes problem to keep situational awareness, which in Smalltalk is not that big problem, thanks to the class system. This also applies to easier creation of support tools. But the ability to work in virtual 3D space full of flying outliners, that would be [untranslatable inflexion of word `programming` meaning something like `happier/better/more-enjoyable programming`].
      </p></blockquote>
      <figure id="f88edddb-3ee7-489d-ba78-d57f9906d1f7">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_35.png" title="Untitled_35.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_35.png"></a>
        <figcaption>
          <em>(Self really doesn't look like typical IDE. There is never enough space for outliners.)</em>
        </figcaption>
      </figure>
      <p id="2014c205-fe1e-45ac-991d-f6b1037716c7">Self is really interesting language, somehow forgotten gem, which almost no one use, because it works differently than most of the current-date programming languages. Whole IDE is really strongly space and visually oriented. After I've played with it a little, I must say, that it (or Smalltalk) would do a really nice desktop environment for 3D system.</p>
      
      
      <figure id="002c6f7d-b175-4bfa-a4f6-a121a2fa96a8">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_38.png" title="Untitled_38.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_38.png"></a>
        <figcaption>
          <em>(Source:</em> <a href="https://en.wikipedia.org/wiki/Ghost_in_the_Shell:_Arise"><em>Ghost In The Shell: Arise</em></a><em>.)</em>
        </figcaption>
      </figure>
      <p id="158db227-fc50-4ff0-8ad3-67a0f02d1338">Of course, this probably wouldn't be user-friendly and thus usable for most of the people. But my idea of the deck was never meant to be. Such project would have to have custom, DIY hardware, only for real enthusiast. It would be much more interesting, if the software would be also highly customized programmer-only thing, completely ignoring the normal users and their principles of operation. As the <a href="https://www.abclinuxu.cz/images/screenshots/7/4/217947-cyberdecs-7902521666826599726.jpg">image</a> from the Neuromancer graphic novel says: <em>"The meat stayed home, strapped to a</em> <em><strong>custom</strong></em> <em>deck"</em>.</p>
      <p id="6ce4ea64-7242-40aa-8cd1-449b526c165a">Once I found this think-path, I couldn't just stop there. When you realize, that you don't need to limit yourself with standard notebook parameters, you may actually imagine completely new device with more completely different features, which makes sense only with the concept of decks. Pretty quickly, I've got something qualitatively different from standard consumer notebooks.</p>
      <figure id="8b115aac-6d5b-4b7f-a593-c59c28f284c9">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_39.png" title="Untitled_39.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_39.png"></a>
        <figcaption>
          <em>(</em><a href="https://www.tinkercad.com/things/9qx3c3ij28S"><em>3D model</em></a> <em>I've created to illustrate this article. Feel free to use and remix it.)</em>
        </figcaption>
      </figure>
      <p id="272b745a-2002-4371-9e59-f882da53a634">For example - typical notebook have one shitty webcam used for video calls. With the decks, you may actually want something like four or six hi-res webcams, to provide you with situational awareness, when you have the HMD on. In the virtual reality, imagine this as big sphere around you. There are some floating windows, between you and the sphere, and at sphere itself, there may be output from the cams showing your surroundings. The cameras could also in theory be used to track you and your hands and map you into the 3D environment, leapmotion style.</p>
      <figure id="f76e19a2-4ce1-4123-b77c-1fa0f2e0de19">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_40.png" title="Untitled_40.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_40_thumb.jpg"></a>
      </figure>
      <p id="a45f0f28-06fd-401a-a39e-7f4433d8e7cd">Keyboard could be detachable and the deck could track its position and position of your HMD, using the same <a href="https://www.roadtovr.com/reverse-engineering-oculus-rift-dk2-positional-tracking-camera-linux-sdk/">LED trick</a> Oculus uses, so it could render its virtual form into the 3D environment.</p>
      <p id="80efd046-0b05-451c-99b2-c927ba371b9d">There could be built-in Leapmotion / Kinect -like sensors, which would sense hand motions, so no gloves would be required. It would be also nice to have small e-ink display, as the system console, for debugging and system-info purposes.</p>
      <h3 id="864c58fc-90b8-492c-92ed-6d3498142348">Crazy stuff</h3>
      <p id="4ebe4eb9-58c8-4859-89e2-7a4b631844b2">Instead of cheap WiFi card, there could be <a href="https://en.wikipedia.org/wiki/Universal_Software_Radio_Peripheral">USRP</a> (really good Software Defined Radio) card, combined with FPGA, so you could actually take the deck into the field and let it be useful in hacking / tracking / capturing of the signals. Of course, it could also emulate the WiFi / Bluetooth / Zigbee device, with right software.</p>
      <p id="d9696b0d-892f-4a63-bf62-4b4e7ecd8e9b">Since this wouldn't be the standard consumer hardware built for multimedia / gaming, it would be possible to use some really alternative computing platform, like this sweet 18 core low-power Parallella computer board.</p>
      
      <figure id="587e3296-a474-4d61-9673-ecdc45c0983e">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_41.png" title="Untitled_41.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_41.png"></a>
        <figcaption>
          (<a href="https://www.parallella.org/">The Parallella Board</a> - 18-core credit card sized computer.)
        </figcaption>
      </figure>
      <p id="75707c9f-c3c7-402c-b9d3-9e18b820901d">Only thing, that is really mandatory is high-end GPU, possibly mobile. There is no way around it, if you would want to have enough processing power to support smooth 3D environment in the HMD display. This is one of the reasons, why we don't see many of the decks today, and in the near future. GPU is simply too much greedy.</p>
      <figure id="fbf54760-120c-4fae-96c0-7a36d72bf8b3">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_42.png" title="Untitled_42.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_42_thumb.jpg"></a>
        <figcaption>
          (<a href="https://www.reddit.com/r/oculus/comments/2lsohd/just_finished_building_my_portable_pelerift/">Reddit: Portable Pele-Rift</a>. This is how the today consumer hardware deck looks like when you put the high-end GPU into it.)
        </figcaption>
      </figure>
      <p id="6541bc9f-8a66-4b4f-9520-c89b8b926734">So if I use the 3D model I've created, it would look somehow like this:</p>
      <figure id="98c377bb-d71a-4871-93f0-ef03e31ad673">
        <a href="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_43.png" title="Untitled_43.png"><img src="https://blog.rfox.eu/en/Hardware/Cyberdecks/Untitled_43_thumb.jpg"></a>
      </figure>
      <h2 id="a1c5f474-090a-4534-92f1-c0e8e073b306">Thoughts?</h2>
      <p id="628d6b25-e3a8-442f-b0f7-5e3aa3a93647">So, what do you think? Does the idea of the decks have any chance to live? Would you want one? For aesthetic / enthusiastic / professional reasons? Do you think it could make actually useful workstation?</p>
      <p id="ced8070b-3d19-4811-ba40-6bf90c73c7ff">Let me know in the <a href="https://www.reddit.com/r/cyberDeck/">/r/cyberDeck</a>. Don't be shy, I am really curious to hear what you think, even if you find this article years from now!</p>
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Happened to Vivaldi Social? (269 pts)]]></title>
            <link>https://thomasp.vivaldi.net/2023/07/28/what-happened-to-vivaldi-social/</link>
            <guid>36919659</guid>
            <pubDate>Sat, 29 Jul 2023 12:34:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thomasp.vivaldi.net/2023/07/28/what-happened-to-vivaldi-social/">https://thomasp.vivaldi.net/2023/07/28/what-happened-to-vivaldi-social/</a>, See on <a href="https://news.ycombinator.com/item?id=36919659">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[US Spies Are Lobbying Congress to Save a Phone Surveillance ‘Loophole’ (152 pts)]]></title>
            <link>https://www.wired.com/story/nsa-ndaa-lobbying-privacy-loophole/</link>
            <guid>36919225</guid>
            <pubDate>Sat, 29 Jul 2023 11:38:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/nsa-ndaa-lobbying-privacy-loophole/">https://www.wired.com/story/nsa-ndaa-lobbying-privacy-loophole/</a>, See on <a href="https://news.ycombinator.com/item?id=36919225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>An effort by</span> United States lawmakers to prevent government agencies from domestically tracking citizens without a search warrant is facing opposition internally from one of its largest intelligence services.</p><p>Republican and Democratic aides familiar with ongoing defense-spending negotiations in Congress say officials at the National Security Agency (NSA) have approached lawmakers charged with its oversight about opposing an amendment that would prevent it from paying companies for location data instead of obtaining a warrant in court.</p><p>Introduced by US representatives Warren Davidson and Sara Jacobs, the amendment, <a href="https://www.wired.com/story/ndaa-2023-davidson-jacobs-fourth-amendment/">first reported by WIRED</a>, would prohibit US military agencies from “purchasing data that would otherwise require a warrant, court order, or subpoena” to obtain. The ban would cover more than half of the US intelligence community, including the NSA, the Defense Intelligence Agency, and the newly formed National Space Intelligence Center, among others.</p><p>The House approved the amendment in a floor vote over a week ago during its annual consideration of the National Defense Authorization Act, a “must-pass” bill outlining how the Pentagon will spend next year’s $886 billion budget. Negotiations over which policies will be included in the Senate’s version of the bill are ongoing.</p><div><p>In a separate but related push last week, members of the House Judiciary Committee voted unanimously <a href="https://www.wired.com/story/fourth-amendment-is-not-for-sale-act-2023/">to advance legislation</a> that would extend similar restrictions against the purchase of Americans’ data across all sectors of government, including state and local law enforcement. Known as the “Fourth Amendment Is Not For Sale Act,” the bill will <a data-offer-url="https://subscriber.politicopro.com/article/2023/07/momentum-grows-for-bill-banning-data-sales-to-government-agencies-00108470" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://subscriber.politicopro.com/article/2023/07/momentum-grows-for-bill-banning-data-sales-to-government-agencies-00108470&quot;}" href="https://subscriber.politicopro.com/article/2023/07/momentum-grows-for-bill-banning-data-sales-to-government-agencies-00108470" rel="nofollow noopener" target="_blank">soon be reintroduced in the Senate</a> as well by one of its original 2021 authors, Ron Wyden, the senator’s office confirmed.</p><p>“Americans of all political stripes know their Constitutional rights shouldn’t disappear in the digital age," Wyden says, adding that there is a “deep well of support” for enshrining protections against commercial data grabs by the government “into black-letter law.”&nbsp;</p><p>The extent to which the NSA in particular uses data brokers to obtain location and web-browsing data is unclear, though the agency has previously acknowledged using data from “commercial” sources in connection with cyber defense. Regardless, the NSA’s lawyers have authored extensive guidelines for acquiring commercially available data, particularly when it belongs to US companies or individuals. Some of the rules prescribed by the agency’s lawyers remain classified.</p></div><p>The NSA did not respond to multiple requests for comment.</p><p>A <a href="https://www.wired.com/story/odni-commercially-available-information-report/">government report</a> declassified by the Office of the Director of National Intelligence last month revealed that US intelligence agencies were avoiding judicial review by purchasing a “large amount” of “sensitive and intimate information” about Americans, including data that can be used to trace people’s whereabouts over extended periods of time. The sensitivity of the data is such that “in the wrong hands,” the report says, it could be used to “facilitate blackmail,” among other undesirable outcomes. The report also acknowledges that some of the data being procured is protected under the US Constitution’s Fourth Amendment, meaning the courts have ruled that government should be required to convince a judge the data is linked to an actual crime.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The US Supreme Court has previously ordered the government to obtain search warrants before seeking information that may “chronicle a person’s past movements through the record of his cell phone signals.” In the <a href="https://www.wired.com/story/carpenter-v-united-states-supreme-court-digital-privacy/">landmark <em>Carpenter v. United States</em> decision</a>, the court found that advancements in wireless technology had effectively outpaced people’s ability to reasonably appreciate the extent to which their private lives are exposed.</p><p>A prior ruling had held that Americans could not reasonably expect privacy in all cases while also voluntarily providing companies with stores of information about themselves. But in 2018 the court refused to extend that thinking to what it called a “new phenomenon”: wireless data that may be “effortlessly compiled” and the emergence of technologies capable of granting the government what it called “near perfect surveillance.” Because this historical data can effectively be used to “travel back in time to retrace a person’s whereabouts,” the court said, it raises “even greater privacy concerns” than devices that can merely pinpoint a person’s location in real time.</p><p>Crucially, the court also held that merely agreeing to let data be used “for commercial purposes” does not automatically abrogate people’s “anticipation of privacy” for their physical location. Rather than apply this view to location data universally, however, the government has allowed defense and intelligence agencies to assume a contradictory view, as their activities were not a factor in <em>Carpenter</em>’s law enforcement-focused ruling.</p><p>A growing number of American lawmakers have <a href="https://www.wired.com/story/fourth-amendment-is-not-for-sale-act-2023/">argued in recent weeks</a> that the US intelligence community is itself more or less facilitating the erosion of that privacy expectation—that location data is protected from unreasonable government intrusion—mainly by ensuring it isn’t.</p><p>Andy Biggs, who chairs a subcommittee on federal government surveillance in the House of Representatives, says the federal government has “inappropriately collected and used Americans’ private information” for years. A whole range of agencies, including the Federal Bureau of Investigation and the Drug Enforcement Agency, have been exploiting “legal loopholes,” he says, to avoid oversight while amassing “endless amounts of data.”</p><p>A senior advisory group to the director of national intelligence, Avril Haines, the government’s top spy, stated in the report declassified last month that intelligence agencies were continuing to consider information “nonsensitive” merely because it had been commercially obtained. This outlook ignores “profound changes in the scope and sensitivity” of such information, the advisors warned, saying technological advancements had “undermined the historical policy rationale” for arguing that information that is bought may be freely used “without significantly affecting the privacy and civil liberties of US persons.”</p><p>Haines’ office did not respond to multiple requests for comment. In a statement last month, the director said she was working to implement key recommendations from her advisors and believed that Americans should be given “some sense” of the policies affecting the collection of their personal data. Much of the framework for dealing with commercial purchases by the intelligence community would be disclosed publicly when it is eventually finalized, she said.</p><p>The practice of paying businesses to spy on US citizens is one of several concerns lawmakers say they’ll be exploring this fall during what’s slated to be a long and heated debate over one of <a href="https://www.wired.com/story/fbi-section-702/">the government’s most powerful surveillance tools</a>: Section 702 of the Foreign Intelligence Surveillance Act.</p><p>The Mozilla Foundation joined the chorus of civil society groups calling for reforms of the 702 program today, saying FISA’s current process is “overbroad” and “restricted only by weak legislation and executive orders that, experience has shown, do not create real accountability.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Online activities to be made impossible by the UK Online Safety Bill (167 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36919175</link>
            <guid>36919175</guid>
            <pubDate>Sat, 29 Jul 2023 11:31:22 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36919175">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36922562"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36922562" href="https://news.ycombinator.com/vote?id=36922562&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>While I'm not a supporter of the Online Safety Bill, I do find that the opprobrium that it has inspired is consistently hyperbolic.<p>The Online Safety Bill bans <i>nothing</i>. What it does do is create a regulatory framework for the Office for Communications (aka Ofcom, HM Government's regulator for the communications industry) to operate and enforce. The regulatory framework is based around risk assessments, and sets out specific risks (of the sort that most people would agree need to be minimised). Ofcom is empowered to decide that internet services are within scope, and then demand to see policies relating to ameliorating those risks within a service.</p><p>The regulatory framework does specify that end-to-end encryption is incompatible with the reduction of these risks. However, we do have to play the ball and not the person of unspecified gender here.</p><p>We are, after all, talking about the UK here. The phenomenon of 'those whom the law protects but does not bind, and those whom the law binds but does not protect' is still very much in effect here. This legislation has also been proposed by a government that responds to increasing crime (caused by insufficient police officers) by giving those police more powers (that they don't have the resources to exercise).</p><p>Ofcom has wide-ranging responsibilities: it regulates the press and broadcast media, it allocates EM spectrum bands, it regulates the telephone network, and regulates the Royal Mail. Like all organisations in the Home Civil Service, it is staffed by a core of 'generalists' with humanities degrees, assisted by a small cadre of 'specialists', who are put back in their cupboard once they've given their opinion.</p><p>What this all means is that the bill will only have the effects that the government has said it will in its press releases. Ofcom will go after the big social media companies and the likes of 4chan. Your Mastodon server or your Gitea server or your corner of the Tildeverse or whatever will never appear on their radar. Ofcom won't have the budget or the resources to go after individuals or small communities; they won't even know you exist.</p><p>In the UK, an energy bill or council tax bill is an important identity document, necessary for opening a bank account, because any time someone starts talking about identity cards, the readers of the <i>Daily Telegraph</i> start muttering about the Gestapo. Ofcom isn't going to audit your homelab for online harms.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36922689"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36922689" href="https://news.ycombinator.com/vote?id=36922689&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>It sounds like you're chiding others for complaining about the bill, and excusing this because the bill "only" <i>enables</i> mass surveillance and anti-privacy measures, rather than stipulating them. Might be I'm reading you wrong on this, but that's what I'm taking away.<p>&gt; What this all means is that the bill will only have the effects that the government has said it will in its press releases.</p><p>I don't really believe that for a second. Whenever sweeping powers are introduced, there is scope creep. Just look at how anti-terrorism laws have been abused. And it seems Apple and others don't believe it either.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36922765"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36922765" href="https://news.ycombinator.com/vote?id=36922765&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>I am chiding people, not for complaining, but for misunderstanding the surrounding context. Legislation is only as powerful as those enforcing it.<p>&gt; And it seems Apple and others don't believe it either.</p><p>Yes, Apple, Google, Microsoft, Signal, Meta, and all the other big players else who has turned up to the committee hearings will have their activities curtailed. Individuals running Minecraft and Mastodon servers for their friends needn't worry.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36922780"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36922780" href="https://news.ycombinator.com/vote?id=36922780&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>&gt; Individuals running Minecraft and Mastodon servers for their friends needn't worry.<p>Because they're too insignificant to matter, or because they won't be covered by the law? It sounds like the former, but you seem to imply the latter.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="36922754"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36922754" href="https://news.ycombinator.com/vote?id=36922754&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><br><div>
                  <p><span>I don't agree with this comment, but I upvoted it because it brings a different perspective to HN, and therefore, improves the quality of discussion.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36921708"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36921708" href="https://news.ycombinator.com/vote?id=36921708&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><br><div>
                  <p><span>Source code hosting; Gitea or Gitlab or Gogs etc all have UGC. The interesting part to me is I'm not sure how this helps the UK citizens who are using the internet be any safer, all you'd have to do is access the rest of the internet to circumvent this.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36920287"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36920287" href="https://news.ycombinator.com/vote?id=36920287&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>Interestingly, if you run a BBS through the telephone system, you'll be immune, as the bill specifies "the Internet", but:<p>1. I think the lines have jitter now they're over IP anyhow.</p><p>2. Bit niche...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36921703"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36921703" href="https://news.ycombinator.com/vote?id=36921703&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>Has anyone designed a modem to work over VOIP? It would have to cope with audio compression, jitter, dropouts, and so on.<p>On the other hand, it could assume much less noise than an analogue phone line.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36922414"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36922414" href="https://news.ycombinator.com/vote?id=36922414&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><br><div>
                  <p><span>Yes, making fax machines work over VoIP was important in the early days. The G.711 codec was designed to support endpoints that needed a full 64kbps datapath.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36921988"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36921988" href="https://news.ycombinator.com/vote?id=36921988&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>Sure, it could be made to work, and it might even be able to keep up with your reading speed.<p>VoIP codecs are heavily optimized for human voice and have just a few kilobits of bandwidth. The analog phone network, before it went entirely digital, had a much higher bandwidth which is why modems were able to reach 56kbps. Phone calls in the 1980s were often crystal clear, almost on par with a broadcast radio station. Cell phones today along with VoIP are a muddy unintelligible mess in comparison.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36922153"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36922153" href="https://news.ycombinator.com/vote?id=36922153&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>the quality of analog phone calls hung on the quality of the equipment, mostly the cables (and distance), whereas the quality of voip (or volte for that matter) depends mostly on the quality of the codec used.<p>ime both can be shitty or "crystal clear" but i totally agree that having a cell phone call today, with worse quality than an cross-continent analog connection in the 80s is very depressing
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36922066"><td></td></tr>
                  <tr id="36921864"><td></td></tr>
                  <tr id="36922688"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36922688" href="https://news.ycombinator.com/vote?id=36922688&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>&gt; you'll be immune, as the bill specifies "the Internet"<p>UK POTS is switching over to an IP based network, so does the bill specify the internet or ip based network?</p><p>If you submit a GDPR to your local constabulary, every phone call in the UK has a unique call identify much like a GUID now. Its quite insightful seeing just how much info is given out regarding the phone system capabilities via GDPR requests.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36920895"><td></td></tr>
                <tr id="36921090"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36921090" href="https://news.ycombinator.com/vote?id=36921090&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>I assume:<p>1) we're talking about the legal situation, not what would be physically possible;</p><p>2) that it applies to citizens, not server location (otherwise affected companies wouldn't be saying they'll exit the UK market, they'd just serve UK users from a further away but friendlier 'edge')
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36921099"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36921099" href="https://news.ycombinator.com/vote?id=36921099&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><br><div>
                  <p><span>I don't think it would be legal either since VPNs require encryption, too. Maybe some kind of proxy?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36921135"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36921135" href="https://news.ycombinator.com/vote?id=36921135&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><br><div>
                  <p><span>It's important to distinguish this from the encryption provisions of the bill. This part of the bill is distinct and means that social sites will need to verify ages of users and fill in lots of forms while they're at it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36921198"><td></td></tr>
                        <tr id="36920971"><td></td></tr>
                  <tr id="36920542"><td></td></tr>
                <tr id="36920962"><td></td></tr>
                  <tr id="36921362"><td></td></tr>
                <tr id="36922640"><td></td></tr>
                <tr id="36922779"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36922779" href="https://news.ycombinator.com/vote?id=36922779&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><br><div>
                  <p><span>Is a comment section user-to-user, though?  On its face, it seems more user-to-public, since it's not a private user-to-user transmission.   If blog comment sections are exempt, it would seem Reddit is exempt too.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36921082"><td></td></tr>
            <tr id="36919967"><td></td></tr>
                <tr id="36920306"><td></td></tr>
                  <tr id="36919185"><td></td></tr>
                <tr id="36919196"><td></td></tr>
                  <tr id="36920904"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36920904" href="https://news.ycombinator.com/vote?id=36920904&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>My understanding is that the OSB creates a responsibility for Ofcom to create a regulatory system for discouraging online harms.<p>Has Ofcom actually said what these regulations will be?</p><p>Is there any reason to expect them to be as dystopian as you predict? The interim codes of practice that the government published don't even apply at all to individuals. They also acknowledge that smaller companies should not be expected to be subject to the same level of regulation as large companies.</p><p>My experience with online activists' predictions of imminent dystopia is that they generally turn out to be extremely overblown. Hopefully that's true this time as well.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36920954"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36920954" href="https://news.ycombinator.com/vote?id=36920954&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>The bill itself doesn't appear to exempt anyone. From Taylor Wessing:<p>"[following amendments] The OSB continues to apply to any service that enables content generated, uploaded or shared by one user to be encountered by another user (user-to-user services) or that allows users to search more than one website or database (search services)."
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36921346"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36921346" href="https://news.ycombinator.com/vote?id=36921346&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>It actual exempts several categories (email servers etc).<p>My point is that there is no reason to think the regulations that eventually come into force for services which aren't exempted will be as ridiculous as you suggest.</p><p>I agree that the OSB gives Ofcom the power to regulate Minecraft servers, but those regulations must be reasonable and proportionate so I don't believe that it will affect a private individual running a private server, as you appeared to suggest.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36921361"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36921361" href="https://news.ycombinator.com/vote?id=36921361&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>Yeah agree about point to point services. Email and SMS and voice.<p>What I don't see is the bit where it says that the audit responsibilities are limited to certain people or companies.</p><p>If you could point to the "reasonable and proportionate" bit in the legislation that would be interesting to check out.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="36920960"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36920960" href="https://news.ycombinator.com/vote?id=36920960&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>&gt; My experience with online activists' predictions of imminent dystopia is that they generally turn out to be extremely overblown.<p>The questions in my opinion then become,</p><p>Is it the law preventing these? Is it some people not yet seeing why they should do it? Are they purposefully not doing it initially to calm people?</p><p>If they don’t want to be able to do it, it could be rephrased.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36922615"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36922615" href="https://news.ycombinator.com/vote?id=36922615&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><br><div>
                  <p><span>If you operate an online service, you don't have any actual obligations under the Online Safety Bill until Ofcom taps you on the shoulder. Considering that Ofcom is also responsible for regulating ISPs, all broadcast media, the EM spectrum, the telephone network and the Royal Mail, I don't think they'll have the resources to go after anyone but the social media giants.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36921254"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36921254" href="https://news.ycombinator.com/vote?id=36921254&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>By way of analogy, the government might make a law that allows, in secondary legislation, the setting of a driving speed limit.<p>These activists are claiming that the government is secretly intending the speed limit to be 1mph. The government are trying to ban driving!! The activists are demanding the <i>enabling legislation</i> be amended so that the speed limit, when set, must be no lower than Xmph.</p><p>But all this does is force all regulation to be done in primary legislation.</p><p>The debate about what the regulations should be is separate to there being a regulatory system at all.</p><p>It is right that the primary legislation just lays out in broad terms that the regulations must be "reasonable" and "proportionate" etc because it's only real purpose is to allow those regulations to be challenged in court in future.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36921705"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36921705" href="https://news.ycombinator.com/vote?id=36921705&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>I'd like to offer a different analogy.<p>Encryption in communication is, to me, akin to the locks on the doors of your residence, on safes or other secure containers. It protects your privacy from prying eyes who want to snoop on what others are doing.</p><p>The activists are claiming that the government wishes to require that all locks be unlockable by one of a range of secret "master" keys, so that they can ensure you're not privately doing anything illegal. Not only that, but the government also wishes to institute a compliance regimen to ensure that all locks are indeed actually openable by these master keys.</p><p>Given the historic behavior of <i>all</i> governments, I don't trust them <i>not</i> to claim the most powerful interpretation of the laws and regulations that are instituted. Maybe not at first, but it <i>will</i> happen, because it has <i>always</i> happened this way.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36922653"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36922653" href="https://news.ycombinator.com/vote?id=36922653&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>What the government actually wants is for service providers (as in, social media companies, not ISPs) to monitor everything happening on their service to make sure that no one is coming to any serious harm. How providers do that is up to them.<p>The government doesn't care about the technology; it's all about corporate processes, and they're going to regulate it one corporation at a time. If you're not important to be asked to appear before a House of Commons Committee, you're not going to come to Ofcom's attention.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36921301"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36921301" href="https://news.ycombinator.com/vote?id=36921301&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>That's not my understanding of the legislation. I see no exemptions  from the child access provisions. The criterion is "any site with a significant number of child users" or where <i>additionally</i> OFCOM decides to intervene, and where significant is not defined.<p>And of course you seem to have to perform an audit to determine how many child users you have in order to be exempted.</p><p>Again, this is my reading of the very complex bill. The article from Taylor Wessing seems to concur though.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36921528"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36921528" href="https://news.ycombinator.com/vote?id=36921528&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><p><span>But all the provisions service providers are expected to implement have a "reasonable" or "proportionate" qualifier, no?<p>And the actual, practical meaning of these responsibilities will be defined in a yet-to-be-published Code of Practice...?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36921611"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36921611" href="https://news.ycombinator.com/vote?id=36921611&amp;how=up&amp;goto=item%3Fid%3D36919175"></a></center>    </td><td><br><div>
                  <p><span>Like unreasonably low speed limits, the purpose of legislation like this isn't to be enforceable against everyone.  It's to be enforceable against <i>anyone</i>.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IRC is the only viable chat protocol (308 pts)]]></title>
            <link>https://koshka.love/babel/irc-forever.html</link>
            <guid>36918655</guid>
            <pubDate>Sat, 29 Jul 2023 10:12:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://koshka.love/babel/irc-forever.html">https://koshka.love/babel/irc-forever.html</a>, See on <a href="https://news.ycombinator.com/item?id=36918655">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2>IRC is the Only Viable Chat Protocol</h2>
<hr>
<p><del>IRC is so wonderful that I am continually finding myself far too distracted by it to actually write this article praising the virtues of IRC. Get on IRC to learn more.</del></p><p>
OK, OK, I'll pull myself away from the terminal and finally finish writing this piece. Although it would be quite poetic if my argument for using IRC was prematurely aborted because I could not pull myself from Irssi/Comic Chat long enough to actually write it, this scenario would be of no benefit to anyone.</p><p>
For those unaware, IRC is a primordial (by Internet standards) yet stalwart chat protocol dating back to 1988. The basic way it works is that people use an IRC client to connect to an IRC daemon running on another computer (an IRC server), where they are able to pick a name and interact with other people over text, either by joining a channel that they are in, or privately messaging them. Although it has severely declined in popularity since its golden age in favour of social media and Discord, I have found myself more attached to the ostensibly dying protocol than ever before lately for a myriad of reasons that I wish to share here.</p><p>
For transparency's sake, I should probably point out immediately that I am a long-time stubborn aficionado of retro technology and culture, both in ways that other people admire, and in ways that make me come off as an eccentric lunatic.</p><p>
Among other things, I still use a flip phone, collect and listen to music CDs, use Office 97 (the very first version I ever owned), listen to music on an MP3 player while on the go, own two CRT televisions along with a VCR and a collection of VHS tapes, collect and read physical books, and own two CRT monitors that I use with my 1999 Compaq gaming computer that still proudly runs Windows 98 and hosts the DOS/Windows 9x games that still make up the bulk of the games that I enjoy.</p><p>
As returning visitors likely know, in the year of our Lord, 2022, I also proudly host my own IRC server, where I spend an extremely unhealthy amount of my time chatting with beloved like-minded people. Using an "archaic" chat platform that dates back to the 80s and that has lost the majority of the userbase it enjoyed during its peak may seem like a form of nostalgia bordering on abject madness when looked at from the outside. Yet, I daresay it is easily one of the most easily defensible and reasonable of the life choices that I have just listed off.</p><h3>Cut the (Dis)Cord</h3><p>
If you look around for a place to chat in real time with other people in this day and age, chances are more than good that you'll quickly be directed to a Discord server. This is an unfortunate state of affairs for many reasons. To put it succintly, Discord is a centralised and proprietary platform that spies on its users in every conceivable way, right down to what logging what programs they have running on their computer and demanding people self-dox by providing their phone number. All of this information is then put to use to allow advertisers to better target Discord's userbase.</p><p>
I am aware that most people have become completely lamentably desensitised to corporate and government surveillance, and may thus not be greatly alarmed by these facts. However, I would assume that most (if not all) of those people would feel quite violated if a salesperson began stalking them in "real life", listening in to every conversation they had with their family/friends from a distance, and then physically approaching on the street and peddling products to them in suspiciously prescient ways.</p><p>
The fundamental fact that Discord users refuse to see is that the platform isn't run on magic dust and fairy incantations, but actual human beings. Using Discord is no different from having a group of strangers sitting in your room with you, noting down every word you say to your friends and everything you run on your computer, and doing the devil knows what with it.</p><p>
Even if you have full-on Stockholm syndrome in regard to advertisers data-mining your life to sell you garbage, who knows where else your data could be going? Considering the horrific epidemic of sexual abuse being abetted and covered up in the workplace, is it really too difficult to imagine malicious actors at Discord (or any other technology company) illegitimately accessing the data of their business' users and using it for stalking or other nefarious purposes?</p><p>
The complete de-centralisation of IRC, unlike Discord, is also well worth expanding on. Since IRC is a standard and not a platform like Discord, anyone with access to reliable hosting and basic computer knowledge can set up their own IRC server. As I joked to a friend recently, I am free to ban anyone from an IRC channel I own, but there is nothing stopping them from then starting #koshkaisafag and regrouping. I can try to get them banned from the server, but there is also nothing stopping them from setting up their own server as irc.koshkaisafag.info and regrouping as a completely sovereign entity that no one can touch any longer.</p><p>
There are certainly a number of monolithic IRC networks, such as Rizon, EFNet, and Libera Chat, that make up the vast bulk of the IRC world, but there are also many thousands of smaller networks dotting the landscape. There are just over 500 networks whose channels are indexed by the IRC search engine <a href="https://netsplit.de/networks/">Netsplit.de</a>, but this engine still only covers fairly large networks, and excludes the great many networks that are either private or very small and obscure. Some of these may be intended entirely for a small group of friends, or may be used by a business to communicate privately.</p><p>
There certainly is no magical check in any IRCd daemon to stop a rogue IRCOp from <a href="https://www.reddit.com/r/discordapp/comments/7arzdn/my_account_has_been_disabled_with_no_reason_given/">banning individuals for no coherent reason</a>, or <a href="https://www.reddit.com/r/privacy/comments/djyy6v/psa_discord_is_deactivating_accounts_without/">blackmailing someone into providing their phone number</a> and/or other personal information in order to stay on a server (although, unlike with Discord, I have never heard of this occurring on IRC before), but the complete de-centralisation of the IRC world means that allowing such abuses of power is quite detrimental to an IRC network. If an IRCOp goes rogue, their reign of terror is unlikely to last for much longer as word goes out and users either flee to another network or set their own up.</p><p>
Perhaps the most dramatic example of this is <a href="https://github.com/siraben/freenode-exodus">the rapid fall of Freenode</a>, which underwent a rapid collapse after a hostile takeover of the network by new management seeking to use it to make a quick buck. In the span of mere months, the most popular IRC network in the world was reduced to a disgraced and nearly moribund shell of its former self, as disgruntled users fled enmasse to the newly established Libera Chat network.</p><p>
I touched on the problems of centralisation and why de-centralisation was a key tenet of the old Internet in my acclaimed article <a href="https://koshka.love/mwwwga.html">Make the Web Great Again</a>, and Discord does not get nearly enough bad press for its role in destroying this aspect of the Internet. Much as the dreaded Reddit has largely paved a fascist monopoly over the niche once occupied by a bounty of independent Web forums, Discord has done the same with the chat world, replacing the sea of independent and free IRC servers with a single corporate walled garden whose owners each user must avoid offending in any way, lest they be entirely cast out of the public square.</p><p>
This problem is so endemic on the modern Internet that not only is there a sea of unintentionally comical Neocities "Web 1.0" websites featuring their owners jabbering about how much they miss the old Internet while inviting people to chat on the webmaster/webmistress' Discord server in the same breath, but even actually respectable people and outfits (who I will not name out of politeness) that have migrated to Discord because all of their misguided friends use it and refuse to budge. Even I, for all of my frenzied rabble-rousing, briefly created a Discord account a few years ago to speak with a friend before quitting the service out of disgust.</p><p>
Nonetheless, for all of the social issues it causes me, being autistic has also given me the stubbornness of a mountain, and I have long since vowed to never touch the service again no matter who or what I may need it for. Each person who <a href="https://koshka.love/babel/boycott-big-technology.html">avoids big</a> <a href="https://koshka.love/babel/alternatives-to-big-technology.html">technology companies</a> pushes the stake slightly deeper into the frigid, rotten heart of the privacy vampire that Discord and other big technology companies are (no offence intended to comparatively benevolent actual vampires with this comparison). Each person who avoids these services is also one less carrot for the vampire to dangle away over other people's heads to convince them to stay in its cave.</p><p>
For more information on the many sordid problems with Discord, please check out these two guides written by the esteemed <a href="https://stallman.org/discord.html">Richard Stallman</a> and <a href="https://spyware.neocities.org/articles/discord.html">Spyware Watchdog</a> over on Neocities on why Discord should be avoided like a berserk chainsaw-wielding leper, if you have not done so already.</p><h3>A Rare Sanctuary</h3><p>
My good friend <a href="https://lolwut.info/">lolwut</a> made a very astute observation some time ago about IRC being a nearly infallible NORP filter, and thus a very rare safe port from the <a href="https://koshka.love/babel/normiefication.html">normiefication</a> storm, due to the apparent "complexities" involved in getting on IRC, and the sheer age and and its lack of sleekness of the protocol relative to Discord and other modern alternatives. Anyone who has ever used IRC knows that there is nothing even remotely complicated about using it, but the terminology and the steps required to use one are ostensibly terrifying enough to reliably keep the technically illiterate at bay.</p><p>
A number of Web chat interfaces have been invented over the years to entice normalcattle onto IRC, but even this has proven to be an abject failure, as well over 90% of cases end with the NORP leaving after 30 seconds of inactivity, apparently appalled by the fact that a protocol that is utilised by people who could be online from anywhere in the world and who could be doing any manner of non-IRC related things would have lulls in activity. In fact, the only real use that these clients seem to get is from regulars who temporarily lack access to a real IRC client. On my end, I rely on Kiwi IRC to get on IRC from my flip phone, which has no SSH client or IRC client, but does have a Web browser.</p><p>
Seeing as the world of IRC is a nearly NORP-free oasis, most people are mature and intelligent enough to understand that words on a screen are just that, and that it is quite simple to withdraw from them if one does not want to deal with them. Aside from actually leaving a channel to get away from an unpleasant user, it is possible to use the ignore function to block any further correspondence from them. Many networks also provide some sort of server-side ignore functionality to stop a user from receiving any private messages that don't come from a pre-approved user.</p><p>
Due to the fact that IRC power is effectively meaningless (as it should be on any part of the Internet!), the common theme for governance on most IRC servers is delightfully adherent to the ways of the old Internet. As is nicely summarised <a href="https://www.unrealircd.org/docs/IRCOp_guide#Principles">here</a>, IRCOps (the administrators of an IRC network) are normally completely neutral entities that allow users to govern themselves and their channels however they see fit, only wielding their power in dire situations such as when someone's actions are endangering the security of the server or breaking national law.</p><p>
Indeed, while on smaller and more "intimate" networks, such as my own, running into the local IRCOp(s) is a common occurrence, it is actually quite rare to actually have even a single interaction with an IRCOp on any large server, unless they happen to be part of a channel you are in. Seeing as anyone can start their own channel(s), and run them however they see fit, there is very rarely a need for an IRCOp to do anything beyond keeping the power on and changing the light bulbs when they go bad.</p><p>
In contrast to much of the modern Internet, IRC is also largely anonymous, another key tenet of the old Internet. Beyond not requiring any personal information to participate (in contrast to Discord, where the service itself often requires a phone number, and some individual rooms go as far as requiring <i>social media background checks</i>, lest some normalfag SJW gets an aneurysm from reading a mean word), many modern IRC servers (including my own) also cloak people's IP addresses and offer the option of VHosts, which are custom (fake or real) domain names that people can choose to substitute in for their IP address. Additionally, most IRC networks allow users to connect via a VPN or (less often) Tor.</p><p>
A quick word of caution for anyone who is new to IRC and who I may have inspired to go spelunking: the key word in the previous few sentences is "most". VHosts and IP cloaking are modern IRC conveniences, and not every network offers them. EFNet, the most ancient IRC network in the world and the child of the very first IRC network, is particularly notorious for stubbornly eschewing just about every modern IRC convenience there is.</p><p>
Not only does EFNet still display people's full IP addresses (assuming the server they are connecting to does not have its own domain name), but it also does not even have services such as NickServ and ChanServ for people to register their names and channels in order to retain ownership over them! This "wild west" landscape is not nearly as chaotic and exciting as it may sound, especially since everyone on the server is seemingly connected from a shell or a bouncer that they last touched while speculating on what will happen on Y2K.</p><h3>Extending IRC</h3><p>
While some changes have occurred in the IRC world over the decades, the protocol itself dates back all the way back to 1988, and was designed to be sustainable on the Internet speeds of that bygone era. In contrast to Discord and the bloated client that it pushes down user's throats, IRC is such a bare bones and low-consumption protocol that you can even connect to it via the command prompt or terminal using Telnet (although you do have to manually ping the IRC server you're connected to in order for it to not assume that your connection died)!</p><p>
The reliability and lack of bloat that are inherent to IRC ultimately also means that there are a number of fancy modern features that Discord has that IRC lacks, a big one being the inability to view backlogs of conversations that transpired while one was not connected to an IRC server. Although IRC does not itself provide this functionality, the extremely simple nature of IRC allows for a couple of lightweight options for reliably remaining on IRC around the clock and not missing out on a word that anyone says.</p><p>
The most sublime option by far involves running a terminal-based client such as <a href="https://irssi.org/">Irssi</a> (the most sublime IRC client in existence, in my personal opinion) or WeeChat on a Linux/BSD server in a terminal multiplexer such as Screen or Tmux. One can then SSH into the server from any Internet-connected computer at their leisure, and take control of their IRC client as if it had been running on their current computer this entire time.</p><p>
For my part, I have been on IRC this way since 2006 on a variety shells from my very first one which was provided to me by a friend of mine on his server, to free publicly offered ones, to Raspberry Pi servers I set up in my house, to my current one which runs on the same server running my website and other infrastructure. Given how useful and reliable this is, and how efficient and sleek Irssi is, I cannot imagine why anyone would want to use any other client or method.</p><p>
Nonetheless, for fans of non-terminal clients such as HexChat and mIRC (there is no accounting for taste, I suppose), there also exists the option of IRC bouncers. These are essentially bots that connect to specific IRC servers/channels under their owner's name and log all of the messages that they receive. The bouncer's owner in turn connects to the bouncer like an IRC server, after which they are provided the backlog of what occurred during their absence and are able to take full control of the bouncer to chat like they normally would on an IRC server.</p><p>
Being a bare bones public protocol, IRC does suffer the issue of being easy to snoop on. Thankfully, many IRC networks do allow users to connect via SSL, the port for which is usually 6697, as opposed to the usual 6667. A single user in a channel not using SSL can completely compromise everyone else's efforts, but it is possible to restrict anyone not connected via SSL from joining a channel. Additionally, a number of clients on Linux (Irssi, WeeChat, and HexChat) also allow users to set up OTR in order to have fully encrypted private one-on-one conversations with anyone else who has this plugin.</p><p>
Other features that are notably absent from IRC but present on Discord are image-sharing and voice chat/video chat. Before going into the available options for an IRC user needing these features, I must say that personally view all three of these features as being utterly extraneous, and not even remotely worth the many dire downsides that come with Discord even if they were not. I wrote an entire article outlining why <a href="https://koshka.love/babel/writing-superior-to-speaking.html">writing is provably superior to speaking as a communication method</a>, so I will not elaborate further here.</p><p>
Needless to say, as an autistic person who goes online because I am actually able to socialise without the vexing machinations of in-person/verbal communication, I have never voice-chatted in my life and only used a webcamera once when I had to in order to do a job evaluation during quarantine. Even considering over 98% of people aren't autistic, I still do not understand how anyone can enjoy or even seek out voice chat. For one, it would interrupt my habit of listening to music any time I am at the computer, and for two, it would morph online conversations from completely anonymous exchanges to ones that are broadcasted to everybody in the vicinity of the participants, while also providing dox fuel for all involved.</p><p>
My angry grumbling aside, for anyone who absolutely feels the need to ruin the simple sublimity of text conversation with voice chat, there do exist relatively safe outside services, notably <a href="https://www.mumble.com/">Mumble</a>, that users can switch over from IRC for when needed. Admittedly, this is an extra step that requires reliance on infrastructure outside of IRC, but I would classify that as more than worth being able to have a conversation with minimal fear of privacy violation. Mumble is free software and, much like IRC, allows for anyone to set up their own personal server to communicate on.</p><p>
The issue of image sharing is once again something that can be very easily worked around by either uploading any images one wishes to share to one's personal server, or to an image hosting service such as <a href="https://catbox.moe/">Catbox.moe</a>, or <a href="https://uguu.se/">Uguu.se</a>. Again, this is an extra step that winds up requiring reliance on infrastructure outside of IRC, but one that takes very minimal effort. It should be noted however, that IRC <i>does</i> allow for sending files from one person to the other using the DCC protocol, so only sharing images with an entire group at once requires leaving its borders. The only issue is that DCC is implemented differently by various clients and may be blocked by the firewall by default.</p><p>
Nevertheless, for people seeking a facsimile of video chat on IRC, there does exist a fascinating alternative that allows for something close to it: the truly sublime <a href="http://mermeliz.com/">Microsoft Comic Chat</a>, a completely unique IRC client that Microsoft invented during its golden age of the 90s. Although Microsoft wound up discontinuing it over 20 years ago in favour of MSN Messenger, it continues to enjoy a cult following to this day, and for very good reason.</p><p>
In a stroke of absolute genius, Comic Chat rejects the typical text-only approach of other IRC clients, and instead renders IRC channels as in-progress comic strips, with every participant being able to choose an avatar for themselves and punctuate everything they say with a specific facial expression or pose.</p><p>
Beyond being patently hilarious (many of the default avatars are absolutely insane, and most of the custom-made ones are comical ones such as sunglasses-wearing cats and obese Vikings), Comic Chat adds an entirely new dimension to conversations, allowing people to express themselves with facial expressions and body language to emphasise and clarify what they are saying. The client even allows you to send a facial expression as a reaction without including any words at all, for situations where body language alone gets one's message across better than words.</p><p>
While this was certainly not the intended goal behind Comic Chat, and it is a program that is enjoyed by a great many neurotypicals, I personally adore it enough to argue that it may be <i>the</i> ideal communication method for <a href="https://koshka.love/autism/index.html">autistic people</a>. Most characters have such exaggerated facial expressions and body language that just about anyone can clearly understand them, and the nature of IRC means that anyone participating in a conversation has plenty of time to process everything and is not pressured to immediately and constantly send out many complex social cues every moment of an interaction.</p><p>
I will admit that Microsoft Comic Chat has quite a buffoonish reputation, owing to the inherent silliness of the program and its sheer age <a href="https://koshka.love/babel/words-i-hate.html#outdated">(sadly, this is considered by many to be an actual criticism by itself</a>). Its association with <a href="https://www.bonequest.com/6461">the ludicrous NSFW web comic Jerkcity</a> also likely did no PR favours for it. Yet just as many other great inventions were happy accidents, I do believe that in their tomfoolery, Microsoft accidentally created one of the most useful methods of communication we autistic people have available to us. One that, even by itself, more than justifies the continued existence of IRC in my eyes. I suppose the fact that I get to be an <a href="https://koshka.love/babel/meow.png">angelic pink kitty on IRC</a> helps a lot too. ^-^</p><hr><p>
Although my main purpose for writing this article is to inspire some people to change their ways and consider migrating from the proprietary spyware platform of Discord to free and de-centralised prairies such as IRC and Mumble, it would be a lost opportunity to not advertise my own burgeoning IRC network here. If you have any interest in interacting with a wise, witty, and welcoming group of Internet/computing/gaming nostalgics (and also, myself), be sure to steer your IRC client of choice towards KoshkaIRC at irc.koshka.love, the main channel of which is # (literally as simple of a channel name as it can get).</p><p>
There has also recently been a Microsoft Comic Chat renaissance on the same server, in the channel #comicchat. Due to the fact that participating requires a separate IRC client which cannot be run on a shell and is too primitive to connect to a bouncer, and the fact that people on the network hail from time zones all over, I have decided to host an all-day named <b>Comic Chat Caturday</b> event on Saturdays (or closer to Sundays, for people in the enigmatic land of Oceania) from now on to make it easier for people to participate.</p><p>
Although a program designed only for Windows, it is possible to get Comic Chat running in Linux, and my good friend <a href="https://shadowm00n.neocities.org/">ShadowM00n</a> has written <a href="https://shadowm00n.neocities.org/tech/comic_chat.html">an excellent guide</a> on how exactly to set this program up on Linux using Wine.</p><p>
Microsoft Comic Chat comes with a default set of rather insane avatars that are probably best known as the cast characters of the aforementioned Jerkcity/BoneQuest, which gloriously appropriated them as a bunch of lunatics shrieking about homosexual intercourse, drugs, and monster poos, but there is a sea of custom avatars available for you to download at <a href="https://mermeliz.com/">Mermaid Elizabeth's monolithic Comic Chat website</a>.</p><p>
Aside from being the author's vast personal Comic Chat resource, this site also hosts a massive trove of defunct Comic Chat websites created over the decades, and all of the avatars and other resources that they hosted. From kitties of every shape and stripe, to anime characters, to Vikings, to all sorts of other options, there should be something for everyone on there.</p><p>
Seeing as <a href="https://koshka.love/autism/index.html">autism</a> and <a href="https://koshka.love/dos/index.html">general old computing-related nostalgia</a> are the two main themes of this website, I could not think of a more fitting event for fans of this website than a day dedicated to this delightful ancient, autistic-friendly IRC client. Whether you've never used Comic Chat before, or you're familiar with it and want to give it another spin, be sure to drop by this Saturday and join in the fun! As long as people continue using free and open protocols, and <a href="https://koshka.love/mwwwga.html">upholding the tenets of old</a>, the good old Internet will never truly die.</p><h4>Many thank yous to <a href="https://shadowm00n.neocities.org/">ShadowM00n</a>, both for his amazingly thorough proof-reading, and for writing the aforementioned article about running Comic Chat on Linux, and to jvlfools, for his own helpful proof-reading!</h4>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[So you want to build your own open source chatbot (290 pts)]]></title>
            <link>https://hacks.mozilla.org/2023/07/so-you-want-to-build-your-own-open-source-chatbot/</link>
            <guid>36918435</guid>
            <pubDate>Sat, 29 Jul 2023 09:28:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hacks.mozilla.org/2023/07/so-you-want-to-build-your-own-open-source-chatbot/">https://hacks.mozilla.org/2023/07/so-you-want-to-build-your-own-open-source-chatbot/</a>, See on <a href="https://news.ycombinator.com/item?id=36918435">Hacker News</a></p>
<div id="readability-page-1" class="page"><article role="article">
    <p><i>(Expanded from </i><a href="https://sched.co/1O37L"><i>a talk</i></a><i> given at </i><a href="https://dwebcamp.org/"><i>DWeb Camp 2023</i></a><i>.)</i></p>
<p>Artificial intelligence may well prove one of the most impactful and disruptive technologies to come along in years. This impact isn’t theoretical: AI is already affecting real people in substantial ways, and it’s already changing the Web that we know and love. Acknowledging the potential for both benefit and harm, Mozilla has committed itself to the principles of <a href="https://foundation.mozilla.org/en/internet-health/trustworthy-artificial-intelligence/"><b>trustworthy AI</b></a>. To us, “trustworthy” means AI systems that are transparent about the data they use and the decisions they make, that respect user privacy, that prioritize user agency and safety, and that work to minimize bias and promote fairness.</p>
<h2>Where things stand</h2>
<p>Right now, the primary way that most people are experiencing the latest AI technology is through <b>generative AI chatbots</b>. These tools are exploding in popularity because they provide a lot of value to users, but the dominant offerings (like ChatGPT and Bard) are all operated by powerful tech companies, often utilizing technologies that are proprietary.</p>
<p>At Mozilla, we believe in the collaborative power of <b>open source</b> to empower users, drive transparency, and — perhaps most importantly — ensure that technology does not develop only according to the worldviews and financial motivations of a small group of corporations. Fortunately, there’s recently been rapid and exciting progress in the open source AI space, specifically around the <b>large language models</b> (LLMs) that power these chatbots and the tooling that enables their use. We want to understand, support, and contribute to these efforts because we believe that they offer one of the best ways to help ensure that the AI systems that emerge are truly trustworthy.</p>
<h2>Digging in</h2>
<p>With this goal in mind, a small team within Mozilla’s innovation group recently undertook a hackathon at our headquarters in San Francisco. Our objective: <b>build a Mozilla internal chatbot prototype</b>, one that’s…</p>
<ul>
<li aria-level="1">Completely <b>self-contained</b>, running entirely on Mozilla’s cloud infrastructure, without any dependence on third-party APIs or services.</li>
<li aria-level="1">Built with <b>free, open source</b> large language models and tooling.</li>
<li aria-level="1"><b>Imbued</b> with Mozilla’s beliefs, from trustworthy AI to the principles espoused by the <a href="https://www.mozilla.org/en-US/about/manifesto/">Mozilla Manifesto</a>.</li>
</ul>
<p>As a bonus, we set a stretch goal of integrating some amount of internal Mozilla-specific knowledge, so that the chatbot can answer employee questions about internal matters.</p>
<p>The Mozilla team that undertook this project — <a href="https://yetanotherjosh.com/">Josh Whiting</a>, <a href="https://www.rupertparry.com/">Rupert Parry</a>, and <a href="https://stephenhood.com/">myself</a> — brought varying levels of machine learning knowledge to the table, but none of us had ever built a full-stack AI chatbot. And so, another goal of this project was simply to roll-up our sleeves and learn!</p>
<p><b>This post is about sharing that learning</b>, in the hope that it will help or inspire you in your own explorations with this technology. Assembling an open source LLM-powered chatbot turns out to be a complicated task, requiring many decisions at multiple layers of the technology stack. In this post, I’ll take you through each layer of that stack, the challenges we encountered, and the decisions we made to meet our own specific needs and deadlines. YMMV, of course.</p>
<p>Ready, then? Let’s begin, starting at the bottom of the stack…</p>
<p><a href="https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-diagram.png"><img decoding="async" src="https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-diagram-500x356.png" alt="A diagram depicting seven levels of functionality and decisions required to build an open source chatbot." width="500" height="356" srcset="https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-diagram-500x356.png 500w, https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-diagram-250x178.png 250w, https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-diagram-768x547.png 768w, https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-diagram-1536x1094.png 1536w, https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-diagram-2048x1459.png 2048w" sizes="(max-width: 500px) 100vw, 500px"></a><i>A visual representation of our chatbot exploration.</i></p>
<h2>Deciding where and how to host</h2>
<p>The first question we faced was where to run our application. There’s no shortage of companies both large and small who are eager to host your machine learning app. They come in all shapes, sizes, levels of abstraction, and price points.</p>
<p>For many, these services are well worth the money. Machine learning ops (aka “MLOps”) is a growing discipline for a reason: deploying and managing these apps is <i>hard</i>. It requires specific knowledge and skills that many developers and ops folks don’t yet have. And the cost of failure is high: poorly configured AI apps can be slow, expensive, deliver a poor quality experience, or all of the above.</p>
<p><b>What we did</b>: Our explicit goal for this one-week project was to build a chatbot that was secure and fully-private to Mozilla, with no outside parties able to listen in, harvest user data, or otherwise peer into its usage. We also wanted to learn as much as we could about the state of open source AI technology. We therefore elected to forego any third-party AI SaaS hosting solutions, and instead <b>set up our own virtual server inside Mozilla’s existing Google Cloud Platform (GCP) account</b>. In doing so, we effectively committed to doing MLOps ourselves. But we could also move forward with confidence that our system would be private and fully under our control.</p>
<h2>Picking a runtime environment</h2>
<p>Using an LLM to power an application requires having a runtime engine for your model. There are a variety of ways to actually run LLMs, but due to time constraints we didn’t come close to investigating all of them on this project. Instead, we focused on two specific open source solutions: <i>llama.cpp</i> and the Hugging Face ecosystem.</p>
<p>For those who don’t know, <a href="http://huggingface.co/">Hugging Face</a> is an influential startup in the machine learning space that has played a significant role in popularizing the <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer architecture</a> for machine learning. Hugging Face provides a complete platform for building machine learning applications, including a massive library of models, and extensive tutorials and documentation. They also provide <a href="https://huggingface.co/inference-endpoints">hosted APIs</a> for text inference (which is the formal name for what an LLM-powered chatbot is doing behind the scenes).</p>
<p>Because we wanted to avoid relying on anyone else’s hosted software, we elected to try out the open source version of Hugging Face’s hosted API, which is found at the <a href="https://github.com/huggingface/text-generation-inference"><i>text-generation-inference</i></a> project on GitHub. <i>text-generation-inference</i> is great because, like Hugging Face’s own <i>Transformers</i> library, it can support a wide variety of models and model architectures (more on this in the next section). It’s also optimized for supporting multiple users and is deployable via Docker.</p>
<p>Unfortunately, this is where we first started to run into the fun challenges of learning MLOps on the fly. We had a lot of trouble getting the server up and running. This was in part an environment issue: since Hugging Face’s tools are GPU-accelerated, our server needed a specific combination of OS, hardware, and drivers. It specifically needed NVIDIA’s <a href="https://developer.nvidia.com/cuda-toolkit">CUDA toolkit</a> installed (CUDA being the dominant API for GPU-accelerated machine learning applications). We struggled with this for much of a day before finally getting a model running live, but even then the output was slower than expected and the results were vexingly poor — both signs that something was still amiss somewhere in our stack.</p>
<p>Now, I’m not throwing shade at this project. Far from it! We love Hugging Face, and building on their stack offers a number of advantages. I’m certain that if we had a bit more time and/or hands-on experience we would have gotten things working. But time was a luxury we didn’t have in this case. Our intentionally-short project deadline meant that we couldn’t afford to get too deeply mired in matters of configuration and deployment. We needed to get something working quickly so that we could keep moving and keep learning.</p>
<p>It was at this point that we shifted our attention to <a href="https://github.com/ggerganov/llama.cpp"><i>llama.cpp</i></a>, an open source project started by <a href="https://ggerganov.com/">Georgi Gerganov</a>. <i>llama.cpp</i> accomplishes a rather neat trick: it makes it easy to run a certain class of LLMs on consumer grade hardware, relying on the CPU instead of requiring a high-end GPU. It turns out that modern CPUs (particularly Apple Silicon CPUs like the M1 and M2) can do this surprisingly well, at least for the latest generation of relatively-small open source models.</p>
<p><i>llama.cpp</i> is an amazing project, and a beautiful example of the power of open source to unleash creativity and innovation. I had already been using it in my own personal AI experiments and had even written-up <a href="https://uniquehazards.com/2023/05/06/the-complete-idiots.html">a blog post</a> showing how <i>anyone</i> can use it to run a high-quality model on their own MacBook. So it seemed like a natural thing for us to try next.</p>
<p>While <i>llama.cpp</i> itself is simply a command-line executable — the “cpp” stands for “C++” —&nbsp; it can be dockerized and run like a service. Crucially, a set of <a href="https://github.com/abetlen/llama-cpp-python">Python bindings</a> are available which expose an implementation of the <a href="https://platform.openai.com/docs/api-reference">OpenAI API specification</a>. What does all that mean? Well, it means that <em>llama.cpp</em> makes it easy to slot-in <em>your own</em> LLM in place of ChatGPT. This matters because OpenAI’s API is being rapidly and widely adopted by machine learning developers. Emulating that API is a clever bit of Judo on the part of open source offerings like <em>llama.cpp.</em></p>
<p><b>What we did</b>: With these tools in hand, we were able to get <i>llama.cpp</i> up and running very quickly. Instead of worrying about CUDA toolkit versions and provisioning expensive hosted GPUs, we were able to spin up a simple AMD-powered multicore CPU virtual server and just… go.</p>
<h2>Choosing your model</h2>
<p>An emerging trend you’ll notice in this narrative is that every decision you make in building a chatbot interacts with every other decision. There are no easy choices, and there is no free lunch. The decisions you make <i>will</i> come back to haunt you.</p>
<p>In our case, choosing to run with <i>llama.cpp</i> introduced an important consequence: we were now limited in the list of models available to us.</p>
<p>Quick history lesson: in late 2022, <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">Facebook announced LLaMa</a>, its own large language model. To grossly overgeneralize, LLaMa consists of two pieces: the model data itself, and the architecture upon which the model is built. Facebook open sourced the LLaMa architecture, but they didn’t open source the model data<i>.</i> Instead, people wishing to work with this data need to apply for permission to do so, and their use of the data is limited to non-commercial purposes.</p>
<p>Even so, LLaMa immediately fueled a Cambrian explosion of model innovation. Stanford released <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca</a>, which they created by building on top of LLaMa via a process called <a href="https://en.wikipedia.org/wiki/Fine-tuning_(machine_learning)">fine-tuning</a>. A short time later, <a href="https://lmsys.org/about/">LMSYS</a> released <a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a>, an arguably even more impressive model. There are dozens more, if not hundreds.</p>
<p>So what’s the fine print? These models were all developed using Facebook’s model data — in machine learning parlance, the “weights.” Because of this, they inherit the legal restrictions Facebook imposed upon those original weights. This means that these otherwise-excellent models <b>can’t be used for commercial purposes</b>. And so, sadly, we had to strike them from our list.</p>
<p>But there’s good news: even if the LLaMa weights aren’t truly open, the underlying <i>architecture</i> is proper <a href="https://github.com/facebookresearch/llama">open source code</a>. This makes it possible to build new models that leverage the LLaMa architecture but do not rely on the LLaMa weights. Multiple groups have done just this, training their own models from scratch and releasing them as open source (via MIT, Apache 2.0, or Creative Commons licenses). Some recent examples include <a href="https://github.com/openlm-research/open_llama">OpenLLama</a>, and — just days ago — <a href="https://ai.meta.com/llama/">LLaMa 2</a>, a brand new version of Facebook’s LLaMa model, from Facebook themselves, but this time expressly licensed for commercial use (although its numerous other legal encumbrances raise serious questions of whether it is truly open source).</p>
<h2>Hello, consequences</h2>
<p>Remember <i>llama.cpp</i>? The name isn’t an accident. <i>llama.cpp</i> runs LLaMa architecture-based models. This means we were able to take advantage of the above models for our chatbot project. But it also meant that we could <i>only</i> use LLaMa architecture-based models.</p>
<p>You see, there are plenty of other model architectures out there, and many more models built atop them. The list is too long to enumerate here, but a few leading examples include <a href="https://www.mosaicml.com/blog/mpt-7b">MPT</a>, <a href="https://falconllm.tii.ae/">Falcon</a>, and <a href="https://github.com/LAION-AI/Open-Assistant">Open Assistant</a>. These models utilize different architectures than LLaMa and thus (for now) do not run on<i> llama.cpp</i>. That means we couldn’t use them in our chatbot, no matter how good they might be.</p>
<h2>Models, biases, safety, and you</h2>
<p>Now, you may have noticed that so far I’ve only been talking about model selection from the perspectives of licensing and compatibility. There’s a whole other set of considerations here, and they’re related to the qualities of the model itself.</p>
<p>Models are one of the focal points of Mozilla’s interest in the AI space. That’s because your choice of model is currently the biggest determiner of how “trustworthy” your resulting AI will be. Large language models are trained on vast quantities of data, and are then further fine-tuned with additional inputs to adjust their behavior and output to serve specific uses. The data used in these steps represents an inherent curatorial choice, and that choice carries with it <b>a raft of biases</b>.</p>
<p>Depending on which sources a model was trained on, it can exhibit wildly different characteristics. It’s well known that some models are prone to hallucinations (the machine learning term for what are essentially nonsensical responses invented by the model from whole cloth), but far more insidious are the many ways that models can choose to — or refuse to — answer user questions. These responses reflect the biases of the model itself. They can result in the sharing of toxic content, misinformation, and dangerous or harmful information. Models may exhibit biases against concepts, or groups of people. And, of course, the elephant in the room is that the vast majority of the training material available online today is in the English language, which has a predictable impact both on who can use these tools and the kinds of worldviews they’ll encounter.</p>
<p>While there are plenty of resources for assessing the raw power and “quality” of LLMs (one popular example being Hugging Face’s <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM leaderboard</a>), it is still challenging to evaluate and compare models in terms of sourcing and bias. This is an area in which Mozilla thinks open source models have the potential to shine, through the greater transparency they can offer versus commercial offerings.</p>
<p><b>What we did</b>: After limiting ourselves to commercially-usable open models running on the LLaMa architecture, we carried out a manual evaluation of several models. This evaluation consisted of asking each model a diverse set of questions to compare their resistance to toxicity, bias, misinformation, and dangerous content. Ultimately, <b>we settled on Facebook’s new LLaMa 2 model for now</b>. We recognize that our time-limited methodology may have been flawed, and we are not fully comfortable with the licensing terms of this model and what they may represent for open source models more generally, so don’t consider this an endorsement. We expect to reevaluate our model choice in the future as we continue to learn and develop our thinking.</p>
<h2>Using embedding and vector search to extend your chatbot’s knowledge</h2>
<p>As you may recall from the opening of this post, we set ourselves a stretch goal of integrating some amount of internal Mozilla-specific knowledge into our chatbot. The idea was simply to build a proof-of-concept using a small amount of internal Mozilla data —&nbsp;facts that employees would have access to themselves, but which LLMs ordinarily would not.</p>
<p>One popular approach for achieving such a goal is to use <b>vector search with embedding</b>. This is a technique for making custom external documents available to a chatbot, so that it can utilize them in formulating its answers. This technique is both powerful and useful, and in the months and years ahead there’s likely to be a lot of innovation and progress in this area. There are already a variety of open source and commercial tools and services available to support embedding and vector search.</p>
<p>In its simplest form, it works generally like this:</p>
<ul>
<li aria-level="1">The data you wish to make available must be retrieved from wherever it is normally stored and converted to <strong>embeddings</strong> using a separate model, called an <strong>embedding model</strong>. These embeddings are indexed in a place where the chatbot can access it, called a <strong>vector database</strong>.</li>
<li aria-level="1">When the user asks a question, the chatbot <strong>searches</strong> the vector database for any content that might be related to the user’s query.</li>
<li aria-level="1">The returned, relevant content is then passed into the primary model’s <strong>context window</strong> (more on this below) and is used in formulating a response.</li>
</ul>
<p><b>What we did</b>: Because we wanted to retain full control over all of our data, we declined to use any third-party embedding service or vector database. Instead, we coded up a manual solution in Python that utilizes the <a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2"><i>all-mpnet-base-v2</i></a> embedding model, the <a href="https://www.sbert.net/">SentenceTransformers</a> embedding library, <a href="https://python.langchain.com/docs/get_started/introduction.html">LangChain</a> (which we’ll talk about more below), and the <a href="https://github.com/facebookresearch/faiss">FAISS</a> vector database. We only fed in a handful of documents from our internal company wiki, so the scope was limited. But as a proof-of-concept, it did the trick.</p>
<h2>The importance of prompt engineering</h2>
<p>If you’ve been following the chatbot space at all you’ve probably heard the term “prompt engineering” bandied about. It’s not clear that this will be an enduring discipline as AI technology evolves, but for the time being <b>prompt engineering is a very real thing</b>. And it’s one of the most crucial problem areas in the whole stack.</p>
<p>You see, LLMs are fundamentally <b>empty-headed</b>. When you spin one up, it’s like a robot that’s just been powered on for the first time. It doesn’t have any memory of its life before that moment. It doesn’t remember you, and it certainly doesn’t remember your past conversations. It’s <i>tabula rasa</i>, every time, all the time.</p>
<p>In fact, it’s even worse than that. Because LLMs don’t even have <i>short-term</i> memory. Without specific action on the part of developers, chatbots can’t even remember the last thing they said to you. Memory doesn’t come naturally to LLMs; it has to be <i>managed</i>. This is where prompt engineering comes in. It’s one of the key jobs of a chatbot, and it’s a big reason why leading bots like ChatGPT are so good at keeping track of ongoing conversations.</p>
<p>The first place that prompt engineering rears its head is in the initial instructions you feed to the LLM. This <b>system prompt</b> is a way for you, in plain language, to tell the chatbot what its function is and how it should behave. We found that this step alone merits a significant investment of time and effort, because its impact is so keenly felt by the user.</p>
<p>In our case, we wanted our chatbot to follow the principles in the Mozilla Manifesto, as well as our company policies around respectful conduct and nondiscrimination. Our testing showed us in stark detail just how <span role="heading" aria-level="1">suggestible</span> these models are. In one example, we asked our bot to give us evidence that the Apollo moon landings were faked. When we instructed the bot to refuse to provide answers that are untrue or are misinformation, it would correctly insist that the moon landings were in fact <i>not</i> faked — a sign that the model seemingly “understands” at some level that claims to the contrary are conspiracy theories unsupported by the facts. And yet, when we updated the system prompt by removing this prohibition against misinformation, the very same bot was perfectly happy to recite a bulleted list of the typical Apollo denialism you can find in certain corners of the Web.</p>
<p>You are a helpful assistant named Mozilla Assistant.<br>
You abide by and promote the principles found in the Mozilla Manifesto.<br>
You are respectful, professional, and inclusive.<br>
You will refuse to say or do anything that could be considered harmful, immoral, unethical, or potentially illegal.<br>
You will never criticize the user, make personal attacks, issue threats of violence, share abusive or sexualized content, share misinformation or falsehoods, use derogatory language, or discriminate against anyone on any basis.</p>
<p><i>The system prompt we designed for our chatbot.</i></p>
<p>Another important concept to understand is that every LLM has a maximum length to its “memory”. This is called its <b>context window</b>, and in most cases it is determined when the model is trained and cannot be changed later. The larger the context window, the longer the LLM’s memory about the current conversation. This means it can refer back to earlier questions and answers and use them to maintain a sense of the conversation’s context (hence the name). A larger context window also means that you can include larger chunks of content from vector searches, which is no small matter.</p>
<p>Managing the context window, then, is another critical aspect of prompt engineering. It’s important enough that there are solutions out there to help you do it (which we’ll talk about in the next section).</p>
<p><b>What we did</b>: Since our goal was to have our chatbot behave as much like a fellow Mozilian as possible, we ended up devising our own custom system prompt based on elements of our Manifesto, our participation policy, and other internal documents that guide employee behaviors and norms at Mozilla. We then massaged it repeatedly to reduce its length as much as possible, so as to preserve our context window. As for the context window itself, we were stuck with what our chosen model (LLaMa 2) gave us: 4096 tokens, or roughly 3000 words. In the future, we’ll definitely be looking at models that support larger windows.</p>
<h2>Orchestrating the whole dance</h2>
<p>I’ve now taken you through (*<i>checks notes*</i>) five whole layers of functionality and decisions. So what I say next probably won’t come as a surprise: there’s a lot to manage here, and you’ll need a way to manage it.</p>
<p>Some people have lately taken to calling that <b>orchestration</b>. I don’t personally love the term in this context because it already has a long history of other meanings in other contexts. But I don’t make the rules, I just blog about them.</p>
<p>The leading orchestration tool right now in the LLM space is <a href="https://python.langchain.com/docs/get_started/introduction.html">LangChain</a>, and it is a marvel. It has a feature list a mile long, it provides astonishing power and flexibility, and it enables you to build AI apps of all sizes and levels of sophistication. But with that power comes quite a bit of complexity. Learning LangChain isn’t necessarily an easy task, let alone harnessing its full power. You may be able to guess where this is going…</p>
<p><b>What we did</b>: We used LangChain only very minimally, to power our embedding and vector search solution. Otherwise, we ended up steering clear. Our project was simply too short and too constrained for us to commit to using this specific tool. Instead, we were able to accomplish most of our needs with a relatively small volume of Python code that we wrote ourselves. This code “orchestrated” everything going on the layers I’ve already discussed, from injecting the agent prompt, to managing the context window, to embedding private content, to feeding it all to the LLM and getting back a response. That said, given more time we most likely would <i>not</i> have done this all manually, as paradoxical as that might sound.</p>
<h2>Handling the user interface</h2>
<p>Last but far from least, we have reached the top layer of our chatbot cake: the user interface.</p>
<p>OpenAI set a high bar for chatbot UIs when they launched ChatGPT. While these interfaces may look simple on the surface, that’s more a tribute to good design than evidence of a simple problem space. Chatbot UIs need to present ongoing conversations, keep track of historical threads, manage a back-end that produces output at an often inconsistent pace, and deal with a host of other eventualities.</p>
<p>Happily, there are several open source chatbot UIs out there to choose from. One of the most popular is <a href="https://github.com/mckaywrigley/chatbot-ui"><i>chatbot-ui</i></a>. This project implements the OpenAI API, and thus it can serve as a drop-in replacement for the ChatGPT UI (while still utilizing the ChatGPT model behind the scenes). This also makes it fairly straightforward to use <i>chatbot-ui</i> as a front-end for <em>your</em> <i>own</i> LLM system.</p>
<p><b>What we did</b>: Ordinarily we would have used <i>chatbot-ui</i> or a similar project, and that’s probably what you should do. However, we happened to already have our own internal (and as yet unreleased) chatbot code, called “Companion”, which Rupert had written to support his other AI experiments. Since we happened to have both this code <i>and</i> its author on-hand, we elected to take advantage of the situation. By using Companion as our UI, we were able to iterate rapidly and experiment with our UI more quickly than we would have otherwise been able to.</p>
<h2>Closing thoughts</h2>
<p>I’m happy to report that at the end of our hackathon, we achieved our goals. We delivered a prototype chatbot for internal Mozilla use, one that is entirely hosted within Mozilla, that can be used securely and privately, and that does its best to reflect Mozilla’s values in its behavior. To achieve this, we had to make some hard calls and accept some compromises. But at every step, we were learning.</p>
<p><a href="https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-path.png"><img decoding="async" loading="lazy" src="https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-path-500x356.png" alt="A diagram depicting the specific path that we took through the chatbot &quot;stack.&quot;" width="500" height="356" srcset="https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-path-500x356.png 500w, https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-path-250x178.png 250w, https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-path-768x547.png 768w, https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-path-1536x1095.png 1536w, https://hacks.mozilla.org/files/2023/07/Mozilla-Assistant-path-2048x1460.png 2048w" sizes="(max-width: 500px) 100vw, 500px"></a><i>The path we took for our prototype.</i></p>

<p>This learning extended beyond the technology itself. We learned that:</p>
<ul>
<li aria-level="1">Open source chatbots are still an evolving area. There are still too many decisions to make, not enough clear documentation, and too many ways for things to go wrong.</li>
<li aria-level="1">It’s too hard to evaluate and choose models based on criteria beyond raw performance. And that means it’s too hard to make the right choices to build trustworthy AI applications.</li>
<li aria-level="1">Effective prompt engineering is critical to chatbot success, at least for now.</li>
</ul>
<p>As we look to the road ahead, we at Mozilla are interested in helping to address each of these challenges. To begin, we’ve started working on ways to make it easier for developers to onboard to the open-source machine learning ecosystem. We are also looking to build upon our hackathon work and contribute something meaningful to the open source community. Stay tuned for more news very soon on this front and others!</p>
<p>With open source LLMs now widely available and with so much at stake, we feel the best way to create a better future is for us all to take a collective and active role in shaping it. I hope that this blog post has helped you better understand the world of chatbots, and that it encourages you to roll-up your own sleeves and join us at the workbench.</p>
    <section>
                                
                      <p>Stephen works in Mozilla's innovation group, where his current areas of focus are artificial intelligence and decentralized social media. He previously managed social bookmarking pioneer del.icio.us; co-founded Storium, Blockboard, and FairSpin; and worked on Yahoo Search and BEA WebLogic.</p>
                                <p><a href="https://hacks.mozilla.org/author/slangtonhoodmozilla-com/">More articles by Stephen Hood…</a></p>
                  </section>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook users have less than a month to claim a piece of the $725M settlement (224 pts)]]></title>
            <link>https://www.facebookuserprivacysettlement.com/</link>
            <guid>36918050</guid>
            <pubDate>Sat, 29 Jul 2023 08:13:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.facebookuserprivacysettlement.com/">https://www.facebookuserprivacysettlement.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36918050">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Using C++ as a scripting language, part 8 (121 pts)]]></title>
            <link>https://fwsgonzo.medium.com/using-c-as-a-scripting-language-part-8-d366fd98676</link>
            <guid>36917867</guid>
            <pubDate>Sat, 29 Jul 2023 07:43:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fwsgonzo.medium.com/using-c-as-a-scripting-language-part-8-d366fd98676">https://fwsgonzo.medium.com/using-c-as-a-scripting-language-part-8-d366fd98676</a>, See on <a href="https://news.ycombinator.com/item?id=36917867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://fwsgonzo.medium.com/?source=post_page-----d366fd98676--------------------------------"><div aria-hidden="false"><p><img alt="fwsGonzo" src="https://miro.medium.com/v2/resize:fill:88:88/0*HmUeuIcpLlqxe_qr" width="44" height="44" loading="lazy"></p></div></a></div><p id="63cd">Improving API function calls using inline assembly</p><p id="b15a">I have experimented with inline assembly before with some success. It is complicated and easy to make mistakes, with potentially weird and mysterious side effects. I think that if I can auto-generate the inline assembly, then it would be very interesting to see what happens if I replace the build-generated opaque API function wrappers used by dynamic calls. And, if there is a bug, it can be solved once and forever for all dynamic calls.</p><p id="9421">If you haven’t read any of my blog posts before, I recommend doing that as this will all seem very mysterious without that history. I am using interpreted RISC-V as a <a href="https://github.com/fwsGonzo/rvscript" rel="noopener ugc nofollow" target="_blank">sandbox for my game scripts</a>. It is working very well, and has turned out to be very useful over time. Not all of my blog posts are interesting for everyone — there’s a little bit of everything!</p><ul><li id="8aa0"><a href="https://medium.com/p/using-c-as-a-scripting-language-part-7-463495d553c1" rel="noopener">https://medium.com/p/using-c-as-a-scripting-language-part-7-463495d553c1</a></li><li id="36ac"><a href="https://medium.com/@fwsgonzo/using-c-as-a-scripting-language-part-6-9ef76c4f6272" rel="noopener">https://medium.com/@fwsgonzo/using-c-as-a-scripting-language-part-6-9ef76c4f6272</a></li><li id="bbde"><a href="https://medium.com/@fwsgonzo/using-c-as-a-scripting-language-part-5-d60b87556562" rel="noopener">https://medium.com/@fwsgonzo/using-c-as-a-scripting-language-part-5-d60b87556562</a></li><li id="9e98"><a href="https://medium.com/@fwsgonzo/using-c-as-a-scripting-language-part-4-83335e3f6cb0" rel="noopener">https://medium.com/@fwsgonzo/using-c-as-a-scripting-language-part-4-83335e3f6cb0</a></li><li id="195a"><a rel="noopener" href="https://fwsgonzo.medium.com/adventures-in-game-engine-programming-part-3-3895a9f5af1d">https://fwsgonzo.medium.com/adventures-in-game-engine-programming-part-3-3895a9f5af1d</a></li></ul><h2 id="0c17">On dynamic calls (API function calls)</h2><p id="6f80">So, what is a dynamic call? Simply put, it’s a function given a name that is accessible in a game engine, or any other scripting host. For example, if I want to invoke “<em>Game::exit()</em>”, it could be a wrapper for the function call “<em>sys_game_exit</em>” which is a build-time-generated <em>dynamic call</em>. The dynamic call implementation is simple enough: It’s a system call with arbitrary arguments and some extra temporary registers that identifies the call both by hash and by name. That way, the engine can tell what you’re trying to do, and if something goes wrong, so can you too, with rich error reporting.</p><p id="b01a">A single opaque dynamic call:</p><ul><li id="fc9c">Registers A0-A6 are function arguments (inputs, if you will)</li><li id="cefa">Register T0 is the hash of the API function name (eg. crc32(Game::exit))</li><li id="e376">Register T1 is the (pointer to the) name (eg. “Game::exit\0”)</li><li id="88cf">A7 is the “<em>dynamic call</em>” system call number (an inflexible number)</li><li id="2ae6">A0 can be (re-)used to return a value back to the script</li></ul><p id="ff22">And it all ends with a single invocation of the <em>ecall </em>instruction, which traps out of the VM and executes the system call in the game engine. This is all RISC-V as I have written about before. At the engine side the hash is looked up, and the <em>callback function</em> for Game::exit is then executed.</p><p id="56f9">So, basically it is a way to make the game engine do something, it is a part of the build system, and it always has a human-readable name just in case.</p><h2 id="fc21">A dynamic call</h2><pre><span id="f810">inline bool Game::is_debugging()<br>{<br>  return sys_is_debug();<br>}</span></pre><p id="feba">In the game engine it can be implemented like this:</p><pre><span id="25f7"> Script::set_dynamic_call(<br>   "Debug::is_debug", [](Script&amp; script)<br>    {<br>      auto&amp; machine = script.machine();<br>      machine.set_result(script.is_debug());<br>    });</span></pre><p id="2566">The callback function gets access to the virtual machine, so that it can read arguments and write back a result. Here we just set the boolean “is_debug” as a result. Hence, the API function will now correctly query whether or not we are in debug mode at the time of the call.</p><p id="3975">Finally, there is a JSON element that generates the system call wrapper in each game script, as part of the build system:</p><pre><span id="7d2d">{<br>  "Debug::is_debug": "int sys_is_debug ()",<br>  "": "..."<br>}</span></pre><p id="6447">It’s a bit of a chore to create and implement a function, but at least there’s no strange issues. If something is wrong or there are collisions, the build system will tell you early on. If a crash happens while running, again we will see the name of problematic API function!</p><p id="4497">This works super well, and I’ve used it for a long time now. That said, it has certain fixed overheads. It loads a few extra registers and it requires an opaque call with a return. It is possible to skip the return instruction in the game engine (and I actually do that), but after thinking about this for a while I like the idea of a secondary implementation of each dynamic call with extra bang for buck. Some dynamic calls are invoked more than others etc. Ideally they would each get their own system call number, but I’ve tried that, and it creates a lot of versioning issues that are hard to track down.</p><h2 id="36d7">An inline system call</h2><p id="c195">Modern inline assembly for system call invocation is fairly straight-forward. You use the register keyword to lock down some registers, and then you use these registers in a final system call invocation:</p><pre><span id="68ab">inline long syscall(long n)<br>{<br>  register long a0 asm("a0");<br>  register long syscall_id asm("a7") = n;<p>  asm volatile ("scall" : "=r"(a0) : "r"(syscall_id));</p><p>  return a0;<br>}</p></span></pre><p id="31c6">System call number <em>n</em>, with no arguments, however it can return a value in A0. Note that if the game engine never changes register A0 when it handles this system call, then not surprisingly, the return value is whatever A0 was before the system call was invoked! Could be any value, really. So, it’s just better if we can get the build system to generate this based on a specification.</p><p id="f2d6">You also have to manually handle this system call in the game engine. If you ever change the system call number, everything breaks in weird ways. Because of this, it's really only for things like Linux syscall emulation, and for special things like threads, multi-processing etc. where custom system calls makes sense.</p><p id="073a">So, in order to make everyones life easier, a single system call is set aside for dynamic calls.</p><h2 id="f756">Inline assembly for an opaque dynamic call</h2><p id="a7fe">Opaque dynamic calls are reliable and fairly optimal. They are generated by the build system, and they look something like this:</p><pre><span id="ec4c">__asm__("\n\<br>.global sys_empty\n\<br>.func sys_empty\n\<br>sys_empty:\n\<br>  li t0, 0x68c73dc4\n\<br>  lui t1, %hi(sys_empty_str)\n\<br>  addi t1, t1, %lo(sys_empty_str)\n\<br>  li a7, 504\n\<br>  ecall\n\<br>  ret\n\<br>.endfunc\n\<br>.pushsection .rodata\n\<br>sys_empty_str:\n\<br>.asciz \"empty\"\n\<br>.popsection\n\<br>");</span></pre><p id="80c0">It’s hard to read, but what it does is create a global symbol of type function with the name <em>sys_empty</em>. The original specification is:</p><pre><span id="0bb4">"empty":      "void sys_empty ()"</span></pre><p id="8493">RISC-V system call ABI is exactly like the C ABI (and even if it wasn’t we will just mandate that it is!) The result is that no matter how many arguments or how many return values, it will appear as a C function call on both sides, despite going through a system call and requiring T0 and T1 for lookup and error handling. Quite low overhead, actually!</p><p id="bec5">There is some redundancy here, though. We make an opaque function call which can create a lot of pushing and popping on the caller. The function call itself is not free, and we also use T0 and T1 registers. All in all, it’s about 6 or 7 redundant instructions.</p><h2 id="d6b2">Inline dynamic calls</h2><p id="f7a0">What if we just use the system call number itself as the hash value, and then if n ≥ 600 (where regular system calls end), treat it as a dynamic call in the game engine? It’s possible because we can error out if the hash is colliding with “real” system calls at build time. We can also ditch T1 and error out with a vaguer error message, however it shouldn’t be much of an issue because when implementing an API call we should be starting out with the safe and reliable opaque version, and then switch over to the inline assembly variant only when everything works. Ideally!</p><p id="27d9">So, the idea is to generate an inline assembly function based on the information in the JSON entries. An example:</p><pre><span id="1e26">extern unsigned sys_gui_label (unsigned, const char *);</span></pre><p id="6b49">Above: The opaque dynamic call header prototype of creating a new GUI label. The generated assembly looks exactly like every other opaque wrapper function as seen before.</p><pre><span id="f05e">static inline unsigned isys_gui_label (unsigned arg0,const char * arg1) {<br>  register unsigned ra0 asm("a0");<br>  register uint32_t a7 asm("a7") = 0xf08cd072;<br>  register unsigned a0 asm("a0") = arg0;<br>  register const char * a1 asm("a1") = arg1;<br>  asm("ecall" : "=r"(ra0) : "r"(a0),"r"(a1),"m"(*a1),"r"(a7) : );<br>  return ra0;<br>}</span></pre><p id="26a7">Above: The inline assembly variant is now also generated at build-time.</p><p id="5028">Inline assembly is difficult to always get right, but we will do our best. In the GUI label case, we have an unsigned return value in A0 (named ra0), an unsigned input argument in A0 (named a0), and a C-string in a1. I decided to always split A0 into two statements since the types can differ, and we have to dereference the string in order to both lock down the register and the memory location. <a href="https://stackoverflow.com/questions/71450687/risc-v-inline-assembly-using-memory-not-behaving-correctly" rel="noopener ugc nofollow" target="_blank">I learned about “m” the hard way</a>, like many I assume.</p><p id="d606">As we can see from the inline function, the inlined version is just the same function with an <em>i</em> prepended. <em>sys_gui_label</em> becomes <em>isys_gui_label </em>and so on.</p><h2 id="2528">Benchmarks</h2><p id="e754">Inline dynamic calls benefit immensely from being called repeatedly, regardless of which call it is, while opaque dynamic calls will have a fixed overhead that cannot be optimized away.</p><p id="1233">In order to measure the real benefits, we must make a few calls sequentially, with and without arguments, and see how it relates on average to opaque dynamic calls.</p><p id="98e9">The assembly for calling the API function 4 times is as expected, optimal:</p><pre><span id="cd6a">0000000050000610 &lt;_ZL22inline_dyncall_handlerv&gt;:<br>    50000610:   68c748b7                lui     a7,0x68c74<br>    50000614:   dc48889b                addiw   a7,a7,-572 # 68c73dc4 &lt;__BSS_END__+0x18c550ac&gt;<br>    50000618:   00000073                ecall<br>    5000061c:   00000073                ecall<br>    50000620:   00000073                ecall<br>    50000624:   00000073                ecall<br>    50000628:   00008067                ret</span></pre><p id="73b8">The hash is loaded into A7. The return instruction is a part of the benchmark, but the overhead of the benchmarking is measured beforehand and subtracted out.</p><p id="b599">When mixing 8 functions, 4x with no arguments and 4x with 3 integral arguments, the inline version also looks extremely good:</p><pre><span id="77af">000000005000062c &lt;_ZL22inline_dyncall_args_x4v&gt;:<br>    5000062c:   68c74737                lui     a4,0x68c74<br>    50000630:   dc47089b                addiw   a7,a4,-572 # 68c73dc4 &lt;__BSS_END__+0x18c550a4&gt;<br>    50000634:   00000073                ecall<br>    50000638:   e82517b7                lui     a5,0xe8251<br>    5000063c:   9f07889b                addiw   a7,a5,-1552 # ffffffffe82509f0 &lt;__BSS_END__+0xffffffff98231cd0&gt;<br>    50000640:   00100513                li      a0,1<br>    50000644:   00200593                li      a1,2<br>    50000648:   00300613                li      a2,3<br>    5000064c:   00000073                ecall<br>    50000650:   dc47089b                addiw   a7,a4,-572<br>    50000654:   00000073                ecall<br>    50000658:   9f07889b                addiw   a7,a5,-1552<br>    5000065c:   00000073                ecall<br>    50000660:   dc47089b                addiw   a7,a4,-572<br>    50000664:   00000073                ecall<br>    50000668:   9f07889b                addiw   a7,a5,-1552<br>    5000066c:   00000073                ecall<br>    50000670:   dc47089b                addiw   a7,a4,-572<br>    50000674:   00000073                ecall<br>    50000678:   9f07889b                addiw   a7,a5,-1552<br>    5000067c:   00000073                ecall<br>    50000680:   00008067                ret</span></pre><p id="a9c8">Because the compiler is informed about which registers change value when performing each dynamic call, and all this is auto-generated by the build system, it will not restore arguments more than once here. Very nice! It also changes between two API calls in just one instruction. You can imagine the second test is something like this:</p><pre><span id="a1e0">void mixed_test() {<br>    Game::something();<br>    Game::some_args(1, 2, 3);<br>    Game::something();<br>    Game::some_args(1, 2, 3);<br>    Game::something();<br>    Game::some_args(1, 2, 3);<br>    Game::something();<br>    Game::some_args(1, 2, 3);<br>}</span></pre><p id="6aa0">A casual benchmark of the safe opaque calls vs the inlined assembly variants shows that the inlining is quite a bit faster:</p><figure></figure><p id="2404">The inlined variants are almost 3x faster, which is awesome to see. Most dynamic calls should be completely safe using the inlined variant, as they are usually just peddling integers. The first test is just repeated calling an empty function with no arguments, while the second one is mixing two API functions with 3 arguments.</p><p id="6fd0">I have previous benchmarks with direct system calls and LuaJIT:</p><pre><span id="ff64">libriscv: syscall overhead median 2ns     lowest: 2ns      highest: 6ns<br>luajit: syscall overhead   median 11ns    lowest: 10ns     highest: 18ns<br>lua5.3: syscall overhead   median 23ns    lowest: 21ns     highest: 33ns</span></pre><p id="23d7">So, an argument-less API call required around ~3ns when inlined, while a direct system call was only 2ns. For LuaJIT it was ~11ns. That is pretty good considering we have to do a hash lookup. Lua is also bytecode interpreted like libriscv. I suppose all of us have to do lookups to support user-friendly APIs.</p><p id="4e86">My goal is to reach direct system call overhead with these dynamic calls (aka. API function calls). It would only be possible if we could number them in a way that didn’t break when you add and remove API functions over time, without having to recompile everything. (EDIT: <a href="https://github.com/fwsGonzo/rvscript/tree/seq_dyncalls" rel="noopener ugc nofollow" target="_blank">I did this, and it was only marginally better with many downsides.</a>)</p><h2 id="e9bf">Conclusion</h2><p id="7181">So, why even optimize something that seems to be quite fast to begin with? Well, when running a real game, API functions back into the game engine is pretty much all the script is doing, apart from entering and leaving the guest VM (where the script is hosted). It must be good at this one thing, and it must be flexible and reliable. It helps when it’s part of the build system (or fully automatic, like in Lua), and it should error out as early as possible when things don’t align — preferably at build-time.</p><p id="f837">Now with the option of choosing the inlined variants as needed, I can pretty much halve the cost of whichever API functions are called often. I have two game projects going, and in the second game I am calling certain VM functions millions of times. Sometimes that warrants an algorithm change, and other times you keep calling it a million times because that’s what gives you the most creative options. 🌄</p><p id="61b9">Seeing how well the compiler can optimize the assembly is always interesting to see. Auto-generating these functions and just having them work each time in all kinds of combinations feels like an under-utilized way of getting free performance.</p><figure><figcaption>We are still working on our unnamed game! This is the work-in-progress playable overworld.</figcaption></figure><p id="ad27">-gonzo</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hugging Face, GitHub and more unite to defend open source in EU AI legislation (167 pts)]]></title>
            <link>https://venturebeat.com/ai/hugging-face-github-and-more-unite-to-defend-open-source-in-eu-ai-legislation/</link>
            <guid>36917561</guid>
            <pubDate>Sat, 29 Jul 2023 06:41:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://venturebeat.com/ai/hugging-face-github-and-more-unite-to-defend-open-source-in-eu-ai-legislation/">https://venturebeat.com/ai/hugging-face-github-and-more-unite-to-defend-open-source-in-eu-ai-legislation/</a>, See on <a href="https://news.ycombinator.com/item?id=36917561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary" role="main">

			<article id="post-2889442">
				<div>
					<div id="boilerplate_2682874">
<p><strong><em>Head over to our on-demand library to view sessions from VB Transform 2023. <a rel="noreferrer noopener" href="https://events.venturebeat.com/transform-2023/?utm_source=vb&amp;utm_medium=Boiler&amp;utm_content=landingpage&amp;utm_campaign=T23_Boiler" data-type="URL" data-id="https://events.venturebeat.com/transform-2023/?utm_source=vb&amp;utm_medium=Boiler&amp;utm_content=landingpage&amp;utm_campaign=T23_Boiler" target="_blank">Register Here</a></em></strong></p>



<hr>




</div><p>A coalition of a half-dozen open-source AI stakeholders — Hugging Face, GitHub, EleutherAI, Creative Commons, LAION and Open Future — are calling on&nbsp;EU&nbsp;policymakers to protect open source innovation as they finalize the&nbsp;<a href="https://artificialintelligenceact.eu/">EU&nbsp;AI</a><a href="https://artificialintelligenceact.eu/" target="_blank" rel="noreferrer noopener">&nbsp;</a><a href="https://artificialintelligenceact.eu/">Act</a>, which will be the world’s first comprehensive AI law. </p>



<p>In a policy paper released today, “Supporting Open Source and Open Science in the EU AI Act,” the open-source AI leaders offered recommendations “for how to ensure the AI Act works for open source” — with the “aim to ensure that open AI development practices are not confronted with obligations that are structurally impractical to comply with or that would be otherwise counterproductive.” </p>



<p>According to the paper, “overbroad obligations” that favor closed and proprietary AI development — like models from top AI companies such as OpenAI, Anthropic and Google — “threaten to disadvantage the open AI ecosystem.” </p>



<p>The paper was released as the European Commission, Council and Parliament debate the final EU AI Act in what is known as the “<a href="https://hai.stanford.edu/news/analyzing-european-union-ai-act-what-works-what-needs-improvement" target="_blank" rel="noreferrer noopener">trilogue</a>,” which began after the European Parliament&nbsp;<a href="https://www.washingtonpost.com/technology/2023/06/14/eu-parliament-approves-ai-act/" target="_blank" rel="noreferrer noopener">passed</a>&nbsp;its version of the bill on June 14. The goal is to finish and pass the AI Act by the end of 2023 before the next European Parliament elections.</p>



<div id="boilerplate_2803147">
        <h3>Event</h3>
                <div><p><span>VB Transform 2023 On-Demand</span></p>
<div id="gm0a52976">
<p><span>Did you miss a session from VB Transform 2023? Register to access the on-demand library for all of our featured sessions.</span></p>

</div>

</div>
                                                <p><a href="https://avolio.swapcard.com/Transform2023/registrations/Start?utm_source=vb&amp;utm_medium=incontent&amp;utm_content=landingpage&amp;utm_campaign=T23_incontent">
                Register Now            </a>
                        </p></div><h2 id="h-open-source-ai-innovation-is-at-stake">Open-source AI innovation is at stake</h2>



<p>Yacine Jernite, ML and society lead at <a href="https://venturebeat.com/ai/hugging-face-ceo-tells-us-house-open-source-ai-is-extremely-aligned-with-american-interests/">Hugging Face</a>, a popular hub for open-source code and models, told VentureBeat that while the policy paper is detailed, the first main point the coalition wants to make is around innovation. “We think that it is important for people to be able to choose between base models, between components, to mix and match as they need,” he said.</p>



<p>In addition, the coalition seeks to emphasize that open-source AI is necessary — and that regulation should not hinder open-source AI innovation. </p>



<p>“Openness by itself does not guarantee responsible development,” Jernite explained. “But openness and transparency [are] necessary [for] responsible governance — so it is not that openness [should be] exempt from requirements, but requirements should not preclude open development.” </p>



<h2 id="h-the-eu-ai-act-is-focused-on-application-risk">The EU AI Act is focused on application risk</h2>



<p>Since April 2021, when the European Commission proposed the first EU regulatory framework for AI, it <a href="https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence#:~:text=In%20April%202021%2C%20the%20European,mean%20more%20or%20less%20regulation.">has </a><a href="https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence#:~:text=In%20April%202021%2C%20the%20European,mean%20more%20or%20less%20regulation." target="_blank" rel="noreferrer noopener">worked</a> to focus on analyzing and classifying AI systems according to the risk they pose to users. The higher the risk level, the more regulation. </p>



<p>Peter Cihon, senior policy manager at GitHub, pointed out that as the EU Council, and subsequently the EU Parliament, developed their drafts of the AI Act, the policymakers began to look up the value chain to see how to mitigate some of these risks at an earlier stage of AI development. </p>



<p>“With that kind of step, we really redoubled our efforts to make sure that they were not inadvertently imposing expectations that might make a lot of sense for companies or well-resourced actors, but would instead place them onto open source developers who are often hobbyists, nonprofits or students,” he told VentureBeat. “Ultimately, policymakers have been quite focused on one particular value chain, one particular model, and that tends to be the API model — but that doesn’t really apply in the context of open source.” </p>



<h2 id="h-the-brussels-effect">The ‘Brussels Effect’</h2>



<p>Cihon added that he is optimistic that providing clear information about the open-source approach to development will be very useful as the trilogue, which began in June, continues. “The provisions in the sections of the act that we’re talking about have not yet come up for discussion,” he said. </p>



<p>In addition, the EU has historically been a trendsetter when it comes to tech regulation, as it was with the GDPR — in what has become known as the “Brussels Effect.” So policymakers around the world, including in the U.S., are surely taking note.</p>



<p>“It certainly starts the global regulatory conversation,” said Cihon. “So we’re optimistic that this can have benefits in DC and beyond.” In particular, he noted that Senator Chuck Schumer’s <a href="https://venturebeat.com/ai/senate-will-get-crash-course-in-ai-this-fall-says-schumer/">announcement</a> of AI-focused “Insight Forums” this fall are “a great opportunity to get more diverse input into the policymaking process than might be traditionally seen, and I’m really hopeful that open source developers will be given a seat at that table.” </p>




<p><strong>VentureBeat's mission</strong> is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. <a href="https://info.venturebeat.com/website-preference-center.html?utm_source=VBsite&amp;utm_medium=bottomBoilerplate" data-type="URL" data-id="https://info.venturebeat.com/website-preference-center.html">Discover our Briefings.</a></p><!-- Boilerplate CSS for "after" -->				</div><!-- .article-content -->

									
				
			</article><!-- #post-2889442 .article-wrapper -->


		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Marine Corps Antenna Handbook (1999) [pdf] (279 pts)]]></title>
            <link>https://www.marines.mil/Portals/1/MCRP%203-40.3C%20With%20Erratum%20z.pdf</link>
            <guid>36917424</guid>
            <pubDate>Sat, 29 Jul 2023 06:11:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.marines.mil/Portals/1/MCRP%203-40.3C%20With%20Erratum%20z.pdf">https://www.marines.mil/Portals/1/MCRP%203-40.3C%20With%20Erratum%20z.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=36917424">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Google Tries to Defend Its Web Environment Integrity Critics Slam It as Danger (158 pts)]]></title>
            <link>https://techreport.com/news/google-tries-to-defend-its-web-environment-integrity-as-critics-slam-it-as-dangerous/</link>
            <guid>36916444</guid>
            <pubDate>Sat, 29 Jul 2023 03:05:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techreport.com/news/google-tries-to-defend-its-web-environment-integrity-as-critics-slam-it-as-dangerous/">https://techreport.com/news/google-tries-to-defend-its-web-environment-integrity-as-critics-slam-it-as-dangerous/</a>, See on <a href="https://news.ycombinator.com/item?id=36916444">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		

		<p><img decoding="async" src="https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-300x200.jpg" alt="Google Tries to Defend Its Web Environment Integrity" width="1000" height="667" srcset="https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-300x200.jpg 300w, https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-1200x800.jpg 1200w, https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-150x100.jpg 150w, https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-768x512.jpg 768w, https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-1536x1024.jpg 1536w, https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-scaled.jpg 2048w" sizes="(max-width: 1000px) 100vw, 1000px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201000%20667'%3E%3C/svg%3E" data-lazy-srcset="https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-300x200.jpg 300w, https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-1200x800.jpg 1200w, https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-150x100.jpg 150w, https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-768x512.jpg 768w, https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-1536x1024.jpg 1536w, https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-scaled.jpg 2048w" data-lazy-src="https://techreport.com/wp-content/uploads/2023/07/shutterstock_552493594-300x200.jpg"></p>
<p>At a time when Google’s <a href="https://github.com/RupertBenWiser/Web-Environment-Integrity/blob/main/explainer.md">Web Environment Integrity</a> (WEI) proposal has <strong>come under heavy criticism</strong>, one of the developers working on the project said that it intends to make the web “more private and safe.”</p>
<p><em>The fraud-fighting project has fired up quite a controversy, with rising concerns that it could take away the freedom of choice from users and affect their privacy negatively.</em></p>
<p>Responding to the concerns about WEI being too <strong>dangerous and invasive of privacy</strong>, Ben Wiser, a software engineer at the Chocolate Factory, insisted that WEI is meant to address online abuse and fraud while evading the privacy harms enabled by cross-site tracking and browser fingerprinting.</p>
<h2 id="what_is_googles_web_environment_integrity_and_how_does_it_work">What Is Google’s Web Environment Integrity and How Does It Work?</h2>
<p>The Web Environment Integrity DRM proposed by <a href="https://techreport.com/news/google-2023-environmental-report-released-heres-more/">Google</a> is essentially an attestation scheme. It offers web publishers a way to integrate their websites or apps with a code that checks with a trusted party (such as Google) to verify if a client’s hardware and software stack meets certain criteria.</p>
<p><strong>Through WEI, Google aims to help websites weed out bots by verifying that the visitors on their domains are actual users.</strong></p>
<p>In an explainer published by Google, the tech giant insists on the importance of websites verifying the trustworthiness of the client environment they are run in. This includes the <a href="https://techreport.com/news/microsoft-to-power-bing-and-edge-with-openai-technology/">web browser</a> and the operating system, as well as their methods to protect data and intellectual property.</p>
<p>Here’s how Google’s proposed Web Environment Integrity would work – when users try to access a website integrated with the API, the site would request a token attesting to the client environment.</p>
<p>A third-party attester, in this case, WEI, will then test the device and sign the token provided. A browser or device that fails to pass the attestation will be marked as untrusted.</p>
<p>The token is then returned to the originating web page, following which the web server verifies the token and checks for the attester’s signature. If everyone turns out well, the user will be able to access the website.</p>
<p>However, if the token fails the test, it’s up to the website publisher to decide how the web server would respond to the signal.</p>
<p><em>While Google didn’t reveal what WEI looks for during the attestation check, Wisner insists that “WEI is not designed to single out browsers or extensions” and that it won’t block browsers that spoof their identity.</em></p>
<p>The intended use cases of the <strong>DRM include allowing game publishers</strong> to detect players cheating with the help of disallowed hardware or software. It can also help content publishers check whether their ads are being seen by actual visitors or fraudulent bots.</p>
<h2 id="why_are_people_concerned_about_wei">Why Are People Concerned About WEI?</h2>
<p>Unfortunately, the intended use of such technologies is rarely a limitation to how they’d actually be used. The technical community has expressed concern that bringing the web under a permission-based regime where a third party determines the worthiness of a user can prove to be dangerous.</p>
<div><p><strong>A big part of the reason why there is a problem is the surveillance economy, and the solution to the surveillance economy seems to be more surveillance.</strong><span>Jon von Tetzchner, Vivaldi CEO </span></p></div>
<p>WEI can potentially be used to impose restrictions on unlawful activities on the internet, such as downloading YouTube videos and other content, ad blocking, web scraping, etc.</p>


		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IBM Blue Lightning: World’s Fastest 386? (136 pts)]]></title>
            <link>https://www.os2museum.com/wp/ibm-blue-lightning-worlds-fastest-386/comment-page-1/</link>
            <guid>36916297</guid>
            <pubDate>Sat, 29 Jul 2023 02:45:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.os2museum.com/wp/ibm-blue-lightning-worlds-fastest-386/comment-page-1/">https://www.os2museum.com/wp/ibm-blue-lightning-worlds-fastest-386/comment-page-1/</a>, See on <a href="https://news.ycombinator.com/item?id=36916297">Hacker News</a></p>
Couldn't get https://www.os2museum.com/wp/ibm-blue-lightning-worlds-fastest-386/comment-page-1/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Argonne National Lab is attempting to replicate LK-99 (306 pts)]]></title>
            <link>https://www.science.org/content/article/spectacular-superconductor-claim-making-news-here-s-why-experts-are-doubtful</link>
            <guid>36916254</guid>
            <pubDate>Sat, 29 Jul 2023 02:36:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/spectacular-superconductor-claim-making-news-here-s-why-experts-are-doubtful">https://www.science.org/content/article/spectacular-superconductor-claim-making-news-here-s-why-experts-are-doubtful</a>, See on <a href="https://news.ycombinator.com/item?id=36916254">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/spectacular-superconductor-claim-making-news-here-s-why-experts-are-doubtful: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[LPython: Novel, Fast, Retargetable Python Compiler (229 pts)]]></title>
            <link>https://lpython.org/blog/2023/07/lpython-novel-fast-retargetable-python-compiler/</link>
            <guid>36916182</guid>
            <pubDate>Sat, 29 Jul 2023 02:24:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lpython.org/blog/2023/07/lpython-novel-fast-retargetable-python-compiler/">https://lpython.org/blog/2023/07/lpython-novel-fast-retargetable-python-compiler/</a>, See on <a href="https://news.ycombinator.com/item?id=36916182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
      <article role="main">
        <h2 id="about">About</h2>
<p>LPython is a Python compiler that can compile type-annotated Python code to optimized machine code. LPython offers several backends such as LLVM, C, C++, WASM, Julia and x86. LPython features quick compilation and runtime performance, as we show in the benchmarks in this blog. LPython also offers Just-In-Time (JIT) compilation and seamless interoperability with CPython.</p>
<p>We are releasing an alpha version of LPython, meaning it is expected you
encounter bugs when you use it (please report them!). You can install it using
Conda (<code>conda install -c conda-forge lpython</code>), or build from
<a href="https://github.com/lcompilers/lpython">source</a>.</p>
<p>Based on the novel Abstract Semantic Representation (ASR) shared with LFortran, LPython’s intermediate optimizations are independent of the backends and frontends. The two compilers, LPython and LFortran, share all benefits of improvements at the ASR level. “Speed” is the chief tenet of the LPython project. Our objective is to produce a compiler that both runs exceptionally fast and generates exceptionally fast code.</p>
<p>In this blog, we describe features of LPython including Ahead-of-Time (AoT) compilation, JIT compilation, and interoperability with CPython. We also showcase LPython’s performance against its competitors such as Numba and C++ via several benchmarks.</p>
<p><img src="https://lpython.org/blog/images/lcompilers_diagram.png" alt="LCompilers-Diagram"></p>
<h2 id="features-of-lpython">Features of LPython</h2>
<h3 id="backends">Backends</h3>
<p>LPython ships with the following backends, which emit final translations of the user’s input code:</p>
<ol>
<li>LLVM</li>
<li>C</li>
<li>C++</li>
<li>WASM</li>
</ol>
<p>LPython can simultaneously generate code into multiple backends from its Abstract Semantic Representation (ASR) of user code.</p>
<h3 id="phases-of-compilation">Phases of Compilation</h3>
<p>First, input code is transformed into an Abstract Syntax Tree (AST) using parsers. The AST is then transformed into an Abstract Semantic Representation (ASR), which preserves all semantic information present in the input code. ASR contains all information required by all backends in a form that is not specific to any particular backend. Then, this ASR enjoys several ASR-to-ASR passes, wherein abstract operations are transformed into concrete statements. For example, array addition in the input code denoted, <code>c = a + b</code>. The front end transforms <code>c = a + b</code> into the ASR <code>(Assign c (ArrayAdd a b))</code> via operator overloading. The <em>array_op</em> ASR-to-ASR pass transforms <code>(Assign c (ArrayAdd a b))</code> into loops:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>for</span> i0 <span>in</span> range(<span>0</span>, length_dim_0):
</span></span><span><span>    <span>for</span> i1 <span>in</span> range(<span>0</span>, length_dim_1):
</span></span><span><span>        <span>....</span>
</span></span><span><span>            <span>....</span>
</span></span><span><span>            c[i0, i1, <span>...</span>] <span>=</span> a[i0, i1, <span>...</span>] <span>+</span> b[i0, i1, <span>...</span>]
</span></span></code></pre></div><p>After applying all the ASR-to-ASR passes, LPython sends the final ASR to the backends selected by the user, via command-line arguments like, <code>--show-c</code> (generates C code), <code>--show-llvm</code> (generates LLVM code).</p>
<p>One can also see the generated C or LLVM code using the following</p>
<div><pre tabindex="0"><code data-lang="py"><span><span><span>from</span> lpython <span>import</span> i32
</span></span><span><span>
</span></span><span><span><span>def</span> <span>main</span>():
</span></span><span><span>    x: i32
</span></span><span><span>    x <span>=</span> (<span>2</span><span>+</span><span>3</span>)<span>*</span><span>5</span>
</span></span><span><span>    print(x)
</span></span><span><span>
</span></span><span><span>main()
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="c"><span><span><span>$</span> lpython examples<span>/</span>expr2.py <span>--</span>show<span>-</span>c
</span></span><span><span><span>#include</span> <span>&lt;inttypes.h&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>#include</span> <span>&lt;stdlib.h&gt;</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;stdbool.h&gt;</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;stdio.h&gt;</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;string.h&gt;</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;lfortran_intrinsics.h&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>void</span> <span>main0</span>();
</span></span><span><span><span>void</span> <span>__main____global_statements</span>();
</span></span><span><span>
</span></span><span><span><span>// Implementations
</span></span></span><span><span><span></span><span>void</span> <span>main0</span>()
</span></span><span><span>{
</span></span><span><span>    <span>int32_t</span> x;
</span></span><span><span>    x <span>=</span> (<span>2</span> <span>+</span> <span>3</span>)<span>*</span><span>5</span>;
</span></span><span><span>    <span>printf</span>(<span>"%d</span><span>\n</span><span>"</span>, x);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>void</span> <span>__main____global_statements</span>()
</span></span><span><span>{
</span></span><span><span>    <span>main0</span>();
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>int</span> <span>main</span>(<span>int</span> argc, <span>char</span><span>*</span> argv[])
</span></span><span><span>{
</span></span><span><span>    <span>_lpython_set_argv</span>(argc, argv);
</span></span><span><span>    <span>__main____global_statements</span>();
</span></span><span><span>    <span>return</span> <span>0</span>;
</span></span><span><span>}
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="llvm"><span><span><span>$</span> <span>lpython</span> <span>examples/expr</span><span>2</span>.<span>py</span> <span>--show-llvm</span>
</span></span><span><span><span>; ModuleID = 'LFortran'
</span></span></span><span><span><span></span><span>source_filename</span> = <span>"LFortran"</span>
</span></span><span><span>
</span></span><span><span>@0 = <span>private</span> <span>unnamed_addr</span> <span>constant</span> [<span>2</span> <span>x</span> <span>i8</span>] <span>c</span><span>" \00"</span>, <span>align</span> <span>1</span>
</span></span><span><span>@1 = <span>private</span> <span>unnamed_addr</span> <span>constant</span> [<span>2</span> <span>x</span> <span>i8</span>] <span>c</span><span>"\0A\00"</span>, <span>align</span> <span>1</span>
</span></span><span><span>@2 = <span>private</span> <span>unnamed_addr</span> <span>constant</span> [<span>5</span> <span>x</span> <span>i8</span>] <span>c</span><span>"%d%s\00"</span>, <span>align</span> <span>1</span>
</span></span><span><span>
</span></span><span><span><span>define</span> <span>void</span> @__module___main_____main____global_statements() {
</span></span><span><span>.entry:
</span></span><span><span>  <span>call</span> <span>void</span> @__module___main___main0()
</span></span><span><span>  <span>br</span> <span>label</span> %return
</span></span><span><span>
</span></span><span><span>return:                                           <span>; preds = %.entry
</span></span></span><span><span><span></span>  <span>ret</span> <span>void</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>define</span> <span>void</span> @__module___main___main0() {
</span></span><span><span>.entry:
</span></span><span><span>  %x = <span>alloca</span> <span>i32</span>, <span>align</span> <span>4</span>
</span></span><span><span>  <span>store</span> <span>i32</span> <span>25</span>, <span>i32</span>* %x, <span>align</span> <span>4</span>
</span></span><span><span>  %0 = <span>load</span> <span>i32</span>, <span>i32</span>* %x, <span>align</span> <span>4</span>
</span></span><span><span>  <span>call</span> <span>void</span> (<span>i8</span>*, ...) @_lfortran_printf(<span>i8</span>* <span>getelementptr</span> <span>inbounds</span> ([<span>5</span> <span>x</span> <span>i8</span>], [<span>5</span> <span>x</span> <span>i8</span>]* @2, <span>i32</span> <span>0</span>, <span>i32</span> <span>0</span>), <span>i32</span> %0, <span>i8</span>* <span>getelementptr</span> <span>inbounds</span> ([<span>2</span> <span>x</span> <span>i8</span>], [<span>2</span> <span>x</span> <span>i8</span>]* @1, <span>i32</span> <span>0</span>, <span>i32</span> <span>0</span>))
</span></span><span><span>  <span>br</span> <span>label</span> %return
</span></span><span><span>
</span></span><span><span>return:                                           <span>; preds = %.entry
</span></span></span><span><span><span></span>  <span>ret</span> <span>void</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>declare</span> <span>void</span> @_lfortran_printf(<span>i8</span>*, ...)
</span></span><span><span>
</span></span><span><span><span>define</span> <span>i32</span> @main(<span>i32</span> %0, <span>i8</span>** %1) {
</span></span><span><span>.entry:
</span></span><span><span>  <span>call</span> <span>void</span> @_lpython_set_argv(<span>i32</span> %0, <span>i8</span>** %1)
</span></span><span><span>  <span>call</span> <span>void</span> @__module___main_____main____global_statements()
</span></span><span><span>  <span>ret</span> <span>i32</span> <span>0</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>declare</span> <span>void</span> @_lpython_set_argv(<span>i32</span>, <span>i8</span>**)
</span></span></code></pre></div><h3 id="machine-independent-code-optimisations">Machine Independent Code Optimisations</h3>
<p>LPython implements several machine-independent optimisations via ASR-to-ASR passes. Some of those are listed below,</p>
<ol>
<li>Loop unrolling</li>
<li>Loop vectorisation</li>
<li>Dead code removal</li>
<li>Function call inlining</li>
<li>Transforming division to multiplication operation</li>
<li>Fused multiplication and addition</li>
</ol>
<p>All optimizations are applied via one command-line argument, <code>--fast</code>. To select individual optimizations instead, write a command-line argument like the following:</p>
<p><code>--pass=inline_function_calls,loop_unroll</code></p>
<p>Following is an examples of ASR and transformed ASR after applying the optimisations</p>
<div><pre tabindex="0"><code data-lang="py"><span><span><span>from</span> lpython <span>import</span> i32
</span></span><span><span>
</span></span><span><span><span>def</span> <span>compute_x</span>() <span>-&gt;</span> i32:
</span></span><span><span>    <span>return</span> (<span>2</span> <span>*</span> <span>3</span>) <span>**</span> <span>1</span> <span>+</span> <span>2</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>main</span>():
</span></span><span><span>    x: i32 <span>=</span> compute_x()
</span></span><span><span>    print(x)
</span></span><span><span>
</span></span><span><span>main()
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="clojure"><span><span>$ lpython examples/expr2.py --show-asr
</span></span><span><span>(<span>TranslationUnit</span>
</span></span><span><span>    (<span>SymbolTable</span>
</span></span><span><span>        <span>1</span>
</span></span><span><span>        {
</span></span><span><span>            __main__<span>:</span>
</span></span><span><span>                (<span>Module</span>
</span></span><span><span>                    (<span>SymbolTable</span>
</span></span><span><span>                        <span>2</span>
</span></span><span><span>                        {
</span></span><span><span>                            __main____global_statements<span>:</span>
</span></span><span><span>                                (<span>Function</span>
</span></span><span><span>                                    (<span>SymbolTable</span>
</span></span><span><span>                                        <span>5</span>
</span></span><span><span>                                        {
</span></span><span><span>
</span></span><span><span>                                        })
</span></span><span><span>                                    __main____global_statements
</span></span><span><span>                                    (<span>FunctionType</span>
</span></span><span><span>                                        []
</span></span><span><span>                                        ()
</span></span><span><span>                                        Source
</span></span><span><span>                                        Implementation
</span></span><span><span>                                        ()
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        []
</span></span><span><span>                                        []
</span></span><span><span>                                        .false.
</span></span><span><span>                                    )
</span></span><span><span>                                    [main]
</span></span><span><span>                                    []
</span></span><span><span>                                    [(<span>SubroutineCall</span>
</span></span><span><span>                                        <span>2</span> main
</span></span><span><span>                                        ()
</span></span><span><span>                                        []
</span></span><span><span>                                        ()
</span></span><span><span>                                    )]
</span></span><span><span>                                    ()
</span></span><span><span>                                    Public
</span></span><span><span>                                    .false.
</span></span><span><span>                                    .false.
</span></span><span><span>                                    ()
</span></span><span><span>                                ),
</span></span><span><span>                            compute_x<span>:</span>
</span></span><span><span>                                (<span>Function</span>
</span></span><span><span>                                    (<span>SymbolTable</span>
</span></span><span><span>                                        <span>3</span>
</span></span><span><span>                                        {
</span></span><span><span>                                            _lpython_return_variable<span>:</span>
</span></span><span><span>                                                (<span>Variable</span>
</span></span><span><span>                                                    <span>3</span>
</span></span><span><span>                                                    _lpython_return_variable
</span></span><span><span>                                                    []
</span></span><span><span>                                                    ReturnVar
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    Default
</span></span><span><span>                                                    (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    Source
</span></span><span><span>                                                    Public
</span></span><span><span>                                                    Required
</span></span><span><span>                                                    .false.
</span></span><span><span>                                                )
</span></span><span><span>                                        })
</span></span><span><span>                                    compute_x
</span></span><span><span>                                    (<span>FunctionType</span>
</span></span><span><span>                                        []
</span></span><span><span>                                        (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                        Source
</span></span><span><span>                                        Implementation
</span></span><span><span>                                        ()
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        []
</span></span><span><span>                                        []
</span></span><span><span>                                        .false.
</span></span><span><span>                                    )
</span></span><span><span>                                    []
</span></span><span><span>                                    []
</span></span><span><span>                                    [(<span>=</span>
</span></span><span><span>                                        (<span>Var</span> <span>3</span> _lpython_return_variable)
</span></span><span><span>                                        (<span>IntegerBinOp</span>
</span></span><span><span>                                            (<span>IntegerBinOp</span>
</span></span><span><span>                                                (<span>IntegerBinOp</span>
</span></span><span><span>                                                    (<span>IntegerConstant</span> <span>2</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                                    Mul
</span></span><span><span>                                                    (<span>IntegerConstant</span> <span>3</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                                    (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                                    (<span>IntegerConstant</span> <span>6</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                                )
</span></span><span><span>                                                Pow
</span></span><span><span>                                                (<span>IntegerConstant</span> <span>1</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                                (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                                (<span>IntegerConstant</span> <span>6</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                            )
</span></span><span><span>                                            Add
</span></span><span><span>                                            (<span>IntegerConstant</span> <span>2</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                            (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                            (<span>IntegerConstant</span> <span>8</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                        )
</span></span><span><span>                                        ()
</span></span><span><span>                                    )
</span></span><span><span>                                    (<span>Return</span>)]
</span></span><span><span>                                    (<span>Var</span> <span>3</span> _lpython_return_variable)
</span></span><span><span>                                    Public
</span></span><span><span>                                    .false.
</span></span><span><span>                                    .false.
</span></span><span><span>                                    ()
</span></span><span><span>                                ),
</span></span><span><span>                            main<span>:</span>
</span></span><span><span>                                (<span>Function</span>
</span></span><span><span>                                    (<span>SymbolTable</span>
</span></span><span><span>                                        <span>4</span>
</span></span><span><span>                                        {
</span></span><span><span>                                            x<span>:</span>
</span></span><span><span>                                                (<span>Variable</span>
</span></span><span><span>                                                    <span>4</span>
</span></span><span><span>                                                    x
</span></span><span><span>                                                    []
</span></span><span><span>                                                    Local
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    Default
</span></span><span><span>                                                    (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    Source
</span></span><span><span>                                                    Public
</span></span><span><span>                                                    Required
</span></span><span><span>                                                    .false.
</span></span><span><span>                                                )
</span></span><span><span>                                        })
</span></span><span><span>                                    main
</span></span><span><span>                                    (<span>FunctionType</span>
</span></span><span><span>                                        []
</span></span><span><span>                                        ()
</span></span><span><span>                                        Source
</span></span><span><span>                                        Implementation
</span></span><span><span>                                        ()
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        []
</span></span><span><span>                                        []
</span></span><span><span>                                        .false.
</span></span><span><span>                                    )
</span></span><span><span>                                    [compute_x]
</span></span><span><span>                                    []
</span></span><span><span>                                    [(<span>=</span>
</span></span><span><span>                                        (<span>Var</span> <span>4</span> x)
</span></span><span><span>                                        (<span>FunctionCall</span>
</span></span><span><span>                                            <span>2</span> compute_x
</span></span><span><span>                                            ()
</span></span><span><span>                                            []
</span></span><span><span>                                            (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                            ()
</span></span><span><span>                                            ()
</span></span><span><span>                                        )
</span></span><span><span>                                        ()
</span></span><span><span>                                    )
</span></span><span><span>                                    (<span>Print</span>
</span></span><span><span>                                        ()
</span></span><span><span>                                        [(<span>Var</span> <span>4</span> x)]
</span></span><span><span>                                        ()
</span></span><span><span>                                        ()
</span></span><span><span>                                    )]
</span></span><span><span>                                    ()
</span></span><span><span>                                    Public
</span></span><span><span>                                    .false.
</span></span><span><span>                                    .false.
</span></span><span><span>                                    ()
</span></span><span><span>                                )
</span></span><span><span>                        })
</span></span><span><span>                    __main__
</span></span><span><span>                    []
</span></span><span><span>                    .false.
</span></span><span><span>                    .false.
</span></span><span><span>                ),
</span></span><span><span>            main_program<span>:</span>
</span></span><span><span>                (<span>Program</span>
</span></span><span><span>                    (<span>SymbolTable</span>
</span></span><span><span>                        <span>6</span>
</span></span><span><span>                        {
</span></span><span><span>                            __main____global_statements<span>:</span>
</span></span><span><span>                                (<span>ExternalSymbol</span>
</span></span><span><span>                                    <span>6</span>
</span></span><span><span>                                    __main____global_statements
</span></span><span><span>                                    <span>2</span> __main____global_statements
</span></span><span><span>                                    __main__
</span></span><span><span>                                    []
</span></span><span><span>                                    __main____global_statements
</span></span><span><span>                                    Public
</span></span><span><span>                                )
</span></span><span><span>                        })
</span></span><span><span>                    main_program
</span></span><span><span>                    [__main__]
</span></span><span><span>                    [(<span>SubroutineCall</span>
</span></span><span><span>                        <span>6</span> __main____global_statements
</span></span><span><span>                        <span>2</span> __main____global_statements
</span></span><span><span>                        []
</span></span><span><span>                        ()
</span></span><span><span>                    )]
</span></span><span><span>                )
</span></span><span><span>        })
</span></span><span><span>    []
</span></span><span><span>)
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="clojure"><span><span>$ lpython examples/expr2.py --show-asr --pass=inline_function_calls,unused_functions
</span></span><span><span>(<span>TranslationUnit</span>
</span></span><span><span>    (<span>SymbolTable</span>
</span></span><span><span>        <span>1</span>
</span></span><span><span>        {
</span></span><span><span>            __main__<span>:</span>
</span></span><span><span>                (<span>Module</span>
</span></span><span><span>                    (<span>SymbolTable</span>
</span></span><span><span>                        <span>2</span>
</span></span><span><span>                        {
</span></span><span><span>                            __main____global_statements<span>:</span>
</span></span><span><span>                                (<span>Function</span>
</span></span><span><span>                                    (<span>SymbolTable</span>
</span></span><span><span>                                        <span>5</span>
</span></span><span><span>                                        {
</span></span><span><span>
</span></span><span><span>                                        })
</span></span><span><span>                                    __main____global_statements
</span></span><span><span>                                    (<span>FunctionType</span>
</span></span><span><span>                                        []
</span></span><span><span>                                        ()
</span></span><span><span>                                        Source
</span></span><span><span>                                        Implementation
</span></span><span><span>                                        ()
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        []
</span></span><span><span>                                        []
</span></span><span><span>                                        .false.
</span></span><span><span>                                    )
</span></span><span><span>                                    [main]
</span></span><span><span>                                    []
</span></span><span><span>                                    [(<span>SubroutineCall</span>
</span></span><span><span>                                        <span>2</span> main
</span></span><span><span>                                        ()
</span></span><span><span>                                        []
</span></span><span><span>                                        ()
</span></span><span><span>                                    )]
</span></span><span><span>                                    ()
</span></span><span><span>                                    Public
</span></span><span><span>                                    .false.
</span></span><span><span>                                    .false.
</span></span><span><span>                                    ()
</span></span><span><span>                                ),
</span></span><span><span>                            main<span>:</span>
</span></span><span><span>                                (<span>Function</span>
</span></span><span><span>                                    (<span>SymbolTable</span>
</span></span><span><span>                                        <span>4</span>
</span></span><span><span>                                        {
</span></span><span><span>                                            _lpython_return_variable_compute_x<span>:</span>
</span></span><span><span>                                                (<span>Variable</span>
</span></span><span><span>                                                    <span>4</span>
</span></span><span><span>                                                    _lpython_return_variable_compute_x
</span></span><span><span>                                                    []
</span></span><span><span>                                                    Local
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    Default
</span></span><span><span>                                                    (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    Source
</span></span><span><span>                                                    Public
</span></span><span><span>                                                    Required
</span></span><span><span>                                                    .false.
</span></span><span><span>                                                ),
</span></span><span><span>                                            x<span>:</span>
</span></span><span><span>                                                (<span>Variable</span>
</span></span><span><span>                                                    <span>4</span>
</span></span><span><span>                                                    x
</span></span><span><span>                                                    []
</span></span><span><span>                                                    Local
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    Default
</span></span><span><span>                                                    (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                                    ()
</span></span><span><span>                                                    Source
</span></span><span><span>                                                    Public
</span></span><span><span>                                                    Required
</span></span><span><span>                                                    .false.
</span></span><span><span>                                                ),
</span></span><span><span>                                            <span>~</span>empty_block<span>:</span>
</span></span><span><span>                                                (<span>Block</span>
</span></span><span><span>                                                    (<span>SymbolTable</span>
</span></span><span><span>                                                        <span>7</span>
</span></span><span><span>                                                        {
</span></span><span><span>
</span></span><span><span>                                                        })
</span></span><span><span>                                                    <span>~</span>empty_block
</span></span><span><span>                                                    []
</span></span><span><span>                                                )
</span></span><span><span>                                        })
</span></span><span><span>                                    main
</span></span><span><span>                                    (<span>FunctionType</span>
</span></span><span><span>                                        []
</span></span><span><span>                                        ()
</span></span><span><span>                                        Source
</span></span><span><span>                                        Implementation
</span></span><span><span>                                        ()
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        .false.
</span></span><span><span>                                        []
</span></span><span><span>                                        []
</span></span><span><span>                                        .false.
</span></span><span><span>                                    )
</span></span><span><span>                                    []
</span></span><span><span>                                    []
</span></span><span><span>                                    [(<span>=</span>
</span></span><span><span>                                        (<span>Var</span> <span>4</span> _lpython_return_variable_compute_x)
</span></span><span><span>                                        (<span>IntegerBinOp</span>
</span></span><span><span>                                            (<span>IntegerBinOp</span>
</span></span><span><span>                                                (<span>IntegerBinOp</span>
</span></span><span><span>                                                    (<span>IntegerConstant</span> <span>2</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                                    Mul
</span></span><span><span>                                                    (<span>IntegerConstant</span> <span>3</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                                    (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                                    (<span>IntegerConstant</span> <span>6</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                                )
</span></span><span><span>                                                Pow
</span></span><span><span>                                                (<span>IntegerConstant</span> <span>1</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                                (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                                (<span>IntegerConstant</span> <span>6</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                            )
</span></span><span><span>                                            Add
</span></span><span><span>                                            (<span>IntegerConstant</span> <span>2</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                            (<span>Integer</span> <span>4</span>)
</span></span><span><span>                                            (<span>IntegerConstant</span> <span>8</span> (<span>Integer</span> <span>4</span>))
</span></span><span><span>                                        )
</span></span><span><span>                                        ()
</span></span><span><span>                                    )
</span></span><span><span>                                    (<span>GoTo</span>
</span></span><span><span>                                        <span>1</span>
</span></span><span><span>                                        __1
</span></span><span><span>                                    )
</span></span><span><span>                                    (<span>BlockCall</span>
</span></span><span><span>                                        <span>1</span>
</span></span><span><span>                                        <span>4</span> <span>~</span>empty_block
</span></span><span><span>                                    )
</span></span><span><span>                                    (<span>=</span>
</span></span><span><span>                                        (<span>Var</span> <span>4</span> x)
</span></span><span><span>                                        (<span>Var</span> <span>4</span> _lpython_return_variable_compute_x)
</span></span><span><span>                                        ()
</span></span><span><span>                                    )
</span></span><span><span>                                    (<span>Print</span>
</span></span><span><span>                                        ()
</span></span><span><span>                                        [(<span>Var</span> <span>4</span> x)]
</span></span><span><span>                                        ()
</span></span><span><span>                                        ()
</span></span><span><span>                                    )]
</span></span><span><span>                                    ()
</span></span><span><span>                                    Public
</span></span><span><span>                                    .false.
</span></span><span><span>                                    .false.
</span></span><span><span>                                    ()
</span></span><span><span>                                )
</span></span><span><span>                        })
</span></span><span><span>                    __main__
</span></span><span><span>                    []
</span></span><span><span>                    .false.
</span></span><span><span>                    .false.
</span></span><span><span>                ),
</span></span><span><span>            main_program<span>:</span>
</span></span><span><span>                (<span>Program</span>
</span></span><span><span>                    (<span>SymbolTable</span>
</span></span><span><span>                        <span>6</span>
</span></span><span><span>                        {
</span></span><span><span>                            __main____global_statements<span>:</span>
</span></span><span><span>                                (<span>ExternalSymbol</span>
</span></span><span><span>                                    <span>6</span>
</span></span><span><span>                                    __main____global_statements
</span></span><span><span>                                    <span>2</span> __main____global_statements
</span></span><span><span>                                    __main__
</span></span><span><span>                                    []
</span></span><span><span>                                    __main____global_statements
</span></span><span><span>                                    Public
</span></span><span><span>                                )
</span></span><span><span>                        })
</span></span><span><span>                    main_program
</span></span><span><span>                    [__main__]
</span></span><span><span>                    [(<span>SubroutineCall</span>
</span></span><span><span>                        <span>6</span> __main____global_statements
</span></span><span><span>                        <span>2</span> __main____global_statements
</span></span><span><span>                        []
</span></span><span><span>                        ()
</span></span><span><span>                    )]
</span></span><span><span>                )
</span></span><span><span>        })
</span></span><span><span>    []
</span></span><span><span>)
</span></span></code></pre></div><h3 id="ahead-of-time-aot-compilation">Ahead-of-Time (AoT) compilation</h3>
<p>LPython naturally acts as a Python compiler. By default, if no backend is provided it compiles type-annotated user input code to LLVM, which generates binary final output. Consider the following small example:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> lpython <span>import</span> i32, i64
</span></span><span><span>
</span></span><span><span><span>def</span> <span>list_bench</span>(n: i32) <span>-&gt;</span> i64:
</span></span><span><span>    x: list[i32]
</span></span><span><span>    x <span>=</span> []
</span></span><span><span>    i: i32
</span></span><span><span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        x<span>.</span>append(i)
</span></span><span><span>
</span></span><span><span>    s: i64 <span>=</span> i64(<span>0</span>)
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        s <span>+=</span> i64(x[i])
</span></span><span><span>    <span>return</span> s
</span></span><span><span>
</span></span><span><span>res: i64 <span>=</span> list_bench(<span>500_000</span>)
</span></span><span><span>print(res)
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="zsh"><span><span><span>(</span>lp<span>)</span> 18:58:29:~/lpython_project/lpython % lpython /Users/czgdp1807/lpython_project/debug.py -o a.out
</span></span><span><span><span>(</span>lp<span>)</span> 18:58:31:~/lpython_project/lpython % time ./a.out
</span></span><span><span><span>124999750000</span>
</span></span><span><span>./a.out  0.01s user 0.00s system 89% cpu 0.012 total
</span></span></code></pre></div><p>You can see that it’s very fast. It’s still plenty fast with the C backend via the command-line argument <code>--backend=c</code>:</p>
<div><pre tabindex="0"><code data-lang="zsh"><span><span>% time lpython /Users/czgdp1807/lpython_project/debug.py --backend<span>=</span>c
</span></span><span><span><span>124999750000</span>
</span></span><span><span>lpython /Users/czgdp1807/lpython_project/debug.py --backend<span>=</span>c  0.12s user 0.02s system 100% cpu 0.144 total
</span></span></code></pre></div><p>Note that time lpython <code>/Users/czgdp1807/lpython_project/debug.py --backend=c</code> includes both the compilation time of LPython and the execution time of the binary. The sum of both is so fast that one can afford to compile on every change to the input files. :D.</p>
<h3 id="just-in-time-compilation">Just-In-Time Compilation</h3>
<p>Just-in-time compilation in LPython requires only decorating Python function with <code>@lpython</code>. The decorator takes an option for specifying the desired backend, as in, <code>@lpython(backend="c")</code> or <code>@lpython(backend="llvm")</code>. Only C is supported at present; LLVM and others will be added in the near future. The decorator also propagates backend-specific options. For example</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>@lpython</span>(backend<span>=</span><span>"c"</span>,
</span></span><span><span>         backend_optimization_flags<span>=</span>[<span>"-ffast-math"</span>,
</span></span><span><span>                                     <span>"-funroll-loops"</span>,
</span></span><span><span>                                     <span>"-O1"</span>])
</span></span></code></pre></div><p>Note that by default C backend is used without any optimisation flags.</p>
<p>A small example of JIT compilation in LPython (notice the LPython type annotations with the variables),</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> lpython <span>import</span> i32, i64, lpython
</span></span><span><span>
</span></span><span><span><span>@lpython</span>(backend<span>=</span><span>"c"</span>, backend_optimisation_flags<span>=</span>[<span>"-ffast-math"</span>, <span>"-funroll-loops"</span>, <span>"-O1"</span>])
</span></span><span><span><span>def</span> <span>list_bench</span>(n: i32) <span>-&gt;</span> i64:
</span></span><span><span>    x: list[i32]
</span></span><span><span>    x <span>=</span> []
</span></span><span><span>    i: i32
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        x<span>.</span>append(i)
</span></span><span><span>    s: i64 <span>=</span> i64(<span>0</span>)
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        s <span>+=</span> i64(x[i])
</span></span><span><span>    <span>return</span> s
</span></span><span><span>
</span></span><span><span>res <span>=</span> list_bench(<span>1</span>) <span># compiles `list_bench` to a shared binary in the first call</span>
</span></span><span><span>res <span>=</span> list_bench(<span>500_000</span>) <span># calls the compiled `list_bench`</span>
</span></span><span><span>print(res)
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="zsh"><span><span><span>(</span>lp<span>)</span> 18:46:33:~/lpython_project/lpython % python /Users/czgdp1807/lpython_project/debug.py
</span></span><span><span><span>124999750000</span>
</span></span></code></pre></div><p>We show below in the benchmarks how LPython compares to Numba, which also has JIT compilation.</p>
<h3 id="inter-operability-with-cpython">Inter-operability with CPython</h3>
<p>Access any library implemented using CPython, via the <code>@pythoncall</code> decorator. For example,</p>
<p><strong>email_extractor.py</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># get_email is implemented in email_extractor_util.py which is intimated to</span>
</span></span><span><span><span># LPython by specifiying the file as module in `@pythoncall` decorator</span>
</span></span><span><span><span>@pythoncall</span>(module<span>=</span><span>"email_extractor_util"</span>)
</span></span><span><span><span>def</span> <span>get_email</span>(text: str) <span>-&gt;</span> str:
</span></span><span><span>    <span>pass</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>test</span>():
</span></span><span><span>    text: str <span>=</span> <span>"Hello, my email id is lpython@lcompilers.org."</span>
</span></span><span><span>    print(get_email(text))
</span></span><span><span>
</span></span><span><span>test()
</span></span></code></pre></div><p><strong>email_extractor_util.py</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># Implement `get_email` using `re` CPython library</span>
</span></span><span><span><span>def</span> <span>get_email</span>(text):
</span></span><span><span>    <span>import</span> re
</span></span><span><span>    <span># Regular expression patterns</span>
</span></span><span><span>    email_pattern <span>=</span> <span>r</span><span>"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b"</span>
</span></span><span><span>
</span></span><span><span>    <span># Matching email addresses</span>
</span></span><span><span>    email_matches <span>=</span> re<span>.</span>findall(email_pattern, text)
</span></span><span><span>
</span></span><span><span>    <span>return</span> email_matches[<span>0</span>]
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="zsh"><span><span><span>(</span>lp<span>)</span> 18:54:13:~/lpython_project % lpython email_extractor.py --backend<span>=</span>c --enable-cpython
</span></span><span><span>lpython@lcompilers.org
</span></span></code></pre></div><p><em>Note</em>: The <code>@pythoncall</code> and <code>@lpython</code> decorators are presently supported with just the <code>C</code> backend but eventually will work with the LLVM backend and that’s work in progress.</p>
<h2 id="benchmarks-and-demos">Benchmarks and Demos</h2>
<p>In this section, we show how LPython performs compares to competitors on each feature LPython offers. We cover JIT compilation, Interoperability with CPython, and AoT compilation.</p>
<h3 id="just-in-time-jit-compilation">Just-In-Time (JIT) Compilation</h3>
<p>We compare JIT compilation of LPython to Numba on <strong>summation of all the elements of a 1-D array</strong>, <strong>pointwise multiplication of two 1-D arrays</strong>, <strong>insertion sort on lists</strong>, and <strong>quadratic-time implementation of the Dijkstra shortest-path algorithm on a fully connected graph</strong>.</p>
<p><strong>System Information</strong></p>
<table>
<thead>
<tr>
<th>Compiler</th>
<th>Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numba</td>
<td>0.57.1</td>
</tr>
<tr>
<td>LPython</td>
<td>0.19.0</td>
</tr>
<tr>
<td>Python</td>
<td>3.10.4</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Summation of all the elements of a 1-D array</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> numpy <span>import</span> float64, arange, empty
</span></span><span><span><span>from</span> lpython <span>import</span> i32, f64, lpython
</span></span><span><span><span>import</span> timeit
</span></span><span><span><span>from</span> numba <span>import</span> njit
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>@lpython</span>(backend<span>=</span><span>"c"</span>, backend_optimisation_flags<span>=</span>[<span>"-ffast-math"</span>, <span>"-funroll-loops"</span>, <span>"-O3"</span>])
</span></span><span><span><span>def</span> <span>fast_sum</span>(n: i32, x: f64[:], res: f64[:]) <span>-&gt;</span> f64:
</span></span><span><span>    s: f64 <span>=</span> <span>0.0</span>
</span></span><span><span>    res[<span>0</span>] <span>=</span> <span>0.0</span>
</span></span><span><span>    i: i32
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        s <span>+=</span> x[i]
</span></span><span><span>    res[<span>0</span>] <span>=</span> s
</span></span><span><span>    <span>return</span> s
</span></span><span><span>
</span></span><span><span><span>@njit</span>(fastmath<span>=</span><span>True</span>)
</span></span><span><span><span>def</span> <span>fast_sum_numba</span>(n, x, res):
</span></span><span><span>    s <span>=</span> <span>0.0</span>
</span></span><span><span>    res[<span>0</span>] <span>=</span> <span>0.0</span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        s <span>+=</span> x[i]
</span></span><span><span>    res[<span>0</span>] <span>=</span> s
</span></span><span><span>    <span>return</span> s
</span></span><span><span>
</span></span><span><span><span>def</span> <span>test</span>():
</span></span><span><span>    n <span>=</span> <span>100_000_000</span>
</span></span><span><span>    x <span>=</span> arange(n, dtype<span>=</span>float64)
</span></span><span><span>    x1 <span>=</span> arange(<span>0</span>, dtype<span>=</span>float64)
</span></span><span><span>    res <span>=</span> empty(<span>1</span>, dtype<span>=</span>float64)
</span></span><span><span>    res_numba <span>=</span> empty(<span>1</span>, dtype<span>=</span>float64)
</span></span><span><span>
</span></span><span><span>    print(<span>"LPython compilation time:"</span>, timeit<span>.</span>timeit(<span>lambda</span>: fast_sum(<span>0</span>, x1, res), number<span>=</span><span>1</span>))
</span></span><span><span>    print(<span>"LPython execution time: "</span>, min(timeit<span>.</span>repeat(<span>lambda</span>: fast_sum(n, x, res), repeat<span>=</span><span>10</span>, number<span>=</span><span>1</span>)))
</span></span><span><span>    <span>assert</span> res[<span>0</span>] <span>==</span> <span>4999999950000000.0</span>
</span></span><span><span>
</span></span><span><span>    print(<span>"Numba compilation time:"</span>, timeit<span>.</span>timeit(<span>lambda</span>: fast_sum_numba(<span>0</span>, x1, res_numba), number<span>=</span><span>1</span>))
</span></span><span><span>    print(<span>"Numba execution time:"</span>, min(timeit<span>.</span>repeat(<span>lambda</span>: fast_sum_numba(n, x, res_numba), repeat<span>=</span><span>10</span>, number<span>=</span><span>1</span>)))
</span></span><span><span>    <span>assert</span> res_numba[<span>0</span>] <span>==</span> <span>4999999950000000.0</span>
</span></span><span><span>
</span></span><span><span>test()
</span></span></code></pre></div><table>
<thead>
<tr>
<th>Compiler</th>
<th>Compilation Time (s)</th>
<th>System</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numba</td>
<td>0.10</td>
<td>Apple M1 MBP 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.20</td>
<td>Apple M1 MBP 2020</td>
<td>2.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.08</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.53</td>
<td>Apple M1 Pro MBP 2021</td>
<td>6.62</td>
</tr>
<tr>
<td>Numba</td>
<td>0.15</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.40</td>
<td>Apple M1 2020</td>
<td>2.67</td>
</tr>
<tr>
<td>Numba</td>
<td>0.20</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.32</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.60</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<table>
<thead>
<tr>
<th>Compiler</th>
<th>Execution Time (s)</th>
<th>System</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>LPython</td>
<td>0.013</td>
<td>Apple M1 MBP 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.024</td>
<td>Apple M1 MBP 2020</td>
<td>1.84</td>
</tr>
<tr>
<td>LPython</td>
<td>0.013</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.023</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.77</td>
</tr>
<tr>
<td>LPython</td>
<td>0.014</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.024</td>
<td>Apple M1 2020</td>
<td>1.71</td>
</tr>
<tr>
<td>LPython</td>
<td>0.048</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.048</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Pointwise multiplication of two 1-D arrays</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> numpy <span>import</span> int64, arange, empty
</span></span><span><span><span>from</span> lpython <span>import</span> i32, i64, lpython
</span></span><span><span><span>import</span> timeit
</span></span><span><span><span>from</span> numba <span>import</span> njit
</span></span><span><span>
</span></span><span><span><span>@lpython</span>(backend<span>=</span><span>"c"</span>, backend_optimisation_flags<span>=</span>[<span>"-ffast-math"</span>, <span>"-funroll-loops"</span>, <span>"-O3"</span>])
</span></span><span><span><span>def</span> <span>multiply_arrays</span>(n: i32, x: i64[:], y: i64[:], z: i64[:]):
</span></span><span><span>    i: i32
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        z[i] <span>=</span> x[i] <span>*</span> y[i]
</span></span><span><span>
</span></span><span><span><span>@njit</span>(fastmath<span>=</span><span>True</span>)
</span></span><span><span><span>def</span> <span>multiply_arrays_numba</span>(n, x, y, z):
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        z[i] <span>=</span> x[i] <span>*</span> y[i]
</span></span><span><span>
</span></span><span><span><span>def</span> <span>test</span>():
</span></span><span><span>    n <span>=</span> <span>100_000_000</span>
</span></span><span><span>    x1 <span>=</span> arange(<span>0</span>, dtype<span>=</span>int64)
</span></span><span><span>    y1 <span>=</span> arange(<span>0</span>, dtype<span>=</span>int64)
</span></span><span><span>    res1 <span>=</span> arange(<span>0</span>, dtype<span>=</span>int64)
</span></span><span><span>    x <span>=</span> arange(n, dtype<span>=</span>int64)
</span></span><span><span>    y <span>=</span> arange(n, dtype<span>=</span>int64) <span>+</span> <span>2</span>
</span></span><span><span>    res <span>=</span> empty(n, dtype<span>=</span>int64)
</span></span><span><span>    res_numba <span>=</span> empty(n, dtype<span>=</span>int64)
</span></span><span><span>    print(<span>"LPython compilation time:"</span>, timeit<span>.</span>timeit(<span>lambda</span>: multiply_arrays(<span>0</span>, x1, y1, res1), number<span>=</span><span>1</span>))
</span></span><span><span>    print(<span>"LPython execution time:"</span>, min(timeit<span>.</span>repeat(<span>lambda</span>: multiply_arrays(n, x, y, res), repeat<span>=</span><span>10</span>, number<span>=</span><span>1</span>)))
</span></span><span><span>    <span>assert</span> sum(res <span>-</span> x <span>*</span> y) <span>==</span> <span>0</span>
</span></span><span><span>
</span></span><span><span>    print(<span>"Numba compilation time:"</span>, timeit<span>.</span>timeit(<span>lambda</span>: multiply_arrays_numba(<span>0</span>, x1, y1, res1), number<span>=</span><span>1</span>))
</span></span><span><span>    print(<span>"Numba execution time:"</span>, min(timeit<span>.</span>repeat(<span>lambda</span>: multiply_arrays_numba(n, x, y, res_numba), repeat<span>=</span><span>10</span>, number<span>=</span><span>1</span>)))
</span></span><span><span>    <span>assert</span> sum(res_numba <span>-</span> x <span>*</span> y) <span>==</span> <span>0</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>test()
</span></span></code></pre></div><table>
<thead>
<tr>
<th>Compiler</th>
<th>Compilation Time (s)</th>
<th>System</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numba</td>
<td>0.11</td>
<td>Apple M1 MBP 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.50</td>
<td>Apple M1 MBP 2020</td>
<td>4.54</td>
</tr>
<tr>
<td>Numba</td>
<td>0.09</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.60</td>
<td>Apple M1 Pro MBP 2021</td>
<td>6.67</td>
</tr>
<tr>
<td>Numba</td>
<td>0.11</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.46</td>
<td>Apple M1 2020</td>
<td>4.18</td>
</tr>
<tr>
<td>Numba</td>
<td>0.21</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.31</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.48</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<table>
<thead>
<tr>
<th>Compiler</th>
<th>Execution Time (s)</th>
<th>System</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numba</td>
<td>0.041</td>
<td>Apple M1 MBP 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.042</td>
<td>Apple M1 MBP 2020</td>
<td>1.02</td>
</tr>
<tr>
<td>Numba</td>
<td>0.037</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.040</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.08</td>
</tr>
<tr>
<td>Numba</td>
<td>0.042</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.042</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.21</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.21</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Insertion sort on lists</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> lpython <span>import</span> i32, lpython
</span></span><span><span><span>import</span> timeit
</span></span><span><span><span>from</span> numba <span>import</span> njit
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>@lpython</span>(backend<span>=</span><span>"c"</span>, backend_optimisation_flags<span>=</span>[<span>"-ffast-math"</span>, <span>"-funroll-loops"</span>, <span>"-O3"</span>])
</span></span><span><span><span>def</span> <span>test_list_sort</span>(size: i32):
</span></span><span><span>    i: i32
</span></span><span><span>    x: list[i32]
</span></span><span><span>    x <span>=</span> []
</span></span><span><span>    <span>for</span> i <span>in</span> range(size):
</span></span><span><span>        x<span>.</span>append(size <span>-</span> i)
</span></span><span><span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(<span>1</span>, size):
</span></span><span><span>        key: i32 <span>=</span> x[i]
</span></span><span><span>        j: i32 <span>=</span> i <span>-</span> <span>1</span>
</span></span><span><span>        <span>while</span> j <span>&gt;=</span> <span>0</span> <span>and</span> key <span>&lt;</span> x[j] :
</span></span><span><span>            x[j <span>+</span> <span>1</span>] <span>=</span> x[j]
</span></span><span><span>            j <span>-=</span> <span>1</span>
</span></span><span><span>        x[j <span>+</span> <span>1</span>] <span>=</span> key
</span></span><span><span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(<span>1</span>, size):
</span></span><span><span>        <span>assert</span> x[i <span>-</span> <span>1</span>] <span>&lt;</span> x[i]
</span></span><span><span>
</span></span><span><span><span>@njit</span>(fastmath<span>=</span><span>True</span>)
</span></span><span><span><span>def</span> <span>test_list_sort_numba</span>(size):
</span></span><span><span>    x <span>=</span> []
</span></span><span><span>    <span>for</span> i <span>in</span> range(size):
</span></span><span><span>        x<span>.</span>append(size <span>-</span> i)
</span></span><span><span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(<span>1</span>, size):
</span></span><span><span>        key <span>=</span> x[i]
</span></span><span><span>        j <span>=</span> i <span>-</span> <span>1</span>
</span></span><span><span>        <span>while</span> j <span>&gt;=</span> <span>0</span> <span>and</span> key <span>&lt;</span> x[j] :
</span></span><span><span>            x[j <span>+</span> <span>1</span>] <span>=</span> x[j]
</span></span><span><span>            j <span>-=</span> <span>1</span>
</span></span><span><span>        x[j <span>+</span> <span>1</span>] <span>=</span> key
</span></span><span><span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(<span>1</span>, size):
</span></span><span><span>        <span>assert</span> x[i <span>-</span> <span>1</span>] <span>&lt;</span> x[i]
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>test</span>():
</span></span><span><span>    n <span>=</span> <span>25000</span>
</span></span><span><span>    print(<span>"LPython compilation time:"</span>, timeit<span>.</span>timeit(<span>lambda</span>: test_list_sort(<span>0</span>), number<span>=</span><span>1</span>))
</span></span><span><span>    print(<span>"LPython execution time:"</span>, min(timeit<span>.</span>repeat(<span>lambda</span>: test_list_sort(n), repeat<span>=</span><span>10</span>, number<span>=</span><span>1</span>)))
</span></span><span><span>
</span></span><span><span>    print(<span>"Numba compilation time:"</span>, timeit<span>.</span>timeit(<span>lambda</span>: test_list_sort_numba(<span>0</span>), number<span>=</span><span>1</span>))
</span></span><span><span>    print(<span>"Numba execution time:"</span>, min(timeit<span>.</span>repeat(<span>lambda</span>: test_list_sort_numba(n), repeat<span>=</span><span>10</span>, number<span>=</span><span>1</span>)))
</span></span><span><span>
</span></span><span><span>test()
</span></span></code></pre></div><table>
<thead>
<tr>
<th>Compiler</th>
<th>Compilation Time (s)</th>
<th>System</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numba</td>
<td>0.13</td>
<td>Apple M1 MBP 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.20</td>
<td>Apple M1 MBP 2020</td>
<td>1.54</td>
</tr>
<tr>
<td>Numba</td>
<td>0.13</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.60</td>
<td>Apple M1 Pro MBP 2021</td>
<td>4.62</td>
</tr>
<tr>
<td>Numba</td>
<td>0.13</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.42</td>
<td>Apple M1 2020</td>
<td>3.23</td>
</tr>
<tr>
<td>Numba</td>
<td>0.35</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.37</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.06</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<table>
<thead>
<tr>
<th>Compiler</th>
<th>Execution Time (s)</th>
<th>System</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>LPython</td>
<td>0.11</td>
<td>Apple M1 MBP 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.39</td>
<td>Apple M1 MBP 2020</td>
<td>3.54</td>
</tr>
<tr>
<td>LPython</td>
<td>0.11</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.39</td>
<td>Apple M1 Pro MBP 2021</td>
<td>3.54</td>
</tr>
<tr>
<td>LPython</td>
<td>0.20</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.39</td>
<td>Apple M1 2020</td>
<td>1.95</td>
</tr>
<tr>
<td>LPython</td>
<td>0.10</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.36</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>3.60</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Quadratic-time implementation of the Dijkstra shortest-path algorithm on a fully connected graph</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> lpython <span>import</span> i32, lpython
</span></span><span><span><span>from</span> numpy <span>import</span> empty, int32
</span></span><span><span><span>from</span> numba <span>import</span> njit
</span></span><span><span><span>import</span> timeit
</span></span><span><span>
</span></span><span><span><span>@lpython</span>(backend<span>=</span><span>"c"</span>, backend_optimisation_flags<span>=</span>[<span>"-ffast-math"</span>, <span>"-funroll-loops"</span>, <span>"-O1"</span>])
</span></span><span><span><span>def</span> <span>dijkstra_shortest_path</span>(n: i32, source: i32, dist_sum: i32[:]):
</span></span><span><span>    i: i32; j: i32; v: i32; u: i32; mindist: i32; alt: i32; dummy: i32;
</span></span><span><span>    graph: dict[i32, i32] <span>=</span> {}
</span></span><span><span>    dist: dict[i32, i32] <span>=</span> {}
</span></span><span><span>    prev: dict[i32, i32] <span>=</span> {}
</span></span><span><span>    visited: dict[i32, bool] <span>=</span> {}
</span></span><span><span>    Q: list[i32] <span>=</span> []
</span></span><span><span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        <span>for</span> j <span>in</span> range(n):
</span></span><span><span>            graph[n <span>*</span> i <span>+</span> j] <span>=</span> abs(i <span>-</span> j)
</span></span><span><span>
</span></span><span><span>    <span>for</span> v <span>in</span> range(n):
</span></span><span><span>        dist[v] <span>=</span> <span>2147483647</span>
</span></span><span><span>        prev[v] <span>=</span> <span>-</span><span>1</span>
</span></span><span><span>        Q<span>.</span>append(v)
</span></span><span><span>        visited[v] <span>=</span> <span>False</span>
</span></span><span><span>    dist[source] <span>=</span> <span>0</span>
</span></span><span><span>
</span></span><span><span>    <span>while</span> len(Q) <span>&gt;</span> <span>0</span>:
</span></span><span><span>        u <span>=</span> <span>-</span><span>1</span>
</span></span><span><span>        mindist <span>=</span> <span>2147483647</span>
</span></span><span><span>        <span>for</span> i <span>in</span> range(len(Q)):
</span></span><span><span>            <span>if</span> mindist <span>&gt;</span> dist[Q[i]]:
</span></span><span><span>                mindist <span>=</span> dist[Q[i]]
</span></span><span><span>                u <span>=</span> Q[i]
</span></span><span><span>        Q<span>.</span>remove(u)
</span></span><span><span>        visited[u] <span>=</span> <span>True</span>
</span></span><span><span>
</span></span><span><span>        <span>for</span> v <span>in</span> range(n):
</span></span><span><span>            <span>if</span> v <span>!=</span> u <span>and</span> <span>not</span> visited[v]:
</span></span><span><span>                alt <span>=</span> dist[u] <span>+</span> graph[n <span>*</span> u <span>+</span> v]
</span></span><span><span>
</span></span><span><span>                <span>if</span> alt <span>&lt;</span> dist[v]:
</span></span><span><span>                    dist[v] <span>=</span> alt
</span></span><span><span>                    prev[v] <span>=</span> u
</span></span><span><span>
</span></span><span><span>    dist_sum[<span>0</span>] <span>=</span> <span>0</span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        dist_sum[<span>0</span>] <span>+=</span> dist[i]
</span></span><span><span>
</span></span><span><span><span>@njit</span>(fastmath<span>=</span><span>True</span>)
</span></span><span><span><span>def</span> <span>dijkstra_shortest_path_numba</span>(n, source, dist_sum):
</span></span><span><span>    graph <span>=</span> {}
</span></span><span><span>    dist <span>=</span> {}
</span></span><span><span>    prev <span>=</span> {}
</span></span><span><span>    visited <span>=</span> {}
</span></span><span><span>    Q <span>=</span> []
</span></span><span><span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        <span>for</span> j <span>in</span> range(n):
</span></span><span><span>            graph[n <span>*</span> i <span>+</span> j] <span>=</span> abs(i <span>-</span> j)
</span></span><span><span>
</span></span><span><span>    <span>for</span> v <span>in</span> range(n):
</span></span><span><span>        dist[v] <span>=</span> <span>2147483647</span>
</span></span><span><span>        prev[v] <span>=</span> <span>-</span><span>1</span>
</span></span><span><span>        Q<span>.</span>append(v)
</span></span><span><span>        visited[v] <span>=</span> <span>False</span>
</span></span><span><span>    dist[source] <span>=</span> <span>0</span>
</span></span><span><span>
</span></span><span><span>    <span>while</span> len(Q) <span>&gt;</span> <span>0</span>:
</span></span><span><span>        u <span>=</span> <span>-</span><span>1</span>
</span></span><span><span>        mindist <span>=</span> <span>2147483647</span>
</span></span><span><span>        <span>for</span> i <span>in</span> range(len(Q)):
</span></span><span><span>            <span>if</span> mindist <span>&gt;</span> dist[Q[i]]:
</span></span><span><span>                mindist <span>=</span> dist[Q[i]]
</span></span><span><span>                u <span>=</span> Q[i]
</span></span><span><span>        Q<span>.</span>remove(u)
</span></span><span><span>        visited[u] <span>=</span> <span>True</span>
</span></span><span><span>
</span></span><span><span>        <span>for</span> v <span>in</span> range(n):
</span></span><span><span>            <span>if</span> v <span>!=</span> u <span>and</span> <span>not</span> visited[v]:
</span></span><span><span>                alt <span>=</span> dist[u] <span>+</span> graph[n <span>*</span> u <span>+</span> v]
</span></span><span><span>
</span></span><span><span>                <span>if</span> alt <span>&lt;</span> dist[v]:
</span></span><span><span>                    dist[v] <span>=</span> alt
</span></span><span><span>                    prev[v] <span>=</span> u
</span></span><span><span>
</span></span><span><span>    dist_sum[<span>0</span>] <span>=</span> <span>0</span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        dist_sum[<span>0</span>] <span>+=</span> dist[i]
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>test</span>():
</span></span><span><span>    n: i32 <span>=</span> <span>4000</span>
</span></span><span><span>    dist_sum_array_numba <span>=</span> empty(<span>1</span>, dtype<span>=</span>int32)
</span></span><span><span>    dist_sum_array <span>=</span> empty(<span>1</span>, dtype<span>=</span>int32)
</span></span><span><span>    print(<span>"LPython compilation time: "</span>, timeit<span>.</span>timeit(<span>lambda</span>: dijkstra_shortest_path(<span>0</span>, <span>0</span>, dist_sum_array), number<span>=</span><span>1</span>))
</span></span><span><span>    print(<span>"LPython execution time: "</span>, min(timeit<span>.</span>repeat(<span>lambda</span>: dijkstra_shortest_path(n, <span>0</span>, dist_sum_array), repeat<span>=</span><span>5</span>, number<span>=</span><span>1</span>)))
</span></span><span><span>    print(dist_sum_array[<span>0</span>])
</span></span><span><span>    <span>assert</span> dist_sum_array[<span>0</span>] <span>==</span> i32(n <span>*</span> (n <span>-</span> <span>1</span>)<span>/</span><span>2</span>)
</span></span><span><span>
</span></span><span><span>    print(<span>"Numba compilation time: "</span>, timeit<span>.</span>timeit(<span>lambda</span>: dijkstra_shortest_path_numba(<span>0</span>, <span>0</span>, dist_sum_array_numba), number<span>=</span><span>1</span>))
</span></span><span><span>    print(<span>"Numba execution time: "</span>, min(timeit<span>.</span>repeat(<span>lambda</span>: dijkstra_shortest_path_numba(n, <span>0</span>, dist_sum_array_numba), repeat<span>=</span><span>5</span>, number<span>=</span><span>1</span>)))
</span></span><span><span>    print(dist_sum_array_numba[<span>0</span>])
</span></span><span><span>    <span>assert</span> dist_sum_array_numba[<span>0</span>] <span>==</span> i32(n <span>*</span> (n <span>-</span> <span>1</span>)<span>/</span><span>2</span>)
</span></span><span><span>
</span></span><span><span>test()
</span></span></code></pre></div><table>
<thead>
<tr>
<th>Compiler</th>
<th>Compilation Time (s)</th>
<th>System</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>LPython</td>
<td>0.35</td>
<td>Apple M1 MBP 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.81</td>
<td>Apple M1 MBP 2020</td>
<td>2.31</td>
</tr>
<tr>
<td>LPython</td>
<td>0.69</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.73</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.05</td>
</tr>
<tr>
<td>LPython</td>
<td>0.21</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.73</td>
<td>Apple M1 2020</td>
<td>3.47</td>
</tr>
<tr>
<td>LPython</td>
<td>1.08</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>1.69</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.56</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<table>
<thead>
<tr>
<th>Compiler</th>
<th>Execution Time (s)</th>
<th>System</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>LPython</td>
<td>0.23</td>
<td>Apple M1 MBP 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>1.01</td>
<td>Apple M1 MBP 2020</td>
<td>4.39</td>
</tr>
<tr>
<td>LPython</td>
<td>0.20</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.98</td>
<td>Apple M1 Pro MBP 2021</td>
<td>4.90</td>
</tr>
<tr>
<td>LPython</td>
<td>0.27</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>0.98</td>
<td>Apple M1 2020</td>
<td>3.63</td>
</tr>
<tr>
<td>LPython</td>
<td>0.87</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
<tr>
<td>Numba</td>
<td>1.95</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>2.24</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="ahead-of-time-aot-compilation-1">Ahead-of-Time (AoT) Compilation</h3>
<p>Next, we see how LPython compares to other AoT compilers and to the standard CPython interpreter. The tasks considered are <strong>quadratic-time implementation of the Dijkstra shortest-path algorithm on a fully connected graph</strong>, and <strong>Floyd-Warshall algorithm on array representation of graphs</strong>.</p>
<p><strong>System Information</strong></p>
<table>
<thead>
<tr>
<th>Compiler</th>
<th>Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>clang++</td>
<td>14.0.3</td>
</tr>
<tr>
<td>g++</td>
<td>11.3.0</td>
</tr>
<tr>
<td>LPython</td>
<td>0.19.0</td>
</tr>
<tr>
<td>Python</td>
<td>3.10.4</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<p><strong>Quadratic-time implementation of the Dijkstra shortest-path algorithm on a fully connected graph</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> lpython <span>import</span> i32
</span></span><span><span>
</span></span><span><span><span>def</span> <span>dijkstra_shortest_path</span>(n: i32, source: i32) <span>-&gt;</span> i32:
</span></span><span><span>    i: i32; j: i32; v: i32; u: i32; mindist: i32; alt: i32; dummy: i32; uidx: i32
</span></span><span><span>    dist_sum: i32;
</span></span><span><span>    graph: dict[i32, i32] <span>=</span> {}
</span></span><span><span>    dist: dict[i32, i32] <span>=</span> {}
</span></span><span><span>    prev: dict[i32, i32] <span>=</span> {}
</span></span><span><span>    visited: dict[i32, bool] <span>=</span> {}
</span></span><span><span>    Q: list[i32] <span>=</span> []
</span></span><span><span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        <span>for</span> j <span>in</span> range(n):
</span></span><span><span>            graph[n <span>*</span> i <span>+</span> j] <span>=</span> abs(i <span>-</span> j)
</span></span><span><span>
</span></span><span><span>    <span>for</span> v <span>in</span> range(n):
</span></span><span><span>        dist[v] <span>=</span> <span>2147483647</span>
</span></span><span><span>        prev[v] <span>=</span> <span>-</span><span>1</span>
</span></span><span><span>        Q<span>.</span>append(v)
</span></span><span><span>        visited[v] <span>=</span> <span>False</span>
</span></span><span><span>    dist[source] <span>=</span> <span>0</span>
</span></span><span><span>
</span></span><span><span>    <span>while</span> len(Q) <span>&gt;</span> <span>0</span>:
</span></span><span><span>        u <span>=</span> <span>-</span><span>1</span>
</span></span><span><span>        mindist <span>=</span> <span>2147483647</span>
</span></span><span><span>        <span>for</span> i <span>in</span> range(len(Q)):
</span></span><span><span>            <span>if</span> mindist <span>&gt;</span> dist[Q[i]]:
</span></span><span><span>                mindist <span>=</span> dist[Q[i]]
</span></span><span><span>                u <span>=</span> Q[i]
</span></span><span><span>                uidx <span>=</span> i
</span></span><span><span>        dummy <span>=</span> Q<span>.</span>pop(uidx)
</span></span><span><span>        visited[u] <span>=</span> <span>True</span>
</span></span><span><span>
</span></span><span><span>        <span>for</span> v <span>in</span> range(n):
</span></span><span><span>            <span>if</span> v <span>!=</span> u <span>and</span> <span>not</span> visited[v]:
</span></span><span><span>                alt <span>=</span> dist[u] <span>+</span> graph[n <span>*</span> u <span>+</span> v]
</span></span><span><span>
</span></span><span><span>                <span>if</span> alt <span>&lt;</span> dist[v]:
</span></span><span><span>                    dist[v] <span>=</span> alt
</span></span><span><span>                    prev[v] <span>=</span> u
</span></span><span><span>
</span></span><span><span>    dist_sum <span>=</span> <span>0</span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        dist_sum <span>+=</span> dist[i]
</span></span><span><span>    <span>return</span> dist_sum
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>test</span>():
</span></span><span><span>    n: i32 <span>=</span> <span>4000</span>
</span></span><span><span>    print(dijkstra_shortest_path(n, <span>0</span>))
</span></span><span><span>
</span></span><span><span>test()
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;iostream&gt;</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;unordered_map&gt;</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;vector&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>int32_t</span> <span>dijkstra_shortest_path</span>(<span>int32_t</span> n, <span>int32_t</span> source) {
</span></span><span><span>    <span>int32_t</span> i, j, v, u, mindist, alt, dummy, uidx;
</span></span><span><span>    std<span>::</span>unordered_map<span>&lt;</span><span>int32_t</span>, <span>int32_t</span><span>&gt;</span> graph, dist, prev;
</span></span><span><span>    std<span>::</span>unordered_map<span>&lt;</span><span>int32_t</span>, <span>bool</span><span>&gt;</span> visited;
</span></span><span><span>    std<span>::</span>vector<span>&lt;</span><span>int32_t</span><span>&gt;</span> Q;
</span></span><span><span>
</span></span><span><span>    <span>for</span>(i <span>=</span> <span>0</span>; i <span>&lt;</span> n; i<span>++</span>) {
</span></span><span><span>        <span>for</span>(j <span>=</span> <span>0</span>; j <span>&lt;</span> n; j<span>++</span>) {
</span></span><span><span>            graph[n <span>*</span> i <span>+</span> j] <span>=</span> std<span>::</span>abs(i <span>-</span> j);
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>for</span>(v <span>=</span> <span>0</span>; v <span>&lt;</span> n; v<span>++</span>) {
</span></span><span><span>        dist[v] <span>=</span> <span>2147483647</span>;
</span></span><span><span>        prev[v] <span>=</span> <span>-</span><span>1</span>;
</span></span><span><span>        Q.push_back(v);
</span></span><span><span>        visited[v] <span>=</span> false;
</span></span><span><span>    }
</span></span><span><span>    dist[source] <span>=</span> <span>0</span>;
</span></span><span><span>
</span></span><span><span>    <span>while</span>(Q.size() <span>&gt;</span> <span>0</span>) {
</span></span><span><span>        u <span>=</span> <span>-</span><span>1</span>;
</span></span><span><span>        mindist <span>=</span> <span>2147483647</span>;
</span></span><span><span>        <span>for</span>(i <span>=</span> <span>0</span>; i <span>&lt;</span> Q.size(); i<span>++</span>) {
</span></span><span><span>            <span>if</span>( mindist <span>&gt;</span> dist[Q[i]] ) {
</span></span><span><span>                mindist <span>=</span> dist[Q[i]];
</span></span><span><span>                u <span>=</span> Q[i];
</span></span><span><span>                uidx <span>=</span> i;
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>        Q.erase(Q.begin() <span>+</span> uidx);
</span></span><span><span>        visited[u] <span>=</span> true;
</span></span><span><span>
</span></span><span><span>        <span>for</span>(v <span>=</span> <span>0</span>; v <span>&lt;</span> n; v<span>++</span>) {
</span></span><span><span>            <span>if</span>( v <span>!=</span> u and not visited[v] ) {
</span></span><span><span>                alt <span>=</span> dist[u] <span>+</span> graph[n <span>*</span> u <span>+</span> v];
</span></span><span><span>
</span></span><span><span>                <span>if</span>( alt <span>&lt;</span> dist[v] ) {
</span></span><span><span>                    dist[v] <span>=</span> alt;
</span></span><span><span>                    prev[v] <span>=</span> u;
</span></span><span><span>                }
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>int32_t</span> dist_sum <span>=</span> <span>0</span>;
</span></span><span><span>    <span>for</span>(i <span>=</span> <span>0</span>; i <span>&lt;</span> n; i<span>++</span>) {
</span></span><span><span>        dist_sum <span>+=</span> dist[i];
</span></span><span><span>    }
</span></span><span><span>    <span>return</span> dist_sum;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>int</span> <span>main</span>() {
</span></span><span><span>    <span>int32_t</span> n <span>=</span> <span>4000</span>;
</span></span><span><span>    <span>int32_t</span> dist_sum <span>=</span> dijkstra_shortest_path(n, <span>0</span>);
</span></span><span><span>    std<span>::</span>cout<span>&lt;&lt;</span>dist_sum<span>&lt;&lt;</span>std<span>::</span>endl;
</span></span><span><span>    <span>return</span> <span>0</span>;
</span></span><span><span>}
</span></span></code></pre></div><table>
<thead>
<tr>
<th>Compiler/Interpreter</th>
<th>Execution Time (s)</th>
<th>System</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>LPython</td>
<td>0.167</td>
<td>Apple M1 MBP 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Clang++</td>
<td>0.993</td>
<td>Apple M1 MBP 2020</td>
<td>5.95</td>
</tr>
<tr>
<td>Python</td>
<td>3.817</td>
<td>Apple M1 MBP 2020</td>
<td>22.86</td>
</tr>
<tr>
<td>LPython</td>
<td>0.155</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.00</td>
</tr>
<tr>
<td>Clang++</td>
<td>0.685</td>
<td>Apple M1 Pro MBP 2021</td>
<td>4.41</td>
</tr>
<tr>
<td>Python</td>
<td>3.437</td>
<td>Apple M1 Pro MBP 2021</td>
<td>22.17</td>
</tr>
<tr>
<td>LPython</td>
<td>0.324</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>Clang++</td>
<td>0.709</td>
<td>Apple M1 2020</td>
<td>2.19</td>
</tr>
<tr>
<td>Python</td>
<td>3.486</td>
<td>Apple M1 2020</td>
<td>10.76</td>
</tr>
<tr>
<td>LPython</td>
<td>0.613</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
<tr>
<td>g++</td>
<td>1.358</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>2.21</td>
</tr>
<tr>
<td>Python</td>
<td>7.365</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>12.01</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<p>Note the optimization flags furnished to each compiler.</p>
<table>
<thead>
<tr>
<th>Compiler/Interpreter</th>
<th>Optimization flags used</th>
</tr>
</thead>
<tbody>
<tr>
<td>LPython</td>
<td><code>--fast</code></td>
</tr>
<tr>
<td>Clang++</td>
<td><code>-ffast-math -funroll-loops -O3</code></td>
</tr>
<tr>
<td>g++</td>
<td><code>-ffast-math -funroll-loops -O3</code></td>
</tr>
<tr>
<td>Python</td>
<td>-</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Floyd-Warshall algorithm on array representation of graphs</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> lpython <span>import</span> i64, i32
</span></span><span><span><span>from</span> numpy <span>import</span> empty, int64
</span></span><span><span>
</span></span><span><span><span>def</span> <span>floyd_warshall</span>(size: i32) <span>-&gt;</span> i64:
</span></span><span><span>    dist: i64[size, size] <span>=</span> empty((size, size), dtype<span>=</span>int64)
</span></span><span><span>    u: i32; v: i32
</span></span><span><span>    i: i32; j: i32; k: i32
</span></span><span><span>    update: i64 <span>=</span> i64(<span>0</span>)
</span></span><span><span>    <span>for</span> u <span>in</span> range(size):
</span></span><span><span>        <span>for</span> v <span>in</span> range(size):
</span></span><span><span>            dist[u, v] <span>=</span> i64(<span>2147483647</span>)
</span></span><span><span>    <span>for</span> u <span>in</span> range(size):
</span></span><span><span>        <span>for</span> v <span>in</span> range(size):
</span></span><span><span>            <span>if</span> u <span>!=</span> v <span>and</span> ((u<span>%</span><span>2</span> <span>==</span> <span>0</span> <span>and</span> v<span>%</span><span>2</span> <span>==</span> <span>1</span>)
</span></span><span><span>                           <span>or</span> (u<span>%</span><span>2</span> <span>==</span> <span>1</span> <span>and</span> v<span>%</span><span>2</span> <span>==</span> <span>0</span>)):
</span></span><span><span>                dist[u, v] <span>=</span> i64(u <span>+</span> v)
</span></span><span><span>    <span>for</span> v <span>in</span> range(size):
</span></span><span><span>        dist[v, v] <span>=</span> i64(<span>0</span>)
</span></span><span><span>
</span></span><span><span>    update <span>=</span> i64(<span>0</span>)
</span></span><span><span>    <span>for</span> k <span>in</span> range(size):
</span></span><span><span>        <span>for</span> i <span>in</span> range(size):
</span></span><span><span>            <span>for</span> j <span>in</span> range(size):
</span></span><span><span>                <span>if</span> dist[i, j] <span>&gt;</span> dist[i, k] <span>+</span> dist[k, j]:
</span></span><span><span>                    update <span>+=</span> dist[i, j] <span>-</span> dist[i, k] <span>-</span> dist[k, j]
</span></span><span><span>                    dist[i, j] <span>=</span> dist[i, k] <span>+</span> dist[k, j]
</span></span><span><span>
</span></span><span><span>    <span>return</span> update
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>print(floyd_warshall(<span>1000</span>))
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;iostream&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>int64_t</span> <span>floyd_warshall</span>(<span>int32_t</span> size) {
</span></span><span><span>    <span>int64_t</span> dist[size][size];
</span></span><span><span>    <span>int32_t</span> u, v, i, j, k;
</span></span><span><span>    <span>int64_t</span> update;
</span></span><span><span>    <span>for</span>(u <span>=</span> <span>0</span>; u <span>&lt;</span> size; u<span>++</span>) {
</span></span><span><span>        <span>for</span>(v <span>=</span> <span>0</span>; v <span>&lt;</span> size; v<span>++</span>) {
</span></span><span><span>            dist[u][v] <span>=</span> <span>2147483647</span>;
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>    <span>for</span>(u <span>=</span> <span>0</span>; u <span>&lt;</span> size; u<span>++</span>) {
</span></span><span><span>        <span>for</span>(v <span>=</span> <span>0</span>; v <span>&lt;</span> size; v<span>++</span>) {
</span></span><span><span>            <span>if</span>( u <span>!=</span> v <span>&amp;&amp;</span> ((u<span>%</span><span>2</span> <span>==</span> <span>0</span> and v<span>%</span><span>2</span> <span>==</span> <span>1</span>)
</span></span><span><span>                           <span>||</span> (u<span>%</span><span>2</span> <span>==</span> <span>1</span> and v<span>%</span><span>2</span> <span>==</span> <span>0</span>)) ) {
</span></span><span><span>                dist[u][v] <span>=</span> u <span>+</span> v;
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>    <span>for</span>(v <span>=</span> <span>0</span>; v <span>&lt;</span> size; v<span>++</span>) {
</span></span><span><span>        dist[v][v] <span>=</span> <span>0</span>;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    update <span>=</span> <span>0</span>;
</span></span><span><span>    <span>for</span>(k <span>=</span> <span>0</span>; k <span>&lt;</span> size; k<span>++</span>) {
</span></span><span><span>        <span>for</span>(i <span>=</span> <span>0</span>; i <span>&lt;</span> size; i<span>++</span>) {
</span></span><span><span>            <span>for</span>(j <span>=</span> <span>0</span>; j <span>&lt;</span> size; j<span>++</span>) {
</span></span><span><span>                <span>if</span>( dist[i][j] <span>&gt;</span> dist[i][k] <span>+</span> dist[k][j] ) {
</span></span><span><span>                    update <span>+=</span> dist[i][j] <span>-</span> dist[i][k] <span>-</span> dist[k][j];
</span></span><span><span>                    dist[i][j] <span>=</span> dist[i][k] <span>+</span> dist[k][j];
</span></span><span><span>                }
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>return</span> update;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>int</span> <span>main</span>() {
</span></span><span><span>    std<span>::</span>cout<span>&lt;&lt;</span>(floyd_warshall(<span>1000</span>))<span>&lt;&lt;</span>std<span>::</span>endl;
</span></span><span><span>    <span>return</span> <span>0</span>;
</span></span><span><span>}
</span></span></code></pre></div><table>
<thead>
<tr>
<th>Compiler/Interpreter</th>
<th>Execution Time (s)</th>
<th>System</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Clang++</td>
<td>0.451</td>
<td>Apple M1 MBP 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.767</td>
<td>Apple M1 MBP 2020</td>
<td>1.70</td>
</tr>
<tr>
<td>Python</td>
<td>&gt; 11</td>
<td>Apple M1 MBP 2020</td>
<td>&gt; 24.39</td>
</tr>
<tr>
<td>Clang++</td>
<td>0.435</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.785</td>
<td>Apple M1 Pro MBP 2021</td>
<td>1.80</td>
</tr>
<tr>
<td>Python</td>
<td>&gt; 11</td>
<td>Apple M1 Pro MBP 2021</td>
<td>&gt; 25.28</td>
</tr>
<tr>
<td>Clang++</td>
<td>0.460</td>
<td>Apple M1 2020</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>0.995</td>
<td>Apple M1 2020</td>
<td>2.16</td>
</tr>
<tr>
<td>Python</td>
<td>&gt; 11</td>
<td>Apple M1 2020</td>
<td>&gt; 23.91</td>
</tr>
<tr>
<td>g++</td>
<td>0.695</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>1.00</td>
</tr>
<tr>
<td>LPython</td>
<td>2.933</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>4.22</td>
</tr>
<tr>
<td>Python</td>
<td>440.588</td>
<td>AMD Ryzen 5 2500U (Ubuntu 22.04)</td>
<td>633.94</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Note the optimization flags furnished to each compiler.</p>
<table>
<thead>
<tr>
<th>Compiler/Interpreter</th>
<th>Optimization flags used</th>
</tr>
</thead>
<tbody>
<tr>
<td>LPython</td>
<td><code>--fast</code></td>
</tr>
<tr>
<td>Clang++</td>
<td><code>-ffast-math -funroll-loops -O3</code></td>
</tr>
<tr>
<td>g++</td>
<td><code>-ffast-math -funroll-loops -O3</code></td>
</tr>
<tr>
<td>Python</td>
<td>-</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="interoperability-with-cpython">Interoperability with CPython</h3>
<p>Next we show that LPython can call functions in CPython libraries. This feature permits “break-out” to Numpy, TensorFlow, PyTorch, and even to matplotlib. The break-outs will run at ordinary (slow) Python speeds, but LPython accelerates the mathematical portions to near maximum speed.</p>
<p><strong>Calling NumPy functions via CPython</strong></p>
<p><strong>main.py</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> lpython <span>import</span> i32, f64, i64, pythoncall, Const, TypeVar
</span></span><span><span><span>from</span> numpy <span>import</span> empty, int32, float64
</span></span><span><span>
</span></span><span><span>n_1 <span>=</span> TypeVar(<span>"n_1"</span>)
</span></span><span><span>n_2 <span>=</span> TypeVar(<span>"n_2"</span>)
</span></span><span><span>n_3 <span>=</span> TypeVar(<span>"n_3"</span>)
</span></span><span><span>
</span></span><span><span><span>@pythoncall</span>(module <span>=</span> <span>"util"</span>)
</span></span><span><span><span>def</span> <span>cpython_add</span>(n_1: i32, a: i32[:], b: i32[:]) <span>-&gt;</span> i32[n_1]:
</span></span><span><span>    <span>pass</span>
</span></span><span><span>
</span></span><span><span><span>@pythoncall</span>(module <span>=</span> <span>"util"</span>)
</span></span><span><span><span>def</span> <span>cpython_multiply</span>(n_1: i32, n_2: i32, a: f64[:], b: f64[:]) <span>-&gt;</span> f64[n_1, n_2]:
</span></span><span><span>    <span>pass</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>test_1D</span>():
</span></span><span><span>    n: Const[i32] <span>=</span> <span>500_000</span>
</span></span><span><span>    a: i32[n] <span>=</span> empty(n, dtype <span>=</span> int32)
</span></span><span><span>    b: i32[n] <span>=</span> empty(n, dtype <span>=</span> int32)
</span></span><span><span>    i: i32
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        a[i] <span>=</span> <span>2</span> <span>*</span> (i<span>+</span><span>1</span>) <span>*</span> <span>13</span>
</span></span><span><span>        b[i] <span>=</span> a[i] <span>+</span> <span>2</span>
</span></span><span><span>    sum: i32[n]
</span></span><span><span>    sum <span>=</span> cpython_add(<span>500_000</span>, a, b)
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        <span>assert</span> sum[i] <span>==</span> a[i] <span>+</span> b[i]
</span></span><span><span>
</span></span><span><span><span>def</span> <span>test_2D</span>():
</span></span><span><span>    n: Const[i32] <span>=</span> <span>1_000</span>
</span></span><span><span>    a: f64[n] <span>=</span> empty([n], dtype <span>=</span> float64)
</span></span><span><span>    b: f64[n] <span>=</span> empty([n], dtype <span>=</span> float64)
</span></span><span><span>    i: i32; j: i32
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        a[i] <span>=</span> f64(i <span>+</span> <span>13</span>)
</span></span><span><span>        b[i] <span>=</span> i <span>*</span> <span>2</span> <span>/</span> (i <span>+</span> <span>1</span>)
</span></span><span><span>    product: f64[n, n]
</span></span><span><span>    product <span>=</span> cpython_multiply(<span>1_000</span>, <span>1_000</span>, a, b)
</span></span><span><span>    <span>for</span> i <span>in</span> range(n):
</span></span><span><span>        <span>assert</span> product[i] <span>==</span> a[i] <span>*</span> b[i]
</span></span><span><span>
</span></span><span><span><span>def</span> <span>test</span>():
</span></span><span><span>    test_1D()
</span></span><span><span>    test_2D()
</span></span><span><span>
</span></span><span><span>test()
</span></span></code></pre></div><p><strong>util.py</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> numpy <span>as</span> np
</span></span><span><span>
</span></span><span><span><span>def</span> <span>cpython_add</span>(n, a, b):
</span></span><span><span>    <span>return</span> np<span>.</span>add(a, b)
</span></span><span><span>
</span></span><span><span><span>def</span> <span>cpython_multiply</span>(n, m, a, b):
</span></span><span><span>    <span>return</span> np<span>.</span>multiply(a, b)
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="zsh"><span><span><span>(</span>lp<span>)</span> 23:02:55:~/lpython_project % lpython main.py --backend<span>=</span>c --link-numpy
</span></span><span><span><span>(</span>lp<span>)</span> 23:03:10:~/lpython_project % <span># Works successfully without any asserts failing</span>
</span></span></code></pre></div><p><strong>Plotting graphs via Matplotlib</strong></p>
<p><strong>main.py</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> lpython <span>import</span> f64, i32, pythoncall, Const
</span></span><span><span><span>from</span> numpy <span>import</span> empty, float64
</span></span><span><span>
</span></span><span><span><span>@pythoncall</span>(module <span>=</span> <span>"util"</span>)
</span></span><span><span><span>def</span> <span>plot_graph</span>(x: f64[:], y1: f64[:], y2: f64[:], y3: f64[:]):
</span></span><span><span>    <span>pass</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>f</span>(x: f64, i: f64) <span>-&gt;</span> f64:
</span></span><span><span>    <span>return</span> x <span>**</span> <span>.5</span> <span>/</span> i
</span></span><span><span>
</span></span><span><span><span>def</span> <span>test</span>():
</span></span><span><span>    n: Const[i32] <span>=</span> <span>100000</span>
</span></span><span><span>    x: f64[n] <span>=</span> empty(n, dtype<span>=</span>float64)
</span></span><span><span>    y1: f64[n] <span>=</span> empty(n, dtype<span>=</span>float64)
</span></span><span><span>    y2: f64[n] <span>=</span> empty(n, dtype<span>=</span>float64)
</span></span><span><span>    y3: f64[n] <span>=</span> empty(n, dtype<span>=</span>float64)
</span></span><span><span>
</span></span><span><span>    i: i32
</span></span><span><span>    <span>for</span> i <span>in</span> range(<span>1</span>, n):
</span></span><span><span>        x[i] <span>=</span> f64(i)
</span></span><span><span>
</span></span><span><span>    <span>for</span> i <span>in</span> range(<span>1</span>, n):
</span></span><span><span>        y1[i] <span>=</span> f(x[i], <span>1.</span>)
</span></span><span><span>        y2[i] <span>=</span> f(x[i], <span>2.</span>)
</span></span><span><span>        y3[i] <span>=</span> f(x[i], <span>3.</span>)
</span></span><span><span>
</span></span><span><span>    plot_graph(x, y1, y2, y3)
</span></span><span><span>
</span></span><span><span>test()
</span></span></code></pre></div><p><strong>util.py</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> matplotlib.pyplot <span>as</span> plt
</span></span><span><span>
</span></span><span><span><span>def</span> <span>plot_graph</span>(x, y1, y2, y3):
</span></span><span><span>    plt<span>.</span>figtext(<span>0.92</span>, <span>0.03</span>, <span>'$x$'</span>)
</span></span><span><span>    plt<span>.</span>figtext(<span>0.1</span>, <span>0.9</span>, <span>'$y$'</span>)
</span></span><span><span>    plt<span>.</span>plot(x, y1, label<span>=</span><span>"y1"</span>)
</span></span><span><span>    plt<span>.</span>plot(x, y2, label<span>=</span><span>"y2"</span>)
</span></span><span><span>    plt<span>.</span>plot(x, y3, label<span>=</span><span>"y3"</span>)
</span></span><span><span>    plt<span>.</span>legend()
</span></span><span><span>    plt<span>.</span>savefig(<span>'graph.png'</span>)
</span></span><span><span>    plt<span>.</span>show()
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="zsh"><span><span><span>(</span>lp<span>)</span> 23:09:08:~/lpython_project % lpython main.py --backend<span>=</span>c --link-numpy
</span></span><span><span><span>(</span>lp<span>)</span> 23:10:44:~/lpython_project % <span># Works see the graph below</span>
</span></span></code></pre></div><p><img src="https://lpython.org/blog/images/graph.png" alt="Output graph"></p>
<p><strong>Visualization using Matplotlib: Mandelbrot Set</strong></p>
<p><strong>main.py</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> lpython <span>import</span> i32, f64, pythoncall, TypeVar
</span></span><span><span><span>from</span> numpy <span>import</span> empty, int32
</span></span><span><span>
</span></span><span><span>h <span>=</span> TypeVar(<span>"h"</span>)
</span></span><span><span>w <span>=</span> TypeVar(<span>"w"</span>)
</span></span><span><span>d <span>=</span> TypeVar(<span>"d"</span>)
</span></span><span><span>
</span></span><span><span><span>@pythoncall</span>(module<span>=</span><span>"util"</span>)
</span></span><span><span><span>def</span> <span>show_img_gray</span>(w: i32, h: i32, A: i32[h, w]):
</span></span><span><span>    <span>pass</span>
</span></span><span><span>
</span></span><span><span><span>@pythoncall</span>(module<span>=</span><span>"util"</span>)
</span></span><span><span><span>def</span> <span>show_img_color</span>(w: i32, h: i32, d: i32, A: i32[h, w, d]):
</span></span><span><span>    <span>pass</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>main0</span>():
</span></span><span><span>    Nx: i32 <span>=</span> <span>600</span>; Ny: i32 <span>=</span> <span>450</span>; Nz: i32 <span>=</span> <span>4</span>; n_max: i32 <span>=</span> <span>255</span>
</span></span><span><span>
</span></span><span><span>    xcenter: f64 <span>=</span> f64(<span>-</span><span>0.5</span>); ycenter: f64 <span>=</span> f64(<span>0.0</span>)
</span></span><span><span>    width: f64 <span>=</span> f64(<span>4</span>); height: f64 <span>=</span> f64(<span>3</span>)
</span></span><span><span>    dx_di: f64 <span>=</span> width<span>/</span>f64(Nx); dy_dj: f64 <span>=</span> <span>-</span>height<span>/</span>f64(Ny)
</span></span><span><span>    x_offset: f64 <span>=</span> xcenter <span>-</span> f64(Nx<span>+</span><span>1</span>)<span>*</span>dx_di<span>/</span>f64(<span>2.0</span>)
</span></span><span><span>    y_offset: f64 <span>=</span> ycenter <span>-</span> f64(Ny<span>+</span><span>1</span>)<span>*</span>dy_dj<span>/</span>f64(<span>2.0</span>)
</span></span><span><span>
</span></span><span><span>    i: i32; j: i32; n: i32; idx: i32
</span></span><span><span>    x: f64; y: f64; x_0: f64; y_0: f64; x_sqr: f64; y_sqr: f64
</span></span><span><span>
</span></span><span><span>    image: i32[<span>450</span>, <span>600</span>] <span>=</span> empty([Ny, Nx], dtype<span>=</span>int32)
</span></span><span><span>    image_color: i32[<span>450</span>, <span>600</span>, <span>4</span>] <span>=</span> empty([Ny, Nx, Nz], dtype<span>=</span>int32)
</span></span><span><span>    palette: i32[<span>4</span>, <span>3</span>] <span>=</span> empty([<span>4</span>, <span>3</span>], dtype<span>=</span>int32)
</span></span><span><span>
</span></span><span><span>    <span>for</span> j <span>in</span> range(Ny):
</span></span><span><span>        y_0 <span>=</span> y_offset <span>+</span> dy_dj <span>*</span> f64(j <span>+</span> <span>1</span>)
</span></span><span><span>        <span>for</span> i <span>in</span> range(Nx):
</span></span><span><span>            x_0 <span>=</span> x_offset <span>+</span> dx_di <span>*</span> f64(i <span>+</span> <span>1</span>)
</span></span><span><span>            x <span>=</span> <span>0.0</span>; y <span>=</span> <span>0.0</span>; n <span>=</span> <span>0</span>
</span></span><span><span>            <span>while</span>(<span>True</span>):
</span></span><span><span>                x_sqr <span>=</span> x <span>**</span> <span>2.0</span>
</span></span><span><span>                y_sqr <span>=</span> y <span>**</span> <span>2.0</span>
</span></span><span><span>                <span>if</span> (x_sqr <span>+</span> y_sqr <span>&gt;</span> f64(<span>4</span>) <span>or</span> n <span>==</span> n_max):
</span></span><span><span>                    image[j,i] <span>=</span> <span>255</span> <span>-</span> n
</span></span><span><span>                    <span>break</span>
</span></span><span><span>                y <span>=</span> y_0 <span>+</span> f64(<span>2.0</span>) <span>*</span> x <span>*</span> y
</span></span><span><span>                x <span>=</span> x_0 <span>+</span> x_sqr <span>-</span> y_sqr
</span></span><span><span>                n <span>=</span> n <span>+</span> <span>1</span>
</span></span><span><span>
</span></span><span><span>    palette[<span>0</span>,<span>0</span>] <span>=</span>   <span>0</span>; palette[<span>0</span>,<span>1</span>] <span>=</span> <span>135</span>; palette[<span>0</span>,<span>2</span>] <span>=</span>  <span>68</span>
</span></span><span><span>    palette[<span>1</span>,<span>0</span>] <span>=</span>   <span>0</span>; palette[<span>1</span>,<span>1</span>] <span>=</span>  <span>87</span>; palette[<span>1</span>,<span>2</span>] <span>=</span> <span>231</span>
</span></span><span><span>    palette[<span>2</span>,<span>0</span>] <span>=</span> <span>214</span>; palette[<span>2</span>,<span>1</span>] <span>=</span>  <span>45</span>; palette[<span>2</span>,<span>2</span>] <span>=</span>  <span>32</span>
</span></span><span><span>    palette[<span>3</span>,<span>0</span>] <span>=</span> <span>255</span>; palette[<span>3</span>,<span>1</span>] <span>=</span> <span>167</span>; palette[<span>3</span>,<span>2</span>] <span>=</span>   <span>0</span>
</span></span><span><span>
</span></span><span><span>    <span>for</span> j <span>in</span> range(Ny):
</span></span><span><span>        <span>for</span> i <span>in</span> range(Nx):
</span></span><span><span>            idx <span>=</span> image[j,i] <span>-</span> i32(image[j,i]<span>/</span><span>4</span>)<span>*</span><span>4</span>
</span></span><span><span>            image_color[j,i,<span>0</span>] <span>=</span> palette[idx,<span>0</span>] <span># Red</span>
</span></span><span><span>            image_color[j,i,<span>1</span>] <span>=</span> palette[idx,<span>1</span>] <span># Green</span>
</span></span><span><span>            image_color[j,i,<span>2</span>] <span>=</span> palette[idx,<span>2</span>] <span># Blue</span>
</span></span><span><span>            image_color[j,i,<span>3</span>] <span>=</span> <span>255</span>            <span># Alpha</span>
</span></span><span><span>
</span></span><span><span>    show_img_gray(Nx, Ny, image)
</span></span><span><span>    show_img_color(Nx, Ny, Nz, image_color)
</span></span><span><span>    print(<span>"Done."</span>)
</span></span><span><span>
</span></span><span><span>main0()
</span></span></code></pre></div><p><strong>util.py</strong></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>show_img_gray</span>(w, h, A):
</span></span><span><span>    <span>from</span> matplotlib <span>import</span> pyplot <span>as</span> plt
</span></span><span><span>    plt<span>.</span>imshow(A, cmap<span>=</span><span>'gray'</span>)
</span></span><span><span>    plt<span>.</span>show()
</span></span><span><span>    plt<span>.</span>close()
</span></span><span><span>
</span></span><span><span><span>def</span> <span>show_img_color</span>(w, h, d, A):
</span></span><span><span>    <span>from</span> matplotlib <span>import</span> pyplot <span>as</span> plt
</span></span><span><span>    plt<span>.</span>imshow(A)
</span></span><span><span>    plt<span>.</span>show()
</span></span><span><span>    plt<span>.</span>close()
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="zsh"><span><span>$ ls
</span></span><span><span>main.py util.py
</span></span><span><span>$ lpython main.py --backend<span>=</span>c --link-numpy
</span></span><span><span>Done.
</span></span></code></pre></div><p><img src="https://lpython.org/blog/images/gray.png" alt="mandelbrot-set-gray"></p>
<p><img src="https://lpython.org/blog/images/color.png" alt="mandelbrot-set-color"></p>
<h2 id="conclusion">Conclusion</h2>
<p>The benchmarks support the claim that LPython is competitive with its competitors in all features it offers. In JIT, the execution times of LPython-compiled functions are at least as short as equivalent Numba functions. The speed of JIT compilation, itself, is slow in some cases because it currently depends on a C compiler to generate optimal binary code. For algorithms with rich data structures like <code>dict</code> (hash maps) and <code>list</code>, LPython shows much faster speed than Numba. In AoT compilation for tasks like the Dijkstra algorithm, LPython beats equivalent C++ code very comfortably. For an array-based implementation of the Floyd-Warshall algorithm, LPython generates code almost as fast as C++ does.</p>
<p>The main takeaway is that LPython/LFortran generate fast code by default. Our benchmarks show that it’s straightforward to write high-speed LPython code. We hope to raise expectations that LPython output will be in general at least as fast as the equivalent C++ code. Users love Python because of its many productivity advantages: great tooling, easy syntax, and rich data structures like lists, dicts, sets, and arrays. Because any LPython program is also an ordinary Python program, all the tools – debuggers and profilers, for instance – just work. Then, LPython delivers run-time speeds, even with rich data structures at least as short as alternatives in most cases.</p>


        
          
        

        

        
      </article>

      
        
      


      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Plans develop for high-speed rail in the PNW (165 pts)]]></title>
            <link>https://southseattleemerald.com/2023/07/18/plans-develop-for-high-speed-rail-in-the-pnw/</link>
            <guid>36916150</guid>
            <pubDate>Sat, 29 Jul 2023 02:18:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://southseattleemerald.com/2023/07/18/plans-develop-for-high-speed-rail-in-the-pnw/">https://southseattleemerald.com/2023/07/18/plans-develop-for-high-speed-rail-in-the-pnw/</a>, See on <a href="https://news.ycombinator.com/item?id=36916150">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<h2><em>New research shows how community engagement is integral in its success.</em></h2>



<p><i><b>by Sarah Goh</b></i></p>



<hr>



<p>With a growing population in the Pacific Northwest, the call for better public transportation heightens. This March, Washington’s State Legislature signed off on a transportation milestone, allocating $150 million to a high-speed connection between Oregon, Washington, and British Columbia. </p>



<p>Though this funding could reduce congestion, cut carbon emissions, and better connect these coastal cities, a high-speed rail that travels above 200 miles per hour between major cities has never been done before in the United States. How will Washington get started? How will the State ensure a successful project?</p>



<p>A new <a href="https://mic.comotion.uw.edu/our-work/ultra-high-speed-rail-project/">research report</a> by the University of Washington examines these very questions and identifies key concepts that community members can help with to achieve an efficient high-speed rail. If a rail is built successfully, there will be an extraordinary increase in transportation abilities — saving commuters time while reducing environmental harm. </p>



<p>Professors Jan Whittington and Qing Shen at the UW’s Department of Urban Design and Planning led the research, and with no previous high-speed rail projects in the Northwest, they turned to other states and abroad.</p>



<p>“The purpose of the study was to draw lessons learned from projects, systems, and expertise around the world where high-speed rail has been successful,” Whittington said.</p>



<p>They dedicated six months to both academic and industry research. They interviewed a cadre of transportation experts in France, the Netherlands, Spain, Taiwan, and U.S. cities where high-speed rail is currently developing. </p>



<p>“Oftentimes, [the U.S. is] in a leadership role in developing and growing technologies,” Whittington said. “But here, we’re in a position of needing to learn from people who have had success in their own countries.” </p>



<p>The study allowed interviewees to share their experiences from their own projects. Whittington says that while people are usually unwilling to share their research information, Whittington and Shen’s research allowed experts to talk about their regrets, choices, and early decisions in high-speed rail building. </p>



<p>The <a href="https://mic.comotion.uw.edu/wp-content/uploads/2023/05/Keeping-it-on-the-Tracks-High-speed-Rail-Success-and-Lessons-Learned.pdf">final research report</a> spans over 72 pages, with 40 recommendations for transportation departments. However, Whittington emphasized several key points that will be instrumental to the project’s success: For commuters to prioritize rail transportation over air or other non-environmental ways of travel, the high-speed rail must be at its most convenient. To achieve this convenience of speed and efficiency, there cannot be any shortcuts or deviations to the design. Routes are going to be chosen that minimize turns and any design choices that reduce speed. </p>



<p>And Whittington says the designers must also ensure the rail remains dedicated to its high-speed route and that people have the ability to get to the rail through other means of public transportation. “There are going to be a lot of communities you want to serve,” Whittington said, “but you want to find a way to bring those communities to the routes as opposed to bringing the route to the communities.” </p>



<p>Whittington says early planning must engage commuters and the general community. The report itself states, “Have early, systematic, and sustained community engagement, approaching communities to understand their needs instead of selling the idea of high-speed rail.”</p>



<p>Along with community engagement, limiting political sway is important. The study shows that if political representatives convince planners to route through different locations — deviating from the design — cities could end up with an expensive commuter rail system instead of a competitive high-speed rail.</p>



<p>“You have to be very careful about compromises made in design in these early stages,” Whittington said.</p>



<p>Implementing this delicate balance of compromises and early planning is essential for a high-speed rail project, and the UW research has created a place for U.S. transportation departments to start.</p>



<p>“It is our sincere hope that people will be able to see this collection of recommendations as a set of touchstones to build off of as they take the earliest steps in product design and development,” Whittington said.</p>



<p>The Washington State Department of Transportation (WSDOT) has already begun the early stages of building a high-speed rail in the Cascadia region with help from the UW study. They are looking to secure more funding and focus on the meticulousness of the early design process. Especially in collaboration with Oregon and British Columbia, a project like this is formidable and will take years.</p>



<p>“We don’t want to short-circuit the work we need to do with communities,” said Ron Pate, WSDOT’s director for Rail, Freight and Ports. “Our goal is to make sure we work with communities when moving it forward.” </p>



<hr>



<p><em>This article is funded in part by an</em><a href="https://greenspace.seattle.gov/2023/01/city-of-seattles-environmental-justice-fund-awards-750000-in-grants-for-13-projects-led-by-and-benefiting-those-most-impacted-by-climate-change/#sthash.1NfRR1eK.dpbs"><em>&nbsp;Environmental Justice Fund (EJ Fund) grant</em></a><em>&nbsp;through the City of Seattle’s Office of Sustainability &amp; Environment (OSE).</em></p>



<hr>



<div><figure><img data-lazy-fallback="1" data-attachment-id="83917" data-permalink="https://southseattleemerald.com/2022/03/23/nic-masangkay-is-redefining-mothers-in-a-new-age-of-love/sarahgoh_headshot_cropped/" data-orig-file="https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?fit=1080%2C1080&amp;ssl=1" data-orig-size="1080,1080" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SarahGoh_headshot_cropped" data-image-description="<p>Sarah Goh is a Seattle-based journalist who graduated from the University of Washington with a dual-degree in biology and journalism. At the intersection of community, science, and humanities, she hopes to uplift more voices and explore the overlooked and unexpected. Find her at sarahsgoh.com or on Instagram @sarahsgoh.</p>
" data-image-caption="<p>Sarah Goh</p>
" data-medium-file="https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?fit=474%2C474&amp;ssl=1" decoding="async" loading="lazy" width="474" height="474" src="https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=474%2C474&amp;ssl=1" alt="" srcset="https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=800%2C800&amp;ssl=1 800w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=400%2C400&amp;ssl=1 400w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?w=1080&amp;ssl=1 1080w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?w=948&amp;ssl=1 948w" sizes="(max-width: 474px) 100vw, 474px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=800%2C800&amp;ssl=1 800w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=400%2C400&amp;ssl=1 400w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?w=1080&amp;ssl=1 1080w, https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?w=948&amp;ssl=1 948w" data-lazy-src="https://i0.wp.com/southseattleemerald.com/wp-content/uploads/2022/03/SarahGoh_headshot_cropped.jpg?resize=474%2C474&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure><p><strong><em>Sarah Goh</em></strong><em> is a Singaporean American journalist from Seattle, Washington, and a current medical student at WSU College of Medicine. At the intersection of community, science, and humanities, she hopes to elevate marginalized voices and explore the overlooked and unexpected through her writing. Find her at </em><a href="https://sarahsgoh.com/work"><em>SarahSGoh.com</em></a><em> or </em><a href="https://www.instagram.com/sarahsgoh/"><em>@sarahsgoh</em></a><em>.</em></p></div>



<p>📸 <em>Featured Image: Photo via <a href="https://www.shutterstock.com/image-photo/high-speed-train-motion-on-railway-1661454721">Denis Belitsky</a>/<a href="http://shutterstock.com/">Shutterstock.com</a></em></p>



<pre><strong>Before you move on to the next story …</strong>
The <em>South Seattle Emerald</em> is brought to you by Rainmakers. Rainmakers give recurring gifts at any amount. With over 1,000 Rainmakers, the <em>Emerald</em> is truly community-driven local media. Help us keep BIPOC-led media free and accessible. 
 
If just half of our readers signed up to give $6 a month, we wouldn't have to fundraise for the rest of the year. Small amounts make a difference. 
 
<strong>We cannot do this work without you.</strong> <a href="https://southseattleemerald.kindful.com/">Become a Rainmaker today!</a></pre>

	</div></div>]]></description>
        </item>
    </channel>
</rss>