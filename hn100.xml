<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 28 Oct 2023 22:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[FCC wants to bolster amateur radio (171 pts)]]></title>
            <link>https://www.radioworld.com/news-and-business/business-and-law/fcc-wants-to-bolster-amateur-radio</link>
            <guid>38052577</guid>
            <pubDate>Sat, 28 Oct 2023 19:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.radioworld.com/news-and-business/business-and-law/fcc-wants-to-bolster-amateur-radio">https://www.radioworld.com/news-and-business/business-and-law/fcc-wants-to-bolster-amateur-radio</a>, See on <a href="https://news.ycombinator.com/item?id=38052577">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
		<p><span>FCC Chairwoman Jessica Rosenworcel says the FCC plans to “incentivize innovation and experimentation in the amateur radio bands” by&nbsp;getting rid of outdated restrictions&nbsp;and providing licensees with the flexibility to use modern digital emissions.</span></p>
<p><span>The commission at its November meeting is expected to take action on a Report and Order that would eliminate the baud rate limitation and establish a bandwidth limitation in the amateur radio bands below 29.7 MHz.&nbsp;</span></p>
<p><span>The order being circulated for tentative consideration by the commission would remove the baud rate limitation — the rate at which the carrier waveform amplitude, frequency and/or phase is varied to transmit information — for data emissions in the amateur radio bands, the FCC says. The current baud rate limits were adopted in 1980.</span></p>
<p><span>The order would implement a 2.8 kilohertz bandwidth limitation in place of the baud rate in amateur radio bands. The 2.8 kHz limitation is consistent with the commission’s treatment of other wireless radio services, the FCC says.</span></p>
<p><span>The current rules limit the baud rate for high-frequency amateur radioteletype/data transmissions to 300 baud for frequencies below 28 MHz (except in the 60-meter band), and 1200 baud in the 10 meter (28-29.7 MHZ) band.</span></p>
<p><span>The Wireless Telecommunications Bureau says the change in technical standards would allow the amateur radio service to operate more efficiently, including during times of emergency to support public safety.</span></p>
<p><span>The American Radio Relay League (ARRL) in 2013 asked the commission to delete references to the baud rate and to establish a bandwidth limitation of 2.8 kHz. The group argued the public safety benefits of making the change. ARRL stated: “[i]ncreasing speed is especially important when amateurs voluntarily assist during and after hurricanes, forest fires and other disasters.”</span></p>
<p><span>At the time, ARRL also told the FCC that eliminating the baud rate limitation will “incentivize innovation by allowing more data to be transmitted within each signal without increasing bandwidth from that currently used.”</span></p>
<p><span>A spokesperson for ARRL says the organization supports the proposed action and the <a href="https://www.radioworld.com/wp-content/uploads/2023/10/DOC-397992A1.pdf" target="_blank" rel="noopener">proposal of further notice.</a>&nbsp;</span></p>
<p><span>In a subsequent Notice of Proposed Rulemaking in 2016 (WT Docket No. 16-239), the FCC tentatively concluded that a 2.8 kilohertz bandwidth limitation for radioteletype and data emissions in the MF/HF bands was not necessary, and sought comment.&nbsp;</span></p>
<p><a href="https://www.radioworld.com/news-and-business/business-and-law/"><b><i>[See Our Business and Law Page]</i></b></a></p>
<p><span>A few commenters at the time of the NPRM opposed any rule change, arguing that the existing rules should be retained in order to protect access to amateur bands by Morse code and other narrowband transmissions.</span></p>
<p><span>However, the commission writes in the order: “Based on the record in this proceeding, we find that the baud rate limitation has become outdated and hampers, rather than promotes, innovation and robust use of the amateur bands.”</span></p>
<p><span>The commission continued in the new order: “We are persuaded by the weight of the record in this proceeding that, without a baud rate or bandwidth limit, data stations using a large amount of spectrum for a single emission could do so to the detriment of simultaneous use by other stations using narrowband emission modes.”&nbsp;</span></p>
<p><span>In essence, the technical change mean amateurs will require less time to transmit messages, which in turn will open up more spectrum in the time domain for more amateurs to use, said David Siddall, general counsel for ARRL.&nbsp;</span></p>
<p><span>“This is a very simple change. In 1980, at the inception of digital technologies that could be used by radio amateurs, the FCC adopted a speed limit of 300 baud for the stated purpose of limiting the amount of spectrum occupied by any single signal,” Siddall said. “Radio amateurs, being tinkerers and experimenters, worked to develop faster and faster speeds that still fit within the standard spectrum bandwidth. Eventually their innovations to the technology significantly increased spectrum efficiency but ran up against the FCC baud rate limit.”</span></p>
<p><span>One of the benefits of the changes will be allowing for “faster emergency communications” by volunteer ham radio operators during emergencies, the FCC says.</span></p>
<p><span>The agency says its Wireless Telecommunications Bureau’s Mobility Division has previously issued waivers allowing amateur operators directly involved with disaster relief efforts to exceed the baud rate limitation in the interest of public safety.</span></p>
<p><span>The FCC at its November meeting will also consider a Further Notice of Proposed Rulemaking (FNPRM) that proposes to remove the baud rate limitation in the 2200 meter and 630 meter bands. The commission also proposes to remove the baud rate limitation in the very high frequency (VHF) and ultra-high frequency (UHF) bands.&nbsp;</span></p>
<p><span>The commission says it expects to seek comment on the appropriate bandwidth limitation for the 2200 meter band, the 630 meter band, and the VHF and UHF bands.</span></p>
<p><span>Steve Stroh, editor of amateur radio newsletter Zero Retries, says the need for improved data communications in amateur radio also coincides with spectrum becoming more “noisy” due to “pollution” by systems such as LED lighting, small switching power supplies and even solar panels.&nbsp;</span></p>
<p><span>“That noise has an outsize impact on analog modes such as voice and very low power transmissions. Improved data communications modes, including digital voice modes, can overcome the noise issues,” Stroh said in an email to Radio World.</span></p>
<p><span>Stroh says he is happy to see the FCC address the same limitation on the amateur radio VHF and UHF bands —&nbsp;where there is arguably much greater potential for technological innovation in data communications technology — if it wasn’t for the data rate and mode limitations.</span></p>
<p><span>“Fortunately, in its proposal, the FCC recognizes that the symbol rate and mode issue does include the Amateur Radio VHF and UHF bands. Thus the FCC’s proposal is a very good one that will significantly benefit Amateur Radio,” he said.&nbsp;</span></p>
<p><span>The FCC’s next meeting is scheduled for November 15. A comment period on the <a href="https://www.radioworld.com/wp-content/uploads/2023/10/DOC-397992A1.pdf" target="_blank" rel="noopener">FNPRM</a> will commence 30 days after the date of publication in the Federal Register.&nbsp;</span></p>
<p><a href="https://www2.smartbrief.com/signupSystem/subscribe.action?pageSequence=1&amp;briefName=RW&amp;campaign=pm_optin_promo_website_RW"><b><i>[Sign Up for Radio World’s SmartBrief Newsletter]</i></b></a></p>
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[McDonalds is giving free French fries in return for waiving the right to sue (119 pts)]]></title>
            <link>https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/</link>
            <guid>38052212</guid>
            <pubDate>Sat, 28 Oct 2023 18:27:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/">https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/</a>, See on <a href="https://news.ycombinator.com/item?id=38052212">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
    <div>
																	
								
							
				
							

							
				<div data-post-url="https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/" data-post-title="McDonald's New Terms And Conditions Have People Deleting The App" data-slide-num="0" data-post-id="1432093">
											
															<picture>
																			<source media="(min-width: 429px)" srcset="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/intro-1698426237.webp" type="image/webp">
										<source media="(max-width: 428px)" srcset="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/intro-1698426237.sm.webp" type="image/webp">
																		<img src="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/intro-1698426237.jpg" data-slide-url="https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/" data-post-id="1432093" data-slide-num="0" data-slide-title="McDonald's New Terms And Conditions Have People Deleting The App: " width="780" height="438" alt="phone displaying McDonald's mobile app">
								</picture>
																					<p><span>Vladimka production/Shutterstock</span>									</p></div>

																													
				<div>
															<p dir="ltr">Practically everyone has clicked "agree" on terms and conditions they didn't read through. Unfortunately, the latest terms and conditions for using the <a href="https://www.mcdonalds.com/us/en-us/terms-and-conditions.html" target="_blank">McDonald's</a> app contain many&nbsp;customer-affecting changes: updates to McDonald's liability in cases of injury, third-party errors, and app malfunction; waivers for a customer's right to a jury trial or class action lawsuit; and an agreement to solve disputes through a strict arbitration process.</p>

<p dir="ltr">Essentially, the new terms state that, if a customer tries to <a href="https://www.mashed.com/1398637/mcdonalds-sued-hot-coffee-again/" target="_blank">sue over hot coffee</a>, for example, they can't take their case to trial. Rather, as laid out in McDonald's 12-step outline, the customer must notify the company of their intent to seek arbitration, meet with the company to discuss the problem, and only then have an arbitrator enter the equation.&nbsp;As you can imagine, this process eliminates decision-making by a dozen jurors, instead giving a single arbitrator discretion in deciding the outcome. The very nature of the arbitration process makes class action nearly impossible, meaning each customer has to file their own dispute with McDonald's.</p>
<p dir="ltr">All that said, perhaps the most controversial piece of these updates is that there's no way to opt out of accepting the new terms and conditions — no box to check saying you disagree. The only choices? Agree or delete the app.</p>


					
									</div>
					</div>

    <div>
			

							                    <h2>McDonald's customers are opting to delete the app</h2>
				
				<div data-slide-num="1" data-post-id="1432093">
											
																					<picture>
																	<source media="(min-width: 429px)" data-srcset="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/mcdonalds-customers-are-opting-to-delete-the-app-1698426091.webp" type="image/webp">
									<source media="(max-width: 428px)" data-srcset="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/mcdonalds-customers-are-opting-to-delete-the-app-1698426091.sm.webp" type="image/webp">
																<img data-lazy-src="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/mcdonalds-customers-are-opting-to-delete-the-app-1698426091.jpg" data-slide-url="https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/slide/mcdonalds-customers-are-opting-to-delete-the-app/" data-post-id="1432093" data-slide-num="1" data-slide-title="McDonald's New Terms And Conditions Have People Deleting The App: McDonald's customers are opting to delete the app" width="780" height="438" alt="McDonald's customer on phone" src="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/mcdonalds-customers-are-opting-to-delete-the-app-1698426091.jpg">
							</picture>
																				<p><span>Sorbis/Shutterstock</span>									</p></div>

				
				<div>
															<p dir="ltr">Understandably, McDonald's customers have been very vocal about the new terms and conditions online, with many saying they're choosing to delete the app rather than sign their legal rights away. Several people pointed out that McDonald's has been pushing its app more frequently in recent months, particularly through app-only offers like <a href="https://www.mashed.com/1427915/mcdonalds-free-french-fry-fridays/" target="_blank">free fries on Fridays</a>. At the same time, one <a href="https://www.tiktok.com/@seansvv/video/7294149648397241642" target="_blank">TikTok</a> user commented that the terms and conditions include a great deal of legal jargon that the "typical fast food patron [can't] read," much less understand. Similarly, a <a href="https://www.reddit.com/r/AskReddit/comments/17fx4b2/comment/k6ct4gh/" target="_blank">Reddit</a> user noted that these terms are intended to stack the deck against anyone who tries to sue McDonald's.</p>

<p dir="ltr">Customers questioned whether or not the terms and conditions are actually legally binding, given that the agreement does not require a signature or date. Unfortunately, if McDonald's has laid out all necessary information and provided app users with the ability to agree or disagree (the latter requiring customers to jump through hoops to delete their account and the app itself), the terms and conditions are assumedly enforceable by law in most states.</p>

					
									</div>
					</div>


</article>


	
	
	
	
	

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting my library cards onto my phone the hard way (154 pts)]]></title>
            <link>https://iliana.fyi/blog/ios-wallet-library-card/</link>
            <guid>38050535</guid>
            <pubDate>Sat, 28 Oct 2023 15:26:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iliana.fyi/blog/ios-wallet-library-card/">https://iliana.fyi/blog/ios-wallet-library-card/</a>, See on <a href="https://news.ycombinator.com/item?id=38050535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Our local libraries, The Seattle Public Library and the King County Library System, issue pieces of plastic with barcodes printed on the back assigned to your borrower account. These cards are not <em>strictly</em> necessary in 2023; most everything at Seattle libraries is self-service, including circulation, and these self-service entrypoints usually have a way to type in a library barcode manually. But having the barcode is far more convenient, and I’d like to have it without having to keep yet another plastic card I rarely use in my wallet.</p><p>So I put it on my phone, in my iPhone’s Wallet app. This became extremely silly extremely quickly, so I’ve decided to document it here for myself and others.</p><h2>A brief introduction to passes</h2><p>The Wallet app can manage many things: payment cards, government/employee/student IDs, house/car/hotel room keys; none of these were part of what Wallet, initially called Passbook, could do at its 2012 launch. At that time, Passbook only managed “passes”.</p><p><a href="https://developer.apple.com/library/archive/documentation/UserExperience/Conceptual/PassKit_PG/index.html">Apple’s documentation on passes</a> covers this in more detail, but they are self-contained zip files full of JSON and PNGs designed to be distributed through email or the web from a vendor to its user. If you have a pass on your phone, you can usually go to Pass Details and find a share icon in the top right, allowing you to send the .pkpass file to somewhere you can unzip it and inspect it.</p><p>The contents are pretty simple. <a href="https://developer.apple.com/library/archive/documentation/UserExperience/Conceptual/PassKit_PG/Creating.html#//apple_ref/doc/uid/TP40012195-CH4-SW52">There’s a specific list of supported images</a>, there’s a <code>pass.json</code> file which describes all of the non-image content of the pass, there’s a <code>manifest.json</code> file which lists the SHA-1 checksum of all the other files, and a <code>signature</code> which is an S/MIME signature of the contents of the <code>manifest.json</code>.</p><p>Our first interesting problem is one of barcode formats. Passes support four types of barcodes: <a href="https://en.wikipedia.org/wiki/QR_code">QR code</a>, <a href="https://en.wikipedia.org/wiki/PDF417">PDF417</a> (commonly used on United States driver licenses), <a href="https://en.wikipedia.org/wiki/Aztec_Code">Aztec Code</a> (used for boarding passes by the airline industry), and <a href="https://en.wikipedia.org/wiki/Code_128">Code 128</a> (the only supported linear symbology). My library card uses… <em>[stares at Wikipedia for half an hour]</em> <a href="https://en.wikipedia.org/wiki/Codabar">Codabar</a>, widely used in libraries<sup><a href="#user-content-fn-blood" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-blood">1</a></sup>, and perhaps one of the cutest barcode symbologies (and names) I’ve ever seen. It’s possible that the barcode scanners at the library support other linear symbologies, but Codabar is the only one I know guaranteed to work at all of them. So we will need to fake it by providing some image that functions as a scannable barcode.</p><p>Our second interesting problem, which is a much worse, “oh no”-level problem: for some reason, passes are cryptographically signed, and they have to be signed with a key known to one of Apple’s certificate authorities. Cryptographically signing these files makes some sense when you consider that passes were designed to get automatic updates from their vendors; for example, your boarding pass for a flight reflecting gate changes or changing your seat assignment.</p><p>If you are already an Apple developer you can get yourself a pass signing key pretty trivially, but I am not, and I do not intend to drop $99 on this.</p><h2>Perfection is the enemy of something or other</h2><p>There are other people who are already Apple developers who have made various apps for designing passes. They are… passable? Unfortunately I am a perfectionist.</p><p>For one thing, there is the matter of the logo in the top left of the pass. Apple has designed this somewhat flexibly, with a maximum height of 50 device-independent pixels, but a square logo with text to the right side is going to most comfortably fit at about 40 pixels tall. Pass developers are expected to provide correctly-sized logos at <code>logo.png</code>, <code>logo@2x.png</code>, <code>logo@3x.png</code><sup><a href="#user-content-fn-1" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-1">2</a></sup> for different device pixel ratios, but these apps tend to let you select a single logo and not give you any control over how it’s scaled. If you give it a 40-pixel image, it’ll be blurry on any currently-supported iPhone; if you give it an 80-pixel image, it’ll be too large. Not great!</p><p>For another thing, I’d really like to have the screen be brighter as I open the pass. Passes with normal, supported barcodes do this to support scanners that need better contrast. To me, the ideal situation here is to trick iOS into making the screen brighter without actually having a non-functional barcode present. I’m not going to be able to get away with this kind of JSON fuzzing without digging into the JSON myself.</p><p>And, these apps tend to be free to download, but only let you save a limited number of passes to Wallet before asking you to pay up. I am not here to judge the developers for doing this but I am probably also not going to pay for your app unless it does what I want it to (and unfortunately what I want is kind of extreme).</p><h2>Finding a key</h2><p>Well, I did just download half a dozen free-to-start pass generator apps.</p><p>You could make these one of two ways. Probably the “correct” way is to have some web service which performs the signing, so that you don’t ship a private key with the application itself. But surely one of these apps I’ve downloaded lets you generate passes offline? Sometimes you want the app to work without having to also maintain a web service; that sounds like a one-way ticket to dealing with a ton of bad reviews and refunds when it inevitably goes down.</p><p>I turned on Airplane Mode, turned off WiFi, and tried them all. Sure enough, at least one does. I’m not going to draw attention to the specific app I used in this post because I don’t want their key to get revoked<sup><a href="#user-content-fn-2" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-2">3</a></sup>. But it was kind of funny how simple the process was:</p><ol><li>Download the app on my Mac, since Apple silicon Macs let you run iOS apps.</li><li>In the wrapped iOS app bundle, observe that there is a very obvious <code>.p12</code> file.</li><li>Run <code>strings</code> on the main binary and look for anything that might be a password (as PKCS#12 files require an import password).</li></ol><p>And, well:</p><pre><code>$ openssl pkcs12 -info -in [redacted].p12 -legacy -nodes
Enter Import Password:
MAC: sha1, Iteration 1
MAC length: 20, salt length: 8
PKCS7 Encrypted data: pbeWithSHA1And40BitRC2-CBC, Iteration 2048
Certificate bag
Bag Attributes
    friendlyName: Pass Type ID: [redacted]
    localKeyID: [redacted]
subject=UID = [redacted], CN = Pass Type ID: [redacted], OU = [redacted], O = [redacted], C = [redacted]
issuer=CN = Apple Worldwide Developer Relations Certification Authority, OU = G4, O = Apple Inc., C = US
...
</code></pre><p>We also need a certificate chain; this certificate is signed with an intermediate. The app needs it too, so it’s probably somewhere in the bundle, but the certificate contains within its X.509 extension fields the URL to download the intermediate if you need it (you can view a certificate’s various fields with <code>openssl x509 -noout -text -in whatever.pem</code>).</p><h2>Laying out the pass</h2><p>First we need to pick a <a href="https://developer.apple.com/library/archive/documentation/UserExperience/Conceptual/PassKit_PG/Creating.html#//apple_ref/doc/uid/TP40012195-CH4-SW45">pass style</a> out of “boarding pass”, “coupon”, “event ticket”, “generic”, or “store card”. We want a layout that lets us put a large horizontal image across the pass somewhere. This limits us to layouts that support the “strip” image: coupon, event ticket, or store card. Out of these three, the store card is most skeuomorphic to our physical library card.</p><p>Let’s type up the start of a <code>pass.json</code>. The documentation for this file is found at the <a href="https://developer.apple.com/library/archive/documentation/UserExperience/Reference/PassKit_Bundle/Chapters/Introduction.html">PassKit Package Format Reference</a>.</p><pre><code data-syntax-highlighted=""><span>{</span>
<span>  </span><span>"passTypeIdentifier"</span><span>: </span><span>"[redacted]"</span><span>,</span>
<span>  </span><span>"teamIdentifier"</span><span>: </span><span>"[redacted]"</span><span>,</span>
<span>  </span><span>"formatVersion"</span><span>: </span><span>1</span><span>,</span>
<span>  </span><span>"serialNumber"</span><span>: </span><span>"whatever"</span><span>,</span>
<span>  </span><span>"organizationName"</span><span>: </span><span>"me!"</span><span>,</span>
<span>  </span><span>"logoText"</span><span>: </span><span>"The Seattle Public Library"</span><span>,</span>
<span>  </span><span>"description"</span><span>: </span><span>"Library Card"</span><span>,</span>
<span>  </span><span>"storeCard"</span><span>: {</span>
<span>    </span><span>"headerFields"</span><span>: [],</span>
<span>    </span><span>"primaryFields"</span><span>: [],</span>
<span>    </span><span>"backFields"</span><span>: [],</span>
<span>    </span><span>"secondaryFields"</span><span>: [],</span>
<span>    </span><span>"auxiliaryFields"</span><span>: []</span>
<span>  },</span>
<span>  </span><span>"backgroundColor"</span><span>: </span><span>"rgb(255, 255, 255)"</span><span>,</span>
<span>  </span><span>"foregroundColor"</span><span>: </span><span>"rgb(0, 0, 0)"</span><span>,</span>
<span>  </span><span>"sharingProhibited"</span><span>: </span><span>false</span>
<span>}</span>
</code></pre><p>The <code>passTypeIdentifier</code> and <code>teamIdentifier</code> must match the <code>UID</code> and <code>OU</code> fields, respectively, of the certificate subject you got from Apple and/or found lying around. <code>serialNumber</code> needs to be unique for each pass you generate with the same <code>passTypeIdentifier</code>. <code>organizationName</code> is ostensibly supposed to be who made and signed the pass, but if you’re never distributing the pass then it probably doesn’t matter.</p><p>Now for some images. <code>icon.png</code> is required but is not shown on the pass itself. <code>logo.png</code> is the logo displayed at the top left. I generated three logo files: a 40×40 <code>logo.png</code>, an 80×80 <code>logo@2x.png</code>, and a 120×120 <code>logo@3x.png</code>; then I copied <code>logo@3x.png</code> to <code>icon.png</code>.</p><p>Finally, we’ll need the <code>strip.png</code>, which will contain our pre-generated barcode.</p><h2>Generating the barcode</h2><p>Fortunately iOS scales and crops the <code>strip.png</code> we generate to fit whatever size box it is on a device, so we don’t need to worry about making three different versions of it.</p><p>Both my library cards use “A” and “D” as the start and stop symbols. If you already have a barcode scanner handy this is the easiest way to figure out what your start and stop symbols are, but you can also compare the beginning and end of the barcode against <a href="https://en.wikipedia.org/wiki/Codabar#Encoding">the symbology table on Wikipedia</a> by eye pretty easily.</p><p>There aren’t many ready-to-use Codabar generators online, but the format is pretty simple to implement yourself. While prototyping I used <a href="https://lib.rs/crates/barcoders">the Barcoders library for Rust</a> to generate an SVG, then tweaked the SVG and exported a PNG. After some experimentation I settled on the following layout (where 1 unit is the width of a narrow bar):</p><ul><li>The barcode height is, in units, twice the number of total symbols (including the start and stop symbols) in the barcode. (For example: a 13-digit barcode number is 15 symbols, and so I made my barcode height 30 units tall.)</li><li>15 units of quiet space is placed before the start and after the end of the barcode. (Various reader documentation I’ve seen suggests 10 units is sufficient but I had a harder time scanning it with my fancy 2D barcode scanner.)</li><li>50 units of padding are placed above and below the barcode. This is overkill, but helps ensure the image is cropped on the top and bottom, not on the left and right.</li><li>Each unit is scaled up to 8 pixels to ensure iOS is always scaling the image down. (This makes the final image size 1040 pixels tall and, for my example 15-symbol barcode, the barcode 240 pixels tall.)</li></ul><p>Codabar has such a simple encoding that I felt an overwhelming urge to write a 69-line shell script that generates a bitmap of an encoded Codabar barcode in the above layout:</p><pre><code data-syntax-highlighted=""><span>#!/usr/bin/env bash</span>
<span>if</span><span> [[ </span><span>$#</span><span> </span><span>-ne</span><span> </span><span>2</span><span> ]]; </span><span>then</span>
<span>    </span><span>&gt;&amp;2</span><span> </span><span>echo</span><span> </span><span>"usage: </span><span>$0</span><span> BARCODE OUTPUT"</span><span>; </span><span>exit</span><span> </span><span>1</span>
<span>fi</span>

<span>scale_factor</span><span>=</span><span>8</span><span>  </span><span># needs to be multiple of 4 for BMP reasons</span>
<span>quiet_space</span><span>=</span><span>15</span>
<span>vert_padding</span><span>=</span><span>50</span>

<span>draw_black</span><span>() { </span><span>head</span><span> </span><span>-c</span><span> </span><span>$((</span><span>$1</span><span> </span><span>*</span><span> scale_factor </span><span>*</span><span> </span><span>3</span><span>))</span><span> </span><span>/dev/zero</span><span>; }</span>
<span>draw_white</span><span>() { </span><span>draw_black</span><span> </span><span>"</span><span>$1</span><span>"</span><span> </span><span>|</span><span> LANG</span><span>=</span><span>C</span><span> tr </span><span>'\0'</span><span> </span><span>'\377'</span><span>; }</span>
<span>encode_long</span><span>() {</span>
<span>    </span><span>for</span><span> __x </span><span>in</span><span> </span><span>0</span><span> </span><span>8</span><span> </span><span>16</span><span> </span><span>24</span><span>; </span><span>do</span>
<span>        </span><span>echo</span><span> </span><span>-en</span><span> </span><span>"\x$(</span><span>printf</span><span> %x $(((</span><span>$1</span><span> </span><span>&gt;&gt;</span><span> __x) </span><span>%</span><span> </span><span>256</span><span>)))"</span>
<span>    </span><span>done</span>
<span>}</span>

<span>workdir</span><span>=</span><span>$(</span><span>mktemp</span><span> </span><span>-d</span><span>)</span>
<span>trap</span><span> </span><span>'rm -rf "$workdir"'</span><span> </span><span>EXIT</span>

<span>{</span>
<span>    </span><span>draw_white</span><span> $quiet_space</span>
<span>    </span><span>echo</span><span> </span><span>-n</span><span> </span><span>"</span><span>$1</span><span>"</span><span> </span><span>|</span><span> </span><span>while</span><span> read -r -N1 symbol; </span><span>do</span>
<span>        </span><span>case</span><span> $symbol </span><span>in</span>
<span>            0|2|6|C|</span><span>\*</span><span>|B|N|.) bars</span><span>=</span><span>0</span><span>001</span><span> ;;&amp;</span>
<span>            1|-|7|D|E|/)      bars</span><span>=</span><span>0</span><span>010</span><span> ;;&amp;</span>
<span>            4|$|8|A|T|:)      bars</span><span>=</span><span>0</span><span>100</span><span> ;;&amp;</span>
<span>            5|9|3|+)          bars</span><span>=</span><span>1000</span><span> ;;&amp;</span>
<span>            0|1|4|5)          spaces</span><span>=</span><span>0</span><span>01</span><span> ;;</span>
<span>            2|-|$|9)          spaces</span><span>=</span><span>0</span><span>10</span><span> ;;</span>
<span>            6|7|8|3)          spaces</span><span>=</span><span>100</span><span> ;;</span>
<span>            C|</span><span>\*</span><span>|D|E|A|T)     spaces</span><span>=</span><span>0</span><span>11</span><span> ;;</span>
<span>            B|N)              spaces</span><span>=</span><span>110</span><span> ;;</span>
<span>            .|/|:|+)          spaces</span><span>=</span><span>0</span><span>00</span><span> ;;</span>
<span>            </span><span>*</span><span>) </span><span>&gt;&amp;2</span><span> echo</span><span> "</span><span>$0</span><span>: warning: ignoring symbol </span><span>$symbol</span><span>"</span><span>; </span><span>continue</span><span> ;;</span>
<span>        </span><span>esac</span>
<span>        </span><span>for</span><span> i </span><span>in</span><span> {0..3}; </span><span>do</span>
<span>            </span><span>draw_black</span><span> </span><span>$((${bars</span><span>:</span><span>$i</span><span>:</span><span>1} </span><span>+</span><span> </span><span>1</span><span>))</span>
<span>            </span><span>draw_white</span><span> </span><span>$((${spaces</span><span>:</span><span>$i</span><span>:</span><span>1} </span><span>+</span><span> </span><span>1</span><span>))</span>
<span>        </span><span>done</span>
<span>    </span><span>done</span>
<span>    </span><span>draw_white</span><span> </span><span>$((quiet_space </span><span>-</span><span> </span><span>1</span><span>))</span>
<span>} </span><span>&gt;</span><span>"</span><span>$workdir</span><span>/line"</span>

<span>image_width</span><span>=</span><span>$(($(wc </span><span>-</span><span>c </span><span>&lt;</span><span>"</span><span>$workdir</span><span>/</span><span>line") </span><span>/</span><span> </span><span>3</span><span>))</span>
<span>barcode_height</span><span>=</span><span>$((${</span><span>#</span><span>1} </span><span>*</span><span> </span><span>2</span><span>))</span>
<span>image_height</span><span>=</span><span>$(((barcode_height </span><span>+</span><span> vert_padding </span><span>*</span><span> </span><span>2</span><span>) </span><span>*</span><span> scale_factor))</span>

<span>{</span>
<span>    </span><span># BMP header</span>
<span>    </span><span>printf</span><span> </span><span>'BM'</span>
<span>    </span><span>encode_long</span><span> </span><span>$((image_width </span><span>*</span><span> image_height </span><span>*</span><span> </span><span>3</span><span> </span><span>+</span><span> </span><span>54</span><span>))</span>
<span>    </span><span>printf</span><span> </span><span>'\0\0\0\0\x36\0\0\0\x28\0\0\0'</span>
<span>    </span><span>encode_long</span><span> $image_width</span>
<span>    </span><span>encode_long</span><span> $image_height</span>
<span>    </span><span>printf</span><span> </span><span>'\x01\0\x18\0\0\0\0\0'</span>
<span>    </span><span>encode_long</span><span> </span><span>$((image_width </span><span>*</span><span> image_height </span><span>*</span><span> </span><span>3</span><span>))</span>
<span>    </span><span>head</span><span> </span><span>-c</span><span> </span><span>16</span><span> </span><span>/dev/zero</span>

<span>    </span><span>draw_white</span><span> </span><span>$((image_width </span><span>*</span><span> vert_padding))</span><span>  </span><span># top vertical padding</span>

<span>    lines</span><span>=</span><span>$((barcode_height </span><span>*</span><span> scale_factor))</span>
<span>    </span><span>while</span><span> ((lines</span><span>--</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>)); </span><span>do</span><span> cat</span><span> "</span><span>$workdir</span><span>/line"</span><span>; </span><span>done</span>

<span>    </span><span>draw_white</span><span> </span><span>$((image_width </span><span>*</span><span> vert_padding))</span><span>  </span><span># bottom vertical padding</span>
<span>} </span><span>&gt;</span><span>"</span><span>$workdir</span><span>/barcode.bmp"</span>

<span># if not on macOS, replace with your image conversion tool of choice</span>
<span>sips</span><span> </span><span>-s</span><span> </span><span>format</span><span> </span><span>png</span><span> </span><span>"</span><span>$workdir</span><span>/barcode.bmp"</span><span> </span><span>--out</span><span> </span><span>"</span><span>$2</span><span>"</span>
</code></pre><p>Writing this script’s output to <code>strip.png</code> is all we need.</p><h2>Adding the barcode number</h2><p>I also wanted the barcode number to display under the barcode; this is simple enough to do with the secondary fields:</p><pre><code data-syntax-highlighted=""><span>    </span><span>"secondaryFields"</span><span>: [</span>
<span>      {</span>
<span>        </span><span>"key"</span><span>: </span><span>"number"</span><span>,</span>
<span>        </span><span>"label"</span><span>: </span><span>"CARD NUMBER"</span><span>,</span>
<span>        </span><span>"value"</span><span>: </span><span>"6942069420"</span>
<span>      }</span>
<span>    ],</span>
</code></pre><h2>Faking the barcode UX the rest of the way</h2><p>When a user selects a barcoded pass, the phone screen gets brighter to assist with scanners. iOS doesn’t think we have a barcode yet. I hoped that specifying an <em>empty</em> barcode would do the trick, and… yeah! It does! Specifying this in the top level keys of <code>pass.json</code> works:</p><pre><code data-syntax-highlighted=""><span>  </span><span>"barcodes"</span><span>: [</span>
<span>    {</span>
<span>      </span><span>"message"</span><span>: </span><span>""</span><span>,</span>
<span>      </span><span>"format"</span><span>: </span><span>"PKBarcodeFormatCode128"</span><span>,</span>
<span>      </span><span>"messageEncoding"</span><span>: </span><span>"iso-8859-1"</span>
<span>    }</span>
<span>  ],</span>
</code></pre><p>It’s the perfect workaround: no barcode is displayed at the bottom of the pass, but the phone acts like there’s a barcode present.</p><h2>Signing and packaging the pass</h2><p>Once all our files are in place, we need to generate the <code>manifest.json</code>, which is an object with filenames as keys and SHA-1 checksums as values. I wrote a terrifying jq filter to generate the manifest for me:</p><pre><code data-syntax-highlighted=""><span>sha1sum</span><span> </span><span>*</span><span>.png</span><span> </span><span>pass.json</span><span> </span><span>|</span><span> </span><span>\</span>
<span>    </span><span>jq</span><span> </span><span>-Rs</span><span> </span><span>'split("\n") | [ .[] | select(. != "") | split("  ") | {(.[1]): .[0]} ] | add'</span>
</code></pre><p>Now we need to sign the manifest, placing the signature at <code>signature</code>. This is done with the <code>openssl smime</code> command.</p><pre><code data-syntax-highlighted=""><span>openssl</span><span> </span><span>smime</span><span> </span><span>-binary</span><span> </span><span>-sign</span><span> </span><span>\</span>
<span>    </span><span>-signer</span><span> </span><span>pkpass.crt</span><span> </span><span>-inkey</span><span> </span><span>pkpass.key</span><span> </span><span>-certfile</span><span> </span><span>wwdrg4.pem</span><span> </span><span>\</span>
<span>    </span><span>-in</span><span> </span><span>manifest.json</span><span> </span><span>-outform</span><span> </span><span>der</span><span> </span><span>-out</span><span> </span><span>signature</span>
</code></pre><p>It’s possible to use whatever signing time you like<sup><a href="#user-content-fn-3" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-3">4</a></sup> with the <code>-attime</code> option, so if the certificate you got from Apple and/or found lying around is expired, you can still sign with it. The <code>-attime</code> option takes a UNIX epoch.</p><p>And now, we create a zip!</p><pre><code data-syntax-highlighted=""><span>zip</span><span> </span><span>-r</span><span> </span><span>out.pkpass</span><span> </span><span>*</span><span>.png</span><span> </span><span>pass.json</span><span> </span><span>manifest.json</span><span> </span><span>signature</span>
</code></pre><p>macOS comes with a pass previewer tool, so you can open this pass to check that it is valid and mostly looks right. (If it’s not valid, look for an error in Console.app.) The previewer isn’t 100% accurate, but it does have a button to send it to your phone via iCloud, which is pretty cool. You can also get it onto your phone in whatever manner is convenient.</p><figure><p><img alt="The final pass, which displays a logo, the text &quot;The Seattle Public Library&quot;, a barcode, and an obviously fake barcode number under it." src="https://iliana.fyi/blog/ios-wallet-library-card/pass.png"></p><figcaption>Not my real card number, sadly.</figcaption></figure><p>I will note that I have not yet tested this pass in a real library yet, but my barcode scanner can read it off my phone just as well as it can read it from my physical plastic card if I turn the screen brightness all the way up (yes, even further than the zero-length barcode workaround causes the screen to get brighter).</p><h2>Closing thoughts</h2><p>I think it’s pretty neat that the pass specification has remained pretty much unchanged in a decade. But I wish, like many things within the Apple ecosystem, that this didn’t require a $99 USD/year membership to get a certificate in order to sign an otherwise harmless pile of PNGs and JSON. There’s a few features you can use in passes that I’m glad require signing, but nothing I did here should require it, and I hope that changes someday.</p><p>Also I think Apple should add Codabar support to Wallet. I’m not aware of any library that supports using a digital form of a library card in-person, and I think with some tweaks the platform could support libraries without requiring an audit to ensure every barcode scanner across the system can support Code 128.</p><section data-footnotes=""><ol><li id="user-content-fn-blood"><p>And blood banks? <a href="#user-content-fnref-blood" aria-label="Back to content" data-footnote-backref="">↩︎</a></p></li><li id="user-content-fn-1"><p><code>@3x</code> was news to me! Apparently some newer phones have a 3× ratio now. <a href="#user-content-fnref-1" aria-label="Back to content" data-footnote-backref="">↩︎</a></p></li><li id="user-content-fn-2"><p>Given that most of these kinds of apps do not make the passes updatable via the internet, that these keys are limited to signing passes, and that the keys are specifically used in “make whatever pass you want” apps, I do not think there’s any reason to revoke the key I found. Unfortunately I do not trust Apple will accept this reasoning. <a href="#user-content-fnref-2" aria-label="Back to content" data-footnote-backref="">↩︎</a></p></li><li id="user-content-fn-3"><p><a href="https://stackoverflow.com/questions/66989389/consequences-of-the-expiration-of-the-signing-certificate-for-a-already-issued-p/66989932#comment130045743_66989932">Astute observation, Michael.</a> <a href="#user-content-fnref-3" aria-label="Back to content" data-footnote-backref="">↩︎</a></p></li></ol></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Samsung disables customer phones remotely, holds data hostage [video] (173 pts)]]></title>
            <link>https://www.youtube.com/watch?v=Ln4rsxWq3WM</link>
            <guid>38050381</guid>
            <pubDate>Sat, 28 Oct 2023 15:08:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=Ln4rsxWq3WM">https://www.youtube.com/watch?v=Ln4rsxWq3WM</a>, See on <a href="https://news.ycombinator.com/item?id=38050381">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Cruel Fantasies of Well-Fed People (118 pts)]]></title>
            <link>https://www.monbiot.com/2023/10/04/the-cruel-fantasies-of-well-fed-people/</link>
            <guid>38050236</guid>
            <pubDate>Sat, 28 Oct 2023 14:50:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.monbiot.com/2023/10/04/the-cruel-fantasies-of-well-fed-people/">https://www.monbiot.com/2023/10/04/the-cruel-fantasies-of-well-fed-people/</a>, See on <a href="https://news.ycombinator.com/item?id=38050236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        <article>

    

    <div>
        

<p>The astonishing story of how a movement’s quest for rural simplicity drifted into a formula for mass death</p>



<p>By George Monbiot, published on monbiot.com, 4<sup>th</sup> October 2023</p>



<div><p>Tourism sells to you the story of what it has taken away. It markets the “traditional” and “unchanging” and, in doing so, changes it. As the old joke goes, “come to this beautiful unspoiled island and spoil it.” The clock starts ticking from the moment the first person says “timeless”. Then everything that is celebrated starts to become the shadow of itself.</p><p>Similarly, food and farming, industries now intimately connected in some places with tourism, are caught in the endless tension between their reality and their representation. As soon as a region becomes celebrated for its food, gastrotourism accelerates its gentrification. Soon the old <em>boulangeries</em>, <em>boucheries</em> and <em>fromageries</em> are replaced with boutiques selling home decor and handbags. Then a new wave of specialist food shops appears among them, selling the old produce at boutique prices, prices that double with every adjective and place name.</p></div>



<p>The products are real but the stories that surround them – the autochthonous peasant economy, the weathered subsistence among woods and meadows – are vitiated by their telling. The tourist dollars they attract internationalise the local economy. They convert the barns and sheds where cows were kept, grapes were pressed, honey was strained into Airbnbs or second homes. Pastures on the valley floor become glamping sites or pony paddocks. The money generated by the myth of timelessness draws local people away from the sparse living the labels revere.</p>



<p>Meanwhile, to keep pace with gastronomic demand, farming relentlessly intensifies. In the famous cheesemaking regions of France, you will scarcely see a dairy cow. Instead, vast tracts are cultivated for maize. Passing tourists romantically assume it’s sweetcorn for human consumption – the French must eat a lot of sweetcorn! No, it will be turned into silage to feed the cattle stalled in the vast steel sheds – cow factories – that have sprung up from Brittany to Savoie, a business as brutal and industrial as any other. Milk is trucked across hundreds of kilometres, trade fairs market the cheese from Dubai to Shanghai.</p>



<p>The further the cycle of intensification turns, the longer the teleconnections of the “local” economy, the more bucolic and homely the marketing becomes: close-ups of cracked and dirt-grained hands, chickens clucking through buttercupped meadows, girls in Heidi costumes and all the other autophagous nonsense of the Spectacle.</p>



<h3>The Spectacle of Production</h3>



<p>To seek to reverse such economies of scale is not a relaxation into a “simpler” mode, but a conscious and frantic race against entropy. Fair play to those who succeed! We need, among others, small local producers, ideally using new forms of high-yield agroecology. But it’s not for the faint-hearted. They are running up the down escalator, and it accelerates every year. Land prices, house rents, low farmgate prices, easier and more rewarding opportunities elsewhere all militate against survival in the new-old economy, let alone success. And you are always in danger (a danger, admittedly, that some embrace) of becoming your own trope, the re-created peasant self-marketed to the gluttonous Spectacle.</p>



<p>Of course, we all re-create ourselves to some extent. We are all self-consumers. But self-creation is seldom more intense than in sectors deemed “authentic”.</p>



<p>Bucolic re-creations bear little relation to the things they claim to be. What we now fetishise as “peasant food” is much richer and more diverse than the food peasants would once have eaten, except, perhaps, on feast days. Meat was, for most, a luxury, cheese eaten less often than we imagine, salads, in many places, not at all. Diets were often inadequate and deficient in crucial components, such as protein. This is one of the reasons – alongside other aspects of health – why the rustic ancestors we celebrate were, on average, <a href="https://www.sciencedirect.com/science/article/abs/pii/S1570677X10000225">tiny by comparison</a> to us.&nbsp;</p>



<p>Many of the “traditional” ingredients considered essential to the cuisine – such as tomatoes in Italy or peppers in Spain – were unknown until surprisingly recently to those whose diets we claim to honour. Much of the protein, insufficient as it was, came not from cheese and meat but from what we now call dhal. It had names (to give some English examples) like pease pottage, pease pudding, mushy peas and pea soup. Few of these dishes are celebrated by gastronomes today. As George Crabbe remarked in <em>The Village</em>, written in 1783, such food was</p>



<p>“Homely not wholesome, plain not plenteous, such</p>



<p>&nbsp;As you who envy would disdain to touch.”</p>



<p>But, to the wealthy people spending lavishly on what they fondly imagine to be peasant diets, every day is a feast day.</p>



<h3>Lands of Plenty</h3>



<p>We benefit above all from a different legacy: the marvel of the past 50 years of falling hunger during a time of rising population, a marvel we in the rich world scarcely acknowledge, so comfortable has it made us. This remarkable phenomenon was widely considered, just 60 or 70 years ago, simply impossible.</p>



<p>There are three things upon which I think we can all agree. First, that this marvel came at a great environmental cost. It was delivered through hungry and thirsty new crop varieties, reliant for their survival on lashings of agrochemicals, unsustainable water use and practices that can accelerate soil degradation. Second, that it also involved severe social and political dislocations, including land-grabbing, enclosures and rising corporate power and concentration. Third, that it might now be running out of road: the <a href="https://www.fao.org/3/CC3017EN/online/CC3017EN.html">prevalence of global undernourishment</a> rose from 613 million (median estimate) in 2019 to 735 million in 2022.</p>



<p>The immediate reasons for this partial reversal are the Covid-19 pandemic and Russia’s invasion of Ukraine, but there are also three deeper and increasingly urgent issues: the decline of crucial resources, such as soil and water, environmental shocks hammering farm production, and the global food chain’s <a href="https://www.monbiot.com/2023/03/09/the-hunger-gap/">loss of systemic resilience</a>.</p>



<p>So the question – one of the key questions of our time – is how we can feed a population likely to rise to 9 or 10 billion by the middle of the century before starting to decline, reliably, equitably and at a much lower environmental cost. In other words, how we might feed the world without devouring the planet, the subject of <a href="https://www.penguin.co.uk/books/317018/regenesis-by-monbiot-george/9780241447642">my book <em>Regenesis</em></a>.</p>



<p>There are, as I found, plenty of possible ways forward. But there are no ways backward. If we were to seek to restore the agricultural systems of, say, 60 or 70 years ago, a time, remember, when many people were deeply pessimistic about human nutrition and expected global starvation as the population rose, their grim predictions would materialise. Why? Because productivity was much lower than it is today. In 2023, a world of 8.1 billion people suffers far less hunger and famine than the world of 3.2 billion did in 1963, the year of my birth.</p>



<p>Let’s pause to consider this for a moment, because it is one of the most remarkable (and, bizarrely, least celebrated) transformations of our time.</p>



<p>The numbers who died in famines were especially high in the 1960s, as a result of China’s Great Leap Backwards. An estimated 16.6 million <a href="https://ourworldindata.org/famines#long-term-trends-in-global-famine-mortality">perished during</a> mass starvation events in that decade. This compares to 8.8 million in the 1950s and 3.4 million in the 1970s. But 3.4 million, by comparison to more recent figures, is massive. Between 2010 and 2016, the most recent years in the standardised dataset, 255,000 people died this way, all of them in the famine that afflicted Somalia. Since then, there have been four major famines: in Yemen, South Sudan, Somalia (again) and Tigray, in which, in total, hundreds of thousands died. All four were caused by conflict. Famine is also much less geographically widespread than it used to be: it now tends to be confined to one nation or province at a time, rather than afflicting vast areas.</p>



<p>To grasp just how astonishing this decline in mass death through hunger has been, we need to look at the <a href="https://ourworldindata.org/uploads/2018/03/Famine-death-rate-since-1860s-revised.png">death <em>rate</em> in famines</a> as a proportion of the population. A century ago, the rate stood at 82 per 100,000 people. In the 1930s, it was 56, in the 1940s, 79, the 1950s, 32, the 1960s, 50, the 1970s, 8.4, and on down to the most recent figures: 0.5. At no known point in recorded history has the third horseman wielded less deadly power.</p>



<p>There’s a similar trend in total deaths from malnutrition (in other words not only those that occurred in the mass events known as famines). These fell, <a href="https://ourworldindata.org/grapher/malnutrition-deaths-by-age">on a fairly steady trajectory</a>, from 656,000 in 1990 to 212,000 in 2019.</p>



<p>What lies behind these extraordinary trends? There are several reasons, but let me dwell on two of the crucial ones. One is the much greater availability of food per person. This is also a remarkable phenomenon. <em>Our World in Data</em>, which collates such global figures, <a href="https://ourworldindata.org/yields-vs-land-use-how-has-the-world-produced-enough-food-for-a-growing-population">shows that</a> between 1961 and 2014 the world’s production of cereals rose by 280%. This is twice the increase in the global population during that period (136%). It was achieved almost entirely through higher crop yields per hectare.</p>



<p>Another is the long-distance transport of food, something that many of us have railed against, but which, for all its downsides, makes an essential contribution to falling rates of hunger. The reason is simple: if there is a bad harvest or outright crop failure in one place, food can now be shifted from regions with a crop surplus, either through trade or through aid and famine relief programmes. The extreme globalisation of the food system has introduced new sets of problems. But without long-distance transport, many more would starve.</p>



<p>Returning to earlier modes of subsistence is a formula for global catastrophe on a scale that defies imagination.</p>



<h3>The Great Divide</h3>



<p>To make these obvious statements is to become the sworn enemy of many food and farming writers, influencers and film-makers (who have a lucrative industry of their own to support). It is to commit the modern equivalent of blasphemy, as food nostalgia inspires semi-religious beliefs.</p>



<p>To make these statements with the support of numbers is to multiply the sin. As I’ve discovered since publishing the book, if there is one habit that incites fury more reliably than any others, it is to put numbers on the problem. Hectares, yields, nutrients, calories, inputs, outputs, costs, emissions, hunger, death: any form of quantification is as welcome in this arena as a tambourine in a Bach sonata.</p>



<p>Why? Because the romantic story of how food “should” be produced is entirely qualitative. It’s an aesthetic reverie. It’s about pictures, poetry, gut feeling – understandable when it comes to food but, literally, lethal when it comes to ensuring everyone has it. It is the great indulgence of those who never miss a meal to celebrate the times and modes in which people missed plenty.</p>



<p>There are two entirely different questions here: “what production systems do certain well-nourished food writers in the rich world want to see?”, and “how might everyone on Earth be fed?”. But, though often leading to very different conclusions, they are endlessly and callously confused with each other.</p>



<p>Fantasising about a food system in which the third horseman would ride victorious again is among the more perverse habits of comfortable people. The anger and passion with which some of them defend their formula for starvation is a wonder to behold. They privilege their aesthetics – their arcadian fantasies – above the wellbeing of 8 billion people.</p>



<p>Perhaps we shouldn’t be surprised: nostalgia is among the most powerful and dangerous of all social forces. Woe betide the person who seeks to disabuse another of their fantasies about the past!</p>



<p>Well, I thought I had seen it all: the full gamut of cruel fantasies which privilege bucolic comfort zones above global necessities. But this was before I read the new book by Chris Smaje, a small farmer and writer with an academic background, called <em><a href="https://www.chelseagreen.com/product/saying-no-to-a-farm-free-future/">Saying No to a Farm-Free Future</a></em>. The book has been praised by a number of prominent food, farming and environmental writers and campaigners, most of whom subscribe to the worldview I have just described. It is becoming something of a bible for their movement. It promotes what appears to be a recipe for mass global starvation.</p>



<p>Before I go any further, I want to emphasise that Chris has every right to write and publish this book. He has been criticised online for fomenting internecine war within left/green circles (as have I). I don’t see it that way. The argument in which he participates is a crucial one, the divisions are real and the debate needs to be had. I believe this is in fact the greatest of all rifts within environmental movements, and we do ourselves no favours by pretending it does not exist. Though his book is framed as an attack on my book <em>Regenesis</em> and, more broadly, on me, I’m glad he has written it. It is highly instructive.</p>



<h3>Mark of the Beast</h3>



<p>Chris launches his attack with labels. Apparently I’m both an “ecomodernist” and an “urbanist”. He fails to define what he means by an ecomodernist. His use of the term would capture anyone who favours new or newish green technologies: solar panels, wind turbines, electrified railways, GIS mapping, induction hobs ….</p>



<p>I <em>do</em> have a definition of ecomodernism:</p>



<p>a movement that treats green technology as a <em>substitute</em> for political and economic change.</p>



<p>I see it in the vision of people like Bill Gates and Ted Nordhaus, a vision <a href="https://www.theguardian.com/commentisfree/2022/may/13/optimism-climate-predictions-techno-polluters">I strongly oppose</a>. I believe technology is just one of the components of the change we need: necessary but not sufficient. I’ve spent my working life pressing for political and economic change, seeking to dethrone the oligarchs (including Bill Gates) and corporations whose economic and political power impede both democracy and human flourishing. &nbsp;</p>



<p>As for the charge of being an “urbanist”, the only evidence he advances is that a Dutch website calls for 90% of the population to live in cities by 2100. I had nothing to do with this, and the idea appals me as much as it appals Chris. But the way he writes this passage makes it look as if it’s my view: a shocking elision. I was not surprised to see a commentary on his book (since changed at my request) <a href="https://www.frontporchrepublic.com/2023/09/hope-for-a-humane-agricultural-future-a-review-of-saying-no-to-a-farm-free-future/">reporting this terrible idea</a> as “one of Monbiot’s proposals”.</p>



<p>On these fictitious grounds, Chris states that I want a “depopulated countryside”, an “un-peopled” nature, to “eliminate” ruralism, to “keep as many people as possible out of garden-sized or small farm-sized patches in the countryside” and to “concentrat[e] people in the cities as helpless consumers”. I want none of these things. In fact, I strongly oppose them all. I will state my position once more, but with no confidence that he or others will hear it: <em>I do not want to see any depopulation of the countryside.</em></p>



<p>But in one respect, though I have no great enthusiasm for cities, I guess you could call me an urbanist. Why? Because I believe urban populations are a reality that cannot be wished away and, crucially, that they should have food. These, if I’m reading his book correctly, appear to be the two great dividing lines between my vision and Chris’s: I don’t want anyone to have to leave their homes; and I believe urban people need to be fed.</p>



<p>The “ecomodernist” and “urbanist” labels could be seen as the usual cut and thrust of debate: by attaching an alienating definition to someone, you might induce people to stop listening to what they’re saying, and to dismiss their evidence and arguments out of hand. It’s the way certain politicians turn complex and difficult questions into culture war fodder. But as the book goes on, his labelling takes a much darker turn.</p>



<h3>Priced Out</h3>



<p>Chris devotes an entire chapter to inveighing against people (I am, apparently, the archetype), who believe that sustaining high crop yields is a good idea. Apparently, this is “agricultural improver ideology”, which is in turn an “urban-industrial articulation of class power against rural and agrarian people”. Sorry, what?</p>



<p>He explains this contention as follows: “The narrative of agricultural improvement has always had this class element to it – a concern with class improvement for unmanaged farmers as well as agricultural improvement for unmanaged farming.” Yup, that’s the real agenda: a cunning plot to improve farmers’ table manners. My interest in high yields couldn’t possibly be because I worry about how, without them, 8 billion people might be fed.</p>



<p>But this is by no means the end of it. If you believe that enough food should be grown to feed &nbsp;everyone, you are also guilty of “productivism”, “consumerism”, even “colonialism”.</p>



<p>This brings us to the issue he carefully swerves throughout the book: that a certain number of people requires a certain amount of food, and this food has both to be produced and to reach those who need it. If there’s not enough food, or it’s not accessible and affordable to everyone, people will starve. It seems extraordinary to have to point this out.</p>



<p>One of the reasons why high yields ensure that more people can be fed is that more supply reduces the price of food, making it more accessible to the poor. Chris flatly rejects this reasoning. He asserts that “Low food prices, high yields and overproduction are absolutely at the root of food system problems, including global poverty and hunger.” He then goes on to make two statements that left my jaw on the floor:</p>



<p>Lower food prices are “the last thing the global poor need. The result is usually more poverty, more hunger”.</p>



<p>and</p>



<p>“Higher food prices might alleviate hunger globally”.</p>



<p>You might have imagined that such astonishing statements would be carefully explained and evidenced. But they are asserted without justification. The closest he gets is to point out that, in countries like the UK, people spend more on housing and energy than they do on food (which is true) and that the cheapness of this food helps the owners of housing and energy to generate more profit, which might be true, but would need some unpacking.</p>



<p>But this says nothing about the situation of the <em>global</em> poor, the subject of those two astonishing statements. So let’s just take a moment.</p>



<p>The <a href="https://www.fao.org/newsroom/detail/global-indicators-on-the-costs-of-healthy-diets-and-how-many-people-can-t-afford-them/en">global definition</a> of an affordable diet is one that costs 52% or less of average household expenditure. Using this definition, 3 billion people – over one-third of the global population – <a href="https://ourworldindata.org/diet-affordability">cannot afford a healthy diet</a>. In other words, buying adequate food would mean spending more on it than on housing, energy, education, health, transport, clothing and all other items put together.</p>



<p>Importantly, the 3 billion below the line include not just urban people and rural people working in the non-agricultural economy, but also many subsistence farmers, some of whom cannot produce enough food, and of sufficient diversity, to meet their nutritional needs.</p>



<p>In some countries, a healthy diet costs more than the median income: even if people spent <em>all</em> their money trying to purchase one, they still couldn’t afford it. Yes, the problem is poverty: a gross maldistribution of wealth. Yes, this maldistribution urgently needs to be addressed, which is why we need political and economic change, not just new technologies. But while I have seen no evidence (and Chris provides none) that higher food prices alleviate global hunger, there is a <a href="https://theconversation.com/further-food-price-rises-could-cause-up-to-1-million-additional-deaths-in-2023-199120">wealth of evidence</a> that they exacerbate it.</p>



<p>Is it really possible that you can write a book on food and farming and fail to grasp this basic fact? Yes, it seems it is. <em>Saying No to a Farm-Free Future</em> is a powerful lesson in how motivated reasoning can lead you to an utterly perverse and ludicrous position.</p>



<h3>Let Them Eat Nothing</h3>



<p>So how does Chris Smaje believe people should be fed? After launching such a ferocious attack on evil bastards like me, you might expect him to produce a clear alternative. But another remarkable aspect of this book, and of the movement it speaks to, is how vague it becomes on such trifling matters as producing sufficient food for 8 billion people. Here are the most specific phrases I could find, while trying to decipher how he proposes that everyone on Earth should be fed.</p>



<p>“Predominantly local self-provisioning of food, fibre and other material requisites of life”</p>



<p>We should “gain autonomy and feed ourselves”</p>



<p>People should “spread themselves out in the landscape and make low energy livelihoods there”</p>



<p>“Repeasantisation, where commercial farmers step off the productivity treadmill and …. orient themselves instead to more autonomous local agricultures geared to local needs”</p>



<p>“We could boost urban food provision by increasing the number of allotments, community gardens, market gardens and truck farms on brownfield sites”</p>



<p>So the question which arises – and please forgive this ecomodernist, urbanist, productivist, consumerist, colonialist framing – is who, in this world of “self-provisioning” and “repeasantised” commercial farmers, will feed those who do not feed themselves?</p>



<p>Most of the places where large numbers of people live do not have sufficient fertile land nearby to support them. A paper in the journal <em>Nature Food</em> <a href="https://www.nature.com/articles/s43016-020-0060-7">found that</a> only a quarter of the world’s people could be fed with staple grain crops grown within 100 kilometres of where they live. The average minimum distance at which the world’s people can be supplied with staple foods, it found, is 2,200 kilometres. Much of the world’s food is grown in vast, lightly-habited lands (US plains, Canadian prairies, Russian steppes etc) and shipped to tight, densely-populated places.</p>



<p>These are the numbers to which people of Chris’s persuasion most furiously object, even though they have no answer to them. Why? Because the numbers are incompatible with their worldview. They show that, while agrarian localism might be great as far as it goes, it simply cannot, by itself, meet the challenge of feeding the world.</p>



<p>Cities can grow only a tiny fraction of their food, as Chris acknowledges elsewhere in his book. Again, it’s not hard to work out why. Urban areas occupy only 1% of the planet’s land, and this land is in high demand for other uses. Allotments, community gardens, market gardens and truck farms are wonderful things to have, enhancing urban life, but they can produce only a very small proportion of a city’s demand for fruit and vegetables, and close to none of its staple foods.</p>



<p>While this is predominantly an urban issue, it’s not just big cities which rely on non-local production. There are many areas dominated by smaller settlements which simply do not have the agricultural capacity to feed themselves. Even regions which are blessed with sufficient agricultural land and water could, as has happened many times in the past, see their production wiped out by local harvest failure, ensuring that a world of agrarian localism would, once more, become a world in which famine is ever-present and widespread.</p>



<p>So how will the 4.5 billion people who live in cities – over 60% of the global population – and many others living where there is little fertile land be fed? Answer comes there none. Seriously: in 159 pages there is no explanation of how they would survive. If you’re not an agrarian localist either producing your own food or buying from local growers, you’re stuffed – or rather, starved. If Chris has a plan for feeding you, he hasn’t mentioned it.</p>



<p>Discussing his own, proudly low-yield production of wheat and potatoes, Chris states:</p>



<p>“there’s no point labouring for next to nothing on someone else’s behalf when you’ve already grown enough to eat for yourself.”</p>



<p>This is why farmers who do not share his worldview pursue higher yields: these yields make it economically worthwhile to produce staple foods that can be sold to other people. We should thank our lucky stars for such people.</p>



<h3>“Human Feedlots”</h3>



<p>How did he get to this point? I can’t see into his mind, but part of the reason might be his hatred of cities. He rails against them like an Old Testament prophet denouncing Sodom and Gomorrah.</p>



<p>He describes them as “human feedlots”, a term I find grotesque and dehumanising. They “consume everything around them and then themselves”. They are “built on cheap and abundant energy and models of globalised trade that aren’t destined to endure.”</p>



<p>It is true that cities rely on unsustainable and exploitative models of extraction, consumption and dumping. But this applies to the economy as a whole, urban or otherwise. The answer, I believe, is not to rain curses on them and their people, but to replace the destructive economic models with systems in which everyone’s needs are met without breaching planetary boundaries. This is what Kate Raworth’s <a href="https://www.theguardian.com/commentisfree/2017/apr/12/doughnut-growth-economics-book-economic-model"><em>Doughnut Economics</em></a> seeks. I believe we can move towards her vision with the help of what I call “<a href="https://centerforneweconomics.org/publications/private-sufficiency-public-luxury-land-is-the-key-to-the-transformation-of-society/">private sufficiency, public luxury</a>”. None of this, of course, magics away the need to produce sufficient food.</p>



<p>If I interpret his airy euphemisms correctly, the question of how urban people should be fed is not worth answering, because cities are soon going to collapse, and their people will have no choice but to “spread themselves out in the landscape”, growing their own “food and fibre, building shelter, producing a modest livelihood from the local ecological base.” (Never mind that in many places the “local ecological base” could support only a small fraction of the region’s people).</p>



<p>When your solution is societal collapse, you should ask yourself some hard questions about what you are trying to solve.</p>



<h3>The Great Cruelty</h3>



<p>I guess there’s a small consolation here: that Chris might have given up on the idea that his <em>xià xiā</em><em>ng</em> – the mass migration to the countryside he envisages – will happen voluntarily. Perhaps he has at last realised that most people have no particular desire to have to grow their own food and fibre, make their own clothes and build their own shelters. Instead, he now appears to believe that urban people will be forced by catastrophe to leave the cities and succumb to “re-ruralisation” and “repeasantisation”. It might be worth noting that the Old Testament prophets also foresaw the imminent collapse of urban life, two and a half thousand years ago.</p>



<p>How mild and gentle he makes it sound! Refugees from the cities “spreading themselves out in the landscape”, “producing a modest livelihood from the local ecological base.” When the fugitives disperse into the countryside, the inhabitants will doubtless greet them with open arms, saying, “Welcome sister! Welcome brother! Here, have some fertile land. Oh, and some water, knowledge, skills, tools, traction and all the other means to grow your own food and live happy lives as re-peasants in our agrarian wonderland.”</p>



<p>If history is any guide, this is not quite how it’s likely to pan out. The more probable outcomes of societal collapse include warlordism or full-scale war, coercion, fascism, slavery, disease, starvation and mass death.</p>



<p>Moreover, if a cataclysm is sufficient to bring the cities down, it is likely also to destroy the basis of much of rural life. After all, the distinctions between the two are not nearly as crisp as Chris would have us believe. In fact, and horrifyingly, it’s likely that, as a result of environmental disaster, rural life in many parts of the world will collapse <em>before</em> urban life does, as suggested by a <a href="https://www.nature.com/articles/s41893-023-01132-6">highly disturbing recent paper</a> in <em>Nature</em>, showing how and where the “human climate niche” is likely to shrink. If anything, we are likely to become more reliant on long-distance transport to deliver our food – a prospect no one, myself included, relishes.</p>



<p>If a catastrophe of the kind Chris envisages – and sometimes appears to yearn – were to materialise, people everywhere are likely to become more desperate. The remaining fertile land and water would be even more valued and fiercely defended than they are today.</p>



<p>To me, Chris’s long-standing plan – to move the people to the food, rather than the food to the people – is a further instance of the Great Cruelty of the past two centuries. The Great Cruelty is common to colonialism, capitalism, communism, Nazism, neoliberalism and all the other conquering and interconnected forces that have dominated thought and action during this period. It can be summarised as follows:</p>



<p>People are counters, to be moved in their millions, as interests or ideology dictate, across the board game called Planet Earth.</p>



<p>It’s consistent with the kind of thinking that characterises cities as “human feedlots”.</p>



<h3>Mysteries and Passions</h3>



<p>But never mind. Whether there is enough food and everyone can afford to eat it is, Chris says, a “secondary goal”. This is because it doesn’t “speak to the mysteries and passions of what animates human (or non-human) life”, which Chris describes as our “primary goals”. In obsessing about “productivity, numbers, yields, costs and so forth”, we “risk missing what makes for the flourishing of humans and other organisms”.</p>



<p>Well, call me old-fashioned, or urbanist, or whatever label you choose to apply to me, but I would say that having enough food is pretty damn primary. In any hierarchy of human needs, it features close to the top. I don’t for a moment deny that mysteries and passions are important to us, or that we need meaning and purpose to lead fulfilling lives, but their pursuit can be somewhat hampered if you are starving to death.</p>



<p>This is the heart of the matter. The particular “mysteries and passions” that appeal to people of Chris’s persuasion come first, and the physical requirements of other humans are secondary: they must either fit in somehow, or fall aside.</p>



<p>There is now a wide movement, some of whose leading figures are quoted on or in Chris’s book, that prioritises its mysteries and passions above other people’s survival to the point at which it promotes the idea of <a href="https://orionmagazine.org/article/confessions-of-a-recovering-environmentalist/">“withdrawing” and “walking away”</a> from the effort to prevent Earth systems collapse. On behalf of the rest of the world, such people grandly proclaim that it’s futile to try to stop the slide. We should give up and “adapt to”, even embrace, whatever awaits.</p>



<p>But there is no “away” to walk to. Ecological and social collapse will find us, wherever we go. What some people <em>can</em> escape is the shared responsibility for facing our multiple crises, and their duty of care towards others.</p>



<p>The acceptance of – sometimes apparent longing for – collapse is among the greatest self-indulgences in human history. It is peculiar to people who are either relatively wealthy and insulated, or have the land and means to grow their own food (or both). It is a variation of the prepper mentality, whose props in this case are not bunkers, bitcoin, tinned food and AR-15s, but hoes, scythes and leather jerkins. (Though these “repeasantised” folk might discover that if the calamity does occur, they’ll also need some heavy weaponry to defend their land and crops).</p>



<p>While the Old Testament prophets had to rely on God’s wrath being visited upon the human feedlots, today the curses have more temporal means of realisation. All we now need do is nothing: let the corporations, the oligarchs and the rising consumer demand that are breaking Earth systems have their way, and some form of collapse is likely to occur, with or without God’s wrath. By deliberately stepping back from the struggle to contain these forces, and even seeking in some cases to dissuade others from participating, they make this possibility more likely.</p>



<p>My belief is that we have no right to grant ourselves this indulgence. Given that rich nations and wealthy people are primarily responsible for the planetary dysbiosis we all face, including the massive burdens the food system imposes on the living world, we all have a duty to engage. Engaging means valuing the lives of others as we value our own. Living on this planet, especially as a member of a privileged society, our lives are intimately bound with the lives of others, including those who live thousands of miles away. We cannot excuse ourselves from the responsibilities we owe to each other. Our aim should be not to use societal collapse as a tool to shape the world to our tastes, but to seek to avert societal collapse.</p>



<h3>Discordant Notes</h3>



<p>There are no perfect solutions in an imperfect world. Everything we might propose, including all the ways forward I suggest in <em>Regenesis</em>, has downsides. We are working in a very tight space, one in which 8 billion people and more need to be fed, within an Earth system whose planetary boundaries have already been breached, to a large extent as a result of food production.</p>



<p>There are no remaining comfort zones. There is no longer – if there ever was – scope for ideological congruence, for solutions that fit snugly into any one worldview. We will find ourselves in disconcerting places. We will be assailed by cognitive dissonance.</p>



<p>In seeking to address our great predicaments, we should be, as much as is humanly possible, open-minded, open-hearted, receptive to evidence, argument and persuasion. The answers, contradictory, incomplete and inadequate as they will always be, will be social, political, economic, organisational <em>and</em> technological. We might not like some of our own conclusions. But it’s not just about us.</p>



<p>When some writers and campaigners, prioritising their own mysteries and passions, appear to treat billions of people as disposable, it should tell us something important: we need to check ourselves. We need to ask what impulses we are following, whether we are really seeking the best outcomes for humanity and the living planet, or simply avoiding cognitive pain. We need, as much as we are able, to set our passions aside.</p>



<p><em>Please feel free to republish this essay, without asking my permission, as long as the original source is linked and credited.</em></p>



<p>www.monbiot.com</p>
    </div>

            
    
</article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MagicaVoxel – A free voxel art editor and interactive path tracing renderer (213 pts)]]></title>
            <link>https://ephtracy.github.io/</link>
            <guid>38050106</guid>
            <pubDate>Sat, 28 Oct 2023 14:32:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ephtracy.github.io/">https://ephtracy.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38050106">Hacker News</a></p>
<div id="readability-page-1" class="page">

    <div>
           <p>

                <a href="#">
                    E
                </a>
            </p>


            <div id="mv_nav">
                <ul>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_main" id="mv_main" mv_content_ref="mv_main.html">MagicaVoxel</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=magicacsg" id="magicacsg" mv_content_ref="magicacsg.html">MagicaCSG</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=aerialod" id="aerialod" mv_content_ref="aerialod.html">Aerialod</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_renderer" id="mv_renderer" mv_content_ref="mv_renderer.html">Viewer</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_controls" id="mv_controls" mv_content_ref="mv_controls.html">Shortcuts</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_commands" id="mv_commands" mv_content_ref="mv_commands.html">Commands</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_resource" id="mv_resource" mv_content_ref="mv_resource.html">Resources</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_contact" id="mv_contact" mv_content_ref="mv_contact.html">Contact</a></li>
                </ul>
            </div>
        </div>

    

    



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The UK's Controversial Online Safety Act Is Now Law (126 pts)]]></title>
            <link>https://www.wired.com/story/the-uks-controversial-online-safety-act-is-now-law/</link>
            <guid>38048811</guid>
            <pubDate>Sat, 28 Oct 2023 10:56:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/the-uks-controversial-online-safety-act-is-now-law/">https://www.wired.com/story/the-uks-controversial-online-safety-act-is-now-law/</a>, See on <a href="https://news.ycombinator.com/item?id=38048811">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Jeremy Wright was the first of five UK ministers charged with pushing through the British government’s landmark legislation on regulating the internet, the Online Safety Bill. The current UK government likes to brand its initiatives as “<a data-offer-url="https://bylinetimes.com/2023/04/28/the-uks-world-beating-rhetoric-a-distraction-from-reality/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://bylinetimes.com/2023/04/28/the-uks-world-beating-rhetoric-a-distraction-from-reality/&quot;}" href="https://bylinetimes.com/2023/04/28/the-uks-world-beating-rhetoric-a-distraction-from-reality/" rel="nofollow noopener" target="_blank">world-beating</a>,” but for a brief period in 2019 that might have been right. Back then, three prime ministers ago, the bill—or at least the white paper that would form its basis—outlined an approach that recognized that social media platforms were already de facto arbiters of what was acceptable speech on large parts of the internet, but that this was a responsibility they didn’t necessarily want and weren’t always capable of discharging. Tech companies were pilloried for things that they missed, but also, by free speech advocates, for those they took down. “There was a sort of emerging realization that self-regulation wasn’t going to be viable for very much longer,” Wright says. “And therefore, governments needed to be involved.”</p><p>The bill set out to define a way to handle “legal but harmful” content—material that wasn’t explicitly against the law but which, individually or in aggregate, posed a risk, such as health care disinformation, posts encouraging suicide or eating disorders, or political disinformation with the potential to undermine democracy or create panic. The bill had its critics—notably, those who worried it gave Big Tech too much power. But it was widely praised as a thoughtful attempt to deal with a problem that was growing and evolving faster than politics and society were able to adapt. Of his 17 years in parliament, Wright says, “I’m not sure I’ve seen anything by way of potential legislation that’s had as broadly based a political consensus behind it.”</p><p>Having passed, eventually, through the UK’s two houses of Parliament, the bill received royal assent today. It is no longer world-beating—the European Union’s competing <a href="https://www.theverge.com/23845672/eu-digital-services-act-explained" target="_blank">Digital Services Act</a> came into force in August. And the Online Safety Act enters into law as a broader, more controversial piece of legislation than the one that Wright championed. The act’s more than 200 clauses cover a wide spectrum of illegal content that platforms will be required to address and give platforms a “duty of care” over what their users—particularly children—see online. Some of the more nuanced principles around the harms caused by legal but harmful content have been watered down, and added in is a highly divisive requirement for messaging platforms to scan users’ messages for illegal material, such as child sexual abuse material, which tech companies and privacy campaigners say is an unwarranted attack on encryption.</p><p>Companies, from Big Tech down to smaller platforms and messaging apps, will need to comply with a long list of new requirements, starting with age verification for their users. (Wikipedia, the eighth-most-visited website in the UK, has said it <a href="https://www.bbc.co.uk/news/technology-65388255">won’t be able to comply</a> with the rule because it violates the Wikimedia Foundation’s principles on collecting data about its users.) Platforms will have to prevent younger users from seeing age-inappropriate content, such as pornography, cyberbullying, and harassment; release risk assessments on potential dangers to children on their services; and give parents easy pathways to report concerns. Sending threats of violence, including rape, online will now be illegal, as will assisting or encouraging self-harm online or transmitting deepfake pornography, and companies will need to quickly act to remove them from their platforms, along with scam adverts.</p><p>In a statement, UK Technology Secretary Michelle Donelan said: “The Bill protects free speech, empowers adults and will ensure that platforms remove illegal content. At the heart of this Bill, however, is the protection of children. I would like to thank the campaigners, parliamentarians, survivors of abuse and charities that have worked tirelessly, not only to get this Act over the finishing line, but to ensure that it will make the UK the safest place to be online in the world.”</p><p>Enforcement of the act will be left to the UK’s telecommunications regulator, Ofcom, which <a data-offer-url="https://www.ofcom.org.uk/online-safety/information-for-industry/roadmap-to-regulation/0623-update" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ofcom.org.uk/online-safety/information-for-industry/roadmap-to-regulation/0623-update&quot;}" href="https://www.ofcom.org.uk/online-safety/information-for-industry/roadmap-to-regulation/0623-update" rel="nofollow noopener" target="_blank">said in June</a> that it would begin consultations with industry after royal assent was granted. It’s unlikely that enforcement will begin immediately, but the law will apply to any platform with a significant number of users in the UK. Companies that fail to comply with the new rules face fines of up to £18 million ($21.9 million) or 10 percent of their annual revenue, whichever is larger.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Some of the controversy around the act is less about what is in it and more about what isn’t. The long passage of the legislation means that its development straddled the Covid-19 pandemic, giving legislators a live view of the social impact of mis- and disinformation. The spread of anti-vaccination and anti-lockdown messages became an impediment to public health initiatives. After the worst of the pandemic was over, those same falsehoods fed into <a href="https://www.wired.com/story/15-minute-cities-conspiracy-climate-denier/">other conspiracy theories</a> that continue to disrupt society. The original white paper that was the bill’s foundation included proposals for compelling platforms to tackle this kind of content—which individually might not be illegal but which en masse creates dangers. That’s not in the final legislation, although the act does create a new offense of “false communications,” criminalizing deliberately causing harm by communicating something the sender knows to be untrue.</p><p>“One of the most important things was tackling harms that happen at scale. And because it’s focused so much on individual pieces of content, it’s missed that,” says Ellen Judson, head of the digital research hub at the think tank Demos. The act includes strict rules forcing platforms to move swiftly to remove any illegal post—such as terrorist content or child sexual abuse material—but not on disinformation campaigns comprised of a drip-drip of misleading content, failing to understand that “when that turns into things going viral and spreading, then the harm can occur cumulatively.”</p><p>Wright says that the exclusion of disinformation and misinformation from the bill was partly due to confusion between the remits of different departments. The Department of Culture, Media and Sport “was told that the Cabinet Office would be taking care of all of this. ‘Don’t you worry your pretty little heads about it, it’ll be done elsewhere in something called the <a data-offer-url="https://hansard.parliament.uk/commons/2019-07-22/debates/19072238000019/DefendingDemocracyProgramme" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://hansard.parliament.uk/commons/2019-07-22/debates/19072238000019/DefendingDemocracyProgramme&quot;}" href="https://hansard.parliament.uk/commons/2019-07-22/debates/19072238000019/DefendingDemocracyProgramme" rel="nofollow noopener" target="_blank">Defending Democracy agenda</a>,’” he says. “And then I think, subsequently, it wasn’t really. So I think … there still is a gap there.”</p><p>Under the Act, bigger platforms will be expected to police potentially harmful, but not illegal, content by applying their own standards more consistently than they currently do—something that free-speech campaigners have decried as giving private companies control over what’s acceptable discourse online, but which some experts on dis- and misinformation say is a cop-out that means Big Tech will be less accountable for spreading falsehoods. Legal experts, however, say compliance with the law will require platforms to be more transparent and proactive. “They have to put all of those processes in place as to how their decisions will be made, or they risk actually being seen as a platform that is controlling all kinds of free speech,” says Emma Wright, technology lead at the law firm Harbottle &amp; Lewis. That’s likely to become quite a significant burden. “It’s the new <a href="https://www.wired.com/story/gdpr-2022/">GDPR</a>,” she says.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>By far the most divisive clause out of the more than 300 pages of the Online Safety Act is Section 122, which has been widely interpreted as compelling companies to scan users’ messages to make sure that they aren’t transmitting illegal material. That would be incredibly difficult—perhaps even impossible—to do without breaking the end-to-end encryption on platforms such as WhatsApp and Signal. End-to-end encryption means that the sender and recipient of a message can see its content but the owner of the platform that it’s sent on cannot. The only way to comply with the law, experts say, would be to put so-called client-side scanning software on users’ devices to examine messages before they’re sent, which would make the encryption largely useless. The government said during the bill’s development that companies could find a technical solution to scan messages without undermining encryption; companies and experts countered that that technology doesn’t, and may never, exist.</p><p>“That gives Ofcom, as a regulator, the ability to obligate people like us to go and put third-party content monitoring [on our products] that unilaterally scans everything going through the apps,” Matthew Hodgson, CEO of encrypted messaging company Element, told WIRED before the bill passed. “That’s undermining the encryption and providing a mechanism where bad actors of any kind could compromise the scanning system in order to steal the data flying around the place.”</p><p>Companies whose products depend on end-to-end encryption threatened to leave the country, including Signal. Meta said it may pull WhatsApp from the UK if the bill were to pass. That cliff edge has come and gone, and both services are still available—albeit after an 11th-hour restatement by the government that it wouldn’t force platforms to adopt nonexistent technology to scan users’ messages—which was seen by some as a climbdown.</p><p>However, the clause remains in the act, which worries privacy and free-speech activists, who see it as part of a spectrum of threats against encryption. If the Online Safety Act means companies have to remove encryption or circumvent it using client-side scanning, “it then potentially opens [data] up to being scooped up into the broader surveillance apparatus,” according to Nik Williams, policy and campaigns officer at the campaign group Index on Censorship.</p><p>The Online Safety Act has concerning overlaps with another piece of legislation, the Investigatory Powers Act, which allows the government to compel platforms to remove encryption. Williams says the overlap between the two pieces of legislation creates “a surveillance gateway between the OSB and the IPA in that this can give the security services, such as MI5, MI6, and GCHQ, access to data they previously could not access … I would say it’s probably an unprecedented expansion of surveillance powers.”</p><p>The morning after the Online Safety Bill passed through the House of Lords, the UK Home Office <a href="https://www.bbc.co.uk/news/technology-66854622">launched a new campaign</a> against encrypted messaging, specifically targeting Facebook Messenger.</p><p>Former minister Jeremy Wright says that the question over encryption “is frankly not resolved. I think the government has sort of dodged around giving a concluded view on what it means for encryption.” However, he says, the answer is unlikely to be as absolute as the act’s opponents are making out. Encryption won’t be banned, he says, but platforms will have to explain how their policies around it balance safety with their users’ right to privacy. “If you can meet those [safety] duties by using encryption or with encryption as part of the service, you’re fine,” he says. If not, “you have a problem … it can’t be true, surely, that a platform is entitled to say, ‘Well, I operate encryption, so that’s a get-out-of-jail-free card for me on the safety duties.’”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google agrees to invest up to $2B in OpenAI rival Anthropic (371 pts)]]></title>
            <link>https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/</link>
            <guid>38048155</guid>
            <pubDate>Sat, 28 Oct 2023 08:24:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/">https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/</a>, See on <a href="https://news.ycombinator.com/item?id=38048155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="primary-image" role="figure" aria-describedby="primary-image-caption"><figure><div data-testid="Image"><p><img src="https://cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg" srcset="https://www.reuters.com/resizer/R4Z2uz8RcluFRA-5t_A4vBPD5ZQ=/480x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 480w,https://www.reuters.com/resizer/0JGEgq7x2OviOX_SI0PEgvGT6O4=/960x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 960w,https://www.reuters.com/resizer/lzEztbC-1tYEiIpLH_S6m41FrYQ=/1080x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 1080w,https://www.reuters.com/resizer/Ht-xjY7xrUmaianFeFUDU9ssJ_U=/1200x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 1200w" sizes="(min-width: 1024px) 560px, (min-width: 1440px) 700px, 100vw" width="5342" height="3027" alt="An illuminated Google logo is seen inside an office building in Zurich"></p></div><p data-testid="Body"><span>An illuminated Google logo is seen inside an office building in Zurich, Switzerland December 5, 2018. REUTERS/Arnd Wiegmann/File Photo <a data-testid="Link" href="https://www.reutersagency.com/en/licensereuterscontent/?utm_medium=rcom-article-media&amp;utm_campaign=rcom-rcp-lead" target="_blank" referrerpolicy="no-referrer-when-downgrade"> Acquire Licensing Rights</a></span></p></figure></div><div><p data-testid="paragraph-0">Oct 27 (Reuters) - Alphabet's <a data-testid="Link" href="https://www.reuters.com/markets/companies/GOOGL.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(GOOGL.O)</a> Google has agreed to invest up to $2 billion in the artificial intelligence company Anthropic, a spokesperson for the startup said on Friday.</p><p data-testid="paragraph-1">The company has invested $500 million upfront into the OpenAI rival and agreed to add $1.5 billion more over time, the spokesperson said.</p><p data-testid="paragraph-2">Google is already an investor in Anthropic, and the fresh investment would underscore a ramp-up in its efforts to better compete with Microsoft <a data-testid="Link" href="https://www.reuters.com/markets/companies/MSFT.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(MSFT.O)</a>, a major backer of ChatGPT creator OpenAI, as Big Tech companies race to infuse AI into their applications.</p><p data-testid="paragraph-3">Amazon.com <a data-testid="Link" href="https://www.reuters.com/markets/companies/AMZN.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(AMZN.O)</a> also said <a data-testid="Link" href="https://www.reuters.com/markets/deals/amazon-steps-up-ai-race-with-up-4-billion-deal-invest-anthropic-2023-09-25/" referrerpolicy="no-referrer-when-downgrade">last month</a> it would invest up to $4 billion in Anthropic to compete with growing cloud rivals on AI.</p><p data-testid="paragraph-4">In Amazon's quarterly report to the U.S. Securities and Exchange Commission this week, the online retailer detailed it had invested in a $1.25 billion note from Anthropic that can convert to equity, while its ability to invest up to $2.75 billion in a second note expires in the first quarter of 2024.</p><p data-testid="paragraph-5">Google declined to comment, and Amazon did not immediately respond to a Reuters request for comment.</p><p data-testid="paragraph-6">The Wall Street Journal earlier reported the news of Google's latest agreement with Anthropic.</p><p data-testid="paragraph-7">The rising number of investments shows ongoing maneuvering by cloud companies to secure ties with the AI startups that are reshaping their industry.</p><p data-testid="paragraph-8">Anthropic, which was co-founded by former OpenAI executives and siblings Dario and Daniela Amodei, has shown efforts to secure the resources and deep-pocketed backers needed to compete with OpenAI and be leaders in the technology sector.</p><p data-testid="Body">Reporting by Krystal Hu in New York and Chavi Mehta in Bengaluru; Additional reporting by Jeffrey Dastin; Editing by Anil D'Silva, Devika Syamnath and Chris Reese</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank" referrerpolicy="no-referrer-when-downgrade">The Thomson Reuters Trust Principles.</a></p><div><address><p data-testid="Body">Krystal reports on venture capital and startups for Reuters. She covers Silicon Valley and beyond through the lens of money and characters, with a focus on growth-stage startups, tech investments and AI. She has previously covered M&amp;A for Reuters, breaking stories on Trump's SPAC and Elon Musk's Twitter financing. Previously, she reported on Amazon for Yahoo Finance, and her investigation of the company's retail practice was cited by lawmakers in Congress. Krystal started a career in journalism by writing about tech and politics in China. She has a master's degree from New York University, and enjoys a scoop of Matcha ice cream as much as getting a scoop at work. </p></address></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WinterJS (133 pts)]]></title>
            <link>https://wasmer.io/posts/announcing-winterjs-service-workers</link>
            <guid>38047872</guid>
            <pubDate>Sat, 28 Oct 2023 07:19:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wasmer.io/posts/announcing-winterjs-service-workers">https://wasmer.io/posts/announcing-winterjs-service-workers</a>, See on <a href="https://news.ycombinator.com/item?id=38047872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today we are incredibly excited to announce <a href="https://github.com/wasmerio/winterjs">WinterJS</a> (<a href="https://wasmer.io/wasmer/winterjs"><code>wasmer/winterjs</code> package</a>).</p>
<p>WinterJS is a JavaScript Service Workers server written in Rust, that uses the SpiderMonkey runtime to execute JavaScript (the same runtime that Firefox uses). We chose to follow the <a href="https://wintercg.org/">WinterCG</a> specification to aim for maximum compatibility with other services such as Cloudflare Workers, Deno Deploy and Vercel (hence the name <em>WinterJS</em>).</p>
<p>WinterJS is not only <em>blazing fast™️</em> but can also be compiled to WebAssembly <a href="https://wasix.org/">thanks to WASIX</a> and thus also run fully with Wasmer.</p>
<p>Let's see how it works. We'll start by creating a simple <code>serviceworker.js</code> file that just returns a simple "hello world";</p>
<pre tabindex="0"><code><span><span>addEventListener</span><span>(</span><span>'fetch'</span><span>, (</span><span>req</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>  req.</span><span>respondWith</span><span>(</span><span>`hello world from ${</span><span>req</span><span>.</span><span>request</span><span>.</span><span>url</span><span>.</span><span>href</span><span>}`</span><span>);</span></span>
<span><span>});</span></span></code></pre>
<p>Running it with WinterJS is as simple as this:</p>
<pre tabindex="0"><code><span><span>$</span><span> wasmer run wasmer</span><span>/</span><span>winterjs </span><span>--</span><span>net </span><span>--</span><span>mapdir </span><span>/</span><span>app</span><span>:.</span><span> </span><span>/</span><span>app</span><span>/</span><span>serviceworker</span><span>.</span><span>js</span></span></code></pre>
<blockquote>
<p>WinterJS can also be run natively with Rust (<code>cargo install --git https://github.com/wasmerio/winterjs &amp;&amp; winterjs serviceworker.js</code>).
You can find the source code of WinterJS in the GitHub repo: <a href="https://github.com/wasmerio/winterjs">https://github.com/wasmerio/winterjs</a></p>
</blockquote>
<p>Thanks to the WASIX capabilities of WinterJS, the JavaScript service worker can also be deployed to <a href="https://wasmer.io/products/edge">Wasmer Edge</a>.
Check the working demo here: <a href="https://js-service-worker-demo.wasmer.app/">https://js-service-worker-demo.wasmer.app/</a></p>
<hr>
<p>And now that you have seen a sneak peak on how to use WinterJS, lets do a deep dive on our journey building it.</p>
<h2>Choosing the JS engine</h2>
<p>Before starting on the quest of creating a JavaScript Service Workers server, we analyzed the Javacript runtimes that we could use.</p>
<p>Here are the main requirements we have for such JavaScript runtime:</p>
<ul>
<li><strong>Speed</strong>: It should be fast to run</li>
<li><strong>Wasm-compatible</strong>: It should be able to run without restrictions in a Wasm environment (such as Wasmer)</li>
<li><strong>Development time</strong>: We should be able to iterate fast on it</li>
</ul>
<p>And here are the JS runtimes that we analyzed:</p>
<ul>
<li><strong>QuickJS.</strong> Challenges:
<ul>
<li>We will need to implement all the JS apis diff (<code>peformance.now()</code>, <code>addEventListener</code>, …)</li>
<li>We need to implement the serviceWorker API entirely in C</li>
</ul>
</li>
<li><strong>Static Hermes.</strong> Challenges:
<ul>
<li>Node.js polyfills not available in static mode</li>
<li>Not a lot of functions (such as http calls) are available in the polyfill</li>
<li>Had to make it compile to Wasm with WASIX</li>
</ul>
</li>
<li><strong>Bun</strong> (JavascriptCore). Challenges:
<ul>
<li>Zig not fully supporting WASIX</li>
<li>Compiling JavascriptCore to WASIX is possible (was done before), but not trivial</li>
</ul>
</li>
<li><strong>MozJS</strong> (SpiderMonkey). Challenges:
<ul>
<li>We will need to implement all the JS apis diff (<code>peformance.now()</code>, <code>addEventListener</code>, …)</li>
<li>We need to implement the serviceWorker API (in Rust)</li>
<li>We will need to plug the service worker with a WASIX http server</li>
</ul>
</li>
<li><strong>Node.js</strong> (v8). Challenges:
<ul>
<li>Compile v8 in jitless mode to Wasm is an unknown-unknown</li>
</ul>
</li>
</ul>
<h2>Using SpiderMonkey with mozjs</h2>
<p>After a few runtime trials we set on SpiderMonkey as the most reasonable approach that fitted our tight timeline.</p>
<p>So we begin porting. We started with a fork of mozjs that supported a <strong><a href="https://cfallin.org/blog/2023/10/11/spidermonkey-pbl/">new compilation tier called PBI</a></strong> (Portable Baseline Interpreter).</p>
<p>After some work on the mozjs build system to target WASIX, we were able to bypass most of the issues, except one: the bindings generation.</p>
<p>The bindings that allow using the SpiderMonkey C++ API from Rust were automatically generated using c-bindgen. Plugging those bindings onto WASIX was a challenge so we simply decided to target a 32 bit system and modify them by hand (a 32,000 file!) to target <a href="https://wasix.org/">WASIX</a>.</p>
<p>And voilá… everything worked!</p>
<p>However, after adding a few missing resources to JS, we realized that perhaps mozjs didn’t have the easiest API to use:</p>
<pre tabindex="0"><code><span><span>unsafe</span><span> </span><span>extern</span><span> </span><span>"C"</span><span> </span><span>fn</span><span> </span><span>base64_encode</span><span>(cx</span><span>:</span><span> </span><span>*mut</span><span> </span><span>JSContext</span><span>, argc</span><span>:</span><span> </span><span>u32</span><span>, vp</span><span>:</span><span> </span><span>*mut</span><span> </span><span>Value</span><span>) </span><span>-&gt;</span><span> </span><span>bool</span><span> {</span></span>
<span><span>    </span><span>let</span><span> args </span><span>=</span><span> </span><span>CallArgs</span><span>::</span><span>from_vp</span><span>(vp, argc);</span></span>
<span></span>
<span><span>    </span><span>if</span><span> args</span><span>.</span><span>argc_ </span><span>&lt;</span><span> </span><span>1</span><span> {</span></span>
<span><span>        </span><span>return</span><span> </span><span>false</span><span>;</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    </span><span>let</span><span> source </span><span>=</span><span> </span><span>js_try!</span><span>(cx, </span><span>raw_handle_to_string</span><span>(cx, args</span><span>.</span><span>get</span><span>(</span><span>0</span><span>)));</span></span>
<span><span>    </span><span>let</span><span> result </span><span>=</span><span> </span><span>::</span><span>base64</span><span>::</span><span>engine</span><span>::</span><span>general_purpose</span><span>::</span><span>STANDARD</span><span>.</span><span>encode</span><span>(bytes);</span></span>
<span></span>
<span><span>    </span><span>rooted!</span><span>(</span><span>in</span><span>(cx) </span><span>let</span><span> </span><span>mut</span><span> rval </span><span>=</span><span> </span><span>UndefinedValue</span><span>());</span></span>
<span><span>    result</span><span>.</span><span>to_jsval</span><span>(cx, rval</span><span>.</span><span>handle_mut</span><span>());</span></span>
<span></span>
<span><span>    args</span><span>.</span><span>rval</span><span>()</span><span>.</span><span>set</span><span>(rval</span><span>.</span><span>get</span><span>());</span></span>
<span></span>
<span><span>    </span><span>true</span></span>
<span><span>}</span></span></code></pre>
<h2>Using SpiderMokey with spiderfire</h2>
<p>Thankfully, the <a href="https://github.com/Redfire75369/spiderfire/">spiderfire</a> project had been working on improving the API surface for using SpiderMonkey from Rust.</p>
<p>So the example laid out before now looks way simpler and easier to read/maintain:</p>
<pre tabindex="0"><code><span><span>#[js_fn]</span></span>
<span><span>fn</span><span> </span><span>btoa</span><span>&lt;'</span><span>cx</span><span>&gt;(val</span><span>:</span><span> </span><span>String</span><span>) </span><span>-&gt;</span><span> </span><span>String</span><span> {</span></span>
<span><span>    </span><span>let</span><span> bytes </span><span>=</span><span> val</span><span>.</span><span>as_bytes</span><span>();</span></span>
<span><span>    </span><span>::</span><span>base64</span><span>::</span><span>engine</span><span>::</span><span>general_purpose</span><span>::</span><span>STANDARD</span><span>.</span><span>encode</span><span>(bytes)</span></span>
<span><span>}</span></span></code></pre>
<h2>Deploying to Wasmer Edge</h2>
<p>Compiling WinterJS to WASIX was challenging, but completely worth it. Thanks to its WASIX capabilities we can now run any Javascript Service Workers workloads in <a href="https://wasmer.io/products/edge">Wasmer Edge</a>.</p>
<p>We have put together an <strong>in depth tutorial on how to use Javascript Service Workers in Wasmer Edge</strong>... please check it out!</p>
<p><a href="https://docs.wasmer.io/edge/quickstart/js-wintercg">https://docs.wasmer.io/edge/quickstart/js-wintercg</a></p>
<hr>
<p>We believe WinterJS will enable many new use cases. For example, running Service Workers natively in your IoT device (where Node is too heavy to run), or even in your browser.</p>
<p>At Wasmer we are incredibly excited to see how you will use WinterJS.</p>
<p>WinterJS on GitHub: <a href="https://github.com/wasmerio/winterjs">https://github.com/wasmerio/winterjs</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The fun factor of the video game Uplink (321 pts)]]></title>
            <link>https://vertette.github.io/post/funfactoruplink</link>
            <guid>38047861</guid>
            <pubDate>Sat, 28 Oct 2023 07:16:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vertette.github.io/post/funfactoruplink">https://vertette.github.io/post/funfactoruplink</a>, See on <a href="https://news.ycombinator.com/item?id=38047861">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Remember family computers? Before we had tablets, middle class families would buy overpriced computers from dodgy computer stores that the whole family got to share. As they were mostly used by children and adults who didn't have the slightest understanding of technology, it didn't take long for the family computer to down to a crawl, plagued by dodgy Kazaa downloads and suspicious Internet Explorer toolbars. Between a lack of money to buy a better computer, no easy way to buy digital games in Europe for the longest time and no nearby stores that sold computer games, it took until my brother moved out of the home before I really started getting into PC gaming.</p>
<p>My mom bought a gaming PC for my 14th birthday, and while it was incredibly overpriced for what it was, that didn't matter to me; it was mine and I could do whatever I wanted with it. As the only computer games I was familiar with were <a href="https://www.newgrounds.com/games">free Flash games on Newgrounds</a>, I asked around on various communities what kind of games I could play on a slightly outdated hunk of junk like mine. I forgot who exactly was responsible, but someone recommended I try this old hacking game called <a href="https://store.steampowered.com/app/1510/Uplink/"><em>Uplink</em></a>, and so I found a torrent on ThePirateBay and tried it. I make the following statement with zero hyperbole - that single throwaway recommendation changed my life. Various hours later, I fell in love with it so hard that I made a Steam account just so I could buy the game and support the developers properly.</p>
<p><img src="https://vertette.github.io/img/uplink_hardware.png" alt="A screenshot of the Uplink hardware upgrade screen">
<em>Uplink's interface might be outdated and slightly janky, but dammit, it still looks very cool.</em></p>
<p>If you're unfamiliar with it, here's how it works: <em>Uplink</em> is a hacking simulator reminiscent of old hacking movies like <a href="https://www.imdb.com/title/tt0105435/"><em>Sneakers</em></a> and <a href="https://www.imdb.com/title/tt0113243/"><em>Hackers</em></a>, where the portrayal is less about realism and more about flashiness. You play as a hacker who does various odd jobs like changing people's identities or destroying valuable data. At the beginning of the game, hacking is as easy as using the password breaker on a password screen and finishing up in less than five minutes to avoid getting caught, but the game quickly starts bombarding you with new concepts - deleting logs to avoid being tracked down, shutting down security systems that get in your way and travelling through local area networks. The game is never outright unfair, as most information you need to get through the game can be found in the in-game help section, but certain concepts require a bit of trial and error before you truly get how they work and that can result in you getting caught by the authorities, which results in an instant game over. Thankfully, starting over and getting back to where you were before isn't as daunting as it seems due to <em>Uplink</em>'s fairly open structure. While the game is a bit on the short side, there's enough depth to its mechanics to feel satisfying to master, and the realization that a game that gave you so much trouble at first has turned into a total cakewalk can't be matched.</p>
<p>Before <em>Uplink</em>, I only really played games like <em>Mario</em>, <em>Grand Theft Auto</em> and <em>Alien Hominid</em>; games that might or might not feature mature content, but were decidedly arcadey in nature. They didn't care that much about immersion or emotional engagement. <em>Uplink</em> was a different beast: it pulled me right in with its <a href="https://www.youtube.com/watch?v=QliQ0livbeQ">beautiful ambient soundtrack</a>, retrofuturistic visuals and gameplay that was unlike anything I had ever experienced. Sure, it might not have any fancy 3D models and complex shaders, but I still felt absorbed in a way no other game had done before. Its gameplay was highly addictive and its presentation deceptively brilliant, with a story that would've been deemed too ambitious for an AAA game even at the time. I became an obsessed man, looking up everything that I could find about the game and its developers, buying their newer games <a href="https://store.steampowered.com/app/1500/Darwinia/"><em>Darwinia</em></a> and <a href="https://store.steampowered.com/app/1520/DEFCON/"><em>DEFCON</em></a> and reading the <em>Uplink</em> design documents on the Bonus Disk religiously. Before <em>Uplink</em>, I never gave game design much consideration. I never thought about all the possibilities games have to tell unique stories or how certain game mechanics can make you feel certain emotions. So what is it about the gameplay that makes it so engaging, so immersive and so much fun? Well, the answer might not be what you'd expect: even though there's plenty to praise about <em>Uplink</em>'s design, it manages to be so engaging and immersive because it isn't actually that much fun.</p>
<p><img src="https://vertette.github.io/img/uplink_hack.png" alt="A screenshot of an Uplink hack in progress">
<em>Fun fact: the Trace Tracker's beeps were a last minute addition. There is a world map upgrade that shows you exactly how far the administrator's trace is, but nobody buys it because it doesn't beep.</em></p>
<p>That might make it sound like yet another pretentious indie game that sacrificies good gameplay in service of a Very Important Message™, but that's not actually the case. The anticipation of planning your next attack, the tension as the trace tracker's beeps become quicker and quicker as the system administrator starts closing in on you, the euphoria of a successful job that gets quickly swallowed up by the creeping paranoia of whether you properly correctly cleaned up after yourself or not - <em>Uplink</em> is a hurricane of emotions, but a lot of the emotions it invokes aren't exactly what you'd call positive ones. In that sense, while the game <em>can</em> be fun, it can also feel very tense, obtuse and frustrating, and that's important. Without that, the experience would not nearly be as effective at making you feel like a real hacker as it is, even if the moment-to-moment gameplay pretty much boils down to a script kiddie simulator. The way it goes about it elevates it to something much grander, something truly innovative and memorable, and in that sense "fun" is simply too limiting a term to describe <em>Uplink</em>'s design.</p>
<p>That might sound silly to a lot of players, because "if the game's not fun, why bother", right? But there is an actual precedence for this claim, for example horror games. Most people play horror games not to feel amused but to feel spooked, and those two emotions are almost directly on the opposite end of the emotion wheel. If a horror game is fun to you, then it's doing a very bad job. Another good example is <em>Pathologic</em>, a game that deliberately goes out of its way to be an unpleasant experience to sell the setting of a plague-ridden town in a very effective and memorable way. Even though I find it hard to recommend it, I also find it hard to dismiss it as not being worth your time. And I don't want to ruin people's laughs from back when <a href="https://www.gamesradar.com/we-dont-use-the-word-fun-says-the-last-of-us-2-director-neil-druckmann/&amp;utm_campaign=buffer_grtw/">Neill Druckmann infamously claimed they didn't use the word "fun" during development of <em>The Last of Us 2</em></a>, but that <em>is</em> a valid way to design your game. The way he phrased it made it come across as more pretentious than he meant it to, but the gameplay of <em>TLOU2</em> invokes a lot of the same emotions that <em>Uplink</em> does: tension, paranoia, and euphoria. If games really are an art form, then limiting the design to what's fun is ignoring so many other emotional reactions your game can inspire in others.</p>
<p>That doesn't mean that designing a fun game isn't valuable, but it <em>does</em> mean that it's worth exploring emotions through your game that aren't directly adjacent to fun. With all the opportunities the medium of video games has over others, it would be a waste not to.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cortex X2: ARM aims high (173 pts)]]></title>
            <link>https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/</link>
            <guid>38047743</guid>
            <pubDate>Sat, 28 Oct 2023 06:51:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/">https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/</a>, See on <a href="https://news.ycombinator.com/item?id=38047743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Arm has traditionally targeted the low end of the power and performance curve, but just as Intel has been looking to expand into the low power market, ARM is looking to expand into higher power and performance segments. The Cortex X series is at the forefront of this effort.</p>
<blockquote>
<p>Delivers ultimate peak performance within an expanded envelope for power and area.</p>
<cite><a href="https://www.arm.com/products/cortex-x">Cortex-X Custom CPU Program</a>, Arm</cite></blockquote>
<p>Here, we’ll be looking at the Cortex X2 as implemented in the Snapdragon 8+ Gen 1. This SoC features a single X2 core, alongside four Cortex A510 and three Cortex A710 cores. The Cortex X2 in this SoC typically runs at 2.8 GHz, although lscpu indicates its clock speed can range from 787.2 MHz to 3.187 GHz. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?ssl=1"><img data-attachment-id="20270" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_boost/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?fit=971%2C476&amp;ssl=1" data-orig-size="971,476" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_boost" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?fit=971%2C476&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?fit=688%2C337&amp;ssl=1" decoding="async" width="688" height="337" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?resize=688%2C337&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?w=971&amp;ssl=1 971w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?resize=768%2C376&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Tested on an Asus Zenfone 9</figcaption></figure></div>
<p>When placed under load, the Cortex X2 quickly boosts to an intermediate clock speed of 2.56 GHz. After 55 ms, it reaches 2.8 GHz. No higher clock speeds were observed when testing over a longer duration.</p>
<h2>Core Overview</h2>
<p>Cortex X2 is similar to its 7-series cousin, the Cortex A710, but is substantially larger. X2 has more reordering capacity, a wider pipeline, and more execution units. Despite these changes, X2 has a 10-stage pipeline just like A710.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?ssl=1"><img data-attachment-id="23077" data-permalink="https://chipsandcheese.com/cortex_x2-drawio-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?fit=1392%2C1188&amp;ssl=1" data-orig-size="1392,1188" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?fit=1392%2C1188&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?fit=688%2C587&amp;ssl=1" decoding="async" width="688" height="587" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=688%2C587&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?w=1392&amp;ssl=1 1392w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=768%2C655&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=1200%2C1024&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=1320%2C1127&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<h2>Branch Prediction</h2>
<p>Branch prediction is important for any CPU because wasted work from mispredicts will hurt both performance and power efficiency. Cortex X2 gets more area and power budget than other ARM cores, and therefore gets a more capable branch predictor. It can recognize somewhat longer patterns than Cortex A710 and its server cousin, Neoverse N2. Alongside that, it does better when there are a ton of branches in play. </p>
<figure><p><img decoding="async" id="20275" src="https://i1.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_branchhist.png?ssl=1" alt="" width="1162" height="691"><img decoding="async" loading="lazy" id="20169" src="https://i2.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/yitian710_branchhist.png?ssl=1" alt="" width="1154" height="690"></p></figure>
<p>However, ARM’s statement that X2 has an “expanded envelope for power and area” has to be taken in context. X2 still goes into mobile chips even if high end SoCs only feature a single X-series core. Passive smartphone cooling means X2 is still working within a much tighter power budget than desktop CPUs. AMD’s Zen 4 in comparison pulls all the stops to maximize branch prediction accuracy. </p>
<figure><p><img decoding="async" id="20275" src="https://i1.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_branchhist.png?ssl=1" alt="" width="1162" height="691"><img decoding="async" loading="lazy" id="11077" src="https://i2.wp.com/chipsandcheese.com/wp-content/uploads/2022/10/zen4_bpu_pattern.png?ssl=1" alt="" width="1159" height="679"></p></figure>
<p>The branch predictor’s job is to make sure the frontend is well-fed with fetch addresses. Accurately predicting branch direction is one component of this. Another component is delivering those fetch addresses quickly. To do so, the branch predictor keeps a cache of branch destinations, called a branch target buffer (BTB). </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?ssl=1"><img data-attachment-id="20280" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_btb/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?fit=1215%2C557&amp;ssl=1" data-orig-size="1215,557" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_btb" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?fit=1215%2C557&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?fit=688%2C315&amp;ssl=1" decoding="async" loading="lazy" width="688" height="315" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?resize=688%2C315&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?w=1215&amp;ssl=1 1215w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?resize=768%2C352&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?resize=1200%2C550&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Cortex X2’s BTB is mostly unchanged from A710’s. A micro-BTB can handle two taken branches per cycle, and can track up to 64 branches. Then we see about 10K branches tracked with 1-2 penalty cycles. Returns are handled with a 14 entry return stack as well.</p>
<h2>Frontend: Fetch and Decode</h2>
<p>Cortex X2 has an enlarged version of A710’s frontend, and enjoys both increased caching capacity and higher throughput. The micro-op cache grows to 3072 entries, making it larger than Sunny Cove’s. Also, X2 mandates a 64 KB instruction cache, while A710 implementers could pick between 32 KB or 64 KB instruction cache. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?ssl=1"><img data-attachment-id="20286" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/cortex_x2_frontend-drawio-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?fit=1613%2C411&amp;ssl=1" data-orig-size="1613,411" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_frontend.drawio-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?fit=1613%2C411&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?fit=688%2C175&amp;ssl=1" decoding="async" loading="lazy" width="688" height="175" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=688%2C175&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?w=1613&amp;ssl=1 1613w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=768%2C196&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1536%2C391&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1200%2C306&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1600%2C408&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1320%2C336&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Compared to AMD’s Zen 4, X2’s micro-op cache is smaller, but its larger instruction cache is a notable advantage for larger code footprints. If code footprints exceed 32 KB (but not 64 KB), and have a lot of unpredictable branches, Zen 4 will suffer from L2 latency and see a lot of frontend bubbles.</p>
<p>In terms of throughput, X2’s micro-op cache can provide 8 operations per cycle, which is more than enough to feed the 6-wide renamer downstream. The 5-wide decoder can provide generous instruction throughput for larger code footprints and compares favorably to the 4-wide decoders found on Zen 4 and A710. X2 can sustain more than four instructions per cycle even when running code from L2.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?ssl=1"><img data-attachment-id="20289" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_ifetch/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?fit=1086%2C465&amp;ssl=1" data-orig-size="1086,465" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_ifetch" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?fit=1086%2C465&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?fit=688%2C295&amp;ssl=1" decoding="async" loading="lazy" width="688" height="295" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?resize=688%2C295&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?w=1086&amp;ssl=1 1086w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?resize=768%2C329&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>However once you get past L2, Zen 4 pulls ahead again. Thanks to AMD’s very high performance L3 and an aggressive branch predictor, Zen 4 can sustain over 3 IPC when running code from L3. Cortex X2 doesn’t do badly and can still average 1.66 IPC in that case.</p>
<h2>Out of Order Execution</h2>
<p>After micro-ops from the frontend have been renamed, out-of-order execution tracks and executes them as their data dependencies become available. X2 has a much larger OoO engine than A710 while enjoying similar instruction fusion optimizations. ROB size increased to 288 entries with other structure sizes scaled up to match.</p>
<figure><table><tbody><tr><td>Structure</td><td>Entry required if instruction…</td><td>Cortex X2 Capacity</td><td>A710 Capacity</td><td>Zen 4 Capacity</td></tr><tr><td>Reorder Buffer</td><td>exists</td><td>288</td><td>160</td><td>320</td></tr><tr><td>Integer Register File</td><td>writes to an integer register</td><td>~213</td><td>~147</td><td>224</td></tr><tr><td>FP/Vector Register File</td><td>writes to a FP/vector register</td><td>~156x 128-bit</td><td>~124x 128-bit</td><td>192x 512-bit</td></tr><tr><td>Flags Register File</td><td>sets condition flags</td><td>70</td><td>46</td><td>108 documented<br>238 measured</td></tr><tr><td>Load Queue</td><td>reads from memory</td><td>174</td><td>64</td><td>88 documented<br>136 measured</td></tr><tr><td>Store Queue</td><td>writes to memory</td><td>72</td><td>36</td><td>64</td></tr><tr><td>Branch Order Buffer</td><td>potentially affects control flow (NT branches tested here)</td><td>68</td><td>44</td><td>118</td></tr></tbody></table></figure>
<p>X2 ends up getting close to Zen 4 in most areas, and even exceeds it in a few. ARM’s core can keep a staggering number of loads in flight. Instruction fusion allows it to track 249 FP operations pending retirement, while Zen 4 can only track 154. However, Zen 4 does better if 512-bit vectors are used because its large AVX-512 register file lets it keep a lot more explicitly parallel work in flight.</p>
<p>A710 had an overbuilt scheduler considering its ROB capacity and other structure sizes. Cortex X2 brings things back into balance. Integer scheduler capacity is surprisingly similar to Zen 4’s, with four 24 entry queues. Zen 4 shares those scheduler queues with the AGUs, while Cortex X2 has separate AGU schedulers.</p>
<h3>FP/Vector Execution</h3>
<p>Arm’s Cortex 7 series cores had weak vector execution thanks to tight area and power constraints. Cortex X2 uses its larger power and area budget to implement a quad-pipe FP and vector setup. All four pipes can handle common math operations and enjoy the same low floating point execution latency that Cortex A710 does. Cortex X2 is therefore a very strong contender for scalar or 128-bit vector operations. </p>
<p>I wasn’t able to fully utilize all four pipes even with instructions that should have been able to do so (according to the optimization guide), but even so, throughput is very good.</p>
<figure><table><tbody><tr><td></td><td>Cortex X2</td><td>Cortex A710</td><td>Zen 4</td></tr><tr><td>FP32 Add</td><td>2.53 per cycle<br>2 cycle latency</td><td>2 per cycle<br>2 cycle latency</td><td>2 per cycle<br>3 cycle latency</td></tr><tr><td>FP fused multiply-add</td><td>2.53 per cycle<br>4 cycle latency</td><td>2 per cycle<br>4 cycle latency</td><td>2 per cycle<br>4 cycle latency</td></tr><tr><td>128-bit vector INT32 add</td><td>2.53 per cycle<br>2 cycle latency</td><td>2 per cycle<br>2 cycle latency</td><td>4 per cycle<br>1 cycle latency</td></tr><tr><td>128-bit vector INT32 multiply</td><td>1.26 per cycle<br>4 cycle latency</td><td>1 per cycle<br>4 cycle latency</td><td>2 per cycle<br>3 cycle latency</td></tr></tbody></table><figcaption>Latency and throughput is identical for vector versions of those FP operations</figcaption></figure>
<p>Zen 4 still has an advantage with longer vector lengths and lower latency for vector integer operations. But even if Zen 4 uses 256-bit vectors, Cortex X2 can put up a decent fight because it has identical theoretical throughput (per cycle) for common operations. For example, Zen 4 can do two 256-bit FMAs per cycle. Cortex X2 can match that by doing four 128-bit FMAs. AMD’s core also enjoys better scheduling capacity. X2 seems to have a pair of 23 entry schedulers. I couldn’t find any operations that only go to one of the ADDV pipes, so I can’t tell if it’s a single 23 entry queue, or a 11+12 entry setup. I think a pair of dual port schedulers is more likely. AMD’s Zen 4 uses a pair of 32 entry schedulers, giving it 64 FP scheduling entries compared to Cortex X2’s 46.</p>
<figure><img data-attachment-id="20349" data-permalink="https://chipsandcheese.com/x2_fp_exec/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?fit=888%2C275&amp;ssl=1" data-orig-size="888,275" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_fp_exec" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?fit=888%2C275&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?fit=688%2C213&amp;ssl=1" decoding="async" loading="lazy" width="688" height="213" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?resize=688%2C213&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?w=888&amp;ssl=1 888w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?resize=768%2C238&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></figure>
<p>Like Zen 4, X2 has a non-scheduling queue (NSQ) in front of the FP schedulers, which lets the core track more incomplete operations without using a larger scheduler. An NSQ can contain a lot more entries than a scheduling queue, because it doesn’t have to check each entry each cycle to see if it’s ready for execution. With its 29 entry NSQ, Cortex X2 can keep a total of 75 incomplete FP operations in flight. X2 is an improvement over A710, but AMD prioritizes FP execution more. Zen 4 uses a larger 64 entry non-scheduling queue and can keep a total of 128 incomplete FP operations in flight.</p>
<h3>Memory Execution</h3>
<p>Cortex X2 handles memory accesses with three address generation units (AGUs), with some similarities to A710 and Zen 4. The memory subsystem can handle three memory accesses per cycle, of which three can be loads and two can be stores. Its scheduling setup appears similar to the one on <a href="https://chipsandcheese.com/2023/09/11/hot-chips-2023-arms-neoverse-v2/">Neoverse V2</a>, but with slightly smaller scheduling queues and tiny non-scheduling queues in front of them.</p>
<p>After addresses are calculated, the load/store unit has to ensure they appear to execute in program order. Loads might have to get their data from prior in-flight stores. Ideally, data from the store gets sent to a dependent load with minimal delay. But detecting dependencies can be complicated because loads and stores can overlap without matching addresses.</p>
<p>Cortex X2 acts a lot like prior ARM cores starting from Neoverse N1. The load/store unit can forward either half of a 64-bit load to a dependent 32-bit store, but can’t handle any other cases. Fast-path store forwarding has a latency of five cycles, while the slow path incurs a 10-11 cycle penalty.</p>

<p>Zen 4 has a far more robust mechanism for resolving memory dependencies. Any load contained within a prior store can have its data forwarded, and exact address matches can be handled with zero latency. ARM is falling a bit behind here with essentially pre-2010s forwarding capability on a core design going for ultimate performance. However, the slow fallback path on Zen 4 is more expensive at 19-20 cycles, likely indicating Zen 4 has more pipeline stages between between address calculation and store retirement.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?ssl=1"><img data-attachment-id="20370" data-permalink="https://chipsandcheese.com/zen4_stlf-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?fit=2718%2C1189&amp;ssl=1" data-orig-size="2718,1189" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_stlf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?fit=2560%2C1120&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?fit=688%2C301&amp;ssl=1" decoding="async" loading="lazy" width="688" height="301" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=688%2C301&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?w=2718&amp;ssl=1 2718w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1536%2C672&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=2048%2C896&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1200%2C525&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1600%2C700&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1320%2C577&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Same test on Zen 4</figcaption></figure></div>
<p>Cortex X2 does better with avoiding misalignment penalties. Zen 4’s data cache has 32B store alignment, so stores that cross a 32B aligned boundary have a throughput of one per two cycles. X2 doesn’t see any penalty unless accesses cross a 64B cacheline boundary.</p>
<p>Henry Wong <a href="https://blog.stuffedcow.net/2014/01/x86-memory-disambiguation/" data-type="link" data-id="https://blog.stuffedcow.net/2014/01/x86-memory-disambiguation/">experimented</a> with smaller load and store sizes and didn’t see a significant difference. However, vector loads do behave differently on on some CPUs. Cortex X2 can again forward either 64-bit half of a 128-bit store, but curiously can also forward the low 32 bits and merge that with another 32 bits from the data cache to quickly complete a partially overlapping 64-bit load.</p>
<figure><img data-attachment-id="20355" data-permalink="https://chipsandcheese.com/x2_128_stlf/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?fit=2738%2C1321&amp;ssl=1" data-orig-size="2738,1321" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_128_stlf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?fit=2560%2C1235&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?fit=688%2C332&amp;ssl=1" decoding="async" loading="lazy" width="688" height="332" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=688%2C332&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?w=2738&amp;ssl=1 2738w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=768%2C371&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1536%2C741&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=2048%2C988&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1200%2C579&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1600%2C772&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1320%2C637&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>Using str q,[x] and ldr d, [x]</figcaption></figure>
<p>Zen 4’s vector side acts a lot like the scalar integer side, but with a couple cycles of additional latency. AMD can impressively handle misaligned loads with no cost, but again is more prone to hitting misaligned store penalties than Cortex X2. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?ssl=1"><img data-attachment-id="20371" data-permalink="https://chipsandcheese.com/zen4_128_stlf/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?fit=2718%2C1171&amp;ssl=1" data-orig-size="2718,1171" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_128_stlf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?fit=2560%2C1103&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?fit=688%2C296&amp;ssl=1" decoding="async" loading="lazy" width="688" height="296" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=688%2C296&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?w=2718&amp;ssl=1 2718w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=768%2C331&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1536%2C662&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=2048%2C882&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1200%2C517&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1600%2C689&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1320%2C569&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Using movups store and movsd load</figcaption></figure></div>
<h3>Address Translation</h3>
<p>User programs don’t directly address locations in DRAM. Instead, they use virtual addresses, and the operating system sets up a map of virtual address to physical addresses for each process. This allows cool things like swapping to disk when physical memory runs low. However, hardware has to translate addresses on the fly while maintaining high performance. Translation lookaside buffers (TLBs) cache virtual to physical address mappings. TLB hits let the CPU avoid traversing the operating system’s paging structures, which would turn one memory access into several dependent ones.</p>
<p>Cortex X2 has a two-level TLB setup. The first TLB level has 48 entries and is fully associative. It’s a welcome size increase over the 32 entries in A710, but is still smaller than Zen 4’s 72 entry DTLB.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?ssl=1"><img data-attachment-id="20377" data-permalink="https://chipsandcheese.com/x2_tlb-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?fit=1355%2C402&amp;ssl=1" data-orig-size="1355,402" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_tlb.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?fit=1355%2C402&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?fit=688%2C204&amp;ssl=1" decoding="async" loading="lazy" width="688" height="204" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=688%2C204&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?w=1355&amp;ssl=1 1355w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=768%2C228&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=1200%2C356&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=1320%2C392&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L1 DTLB misses can be caught by Cortex X2’s 2048 entry L2 TLB, with a cost of 5 extra cycles. This is a welcome improvement over the Cortex A710’s 1024 entry TLB, and Neoverse N2’s 1280 entries. Cortex X2’s improved TLB sizes let it incur less address translation latency for programs with larger data footprints. It’s still a step behind Zen 4’s 3072 entry L2 TLB, but it matches Zen 2.</p>
<h2>Cache and Memory</h2>
<p>Caching is an important component of a CPU’s performance. In the Snapdragon 8+ Gen 1, Cortex X2 gets a triple level cache hierarchy. The large 64 KB L1D has 4 cycle latency. It’s not the best for a CPU clocked below 3 GHz, considering the old AMD Athlon and Phenom CPUs achieved 3 cycle L1D latency years ago. As a consolation, indexed addressing doesn’t cost an extra cycle like on recent AMD and Intel CPUs.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23091"><img data-attachment-id="23091" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/cortex_x2_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?fit=1239%2C604&amp;ssl=1" data-orig-size="1239,604" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?fit=1239%2C604&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?fit=688%2C335&amp;ssl=1" decoding="async" loading="lazy" width="688" height="335" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?resize=688%2C335&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?w=1239&amp;ssl=1 1239w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?resize=768%2C374&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?resize=1200%2C585&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Arm mandates a 64 KB L1D on Cortex X2, but lets implementers configure the L2 with 512 KB or 1 MB of capacity. The L2 is inclusive of the L1D, so Arm is making a good decision in not offering smaller L2 options. Both L2 configurations have 8-way associativity, so Arm is changing capacity by increasing the number of sets. Qualcomm picked the 1 MB option on the Snapdragon 8+ Gen 1. L2 hits have 11 cycle latency, which comes out to just under 4 nanoseconds. Cortex X2 can’t clock as high as Zen 4, but the short L2 pipeline helps close some of the gap. Just like the L1D, the L2 is always ECC protected. I’m glad Arm isn’t making ECC protection optional.</p>
<p>The L2 has a 256-bit bus to the DSU-110, which connects cores to the rest of the system. Arm lets implementers configure the DSU-110 with up to 16 MB of L3 cache. The L3 is 16-way set associative with power of two capacities, or 12-way set associative if capacity is divisible by 3. Qualcomm in their infinite wisdom has chosen 6 MB of L3 cache, so the Snapdragon 8+ Gen 1’s L3 is 12-way set associative.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23114"><img data-attachment-id="23114" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/dsu-110_layout/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?fit=632%2C334&amp;ssl=1" data-orig-size="632,334" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dsu-110_layout" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?fit=632%2C334&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?fit=632%2C334&amp;ssl=1" decoding="async" loading="lazy" width="632" height="334" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?resize=632%2C334&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>Figure from Arm’s DSU-110 Technical Reference Manual</figcaption></figure></div>
<p>The L3 is arranged into slices, and is filled by victims from core private caches. Cortex X2 suffers higher L3 latency than Zen 4. At the 4 MB test size, its 18.18 ns result is similar to the 17.41 ns seen by the Intel Core i9-12900K’s E-cores. A small 6 MB cache should make up for its lack of capacity by at least being fast, but I suppose that would be asking too much from a mobile SoC. At least it’s reasonable from the ~51 core cycle latency.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23124"><img data-attachment-id="23124" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/dsu-110_l3_configurable_cycles/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?fit=623%2C229&amp;ssl=1" data-orig-size="623,229" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dsu-110_l3_configurable_cycles" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?fit=623%2C229&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?fit=623%2C229&amp;ssl=1" decoding="async" loading="lazy" width="623" height="229" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?resize=623%2C229&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>Part of the L3 pipeline is configurable. Image from Arm’s DSU-110 Technical Reference Manual</figcaption></figure></div>
<p>Arm’s Technical Reference manual suggests five to seven cycles are spent accessing L3 data storage, so the remaining cycles are spent checking tags, traversing the interconnect, and at upper level caches. Program-visible L3 latency includes time spent accessing the L2 TLB, since the L1 TLB is not large enough to cover the L3 cache.</p>
<p>At the 1 GB test size, we see 202 ns of DRAM latency. L2 TLB misses and page walks add potentially heavy address translation latency on top, but separating that from DRAM latency is difficult because there’s no way to use huge pages on Android. It’s not too bad for a cell phone SoC, but is a world apart from desktop or laptop CPUs. It’s also worse than Apple’s M1, which should worry Qualcomm because Apple shares designs across phones and tablets.</p>
<div>
<figure><a href="https://chipsandcheese.com/x2_m1_latency/"><img data-attachment-id="23150" data-permalink="https://chipsandcheese.com/x2_m1_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?fit=1374%2C714&amp;ssl=1" data-orig-size="1374,714" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_m1_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?fit=1374%2C714&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?fit=688%2C358&amp;ssl=1" decoding="async" loading="lazy" width="688" height="358" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=688%2C358&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?w=1374&amp;ssl=1 1374w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=768%2C399&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=1200%2C624&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=1320%2C686&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Apple’s 12 MB shared L2 serves the same role as the Snapdragon 8+ Gen 1’s 6 MB L3, but has both higher capacity and lower latency. I wonder how Cortex X2 would do if it were better fed.</p>
<h3> Bandwidth</h3>
<p>Cortex X2’s three AGUs and triple port data cache allow it to service three 128-bit accesses per cycle. The core therefore can get the same per-cycle L1D bandwidth as A710 and Apple’s M1, and beats older Arm cores like the Neoverse N1 by a large margin. Apple’s M1 still gets an absolute bandwidth lead thanks to higher clocks. Compared to recent x86 cores, X2’s L1D bandwidth is still low due to lower clocks and lack of wider vector support.</p>
<div>
<figure><a href="https://chipsandcheese.com/cortex_x2_read_vs_arm/"><img data-attachment-id="23128" data-permalink="https://chipsandcheese.com/cortex_x2_read_vs_arm/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?fit=1243%2C581&amp;ssl=1" data-orig-size="1243,581" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_read_vs_arm" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?fit=1243%2C581&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?fit=688%2C322&amp;ssl=1" decoding="async" loading="lazy" width="688" height="322" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?resize=688%2C322&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?w=1243&amp;ssl=1 1243w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?resize=768%2C359&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?resize=1200%2C561&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L2 bandwidth is decent at 28 bytes per cycle. It’s close to Apple M1’s L2 bandwidth. Zen 4 and Skylake again enjoy a large L2 bandwidth lead over Cortex X2 thanks to higher clock speeds.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23092"><img data-attachment-id="23092" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/cortex_x2_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?fit=1248%2C578&amp;ssl=1" data-orig-size="1248,578" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?fit=1248%2C578&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?fit=688%2C319&amp;ssl=1" decoding="async" loading="lazy" width="688" height="319" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?resize=688%2C319&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?w=1248&amp;ssl=1 1248w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?resize=768%2C356&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?resize=1200%2C556&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L2 misses go into a transaction queue with size configurable from 72 to 96 entries. The large transaction queue helps the core cope with high L3 latency, so X2’s L3 bandwidth is on par with Skylake. DRAM bandwidth from the single Cortex X2 core is decent at 32.5 GB/s, hinting at the L3’s ability to track a lot of pending misses. The DSU-110’s CHI (Coherent Hub Interface) can track up to 128 reads per master port. If Qualcomm is using that to connect memory controllers, it would explain the decent memory bandwidth in the face of high latency.</p>
<h3>Write Bandwidth</h3>
<p>We can examine bandwidth without latency restrictions by testing writes instead of reads. Normally, writes have much lower bandwidth because a write access involves a read-for-ownership first to fill the line into cache. However, Cortex X2 detects when entire cachelines are being overwritten without any of the data getting read. If that happens to enough consecutive lines, the core’s bus interface switches into write streaming mode. In write streaming mode, cache misses don’t cause fills and simply write out the data. Thus, writes won’t be held back by read latency and RFO bandwidth won’t compete with writebacks.</p>
<div>
<figure><a href="https://chipsandcheese.com/x2_write/"><img data-attachment-id="23133" data-permalink="https://chipsandcheese.com/x2_write/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?fit=1244%2C579&amp;ssl=1" data-orig-size="1244,579" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="X2_write" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?fit=1244%2C579&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?fit=688%2C320&amp;ssl=1" decoding="async" loading="lazy" width="688" height="320" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?resize=688%2C320&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?w=1244&amp;ssl=1 1244w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?resize=768%2C357&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?resize=1200%2C559&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Bandwidth from L1D is lower because only two AGUs can handle writes. But every lower level in the cache hierarchy benefits. L2 bandwidth goes up to 30 bytes per cycle, while L3 bandwidth reaches 67 GB/s. Finally, DRAM bandwidth sits around 41.2 GB/s. I suspect that’s a better reflection of what the memory controller can deliver using its 64bit LPDDR5-6400 interface.</p>
<h2>Final Words</h2>
<p>Arm’s Cortex X line reaches for higher performance with an increased power and area budget. Cortex X2 is the second member of that line. It apparently has an <a href="https://www.eetasia.com/express/mediatek-delivers-efficient-cortex-x2/">area of about 2.1 mm<sup>2</sup></a>, making it just slightly smaller than Zen 4c. While Arm tries to move up the performance ladder, Intel and AMD are trying to move down to hit lower power and area targets. Arm’s efforts to move up the performance ladder mean it’s starting to overlap with AMD and Intel as those x86 companies try to move down into lower power and area targets.</p>
<p>AMD, Arm, and Intel share another commonality. They all have to maintain multiple cores to broaden their coverage of performance targets. AMD has the most modest and cost efficient approach. Zen 4c uses a different physical implementation of the Zen 4 architecture to reduce core area at the cost of clock speed. Intel goes all the way for maximum flexibility. Gracemont is a completely different core than Golden Cove, so Intel is splitting engineering effort between two core lines. Arm lands in the middle. Cortex X2 is a scaled up A710. The two cores have similar scheduler layouts and instruction fusion optimizations, so they’re really siblings rather than completely different designs. Some of Arm’s engineering effort can be shared across both cores, but additional time has to be spent tuning and validating A710 and X2.</p>
<p>To build Cortex X2, Arm took everything in A710 and moved the sliders up. Out-of-order structures enjoy increased capacity. L1, L2, and micro-op cache sizes get larger. X2 gets a quad pipe FPU, giving it a welcome upgrade over A710’s dual pipe one. Floating point units are area hungry because FP operations involve several basic operations under the hood, so X2’s larger area budget is getting put to good use. The L2 TLB is another good use of extra area. A710’s 1024 entry L2 TLB was small by modern standards, so X2’s 2048 entry one is great to see.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23142"><img data-attachment-id="23142" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_arm_slide/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?fit=2264%2C1272&amp;ssl=1" data-orig-size="2264,1272" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_arm_slide" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?fit=2264%2C1272&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?fit=688%2C387&amp;ssl=1" decoding="async" loading="lazy" width="688" height="387" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?w=2264&amp;ssl=1 2264w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1536%2C863&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=2048%2C1151&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1200%2C674&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1600%2C899&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1320%2C742&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Arm’s slide. Labels for some core components added in red</figcaption></figure></div>
<p>Cortex X2 is therefore a cool showing of what Arm’s out-of-order architecture can do when allowed to stretch its legs. Arm’s engineers have made good use of their increased area and power budget to patch up A710’s weakest areas. Newer Cortex X cores carry forward X2’s strengths while using increased transistor budgets to continue patching weaknesses.</p>
<figure><table><tbody><tr><td></td><td>Cortex X2</td><td>Cortex X4</td></tr><tr><td>L1 TLBs</td><td>48 entry iTLB<br>48 entry dTLB</td><td>128 entry iTLB<br>96 entry dTLB</td></tr><tr><td>L2 Cache</td><td>512 KB or 1 MB</td><td>512 KB, 1 MB, or 2 MB</td></tr></tbody></table><figcaption>From looking through the Cortex X4 TRM</figcaption></figure>
<p>I like where Cortex X is going and can see Arm putting pressure on AMD and Intel to keep up the pace. But when your core is stuffed into a SoC with a slow L3 and horrible DRAM latency, it’s going to suffer even when core width and structure sizes look competitive. I hope future implementations will better showcase Cortex X’s potential. </p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80" loading="lazy" decoding="async"> </p>

</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Study Says Maybe Helicopter Parenting Is Making Kids Depressed (134 pts)]]></title>
            <link>https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/</link>
            <guid>38046910</guid>
            <pubDate>Sat, 28 Oct 2023 03:40:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/">https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/</a>, See on <a href="https://news.ycombinator.com/item?id=38046910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-423910">


<h3>from the <i>correlation-and-causation</i> dept</h3>

<p>We’ve been covering, at great length, the moral panic around the claims that social media is what’s making kids depressed. The problem with this narrative is that there’s basically no real evidence to support it. As the American Psychological Association found when it reviewed all the literature, despite many, many dozens of studies done on the impact of social media on kids, <a target="_blank" rel="noreferrer noopener" href="https://www.techdirt.com/2023/05/12/apa-report-says-that-media-politicians-are-simply-wrong-about-kids-social-media-media-then-lies-about-report/">no one was able to establish a causal relationship</a>.</p>
<p>As that report noted, the research seemed to show no inherent benefit or harm for most kids. For some, it showed a real benefit (often around kids being able to find like-minded people online to communicate with). For a very small percentage, it appeared to potentially exacerbate existing issues. And those are really the cases that we should be focused on.</p>
<p>But, instead, the narrative that continues to make the rounds is that social media is inherently bad for kids. That leads to various bills around age verification and age gating to keep kids off of social media.</p>
<p>Supporters of these bills will point to charts like this one, regarding teen suicide rates, noting the uptick correlates with the rise of social media.</p>

<p>Of course, they seem to cherry pick the start date of that chart, because if you go back further, you realize that while the uptick is a concern, it’s still way below what it had been in the 1990s (pre-social media).</p>

<p>In case that embed isn’t working, here’s an image of it:</p>
<figure><img decoding="async" src="https://i0.wp.com/lex-img-p.s3.us-west-2.amazonaws.com/img/bd036c6a-0422-4e33-9c37-427f9f9ddc81-RackMultipart20231025-142-jzngu7.png?ssl=1" alt="Image" data-recalc-dims="1"></figure>
<p>Obviously, the increase in suicides is a concern. But, considering that every single study that tries to link it to social media ends up failing to do so, that suggests that there might be some other factor at play here.</p>
<p>A recent study in the Journal of Pediatrics suggests a compelling alternative. It’s not social media, but the rise of helicopter parenting, in which kids no longer have spaces to just hang out with each other and be kids. It’s titled: <a target="_blank" rel="noreferrer noopener" href="https://doi.org/10.1016/j.jpeds.2023.02.004">Decline in Independent Activity as a Cause of Decline in Children’s Mental Well-being: Summary of the Evidence</a>. If you can’t see the full version, there’s <a target="_blank" rel="noreferrer noopener" href="https://cdn2.psychologytoday.com/assets/2023-02/Children's%20Independence%20IN%20PRESS%20.pdf">a preprint version</a> here.</p>
<p>The research summarizes the decline in “independent mobility” for kids over the last few decades:</p>
<blockquote>
<p><em>Considerable research, mostly in Europe, has focused on children’s independent mobility (CIM), defined as children’s freedom to travel in their neighborhood or city without adult accompaniment. That research has revealed significant declines in CIM, especially between 1970 and 1990, but also some large national differences. For example, surveys regarding the “licenses” (permissions) parents grant to their elementary school children revealed that in England, license to walk home alone from school dropped from 86% in 1971 to 35% in 1990 and 25% in 2010; and license to use public buses alone dropped from 48% in 1971 to 15% in 1990 to 12% in 2010.11 In another study, comparing CIM in 16 different countries (US not included), conducted from 2010 to 2012, Finland stood out as allowing children the greatest freedom of movement. The authors wrote: “At age 7, a majority of Finnish children can already travel to places within walking distance or cycle to places alone; by age 8 a majority can cross main roads, travel home from school and go out after dark alone, by age 9 a majority can cycle on main roads alone, and by age 10 a majority can travel on local buses alone.” Although we have found no similar studies of parental permissions for US children, other data indicate that the US is more like the UK concerning children’s independent mobility than like Finland. For example, National Personal Transportation Surveys revealed that only 12.7% walked or biked to school in 2009 compared with 47.7% in 1969.</em></p>
</blockquote>
<p>And then it notes the general decline in mental health as well, which they highlight started long before social media existed:</p>
<blockquote>
<p><em>Perhaps the most compelling and disturbing evidence comes from studies of suicide and suicidal thoughts. Data compiled by the CDC indicate that the rate of suicide among children under age 15 rose 3.5-fold between 1950 and 2005 and by another 2.4-fold between 2005 and 2020. No other age group showed increases nearly this large. By 2019, suicide was the second leading cause of death for children from age 10 through 15, behind only unintentional injury. Moreover, the 2019 YRBS survey revealed that during the previous year 18.8% of US high school students seriously considered attempting suicide, 15.7% made a suicide plan, 8.9% attempted suicide one or more times, and 2.5% made a suicide attempt requiring medical treatment. We are clearly experiencing an epidemic of psychopathology among young people.</em></p>
</blockquote>
<p>But, unlike those who assume correlation is causation with regards to social media, the researchers here admit there needs to be more. And they bring the goods, pointing to multiple studies that suggest a pretty clear causal relationship, rather than just correlation.</p>
<blockquote>
<p><em>Several studies have examined relationships between the amount of time young children have for self-directed activities at home and psychological characteristics predictive of future wellbeing. These have revealed significant positive correlations between amount of self-structured time (largely involving free play) and (a) scores on two different measures of executive functioning; (b) indices of emotional control and social ability; and (c) scores, two years later, on a measure of self-regulation. There is also evidence that risky play, where children deliberately put themselves in moderately frightening situations (such as climbing high into a tree) helps protect against the development of phobias and reduces future anxiety by increasing the person’s confidence that they can deal effectively with emergencies.</em></p>
<p><em>Studies with adults involving retrospections about their childhood experiences provide another avenue of support for the idea that early independent activity promotes later wellbeing. In one such study, those who reported much free and adventurous play in their elementary school years were assessed as having more social success, higher self-esteem, and better overall psychological and physical health in adulthood than those who reported less such play. In another very similar study, amount of reported free play in childhood correlated positively with measures of social success and goal flexibility (ability to adapt successfully to changes in life conditions) in adulthood. Also relevant here are studies in which adults (usually college students) rated the degree to which their parents were overprotective and overcontrolling (a style that would reduce opportunity for independent activity) and were also assessed for their current levels of anxiety and depression. A systematic review of such studies revealed, overall, positive correlations between the controlling, overprotective parenting style and the measures of anxiety and depression.</em></p>
</blockquote>
<p>They also note that they are not claiming (of course) that this is the sole reason for the declines in mental health. Just that there is strong evidence that it is a key component. They explore a few other options that may contribute, including increased pressure at schools and societal changes. They also consider the impact of social media and digital technologies and note (as we have many times) that there just is no real evidence to support the claims:</p>
<blockquote>
<p><em>Much recent discussion of young people’s mental health has focused on the role of increased use of digital technologies, especially involvement with social media. However, systematic reviews of research into this have provided little support for the contention that either total screen time or time involved with social media is a major cause of, or even correlate of, declining mental health. One systematic review concluded that research on links between digital technology use and teens’ mental health “has generated a mix of often conflicting small positive, negative and null associations” (Odgers &amp; Jensen, 2020). Another, a “review of reviews” concluded that “the association between digital technology use, or social media use in particular, and psychological well-being is, on average, negative but very small” and noted some evidence, from longitudinal research, that negative correlations may result from declining mental health leading to more social media use rather than the reverse (Orben, 2020)</em></p>
</blockquote>
<p>Indeed, if this theory is true, that the lack of spaces for kids to explore and play and experiment without adult supervision <em>is</em> a leading cause of mental health decline, you could easily see how those who are depressed are more likely to <em>seek out</em> those private spaces, and turn to social media, given the lack of any such spaces they can go to physically.</p>
<p>And, if that’s the case, then all of these efforts to ban social media for kids, or to make social media <a rel="noreferrer noopener" href="https://www.techdirt.com/2022/09/20/the-internet-is-not-disneyland-people-should-stop-demanding-it-become-disneyland/" target="_blank">more like Disneyland</a>, could likely end up doing <strong>a lot more harm than good</strong> by cutting off one of the last remaining places where kids can communicate with their peers without adults watching over their every move. Indeed, the various proposals to give parents more access to what their kids are doing online could worsen the problem as well, taking away yet another independent space for kids.</p>
<p>Over the last few years, there’s been a push to <a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2019/05/10/well/family/adventure-playgrounds-junk-playgrounds.html">bring back more “dangerous” play for kids</a>, as people have begun to realize that things may have gone too far in the other direction. Perhaps it’s time we realize that social media fits into that category as well.</p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/age-appropriate-design/" rel="tag">age appropriate design</a>, <a href="https://www.techdirt.com/tag/age-verification/" rel="tag">age verification</a>, <a href="https://www.techdirt.com/tag/depression/" rel="tag">depression</a>, <a href="https://www.techdirt.com/tag/independent-spaces/" rel="tag">independent spaces</a>, <a href="https://www.techdirt.com/tag/mental-health/" rel="tag">mental health</a>, <a href="https://www.techdirt.com/tag/social-media/" rel="tag">social media</a>, <a href="https://www.techdirt.com/tag/studies/" rel="tag">studies</a>, <a href="https://www.techdirt.com/tag/suicide/" rel="tag">suicide</a>, <a href="https://www.techdirt.com/tag/teens/" rel="tag">teens</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A small warning about UDP based protocols (171 pts)]]></title>
            <link>https://boston.conman.org/2023/10/25.1</link>
            <guid>38046448</guid>
            <pubDate>Sat, 28 Oct 2023 02:13:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boston.conman.org/2023/10/25.1">https://boston.conman.org/2023/10/25.1</a>, See on <a href="https://news.ycombinator.com/item?id=38046448">Hacker News</a></p>
Couldn't get https://boston.conman.org/2023/10/25.1: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Staring at a Wall: Embracing Deliberate Boredom (129 pts)]]></title>
            <link>https://www.ch3ngl0rd.com/staring-at-a-wall/</link>
            <guid>38046396</guid>
            <pubDate>Sat, 28 Oct 2023 02:05:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ch3ngl0rd.com/staring-at-a-wall/">https://www.ch3ngl0rd.com/staring-at-a-wall/</a>, See on <a href="https://news.ycombinator.com/item?id=38046396">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    







<p>
    <i>
        <time datetime="2023-10-28">
            28 Oct, 2023
        </time>
    </i>
</p>

<p>You should spend more time being bored.</p>
<p>I spent twenty minutes staring at a wall. Was it worth my time? Yes. Did I look a little bit crazy doing it? Maybe a little.</p>
<p>My friend <a href="https://www.joshshipton.com/">Josh Shipton</a> recently talked about the <a href="https://www.joshshipton.com/boredom.html">Power of Embracing Boredom</a>, and how boredom is needed for your mind to process your thoughts. One exercise he recommends is to sit down, and stare at a wall. I had my doubts, but after one session of staring at a wall, I found it extremely rewarding.</p>
<p>The exercise is quite simple to do:</p>
<ol>
<li>Set up a timer for ten to thirty minutes</li>
<li>Stare at a wall</li>
</ol>
<p>I found the exercise to be most effective with twenty minutes and a white wall.</p>
<h2 id="unexpected-insights">Unexpected Insights</h2>
<p><img alt="thoughts by ch3ngl0rd ᕦʕ •ᴥ•ʔᕤ" src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/ch3ngl0rd-1698456991-0.jpg"></p>
<p>During my walling session, a scene from The Lego Movie unexpectedly came to mind: the moment when the old wizard and the emo girl discover Emmet's profoundly empty mind. While they initially mock him for this and crush the idea of a double-decker couch, their laughter is cut short upon witnessing the vision of 'The Man Upstairs'.</p>
<blockquote>
<p>Master Builders spend years training themselves to clear their minds enough to have even a fleeting glimpse of The Man Upstairs.</p>
<p>- The Old Wizard Guy</p>
</blockquote>
<p>This exercise taps into the same power that many meditation practices aim for - an uncluttered mind. When our minds are clear, they become fertile grounds for introspection and fresh ideas. In just twenty minutes, I generated more insights and processed more unfinished thoughts than if I had simply tried to write them down at a desk.</p>
<p>One aspect I appreciate is how simple this exercise is. In the past, I've tried other meditation techniques, only to feel lost as if I was somehow meditating incorrectly. But sitting in front of a wall? There's some unexpected beauty to it. The sheer emptiness seems to prompt the mind better than just closing my eyes and thinking. And the slight discomfort of staring at a wall for twenty minutes provides just enough sensation to anchor you to the present. It's the perfect nudge to keep you grounded in the moment.</p>
<h2 id="don-t-get-lost-in-the-sauce">Don't get Lost in the Sauce</h2>
<p>During my "walling" session, I reflected about the importance of <strong>Taking a Step Back</strong> - we should schedule times in the future to take a step back to make sure that we're not losing sight of the bigger picture or becoming too obsessed with minor details - essentially, not getting "lost in the sauce".</p>
<p>Lately, I've become interested in entrepreneurship and how startups work - reading books like <em>The Lean Startup, Zero to One and The Mom Test</em>. While they've offered invaluable insights, I find myself at a crossroads. Should I continue grokking knowledge through reading, or is it time to take the leap and start building a startup?</p>
<p>Take "The Lean Startup" for instance. It emphasises that startups operate under <strong>conditions of extreme uncertainty</strong>. The key to succeeding is to write down the riskiest assumptions and validate or invalidate them with MVPs (Minimal Viable Products). This insight alone has probably saved me a huge mistake in the future. Yet I'm left wondering: Would diving into more books be as beneficial as actually building and bringing my startup idea to life? Are there insights in the next chapter that could supercharge my performance? It's hard to say.</p>
<p>While staring at the wall, I realised that just as it's easy to get lost in thought, it's also easy to get lost in endless reading and preparation. Sometimes, we need to step back, reflect on what we've learned, and take action. Reading about startups is helpful, but at some point, we need to start building one.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>I think you should stare at a wall at least once. If that doesn't suit you, go on a walk and deliberately plan to be bored. Leave your phone at home. Don't bring your headphones. Simply immerse yourself in your surroundings, free from distractions. Lose track of time and appreciate the world around you.</p>
<p><em>You should spend more time being bored.</em></p>
<center><b>ch3ngl0rd out.</b></center>
<p><a href="https://news.ycombinator.com/item?id=38046396">Discuss</a> or <a href="https://www.linkedin.com/in/zachary-cheng-19a395212/">Get in Touch</a>.</p>



    

    
    



    



  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When gradient descent is a kernel method (189 pts)]]></title>
            <link>https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html</link>
            <guid>38045665</guid>
            <pubDate>Sat, 28 Oct 2023 00:23:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html">https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html</a>, See on <a href="https://news.ycombinator.com/item?id=38045665">Hacker News</a></p>
<div id="readability-page-1" class="page">
    
    
    <p>Suppose that we sample a large number <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> of independent random functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi mathvariant="double-struck">R</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f_i \colon \R \to \R</annotation></semantics></math></span></span> from a certain distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> and propose to solve a regression problem by choosing a linear combination
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>λ</mi><mi>i</mi></msub><msub><mi>f</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f} = \sum_i \lambda_i f_i.</annotation></semantics></math></span></span></span>
For large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">N,</annotation></semantics></math></span></span> adjusting the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span> to fit some fixed constraints of the form <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\bar{f}(t_i) = y_i</annotation></semantics></math></span></span> amounts to solving a highly underdetermined linear system, meaning that a high-dimensional space <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> of vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>λ</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>λ</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\lambda_1, \dots, \lambda_N)</annotation></semantics></math></span></span> fit our constraints perfectly. So, choosing one element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> requires some additional decision-making. To use the picturesque idea of a
"loss landscape" over parameter space, our problem will have a <em>ridge</em> of equally performing parameters rather than just a single optimal <em>peak</em>.</p>
<p>Now, we make a very strange proposal. What if we simply initialize <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mi mathvariant="normal">/</mi><msqrt><mi>n</mi></msqrt></mrow><annotation encoding="application/x-tex">\lambda_i = 1/\sqrt{n}</annotation></semantics></math></span></span> for all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span> and proceed by minimizing some loss function using gradient descent? If all goes well, we should end up with an element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Lambda.</annotation></semantics></math></span></span> Of course, many elements of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> give very bad models. To see this, it's enough to remember that we can expect a linear combination of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> random functions to fit <em>any</em> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> data points, so if we have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span> data points, there exist models in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> that perfectly interpolate any adversarial selection of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>−</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">N - m</annotation></semantics></math></span></span> additional data points! Does gradient descent tend to make a "good" choice?</p>
<p>Let's test this empirically. In the widget below, I've chosen functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span></span> by sampling <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span></span> trajectories of a Wiener process, also known as Brownian noise. Click anywhere to introduce data points and click the play button in the top right to run gradient descent for the squared loss <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\sum_i (\bar{f}(t_i) - y_i)^2.</annotation></semantics></math></span></span></p>

<p>Interestingly, the functions we obtain are not that bad. They seem to concentrate around piecewise linear interpolations of our data. In fact, in the limit <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">N \to \infty</annotation></semantics></math></span></span> of many random functions, it turns out that running gradient descent to convergence has a meaningful statistical interpretation. Specifically, if we view the Wiener process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> as a prior, then <strong>running gradient descent to convergence samples from the posterior for our data points.</strong> Since many optimal solutions to our minimization problem are meaningless, it is not possible to explain this fact if we see gradient descent as "just some optimization method." What explains its relative success?</p>
<p>As we will show in this post, our intriguing Bayesian interpretation can be explained by the relationship between the behavior of gradient descent steps, the statistical properties of our random functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span></span>, and our initialization. In particular, it does <em>not</em> depend on the loss function—so long as it leads gradient descent to converge to an exact interpolation—but <em>does</em> depend significantly on our choice of parameters at initialization. Our analysis will rely on a "tangent kernel" of the sort introduced in the <em>Neural Tangent Kernel</em> paper by Jacot et al.. Specifically, viewing gradient descent as a process occurring in the function space of our regression problem, we will find that its dynamics can be described in terms of a certain kernel function, which in this case is just the kernel function of the process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Fc.</annotation></semantics></math></span></span></p>
<p>Of course, there are much easier ways to sample posteriors for low-dimensional Gaussian processes. Nevertheless, it's interesting to notice a relationship between Bayesian inference and gradient descent methods at large, since the latter tend to apply to situations where direct estimation of a posterior distribution is not practical. Furthermore, the results of Jacot suggest that kernel-based interpretations may also hold for large, non-trivial neural networks. To the extent that this is true, we can use the kernel interpretation to reason about many sorts of neural network phenomena, including the benefits of early stopping, the existence of "implicit regularization", and the fact that overparameterization often <em>increases</em> performance despite the apparent risk of overfitting.</p>
<p>In this post, we will focus exclusively on the toy problem introduced above. Our discussion (which admittedly takes a bit of a scenic route) is divided into three sections.</p>
<ul>
<li>First, we will discover how the covariance kernel of the process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> is related to the dynamics of gradient descent.</li>
<li>Next, we recall the theory of reproducing kernel Hilbert spaces and show how the kernel-based behavior of gradient descent is related to regularization.</li>
<li>Finally, we recall some special properties of Gaussian processes and explain why regularization is related to the Bayesian interpretation of our trained model.</li>
</ul>
<h2>Kernel Functions</h2>
<p>Let's begin by considering the effect that a single step of gradient descent has on our function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f}.</annotation></semantics></math></span></span> In general, the differential of a loss can be written as a sum of differentials <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span></span> is the evaluation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> at an input <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t,</annotation></semantics></math></span></span> so by linearity it is enough for us to understand how <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> "responds" to differentials of this form.</p>
<p>In response to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">d \pi_t,</annotation></semantics></math></span></span> the parameters <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span> are assigned differentials
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>λ</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial \pi_t}{\partial \lambda_i} = f_i(t).</annotation></semantics></math></span></span></span>
So, gradient descent will increase <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span> proportional to the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f_i(t).</annotation></semantics></math></span></span> In terms of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\bar{f},</annotation></semantics></math></span></span> we find that the value <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\bar{f}(s)</annotation></semantics></math></span></span> at another input <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span></span> will increase proportional to
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\Delta_t(s) = \sum_{i = 1}^N f_i(s) f_i(t). \tag{1}</annotation></semantics></math></span></span></span>
Note that this expression is independent of the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\lambda_i.</annotation></semantics></math></span></span> This means that the gradient descent step we apply to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> depends only on our learning rate and differential of the loss at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f}.</annotation></semantics></math></span></span> In other words, we can view gradient descent as a process happening in the function space of our regression problem.</p>
<p>For large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> we get the approximation
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>≈</mo><msub><mi>E</mi><mrow><mi>f</mi><mo>∼</mo><mi mathvariant="script">F</mi></mrow></msub><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\frac{1}{N} \Delta_t(s) \approx E_{f \sim \Fc}[f(s) f(t)]. \tag{2}</annotation></semantics></math></span></span></span>
This last expression is familiar in the study of Gaussian processes; it is called the covariance kernel of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> and denoted <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t).</annotation></semantics></math></span></span> For the Wiener process, the covariance kernel takes the form
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t) = \min(s, t).</annotation></semantics></math></span></span></span>
In the following, we'll assume <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> is large enough for us to make the approximation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(2)</annotation></semantics></math></span></span> confidently. Then, we conclude that a request of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span></span> will cause gradient descent to push <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> in the direction of the function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span></span></p>
<p>You can get a visual sense of this behavior in the widget below. As above, I've generated <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span></span> trials of the Wiener process to use as my functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f_i.</annotation></semantics></math></span></span> You can choose the request <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span></span> by clicking on the graph, and your browser will compute the corresponding response <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mi mathvariant="normal">/</mi><mi>N</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Delta_t/N.</annotation></semantics></math></span></span> For comparison I've also drawn the prediction <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span></span></p>

<p>This is already a significant conclusion. In particular, it means that every step of gradient descent modifies <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> by a linear combination of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K(-, t_i),</annotation></semantics></math></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">t_i</annotation></semantics></math></span></span> ranges over the inputs in our training set. Since the linear span of these functions is a certain space of piecewise affine functions, if we initialize <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_i = 0</annotation></semantics></math></span></span> and run gradient descent to convergence with <em>any</em> reasonable loss function, we should approximately converge to a piecewise affine interpolation of our data points. We've run this experiment in the widget below.</p>

<p>This new model has less variance than before. In fact, in the large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> limit, its behavior will be exactly deterministic! In comparison, our previous model will <em>always</em> exhibit variance due to initialization of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span></span> even for large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">N.</annotation></semantics></math></span></span> In other words, when given a finite training set, gradient descent cannot entirely "forget" its initialization even when run to convergence.</p>
<p>Another important conclusion is that, when we optimize least squares with gradient descent, the evolution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> is linear in the sense of approximately obeying a linear ODE. Indeed, for data points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(t_i, y_i)</annotation></semantics></math></span></span> our loss differential will be
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>d</mi><munder><mo>∑</mo><mi>i</mi></munder><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>d</mi><msub><mi>π</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{1}{2} d \sum_i (\bar{f}(t_i) - y_i)^2 = \sum_i d \pi_{t_i} (\bar{f}(t_i) - y_i).</annotation></semantics></math></span></span></span>
So, if we view <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> as evolving under the flow of gradient descent with respect to a continuous parameter <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\tau,</annotation></semantics></math></span></span> we have
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>d</mi><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><mrow><mi>d</mi><mi>τ</mi></mrow></mfrac><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\frac{d \bar{f}}{d \tau} = \sum_i K(-, t_i) (y_i - \bar{f}(t_i)),</annotation></semantics></math></span></span></span>
where the right-hand side is a linear function of the empirical error vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">h = (y_i - \bar{f}(t_i)).</annotation></semantics></math></span></span> Restricting this equation to the evaluation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> over the input points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t_i,</annotation></semantics></math></span></span> we find that the error vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> solves the ODE
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>h</mi></mrow><mrow><mi>d</mi><mi>τ</mi></mrow></mfrac><mo>=</mo><mo>−</mo><mi>K</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">\frac{d h}{d \tau} = -K h</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> is the matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(K(t_i, t_j)).</annotation></semantics></math></span></span> Since this matrix is positive-definite—it is a covariance matrix—we conclude that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> will converge to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span> over the training process if our learning rate is sufficiently small. Furthermore, knowing the eigenvalues of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> lets us understand the nature of our convergence; gradient descent will "correct" the error <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> along the components of the eigenbasis for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> with largest eigenvalues first, and take longer to correct components with smaller eigenvalues.</p>
<p>Now, we could have chosen a distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> of random functions with a different covariance kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K.</annotation></semantics></math></span></span> Here the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(-, t)</annotation></semantics></math></span></span> were easy to interpret, but in general, what does it mean to fit a data set using a linear combination of functions like these? One interesting perspective comes from the idea of <em>regularization</em>, which we discuss next.</p>
<h2>Regularization</h2>
<p>Consider a Hilbert space <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> equipped with bounded projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>H</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\pi_t \colon H \to \R</annotation></semantics></math></span></span> for each <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t \in \R,</annotation></semantics></math></span></span> and suppose that we want to find an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">v \in H</annotation></semantics></math></span></span> that minimizes some loss function depending on the values <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_{t}(v)</annotation></semantics></math></span></span> for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span> belonging to some collection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\{t_i\}.</annotation></semantics></math></span></span> (Note that elements of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> can be viewed as functions from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\R</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\R</annotation></semantics></math></span></span> by viewing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span></span> as the evaluation at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t.</annotation></semantics></math></span></span>) If this problem is underdetermined—which necessarily will happen if <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> is infinite-dimensional and our collection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{t_i\}</annotation></semantics></math></span></span> is finite—then we may ask for an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> that both minimizes our loss and has minimal norm in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H.</annotation></semantics></math></span></span> In machine learning, this is called regularization.</p>
<p>Let's write <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> for the product of the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><msub><mi>t</mi><mi>i</mi></msub></msub></mrow><annotation encoding="application/x-tex">\pi_{t_i}</annotation></semantics></math></span></span> Because <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> is chosen with minimal norm, it cannot be made smaller by adjusting it by an element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\ker \Pi,</annotation></semantics></math></span></span> so <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> is orthogonal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\ker \Pi.</annotation></semantics></math></span></span> But since the maps <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{t}</annotation></semantics></math></span></span> are continuous, they can be represented by vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">K_{t}</annotation></semantics></math></span></span> in the sense that
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mo>−</mo><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo><mo>−</mo><mo stretchy="false">⟩</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\pi_{t}(-) = \langle K_{t}, - \rangle.</annotation></semantics></math></span></span></span>
(This is the Riesz representation theorem.) Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\ker \Pi</annotation></semantics></math></span></span> can be described as the orthogonal complement to the set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>K</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mo stretchy="false">}</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\{K_{t_i}\},</annotation></semantics></math></span></span> the orthogonal complement to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\ker \Pi</annotation></semantics></math></span></span> is exactly the closure of the span of the vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K_{t_i}.</annotation></semantics></math></span></span> We conclude that any regularized solution to our loss function is a (limit of) linear combinations of these vectors.</p>
<p>What are the projections of the "representative elements" <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">K_{t}</annotation></semantics></math></span></span>? By our own definition, we have
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>s</mi></msub><mo stretchy="false">(</mo><msub><mi>K</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>s</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>t</mi></msub><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\pi_s(K_t) = \langle K_s, K_t \rangle</annotation></semantics></math></span></span></span>
for any other <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">s \in \R.</annotation></semantics></math></span></span> This last expression is a positive semidefinite kernel, which we will denote <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t).</annotation></semantics></math></span></span> In other words, the norm on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> and the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span></span> work together to produce a kernel function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> whose partial evaluations <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(-, t)</annotation></semantics></math></span></span> help us solve optimization problems regularized by the norm of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H.</annotation></semantics></math></span></span></p>
<p>In the literature, a Hilbert space equipped with bounded projections indexed over a set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span></span> is called a reproducing kernel Hilbert space (or RKHS). In fact, we can also go in the other direction: every positive definite kernel on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span></span> is "reproduced" by some RKHS, which also turns out to be unique in a certain sense. This is known as the Moore-Aronszajn theorem.</p>
<p>What RKHS corresponds to our kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(s, t) = \min(s, t)</annotation></semantics></math></span></span>? In general, determining the RKHS of a kernel is not entirely straightforward. In fact, notice that for positive definite kernels over a finite set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">I,</annotation></semantics></math></span></span> the inner product for the RKHS expressed in the dual basis for our projections turns out to be the <em>inverse</em> of the matrix encoded by our kernel. Indeed, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">K_i</annotation></semantics></math></span></span> are representatives of the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\pi_i</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">e_i</annotation></semantics></math></span></span> is a dual basis verifying <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\pi_i(e_j) = \delta_{i, j},</annotation></semantics></math></span></span> we find that
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">⟩</mo><mi>K</mi><mo stretchy="false">(</mo><mi>j</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">⟩</mo><msub><mi>π</mi><mi>j</mi></msub><mo stretchy="false">(</mo><msub><mi>K</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>k</mi></msub><mo stretchy="false">⟩</mo><mo>=</mo><msub><mi>π</mi><mi>k</mi></msub><mo stretchy="false">(</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle e_i, e_j \rangle K(j, k) = \langle e_i, e_j \rangle \pi_j(K_k) = \langle e_i, K_k \rangle = \pi_k(e_j) = \delta_{i, k}.</annotation></semantics></math></span></span></span>
So, interpreting a RKHS norm in terms of projections of elements requires solving some sort of inverse problem.</p>
<p>The RKHS for a centered Gaussian process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(X_t)</annotation></semantics></math></span></span> can be viewed as an isometric embedding of the observables <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span></span> with respect to the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L^2</annotation></semantics></math></span></span> norm for the process measure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Fc.</annotation></semantics></math></span></span> Specifically, if we define <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">f(X_t) = K_t,</annotation></semantics></math></span></span> then clearly
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>X</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>X</mi><mi>k</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><msup><mi>L</mi><mn>2</mn></msup><mi mathvariant="script">F</mi></mrow></msub><mo>=</mo><mo stretchy="false">⟨</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mo stretchy="false">⟩</mo><mtext>RKHS</mtext></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle X_t, X_k \rangle_{L^2 \Fc} = \langle f(X_t), f(X_k) \rangle_\text{RKHS}.</annotation></semantics></math></span></span></span>
Indeed, the space of observables of a Gaussian process is already a RKHS for its covariance kernel, if we take the projections to be the maps <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>X</mi><mi>t</mi></msub><mo separator="true">,</mo><mo>−</mo><mo stretchy="false">⟩</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle X_t, - \rangle.</annotation></semantics></math></span></span> However, we would like to view the RKHS more directly as a space of functions.</p>
<p>We may begin by observing, then, that observables of the Wiener process can be isometrically mapped into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>1</mn></msup><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L^1 [0, \infty)</annotation></semantics></math></span></span> by sending <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span></span> to
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mi>s</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><mo>:</mo><mi>t</mi><mo>≤</mo><mi>s</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>0</mn><mo>:</mo><mtext>otherwise.</mtext></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">K_s(t) = \begin{cases}
1 : t \le s \\
0 : \text{otherwise.}
\end{cases}</annotation></semantics></math></span></span></span>
Under this perspective, our projections become
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">⟩</mo><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>t</mi></msubsup><mi>f</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mtext> </mtext><mi>d</mi><mi>s</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\pi_t(f) = \langle K_t, f \rangle = \int_0^t f(s) \, ds.</annotation></semantics></math></span></span></span>
Ultimately, we are led to view the RKHS of the Wiener process as the Sobolev space of absolutely continuous functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f \colon [0, \infty) \to \R</annotation></semantics></math></span></span> such that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f(0) = 0</annotation></semantics></math></span></span> and such that the norm
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">∥</mo><mi>f</mi><mo stretchy="false">∥</mo><mo>=</mo><mrow><mo fence="true">(</mo><msubsup><mo>∫</mo><mn>0</mn><mi mathvariant="normal">∞</mi></msubsup><mo stretchy="false">(</mo><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mtext> </mtext><mi>d</mi><mi>t</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lVert f \rVert = \left( \int_0^\infty (f'(t))^2 \, dt \right)</annotation></semantics></math></span></span></span>
is finite. In fact, solving the regularized interpolation problem
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext mathvariant="bold">minimize</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><msubsup><mo>∫</mo><mn>0</mn><mi mathvariant="normal">∞</mi></msubsup><mo stretchy="false">(</mo><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mtext> </mtext><mi>d</mi><mi>t</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext mathvariant="bold">subject</mtext><mtext>&nbsp;</mtext><mtext mathvariant="bold">to</mtext></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mi>f</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mtext>  </mtext><mtext>for&nbsp;all&nbsp;</mtext><mi>i</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\textbf{minimize} &amp; \quad \int_0^\infty (f'(s))^2 \, dt \\
\textbf{subject to} &amp; \quad f(0) = 0, f(t_i) = y_i \; \text{for all }i
\end{align*}</annotation></semantics></math></span></span></span>
results in the piecewise affine interpolations we observed in the widget above.</p>
<p>So far, we have shown that its relationship with kernel functions gives gradient descent a distinct flavor of <em>implicit regularization</em>. We did not have a penalty function in mind when we set up our problem, but our distribution of random functions ended up making our gradient updates interpretable in terms of a RKHS for an associated kernel function. In the last section of this post, we address how this fact is related to the statistical idea of a conditional distribution for a Gaussian process.</p>
<h2>Bayesian Interpretation</h2>
<p>When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> are jointly Gaussian distributed, we know that the remainder
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span></span></span>
of the conditional expectation is independent from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">X.</annotation></semantics></math></span></span> (Remember that for other distributions, this remainder will have zero covariance with all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span>-measurable events but will not necessarily be independent!) So, we can decompose <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> into two components
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">(</mo><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">Y = (Y - E(Y|X)) + E(Y|X),</annotation></semantics></math></span></span></span>
the first being a Gaussian variable independent from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> and the second being <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span>-measurable. This clarifies the nature of the conditional distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math></span></span>: it will have constant variance equal to the variance of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span></span> and mean equal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">E(Y|X),</annotation></semantics></math></span></span> a linear function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">X.</annotation></semantics></math></span></span> In particular, if we want to sample the conditional distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> given <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X = x,</annotation></semantics></math></span></span> we could take
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Y</mi><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo>−</mo><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">Y + E(Y|X = x) - E(Y|X) = Y + E(Y|X = x - X),</annotation></semantics></math></span></span></span>
where the apparently nonsensical conditional expectation on the RHS should be interpreted as the evaluation of the conditional expectation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">E(Y|X),</annotation></semantics></math></span></span> viewed as a function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X,</annotation></semantics></math></span></span> at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>−</mo><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">x - X.</annotation></semantics></math></span></span> Keep in mind that this is a very special property of Gaussian distributions; in general, the distribution of the remainder <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span></span> conditional on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> will depend on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X,</annotation></semantics></math></span></span> and so we won't be able to sample the conditional distribution under another "counterfactual" value <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math></span></span> simply by translating a sample of the remainder.</p>
<p>Now, consider a random function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> drawn from a Gaussian distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> and let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> give the values of our trajectory on a finite set of inputs. If we want to produce a sample from the distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> conditional on some data <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><mi>π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Pi = \pi,</annotation></semantics></math></span></span> we can take
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><mi>π</mi><mo>−</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f - E(f | \Pi = \pi - \Pi).</annotation></semantics></math></span></span></span>
But, as it turns out, the conditional expectation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f|\Pi = \pi^*)</annotation></semantics></math></span></span> will be exactly the function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> in the RKHS of our process that solves the constraint <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\Pi = \pi^*</annotation></semantics></math></span></span> regularized by the RKHS norm! This explains the Bayesian interpretation of our model.</p>
<p>One way to understand this is just to write out the expression for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi)</annotation></semantics></math></span></span> at a given value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t.</annotation></semantics></math></span></span> We know that this will be the linear function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\lambda \Pi</annotation></semantics></math></span></span> of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> uniquely determined by the equation
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Cov</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>−</mo><mi>λ</mi><mi mathvariant="normal">Π</mi><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.</mn></mrow><annotation encoding="application/x-tex">\Cov(f(t) - \lambda \Pi, \Pi) = 0.</annotation></semantics></math></span></span></span>
Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">K = [K(t_i, t_j)]</annotation></semantics></math></span></span> is the covariance matrix of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="normal">Cov</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">v = [\Cov(f(t), f(t_i))]</annotation></semantics></math></span></span> gives the covariance of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math></span></span> with the components <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t_i)</annotation></semantics></math></span></span> of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Pi,</annotation></semantics></math></span></span> this equation can be written as
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>v</mi><mo>−</mo><mi>λ</mi><mi>K</mi><mo>=</mo><mn>0</mn><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">v - \lambda K = 0,</annotation></semantics></math></span></span></span>
and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mo>=</mo><mi>v</mi><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi mathvariant="normal">Π</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">E(f(t) | \Pi) = v K^{-1} \Pi.</annotation></semantics></math></span></span> We conclude that the function
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>t</mi><mo>↦</mo><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t \mapsto E(f(t)|\Pi)</annotation></semantics></math></span></span></span>
is a linear combination of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(t, t_i)</annotation></semantics></math></span></span>—the coordinates of the vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span>—with constant coefficients, determined by the constraints that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi)</annotation></semantics></math></span></span> should agree with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> at the points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><msub><mi>t</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t = t_i.</annotation></semantics></math></span></span> But, as we saw above, this is the same as the solution to the problem of interpolating some constraints <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f(t_i) = y_i</annotation></semantics></math></span></span> regularized by the RKHS norm of our process.</p>
<p>To see this connection more directly, remember that the mean of a Gaussian distribution coincides with the mode—the point of highest probability density under linear coordinates. So, for example, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi = \pi^*)</annotation></semantics></math></span></span> is exactly the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math></span></span> that minimizes
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>ln</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mo>+</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mtext>  </mtext><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mtext>  </mtext><mo>…</mo><mtext>  </mtext><mi>π</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\ln(p(f(t), \pi^*)) = C + -\frac{1}{2} 
[f(t)  \; \pi^*(1)  \; \dots \; \pi(m)] K^{-1}
\begin{bmatrix}
f(t)   \\
\pi^*(1)   \\
\vdots  \\
\pi(m)]
\end{bmatrix}</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> is the inverse of the covariance matrix for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(f(t), \Pi).</annotation></semantics></math></span></span> But from the previous section we know that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">K^{-1}</annotation></semantics></math></span></span> expresses the inner product of the RKHS derived from the covariance kernel of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t), \Pi)</annotation></semantics></math></span></span> in the dual basis for the projections. So in fact we are asking for the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t), \Pi)</annotation></semantics></math></span></span> that satisfies the constraint <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\Pi = \pi^*</annotation></semantics></math></span></span> and is regularized by the RKHS norm corresponding to a restriction of the covariance kernel of our process, which by the representer theorem will be a linear combination of restrictions of functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span></span></p>
<p>Abstractly, we are relying on the fact that whenever we have a positive definite kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>I</mi><mo>×</mo><mi>I</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">K \colon I \times I \to \R</annotation></semantics></math></span></span> and a finite subset <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>⊆</mo><mi>I</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">J \subseteq I,</annotation></semantics></math></span></span> we get a natural projection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>H</mi><mo>→</mo><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">P \colon H \to H_J</annotation></semantics></math></span></span> onto the RKHS for the kernel restricted to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>×</mo><mi>J</mi></mrow><annotation encoding="application/x-tex">J \times J</annotation></semantics></math></span></span> given by
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><msub><mi>π</mi><mi>j</mi></msub><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><msub><mi>K</mi><mi>j</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(v) = \sum_j \pi_j(v) K_j.</annotation></semantics></math></span></span></span>
Given a vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><mi>H</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">v \in H,</annotation></semantics></math></span></span> what is the norm of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(v)</annotation></semantics></math></span></span> in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span></span>? Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span></span> is an isometry over the span of the elements <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K_j,</annotation></semantics></math></span></span> we can view <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">∥</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><msub><mo stretchy="false">∥</mo><msub><mi>H</mi><mi>J</mi></msub></msub></mrow><annotation encoding="application/x-tex">\lVert P(v) \rVert_{H_J}</annotation></semantics></math></span></span> as the minimum possible norm for an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>∈</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">w \in H</annotation></semantics></math></span></span> solving the equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(w) = P(v).</annotation></semantics></math></span></span> In particular, solving a regularized problem over <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> that depends on projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\pi_j</annotation></semantics></math></span></span> for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>∈</mo><mi>J</mi></mrow><annotation encoding="application/x-tex">j \in J</annotation></semantics></math></span></span> and then restricting the solution to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span></span> is the same as restricting to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span></span> and solving the regularized problem with respect to the norm on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H_J.</annotation></semantics></math></span></span></p>
<p>As a final remark, note that we can informally imagine the RKHS of a Gaussian process as specifying the "energy" of the process in a statistical mechanics sense; although the norm of the RKHS is not defined over the same function space that the process takes values, we get the energy for the joint distribution of any finite projection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t_1), \dots, f(t_m))</annotation></semantics></math></span></span> as a function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(y_1, \dots, y_m)</annotation></semantics></math></span></span> by solving
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext mathvariant="bold">minimize</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mo stretchy="false">∥</mo><mi>f</mi><msub><mo stretchy="false">∥</mo><mi>H</mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext mathvariant="bold">subject</mtext><mtext>&nbsp;</mtext><mtext mathvariant="bold">to</mtext></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\textbf{minimize} &amp; \quad \lVert f \rVert_H \\
\textbf{subject to} &amp; \quad f(t_i) = y_i.
\end{align*}</annotation></semantics></math></span></span></span>
This is the most satisfactory way that I've found to connect the interpretation of kernel functions in terms of regularization with their interpretation in terms of conditional expectations of a Gaussian process.</p>

    


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The destruction of Gaza's internet is complete (128 pts)]]></title>
            <link>https://www.wired.com/story/gaza-internet-blackout-israel/</link>
            <guid>38045514</guid>
            <pubDate>Sat, 28 Oct 2023 00:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/gaza-internet-blackout-israel/">https://www.wired.com/story/gaza-internet-blackout-israel/</a>, See on <a href="https://news.ycombinator.com/item?id=38045514">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>For more than three weeks, Gaza has faced an almost total internet blackout. The cables, cell towers, and infrastructure needed to keep people online have been damaged or destroyed as Israel launched thousands of missiles in response to <a href="https://www.wired.com/story/israel-hamas-war-surveillance/">Hamas attacking Israel</a> and taking <a href="https://www.wired.com/story/livestream-hostages-israel-hamas-war/">hundreds of hostages</a> on October 7. Then, this evening, amid reports of heavy bombing in Gaza, some of the last remaining connectivity disappeared.</p><p>In the days after October 7, people living in Gaza have been unable to communicate with family or friends, leaving them unsure whether loved ones are alive. Finding reliable news about events has become harder. Rescue workers have not been able to connect to mobile networks, hampering recovery efforts. And information flowing out of Gaza, showing the conditions on the ground, has been stymied.</p><p>As the Israel Defense Forces said it was expanding its ground operations in Gaza this evening, internet connectivity fell further. Paltel, the main Palestinian communications company, has been able to keep some of its services online during Israel’s military response to Hamas’ attack. However, at around 7:30 pm local time today, internet monitoring firm NetBlocks <a data-offer-url="https://twitter.com/netblocks/status/1717942556703551590" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://twitter.com/netblocks/status/1717942556703551590&quot;}" href="https://twitter.com/netblocks/status/1717942556703551590" rel="nofollow noopener" target="_blank">confirmed</a> a “collapse” in connectivity in the Gaza Strip, mostly impacting remaining Paltel services.</p><p>“We regret to announce a complete interruption of all communications and internet services within the Gaza Strip,” Paltel <a data-offer-url="https://www.facebook.com/paltel.970/posts/pfbid02RhT28obiwuAnasm2P64TbwCkTWssucQUWJqpBNyHDhtWS5fdWDGsSwXwa56vQq3l" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.facebook.com/paltel.970/posts/pfbid02RhT28obiwuAnasm2P64TbwCkTWssucQUWJqpBNyHDhtWS5fdWDGsSwXwa56vQq3l&quot;}" href="https://www.facebook.com/paltel.970/posts/pfbid02RhT28obiwuAnasm2P64TbwCkTWssucQUWJqpBNyHDhtWS5fdWDGsSwXwa56vQq3l" rel="nofollow noopener" target="_blank">posted</a> in a post on its Facebook page. The company claimed that bombing had “caused the destruction of all remaining international routes.” An identical post was made on the Facebook page of Jawwal, the region’s biggest mobile provider, which is owned by Paltel. Separately, Palestinian Red Crescent, a humanitarian organization, <a data-offer-url="https://twitter.com/PalestineRCS/status/1717953723605901373?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://twitter.com/PalestineRCS/status/1717953723605901373?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet&quot;}" href="https://twitter.com/PalestineRCS/status/1717953723605901373?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet" rel="nofollow noopener" target="_blank">said on X</a> (formerly Twitter) that it had lost contact with its operation room in Gaza and is “deeply concerned” about its ability to keep caring for people, with landline, cell, and internet connections being inaccessible.</p><p>“This is a terrifying development,” Marwa Fatafta, a policy manager focusing on the Middle East and North Africa at the <a data-offer-url="https://www.accessnow.org/press-release/communications-blackout-gaza-strip/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.accessnow.org/press-release/communications-blackout-gaza-strip/&quot;}" href="https://www.accessnow.org/press-release/communications-blackout-gaza-strip/" rel="nofollow noopener" target="_blank">digital rights group Access Now</a>, tells WIRED. “Taking Gaza completely off the grid while launching an unprecedented bombardment campaign only means something atrocious is about to happen.”</p><p>A WIRED review of internet analysis data, social media posts, and Palestinian internet and telecom company statements shows how connectivity in the Gaza Strip drastically plummeted after October 7 and how some buildings linked to internet firms have been damaged in attacks. Photos and videos show sites that house various internet and telecom firms have been damaged, while reports from official organizations, including the United Nations, describe the impact of people being offline.</p><p>Damaged Lines</p><p>Around the world, the internet and telecoms networks that typically give web users access to international video calls, online banking, and endless social media are a complicated, sprawling mix of hardware and software. Networks of networks, combining data centers, servers, switches, and reams of cables, communicate with each other and send data globally. Local internet access is provided by a mix of companies with no clear public documentation of their infrastructure, making it difficult to monitor the overall status of the system as a whole. In Gaza, experts say, internet connectivity is heavily reliant on Israeli infrastructure to connect to the outside world.</p><p>Amid Israel’s intense bombing of Gaza, physical systems powering the internet have been destroyed. On October 10, the United Nations’ Office for the Coordination of Humanitarian Affairs (OCHA), which oversees emergency responses, <a data-offer-url="https://www.ochaopt.org/content/hostilities-gaza-strip-and-israel-flash-update-4" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ochaopt.org/content/hostilities-gaza-strip-and-israel-flash-update-4&quot;}" href="https://www.ochaopt.org/content/hostilities-gaza-strip-and-israel-flash-update-4" rel="nofollow noopener" target="_blank">said</a> air strikes “targeted several telecommunication installations” and had destroyed two of the three main lines of communications going into Gaza.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google can turn ANC earbuds into a heart rate monitor with no extra hardware (179 pts)]]></title>
            <link>https://9to5google.com/2023/10/27/google-anc-earbuds-heart-rate/</link>
            <guid>38045342</guid>
            <pubDate>Fri, 27 Oct 2023 23:36:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5google.com/2023/10/27/google-anc-earbuds-heart-rate/">https://9to5google.com/2023/10/27/google-anc-earbuds-heart-rate/</a>, See on <a href="https://news.ycombinator.com/item?id=38045342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?quality=82&amp;strip=all&amp;w=1600" srcset="https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1600" height="800" alt="" fetchpriority="high">
	
	</figure>

<p>Google today <a href="https://research.google/pubs/pub52460/">detailed its research</a> into audioplethysmography (APG) that adds heart rate sensing capabilities to active noise canceling (ANC) headphones and earbuds “with a simple software upgrade.”</p>



<p><a href="https://blog.research.google/2023/10/audioplethysmography-for-cardiac.html?m=1">Google says</a> the “ear canal [is] an ideal location for health sensing” given that the deep ear artery “forms an intricate network of smaller vessels that extensively permeate the auditory canal.”</p>



<p>This audioplethysmography approach works by “sending a low intensity ultrasound probing signal through an ANC headphone’s speakers.”</p>



<blockquote>
<p>This signal triggers echoes, which are received via on-board feedback microphones. We observe that the tiny ear canal skin displacement and heartbeat vibrations modulate these ultrasound echoes.</p>
</blockquote>



<p>A model that Google created works to process that feedback into a heart rate reading, as well as heart rate variability (HRV) measurement. This technique works even with music playing and “bad earbuds seals.” However, it was impacted by body motion, and Google countered with a multi-tone approach that serves as a calibration tool to “find the best frequency that measures heart rate, and use only the best frequency to get high-quality pulse waveform.”</p>



<p>Google performed two sets of studies with 153 people that found APG “achieves consistently accurate heart rate (3.21% median error across participants in all activity scenarios) and heart rate variability (2.70% median error in inter-beat interval) measurements.”</p>



<p>Compared to existing HR sensors, it’s not impacted by skin tones. Ear canal size and “sub-optimal seal conditions” also do not impact accuracy. Google believes this is a better approach than putting traditional&nbsp;photoplethysmograms (PPG) and electrocardiograms (ECG) sensors, as well as a microcontroller, in headphones/earbuds:</p>



<blockquote>
<p>…this sensor mounting paradigm inevitably adds cost, weight, power consumption, acoustic design complexity, and form factor challenges to hearables, constituting a strong barrier to its wide adoption.</p>
</blockquote>



<p>Google closes on:&nbsp;&nbsp;</p>



<blockquote>
<p>APG transforms any TWS ANC headphones into smart sensing headphones with a simple software upgrade, and works robustly across various user activities. The sensing carrier signal is completely inaudible and not impacted by music playing. More importantly, APG represents new knowledge in biomedical and mobile research and unlocks new possibilities for low-cost health sensing.</p>
</blockquote>



<p>“APG is the result of collaboration across Google Health, product, UX and legal teams,” so this coming to <a href="https://9to5google.com/guides/google-pixel-buds-pro/">Pixel Buds</a> is far from guaranteed at this point.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMMqA-Qow-c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Google to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing for Professional and Technical Audiences (133 pts)]]></title>
            <link>https://pnewman.org/writing.html</link>
            <guid>38045262</guid>
            <pubDate>Fri, 27 Oct 2023 23:26:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pnewman.org/writing.html">https://pnewman.org/writing.html</a>, See on <a href="https://news.ycombinator.com/item?id=38045262">Hacker News</a></p>
<div id="readability-page-1" class="page">


		<p>by Patrick Newman (<a href="mailto:patorick002@gmail.com">patorick002@gmail.com</a>) and Azeem Jiva. Originally written in 2022, revised 2023-October.</p>


		<hr>


		<p>A major part of my job at large technology companies has been reading, writing, and editing documents explaining various issues and what can be done to address them. I think it's a more important skill than people generally realize, as it may be the one way to reach someone in a different time zone, or who couldn't make a meeting, or who you will never speak with in person. Effectively written documents also have a way of hanging around, so investing in them can pay back over the long term.</p>

		<p>I've also learned through experience that there is value in good writing in ways that aren't always obvious. A well-articulated error message in a log file or api response can save an engineer significant time, for example.</p>

		<p>Over the years I've gathered a set of guidelines that have worked for me. I've made it a habit of sharing this type of stuff internally at the companies I work at, but none of this is specific to any company, so I'd like to share publicly this time.</p>

		<p>This list benefited from feedback and revisions provided by professional colleagues of mine. I won't name them here, but thank you to those of you who read drafts of this and offered feedback.</p>

		<h2>Know Your Audience</h2>

		<p>Know who you want to reach, what you need them to know, what context they have/don't have, and what you want them to do with the information you will convey. Make it as easy as possible for them to get what you need from your communication. Be clear if there is a call to action or follow up you need from them, and by when.</p>

		<h2>Headings</h2>

		<ul>
			<li>Titles: Make your doc title meaningful and discoverable in search.
			</li><li>Email Subjects: Make the subject scannable so the reader knows whether they need to read and if there's an action that they need to take.
			</li><li>Subheadings: use them to make your long document or email more easily scannable.</li>
			<li>The subheading of a paragraph should summarize what the paragraph is about.</li>
			<li>Write the names of the authors, contributors, and reviewers at the top of the document. Most templates have this. Do it for everything.</li>
			<li>Include the date the document was started. I prefer the format YYYY-MM-DD because it is the international standard, and avoids ambiguity between the American and British formats. Spell out the month for extra clarity.</li>
		</ul>

		<h2>Introductions</h2>
		<ul>
			<li>If there is a specific proposal in the document, put it right at the top. This gives the audience the option to decide whether to read the whole document or not.</li>
			<ul><li>Example: "This document proposes developing a Twitter client for the Atari 2600 platform, funded with four software engineering headcount and a target release date of June 1983."</li></ul>
			<li> Set a clear expectation for the audience up front. Sometimes I write a "How to read this document" section to clarify what to expect.</li>
			<ul><li> Example: "this is a collection of notes on an early stage product, no major decisions are being proposed yet."</li></ul>
			<li> As a rule of thumb, I normally write my introductions last, after I have written the rest of the document. This is because I don't necessarily know what I will write in advance.</li>
		</ul>

		<h2>Content</h2>


		<p>This set of suggestions is primarily relevant to longer form documents, proposals, and emails.</p>
		<ul>
		<li> Avoid making assumptions about what your audience knows. If assumptions are unavoidable, make it clear what the assumed knowledge is.
		</li><li> Define key terms that may be unfamiliar to the audience. Example: "Atari 2600 is a video game console that was popular in America during the early 1980s."
		</li><li> Be as specific as possible. Words such as many, almost, nearly, significantly, better, worse, etc imply different things to different people. Being specific takes away this ambiguity. 93% of bugs, increase of 7% performance, etc. The specifics also show that you are speaking from a place of knowledge instead of filling in content.

			</li><li> Follow the <a href="https://twitter.com/david_perell/status/1208930197702995968">Amazon rules</a>, particularly:
			<ul>
				<li>Replace adjectives with data</li>
				<li>Avoid "weasel words" - words that are ambiguous or misleading. Examples, taken from Wikipedia, including</li>
				<ul>
					<li>"People are saying..." (Which people? How do they know?)</li>
					<li> "It has been claimed that..." (By whom, where, when?)</li>
					<li>"Critics claim..." (Which critics?)</li>
					<li>"Questions have been raised..." (Implies a fatal flaw has been discovered)</li>
				</ul>
			</ul>
		</li><li> Avoid inside jokes and pop cultural references in the core points of the document. These might increase engagement for some, but exclude others who aren't in on the joke.
		</li><li>Read up on inclusive language and make sure you aren't including outdated terms.
		</li><li>Put reference links in an appendix section.
		</li><li> Define acronyms, abbreviations and other key terms that reader may be unfamiliar with at the beginning of your document
		</li><li> Add visuals if possible - charts, graphs, architectural diagrams, etc.
		</li><li> Avoid condescending words (obviously, of course, clearly)
		</li></ul>

<h2>Editing</h2>

<ul>
	<li>Edit your documents for clarity and correctness. Take a break between writing and editing.</li>
<li> Have a few trusted early draft reviewers, who can give you feedback before you share widely.</li>
<li> I try to focus on removing unnecessary detail and content when I edit my writing, to amplify the most important points.</li>
<li> Remove redundancy. Are you saying the same thing 3 different ways?</li>
</ul>

<h2>Reading</h2>
<ul>
<li> Make time to read others' documents.</li>
<li> Share interesting things you read with others.</li>
<li> Don't be afraid to give positive feedback.</li>
</ul>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists simulate backward time travel using quantum entanglement (109 pts)]]></title>
            <link>https://thedebrief.org/scientists-successfully-simulate-backward-time-travel-with-a-25-chance-of-actually-changing-the-past/</link>
            <guid>38045112</guid>
            <pubDate>Fri, 27 Oct 2023 23:03:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thedebrief.org/scientists-successfully-simulate-backward-time-travel-with-a-25-chance-of-actually-changing-the-past/">https://thedebrief.org/scientists-successfully-simulate-backward-time-travel-with-a-25-chance-of-actually-changing-the-past/</a>, See on <a href="https://news.ycombinator.com/item?id=38045112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
																										<p>Scientists trying to take advantage of the unusual properties of the quantum realm say they have successfully simulated a method of backward time travel that allowed them to change an event after the fact one out of four times. The Cambridge University team is quick to caution that they have not built a time machine, per se, but also note how their process doesn’t violate physics while changing past events after they have happened.</p>
<p>“Imagine that you want to send a gift to someone: you need to send it on day one to make sure it arrives on day three,” <a href="https://www.cam.ac.uk/research/news/simulations-of-backwards-time-travel-can-improve-scientific-experiments">explained</a> lead author David Arvidsson-Shukur from the Cambridge Hitachi Laboratory. “However, you only receive that person’s wish list on day two.”</p>
<p>To respect the gift recipient’s timeline, you would need to send it on day one. But, as Arvidsson-Shukur notes, you won’t know what gift to send until later, meaning your gift will either be late or be wrong.</p>
<p>“Now imagine you can change what you send on day one with the information from the wish list received on day two,” he adds. It is exactly this phenomenon that the researchers say can happen in the right scenario.</p>
<p>“Our simulation uses quantum entanglement manipulation to show how you could retroactively change your previous actions to ensure the final outcome is the one you want.”</p>
<h2><strong>Using Quantum Entanglement to Change Your Gift to the Right One After it was Sent</strong></h2>
<p>Quantum entanglement is a process where certain fundamental aspects of quantum particles are shared by two or more particles, and changing those properties in any single particle causes the same change in the others. In their simulations, which were <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.131.150202">published</a> in the journal <em>Physical Review Letters</em>, the Cambridge research team simulated the entanglement of two particles. One of the particles is then sent off to be used in an experiment.</p>
<p>After the experiment’s completion, they gained new information, which would have caused them to act differently. In this situation, “the experimentalist manipulates the second particle to effectively alter the first particle’s past state, changing the outcome of the experiment.” explained the study’s co-author Nicole Yunger Halpern, a researcher at the National Institute of Standards and Technology (NIST) and the University of Maryland.</p>
<p>In effect, by changing the remaining particle, the researchers changed the past. At least in their simulations. It’s an effect the researchers described as “remarkable.” However, they say there is a catch. The experiment only changes the past with the new information about 25% of the time.</p>
<p>“In other words, the simulation has a 75% chance of failure,” says Arvidsson-Shukur. “If we stay with our gift analogy, one out of four times, the gift will be the desired one (for example, a pair of trousers), another time it will be a pair of trousers but in the wrong size, or the wrong colour, or it will be a jacket.”</p>
<p>Fortunately, in their simulations, they at least know when they have failed, which can still allow the researcher to rework the system to effectively achieve backward time travel and get the result that they wanted. To achieve this seemingly impossible goal, they propose using a filter that will allow their theoretical experimenter to send a number of solutions and then simply filter out the 75% that they didn’t want.</p>
<p>“Let’s say sending gifts is inexpensive, and we can send numerous parcels on day one,” said co-author Aidan McConnell, Ph.D., who carried out this research during his master’s degree at the Cavendish Laboratory in Cambridge. “On day two, we know which gift we should have sent. By the time the parcels arrive on day three, one out of every four gifts will be correct, and we select these by telling the recipient which deliveries to throw away.”</p>
<h2><strong>Not a Time Machine, But a Backward Time Travel System?</strong></h2>
<p>The researchers made sure to point out that these are just simulations, albeit successful ones programmed in the known behaviors of entangled particles. So even though it effectively proved a way to change the results of an experiment in the past, 25% of the time at least, its ability to achieve a form of backward time travel still shouldn’t necessarily be compared to a certain Flux Capacitor-equipped DeLorean built by a certain Doc Emmet Brown.</p><div id="block-wrap-46874" data-id="46874">		<article>
					<p><a href="https://thedebrief.org/this-futuristic-bandage-can-heal-wounds-by-harvesting-electricity-out-of-thin-air/">
				<img width="120" height="120" src="https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-120x120.jpg" alt="bandage harvesting electricity" decoding="async" srcset="https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-120x120.jpg 120w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-150x150.jpg 150w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-70x70.jpg 70w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-240x240.jpg 240w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-360x360.jpg 360w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-540x540.jpg 540w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-720x720.jpg 720w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-125x125.jpg 125w" sizes="(max-width: 120px) 100vw, 120px" data-srcset="https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-120x120.jpg 120w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-150x150.jpg 150w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-70x70.jpg 70w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-240x240.jpg 240w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-360x360.jpg 360w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-540x540.jpg 540w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-720x720.jpg 720w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-125x125.jpg 125w" data-src="https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-120x120.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">			</a>
		</p>
					
		</article>
		</div>
<p>“We are not proposing a time travel machine, but rather a deep dive into the fundamentals of quantum mechanics,” said Arvidsson-Shukur. “These simulations do not allow you to go back and alter your past, but they do allow you to create a better tomorrow by fixing yesterday’s problems today.”</p>
<p>The researcher also notes that the failure rate of their backward time travel system is somewhat reassuring, especially for physicists who depend on Einstein’s theories of relativity and all of the science built upon those theories.</p>
<p>“That we need to use a filter to make our experiment work is actually pretty reassuring,” he said. “The world would be very strange if our time-travel simulation worked every time. Relativity and all the theories that we are building our understanding of our universe on would be out of the window.”</p>
<p><strong><em>Christopher Plain is a Science Fiction and Fantasy novelist and Head Science Writer at The Debrief. Follow and connect with him on <a href="https://twitter.com/plain_fiction" target="_blank" rel="noopener">X</a>, learn about his books at <u><a href="https://plainfiction.com/">plainfiction.com</a></u>, or email him directly at <u><a href="mailto:christopher@thedebrief.org">christopher@thedebrief.org</a></u>.</em></strong></p>
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Algebra (224 pts)]]></title>
            <link>https://www.feynmanlectures.caltech.edu/I_22.html</link>
            <guid>38044997</guid>
            <pubDate>Fri, 27 Oct 2023 22:48:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.feynmanlectures.caltech.edu/I_22.html">https://www.feynmanlectures.caltech.edu/I_22.html</a>, See on <a href="https://news.ycombinator.com/item?id=38044997">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Ch22">
<h2>
<span>22<a href="https://www.feynmanlectures.caltech.edu/flpphotos.html#23" target="_blank" title="View original lecture photos"><img src="https://www.feynmanlectures.caltech.edu/img/camera.svg"></a></span>Algebra
<p><a href="http://www.feynman.com/the-animated-feynman-lectures/" target="_blank">[Special message</a> from Ralph Leighton, with audio from the lecture.]
</p>
</h2>

<p><img src="https://www.feynmanlectures.caltech.edu/img/FLP_I/f22-00/f22-00.jpg">
</p>



<div id="Ch22-S1">
<h3>
<span>22–1</span>Addition and multiplication</h3>
<p>In our study of oscillating systems we shall have occasion
to use one of the most remarkable, almost astounding, formulas in all of
mathematics. From the physicist’s point of view we could bring forth
this formula in two minutes or so, and be done with it. But science is
as much for intellectual enjoyment as for practical utility, so instead
of just spending a few minutes on this amazing jewel, we shall surround
the jewel by its proper setting in the grand design of that branch of
mathematics which is called elementary algebra.</p>
<p>Now you may ask, “What is mathematics doing in a physics lecture?”
We have several possible excuses: first, of course, mathematics is an
important tool, but that would only excuse us for giving the formula
in two minutes. On the other hand, in theoretical physics we discover
that all our laws can be written in mathematical form; and that this
has a certain simplicity and beauty about it. So, ultimately, in order
to understand nature it may be necessary to have a deeper
understanding of mathematical relationships. But the real reason is
that the subject is enjoyable, and although we humans cut nature up in
different ways, and we have different courses in different
departments, such compartmentalization is really artificial, and we
should take our intellectual pleasures where we find them.</p>
<p>Another reason for looking more carefully at algebra now, even though
most of us studied algebra in high school, is that that was the first
time we studied it; all the equations were unfamiliar, and it was hard
work, just as physics is now. Every so often it is a great pleasure to
look back to see what territory has been covered, and what the great
map or plan of the whole thing is. Perhaps some day somebody in the
Mathematics Department will present a lecture on mechanics in such a
way as to show what it was we were trying to learn in the physics
course!</p>
<p>The subject of algebra will not be developed from the point of view of
a mathematician, exactly, because the mathematicians are mainly
interested in how various mathematical facts are demonstrated, and how
many assumptions are absolutely required, and what is not
required. They are not so interested in the result of what they
prove. For example, we may find the Pythagorean theorem quite
interesting, that the sum of the squares of the sides of a right
triangle is equal to the square of the hypotenuse; that is an
interesting fact, a curiously simple thing, which may be appreciated
without discussing the question of how to prove it, or what axioms are
required. So, in the same spirit, we shall describe qualitatively, if
we may put it that way, the system of elementary algebra. We say
<em>elementary</em> algebra because there is a branch of mathematics
called <em>modern</em> algebra in which some of the rules such as $ab =
ba$, are abandoned, and it is still called algebra, but we shall not
discuss that.</p>
<p>To discuss this subject we start in the middle. We suppose that we
already know what integers are, what zero is, and what it means to
increase a number by one unit. You may say, “That is not in the
middle!” But it is the middle from a mathematical standpoint, because
we could go even further back and describe the theory of sets in order
to <em>derive</em> some of these properties of integers. But we are not
going in that direction, the direction of mathematical philosophy and
mathematical logic, but rather in the other direction, from the
assumption that we know what integers are and we know how to count.</p>
<p>If we start with a certain number&nbsp;$a$, an integer, and we count
successively one unit $b$&nbsp;times, the number we arrive at we call
$a+b$, and that defines <em>addition</em> of integers.</p>
<p>Once we have defined addition, then we can consider this: if we start
with nothing and add $a$ to it, $b$&nbsp;times in succession, we call the
result <em>multiplication</em> of integers; we call it $b$ <em>times</em>&nbsp;$a$.</p>
<p>Now we can also have a <em>succession of multiplications:</em> if we
start with $1$ and multiply by&nbsp;$a$, $b$&nbsp;times in succession, we call
that <em>raising to a power:</em>&nbsp;$a^b$.</p>
<p>Now as a consequence of these definitions it can be easily shown that
all of the following relationships are true:
<span>
\begin{equation}
\begin{alignedat}{4}
&amp;(\text{a})&amp;\quad
&amp;a+b=b+a&amp;\quad\quad
&amp;(\text{b})&amp;\quad
&amp;a+(b+c)=(a+b)+c\\
&amp;(\text{c})&amp;\quad
&amp;ab=ba&amp;\quad\quad
&amp;(\text{d})&amp;\quad
&amp;a(b+c)=ab+ac\\
&amp;(\text{e})&amp;\quad
&amp;(ab)c=a(bc)&amp;\quad\quad
&amp;(\text{f})&amp;\quad
&amp;(ab)^c=a^cb^c\\
&amp;(\text{g})&amp;\quad
&amp;a^ba^c=a^{(b+c)}&amp;\quad\quad
&amp;(\text{h})&amp;\quad
&amp;(a^b)^c=a^{(bc)}\\
&amp;(\text{i})&amp;\quad
&amp;a+0=a&amp;\quad\quad
&amp;(\text{j})&amp;\quad
&amp;a\cdot 1=a\\
&amp;(\text{k})&amp;\quad
&amp;a^1=a
\end{alignedat}
\label{Eq:I:22:1}
\end{equation}
</span>
<span>
\begin{equation}
\begin{alignedat}{4}
&amp;(\text{a})&amp;\quad&amp;a+b=b+a\\
&amp;(\text{b})&amp;\quad&amp;a+(b+c)=(a+b)+c\\
&amp;(\text{c})&amp;\quad&amp;ab=ba\\
&amp;(\text{d})&amp;\quad&amp;a(b+c)=ab+ac\\
&amp;(\text{e})&amp;\quad&amp;(ab)c=a(bc)\\
&amp;(\text{f})&amp;\quad&amp;(ab)^c=a^cb^c\\
&amp;(\text{g})&amp;\quad&amp;a^ba^c=a^{(b+c)}\\
&amp;(\text{h})&amp;\quad&amp;(a^b)^c=a^{(bc)}\\
&amp;(\text{i})&amp;\quad&amp;a+0=a\\
&amp;(\text{j})&amp;\quad&amp;a\cdot 1=a\\
&amp;(\text{k})&amp;\quad&amp;a^1=a
\end{alignedat}
\label{Eq:I:22:1}
\end{equation}
</span>
These results are well known and we shall not belabor the point, we
merely list them. Of course, $1$ and&nbsp;$0$ have special properties; for
example, $a + 0$ is&nbsp;$a$, $a$ times $1= a$, and $a$ to the first power
is&nbsp;$a$.</p>
<p>In this discussion we must also assume a few other properties like
continuity and ordering, which are very hard to define; we will let
the rigorous theory do it. Furthermore, it is definitely true that we
have written down too many “rules”; some of them may be deducible
from the others, but we shall not worry about such matters.</p>
</div>
<div id="Ch22-S2">
<h3>
<span>22–2</span>The inverse operations</h3>
<p>In addition to the direct operations of addition, multiplication, and
raising to a power, we have also the <em>inverse</em> operations, which
are defined as follows. Let us assume that $a$ and&nbsp;$c$ are given, and
that we wish to find what values of&nbsp;$b$ satisfy such equations as $a +
b = c$, $ab = c$, $b^a = c$. If $a + b= c$, $b$ is defined as $c - a$,
which is called <em>subtraction</em>. The operation called division is
also clear: if $ab = c$, then $b = c/a$ defines division—a solution
of the equation $ab = c$ “backwards.” Now if we have a power $b^a =
c$ and we ask ourselves, “What is&nbsp;$b$?,” it is called the
$a$th&nbsp;<em>root</em> of&nbsp;$c$: $b = \sqrt[a]{c}$. For instance, if we ask
ourselves the following question, “What integer, raised to the third
power, equals&nbsp;$8$?,” then the answer is called the <em>cube root</em>
of&nbsp;$8$; it is&nbsp;$2$. Because $b^a$ and&nbsp;$a^b$ are not equal, there are
<em>two</em> inverse problems associated with powers, and the other
inverse problem would be, “To what power must we raise&nbsp;$2$ to
get&nbsp;$8$?”  This is called taking the <em>logarithm</em>. If
$a^b = c$, we
write $b = \log_ac$. The fact that it has a cumbersome notation
relative to the others does not mean that it is any less elementary,
at least applied to integers, than the other processes. Although
logarithms come late in an algebra class, in practice they
are, of
course, just as simple as roots; they are just a different kind of
solution of an algebraic equation. The direct and inverse operations
are summarized as follows:
\begin{equation}
\begin{alignedat}{5}
&amp;(\text{a})&amp;&amp;\quad
\text{addition}&amp;&amp;\quad
&amp;&amp;(\text{a}')&amp;&amp;\quad
\text{subtraction}\\
&amp; &amp;&amp;\quad a+b=c&amp;&amp;\quad
&amp;&amp; &amp;&amp;\quad b=c-a\\
&amp;(\text{b})&amp;&amp;\quad
\text{multiplication}&amp;&amp;\quad
&amp;&amp;(\text{b}')&amp;&amp;\quad
\text{division}\\
&amp; &amp;&amp;\quad ab=c&amp;&amp;\quad
&amp;&amp; &amp;&amp;\quad b=c/a\\
&amp;(\text{c})&amp;&amp;\quad
\text{power}&amp;&amp;\quad
&amp;&amp;(\text{c}')&amp;&amp;\quad
\text{root}\\
&amp; &amp;&amp;\quad b^a=c&amp;&amp;\quad
&amp;&amp; &amp;&amp;\quad b=\sqrt[a]{c}\\
&amp;(\text{d})&amp;&amp;\quad
\text{power}&amp;&amp;\quad
&amp;&amp;(\text{d}')&amp;&amp;\quad
\text{logarithm}\\
&amp; &amp;&amp;\quad a^b=c&amp;&amp;\quad
&amp;&amp; &amp;&amp;\quad b=\log_ac\\
\end{alignedat}
\label{Eq:I:22:2}
\end{equation}
</p>
<p>Now here is the idea. These relationships, or rules, are correct for
integers, since they follow from the definitions of addition,
multiplication, and raising to a power. <em>We are going to discuss
whether or not we can broaden the class of objects which $a$,&nbsp;$b$,
and&nbsp;$c$ represent so that they will obey these same rules</em>, although the
processes for $a + b$, and so on, will not be definable in terms of
the direct action of adding&nbsp;$1$, for instance, or successive
multiplications by integers.</p>
</div>
<div id="Ch22-S3">
<h3>
<span>22–3</span>Abstraction and generalization</h3>
<p>When we try to solve simple algebraic equations using all these
definitions, we soon discover some insoluble problems, such as the
following. Suppose that we try to solve the equation $b = 3 - 5$. That
means, according to our definition of subtraction, that we must find a
number which, when added to&nbsp;$5$, gives&nbsp;$3$. And of course there
<em>is</em> no such number, because we consider only positive integers;
this is an insoluble problem. However, the plan, the great idea, is
this: <em>abstraction and generalization</em>. From the whole structure
of algebra, rules plus integers, we abstract the original definitions
of addition and multiplication, but we leave the rules (<a href="#mjx-eqn-EqI221">22.1</a>)
and&nbsp;(<a href="#mjx-eqn-EqI222">22.2</a>), and assume these to be true <em>in general</em> on
a wider class of numbers, even though they are originally derived on a
smaller class. Thus, rather than using integers symbolically to define
the rules, we use the rules as the definition of the symbols, which then
represent a more general kind of number. As an example, by working with
the rules alone we can show that $3 - 5 = 0 - 2$. In fact we can show
that one can make <em>all</em> subtractions, provided we define a whole
set of new numbers: $0 - 1$, $0 - 2$, $0 - 3$, $0 - 4$, and so on,
called the <em>negative integers</em>. Then we may use all the other
rules, like $a(b + c) = ab + ac$ and so forth, to find what the rules
are for multiplying negative numbers, and we will discover, in fact,
that all of the rules can be maintained with negative as well as
positive integers.</p>
<p>So we have increased the range of objects over which the rules work,
but the meaning of the symbols is different.</p>
<p>One cannot say, for instance, that $-2$ times&nbsp;$5$ really means to
add&nbsp;$5$ together successively $-2$&nbsp;times. That means nothing. But
nevertheless everything will work out all right according to the
rules.</p>
<p>An interesting problem comes up in taking powers. Suppose that we wish
to discover what $a^{(3-5)}$ means. We know only that $3 - 5$ is a
solution of the problem, $(3 - 5) + 5 = 3$. Knowing that, we know that
$a^{(3-5)}a^5 = a^3$. Therefore $a^{(3-5)} = a^3/a^5$, by the
definition of division. With a little more work, this can be reduced
to&nbsp;$1/a^2$. So we find that the negative powers are the reciprocals of
the positive powers, but $1/a^2$ is a meaningless symbol, because if
$a$ is a positive or negative integer, the square of it can be greater
than&nbsp;$1$, and we do not yet know what we mean by&nbsp;$1$ divided by a
number greater than&nbsp;$1$!</p>
<p>Onward! The great plan is to continue the process of generalization;
whenever we find another problem that we cannot solve we extend our
realm of numbers. Consider division: we cannot find a number which is
an integer, even a negative integer, which is equal to the result of
dividing $3$ by&nbsp;$5$. But if we suppose that all fractional numbers
also satisfy the rules, then we can talk about multiplying and adding
fractions, and everything works as well as it did before.</p>
<p>Take another example of powers: what is&nbsp;$a^{3/5}$? We know only that
$(3/5)5 = 3$, since that was the definition of&nbsp;$3/5$. So we know also
that $(a^{(3/5)})^5 =$ $a^{(3/5)(5)}=$ $a^3$, because this is one of the
rules. Then by the definition of roots we find that $a^{(3/5)} =
\sqrt[5]{a^3}$.</p>
<p>In this way, then, we can define what we mean by putting fractions in
the various symbols, by using the rules themselves to help us
determine the definition—it is not arbitrary. It is a remarkable
fact that all the rules still work for positive and negative integers,
as well as for fractions!</p>
<p>We go on in the process of generalization. Are there any other
equations we cannot solve? Yes, there are. For example, it is
impossible to solve this equation: $b =$ $2^{1/2} =$ $\sqrt{2}$. It is
impossible to find a number which is rational (a fraction) whose
square is equal to&nbsp;$2$. It is very easy for us in modern days to
answer this question. We know the decimal system, and so we have no
difficulty in appreciating the meaning of an unending decimal as a
type of approximation to the square root of&nbsp;$2$. Historically, this
idea presented great difficulty to the Greeks. To really define
<em>precisely</em> what is meant here requires that we add some
substance of continuity and ordering, and it is, in fact, quite the
most difficult step in the processes of generalization just at this
point. It was made, formally and rigorously, by
Dedekind. However, without
worrying about the mathematical rigor of the thing, it is quite easy to
understand that what we mean is that we are going to find a whole
sequence of approximate fractions, perfect fractions (because any
decimal, when stopped somewhere, is of course rational), which just
keeps on going, getting closer and closer to the desired result. That is
good enough for what we wish to discuss, and it permits us to involve
ourselves in irrational numbers, and to calculate things like the square
root of&nbsp;$2$ to any accuracy that we desire, with enough work.</p>
</div>
<div id="Ch22-S4">
<h3>
<span>22–4</span>Approximating irrational numbers</h3>
<p>The next problem comes with what happens with the irrational
powers. Suppose that we want to define, for instance,
$10^{\sqrt{2}}$. In principle, the answer is simple enough. If we
approximate the square root of&nbsp;$2$ to a certain number of decimal
places, then the power is rational, and we can take the approximate
root, using the above method, and get an <em>approximation</em>
to&nbsp;$10^{\sqrt{2}}$. Then we may run it up a few more decimal places (it
is again rational), take the appropriate root, this time a much higher
root because there is a much bigger denominator in the fraction, and
get a better approximation. Of course we are going to get some
enormously high roots involved here, and the work is quite
difficult. How can we cope with this problem?</p>
<p>In the computations of square roots, cube roots, and other small
roots, there is an arithmetical process available by which we can get
one decimal place after another. But the amount of labor needed to
calculate irrational powers and the logarithms that go with them (the
inverse problem) is so great that there is no simple arithmetical
process we can use. Therefore tables have been built up which permit
us to calculate these powers, and these are called the tables of
logarithms, or the tables of powers, depending on which way the table
is set up. It is merely a question of saving time; if we must raise
some number to an irrational power, we can look it up rather than
having to compute it. Of course, such a computation is just a
technical problem, but it is an interesting one, and of great
historical value. In the first place, not only do we have the problem
of solving $x=10^{\sqrt{2}}$, but we also have the problem of solving
$10^x = 2$, or $x = \log_{10} 2$. This is not a problem where we have
to define a new kind of number for the result, it is merely a
computational problem. The answer is simply an irrational number, an
unending decimal, not a new kind of a number.</p>
<p>Let us now discuss the problem of calculating solutions of such
equations. The general idea is really very simple. If we could
calculate $10^1$, and $10^{4/10}$, and $10^{1/100}$, and $10^{4/1000}$
and so on, and multiply them all together, we would get
$10^{1.414\dots}$ or&nbsp;$10^{\sqrt{2}}$, and that is the general idea on
which things work. But instead of calculating $10^{1/10}$ and so on,
we shall calculate $10^{1/2}$, $10^{1/4}$, and so on. Before we start,
we should explain why we make so much work with $10$, instead of some
other number. Of course, we realize that logarithm tables are of great
practical utility, quite aside from the mathematical problem of taking
roots, since with any base at all,
\begin{equation}
\label{Eq:I:22:3}
\log_b(ac)=\log_ba+\log_bc.
\end{equation}
We are all familiar with the fact that one can use this fact in a
practical way to multiply numbers if we have a table of
logarithms. The only question is, with what base&nbsp;$b$ shall we compute?
It makes no difference what base is used; we can use the same
principle all the time, and if we are using logarithms to any
particular base, we can find logarithms to any other base merely by a
change in scale, a multiplying factor. If we multiply
Eq.&nbsp;(<a href="#mjx-eqn-EqI223">22.3</a>) by&nbsp;$61$, it is just as true, and if we had a table
of logs with a base&nbsp;$b$, and somebody else multiplied all of our table
by&nbsp;$61$, there would be no essential difference. Suppose that we know
the logarithms of all the numbers to the base&nbsp;$b$. In other words, we
can solve the equation $b^a = c$ for any $c$ because we have a table.
The problem is to find the logarithm of the same number&nbsp;$c$ to some
other base, let us say the base&nbsp;$x$. We would like to solve $x^{a'} =
c$. It is easy to do, because we can always write $x = b^t$, which
defines $t$, knowing $x$ and&nbsp;$b$. As a matter of fact, $t = \log_b x$.
Then if we put that in and solve for $a'$, we see that $(b^t)^{a'} =
b^{a't} = c$. In other words, $ta'$ is the logarithm of&nbsp;$c$ in base&nbsp;$b$.
Thus $a' = a/t$. Thus logs to base&nbsp;$x$ are just $1/t$, which is a
constant, times the logs to the base, $b$. Therefore any log table is
equivalent to any other log table if we multiply by a constant, and the
constant is $1/\log_b x$. This permits us to choose a particular base,
and for convenience we take the base&nbsp;$10$. (The question may arise as to
whether there is any natural base, any base in which things are somehow
simpler, and we shall try to find an answer to that later. At the moment
we shall just use the base&nbsp;$10$.)</p>



<div id="Ch22-T1">
<p><span>Table 22–1</span>Successive Square Roots of Ten
</p>
<table>
<thead>
<tr>
    <td>Power $s$</td>
    <td>$1024\,s$</td>
    <td>$10^s$</td>
    <td>$(10^s-1)/s$</td></tr></thead>
<tbody>
<tr>
    <td>$1\phantom{/1024}$</td>
    <td>$1024$</td>
    <td>$10.00000\hphantom{00}$</td>
    <td>$9.00\hphantom{00^{000}}$</td></tr>
<tr>
    <td>$1/2\phantom{000}$</td>
    <td>$\phantom{1}512$</td>
    <td>$\phantom{1}3.16228\hphantom{00}$</td>
    <td>$4.32\hphantom{00^{000}}$</td></tr>
<tr>
    <td>$1/4\phantom{000}$</td>
    <td>$\phantom{1}256$</td>
    <td>$\phantom{1}1.77828\hphantom{00}$</td>
    <td>$3.113\hphantom{0^{000}}$</td></tr>
<tr>
    <td>$1/8\phantom{000}$</td>
    <td>$\phantom{1}128$</td>
    <td>$\phantom{1}1.33352\hphantom{00}$</td>
    <td>$2.668\hphantom{0^{000}}$</td></tr>
<tr>
    <td>$1/16\phantom{00}$</td>
    <td>$\phantom{10}64$</td>
    <td>$\phantom{1}1.15478\hphantom{00}$</td>
    <td>$2.476\hphantom{0^{000}}$</td></tr>
<tr>
    <td>$1/32\phantom{00}$</td>
    <td>$\phantom{10}32$</td>
    <td>$\phantom{1}1.074607\hphantom{0}$</td>
    <td>$2.3874\hphantom{^{000}}$</td></tr>
<tr>
    <td>$1/64\phantom{00}$</td>
    <td>$\phantom{10}16$</td>
    <td>$\phantom{1}1.036633\hphantom{0}$</td>
    <td>$2.3445\hphantom{^{000}}$</td></tr>
<tr>
    <td>$1/128\phantom{0}$</td>
    <td>$\phantom{100}8$</td>
    <td>$\phantom{1}1.018152\hphantom{0}$</td>
    <td>$2.3234^{211}$</td></tr>
<tr>
    <td>$1/256\phantom{0}$</td>
    <td>$\phantom{100}4$</td>
    <td>$\phantom{1}1.0090350$</td>
    <td>$2.3130^{104}$</td></tr>
<tr>
    <td>$1/512\phantom{0}$</td>
    <td>$\phantom{100}2$</td>
    <td>$\phantom{1}1.0045073$</td>
    <td>$2.3077^{\phantom{1}53}$</td></tr>
<tr>
    <td>$1/1024$</td>
    <td>$\phantom{100}1$</td>
    <td>$\phantom{1}1.0022511$</td>
    <td>$2.3051^{\phantom{1}26}$</td></tr>
<tr>
    <td></td>
    <td></td>
    <td></td>
    <td>$\phantom{00}\Big\downarrow\hspace 3ex^{26}$</td></tr>
<tr>
    <td>$\Delta/1024$</td>
    <td>$\phantom{102}\Delta$</td>
    <td>$1+0.0022486\Delta\overleftarrow{\kern 1.5em}$</td>
    <td>$\raise.5ex\overline{\kern 1em}2.3025$</td></tr>
<tr>
    <td>$(\Delta\to 0)$</td>
    <td></td>
    <td></td>
    <td></td></tr></tbody>
</table>
</div>

<p>Now let us see how to calculate logarithms. We begin by computing
successive square roots of&nbsp;$10$, by cut and try. The results are shown
in Table&nbsp;<a href="#Ch22-T1">22–1</a>. The powers of&nbsp;$10$ are given in the first
column, and the result, $10^s$, is given in the third column. Thus
$10^1 = 10$. The one-half power of&nbsp;$10$ we can easily work out,
because that is the square root of&nbsp;$10$, and there is a known, simple
process for taking square roots of any number.<a id="footnote_source_1" href="#footnote_1"><sup>1</sup></a> Using this process, we find
the first square root to be&nbsp;$3.16228$. What good is that? It already
tells us something, it tells us how to take $10^{0.5}$, so we now know
at least <em>one</em> logarithm, if we happen to need the logarithm
of&nbsp;$3.16228$, we know the answer is close to&nbsp;$0.50000$. But we must do a
little bit better than that; we clearly need more information. So we
take the square root again, and find $10^{1/4}$, which
is&nbsp;$1.77828$. Now we have the logarithm of more numbers than we had
before, $1.250$ is the logarithm of&nbsp;$17.78$ and, incidentally, if it
happens that somebody asks for $10^{0.75}$, we can get it, because
that is $10^{(0.5+0.25)}$; it is therefore the product of the second
and third numbers. If we can get enough numbers in column&nbsp;$s$ to be
able to make up almost any number, then by multiplying the proper
things in column&nbsp;3, we can get $10$ to any power; that is the plan. So
we evaluate ten successive square roots of&nbsp;$10$, and that is the main
work which is involved in the calculations.</p>
<p>Why don’t we keep on going for more and more accuracy? Because we
begin to notice something. When we raise $10$ to a very small power,
we get $1$ plus a small amount. The reason for this is clear, because
we are going to have to take the $1000$th&nbsp;power of&nbsp;$10^{1/1000}$ to
get back to&nbsp;$10$, so we had better not start with too big a number; it
has to be close to&nbsp;$1$. What we notice is that the small numbers that
are added to&nbsp;$1$ begin to look as though we are merely dividing by&nbsp;$2$
each time; we see $1815$ becomes $903$, then $450$, $225$; so it is
clear that, to an excellent approximation, if we take another root, we
shall get $1.00112$ something, and rather than actually <em>take</em>
all the square roots, we <em>guess</em> at the ultimate limit. When we
take a small fraction&nbsp;$\Delta/1024$ as $\Delta$ approaches zero,
what will the answer be? Of course it will be some number close
to&nbsp;$1+0.0022511\,\Delta$. Not exactly $1+0.0022511\,\Delta$, however—we
can get a better value by the following trick: we subtract the $1$,
and then divide by the power&nbsp;$s$. This ought to correct all the
excesses to the same value. We see that they are very closely
equal. At the top of the table they are not equal, but as they come
down, they get closer and closer to a constant value. What is the
value? Again we look to see how the series is going, how it has
changed with&nbsp;$s$. It changed by $211$, by $104$, by $53$, by
$26$. These changes are obviously half of each other, very closely, as
we go down. Therefore, if we kept going, the changes would be
$13$,&nbsp;$7$, $3$, $2$ and&nbsp;$1$, more or less, or a total of&nbsp;$26$. Thus we have
only&nbsp;$26$ more to go, and so we find that the true number
is&nbsp;$2.3025$. (Actually, we shall later see that the <em>exact</em> number
should be&nbsp;$2.3026$, but to keep it realistic, we shall not alter
anything in the arithmetic.) From this table we can now calculate any
power of&nbsp;$10$, by compounding the power out of&nbsp;$1024$ths.</p>
<p>Let us now actually calculate a logarithm, because the process we
shall use is where logarithm tables actually come from. The procedure
is shown in Table&nbsp;<a href="#Ch22-T2">22–2</a>, and the numerical values are shown
in Table&nbsp;<a href="#Ch22-T1">22–1</a> (columns 2 and 3).</p>



<div id="Ch22-T2">
<p><span>Table 22–2</span>Calculation of a logarithm: $\boldsymbol{\log_{10} 2}$
</p>
<table>
<tbody>
<tr>
    <td></td>
    <td>$2 \div 1.77828 = 1.124682$</td></tr>
<tr>
    <td></td>
    <td>$1.124682 \div 1.074607 = 1.046598$, etc.</td></tr>
<tr>
    <td>$\therefore\,$</td>
    <td>$2=(1.77828)(1.074607)(1.036633)(1.0090350)(1.000573)$</td></tr>
<tr>
    <td></td>
    <td>$\phantom{2}=10^{\biggl[\dfrac{1}{1024}\mbox{(256+32+16+4+0.254)}\biggr]}=10^{\biggl[\dfrac{308.254}{1024}\biggr]}$</td></tr>
<tr>
    <td></td>
    <td>$\phantom{2}=10^{0.30103}\phantom{(256+32+16+4}\biggl(\dfrac{573}{2249}=0.254\biggr)$</td></tr>
<tr>
    <td>$\therefore\,$</td>
    <td>$\log_{10}2=0.30103$</td></tr></tbody>
</table>
</div>

<p>Suppose we want the logarithm of&nbsp;$2$. That is, we want to know to what
power we must raise $10$ to get $2$. Can we raise $10$ to the
$1/2$&nbsp;power?  No; that is too big. In other words, we can see that the
answer is going to be bigger than $1/4$, and less than $1/2$. Let us
take the factor&nbsp;$10^{1/4}$ out; we divide $2$ by&nbsp;$1.778\dots$, and get
$1.124\dots$, and so on, and now we know that we have taken away
$0.250000$ from the logarithm. The number&nbsp;$1.124\dots$, is now the
number whose logarithm we need. When we are finished we shall add back
the $1/4$, or $256/1024$. Now we look in the table for the next number
just below $1.124\dots$, and that is&nbsp;$1.074607$. We therefore divide
by&nbsp;$1.074607$ and get $1.046598$. From that we discover that $2$ can
be made up of a product of numbers that are in Table&nbsp;<a href="#Ch22-T1">22–1</a>,
as follows:
<span>
\begin{equation*}
2 = (1.77828)(1.074607)(1.036633)(1.0090350)(1.000573).
\end{equation*}
</span>
<span>
\begin{gather*}
2 = (1.77828)(1.074607)(1.036633)\;\times\\
(1.0090350)(1.000573).
\end{gather*}
</span>



There was one factor&nbsp;$(1.000573)$ left over, naturally, which is
beyond the range of our table. To get the logarithm of this factor, we
use our result that $10^{\Delta/1024} \approx 1+ 2.3025
\Delta/1024$. We find $\Delta= 0.254$. Therefore our answer is&nbsp;$10$ to
the following power: $(256 + 32 + 16 + 4 + 0.254)/1024$. Adding those
together, we get $308.254/1024$. Dividing, we get $0.30103$, so we
know that the $\log_{10} 2 = 0.30103$, which happens to be right
to&nbsp;$5$ figures!</p>
<p>This is how logarithms were originally computed by Mr.&nbsp;Briggs of Halifax, in&nbsp;1620. He said, “I computed successively
$54$&nbsp;square roots of&nbsp;$10$.” We know he really computed only the first $27$,
because the rest of them can be obtained by this trick with $\Delta$. His work
involved calculating the square root of&nbsp;$10$ twenty-seven times, which is not
much more than the ten times we did; however, it was more work because he
calculated to sixteen decimal places, and then reduced his answer to fourteen
when he published it, so that there were no rounding errors. He made tables of
logarithms to fourteen decimal places by this method, which is quite tedious.
But all logarithm tables for three hundred years were borrowed from
Mr.&nbsp;Briggs’ tables by reducing the number
of decimal places. Only in modern times, with the WPA and computing machines,
have new tables been independently computed. There are much more efficient
methods of computing logarithms today, using certain series expansions.
</p>
<p>In the above process, we discovered something rather interesting, and
that is that for very small powers&nbsp;$\epsilon$ we can calculate
$10^\epsilon$ easily; we have discovered that $10^\epsilon = 1+
2.3025\epsilon$, by sheer numerical analysis. Of course this also
means that $10^{n/2.3025} = 1+ n$ if $n$ is very small. Now logarithms
to any other base are merely multiples of logarithms to the
base&nbsp;$10$. The base&nbsp;$10$ was used only because we have $10$&nbsp;fingers, and
the arithmetic of it is easy, but if we ask for a mathematically
natural base, one that has nothing to do with the number of fingers on
human beings, we might try to change our scale of logarithms in some
convenient and natural manner, and the method which people have chosen
is to redefine the logarithms by multiplying all the logarithms to the
base&nbsp;$10$ by $2.3025\dots$ This then corresponds to using some other
base, and this is called the <em>natural</em> base, or base&nbsp;$e$. Note
that $\log_e (1 + n) \approx n$, or $e^n \approx 1+ n$ as $n\to0$.</p>
<p>It is easy enough to find out what $e$ is: $e = 10^{1/2.3025\dots}$
or&nbsp;$10^{0.434310\dots}$, an irrational power. Our table of the successive
square roots of&nbsp;$10$ can be used to compute, not just logarithms, but
also $10$ to any power, so let us use it to calculate this natural
base&nbsp;$e$. For convenience we transform $0.434310\dots$ into
$444.73/1024$. Now, $444.73$ is $256 + 128 + 32 + 16 + 8 + 4 +
0.73$. Therefore $e$, since it is an exponent of a sum, will be a
product of the numbers
<span>
\begin{equation*}
(1.77828)\!(1.33352)\!(1.074607)\!(1.036633)\!(1.018152)\!
(1.009035)\!(1.001643) = 2.7184.
\end{equation*}
</span>
<span>
\begin{align*}
(1.&amp;77828)\!(1.33352)\!(1.074607)\!(1.036633)\;\times\\
&amp;(1.018152)\!(1.009035)\!(1.001643)= 2.7184.
\end{align*}
</span>
(The only problem is the last one, which is&nbsp;$0.73$, and which is not
in the table, but we know that if $\Delta$ is small enough, the answer
is&nbsp;$1 + 0.0022486\,\Delta$.) When we multiply all these together, we get
$2.7184$ (it should be&nbsp;$2.7183$, but it is good enough). The use of
such tables, then, is the way in which irrational powers and the
logarithms of irrational numbers are all calculated. That takes care
of the irrationals.</p>
</div>
<div id="Ch22-S5">
<h3>
<span>22–5</span>Complex numbers</h3>
<p>Now it turns out that after all that work we <em>still</em> cannot solve
every equation!  For instance, what is the square root of&nbsp;$-1$?
Suppose we have to find $x^2 =-1$.  The square of no rational, of no
irrational, of <em>nothing</em> that we have discovered so far, is equal
to&nbsp;$-1$. So we again have to generalize our numbers to a still wider
class. Let us suppose that a specific solution of&nbsp;$x^2 =-1$ is called
something, we shall call it&nbsp;$i$; $i$ has the property, by definition,
that its square is&nbsp;$-1$. That is about all we are going to say about
it; of course, there is more than one root of the equation $x^2
=-1$. Someone could write $i$, but another could say, “No, I prefer
$-i$. My $i$ is minus your $i$.” It <i>is</i> just as good a solution, and
since the only definition that $i$ has is that $i^2=-1$, it must be
true that any equation we can write is equally true if the sign of&nbsp;$i$
is changed everywhere. This is called taking the <em>complex
conjugate</em>. Now we are going to make up numbers by adding
successive&nbsp;$i$’s, and multiplying $i$’s by numbers, and adding other numbers,
and
so on, according to all of our rules. In this way we find that numbers
can all be written in the form $p + iq$, where $p$ and&nbsp;$q$ are what we
call <em>real</em> numbers, i.e., the numbers we have been defining up
until now. The number&nbsp;$i$ is called the <em>unit imaginary</em> number.
Any real multiple of&nbsp;$i$ is called <em>pure imaginary</em>. The most
general number, $a$, is of the form $p+iq$ and is called a
<em>complex number</em>. Things do not get any worse if, for instance,
we multiply two such numbers, let us say $(r + is)(p + iq)$. Then,
using the rules, we get
\begin{align}
(r + is)(p + iq) &amp;= rp + r(iq) + (is)p + (is)(iq)\notag\\[1ex]
&amp;= rp + i(rq) + i(sp) + (ii)(sq)\notag\\[1ex]
\label{Eq:I:22:4}
&amp;= (rp - sq) + i(rq + sp),
\end{align}
since $ii =$ $i^2 =$ $-1$. Therefore all the numbers that now belong in
the rules&nbsp;(<a href="#mjx-eqn-EqI221">22.1</a>) have this mathematical form.</p>
<p>Now you say, “This can go on forever! We have defined powers of
imaginaries and all the rest, and when we are all finished, somebody
else will come along with another equation which cannot be solved,
like $x^6 + 3x^2 =-2$. Then we have to generalize all over again!”
But it turns out that <em>with this one more invention</em>, just the
square root of&nbsp;$-1$, <em>every algebraic equation can be solved!</em>
This is a fantastic fact, which we must leave to the Mathematics
Department to prove. The proofs are very beautiful and very
interesting, but certainly not self-evident. In fact, the most obvious
supposition is that we are going to have to invent again and again and
again. But the greatest miracle of all is that we do not. This is the
last invention. After this invention of complex numbers, we find that
the rules still work with complex numbers, and we are finished
inventing new things. We can find the complex power of any complex
number, we can solve any equation that is written algebraically, in
terms of a finite number of those symbols. We do not find any new
numbers. The square root of&nbsp;$i$, for instance, has a definite result,
it is not something new; and $i^i$ is something. We will discuss that
now.</p>
<p>We have already discussed multiplication, and addition is also easy;
if we add two complex numbers, $(p + iq) + (r + is)$, the answer is
$(p + r) + i(q + s)$. Now we can add and multiply complex numbers. But
the real problem, of course, is to compute <em>complex powers of
complex numbers</em>. It turns out that the problem is actually no more
difficult than computing complex powers of real numbers. So let us
concentrate now on the problem of calculating $10$ to a complex power,
not just an irrational power, but $10^{(r+is)}$. Of course, we must at
all times use our rules (<a href="#mjx-eqn-EqI221">22.1</a>) and&nbsp;(<a href="#mjx-eqn-EqI222">22.2</a>). Thus
\begin{equation}
\label{Eq:I:22:5}
10^{(r+is)}=10^r10^{is}.
\end{equation}
But $10^r$ we already know how to compute, and we can always multiply
anything by anything else; therefore the problem is to compute only
$10^{is}$. Let us call it some complex number, $x + iy$. Problem:
given $s$, find $x$, find $y$. Now if
\begin{equation*}
10^{is}=x+iy,
\end{equation*}
then the complex conjugate of this equation must also be true, so that
\begin{equation*}
10^{-is}=x-iy.
\end{equation*}
(Thus we see that we can deduce a number of things without actually
computing anything, by using our rules.) We deduce another interesting
thing by multiplying these together:
<span>
\begin{equation}
\label{Eq:I:22:6}
10^{is}10^{-is}=10^0=1=(x+iy)(x-iy)=x^2+y^2.
\end{equation}
</span>
<span>
\begin{equation}
\begin{gathered}
\label{Eq:I:22:6}
10^{is}10^{-is}=10^0=1\\
=(x+iy)(x-iy)=x^2+y^2.
\end{gathered}
\end{equation}
</span>
Thus if we find $x$, we have $y$ also.</p>
<p>Now the problem is <em>how</em> to compute $10$ to an imaginary
power. What guide is there? We may work over our rules until we can go
no further, but here is a reasonable guide: if we can compute it for
any particular $s$, we can get it for all the rest. If we know
$10^{is}$ for any one $s$ and then we want it for twice that $s$, we
can square the number, and so on. But how can we find $10^{is}$ for
even one special value of&nbsp;$s$? To do so we shall make one additional
assumption, which is not quite in the category of all the other rules,
but which leads to reasonable results and permits us to make progress:
when the power is small, we shall suppose that the “law”
$10^\epsilon = 1+ 2.3025\epsilon$ is right, as $\epsilon$ gets very
small, not only for real $\epsilon$, <em>but for complex $\epsilon$
as well</em>. Therefore, we begin with the supposition that this law is
true in general, and that tells us that $10^{is} = 1+ 2.3025\cdot is$,
for $s\to0$. So we assume that if $s$ is very small, say one part
in&nbsp;$1024$, we have a rather good approximation to&nbsp;$10^{is}$.</p>
<p>Now we make a table by which we can compute <em>all</em> the imaginary
powers of&nbsp;$10$, that is, compute $x$ and&nbsp;$y$. It is done as
follows. The first power we start with is the $1/1024$ power, which we
presume is very nearly $1+ 2.3025i/1024$. Thus we start with
\begin{equation}
\label{Eq:I:22:7}
10^{i/1024}=1.00000+0.0022486i,
\end{equation}
and if we keep multiplying the number by itself, we can get to a
higher imaginary power. In fact, we may just reverse the procedure we
used in making our logarithm table, and calculate the square, $4$th
power, $8$th power, etc., of&nbsp;(<a href="#mjx-eqn-EqI227">22.7</a>), and thus build up the
values shown in Table&nbsp;<a href="#Ch22-T3">22–3</a>. We notice an interesting
thing, that the $x$ numbers are positive at first, but then swing
negative. We shall look into that a little bit more in a moment. But
first we may be curious to find for what number&nbsp;$s$ the real part
of&nbsp;$10^{is}$ is <em>zero</em>. The $y$-value would be&nbsp;$1$, and so we would
have $10^{is} = 1i$, or $is = \log_{10} i$. As an example of how to use
this table, just as we calculated $\log_{10} 2$ before, let us now use
Table&nbsp;<a href="#Ch22-T3">22–3</a> to find $\log_{10} i$.</p>



<div id="Ch22-T3">
<p><span>Table 22–3</span>Successive Squares of $\boldsymbol{10^{i/1024} = 1 + 0.0022486i}$
</p>
<table>
<thead>
<tr>
    <td>Power $is$</td>
    <td>$1024s$</td>
    <td>$10^{is}$</td></tr></thead>
<tbody>
<tr>
    <td>$i/1024$</td>
    <td>$\phantom{000}1$</td>
    <td>$\phantom{-}1.00000 + 0.00225i$*</td></tr>
<tr>
    <td>$i/512\phantom{0}$</td>
    <td>$\phantom{000}2$</td>
    <td>$\phantom{-}1.00000 + 0.00450i$</td></tr>
<tr>
    <td>$i/256\phantom{0}$</td>
    <td>$\phantom{000}4$</td>
    <td>$\phantom{-}0.99996 + 0.00900i$</td></tr>
<tr>
    <td>$i/128\phantom{0}$</td>
    <td>$\phantom{000}8$</td>
    <td>$\phantom{-}0.99984 + 0.01800i$</td></tr>
<tr>
    <td>$i/64\phantom{00}$</td>
    <td>$\phantom{00}16$</td>
    <td>$\phantom{-}0.99936 + 0.03599i$</td></tr>
<tr>
    <td>$i/32\phantom{00}$</td>
    <td>$\phantom{00}32$</td>
    <td>$\phantom{-}0.99742 + 0.07193i$</td></tr>
<tr>
    <td>$i/16\phantom{00}$</td>
    <td>$\phantom{00}64$</td>
    <td>$\phantom{-}0.98967 + 0.14349i$</td></tr>
<tr>
    <td>$i/8\phantom{000}$</td>
    <td>$\phantom{0}128$</td>
    <td>$\phantom{-}0.95885 + 0.28402i$</td></tr>
<tr>
    <td>$i/4\phantom{000}$</td>
    <td>$\phantom{0}256$</td>
    <td>$\phantom{-}0.83872 + 0.54467i$</td></tr>
<tr>
    <td>$i/2\phantom{000}$</td>
    <td>$\phantom{0}512$</td>
    <td>$\phantom{-}0.40679 + 0.91365i$</td></tr>
<tr>
    <td>$i/1\phantom{000}$</td>
    <td>$1024$</td>
    <td>$-0.66928 + 0.74332i$</td></tr>
<tr>
    <td></td>
    <td colspan="2">* Should be $0.0022486i$</td></tr></tbody>
</table>
</div>

<p>Which of the numbers in Table&nbsp;<a href="#Ch22-T3">22–3</a> do we have to multiply
together to get a pure imaginary result? After a little trial and
error, we discover that to reduce $x$ the most, it is best to multiply
“$512$” by&nbsp;“$128$.” This gives $0.13056 + 0.99159i$. Then we
discover that we should multiply this by a number whose imaginary part
is about equal to the size of the real part we are trying to
remove. Thus we choose “$64$” whose $y$-value is&nbsp;$0.14349$, since
that is closest to&nbsp;$0.13056$. This then gives $-0.01308 +
1.00008i$. Now we have overshot, and must <em>divide</em> by $0.99996 +
0.00900i$. How do we do that? By changing the sign of&nbsp;$i$ and
multiplying by $0.99996 - 0.00900i$ (which works if $x^2 + y^2 =
1$). Continuing in this way, we find that the entire power to which
$10$ must be raised to give $i$ is $i(512 + 128 + 64 - 4 - 2 +
0.20)/1024$, or $698.20i/1024$. If we raise $10$ to that power, we can
get $i$. Therefore $\log_{10} i = 0.68184i$.</p>

</div>
<div id="Ch22-S6">
<h3>
<span>22–6</span>Imaginary exponents</h3>

<div id="Ch22-T4">
<p><span>Table 22–4</span>Successive Powers of $\boldsymbol{10^{i/8}}$
</p>
<table>
<thead>
<tr>
    <td>$p=$ $\text{power}\cdot8/i$</td>
    <td>$10^{ip/8}$</td></tr></thead>
<tbody>
<tr>
    <td>$\phantom{0}0$</td>
    <td>$\phantom{-}1.00000+0.00000i$</td></tr>
<tr>
    <td>$\phantom{0}1$</td>
    <td>$\phantom{-}0.95882+0.28402i$</td></tr>
<tr>
    <td>$\phantom{0}2$</td>
    <td>$\phantom{-}0.83867+0.54465i$</td></tr>
<tr>
    <td>$\phantom{0}3$</td>
    <td>$\phantom{-}0.64944+0.76042i$</td></tr>
<tr>
    <td>$\phantom{0}4$</td>
    <td>$\phantom{-}0.40672+0.91356i$</td></tr>
<tr>
    <td>$\phantom{0}5$</td>
    <td>$\phantom{-}0.13050+0.99146i$</td></tr>
<tr>
    <td>$\phantom{0}6$</td>
    <td>$-0.15647+0.98770i$</td></tr>
<tr>
    <td>$\phantom{0}7$</td>
    <td>$-0.43055+0.90260i$</td></tr>
<tr>
    <td>$\phantom{0}8$</td>
    <td>$-0.66917+0.74315i$</td></tr>
<tr>
    <td>$\phantom{0}9$</td>
    <td>$-0.85268+0.52249i$</td></tr>
<tr>
    <td>$10$</td>
    <td>$-0.96596+0.25880i$</td></tr>
<tr>
    <td>$11$</td>
    <td>$-0.99969-0.02620i$</td></tr>
<tr>
    <td>$12$</td>
    <td>$-0.95104-0.30905i$</td></tr>
<tr>
    <td>$14$</td>
    <td>$-0.62928-0.77717i$</td></tr>
<tr>
    <td>$16$</td>
    <td>$-0.10447-0.99453i$</td></tr>
<tr>
    <td>$18$</td>
    <td>$+0.45454-0.89098i$</td></tr>
<tr>
    <td>$20$</td>
    <td>$+0.86648-0.49967i$</td></tr>
<tr>
    <td>$22$</td>
    <td>$+0.99884+0.05287i$</td></tr>
<tr>
    <td>$24$</td>
    <td>$+0.80890+0.58836i$</td></tr></tbody>
</table>
</div>

<p>To further investigate the subject of taking complex imaginary powers,
let us look at the powers of&nbsp;$10$ taking <em>successive powers</em>, not
doubling the power each time, in order to follow Table&nbsp;<a href="#Ch22-T3">22–3</a>
further and to see what happens to those minus signs. This is shown in
Table&nbsp;<a href="#Ch22-T4">22–4</a>, in which we take $10^{i/8}$, and just keep
multiplying it. We see that $x$ decreases, passes through zero, swings
almost to&nbsp;$-1$ (if we could get in between $p = 10$ and&nbsp;$p = 11$ it
would obviously swing to&nbsp;$-1$), and swings back. The $y$-value is going
back and forth too.</p>
<div id="Ch22-F1">
<p><img data-src="img/FLP_I/f22-01/f22-01_tc_big.svgz"></p><p><span>Figure 22–1</span>
</p>
</div>
<p>In Fig.&nbsp;<a href="#Ch22-F1">22–1</a> the dots represent the numbers that appear
in Table&nbsp;<a href="#Ch22-T4">22–4</a>, and the lines are just drawn to help you
visually. So we see that the numbers $x$ and&nbsp;$y$ oscillate; $10^{is}$
<em>repeats itself</em>, it is a <em>periodic</em> thing, and as such, it
is easy enough to explain, because if a certain power is&nbsp;$i$, then the
fourth power of that would be&nbsp;$i^2$ <em>squared</em>. It would be&nbsp;$+1$
again, and therefore, since $10^{0.68i}$ is equal to&nbsp;$i$, by taking
the fourth power we discover that $10^{2.72i}$ is equal to&nbsp;$+1$.
Therefore, if we wanted $10^{3.00i}$, for instance, we could write it
as $10^{2.72i}$ times&nbsp;$10^{0.28i}$. In other words, it has a period,
it repeats. Of course, we recognize what the curves look like! They
look like the sine and cosine, and we shall call them, for a while,
the algebraic sine and algebraic cosine. However, instead of using the
base&nbsp;$10$, we shall put them into our natural base, which only changes
the horizontal scale; so we denote $2.3025s$ by&nbsp;$t$, and write
$10^{is} = e^{it}$, where $t$ is a real number. Now $e^{it} = x + iy$,
and we shall write this as the algebraic cosine of&nbsp;$t$ plus $i$&nbsp;times
the algebraic sine of&nbsp;$t$. Thus
\begin{equation}
\label{Eq:I:22:8}
e^{it}=\operatorname{\underline{\cos}}t+
i\operatorname{\underline{\sin}}t.
\end{equation}
What are the properties of $\operatorname{\underline{\cos}} t$
and&nbsp;$\operatorname{\underline{\sin}} t$? First, we know, for instance,
that $x^2 + y^2$ must be&nbsp;$1$; we have proved that before, and it is
just as true for base&nbsp;$e$ as for base&nbsp;$10$. Therefore
$\operatorname{\underline{\cos}}^2 t+
\operatorname{\underline{\sin}}^2 t= 1$. We also know that, for small
$t$, $e^{it} = 1+it$, and therefore $\operatorname{\underline{\cos}}
t$ is nearly $1$, and $\operatorname{\underline{\sin}} t$ is nearly
$t$, and so it goes, that <em>all of the various properties of these
remarkable functions</em>, which come from taking imaginary powers,
<em>are the same as the sine and cosine of trigonometry</em>.</p>
<p>Is the period the same? Let us find out. $e$ to what power is equal
to&nbsp;$i$? What is the logarithm of&nbsp;$i$ to the base&nbsp;$e$? We worked it out
before, in the base&nbsp;$10$ it was $0.68184i$, but when we change our
logarithmic scale to&nbsp;$e$, we have to multiply by&nbsp;$2.3025$, and if we
do that it comes out $1.570$. So this will be called “algebraic
$\pi/2$.”  But, we see, it differs from the regular $\pi/2$ by only
one place in the last point, and that, of course, is the result of
errors in our arithmetic! So we have created two new functions in a
purely algebraic manner, the cosine and the sine, which belong to
algebra, and only to algebra. We wake up at the end to discover the
very functions that are natural to geometry. So there is a connection,
ultimately, between algebra and geometry.</p>
<p>We summarize with this, the most remarkable formula in mathematics:
\begin{equation}
\label{Eq:I:22:9}
e^{i\theta}=\cos\theta+i\sin\theta.
\end{equation}
This is our jewel.</p>
<p>We may relate the geometry to the algebra by representing complex
numbers in a plane; the horizontal position of a point is&nbsp;$x$, the
vertical position of a point is&nbsp;$y$ (Fig.&nbsp;<a href="#Ch22-F2">22–2</a>). We
represent every complex number, $x+iy$. Then if the radial distance to
this point is called $r$ and the angle is called $\theta$, the
algebraic law is that $x+iy$ is written in the form $re^{i\theta}$,
where the geometrical relationships between $x$,&nbsp;$y$, $r$,
and&nbsp;$\theta$ are as shown. This, then, is the unification of algebra and
geometry.</p>
<div id="Ch22-F2">
<p><img data-src="img/FLP_I/f22-02/f22-02_tc_big.svgz"></p><p><span>Fig. 22–2.</span>$x + iy = re^{i\theta}$.
</p>
</div>
<p>When we began this chapter, armed only with the basic notions of
integers and counting, we had little idea of the power of the
processes of abstraction and generalization. Using the set of
algebraic “laws,” or properties of numbers, Eq.&nbsp;(<a href="#mjx-eqn-EqI221">22.1</a>),
and the definitions of inverse operations&nbsp;(<a href="#mjx-eqn-EqI222">22.2</a>), we have
been able here, ourselves, to manufacture not only numbers but useful
things like tables of logarithms, powers, and trigonometric functions
(for these are what the imaginary powers of real numbers are), all
merely by extracting ten successive square roots of ten!</p>
</div>
<ol id="footnotes"><li>
  <a id="footnote_1"></a>
  There is a
definite arithmetic procedure, but the easiest way to find the square
root of any number&nbsp;$N$ is to choose some&nbsp;$a$ fairly close, find $N/a$,
average $a' = \tfrac{1}{2}[a + (N/a)]$, and use this average&nbsp;$a'$ for
the next choice for&nbsp;$a$. The convergence is very rapid—the number of
significant figures doubles each time.
  <a href="#footnote_source_1">↩</a>
</li></ol>
</div></div>]]></description>
        </item>
    </channel>
</rss>