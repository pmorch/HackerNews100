<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 08 May 2024 20:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Development Notes from xkcd's "Machine" (141 pts)]]></title>
            <link>https://chromakode.com/post/xkcd-machine/</link>
            <guid>40300454</guid>
            <pubDate>Wed, 08 May 2024 17:09:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chromakode.com/post/xkcd-machine/">https://chromakode.com/post/xkcd-machine/</a>, See on <a href="https://news.ycombinator.com/item?id=40300454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-huxyrzvi="">   <div><astro-island uid="ERkgW" component-url="/_astro/LazyBunnyVideo.Bs0X-7iO.js" component-export="default" renderer-url="/_astro/client.DwnJltFX.js" props="{&quot;width&quot;:[0,&quot;1278&quot;],&quot;height&quot;:[0,&quot;720&quot;],&quot;url&quot;:[0,&quot;https://vz-ab48ae95-d30.b-cdn.net/b5ce29e4-edf6-4312-becc-8ca5802b4fe0&quot;],&quot;poster&quot;:[0,&quot;https://vz-ab48ae95-d30.b-cdn.net/b5ce29e4-edf6-4312-becc-8ca5802b4fe0/thumbnail_333a166d.jpg&quot;],&quot;muted&quot;:[0,true],&quot;playsinline&quot;:[0,true],&quot;autoplay&quot;:[0,true],&quot;loop&quot;:[0,true],&quot;controls&quot;:[0,true]}" ssr="" client="load" opts="{&quot;name&quot;:&quot;LazyBunnyVideo&quot;,&quot;value&quot;:true}" await-children=""><img width="1278" height="720" src="https://vz-ab48ae95-d30.b-cdn.net/b5ce29e4-edf6-4312-becc-8ca5802b4fe0/thumbnail_333a166d.jpg"><!--astro:end--></astro-island></div>
<p>On April 5th, xkcd released <strong><a href="https://xkcd.com/2916/">Machine</a></strong>, our 15th annual April Fools project.</p>
<p>It’s a game we’d been dreaming of for years: a giant <a href="https://en.wikipedia.org/wiki/Rube_Goldberg_machine">rube goldberg machine</a> builder in the style of the classic <a href="https://en.wikipedia.org/wiki/The_Incredible_Machine">Incredible Machine</a> games, made of a patchwork of machines created by individual xkcd readers.</p>
<p>This is the story of how we built Machine in 3 weeks, and what I learned along the way.</p>
<details><summary>This project had our largest group of contributors to date! Expand for full credits.</summary><ul>
<li>Randall, davean, and I created the art, backend, and frontend respectively.</li>
<li><a href="https://github.com/spyhi">Ed White</a> designed and built the moderator UI.</li>
<li><a href="https://twitter.com/uh_oh_thats_bad">Alex Garcia</a> implemented the hook, wheel, prism, and cat widgets, with contributions to the React physics integration.</li>
<li><a href="https://twitter.com/cotrone">Kevin Cotrone</a> wrote the meta machine generator which determines which inputs and outputs each tile has, and built backend infrastructure.</li>
<li><a href="http://burningcandle.io/">Conor Stokes</a> (with his daughter Ami) implemented the cushion, bumper family of widgets, and refined the physics stepper.</li>
<li><a href="https://liranuna.com/">Liran Nuna</a> implemented the boat that floats at the bottom of the comic.</li>
<li><a href="https://github.com/benley">Benjamin Staffin</a> improved the deployment pipeline and moderated submissions.</li>
<li><a href="https://manishearth.github.io/">Manish Goregaokar</a>, <a href="https://www.instagram.com/fading_interest">Patrick</a>, <a href="https://github.com/ayust">Amber</a>, and <a href="https://github.com/dyfrgi">Michael Leuchtenburg</a> moderated submissions and gave creative feedback.</li>
</ul></details>
<hr>
<h2 id="early-machinations">Early machinations</h2>
<p>It took us deep into March, turning around ideas we were <em>kinda</em> excited about, to find the one that had us all sitting bolt upright.</p>
<blockquote>
<p>”Could we make a really big tiled mechanism like the blue balls GIF? Where everyone contributes a small square?”</p>
</blockquote>
<p>This referenced a <a href="https://blueballfixed.ytmnd.com/">classic viral GIF from 2005</a> (warning: loud music), which was a collaboration <a href="https://ytmnd-fads.fandom.com/wiki/Blue_Ball_Machine">composed of tiles made by Something Awful users</a>:</p>
<p><img alt="A classic internet GIF animation of blue colored balls moving around a complicated rube goldberg machine mechanism" src="https://chromakode.com/post/xkcd-machine/blueballfixed.gif"></p>
<p>Sometimes an idea <em>feels</em> like it emerges fully-formed, but when you start talking about it, you realize there’s still a dizzying array of decisions to make. Thus ensued 5 days of brainstorming to discover each of us had slightly different core beliefs about what this comic should be:</p>
<ul>
<li>Where do the balls come from?</li>
<li>Does everyone see the same machine? What is its purpose?</li>
<li>How can players interact with it?</li>
<li>And most importantly… <em>why do they</em>?</li>
</ul>
<h2 id="learning-from-previous-attempts">Learning from previous attempts</h2>
<p>My favorite and least favorite interactive comics we’ve ever done have centered around user contributed content. My personal fave was <a href="https://xkcd.com/1350">Lorenz</a>, an <a href="https://en.wikipedia.org/wiki/Exquisite_corpse">exquisite corpse</a> where readers evolved jokes and storylines by writing in panel text. So much fun!</p>
<p><a href="https://xkcd.com/1350"><img src="https://chromakode.com/_astro/lorenz.96AGGFTM_Z2iKvi4.webp" alt="Screenshot of comic #1350, &quot;Lorenz&quot;" width="1405" height="1360" loading="lazy" decoding="async"></a></p>
<p>It doesn’t always work out how we hoped, though. Take 2020’s <a href="https://xkcd.com/2288">Collector’s Edition</a>:</p>
<p><a href="https://xkcd.com/2288"><img src="https://chromakode.com/_astro/collectors-edition-7828-4530.Bps98TwS_Z1EIMOB.webp" alt="Screenshot of comic #2288, &quot;Collector's Edition&quot;" width="1389" height="782" loading="lazy" decoding="async"></a></p>
<p>In Collector’s Edition, players found stickers scattered across the xkcd archives. They could then place each sticker once, permanently, on a global shared canvas.</p>
<p>Wouldn’t it be cool if readers could make their own comic panels together? This was the idea we started with, which got pared down to the sticker concept.</p>
<p>Unfortunately, the game design didn’t yield the desired results:</p>
<ul>
<li>
<p>The initial view for all players was the center of the map, which was initially blank. It quickly descended into chaos. Chaos became every player’s first impression of the game.</p>
</li>
<li>
<p>There was no incentive to carefully consider where to place a sticker. Players didn’t have enough agency to advance the plot through their individual action. This limited creativity to simple patterns like tiling similar stickers or forming lines.</p>
</li>
<li>
<p>We didn’t provide an overarching story or goal. The stickers you had didn’t obviously relate to the others already on the page (the fridge poetry magnets were fun, though).</p>
</li>
</ul>
<p>For a collective canvas to shine, the experience should teach you by example what’s cool to make with it. It helps to have a shared context and purpose which motivates what to create.</p>
<h2 id="designing-constraints">Designing constraints</h2>
<p>Once we knew we were building a big collaborative marble drop, we were awash with too many choices. Many early approaches seemed like unsatisfying trade-offs, or very difficult to implement. The only thing we were really sure of was there would be a grid of interconnected machines players would create.</p>
<p>How big should the overall machine be? Let’s consider 100x100, arbitrarily. How would we simulate it? Running 10,000 tiles in realtime on the client, each with tens of balls, seemed like a risky goal.</p>
<p>Also, how could players create subdivisions of a large, complex machine without communicating directly? How would we know tiles designed in isolation would work when integrated together?</p>
<p>Many thought experiments later, we ended up with <strong>3 core principles</strong>:</p>
<h3 id="1-maximize-player-expressiveness-at-the-cost-of-correctness">1. Maximize player expressiveness at the cost of correctness.</h3>
<p>How predictable did the machine need to be? We considered running the whole thing server side. Another option was to simulate individual machine tiles to validate them. This would give us some assurance that when everything was connected, the machine would work.</p>
<p>Perhaps if the machines were deterministic enough, we could also estimate the rate balls exited each tile. We could use that to approximate the overall flow of the machine, so we could feed tiles balls at the proper rate without running every tile.</p>
<p>Once we had a prototype editor running, Davean quickly dispelled this idea by creating a machine with long patterns of chaotic ball collisions:</p>
<div><astro-island uid="1zt2Hi" component-url="/_astro/LazyBunnyVideo.Bs0X-7iO.js" component-export="default" renderer-url="/_astro/client.DwnJltFX.js" props="{&quot;width&quot;:[0,1264],&quot;height&quot;:[0,1196],&quot;url&quot;:[0,&quot;https://vz-ab48ae95-d30.b-cdn.net/293f6bc1-8d78-47b9-8853-191a2a28f075&quot;],&quot;muted&quot;:[0,true],&quot;playsinline&quot;:[0,true],&quot;autoplay&quot;:[0,true],&quot;loop&quot;:[0,true],&quot;controls&quot;:[0,true]}" ssr="" client="idle" opts="{&quot;name&quot;:&quot;LazyBunnyVideo&quot;,&quot;value&quot;:true}" await-children=""><img width="1264" height="1196" src="https://vz-ab48ae95-d30.b-cdn.net/293f6bc1-8d78-47b9-8853-191a2a28f075/thumbnail.jpg"><!--astro:end--></astro-island></div>
<p>Unless balls moved in straight uninterrupted paths, clearly it was easy for players to make very unpredictable machines. Randall wryly suggested we add <a href="https://en.wikipedia.org/wiki/Double_pendulum">double pendulums</a>.</p>
<p>From a design standpoint, this settled that making the machines more predictable would trade against degrees of freedom players had. Also, in the face of a tight deadline, it’s best to keep it simple, which favored an approach light on prediction or simulation.</p>
<p>We decided to prioritize players having tons of flexibility in what they could build — even extremely nondeterministic or broken machines. This meant we’d need active moderation, both to verify that machines satisfied the constraints, and to remove any offensive content.</p>
<h3 id="2-give-players-firm-constraints-that-encourage-resilient-interchangeable-machines">2. Give players firm constraints that encourage resilient, interchangeable machines.</h3>
<p>Accepting moderation and unpredictable player machines made another useful decision for us: ironically, it forced us to require more order between the machines.</p>
<p>Early on, we’d considered making the inputs and outputs of machines totally free-form: where previous tiles output balls on their edges, future players would build outwards incrementally. Then we looked at how moderation would work. There was the possibility that we’d need to replace a tile from early on.</p>
<p>If tile designs depended on previous ones, this could break a large portion of the machine. This led us to design tight enough constraints that multiple players would create compatible designs within the same tile space.</p>
<div><astro-island uid="Zwy1TA" component-url="/_astro/LazyBunnyVideo.Bs0X-7iO.js" component-export="default" renderer-url="/_astro/client.DwnJltFX.js" props="{&quot;width&quot;:[0,1268],&quot;height&quot;:[0,808],&quot;url&quot;:[0,&quot;https://vz-ab48ae95-d30.b-cdn.net/d619a23e-093b-47d4-aef8-e8dac0c65f55&quot;],&quot;muted&quot;:[0,true],&quot;playsinline&quot;:[0,true],&quot;autoplay&quot;:[0,true],&quot;loop&quot;:[0,true],&quot;controls&quot;:[0,true]}" ssr="" client="idle" opts="{&quot;name&quot;:&quot;LazyBunnyVideo&quot;,&quot;value&quot;:true}" await-children=""><img width="1268" height="808" src="https://vz-ab48ae95-d30.b-cdn.net/d619a23e-093b-47d4-aef8-e8dac0c65f55/thumbnail.jpg"><!--astro:end--></astro-island></div>
<p>This is the <a href="https://en.wikipedia.org/wiki/Robustness_principle">Robustness principle</a> in action: “be conservative in what you send, be liberal in what you accept”.</p>
<p>To provide players with input and output constraints, we’d need a map of the whole machine from the start. Generating the map also gave us the opportunity to vary how challenging the machines would be (we called the tile configurations “puzzles”). Kevin’s <a href="https://github.com/xkcd/incredible/blob/3f8660708203d7e60c146ef4998cfd377c6af2b8/src/Incredible/Puzzle.hs#L89">map generator</a> transitions from simple single-input single-output puzzles to complex 4-in-4-out merges in the middle, back to 2 outputs per tile at the end.</p>
<p>On the player side, we designed the constraints so we could give players realtime feedback as they constructed their tile. By requiring that tiles output balls on average at roughly the same rate as they received them, we could discourage machines that ate balls or created a lot of latency (e.g. pooling them up). We <a href="https://en.wikipedia.org/wiki/Chaos_engineering">chaos tested</a> tiles by randomizing the rate of balls entering the editor to reflect the variance upstream.</p>
<p>Our general philosophy became “run the machines for a while, see if on average they meet the constraints given uneven input”.</p>
<h3 id="3-machines-should-reach-a-steady-state-in-the-first-30-seconds">3. Machines should reach a steady state in the first 30 seconds.</h3>
<p>This led to a new question: how long would moderators have to watch? We made the arbitrary decision that it should take 30 seconds for machines to enter a steady state, based on napkin math for how long it’d take to moderate the whole machine (e.g. 10k tiles =&gt; 83.3 hours).</p>
<p>We also made balls expire after 30s. Initially, when there was no expiration, I noticed that everyone’s first experience was balls piling up and filling their screen while they learned how to play the game. This would also bog down the physics simulation as it accumulated a huge number of active rigid bodies. Instead of being fun, the balls were getting in the way!</p>
<p><img src="https://chromakode.com/_astro/ball-expiry.EOhkT2a5_ZPE0Oq.webp" alt="Screenshot of Machine with a tutorial popup reading &quot;For security reasons, balls that remain in your device for mosre than 30 seconds will be removed and destroyed.&quot;" width="921" height="922" loading="lazy" decoding="async"></p>
<p>Expiring the balls helped players fall into a pit of success, because machines would not accumulate errors over time. It also drastically simplified moderation, because after watching for 30 seconds, you’ve seen where most balls can end up in their lifetime.</p>
<h2 id="simulation-and-hyperreality">Simulation and hyperreality</h2>
<p><strong>The architecture of Machine made two big bets</strong>. The first was: with all of the above design constraints in place, connecting together disparate tiles into an overall machine would work. We generated and solved a few smaller maps to shake that out.</p>
<p>Back to another problem, though: how could we display a giant machine if we couldn’t run it in realtime on either the server or client?</p>
<p>Before reading further, I’d encourage you to send a little time <a href="https://xkcd.com/2916">scrolling around the comic</a> and imagine how it works. Because what follows will spoil it in a big way.</p>
<p>As a northstar, I wanted it to be possible to follow a single ball from the top of the machine to the bottom. This meant that even if the whole machine wasn’t being simulated, a window around what the player sees would need to be.</p>
<p>Once an early version of the map viewer was working, I started testing out an infinite map with only the viewable area simulated. It looked pretty good — but you can see gaps in the flow when I scroll up, because the initial state of the tiles was empty as they enter the simulation.</p>
<div><astro-island uid="1HOKfP" component-url="/_astro/LazyBunnyVideo.Bs0X-7iO.js" component-export="default" renderer-url="/_astro/client.DwnJltFX.js" props="{&quot;width&quot;:[0,694],&quot;height&quot;:[0,694],&quot;url&quot;:[0,&quot;https://vz-ab48ae95-d30.b-cdn.net/8d018edd-bfba-4c00-ab49-ef34670bbfcd&quot;],&quot;muted&quot;:[0,true],&quot;playsinline&quot;:[0,true],&quot;autoplay&quot;:[0,true],&quot;loop&quot;:[0,true],&quot;controls&quot;:[0,true]}" ssr="" client="idle" opts="{&quot;name&quot;:&quot;LazyBunnyVideo&quot;,&quot;value&quot;:true}" await-children=""><img width="694" height="694" src="https://vz-ab48ae95-d30.b-cdn.net/8d018edd-bfba-4c00-ab49-ef34670bbfcd/thumbnail.jpg"><!--astro:end--></astro-island></div>
<p>Instead of an empty tile, we needed them to appear to already have activity in them. So here’s the second bet: we’d snapshot tiles after they’d reached their steady state, only bringing the snapshots into existence just before they scrolled into view. Would players notice?</p>
<p>Here’s a view of the final comic, with display clipping turned off (you can do this by disabling the <code>overflow: hidden</code> and <code>contain: paint</code> CSS properties on the containers):</p>
<div><astro-island uid="Z7Y9Tp" component-url="/_astro/LazyBunnyVideo.Bs0X-7iO.js" component-export="default" renderer-url="/_astro/client.DwnJltFX.js" props="{&quot;width&quot;:[0,1788],&quot;height&quot;:[0,1006],&quot;url&quot;:[0,&quot;https://vz-ab48ae95-d30.b-cdn.net/60f488b9-0e39-4d4a-834f-1f7997c57002&quot;],&quot;muted&quot;:[0,true],&quot;playsinline&quot;:[0,true],&quot;autoplay&quot;:[0,true],&quot;loop&quot;:[0,true],&quot;controls&quot;:[0,true]}" ssr="" client="idle" opts="{&quot;name&quot;:&quot;LazyBunnyVideo&quot;,&quot;value&quot;:true}" await-children=""><img width="1788" height="1006" src="https://vz-ab48ae95-d30.b-cdn.net/60f488b9-0e39-4d4a-834f-1f7997c57002/thumbnail.jpg"><!--astro:end--></astro-island></div>
<p>Did you notice the snapshots? Unless I’m really looking for them, I don’t.</p>
<p>Only the tiles you see rendered exist in the physics simulation. Note that there’s also a minor display optimization going on: even though you only see the balls inside the viewing area, they’re simulated within the whole tile extents. To pretend there’s more machine up above the view, balls are created and fed to the tiles at the top row of the simulation (based on the expected rate of their input constraints).</p>
<p>To create snapshots, we tied them into the moderation UI. Mods must wait at least 30 seconds before approving a tile. We then take the snapshot when they click the approve button. This gives mods discretion to wait a little longer for the machine to enter a nice looking state.</p>
<div><astro-island uid="Z1kLMuo" component-url="/_astro/LazyBunnyVideo.Bs0X-7iO.js" component-export="default" renderer-url="/_astro/client.DwnJltFX.js" props="{&quot;width&quot;:[0,1050],&quot;height&quot;:[0,1050],&quot;url&quot;:[0,&quot;https://vz-ab48ae95-d30.b-cdn.net/49686202-b3a6-4149-8b12-9461f827d16e&quot;],&quot;muted&quot;:[0,true],&quot;playsinline&quot;:[0,true],&quot;autoplay&quot;:[0,true],&quot;loop&quot;:[0,true],&quot;controls&quot;:[0,true]}" ssr="" client="idle" opts="{&quot;name&quot;:&quot;LazyBunnyVideo&quot;,&quot;value&quot;:true}" await-children=""><img width="1050" height="1050" src="https://vz-ab48ae95-d30.b-cdn.net/49686202-b3a6-4149-8b12-9461f827d16e/thumbnail.jpg"><!--astro:end--></astro-island></div>
<p>Snapshotting worked way better than we expected. A really nice consequence is that it resets accumulated error in the machine. As you scroll around, your first impression of a tile is a clean good state that a moderator liked. In practice, if you watch long enough, many machines can get wedged into stuck or broken states, but you’ll never see them if you keep exploring, because you’ll enter fresh snapshots.</p>
<p>The machine you’re scrolling around in the comic isn’t real. It’s <a href="https://en.wikipedia.org/wiki/Hyperreality">hyperreal</a>. The whole thing is never simulated in its entirely, and I think turned out better that way!</p>
<h2 id="rendering-thousands-of-balls-with-react-and-dom">Rendering thousands of balls with React and DOM</h2>
<p>Machine is built on the <a href="https://rapier.rs/">Rapier</a> physics engine. Rapier was fantastic to work with: it has great docs, a clean API with lots of useful primitives, and has impressive performance thanks to its Rust implementation (running as WASM in the browser). I was also initially drawn to Rapier’s <a href="https://rapier.rs/docs/user_guides/javascript/determinism">determinism guarantees</a>, though we didn’t end up doing any server side simulation.</p>
<p>On top of Rapier, I wrote a custom <a href="https://react.dev/learn/passing-data-deeply-with-context">React context</a>, <a href="https://github.com/xkcd/incredible/blob/3f8660708203d7e60c146ef4998cfd377c6af2b8/client/src/components/PhysicsContext.tsx"><code>&lt;PhysicsContext&gt;</code></a>, which creates Rapier physics objects and manages them within the React component lifecycle. This made it easy to develop a “widget” component for each placeable object with physics or collision surfaces. Effectively, React functioned as a quick and dirty <a href="https://en.wikipedia.org/wiki/Scene_graph">scene graph</a>. This simplified loading and unloading tiles as the view scrolled: when a tile unmounts, all of the physics and DOM are cleaned up. As a bonus, it made it easy to wire up hot reloading with fast refresh, which was <em>really</em> nice for tweaking collision shapes:</p>
<div><astro-island uid="Z1UVqDV" component-url="/_astro/LazyBunnyVideo.Bs0X-7iO.js" component-export="default" renderer-url="/_astro/client.DwnJltFX.js" props="{&quot;width&quot;:[0,1694],&quot;height&quot;:[0,1130],&quot;url&quot;:[0,&quot;https://vz-ab48ae95-d30.b-cdn.net/baa50578-dedc-482c-84c1-d14ffcd2f3cb&quot;],&quot;muted&quot;:[0,true],&quot;playsinline&quot;:[0,true],&quot;autoplay&quot;:[0,true],&quot;loop&quot;:[0,true],&quot;controls&quot;:[0,true]}" ssr="" client="idle" opts="{&quot;name&quot;:&quot;LazyBunnyVideo&quot;,&quot;value&quot;:true}" await-children=""><img width="1694" height="1130" src="https://vz-ab48ae95-d30.b-cdn.net/baa50578-dedc-482c-84c1-d14ffcd2f3cb/thumbnail.jpg"><!--astro:end--></astro-island></div>
<p>Another cool aspect of the React context approach is that all of the physics hooks noop when they’re not inside a <code>&lt;PhysicsContext&gt;</code>. This is used to render static previews of tiles for the moderation UI.</p>
<p>I wish I had used components instead of hooks to create rapier objects. I later discovered this is the approach <a href="https://github.com/pmndrs/react-three-rapier/tree/main">react-three-rapier</a> takes, and it fits better with React diffing (vs. <code>useEffect</code> which destroys the old instance and recreates on dependency change).</p>
<p>Machine is rendered entirely using the DOM. During early dev I was leery I’d reaching the end of my rope perf-wise. I expected I’d eventually ditch DOM rendering for <a href="https://pixijs.com/">PixiJS</a> or canvas when it got too slow. However, I wanted to see how far I could take it, since it meant less to build.</p>
<p>To optimize rendering performance, the frame loop applies styles directly to widgets with physics simulation. Thus React’s diff only runs when structural changes are made to the scene graph. Initially balls were rendered by React, but the frequent creates / removes were low hanging fruit for reducing diffs, so I created their own optimized renderer. Another win was draw culling for balls and widgets out of view. This performed well with 4000 balls in simulation and hundreds onscreen, so I settled on the DOM-only rendering approach.</p>
<p>I’ve heard comparisons drawn between modern browsers and game engines, with their tightly optimized GPU rendering and DOM / scene graph. The similarities have never felt more apt.</p>
<h2 id="api-and-moderation">API and Moderation</h2>
<p>Machine’s backend was written in Haskell by davean and Kevin, with redis as backing store. We used OpenAPI with <a href="https://openapi-ts.pages.dev/openapi-fetch">OpenAPI fetch</a> to share types between the codebases. This approach had some teething pains adapting Haskell types, but ended up very helpful for coordinating late breaking API changes. This was also my first project using <a href="https://tanstack.com/query/latest">TanStack Query</a>, which was quite handy for caching and automatically refreshing the machine without server push.</p>
<p>The moderation UI, designed by <a href="https://github.com/spyhi">Ed White</a>, was critical for us because it bottlenecks all submissions being published. Mods must choose from potentially hundreds of designs for a particular tile. We used a simple approach but unreasonably effective approach to prioritize the queue. Each type of widget has an <a href="https://github.com/xkcd/incredible/blob/3f8660708203d7e60c146ef4998cfd377c6af2b8/client/src/components/moderation/interestingWeights.ts">interestingness score</a>, and we count each instance to sort candidate tiles. This biases towards maximalist solutions, though mods counteract that by reviewing the middle of the list for more minimal ones.</p>
<p>The large imbalance between the number of submitted designs and those published in the machine is unfortunate — it’s my least favorite thing about this comic. We searched for a way to make more of the back catalog available prior to launching, but there wasn’t a good compromise given our moderation time constraints. We’d like to find ways to share more of the submission dataset after live submissions are finished.</p>
<p>One nice UX finding came from the moderation approve cooldown. Since tile snapshot quality is so important, I hacked in a countdown timer which disabled the moderator approve button until at least 30 seconds had passed running the simulation. This ensures that snapshots are taken of a steady state, and gives time to check that outputs are receiving balls at the expected rate. I initially expected this to be annoying to mods, but to my surprise, they liked how it prevented hasty decisions.</p>
<p>Post-launch, I added a slider that allows moderators to speed up the simulation to much faster than realtime. This saves a <em>ton</em> of moderator time, because now the first 30 seconds of a submission can be viewed in under 5 seconds. It’s also quite useful for reviewing the behavior over a longer span of time.</p>
<div><astro-island uid="Z2i8lwv" component-url="/_astro/LazyBunnyVideo.Bs0X-7iO.js" component-export="default" renderer-url="/_astro/client.DwnJltFX.js" props="{&quot;width&quot;:[0,984],&quot;height&quot;:[0,984],&quot;url&quot;:[0,&quot;https://vz-ab48ae95-d30.b-cdn.net/f4b881d5-bb41-4607-9a44-eea326332a42&quot;],&quot;muted&quot;:[0,true],&quot;playsinline&quot;:[0,true],&quot;autoplay&quot;:[0,true],&quot;loop&quot;:[0,true],&quot;controls&quot;:[0,true]}" ssr="" client="idle" opts="{&quot;name&quot;:&quot;LazyBunnyVideo&quot;,&quot;value&quot;:true}" await-children=""><img width="984" height="984" src="https://vz-ab48ae95-d30.b-cdn.net/f4b881d5-bb41-4607-9a44-eea326332a42/thumbnail.jpg"><!--astro:end--></astro-island></div>
<h2 id="a-note-of-appreciation-for-the-jamslunt-interfoggle">A note of appreciation for the “Jamslunt Interfoggle”</h2>
<p>Finally, I’d to take a moment to appreciate one of my favorite machines. It’s a great example of how even with all our editor constraints in place, serendipitous and funny unintended consequences happen between tiles.</p>
<p>The “<a href="https://xkcd.com/2916/#xt=2&amp;yt=2&amp;v=1355">Jamslunt Interfoggle</a>” was posted within the first couple hours the comic was up. It’s a clever mechanism that exploits the narrow field of fans. It queues blue colored balls in a chute until they accumulate enough weight to spill out the sides.</p>
<p><em>However</em>.</p>
<p>The tile that ended up above the Interfoggle, “<a href="https://xkcd.com/2916/#xt=2&amp;yt=1&amp;v=1355">Bouncy</a>”, is a chaos engine launching balls across 3 crossing paths. Every once in a while, it will send a green ball through the wrong output, which wrecking-balls through the logjam and sends a cascade of blue balls through the Interfoggle.</p>
<div><astro-island uid="Z1IQTGo" component-url="/_astro/LazyBunnyVideo.Bs0X-7iO.js" component-export="default" renderer-url="/_astro/client.DwnJltFX.js" props="{&quot;width&quot;:[0,1382],&quot;height&quot;:[0,1382],&quot;url&quot;:[0,&quot;https://vz-ab48ae95-d30.b-cdn.net/38361a2e-2b62-44f2-b1fb-98ccca87e938&quot;],&quot;muted&quot;:[0,true],&quot;playsinline&quot;:[0,true],&quot;autoplay&quot;:[0,true],&quot;loop&quot;:[0,true],&quot;controls&quot;:[0,true]}" ssr="" client="idle" opts="{&quot;name&quot;:&quot;LazyBunnyVideo&quot;,&quot;value&quot;:true}" await-children=""><img width="1382" height="1382" src="https://vz-ab48ae95-d30.b-cdn.net/38361a2e-2b62-44f2-b1fb-98ccca87e938/thumbnail.jpg"><!--astro:end--></astro-island></div>
<p>The Interfoggle can’t have been designed with this behavior in mind, because we only feed the correct color in the editor (this was a conscious decision to make inputs easier to understand). Yet, this machine is so much better with the green balls in the mix.</p>
<p>One of the great joys of making a project like this is discovering all the creative ways people use it, intentional or not. Even though I know it’s coming, I’m continually amazed by how brilliant the internet is when given a shared canvas. Thanks to everyone who contributed tiles.</p>
<p>At the time of writing, there’s still a little time to <a href="https://xkcd.com/2916">add your own design</a> to the final machine.</p>
<hr>
<p>You can check out the <a href="https://github.com/xkcd/incredible">source code of Machine here</a>. Feel free to <a href="https://mastodon.social/@chromakode">drop me a line on Mastodon</a> if you have any questions about it. One cool thing to hack on would be implementing a full global simulation of the machine. I’m quite curious to see how well it works.</p>
<p>I hope you’ve enjoyed this deep dive into “Machine”. For more xkcd stories, check out these notes from our <a href="https://chromakode.com/post/xkcd-gravity-escape-speed">space exploration games</a> and 2021’s <a href="https://chromakode.com/post/checkbox">Morse Code April Fool’s comic</a>.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built a non-linear UI for ChatGPT (141 pts)]]></title>
            <link>https://www.grafychat.com</link>
            <guid>40300126</guid>
            <pubDate>Wed, 08 May 2024 16:41:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.grafychat.com">https://www.grafychat.com</a>, See on <a href="https://news.ycombinator.com/item?id=40300126">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Steve Albini has died (246 pts)]]></title>
            <link>https://pitchfork.com/news/steve-albini-storied-producer-and-icon-of-the-rock-underground-dies-at-61/</link>
            <guid>40300023</guid>
            <pubDate>Wed, 08 May 2024 16:33:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pitchfork.com/news/steve-albini-storied-producer-and-icon-of-the-rock-underground-dies-at-61/">https://pitchfork.com/news/steve-albini-storied-producer-and-icon-of-the-rock-underground-dies-at-61/</a>, See on <a href="https://news.ycombinator.com/item?id=40300023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><a href="https://pitchfork.com/artists/6367-steve-albini/">Steve Albini</a>, an icon of indie rock as both a producer and performer, died on Tuesday, May 7, of a heart attack, staff at his recording studio, <a data-offer-url="https://www.electricalaudio.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.electricalaudio.com/&quot;}" href="https://www.electricalaudio.com/" rel="noopener" target="_blank">Electrical Audio</a>, confirmed to Pitchfork. As well as fronting underground rock lynchpins including <a href="https://pitchfork.com/artists/3763-shellac/">Shellac</a> and <a href="https://pitchfork.com/artists/6076-big-black/">Big Black</a>, Albini was a legend of the recording studio, though he preferred the term “engineer” to “producer.” He recorded <a href="https://pitchfork.com/artists/3046-nirvana/">Nirvana</a>’s <a href="https://pitchfork.com/reviews/albums/18517-nirvana-in-utero-20th-anniversary-edition/"><em>In Utero</em></a>, <a href="https://pitchfork.com/artists/3324-pixies/">Pixies</a>’ <a href="https://pitchfork.com/reviews/albums/19282-pixies-catalogue/"><em>Surfer Rosa</em></a>, <a href="https://pitchfork.com/artists/1896-pj-harvey/">PJ Harvey</a>’s <a href="https://pitchfork.com/reviews/albums/pj-harvey-rid-of-me/"><em>Rid of Me</em></a>, and countless more classic albums, and remained an outspoken critic of exploitative music industry practices until his final years. Shellac were preparing to tour their first album in a decade, <a href="https://pitchfork.com/news/shellac-announce-first-new-album-in-a-decade/"><em>To All Trains</em></a>, which is scheduled for release next week. Steve Albini was 61 years old.</p><p>Despite his insistence that he would work with any artist who paid his fee, Albini’s catalog as a self-described audio engineer encompasses a swath of alternative rock that is practically a genre unto itself. After early work on <em>Surfer Rosa,</em> <a href="https://pitchfork.com/artists/5860-slint/">Slint</a>’s <em>Tweez</em>, and <a href="https://pitchfork.com/artists/463-the-breeders/">the Breeders</a>’ <a href="https://pitchfork.com/reviews/albums/the-breeders-pod/"><em>Pod</em></a>, he became synonymous with brutal, live-sounding analog production that carried palpable raw energy. His unparalleled résumé in the late 1980s and 1990s includes <a href="https://pitchfork.com/artists/2187-the-jesus-lizard/">the Jesus Lizard</a>’s influential <a href="https://pitchfork.com/reviews/albums/13636-head-goat-liar-down/">early albums</a>, <a href="https://pitchfork.com/artists/4564-the-wedding-present/">the Wedding Present</a>’s <em>Seamonsters</em>, <a href="https://pitchfork.com/artists/451-brainiac/">Brainiac</a>’s <em>Hissing Prigs in Static Couture</em>, and records by <a href="https://pitchfork.com/artists/2546-low/">Low</a>, <a href="https://pitchfork.com/artists/1089-dirty-three/">Dirty Three</a>, <a href="https://pitchfork.com/artists/1922-helmet/">Helmet</a>, <a href="https://pitchfork.com/artists/434-boss-hog/">Boss Hog</a>, <a href="https://pitchfork.com/artists/2205-the-jon-spencer-blues-explosion/">Jon Spencer Blues Explosion</a>, <a href="https://pitchfork.com/artists/1996-hum/">Hum</a>, <a href="https://pitchfork.com/artists/3998-superchunk/">Superchunk</a>, and dozens more. His influence rang through to the next generations of rock, punk, and metal at home and abroad, many of whom he went on to produce—the likes of <a href="https://pitchfork.com/artists/2801-mogwai/">Mogwai</a>, <a href="https://pitchfork.com/artists/2720-mclusky/">Mclusky</a>, <a href="https://pitchfork.com/artists/28679-cloud-nothings/">Cloud Nothings</a>, <a href="https://pitchfork.com/artists/2816-mono/">Mono</a>, <a href="https://pitchfork.com/artists/27962-ty-segall/">Ty Segall</a>, and <a href="https://pitchfork.com/artists/3984-sunn-o/">Sunn O)))</a>. He also recorded enduring greats of the singer-songwriter canon: <a href="https://pitchfork.com/artists/3096-joanna-newsom/">Joanna Newsom</a>’s <a href="https://pitchfork.com/reviews/albums/9616-ys/"><em>Ys</em></a>, <a href="https://pitchfork.com/artists/3003-nina-nastasia/">Nina Nastasia</a>’s early records, and much of the <a href="https://pitchfork.com/artists/2899-jason-molina/">Jason Molina</a> catalog among them.</p><p>Albini was born in Pasadena, California, and lived a peripatetic childhood before his family settled in Missoula, Montana. As a teenager, his discovery of Ramones transformed what he <a href="https://www.theguardian.com/music/2023/aug/15/the-evolution-of-steve-albini-if-the-dumbest-person-is-on-your-side-youre-on-the-wrong-side">described</a>, to <a href="https://pitchfork.com/staff/jeremy-gordon/">Jeremy Gordon</a> for <a href="https://www.theguardian.com/music/2023/aug/15/the-evolution-of-steve-albini-if-the-dumbest-person-is-on-your-side-youre-on-the-wrong-side"><em>The Guardian</em></a>, as a “normal Montana childhood” into an altogether wilder entity. In the subsequent years, while studying journalism in Illinois, he was drawn into the Chicago punk scene that his music would come to both defy and define. Albini spent his days at the record store Wax Trax, buying every record that “looked interesting” and talking to “everybody with a funny haircut,” he told <a href="https://www.npr.org/2023/01/20/1150270026/inside-legendary-audio-engineer-steve-albini-chicago-studio-electrical-audio">NPR</a>.</p><p>“It was an extremely active, very fertile scene where everybody was participating on every level,” Albini <a href="https://www.npr.org/2023/01/20/1150270026/inside-legendary-audio-engineer-steve-albini-chicago-studio-electrical-audio">said</a> of Chicago’s music scene. “The community that I joined when I came to Chicago enabled me to continue on with a life in music. I didn’t do this by myself. I did this as a participant in a scene, in a community, in a culture, and when I see somebody extracting from that rather than participating in it as a peer, it makes me think less of that person.… My participation in all of this is going to come to an end at some point. The only thing that I can say for myself is that, along the way, it was a cool thing that I participated in, and on the way out, I want to make sure that I don’t take it with me.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>He began recording as Big Black in the early 1980s, channeling antisocial, sometimes violent themes through buzzsaw riffs and histrionic barks, grunts, and whelps, at first backed only by a drum machine (which remained a constant, pounding presence) and soon joined by Naked Raygun’s Jeff Pezzati and Santiago Durango; Dave Riley replaced Pezzati on bass for the band’s two landmark studio albums, <em>Atomizer</em> and <em>Songs About Fucking</em>. In his spare time, Albini would pen screeds in the 1980s zine <em>Matter</em>, admonishing bands in neighboring scenes, establishing the firebrand reputation that established him as an eminent rock grouch and refusenik.</p><p>After Big Black, Albini formed the short-lived Rapeman—a name he came to regret, despite the sardonic intent—before founding Shellac in the early 1990s, with Bob Weston and Todd Trainer. After a string of EPs through his longtime home of <a data-offer-url="http://touchandgorecords.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;http://touchandgorecords.com/&quot;}" href="http://touchandgorecords.com/" rel="noopener" target="_blank">Touch and Go</a> and <a data-offer-url="https://www.dragcity.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.dragcity.com/&quot;}" href="https://www.dragcity.com/" rel="noopener" target="_blank">Drag City</a>, the band extensively toured (including an all-but-residency at Primavera Sound, the only music festival Albini was happy to play) and released five beloved albums: 1994’s <em>At Action Park</em>, 1998’s <em>Terraform</em>, 2000’s <em>1000 Hurts</em>, 2007’s <a href="https://pitchfork.com/reviews/albums/10314-excellent-italian-greyhound/"><em>Excellent Italian Greyhound</em></a>, and 2014’s <a href="https://pitchfork.com/reviews/albums/19842-shellac-dude-incredible/"><em>Dude Incredible</em></a>.</p><p>Throughout his career, Albini courted controversy through provocative band names (Rapeman, Run N***er Run), song titles ( “Pray I Don’t Kill You F***ot,” “My Black Ass”), and offhand statements (“I want to strangle Odd Future”). While he refused to apologize for his choice in names and jokes, in Michael Azerrad’s 2001 book <em>Our Band Could Be Your Life</em>, Albini made it clear that he believed his real stances on race, gender, LGBTQ rights, and politics were obvious. “I have less respect for the man who bullies his girlfriend and calls her ‘Ms’ than a guy who treats women reasonably and respectfully and calls them ‘Yo! Bitch,’” Albini told Azerrad. “The point of all this is to change the way you live your life, not the way you speak.”</p><p>Later in life, however, Albini repeatedly apologized for his past controversies, realizing that intent and moral clarity went only so far. “A lot of things I said and did from an ignorant position of comfort and privilege are clearly awful and I regret them. It’s nobody’s obligation to overlook that, and I do feel an obligation to redeem myself,” Albini wrote on <a data-offer-url="https://twitter.com/electricalWSOP/status/1448050175658713092/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://twitter.com/electricalWSOP/status/1448050175658713092/&quot;}" href="https://twitter.com/electricalWSOP/status/1448050175658713092/" rel="noopener" target="_blank">X</a> in 2021. “If anything, we were trying to underscore the banality, the everyday nonchalance toward our common history with the atrocious, all while laboring under the tacit *mistaken* notion that things were getting better. I’m overdue for a conversation about my role in inspiring ‘edgelord’ shit. Believe me, I’ve met my share of punishers at gigs and I sympathize with anybody who isn’t me but still had to suffer them.” He talked in depth about his regrets with <a href="https://www.theguardian.com/music/2023/aug/15/the-evolution-of-steve-albini-if-the-dumbest-person-is-on-your-side-youre-on-the-wrong-side"><em>The Guardian</em></a>, <a data-offer-url="https://melmagazine.com/en-us/story/steve-albini-counsel-culture-interview#google_vignette" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://melmagazine.com/en-us/story/steve-albini-counsel-culture-interview#google_vignette&quot;}" href="https://melmagazine.com/en-us/story/steve-albini-counsel-culture-interview#google_vignette" rel="noopener" target="_blank"><em>MEL Magazine</em></a>, and others.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Amid all of his ongoing work, Albini was a remarkable poker player. In 2022, he won <a href="https://pitchfork.com/news/steve-albini-wins-major-prize-at-2022-world-series-of-poker/">a World Series of Poker gold bracelet</a> after beating 773 other players in the $1,500 entry H.O.R.S.E. competition for a huge prize of $196,089. While most players dressed in button-up shirts and plain tees, Albini wore a furry, white hat shaped like a bear and a red Jack O’ Nuts shirt, saying the Athens noise-rock musicians “bring me luck.” He won <a href="https://pitchfork.com/news/steve-albini-wins-major-prize-at-2018-world-series-of-poker/">another WSOP gold bracelet in 2018</a> for beating 310 players in seven card stud to the tune of $105,629. Back then, he was wearing a Cocaine Piss shirt during the big win. He had a massive grin on his face in the photos documenting both wins.</p><p>When asked how his career would be regarded if he ever retired, Albini told <em>The Guardian</em>, “I don’t give a shit. I’m doing it, and that’s what matters to me—the fact that I get to keep doing it. That’s the whole basis of it. I was doing it yesterday, and I’m gonna do it tomorrow, and I’m gonna carry on doing it.”</p><p>Head <a href="https://pitchfork.com/news/steve-albini-remembered-react-to-death-of-legendary-rock-figure/">here</a> for remembrances of Albini from Cloud Nothings’ Dylan Baldi, Pixies, Michael Azerrad, Elijah Wood, Jon Wurster, and more.</p><figure data-testid="IframeEmbed"><div data-testid="IframeEmbedContainer"><p><iframe height="113" width="200" sandbox="allow-scripts allow-popups allow-same-origin" title="Embedded Frame" src="https://www.youtube-nocookie.com/embed/0vRiteJXNYU" allow="autoplay *; encrypted-media *; clipboard-write; autoplay; fullscreen; picture-in-picture"></iframe></p></div></figure></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a better Perplexity for developers (127 pts)]]></title>
            <link>https://devv.ai</link>
            <guid>40299091</guid>
            <pubDate>Wed, 08 May 2024 15:19:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devv.ai">https://devv.ai</a>, See on <a href="https://news.ycombinator.com/item?id=40299091">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div tabindex="-1"><div><div><p>⚡️ </p><!-- --><p>Fast Mode</p></div><p><span>Free</span></p></div><div><p>Lightning-fast answers, documentation, and code snippets for your dev queries.</p></div></div><div tabindex="-1"><div><div><p>🤖️ </p><!-- --><p>Agent Mode</p></div><p><span>GPT-4</span></p></div><div><p>AI-powered agents decipher your complex questions and craft tailored solutions.</p></div></div><div tabindex="-1"><div><div><svg width="19" height="19" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M7.49933 0.25C3.49635 0.25 0.25 3.49593 0.25 7.50024C0.25 10.703 2.32715 13.4206 5.2081 14.3797C5.57084 14.446 5.70302 14.2222 5.70302 14.0299C5.70302 13.8576 5.69679 13.4019 5.69323 12.797C3.67661 13.235 3.25112 11.825 3.25112 11.825C2.92132 10.9874 2.44599 10.7644 2.44599 10.7644C1.78773 10.3149 2.49584 10.3238 2.49584 10.3238C3.22353 10.375 3.60629 11.0711 3.60629 11.0711C4.25298 12.1788 5.30335 11.8588 5.71638 11.6732C5.78225 11.205 5.96962 10.8854 6.17658 10.7043C4.56675 10.5209 2.87415 9.89918 2.87415 7.12104C2.87415 6.32925 3.15677 5.68257 3.62053 5.17563C3.54576 4.99226 3.29697 4.25521 3.69174 3.25691C3.69174 3.25691 4.30015 3.06196 5.68522 3.99973C6.26337 3.83906 6.8838 3.75895 7.50022 3.75583C8.1162 3.75895 8.73619 3.83906 9.31523 3.99973C10.6994 3.06196 11.3069 3.25691 11.3069 3.25691C11.7026 4.25521 11.4538 4.99226 11.3795 5.17563C11.8441 5.68257 12.1245 6.32925 12.1245 7.12104C12.1245 9.9063 10.4292 10.5192 8.81452 10.6985C9.07444 10.9224 9.30633 11.3648 9.30633 12.0413C9.30633 13.0102 9.29742 13.7922 9.29742 14.0299C9.29742 14.2239 9.42828 14.4496 9.79591 14.3788C12.6746 13.4179 14.75 10.7025 14.75 7.50024C14.75 3.49593 11.5036 0.25 7.49933 0.25Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><p>GitHub Mode</p></div><p><span>New</span></p></div><p>Seamlessly interact with your repositories for contextualized search and assistance.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AlphaFold 3 predicts the structure and interactions of all of life's molecules (559 pts)]]></title>
            <link>https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/</link>
            <guid>40298927</guid>
            <pubDate>Wed, 08 May 2024 15:07:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/">https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/</a>, See on <a href="https://news.ycombinator.com/item?id=40298927">Hacker News</a></p>
<div id="readability-page-1" class="page"><article ng-init="drawerToggle = {'open': true}">

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
  }">
      <div>
          <p>May 08, 2024</p>
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Introducing AlphaFold 3, a new AI model developed by Google DeepMind and Isomorphic Labs. By accurately predicting the structure of proteins, DNA, RNA, ligands and more, and how they interact, we hope it will transform our understanding of the biological world and drug discovery.
        </p>
      
    </div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2.width-1200.format-webp.webp" fetchpriority="high" alt="Colorful protein structure against an abstract gradient background.">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to Revealing life’s molecules" href="#life-molecules" id="life-molecules-anchor">Revealing life’s molecules</a>
        </li>
        
        <li>
          <a aria-label="link to Leading drug discovery" href="#drug-discovery" id="drug-discovery-anchor">Leading drug discovery</a>
        </li>
        
        <li>
          <a aria-label="link to Introducing AlphaFold Server" href="#alphafold-server" id="alphafold-server-anchor">Introducing AlphaFold Server</a>
        </li>
        
        <li>
          <a aria-label="link to Sharing responsibly" href="#responsibility" id="responsibility-anchor">Sharing responsibly</a>
        </li>
        
        <li>
          <a aria-label="link to Future of AI-powered cell biology" href="#future-cell-biology" id="future-cell-biology-anchor">Future of AI-powered cell biology</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-drop-cap|uni-tombstone">

            
              


<google-read-aloud-player data-analytics-module="{
        &quot;event&quot;: &quot;module_impression&quot;,
        &quot;module_name&quot;: &quot;ai_audio&quot;,
        &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
    }" data-date-modified="2024-05-08T15:00:00.987420+00:00" data-progress-bar-style="half-wave" data-api-key="AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac" data-article-style="style9" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-layout-style="style1" data-highlight-mode="word-over-paragraph" data-highlight-text-color="#000000" data-highlight-word-background="#8AB4F8" data-highlight-paragraph-background="#D2E3FC" data-background="linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)" data-foreground-color="#202124" data-font="600 16px Google Sans, sans-serif" data-box-shadow="0px 1px 3px 1px rgba(60, 64, 67, 0.15)">
</google-read-aloud-player>




            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><p data-block-key="t1yf2">Inside every plant, animal and human cell are billions of molecular machines. They’re made up of proteins, DNA and other molecules, but no single piece works on its own. Only by seeing how they interact together, across millions of types of combinations, can we start to truly understand life’s processes.</p><p data-block-key="37r0c">In a paper published in <a href="https://www.nature.com/articles/s41586-024-07487-w" rt-link-type="external"><i>Nature</i></a>, we introduce AlphaFold 3, a revolutionary model that can predict the structure and interactions of all life’s molecules with unprecedented accuracy. For the interactions of proteins with other molecule types we see at least a 50% improvement compared with existing prediction methods, and for some important categories of interaction we have doubled prediction accuracy.</p><p data-block-key="2am3m">We hope AlphaFold 3 will help transform our understanding of the biological world and drug discovery. Scientists can access the majority of its capabilities, for free, through our newly launched <a href="http://alphafoldserver.com/" rt-link-type="external">AlphaFold Server</a>, an easy-to-use research tool. To build on AlphaFold 3’s potential for drug design, <a href="https://www.isomorphiclabs.com/articles/rational-drug-design-with-alphafold-3" rt-link-type="external">Isomorphic Labs</a> is already collaborating with pharmaceutical companies to apply it to real-world drug design challenges and, ultimately, develop new life-changing treatments for patients.</p><p data-block-key="79nqe">Our new model builds on the foundations of AlphaFold 2, which in 2020 made a <a href="https://deepmind.google/discover/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology/" rt-link-type="external">fundamental breakthrough in protein structure prediction</a>. So far, <a href="https://deepmind.google/impact/meet-the-scientists-using-alphafold/" rt-link-type="external">millions of researchers</a> globally have used AlphaFold 2 to make discoveries in areas including malaria vaccines, cancer treatments and enzyme design. AlphaFold has been cited more than 20,000 times and its scientific impact recognized through many prizes, most recently the <a href="https://breakthroughprize.org/News/73" rt-link-type="external">Breakthrough Prize in Life Sciences</a>. AlphaFold 3 takes us beyond proteins to a broad spectrum of biomolecules. This leap could unlock more transformative science, from developing biorenewable materials and more resilient crops, to accelerating drug design and genomics research.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/AFS-anim-7PNM.mp4" type="video/mp4" title="GIF of a rotating spike protein structure against a white background, with ground truth shown in gray." alt="7PNM">
            Video format not supported
          </video>
        
      
    
    </p>
    
      <figcaption><p data-block-key="mv0fd">7PNM - Spike protein of a common cold virus (Coronavirus OC43): AlphaFold 3’s structural prediction for a spike protein (blue) of a cold virus as it interacts with antibodies (turquoise) and simple sugars (yellow), accurately matches the true structure (gray). The animation shows the protein interacting with an antibody, then a sugar. Advancing our knowledge of such immune-system processes helps better understand coronaviruses, including COVID-19, raising possibilities for improved treatments.</p></figcaption>
    
  
    </div>
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><h2 data-block-key="t1yf2">How AlphaFold 3 reveals life’s molecules</h2><p data-block-key="9sni9">Given an input list of molecules, <a href="https://www.nature.com/articles/s41586-024-07487-w" rt-link-type="external">AlphaFold 3</a> generates their joint 3D structure, revealing how they all fit together. It models large biomolecules such as proteins, DNA and RNA, as well as small molecules, also known as ligands — a category encompassing many drugs. Furthermore, AlphaFold 3 can model chemical modifications to these molecules which control the healthy functioning of cells, that when disrupted can lead to disease.</p><p data-block-key="bkj8h">AlphaFold 3’s capabilities come from its next-generation architecture and training that now covers all of life’s molecules. At the core of the model is an improved version of our <a href="https://www.nature.com/articles/s41586-021-03819-2" rt-link-type="external">Evoformer module</a> — a deep learning architecture that underpinned AlphaFold 2’s incredible performance. After processing the inputs, AlphaFold 3 assembles its predictions using a diffusion network, akin to those found in AI image generators. The diffusion process starts with a cloud of atoms, and over many steps converges on its final, most accurate molecular structure.</p><p data-block-key="bc95f">AlphaFold 3’s predictions of molecular interactions surpass the accuracy of all existing systems. As a single model that computes entire molecular complexes in a holistic way, it’s uniquely able to unify scientific insights.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/AFS-anim-7R6R.mp4" type="video/mp4" title="GIF of a rotating DNA binding protein structure against a white background, with ground truth shown in gray." alt="7R6R">
            Video format not supported
          </video>
        
      
    
    </p>
    
      <figcaption><p data-block-key="hhhu1">7R6R - DNA binding protein: AlphaFold 3’s prediction for a molecular complex featuring a protein (blue) bound to a double helix of DNA (pink) is a near-perfect match to the true molecular structure discovered through painstaking experiments (gray).</p></figcaption>
    
  
    </div>
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><h2 data-block-key="t1yf2">Leading drug discovery at Isomorphic Labs</h2><p data-block-key="5l6pl">AlphaFold 3 creates capabilities for drug design with predictions for molecules commonly used in drugs, such as ligands and antibodies, that bind to proteins to change how they interact in human health and disease.</p><p data-block-key="beh6j">AlphaFold 3 achieves unprecedented accuracy in predicting drug-like interactions, including the binding of proteins with ligands and antibodies with their target proteins. AlphaFold 3 is 50% more accurate than the best traditional methods on the <a href="https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc04185a" rt-link-type="external">PoseBusters benchmark</a> without needing the input of any structural information, making AlphaFold 3 the first AI system to surpass physics-based tools for biomolecular structure prediction. The ability to predict antibody-protein binding is critical to understanding aspects of the human immune response and the design of new antibodies — a growing class of therapeutics.</p><p data-block-key="4s1o1">Using AlphaFold 3 in combination with a complementary suite of in-house AI models, <a href="https://www.isomorphiclabs.com/articles/rational-drug-design-with-alphafold-3" rt-link-type="external">Isomorphic Labs</a> is working on drug design for internal projects as well as with pharmaceutical partners. Isomorphic Labs is using AlphaFold 3 to accelerate and improve the success of drug design — by helping understand how to approach new disease targets, and developing novel ways to pursue existing ones that were previously out of reach.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }">
        <p><h2 data-block-key="t1yf2">AlphaFold Server: A free and easy-to-use research tool</h2></p>
      </div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/AFS-anim-8AW3.mp4" type="video/mp4" title="GIF of a rotating RNA modifying protein structure against a white background, with ground truth shown in gray." alt="8AW3">
            Video format not supported
          </video>
        
      
    
    </p>
    
      <figcaption><p data-block-key="mc56w">8AW3 - RNA modifying protein: AlphaFold 3’s prediction for a molecular complex featuring a protein (blue), a strand of RNA (purple), and two ions (yellow) closely matches the true structure (gray). This complex is involved with the creation of other proteins — a cellular process fundamental to life and health.</p></figcaption>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><p data-block-key="t1yf2">Google DeepMind’s newly launched <a href="http://alphafoldserver.com/" rt-link-type="external">AlphaFold Server</a> is the most accurate tool in the world for predicting how proteins interact with other molecules throughout the cell. It is a free platform that scientists around the world can use for non-commercial research. With just a few clicks, biologists can harness the power of AlphaFold 3 to model structures composed of proteins, DNA, RNA and a selection of ligands, ions and chemical modifications.</p><p data-block-key="di2e8">AlphaFold Server helps scientists make novel hypotheses to test in the lab, speeding up workflows and enabling further innovation. Our platform gives researchers an accessible way to generate predictions, regardless of their access to computational resources or their expertise in machine learning.</p><p data-block-key="8ca9a">Experimental protein-structure prediction can take about the length of a PhD and cost hundreds of thousands of dollars. Our previous model, AlphaFold 2, has been used to predict hundreds of millions of structures, which would have taken hundreds of millions of researcher-years at the current rate of experimental structural biology.</p></div>
  

  
    <section data-analytics-module="{
       &quot;module_name&quot;: &quot;Pull Quote&quot;,
       &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
     }">
    <p><q>With AlphaFold Server, it’s not only about predicting structures anymore, it’s about generously giving access: allowing researchers to ask daring questions and accelerate discoveries.</q>

      
        <cite>
          
          
            <span>
              
                <strong>Céline Bouchoux</strong><br>
              
              
                The Francis Crick Institute
              
            </span>
          
        </cite>
      
    </p>
  </section>


  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="AlphaFold 3 predicts the structure and interactions of all of life’s molecules" data-video-id="9ufplEgtq8w" data-index-id="13" data-type="video" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          <p><img alt="Demo video showing the capabilities of the server." src="https://i.ytimg.com/vi_webp/9ufplEgtq8w/default.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/9ufplEgtq8w/sddefault.webp&quot;,
                &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/9ufplEgtq8w/hqdefault.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20240416-1649#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20240416-1649#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    

    
  </div>

  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><h2 data-block-key="t1yf2">Sharing the power of AlphaFold 3 responsibly</h2><p data-block-key="epbhg">With each AlphaFold release, <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphafold-3-predicts-the-structure-and-interactions-of-all-lifes-molecules/Our-approach-to-biosecurity-for-AlphaFold-3-08052024" rt-link-type="external">we’ve sought to understand the broad impact of the technology</a>, working together with the research and safety community. We take a science-led approach and have conducted extensive assessments to mitigate potential risks and share the widespread benefits to biology and humanity.</p><p data-block-key="25l72">Building on the external consultations we carried out for AlphaFold 2, we’ve now engaged with more than 50 domain experts, in addition to specialist third parties, across biosecurity, research and industry, to understand the capabilities of successive AlphaFold models and any potential risks. We also participated in community-wide forums and discussions ahead of AlphaFold 3’s launch.</p><p data-block-key="bp8tm">AlphaFold Server reflects our ongoing commitment to share the benefits of AlphaFold, including our <a href="https://alphafold.ebi.ac.uk/" rt-link-type="external">free database</a> of 200 million protein structures. We’ll also be expanding our free<a href="https://www.ebi.ac.uk/training/online/courses/alphafold/" rt-link-type="external"> AlphaFold education online course</a> with <a href="https://www.ebi.ac.uk/" rt-link-type="external">EMBL-EBI</a> and partnerships with organizations in the Global South to equip scientists with the tools they need to accelerate adoption and research, including on underfunded areas such as neglected diseases and food security. We’ll continue to work with the scientific community and policy makers to develop and deploy AI technologies responsibly.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }">
        <p><h2 data-block-key="t1yf2">Opening up the future of AI-powered cell biology</h2></p>
      </div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/AFS-anim-7BBV.mp4" type="video/mp4" title="GIF of an enzyme protein structure against a white background, with ground truth shown in gray." alt="7BBV">
            Video format not supported
          </video>
        
      
    
    </p>
    
      <figcaption><p data-block-key="2em21">7BBV - Enzyme: AlphaFold 3’s prediction for a molecular complex featuring an enzyme protein (blue), an ion (yellow sphere) and simple sugars (yellow), along with the true structure (gray). This enzyme is found in a soil-borne fungus (Verticillium dahliae) that damages a wide range of plants. Insights into how this enzyme interacts with plant cells could help researchers develop healthier, more resilient crops.</p></figcaption>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><p data-block-key="t1yf2">AlphaFold 3 brings the biological world into high definition. It allows scientists to see cellular systems in all their complexity, across structures, interactions and modifications. This new window on the molecules of life reveals how they’re all connected and helps understand how those connections affect biological functions — such as the actions of drugs, the production of hormones and the health-preserving process of DNA repair.</p><p data-block-key="8bon2">The impacts of AlphaFold 3 and our free AlphaFold Server will be realized through how they empower scientists to accelerate discovery across open questions in biology and new lines of research. We’re just beginning to tap into AlphaFold 3’s potential and can’t wait to see what the future holds.</p></div>
  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla is being investigated for securities and wire fraud for self-driving claim (148 pts)]]></title>
            <link>https://www.theverge.com/2024/5/8/24151881/tesla-justice-investigation-securities-wire-fraud-self-driving</link>
            <guid>40298486</guid>
            <pubDate>Wed, 08 May 2024 14:26:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/5/8/24151881/tesla-justice-investigation-securities-wire-fraud-self-driving">https://www.theverge.com/2024/5/8/24151881/tesla-justice-investigation-securities-wire-fraud-self-driving</a>, See on <a href="https://news.ycombinator.com/item?id=40298486">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Department of Justice is looking into whether Tesla committed securities and wire fraud around its self-driving vehicle claims, <a href="https://www.reuters.com/business/autos-transportation/tesla-autopilot-probe-us-prosecutors-focus-securities-wire-fraud-2024-05-08/"><em>Reuters </em>reports today</a>, citing three sources familiar with the matter. </p><p>The investigation, which was <a href="https://www.theverge.com/2022/10/26/23425335/tesla-autopilot-justice-department-criminal-investigation">first reported in October 2022</a> but has been going on since at least late 2021, involves federal prosecutors in Washington and San Francisco who are examining whether Tesla executives misled consumers, investors, and regulators by making unsupported claims about its autonomous capabilities. Now, it appears that investigators are zeroing in on specific charges against the company: securities and wire fraud. </p><p>According to <em>Reuters</em>, the probe is looking into statements made by Tesla CEO Elon Musk in particular. For years, Musk has been promising fully autonomous Tesla vehicles are just around the corner —&nbsp;while also admitting that he often sets overly optimistic timelines. Meanwhile, the company’s advanced driver-assist features, Autopilot and Full Self-Driving, do not make the vehicles autonomous and require drivers to keep their hands on the steering wheel and eyes on the road.</p><div><p>According to <em>Reuters</em>, the probe is looking into Tesla CEO Elon Musk’s claims in particular</p></div><p>Tesla has repeatedly pushed the boundaries of safety by allowing its customers to beta test products that may not be ready for wide release. Tesla vehicles using Autopilot have been subject to numerous recalls and involved in <a href="https://www.theverge.com/2024/5/7/24151077/tesla-autopilot-nhtsa-recall-crash-data-request">hundreds of crashes over the years</a>, dozens of which have been fatal. <a href="https://www.theverge.com/2023/12/13/23999683/tesla-autopilot-defect-software-update-recall-nhtsa">The most recent recall</a>, which applied to every single Tesla sold to date, <a href="https://www.theverge.com/2024/4/26/24141403/tesla-autopilot-nhtsa-investigation-recall-software-fix">has now come under a new investigation</a> for its failure to prevent driver misuse and correct the flaws identified in the first recall. </p><p>Wire fraud involves deceiving customers in interstate communications, whereas securities fraud relates to misleading investors. The Securities and Exchange Commission is also looking into whether Tesla lied in its communications about self-driving vehicles, <em>Reuters</em> says. </p><p>The Justice Department is said to also be <a href="https://www.theverge.com/2023/10/23/23928563/tesla-doj-ev-range-exaggerate-investigation">looking into Tesla’s vehicle range claims</a>. Tesla customers have long complained that the company’s listed vehicle ranges often don’t match up to the reality of what the cars are capable of. </p><p>In its latest securities filing, Tesla acknowledged “regularly” receiving subpoenas and requests for information from the SEC and Justice Department, some of which involve Autopilot and Full Self-Driving. </p><p>“To our knowledge no government agency in any ongoing investigation has concluded that any wrongdoing occurred,” the company said. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TimesFM (Time Series Foundation Model) for time-series forecasting (160 pts)]]></title>
            <link>https://github.com/google-research/timesfm</link>
            <guid>40297946</guid>
            <pubDate>Wed, 08 May 2024 13:34:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google-research/timesfm">https://github.com/google-research/timesfm</a>, See on <a href="https://news.ycombinator.com/item?id=40297946">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">TimesFM</h2><a id="user-content-timesfm" aria-label="Permalink: TimesFM" href="#timesfm"></a></p>
<p dir="auto">TimesFM  (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google
Research for time-series forecasting.</p>
<ul dir="auto">
<li>Paper: <a href="https://arxiv.org/abs/2310.10688" rel="nofollow">A decoder-only foundation model for time-series forecasting</a>, to appear in ICML 2024.</li>
<li><a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/" rel="nofollow">Google Research blog</a></li>
<li><a href="https://huggingface.co/google/timesfm-1.0-200m" rel="nofollow">Hugging Face checkpoint repo</a></li>
</ul>
<p dir="auto">This repo contains the code to load public TimesFM checkpoints and run model
inference. Please visit our
<a href="https://huggingface.co/google/timesfm-1.0-200m" rel="nofollow">Hugging Face checkpoint repo</a>
to download model checkpoints.</p>
<p dir="auto">This is not an officially supported Google product.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Checkpoint timesfm-1.0-200m</h2><a id="user-content-checkpoint-timesfm-10-200m" aria-label="Permalink: Checkpoint timesfm-1.0-200m" href="#checkpoint-timesfm-10-200m"></a></p>
<p dir="auto">timesfm-1.0-200m is the first open model checkpoint:</p>
<ul dir="auto">
<li>It performs univariate time series forecasting for context lengths up tp 512 timepoints and any horizon lengths, with an optional frequency indicator.</li>
<li>It focuses on point forecasts, and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.</li>
<li>It requires the context to be contiguous (i.e. no "holes"), and the context and the horizon to be of the same frequency.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmarks</h2><a id="user-content-benchmarks" aria-label="Permalink: Benchmarks" href="#benchmarks"></a></p>
<p dir="auto">Please refer to our result tables on the <a href="https://github.com/google-research/timesfm/blob/master/experiments/extended_benchmarks/tfm_results.png">extended benchmarks</a> and the <a href="https://github.com/google-research/timesfm/blob/master/experiments/long_horizon_benchmarks/tfm_long_horizon.png">long horizon benchmarks</a>.</p>
<p dir="auto">Please look into the README files in the respective benchmark directories within <code>experiments/</code> for instructions for running TimesFM on the respective benchmarks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">We have two environment files. For GPU installation (assuming CUDA 12 has been
setup), you can create a conda environment <code>tfm_env</code> from the base folder
through:</p>
<div data-snippet-clipboard-copy-content="conda env create --file=environment.yml"><pre><code>conda env create --file=environment.yml
</code></pre></div>
<p dir="auto">For a CPU setup please use,</p>
<div data-snippet-clipboard-copy-content="conda env create --file=environment_cpu.yml"><pre><code>conda env create --file=environment_cpu.yml
</code></pre></div>
<p dir="auto">to create the environment instead.</p>
<p dir="auto">Follow by</p>
<div data-snippet-clipboard-copy-content="conda activate tfm_env
pip install -e ."><pre><code>conda activate tfm_env
pip install -e .
</code></pre></div>
<p dir="auto">to install the package.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Initialize the model and load a checkpoint.</h3><a id="user-content-initialize-the-model-and-load-a-checkpoint" aria-label="Permalink: Initialize the model and load a checkpoint." href="#initialize-the-model-and-load-a-checkpoint"></a></p>
<p dir="auto">Then the base class can be loaded as,</p>
<div dir="auto" data-snippet-clipboard-copy-content="import timesfm

tfm = timesfm.TimesFm(
    context_len=<context>,
    horizon_len=<horizon>,
    input_patch_len=32,
    output_patch_len=128,
    num_layers=20,
    model_dims=1280,
    backend=<backend>,
)
tfm.load_from_checkpoint(<checkpoint_path>)"><pre><span>import</span> <span>timesfm</span>

<span>tfm</span> <span>=</span> <span>timesfm</span>.<span>TimesFm</span>(
    <span>context_len</span><span>=</span><span>&lt;</span><span>context</span><span>&gt;</span>,
    <span>horizon_len</span><span>=</span><span>&lt;</span><span>horizon</span><span>&gt;</span>,
    <span>input_patch_len</span><span>=</span><span>32</span>,
    <span>output_patch_len</span><span>=</span><span>128</span>,
    <span>num_layers</span><span>=</span><span>20</span>,
    <span>model_dims</span><span>=</span><span>1280</span>,
    <span>backend</span><span>=</span><span>&lt;</span><span>backend</span><span>&gt;</span>,
)
<span>tfm</span>.<span>load_from_checkpoint</span>(<span>&lt;</span><span>checkpoint_path</span><span>&gt;</span>)</pre></div>
<p dir="auto">Note that the four parameters are fixed to load the 200m model</p>
<div dir="auto" data-snippet-clipboard-copy-content="input_patch_len=32,
output_patch_len=128,
num_layers=20,
model_dims=1280,"><pre><span>input_patch_len</span><span>=</span><span>32</span>,
<span>output_patch_len</span><span>=</span><span>128</span>,
<span>num_layers</span><span>=</span><span>20</span>,
<span>model_dims</span><span>=</span><span>1280</span>,</pre></div>
<ol dir="auto">
<li>
<p dir="auto">The context_len here can be set as the max context length <strong>of the model</strong>. You can provide shorter series to the <code>tfm.forecast()</code> function and the model will handle it. Currently the model handles a max context length of 512, which can be increased in later releases. The input time series can have <strong>any context length</strong>. Padding / truncation will be handled by the inference code if needed.</p>
</li>
<li>
<p dir="auto">The horizon length can be set to anything. We recommend setting it to the largest horizon length you would need in the forecasting tasks for your application. We generally recommend horizon length &lt;= context length but it is not a requirement in the function call.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Perform inference</h3><a id="user-content-perform-inference" aria-label="Permalink: Perform inference" href="#perform-inference"></a></p>
<p dir="auto">We provide APIs to forecast from either array inputs or <code>pandas</code> dataframe. Both forecast methods expect (1) the input time series contexts, (2) along with their frequencies. Please look at the documentation of the functions <code>tfm.forecast()</code> and <code>tfm.forecast_on_df()</code> for detailed instructions.</p>
<p dir="auto">In particular regarding the frequency, TimesFM expects a categorical indicator valued in {0, 1, 2}:</p>
<ul dir="auto">
<li><strong>0</strong> (default): high frequency, long horizon time series. We recommend to use this for time series up to daily granularity.</li>
<li><strong>1</strong>: medium frequency time series. We recommend to use this for weekly and monthly data.</li>
<li><strong>2</strong>: low frequency, short horizon time series. We recommend to use this for anything beyond monthly, e.g. quarterly or yearly.</li>
</ul>
<p dir="auto">This categorical value should be directly provided with the array inputs. For dataframe inputs, we convert the conventional letter coding of frequencies to our expected categories, that</p>
<ul dir="auto">
<li><strong>0</strong>: T, MIN, H, D, B, U</li>
<li><strong>1</strong>: W, M</li>
<li><strong>2</strong>: Q, Y</li>
</ul>
<p dir="auto">Notice you do <strong>NOT</strong> have to strictly follow our recommendation here. Although this is our setup during model training and we expect it to offer the best forecast result, you can also view the frequency input as a free parameter and modify it per your specific use case.</p>
<p dir="auto">Examples:</p>
<p dir="auto">Array inputs, with the frequencies set to low, medium and high respectively.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import numpy as np
forecast_input = [
    np.sin(np.linspace(0, 20, 100))
    np.sin(np.linspace(0, 20, 200)),
    np.sin(np.linspace(0, 20, 400)),
]
frequency_input = [0, 1, 2]

point_forecast, experimental_quantile_forecast = tfm.forecast(
    forecast_input,
    freq=frequency_input,
)"><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>forecast_input</span> <span>=</span> [
    <span>np</span>.<span>sin</span>(<span>np</span>.<span>linspace</span>(<span>0</span>, <span>20</span>, <span>100</span>))
    <span>np</span>.<span>sin</span>(<span>np</span>.<span>linspace</span>(<span>0</span>, <span>20</span>, <span>200</span>)),
    <span>np</span>.<span>sin</span>(<span>np</span>.<span>linspace</span>(<span>0</span>, <span>20</span>, <span>400</span>)),
]
<span>frequency_input</span> <span>=</span> [<span>0</span>, <span>1</span>, <span>2</span>]

<span>point_forecast</span>, <span>experimental_quantile_forecast</span> <span>=</span> <span>tfm</span>.<span>forecast</span>(
    <span>forecast_input</span>,
    <span>freq</span><span>=</span><span>frequency_input</span>,
)</pre></div>
<p dir="auto"><code>pandas</code> dataframe, with the frequency set to "M" monthly.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import pandas as pd

# e.g. input_df is
#       unique_id  ds          y
# 0     T1         1975-12-31  697458.0
# 1     T1         1976-01-31  1187650.0
# 2     T1         1976-02-29  1069690.0
# 3     T1         1976-03-31  1078430.0
# 4     T1         1976-04-30  1059910.0
# ...   ...        ...         ...
# 8175  T99        1986-01-31  602.0
# 8176  T99        1986-02-28  684.0
# 8177  T99        1986-03-31  818.0
# 8178  T99        1986-04-30  836.0
# 8179  T99        1986-05-31  878.0

forecast_df = tfm.forecast_on_df(
    inputs=input_df,
    freq=&quot;M&quot;,  # monthly
    value_name=&quot;y&quot;,
    num_jobs=-1,
)"><pre><span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>

<span># e.g. input_df is</span>
<span>#       unique_id  ds          y</span>
<span># 0     T1         1975-12-31  697458.0</span>
<span># 1     T1         1976-01-31  1187650.0</span>
<span># 2     T1         1976-02-29  1069690.0</span>
<span># 3     T1         1976-03-31  1078430.0</span>
<span># 4     T1         1976-04-30  1059910.0</span>
<span># ...   ...        ...         ...</span>
<span># 8175  T99        1986-01-31  602.0</span>
<span># 8176  T99        1986-02-28  684.0</span>
<span># 8177  T99        1986-03-31  818.0</span>
<span># 8178  T99        1986-04-30  836.0</span>
<span># 8179  T99        1986-05-31  878.0</span>

<span>forecast_df</span> <span>=</span> <span>tfm</span>.<span>forecast_on_df</span>(
    <span>inputs</span><span>=</span><span>input_df</span>,
    <span>freq</span><span>=</span><span>"M"</span>,  <span># monthly</span>
    <span>value_name</span><span>=</span><span>"y"</span>,
    <span>num_jobs</span><span>=</span><span>-</span><span>1</span>,
)</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Underwater bicycle' propels swimmers forward at superhuman speed (298 pts)]]></title>
            <link>https://newatlas.com/marine/seabike-swimming-propeller/</link>
            <guid>40297748</guid>
            <pubDate>Wed, 08 May 2024 13:13:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/marine/seabike-swimming-propeller/">https://newatlas.com/marine/seabike-swimming-propeller/</a>, See on <a href="https://news.ycombinator.com/item?id=40297748">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I can't say I've seen anything like this "underwater mobility device" before. The idea is simple enough; you extend the Seabike's pole to the appropriate length, then strap it to your waist with a belt. Then you find the pedals with your feet, and start turning the crank, with the waist strap to push against. </p><p>This drives what looks like about a 15-inch (38-cm) propeller. At this point, you start gliding through the water with the splendid, gracious ease of a cruising dugong with an outboard up its bum. You can swim with your arms as well, which creates a surreal visual effect somewhat akin to watching somebody walking along an airport travelator:</p><p>Or you can laze along, arms held out Superman-style. Or indeed, you can angle your nose down, go fully underwater and make like a pedal-powered fish. It's fully compatible with a SCUBA setup if you want to really go nuts down there, although you wouldn't want to take it down too deep and overexert yourself. </p><p>Propellers work both ways, too –&nbsp;so you can also flip the thing upside down, hold the propeller out in front of you, stick some handles on in place of the pedals, and drive the thing with your arms instead. Mind you, this looks a lot less fun. </p><p>Seabike says the prop turns slowly enough that you can safely use it at the local pool – although you'll certainly cop some dirty looks from the Speedo brigade in the fast lane. It's also buoyant, so you won't have to dive to find it if the thing comes off somehow. </p><p>It looks like an incredibly fun way to cover distance in open water, too. Seabike runs its own snorkeling tours out of Cannes, and also sells it with snorkel boards and spear fishing kits. Does it pack down for easy storage? You know it does. </p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f6345bff7244e45f595a795d03a1f64bc" data-video-id="6btiHaTwFKw" data-video-title="SEABIKE PRO +">

    <iframe id="YouTubeVideoPlayer-f6345bff7244e45f595a795d03a1f64bc" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/6btiHaTwFKw?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>SEABIKE PRO +</p>
    
</div><p>Best of all, you can instantly charge this device by eating a hot dog. In an age where everything is going electric, something so simple and mechanical is a welcome change. </p><p>It appears Seabike has been making these things for at least a year, selling for prices starting at EU€290 (US$310). The idea doesn't seem to have received much attention yet, but that strikes us as just a matter of time; it's a simple, clever gadget that looks like a ton of fun.</p><p>Personally, I've never really known what to do with my legs on a swim. Nobody's ever properly convinced me that kicking my feet around is worth the effort, absent a set of swim fins. This jigger, according to the manufacturers, makes you handily quicker than an equivalent swimmer with fins on. Sign me up, I'd love to give one a crack!</p><p>Source: <a href="https://www.seabike.fr/" target="_blank" data-cms-ai="0">Seabike</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[40 years later, a game for the ZX Spectrum will be again broadcast over FM radio (245 pts)]]></title>
            <link>https://www.racunalniski-muzej.si/en/40-years-later-a-game-for-the-zx-spectrum-will-be-once-again-broadcast-over-fm-radio/</link>
            <guid>40296926</guid>
            <pubDate>Wed, 08 May 2024 11:49:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.racunalniski-muzej.si/en/40-years-later-a-game-for-the-zx-spectrum-will-be-once-again-broadcast-over-fm-radio/">https://www.racunalniski-muzej.si/en/40-years-later-a-game-for-the-zx-spectrum-will-be-once-again-broadcast-over-fm-radio/</a>, See on <a href="https://news.ycombinator.com/item?id=40296926">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
	
	
	
		<article id="post-12199">
				
				
			        <p><img width="2560" height="1920" src="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-scaled.jpg" alt="" loading="lazy" srcset="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-scaled.jpg 2560w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-300x225.jpg 300w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-1024x768.jpg 1024w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-768x576.jpg 768w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-1536x1152.jpg 1536w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-2048x1536.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px">			        </p>

		        

			    <div>
		
<p>There were times when Sinclair ZX Spectrum games were copied over the radio waves across Slovenia. <a href="https://radiostudent.si/" title="">Radio Študent</a> broadcast screeching, beeping and whining, which we recorded on tape and played a game a few hours later. Those times are long gone, but we can take a walk through the past today. Radio Študent, which is celebrating its 55th anniversary this week, will invite two members of the legendary Software editorial team to the microphone.</p>



<figure><img src="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/k2-short.gif" alt=""></figure>



<p>Today at 20:30 the guests will be Žiga Turk, who we know as the co-founder of the magazine <a href="https://en.wikipedia.org/wiki/Moj_mikro" title="">Moj Mikro</a>. As one of the pioneers of the Internet in Slovenia, he wrote the Virtual Shareware Library and Wodo. Together with another guest, Matevž Kmet, he also wrote the famous “Kontrabant”, a cult Slovenian text adventure, and its successor “<a href="https://worldofspectrum.org/software?id=0021603" title="">Kontrabant 2</a>“. The talk will take place in the <a href="https://www.racunalniski-muzej.si/en/home-english/" title="">Computer Museum</a> until 21:30.</p>



<p>This will be followed by a nostalgic broadcast of the game Kontrabant 2 via radio waves at the frequency of 89.3 MHz, which will begin around 21:30. Anyone who still has a working Spectrum ZX will then be able to test the game. Those who do not have one can do so at the Computer Museum or online.</p>



<figure><ul><li><figure><img loading="lazy" width="576" height="1024" src="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1-576x1024.jpg" alt="" data-id="12200" data-full-url="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1.jpg" data-link="https://www.racunalniski-muzej.si/?attachment_id=12200" srcset="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1-576x1024.jpg 576w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1-169x300.jpg 169w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1-768x1365.jpg 768w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1-864x1536.jpg 864w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1.jpg 1080w" sizes="(max-width: 576px) 100vw, 576px"></figure></li></ul></figure>
	</div>

			    
			    			    
					
							    
			    			    				    	<!-- end get_the_author_meta -->
		</article>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Who Wants to Be a Thousandaire? (2011) (192 pts)]]></title>
            <link>https://www.damninteresting.com/who-wants-to-be-a-thousandaire/</link>
            <guid>40296744</guid>
            <pubDate>Wed, 08 May 2024 11:27:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.damninteresting.com/who-wants-to-be-a-thousandaire/">https://www.damninteresting.com/who-wants-to-be-a-thousandaire/</a>, See on <a href="https://news.ycombinator.com/item?id=40296744">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                    

                    <p><span>

                                                	<span>Long-Form:</span>
                            <span>Michael Larson had a lot of time and TVs on his hands, and he used them to hack one of his favorite game shows.<br></span>
                            <span>Written by <a href="https://www.damninteresting.com/contributors/alan-bellows/">Alan Bellows</a></span>
                        
                                                	•
                        	<span>Non-Fiction</span>
                        
		        					        		•
			        		<span>September 2011</span>
		        		                    </span>


                </p></div><div>

		            				<article>
											<p>
			© 2011 All Rights Reserved. Do not distribute or repurpose this work without written permission from the copyright holder(s).
	</p>
<p>
	Printed from https://www.damninteresting.com/who-wants-to-be-a-thousandaire/<br>
</p>
												
		                
						
						
											
					<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.734375" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/larson-mug.jpg" alt="" title=""></p></figure>
<p>On the 19th of May 1984, at CBS Television City in Hollywood, a curious air of tension hung over the studio during the taping of the popular game show <em>Press Your Luck</em>. Ordinarily a live studio audience could be counted upon to holler and slap their hands together, but something was keeping them unusually subdued. The object of the audience’s awe was sitting at the center podium on the stage, looking rather unremarkable in his thrift-store shirt and slicked-back graying hair. His name was Michael Larson.</p>
<p>“You’re going to go again?” asked the show’s host Peter Tomarken as Larson gesticulated. Gasps and murmurs punctuated the audience’s cautious applause, and the contestants sitting on either side of Larson clapped in stunned silence. “Michael’s going <em>again</em>,” Tomarken announced incredulously. “We’ve never had anything like this before.”</p>
<p>The scoreboard on Larson’s podium read “$90,351,” an amount unheard of in the history of <em>Press Your Luck</em>. In fact, this total was far greater than any person had ever earned in one sitting on any television game show. With each spin on the randomized “Big Board” Larson took a one-in-six chance of hitting a “Whammy” space that would strip him of all his spoils, yet for 36 consecutive spins he had somehow missed the whammies, stretched the show beyond its 30-minute format, and accumulated extraordinary winnings. Such a streak was astronomically unlikely, but Larson was not yet ready to stop. He was convinced that he knew exactly what he was doing.
</p>
<p>
Michael Larson was born in the small town of Lebanon, Ohio in 1949. Although he was generally regarded as creative and intelligent, he had an inexplicable preference for shady enterprises over gainful employment. One of his earliest exploits was in middle school, where he smuggled candy bars into class and profitably peddled them on the sly. This innocuous operation was just the first in a decreasingly scrupulous series of ventures. One of his later schemes involved opening a checking account with a bank that was offering a promotional $500 to each new customer; he would withdraw the cash at the earliest opportunity, close the account, then repeat the process over and over under assumed names. </p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.740625" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/larson-dinwitty.jpg" alt="Michael Larson and Teresa Dinwitty on vinyl." title="Michael Larson and Teresa Dinwitty on vinyl."></p><figcaption>Michael Larson and Teresa Dinwitty on vinyl.</figcaption></figure>
<p>On another occasion he created a fake business under a family member’s name, hired himself as an employee, then laid himself off to collect unemployment wages.</p>
<p>By 1983 Michael Larson had been married and divorced twice and was living with his girlfriend Teresa Dinwitty. During the summers he operated a Mister Softee ice cream truck, and during the off-season he passed the time poring through piles of periodicals in search of money-making schemes. Michael also spent much of the day with his console television, scanning the airwaves for lucrative opportunities. One day it occurred to him that he could double his information intake by setting a second console TV to beside the first and tuning it to a different channel. Soon he procured a third. Eventually he added a row of smaller televisions atop the three consoles, and yet another row of tubes was later stacked atop that. Now he could watch 12 channels at once.</p>
<p>The warm, buzzing television tumor metastasized into adjacent rooms, filling the house with a goulash of infomercials, news programs, game shows, and advertisements for money-making schemes. Larson watched them in a trance-like state, sometimes throughout the night. Dinwitty would later say of her boyfriend and common-law husband, “He always thought he was smarter than everybody else,” and that he had a “constant yearning for knowledge.” But when visitors asked about the chattering mass of receivers she found it easier to just tell them that Michael was crazy.</p>
<p>One fateful November day in 1983, Peter Tomarken’s dapper countenance appeared on one of Michael’s many monitors. Tomarken was the host of a new game show called <em>Press Your Luck</em> which was giving away more money than any other game shows at the time. What most interested Michael was the game’s “Big Board,” an electronic array of prize boxes which operated by lighting up squares in a rapid and random fashion until the player pressed a big red button to stop the action. The player’s randomly selected box might contain a vacation, a prize, cash rewards, and/or extra spins. But with each spin there was also a one-in-six chance of hitting a Whammy which would cause an animated character to appear on the screen and expunge all of a player’s winnings.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.73125" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/pyl-big-board.jpg" alt="Michael's secret safe spots are the ones that contain $3000 and $2000 prizes at the time this picture was taken." title="Michael's secret safe spots are the ones that contain $3000 and $2000 prizes at the time this picture was taken."></p><figcaption>Michael's secret safe spots are the ones that contain $3000 and $2000 prizes at the time this picture was taken.</figcaption></figure>
<p>Larson invested in a newfangled video cassette recorder and began taping episodes of <em>Press Your Luck</em>. After weeks of painstaking scrutiny Michael realized that the bouncing prize selector did not actually move randomly; it always followed one of five lengthy sequences. This information was only moderately useful due to the rapidly shuffling positions of the prizes and penalties, but his methodical analysis led to another finding. Of the eighteen squares on the Big Board there were two that never had Whammies: #4 and #8. This meant that all a player must do to avoid Whammies⁠—and thus retain his hundreds of dollars in winnings⁠—would be to memorize five interminable series of numbers and develop superhuman reflexes. Giddy with the thrill of discovery, Larson began fine-tuning his timing using his VCR’s pause key as a surrogate big red button.</p>
<p>Six months later, in May 1984, Michael Larson sat beardily in the interview room for the <em>Press Your Luck</em> auditions in Hollywood. His story left few heartstrings unpulled: He explained that he was an unemployed ice cream truck driver. He had borrowed the bus money to get to Hollywood from Ohio because he loved <em>Press Your Luck</em>. He had stopped at a thrift store down the street to buy a 65 cent dress shirt. And he was unable to afford a gift for his six-year-old daughter’s upcoming birthday. Executive producer Bill Carruthers said of Larson’s audition, “He really impressed us. He had charisma.” Contestant coordinator Bob Edwards was uneasy about Larson, but he couldn’t quite articulate why, so Bill overruled him. “I should have listened to Bob,” Carruthers later chuckled.</p>
<p>Taping occurred the following Saturday. Returning champion Ed Long sat on Michael’s right and contestant Janie Litras sat on his left. Host Peter Tomarken made boilerplate game-showey chit-chat with each contestant, and he asked Michael about his ice cream truck. “You’ve kind of OD’d on ice cream, right?” he asked Larson, who agreed. “Well hopefully you won’t OD on money, Michael.”</p>
<p>Michael earned 3 spins on the Big Board in the first question round, giving him 3 opportunities to test the skills he had cultivated over the past six months. The board’s incandescent selector began its distinctive pseudo-random maneuvers. “Come on…big bucks,” Michael chanted, as was customary for players when up against the Big Board. “STOP!” he shouted as he slapped the button with both hands. The selector was stopped on a Whammy in slot #17. Michael shook his head and forced an embarrassed smile, but now he knew exactly how the board was timed with respect to the button. With his second and third spins Michael found his stride. He dropped all pretenses and remained silent as he concentrated on the light bouncing around the big board. Both times he successfully landed on space #4, and he ended the first half of the game with $2,500.</p>
<figure data-embiggen="true"><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.72058823529412" data-lazy-load-src="https://damn-8791.kxcdn.com/wp-content/uploads/2011/09/larson-enlarged.jpg" alt="" title=""></p></figure>
<p>In the second and more lucrative half of the game, Michael managed to acquire seven spins to use on the big board. Since he was in last place he was the first to spin. He positioned his hands over the button with interlocked fingers and impatiently interrupted the host’s banter by shouting, “I’m ready, I’m ready!” Tomarken indulged him, and the light on the big board began bouncing. Again, Larson was silent as he frowned at the board. Fellow contestant Ed Long would later say of Larson during these moments that “he went into a trance.”</p>
<p>Thus began Larson’s inconceivable procession of winning spins. His demeanor alternated between intense concentration and jubilation. The strategy worked even better than he had anticipated due to the large number of Free Spin bonuses that appeared in his safe slots. Host Peter Tomarken became increasingly flabbergasted each time Larson made the “spin again” gesture. $30,000 was considered an extraordinary payoff for one day on any game show at that time, and the likelihood of missing the whammies for more than a dozen spins was considered to be vanishingly small. By his 13th spin Michael had $32,351 and nervous giggles. By his 21st spin he had $47,601 and conspicuous anxiety. But he pressed on.</p>
<p>The <em>Press Your Luck</em> control booth had grown silent as the show’s producers began to realize that Larson was consistently winning on the same two spaces. In a panic, the booth operators called Michael Brockman, CBS’s head of daytime programming. “Something was very wrong,” Brockman said in a <em>TV Guide</em> interview. “Here was this guy from nowhere, and he was hitting the bonus box every time. It was bedlam, I can tell you.” Producers asked if they should stop the show, but Larson did not appear to be breaking any rules so they were forced to allow the episode to play out.</p>
<p>Back on the stage, Ed and Janie clapped incredulously on either side of Michael, still waiting for their turns on the board. Janie let slip a snort of disgust after Michael’s 26th successful spin. Tomarken covered his face with his hand in disbelief as Larson risked almost $75k on his 32nd spin. But Michael’s zen-like concentration was beginning to falter. He paused to set his head on the podium and let out a whimper of exhaustion. Still he motioned to continue. The studio audience worried that he’d hit a whammy and experience an unfortunate reversal of fortune, while the producers in the control booth worried that he wouldn’t.</p>
<p>On his 40th spin Larson’s scoreboard debt-clocked his dollar sign to make room for another digit; he surpassed $100,000. Larson, his shoulders slumped, passed his remaining spins to the bewildered Ed Long. Ed immediately hit a whammy.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.73125" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/tomarken.jpg" alt="Host Peter Tomarken failing to believe what he is seeing." title="Host Peter Tomarken failing to believe what he is seeing."></p><figcaption>Host Peter Tomarken failing to believe what he is seeing.</figcaption></figure>
<p> Michael sat in a twitchy daze as Ed and Janie went through their much more pedestrian turns at the board. But Larson was snapped back to reality when Janie passed 3 of her spins to him. According to the game rules he was obligated to use them. He did not appear pleased.</p>
<p>“I didn’t want them,” Larson joked nervously as the light began bouncing around the big board, yet almost immediately he punched the big red button and landed on $4,000 in slot #4. Janie let out a squeal. The board started again. After a longer than usual delay, Larson hit the button again, landing safely in slot #8. He had just one mandatory spin remaining. The board started flashing, and Larson let out a sigh. “STOP!” he shouted as he slapped the button, but he had pressed it a fraction of a second too soon. Slot #17 was lit, the same slot where he’d hit a whammy on his first spin. As luck would have it, however, the slot contained a trip to the Bahamas. It was over; Michael had won. Larson gave Ed an awkward embrace and offered Janie a firm handshake. In total, Larson won $110,237 in cash and prizes, including two tropical vacations and a sailboat. Reportedly this was more than triple the previous record for winnings in a single episode of a game show.</p>
<p>A clearly discombobulated Peter Tomarken engaged Larson in an impromptu interview after the show. “Why did you keep going?” he asked.</p>
<p>“Well, two things:” Michael replied. “One, it felt right. And second, I still had seven spins and if I passed them, somebody could have done what I did.”</p>
<p>Tomarken was too polite to remark on the ridiculousness of that suggestion. “What are you going to do with the money, Michael?”</p>
<p>“Invest in houses.”</p>
<p>Larson was not allowed to return as champion since he had surpassed CBS’s $25k winnings limit. As all of the perplexed parties parted ways, CBS executives were called to a meeting to dissect the episode frame-by-frame. In spite of their efforts they could find no evidence of wrongdoing or rule-breaking, so after a few weeks they grudgingly mailed Larson his check. Some people at CBS didn’t want the over-extended episode to be released to the public at all, but it was ultimately decided to air it in June as an awkwardly edited two-parter. </p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.590625" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/architect.jpg" alt="" title=""></p></figure>
<p>Executives insisted that the episode never be seen again. In the meantime <em>Press Your Luck</em> paid to add some more sequences to the Big Board to prevent future contestants from mimicking Michael’s strategy.</p>
<p>Upon his return home, neighbors were shocked to learn of “crazy” Michael Larson’s accomplishment. True to his word, he regaled his daughter with expensive birthday gifts and invested some of his spoils in real estate. But his fondness for dicey get-rich-quick deals ensnared him in a Ponzi scheme, and he lost enough money to lose his appetite for houses.</p>
<p>Some months later Michael Larson saw another opportunity to stack the odds in his favor with a dash of ingenuity. He walked into his bank one day and asked to withdraw his entire account balance, but with an unusual stipulation: He wanted as much of the cash as possible in one dollar notes. The bank complied with his unorthodox request, and from there he proceeded to another bank to trade even more of his savings for singles. Over a two week period he converted the $100,000 or so that remained of his personal savings into 100,000 one dollar bills.</p>
<p>The motivation for this aberrant behavior was a contest put on by a local radio station. Each day a disk jockey would read a serial number aloud on the air, and if any listener was able to produce the matching dollar bill they would win $30,000. Michael reasoned that 100,000 one dollar bills was 100,000 opportunities to win the prize, giving him a statistical advantage. And even if his scheme proved fruitless he would just redeposit his money, so he figured he had nothing to lose.</p>
<p>Michael and Teresa spent each day rifling through piles of cash looking for matches, pausing only for such distractions as eating, bathing, and excreting. They soon realized that it was impossible for two people to examine that much money in the allotted time, so Michael redeposited a portion of it. After a few weeks, Michael’s obsession over the contest began to put considerable strain on his relationship with Teresa, and on his relationship with reality. The cash was stashed in kitchen drawers, up the stairs, and on bedroom floors. They kept the bills in burlap sacks, grocery bags, and unkempt stacks. And though his girlfriend would scream and shout, he simply would not take the cash bags out.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.753125" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/larson-stare.jpg" alt="" title=""></p></figure>
<p>One evening, seeking refuge from the endless hours of cash-collating, Michael and Teresa accepted an invitation to attend a Christmas party. When they returned home at about 1:00 am, they found the back door of the house had been brutalized. Apparently the pair had unwittingly left a sizable tip for an unsolicited cleaning service: about $50,000. According to Dinwitty, Michael immediately accused her of being an accessory to the heist. She denied involvement, and police found no evidence of her guilt, but she says that Larson was never convinced. She claimed that Michael would stand and stare at her while she slept, which made her fear for her safety. One day while Michael was away she took $5,000 that he had hidden in a dresser drawer and absconded with the kids. She called him from a hotel to tell him to move out of her house. His only response was, “I want my money back.” He packed his belongings and departed, leaving one wall of the living room blemished and peeling from the heat of his once-formidable tower of televisions.</p>
<p>Police never identified the thieves. In 1994, about 10 years after his pivotal <em>Press Your Luck</em> appearance, Larson was invited to be a guest on ABC’s <em>Good Morning America</em> to discuss the movie <em>Quiz Show</em>. With a raspy voice he unbeardily reminisced about his game show exploits and expressed regret that he was never able to play on Jeopardy, because, he explained, “I think I have figured out some angles on that.” Around that same time he was also interviewed by <em>TV Guide</em> magazine. When asked about the whereabouts of his <em>Press Your Luck</em> winnings, he replied, “It didn’t work out. We had a cash-flow problem, and I lost everything.”</p>
<p>In March of the following year, Michael fled from Ohio with agents from the SEC, IRS, and FBI hot on his heels. He was implicated as one of the architects of a cash-flow solution that operated under the name Pleasure Time Incorporated. It was a pyramid scam selling shares in a fraudulent “American Indian Lottery” which had hoodwinked 20,000 investors out of 3 million dollars. The Pleasure Time flimflam was historic in that it was the first time the SEC pursued a case where the bulk of the fraud took place in newfangled “cyberspace.” Michael Larson was a fugitive from justice for four years until 1999, when he turned up in Apopka, Florida. He had succumbed to throat cancer.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.746875" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/larson-gma.jpg" alt="Michael Larson's appearance on Good Morning America" title="Michael Larson's appearance on Good Morning America"></p><figcaption>Michael Larson's appearance on <em>Good Morning America</em></figcaption></figure>
<p>Michael Larson held the record for the most game-show winnings in a single day until 2006, when it was broken by Vickyann Chrobak-Sadowski on <i>The Price is Right</i>. Larson’s handiwork on <em>Press Your Luck</em> was sufficiently extraordinary that he has become a strange kind of folk hero to some. Others regard him as a cheap huckster or a likable-but-occasionally-creepy crackpot. The real Michael Larson was arguably an amalgam of these qualities. His shenanigans on <em>Press Your Luck</em> are oft described as a “scam,” “scandal,” or a “cheat,” but even the CBS executives ultimately admitted that he had broken nary a rule. In the end, his impressive performance on <em>Press Your Luck</em> may be one of the only honest days of work that Michael Larson ever did.</p>

				

										
										
						<p>
			© 2011 All Rights Reserved. Do not distribute or repurpose this work without written permission from the copyright holder(s).
	</p>
<p>
	Printed from https://www.damninteresting.com/who-wants-to-be-a-thousandaire/<br>
</p>
						<p>
							<i>Since you enjoyed our work enough to print it out, and read it clear to the end, would you consider donating a few dollars at https://www.damninteresting.com/donate</i> ?
						</p>
									</article>
			
		            	            		
	            	
		            			

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The search for easier safe systems programming (176 pts)]]></title>
            <link>https://www.sophiajt.com/search-for-easier-safe-systems-programming/</link>
            <guid>40295624</guid>
            <pubDate>Wed, 08 May 2024 08:26:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sophiajt.com/search-for-easier-safe-systems-programming/">https://www.sophiajt.com/search-for-easier-safe-systems-programming/</a>, See on <a href="https://news.ycombinator.com/item?id=40295624">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>I've been involved in the Rust project in some form or another since 2016, and it's a language I'm very comfortable using. Many Rust programmers could <a href="https://blog.rust-lang.org/2024/02/19/2023-Rust-Annual-Survey-2023-results.html">say the same</a>. But, if we take a step back and are honest with ourselves, we'd admit that the road to getting to that level of comfort was difficult.</p>
<p>I taught Rust professionally for two years. Watching the faces of people trying to learn Rust for the first time reminded me just how hard this language is to learn.</p>
<p>After two years of that, I wanted to answer a question I wasn't entirely sure had an answer: <em>Is it possible to make an easy-to-use, easy-to-learn, and easy-to-teach safe systems language?</em> Could I put my career working on programming languages (TypeScript, Rust, Nushell, etc) to use and find a solution?</p>
<h2 id="enter-june">Enter June</h2>
<p>For the last year and a half, I and my recently-added collaborator Jane Losare-Lusby have been working in secret on a safe systems language that could be learned about as quickly as one can learn Go. I think we might have something worth exploring.</p>
<h2 id="changing-how-we-think-of-memory">Changing how we think of memory</h2>
<p>In Rust, we think of each piece of memory as having its own lifetime. Each of these lifetimes must be tracked, sometimes leading to rather complex code, complex error messages, and/or complex mental models of what is happening. The complexity of course comes with the benefit of being highly precise about each and every piece of memory and its reclamation.</p>
<p>Using a Rust example:</p>
<pre><code><span>struct </span><span>Node {
    </span><span>data1</span><span>: &amp;Data
    data2: &amp;Data
    data3: &amp;Data
}
</span></code></pre>
<p>Rust developers will spot right away that this is an incomplete example. We need two more things: Lifetime Parameters and Lifetime Annotations. Adding those, we get:</p>
<pre><code><span>struct </span><span>Node&lt;</span><span>'a</span><span>, </span><span>'b</span><span>, </span><span>'c</span><span>&gt; {
    </span><span>data1</span><span>: &amp;</span><span>'a</span><span> Data
    data2: &amp;</span><span>'b</span><span> Data
    data3: &amp;</span><span>'c</span><span> Data
}
</span></code></pre>
<p>The concept count for this example ends up being pretty substantial. Counting them off, we get:</p>
<ul>
<li>Lifetimes</li>
<li>Lifetime annotations</li>
<li>Lifetime parameters</li>
<li>Ownership and borrowing</li>
<li>Generics</li>
</ul>
<p>When I showed examples like this to my class when I taught Rust, I had to walk them through each of those concepts first before I could show the full example.</p>
<p>The question then is: can we make this easier?</p>
<h2 id="what-if-memory-was-grouped">What if memory was grouped?</h2>
<p>What if instead of having to track every piece of memory's lifetime separately, we let groups of related allocations share a lifetime?</p>
<p>Effectively, this would mean that a data structure, like a linked list, would have a pointer pointing to the head which has a lifetime, and then every node in the list you can reach from that head has the same lifetime.</p>
<p>There are some benefits to this approach, as well as a few drawbacks. Let's take a look at the benefits first.</p>
<h2 id="benefits-of-grouped-allocations">Benefits of grouped allocations</h2>
<p>Exploring grouped allocations, we noticed some immediate benefits. The first is that we could treat all user-defined values as pointers, and these pointers could also represent their own lifetimes (without needing lifetime parameters). This makes the code feel quite a bit lighter:</p>
<pre><code><span>struct </span><span>Node {
    </span><span>data</span><span>: </span><span>i64</span><span>
    next: Node?
}

</span><span>let</span><span> n = new Node(data: </span><span>123</span><span>, next: none)
</span></code></pre>
<p>Since all user data is pointers, we can use the name of the type to mean "pointer to this structured data".</p>
<p>The next thing we noticed is that both lifetimes and inference for lifetimes becomes significantly simpler.</p>
<p>Let's take a variation of the example above:</p>
<pre><code><span>struct </span><span>Node {
    </span><span>data</span><span>: </span><span>i64</span><span>
    next: Node?
}

fun </span><span>do_this</span><span>() {
    </span><span>let</span><span> n = new Node(data: </span><span>123</span><span>, next: none)
}
</span></code></pre>
<p>We can infer that the allocation that creates <code>new Node(...)</code> has a lifetime and what it is. Because this allocation never "escapes" the function - that is, it never leaves the function in any way - then we can call its lifetime "Local".</p>
<p>As we'll find out, each of the lifetime possibities is a readable name that we can show the user in error messages. It also makes things significantly easier to teach.</p>
<p>Let's look at another example to see a different lifetime.</p>
<pre><code><span>struct </span><span>Stats {
    </span><span>age</span><span>: </span><span>i64
</span><span>}

</span><span>struct </span><span>Employee {
    </span><span>name</span><span>: c_string,
    </span><span>stats</span><span>: Stats,
}

fun </span><span>set_stats</span><span>(</span><span>mut</span><span> employee: Employee) {
    employee.stats = new Stats(age: </span><span>33</span><span>)
}

fun </span><span>main</span><span>() {
    </span><span>mut</span><span> employee = new Employee(name: c"</span><span>Soph</span><span>", stats: new Stats(age: </span><span>22</span><span>))
    </span><span>set_stats</span><span>(employee)
    </span><span>println</span><span>(employee.stats.age)
}
</span></code></pre>
<p>A bit of a longer example this time, but let's focus on this function:</p>
<pre><code><span>fun </span><span>set_stats</span><span>(</span><span>mut</span><span> employee: Employee) {
    employee.stats = new Stats(age: </span><span>33</span><span>)
}
</span></code></pre>
<p>What's the lifetime of this <code>new Stats(..)</code> allocation? In this example, we do see the new pointer escape the function via a parameter. We can also give this a readable lifetime: <code>Param(employee)</code></p>
<p>In all, we have three lifetimes an allocation can have:</p>
<ul>
<li>Local</li>
<li>Param(xxxx)</li>
<li>Return</li>
</ul>
<h2 id="any-data-structure-you-want">Any data structure you want</h2>
<p>Another big advantage of grouping our allocations is that we no longer have to worry about a drop order. This means we can think of the whole thing as dropping all at once. For large structures, that can be a speed-up over languages with a required drop order.</p>
<p>Additionally, we get another major benefit. We can now create arbitrary data structures.</p>
<pre><code><span>struct </span><span>Node {
    </span><span>data</span><span>: </span><span>i64</span><span>
    next: Node?
}

</span><span>mut</span><span> node3 = new Node(data: </span><span>3</span><span>, next: none)
</span><span>let</span><span> node2 = new Node(data: </span><span>2</span><span>, next: node3)
</span><span>let</span><span> node1 = new Node(data: </span><span>1</span><span>, next: node2)

node3.next = node1
</span></code></pre>
<p>And just like that, we've made a circular linked list. Creating a similar example in Rust is certainly more of a challenge.</p>
<p>But, something fishy is going on here.</p>
<p>To make the above work, we're using shared, mutable pointers. This is explicitly forbidden in Rust. Why is it okay here?</p>

<p>Rust disallows holding two mutable references to the same memory location and for good reason. Well, multiple reasons actually.</p>
<p>First, having two copies of a mutable pointer where two separate threads each hold a copy means we have the possibility for a race condition. This can leave us with incoherent data that's difficult to debug.</p>
<p>Second, even if these two multiple pointers are limited to the same thread, we get what we might call "spooky action at a distance". The modification of one pointer is then visible to the holder of the other pointer, which might be far away from the source of the mutation.</p>
<p>For us to reasonably use shared, mutable pointers, we need to tame both of these. The first issue, the race condition, is easy enough: we prevent sending shared, mutable pointers between threads. This limits them to a single thread.</p>
<p>The second issue is decidedly harder. There have been many attempts at ways of handling this through rules enforced by the type system.</p>
<p>In June, we're trying something a bit different. We'll let developers use shared, mutable pointers, but then offer a "carrot" to opt-in to restrictions around using them. The carrot ends up pulling from a classic technique of software engineering: encapsulation.</p>
<h2 id="the-full-power-of-encapsulation">The full power of encapsulation</h2>
<p>In traditional encapsulation, programmers make a kind of "best effort" to hide implementation details from the world around them. Keeping private state private grants the benefits of better code reuse, ease of updating implementation details, and more.</p>
<p>But as often is the case, if that kind of rule isn't enforced, over time APIs get designed where internal implementation details leak out.</p>
<p>Something very interesting happens if we don't allow this to happen. If an encapsulation can be checked by the compiler, and the compiler enforces that no private details leak, we have what you might call "full encapsulation".</p>
<p>These kinds of encapsulations wouldn't allow any aliasing of pointers into them. They'd have their internal pointers fully isolated from the rest of the program.</p>
<p>Once we have this, some new capabilities start opening up:</p>
<ul>
<li>We can "fence off" our shared, mutable pointers, making it possible to create single-owner encapsulations that can be sent safely between threads.</li>
<li>We can lean people in the direction of cleaner API design, as now we have a way to truly keep private implementation details private.</li>
<li>We can handle some of the drawbacks of grouped allocations.</li>
</ul>
<p>What kind of drawbacks, you might ask? It's high time we talked about them.</p>
<h2 id="drawbacks-of-grouped-allocations">Drawbacks of grouped allocations</h2>
<p>If we go back to our earlier example and look carefully, we'll notice something:</p>
<pre><code><span>fun </span><span>set_stats</span><span>(</span><span>mut</span><span> employee: Employee) {
    employee.stats = new Stats(age: </span><span>33</span><span>)
}

fun </span><span>main</span><span>() {
    </span><span>mut</span><span> employee = new Employee(name: c"</span><span>Soph</span><span>", stats: new Stats(age: </span><span>22</span><span>))
    </span><span>set_stats</span><span>(employee)
    </span><span>println</span><span>(employee.stats.age)
}
</span></code></pre>
<p>The question is: what happened to the <code>new Stats(age: 22)</code> allocation?</p>
<p>Remembering that June is a systems language, we can't say "the garbage collector handled it" because we have no garbage collector. Nor can we say "the refcount hit zero, so we reclaimed it" as we don't use refcounting. As a systems language, we can't allow hidden or difficult-to-predict overhead to happen.</p>
<p>It's not actually leaked either, as even the memory it occupies will be reclaimed once the entire group is reclaimed. For all intents and purposes, though, it's lost to the user until the group is no longer live. It's a kind of "memory bloat" that happens if we group our allocations.</p>
<p>To handle this, we'll need some way of recycling that memory. I say "recycling" specifically because in June we can't free the memory, as the group is treated together as a single entity where all the allocations in the group are freed at once. If we instead recycle the memory, we can reuse that same memory while the group is live.</p>
<p>Techniques to do this have been around for decades, and often people use "free lists" to keep a list of nodes that have been recycled, so they can be reused when the next allocation happens.</p>
<p>The problem with free lists is that they aren't safe. If you're not careful, you'll create a security vulnerability and/or an incredibly hard bug to find.</p>
<p>Instead, we need to build in a safe way of recycling memory into the language.</p>
<h2 id="safe-memory-recycling">Safe memory recycling</h2>
<p>Using the idea of full encapsulation from earlier, we can create "fenced in" sets of pointers that we know aren't shared with the rest of the world. Once we have them, it's possible to track the pointers inside. These pointers can get a "copy count", so we know how many copies are live at any point in time (not dissimilar from a refcount, though this has no automatic reclamation).</p>
<p>Once we have a copy count for each internal pointer, we can give developers a built-in <code>recycle</code> command.</p>
<pre><code><span>let x = new Foo()
recycle x
</span></code></pre>
<p>Recycling would start at the given pointer and would check the pointers reachable from it. Each pointer it finds that it can recycle would go into the safe free list.</p>
<p>You might wonder "why not do this automatically?". There are a couple reasons:</p>
<ul>
<li>The operation is linear time based on your transitively-reachable pointers. This means you may incur a noticeable overhead when recycling</li>
<li>Because of the first point, it's important to make places where this occurs visible</li>
</ul>
<p>If this sounds like a kind of manual garbage collection, you're right. My collaborator Jane calls this "semi-automatic" memory reclamation. You ask once, and when you ask you get a kind of highly focused mark and sweep for that single pointer and the pointers reachable from it.</p>
<p><em>Note: this feature is not yet in the reference compiler. We're hoping to implement it in the coming weeks.</em></p>
<h2 id="more-work-ahead">More work ahead</h2>
<p>We have a way of simplifying lifetimes, making for readable code that people from various languages should be able to understand and use. We can also give clear, easy-to-understand lifetime errors when they arise.</p>
<p>Having safe memory recycling gives us a way to keep groups and still offer things like <code>delete</code> in a linked list abstraction. It's convenient but not so automatic that we lose the visibility into the costs of memory management.</p>
<p>That said, there are still some challenge ahead that will need to be solved in the language design and tooling. For example, how do you know when the program is bloating memory? We'll need some way of doing a memory trace when the program is running to detect this and warn the developer.</p>
<p>I see this in a way as a more incremental/prototype-friendly way of development. June is always memory safe, but the first version of a program may not be as efficient as it could be in terms of memory usage. That's a process we often go through as developers. First, we "make it work" before we "make it good".</p>
<p>In June, we keep it lightweight as we keep your programs memory safe, and then we provide tools and support for incrementally improving code.</p>
<h2 id="future-possibilities">Future possibilities</h2>
<h3 id="relationship-to-rust">Relationship to Rust</h3>
<p>June has a real opportunity to be a good complement to Rust. Rust's focus on embedded and system's development is a core strength. June, on the other hand, has a lean towards application development with a system's approach. This lets both co-exist and offer safe systems programming to a larger audience.</p>
<p>An even better end state requires Rust to have a stable ABI. Once it does, June will be able to call into Rust crates to get the benefits of Rust's substantial crate ecosystem. We're looking forward to collaborating on this in the future.</p>
<h3 id="going-beyond-oop">Going beyond OOP</h3>
<p>OOP has for decades been the way many applications are written, but it's not without its flaws. Many OOP languages allow programmers to freely break good rules of thumb, like the Liskov substitution principle, or to create a mess of interwoven code between parent and child classes that's difficult to maintain.</p>
<p>We're currently investigating other ways of making code reuse easier, more modular, and more composible. We're not quite ready to talk about this, though we hope to soon.</p>

<p>Over the years, there have been a <a href="https://verdagon.dev/grimoire/grimoire">number of memory management techniques tried</a>, including many that lie outside of the ones commonly found in languages today. We'd like to explore these more deeply to see which, if any, may help June.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We had the help of dozens of experts in various fields as we brainstormed the initial design for June, and for their contributions, we're thankful. We'd especially like to thank the collaborators who went above and beyond with their time across multiple brainstorming sessions to help June grow to where it is.</p>
<ul>
<li>Andreas Kling</li>
<li>Doug Gregor</li>
<li>Jason Turner</li>
<li>Mads Torgersen</li>
<li>Mae Milano</li>
<li>Steve Francia</li>
</ul>
<p>Also, special thanks to our private beta testers for testing out June and giving us feedback.</p>
<h2 id="checking-it-out">Checking it out</h2>
<p>Documentation on the June language and the June reference compiler are now available via the <a href="https://github.com/sophiajt/june">June repo</a>.</p>
<p>Please note: the reference compiler is pre-alpha quality.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XLSTM: Extended Long Short-Term Memory (167 pts)]]></title>
            <link>https://arxiv.org/abs/2405.04517</link>
            <guid>40294650</guid>
            <pubDate>Wed, 08 May 2024 05:28:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2405.04517">https://arxiv.org/abs/2405.04517</a>, See on <a href="https://news.ycombinator.com/item?id=40294650">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2405.04517">View PDF</a></p><blockquote>
            <span>Abstract:</span>In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Maximilian Beck [<a href="https://arxiv.org/show-email/ed28509f/2405.04517">view email</a>]      <br>    <strong>[v1]</strong>
        Tue, 7 May 2024 17:50:21 UTC (1,455 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Rules Apple Illegally Interrogated Staff and Confiscated Union Flyers (479 pts)]]></title>
            <link>https://www.forbes.com/sites/antoniopequenoiv/2024/05/06/us-labor-board-rules-apple-illegally-interrogated-staff-and-confiscated-union-flyers/</link>
            <guid>40294630</guid>
            <pubDate>Wed, 08 May 2024 05:25:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.forbes.com/sites/antoniopequenoiv/2024/05/06/us-labor-board-rules-apple-illegally-interrogated-staff-and-confiscated-union-flyers/">https://www.forbes.com/sites/antoniopequenoiv/2024/05/06/us-labor-board-rules-apple-illegally-interrogated-staff-and-confiscated-union-flyers/</a>, See on <a href="https://news.ycombinator.com/item?id=40294630">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Topline</h2>
<p>The National Labor Relations Board <a href="https://www.nlrb.gov/case/02-CA-295979" rel="nofollow noopener noreferrer" target="_blank" title="https://www.nlrb.gov/case/02-CA-295979" data-ga-track="ExternalLink:https://www.nlrb.gov/case/02-CA-295979" aria-label="ruled">ruled</a> Monday that Apple illegally questioned staff of its World Trade Center store in New York City in 2022, affirming findings from a judge who determined employees were specifically questioned over <a href="https://www.bloomberg.com/news/articles/2023-06-21/apple-illegally-interrogated-staff-about-union-judge-rules" rel="nofollow noopener noreferrer" target="_blank" title="https://www.bloomberg.com/news/articles/2023-06-21/apple-illegally-interrogated-staff-about-union-judge-rules" data-ga-track="ExternalLink:https://www.bloomberg.com/news/articles/2023-06-21/apple-illegally-interrogated-staff-about-union-judge-rules" aria-label="their pro-union sympathies">their pro-union sympathies</a>.</p>
<figure role="presentation"><figcaption><fbs-accordion current="-1"><p>Apple has not received any punishment or been ordered to pay damages by the board for the <span data-ga-track="caption expand">... [+]</span><span> violations. (Photo by Gary Hershorn/Getty Images)</span></p></fbs-accordion><small>Getty Images</small></figcaption></figure> 

<h2>Key Facts</h2>
<div>
 <div>
  <p>The board affirmed the decision of administrative law Judge Lauren Esposito, who ruled last year that Apple illegally stopped workers from placing union flyers on a table in the break room of the World Trade Center store, confiscated the flyers and interrogated staff over their “protected concerted activity.”</p>
  
 </div>
 <div>
  <p>Esposito ordered Apple cease and desist from illegally questioning workers about union matters in addition to confiscating union flyers from the store’s employee break room.</p>
  
 </div>
 <div>
  <p>Monday’s ruling is the board’s first decision against Apple, according to <a href="https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" rel="nofollow noopener noreferrer" target="_blank" title="https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" data-ga-track="ExternalLink:https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" aria-label="Bloomberg">Bloomberg</a>, which first reported the ruling and cited agency spokesperson Kayla Blado.</p>
  
 </div>
 <p>The board cannot impose fines or direct punishments against Apple for its violations.</p>
 <p>Apple didn’t immediately respond to Forbes’ request for comment.</p>
</div>
<p><em>Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up </em><a href="https://joinsubtext.com/forbes" rel="nofollow noopener noreferrer" target="_blank" title="https://joinsubtext.com/forbes" data-ga-track="ExternalLink:https://joinsubtext.com/forbes" aria-label="here"><em data-ga-track="ExternalLink:https://joinsubtext.com/forbes">here</em></a><em>.</em></p>


<h2>Key Background</h2>
<p>Other cases against Apple are still pending, according to Bloomberg, which noted a case in which a National Labor Relations Board member accused the company of illegally excluding unionized workers from certain benefits. Several Apple stores have <a href="https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" rel="nofollow noopener noreferrer" target="_blank" title="https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" data-ga-track="ExternalLink:https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" aria-label="moved to unionize">moved to unionize</a> in recent years including ones in Short Hills, New Jersey, Oklahoma City and Towson, Maryland, with the latter two locations successfully establishing a union. Apple employees outside of the World Trade Center store staffers have also run into opposition while seeking to unionize. The National Labor Relations Board found in late 2022 that Apple hosted mandatory <a href="https://www.bloomberg.com/news/articles/2022-12-05/apple-s-anti-union-tactics-in-atlanta-were-illegal-us-officials-say" rel="nofollow noopener noreferrer" target="_blank" title="https://www.bloomberg.com/news/articles/2022-12-05/apple-s-anti-union-tactics-in-atlanta-were-illegal-us-officials-say" data-ga-track="ExternalLink:https://www.bloomberg.com/news/articles/2022-12-05/apple-s-anti-union-tactics-in-atlanta-were-illegal-us-officials-say" aria-label="anti-union meetings">anti-union meetings</a> at an Atlanta store where management made coercive statements against employees.</p>


<h2>Further Reading</h2>
<p><a href="https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" rel="nofollow noopener noreferrer" target="_blank" title="https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" data-ga-track="ExternalLink:https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" aria-label="Apple Illegally Interrogated NYC Retail Staff, US Labor Board Rules">Apple Illegally Interrogated NYC Retail Staff, US Labor Board Rules</a> (Bloomberg)</p>
<p><a href="https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" rel="nofollow noopener noreferrer" target="_blank" title="https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" data-ga-track="ExternalLink:https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" aria-label="Apple Store employees in New Jersey are trying to unionize">Apple Store employees in New Jersey are trying to unionize</a> (The Verge)</p>
</div><div><p><span>Follow me on&nbsp;</span><a href="https://www.twitter.com/pequeno04" rel="nofollow noopener noreferrer" target="_blank">Twitter</a>&nbsp;or&nbsp;<a href="https://www.linkedin.com/in/antonio-peque%C3%B1o-iv/" rel="nofollow noopener noreferrer" target="_blank">LinkedIn</a>.&nbsp;<span>Send me a secure&nbsp;<a href="https://www.forbes.com/tips/" rel="nofollow noopener noreferrer" target="_blank">tip</a></span>.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The C++ Iceberg (115 pts)]]></title>
            <link>https://fouronnes.github.io/cppiceberg/</link>
            <guid>40294555</guid>
            <pubDate>Wed, 08 May 2024 05:12:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fouronnes.github.io/cppiceberg/">https://fouronnes.github.io/cppiceberg/</a>, See on <a href="https://news.ycombinator.com/item?id=40294555">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Why use ECC? (2015) (154 pts)]]></title>
            <link>https://danluu.com/why-ecc/</link>
            <guid>40293943</guid>
            <pubDate>Wed, 08 May 2024 03:02:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danluu.com/why-ecc/">https://danluu.com/why-ecc/</a>, See on <a href="https://news.ycombinator.com/item?id=40293943">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>Jeff Atwood, perhaps the most widely read programming blogger, has a post that makes <a rel="nofollow" href="http://blog.codinghorror.com/to-ecc-or-not-to-ecc/">a case against using ECC memory</a>. My read is that his major points are:</p> <ol> <li>Google didn't use ECC when they built their servers in 1999</li> <li>Most RAM errors are hard errors and not soft errors</li> <li>RAM errors are rare because hardware has improved</li> <li>If ECC were actually important, it would be used everywhere and not just servers. Paying for optional stuff like this is "awfully enterprisey"</li> </ol>  <p>Let's take a look at these arguments one by one:</p> <h2 id="1-google-didn-t-use-ecc-in-1999">1. Google didn't use ECC in 1999</h2> <p>Not too long after Google put these non-ECC machines into production, they realized this was a serious error and not worth the cost savings. If you think cargo culting what Google does is a good idea because it's Google, here are some things you might do:</p> <h4 id="a-put-your-servers-into-shipping-containers">A. Put your servers into shipping containers.</h4> <p>Articles are still written today about what a great idea this is, even though this was an experiment at Google that was deemed unsuccessful. Turns out, even Google's experiments don't always succeed. In fact, their propensity for “moonshots” in the early days meannt that they had more failed experiments that most companies. Copying their failed experiments isn't a particularly good strategy.</p> <h4 id="b-cause-fires-in-your-own-datacenters">B. Cause fires in your own datacenters</h4> <p>Part of the post talks about how awesome these servers are:</p> <blockquote> <p>Some people might look at these early Google servers and see an amateurish fire hazard. Not me. I see a prescient understanding of how inexpensive commodity hardware would shape today's internet. I felt right at home when I saw this server; it's exactly what I would have done in the same circumstances</p> </blockquote> <p>The last part of that is true. But the first part has a grain of truth, too. When Google started designing their own boards, one generation had a regrowth<sup id="fnref:R"><a rel="footnote" href="#fn:R">1</a></sup> issue that caused a non-zero number of fires.</p> <p>BTW, if you click through to Jeff's post and look at the photo that the quote refers to, you'll see that the boards have a lot of flex in them. That caused problems and was fixed in the next generation. You can also observe that the cabling is quite messy, which also caused problems, and was also fixed in the next generation. There were other problems as well. <abbr title="When someone looks in the answer key and says, 'I would've come up with that', that's often plausible when their answer is perfect. But when they say that after seeing a specific imperfect answer, it's a bit less plausible that they'd reproduce the exact same mistakes">Jeff's argument here appears to be that, if he were there at the time, he would've seen the exact same opportunities that early Google enigneers did, and since Google did this, it must've been the right thing even if it doesn't look like it. But, a number of things that make it look like not the right thing actually made it not the right thing.</abbr></p> <h4 id="c-make-servers-that-injure-your-employees">C. Make servers that injure your employees</h4> <p>One generation of Google servers had infamously sharp edges, giving them the reputation of being made of “razor blades and hate”.</p> <h4 id="d-create-weather-in-your-datacenters">D. Create weather in your datacenters</h4> <p>From talking to folks at a lot of large tech companies, it seems that most of them have had a climate control issue resulting in clouds or fog in their datacenters. You might call this a clever plan by Google to reproduce Seattle weather so they can poach MS employees. Alternately, it might be a plan to create literal cloud computing. Or maybe not.</p> <p>Note that these are all things Google tried and then changed. Making mistakes and then fixing them is common in every successful engineering organization. If you're going to cargo cult an engineering practice, you should at least cargo cult current engineering practices, not <a href="https://danluu.com/butler-lampson-1999/">something that was done in 1999</a>.</p> <p>When Google used servers without ECC back in 1999, they found a number of symptoms that were ultimately due to memory corruption, including a search index that returned effectively random results to queries. The actual failure mode here is instructive. I often hear that it's ok to ignore ECC on these machines because it's ok to have errors in individual results. But even when you can tolerate occasional errors, ignoring errors means that you're exposing yourself to total corruption, unless you've done a very careful analysis to make sure that a single error can only contaminate a single result. In research that's been done on filesystems, it's been repeatedly shown that despite making valiant attempts at creating systems that are robust against a single error, it's extremely hard to do so and basically every heavily tested filesystem can have a massive failure from a single error (<a href="https://danluu.com/file-consistency/">see the output of Andrea and Remzi's research group at Wisconsin if you're curious about this</a>). I'm not knocking filesystem developers here. They're better at that kind of analysis than 99.9% of programmers. It's just that this problem has been repeatedly shown to be hard enough that humans cannot effectively reason about it, and automated tooling for this kind of analysis is still far from a push-button process. In their book on <a href="http://www.morganclaypool.com/doi/abs/10.2200/S00516ED2V01Y201306CAC024">warehouse scale computing</a>, Google discusses error correction and detection and ECC is cited as their slam dunk case for when it's obvious that you should use hardware error correction<sup id="fnref:P"><a rel="footnote" href="#fn:P">2</a></sup>.</p> <p>Google has great infrastructure. From what I've heard of the infra at other large tech companies, Google's sounds like the best in the world. But that doesn't mean that you should copy everything they do. Even if you look at their good ideas, it doesn't make sense for most companies to copy them. They <a href="https://danluu.com/intel-cat/">created a replacement for Linux's work stealing scheduler that uses both hardware run-time information and static traces to allow them to take advantage of new hardware in Intel's server processors that lets you dynamically partition caches between cores</a>. If used across their entire fleet, that could easily save Google more money in a week than stackexchange has spent on machines in their entire history. Does that mean you should copy Google? No, not unless you've already captured all the lower hanging fruit, which includes things like making sure that your core infrastructure is written in highly optimized C++, not Java or (god forbid) Ruby. And the thing is, for the vast majority of companies, writing in a language that imposes a 20x performance penalty is a totally reasonable decision.</p> <h2 id="2-most-ram-errors-are-hard-errors">2. Most RAM errors are hard errors</h2> <p>The case against ECC quotes <a href="http://selse.org//images/selse_2012/Papers/selse2012_submission_4.pdf">this section of a study on DRAM errors</a> (the bolding is Jeff's):</p> <blockquote> <p>Our study has several main findings. First, we find that approximately <strong>70% of DRAM faults are recurring (e.g., permanent) faults, while only 30% are transient faults.</strong> Second, we find that large multi-bit faults, such as faults that affects an entire row, column, or bank, constitute over 40% of all DRAM faults. Third, we find that almost 5% of DRAM failures affect board-level circuitry such as data (DQ) or strobe (DQS) wires. Finally, we find that chipkill functionality reduced the system failure rate from DRAM faults by 36x.</p> </blockquote> <p>This seems to betray a lack of understanding of the implications of this study, as this quote doesn't sound like an argument against ECC; it sounds like an argument for "chipkill", a particular class of ECC. Putting that aside, Jeff's post points out that hard errors are twice as common as soft errors, and then mentions that they run memtest on their machines when they get them. First, a 2:1 ratio isn't so large that you can just ignore soft errors. Second the post implies that Jeff believes that hard errors are basically immutable and can't surface after some time, which is incorrect. You can think of electronics as wearing out just the same way mechanical devices wear out. The mechanisms are different, but the effects are similar. In fact, if you compare reliability analysis of chips vs. other kinds of reliability analysis, you'll find they often use the same families of distributions to model failures. And, if hard errors were immutable, they would generally get caught in testing by the manufacturer, who can catch errors much more easily than consumers can because they have hooks into circuits that let them test memory much more efficiently than you can do in your server or home computer. Third, Jeff's line of reasoning implies that ECC can't help with detection or correction of hard errors, which is not only incorrect but directly contradicted by the quote.</p> <p>So, how often are you going to run memtest on your machines to try to catch these hard errors, and how much data corruption are you willing to live with? One of the key uses of ECC is not to correct errors, but to signal errors so that hardware can be replaced before silent corruption occurs. No one's going to consent to shutting down everything on a machine every day to run memtest (that would be more expensive than just buying ECC memory), and even if you could convince people to do that, it won't catch as many errors as ECC will.</p> <p>When I worked at a company that owned about 1000 machines, we noticed that we were getting strange consistency check failures, and after maybe half a year we realized that the failures were more likely to happen on some machines than others. The failures were quite rare, maybe a couple times a week on average, so it took a substantial amount of time to accumulate the data, and more time for someone to realize what was going on. Without knowing the cause, analyzing the logs to figure out that the errors were caused by single bit flips (with high probability) was also non-trivial. We were lucky that, as a side effect of the process we used, the checksums were calculated in a separate process, on a different machine, at a different time, so that an error couldn't corrupt the result and propagate that corruption into the checksum. If you merely try to protect yourself with in-memory checksums, there's a good chance you'll perform a checksum operation on the already corrupted data and compute a valid checksum of bad data unless you're doing some really fancy stuff with calculations that carry their own checksums (and if you're that serious about error correction, you're probably using ECC regardless). Anyway, after completing the analysis, we found that memtest couldn't detect any problems, but that replacing the RAM on the bad machines caused a one to two order of magnitude reduction in error rate. Most services don't have this kind of checksumming we had; those services will simply silently write corrupt data to persistent storage and never notice problems until a customer complains.</p> <h2 id="3-due-to-advances-in-hardware-manufacturing-errors-are-very-rare">3. Due to advances in hardware manufacturing, errors are very rare</h2> <p>Jeff says</p> <blockquote> <p>I do seriously question whether ECC is as operationally critical as we have been led to believe [for servers], and I think the data shows modern, non-ECC RAM is already extremely reliable ... Modern commodity computer parts from reputable vendors are amazingly reliable. And their trends show from 2012 onward essential PC parts have gotten more reliable, not less. (I can also vouch for the improvement in SSD reliability as we have had zero server SSD failures in 3 years across our 12 servers with 24+ drives ...</p> </blockquote> <p>and quotes a study.</p> <p>The data in the post isn't sufficient to support this assertion. Note that since RAM usage has been increasing and continues to increase at a fast exponential rate, RAM failures would have to decrease at a greater exponential rate to actually reduce the incidence of data corruption. Furthermore, as chips continue shrink, features get smaller, making the kind of wearout issues discussed in “2” more common. For example, at 20nm, a DRAM capacitor might hold something like 50 electrons, and that number will get smaller for next generation DRAM and things continue to shrink.</p> <p>The <a href="http://selse.org//images/selse_2012/Papers/selse2012_submission_4.pdf">2012 study that Atwood quoted</a> has this graph on corrected errors (a subset of all errors) on ten randomly selected failing nodes (6% of nodes had at least one failure):</p> <p><img src="https://danluu.com/images/why-ecc/one_month_ecc_errors.png"></p> <p>We're talking between 10 and 10k errors for a typical node that has a failure, and that's a cherry-picked study from a post that's arguing that you don't need ECC. Note that the nodes here only have 16GB of RAM, which is an order of magnitude less than modern servers often have, and that this was on an older process node that was less vulnerable to noise than we are now. For anyone who's used to dealing with reliability issues and just wants to know the FIT rate, the study finds a FIT rate of between 0.057 and 0.071 faults per Mbit (which, contra Atwood's assertion, is not a shockingly low number). If you take the most optimistic FIT rate, .057, and do the calculation for a server without much RAM (here, I'm using 128GB, since the servers I see nowadays typically have between 128GB and 1.5TB of RAM)., you get an expected value of .057 * 1000 * 1000 * 8760 / 1000000000 = .5 faults per year per server. Note that this is for faults, not errors. From the graph above, we can see that a fault can easily cause hundreds or thousands of errors per month. Another thing to note is that there are multiple nodes that don't have errors at the start of the study but develop errors later on. So, in fact, the cherry-picked study that Jeff links contradicts Jeff's claim about reliability.</p> <p>Sun/Oracle famously ran into this a number of decades ago. Transistors and DRAM capacitors were getting smaller, much as they are now, and memory usage and caches were growing, much as they are now. Between having smaller transistors that were less resilient to transient upset as well as more difficult to manufacture, and having more on-chip cache, the vast majority of server vendors decided to add ECC to their caches. Sun decided to save a few dollars and skip the ECC. The direct result was that a number of Sun customers reported sporadic data corruption. It took Sun multiple years to spin a new architecture with ECC cache, and Sun made customers sign an NDA to get replacement chips. Of course there's no way to cover up this sort of thing forever, and when it came up, Sun's reputation for producing reliable servers took a permanent hit, much like the time they tried to <a href="https://danluu.com/anon-benchmark/">cover up poor performance results by introducing a clause into their terms of services disallowing benchmarking</a>.</p> <p>Another thing to note here is that when you're paying for ECC, you're not just paying for ECC, you're paying for parts (CPUs, boards) that have been qual'd more thoroughly. You can easily see this with disk failure rates, and I've seen many people observe this in their own private datasets. In terms of public data, I believe Andrea and Remzi's group had a SIGMETRICS paper a few years back that showed that SATA drives were 4x more likely than SCSI drives to have disk read failures, and 10x more likely to have silent data corruption. This relationship held true even with drives from the same manufacturer. There's no particular reason to think that the SCSI interface should be more reliable than the SATA interface, but it's not about the interface. It's about buying a high-reliability server part vs. a consumer part. Maybe you don't care about disk reliability in particular because you checksum everything and can easily detect disk corruption, but there are some kinds of corruption that are harder to detect.</p> <p>[2024 update, almost a decade later]: looking at this retrospectively, we can see that Jeff's assertion that commodity parts are reliable, "modern commodity computer parts from reputable vendors are amazingly reliable" is still not true. Looking at real-world user data from Firefox, <a href="https://fosstodon.org/@gabrielesvelto/112401643131904845">Gabriele Svelto estimated that approximately 10% to 20% of all Firefox crashes were due to memory corruption</a>. Various game companies that track this kind of thing also report a significant fraction of user crashes appear to be due to data corruption, although I don't have an estimate from any of those companies handy. A more direct argument is that if you talk to folks at big companies that run a lot of ECC memory and look at the rate of ECC errors, there are quite a few errors detected by ECC memory despite ECC memory typically having a lower error rate than random non-ECC memory. This kind of argument is frequently made (here, it was detailed above a decade ago, and when I looked at this when I worked at Twitter fairly recently and there has not been a revolution in memory technology that has reduced the need for ECC over the rates discussed in papers a decade ago), but it often doesn't resontate with folks who say things like "well, those bits probably didn't matter anyway", "most memory ends up not getting read", etc. Looking at real-world crashes and noting that the amount of silent data corruption should be expected to be much higher than the rate of crashes seems to resonate with people who aren't excited by looking at raw FIT rates in datacenters.</p> <h2 id="4-if-ecc-were-actually-important-it-would-be-used-everywhere-and-not-just-servers">4. If ECC were actually important, it would be used everywhere and not just servers.</h2> <p><a href="https://danluu.com/cocktail-ideas/">One way to rephrase this is as a kind of cocktail party efficient markets hypothesis. This can't be important, because if it was, we would have it</a>. Of course this is incorrect and there are many things that would be beneficial to consumers that we don't have, such as <a href="https://danluu.com/car-safety/">cars that are designed to safe instead of just getting the maximum score in crash tests</a>. Looking at this with respect to the server and consumer markets, this argument can be rephrased as “If this feature were actually important for servers, it would be used in non-servers”, which is incorrect. A primary driver of what's available in servers vs. non-servers is what can be added that buyers of servers will pay a lot for, to allow for price discrimination between server and non-server parts. This is actually one of the more obnoxious problems facing large cloud vendors — hardware vendors are able to jack up the price on parts that have server features because the features are much more valuable in server applications than in desktop applications. Most home users don't mind, giving hardware vendors a mechanism to extract more money out of people who buy servers while still providing cheap parts for consumers.</p> <p>Cloud vendors often have enough negotiating leverage to get parts at cost, but that only works where there's more than one viable vendor. Some of the few areas where there aren't any viable competitors include CPUs and GPUs. There have been a number of attempts by CPU vendors to get into the server market, but each attempt so far has been fatally flawed in a way that made it obvious from an early stage that the attempt was doomed (and these are often 5 year projects, so that's a lot of time to spend on a doomed project). The Qualcomm effort has been getting a lot of hype, but when I talk to folks I know at Qualcomm they all tell me that the current chip is basically for practice, since Qualcomm needed to learn how to build a server chip from all the folks they poached from IBM, and that the next chip is the first chip that has any hope of being competitive. I have high hopes for Qualcomm as well an ARM effort to build good server parts, but those efforts are still a ways away from bearing fruit.</p> <p>The near total unsuitability of current ARM (and POWER) options (not including hypothetical variants of Apple's impressive ARM chip) for most server workloads in terms of performance per TCO dollar is a bit of a tangent, so I'll leave that for another post, but the point is that Intel has the market power to make people pay extra for server features, and they do so. Additionally, some features are genuinely more important for servers than for mobile devices with a few GB of RAM and a power budget of a few watts that are expected to randomly crash and reboot periodically anyway.</p> <h2 id="conclusion">Conclusion</h2> <p>Should you buy ECC RAM? That depends. For servers, it's probably a good bet considering the cost, although it's hard to really do a cost/benefit analysis because it's really hard to figure out the cost of silent data corruption, or the cost of having some risk of burning half a year of developer time tracking down intermittent failures only to find that the were caused by using non-ECC memory.</p> <p>For normal desktop use, I'm pro-ECC, but if you don't have <a href="https://www.reddit.com/r/programming/comments/adoux/coding_horror_and_blogsstackoverflowcom/">regular backups</a> set up, doing backups probably has a better ROI than ECC. But once you have the absolute basics set up, there's a fairly strong case for ECC for consumer machines. For example, if you have backups without ECC, you can easily write corrupt data into your primary store and replicate that corrupt data into backup. But speaking more generally, big companies running datacenters are probably better set up to detect data corruption and more likely to have error correction at higher levels that allow them to recover from data corruption than consumers, so the case for consumers is arguably stronger than it is for servers, where the case is strong enough that's generally considered a no brainer. A major reason consumers don't generally use ECC isn't that it isn't worth it for them, it's that they just have no idea how to attribute crashes and data corruption when they happen. Once you start doing this, as Google and other large companies do, it's immediately obvious that ECC is worth the cost even when you have multiple levels of error correction operating at higher levels.</p> <h3 id="appendix-security">Appendix: security</h3> <p>If you allow any sort of code execution, even sandboxed execution, there are attacks <a href="https://en.wikipedia.org/wiki/Row_hammer">like rowhammer</a> which can allow users to cause data corruption and there have been instances where this has allowed for privilege escalation. ECC doesn't completely mitigate the attack, but it makes it much harder.</p> <p><small> Thanks to Prabhakar Ragde, Tom Murphy, Jay Weisskopf, Leah Hanson, Joe Wilder, and Ralph Corderoy for discussion/comments/corrections. Also, thanks (or maybe anti-thanks) to Leah for convincing me that I should write up this off the cuff verbal comment as a blog post. Apologies for any errors, the lack of references, and the stilted prose; this is basically a transcription of half of a conversation and I haven't explained terms, provided references, or checked facts in the level of detail that I normally do. </small></p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Revokes Intel, Qualcomm Licenses to Sell Chips to Huawei (120 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-05-07/us-revokes-intel-qualcomm-licenses-to-sell-chips-to-huawei</link>
            <guid>40293614</guid>
            <pubDate>Wed, 08 May 2024 01:55:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-05-07/us-revokes-intel-qualcomm-licenses-to-sell-chips-to-huawei">https://www.bloomberg.com/news/articles/2024-05-07/us-revokes-intel-qualcomm-licenses-to-sell-chips-to-huawei</a>, See on <a href="https://news.ycombinator.com/item?id=40293614">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Decker: A fantastic reincarnation of HyperCard with 1-bit graphics (269 pts)]]></title>
            <link>https://www.beyondloom.com/decker/index.html</link>
            <guid>40292181</guid>
            <pubDate>Tue, 07 May 2024 22:15:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.beyondloom.com/decker/index.html">https://www.beyondloom.com/decker/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=40292181">Hacker News</a></p>
<div id="readability-page-1" class="page"><h2>Decker</h2>

<p>Decker is a multimedia platform for creating and sharing interactive documents, with sound, images, hypertext, and scripted behavior. You can try it in your web browser <b><a href="https://www.beyondloom.com/decker/tour.html">right now</a></b>.</p>

<center>
	<a href="https://www.beyondloom.com/decker/tour.html"><img src="https://www.beyondloom.com/decker/images/wings.gif"></a>
</center>

<p>Decker builds on the legacy of <a href="https://en.wikipedia.org/wiki/HyperCard">HyperCard</a> and the visual aesthetic of classic MacOS. It retains the simplicity and ease of learning that HyperCard provided, while adding many subtle and overt quality-of-life improvements, like deep undo history, support for scroll wheels and touchscreens, more modern keyboard navigation, and bulk editing operations.</p>

<p>Anyone can use Decker to create E-Zines, organize their notes, give presentations, build adventure games, or even just doodle some 1-bit pixel art. The holistic "ditherpunk" aesthetic is cozy, a bit nostalgic, and provides fun and distinctive creative constraints. As a prototyping tool, Decker encourages embracing a sketchy, imperfect approach. Finished decks can be saved as standalone <fixed>.html</fixed> documents which self-execute in a web browser and can be shared anywhere you can host or embed a web page. Decker also runs natively on MacOS, Windows, and Linux.</p>

<p>For more complex projects, Decker features a novel scripting language named <i>Lil</i> which is strongly influenced by both <a href="http://www.lua.org/">Lua</a>, an imperative language popular for embedding in tools and game engines, and <a href="https://en.wikipedia.org/wiki/Q_%28programming_language_from_Kx_Systems%29">Q</a>, a functional language in the APL family used with time-series databases. Lil is easy to learn and conventional enough not to ruffle any feathers for users with prior programming experience, but also includes pleasant surprises like implicit scalar-vector arithmetic and an integrated SQL-like query language. A few lines of Lil can go a long way.</p>

<center>
	<img src="https://www.beyondloom.com/decker/images/calc.gif">
</center>

<p>Decker provides a small collection of built-in interactive widgets for building interfaces, as well as a facility for <a href="https://www.beyondloom.com/decker/decker.html#customwidgets">defining new ones</a>. Custom widgets and their definitions can be copied and pasted using the system clipboard, which also makes it possible to share them anywhere you can share or store text. Every deck is a toolkit of reusable parts that can be harvested and repurposed for another project.</p>

<center>
	<img src="https://www.beyondloom.com/decker/images/contrap.gif">
</center>

<p>Decker is command-line friendly: when built from source, it comes with <a href="https://www.beyondloom.com/decker/lilt.html">Lilt</a>, a standalone Lil interpreter which can (among other things) read, write, manipulate, and even execute Decker documents "headlessly". Lilt has even fewer dependencies than Decker itself, so it can also be compiled as a cross-platform <a href="https://justine.lol/ape.html">APE executable</a>, ready for writing run-anywhere shell scripts. Would you believe there's a Lil interpreter <a href="https://www.beyondloom.com/blog/lila.html">that runs on POSIX AWK</a>? Decks are stored in a line-oriented text format which interoperates well with existing source control tools like Git and SVN.</p>

<p>Decker includes no advertising, telemetry, gamification, or other intrusions on user privacy and autonomy. If you like Decker, please share it with other people who might enjoy it. Build something that makes you happy.</p>

<h2>Examples</h2>
<ul>
	<li><a href="https://www.beyondloom.com/decker/tour.html">Decker: A Guided Tour</a></li>
	<li><a href="https://www.beyondloom.com/decker/guis.html">5GUIs</a></li>
	<li><a href="https://www.beyondloom.com/decker/chip8.html">A CHIP-8 Interpreter</a></li>
	<li><a href="https://www.beyondloom.com/decker/draggable.html">All About Draggable</a></li>
	<li><a href="https://www.beyondloom.com/decker/sound.html">All About Sound</a></li>
	<li><a href="https://www.beyondloom.com/decker/goofs/sokoban.html">Sokoban: A Block-Pushing Puzzle Game</a></li>
</ul>

<h2>Modules</h2>
<ul>
	<li><a href="https://www.beyondloom.com/decker/plot.html">Plot: Simple Graphs for Decker</a></li>
	<li><a href="https://www.beyondloom.com/decker/zazz.html">Zazz: Animation Helpers for Decker</a></li>
	<li><a href="https://www.beyondloom.com/decker/ease.html">Ease: Easing Functions for Decker</a></li>
	<li><a href="https://www.beyondloom.com/decker/dialog.html">Dialogizer: Visual-Novel Modals for Decker</a></li>
	<li><a href="https://www.beyondloom.com/decker/puppeteer.html">Puppeteer: Visual-Novel Sprite Animation for Decker</a></li>
</ul>

<h2>Documentation</h2>
<ul>
	<li><a href="https://www.beyondloom.com/decker/decker.html">The Decker reference manual</a></li>
	<li><a href="https://www.beyondloom.com/decker/format.html">The Decker document format</a></li>
	<li><a href="https://www.beyondloom.com/decker/lil.html">The Lil programming language</a></li>
	<li><a href="https://www.beyondloom.com/decker/learnlil.html">Learn Lil in 10 Minutes</a></li>
	<li><a href="https://www.beyondloom.com/tools/trylil.html">The Lil playground</a></li>
	<li><a href="https://www.beyondloom.com/decker/lilquickref.html">Lil Quick-reference card</a></li>
	<li><a href="https://www.beyondloom.com/decker/lilt.html">Lilt: the Lil Terminal</a></li>
	<li><a href="https://www.beyondloom.com/blog/responses.html">Decker: Responding to Responses</a></li>
</ul>

<h2>Additional Resources</h2>

<p>Browsable source code and a bug-tracker are available on <a href="https://github.com/JohnEarnest/Decker">GitHub</a>. Decker is free and open-source, under a permissive <a href="https://mit-license.org/">MIT license</a>.</p>

<p>Periodic binary releases for MacOS and Windows are available on <a href="https://internet-janitor.itch.io/decker">Itch.io</a>. The Itch page includes a <a href="https://internet-janitor.itch.io/decker/community">community forum</a> for discussing Decker and sharing projects made with Decker.</p>

<a href="https://www.beyondloom.com/index.html">back</a>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[IBM Granite: A Family of Open Foundation Models for Code Intelligence (232 pts)]]></title>
            <link>https://github.com/ibm-granite/granite-code-models</link>
            <guid>40291598</guid>
            <pubDate>Tue, 07 May 2024 21:16:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ibm-granite/granite-code-models">https://github.com/ibm-granite/granite-code-models</a>, See on <a href="https://news.ycombinator.com/item?id=40291598">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/ibm-granite/granite-code-models/blob/main/figures/granite-code-models-3x-v4.png"><img src="https://github.com/ibm-granite/granite-code-models/raw/main/figures/granite-code-models-3x-v4.png"></a>
</p>
<p dir="auto">
  📚 <a href="https://github.com/ibm-granite/granite-code-models/blob/main/paper.pdf">Paper</a>&nbsp; | 🤗 <a href="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330" rel="nofollow">HugginFace Collection</a>&nbsp; | 
  💬 <a href="https://github.com/orgs/ibm-granite/discussions">Discussions Page</a>&nbsp; | 📰 <a href="http://" rel="nofollow">Blog (coming soon)</a>&nbsp;
<br>
</p><hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction to Granite Code Models</h2><a id="user-content-introduction-to-granite-code-models" aria-label="Permalink: Introduction to Granite Code Models" href="#introduction-to-granite-code-models"></a></p>
<p dir="auto">We introduce the Granite series of decoder-only code models for code generative tasks (e.g., fixing bugs, explaining code, documenting code), trained with code written in 116 programming languages. A comprehensive evaluation of the Granite Code model family on diverse tasks demonstrates that our models consistently reach state-of-the-art performance among available open-source code LLMs.&nbsp;</p>
<p dir="auto">The key advantages of Granite Code models include:</p>
<ul dir="auto">
<li>All-rounder Code LLM: Granite Code models achieve competitive or state-of-the-art performance on different kinds of code-related tasks, including code generation, explanation, fixing, editing, translation, and more. Demonstrating their ability to solve diverse coding tasks.</li>
<li>Trustworthy Enterprise-Grade LLM: All our models are trained on license-permissible data collected following <a href="https://www.ibm.com/impact/ai-ethics" rel="nofollow">IBM's AI Ethics principles</a> and guided by IBM’s Corporate Legal team for trustworthy enterprise usage. We release all our Granite Code models under an <a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache 2.0 license</a> license for research and commercial use.</li>
</ul>
<p dir="auto">The family of <strong>Granite Code Models</strong> comes in two main variants:</p>
<ul dir="auto">
<li>Granite Code Base Models: base foundational models designed for code-related tasks (e.g., code repair, code explanation, code synthesis).</li>
<li>Granite Code Instruct Models: instruction following models finetuned using a combination of Git commits paired with human instructions and open-source synthetically generated code instruction datasets.</li>
</ul>
<p dir="auto">Both base and instruct models are available in sizes of 3B, 8B, 20B, and 34B parameters.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Data Collection</h2><a id="user-content-data-collection" aria-label="Permalink: Data Collection" href="#data-collection"></a></p>
<p dir="auto">Our process to prepare code pretraining data involves several stages. First, we collect a combination of publicly available datasets (e.g., GitHub Code Clean, Starcoder data), public code repositories, and issues from GitHub. Second, we filter the code data collected based on the programming language in which data is written (which we determined based on file extension). Then, we also filter out data with low code quality. Third, we adopt an aggressive deduplication strategy that includes both exact and fuzzy deduplication to remove documents having (near) identical code content. Finally, we apply a HAP content filter that reduces models' likelihood of generating hateful, abusive, or profane language. We also make sure to redact Personally Identifiable Information (PII) by replacing PII content (e.g., names, email addresses, keys, passwords) with corresponding tokens (e.g., ⟨NAME⟩, ⟨EMAIL⟩, ⟨KEY⟩, ⟨PASSWORD⟩). We also scan all datasets using ClamAV to identify and remove instances of malware in the source code. In addition to collecting code data for model training, we curate several publicly available high-quality natural language datasets for improving the model’s proficiency in language understanding and mathematical reasoning.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pretraining</h2><a id="user-content-pretraining" aria-label="Permalink: Pretraining" href="#pretraining"></a></p>
<p dir="auto">The <strong>Granite Code Base</strong> models are trained on 3-4T tokens of code data and natural language datasets related to code. Data is tokenized via byte pair encoding (BPE), employing the same tokenizer as StarCoder. We utilize high-quality data with two phases of training as follows:</p>
<ul dir="auto">
<li>Phase 1 (code only training): During phase 1, 3B and 8B models are trained for 4 trillion tokens of code data comprising 116 languages. The 20B parameter model is trained on 3 trillion tokens of code. The 34B model is trained on 1.4T tokens after the depth upscaling which is done on the 1.6T checkpoint of 20B model.</li>
<li>Phase 2 (code + language training): In phase 2, we include additional high-quality publicly available data from various domains, including technical, mathematics, and web documents, to further improve the model’s performance. We train all our models for 500B tokens (80% code-20% language mixture) in phase 2 training.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Instruction Tuning</h2><a id="user-content-instruction-tuning" aria-label="Permalink: Instruction Tuning" href="#instruction-tuning"></a></p>
<p dir="auto">Granite Code Instruct models are finetuned on the following types of instruction data: 1) code commits sourced from <a href="https://huggingface.co/datasets/bigcode/commitpackft" rel="nofollow">CommitPackFT</a>, 2) high-quality math datasets, specifically we used <a href="https://huggingface.co/datasets/TIGER-Lab/MathInstruct" rel="nofollow">MathInstruct</a> and <a href="https://huggingface.co/datasets/meta-math/MetaMathQA" rel="nofollow">MetaMathQA</a>, 3) Code instruction datasets such as <a href="https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v3" rel="nofollow">Glaive-Code-Assistant-v3</a>, <a href="https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k" rel="nofollow">Self-OSS-Instruct-SC2</a>, <a href="https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2" rel="nofollow">Glaive-Function-Calling-v2</a>, <a href="https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction" rel="nofollow">NL2SQL11</a> and a small collection of synthetic API calling datasets, and 4) high-quality language instruction datasets such as <a href="https://huggingface.co/datasets/nvidia/HelpSteer" rel="nofollow">HelpSteer</a> and an open license-filtered version of <a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus" rel="nofollow">Platypus</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Evaluation Results</h2><a id="user-content-evaluation-results" aria-label="Permalink: Evaluation Results" href="#evaluation-results"></a></p>
<p dir="auto">We conduct an extensive evaluation of our code models on a comprehensive list of benchmarks that includes but is not limited to HumanEvalPack, MBPP, and MBPP+. This set of benchmarks encompasses different coding tasks across commonly used programming languages (e.g., Python, JavaScript, Java, Go, C++, Rust).</p>
<p dir="auto">Our findings reveal that Granite Code models outperform strong open-source models across model sizes. The figure below illustrates how <code>Granite-8B-Code-Base</code> outperforms <code>Mistral-7B</code>, <code>LLama-3-8B</code>, and other open-source models in three coding tasks. We provide further evaluation results in our paper.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ibm-granite/granite-code-models/blob/main/figures/GraniteCodeFigure1.jpg"><img src="https://github.com/ibm-granite/granite-code-models/raw/main/figures/GraniteCodeFigure1.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Use our Models?</h2><a id="user-content-how-to-use-our-models" aria-label="Permalink: How to Use our Models?" href="#how-to-use-our-models"></a></p>
<p dir="auto">To use any of our models, pick an appropriate <code>model_path</code> from:</p>
<ol dir="auto">
<li><code>ibm-granite/granite-3b-code-base</code></li>
<li><code>ibm-granite/granite-3b-code-instruct</code></li>
<li><code>ibm-granite/granite-8b-code-base</code></li>
<li><code>ibm-granite/granite-8b-code-instruct</code></li>
<li><code>ibm-granite/granite-20b-code-base</code></li>
<li><code>ibm-granite/granite-20b-code-instruct</code></li>
<li><code>ibm-granite/granite-34b-code-base</code></li>
<li><code>ibm-granite/granite-34b-code-instruct</code></li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inference</h3><a id="user-content-inference" aria-label="Permalink: Inference" href="#inference"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer

device = &quot;cuda&quot; # or &quot;cpu&quot;
model_path = &quot;ibm-granite/granite-3b-code-base&quot; # pick anyone from above list

tokenizer = AutoTokenizer.from_pretrained(model_path)

# drop device_map if running on CPU
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)
model.eval()

# change input text as desired
input_text = &quot;def generate():&quot;
# tokenize the text
input_tokens = tokenizer(input_text, return_tensors=&quot;pt&quot;)

# transfer tokenized inputs to the device
for i in input_tokens:
    input_tokens[i] = input_tokens[i].to(device)

# generate output tokens
output = model.generate(**input_tokens)
# decode output tokens into text
output = tokenizer.batch_decode(output)

# loop over the batch to print, in this example the batch size is 1
for i in output:
    print(i)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>

<span>device</span> <span>=</span> <span>"cuda"</span> <span># or "cpu"</span>
<span>model_path</span> <span>=</span> <span>"ibm-granite/granite-3b-code-base"</span> <span># pick anyone from above list</span>

<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_path</span>)

<span># drop device_map if running on CPU</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>model_path</span>, <span>device_map</span><span>=</span><span>device</span>)
<span>model</span>.<span>eval</span>()

<span># change input text as desired</span>
<span>input_text</span> <span>=</span> <span>"def generate():"</span>
<span># tokenize the text</span>
<span>input_tokens</span> <span>=</span> <span>tokenizer</span>(<span>input_text</span>, <span>return_tensors</span><span>=</span><span>"pt"</span>)

<span># transfer tokenized inputs to the device</span>
<span>for</span> <span>i</span> <span>in</span> <span>input_tokens</span>:
    <span>input_tokens</span>[<span>i</span>] <span>=</span> <span>input_tokens</span>[<span>i</span>].<span>to</span>(<span>device</span>)

<span># generate output tokens</span>
<span>output</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>**</span><span>input_tokens</span>)
<span># decode output tokens into text</span>
<span>output</span> <span>=</span> <span>tokenizer</span>.<span>batch_decode</span>(<span>output</span>)

<span># loop over the batch to print, in this example the batch size is 1</span>
<span>for</span> <span>i</span> <span>in</span> <span>output</span>:
    <span>print</span>(<span>i</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Finetuning</h3><a id="user-content-finetuning" aria-label="Permalink: Finetuning" href="#finetuning"></a></p>
<p dir="auto">Codebase coming soon.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Model Cards</h2><a id="user-content-model-cards" aria-label="Permalink: Model Cards" href="#model-cards"></a></p>
<p dir="auto">The model cards for each model variant are available in their respective HuggingFace repository. Please visit our collection <a href="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330" rel="nofollow">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Download our Models?</h2><a id="user-content-how-to-download-our-models" aria-label="Permalink: How to Download our Models?" href="#how-to-download-our-models"></a></p>
<p dir="auto">The model of choice (granite-3b-code-base in this example) can be cloned using:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://huggingface.co/ibm-granite/granite-3b-code-base"><pre>git clone https://huggingface.co/ibm-granite/granite-3b-code-base</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">All Granite Code Models are distributed under <a href="https://github.com/ibm-granite/granite-code-models/blob/main/LICENSE">Apache 2.0</a> license.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Would you like to provide feedback?</h2><a id="user-content-would-you-like-to-provide-feedback" aria-label="Permalink: Would you like to provide feedback?" href="#would-you-like-to-provide-feedback"></a></p>
<p dir="auto">Please let us know your comments about our family of code models by visiting our <a href="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330" rel="nofollow">collection</a>. Select the repository of the model you would like to provide feedback about. Then, go to <em>Community</em> tab, and click on <em>New discussion</em>. Alternatively, you can also post any questions/comments on our <a href="https://github.com/orgs/ibm-granite/discussions">github discussions page</a>.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>