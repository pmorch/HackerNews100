<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 23 Sep 2025 15:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[MrBeast Failed to Disclose Ads and Improperly Collected Children's Data (260 pts)]]></title>
            <link>https://bbbprograms.org/media/newsroom/decisions/mrbeast-feastables</link>
            <guid>45346950</guid>
            <pubDate>Tue, 23 Sep 2025 13:44:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bbbprograms.org/media/newsroom/decisions/mrbeast-feastables">https://bbbprograms.org/media/newsroom/decisions/mrbeast-feastables</a>, See on <a href="https://news.ycombinator.com/item?id=45346950">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
                

                New York, NY – September 18, 2025 - Following its marketplace monitoring of children’s advertising, BBB National Programs’ Children’s Advertising Review Unit (CARU) recommended that MrBeastYouTube, LLC, and its affiliate Feastables, update their advertising and data collection practices for the MrBeast YouTube channel, Feastables’ Sweepstakes, and the Feastables website to comply with CARU’s Advertising Guidelines, CARU’s Privacy Guidelines, and the Children’s Online Privacy Protection Act (COPPA).</p><p>

MrBeast, the online persona of Jimmy Donaldson, is one of the world's most popular content creators on the YouTube platform, with a large fan base of children and young adults. The MrBeast YouTube channel has over 400 million subscribers. In 2022, MrBeast launched Feastables, a chocolate brand that generated over $215 million in revenue in 2024.</p><p>

CARU’s inquiry focused on whether certain advertising on the MrBeast YouTube channel was identifiable as advertising; whether certain claims for Feastables chocolate bars were misleading; whether promotions for Feastables sweepstakes clearly disclosed free means of entry, odds of winning, and minimum age requirements; and whether personally identifiable information was collected from children prior to obtaining verifiable parental consent (VPC).</p><h2>MrBeast YouTube Advertising</h2><p>
CARU observed several MrBeast YouTube videos where the video descriptions and pinned comments contained advertising content unrelated to the videos. This content was not clearly labeled or otherwise identifiable as advertising to children in the audience.&nbsp;</p><p>

Because it would not be clear to children that the video descriptions and pinned comments were advertising messages, CARU determined that MrBeast’s YouTube channel did not comply with CARU’s Ad Guidelines’ provision that advertising should be presented in a way that makes clear to the child audience that it is advertising.&nbsp;</p><p>

CARU accordingly recommended that the channel update its advertising practices to ensure that all advertisements are identifiable as ads to children.</p><p>
In a promotional video later removed from his YouTube channel, MrBeast posted a “Massive Announcement!” where MrBeast debuted the new Feastables chocolate bars and purported to conduct a blind taste test between the new Feastables chocolate bars and “top European chocolates” that showed all tasters preferring the Feastables bar.</p><p>

Although MrBeast contended the taste test demonstration was not intended to be taken seriously, CARU concluded that it would appear to be a valid taste test to children.&nbsp;</p><p>

CARU determined Feastables violated CARU’s Ad Guidelines by misrepresenting that the taste test was a valid demonstration that consumers prefer Feastables chocolate bars’ taste compared to similar chocolates. &nbsp;&nbsp;</p><p>

CARU recommended that Feastables ensure ad claims are truthful and not misleading to children.&nbsp;</p><h2>Feastables Sweepstakes</h2><p>
At the end of the “Massive Announcement” Feastables video, MrBeast announced the “Blue Wave $10K sweepstakes” saying, “…for 30 days straight, we are giving away $10,000 to a lucky customer who scans the QR code on the back of any new Feastables bar…You might win 10 grand…” and to “Upload Your Receipt &amp; Enter to Win. Confirm your purchase below to enter for a chance to win 10k + a year’s supply of our new chocolate. MORE BARS = MORE ENTRIES.”&nbsp;</p><p>

Contrary to CARU’s Ad Guidelines’ provision that sweepstakes promotions must clearly and conspicuously disclose a free means of entry, the free means of entering the sweepstakes was only disclosed in the FAQ section linked to halfway down the page. The FAQ also stated that the minimum age of participation is 16.&nbsp;</p><p>

Based on the overall net impression of the ads for the Blue Wave sweepstakes, CARU determined that the free method of entry was not adequately disclosed. CARU determined that a child may have the reasonable takeaway that they must purchase Feastables chocolate bars to enter the sweepstakes. A second reasonable takeaway is that a child should purchase as many as 10 chocolate bars every day and scan the QR codes to maximize their chances of winning.</p><p>

In MrBeast’s 2024 Halloween sweepstakes, Feastables encouraged participants to submit up to 24 entries daily until October 30 for a chance to win $10,000, with a grand prize of $1,000,000 on Halloween Day. The ad copy stated, “$10,000 USD Daily Winner. Enter with Purchase Through October 30.” In very small print was the disclaimer, “No purchase necessary, Click below for details.” The official rules stated that participants must be at least 13 years old with parental permission and entrants under 13 are not allowed.&nbsp;</p><p>

CARU noted the sweepstakes did not include an age gate to ensure that participants were at least 13 years old, nor did the sweepstakes provide a method, at any point in the sweepstakes entry flow, where participants could enter their parent or guardian’s information to obtain VPC. The entry form required a participant to enter personal information including full name, phone number, address, and email address. To upload the receipt photo, the prompt, “I affirm that I have read, understand and agree to the Official Rules,” was pre-checked.&nbsp;</p><p>

Additionally, the advertisement featured a large countdown timer with the text “Time is Running Out. Buy Feastables for a chance to win $10K,” which CARU determined constituted sales pressure.&nbsp;</p><h2>Sweepstakes Advertising Issues</h2><p>
CARU determined that the Feastables Blue Wave $10K and Halloween $10K sweepstakes did not comply with CARU’s Ad Guidelines by failing to clearly and conspicuously disclose the free means of entry, the minimum age requirements, and the likelihood of winning, and by promoting the overconsumption of chocolate bars.&nbsp;</p><p>

CARU recommended ensuring that material information is disclosed clearly and conspicuously to children in language they can understand, that advertising is clearly identifiable as advertising to the children, and that advertising is truthful, not misleading, and appropriate to the child audience.</p><h2>Sweepstakes Privacy Issues</h2><p>
CARU’s Privacy Guidelines and COPPA require websites directed to children, including mixed audience sites, to obtain VPC before collecting, using, or disclosing personal information from children under 13. Pursuant to COPPA, mixed audience sites may implement a neutral age-screening mechanism to ensure the site does not collect personally identifiable information from children under 13 without first obtaining VPC.</p><p>

CARU determined that the Feastables website was a mixed audience site that appealed to children under the age of 13 as a secondary audience and, therefore, Feastables had a reasonable expectation that children under 13 would visit the website.&nbsp;</p><p>

Therefore, CARU determined that the Feastables Blue Wave $10K and Halloween $10K sweepstakes did not comply with CARU’s Privacy Guidelines and potentially COPPA by failing to provide a neutral age-screening mechanism to ensure the sweepstakes website did not collect personally identifiable information from children under the age of 13 without first obtaining VPC.&nbsp;</p><h2>Other Feastables Website Privacy Issues</h2><p>
In its review of the Feastables website, CARU observed a full-page popup that repeatedly solicited the user’s email address with a call to action stating, “MrBeast Wants You to Join the Crew.” When CARU provided an email address through this popup, it discovered that a second popup would generate, soliciting the user’s phone number this time. When testing the network traffic of the site during this interaction, CARU saw evidence that the email and phone contact information was sent to non-affiliate third parties.</p><p>

In light of Feastables’ future plans to solely run sweepstakes for ages 18 and up, CARU further recommended that Feastables consider whether a neutral and effective age gate would be appropriate for future sweepstakes and promotions.</p><p>

Since the opening of the inquiry, CARU has worked with the MrBeast team, which has cooperated to implement CARU’s recommendations regarding CARU’s Advertising Guidelines, CARU’s Privacy Guidelines, and COPPA.</p><p>

In the advertiser statement, MrBeast and Feastables stated that it “appreciates CARU’s mission to promote responsible children's advertising. However, they do not agree with all the conclusions made in the decision or the premises on which they were based. Furthermore, a variety of the issues raised by CARU relate to practices long since revised and/or discontinued. Notwithstanding, MrBeast and Feastables certainly will take CARU's concerns under advisement as it develops future advertisements which appear in children's media.”</p><p>

All BBB National Programs case decision summaries can be found in the <a href="https://bbbprograms.org/media/newsroom/decisions">case decision library</a>. For the full text of NAD, NARB, and CARU decisions, subscribe to the <a href="https://bbbprograms.org/media/online-archive">Online Archive</a>.&nbsp;

                    </p><p>
                    <time datetime="9/18/2025 12:00:00 AM">September 18, 2025</time></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cache of Devices Capable of Crashing Cell Network Is Found Near U.N (157 pts)]]></title>
            <link>https://www.nytimes.com/2025/09/23/us/politics/secret-service-sim-cards-servers-un.html</link>
            <guid>45345514</guid>
            <pubDate>Tue, 23 Sep 2025 11:29:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/09/23/us/politics/secret-service-sim-cards-servers-un.html">https://www.nytimes.com/2025/09/23/us/politics/secret-service-sim-cards-servers-un.html</a>, See on <a href="https://news.ycombinator.com/item?id=45345514">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/09/23/us/politics/secret-service-sim-cards-servers-un.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Structured Outputs in LLMs (106 pts)]]></title>
            <link>https://parthsareen.com/blog.html#sampling.md</link>
            <guid>45345207</guid>
            <pubDate>Tue, 23 Sep 2025 10:40:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://parthsareen.com/blog.html#sampling.md">https://parthsareen.com/blog.html#sampling.md</a>, See on <a href="https://news.ycombinator.com/item?id=45345207">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div>
        <header>
            <a href="https://parthsareen.com/index.html">
                <img src="https://parthsareen.com/zukohere.png" alt="Profile Image">
            </a>
            <h2>Writings</h2>
        </header>
        <main>
            <article id="post-content"></article>
            <section>
                <ul id="post-list"></ul>
            </section>
        </main>
        
    </div>
    




</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go has added Valgrind support (267 pts)]]></title>
            <link>https://go-review.googlesource.com/c/go/+/674077</link>
            <guid>45344708</guid>
            <pubDate>Tue, 23 Sep 2025 09:26:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://go-review.googlesource.com/c/go/+/674077">https://go-review.googlesource.com/c/go/+/674077</a>, See on <a href="https://news.ycombinator.com/item?id=45344708">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The YAML Document from Hell (126 pts)]]></title>
            <link>https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell</link>
            <guid>45344554</guid>
            <pubDate>Tue, 23 Sep 2025 09:04:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell">https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell</a>, See on <a href="https://news.ycombinator.com/item?id=45344554">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope=""><header><p>written by <br>published <time datetime="2023-01-11" itemprop="datePublished">11 January 2023</time></p></header><p><span>For a data format</span>, yaml is extremely complicated. It aims to be a human-friendly format, but in striving for that it introduces so much complexity, that I would argue it achieves the opposite result. Yaml is full of footguns and its friendliness is deceptive. In this post I want to demonstrate this through an example.</p><p>This post is a rant, and more opinionated than my usual writing.</p><h2 id="yaml-is-really-really-complex"><a href="#yaml-is-really-really-complex"></a>Yaml is really, really complex</h2><p>Json is simple. <a href="https://www.json.org/json-en.html">The entire json spec</a> consists of six railroad diagrams. It’s a simple data format with a simple syntax and that’s all there is to it. Yaml on the other hand, is complex. So complex, that <a href="https://yaml.org/spec/1.2.2/">its specification</a> consists of <em>10 chapters</em> with sections numbered four levels deep and a dedicated <a href="https://yaml.org/spec/1.2/errata.html">errata page</a>.</p><p>The json spec is not versioned. There were <a href="https://youtu.be/-C-JoyNuQJs?t=965">two changes</a> to it in 2005 (the removal of comments, and the addition of scientific notation for numbers), but it has been frozen since — almost two decades now. The yaml spec on the other hand is versioned. The latest revision is fairly recent, 1.2.2 from October 2021. Yaml 1.2 differs substantially from 1.1: the same document can parse differently under different yaml versions. We will see multiple examples of this later.</p><p>Json is so obvious that Douglas Crockford claims <a href="https://www.youtube.com/watch?v=-C-JoyNuQJs">to have discovered it</a> — not invented. I couldn’t find any reference for how long it took him to write up the spec, but it was probably hours rather than weeks. The change from yaml 1.2.1 to 1.2.2 on the other hand, was <a href="https://yaml.com/blog/2021-10/new-yaml-spec/">a multi-year effort by a team of experts</a>:</p><blockquote><p>This revision is the result of years of work by the new <abbr>YAML</abbr> language development team. Each person on this team has a deep knowledge of the language and has written and maintains important open source <abbr>YAML</abbr> frameworks and tools.</p></blockquote><p>Furthermore this team plans to actively evolve yaml, rather than to freeze it.</p><p>When you work with a format as complex as yaml, it is difficult to be aware of all the features and subtle behaviors it has. There is <a href="https://yaml-multiline.info/">an entire website</a> dedicated to picking one of <a href="https://stackoverflow.com/a/21699210/135889">the 63 different multi-line string syntaxes</a>. This means that it can be very difficult for a human to predict how a particular document will parse. Let’s look at an example to highlight this.</p><h2 id="the-yaml-document-from-hell"><a href="#the-yaml-document-from-hell"></a>The yaml document from hell</h2><p>Consider the following document.</p><pre><code>server_config:
  port_mapping:
    # Expose only ssh and http to the public internet.
    - 22:22
    - 80:80
    - 443:443

  serve:
    - /robots.txt
    - /favicon.ico
    - *.html
    - *.png
    - !.git  # Do not expose our Git repository to the entire world.

  geoblock_regions:
    # The legal team has not approved distribution in the Nordics yet.
    - dk
    - fi
    - is
    - no
    - se

  flush_cache:
    on: [push, memory_pressure]
    priority: background

  allow_postgres_versions:
    - 9.5.25
    - 9.6.24
    - 10.23
    - 12.13</code></pre><p>Let’s break this down section by section and see how the data maps to json.</p><h2 id="sexagesimal-numbers"><a href="#sexagesimal-numbers"></a>Sexagesimal numbers</h2><p>Let’s start with something that you might find in a container runtime configuration:</p><pre><code>port_mapping:
  - 22:22
  - 80:80
  - 443:443</code></pre><div id="cb3"><pre><code><span id="cb3-1"><span>{</span><span>"port_mapping"</span><span>:</span> <span>[</span><span>1342</span><span>,</span> <span>"80:80"</span><span>,</span> <span>"443:443"</span><span>]</span><span>}</span></span></code></pre></div><p>Huh, what happened here? As it turns out, numbers from 0 to 59 separated by colons are <a href="https://yaml.org/spec/1.1/#id858600">sexagesimal (base 60) number literals</a>. This arcane feature was present in yaml 1.1, but silently removed from yaml 1.2, so the list element will parse as <code>1342</code> or <code>"22:22"</code> depending on which version your parser uses. Although yaml 1.2 is more than 10 years old by now, you would be mistaken to think that it is widely supported: the latest version libyaml at the time of writing (which is used among others by <a href="https://pypi.org/project/PyYAML/6.0/">PyYAML</a>) implements yaml 1.1 and parses <code>22:22</code> as <code>1342</code>.</p><p>The following snippet is actually invalid:</p><pre><code>serve:
  - /robots.txt
  - /favicon.ico
  - *.html
  - *.png
  - !.git</code></pre><p>Yaml allows you to create an <em>anchor</em> by adding an <code>&amp;</code> and a name in front of a value, and then you can later reference that value with an <em>alias</em>: a <code>*</code> followed by the name. In this case no anchors are defined, so the aliases are invalid. Let’s avoid them for now and see what happens.</p><pre><code>serve:
  - /robots.txt
  - /favicon.ico
  - !.git</code></pre><div id="cb6"><pre><code><span id="cb6-1"><span>{</span><span>"serve"</span><span>:</span> <span>[</span><span>"/robots.txt"</span><span>,</span> <span>"/favicon.ico"</span><span>,</span> <span>""</span><span>]</span><span>}</span></span></code></pre></div><p>Now the interpretation depends on the parser you are using. The element starting with <code>!</code> is a <a href="https://yaml.org/spec/1.2.2/#3212-tags">tag</a>. This feature is intended to enable a parser to convert the fairly limited yaml data types into richer types that might exist in the host language. A tag starting with <code>!</code> is up to the parser to interpret, often by calling a constructor with the given name and providing it the value that follows after the tag. This means that <strong>loading an untrusted yaml document is generally unsafe</strong>, as it may lead to arbitrary code execution. (In Python, you can avoid this pitfall by using <code>yaml.safe_load</code> instead of <code>yaml.load</code>.) In our case above, PyYAML fails to load the document because it doesn’t know the <code>.git</code> tag. Go’s yaml package is less strict and returns an empty string.</p><h2 id="the-norway-problem"><a href="#the-norway-problem"></a>The Norway problem</h2><p>This pitfall is so infamous that it became known as “<a href="https://hitchdev.com/strictyaml/why/implicit-typing-removed/">the Norway problem</a>”:</p><pre><code>geoblock_regions:
  - dk
  - fi
  - is
  - no
  - se</code></pre><div id="cb8"><pre><code><span id="cb8-1"><span>{</span><span>"geoblock_regions"</span><span>:</span> <span>[</span><span>"dk"</span><span>,</span> <span>"fi"</span><span>,</span> <span>"is"</span><span>,</span> <span>false</span><span>,</span> <span>"se"</span><span>]</span><span>}</span></span></code></pre></div><p>What is that <code>false</code> doing there? The literals <code>off</code>, <code>no</code>, and <code>n</code>, in various capitalizations (<a href="https://yaml.org/type/bool.html">but not any capitalization</a>!), are all <code>false</code> in yaml 1.1, while <code>on</code>, <code>yes</code>, and <code>y</code> are true. In yaml 1.2 these alternative spellings of the boolean literals are no longer allowed, but they are so pervasive in the wild that a compliant parser would have a hard time reading many documents. Go’s yaml library therefore <a href="https://github.com/go-yaml/yaml/tree/v3.0.1#compatibility">made the choice</a> of implementing a custom variant somewhere in between yaml 1.1 and 1.2 that behaves differently depending on the context:</p><blockquote><p>The yaml package supports most of <abbr>YAML</abbr> 1.2, but preserves some behavior from 1.1 for backwards compatibility. <abbr>YAML</abbr> 1.1 bools (yes/no, on/off) are supported as long as they are being decoded into a typed bool value. Otherwise they behave as a string.</p></blockquote><p>Note that it only does that since version 3.0.0, which was released in May 2022. <a href="https://github.com/go-yaml/yaml/commit/b145382a4cda47600eceb779844b8090b5807c4f">Earlier versions behave differently</a>.</p><h2 id="non-string-keys"><a href="#non-string-keys"></a>Non-string keys</h2><p>While keys in json are always strings, in yaml they can be any value, including booleans.</p><pre><code>flush_cache:
  on: [push, memory_pressure]
  priority: background</code></pre><div id="cb10"><pre><code><span id="cb10-1"><span>{</span></span>
<span id="cb10-2">  <span>"flush_cache"</span><span>:</span> <span>{</span></span>
<span id="cb10-3">    <span>"True"</span><span>:</span> <span>[</span><span>"push"</span><span>,</span> <span>"memory_pressure"</span><span>]</span><span>,</span></span>
<span id="cb10-4">    <span>"priority"</span><span>:</span> <span>"background"</span></span>
<span id="cb10-5">  <span>}</span></span>
<span id="cb10-6"><span>}</span></span></code></pre></div><p>Combined with the previous feature of interpreting <code>on</code> as a boolean, this leads to a dictionary with <code>true</code> as one of the keys. It depends on the language how that maps to json, if at all. In Python it becomes the string <code>"True"</code>. The key <code>on</code> is common in the wild because <a href="https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#on">it is used in GitHub Actions</a>. I would be really curious to know whether GitHub Actions’ parser looks at <code>"on"</code> or <code>true</code> under the hood.</p><h2 id="accidental-numbers"><a href="#accidental-numbers"></a>Accidental numbers</h2><p>Leaving strings unquoted can easily lead to unintentional numbers.</p><pre><code>allow_postgres_versions:
  - 9.5.25
  - 9.6.24
  - 10.23
  - 12.13</code></pre><div id="cb12"><pre><code><span id="cb12-1"><span>{</span><span>"allow_postgres_versions"</span><span>:</span> <span>[</span><span>"9.5.25"</span><span>,</span> <span>"9.6.24"</span><span>,</span> <span>10.23</span><span>,</span> <span>12.13</span><span>]</span><span>}</span></span></code></pre></div><p>Maybe the list is a contrived example, but imagine updating a config file that lists a single value of 9.6.24 and changing it to 10.23. Would you remember to add the quotes? What makes this even more insidious is that many dynamically typed applications implicitly convert the number to a string when needed, so your document works fine most of the time, except in some contexts it doesn’t. For example, the following Jinja template accepts both <code>version: "0.0"</code> and <code>version: 0.0</code>, but it only takes the true-branch for the former.</p><pre><code>{% if version %}
  Latest version: {{ version }}
{% else %}
  Version not specified
{% endif %}</code></pre><h2 id="runners-up"><a href="#runners-up"></a>Runners-up</h2><p>There is only so much I can fit into one artifical example. Some arcane yaml behaviors that did not make it in are <a href="https://yaml.org/spec/1.2.2/#68-directives">directives</a>, integers starting with <code>0</code> being octal literals (but only in yaml 1.1), <code>~</code> being an alternative spelling of <code>null</code>, and <code>?</code> introducing a <a href="https://yaml.org/spec/1.2.2/#example-mapping-between-sequences">complex mapping key</a>.</p><h2 id="syntax-highlighting-will-not-save-you"><a href="#syntax-highlighting-will-not-save-you"></a>Syntax highlighting will not save you</h2><p>You may have noticed that none of my examples have syntax highlighting enabled. Maybe I am being unfair to yaml, because syntax highlighting would highlight special constructs, so you can at least see that some values are not normal strings. However, due to multiple yaml versions being prevalent, and highlighters having different levels of sophistication, you can’t rely on this. I’m not trying to nitpick here: Vim, my blog generator, GitHub, and Codeberg, all have a unique way to highlight the example document from this post. No two of them pick out the same subset of values as non-strings!</p><h2 id="templating-yaml-is-a-terrible-terrible-idea"><a href="#templating-yaml-is-a-terrible-terrible-idea"></a>Templating yaml is a terrible, terrible idea</h2><p>I hope it is clear by now that working with yaml is subtle at the very least. What is even more subtle is concatenating and escaping arbitrary text fragments in such a way that the result is a valid yaml document, let alone one that does what you expect. Add to this the fact that whitespace is significant in yaml, and the result is a format that is <a href="https://twitter.com/memenetes/status/1600898397279502336">meme-worthily</a> difficult to template correctly. I truly do not understand why <a href="https://helm.sh/docs/chart_best_practices/templates/">tools based on such an error-prone practice</a> have gained so much mindshare, when there is a safer, easier, and more powerful alternative: generating json.</p><h2 id="alternative-configuration-formats"><a href="#alternative-configuration-formats"></a>Alternative configuration formats</h2><p>I think the main reason that yaml is so prevalent despite its pitfalls, is that for a long time it was the only viable configuration format. Often we need lists and nested data, which rules out flat formats like ini. Xml is noisy and annoying to write by hand. But most of all, we need comments, which rules out json. (As we saw before, json had comments very early on, but they were removed because people started putting parsing directives in there. I think this is the right call for a serialization format, but it makes json unsuitable as a configuration language.) So if what we really need is the json data model but a syntax that allows comments, what are some of the options?</p><ul><li><a href="https://toml.io/en/"><strong>Toml</strong></a> — Toml is similar to yaml in many ways: it has mostly the same data types; the syntax is not as verbose as json; and it allows comments. Unlike yaml it is not full of footguns, mostly because strings are always quoted, so you don’t have values that look like strings but aren’t. Toml is widely supported, you can probably find a toml parser for your favorite language. It’s even in the Python standard library — unlike yaml! A weak spot of toml is deeply nested data.</li><li><a href="https://code.visualstudio.com/docs/languages/json#_json-with-comments"><strong>Json with comments</strong></a>, <a href="https://nigeltao.github.io/blog/2021/json-with-commas-comments.html"><strong>Json with commas and comments</strong></a> — There exist various extensions of json that extend it just enough to make it a usable config format without introducing too much complexity. Json with comments is probably the most widespread, as it is used as the config format for Visual Studio Code. The main downside of these is that they haven’t really caught on (yet!), so they aren’t as widely supported as json or yaml.</li><li><strong>A simple subset of yaml</strong> — Many of the problems with yaml are caused by unquoted things that look like strings but behave differently. This is easy to avoid: always quote all strings. (Indeed, you can tell that somebody is an experienced yaml engineer when they defensively quote all the strings.) We can choose to always use <code>true</code> and <code>false</code> rather than <code>yes</code> and <code>no</code>, and generally stay away from the arcane features. The challenge with this is that any construct not explicitly forbidden will eventually make it into your codebase, and I am not aware of any good tool that can enforce a sane yaml subset.</li></ul><h2 id="generating-json-as-a-better-yaml"><a href="#generating-json-as-a-better-yaml"></a>Generating json as a better yaml</h2><p>Often the choice of format is not ours to make, and an application only accepts yaml. Not all is lost though, because yaml is a superset of json, so any tool that can produce json can be used to generate a yaml document.</p><p>Sometimes an application will start out with a need for just a configuration format, but over time you end up with many many similar stanzas, and you would like to share parts between them, and abstract some repetition away. This tends to happen in for example Kubernetes and GitHub Actions. When the configuration language does not support abstraction, people often reach for templating, which is a bad idea for the reasons explained earlier. Proper programming languages, possibly domain-specific ones, are a better fit. Some of my favorites are Nix and Python:</p><ul><li><a href="https://nixos.org/manual/nix/stable/language/index.html"><strong>Nix</strong></a> — Nix is the language used by the <a href="https://nixos.org/">Nix package manager</a>. It was created for writing package definitions, but it works remarkably well as a configuration format (and indeed it is used to configure NixOS). Functions, let-bindings, and string interpolation make it powerful for abstracting repetitive configuration. The syntax is light like toml, and it can <a href="https://nixos.org/manual/nix/stable/language/builtins.html#builtins-toJSON">export to json</a> or xml. It works well for simplifying a repetitive GitHub Actions workflow file, for example.</li><li><a href="https://www.python.org/"><strong>Python</strong></a> — Json documents double as valid Python literals with minimal adaptation, and Python supports trailing commas and comments. It has variables and functions, powerful string interpolation, and <a href="https://docs.python.org/3/library/json.html?highlight=json%20dump#json.dump"><code>json.dump</code></a> built in. A self-contained Python file that prints json to stdout goes a long way!</li></ul><p>Finally there are some tools in this category that I haven’t used enough to confidently recommend, but which deserve to be mentioned:</p><ul><li><a href="https://dhall-lang.org/"><strong>Dhall</strong></a> — Dhall is like Nix, but with types. It is less widespread, and personally I find the built-in function names unwieldy.</li><li><a href="https://cuelang.org/"><strong>Cue</strong></a> — Like Dhall, Cue integrates type/schema information into the config format. Cue is a superset of json, but despite that, I find the files that actually use Cue’s features to look foreign to me. Cue is on my radar to evaluate further, but I haven’t encountered a problem where Cue looked like the most suitable solution yet.</li><li><a href="https://github.com/hashicorp/hcl"><strong>Hashicorp Configuration Language</strong></a> — I haven’t used <abbr>HCL</abbr> extensively enough to have a strong opinion on it, but in the places where I worked with it, the potential for abstraction seemed more limited than what you can achieve with e.g. Nix.</li></ul><p><strong>2025 update:</strong> After having used <abbr>HCL</abbr> more in practice, I consider it too ad-hoc to seriously recommend. My frustration with <abbr>HCL</abbr> is what prompted me to create <a href="https://rcl-lang.org/"><abbr>RCL</abbr></a>. It <a href="https://ruudvanasseldonk.com/2024/a-reasonable-configuration-language">started as a toy project</a>, but is now at a point where it is both usable and useful.</p><h2 id="conclusion"><a href="#conclusion"></a>Conclusion</h2><p>Yaml aims to be a more human-friendly alternative to json, but with all of its features, it became such a complex format with so many bizarre and unexpected behaviors, that it is difficult for humans to predict how a given yaml document will parse. If you are looking for a configuration format, toml is a friendly format without yaml’s footguns. For cases where you are stuck with yaml, generating json from a more suitable language can be a viable approach. Generating json also opens up the possibility for abstraction and reuse, in a way that is difficult to achieve safely by templating yaml.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Altoids by the Fistful (161 pts)]]></title>
            <link>https://www.scottsmitelli.com/articles/altoids-by-the-fistful/</link>
            <guid>45343449</guid>
            <pubDate>Tue, 23 Sep 2025 06:24:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scottsmitelli.com/articles/altoids-by-the-fistful/">https://www.scottsmitelli.com/articles/altoids-by-the-fistful/</a>, See on <a href="https://news.ycombinator.com/item?id=45343449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p>“Wh— what did you say?”</p><p>It’s close to six o’clock on a weekday afternoon and the bar is starting to get noisy with the after-work crowd. It’s entirely possible I misheard that last part.</p><p>“Altoids! I find the spearmint works a little better overall, but recently I’ve started switching flavors depending on the situation.”</p><p>I’ve worked with James—“Jim” as everyone on the team knows him—for a little over two years and I’m used to this dance now. He gets a kind of tunnel vision in his excitement about whatever shiny new thing has captured his attention. It’s usually pretty easy to shake him out of it.</p><p>“No, Jim, the part before that.”</p><p>He looks at me for a moment, inquisitive, before pushing his beer aside. “Here, let me show you.” He reaches underneath the table and produces his beige-on-brown Timbuk2 messenger bag. There is a small wet spot left behind from his drink, and the bag plops right onto it. I watch as one of his stubby hands unbuckles the outermost pouch while the other one pulls out a small green and white tin. I am obviously intended to see this as clearly as possible, evidenced by the way he places it front and center between us.</p><p>“Regular everyday Altoids, right? You take about four of them, maybe five.” He flips the lid open and traps the requisite number of small white mints between his fingertips, which he then pops into his mouth. “This is the trick; you gotta half-chew it first.” At least two tiny shards fly in my direction as he speaks these words. It is like listening to a slow K-turn executed on a road covered in gravel and seashells. Three more slow and deliberate chomps, then his bite eases. “Mmm.” The communication style switches to mime: an index finger raised in a “one moment” gesture, followed by an exaggerated point downwards while unzipping the main pouch of the bag. It takes a few seconds of rooting around before the star of this particular show is found.</p><p>My eyes barely have enough time to resolve the object under the dismal light at this end of the bar before it’s in his mouth. He’s chewing the full concoction now—mouth closed, thank God. The crunching softens, then fades into the din from a nearby table of sales bros laughing at their sales bro anecdote. Jim is looking at me with a kind of confident smugness I haven’t seen since I bet my buddy at Guitar Center that he couldn’t spontaneously play “Everlong” from memory. A bet I lost, I might add.</p><p>There is a degree of intentional spectacle to this, I’d have to imagine. Each jaw movement is deliberate. Precise. He does not break eye contact with me, though I desperately want to break it with him. I can’t though. The absurdity of the scene is absolutely hypnotizing. One final swallow, a smack of his lips, then he opens his mouth wide like a child proving that they finished all their vegetables and have earned their dessert. “Easy peasy, no problem.”</p><p>“That was…” It’s like a significant piece of my brain has just completely locked up. I’m just saying words without thinking, filling the empty air.</p><p>“A cat turd!” he proclaims, finishing my sentence.</p><p>A beat.</p><p>“You just ate a cat turd.” It’s all I can do in this moment to plainly restate the facts as I understand them, although the sense of alarm is definitely carrying in my voice.</p><p>“Yup, and it didn’t taste bad at all. The spearmint masks it <em>completely.</em> Watch, I’ll do another one.” My eyes widen in dread as I shake my head weakly. I didn’t want to see him do that the first time; I sure as shit don’t want to see it again.</p><p>“No, that’s alright,” I balk.</p><p>There is an awkward reach across the bag as he grabs his glass, tips it toward me in a silent toast, then takes a long swill. Whether he admits it or not, there’s evidently something that needs to be washed down. He lets out a contented sigh as the almost-empty glass thumps back down on the table. I glance down at the chicken wings and carrot sticks I had been picking at. A minute ago, they were kinda bland—merely <em>okay</em> by the standards of pub food. With the abrupt loss of my appetite, now they are destined for the dumpster out back.</p><p>He lifts the small tin of mints and gives it a little shake in front of my face. It sounds a lot more papery and a lot less metallic than I would’ve guessed. “Altoids. I’m not exaggerating when I say these have completely changed the way I work.” I follow this little miracle box as they get tucked back into the bag, the buckles snapping shut to shield them from the lustful gaze of an angry world. He pauses and looks up at me again. “Would you like to try?”</p><p>“No, Jim, I don’t want your cat turds.”</p><p>I don’t want <em>your</em> cat turds. Why did I say it like that? I don’t want <em>anybody’s</em> cat turds!</p><p><em>…Right?</em></p><p>“Completely changed the way I work,” he repeats mechanically, sliding his bag onto the empty seat to his left. I’m finding it quite difficult to look at Jim, so I instead follow the motions of the bag until it is completely out of my view. How many more are in there?</p><p>“I used to spend so much of my day on cat turds, psyching myself up, trying strategies that didn’t work, all the cleanup when I was finished. That’s all gone now. I can never go back to the old way.”</p><p>“I just… I mean…” My brain has started working again, at least superficially, and it has generated so many questions that I’m having a hard time selecting which one to ask first. “How long have you been eating cat turds?” A fine question for this moment, I suppose.</p><p>“What do you mean? I’ve always had to eat cat turds. Since I was a kid in school, on through college, in all my jobs… They keep giving me cat turds and I keep having to eat them, otherwise it starts to pile up and then things really get messy.”</p><p>His face turns slightly serious as he parses my expression, his head tilting in suspicion. “You eat cat turds too, yeah?” I choose not to answer that question. He continues anyway. “Sure. We all do. We have to, ya know?”</p><hr><p><em>We all do.</em></p><p>Those words have been repeating in my head with the consistency of a drumline cadence. <em>We all do.</em></p><p>“Walk sign is on to cross Pawk Avenue. Walk sign is on to cross Pawk Avenue.” I’ve heard this prerecorded voice, clearly belonging to the most disgruntled DOT Traffic Signals employee available at the time of this crosswalk’s construction, at least twice per workday for the last two years. It stirs up a half-remembered dream of a career spent shoveling dirt into a hole—something that feels more like the idea of “honest work” than what I get paid to do every day. I bet nobody on the construction crew spent an entire workday fighting around with brittle, poorly designed automation tooling like I did today.</p><p>I’m quickly but unintentionally refilling my conscious mind with the task I had gleefully abandoned when Jim invited me out to after-work drinks. Normally I’d be irritated to spend more of my waking life thinking about this stuff, but after what I witnessed at the bar I welcome <em>any</em> distraction at all.</p><p>“Okay. So, usually we have a string. This is one of many values inside a mapping type, within a list of similar mappings.” I’m narrating to myself silently, imagining little bits of JSON syntax stamped on rectangles that are kind of stacked on top of each other like playing cards. “But ever since the schema change in V3, sometimes the value is another mapping type that wraps the string we want…” I’m visualizing another square to the right of the existing one. This one is yellow, distinct from the light blue of all the others, and it never occurs to me to question why that is.</p><p>“But because this is actually YAML, and the value comes from a template call, both the string form and the mapping form need to be escaped and indented in a way that works in both cases.” I’m chewing on the problem in pretty much the same mindset I had during work, only now I’m walking across midtown instead of staring at a computer screen. “We could just revert that change, keep the value as JSON like it used to be and insert it verbatim… but DevEx owns that part and I wouldn’t want to have to fight to get that PR approved.”</p><p>“Piece of shit.” I speak that bit out loud without really intending to. I snap back into awareness of my surroundings and look around. Nobody was near enough to hear it. They probably wouldn’t have cared if they were.</p><p>It occurs to me that, whenever anybody asks me what I do for a living and I wave my hand and say “Computers,” this is what I’m trying to avoid needing to have to explain. None of these words are being used in a way that would mean anything to most people. If one were to take the time to carefully define them all and how they fit together semantically, they describe concepts so abstract and detached from any kind of tangible shared experience that you’d hit a second wall trying to explain <em>that</em>.</p><p>“Oh, but wait, we have the <code>nindent</code> function. I could just count up the indentation level of the outer list and… Ah, hell, I forgot this template is transcluded into pod <em>and</em> deployment specs and the nesting levels would be different between the two.” I briefly try to think of which chucklefuck I could blame this design on, but truth be told I rubber-stamped enough questionable pull requests in my time here that a fair amount of this situation is a mess of my own damn making.</p><p>Huh. I really do wonder what I would sound like trying to explain this to somebody who had no experience in the industry. I suppose if I was very excited about it, I might come across like an energetic kid going on and on about all the different Pokémon they know about and all the special attacks and vulnerabilities. But without that spark of passion, and in its place a jaded voice tinged with frustration and contempt, I would probably just sound like a raving lunatic. These words don’t mean anything. I’m not describing something that actually exists. I’m playing the part of an observer in a universe of little floating boxes, becoming physically agitated about a superficial difference within the yellow one, and <em>none of it is real.</em></p><p>I’m definitely not feeling the passion on this one. This code runs deep inside a build-deploy pipeline that I have no hope of ever running directly on the computer I’m using. So I write the code, push it to CI, wait for a bunch of stuff I’m not interested in to finish running, then get to watch my change fail to work for either the stupidest typo that I never should’ve made in the first place, or due to some error that is so novel that even the search engines assume I must really be having some other <em>much more popular</em> error instead of the one I provided. It feels like I am performing surgery using a scalpel held by a boardwalk arcade claw machine, complete with the constant squawking and shitting of project management seagulls.</p><p>And even if I could concisely explain all of that to my hypothetical interlocutor, there’s the even higher-level question: <em>Why?</em> Why did we even make this change? What was so irredeemably wrong with the last two versions of this thing that we’re now doing it all again a third time? What exactly is the goal we’re trying to achieve here? I can’t really say. It’s a question I never asked, partly because I learned a long time ago that asking questions just causes friction. Just nod and shut up. Put a +1 on a sketchy PR and get it out of here. Don’t hold up the pipeline. Recover enough stamina to face down the next eldritch nightmare that slithers its way to the top of my Jira swimlane. “Sounds great, thanks.” Thumbs-up. Grit my teeth through to the next direct deposit, convince myself it’s not so bad. Do it over and over until some ill-defined end condition is met. I’ll know it when I see it. I hope.</p><p>I catch myself at the tail end of a sigh. I fake like I’m yawning to stretch my upper body for a second. Approximately every muscle in my back now aches.</p><p>There’s this very real sense that I don’t… I don’t want to solve this problem. There is no intellectual reward at the end of this journey. It’s not interesting to me. This isn’t something that needs to be fixed, because it’s not a situation that ever should’ve been permitted to happen in the first place. This is just a bunch of contrived nonsense that I <em>must</em> work through because the broader situation dictates it. It doesn’t matter if the solution is good or elegant. It doesn’t matter if it barely works. It doesn’t matter if it causes another problem that I stub my toe on in three weeks. It’s just… what I have to do.</p><p>I stop in my tracks.</p><p>These kinds of problems are <em>my cat turds.</em></p><p>Unlike Jim, though, I can’t just cram a bunch of breath mints into my face to make this go away.</p><hr><p>The “down” escalator into the train station is out of service, and it has been this way all summer. A pair of orange plastic barricades block the landings at both ends. I walk down two flights of stairs alongside a half-dozen other commuters. Having concluded that the template problem simply isn’t worth thinking any further about, I’m back on the cat turds. I understand what Jim was talking about now. This has been happening for almost my entire life, even going back to my days in elementary school.</p><p>All of the homework assignments that were blindly graded against answer keys from the back of a Teacher Edition of the textbook: Cat turds. College admission essays where I profused a longing desire to attend the distinguished universities that my parents and guidance counselor told me I should set my ambitions toward: Cat turds. Probably hundreds of cover letters submitted alongside job applications throughout the years, skimmed by perhaps tens of internal recruiters and hiring managers: Cat turds.</p><p>The notion that it was a good idea to manipulate highly whitespace-sensitive YAML data with the Go <code>text/template</code> package. CI workflows that take 75 minutes to reach the one step in the entire process that fails. Tools and interfaces that force-update and introduce breaking changes for seemingly no justifiable reason, removing or kneecapping features that were being relied on, with issue trackers guarded by thickheaded bots that dismissively auto-close feature requests that kindly ask for consideration for those use cases. Massively over-complicated software that tries to be everything to everybody, but in reality ends up being a gigantic lumbering pile of failure and frustration. <em>Cat turds.</em></p><p>I used to love this stuff. I still do. Except… I don’t. Not lately, anyway. A long time ago, this was unquestionably what I wanted to do with my life. I would stay up late, pushing back my bedtime for a few more minutes with these glorious machines, hacking away on some little project. Then I’d get up early the following morning, excited to jump back into the project before my day out in the world began. I don’t even clearly remember what I was building toward, but I know it had basically zero utility or market potential. The point of doing the project was simply to <em>do the project</em>—to press through problems, to learn new things, and to end the day with more skills and experience than I started with.</p><p>At one point, I had the 7-bit ASCII table memorized. Just the decimal codes; I didn’t really understand the usefulness of the hexadecimal representations, and it never occurred to me that the hex values would work much better in mnemonics. I don’t know why I took the time to learn that. I never really used that knowledge in any real day-to-day work, and it began to fade from my mind as soon as I found some other pointless esoterica to wallpaper over it.</p><p>Look at me now, having to Google how to read a text file line-by-line in Python despite having done it a hundred times at this point. The knowledge is up there somewhere, I’m sure of it. I just can’t always think of the idiom in the heat of the moment. Just a little hint to jog the old brain, that’s all I need.</p><p>I often wonder what my Younger Self would think of me now, failing to remember a two-line snippet of code that you’d find in the first ten pages of any beginner’s guide to the language. He’d probably sneer and say I need to devote more time to studying. But I’m an adult with things to do; I can’t spend all my time just memorizing things just in case I might need the information someday. Oh, and by the way: Younger Self, if you were such a friggin’ hotshot, why did it take you <em>fifteen years</em> to finally wrap your head around regular expressions? What’s that? Because they were <em>hard?</em> So you spent all your time memorizing easy and pointless trivia rather than tackling anything that was genuinely challenging? And then building up a whole air of superiority based on the number of discrete facts you could rattle off, rather than their practical utility? What, were you trying to become a contestant on <em>Computer Jeopardy!</em> or something?</p><p>No wonder Younger Self grew up to be kind of an asshole.</p><p>I mean, I didn’t <em>try</em> to be an asshole. It’s just that I tended to gauge my own self-worth relative to others based on the only social currency we could accurately compare: the amount of “stuff” we knew. Some people memorize car engine displacements, others carry in their noggins enough digits of pi to resolve the observable universe down to the width of a hydrogen atom. I had a litany of command-line switches that I never used for anything, HTML character entity names for writing systems I couldn’t comprehend, and tales of tweaking settings deep inside the Windows 98 Device Manager just so I could brag about having been in there in the first place. I also at one time sincerely believed that maybe if I taught myself to—I’m picking one example out of many—decode Code 39 barcodes in my head, it would somehow make me cool and desirable during otherwise awkward social functions. (I did get reasonably good at it. All it takes is memorizing a couple of three-digit sequences. Having a teenager’s near-field visual acuity certainly helped.)</p><p>Everybody else who didn’t know those little pieces of nothing? They were the lessers. They didn’t put in the time to grind for this knowledge. They had never scaled the peaks of Mount AltaVista, nor had they knelt in the temple of the <em>MSDN Library for Visual Studio</em> on a banged-up pair of CD-Rs. I knew things they did not, therefore I felt I was higher-and-mightier than they were. I and I alone <em>suffered</em> for this knowledge. This attitude manifested itself in one of two ways: In the first case, I would barge my way into situations where my involvement wasn’t needed or appreciated, thinking I could “save” others from the pain I once had to contend with. More often than not, though, I would simply mock people for not knowing things—usually inside my own head, but sometimes outwardly on mailing lists and message boards. There were times when I judged a person to have failed to put in the necessary amount of work, so therefore they did not deserve to rise anywhere near where I considered my own level to be. It didn’t matter if the subject was deeply technical or a disagreement on the precise phrasing of a <em>Simpsons</em> quote. Somebody got something wrong, and it was my job to rectify that.</p><p>I feel bad for the people who worked on teams where Younger Self was the senior engineer. I was full of ideals and convictions back then. “No, we’re not doing that. We’re going to Do It Right instead.” I was full of piss and vinegar. “Here, give me that; I’ll just do it myself.” I was full of shit.</p><p>I now realize that everything I lorded over other people—all the things I gatekept without consciously understanding that this was what I was doing—I didn’t need to do that. It really didn’t help anything. For some number of people who interacted with me, <em>I</em> was the problem. I could’ve been more tolerant or forgiving, I could’ve said “let’s find out together,” I could’ve let other people have the fun once in a while. I could have minded my own damn business and saved everybody the hassle.</p><p>There were people out there who must’ve felt that <em>I was their cat turds.</em></p><p>I’ll never be able to track down and apologize to every person I treated that way. And why did I even build that fiefdom and protect it so jealously? Why was I so insecure? Why did I have to always be right and have a ready justification for why everybody else was wrong?</p><p>It was just me, alone in my tiny sandbox, safe and secure behind my towering fortress of cat turds.</p><hr><p>My usual train, the one packed so full that some riders have to stand in the aisles until after the first or second stop, usually leaves at 5:50. Now about three hours later, one can sometimes get an entire car to themselves. I settle down in a window seat looking out at the desolate platform. Evidently there aren’t all that many people interested in traveling across the river at this hour on a Wednesday evening. It feels nice to sit, despite the fact that I’ve probably sat for a cumulative ten hours—at least—over the course of this day.</p><p>As sometimes happens, another rider boards the train and enters what had up to this point been my personal rail car. He selects the aisle seat in the row directly in front of me. At least 110 other seats in this car, every single one of them empty, and his choice is to sit right here. <em>Sigh.</em> I could get up and move to another seat but I’m… exhausted. I’m here, I’m settled in, and above all I’m just completely out of ambition. I guess it’s fine as long as he doesn’t start playing music or TikTok videos without headphones.</p><p>A long blow from the locomotive horn, and the train begins to creep forward. Right on schedule. We’re in a tunnel deep below the city’s west side, and the view out the window is pitch black aside from the occasional glow from a mercury-vapor emergency light. On the wall beneath each of these lights, patches of graffiti framed by concrete pillars. I wouldn’t say I’ve memorized them all by heart; I can’t even read the tags on the majority of them. But they are at least familiar, and I’ve found some of them serve as convenient signposts along this portion of the trip. I’m not really paying attention to any of them tonight, instead I’m staring blankly at a little patch of window glass as the scene rolls past.</p><p>I refocus my eyes a bit and realize I’m looking at the reflection of a screen, or at least the top corner of one. I turn away from the window and find the source of the light. The man in front of me has opened his laptop—a chunky Dell Latitude or something very close to it—and perched it on a small lap desk fashioned from his leather bag. He opens a web browser and logs into a Microsoft account, one key at a time, hunt-and-peck style. It prompts him for his second factor and he shifts awkwardly in the seat to retrieve his phone. The login process succeeds and, after a few clicks and a fair bit of both of our finite lifetimes spent staring at loading spinners, opens what appears to be a Word document. I can’t read anything on his screen, which is more a testament to how wrecked my eyes have become than anything else, but I can see that there’s about four, perhaps five lines of unformatted text up there already. He strokes his chin while giving it a good read-through, then his hands take their position on the trackpad. Right index finger moves the cursor, left index finger does the clicking. The screen flips to another browser tab, his left hand gratuitously double-clicks on the website suggested by the first tile on the screen, and the page loads.</p><p>I never learned to tell any of these sites apart from each other. I see lots of people using the one with the spirograph logo. The one that looks like a cartoon butthole is also quite popular among some departments at my job. This guy is using the one that’s represented by a symmetrical color blob. Not that one, the other color blob one. Yeah.</p><p>He has opened a chat session that has evidently been going for some time. The text entry box at the bottom of the window waits patiently for fresh input. Letter by agonizing letter, the keys needed to express his thoughts are pressed. The most-pressed key, however, is Backspace. This man is, using the most generous language possible, not a particularly fast or accurate typist. In total, he enters about ten words before pressing Enter. A short moment later, the machine responds. Entire sentences appear in the time it took him to type a single word. Multiple paragraphs with subheadings and bulleted lists scroll into view. The screen fills completely with this fresh text. He looks at this for a moment, moves his hands back to the trackpad, and selects a complete paragraph. His finger presses down with immense force as he drags the selection area ever wider, as if his catch is in danger of wriggling through his fingers if he doesn’t hold the button down hard enough. He flips back to his Word document and pastes the paragraph. Then back to the chat window. He begins typing again. Slowly. Excruciatingly.</p><p>This cycle repeats several times, incrementally building his document up to four or five double-spaced pages in length. It’s not exactly a fast process, but certainly faster than if he had thought up and typed out all that content the old-fashioned way. It’s certainly plausible that he at least read everything that went into the document, but I wouldn’t be able to prove it.</p><p>He selects another piece of text, this one substantially smaller than the other specimens that he’d been handling up until this point. This one is pasted into a discussion thread on Teams. He waits a moment for responses, closes the lid, and the laptop goes back into his bag. The man stands up, wraps the strap over his shoulder, and walks to the front of the car as the train brakes to a full stop. This is where our paths diverge, it would seem. The doors open and he steps out into the night.</p><p>Alone in the train car again, with nothing interesting to eavesdrop on, my mind begins to wander again. I wonder what the purpose of that document was. Why was it being prepared? Who dictated that a half-dozen input phrases needed to be inflated into a thousand-word wall of text? Who was going to sit and read all of that, anyway? And for what purpose?</p><p>I really don’t know. But I do know one thing: It’s cat turds.</p><p>This guy obviously didn’t want to do that task. Whether that was due to lack of passion and interest, or lack of skill and ability, he had a cat turd to eat and he found a little pack of Altoids that he could use to get through it with minimal suffering. The people who have to read it? There’s a good chance they’ll be dealing with a cat turd too. Maybe they can choose to employ a chatbot to summarize it back down to his original inputs. Maybe it’ll even do a passable job preserving the essence of the guy’s prompts.</p><p>It makes sense why a person or group of people would flock to anything that makes life’s demands a little less difficult for themselves. You’d have to be pretty dumb to want to do a task like that manually.</p><p>There’s still the question, though. Why are we all eating cat turds? When did we all collectively agree that we were all a-okay with the idea that we had to subject ourselves to this constant grind of doing shit that doesn’t really need to be done to satisfy requirements that were put in place simply “because” and that seemingly only create more pointless work for other people (or ourselves!) to have to do later?</p><p>One of the defining characteristics of humanity is its ability to build and wield tools that make difficult tasks easier. One would presume there would also be a certain wisdom in knowing which of the difficult tasks were worth doing in the first place but… Well. When you presume, you make a pres out of u and me.</p><hr><p>If I had known ahead of time that I’d be out this late, I would’ve brought a jacket. The early autumn air is calm but crisp, and my borough’s train platform offers very little protection from the chill. The crickets are still chirping, but their song has slowed substantially compared to how they sounded a few weeks back. I stopped parking at the station a long time ago—the monthly pass costs well over $150 now, and most days the parking lot is completely full before six o’clock in the morning anyway. It’s only a mile to the house, and this twenty-minute walk is pretty much the only exercise I get nowadays.</p><p>Once I cross the main boulevard at the four-way stop, it’s all suburban residential side streets. There is basically no traffic at this time of night in my sleepy little bedroom community. All the dogs have been walked, the kids have been put to bed, and the adults… Well, I’m sure there are at least a couple people around here drinking or smoking the memory of their cat turds away.</p><p>I’m no closer to anything resembling inner peace. I find I’ve grown to despise large swaths of the only thing I’ve ever been able to earn reliable income from. I tire of walking a path that has seemingly shifted beneath my feet to point toward a destination I no longer recognize. I’m embarrassed by the jerk my Younger Self used to be, and simultaneously ashamed of the energy I lost as I matured. I don’t really want to do most of what I have to do, while feeling a deep unsated need to achieve something that I have neither the stamina nor the freedom to pursue. At some point I’m going to reach down deep into the well of ambition to discover there ain’t nothing there to pull out anymore. And then?</p><p><em>Something</em> percent of success is simply showing up. That’s roughly how the quote goes, right? I’ve heard seventy percent, ninety percent, hell, let’s call it seventy-eight. It doesn’t matter because it isn’t a real thing that can be measured in any objective way. The idea is to inspire people to at least try. Put your butt in the chair, log into Teams, trick yourself into thinking, well, I made it all the way here, might as well prune my stale Git branches or something so I can feel like I’m doing real work. Push aside distractions, shake off procrastination, kindle that tiny spark into enough momentum to break through whatever barrier is standing in the way of getting something done. If only that worked with any degree of predictability.</p><p>There’s a metaphor that talks about painting the backs of cabinets. The idea is that, when you’re putting paint, stain, varnish, <em>whatever</em> on some cabinets, there’s no need to paint the surfaces that face toward the wall. From the day the units are mounted, to a day forty years from now when they are ripped down and thrown into a construction dumpster during a subsequent kitchen renovation, nobody will see the backs of any of those cabinets. Painting them would be a waste of time and materials. Nobody would know if it was done or not.</p><p>“Yes, but <em>I</em> would know.” That’s something my Dad would often say. His tendency has always been to be overly thorough, exacting and precise in any craft he partakes in. Everything—from the doors in the house to the stripes cut into the front lawn—was always level, plumb, square, centered, polished, dust-free, squeak-free, <em>fingerprint</em>-free… He even demonstrated meticulous care in breaking down cardboard and filling up the waste bins at the curb. I still have no idea how he was able to raise two kids in that house without exploding from the chaos we brought.</p><p>Maybe it was genetic, or maybe I voluntarily developed it so my dad would be proud of me just like he was proud of the other things he made. Either way, I definitely started to take after him in those ways and I now recognize this same kind of care in myself all the time. Not just in the way I prefer all my clocks to read the exact correct time or my knack for always noticing the way the receptacle face isn’t exactly flush with the wall plate… but in a fundamental inability to <em>not care</em> about quality or craft. Even when the task doesn’t matter. Even if it results in an entire afternoon spent painting a piece of carpentry that nobody will ever see. I can’t not care.</p><p>All that stuff Younger Self struggled with—the self-superiority, the sense that I had to be the one who did it if it needed to be done correctly, the derision and borderline abuse I gave others—that was all just a big dogmatic ball of caring a whole lot about quality and craft, being rolled around by a kid who didn’t understand what to do with it. I had to work so hard to care so much, and these other people didn’t, and everything worked out for them anyway, and <em>that wasn’t fair.</em> Decades later, I still feel that way sometimes.</p><p>My parents still live in that house, surrounded by all the things my dad cared so much about. Aside from a whole bunch of trees that died and needed to be cut down to stumps, everything is still pretty much pristine. But if you start to look around, really scrutinize, you’ll start to notice some things have slipped. There’s a film of dust on the higher wall decorations. Some of the brass knobs are becoming tarnished. A few of the light bulbs in the hallway fixtures don’t match. My dad seemed tired the last time we talked, and more than once he expressed the sentiment that “everything he owns is falling apart.” Is it simply the onset of physical old age that has limited his ability to stay on top of these things, or is he beginning to leave behind his era of caring?</p><p>Now that I think about it, I don’t think we’ve ever really talked about how care factored into his career philosophy. I had always implicitly assumed that it was the same as it currently is with me: Work or play, it’s always there. Can’t turn it off even if I wanted to. But what if he could? What if all the care he demonstrated in projects around the house was compensation for all the things he deliberately avoided caring about at work? It would certainly explain how he was able to consistently sustain those standards. But then, that would mean that I modeled my own principles and tastes on a distorted view of my dad, untempered by whatever he didn’t let me see about his workplace persona.</p><p>How would I begin to—well, I don’t want to say “not care,” that sounds too extreme. But maybe… <em>selectively</em> care? To care about the things that matter, the things that spark passion and joy and remind me why I spent so much time practicing this godforsaken occupation. While at the same time recognizing the things that don’t matter, the problems for which the optimal solution is to stop insisting on having that problem in the first place. The kinds of tasks for which the 78% showing-up baseline score is plenty good enough. Tasks on which care would be utterly wasted, the cases where the cabinets are so irredeemably fucked up that the lack of paint on the back is the last thing anybody’s going to worry about. Those are the tasks that hurt the most, because I find it basically impossible to make myself care about them. It offends my soul to try to force it, and it drains me of all ambition to move onto the next potentially heartening opportunity. It’s a real problem, and I find it always has been: If I can’t care about it, I have an extremely hard time bringing myself to do it at all.</p><p>Well, I suppose that’s when I open a chatbot session of my own. “Hey there Chat. Uh, we’ve never spoken before but, uh… Well, my entire system of self-motivation just completely broke down but I still need to keep moving forward. Can you help me out of this bind?” There’s a whole discipline—they call it Prompt Engineering—that’s just a fancy form of throwing your hands up and pressing the Care About It For Me button. That’s pretty much how it works. Provide it with any cat turd under the sun, it doesn’t matter. Chat will gobble them all up for you like a coprophagic dog.</p><p>I’d be lying if I said the idea didn’t make my skin crawl a little. Every fiber of my being says that this is a weight to be borne by me and me alone. This is <em>my</em> cat turd to eat; they gave it to <em>me</em>. When it’s done, I can open my grinning maw and say without equivocation that I was the one who got through it. I painted the back of this cabinet. I worked way too hard and poured far too much of my blood, sweat and tears into this thing. And my reward for a job well done is… debilitating exhaustion, most of the time. Getting a fresh cat turd to eat tomorrow. And the day after.</p><p>Of course, Chat can’t really care. It does a passable job <em>pretending</em> like it cares, saying the words that convey the illusion of care to any reader not paying very close attention. Where do I draw the line between fostering real care, versus passing off a degraded third-generation photocopy of some tokenization of what may have at one point been somebody else’s care? Is the line simply the boundary between the tasks I’m excited to do and the ones I put off until I’ve depleted enough mental reserves to <em>sorta</em> care?</p><p>It really does feel like the average person has made a choice to abandon a great deal of care, at least in their professional capacity. Take a look around at all these people with their fake shit-eating grins, passing off a machine’s effort as their own and experiencing no consequences. Sometimes they’re rewarded for doing so. There are organizations that are beginning to mandate it now. These people aren’t eating their cat turds anymore, why am I still sitting here eating mine?</p><p>I round the final curve leading to the corner of my block. As I pass under the streetlight, I cast a shadow on the asphalt ahead. With each step it grows longer and more distorted. There’s a rustle from the shrubs bordering my neighbor’s driveway, and a small dark form emerges. It crosses the street halfway then abruptly stops. I stop as well. A pair of glowing yellow eyes look back at me. I stare at it, it stares at me. A possum, perhaps? Somebody’s outdoor cat? It’s just watching me, seemingly peering straight into my very soul. Can it see what I’m grappling with here? Is it passing judgement on me for thinking these thoughts? It sizes me up for a moment longer, turns its head, and becomes a black apparition once more. I struggle to track it as it continues across the street, and I lose sight of it entirely.</p><p>I arrive at home and shut the door behind me. Sunset was over two hours ago and it’s nearly pitch black in the hallway. I fumble around for the light switch, kick my shoes off next to the doorway, and hang my bag on its hook in the coat closet. Something grabs my attention, just above eye level, slightly overhanging the edge of the top shelf. I slide it out of its resting place and carry it into the kitchen. I sit down at the table and inspect it.</p><p>This object is a round metal cookie tin about twelve inches in diameter. Beneath a thin coat of dust, it is a deep red with a repeating pattern of snowmen and white snowflakes, and quite obviously once held winter holiday–themed cookies. I repurposed it many years ago to hold the only vice I currently permit myself to indulge in: a meticulously curated collection of all different types of chocolate candies. I remove the lid and set it aside. I survey the contents, a sea of differently-shaped naked chocolate morsels. I don’t remember why I chose to remove all the foil and paper wrapping before putting these in here. From my vantage point, everything looks vaguely the same—I can’t readily spot any differences between milk chocolate and dark, or those filled with caramel versus crème.</p><p>One particular piece near the edge catches my eye, and I carefully select it for inspection. It’s not a very pleasing color or shape—oddly asymmetrical. I roll it around between my fingers. There’s a hair on it. I hold it up to my nose and take a whiff, hoping to detect the aroma of the cacao. Try as I might, I can’t pick up any trace of its scent.</p><p>Come to think of it, I can’t remember the last time I smelled anything.</p><a href="https://www.scottsmitelli.com/articles/" title="Articles">«&nbsp;Back to Articles</a></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Delete FROM users WHERE location = 'Iran'; (765 pts)]]></title>
            <link>https://gist.github.com/avestura/ce2aa6e55dad783b1aba946161d5fef4</link>
            <guid>45343108</guid>
            <pubDate>Tue, 23 Sep 2025 05:30:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/avestura/ce2aa6e55dad783b1aba946161d5fef4">https://gist.github.com/avestura/ce2aa6e55dad783b1aba946161d5fef4</a>, See on <a href="https://news.ycombinator.com/item?id=45343108">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-delete-from-users-where-location-iran-md" tabindex="0" role="region" aria-label="delete-from-users-where-location-iran.md content, created by avestura on 05:27AM today.">
    <article itemprop="text"><p dir="auto"><h2 dir="auto"><code>DELETE FROM users WHERE location = 'IRAN';</code></h2><a id="user-content-delete-from-users-where-location--iran" aria-label="Permalink: DELETE FROM users WHERE location = 'IRAN';" href="#delete-from-users-where-location--iran"></a></p>
<p dir="auto">Hi! I am an Iranian Software Engineer, and in this torn paper note, I want to talk about
some funny moments I had online related to the fact that I was spawned in this specific region of the world: Iran.</p>
<p dir="auto"><h2 dir="auto">Microsoft deleted my app, ignored my mails</h2><a id="user-content-microsoft-deleted-my-app-ignored-my-mails" aria-label="Permalink: Microsoft deleted my app, ignored my mails" href="#microsoft-deleted-my-app-ignored-my-mails"></a></p>
<p dir="auto">Back when I was a student, I got access to the <a href="https://imaginecup.microsoft.com/en-us" rel="nofollow">Microsoft Imagine</a>, and as a result, I got
access to the Microsoft Store as a developer. This inspired me write one of my open-source projects called <a href="https://github.com/avestura/EyesGuard">EyesGuard</a>
and publish it on Microsoft Store. However, one day, somebody told me that they can no longer find EyesGuard on the store.</p>
<p dir="auto">I came to the realization that Microsoft deleted my app, my developer account, and all those comments on my app supporting me and suggesting
ideas on how to improve the program. I tried to contact the support and email whoever I could, but I was ghosted.
Nobody ever explained to me why, but I assume it's because of the sanctions.</p>
<p dir="auto"><h2 dir="auto">Notion wiped me out of existence</h2><a id="user-content-notion-wiped-me-out-of-existence" aria-label="Permalink: Notion wiped me out of existence" href="#notion-wiped-me-out-of-existence"></a></p>
<p dir="auto">Notion is a great product, and it was the primary tool I used to manage my personal notes. Not until they suddenly decided to
wipe out every data related to the users residing in Iran. Hopefully, they actually responded to my support message:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9091958/492630193-dae97147-1dce-4619-8e3a-91ea51d1a506.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTg2MDkzMDIsIm5iZiI6MTc1ODYwOTAwMiwicGF0aCI6Ii85MDkxOTU4LzQ5MjYzMDE5My1kYWU5NzE0Ny0xZGNlLTQ2MTktOGUzYS05MWVhNTFkMWE1MDYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDkyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA5MjNUMDYzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWJlNTIwYmNiZGQwNTUwNmQyNzVmMmYzYzJiYWY2ZDc4NDgyMjdmYjQxZmQ4ZDI0OWIzOGY0MjllZDYzNzQ5MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.qGT2Dm1lgvCTESM1K6DgzwP55Tv40w_2kS7CL0jno_I"><img width="408" height="398" alt="image" src="https://private-user-images.githubusercontent.com/9091958/492630193-dae97147-1dce-4619-8e3a-91ea51d1a506.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTg2MDkzMDIsIm5iZiI6MTc1ODYwOTAwMiwicGF0aCI6Ii85MDkxOTU4LzQ5MjYzMDE5My1kYWU5NzE0Ny0xZGNlLTQ2MTktOGUzYS05MWVhNTFkMWE1MDYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDkyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA5MjNUMDYzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWJlNTIwYmNiZGQwNTUwNmQyNzVmMmYzYzJiYWY2ZDc4NDgyMjdmYjQxZmQ4ZDI0OWIzOGY0MjllZDYzNzQ5MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.qGT2Dm1lgvCTESM1K6DgzwP55Tv40w_2kS7CL0jno_I"></a>
<p dir="auto">It was because of sanctions. However, they told me that they will not restore the data, even if I leave Iran someday:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9091958/492630571-c934ce8b-ca83-471c-991a-269548a9e584.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTg2MDkzMDIsIm5iZiI6MTc1ODYwOTAwMiwicGF0aCI6Ii85MDkxOTU4LzQ5MjYzMDU3MS1jOTM0Y2U4Yi1jYTgzLTQ3MWMtOTkxYS0yNjk1NDhhOWU1ODQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDkyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA5MjNUMDYzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YjMwOWQ2NDAzYmU1NzBiOWY0Mzg3YTkxYzg4NzE4Zjg4NzkxYjM4ZTNkNjdlM2UxMzA5ZTQxZGRiZTdjM2FiZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.AFrKoAhkapFhYHrIBuRAPkgqpnyx3MPOIp9ElPC1qWM"><img width="429" height="675" alt="image" src="https://private-user-images.githubusercontent.com/9091958/492630571-c934ce8b-ca83-471c-991a-269548a9e584.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTg2MDkzMDIsIm5iZiI6MTc1ODYwOTAwMiwicGF0aCI6Ii85MDkxOTU4LzQ5MjYzMDU3MS1jOTM0Y2U4Yi1jYTgzLTQ3MWMtOTkxYS0yNjk1NDhhOWU1ODQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDkyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA5MjNUMDYzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YjMwOWQ2NDAzYmU1NzBiOWY0Mzg3YTkxYzg4NzE4Zjg4NzkxYjM4ZTNkNjdlM2UxMzA5ZTQxZGRiZTdjM2FiZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.AFrKoAhkapFhYHrIBuRAPkgqpnyx3MPOIp9ElPC1qWM"></a>
<p dir="auto">That said, I am very happy with my own self-hosted <a href="https://github.com/siyuan-note/siyuan">Siyuan</a> now.</p>
<p dir="auto"><h2 dir="auto">Mike Cardwell kindly asked me to fuck off</h2><a id="user-content-mike-cardwell-kindly-asked-me-to-fuck-off" aria-label="Permalink: Mike Cardwell kindly asked me to fuck off" href="#mike-cardwell-kindly-asked-me-to-fuck-off"></a></p>
<p dir="auto">I read hackernews on a daily basis and I visit lots of different websites regularly. I am almost always on my VPN as I am internally firewalled by
the government and externally shooed because of the sanctions, so I am probably missing some of these heart-warming messages:</p>
<blockquote>
<p dir="auto">Iranian IPs are blocked here, due to your decision to arm Russia with drones so that they can indiscriminately massacre civilians.</p>
</blockquote>
<p dir="auto">My VPN turned off, and opening <a href="https://www.grepular.com/" rel="nofollow">https://www.grepular.com</a> showed me this message. I actually do not blame the people who do this. I think there is
a fundamental misconception that people think because "Islamic Republic" has the word "Republic" in it, it must be a government of people in charge. That's not the case. I have yet to see anyone who actually supports Russian aggression in my real life in Iran.
Funny enough, Iran's history is full of backstabs by the Russian government.</p>
<p dir="auto">I tried contacting the author by sending this email:</p>
<pre><code>Hi Mark,

I hope this message finds you well.

While browsing HackerNews, I came across your website but was greeted with this message:

&gt; Iranian IPs are blocked here, due to your decision to arm Russia with drones so that they can indiscriminately massacre civilians.

I wanted to clarify that the decision to support Russia does not represent the Iranian people. That "your decision" refers to the regime, a theocratic minority that rules Iran without democratic legitimacy. The people of Iran have long protested and revolted against this regime, but unfortunately, they face brutal suppression while unarmed.

In my experience, most Iranians around me, including myself, stand firmly with Ukraine and against Russian aggression.

I’m not asking you to reconsider the IP restriction, you have your reasons and I respect that. I simply wanted to share this perspective and express my solidarity with Ukraine.

Slava Ukraini!

Best regards,
Avestura
</code></pre>
<p dir="auto">I got no replies from them, and I actually didn't expect one.</p>
<p dir="auto"><h2 dir="auto">GitHub freaked me out</h2><a id="user-content-github-freaked-me-out" aria-label="Permalink: GitHub freaked me out" href="#github-freaked-me-out"></a></p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9091958/492637005-641fc0ea-69f7-4fcd-acdd-0e51fc805f85.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTg2MDkzMDIsIm5iZiI6MTc1ODYwOTAwMiwicGF0aCI6Ii85MDkxOTU4LzQ5MjYzNzAwNS02NDFmYzBlYS02OWY3LTRmY2QtYWNkZC0wZTUxZmM4MDVmODUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDkyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA5MjNUMDYzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NWMzNjRhYWYwOTRiYjkwYjNkMDcwODJlOTUzODE3OTQ1YjYzZDc2MGQ3ZDJjNjc4MTA4NzZjYTYzMWM1ZWI5NCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.iGhqNMK8FIXGztELHd2ZcypMrZejiPHcDtSVz9aGYTM"><img width="2000" height="321" alt="image" src="https://private-user-images.githubusercontent.com/9091958/492637005-641fc0ea-69f7-4fcd-acdd-0e51fc805f85.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTg2MDkzMDIsIm5iZiI6MTc1ODYwOTAwMiwicGF0aCI6Ii85MDkxOTU4LzQ5MjYzNzAwNS02NDFmYzBlYS02OWY3LTRmY2QtYWNkZC0wZTUxZmM4MDVmODUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDkyMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA5MjNUMDYzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NWMzNjRhYWYwOTRiYjkwYjNkMDcwODJlOTUzODE3OTQ1YjYzZDc2MGQ3ZDJjNjc4MTA4NzZjYTYzMWM1ZWI5NCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.iGhqNMK8FIXGztELHd2ZcypMrZejiPHcDtSVz9aGYTM"></a>
<p dir="auto">I woke up to the news that GitHub has removed the access of Iranians to their private repositories. Well, that was not good. I tried to launch my own self-hosted instance of Gitea to reduce the damage. However, later, GitHub announced that <a href="https://github.blog/news-insights/policy-news-and-insights/advancing-developer-freedom-github-is-fully-available-in-iran/" rel="nofollow">github is now available in Iran</a> by securing a license from the US government, and we're now good. You see? The weather is good, the birds are singing, GitHub is free again. Fantastic!</p>
<p dir="auto"><h2 dir="auto">GitLab freaked me out</h2><a id="user-content-gitlab-freaked-me-out" aria-label="Permalink: GitLab freaked me out" href="#gitlab-freaked-me-out"></a></p>
<p dir="auto">Similarly, GitLab banned every account that once accessed from an Iranian IP, however, to this day, they never lifted the
ban, even on public repositories. I guess they couldn't secure a license from the US government, or they simply never cared.
Good luck to them in either case, though. GitLab is an amazing software. One can always self-host it.</p>
<p dir="auto"><h2 dir="auto">List goes on</h2><a id="user-content-list-goes-on" aria-label="Permalink: List goes on" href="#list-goes-on"></a></p>
<p dir="auto">The list goes on, and almost all of the services you probabelly heard of is banned here: Cloud platforms (AWS, GCP, Azure, ...),
Educational platforms (coursera, udemy, etc), Payment software (stripe, paypal, ...).</p>
<p dir="auto"><h2 dir="auto">Lessons Learned for me</h2><a id="user-content-lessons-learned-for-me" aria-label="Permalink: Lessons Learned for me" href="#lessons-learned-for-me"></a></p>
<p dir="auto">I don't think any of these companies have bad intentions towards any group of people. They are a business after all. They don't hate their customers; they are just playing the game, and the game has such rules.
But if someday some law or government forces me to prevent my services from a group, I'll think twice before writing those <code>if</code> statements. I'll try to have more empathy. People behind those screens are more important than just some rows in my tables.</p>
<p dir="auto"><h2 dir="auto">Footnote</h2><a id="user-content-footnote" aria-label="Permalink: Footnote" href="#footnote"></a></p>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">In this text, I am NOT asking for the removal of
the sanctions targeted at the Islamic Republic of Iran. I am merely remembering some moments on top of my head.
For the record, I do not support
the actions of the Islamic Republic, and on the contrary, I am in favor of the movements that release the people from such
a mafia-like cult ruling a country with thousands of years of history.
The actions of the group in charge of Iran are not defensible, and as a matter of fact,
the people of Iran are the first layer of victims. Some examples are listed <a href="https://en.wikipedia.org/wiki/Human_rights_in_the_Islamic_Republic_of_Iran" rel="nofollow">here</a>.
I especially feel it differently, as regime thugs put a gun to the throat of a dear person to me, and threatened to kill him if he showed up in
<a href="https://en.wikipedia.org/wiki/Mahsa_Amini_protests" rel="nofollow">protests</a>.</p>
</div>
<hr>
<p dir="auto">By the way, did you know you could return <code>451 Unavailable For Legal Reasons</code> instead of <code>403 Forbidden</code> when you're going to ban me next time?</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zoxide: A Better CD Command (228 pts)]]></title>
            <link>https://github.com/ajeetdsouza/zoxide</link>
            <guid>45342943</guid>
            <pubDate>Tue, 23 Sep 2025 04:48:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ajeetdsouza/zoxide">https://github.com/ajeetdsouza/zoxide</a>, See on <a href="https://news.ycombinator.com/item?id=45342943">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto">
<p dir="auto"><sup>Special thanks to:</sup></p>

<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/warpdotdev/brand-assets/refs/heads/main/Github/Sponsor/Warp-Github-LG-03.png"><img alt="Sponsored by Warp" width="230" src="https://raw.githubusercontent.com/warpdotdev/brand-assets/refs/heads/main/Github/Sponsor/Warp-Github-LG-03.png"></a></p>
<p><sup><b>Warp, built for coding with multiple AI agents.</b></sup></p>
<p><sup>Available for macOS, Linux, and Windows.</sup></p>

<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">zoxide</h2><a id="user-content-zoxide" aria-label="Permalink: zoxide" href="#zoxide"></a></p>
<p dir="auto"><a href="https://crates.io/crates/zoxide" rel="nofollow"><img src="https://camo.githubusercontent.com/c600cc259ea1db64e40df12e33b80707dd3a383f09bbd72c1315225d8b29183f/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f7a6f786964653f6c6f676f3d72757374266c6f676f436f6c6f723d7768697465267374796c653d666c61742d737175617265" alt="crates.io" data-canonical-src="https://img.shields.io/crates/v/zoxide?logo=rust&amp;logoColor=white&amp;style=flat-square"></a>
<a href="https://github.com/ajeetdsouza/zoxide/releases"><img src="https://camo.githubusercontent.com/4724c9dca44447f4a8b3d5e8e36354c960f0642ef2c7bf75ee4a406d334f5690/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f616a65657464736f757a612f7a6f786964652f746f74616c3f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465267374796c653d666c61742d737175617265" alt="Downloads" data-canonical-src="https://img.shields.io/github/downloads/ajeetdsouza/zoxide/total?logo=github&amp;logoColor=white&amp;style=flat-square"></a>
<a href="https://builtwithnix.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/1020260f225ef60f8901026b2dea1ce80fa2208b469b11b03a140e779e65de13/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c74776974682d6e69782d3764383166373f6c6f676f3d6e69786f73266c6f676f436f6c6f723d7768697465267374796c653d666c61742d737175617265" alt="Built with Nix" data-canonical-src="https://img.shields.io/badge/builtwith-nix-7d81f7?logo=nixos&amp;logoColor=white&amp;style=flat-square"></a></p>
<p dir="auto">zoxide is a <strong>smarter cd command</strong>, inspired by z and autojump.</p>
<p dir="auto">It remembers which directories you use most frequently, so you can "jump" to
them in just a few keystrokes.<br>
zoxide works on all major shells.</p>
<p dir="auto"><a href="#getting-started">Getting started</a> •
<a href="#installation">Installation</a> •
<a href="#configuration">Configuration</a> •
<a href="#third-party-integrations">Integrations</a></p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ajeetdsouza/zoxide/blob/main/contrib/tutorial.webp"><img src="https://github.com/ajeetdsouza/zoxide/raw/main/contrib/tutorial.webp" alt="Tutorial"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="z foo              # cd into highest ranked directory matching foo
z foo bar          # cd into highest ranked directory matching foo and bar
z foo /            # cd into a subdirectory starting with foo

z ~/foo            # z also works like a regular cd command
z foo/             # cd into relative path
z ..               # cd one level up
z -                # cd into previous directory

zi foo             # cd with interactive selection (using fzf)

z foo<SPACE><TAB>  # show interactive completions (zoxide v0.8.0+, bash 4.4+/fish/zsh only)"><pre>z foo              <span><span>#</span> cd into highest ranked directory matching foo</span>
z foo bar          <span><span>#</span> cd into highest ranked directory matching foo and bar</span>
z foo /            <span><span>#</span> cd into a subdirectory starting with foo</span>

z <span>~</span>/foo            <span><span>#</span> z also works like a regular cd command</span>
z foo/             <span><span>#</span> cd into relative path</span>
z ..               <span><span>#</span> cd one level up</span>
z -                <span><span>#</span> cd into previous directory</span>

zi foo             <span><span>#</span> cd with interactive selection (using fzf)</span>

z foo<span>&lt;</span>SPACE&gt;&lt;TAB<span>&gt;</span>  <span><span>#</span> show interactive completions (zoxide v0.8.0+, bash 4.4+/fish/zsh only)</span></pre></div>
<p dir="auto">Read more about the matching algorithm <a href="https://github.com/ajeetdsouza/zoxide/wiki/Algorithm#matching">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">zoxide can be installed in 4 easy steps:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Install binary</strong></p>
<p dir="auto">zoxide runs on most major platforms. If your platform isn't listed below,
please <a href="https://github.com/ajeetdsouza/zoxide/issues/new">open an issue</a>.</p>
<details>
<summary>Linux / WSL</summary>
<blockquote>
<p dir="auto">The recommended way to install zoxide is via the install script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh"><pre>curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh <span>|</span> sh</pre></div>
<p dir="auto">Or, you can use a package manager:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Distribution</th>
<th>Repository</th>
<th>Instructions</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><em>Any</em></strong></td>
<td><strong><a href="https://crates.io/crates/zoxide" rel="nofollow">crates.io</a></strong></td>
<td><code>cargo install zoxide --locked</code></td>
</tr>
<tr>
<td><em>Any</em></td>
<td><a href="https://github.com/asdf-vm/asdf">asdf</a></td>
<td><code>asdf plugin add zoxide https://github.com/nyrst/asdf-zoxide.git</code> <br> <code>asdf install zoxide latest</code></td>
</tr>
<tr>
<td><em>Any</em></td>
<td><a href="https://anaconda.org/conda-forge/zoxide" rel="nofollow">conda-forge</a></td>
<td><code>conda install -c conda-forge zoxide</code></td>
</tr>
<tr>
<td><em>Any</em></td>
<td><a href="https://packages.guix.gnu.org/packages/zoxide/" rel="nofollow">guix</a></td>
<td><code>guix install zoxide</code></td>
</tr>
<tr>
<td><em>Any</em></td>
<td><a href="https://formulae.brew.sh/formula-linux/zoxide" rel="nofollow">Linuxbrew</a></td>
<td><code>brew install zoxide</code></td>
</tr>
<tr>
<td><em>Any</em></td>
<td><a href="https://github.com/NixOS/nixpkgs/blob/master/pkgs/by-name/zo/zoxide/package.nix">nixpkgs</a></td>
<td><code>nix-env -iA nixpkgs.zoxide</code></td>
</tr>
<tr>
<td>AlmaLinux</td>
<td></td>
<td><code>dnf install zoxide</code></td>
</tr>
<tr>
<td>Alpine Linux 3.13+</td>
<td><a href="https://pkgs.alpinelinux.org/packages?name=zoxide" rel="nofollow">Alpine Linux Packages</a></td>
<td><code>apk add zoxide</code></td>
</tr>
<tr>
<td>Arch Linux</td>
<td><a href="https://archlinux.org/packages/extra/x86_64/zoxide/" rel="nofollow">Arch Linux Extra</a></td>
<td><code>pacman -S zoxide</code></td>
</tr>
<tr>
<td>CentOS Stream</td>
<td></td>
<td><code>dnf install zoxide</code></td>
</tr>
<tr>
<td><del>Debian 11+</del><sup><a href="#user-content-fn-1-fad79386de4d55dbdbd472811d9bcdf2" id="user-content-fnref-1-fad79386de4d55dbdbd472811d9bcdf2" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></td>
<td><del><a href="https://packages.debian.org/stable/admin/zoxide" rel="nofollow">Debian Packages</a></del></td>
<td><del><code>apt install zoxide</code></del></td>
</tr>
<tr>
<td>Devuan 4.0+</td>
<td><a href="https://pkginfo.devuan.org/cgi-bin/package-query.html?c=package&amp;q=zoxide" rel="nofollow">Devuan Packages</a></td>
<td><code>apt install zoxide</code></td>
</tr>
<tr>
<td>Exherbo Linux</td>
<td><a href="https://gitlab.exherbo.org/exherbo/rust/-/tree/master/packages/sys-apps/zoxide" rel="nofollow">Exherbo packages</a></td>
<td><code>cave resolve -x repository/rust</code> <br> <code>cave resolve -x zoxide</code></td>
</tr>
<tr>
<td>Fedora 32+</td>
<td><a href="https://src.fedoraproject.org/rpms/rust-zoxide" rel="nofollow">Fedora Packages</a></td>
<td><code>dnf install zoxide</code></td>
</tr>
<tr>
<td>Gentoo</td>
<td><a href="https://packages.gentoo.org/packages/app-shells/zoxide" rel="nofollow">Gentoo Packages</a></td>
<td><code>emerge app-shells/zoxide</code></td>
</tr>
<tr>
<td>Linux Mint</td>
<td><a href="https://apt.cli.rs/" rel="nofollow">apt.cli.rs</a> (unofficial)</td>
<td><a href="https://github.com/emmatyping/apt.cli.rs#how-to-add-the-repo">Setup the repository</a>, then <code>apt install zoxide</code></td>
</tr>
<tr>
<td>Manjaro</td>
<td></td>
<td><code>pacman -S zoxide</code></td>
</tr>
<tr>
<td>openSUSE Tumbleweed</td>
<td><a href="https://build.opensuse.org/package/show/openSUSE:Factory/zoxide" rel="nofollow">openSUSE Factory</a></td>
<td><code>zypper install zoxide</code></td>
</tr>
<tr>
<td><del>Parrot OS</del><sup><a href="#user-content-fn-1-fad79386de4d55dbdbd472811d9bcdf2" id="user-content-fnref-1-2-fad79386de4d55dbdbd472811d9bcdf2" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></td>
<td></td>
<td><del><code>apt install zoxide</code></del></td>
</tr>
<tr>
<td><del>Raspbian 11+</del><sup><a href="#user-content-fn-1-fad79386de4d55dbdbd472811d9bcdf2" id="user-content-fnref-1-3-fad79386de4d55dbdbd472811d9bcdf2" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></td>
<td><del><a href="https://archive.raspbian.org/raspbian/pool/main/r/rust-zoxide/" rel="nofollow">Raspbian Packages</a></del></td>
<td><del><code>apt install zoxide</code></del></td>
</tr>
<tr>
<td>RHEL 8+</td>
<td></td>
<td><code>dnf install zoxide</code></td>
</tr>
<tr>
<td>Rhino Linux</td>
<td><a href="https://pacstall.dev/packages/zoxide-deb" rel="nofollow">Pacstall Packages</a></td>
<td><code>pacstall -I zoxide-deb</code></td>
</tr>
<tr>
<td>Rocky Linux</td>
<td></td>
<td><code>dnf install zoxide</code></td>
</tr>
<tr>
<td>Slackware 15.0+</td>
<td><a href="https://slackbuilds.org/repository/15.0/system/zoxide/" rel="nofollow">SlackBuilds</a></td>
<td><a href="https://slackbuilds.org/howto/" rel="nofollow">Instructions</a></td>
</tr>
<tr>
<td>Solus</td>
<td><a href="https://github.com/getsolus/packages/tree/main/packages/z/zoxide/">Solus Packages</a></td>
<td><code>eopkg install zoxide</code></td>
</tr>
<tr>
<td>Ubuntu</td>
<td><a href="https://apt.cli.rs/" rel="nofollow">apt.cli.rs</a> (unofficial)</td>
<td><a href="https://github.com/emmatyping/apt.cli.rs#how-to-add-the-repo">Setup the repository</a>, then <code>apt install zoxide</code></td>
</tr>
<tr>
<td>Void Linux</td>
<td><a href="https://github.com/void-linux/void-packages/tree/master/srcpkgs/zoxide">Void Linux Packages</a></td>
<td><code>xbps-install -S zoxide</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</blockquote>
</details>
<details>
<summary>macOS</summary>
<blockquote>
<p dir="auto">To install zoxide, use a package manager:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Repository</th>
<th>Instructions</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><a href="https://crates.io/crates/zoxide" rel="nofollow">crates.io</a></strong></td>
<td><code>cargo install zoxide --locked</code></td>
</tr>
<tr>
<td><strong><a href="https://formulae.brew.sh/formula/zoxide" rel="nofollow">Homebrew</a></strong></td>
<td><code>brew install zoxide</code></td>
</tr>
<tr>
<td><a href="https://github.com/asdf-vm/asdf">asdf</a></td>
<td><code>asdf plugin add zoxide https://github.com/nyrst/asdf-zoxide.git</code> <br> <code>asdf install zoxide latest</code></td>
</tr>
<tr>
<td><a href="https://anaconda.org/conda-forge/zoxide" rel="nofollow">conda-forge</a></td>
<td><code>conda install -c conda-forge zoxide</code></td>
</tr>
<tr>
<td><a href="https://ports.macports.org/port/zoxide/summary" rel="nofollow">MacPorts</a></td>
<td><code>port install zoxide</code></td>
</tr>
<tr>
<td><a href="https://github.com/NixOS/nixpkgs/blob/master/pkgs/by-name/zo/zoxide/package.nix">nixpkgs</a></td>
<td><code>nix-env -iA nixpkgs.zoxide</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Or, run this command in your terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh"><pre>curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh <span>|</span> sh</pre></div>
</blockquote>
</details>
<details>
<summary>Windows</summary>
<blockquote>
<p dir="auto">zoxide works with PowerShell, as well as shells running in Cygwin, Git
Bash, and MSYS2.</p>
<p dir="auto">The recommended way to install zoxide is via <code>winget</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="winget install ajeetdsouza.zoxide"><pre>winget install ajeetdsouza.zoxide</pre></div>
<p dir="auto">Or, you can use an alternative package manager:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Repository</th>
<th>Instructions</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><a href="https://crates.io/crates/zoxide" rel="nofollow">crates.io</a></strong></td>
<td><code>cargo install zoxide --locked</code></td>
</tr>
<tr>
<td><a href="https://community.chocolatey.org/packages/zoxide" rel="nofollow">Chocolatey</a></td>
<td><code>choco install zoxide</code></td>
</tr>
<tr>
<td><a href="https://anaconda.org/conda-forge/zoxide" rel="nofollow">conda-forge</a></td>
<td><code>conda install -c conda-forge zoxide</code></td>
</tr>
<tr>
<td><a href="https://github.com/ScoopInstaller/Main/tree/master/bucket/zoxide.json">Scoop</a></td>
<td><code>scoop install zoxide</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">If you're using Cygwin, Git Bash, or MSYS2, you can also use the install script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh"><pre>curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh <span>|</span> sh</pre></div>
</blockquote>
</details>
<details>
<summary>BSD</summary>
<blockquote>
<p dir="auto">To install zoxide, use a package manager:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Distribution</th>
<th>Repository</th>
<th>Instructions</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><em>Any</em></strong></td>
<td><strong><a href="https://crates.io/crates/zoxide" rel="nofollow">crates.io</a></strong></td>
<td><code>cargo install zoxide --locked</code></td>
</tr>
<tr>
<td>DragonFly BSD</td>
<td><a href="https://github.com/DragonFlyBSD/DPorts/tree/master/sysutils/zoxide">DPorts</a></td>
<td><code>pkg install zoxide</code></td>
</tr>
<tr>
<td>FreeBSD</td>
<td><a href="https://www.freshports.org/sysutils/zoxide/" rel="nofollow">FreshPorts</a></td>
<td><code>pkg install zoxide</code></td>
</tr>
<tr>
<td>NetBSD</td>
<td><a href="https://pkgsrc.se/sysutils/zoxide" rel="nofollow">pkgsrc</a></td>
<td><code>pkgin install zoxide</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Or, run this command in your terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -sS https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | bash"><pre>curl -sS https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh <span>|</span> bash</pre></div>
</blockquote>
</details>
<details>
<summary>Android</summary>
<blockquote>
<p dir="auto">To install zoxide, use a package manager:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Repository</th>
<th>Instructions</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/termux/termux-packages/tree/master/packages/zoxide">Termux</a></td>
<td><code>pkg install zoxide</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Or, run this command in your terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -sS https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | bash"><pre>curl -sS https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh <span>|</span> bash</pre></div>
</blockquote>
</details>
</li>
<li>
<p dir="auto"><strong>Setup zoxide on your shell</strong></p>
<p dir="auto">To start using zoxide, add it to your shell.</p>
<details>
<summary>Bash</summary>
<blockquote>
<p dir="auto">Add this to the <ins><strong>end</strong></ins> of your config file (usually <code>~/.bashrc</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="eval &quot;$(zoxide init bash)&quot;"><pre><span>eval</span> <span><span>"</span><span><span>$(</span>zoxide init bash<span>)</span></span><span>"</span></span></pre></div>
</blockquote>
</details>
<details>
<summary>Elvish</summary>
<blockquote>
<p dir="auto">Add this to the <ins><strong>end</strong></ins> of your config file (usually <code>~/.elvish/rc.elv</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="eval (zoxide init elvish | slurp)"><pre><span>eval</span> (zoxide init elvish <span>|</span> slurp)</pre></div>
<p dir="auto"><strong>Note</strong>
zoxide only supports elvish v0.18.0 and above.</p>
</blockquote>
</details>
<details>
<summary>Fish</summary>
<blockquote>
<p dir="auto">Add this to the <ins><strong>end</strong></ins> of your config file (usually
<code>~/.config/fish/config.fish</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="zoxide init fish | source"><pre>zoxide init fish <span>|</span> <span>source</span></pre></div>
</blockquote>
</details>
<details>
<summary>Nushell</summary>
<blockquote>
<p dir="auto">Add this to the <ins><strong>end</strong></ins> of your env file (find it by running <code>$nu.env-path</code>
in Nushell):</p>
<div dir="auto" data-snippet-clipboard-copy-content="zoxide init nushell | save -f ~/.zoxide.nu"><pre>zoxide init nushell <span>|</span> save -f <span>~</span>/.zoxide.nu</pre></div>
<p dir="auto">Now, add this to the <ins><strong>end</strong></ins> of your config file (find it by running
<code>$nu.config-path</code> in Nushell):</p>

<p dir="auto"><strong>Note</strong>
zoxide only supports Nushell v0.89.0+.</p>
</blockquote>
</details>
<details>
<summary>PowerShell</summary>
<blockquote>
<p dir="auto">Add this to the <ins><strong>end</strong></ins> of your config file (find it by running
<code>echo $profile</code> in PowerShell):</p>
<div dir="auto" data-snippet-clipboard-copy-content="Invoke-Expression (&amp; { (zoxide init powershell | Out-String) })"><pre><span>Invoke-Expression</span> (<span>&amp;</span> { (zoxide init powershell <span>|</span> <span>Out-String</span>) })</pre></div>
</blockquote>
</details>
<details>
<summary>Tcsh</summary>
<blockquote>
<p dir="auto">Add this to the <ins><strong>end</strong></ins> of your config file (usually <code>~/.tcshrc</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="zoxide init tcsh > ~/.zoxide.tcsh
source ~/.zoxide.tcsh"><pre>zoxide init tcsh <span>&gt;</span> <span>~</span>/.zoxide.tcsh
<span>source</span> <span>~</span>/.zoxide.tcsh</pre></div>
</blockquote>
</details>
<details>
<summary>Xonsh</summary>
<blockquote>
<p dir="auto">Add this to the <ins><strong>end</strong></ins> of your config file (usually <code>~/.xonshrc</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="execx($(zoxide init xonsh), 'exec', __xonsh__.ctx, filename='zoxide')"><pre><span>execx</span>($(<span>zoxide</span> <span>init</span> <span>xonsh</span>), <span>'exec'</span>, <span>__xonsh__</span>.<span>ctx</span>, <span>filename</span><span>=</span><span>'zoxide'</span>)</pre></div>
</blockquote>
</details>
<details>
<summary>Zsh</summary>
<blockquote>
<p dir="auto">Add this to the <ins><strong>end</strong></ins> of your config file (usually <code>~/.zshrc</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="eval &quot;$(zoxide init zsh)&quot;"><pre><span>eval</span> <span><span>"</span><span><span>$(</span>zoxide init zsh<span>)</span></span><span>"</span></span></pre></div>
<p dir="auto">For completions to work, the above line must be added <em>after</em> <code>compinit</code> is
called. You may have to rebuild your completions cache by running
<code>rm ~/.zcompdump*; compinit</code>.</p>
</blockquote>
</details>
<details>
<summary>Any POSIX shell</summary>
<blockquote>
<p dir="auto">Add this to the <ins><strong>end</strong></ins> of your config file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="eval &quot;$(zoxide init posix --hook prompt)&quot;"><pre><span>eval</span> <span><span>"</span><span><span>$(</span>zoxide init posix --hook prompt<span>)</span></span><span>"</span></span></pre></div>
</blockquote>
</details>
</li>
<li>
<p dir="auto"><strong>Install fzf</strong> <sup>(optional)</sup></p>
<p dir="auto"><a href="https://github.com/junegunn/fzf">fzf</a> is a command-line fuzzy finder, used by zoxide for completions /
interactive selection. It can be installed from <a href="https://github.com/junegunn/fzf#installation">here</a>.</p>
<blockquote>
<p dir="auto"><strong>Note</strong>
The minimum supported fzf version is v0.51.0.</p>
</blockquote>
</li>
<li>
<p dir="auto"><strong>Import your data</strong> <sup>(optional)</sup></p>
<p dir="auto">If you currently use any of these plugins, you may want to import your data
into zoxide:</p>
<details>
<summary>autojump</summary>
<blockquote>
<p dir="auto">Run this command in your terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="zoxide import --from=autojump &quot;/path/to/autojump/db&quot;"><pre>zoxide import --from=autojump <span><span>"</span>/path/to/autojump/db<span>"</span></span></pre></div>
<p dir="auto">The path usually varies according to your system:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>OS</th>
<th>Path</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux</td>
<td><code>$XDG_DATA_HOME/autojump/autojump.txt</code> or <code>$HOME/.local/share/autojump/autojump.txt</code></td>
<td><code>/home/alice/.local/share/autojump/autojump.txt</code></td>
</tr>
<tr>
<td>macOS</td>
<td><code>$HOME/Library/autojump/autojump.txt</code></td>
<td><code>/Users/Alice/Library/autojump/autojump.txt</code></td>
</tr>
<tr>
<td>Windows</td>
<td><code>%APPDATA%\autojump\autojump.txt</code></td>
<td><code>C:\Users\Alice\AppData\Roaming\autojump\autojump.txt</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</blockquote>
</details>
<details>
<summary>fasd, z, z.lua, zsh-z</summary>
<blockquote>
<p dir="auto">Run this command in your terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="zoxide import --from=z &quot;path/to/z/db&quot;"><pre>zoxide import --from=z <span><span>"</span>path/to/z/db<span>"</span></span></pre></div>
<p dir="auto">The path usually varies according to your system:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Plugin</th>
<th>Path</th>
</tr>
</thead>
<tbody>
<tr>
<td>fasd</td>
<td><code>$_FASD_DATA</code> or <code>$HOME/.fasd</code></td>
</tr>
<tr>
<td>z (bash/zsh)</td>
<td><code>$_Z_DATA</code> or <code>$HOME/.z</code></td>
</tr>
<tr>
<td>z (fish)</td>
<td><code>$Z_DATA</code> or <code>$XDG_DATA_HOME/z/data</code> or <code>$HOME/.local/share/z/data</code></td>
</tr>
<tr>
<td>z.lua (bash/zsh)</td>
<td><code>$_ZL_DATA</code> or <code>$HOME/.zlua</code></td>
</tr>
<tr>
<td>z.lua (fish)</td>
<td><code>$XDG_DATA_HOME/zlua/zlua.txt</code> or <code>$HOME/.local/share/zlua/zlua.txt</code> or <code>$_ZL_DATA</code></td>
</tr>
<tr>
<td>zsh-z</td>
<td><code>$ZSHZ_DATA</code> or <code>$_Z_DATA</code> or <code>$HOME/.z</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</blockquote>
</details>
<details>
<summary>ZLocation</summary>
<blockquote>
<p dir="auto">Run this command in PowerShell:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$db = New-TemporaryFile
(Get-ZLocation).GetEnumerator() | ForEach-Object { Write-Output ($_.Name+'|'+$_.Value+'|0') } | Out-File $db
zoxide import --from=z $db"><pre><span>$db</span> <span>=</span> <span>New-TemporaryFile</span>
(<span>Get-ZLocation</span>).GetEnumerator() <span>|</span> <span>ForEach-Object</span> { <span>Write-Output</span> (<span>$_<span>.Name</span></span><span>+</span><span><span>'</span>|<span>'</span></span><span>+</span><span>$_<span>.Value</span></span><span>+</span><span><span>'</span>|0<span>'</span></span>) } <span>|</span> <span>Out-File</span> <span>$db</span>
zoxide import <span>--</span>from<span>=</span>z <span>$db</span></pre></div>
</blockquote>
</details>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Flags</h3><a id="user-content-flags" aria-label="Permalink: Flags" href="#flags"></a></p>
<p dir="auto">When calling <code>zoxide init</code>, the following flags are available:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>--cmd</code></p>
<ul dir="auto">
<li>Changes the prefix of the <code>z</code> and <code>zi</code> commands.</li>
<li><code>--cmd j</code> would change the commands to (<code>j</code>, <code>ji</code>).</li>
<li><code>--cmd cd</code> would replace the <code>cd</code> command.</li>
</ul>
</li>
<li>
<p dir="auto"><code>--hook &lt;HOOK&gt;</code></p>
<ul dir="auto">
<li>
<p dir="auto">Changes how often zoxide increments a directory's score:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Hook</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>none</code></td>
<td>Never</td>
</tr>
<tr>
<td><code>prompt</code></td>
<td>At every shell prompt</td>
</tr>
<tr>
<td><code>pwd</code> (default)</td>
<td>Whenever the directory is changed</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</li>
</ul>
</li>
<li>
<p dir="auto"><code>--no-cmd</code></p>
<ul dir="auto">
<li>Prevents zoxide from defining the <code>z</code> and <code>zi</code> commands.</li>
<li>These functions will still be available in your shell as <code>__zoxide_z</code> and
<code>__zoxide_zi</code>, should you choose to redefine them.</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Environment variables</h3><a id="user-content-environment-variables" aria-label="Permalink: Environment variables" href="#environment-variables"></a></p>
<p dir="auto">Environment variables<sup><a href="#user-content-fn-2-fad79386de4d55dbdbd472811d9bcdf2" id="user-content-fnref-2-fad79386de4d55dbdbd472811d9bcdf2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup> can be used for configuration. They must be set before
<code>zoxide init</code> is called.</p>
<ul dir="auto">
<li>
<p dir="auto"><code>_ZO_DATA_DIR</code></p>
<ul dir="auto">
<li>
<p dir="auto">Specifies the directory in which the database is stored.</p>
</li>
<li>
<p dir="auto">The default value varies across OSes:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>OS</th>
<th>Path</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux / BSD</td>
<td><code>$XDG_DATA_HOME</code> or <code>$HOME/.local/share</code></td>
<td><code>/home/alice/.local/share</code></td>
</tr>
<tr>
<td>macOS</td>
<td><code>$HOME/Library/Application Support</code></td>
<td><code>/Users/Alice/Library/Application Support</code></td>
</tr>
<tr>
<td>Windows</td>
<td><code>%LOCALAPPDATA%</code></td>
<td><code>C:\Users\Alice\AppData\Local</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</li>
</ul>
</li>
<li>
<p dir="auto"><code>_ZO_ECHO</code></p>
<ul dir="auto">
<li>When set to 1, <code>z</code> will print the matched directory before navigating to
it.</li>
</ul>
</li>
<li>
<p dir="auto"><code>_ZO_EXCLUDE_DIRS</code></p>
<ul dir="auto">
<li>
<p dir="auto">Excludes the specified directories from the database.</p>
</li>
<li>
<p dir="auto">This is provided as a list of <a href="https://man7.org/linux/man-pages/man7/glob.7.html" rel="nofollow">globs</a>, separated by OS-specific
characters:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>OS</th>
<th>Separator</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux / macOS / BSD</td>
<td><code>:</code></td>
<td><code>$HOME:$HOME/private/*</code></td>
</tr>
<tr>
<td>Windows</td>
<td><code>;</code></td>
<td><code>$HOME;$HOME/private/*</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</li>
<li>
<p dir="auto">By default, this is set to <code>"$HOME"</code>.</p>
</li>
</ul>
</li>
<li>
<p dir="auto"><code>_ZO_FZF_OPTS</code></p>
<ul dir="auto">
<li>Custom options to pass to <a href="https://github.com/junegunn/fzf">fzf</a> during interactive selection. See
<a href="https://manpages.ubuntu.com/manpages/en/man1/fzf.1.html" rel="nofollow"><code>man fzf</code></a> for the list of options.</li>
</ul>
</li>
<li>
<p dir="auto"><code>_ZO_MAXAGE</code></p>
<ul dir="auto">
<li>Configures the <a href="https://github.com/ajeetdsouza/zoxide/wiki/Algorithm#aging">aging algorithm</a>, which limits the maximum
number of entries in the database.</li>
<li>By default, this is set to 10000.</li>
</ul>
</li>
<li>
<p dir="auto"><code>_ZO_RESOLVE_SYMLINKS</code></p>
<ul dir="auto">
<li>When set to 1, <code>z</code> will resolve symlinks before adding directories to the
database.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Third-party integrations</h2><a id="user-content-third-party-integrations" aria-label="Permalink: Third-party integrations" href="#third-party-integrations"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Application</th>
<th>Description</th>
<th>Plugin</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/rjarry/aerc">aerc</a></td>
<td>Email client</td>
<td>Natively supported</td>
</tr>
<tr>
<td><a href="https://www.alfredapp.com/" rel="nofollow">alfred</a></td>
<td>macOS launcher</td>
<td><a href="https://github.com/yihou/alfred-zoxide">alfred-zoxide</a></td>
</tr>
<tr>
<td><a href="https://github.com/mridgers/clink">clink</a></td>
<td>Improved cmd.exe for Windows</td>
<td><a href="https://github.com/shunsambongi/clink-zoxide">clink-zoxide</a></td>
</tr>
<tr>
<td><a href="https://www.gnu.org/software/emacs/" rel="nofollow">emacs</a></td>
<td>Text editor</td>
<td><a href="https://gitlab.com/Vonfry/zoxide.el" rel="nofollow">zoxide.el</a></td>
</tr>
<tr>
<td><a href="https://github.com/kyoheiu/felix">felix</a></td>
<td>File manager</td>
<td>Natively supported</td>
</tr>
<tr>
<td><a href="https://github.com/kamiyaa/joshuto">joshuto</a></td>
<td>File manager</td>
<td>Natively supported</td>
</tr>
<tr>
<td><a href="https://github.com/gokcehan/lf">lf</a></td>
<td>File manager</td>
<td>See the <a href="https://github.com/gokcehan/lf/wiki/Integrations#zoxide">wiki</a></td>
</tr>
<tr>
<td><a href="https://github.com/jarun/nnn">nnn</a></td>
<td>File manager</td>
<td><a href="https://github.com/jarun/nnn/blob/master/plugins/autojump">nnn-autojump</a></td>
</tr>
<tr>
<td><a href="https://github.com/ranger/ranger">ranger</a></td>
<td>File manager</td>
<td><a href="https://github.com/jchook/ranger-zoxide">ranger-zoxide</a></td>
</tr>
<tr>
<td><a href="https://www.raycast.com/" rel="nofollow">raycast</a></td>
<td>macOS launcher</td>
<td><a href="https://www.raycast.com/mrpunkin/raycast-zoxide" rel="nofollow">raycast-zoxide</a></td>
</tr>
<tr>
<td><a href="https://github.com/dsxmachina/rfm">rfm</a></td>
<td>File manager</td>
<td>Natively supported</td>
</tr>
<tr>
<td><a href="https://github.com/joshmedeski/sesh">sesh</a></td>
<td><code>tmux</code> session manager</td>
<td>Natively supported</td>
</tr>
<tr>
<td><a href="https://github.com/nvim-telescope/telescope.nvim">telescope.nvim</a></td>
<td>Fuzzy finder for Neovim</td>
<td><a href="https://github.com/jvgrootveld/telescope-zoxide">telescope-zoxide</a></td>
</tr>
<tr>
<td><a href="https://github.com/27medkamal/tmux-session-wizard">tmux-session-wizard</a></td>
<td><code>tmux</code> session manager</td>
<td>Natively supported</td>
</tr>
<tr>
<td><a href="https://github.com/omerxx/tmux-sessionx">tmux-sessionx</a></td>
<td><code>tmux</code> session manager</td>
<td>Natively supported</td>
</tr>
<tr>
<td><a href="https://github.com/vim/vim">vim</a> / <a href="https://github.com/neovim/neovim">neovim</a></td>
<td>Text editor</td>
<td><a href="https://github.com/nanotee/zoxide.vim">zoxide.vim</a></td>
</tr>
<tr>
<td><a href="https://github.com/sayanarijit/xplr">xplr</a></td>
<td>File manager</td>
<td><a href="https://github.com/sayanarijit/zoxide.xplr">zoxide.xplr</a></td>
</tr>
<tr>
<td><a href="https://github.com/xxh/xxh">xxh</a></td>
<td>Transports shell configuration over SSH</td>
<td><a href="https://github.com/xxh/xxh-plugin-prerun-zoxide">xxh-plugin-prerun-zoxide</a></td>
</tr>
<tr>
<td><a href="https://github.com/sxyazi/yazi">yazi</a></td>
<td>File manager</td>
<td>Natively supported</td>
</tr>
<tr>
<td><a href="https://github.com/Mellbourn/zabb">zabb</a></td>
<td>Finds the shortest possible query for a path</td>
<td>Natively supported</td>
</tr>
<tr>
<td><a href="https://github.com/roberte777/zesh">zesh</a></td>
<td><code>zellij</code> session manager</td>
<td>Natively supported</td>
</tr>
<tr>
<td><a href="https://github.com/marlonrichert/zsh-autocomplete">zsh-autocomplete</a></td>
<td>Realtime completions for zsh</td>
<td>Natively supported</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<section data-footnotes="">
<ol dir="auto">
<li id="user-content-fn-1-fad79386de4d55dbdbd472811d9bcdf2">
<p dir="auto">Debian and its derivatives update their packages very slowly. If you're
using one of these distributions, consider using the install script instead. <a href="#user-content-fnref-1-fad79386de4d55dbdbd472811d9bcdf2" data-footnote-backref="" aria-label="Back to reference 1">↩</a> <a href="#user-content-fnref-1-2-fad79386de4d55dbdbd472811d9bcdf2" data-footnote-backref="" aria-label="Back to reference 1-2">↩<sup>2</sup></a> <a href="#user-content-fnref-1-3-fad79386de4d55dbdbd472811d9bcdf2" data-footnote-backref="" aria-label="Back to reference 1-3">↩<sup>3</sup></a></p>
</li>
<li id="user-content-fn-2-fad79386de4d55dbdbd472811d9bcdf2">
<p dir="auto">If you're not sure how to set an environment variable on your shell, check
out the <a href="https://github.com/ajeetdsouza/zoxide/wiki/HOWTO:-set-environment-variables" title="HOWTO: set environment variables">wiki</a>. <a href="#user-content-fnref-2-fad79386de4d55dbdbd472811d9bcdf2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
</ol>
</section>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nine Things I Learned in Ninety Years (636 pts)]]></title>
            <link>http://edwardpackard.com/wp-content/uploads/2025/09/Nine-Things-I-Learned-in-Ninety-Years.pdf</link>
            <guid>45342364</guid>
            <pubDate>Tue, 23 Sep 2025 03:03:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://edwardpackard.com/wp-content/uploads/2025/09/Nine-Things-I-Learned-in-Ninety-Years.pdf">http://edwardpackard.com/wp-content/uploads/2025/09/Nine-Things-I-Learned-in-Ninety-Years.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45342364">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Pocket Casts, You Altered the Deal, So I Will Alter Your App (111 pts)]]></title>
            <link>https://blog.matthewbrunelle.com/podcasts-you-altered-the-deal-so-i-will-alter-your-app/</link>
            <guid>45342319</guid>
            <pubDate>Tue, 23 Sep 2025 02:56:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.matthewbrunelle.com/podcasts-you-altered-the-deal-so-i-will-alter-your-app/">https://blog.matthewbrunelle.com/podcasts-you-altered-the-deal-so-i-will-alter-your-app/</a>, See on <a href="https://news.ycombinator.com/item?id=45342319">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
<main>

        <article>

        

    <div>
        
<h3 id="history-so-far">History so far</h3>
<p>Pocket Casts, people are not upset you are trying to find revenue sources. People are upset you are reneging on your promise.</p>
<p>You were a pay-once app. Released in 2011, pay once each for Android, iOS, and Web and keep for life.</p>
<p>In 2018 NPR <a href="https://www.npr.org/about-npr/607823388/pocket-cast-acquired?ref=blog.matthewbrunelle.com">acquired you</a>.  Then in 2019 you moved to a subscription for all users, including the paid ones. People were upset so you backtracked and <a href="https://blog.pocketcasts.com/2019/09/18/major-new-update/?ref=blog.matthewbrunelle.com">grandfathered them into</a> a lifetime plus membership. Sadly, in 2020 NPR lost <a href="https://current.org/2021/01/public-media-owners-agree-to-sell-pocket-casts-podcast-platform/?ref=blog.matthewbrunelle.com">$800k from running you</a> so you were <a href="https://wordpress.com/blog/2021/07/16/popular-podcast-app-pocket-casts-joins-automattic/?ref=blog.matthewbrunelle.com">sold to Automattic</a>.</p>
<p>In 2024, you renamed all "Pocket Casts Lifetime Members" to <a href="https://support.pocketcasts.com/knowledge-base/lifetime-access-to-pocket-casts-plus/?ref=blog.matthewbrunelle.com">"Pocket Casts Champions"</a> and we knew shenanigans were afoot.</p>
<p>This week you flipped the switch to turn on ads in the app:<br>
<img src="https://blog.matthewbrunelle.com/content/images/2025/09/1000002462.png" alt="1000002462.png" loading="lazy"></p>
<p>However, in 2022 you <a href="https://blog.pocketcasts.com/2022/10/19/pocket-casts-mobile-apps-are-now-open-source/?ref=blog.matthewbrunelle.com">made your mobile apps open source</a>, so there is something that can be done: I <a href="https://github.com/ciferkey/pocket-casts-android?ref=blog.matthewbrunelle.com">forked</a> the repo.</p>
<hr>
<h3 id="exorcising-the-ads">Exorcising the ads</h3>
<p>In the app you may be able to see the new nag that says: "Unlock folders, bookmarks, transcripts and more with Pocket Casts Plus" just above the artwork in the player screen. I don't see that exact string in the <a href="https://blog.matthewbrunelle.com/podcasts-you-altered-the-deal-so-i-will-alter-your-app/github.com/Automattic/pocket-casts-android">GitHub repo</a>, I think that's because of internationalization.</p>
<p>However, I found where the <a href="https://github.com/Automattic/pocket-casts-android/blob/f17d2a0408dd299684eb4afa11fb07f4df1fc59e/modules/features/player/src/main/java/au/com/shiftyjelly/pocketcasts/player/view/PlayerHeaderFragment.kt?ref=blog.matthewbrunelle.com#L99">artwork view fragment is</a> and that it contains <code>AdAndArtworkHorizontal</code>. Looking at the changelog I can see that this was added in <a href="https://github.com/Automattic/pocket-casts-android/pull/4086?ref=blog.matthewbrunelle.com">Display ads in the player #4086</a> which created the flag <code>Feature.BANNER_ADS</code>.</p>
<p>In the project, a feature flag has the following details:</p>
<pre><code>enum class Feature(  
    val key: String,  
    val title: String,  
    val defaultValue: Boolean,  
    val tier: FeatureTier,  
    val hasFirebaseRemoteFlag: Boolean,  
    val hasDevToggle: Boolean,  
)
</code></pre>
<p>With some further updates to the project, the flag is now <a href="https://github.com/Automattic/pocket-casts-android/blob/f17d2a0408dd299684eb4afa11fb07f4df1fc59e/modules/services/utils/src/main/java/au/com/shiftyjelly/pocketcasts/utils/featureflag/Feature.kt?ref=blog.matthewbrunelle.com#L125-L140">broken in two</a>:</p>
<pre><code>BANNER_ADS_PLAYER(  
    key = "banner_ad_player",  
    title = "Banner Ads Player",  
    defaultValue = true,  
    tier = FeatureTier.Free,  
    hasFirebaseRemoteFlag = true,  
    hasDevToggle = true,  
),  
BANNER_ADS_PODCASTS(  
    key = "banner_ad_podcasts",  
    title = "Banner Ads Podcasts",  
    defaultValue = true,  
    tier = FeatureTier.Free,  
    hasFirebaseRemoteFlag = true,  
    hasDevToggle = true,  
),
</code></pre>
<p>The flags are ultimately used by <a href="https://github.com/Automattic/pocket-casts-android/blob/f17d2a0408dd299684eb4afa11fb07f4df1fc59e/modules/services/model/src/main/java/au/com/shiftyjelly/pocketcasts/models/type/BlazeAdLocation.kt?ref=blog.matthewbrunelle.com#L11">BlazeAdLocation</a>.</p>
<p>There are a couple of different ways we could slice out the ads. For testing, I went with the simplest approach to start:</p>
<ul>
<li>Flip <code>defaultValue</code> to false.</li>
<li>Flip <code>hasFirebaseRemoteFlag</code> to false so the flag will not be remotely overridden.</li>
</ul>
<p>Anyway, based on the readme I just needed to<code>./gradlew :app:assembleDebugProd</code> and <code>./gradlew :app:installDebugProd</code>. This will install a different debug version of the app:<br>
<img src="https://blog.matthewbrunelle.com/content/images/2025/09/1000002460.png" alt="1000002460.png" loading="lazy"></p>
<p>Voila, ads gone. Though in the future this process may be tougher because of Google's <a href="https://android-developers.googleblog.com/2025/08/elevating-android-security.html?ref=blog.matthewbrunelle.com">impending crackdown on loading APKs</a>.</p>
<p>Now, after doing all that work, guess what I realized? <code>hasDevToggle</code> field in the Features class? Well the debug build has a feature toggles section:<br>
<img src="https://blog.matthewbrunelle.com/content/images/2025/09/1000002458.png" alt="1000002458.png" loading="lazy"></p>
<p>So for now you do not need to modify the project's code. You can get by with just using the debug build and toggling the feature off. Feature flags only exist for testing features though. Eventually the flag will go away and I'll have to disable the ads more directly.</p>
<hr>
<h3 id="why-did-we-end-up-here">Why did we end up here?</h3>
<p>Pocket Casts, your current approach to monetization seems extreme. Are you in a similar situation to 2020? Back then the NPR <a href="https://media.npr.org/documents/about/statements/fy2020/National%20Public%20Radio%20-%20Consolidated%20Financial%20Statements%202020.pdf?ref=blog.matthewbrunelle.com">financial report</a> only had a single line item of $812,129 for "Minus: Share of Podcast Media’s net loss". Why does a podcast service cost so much to run? If you give us more transparency, maybe we will better understand why you need to make changes.</p>
<p>Maybe more of the work can be moved on-device. You open-sourced your app. Maybe the community can help you out and contribute those changes? We're not too scary. You extended an olive branch by open-sourcing the project, so let's work together. Hopefully Automattic doesn't pull back on contributions like they <a href="https://automattic.com/2025/01/09/aligning-automattics-sponsored-contributions-to-wordpress/?ref=blog.matthewbrunelle.com">did with WordPress this year</a>.</p>
<hr>
<h3 id="an-entirely-different-option">An entirely different option</h3>
<p>As I mentioned in my post about <a href="https://blog.matthewbrunelle.com/the-apps-that-i-kept-on-grapheneos-2/#the-media-apps">the apps I kept when switching to GrapheneOS</a>,  self-hosting PinePods and using AntennaPod on your phone is a great option! That way you can still have web and desktop sync. I also said in that post:</p>
<blockquote>
<p>If I wasn't grandfathered into the free premium tier I would be more tempted to jump ship.</p>
</blockquote>
<p>The time to jump ship seems to be getting closer. I think Pocket Casts will join a future version of my <a href="https://blog.matthewbrunelle.com/the-apps-that-i-got-rid-of-when-trying-out-grapheneos/">apps that I got rid of when switching to GrapheneOS</a> post.</p>
<p>Oh, and if anyone is looking for new podcasts to listen to, <a href="https://blog.matthewbrunelle.com/what-podcasts-am-i-listening-to-2025/">I have recommendations</a>.</p>

    </div>
</article>
                
                

</main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fall Foliage Map 2025 (214 pts)]]></title>
            <link>https://www.explorefall.com/fall-foliage-map</link>
            <guid>45341324</guid>
            <pubDate>Tue, 23 Sep 2025 00:14:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.explorefall.com/fall-foliage-map">https://www.explorefall.com/fall-foliage-map</a>, See on <a href="https://news.ycombinator.com/item?id=45341324">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="more-info-4">
          <p>If you've ever traveled in search of fall foliage before, you likely know how tricky it can be to be in the right place at the right time. The timing of peak color varies signficantly season-to-season, meaning what worked one year might not work the next! The best fall trips take careful planning, a lot of patience, and a reliable fall foliage map.</p>
          <p>It's helpful to establish a baseline for when leaves <i>normally</i> change. Maps, like the one in the above section, can help you identify roughly when in the season you should be planning your trip. From there, you should consult a real-time fall foliage map like ours to see if fall foliage is on-time or running early/late due to ongoing weather conditions.</p>
          <p>If at all possible, don't solidify your plans until you're two weeks out from peak fall foliage. This allows you to be flexible should extreme weather rear its head and disrupt the normal progression of fall foliage. Should that not be an option for you, do your planning in early September when fall foliage experts can give you an idea of whether or not fall color is on-time this year.</p>
          <p>You'll want to make the most of your time in fall's splendor, so be sure to pick out a few beautiful hikes or drives on which you can truly be emersed in autumnal glory. If you're looking to beat the crowds, consider going to popular locations very early in the morning, before the majority of people arrive. Sunrise bathes fall foliage in golden hues, making early morning one of the best times to venture out!</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Designating Antifa as a domestic terrorist organization (124 pts)]]></title>
            <link>https://www.whitehouse.gov/presidential-actions/2025/09/designating-antifa-as-a-domestic-terrorist-organization/</link>
            <guid>45340814</guid>
            <pubDate>Mon, 22 Sep 2025 23:16:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.whitehouse.gov/presidential-actions/2025/09/designating-antifa-as-a-domestic-terrorist-organization/">https://www.whitehouse.gov/presidential-actions/2025/09/designating-antifa-as-a-domestic-terrorist-organization/</a>, See on <a href="https://news.ycombinator.com/item?id=45340814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>




<p><span>Section</span> <span>1</span>. &nbsp;<span>Antifa as a Terrorist Threat</span>.&nbsp; Antifa is a militarist, anarchist enterprise that explicitly calls for the overthrow of the United States Government, law enforcement authorities, and our system of law.&nbsp; It uses illegal means to organize and execute a campaign of violence and terrorism nationwide to accomplish these goals.&nbsp; This campaign involves coordinated efforts to obstruct enforcement of Federal laws through armed standoffs with law enforcement, organized riots, violent assaults on Immigration and Customs Enforcement and other law enforcement officers, and routine doxing of and other threats against political figures and activists. &nbsp;Antifa recruits, trains, and radicalizes young Americans to engage in this violence and suppression of political activity, then employs elaborate means and mechanisms to shield the identities of its operatives, conceal its funding sources and operations in an effort to frustrate law enforcement, and recruit additional members.&nbsp; Individuals associated with and acting on behalf of Antifa further coordinate with other organizations and entities for the purpose of spreading, fomenting, and advancing political violence and suppressing lawful political speech.&nbsp; This organized effort designed to achieve policy objectives by coercion and intimidation is domestic terrorism.</p>



<p><span>Sec</span>. <span>2</span>. &nbsp;<span>Designation as a Domestic Terrorist Organization</span>.&nbsp; Because of the aforementioned pattern of political violence designed to suppress lawful political activity and obstruct the rule of law, I hereby designate Antifa as a “domestic terrorist organization.”&nbsp; All relevant executive departments and agencies shall utilize all applicable authorities to investigate, disrupt, and dismantle any and all illegal operations — especially those involving terrorist actions — conducted by Antifa or any person claiming to act on behalf of Antifa, or for which Antifa or any person claiming to act on behalf of Antifa provided material support, including necessary investigatory and prosecutorial actions against those who fund such operations.&nbsp;</p>



<p><span>Sec</span>. <span>3</span>. &nbsp;<span>General Provisions</span>.&nbsp; (a)&nbsp; This order shall be implemented consistent with applicable law.&nbsp; This order is not intended to, and does not, create any right or benefit, substantive or procedural, enforceable at law or in equity by any party against the United States, its departments, agencies, or entities, its officers, employees, or agents, or any other person.</p>



<p>(b)&nbsp; This order shall be published in the <em>Federal Register.</em></p>



<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DONALD J. TRUMP</p>



<p>THE WHITE HOUSE,</p>



<p>&nbsp;&nbsp;&nbsp; September 22, 2025.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In Maine, prisoners are thriving in remote jobs, other states are taking notice (237 pts)]]></title>
            <link>https://www.mainepublic.org/2025-08-29/in-maine-prisoners-are-thriving-in-remote-jobs-and-other-states-are-taking-notice</link>
            <guid>45340600</guid>
            <pubDate>Mon, 22 Sep 2025 22:51:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mainepublic.org/2025-08-29/in-maine-prisoners-are-thriving-in-remote-jobs-and-other-states-are-taking-notice">https://www.mainepublic.org/2025-08-29/in-maine-prisoners-are-thriving-in-remote-jobs-and-other-states-are-taking-notice</a>, See on <a href="https://news.ycombinator.com/item?id=45340600">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
        Published&nbsp;August 29, 2025 at 5:12 PM EDT
    </p>
    <meta content="2025-08-29T21:12:00.981Z">


                                        </div><div>
                                        <p>People who are incarcerated are paid notoriously low wages for kitchen, laundry work and maintenance.</p><p>But the expanded use of laptops is creating other opportunities.</p>
<p><i>This is part two in a two-part series about remote work in Maine prisons. </i><a href="https://www.mainepublic.org/maine/2025-08-28/cracking-the-code-how-technology-and-education-are-changing-life-in-prison" target="_blank"><i>To read part one, click here</i></a><i>.</i></p><p>Preston Thorpe is only 32, but he says he's already landed his dream job as a senior software engineer and bought a modest house with his six-figure salary. It was all accomplished by putting in long days from his cell at the Mountain View Correctional Center in Charleston.</p><p>"It's not normal to have 15-17 hours a day to really focus on something and learn something, like deeply," Thorpe says. "And fortunately tech is one of the few areas where they're not concerned with your college degree. They're really only concerned with your ability to write code."</p>
<p>A self-described "computer geek," Thorpe says he built his first computer at age 13. In high school he always expected he'd have a career in tech. But he also had a rebellious side. He got into trouble with drugs, using them and selling them. He says his parents kicked him out of the house and he ended up in prison for the first time at age 20.</p><p>"You know, I was worried and pretty hopeless that I had messed my life up so bad that it was no longer possible to have like a normal life and normal career," he says.</p><p>When you have nothing to lose, Thorpe says it's pretty easy to behave that way. And when you're out of prison with a criminal record, no money and an identity as a convict, he says the likelihood you're going to improve your life in any way is zero.</p><p>His own circumstances changed in 2019 when he got transferred from the New Hampshire prison system to Maine, where he discovered laptops with limited internet access were available for education. That's when he says he had an epiphany that he could change himself by pursuing his passion. And about two years ago, he became one of the first incarcerated people in the country to get hired for a remote job.</p><p>"Now I feel like my life has purpose," Thorpe says.</p><div data-align-center="">
<figure>
    
        
            <picture>
    
    
        
            
    
            <source media="(max-width: 768px)" type="image/webp" width="420" height="280" srcset="https://npr.brightspotcdn.com/dims4/default/b201217/2147483647/strip/true/crop/3600x2400+0+0/resize/840x560!/format/webp/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F2a%2Fac%2F6bc44f5c43899b25843a9d43acad%2Fprestont7horpe-kb.JPG 2x" data-size="fallbackImageSizeMobile">
    

    
        <source media="(max-width: 768px)" width="420" height="280" srcset="https://npr.brightspotcdn.com/dims4/default/c9f7465/2147483647/strip/true/crop/3600x2400+0+0/resize/420x280!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F2a%2Fac%2F6bc44f5c43899b25843a9d43acad%2Fprestont7horpe-kb.JPG" data-size="fallbackImageSizeMobile">
    

        
    

    
    
        
            
        
    

    
    
        
            
        
    

    
    
        
            
    
            <source type="image/webp" width="880" height="587" srcset="https://npr.brightspotcdn.com/dims4/default/9838af1/2147483647/strip/true/crop/3600x2400+0+0/resize/1760x1174!/format/webp/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F2a%2Fac%2F6bc44f5c43899b25843a9d43acad%2Fprestont7horpe-kb.JPG 2x" data-size="fallbackImageSize">
    

    
        <source width="880" height="587" srcset="https://npr.brightspotcdn.com/dims4/default/47baa79/2147483647/strip/true/crop/3600x2400+0+0/resize/880x587!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F2a%2Fac%2F6bc44f5c43899b25843a9d43acad%2Fprestont7horpe-kb.JPG" data-size="fallbackImageSize">
    

        
    

    
    <img alt="Maine inmate Preston Thorpe talks with his team during a group work session from his cell at the Mountain View Correctional Facility in Charleston, Maine, on Aug. 18, 2025." srcset="https://npr.brightspotcdn.com/dims4/default/0d3fa0e/2147483647/strip/true/crop/3600x2400+0+0/resize/1760x1174!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F2a%2Fac%2F6bc44f5c43899b25843a9d43acad%2Fprestont7horpe-kb.JPG 2x" width="880" height="587" loading="lazy" src="https://npr.brightspotcdn.com/dims4/default/47baa79/2147483647/strip/true/crop/3600x2400+0+0/resize/880x587!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F2a%2Fac%2F6bc44f5c43899b25843a9d43acad%2Fprestont7horpe-kb.JPG">


</picture>
        
    
    
    
    
    <div>
    

    <div><p>Kevin Bennett</p>
            
                <p>/</p>
            
        
            <p> For Maine Public</p></div><figcaption>Maine inmate Preston Thorpe talks with his team during a group work session from his cell at the Mountain View Correctional Facility in Charleston, Maine, on Aug. 18, 2025.</figcaption>
    </div>
    
</figure>
</div>
<p>Glauber Costa says he first became aware of Thorpe through his contributions to an online public software project. Costa is the CEO of Turso, an international database company. He was impressed by Thorpe's work and had no idea he was incarcerated.</p><p>Once he found out, he thought it would be impossible to talk to Thorpe, let alone hire him.</p><p>"But then," Costa says, "it turns out that he can take video calls. And then by talking to him, it became very, very clear to me that if this is not a reformed person I don't know what is."</p><p>Costa says he was also surprised to learn that Thorpe was eligible for remote work while he was in prison. He hired him in June. He figured Thorpe might have trouble clearing the company's background check and he says he prepared himself for that. But since it only searches back seven years and since Thorpe has been in prison for more than a decade, "He is actually our cleanest background check," Costa says.</p><p>"He doesn't have a parking ticket."</p><p>Several dozen other prisoners are also working remote jobs. At the Maine Correctional Center in Windham, Darlene George is a certified recovery coach, a scholar and a teaching assistant who's serving a 40-year sentence for the murder of her husband.</p><div data-align-center="">
<figure>
    
        
            <picture>
    
    
        
            
    
            <source media="(max-width: 768px)" type="image/webp" width="420" height="315" srcset="https://npr.brightspotcdn.com/dims4/default/37aabe5/2147483647/strip/true/crop/4000x3000+0+0/resize/840x630!/format/webp/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fe0%2Fbb%2F1107dbaf41da9e8e92ef0e660f23%2F20250709-105418.jpg 2x" data-size="fallbackImageSizeMobile">
    

    
        <source media="(max-width: 768px)" width="420" height="315" srcset="https://npr.brightspotcdn.com/dims4/default/e1c9445/2147483647/strip/true/crop/4000x3000+0+0/resize/420x315!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fe0%2Fbb%2F1107dbaf41da9e8e92ef0e660f23%2F20250709-105418.jpg" data-size="fallbackImageSizeMobile">
    

        
    

    
    
        
            
        
    

    
    
        
            
        
    

    
    
        
            
    
            <source type="image/webp" width="880" height="660" srcset="https://npr.brightspotcdn.com/dims4/default/fa561a2/2147483647/strip/true/crop/4000x3000+0+0/resize/1760x1320!/format/webp/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fe0%2Fbb%2F1107dbaf41da9e8e92ef0e660f23%2F20250709-105418.jpg 2x" data-size="fallbackImageSize">
    

    
        <source width="880" height="660" srcset="https://npr.brightspotcdn.com/dims4/default/5d89c00/2147483647/strip/true/crop/4000x3000+0+0/resize/880x660!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fe0%2Fbb%2F1107dbaf41da9e8e92ef0e660f23%2F20250709-105418.jpg" data-size="fallbackImageSize">
    

        
    

    
    <img alt="Darlene George" srcset="https://npr.brightspotcdn.com/dims4/default/0ae6bfd/2147483647/strip/true/crop/4000x3000+0+0/resize/1760x1320!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fe0%2Fbb%2F1107dbaf41da9e8e92ef0e660f23%2F20250709-105418.jpg 2x" width="880" height="660" loading="lazy" src="https://npr.brightspotcdn.com/dims4/default/5d89c00/2147483647/strip/true/crop/4000x3000+0+0/resize/880x660!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fe0%2Fbb%2F1107dbaf41da9e8e92ef0e660f23%2F20250709-105418.jpg">


</picture>
        
    
    
    
    
    <div>
    

    <div><p>Susan Sharon</p>
            
                <p>/</p>
            
        
            <p>Maine Public </p></div><figcaption> Darlene George</figcaption>
    </div>
    
</figure>
</div>
<p>"I became incarcerated in 2009 and I've been here 16 years," George says.</p><p>Unlike most women in prison, George had a college degree before she was incarcerated. She says she still tries to make the most of every opportunity she can. For the past two years, she's held a full-time remote job, first as a grant writer and now as a program coordinator, for a Maine-based health care company.</p><p>"The work, it just, it really makes you feel good," she says. "When I — when I put my head down at night I can say I'm giving something back."</p><p>George relishes her role as a decision maker and an advocate for clients' health care. She makes a competitive salary. And she says her boss and her co-workers are extremely supportive of her situation, and so are the other women at the Maine Correctional Center.</p><p>"Literally, I work from my room. There's a sign that I put out that lets people know I'm Zooming or in meetings," she says. "They try not to be noisy on the floor ... because they're like, 'Well, we want a job, too.'"</p><div data-align-center="">
<figure>
    
        
            <picture>
    
    
        
            
    
            <source media="(max-width: 768px)" type="image/webp" width="420" height="280" srcset="https://npr.brightspotcdn.com/dims4/default/bc06d52/2147483647/strip/true/crop/3600x2400+0+0/resize/840x560!/format/webp/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Ffb%2F19%2F980ba2b14acfafd7699b7785839d%2Fprestont8horpe-kb.JPG 2x" data-size="fallbackImageSizeMobile">
    

    
        <source media="(max-width: 768px)" width="420" height="280" srcset="https://npr.brightspotcdn.com/dims4/default/a89176d/2147483647/strip/true/crop/3600x2400+0+0/resize/420x280!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Ffb%2F19%2F980ba2b14acfafd7699b7785839d%2Fprestont8horpe-kb.JPG" data-size="fallbackImageSizeMobile">
    

        
    

    
    
        
            
        
    

    
    
        
            
        
    

    
    
        
            
    
            <source type="image/webp" width="880" height="587" srcset="https://npr.brightspotcdn.com/dims4/default/f00a0d7/2147483647/strip/true/crop/3600x2400+0+0/resize/1760x1174!/format/webp/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Ffb%2F19%2F980ba2b14acfafd7699b7785839d%2Fprestont8horpe-kb.JPG 2x" data-size="fallbackImageSize">
    

    
        <source width="880" height="587" srcset="https://npr.brightspotcdn.com/dims4/default/08af25b/2147483647/strip/true/crop/3600x2400+0+0/resize/880x587!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Ffb%2F19%2F980ba2b14acfafd7699b7785839d%2Fprestont8horpe-kb.JPG" data-size="fallbackImageSize">
    

        
    

    
    <img alt="Maine prison inmate Preston Thorpe talks with his team during a group work session from his cell at the Mountain View Correctional Facility in Charleston, Maine, on Aug. 18, 2025." srcset="https://npr.brightspotcdn.com/dims4/default/7b68e2d/2147483647/strip/true/crop/3600x2400+0+0/resize/1760x1174!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Ffb%2F19%2F980ba2b14acfafd7699b7785839d%2Fprestont8horpe-kb.JPG 2x" width="880" height="587" loading="lazy" src="https://npr.brightspotcdn.com/dims4/default/08af25b/2147483647/strip/true/crop/3600x2400+0+0/resize/880x587!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Ffb%2F19%2F980ba2b14acfafd7699b7785839d%2Fprestont8horpe-kb.JPG">


</picture>
        
    
    
    
    
    <div>
    

    <div><p>Kevin Bennett</p>
            
                <p>/</p>
            
        
            <p> For Maine Public</p></div><figcaption>Maine prison inmate Preston Thorpe talks with his team during a group work session from his cell at the Mountain View Correctional Facility in Charleston, Maine, on Aug. 18, 2025.</figcaption>
    </div>
    
</figure>
</div>
<p>Mara Sanchez, the program director for the Alliance for Higher Education in Prison, says Maine's Department of Corrections was the first to have a remote work policy.</p><p>"Their implementation, willingness to try remote work for incarcerated students has really kind of set the bar for other states and been very inspiring to other states," she says.</p><p>Maine Corrections Commissioner Randall Liberty says remote work is an outgrowth of expanded educational opportunities in prison. There are 800 residents who now have access to the internet.</p><p>"We have technicians that are watching where they're going and what they're doing and we've had very few problems," Liberty says. "If it provides meaningful employment for them ... it also allows for a transition back into the community."</p><p>For example, Liberty says one resident worked as a paralegal for a law firm and continued with the job after he got out. Wages are garnished for child support, victim restitution and other fees. And for those who earn above a certain amount, 10% goes to the Department of Corrections for room and board. But residents can also save money or send it home. And Liberty says between educational programs and remote work, the prison environment is better for everyone.</p><div data-align-center="">
<figure>
    
        
            <picture>
    
    
        
            
    
            <source media="(max-width: 768px)" type="image/webp" width="420" height="280" srcset="https://npr.brightspotcdn.com/dims4/default/c19c10c/2147483647/strip/true/crop/3600x2400+0+0/resize/840x560!/format/webp/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F72%2F65%2F31e5663d4a6d8efd2e6af082c146%2Fprestont6horpe-kb.JPG 2x" data-size="fallbackImageSizeMobile">
    

    
        <source media="(max-width: 768px)" width="420" height="280" srcset="https://npr.brightspotcdn.com/dims4/default/d287b2d/2147483647/strip/true/crop/3600x2400+0+0/resize/420x280!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F72%2F65%2F31e5663d4a6d8efd2e6af082c146%2Fprestont6horpe-kb.JPG" data-size="fallbackImageSizeMobile">
    

        
    

    
    
        
            
        
    

    
    
        
            
        
    

    
    
        
            
    
            <source type="image/webp" width="880" height="587" srcset="https://npr.brightspotcdn.com/dims4/default/e6d2044/2147483647/strip/true/crop/3600x2400+0+0/resize/1760x1174!/format/webp/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F72%2F65%2F31e5663d4a6d8efd2e6af082c146%2Fprestont6horpe-kb.JPG 2x" data-size="fallbackImageSize">
    

    
        <source width="880" height="587" srcset="https://npr.brightspotcdn.com/dims4/default/8f60449/2147483647/strip/true/crop/3600x2400+0+0/resize/880x587!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F72%2F65%2F31e5663d4a6d8efd2e6af082c146%2Fprestont6horpe-kb.JPG" data-size="fallbackImageSize">
    

        
    

    
    <img alt="A list of approved computer items is taped to the door of Maine inmate Preston Thorpe, who works as a senior software engineer from his cell at the Mountain View Correctional Facility in Charleston, Maine." srcset="https://npr.brightspotcdn.com/dims4/default/b5dadba/2147483647/strip/true/crop/3600x2400+0+0/resize/1760x1174!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F72%2F65%2F31e5663d4a6d8efd2e6af082c146%2Fprestont6horpe-kb.JPG 2x" width="880" height="587" loading="lazy" src="https://npr.brightspotcdn.com/dims4/default/8f60449/2147483647/strip/true/crop/3600x2400+0+0/resize/880x587!/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F72%2F65%2F31e5663d4a6d8efd2e6af082c146%2Fprestont6horpe-kb.JPG">


</picture>
        
    
    
    
    
    <div>
    

    <div><p>Kevin Bennett</p>
            
                <p>/</p>
            
        
            <p> For Maine Public</p></div><figcaption>A list of approved computer items is taped to the door of Maine inmate Preston Thorpe, who works as a senior software engineer from his cell at the Mountain View Correctional Facility in Charleston, Maine.</figcaption>
    </div>
    
</figure>
</div>
<p>"It's important that the officers work with residents that have hope and have meaning in their life," he says. "We had 87 assaults on staff in 2017. Last year, we had seven assaults on staff. So all of this work isn't just about the residents. It's about the community ... the officers that go to work everyday and don't feel like their life is at risk."</p><p>Liberty is optimistic that remote work can be expanded to other people in prison as the network of employers who understand their value grows.</p><p>"I think it can become the norm," he says. "This isn't a reckless attempt at finding work for individuals. This is a well-thought out plan with lessons learned and consequences. ... The last thing anybody wants is to lose their laptop."</p><p>Preston Thorpe would agree. He's hoping to be released sometime next year. He never expected to have started a successful career in prison or to have bought a house. But he says what he's most proud of is that after everything he's put his parents through, they are proud of him.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Federal judge lifts administration halt of offshore wind farm in New England (215 pts)]]></title>
            <link>https://apnews.com/article/trump-renewable-energy-offshore-wind-revolution-wind-f1cbe85a829e3d5e5496f834bcb617d1</link>
            <guid>45340550</guid>
            <pubDate>Mon, 22 Sep 2025 22:45:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/trump-renewable-energy-offshore-wind-revolution-wind-f1cbe85a829e3d5e5496f834bcb617d1">https://apnews.com/article/trump-renewable-energy-offshore-wind-revolution-wind-f1cbe85a829e3d5e5496f834bcb617d1</a>, See on <a href="https://news.ycombinator.com/item?id=45340550">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>WASHINGTON (AP) — A federal judge ruled Monday that a nearly complete offshore wind project halted by the administration can resume, dealing President Donald Trump a setback in his ongoing effort to restrict the fledgling industry.</p><p>Work on the nearly completed Revolution Wind project for Rhode Island and Connecticut <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/offshore-revolution-wind-project-stopped-trump-33214b9efb8f3f7a98c58299581bff9f">has been paused since Aug. 22</a></span> when the Bureau of Ocean Energy Management issued a stop-work order for what it said were national security concerns. The Interior Department agency did not specify those concerns at the time. Both the <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-renewable-energy-offshore-wind-revolution-wind-356d6be1f0967302cd8414b2fb881308">developer and the two states sued</a></span> in federal courts.</p><p>Danish energy company Orsted and its joint venture partner Skyborn Renewables sought a preliminary injunction in U.S. District Court that would allow them to move forward with the project.</p><p>At a hearing Monday, Judge Royce Lamberth said he considered how Revolution Wind has relied on its federal approval, the delays are costing $2.3 million a day and if the project can’t meet deadlines, the entire enterprise could collapse. After December, the specialized ship needed to complete the project won’t be available until at least 2028, he said. More than 1,000 people have been working on the wind farm, which is 80% complete.</p>
    
<p>“There is no question in my mind of irreparable harm to the plaintiffs,” Lamberth said, as he granted the motion for the preliminary injunction. In his written ruling, he said Revolution Wind had “demonstrated likelihood of success on the merits” of its claim, adding that granting the injunction is in the public interest.</p>



<p>Interior Department spokeswoman Elizabeth Peace said the ruling means Revolution Wind “will be able to resume construction” while the Bureau of Ocean Energy Management “continues its investigation into possible impacts by the project to national security and prevention of other uses on the Outer Continental Shelf.”</p>
    
    
    
<p>The administration said in a court filing this month that while BOEM approved the wind farm, it stipulated that the developer continue to work with the Department of Defense to mitigate national security concerns. It said the Interior Department, to date, has not received any information that these concerns have been addressed.</p>
    
<p>Orsted said Monday that construction will resume as soon as possible, and it will continue to seek to work collaboratively with the administration.</p><p>Nancy Pyne of the Sierra Club said the court ruling “reaffirms that Donald Trump and his administration’s attacks on clean energy are not only reckless and harmful to our communities, but they are also illegal.” Trump is trying to “kneecap” renewable energy “in favor of dirty and expensive fossil fuels,” she said. </p><p>White House spokeswoman Anna Kelly said Trump was elected with a mandate to “restore our country’s energy dominance — which includes prioritizing the most effective and reliable tools to power our country. This will not be the final say on the matter.” </p><p>On the campaign trail, <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-offshore-wind-energy-4e5b18ecd4799cc4cfd8cd7dc7b326ee">Trump vowed to end the offshore wind industry</a></span> as soon as he returned to the White House. He wants to boost production of fossil fuels such as oil, natural gas and coal, which emit greenhouse gases that cause climate change, in order for the U.S. to have the lowest-cost energy and electricity of any nation in the world, he says.</p>
    
<p>His administration has <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/offshore-revolution-wind-project-stopped-trump-33214b9efb8f3f7a98c58299581bff9f">stopped construction on major offshore wind farms</a></span>, revoked wind energy permits and <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/wind-energy-offshore-turbines-trump-executive-order-995a744c3c1a2eddb30cacf50b681f13">paused permitting</a></span>, canceled plans to use <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-wind-permitting-offshore-7a05dff77ba92e4a7761604583a6d208">large areas of federal waters</a></span> for new offshore wind development and stopped <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-offshore-wind-renewable-energy-transportation-8578da8b985b6d4eef20ec4d85c21b5d">$679 million in federal funding</a></span> for a dozen offshore wind projects. </p><p>Last week, the administration moved to block a <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-offshore-wind-southcoast-massachusetts-125266070fcc8c784c92ca83d6f14300">separate Massachusetts offshore wind farm</a></span>. That was just days after the Interior Department asked a federal judge in Baltimore to cancel previous approval to build an offshore wind project in Maryland. </p><p>Revolution Wind is supposed to be Rhode Island’s and Connecticut’s first large offshore wind farm, capable of supplying power to more than 350,000 homes, about 2.5% of the region’s electricity needs.</p><p>Connecticut Attorney General William Tong and Rhode Island Attorney General Peter Neronha, who are both Democrats, called the judge’s ruling a major win for workers and families, who need the project to stay on track so it can start to drive down unaffordable energy bills. </p>
    
<p>Connecticut Rep. Joe Courtney, a Democrat, said a multibillion-dollar project that is 80% complete and was fully permitted with input by the Pentagon is not a national security problem. The Interior Department “should take the hint and let the thousands of construction workers finish the job,” he said.</p><p>Orsted began construction in 2024 about 15 miles (24 kilometers) south of the Rhode Island coast. It says in its complaint that about $5 billion has been spent or committed, and it expects more than $1 billion in costs if the project is canceled. Rhode Island is already home to one offshore wind farm, the five-turbine Block Island Wind Farm.</p>
    
<p>___</p><p>McDermott reported from Providence, Rhode Island. AP Writer Susan Haigh in Hartford, Connecticut, contributed to this report.</p><p>___</p><p>The Associated Press’ climate and environmental coverage receives financial support from multiple private foundations. AP is solely responsible for all content. Find AP’s <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://www.ap.org/about/standards-for-working-with-outside-groups/" target="_blank" rel="noopener">standards</a></span> for working with philanthropies, a list of supporters and funded coverage areas at <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://www.ap.org/discover/Supporting-AP" target="_blank" rel="noopener">AP.org</a></span>.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kevo app shutdown (122 pts)]]></title>
            <link>https://www.kwikset.com/support/answers/what-does-the-kevo-app-shutdown-mean-to-my-kevo-door-lock</link>
            <guid>45340192</guid>
            <pubDate>Mon, 22 Sep 2025 22:07:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kwikset.com/support/answers/what-does-the-kevo-app-shutdown-mean-to-my-kevo-door-lock">https://www.kwikset.com/support/answers/what-does-the-kevo-app-shutdown-mean-to-my-kevo-door-lock</a>, See on <a href="https://news.ycombinator.com/item?id=45340192">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                    
                    <span>
                        <div><p>After more than a decade of service, as of November 14, 2025 the Kevo app and web portal will no longer be available.</p><p>

ASSA ABLOY Americas Residential Inc. (“ASSA ABLOY”, “we” and “us”), which is the successor to the company that previously marketed Kwikset Kevo, Weiser Kevo and Baldwin Evolved smart door locks, will cease supporting your Kevo lock’s remote functionality.</p></div>
<p><strong><em>Locks Affected (All Generations): </em></strong><em>Kevo, Kevo Convert, Kevo Plus, Baldwin Evolved</em></p>
<p><strong><em>Brands Affected: </em></strong><em>Kwikset, Weiser, Baldwin</em></p>
<p><strong>Impact</strong></p>
<ul>
    <li>Users can no longer open/close or manage their door lock via the mobile app or web portal.</li>
</ul>

<p><strong>Not Impacted</strong></p>
<ul>
    <li>Physical Key, users will be able to unlock or lock the deadbolt with the physical key</li>
    <li>Key FOB, users will be able to unlock or lock the deadbolt with the Key FOB</li>
</ul>

<p><strong>Required User Action</strong></p>
<p>Prepare in advance for the Kevo app shutdown.&nbsp; Ensure that you have the physical key or key fob to unlock and lock the door moving forward or you can redeem the unique promotional offer that existing Kevo users received via e-mail and replace the Kevo deadbolt entirely.</p>
<p><strong>Replacement Door Lock Discount</strong></p>
<p>To help make this transition easier, we’re offering our steepest discounts ever on trusted smart lock replacements, available exclusively to Kevo users.</p>
<p><strong><em>&nbsp;(United States Only)</em></strong></p>
<p>Offers will be fulfilled by our partners at Level, a fellow ASSA ABLOY brand.&nbsp; Your orders will be securely processed and shipped through Level’s website.</p>
<p>Available options include:</p>
<ul>
    <li>$80 off Kwikset Halo Keypad Wi-Fi Smart Lock</li>
    <li>$130 off Level Lock+&nbsp;</li>
</ul>
<img src="https://images.kwikset.com/is/image/Kwikset/Kwikset%5FPromo?scl=1" alt="How to Redeem">
<p>

<img src="https://images.kwikset.com/is/image/Kwikset/Level%5FPromo?scl=1" alt="How to Redeem">
<br>
How to Redeem</p><ol>
    <li>Use the following link to visit <a href="http://www.level.co/kevo">
    </a>
    <p><a href="http://www.level.co/kevo"></a><a href="http://www.level.co/kevo"></a><a href="http://www.level.co/kevo"></a><a href="http://www.level.co/kevo">www.level.co/kevo</a>
    </p>
    </li>
    <li>Choose the replacement deadbolt that is right for you</li>
    <li>Enter your unique promotional code at checkout</li>
</ol>
<ul>
    <li>Your unique promotional code was sent to your registered Kevo e-mail address, notifying you of the Kevo app shutdown</li>
</ul>
<div><p>The above offer is final, and no other offers will ensue with respect to the loss of remote functionality of your Kevo door lock.&nbsp; This offer will expire December 14, 2025.</p><p>


<strong>(Canada Only)</strong><br>
Orders will be securely processed and shipped through Weiser’s customer service team.<br>
Available options include:<br>
- $89 (CDN) off Weiser Halo Keypad Wi-Fi Smart Lock</p><p>
&nbsp;
<img src="https://images.kwikset.com/is/image/Kwikset/Weiser%5FPromo?scl=1" alt="How to Redeem"></p><p>

How to Redeem<br>
1. Call our Weiser customer service team: 1-800-501-9471<br>
2. Ask the service team member about claiming your Kevo replacement offer<br>
3. Provide your unique promo code<br>
o Your unique promotional code was sent to your registered Kevo e-mail address, notifying you of the Kevo app shutdown<br>
These offers are made in connection with the Kevo app shutdown; and available through December 14, 2025—without further extension.</p></div>

                    </span>
                </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Paper2Agent: Stanford Reimagining Research Papers as Interactive AI Agents (137 pts)]]></title>
            <link>https://arxiv.org/abs/2509.06917</link>
            <guid>45340133</guid>
            <pubDate>Mon, 22 Sep 2025 22:02:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2509.06917">https://arxiv.org/abs/2509.06917</a>, See on <a href="https://news.ycombinator.com/item?id=45340133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2509.06917">View PDF</a>
    <a href="https://arxiv.org/html/2509.06917v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Jiacheng Miao [<a href="https://arxiv.org/show-email/844f854d/2509.06917" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 8 Sep 2025 17:28:42 UTC (6,422 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disney reinstates Jimmy Kimmel after backlash over capitulation to FCC (195 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/09/disney-abc-reinstate-jimmy-kimmel-amid-uproar-over-government-censorship/</link>
            <guid>45339428</guid>
            <pubDate>Mon, 22 Sep 2025 21:05:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/09/disney-abc-reinstate-jimmy-kimmel-amid-uproar-over-government-censorship/">https://arstechnica.com/tech-policy/2025/09/disney-abc-reinstate-jimmy-kimmel-amid-uproar-over-government-censorship/</a>, See on <a href="https://news.ycombinator.com/item?id=45339428">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<h2>Station owners can still block Kimmel show</h2>
<p>Apparently buoyed by Disney caving to his demand to "take action" against Kimmel, Carr went on to <a href="https://arstechnica.com/tech-policy/2025/09/after-getting-jimmy-kimmel-suspended-fcc-chair-threatens-abcs-the-view/">make threats</a> regarding ABC's <em>The View</em> and NBC late-night hosts Seth Meyers and Jimmy Fallon. Carr's threats were criticized by Democratic lawmakers en masse, and a couple of prominent Republicans <a href="https://arstechnica.com/tech-policy/2025/09/rand-paul-fcc-chair-had-no-business-intervening-in-abc-kimmel-controversy/">sided against him</a>. Sen. Rand Paul (R-Ky.) said that "Brendan Carr has got no business weighing in on this," while Sen. Ted Cruz (R-Texas) said that Carr's threat to ABC was "right outta <em>Goodfellas</em>."</p>
<p>Disney reinstating Kimmel doesn't necessarily mean his show will immediately appear on all ABC-affiliated networks. Conservative broadcaster Sinclair <a href="https://sbgi.net/sinclair-says-kimmel-suspension-is-not-enough-calls-on-fcc-and-abc-to-take-additional-action/">said last week</a> that "regardless of ABC's plans for the future of the program, Sinclair intends not to return <em>Jimmy Kimmel Live!</em> to our air until we are confident that appropriate steps have been taken to uphold the standards expected of a national broadcast platform."</p>
<p>Station owner Nexstar helped pressure Disney into suspending Kimmel's show last week when it <a href="https://www.nexstar.tv/nexstar-abc-affiliates-to-preempt-jimmy-kimmel-live-indefinitely-beginning-tonight/">announced</a> its ABC-affiliated stations would not air the show "for the foreseeable future."</p>
<p>Part of Carr's strategy was to urge station owners to demand that Kimmel be taken off the air. "The individual licensed stations that are taking their content, it's time for them to step up and say this garbage isn't something that we think serves the needs of our local communities," Carr said.</p>
<p>The pressure from broadcasters came at a time when both <a href="https://variety.com/2025/tv/news/broadcaster-fcc-abolish-national-tv-ownership-rule-1236496159/">Nexstar</a> and <a href="https://www.bloomberg.com/news/articles/2025-04-23/doj-probes-disney-fubo-deal-over-competition-concerns">Disney</a> are seeking Trump administration approval for mergers. Anna Gomez, the one Democrat on the Republican-majority FCC, said that companies seeking merger approvals are "vulnerable to pressure to bend to the government's ideological demands."</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Umberto Eco: Ur-Fascism (106 pts)]]></title>
            <link>https://bobmschwartz.com/2017/12/28/umberto-eco-ur-fascism/</link>
            <guid>45338990</guid>
            <pubDate>Mon, 22 Sep 2025 20:25:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bobmschwartz.com/2017/12/28/umberto-eco-ur-fascism/">https://bobmschwartz.com/2017/12/28/umberto-eco-ur-fascism/</a>, See on <a href="https://news.ycombinator.com/item?id=45338990">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>Celebrated Italian author and scholar <a href="http://amzn.to/2loibAd">Umberto Eco</a> (1932-2016)&nbsp; published an article in 1995 entitled <a href="http://www.nybooks.com/articles/1995/06/22/ur-fascism/">Ur-Fascism</a> .</p>
<p>Eco grew up during the time of Mussolini. In the article, he jumps from memories of that experience to describe some varieties of fascism and other types of totalitarianism. Not all are well-defined fascism, he says, but he does identify the core characteristics of what he calls Ur-Fascism.</p>
<p>I think it is possible to outline a list of features that are typical of what I would like to call Ur-Fascism, or Eternal Fascism. These features cannot be organized into a system; many of them contradict each other, and are also typical of other kinds of despotism or fanaticism. But it is enough that one of them be present to allow fascism to coagulate around it.</p>
<p>Eco goes on to list 14 features of Ur-Fascism. This is the excerpted list; please read the article for an expanded explanation. And as you read it, please consider which of those features you might be seeing now.</p>
<p>1. The first feature of Ur-Fascism is the cult of tradition….As a consequence, there can be no advancement of learning.</p>
<p>2. Traditionalism implies the rejection of modernism….In this sense Ur-Fascism can be defined as irrationalism.</p>
<p>3. Irrationalism also depends on the cult of action for action’s sake. Action being beautiful in itself, it must be taken before, or without, any previous reflection.</p>
<p>4. No syncretistic faith can withstand analytical criticism. The critical spirit makes distinctions, and to distinguish is a sign of modernism.</p>
<p>5. Besides, disagreement is a sign of diversity. Ur-Fascism grows up and seeks for consensus by exploiting and exacerbating the natural fear of difference.</p>
<p>6. Ur-Fascism derives from individual or social frustration. That is why one of the most typical features of the historical fascism was the appeal to a frustrated middle class, a class suffering from an economic crisis or feelings of political humiliation, and frightened by the pressure of lower social groups.</p>
<p>7. To people who feel deprived of a clear social identity, Ur-Fascism says that their only privilege is the most common one, to be born in the same country. This is the origin of nationalism.</p>
<p>8. The followers must feel humiliated by the ostentatious wealth and force of their enemies….Thus, by a continuous shifting of rhetorical focus, the enemies are at the same time too strong and too weak.</p>
<p>9. For Ur-Fascism there is no struggle for life but, rather, life is lived for struggle.</p>
<p>10. Elitism is a typical aspect of any reactionary ideology, insofar as it is fundamentally aristocratic, and aristocratic and militaristic elitism cruelly implies contempt for the weak. Ur-Fascism can only advocate a popular elitism.</p>
<p>11. In such a perspective everybody is educated to become a hero. In every mythology the hero is an exceptional being, but in Ur-Fascist ideology, heroism is the norm.</p>
<p>12. Since both permanent war and heroism are difficult games to play, the Ur-Fascist transfers his will to power to sexual matters. This is the origin of machismo (which implies both disdain for women and intolerance and condemnation of nonstandard sexual habits, from chastity to homosexuality). Since even sex is a difficult game to play, the Ur-Fascist hero tends to play with weapons—doing so becomes an ersatz phallic exercise.</p>
<p>13. Ur-Fascism is based upon a selective populism, a qualitative populism, one might say. In a democracy, the citizens have individual rights, but the citizens in their entirety have a political impact only from a quantitative point of view—one follows the decisions of the majority. For Ur-Fascism, however, individuals as individuals have no rights, and the People is conceived as a quality, a monolithic entity expressing the Common Will. Since no large quantity of human beings can have a common will, the Leader pretends to be their interpreter….Because of its qualitative populism Ur-Fascism must be against “rotten” parliamentary governments.</p>
<p>14. Ur-Fascism speaks Newspeak.</p>
<p>Eco closes with this:</p>
<p>Ur-Fascism is still around us, sometimes in plainclothes. It would be so much easier, for us, if there appeared on the world scene somebody saying, “I want to reopen Auschwitz, I want the Black Shirts to parade again in the Italian squares.” Life is not that simple. Ur-Fascism can come back under the most innocent of disguises. Our duty is to uncover it and to point our finger at any of its new instances—every day, in every part of the world.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rand Paul: FCC chair had "no business" intervening in ABC/Kimmel controversy (121 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/09/rand-paul-fcc-chair-had-no-business-intervening-in-abc-kimmel-controversy/</link>
            <guid>45338798</guid>
            <pubDate>Mon, 22 Sep 2025 20:09:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/09/rand-paul-fcc-chair-had-no-business-intervening-in-abc-kimmel-controversy/">https://arstechnica.com/tech-policy/2025/09/rand-paul-fcc-chair-had-no-business-intervening-in-abc-kimmel-controversy/</a>, See on <a href="https://news.ycombinator.com/item?id=45338798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>Democratic lawmakers have criticized Carr en masse, saying he abused his power in order to censor speech. Not many Republican lawmakers have done so, but Carr did take heat from Sen. Ted Cruz (R-Texas), who chairs the Senate Commerce Committee that has oversight over the FCC.</p>
<p>On <a href="https://www.youtube.com/watch?v=l6KYixCBTJc">his podcast</a> Friday, Cruz discussed the Carr comment in which the FCC chair said of ABC, "We can do this the easy way or the hard way. These companies can find ways to change conduct, to take action, frankly on Kimmel, or there's going to be additional work for the FCC ahead."</p>
<p>"[Carr] says, 'We can do this the easy way or we can do this the hard way.' And I gotta say, that's right outta <em>Goodfellas</em>, that's right out of a mafioso coming into a bar going, 'Nice bar you have here, it'd be a shame if something happened to it,'" Cruz said.</p>

<h2>Cruz: Resist urge to censor opponents</h2>
<p>Cruz said he is "thrilled" that Kimmel was suspended, noting that "Jimmy Kimmel has mocked me so many times I cannot count." But Cruz said that using the government to dictate what the media can say "will end up bad for conservatives," and that what Carr said is "dangerous as hell." In a J.R.R. Tolkien reference, Cruz compared the power of government to the power and allure of Sauron's One Ring.</p>
<p>"It reminds me of like the Ring of Power," Cruz said. "It is so attractive, it is sort of like conservatives saying, 'Wait, if we have government, we have power. We can ban the media.' Let me tell you what will happen. Going down this road, there will come a time when a Democrat wins again, wins the White House. They will silence us, they will use this power, and they will use it ruthlessly."</p>
<p>The Kimmel controversy is over a monologue in which he said, "We hit some new lows over the weekend with the MAGA gang desperately trying to characterize this kid who murdered Charlie Kirk as anything other than one of them and with everything they can to score political points from it."</p>

          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Choose Your Own Adventure (148 pts)]]></title>
            <link>https://www.filfre.net/2025/09/choose-your-own-adventure/</link>
            <guid>45337450</guid>
            <pubDate>Mon, 22 Sep 2025 18:22:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.filfre.net/2025/09/choose-your-own-adventure/">https://www.filfre.net/2025/09/choose-your-own-adventure/</a>, See on <a href="https://news.ycombinator.com/item?id=45337450">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><em>In 1999, after twenty years and many tens of millions of books sold,<span><a role="button" tabindex="0" onclick="footnote_moveToReference_6462_1('footnote_plugin_reference_6462_1_1');" onkeypress="footnote_moveToReference_6462_1('footnote_plugin_reference_6462_1_1');"><sup id="footnote_plugin_tooltip_6462_1_1">[1]</sup></a><span id="footnote_plugin_tooltip_text_6462_1_1">A truly incredible figure of 250 million copies sold is frequently cited for the original <em>Choose Your Own Adventure</em> series today, apparently on the basis of a statement released in January of 2007 by Choosco, a company which has repeatedly attempted to reboot the series in the post-millennial era. Based upon the running tally of sales which appeared in <em>Publishers Weekly</em> during the books’ 1980s heyday, I struggle to see how this figure can be correct. That journal of record reported 34 million <em>Choose Your Own Adventure</em> books sold in North America as of December 1, 1989. By that time, the series’s best years as a commercial proposition were already behind it. Even when factoring in international sales, which were definitely considerable, it is difficult to see how the total figure could have exceeded 100 million books sold at the outside. Having said that, however, the fact remains that the series sold an awful lot of books by any standard.</span></span></em><em> Bantam Books announced that it would no longer be publishing its Choose Your Own Adventure line of children’s paperbacks. So, since these histories currently find themselves in 1999, this seems like a good time to look back on one of the formative influences upon the computer games I’ve been covering for so many years now, as well as upon the people who played them — not least, yours truly. Or maybe that’s just an excuse for me to finally write an article I should have written a long time ago. Either way, I hope you don’t mind if I step out of the chronology today and <a href="https://www.youtube.com/watch?v=gQMYiTYGpXk">take you way, way back</a>, to steal a phrase from Van Morrison.</em></p><div>
<hr>


<blockquote><p>These books were the gateway drugs of interactive entertainment.</p>
<p>— Choose Your Own Adventure historian Christian Swineheart</p>
</blockquote>
<p>My first experience with interactive media wasn’t mediated by any sort of digital technology. Instead it came courtesy of a “technology” that was already more than half a millennium old at the time: the printed book.</p>
<p>In the fall of 1980, I was eight years old, and doing my childish best to adjust to life in a suburb of Dallas, Texas, where my family had moved the previous summer from the vicinity of Youngstown, Ohio. I was a skinny, frail kid who wasn’t very good at throwing balls or throwing punches, which did nothing to ease the transition. Even when I wasn’t being actively picked on, I was bewildered at my new classmates’ turns of phrase (“I reckon,” “y’all,” “I’m fixin’ to”) that I had previously heard only in the John Wayne movies I watched on my dad’s knee. In their eyes, my birthplace north of the Mason Dixon Line meant that I could be dismissed as just another clueless, borderline useless “Yankee,” a heathen in the eyes of those who adhered to my new state’s twin religions of Baptist Christianity and Friday-night football.</p>
<p>I found my refuge in my imagination. I was interested in just about everything — a trait I’ve never lost, both to my benefit and my detriment in life — and I could sit for long periods of time in my room, spinning out fantasies in my head about school lessons, about books I’d read, about television shows I’d seen, even about songs I’d heard on the radio. I actually framed this as a distinct activity in my mind: “I’m going to go imagine now.” If nothing else, it was good training for becoming a writer. As they say, the child is the father of the man.</p>
<p>One Friday afternoon, I discovered a slim, well-thumbed volume in my elementary school’s scanty library. Above the title <a href="https://gamebooks.org/Item/518/Show"><em>The Cave of Time</em></a> was the now-iconic&nbsp;<em>Choose Your Own Adventure</em> masthead, proclaiming it to be the first book in a series. Curious as always, I opened it to the first page. I was precocious enough to know what was meant by a first-person and third-person narrator of written fiction, but this was something else: this book was written in the <em>second</em> person.</p>
<blockquote><p>You’ve hiked through Snake Canyon once before while visiting your Uncle Howard at Red Creek Ranch, but you never noticed any cave entrance. It looks as though a recent rock slide has uncovered it.</p>
<p>Though the late afternoon sun is striking the surface of the cave, the interior remains in total darkness. You step inside a few feet, trying to get an idea of how big it is. As your eyes become used to the dark, you see what looks like a tunnel ahead, dimly lit by some kind of phosphorescent material on its walls. The tunnel walls are smooth, as if they were shaped by running water. After twenty feet or so, the tunnel curves. You wonder where it leads. You venture in a bit further, but you feel nervous being alone in such a strange place. You turn and hurry out.</p>
<p>A thunderstorm may be coming, judging by how dark it looks outside. Suddenly you realize the sun has long since set, and the landscape is lit only by the pale light of the full moon. You must have fallen asleep and woken up hours later. But then you remember something even more strange. Just last evening, the moon was only a slim crescent in the sky.</p>
<p>You wonder how long you’ve been in the cave. You are not hungry. You don’t feel you have been sleeping. You wonder whether to try to walk back home by moonlight or whether to wait for dawn, rather than risk your footing on the steep and rocky trail.</p></blockquote>
<p>All of this was intriguing enough already for a kid like me, but now came the kicker. The book asked me — asked <em>me</em>!! — whether I wanted to “start back home” (“turn to page 4”) or to “wait” (“turn to page 5”). This was the book I had never known I needed, a vehicle for the imagination like no other.</p>
<p>I took&nbsp;<em>The Cave of Time</em> home and devoured it that weekend. Through the simple expedient of flipping through its pages, I time-traveled to the age of dinosaurs, to the Battle of Gettysburg, to London during the Blitz, to the building of <a href="https://analog-antiquarian.net/2021/12/17/introduction-walls-walls-and-yet-again-walls/">the Great Wall of China</a>, to <a href="https://www.filfre.net/2022/09/titanic-visions-part-1-sifting-through-the-wreckage">the <em>Titanic</em></a> and the Ice Age and <a href="https://analog-antiquarian.net/2023/03/30/chapter-1-a-dawning-age-of-faith/">the Middle Ages</a>. Much of this history was entirely new to me, igniting whole new avenues of interest. Today, it’s all too easy to see all of the limitations and infelicities of <em>The Cave of Time</em> and its successors: a book of 115 pages that had, as it proudly trumpeted on the cover, 40 possible endings meant that the sum total of any given adventure wasn’t likely to span more than about three choices if you were lucky. But to a lonely, hyper-imaginative eight-year-old, none of that mattered. I was well and truly smitten, not so much by what the book was as by what I wished it to be, by what I was able to turn it into in my mind by the sheer intensity of that wish.</p>
<p>I remained a devoted&nbsp;<em>Choose Your Own Adventure</em> reader for the next couple of years. Back in those days, each book could be had for just $1.25, well within reach of a young boy’s allowance even at a time when a dollar was worth a lot more than it is today. Each volume had some archetypal-feeling adventurous theme that made it catnip for a kid who was also discovering Jules Verne and beginning to flirt with golden-age science fiction (the golden age being, of course, age twelve): <a href="https://gamebooks.org/Item/163/Show">deep-sea diving</a>, <a href="https://gamebooks.org/Item/563/Show">a journey by hot-air balloon</a>, <a href="https://gamebooks.org/Item/333/Show">the Wild West</a>, <a href="https://gamebooks.org/Item/552/Show">a cross-country auto race</a>, <a href="https://gamebooks.org/Item/551/Show">the Egyptian pyramids</a>, <a href="https://gamebooks.org/Item/553/Show">a hunt for the Abominable Snowman</a>. What they evoked in me was as important as what was actually printed on the page; each was a springboard for another weekend of fantasizing about exotic undertakings where nobody mocked you because you had two left feet in gym class and spoke with a stubbornly persistent Northern accent. And each was a springboard for learning as well; this process usually started with pestering my parents, and then, if I didn’t get everything I needed from that source, ended with me turning to the family set of <em>Encyclopedia Britannica</em> in the study. (I remember how when reading <em>Journey Under the Sea</em> I was confused by frequent references to “the bends.” I asked my mom what that meant, and, bless her heart, she said she thought the bends were diarrhea. Needless to say, this put a whole new spin on my underwater exploits until I finally did a bit of my own research about diving.)</p>
<p>Inevitably, I did begin to see the limitations of the format in time — right about the time that some of my nerdier classmates, whom I had by now managed to connect with, started to show me a tabletop game called <a href="https://www.filfre.net/2011/07/dungeons-and-dragons"><em>Dungeons &amp; Dragons</em></a>.&nbsp;<em>Choose Your Own Adventure</em> had primed me to understand and respond to it right away; it would be no exaggeration to say that I saw this game that would remake so much of the entertainment landscape in its image as simply a <em>better</em>, less constrained take on the same core concept. Ditto the computer games that I began to notice in a corner of the bookstore I haunted circa 1984. When <a href="https://www.filfre.net/tag/infocom/?order=asc">Infocom</a> promised me that playing one of their games meant “waking up inside a story,” I knew exactly what they must mean: <em>Choose Your Own Adventure</em> done&nbsp;<em>right</em>. For the Christmas of 1984, I convinced my parents to buy me a disk drive for the <a href="https://www.filfre.net/2012/12/the-commodore-64">Commodore 64</a> they had bought me the year before. And so the die was cast. If <em>Choose Your Own Adventure</em> hadn’t come along, I don’t think that I would be the Digital Antiquarian today.</p>
<p>But since I am the Digital Antiquarian, I have my usual array of questions to ask. Where did <em>Choose Your Own Adventure</em>, that gateway drug for the first generation to be raised on interactive media, come from? Who was responsible for it? The most obvious answer is the authors Edward Packard and R.A. Montgomery, one or the other of whose name could be seen on most of the early books in the series. But two authors alone do not a cultural phenomenon make.</p>
<hr>

<blockquote><p>“Will you read me a story?”</p>
<p>“Read you a story? What fun would that be? I’ve got a better idea: let’s tell a story together.”</p>
<p>— Adam Cadre, <a href="https://www.filfre.net/2024/11/retro-no-more-interactive-fiction-of-the-early-comp-era">Photopia</a></p>
</blockquote>
<p>During the twentieth century, when print still ruled the roost, the hidden hands behind the American cultural zeitgeist were the agents, editors, and marketers in and around the big Manhattan publishing houses, who decided which books were worth publishing and promoting, who decided what they would look like and even to a large extent how they would read. No one outside of the insular world of print publishing knew these people’s names, but the power they had to shape hearts and minds was enormous — arguably more so than that of any of the writers they served. After all, even the most prolific author of fiction or non-fiction usually couldn’t turn out more than one book per year, whereas an agent or editor could quietly, anonymously leave her fingerprints on dozens. Amy Berkower, a name I’m pretty sure you’ve never heard of, is a fine case in point.</p>
<p>Berkower joined Writers House, one of the most prestigious of the New York literary agencies, during the mid-1970s as a “secretarial girl.” Having shown herself to be an enthusiastic go-getter by working long hours and sitting in on countless meetings, she was promoted to the role of agent in 1977, but assigned to “juvenile publishing,” largely because nobody else in the organization wanted to work with such non-prestigious books. Yet the assignment suited Berkower just fine. “As a kid, I read and loved Nancy Drew before I went on to Camus,” she says. “I was in the right place at the right time. I didn’t have the bias that juvenile series wouldn’t lead to Camus.”</p>
<p>Thus when a fellow named Ray Montgomery came to her with a unique concept he called&nbsp;<em>Adventures of You</em>, he found a receptive audience. Montgomery was the co-owner of a small press called Vermont Crossroads, far removed from the glitz and glamor of Manhattan. Crossroads’s typical fare was esoteric volumes like <em>Hemingway in Michigan</em> and&nbsp;<em>The Male Nude in Photography</em> that generally weren’t expected to break four digits in total unit sales. A few years earlier, however, Montgomery had himself been approached by Edward Packard, a lawyer by trade who had already pitched a multiple-choice children’s book called <em>Sugarcane Island</em> to what felt like every other publisher in the country without success.</p>
<p>As he would find himself relating again and again to curious journalists in the decades to come, Packard had come up with his idea for an interactive book by making a virtue of necessity. During the 1960s, he was an up-and-coming attorney who worked long days in Manhattan, to which he commuted by train from his and his wife’s home in Greenwich, Connecticut. He often arrived home in the evening just in time to put his two daughters to bed. They liked to be told a bedtime story, but Packard was usually so exhausted that he had trouble coming up with one. So, he slyly enlisted his daughters’ help with the creative process. He would feed them a little bit of a story in which <em>they</em> were the stars, then ask them what they wanted to do next. Their answers would jog his tired imagination, and he would be off and running once again.</p>
<p>Sometimes, though, the girls would each want to do something different. “What would happen if you wrote both endings?” Packard mused to himself. A long-time frustrated writer as well as a self-described “lawyer who was never comfortable with the law,” Packard began to wonder whether he could turn his interactive bedtime stories into a new kind of book. By as early as 1969, he had invented the classic <em>Choose Your Own Adventure</em> format — turn to this page to do this, turn to that page to do that — and produced his first finished work in the style: the aforementioned <em>Sugarcane Island</em>, about a youngster who gets swept off the deck of a scientific research vessel by a sudden tidal wave and washed ashore on a mysterious Pacific island that has monsters, pirates, sharks, headhunters, and many another staple of more traditional children’s adventure fiction to contend with.</p>
<p>He was sure that it was “such a wonderful idea, I’d immediately find a big publisher.” He signed on with an agent, who “said he would be surprised if there were no takers,” recalls Packard. “Then he proceeded to be surprised.” One rejection letter stated that “it’s hard enough to get children to read, and you’re just making it harder with all these choices.” Letters like that came over and over again, over a period of years.</p>
<p>By 1975, Edward Packard was divorced from both his agent and his wife. With his daughters no longer of an age to beg for bedtime stories, he had just about resigned himself to being a lawyer forever. Then, whilst flipping through an issue of <em>Vermont Life</em> during a stay at a ski lodge, he happened upon a small advertisement from Crossroads Press. “Authors Wanted,” it read. Crossroads wasn’t the bright-lights, big-city publisher Packard had once dreamed of, but on a lark he sent a copy of <em>Sugarcane Island</em> to the address in the magazine.</p>
<p>It arrived on the desk of Ray Montgomery, who was instantly intrigued. “I Xeroxed 50 copies of Ed’s manuscript and took it to a reading teacher in Stowe,” Montgomery told <em>The New York Times</em> in 1981. “His kids — third grade through junior high — couldn’t get enough of it.” Satisfied by that proof of concept, Montgomery agreed to publish the book. Crossroads Press sold 8000 copies of&nbsp;<em>Sugarcane Island</em> over the next couple of years, a figure that was “unbelievable” by their modest standards. Montgomery was inspired to pen a book of his own in the same style, which he called&nbsp;<em>Journey Under the Sea</em>. The budding series was given the name&nbsp;<em>Adventures of You</em> — a proof that, whatever else they may have had going for them, branding was not really Crossroads Press’s strength.</p>
<p><a href="https://www.filfre.net/2025/09/choose-your-own-adventure/sugarcane/" rel="attachment wp-att-6468"><img decoding="async" src="https://www.filfre.net/wp-content/uploads/2025/08/sugarcane-203x300.jpg" alt="" width="304" height="450" srcset="https://www.filfre.net/wp-content/uploads/2025/08/sugarcane-203x300.jpg 203w, https://www.filfre.net/wp-content/uploads/2025/08/sugarcane.jpg 540w" sizes="(max-width: 304px) 100vw, 304px"></a></p>
<p>Indeed, Montgomery himself was well able to see that he had stumbled over a concept that was too big for his little press. He sent the two extant books to Amy Berkower at Writers House and asked her what she thought. Having grown up on Nancy Drew, she was inclined to judge them less on their individual merits than on their prospects as a franchise in the making. A concept this new, she judged, had to have a strong brand of its own in order for children to get used to it. It would take her some time to find a publisher who agreed with her.</p>
<p>In the meantime, Edward Packard, heartened by the relative success of <em>Sugarcane Island</em>, was writing more interactive books. Although their names were destined to be indelibly linked in the annals of pop-culture history, Packard and Montgomery would never really be friends; they would always have a somewhat prickly, contentious relationship with one another. In an early signal of this, Packard chose not to publish more books through Crossroads. Instead he convinced the mid-list Philadelphia-based publisher J.B. Lippincott to take on <em>Deadwood City</em>, a Western, and&nbsp;<a href="https://gamebooks.org/Item/332/Show"><em>Third Planet from Altair</em></a>, a sci-fi tale. These served ironically to confirm Amy Berkower’s belief that there needed to be a concerted push behind the concept as a branded series; released with no fanfare whatsoever, neither sold all that well. Yet Lippincott did do Packard one brilliant service. Above the titles on the covers of the books, it placed the words “Choose your own adventures in the Wild West!” and “Choose your own adventures in outer space!” There was a brand in the offing in those phrases, even if Lippincott didn’t realize it.</p>
<p><a href="https://www.filfre.net/2025/09/choose-your-own-adventure/deadwoodpb/" rel="attachment wp-att-6469"><img decoding="async" src="https://www.filfre.net/wp-content/uploads/2025/08/deadwoodpb-195x300.jpg" alt="" width="293" height="450" srcset="https://www.filfre.net/wp-content/uploads/2025/08/deadwoodpb-195x300.jpg 195w, https://www.filfre.net/wp-content/uploads/2025/08/deadwoodpb-667x1024.jpg 667w, https://www.filfre.net/wp-content/uploads/2025/08/deadwoodpb-768x1180.jpg 768w, https://www.filfre.net/wp-content/uploads/2025/08/deadwoodpb.jpg 778w" sizes="(max-width: 293px) 100vw, 293px"></a></p>
<p>For her part, Berkower was now more convinced than ever that this book-by-book approach was the wrong one. There needed to be <em>a lot</em> of these books, quickly, in order for them to take off properly. She made the rounds of the big publishing houses one more time. She finally found the ally she was looking for in Joëlle Delbourgo at Bantam Books. Delbourgo recalls getting “really excited” by the concept: “I said, ‘Amy, this is revolutionary.’ This is pre-computer, remember. The idea of interactive fiction, choosing an ending, was fresh and novel. It tapped into something very fundamental. I remember how I felt when I read the books, and how excited I got, the clarity I had about them.”</p>
<p>Seeing eye to eye on what needed to be done to cement the concept in the minds of the nation’s children, the two women drew up a contract under whose terms Bantam would publish an initial order of no fewer than six books in two slates of three. They would appear under a distinctive series trade dress, with each volume numbered to feed young readers’ collecting instinct. Barbara Marcus, Bantam’s marketing director for children’s books, needed only slightly modify the phrases deployed by J.B. Lippincott to create the perfect, pithy, and as-yet un-trademarked name for the series: <em>Choose Your Own Adventure</em>.</p>
<p>Berkower was acting as the agent of Montgomery alone up to this point. There are conflicting reports as to how and why Packard was brought into the fold. The widow of Ray Montgomery, who died in 2014, told <em>The New Yorker</em> in 2022 that her husband’s innate sense of fair play, plus the need to provide a lot of books quickly, prompted him to voluntarily bring Packard on as an equal partner. Edward Packard told the same magazine that it was Bantam who insisted that he be included, possibly in order to head off potential legal problems in the future.</p>
<p>At any rate, the first three <em>Choose Your Own Adventure</em> paperbacks arrived in bookstores in July of 1979. They were <em>The Cave of Time</em>, a new effort by Packard, written with some assistance from his daughter Andrea, she for whom he had first begun to tell his interactive stories; Montgomery’s journeyman <em>Journey Under the Sea</em>; and&nbsp;<em>By Balloon to the Sahara</em>, which Packard and Montgomery had subcontracted out to Douglas Terman, normally an author of adult military thrillers. Faced with an advertising budget that was almost nonexistent, Barbara Marcus devised an unusual grass-roots marketing strategy: “We did absolutely nothing except give the books away. We gave thousands of books to our salesmen and told them to give five to each bookseller and tell him to give them to the first five kids into his shop.”</p>
<p>The series sold itself, just as Marcus had believed it would. As <em>The New York Times</em> would soon write with a mixture of bemusement and condescension, it proved “contagious as chickenpox.” By September of 1980, around the time that I first discovered <em>The Cave of Time</em>,&nbsp;<em>Publishers Weekly</em> could report that <em>Choose Your Own Adventure</em> had become a “bonanza” for Bantam, which had sold more than 1 million copies of the first six volumes, with Packard and Montgomery now contracted to provide many more. A year later, eleven books in all had come out and the total sold was 4 million, with the series accounting for eight of the 25 bestselling children’s books at B. Dalton’s, the nation’s largest bookstore chain. A year after that, 10 million copies had been sold. By decade’s end, the total domestic sales of <em>Choose Your Own Adventure</em> would reach 34 million copies, with possibly that many or more again having been sold internationally after being translated into dozens of languages. The series was approaching its hundredth numbered volume by that point. It was a few years past its commercial peak already, but would continue on for another decade, until 184 volumes in all had come out.</p>
<p>Edward Packard, who turned 50 in 1981, could finally call himself an author rather than a lawyer by trade — and an astonishingly successful author at that, if not one who was likely to be given any awards by the literary elite. He and Ray Montgomery alone wrote about half of the 184 <em>Choose Your Own Adventure</em> installments. Packard’s prose was consistently solid and evocative without ever feeling like he was writing down to his audience, as the extract from <em>The Cave of Time</em> near the beginning of this article will attest; not all authors of children’s books, then or now, would dare to use a word like “phosphorescent.” If Montgomery was generally a less skilled wordsmith than Packard, and one who displayed less interest in producing internally consistent story spaces — weaknesses that I could see even as a young boy — he does deserve a full measure of credit for the pains he took to get the series off the ground in the first place. Looking back on the long struggle to get his brainstorm into print, Packard liked to quote the philosopher Arthur Schopenhauer: “Every original idea is first ridiculed, then vigorously attacked, and finally taken for granted.”</p>
<p>Although Packard at least was always careful to make his protagonists androgynous, it was no secret that <em>Choose Your Own Adventure</em> appealed primarily to boys — which was no bad thing on the whole, given that it was also no secret that reading in general was a harder sell with little boys than it was with little girls. Some educators and child psychologists kvetched about the violence that was undoubtedly one of the sources of the series’s appeal for boys — in just about all of the books, it was disarmingly easy to get yourself flamboyantly and creatively killed&nbsp; — but Packard was quick to counter that the mayhem was all very stylized, “exaggerated and melodramatic” rather than “harsh or nasty.” “Stupid” choices were presented to you all the time, he noted, but never “cruel” ones: “You as [the] reader never hurt anyone.”</p>
<div id="attachment_6472"><p><a href="https://www.filfre.net/2025/09/choose-your-own-adventure/kingdom/" rel="attachment wp-att-6472"><img decoding="async" aria-describedby="caption-attachment-6472" src="https://www.filfre.net/wp-content/uploads/2025/08/kingdom-182x300.jpg" alt="" width="273" height="450" srcset="https://www.filfre.net/wp-content/uploads/2025/08/kingdom-182x300.jpg 182w, https://www.filfre.net/wp-content/uploads/2025/08/kingdom-622x1024.jpg 622w, https://www.filfre.net/wp-content/uploads/2025/08/kingdom.jpg 624w" sizes="(max-width: 273px) 100vw, 273px"></a></p><p id="caption-attachment-6472">Although Packard always strained to present an <a href="https://www.filfre.net/2024/06/the-last-days-of-zork">“AFGNCAAP”</a> protagonist (“Ageless, Faceless, Gender-Neutral, Culturally Ambiguous Adventure Person”), when the stars of the books were depicted on the covers they were almost always boys. Bantam explained to a disgruntled Packard that it had many years of market research showing that, while little girls were willing to buy books that showed a hero of the opposite gender on the cover, little boys were not similarly open-minded.</p></div>
<p>One had to be a publishing insider to know that this “boys series” owed its enormous success as much to the packaging and promotional skills of three women — Amy Berkower, Joëlle Delbourgo, and Barbara Marcus — as it did to the literary talents of Packard and Montgomery. Berkower in particular became a superstar within the publishing world in the wake of <em>Choose Your Own Adventure</em>. Incredibly, the latter became only her second most successful children’s franchise, after the girl-focused&nbsp;<em>Sweet Valley High</em>, which could boast of 54 million copies sold domestically by the end of the 1980s; meanwhile <em>The Baby-Sitters Club</em> was coming up fast behind <em>Choose Your Own Adventure</em>, with 27 million copies sold. In short, her books were reaching millions upon millions of children every single month. Small wonder that she was made a full partner at Writers House in 1988; she was moving far more books each month than anyone else there.</p>
<p>Of course, any hit on the scale of&nbsp;<em>Choose Your Own Adventure</em> is bound to be copied. And this hit most certainly was, prolifically and unashamedly. During the middle years of the 1980s, when the format was at its peak, interactive books had whole aisles dedicated to them in bookstores. <a href="https://gamebooks.org/Series/29/Show"><em>Which Way?</em></a>, <a href="https://gamebooks.org/Series/104/Show"><em>Decide Your Own Adventure</em></a>,&nbsp;<a href="https://gamebooks.org/Series/304/Show"><em>Pick-a-Path</em></a>,&nbsp;<a href="https://gamebooks.org/Series/1571/Show"><em>Twisted Tales</em></a>… branders did what they could when the best brand was already taken. While&nbsp;<em>Choose Your Own Adventure</em> remained archetypal in its themes and settings, other lines were unabashedly idiosyncratic: anyone up for a&nbsp;<a href="https://gamebooks.org/Series/115/Show"><em>Do-It-Yourself Jewish Adventure</em></a>?&nbsp;Publishers were quick to leverage other properties for which they owned the rights, from <a href="https://gamebooks.org/Series/116/Show"><em>Doctor Who</em></a> to&nbsp;<em><a href="https://gamebooks.org/Series/270/Show">The Lord of the Rings</a></em>. TSR, the maker of that other school-cafeteria sensation&nbsp;<em>Dungeons &amp; Dragons</em>, introduced <a href="https://gamebooks.org/Series/79/Show">an interactive-book line drawn from the game</a>; even this website’s old friend Infocom came out with <a href="https://gamebooks.org/Series/311/Show"><em>Zork</em> books</a>, written by the star computer-game implementor <a href="https://www.filfre.net/tag/meretzky/?order=asc">Steve Meretzky</a>. Many of these books were content with the <em>Choose Your Own Adventure</em> approach of nothing but chunks of text tied to arbitrarily branching choices, but others grafted rules systems onto the format to effectively become solo role-playing games packaged as paperback books, with character creation and advancement, a dice-driven combat system, etc. The most successful of these lines was <a href="https://gamebooks.org/Series/11/Show"><em>Fighting Fantasy</em></a>, a name that is today almost as well-remembered as <em>Choose Your Own Adventure</em> itself in some quarters.</p>
<p><a href="https://www.filfre.net/2025/09/choose-your-own-adventure/warlock/" rel="attachment wp-att-6473"><img decoding="async" src="https://www.filfre.net/wp-content/uploads/2025/08/warlock-188x300.jpg" alt="" width="282" height="450" srcset="https://www.filfre.net/wp-content/uploads/2025/08/warlock-188x300.jpg 188w, https://www.filfre.net/wp-content/uploads/2025/08/warlock.jpg 631w" sizes="(max-width: 282px) 100vw, 282px"></a></p>
<p>The gamebook boom was big and real, but relatively short-lived. By 1987, the decline had begun, for both <em>Choose Your Own Adventure</em> and all of the copycats and expansions upon its formula that it had spawned. Although a few of the most lucrative series, like <em>Fighting Fantasy</em>, would join the ur-property of the genre in surviving well into the 1990s, the majority were already starting to shrivel and fall away like apples in November. Demian Katz, the Internet’s foremost archivist of gamebooks, notes that this pattern has tended to hold true “in every country” where they make an appearance: “A few come out, they become explosively popular, a flood of knock-offs are released, they reach critical mass and then drop off into nothing.” It isn’t hard to spot the reason why in the context of 1980s North America. Computers were becoming steadily more commonplace — computers that were capable of bringing vastly more flexible forms of interactive storytelling to American children, via games that didn’t require one to read the same passages of text over and over again or to toss dice and keep track of a list of statistics on paper. The same pattern would be repeated elsewhere, such as in the former Soviet countries, most of which experienced their own gamebook boom and bust during the 1990s. It seems that the arrival of the commercial mass-market publishing infrastructure that makes gamebooks go is generally followed in short order by the arrival of affordable digital technology for the home, which stops them cold.</p>
<p>In the United States, Bantam Books tried throughout the 1990s to make <em>Choose Your Own Adventure</em> feel relevant to the children of that decade, introducing a more photo-realistic art style to accompany edgier, more traditionally novelistic plots. None of it worked. In 1999, after a good twelve years of slowly but steadily declining sales, Bantam finally pulled the plug on the series. <em>Choose Your Own Adventure</em> became just another nostalgic relic of the day-glo decade, to be placed on the shelf next to Michael Jackson’s <em>Thriller</em>, a Jane Fonda workout video, and that old&nbsp;<em>Dungeons &amp; Dragons</em> Basic Set.</p>
<div id="attachment_6474"><p><a href="https://www.filfre.net/2025/09/choose-your-own-adventure/cyoa184/" rel="attachment wp-att-6474"><img decoding="async" aria-describedby="caption-attachment-6474" src="https://www.filfre.net/wp-content/uploads/2025/08/cyoa184-208x300.jpg" alt="" width="312" height="450" srcset="https://www.filfre.net/wp-content/uploads/2025/08/cyoa184-208x300.jpg 208w, https://www.filfre.net/wp-content/uploads/2025/08/cyoa184-710x1024.jpg 710w, https://www.filfre.net/wp-content/uploads/2025/08/cyoa184-768x1107.jpg 768w, https://www.filfre.net/wp-content/uploads/2025/08/cyoa184.jpg 788w" sizes="(max-width: 312px) 100vw, 312px"></a></p><p id="caption-attachment-6474">Appropriately enough, the very last <em>Choose Your Own Adventure</em> book was written by Edward&nbsp; and Andrea Packard, the latter being the grown-up version of one of the little girls to whom he had once told interactive bedtime stories.</p></div>
<p>As of this writing, <em>Choose Your Own Adventure</em> is still around in a way, but the only real <em>raison d’être</em> it has left is nostalgia. In 2003, Ray Montgomery saw that Bantam Books had let the trademark for the series lapse, and formed his own company called Chooseco to try to revive it, mostly by republishing the old books that he had written himself. He met with mixed results at best. Since Montgomery’s death in 2014, Chooseco has continued to be operated by his family, who have used it increasingly as an instrument of litigation. In 2020, for example, Netflix agreed to settle for an undisclosed sum a lawsuit over <a href="https://www.imdb.com/title/tt9495224">“Bandersnatch,”</a> a bold interactive episode of the critically lauded streaming series <em>Black Mirror</em> whose script unwisely mentioned the book series from which it drew inspiration.</p>
<p>A worthier successor on the whole is <a href="https://www.choiceofgames.com/">Choice Of Games</a>, a name whose similarity to <em>Choose Your Own Adventure</em> can hardly be coincidental. Born out of a revival of the old menu-driven computer game <a href="https://www.filfre.net/2014/11/alter-ego"><em>Alter Ego</em></a>, Choice Of has released dozens of digital branching stories over the past fifteen years. In being more adventurous than literary and basing themselves around broad, archetypal ideas — <a href="https://ifdb.org/viewgame?id=y6act0sdadab6l1n"><em>Choice of the Dragon</em></a>, <a href="https://ifdb.org/viewgame?id=zl55orcu76ngwf6t"><em>Choice of Broadsides</em></a>, <a href="https://ifdb.org/viewgame?id=66z6d3qdh378cti8"><em>Choice of the Vampire</em></a> — these games, which can run on just about any digital device capable of putting words on a screen, have done a fine job of carrying the spirit of <em>Choose Your Own Adventure</em> forward into this century. That said, there is one noteworthy difference: they are aimed at post-pubescent teens and adults — perhaps ones with fond memories of <em>Choose Your Own Adventure</em> — instead of children. “Play as male, female, or nonbinary; cis or trans; gay, straight, or bisexual; asexual and/or aromantic; allosexual and/or alloromantic; monogamous or polyamorous!” (Boring middle-aged married guy that I am, I must confess that I have no idea what three of those words even mean.)</p>
<p>Edward Packard, the father of it all, is still with us at age 94, still <a href="https://edwardpackard.com/personal-blog/">blogging</a> from time to time, still a little bemused at how he became one of the most successful working authors in the United States during the 1980s. In a plot twist almost as improbable as some of his stranger <em>Choose Your Own Adventure</em> endings, his grandson is David Corenswet, the latest actor to play Superman on the silver screen. Never a computer gamer, Packard would doubtless be baffled by most of what is featured on this website. And yet I owe him an immense debt of gratitude, for giving me my first glimpse of the potential of interactive storytelling, thus igniting a lifelong obsession. I suspect that more than one of you out there might be able to say the same.</p>
<hr>
<p><code> </code><br>
<strong>Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like.</strong></p>
<p><a href="https://www.patreon.com/DigitalAntiquarian" rel="attachment wp-att-5598"><img decoding="async" src="https://www.filfre.net/wp-content/uploads/2023/04/Patreon-300x133-1.png" alt="" width="300" height="133"></a></p>
<hr>

<p><strong>Sources</strong><strong>: </strong><em>Publishers Weekly</em> of February 29 1980, September 26 1980, October 8 1982, July 25 1986, August 12 1988, December 1 1989, July 6 1990, February 23 1998; <em>New York Times</em> of August 25 1981;&nbsp;<em>Beaver County Times</em> of March 30 1986; <em>New Yorker</em> of September 19 2022;&nbsp;<em>Journal of American Studies</em> of May 2021.</p>
<p>Online sources include <a href="https://www.mentalfloss.com/article/56160/brief-history-choose-your-own-adventure">“A Brief History of&nbsp;<em>Choose Your Own Adventure</em>“</a> by Jake Rossen at&nbsp;<em>Mental Floss</em>, <a href="https://slate.com/culture/2011/02/choose-your-own-adventure-books-how-the-cave-of-time-taught-us-to-love-interactive-entertainment.html">“<em>Choose Your Own Adventure</em>: How&nbsp;<em>The Cave of Time</em> Taught Us to Love Interactive Entertainment”</a> by Grady Hendrix at&nbsp;<em>Slate</em>, <a href="https://web.archive.org/web/20220504140422/https://www.smithsonianmag.com/innovation/surprisingly-long-history-of-choose-your-own-adventure-stories-180980014/">“The Surprising Long History of&nbsp;<em>Choose Your Own Adventure</em> Stories”</a> by Jackie Mansky at the Smithsonian’s website, and <a href="https://www.hollywoodreporter.com/tv/tv-features/choose-your-own-adventure-edward-packard-bandersnatch-knives-out-1235261356/">“Meet the 91-Year-Old Mastermind Behind&nbsp;<em>Choose Your Own Adventure</em>“</a> by Seth Abramovitch at <em>The Hollywood Reporter</em>. Plus <a href="https://edwardpackard.com/">Edward Packard’s personal site</a>. And <a href="https://gamebooks.org/">Damian Katz’s exhaustive gamebook site</a> is essential to anyone interested in these subjects; all of the book covers shown in this article were taken from his site.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI-generated “workslop” is destroying productivity? (214 pts)]]></title>
            <link>https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity</link>
            <guid>45337253</guid>
            <pubDate>Mon, 22 Sep 2025 18:07:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity">https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity</a>, See on <a href="https://news.ycombinator.com/item?id=45337253">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="main"><svg x="0" y="0" viewBox="0 0 130 148" aria-labelledby="logo:Ram6:" role="img"><title id="logo:Ram6:">Harvard Business Review Logo</title><path d="M21.3729 54.332V50.78C21.3729 49.2013 20.5031 47.9022 19.0534 47.3102L18.8546 47.228L19.0534 47.154C20.4037 46.6689 21.2072 45.3862 21.2072 43.7171V40.6667C21.2072 37.3531 19.3434 35.6758 15.6736 35.6758H9.05469V59.3229H15.8392C19.509 59.3229 21.3729 57.6456 21.3729 54.332ZM13.9671 39.7869H15.1682C15.8807 39.7869 16.2866 40.2884 16.2866 41.16V43.75C16.2866 44.6216 15.8807 45.1231 15.1682 45.1231H13.9671V39.7869ZM13.9671 55.2036V49.1684H15.1351C15.9884 49.1684 16.4191 49.6618 16.4191 50.6402V53.8222C16.4191 54.7102 16.0132 55.1953 15.2676 55.1953H13.9671V55.2036Z"></path><path d="M34.8668 35.6758H29.8882V54.036C29.8882 54.9487 29.4823 55.4749 28.7698 55.4749C28.0574 55.4749 27.6184 54.9322 27.6184 54.036V35.6758H22.6729V54.1676C22.6729 57.4647 25.0669 59.5942 28.7698 59.5942C32.4728 59.5942 34.8668 57.4647 34.8668 54.1676V35.6758Z"></path><path d="M41.908 59.5859C45.6109 59.5859 48.005 57.4563 48.005 54.1592V51.0759C48.005 49.0779 47.392 47.9267 45.7352 46.8332L41.8252 44.0787C41.2287 43.6347 41.0216 43.199 41.0216 42.385V40.9625C41.0216 39.7703 41.6098 39.5236 42.1068 39.5236C42.7861 39.5236 43.1589 40.0334 43.1589 40.9625V43.7827H47.9719V40.831C47.9719 37.4352 45.7766 35.4043 42.1068 35.4043C38.437 35.4043 36.043 37.5339 36.043 40.831V43.3881C36.043 45.3861 36.656 46.5372 38.3128 47.6307L42.2228 50.4181C42.753 50.8127 43.0263 51.1827 43.0263 52.1119V54.0359C43.0263 54.965 42.6536 55.4747 41.9743 55.4747C41.4772 55.4747 40.8891 55.2281 40.8891 54.0359V50.7881H36.043V54.1674C36.043 57.5632 38.2382 59.5941 41.908 59.5941V59.5859Z"></path><path d="M54.334 35.6758H49.3223V59.3229H54.334V35.6758Z"></path><path d="M64.3746 59.3229H68.7568V35.6758H64.2752V41.8589L64.3497 47.0142H63.9024L61.2681 35.6758H56.2811V59.3229H60.7628V51.446L60.6965 46.488H61.1355L64.3746 59.3229Z"></path><path d="M80.1721 40.05V35.6758H70.7118V59.3229H80.1721V54.9404H75.591V49.3082H79.6005V44.9258H75.591V40.05H80.1721Z"></path><path d="M92.9786 51.0846C92.9786 49.0866 92.3656 47.9355 90.7088 46.842L86.7988 44.0875C86.2024 43.6435 85.9952 43.2078 85.9952 42.3938V40.9713C85.9952 39.7791 86.5834 39.5324 87.0805 39.5324C87.7597 39.5324 88.1325 40.0422 88.1325 40.9713V43.7915H92.9455V40.8398C92.9455 37.444 90.7502 35.4131 87.0805 35.4131C83.4107 35.4131 81.0166 37.5426 81.0166 40.8398V43.3969C81.0166 45.3949 81.6296 46.546 83.2864 47.6395L87.1964 50.4269C87.7266 50.8215 88 51.1915 88 52.1206V54.0446C88 54.9738 87.6272 55.4835 86.9479 55.4835C86.4509 55.4835 85.8627 55.2369 85.8627 54.0446V50.7969H81.0166V54.1762C81.0166 57.572 83.2118 59.6029 86.8816 59.6029C90.5514 59.6029 92.9786 57.4733 92.9786 54.1762V51.0846Z"></path><path d="M106.076 51.0846C106.076 49.0866 105.463 47.9355 103.806 46.842L99.8965 44.0875C99.3 43.6435 99.0929 43.2078 99.0929 42.3938V40.9713C99.0929 39.7791 99.6811 39.5324 100.178 39.5324C100.857 39.5324 101.23 40.0422 101.23 40.9713V43.7915H106.043V40.8398C106.043 37.444 103.848 35.4131 100.178 35.4131C96.5083 35.4131 94.1143 37.5426 94.1143 40.8398V43.3969C94.1143 45.3949 94.7273 46.546 96.3841 47.6395L100.294 50.4269C100.824 50.8215 101.098 51.1915 101.098 52.1206V54.0446C101.098 54.9738 100.725 55.4835 100.046 55.4835C99.5485 55.4835 98.9604 55.2369 98.9604 54.0446V50.7969H94.1143V54.1762C94.1143 57.572 96.3095 59.6029 99.9793 59.6029C103.649 59.6029 106.076 57.4733 106.076 54.1762V51.0846Z"></path><path d="M21.696 32.6417V8.98633H16.7091V18.2446H14.0748V8.98633H9.05469V32.6417H14.0748V22.6188H16.7091V32.6417H21.696Z"></path><path d="M30.8663 27.8651L31.3634 32.6422H36.2095L32.987 8.99512H25.7137L22.5244 32.6422H27.2048L27.6687 27.8651H30.8663ZM28.3977 19.8813L28.8782 15.3345H29.6238L30.1456 19.8813L30.6344 23.9513H27.909L28.3977 19.8813Z"></path><path d="M49.7285 32.6418C49.4386 31.7785 49.3972 30.5945 49.3972 29.542V24.4032C49.3972 22.6847 48.6599 21.4514 47.3096 20.9334L47.1025 20.8512L47.3096 20.7772C48.6599 20.292 49.4635 19.0094 49.4635 17.3403V13.9938C49.4635 10.6803 47.5996 9.00293 43.9298 9.00293H37.1121V32.65H42.0576V23.03H43.1925C43.7641 23.03 44.4434 23.2849 44.4434 24.5018V29.7065C44.4434 30.652 44.4848 31.8278 44.783 32.6418H49.7285ZM43.3665 18.812H42.0659V13.2127H43.3665C44.0706 13.2127 44.4848 13.7307 44.4848 14.5858V17.4389C44.4848 18.3105 44.0789 18.812 43.3665 18.812Z"></path><path d="M58.5261 8.9873L57.4989 19.7913L57.0598 25.0782H56.538L56.0989 19.7913L55.0303 8.9873H49.9688L53.5143 32.6426H59.8101L63.3971 8.9873H58.5261Z"></path><path d="M84.5049 32.6428H89.4422C89.1522 31.7795 89.1108 30.5955 89.1108 29.543V24.4041C89.1108 22.6857 88.3735 21.4524 87.0232 20.9343L86.8244 20.8521L87.0315 20.7781C88.3818 20.293 89.1854 19.0104 89.1854 17.3412V13.9948C89.1854 10.6812 87.3215 9.00391 83.6517 9.00391H76.834V32.651H81.7795V23.031H82.9144C83.486 23.031 84.1653 23.2859 84.1653 24.5028V29.7075C84.1653 30.653 84.2067 31.8288 84.5049 32.6428ZM83.0801 18.813H81.7795V13.2137H83.0801C83.7842 13.2137 84.1984 13.7317 84.1984 14.5868V17.4399C84.1984 18.3115 83.7925 18.813 83.0801 18.813Z"></path><path d="M103.276 14.414C103.276 11.0675 100.973 8.9873 97.2784 8.9873H90.9246V32.6344H97.2784C100.981 32.6344 103.276 30.5542 103.276 27.2077V14.4057V14.414ZM98.2973 27.0515C98.2973 27.9149 97.8831 28.4246 97.179 28.4246H95.8784V13.2053H97.179C97.8831 13.2053 98.2973 13.7233 98.2973 14.5784V27.0515Z"></path><path d="M70.6041 27.8651L71.1094 32.6422H75.9555L72.7331 8.99511H65.4598L62.2704 32.6422H66.9509L67.4148 27.8651H70.6041ZM68.1355 19.8813L68.616 15.3344H69.3615L69.8834 19.8813L70.3721 23.9513H67.6467L68.1355 19.8813Z"></path><path d="M16.7336 86.0129H21.6708C21.3809 85.1496 21.3394 83.9656 21.3394 82.9131V77.7742C21.3394 76.0558 20.6022 74.8225 19.2519 74.3045L19.0531 74.2222L19.2602 74.1482C20.6105 73.6631 21.414 72.3805 21.414 70.7114V67.3649C21.414 64.0514 19.5501 62.374 15.8803 62.374H9.06264V86.0211H14.0082V76.4011H15.1431C15.7146 76.4011 16.3939 76.656 16.3939 77.8729V83.0776C16.3939 84.0231 16.4354 85.1989 16.7336 86.0129ZM15.3087 72.1831H14.0082V66.5838H15.3087C16.0129 66.5838 16.4271 67.1018 16.4271 67.9569V70.81C16.4271 71.6816 16.0212 72.1831 15.3087 72.1831Z"></path><path d="M32.5472 66.7399V62.3574H23.0869V86.0128H32.5472V81.6303H27.9662V75.9899H31.9756V71.6156H27.9662V66.7399H32.5472Z"></path><path d="M52.3956 62.3574H47.3838V86.0045H52.3956V62.3574Z"></path><path d="M63.7367 66.7398V62.3573H54.2764V86.0127H63.7367V81.6302H59.1557V75.9898H63.1651V71.6156H59.1557V66.7398H63.7367Z"></path><path d="M78.9626 62.3573L77.9851 76.0227H77.2395L77.2313 75.9487L76.3035 62.3573H72.2609L71.3911 76.0227H70.6372L70.2562 71.1058L69.6266 62.3573H64.4491L67.1497 86.0127H72.8408L73.7768 73.3093H74.4478L75.3508 86.0127H81.0419L83.7424 62.3573H78.9626Z"></path><path d="M41.7842 62.3573L40.757 73.1613L40.3097 78.4482H39.7961L39.3488 73.1613L38.2884 62.3573H33.2186L36.7724 86.0127H43.0682L46.6469 62.3573H41.7842Z"></path><path d="M64.9959 148L64.0929 147.556C39.1499 135.387 22.9548 124.509 13.1218 113.319C4.05085 103 0 92.1464 0 78.1276V0H130V78.1276C130 92.1464 125.957 103 116.878 113.319C107.045 124.509 90.8418 135.387 65.9071 147.556L65.0042 148H64.9959ZM4.10884 4.07822V78.1276C4.10884 91.0858 7.8449 101.117 16.2117 110.638C25.5643 121.27 41.0801 131.712 64.9959 143.453C88.9116 131.712 104.436 121.27 113.78 110.638C122.155 101.117 125.883 91.094 125.883 78.1276V4.07822H4.10055H4.10884Z"></path></svg><div><div><p><span>September 22, 2025<!-- -->, Updated September 22, 2025</span></p></div><div><p><img src="https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid.jpg" sizes="(min-width: 64em) 84vw, 100vw" srcset="https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid.jpg 1200w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-300x169.jpg 300w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-1024x576.jpg 1024w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-768x432.jpg 768w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-500x281.jpg 500w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-383x215.jpg 383w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-700x394.jpg 700w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-850x478.jpg 850w" alt=""></p><p><span>HBR Staff/AI</span></p></div><p data-first-paragraph="true">A confusing contradiction is unfolding in companies embracing generative AI tools: while workers are largely following mandates to embrace the technology, few are seeing it create real value. Consider, for instance, that the number of companies with fully AI-led processes <a href="https://www.accenture.com/content/dam/accenture/final/accenture-com/document-3/Accenture-Reinventing-Enterprise-Operations-FA-9-25-24.pdf">nearly doubled</a> last year, while AI use has likewise <a href="https://www.gallup.com/workplace/691643/work-nearly-doubled-two-years.aspx">doubled</a> at work since 2023. Yet a <a href="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf">recent report</a> from the MIT Media Lab found that 95% of organizations see no measurable return on their investment in these technologies. So much activity, so much enthusiasm, so little return. Why?</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen3-Omni: Native Omni AI model for text, image and video (522 pts)]]></title>
            <link>https://github.com/QwenLM/Qwen3-Omni</link>
            <guid>45336989</guid>
            <pubDate>Mon, 22 Sep 2025 17:50:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/QwenLM/Qwen3-Omni">https://github.com/QwenLM/Qwen3-Omni</a>, See on <a href="https://news.ycombinator.com/item?id=45336989">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Qwen3-Omni</h2><a id="user-content-qwen3-omni" aria-label="Permalink: Qwen3-Omni" href="#qwen3-omni"></a></p>

<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1df0c9499512c1ce1c446481874a712de3eacdb6a5b674c1cd3bf0fafd4280cd/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f2f5177656e332d4f6d6e692f7177656e335f6f6d6e695f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/1df0c9499512c1ce1c446481874a712de3eacdb6a5b674c1cd3bf0fafd4280cd/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f2f5177656e332d4f6d6e692f7177656e335f6f6d6e695f6c6f676f2e706e67" width="400" data-canonical-src="https://qianwen-res.oss-cn-beijing.aliyuncs.com//Qwen3-Omni/qwen3_omni_logo.png"></a>
</p><p dir="auto">
        💜 <a href="https://chat.qwen.ai/" rel="nofollow"><b>Qwen Chat</b></a>&nbsp;&nbsp; | &nbsp;&nbsp;🤗 <a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe" rel="nofollow">Hugging Face</a>&nbsp;&nbsp; | &nbsp;&nbsp;🤖 <a href="https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f" rel="nofollow">ModelScope</a>&nbsp;&nbsp; | &nbsp;&nbsp;📑 <a href="https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&amp;from=research.latest-advancements-list" rel="nofollow">Blog</a>&nbsp;&nbsp; | &nbsp;&nbsp;📚 <a href="https://github.com/QwenLM/Qwen3-Omni/tree/main/cookbooks">Cookbooks</a>&nbsp;&nbsp; | &nbsp;&nbsp;📑 <a href="https://github.com/QwenLM/Qwen3-Omni/tree/main/assets/Qwen3_Omni.pdf">Paper</a>&nbsp;&nbsp;
<br>
🖥️ <a href="https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo" rel="nofollow">Hugging Face Demo</a>&nbsp;&nbsp; | &nbsp;&nbsp; 🖥️ <a href="https://modelscope.cn/studios/Qwen/Qwen3-Omni-Demo" rel="nofollow">ModelScope Demo</a>&nbsp;&nbsp; | &nbsp;&nbsp;💬 <a href="https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png">WeChat (微信)</a>&nbsp;&nbsp; | &nbsp;&nbsp;🫨 <a href="https://discord.gg/CV4E9rpNSD" rel="nofollow">Discord</a>&nbsp;&nbsp; | &nbsp;&nbsp;📑 <a href="https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni" rel="nofollow">API</a>
</p>
<p dir="auto">We release <strong>Qwen3-Omni</strong>, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information 😃</p>
<details open="">
<summary>English Version</summary>
<a href="https://youtu.be/_zdOrPju4_g" rel="nofollow">
  <img src="https://camo.githubusercontent.com/c71f457935383f018f713df40205bb87a7bb92a1f46e3456e14e6f81b438541c/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f766964656f636f7665722e706e67" alt="Open English Video" data-canonical-src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png">
</a>
</details>
<details>
<summary>Chinese Version</summary>
<a href="https://youtu.be/Wtjsw5deXfQ" rel="nofollow">
  <img src="https://camo.githubusercontent.com/c71f457935383f018f713df40205bb87a7bb92a1f46e3456e14e6f81b438541c/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f766964656f636f7665722e706e67" alt="打开中文视频" data-canonical-src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png">
</a>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">News</h2><a id="user-content-news" aria-label="Permalink: News" href="#news"></a></p>
<ul dir="auto">
<li>2025.09.22: 🎉🎉🎉 We have released <a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe" rel="nofollow">Qwen3-Omni</a>. For more details, please check our <a href="https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&amp;from=research.latest-advancements-list" rel="nofollow">blog</a>!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents </h2><a id="user-content-contents-" aria-label="Permalink: Contents " href="#contents-"></a></p>
<ul dir="auto">
<li><a href="#overview">Overview</a>
<ul dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#model-architecture">Model Architecture</a></li>
<li><a href="#cookbooks-for-usage-cases">Cookbooks for Usage Cases</a></li>
</ul>
</li>
<li><a href="#quickstart">QuickStart</a>
<ul dir="auto">
<li><a href="#model-description-and-download">Model Description and Download</a></li>
<li><a href="#transformers-usage">Transformers Usage</a></li>
<li><a href="#vllm-usage">vLLM Usage</a></li>
<li><a href="#dashscope-api-usage">DashScope API Usage</a></li>
<li><a href="#usage-tips-recommended-reading">Usage Tips (Recommended Reading)</a></li>
</ul>
</li>
<li><a href="#interaction-with-qwen3-omni">Interaction with Qwen3-Omni</a>
<ul dir="auto">
<li><a href="#online-demo">Online Demo</a></li>
<li><a href="#real-time-interaction">Real-Time Interaction</a></li>
<li><a href="#launch-local-web-ui-demo">Launch Local Web UI Demo</a></li>
</ul>
</li>
<li><a href="#-docker">Docker</a></li>
<li><a href="#evaluation">Evaluation</a>
<ul dir="auto">
<li><a href="#performance-of-qwen3-omni">Performance of Qwen3-Omni</a></li>
<li><a href="#setting-for-evaluation">Setting for Evaluation</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Introduction</h3><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7d96c10768d1a12d6718515935a0da75295e66b53a76407358205764576cb1e7/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f71336f5f696e74726f64756374696f6e2e706e67"><img src="https://camo.githubusercontent.com/7d96c10768d1a12d6718515935a0da75295e66b53a76407358205764576cb1e7/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f71336f5f696e74726f64756374696f6e2e706e67" width="90%" data-canonical-src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/q3o_introduction.png"></a>
</p><p dir="auto">Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>State-of-the-art across modalities</strong>: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.</p>
</li>
<li>
<p dir="auto"><strong>Multilingual</strong>: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.</p>
<ul dir="auto">
<li><strong>Speech Input</strong>: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.</li>
<li><strong>Speech Output</strong>: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Novel Architecture</strong>: MoE-based Thinker–Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.</p>
</li>
<li>
<p dir="auto"><strong>Real-time Audio/Video Interaction</strong>: Low-latency streaming with natural turn-taking and immediate text or speech responses.</p>
</li>
<li>
<p dir="auto"><strong>Flexible Control</strong>: Customize behavior via system prompts for fine-grained control and easy adaptation.</p>
</li>
<li>
<p dir="auto"><strong>Detailed Audio Captioner</strong>: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Model Architecture</h3><a id="user-content-model-architecture" aria-label="Permalink: Model Architecture" href="#model-architecture"></a></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c4e0a9eeee86b979ea2d52f01ec3ec13f133324ef14e01c2c0d42ba8044da756/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f6f766572766965772e706e67"><img src="https://camo.githubusercontent.com/c4e0a9eeee86b979ea2d52f01ec3ec13f133324ef14e01c2c0d42ba8044da756/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f6f766572766965772e706e67" width="80%" data-canonical-src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/overview.png"></a>
</p><p dir="auto"><h3 tabindex="-1" dir="auto">Cookbooks for Usage Cases</h3><a id="user-content-cookbooks-for-usage-cases" aria-label="Permalink: Cookbooks for Usage Cases" href="#cookbooks-for-usage-cases"></a></p>
<p dir="auto">Qwen3-Omni supports a wide range of multimodal application scenarios, covering various domain tasks involving audio, image, video, and audio-visual modalities. Below are several cookbooks demonstrating the usage cases of Qwen3-Omni and these cookbooks include our actual execution logs. You can first follow the <a href="#quickstart">QuickStart</a> guide to download the model and install the necessary inference environment dependencies, then run and experiment locally—try modifying prompts or switching model types, and enjoy exploring the capabilities of Qwen3-Omni!</p>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Cookbook</th>
      <th>Description</th>
      <th>Open</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="6">Audio</td>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_recognition.ipynb">Speech Recognition</a></td>
      <td>Speech recognition, supporting multiple languages and long audio.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_recognition.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_translation.ipynb">Speech Translation</a></td>
      <td>Speech-to-Text / Speech-to-Speech translation.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_translation.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/music_analysis.ipynb">Music Analysis</a></td>
      <td>Detailed analysis and appreciation of any music, including style, genre, rhythm, etc.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/music_analysis.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/sound_analysis.ipynb">Sound Analysis</a></td>
      <td>Description and analysis of various sound effects and audio signals.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/sound_analysis.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_caption.ipynb">Audio Caption</a></td>
      <td>Audio captioning, detailed description of any audio input.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_caption.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/mixed_audio_analysis.ipynb">Mixed Audio Analysis</a></td>
      <td>Analysis of mixed audio content, such as speech, music, and environmental sounds.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/mixed_audio_analysis.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td rowspan="7">Visual</td>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/ocr.ipynb">OCR</a></td>
      <td>OCR for complex images.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/ocr.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/object_grounding.ipynb">Object Grounding</a></td>
      <td>Target detection and grounding.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/object_grounding.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_question.ipynb">Image Question</a></td>
      <td>Answering arbitrary questions about any image.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_question.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_math.ipynb">Image Math</a></td>
      <td>Solving complex mathematical problems in images, highlighting the capabilities of the Thinking model.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_math.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_description.ipynb">Video Description</a></td>
      <td>Detailed description of video content.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_description.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_navigation.ipynb">Video Navigation</a></td>
      <td>Generating navigation commands from first-person motion videos.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_navigation.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_scene_transition.ipynb">Video Scene Transition</a></td>
      <td>Analysis of scene transitions in videos.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_scene_transition.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td rowspan="3">Audio-Visual</td>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_question.ipynb">Audio Visual Question</a></td>
      <td>Answering arbitrary questions in audio-visual scenarios, demonstrating the model's ability to model temporal alignment between audio and video.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_question.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_interaction.ipynb">Audio Visual Interaction</a></td>
      <td>Interactive communication with the model using audio-visual inputs, including task specification via audio.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_interaction.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_dialogue.ipynb">Audio Visual Dialogue</a></td>
      <td>Conversational interaction with the model using audio-visual inputs, showcasing its capabilities in casual chat and assistant-like behavior.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_dialogue.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td>Agent</td>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_function_call.ipynb">Audio Function Call</a></td>
      <td>Using audio input to perform function calls, enabling agent-like behaviors.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_function_call.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td>Downstream Task Fine-tuning</td>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb">Omni Captioner</a></td>
      <td>Introduction and capability demonstration of <strong>Qwen3-Omni-30B-A3B-Captioner</strong>, a downstream fine-tuned model based on Qwen3-Omni-30B-A3B-Instruct, illustrating the strong generalization ability of the Qwen3-Omni foundation model.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">QuickStart</h2><a id="user-content-quickstart" aria-label="Permalink: QuickStart" href="#quickstart"></a></p>
<p dir="auto">Here, we provide several methods to quickly get started with Qwen3-Omni. If you want complete experience of Qwen3-Omni, you can use <a href="#transformers-usage">Hugging Face Transformers</a>. However, since Qwen3-Omni employs an MoE architecture, inference speed with Hugging Face Transformers on MoE models can be very slow. For large-scale invocation or low-latency requirements, we highly recommend using <a href="#vllm-usage">vLLM</a> or performing inference via the <a href="#dashscope-api-usage">DashScope API</a>. We also strongly suggest using our provided <a href="#-docker">Docker</a> image, which includes a complete runtime environment for both Hugging Face Transformers and vLLM. In addition, our <a href="https://github.com/QwenLM/Qwen3-Omni/tree/main/cookbooks">cookbooks</a> offer some use cases to show Qwen3-Omni's capabilities. Welcome to learn more!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Model Description and Download</h3><a id="user-content-model-description-and-download" aria-label="Permalink: Model Description and Download" href="#model-description-and-download"></a></p>
<p dir="auto">Below is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen3-Omni-30B-A3B-Instruct</td>
<td>The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the <a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf">Qwen3-Omni Technical Report</a>.</td>
</tr>
<tr>
<td>Qwen3-Omni-30B-A3B-Thinking</td>
<td>The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the <a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf">Qwen3-Omni Technical Report</a>.</td>
</tr>
<tr>
<td>Qwen3-Omni-30B-A3B-Captioner</td>
<td>A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's <a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb">cookbook</a> or <a href="https://huggingface.co/spaces/Qwen/Qwen3-Omni-Captioner-Demo" rel="nofollow">Hugging Face Demo</a> and <a href="https://modelscope.cn/studios/Qwen/Qwen3-Omni-Captioner-Demo" rel="nofollow">ModelScope Demo</a>.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">During loading in Hugging Face Transformers or vLLM, model weights will be automatically downloaded based on the model name. However, if your runtime environment is not conducive to downloading weights during execution, you can refer to the following commands to manually download the model weights to a local directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Download through ModelScope (recommended for users in Mainland China)
pip install -U modelscope
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner

# Download through Hugging Face
pip install -U &quot;huggingface_hub[cli]&quot;
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner"><pre><span><span>#</span> Download through ModelScope (recommended for users in Mainland China)</span>
pip install -U modelscope
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner

<span><span>#</span> Download through Hugging Face</span>
pip install -U <span><span>"</span>huggingface_hub[cli]<span>"</span></span>
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Transformers Usage</h3><a id="user-content-transformers-usage" aria-label="Permalink: Transformers Usage" href="#transformers-usage"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installation</h4><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">The Hugging Face Transformers code for Qwen3-Omni has been successfully merged, but the PyPI package has not yet been released. Therefore, you need to install it from source using the following command. We strongly recommend that you <strong>create a new Python environment</strong> or use our <a href="#-docker">Docker</a> to avoid environment runtime issues.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# If you already have transformers installed, please uninstall it first, or create a new Python environment
# pip uninstall transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate"><pre><span><span>#</span> If you already have transformers installed, please uninstall it first, or create a new Python environment</span>
<span><span>#</span> pip uninstall transformers</span>
pip install git+https://github.com/huggingface/transformers
pip install accelerate</pre></div>
<p dir="auto">We offer a toolkit to help you handle various types of audio and visual input more conveniently, providing an API-like experience. This includes support for base64, URLs, and interleaved audio, images, and videos. You can install it using the following command and make sure your system has <code>ffmpeg</code> installed:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install qwen-omni-utils -U"><pre>pip install qwen-omni-utils -U</pre></div>
<p dir="auto">Additionally, we recommend using FlashAttention 2 when running with Hugging Face Transformers to reduce GPU memory usage. However, if you are primarily using <a href="#vllm-usage">vLLM</a> for inference, this installation is not necessary, as vLLM includes FlashAttention 2 by default.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -U flash-attn --no-build-isolation"><pre>pip install -U flash-attn --no-build-isolation</pre></div>
<p dir="auto">Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention repository</a>. FlashAttention 2 can only be used when a model is loaded in <code>torch.float16</code> or <code>torch.bfloat16</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Code Snippet</h4><a id="user-content-code-snippet" aria-label="Permalink: Code Snippet" href="#code-snippet"></a></p>
<p dir="auto">Here is a code snippet to show you how to use Qwen3-Omni with <code>transformers</code> and <code>qwen_omni_utils</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import soundfile as sf

from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;
# MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Thinking&quot;

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    attn_implementation=&quot;flash_attention_2&quot;,
)

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

conversation = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;},
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see and hear? Answer in one short sentence.&quot;}
        ],
    },
]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for inference
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors=&quot;pt&quot;, 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Inference: Generation of the output text and audio
text_ids, audio = model.generate(**inputs, 
                                 speaker=&quot;Ethan&quot;, 
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs[&quot;input_ids&quot;].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)
if audio is not None:
    sf.write(
        &quot;output.wav&quot;,
        audio.reshape(-1).detach().cpu().numpy(),
        samplerate=24000,
    )"><pre><span>import</span> <span>soundfile</span> <span>as</span> <span>sf</span>

<span>from</span> <span>transformers</span> <span>import</span> <span>Qwen3OmniMoeForConditionalGeneration</span>, <span>Qwen3OmniMoeProcessor</span>
<span>from</span> <span>qwen_omni_utils</span> <span>import</span> <span>process_mm_info</span>

<span>MODEL_PATH</span> <span>=</span> <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>
<span># MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"</span>

<span>model</span> <span>=</span> <span>Qwen3OmniMoeForConditionalGeneration</span>.<span>from_pretrained</span>(
    <span>MODEL_PATH</span>,
    <span>dtype</span><span>=</span><span>"auto"</span>,
    <span>device_map</span><span>=</span><span>"auto"</span>,
    <span>attn_implementation</span><span>=</span><span>"flash_attention_2"</span>,
)

<span>processor</span> <span>=</span> <span>Qwen3OmniMoeProcessor</span>.<span>from_pretrained</span>(<span>MODEL_PATH</span>)

<span>conversation</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"</span>},
            {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you see and hear? Answer in one short sentence."</span>}
        ],
    },
]

<span># Set whether to use audio in video</span>
<span>USE_AUDIO_IN_VIDEO</span> <span>=</span> <span>True</span>

<span># Preparation for inference</span>
<span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(<span>conversation</span>, <span>add_generation_prompt</span><span>=</span><span>True</span>, <span>tokenize</span><span>=</span><span>False</span>)
<span>audios</span>, <span>images</span>, <span>videos</span> <span>=</span> <span>process_mm_info</span>(<span>conversation</span>, <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)
<span>inputs</span> <span>=</span> <span>processor</span>(<span>text</span><span>=</span><span>text</span>, 
                   <span>audio</span><span>=</span><span>audios</span>, 
                   <span>images</span><span>=</span><span>images</span>, 
                   <span>videos</span><span>=</span><span>videos</span>, 
                   <span>return_tensors</span><span>=</span><span>"pt"</span>, 
                   <span>padding</span><span>=</span><span>True</span>, 
                   <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)
<span>inputs</span> <span>=</span> <span>inputs</span>.<span>to</span>(<span>model</span>.<span>device</span>).<span>to</span>(<span>model</span>.<span>dtype</span>)

<span># Inference: Generation of the output text and audio</span>
<span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>**</span><span>inputs</span>, 
                                 <span>speaker</span><span>=</span><span>"Ethan"</span>, 
                                 <span>thinker_return_dict_in_generate</span><span>=</span><span>True</span>,
                                 <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)

<span>text</span> <span>=</span> <span>processor</span>.<span>batch_decode</span>(<span>text_ids</span>.<span>sequences</span>[:, <span>inputs</span>[<span>"input_ids"</span>].<span>shape</span>[<span>1</span>] :],
                              <span>skip_special_tokens</span><span>=</span><span>True</span>,
                              <span>clean_up_tokenization_spaces</span><span>=</span><span>False</span>)
<span>print</span>(<span>text</span>)
<span>if</span> <span>audio</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
    <span>sf</span>.<span>write</span>(
        <span>"output.wav"</span>,
        <span>audio</span>.<span>reshape</span>(<span>-</span><span>1</span>).<span>detach</span>().<span>cpu</span>().<span>numpy</span>(),
        <span>samplerate</span><span>=</span><span>24000</span>,
    )</pre></div>
<p dir="auto">Here are some more advanced usage examples. You can expand the sections below to learn more.</p>
<details>
<summary>Batch inference</summary>
<p dir="auto">The model can batch inputs composed of mixed samples of various types such as text, images, audio, and videos as input when <code>return_audio=False</code> is set. Here is an example.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;
# MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Thinking&quot;

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    attn_implementation=&quot;flash_attention_2&quot;,
)
model.disable_talker()

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

# Conversation with image only
conversation1 = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see in this image? Answer in one sentence.&quot;},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you hear in this audio?&quot;},
        ]
    }
]

# Conversation with pure text and system prompt
conversation3 = [
    {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are Qwen-Omni.&quot;}
        ],
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;Who are you?&quot;
    }
]

# Conversation with mixed media
conversation4 = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;},
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see and hear? Answer in one sentence.&quot;}
        ],
    }
]

# Combine messages for batch processing
conversations = [conversation1, conversation2, conversation3, conversation4]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for batch inference
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors=&quot;pt&quot;, 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Batch inference does not support returning audio
text_ids, audio = model.generate(**inputs,
                                 return_audio=False,
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs[&quot;input_ids&quot;].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>Qwen3OmniMoeForConditionalGeneration</span>, <span>Qwen3OmniMoeProcessor</span>
<span>from</span> <span>qwen_omni_utils</span> <span>import</span> <span>process_mm_info</span>

<span>MODEL_PATH</span> <span>=</span> <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>
<span># MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"</span>

<span>model</span> <span>=</span> <span>Qwen3OmniMoeForConditionalGeneration</span>.<span>from_pretrained</span>(
    <span>MODEL_PATH</span>,
    <span>dtype</span><span>=</span><span>"auto"</span>,
    <span>device_map</span><span>=</span><span>"auto"</span>,
    <span>attn_implementation</span><span>=</span><span>"flash_attention_2"</span>,
)
<span>model</span>.<span>disable_talker</span>()

<span>processor</span> <span>=</span> <span>Qwen3OmniMoeProcessor</span>.<span>from_pretrained</span>(<span>MODEL_PATH</span>)

<span># Conversation with image only</span>
<span>conversation1</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you see in this image? Answer in one sentence."</span>},
        ]
    }
]

<span># Conversation with audio only</span>
<span>conversation2</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you hear in this audio?"</span>},
        ]
    }
]

<span># Conversation with pure text and system prompt</span>
<span>conversation3</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"system"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"You are Qwen-Omni."</span>}
        ],
    },
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: <span>"Who are you?"</span>
    }
]

<span># Conversation with mixed media</span>
<span>conversation4</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"</span>},
            {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you see and hear? Answer in one sentence."</span>}
        ],
    }
]

<span># Combine messages for batch processing</span>
<span>conversations</span> <span>=</span> [<span>conversation1</span>, <span>conversation2</span>, <span>conversation3</span>, <span>conversation4</span>]

<span># Set whether to use audio in video</span>
<span>USE_AUDIO_IN_VIDEO</span> <span>=</span> <span>True</span>

<span># Preparation for batch inference</span>
<span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(<span>conversations</span>, <span>add_generation_prompt</span><span>=</span><span>True</span>, <span>tokenize</span><span>=</span><span>False</span>)
<span>audios</span>, <span>images</span>, <span>videos</span> <span>=</span> <span>process_mm_info</span>(<span>conversations</span>, <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)

<span>inputs</span> <span>=</span> <span>processor</span>(<span>text</span><span>=</span><span>text</span>, 
                   <span>audio</span><span>=</span><span>audios</span>, 
                   <span>images</span><span>=</span><span>images</span>, 
                   <span>videos</span><span>=</span><span>videos</span>, 
                   <span>return_tensors</span><span>=</span><span>"pt"</span>, 
                   <span>padding</span><span>=</span><span>True</span>, 
                   <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)
<span>inputs</span> <span>=</span> <span>inputs</span>.<span>to</span>(<span>model</span>.<span>device</span>).<span>to</span>(<span>model</span>.<span>dtype</span>)

<span># Batch inference does not support returning audio</span>
<span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>**</span><span>inputs</span>,
                                 <span>return_audio</span><span>=</span><span>False</span>,
                                 <span>thinker_return_dict_in_generate</span><span>=</span><span>True</span>,
                                 <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)

<span>text</span> <span>=</span> <span>processor</span>.<span>batch_decode</span>(<span>text_ids</span>.<span>sequences</span>[:, <span>inputs</span>[<span>"input_ids"</span>].<span>shape</span>[<span>1</span>] :],
                              <span>skip_special_tokens</span><span>=</span><span>True</span>,
                              <span>clean_up_tokenization_spaces</span><span>=</span><span>False</span>)
<span>print</span>(<span>text</span>)</pre></div>
</details>
<details>
<summary>Use audio output or not</summary>
<p dir="auto">The model supports both text and audio outputs. If users do not need audio outputs, they can call <code>model.disable_talker()</code> after initializing the model. This option will save about <code>10GB</code> of GPU memory, but the <code>return_audio</code> option for the <code>generate</code> function will only allow <code>False</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;,
    dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    attn_implementation=&quot;flash_attention_2&quot;,
)
model.disable_talker()"><pre><span>model</span> <span>=</span> <span>Qwen3OmniMoeForConditionalGeneration</span>.<span>from_pretrained</span>(
    <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>,
    <span>dtype</span><span>=</span><span>"auto"</span>,
    <span>device_map</span><span>=</span><span>"auto"</span>,
    <span>attn_implementation</span><span>=</span><span>"flash_attention_2"</span>,
)
<span>model</span>.<span>disable_talker</span>()</pre></div>
<p dir="auto">For a more flexible experience, we recommend that users decide whether to return audio when the <code>generate</code> function is called. If <code>return_audio</code> is set to <code>False</code>, the model will only return text outputs, resulting in faster text responses.</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;,
    dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    attn_implementation=&quot;flash_attention_2&quot;,
)
...
text_ids, _ = model.generate(..., return_audio=False)```

</details>

<details>
<summary>Change voice type of output audio</summary>

Qwen3-Omni supports changing the voice of the output audio. The `&quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;` checkpoint supports three voice types as follows:

| Voice Type | Gender | Description |
|------------|--------|-------------|
| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe. |
| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity. |
| Aiden      | Male   | A warm, laid-back American voice with a gentle, boyish charm. |

Users can use the `speaker` parameter of the `generate` function to specify the voice type. By default, if `speaker` is not specified, the voice type is `Ethan`.

```python
text_ids, audio = model.generate(..., speaker=&quot;Ethan&quot;)"><pre><span>model</span> <span>=</span> <span>Qwen3OmniMoeForConditionalGeneration</span>.<span>from_pretrained</span>(
    <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>,
    <span>dtype</span><span>=</span><span>"auto"</span>,
    <span>device_map</span><span>=</span><span>"auto"</span>,
    <span>attn_implementation</span><span>=</span><span>"flash_attention_2"</span>,
)
...
<span>text_ids</span>, <span>_</span> <span>=</span> <span>model</span>.<span>generate</span>(..., <span>return_audio</span><span>=</span><span>False</span>)<span>``</span>`

<span>&lt;</span><span>/</span><span>details</span><span>&gt;</span>

<span>&lt;</span><span>details</span><span>&gt;</span>
<span>&lt;</span><span>summary</span><span>&gt;</span><span>Change</span> <span>voice</span> type <span>of</span> <span>output</span> <span>audio</span><span>&lt;</span><span>/</span><span>summary</span><span>&gt;</span>

<span>Qwen3</span><span>-</span><span>Omni</span> <span>supports</span> <span>changing</span> <span>the</span> <span>voice</span> <span>of</span> <span>the</span> <span>output</span> <span>audio</span>. <span>The</span> <span>`"Qwen/Qwen3-Omni-30B-A3B-Instruct"`</span> <span>checkpoint</span> <span>supports</span> <span>three</span> <span>voice</span> <span>types</span> <span>as</span> <span>follows</span>:

<span>|</span> <span>Voice</span> <span>Type</span> <span>|</span> <span>Gender</span> <span>|</span> <span>Description</span> <span>|</span>
<span>|</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>|</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>|</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>|</span>
<span>|</span> <span>Ethan</span>      <span>|</span> <span>Male</span>   <span>|</span> <span>A</span> <span>bright</span>, <span>upbeat</span> <span>voice</span> <span>with</span> <span>infectious</span> <span>energy</span> <span>and</span> <span>a</span> <span>warm</span>, <span>approachable</span> <span>vibe</span>. <span>|</span>
<span>|</span> <span>Chelsie</span>    <span>|</span> <span>Female</span> <span>|</span> <span>A</span> <span>honeyed</span>, <span>velvety</span> <span>voice</span> <span>that</span> <span>carries</span> <span>a</span> <span>gentle</span> <span>warmth</span> <span>and</span> <span>luminous</span> <span>clarity</span>. <span>|</span>
<span>|</span> <span>Aiden</span>      <span>|</span> <span>Male</span>   <span>|</span> <span>A</span> <span>warm</span>, <span>laid</span><span>-</span><span>back</span> <span>American</span> <span>voice</span> <span>with</span> <span>a</span> <span>gentle</span>, <span>boyish</span> <span>charm</span>. <span>|</span>

<span>Users</span> <span>can</span> <span>use</span> <span>the</span> <span>`speaker`</span> <span>parameter</span> <span>of</span> <span>the</span> <span>`generate`</span> <span>function</span> <span>to</span> <span>specify</span> <span>the</span> <span>voice</span> <span>type</span>. <span>By</span> <span>default</span>, <span>if</span> <span>`speaker`</span> <span><span>is</span> <span>not</span></span> <span>specified</span>, <span>the</span> <span>voice</span> <span>type</span> <span>is</span> <span>`Ethan`</span>.

<span>``</span>`<span>python</span>
<span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(..., <span>speaker</span><span>=</span><span>"Ethan"</span>)</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="text_ids, audio = model.generate(..., speaker=&quot;Chelsie&quot;)"><pre><span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(..., <span>speaker</span><span>=</span><span>"Chelsie"</span>)</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="text_ids, audio = model.generate(..., speaker=&quot;Aiden&quot;)"><pre><span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(..., <span>speaker</span><span>=</span><span>"Aiden"</span>)</pre></div>
</details>
<p dir="auto">Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to <a href="#usage-tips-recommended-reading">Usage Tips</a> and <a href="#cookbooks-for-usage-cases">Cookbooks for Usage Cases</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">vLLM Usage</h3><a id="user-content-vllm-usage" aria-label="Permalink: vLLM Usage" href="#vllm-usage"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installation</h4><a id="user-content-installation-1" aria-label="Permalink: Installation" href="#installation-1"></a></p>
<p dir="auto">We strongly recommend using vLLM for inference and deployment of the Qwen3-Omni series models. Since our code is currently in the pull request stage, and <strong>audio output inference support for the Instruct model will be released in the near future</strong>, you can follow the commands below to install vLLM from source. Please note that we recommend you <strong>create a new Python environment</strong> or use our provided <a href="#-docker">Docker</a> to avoid runtime environment conflicts and incompatibilities. For more details on compiling vLLM from source, please refer to the <a href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html#set-up-using-python-only-build-without-compilation" rel="nofollow">vLLM official documentation</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone -b qwen3_omni https://github.com/wangxiongts/vllm.git
cd vllm
pip install -r requirements/build.txt
pip install -r requirements/cuda.txt
export VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl
VLLM_USE_PRECOMPILED=1 pip install -e . -v --no-build-isolation
# If you meet an &quot;Undefined symbol&quot; error while using VLLM_USE_PRECOMPILED=1, please use &quot;pip install -e . -v&quot; to build from source.
# Install the Transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-omni-utils -U
pip install -U flash-attn --no-build-isolation"><pre>git clone -b qwen3_omni https://github.com/wangxiongts/vllm.git
<span>cd</span> vllm
pip install -r requirements/build.txt
pip install -r requirements/cuda.txt
<span>export</span> VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl
VLLM_USE_PRECOMPILED=1 pip install -e <span>.</span> -v --no-build-isolation
<span><span>#</span> If you meet an "Undefined symbol" error while using VLLM_USE_PRECOMPILED=1, please use "pip install -e . -v" to build from source.</span>
<span><span>#</span> Install the Transformers</span>
pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-omni-utils -U
pip install -U flash-attn --no-build-isolation</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Inference</h4><a id="user-content-inference" aria-label="Permalink: Inference" href="#inference"></a></p>
<p dir="auto">You can use the following code for vLLM inference. The <code>limit_mm_per_prompt</code> parameter specifies the maximum number of each modality's data allowed per message. Since vLLM needs to pre-allocate GPU memory, larger values will require more GPU memory; if OOM issues occur, try reducing this value. Setting <code>tensor_parallel_size</code> greater than one enables multi-GPU parallel inference, improving concurrency and throughput. In addition, <code>max_num_seqs</code> indicates the number of sequences that vLLM processes in parallel during each inference step. A larger value requires more GPU memory but enables higher batch inference speed. For more details, please refer to the <a href="https://docs.vllm.ai/en/latest/api/vllm/index.html#vllm.LLM" rel="nofollow">vLLM official documentation</a>. Below is a simple example of how to run Qwen3-Omni with vLLM:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;
    # MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Thinking&quot;

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    messages = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;video&quot;, &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4&quot;}
            ], 
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        &quot;mm_processor_kwargs&quot;: {
            &quot;use_audio_in_video&quot;: True,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios

    outputs = llm.generate([inputs], sampling_params=sampling_params)

    print(outputs[0].outputs[0].text)"><pre><span>import</span> <span>os</span>
<span>import</span> <span>torch</span>

<span>from</span> <span>vllm</span> <span>import</span> <span>LLM</span>, <span>SamplingParams</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>Qwen3OmniMoeProcessor</span>
<span>from</span> <span>qwen_omni_utils</span> <span>import</span> <span>process_mm_info</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>'__main__'</span>:
    <span># vLLM engine v1 not supported yet</span>
    <span>os</span>.<span>environ</span>[<span>'VLLM_USE_V1'</span>] <span>=</span> <span>'0'</span>

    <span>MODEL_PATH</span> <span>=</span> <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>
    <span># MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"</span>

    <span>llm</span> <span>=</span> <span>LLM</span>(
            <span>model</span><span>=</span><span>MODEL_PATH</span>, <span>trust_remote_code</span><span>=</span><span>True</span>, <span>gpu_memory_utilization</span><span>=</span><span>0.95</span>,
            <span>tensor_parallel_size</span><span>=</span><span>torch</span>.<span>cuda</span>.<span>device_count</span>(),
            <span>limit_mm_per_prompt</span><span>=</span>{<span>'image'</span>: <span>3</span>, <span>'video'</span>: <span>3</span>, <span>'audio'</span>: <span>3</span>},
            <span>max_num_seqs</span><span>=</span><span>8</span>,
            <span>max_model_len</span><span>=</span><span>32768</span>,
            <span>seed</span><span>=</span><span>1234</span>,
    )

    <span>sampling_params</span> <span>=</span> <span>SamplingParams</span>(
        <span>temperature</span><span>=</span><span>0.6</span>,
        <span>top_p</span><span>=</span><span>0.95</span>,
        <span>top_k</span><span>=</span><span>20</span>,
        <span>max_tokens</span><span>=</span><span>16384</span>,
    )

    <span>processor</span> <span>=</span> <span>Qwen3OmniMoeProcessor</span>.<span>from_pretrained</span>(<span>MODEL_PATH</span>)

    <span>messages</span> <span>=</span> [
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: [
                {<span>"type"</span>: <span>"video"</span>, <span>"video"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4"</span>}
            ], 
        }
    ]

    <span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(
        <span>messages</span>,
        <span>tokenize</span><span>=</span><span>False</span>,
        <span>add_generation_prompt</span><span>=</span><span>True</span>,
    )
    <span>audios</span>, <span>images</span>, <span>videos</span> <span>=</span> <span>process_mm_info</span>(<span>messages</span>, <span>use_audio_in_video</span><span>=</span><span>True</span>)

    <span>inputs</span> <span>=</span> {
        <span>'prompt'</span>: <span>text</span>,
        <span>'multi_modal_data'</span>: {},
        <span>"mm_processor_kwargs"</span>: {
            <span>"use_audio_in_video"</span>: <span>True</span>,
        },
    }

    <span>if</span> <span>images</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'image'</span>] <span>=</span> <span>images</span>
    <span>if</span> <span>videos</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'video'</span>] <span>=</span> <span>videos</span>
    <span>if</span> <span>audios</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'audio'</span>] <span>=</span> <span>audios</span>

    <span>outputs</span> <span>=</span> <span>llm</span>.<span>generate</span>([<span>inputs</span>], <span>sampling_params</span><span>=</span><span>sampling_params</span>)

    <span>print</span>(<span>outputs</span>[<span>0</span>].<span>outputs</span>[<span>0</span>].<span>text</span>)</pre></div>
<p dir="auto">Here are some more advanced usage examples. You can expand the sections below to learn more.</p>
<details>
<summary>Batch inference</summary>
<p dir="auto">Using vLLM enables fast batch inference, which can help you efficiently process large volumes of data or conduct benchmarking. Refer to the following code example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

def build_input(processor, messages, use_audio_in_video):
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=use_audio_in_video)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        &quot;mm_processor_kwargs&quot;: {
            &quot;use_audio_in_video&quot;: use_audio_in_video,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios
    
    return inputs

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;
    # MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Thinking&quot;

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    # Conversation with image only
    conversation1 = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;},
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see in this image? Answer in one sentence.&quot;},
            ]
        }
    ]

    # Conversation with audio only
    conversation2 = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav&quot;},
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you hear in this audio?&quot;},
            ]
        }
    ]

    # Conversation with pure text and system prompt
    conversation3 = [
        {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are Qwen-Omni.&quot;}
            ],
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Who are you? Answer in one sentence.&quot;
        }
    ]

    # Conversation with mixed media
    conversation4 = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;},
                {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav&quot;},
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see and hear? Answer in one sentence.&quot;}
            ],
        }
    ]
    
    USE_AUDIO_IN_VIDEO = True

    # Combine messages for batch processing
    conversations = [conversation1, conversation2, conversation3, conversation4]
    inputs = [build_input(processor, messages, USE_AUDIO_IN_VIDEO) for messages in conversations]

    outputs = llm.generate(inputs, sampling_params=sampling_params)

    result = [outputs[i].outputs[0].text for i in range(len(outputs))]
    print(result)"><pre><span>import</span> <span>os</span>
<span>import</span> <span>torch</span>

<span>from</span> <span>vllm</span> <span>import</span> <span>LLM</span>, <span>SamplingParams</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>Qwen3OmniMoeProcessor</span>
<span>from</span> <span>qwen_omni_utils</span> <span>import</span> <span>process_mm_info</span>

<span>def</span> <span>build_input</span>(<span>processor</span>, <span>messages</span>, <span>use_audio_in_video</span>):
    <span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(
        <span>messages</span>,
        <span>tokenize</span><span>=</span><span>False</span>,
        <span>add_generation_prompt</span><span>=</span><span>True</span>,
    )
    <span>audios</span>, <span>images</span>, <span>videos</span> <span>=</span> <span>process_mm_info</span>(<span>messages</span>, <span>use_audio_in_video</span><span>=</span><span>use_audio_in_video</span>)

    <span>inputs</span> <span>=</span> {
        <span>'prompt'</span>: <span>text</span>,
        <span>'multi_modal_data'</span>: {},
        <span>"mm_processor_kwargs"</span>: {
            <span>"use_audio_in_video"</span>: <span>use_audio_in_video</span>,
        },
    }

    <span>if</span> <span>images</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'image'</span>] <span>=</span> <span>images</span>
    <span>if</span> <span>videos</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'video'</span>] <span>=</span> <span>videos</span>
    <span>if</span> <span>audios</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'audio'</span>] <span>=</span> <span>audios</span>
    
    <span>return</span> <span>inputs</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>'__main__'</span>:
    <span># vLLM engine v1 not supported yet</span>
    <span>os</span>.<span>environ</span>[<span>'VLLM_USE_V1'</span>] <span>=</span> <span>'0'</span>

    <span>MODEL_PATH</span> <span>=</span> <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>
    <span># MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"</span>

    <span>llm</span> <span>=</span> <span>LLM</span>(
            <span>model</span><span>=</span><span>MODEL_PATH</span>, <span>trust_remote_code</span><span>=</span><span>True</span>, <span>gpu_memory_utilization</span><span>=</span><span>0.95</span>,
            <span>tensor_parallel_size</span><span>=</span><span>torch</span>.<span>cuda</span>.<span>device_count</span>(),
            <span>limit_mm_per_prompt</span><span>=</span>{<span>'image'</span>: <span>3</span>, <span>'video'</span>: <span>3</span>, <span>'audio'</span>: <span>3</span>},
            <span>max_num_seqs</span><span>=</span><span>8</span>,
            <span>max_model_len</span><span>=</span><span>32768</span>,
            <span>seed</span><span>=</span><span>1234</span>,
    )

    <span>sampling_params</span> <span>=</span> <span>SamplingParams</span>(
        <span>temperature</span><span>=</span><span>0.6</span>,
        <span>top_p</span><span>=</span><span>0.95</span>,
        <span>top_k</span><span>=</span><span>20</span>,
        <span>max_tokens</span><span>=</span><span>16384</span>,
    )

    <span>processor</span> <span>=</span> <span>Qwen3OmniMoeProcessor</span>.<span>from_pretrained</span>(<span>MODEL_PATH</span>)

    <span># Conversation with image only</span>
    <span>conversation1</span> <span>=</span> [
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: [
                {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"</span>},
                {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you see in this image? Answer in one sentence."</span>},
            ]
        }
    ]

    <span># Conversation with audio only</span>
    <span>conversation2</span> <span>=</span> [
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: [
                {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"</span>},
                {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you hear in this audio?"</span>},
            ]
        }
    ]

    <span># Conversation with pure text and system prompt</span>
    <span>conversation3</span> <span>=</span> [
        {
            <span>"role"</span>: <span>"system"</span>,
            <span>"content"</span>: [
                {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"You are Qwen-Omni."</span>}
            ],
        },
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: <span>"Who are you? Answer in one sentence."</span>
        }
    ]

    <span># Conversation with mixed media</span>
    <span>conversation4</span> <span>=</span> [
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: [
                {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"</span>},
                {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav"</span>},
                {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you see and hear? Answer in one sentence."</span>}
            ],
        }
    ]
    
    <span>USE_AUDIO_IN_VIDEO</span> <span>=</span> <span>True</span>

    <span># Combine messages for batch processing</span>
    <span>conversations</span> <span>=</span> [<span>conversation1</span>, <span>conversation2</span>, <span>conversation3</span>, <span>conversation4</span>]
    <span>inputs</span> <span>=</span> [<span>build_input</span>(<span>processor</span>, <span>messages</span>, <span>USE_AUDIO_IN_VIDEO</span>) <span>for</span> <span>messages</span> <span>in</span> <span>conversations</span>]

    <span>outputs</span> <span>=</span> <span>llm</span>.<span>generate</span>(<span>inputs</span>, <span>sampling_params</span><span>=</span><span>sampling_params</span>)

    <span>result</span> <span>=</span> [<span>outputs</span>[<span>i</span>].<span>outputs</span>[<span>0</span>].<span>text</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>len</span>(<span>outputs</span>))]
    <span>print</span>(<span>result</span>)</pre></div>
</details>
<details>
<summary>vLLM Serve Usage</summary>
<p dir="auto">vLLM serve for Qwen3-Omni currently only supports the thinker model. The <code>use_audio_in_video</code> parameter is not available in vLLM serve; you can handle this by separately passing video and audio inputs for processing. You can start vLLM serve through the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Qwen3-Omni-30B-A3B-Instruct for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4
# Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4"><pre><span><span>#</span> Qwen3-Omni-30B-A3B-Instruct for single GPU</span>
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
<span><span>#</span> Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)</span>
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4
<span><span>#</span> Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU</span>
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
<span><span>#</span> Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)</span>
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4</pre></div>
<p dir="auto">Then you can use the chat API as below (via curl, for example):</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl http://localhost:8901/v1/chat/completions \
    -H &quot;Content-Type: application/json&quot; \
    -d '{
    &quot;messages&quot;: [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [
        {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;}},
        {&quot;type&quot;: &quot;audio_url&quot;, &quot;audio_url&quot;: {&quot;url&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav&quot;}},
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see and hear? Answer in one sentence.&quot;}
    ]}
    ]
    }'"><pre>curl http://localhost:8901/v1/chat/completions \
    -H <span><span>"</span>Content-Type: application/json<span>"</span></span> \
    -d <span><span>'</span>{</span>
<span>    "messages": [</span>
<span>    {"role": "system", "content": "You are a helpful assistant."},</span>
<span>    {"role": "user", "content": [</span>
<span>        {"type": "image_url", "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}},</span>
<span>        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}},</span>
<span>        {"type": "text", "text": "What can you see and hear? Answer in one sentence."}</span>
<span>    ]}</span>
<span>    ]</span>
<span>    }<span>'</span></span></pre></div>
</details>
<p dir="auto">Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to <a href="#usage-tips-recommended-reading">Usage Tips</a> and <a href="#cookbooks-for-usage-cases">Cookbooks for Usage Cases</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DashScope API Usage</h3><a id="user-content-dashscope-api-usage" aria-label="Permalink: DashScope API Usage" href="#dashscope-api-usage"></a></p>
<p dir="auto">To further explore Qwen3-Omni, we encourage you to try our DashScope API for a faster and more efficient experience. For detailed API information and documentation, please refer to the following:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>API Description</th>
<th>API Documentation (Mainland China)</th>
<th>API Documentation (International)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Offline API for Qwen3-Omni-Flash, including Instruct and Thinking models</td>
<td><a href="https://help.aliyun.com/zh/model-studio/qwen-omni" rel="nofollow">https://help.aliyun.com/zh/model-studio/qwen-omni</a></td>
<td><a href="https://www.alibabacloud.com/help/en/model-studio/qwen-omni" rel="nofollow">https://www.alibabacloud.com/help/en/model-studio/qwen-omni</a></td>
</tr>
<tr>
<td>Real-time API for Qwen3-Omni-Flash, supporting end-to-end real-time interaction</td>
<td><a href="https://help.aliyun.com/zh/model-studio/realtime" rel="nofollow">https://help.aliyun.com/zh/model-studio/realtime</a></td>
<td><a href="https://www.alibabacloud.com/help/en/model-studio/realtime" rel="nofollow">https://www.alibabacloud.com/help/en/model-studio/realtime</a></td>
</tr>
<tr>
<td>API for Qwen3-Omni-30B-A3B-Captioner model</td>
<td><a href="https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner" rel="nofollow">https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner</a></td>
<td><a href="https://www.alibabacloud.com/help/zh/model-studio/qwen3-omni-captioner" rel="nofollow">https://www.alibabacloud.com/help/zh/model-studio/qwen3-omni-captioner</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage Tips (Recommended Reading)</h3><a id="user-content-usage-tips-recommended-reading" aria-label="Permalink: Usage Tips (Recommended Reading)" href="#usage-tips-recommended-reading"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Minimum GPU memory requirements</h4><a id="user-content-minimum-gpu-memory-requirements" aria-label="Permalink: Minimum GPU memory requirements" href="#minimum-gpu-memory-requirements"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Precision</th>
<th>15s Video</th>
<th>30s Video</th>
<th>60s Video</th>
<th>120s Video</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen3-Omni-30B-A3B-Instruct</td>
<td>BF16</td>
<td>78.85 GB</td>
<td>88.52 GB</td>
<td>107.74 GB</td>
<td>144.81 GB</td>
</tr>
<tr>
<td>Qwen3-Omni-30B-A3B-Thinking</td>
<td>BF16</td>
<td>68.74 GB</td>
<td>77.79 GB</td>
<td>95.76 GB</td>
<td>131.65 GB</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Note</strong>: The table above presents the theoretical minimum memory requirements for inference with <code>transformers</code> and <code>BF16</code> precision, tested with <code>attn_implementation="flash_attention_2"</code>. The Instruct model includes both the <strong>thinker</strong> and <strong>talker</strong> components, whereas the Thinking model includes only the <strong>thinker</strong> part.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Prompt for Audio-Visual Interaction</h4><a id="user-content-prompt-for-audio-visual-interaction" aria-label="Permalink: Prompt for Audio-Visual Interaction" href="#prompt-for-audio-visual-interaction"></a></p>
<p dir="auto">When using Qwen3-Omni for audio-visual multimodal interaction, where the input consists of a video and its corresponding audio (with the audio serving as a query), we recommend using the <strong>following system prompt</strong>. This setup helps the model maintain high reasoning capability while better assuming interactive roles such as a smart assistant. Additionally, the text generated by the thinker will be more readable, with a natural, conversational tone and without complex formatting that is difficult to vocalize, leading to more stable and fluent audio output from the talker. You can customize the <code>user_system_prompt</code> field in the system prompt to include character settings or other role-specific descriptions as needed.</p>
<div data-snippet-clipboard-copy-content="user_system_prompt = &quot;You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen.&quot;
message = {
    &quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: [
          {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: f&quot;{user_system_prompt} You are a virtual voice assistant with no gender or age.\nYou are communicating with the user.\nIn user messages, “I/me/my/we/our” refer to the user and “you/your” refer to the assistant. In your replies, address the user as “you/your” and yourself as “I/me/my”; never mirror the user’s pronouns—always shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \nYour output must consist only of the spoken content you want the user to hear. \nDo not include any descriptions of actions, emotions, sounds, or voice changes. \nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \nYou must answer users' audio or text questions, do not directly describe the video content. \nYou should communicate in the same language strictly as the user unless they request otherwise.\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\nKeep replies concise and conversational, as if talking face-to-face.&quot;}
    ]
}"><pre><code>user_system_prompt = "You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen."
message = {
    "role": "system",
    "content": [
          {"type": "text", "text": f"{user_system_prompt} You are a virtual voice assistant with no gender or age.\nYou are communicating with the user.\nIn user messages, “I/me/my/we/our” refer to the user and “you/your” refer to the assistant. In your replies, address the user as “you/your” and yourself as “I/me/my”; never mirror the user’s pronouns—always shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \nYour output must consist only of the spoken content you want the user to hear. \nDo not include any descriptions of actions, emotions, sounds, or voice changes. \nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \nYou must answer users' audio or text questions, do not directly describe the video content. \nYou should communicate in the same language strictly as the user unless they request otherwise.\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\nKeep replies concise and conversational, as if talking face-to-face."}
    ]
}
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Best Practices for the Thinking Model</h4><a id="user-content-best-practices-for-the-thinking-model" aria-label="Permalink: Best Practices for the Thinking Model" href="#best-practices-for-the-thinking-model"></a></p>
<p dir="auto">The <code>Qwen3-Omni-30B-A3B-Thinking</code> model is primarily designed for understanding and interacting with multimodal inputs, including text, audio, image, and video. To achieve optimal performance, we recommend that users include an explicit textual instruction or task description in each round of dialogue alongside the multimodal input. This helps clarify the intent and significantly enhances the model's ability to leverage its reasoning capabilities. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;/path/to/audio.wav&quot;},
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;/path/to/image.png&quot;},
            {&quot;type&quot;: &quot;video&quot;, &quot;video&quot;: &quot;/path/to/video.mp4&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Analyze this audio, image, and video together.&quot;},
        ], 
    }
]"><pre><span>messages</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"/path/to/audio.wav"</span>},
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"/path/to/image.png"</span>},
            {<span>"type"</span>: <span>"video"</span>, <span>"video"</span>: <span>"/path/to/video.mp4"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"Analyze this audio, image, and video together."</span>},
        ], 
    }
]</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Use audio in video</h4><a id="user-content-use-audio-in-video" aria-label="Permalink: Use audio in video" href="#use-audio-in-video"></a></p>
<p dir="auto">In multimodal interaction, user-provided videos are often accompanied by audio (such as spoken questions or sounds from events in the video). This information helps the model provide a better interactive experience. We provide the following options for users to decide whether to use the audio from a video.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# In data preprocessing
audios, images, videos = process_mm_info(messages, use_audio_in_video=True)"><pre><span># In data preprocessing</span>
<span>audios</span>, <span>images</span>, <span>videos</span> <span>=</span> <span>process_mm_info</span>(<span>messages</span>, <span>use_audio_in_video</span><span>=</span><span>True</span>)</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# For Transformers
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=&quot;pt&quot;, 
                   padding=True, use_audio_in_video=True)
text_ids, audio = model.generate(..., use_audio_in_video=True)

# For vLLM
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = {
    'prompt': text,
    'multi_modal_data': {},
    &quot;mm_processor_kwargs&quot;: {
        &quot;use_audio_in_video&quot;: True,
    },
}"><pre><span># For Transformers</span>
<span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(<span>messages</span>, <span>add_generation_prompt</span><span>=</span><span>True</span>, <span>tokenize</span><span>=</span><span>False</span>)
<span>inputs</span> <span>=</span> <span>processor</span>(<span>text</span><span>=</span><span>text</span>, <span>audio</span><span>=</span><span>audios</span>, <span>images</span><span>=</span><span>images</span>, <span>videos</span><span>=</span><span>videos</span>, <span>return_tensors</span><span>=</span><span>"pt"</span>, 
                   <span>padding</span><span>=</span><span>True</span>, <span>use_audio_in_video</span><span>=</span><span>True</span>)
<span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(..., <span>use_audio_in_video</span><span>=</span><span>True</span>)

<span># For vLLM</span>
<span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(<span>messages</span>, <span>add_generation_prompt</span><span>=</span><span>True</span>, <span>tokenize</span><span>=</span><span>False</span>)
<span>inputs</span> <span>=</span> {
    <span>'prompt'</span>: <span>text</span>,
    <span>'multi_modal_data'</span>: {},
    <span>"mm_processor_kwargs"</span>: {
        <span>"use_audio_in_video"</span>: <span>True</span>,
    },
}</pre></div>
<p dir="auto">It is worth noting that during a multi-round conversation, the <code>use_audio_in_video</code> parameter must be set consistently across these steps; otherwise, unexpected results may occur.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interaction with Qwen3-Omni</h2><a id="user-content-interaction-with-qwen3-omni" aria-label="Permalink: Interaction with Qwen3-Omni" href="#interaction-with-qwen3-omni"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Online Demo</h3><a id="user-content-online-demo" aria-label="Permalink: Online Demo" href="#online-demo"></a></p>
<p dir="auto">Without local deployment, you can experience an online web demo directly by visiting our <a href="https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo" rel="nofollow">Hugging Face Spaces</a> and <a href="https://modelscope.cn/studios/Qwen/Qwen3-Omni-Demo" rel="nofollow">ModelScope Studio</a>. This includes quick hands-on experiences for Qwen3-Omni-Realtime, Qwen3-Omni (Instruct and Thinking), and Qwen3-Omni-30B-A3B-Captioner.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Real-Time Interaction</h3><a id="user-content-real-time-interaction" aria-label="Permalink: Real-Time Interaction" href="#real-time-interaction"></a></p>
<p dir="auto">Real-time streaming interaction with Qwen3-Omni is available now. Please visit <a href="https://chat.qwen.ai/" rel="nofollow">Qwen Chat</a> and select the voice/video call option in the chat box to experience it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Launch Local Web UI Demo</h3><a id="user-content-launch-local-web-ui-demo" aria-label="Permalink: Launch Local Web UI Demo" href="#launch-local-web-ui-demo"></a></p>
<p dir="auto">In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with the model through a web browser. Follow the steps below to get start :)</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installation</h4><a id="user-content-installation-2" aria-label="Permalink: Installation" href="#installation-2"></a></p>
<p dir="auto">Before you begin, we strongly recommend that you refer to the <strong>Installation</strong> section in <a href="#vllm-usage">vLLM Usage</a> to set up your environment, which will allow you to seamlessly use both the vLLM and Transformers backends. However, if you only intend to use the Transformers backend (<strong>note that this will result in significantly slower inference</strong>), please follow the installation instructions in <a href="#transformers-usage">Transformers Usage</a>. That said, we still highly recommend using our <a href="#-docker">Docker</a> image to avoid potential environment-related issues. Additionally, if you are running locally, make sure your system has <code>ffmpeg</code> installed and you install the following dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install gradio==5.44.1 gradio_client==1.12.1 soundfile==0.13.1"><pre>pip install gradio==5.44.1 gradio_client==1.12.1 soundfile==0.13.1</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Running the Demo</h4><a id="user-content-running-the-demo" aria-label="Permalink: Running the Demo" href="#running-the-demo"></a></p>
<p dir="auto">Once the required packages are installed, you can launch the web demo using the following commands. These commands will start a web server and provide you with a link to access the UI in your web browser. You can run <code>python web_demo.py --help</code> and <code>python web_demo_captioner.py --help</code> to learn about more options.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# For Qwen3-Omni-30B-A3B-Instruct with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio --flash-attn2"><pre><span><span>#</span> For Qwen3-Omni-30B-A3B-Instruct with vLLM backend</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct
<span><span>#</span> For Qwen3-Omni-30B-A3B-Instruct with Transformers backend</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio
<span><span>#</span> For Qwen3-Omni-30B-A3B-Instruct with Transformers backend and FlashAttention support</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio --flash-attn2</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# For Qwen3-Omni-30B-A3B-Thinking with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers --flash-attn2"><pre><span><span>#</span> For Qwen3-Omni-30B-A3B-Thinking with vLLM backend</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking
<span><span>#</span> For Qwen3-Omni-30B-A3B-Thinking with Transformers backend</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers
<span><span>#</span> For Qwen3-Omni-30B-A3B-Thinking with Transformers backend and FlashAttention support</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers --flash-attn2</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# For Qwen3-Omni-30B-A3B-Captioner with vLLM backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend and FlashAttention support
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers --flash-attn2"><pre><span><span>#</span> For Qwen3-Omni-30B-A3B-Captioner with vLLM backend</span>
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner
<span><span>#</span> For Qwen3-Omni-30B-A3B-Captioner with Transformers backend</span>
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers
<span><span>#</span> For Qwen3-Omni-30B-A3B-Captioner with Transformers backend and FlashAttention support</span>
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers --flash-attn2</pre></div>
<p dir="auto">After running the command, you’ll see a link generated in the terminal similar to this:</p>
<div data-snippet-clipboard-copy-content="Running on local: http://127.0.0.1:8901/"><pre><code>Running on local: http://127.0.0.1:8901/
</code></pre></div>
<p dir="auto">If you are running locally, copy this link and paste it into your browser to access the web UI. If you are running on a server or in a <code>docker</code> container, please configure the address according to the server's actual IP, or set up port forwarding where necessary. For instructions on how to configure port forwarding from the official <code>docker</code> container to the host machine, please refer to <a href="#-docker">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🐳 Docker</h2><a id="user-content--docker" aria-label="Permalink: 🐳 Docker" href="#-docker"></a></p>
<p dir="auto">To simplify the deployment process, we provide Docker images with pre-built environments: <a href="https://hub.docker.com/r/qwenllm/qwen3-omni" rel="nofollow">qwenllm/qwen3-omni</a>. You only need to install the driver and download model files to launch the demos. Please refer to the <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" rel="nofollow">guide</a> to install the NVIDIA Container Toolkit, ensuring that your Docker can access the GPU. For users in mainland China who may have difficulty accessing Docker Hub, you can use mirror acceleration services to pull the images. First, run the following command to pull and initialize the container:</p>
<div dir="auto" data-snippet-clipboard-copy-content="LOCAL_WORKDIR=/path/to/your/workspace
HOST_PORT=8901
CONTAINER_PORT=80
docker run --gpus all --name qwen3-omni \
    -v /var/run/docker.sock:/var/run/docker.sock -p $HOST_PORT:$CONTAINER_PORT \
    --mount type=bind,source=$LOCAL_WORKDIR,target=/data/shared/Qwen3-Omni \
    --shm-size=4gb \
    -it qwenllm/qwen3-omni:3-cu124"><pre>LOCAL_WORKDIR=/path/to/your/workspace
HOST_PORT=8901
CONTAINER_PORT=80
docker run --gpus all --name qwen3-omni \
    -v /var/run/docker.sock:/var/run/docker.sock -p <span>$HOST_PORT</span>:<span>$CONTAINER_PORT</span> \
    --mount type=bind,source=<span>$LOCAL_WORKDIR</span>,target=/data/shared/Qwen3-Omni \
    --shm-size=4gb \
    -it qwenllm/qwen3-omni:3-cu124</pre></div>
<p dir="auto">After executing the command, you will enter the bash shell of the container. Your local model and data directory (<strong>please replace</strong> <code>/path/to/your/workspace</code> <strong>with the actual path</strong>) will be mounted to the container's internal path <code>/data/shared/Qwen3-Omni</code>. The host's port <code>8901</code> is mapped to port <code>80</code> in the container, meaning you can access the service inside the container by visiting port <code>8901</code> on the host machine.</p>
<p dir="auto">Please note that services inside the container must be started with the IP <code>0.0.0.0</code> to ensure proper port forwarding. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run this command inside the Docker container
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --server-port 80 --server-name 0.0.0.0"><pre><span><span>#</span> Run this command inside the Docker container</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --server-port 80 --server-name 0.0.0.0</pre></div>
<p dir="auto">For more ways to launch the web demo, please refer to <a href="#launch-local-web-ui-demo">Launch Local Web UI Demo</a>. If you exit the container, you can re-enter it using the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker start qwen3-omni
docker exec -it qwen3-omni bash"><pre>docker start qwen3-omni
docker <span>exec</span> -it qwen3-omni bash</pre></div>
<p dir="auto">Or if you want to completely remove the container, please run:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Evaluation</h2><a id="user-content-evaluation" aria-label="Permalink: Evaluation" href="#evaluation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Performance of Qwen3-Omni</h3><a id="user-content-performance-of-qwen3-omni" aria-label="Permalink: Performance of Qwen3-Omni" href="#performance-of-qwen3-omni"></a></p>
<p dir="auto">Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro and GPT-4o.</p>
<details>
<summary>Text -&gt; Text</summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th colspan="2"></th>
      <th>GPT-4o-0327</th>
      <th>Qwen3-235B-A22B<br>Non Thinking</th>
      <th>Qwen3-30B-A3B-Instruct-2507</th>
      <th>Qwen3-Omni-30B-A3B-Instruct</th>
      <th>Qwen3-Omni-Flash-Instruct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2">General<br>Tasks</td>
      <td>MMLU-Redux</td>
      <td><strong>91.3</strong></td>
      <td>89.2</td>
      <td>89.3</td>
      <td>86.6</td>
      <td>86.8</td>
    </tr>
    <tr>
      <td>GPQA</td>
      <td>66.9</td>
      <td>62.9</td>
      <td><strong>70.4</strong></td>
      <td>69.6</td>
      <td>69.7</td>
    </tr>
    <tr>
      <td rowspan="2">Reasoning</td>
      <td>AIME25</td>
      <td>26.7</td>
      <td>24.7</td>
      <td>61.3</td>
      <td>65.0</td>
      <td><strong>65.9</strong></td>
    </tr>
    <tr>
      <td>ZebraLogic</td>
      <td>52.6</td>
      <td>37.7</td>
      <td><strong>90.0</strong></td>
      <td>76.0</td>
      <td>76.1</td>
    </tr>
    <tr>
      <td>Code</td>
      <td>MultiPL-E</td>
      <td>82.7</td>
      <td>79.3</td>
      <td><strong>83.8</strong></td>
      <td>81.4</td>
      <td>81.5</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td rowspan="3">Alignment<br>Tasks</td>
      <td>IFEval</td>
      <td>83.9</td>
      <td>83.2</td>
      <td><strong>84.7</strong></td>
      <td>81.0</td>
      <td>81.7</td>
    </tr>
    <tr>
      <td>Creative Writing v3</td>
      <td>84.9</td>
      <td>80.4</td>
      <td><strong>86.0</strong></td>
      <td>80.6</td>
      <td>81.8</td>
    </tr>
    <tr>
      <td>WritingBench</td>
      <td>75.5</td>
      <td>77.0</td>
      <td><strong>85.5</strong></td>
      <td>82.6</td>
      <td>83.0</td>
    </tr>
    <tr>
      <td>Agent</td>
      <td>BFCL-v3</td>
      <td>66.5</td>
      <td><strong>68.0</strong></td>
      <td>65.1</td>
      <td>64.4</td>
      <td>65.0</td>
    </tr>
    <tr>
      <td rowspan="2">Multilingual<br>Tasks</td>
      <td>MultiIF</td>
      <td><strong>70.4</strong></td>
      <td>70.2</td>
      <td>67.9</td>
      <td>64.0</td>
      <td>64.7</td>
    </tr>
    <tr>
      <td>PolyMATH</td>
      <td>25.5</td>
      <td>27.0</td>
      <td><strong>43.1</strong></td>
      <td>37.9</td>
      <td>39.3</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th>Gemini-2.5-Flash<br>Thinking</th>
      <th>Qwen3-235B-A22B<br>Thinking</th>
      <th>Qwen3-30B-A3B-Thinking-2507</th>
      <th>Qwen3-Omni-30B-A3B-Thinking</th>
      <th>Qwen3-Omni-Flash-Thinking</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2"><em>General<br>Tasks</em></td>
      <td>MMLU-Redux</td>
      <td>92.1</td>
      <td><b>92.7</b></td>
      <td>91.4</td>
      <td>88.8</td>
      <td>89.7</td>
    </tr>
    <tr>
      <td>GPQA</td>
      <td><b>82.8</b></td>
      <td>71.1</td>
      <td>73.4</td>
      <td>73.1</td>
      <td>73.1</td>
    </tr>
    <tr>
      <td rowspan="2"><em>Reasoning</em></td>
      <td>AIME25</td>
      <td>72.0</td>
      <td>81.5</td>
      <td><b>85.0</b></td>
      <td>73.7</td>
      <td>74.0</td>
    </tr>
    <tr>
      <td>LiveBench 20241125</td>
      <td>74.3</td>
      <td><b>77.1</b></td>
      <td>76.8</td>
      <td>71.8</td>
      <td>70.3</td>
    </tr>
    <tr>
      <td><em>Code</em></td>
      <td>MultiPL-E</td>
      <td><b>84.5</b></td>
      <td>79.9</td>
      <td>81.3</td>
      <td>80.6</td>
      <td>81.0</td>
    </tr>
    <tr>
      <td rowspan="4"><em>Alignment<br>Tasks</em></td>
      <td>IFEval</td>
      <td><b>89.8</b></td>
      <td>83.4</td>
      <td>88.9</td>
      <td>85.1</td>
      <td>85.2</td>
    </tr>
    <tr>
      <td>Arena-Hard v2</td>
      <td>56.7</td>
      <td><b>61.5</b></td>
      <td>56.0</td>
      <td>55.1</td>
      <td>57.8</td>
    </tr>
    <tr>
      <td>Creative Writing v3</td>
      <td><b>85.0</b></td>
      <td>84.6</td>
      <td>84.4</td>
      <td>82.5</td>
      <td>83.6</td>
    </tr>
    <tr>
      <td>WritingBench</td>
      <td>83.9</td>
      <td>80.3</td>
      <td>85.0</td>
      <td>85.5</td>
      <td><b>85.9</b></td>
    </tr>
    <tr>
      <td><em>Agent</em></td>
      <td>BFCL-v3</td>
      <td>68.6</td>
      <td>70.8</td>
      <td><b>72.4</b></td>
      <td>63.2</td>
      <td>64.5</td>
    </tr>
    <tr>
      <td rowspan="2"><em>Multilingual<br>Tasks</em></td>
      <td>MultiIF</td>
      <td>74.4</td>
      <td>71.9</td>
      <td><b>76.4</b></td>
      <td>72.9</td>
      <td>73.2</td>
    </tr>
    <tr>
      <td>PolyMATH</td>
      <td>49.8</td>
      <td><b>54.7</b></td>
      <td>52.6</td>
      <td>47.1</td>
      <td>48.7</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>Audio -&gt; Text</summary>
<markdown-accessiblity-table><table>
<thead>
  <tr>
    <th></th>
    <th>Seed-ASR</th>
    <th>Voxtral-Mini</th>
    <th>Voxtral-Small</th>
    <th>GPT-4o-Transcribe</th>
    <th>Gemini-2.5-Pro</th>
    <th>Qwen2.5-Omni</th>
    <th>Qwen3-Omni-30B-A3B-Instruct</th>
    <th>Qwen3-Omni-Flash-Instruct</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td colspan="9"><em>EN &amp; ZH ASR (wer)</em></td>
  </tr>
  <tr>
    <td>Wenetspeech<br><em>net</em> | <em>meeting</em></td>
    <td>4.66 | <strong>5.69</strong></td>
    <td>24.30 | 31.53</td>
    <td>20.33 | 26.08</td>
    <td>15.30 | 32.27</td>
    <td>14.43 | 13.47</td>
    <td>5.91 | 7.65</td>
    <td>4.69 | 5.89</td>
    <td><strong>4.62</strong> | 5.75</td>
  </tr>
  <tr>
    <td>Librispeech<br><em>clean</em> | <em>other</em></td>
    <td>1.58 | 2.84</td>
    <td>1.88 | 4.12</td>
    <td>1.56 | 3.30</td>
    <td>1.39 | 3.75</td>
    <td>2.89 | 3.56</td>
    <td>1.74 | 3.45</td>
    <td><strong>1.22</strong> | 2.48</td>
    <td>1.27 | <strong>2.44</strong></td>
  </tr>
  <tr>
    <td>CV15-en</td>
    <td>-</td>
    <td>9.47</td>
    <td>7.79</td>
    <td>10.01</td>
    <td>9.89</td>
    <td>7.61</td>
    <td>6.05</td>
    <td><strong>5.94</strong></td>
  </tr>
  <tr>
    <td>CV15-zh</td>
    <td>-</td>
    <td>24.67</td>
    <td>19.30</td>
    <td>9.84</td>
    <td>8.00</td>
    <td>5.13</td>
    <td>4.31</td>
    <td><strong>4.28</strong></td>
  </tr>
  <tr>
    <td>Fleurs-en</td>
    <td>3.40</td>
    <td>3.96</td>
    <td>3.77</td>
    <td>3.32</td>
    <td>2.94</td>
    <td>3.77</td>
    <td><strong>2.72</strong></td>
    <td>2.74</td>
  </tr>
  <tr>
    <td>Fleurs-zh</td>
    <td>2.69</td>
    <td>12.22</td>
    <td>7.98</td>
    <td>2.44</td>
    <td>2.71</td>
    <td>2.54</td>
    <td>2.20</td>
    <td><strong>2.19</strong></td>
  </tr>
  <tr>
    <td colspan="9"><em>Multilingual ASR (wer)</em></td>
  </tr>
  <tr>
    <td>Fleurs-avg<br>(19 lang)</td>
    <td>-</td>
    <td>15.67</td>
    <td>8.09</td>
    <td>4.48</td>
    <td>5.55</td>
    <td>14.04</td>
    <td>5.33</td>
    <td><strong>5.31</strong></td>
  </tr>
  <tr>
    <td colspan="9"><em>Lyric ASR (wer)</em></td>
  </tr>
  <tr>
    <td>MIR-1K (vocal-only)</td>
    <td>6.45</td>
    <td>23.33</td>
    <td>18.73</td>
    <td>11.87</td>
    <td>9.85</td>
    <td>8.15</td>
    <td>5.90</td>
    <td><strong>5.85</strong></td>
  </tr>
  <tr>
    <td>Opencpop-test</td>
    <td>2.98</td>
    <td>31.01</td>
    <td>16.06</td>
    <td>7.93</td>
    <td>6.49</td>
    <td>2.84</td>
    <td><strong>1.54</strong></td>
    <td>2.02</td>
  </tr>
  <tr>
    <td colspan="9"><em>S2TT (BLEU)</em></td>
  </tr>
  <tr>
    <td>Fleurs-en2xx</td>
    <td>-</td>
    <td>30.35</td>
    <td>37.85</td>
    <td>-</td>
    <td><strong>39.25</strong></td>
    <td>29.22</td>
    <td>37.50</td>
    <td>36.22</td>
  </tr>
  <tr>
    <td>Fleurs-xx2en</td>
    <td>-</td>
    <td>27.54</td>
    <td>32.81</td>
    <td>-</td>
    <td><strong>35.41</strong></td>
    <td>28.61</td>
    <td>31.08</td>
    <td>30.71</td>
  </tr>
  <tr>
    <td>Fleurs-zh2xx</td>
    <td>-</td>
    <td>17.03</td>
    <td>22.05</td>
    <td>-</td>
    <td><strong>26.63</strong></td>
    <td>17.97</td>
    <td>25.17</td>
    <td>25.10</td>
  </tr>
  <tr>
    <td>Fleurs-xx2zh</td>
    <td>-</td>
    <td>28.75</td>
    <td>34.82</td>
    <td>-</td>
    <td><strong>37.50</strong></td>
    <td>27.68</td>
    <td>33.13</td>
    <td>31.19</td>
  </tr>
</tbody>
</table></markdown-accessiblity-table>

      <markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th></th>
      <th>GPT-4o-Audio</th>
      <th>Gemini-2.5-Flash</th>
      <th>Gemini-2.5-Pro</th>
      <th>Qwen2.5-Omni</th>
      <th>Qwen3-Omni-30B-A3B-Instruct</th>
      <th>Qwen3-Omni-30B-A3B-Thinking</th>
      <th>Qwen3-Omni-Flash-Instruct</th>
      <th>Qwen3-Omni-Flash-Thinking</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="9"><strong>VoiceBench</strong></td>
    </tr>
    <tr>
      <td>AlpacaEval</td>
      <td>95.6</td>
      <td>96.1</td>
      <td>94.3</td>
      <td>89.9</td>
      <td>94.8</td>
      <td>96.4</td>
      <td>95.4</td>
      <td><strong>96.8</strong></td>
    </tr>
    <tr>
      <td>CommonEval</td>
      <td>89.8</td>
      <td>88.3</td>
      <td>88.4</td>
      <td>76.7</td>
      <td>90.8</td>
      <td>90.5</td>
      <td><strong>91.0</strong></td>
      <td>90.9</td>
    </tr>
    <tr>
      <td>WildVoice</td>
      <td>91.6</td>
      <td>92.1</td>
      <td>93.4</td>
      <td>77.7</td>
      <td>91.6</td>
      <td>90.5</td>
      <td><strong>92.3</strong></td>
      <td>90.9</td>
    </tr>
    <tr>
      <td>SD-QA</td>
      <td>75.5</td>
      <td>84.5</td>
      <td><strong>90.1</strong></td>
      <td>56.4</td>
      <td>76.9</td>
      <td>78.1</td>
      <td>76.8</td>
      <td>78.5</td>
    </tr>
    <tr>
      <td>MMSU</td>
      <td>80.3</td>
      <td>66.1</td>
      <td>71.1</td>
      <td>61.7</td>
      <td>68.1</td>
      <td>83.0</td>
      <td>68.4</td>
      <td><strong>84.3</strong></td>
    </tr>
    <tr>
      <td>OpenBookQA</td>
      <td>89.2</td>
      <td>56.9</td>
      <td>92.3</td>
      <td>80.9</td>
      <td>89.7</td>
      <td>94.3</td>
      <td>91.4</td>
      <td><strong>95.0</strong></td>
    </tr>
    <tr>
      <td>BBH</td>
      <td>84.1</td>
      <td>83.9</td>
      <td><strong>92.6</strong></td>
      <td>66.7</td>
      <td>80.4</td>
      <td>88.9</td>
      <td>80.6</td>
      <td>89.6</td>
    </tr>
    <tr>
      <td>IFEval</td>
      <td>76.0</td>
      <td>83.8</td>
      <td><strong>85.7</strong></td>
      <td>53.5</td>
      <td>77.8</td>
      <td>80.6</td>
      <td>75.2</td>
      <td>80.8</td>
    </tr>
    <tr>
      <td>AdvBench</td>
      <td>98.7</td>
      <td>98.9</td>
      <td>98.1</td>
      <td>99.2</td>
      <td><strong>99.3</strong></td>
      <td>97.2</td>
      <td><strong>99.4</strong></td>
      <td>98.9</td>
    </tr>
    <tr>
      <td>Overall</td>
      <td>86.8</td>
      <td>83.4</td>
      <td><strong>89.6</strong></td>
      <td>73.6</td>
      <td>85.5</td>
      <td>88.8</td>
      <td>85.6</td>
      <td>89.5</td>
    </tr>
    <tr>
      <td colspan="9"><strong>Audio Reasoning</strong></td>
    </tr>
    <tr>
      <td>MMAU-v05.15.25</td>
      <td>62.5</td>
      <td>71.8</td>
      <td>77.4</td>
      <td>65.5</td>
      <td>77.5</td>
      <td>75.4</td>
      <td><strong>77.6</strong></td>
      <td>76.5</td>
    </tr>
    <tr><td>MMSU</td>
      <td>56.4</td>
      <td>70.2</td>
      <td><strong>77.7</strong></td>
      <td>62.6</td>
      <td>69.0</td>
      <td>70.2</td>
      <td>69.1</td>
      <td>71.3</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th></th>
      <th>Best Specialist<br>Models</th>
      <th>GPT-4o-Audio</th>
      <th>Gemini-2.5-Pro</th>
      <th>Qwen2.5-Omni</th>
      <th>Qwen3-Omni-30B-A3B-Instruct</th>
      <th>Qwen3-Omni-Flash-Instruct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RUL-MuchoMusic</td>
      <td>47.6 (Audio Flamingo 3)</td>
      <td>36.1</td>
      <td>49.4</td>
      <td>47.3</td>
      <td>52.0</td>
      <td><strong>52.1</strong></td>
    </tr>
    <tr>
      <td>GTZAN<br><em>Acc.</em></td>
      <td>87.9 (CLaMP 3)</td>
      <td>76.5</td>
      <td>81.0</td>
      <td>81.7</td>
      <td>93.0</td>
      <td><strong>93.1</strong></td>
    </tr>
    <tr>
      <td>MTG Genre<br><em>Micro F1</em></td>
      <td>35.8 (MuQ-MuLan)</td>
      <td>25.3</td>
      <td>32.6</td>
      <td>32.5</td>
      <td>39.0</td>
      <td><strong>39.5</strong></td>
    </tr>
    <tr>
      <td>MTG Mood/Theme<br><em>Micro F1</em></td>
      <td>10.9 (MuQ-MuLan)</td>
      <td>11.3</td>
      <td>14.1</td>
      <td>8.9</td>
      <td>21.0</td>
      <td><strong>21.7</strong></td>
    </tr>
    <tr>
      <td>MTG Instrument<br><em>Micro F1</em></td>
      <td>39.8 (MuQ-MuLan)</td>
      <td>34.2</td>
      <td>33.0</td>
      <td>22.6</td>
      <td>40.5</td>
      <td><strong>40.7</strong></td>
    </tr>
    <tr>
      <td>MTG Top50<br><em>Micro F1</em></td>
      <td>33.2 (MuQ-MuLan)</td>
      <td>25.0</td>
      <td>26.1</td>
      <td>21.6</td>
      <td>36.7</td>
      <td><strong>36.9</strong></td>
    </tr>
    <tr>
      <td>MagnaTagATune<br><em>Micro F1</em></td>
      <td>41.6 (MuQ)</td>
      <td>29.2</td>
      <td>28.1</td>
      <td>30.1</td>
      <td>44.3</td>
      <td><strong>46.8</strong></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>Vision -&gt; Text</summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Datasets</th>
      <th>GPT4-o</th>
      <th>Gemini-2.0-Flash</th>
      <th>Qwen2.5-VL<br>72B</th>
      <th>Qwen3-Omni-30B-A3B<br>-Instruct</th>
      <th>Qwen3-Omni-Flash<br>-Instruct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="6">General Visual Question Answering</td>
    </tr>
    <tr>
      <td>MMStar</td>
      <td>64.7</td>
      <td><strong>71.4</strong></td>
      <td>70.8</td>
      <td>68.5</td>
      <td>69.3</td>
    </tr>
    <tr>
      <td>HallusionBench</td>
      <td>55.0</td>
      <td>56.3</td>
      <td>55.2</td>
      <td><strong>59.7</strong></td>
      <td>58.5</td>
    </tr>
    <tr>
      <td>MM-MT-Bench</td>
      <td><strong>7.7</strong></td>
      <td>6.7</td>
      <td>7.6</td>
      <td>7.4</td>
      <td>7.6</td>
    </tr>
    <tr>
      <td colspan="6">Math &amp; STEM</td>
    </tr>
    <tr>
      <td>MMMU_val</td>
      <td>69.1</td>
      <td><strong>71.3</strong></td>
      <td>70.2</td>
      <td>69.1</td>
      <td>69.8</td>
    </tr>
    <tr>
      <td>MMMU_pro</td>
      <td>51.9</td>
      <td>56.1</td>
      <td>51.1</td>
      <td>57.0</td>
      <td><strong>57.6</strong></td>
    </tr>
    <tr>
      <td>MathVista_mini</td>
      <td>63.8</td>
      <td>71.4</td>
      <td>74.8</td>
      <td>75.9</td>
      <td><strong>77.4</strong></td>
    </tr>
    <tr>
      <td>MathVision_full</td>
      <td>30.4</td>
      <td>48.6</td>
      <td>38.1</td>
      <td>56.3</td>
      <td><strong>58.3</strong></td>
    </tr>
    <tr>
      <td colspan="6">Documentation Understanding</td>
    </tr>
    <tr>
      <td>AI2D</td>
      <td>84.6</td>
      <td>86.7</td>
      <td><strong>88.7</strong></td>
      <td>85.2</td>
      <td>86.4</td>
    </tr>
    <tr>
      <td>ChartQA_test</td>
      <td>86.7</td>
      <td>64.6</td>
      <td><strong>89.5</strong></td>
      <td>86.8</td>
      <td>87.1</td>
    </tr>
    <tr>
      <td colspan="6">Counting</td>
    </tr>
    <tr>
      <td>CountBench</td>
      <td>87.9</td>
      <td>91.2</td>
      <td><strong>93.6</strong></td>
      <td>90.0</td>
      <td>90.0</td>
    </tr>
    <tr>
      <td colspan="6">Video Understanding</td>
    </tr>
    <tr>
      <td>Video-MME</td>
      <td>71.9</td>
      <td>72.4</td>
      <td><strong>73.3</strong></td>
      <td>70.5</td>
      <td>71.4</td>
    </tr>
    <tr>
      <td>LVBench</td>
      <td>30.8</td>
      <td><strong>57.9</strong></td>
      <td>47.3</td>
      <td>50.2</td>
      <td>51.1</td>
    </tr>
    <tr>
      <td>MLVU</td>
      <td>64.6</td>
      <td>71.0</td>
      <td>74.6</td>
      <td>75.2</td>
      <td><strong>75.5</strong></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Datasets</th>
      <th>Gemini-2.5-flash-thinking</th>
      <th>InternVL-3.5-241B-A28B</th>
      <th>Qwen3-Omni-30B-A3B-Thinking</th>
      <th>Qwen3-Omni-Flash-Thinking</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="5">General Visual Question Answering</td>
    </tr>
    <tr>
      <td>MMStar</td>
      <td>75.5</td>
      <td><b>77.9</b></td>
      <td>74.9</td>
      <td>75.5</td>
    </tr>
    <tr>
      <td>HallusionBench</td>
      <td>61.1</td>
      <td>57.3</td>
      <td>62.8</td>
      <td><b>63.4</b></td>
    </tr>
    <tr>
      <td>MM-MT-Bench</td>
      <td>7.8</td>
      <td>–</td>
      <td><b>8.0</b></td>
      <td><b>8.0</b></td>
    </tr>
    <tr>
      <td colspan="5">Math &amp; STEM</td>
    </tr>
    <tr>
      <td>MMMU_val</td>
      <td>76.9</td>
      <td><b>77.7</b></td>
      <td>75.6</td>
      <td>75.0</td>
    </tr>
    <tr>
      <td>MMMU_pro</td>
      <td><b>65.8</b></td>
      <td>–</td>
      <td>60.5</td>
      <td>60.8</td>
    </tr>
    <tr>
      <td>MathVista_mini</td>
      <td>77.6</td>
      <td><b>82.7</b></td>
      <td>80.0</td>
      <td>81.2</td>
    </tr>
    <tr>
      <td>MathVision_full</td>
      <td>62.3</td>
      <td><b>63.9</b></td>
      <td>62.9</td>
      <td>63.8</td>
    </tr>
    <tr>
      <td colspan="5">Documentation Understanding</td>
    </tr>
    <tr>
      <td>AI2D_test</td>
      <td><b>88.6</b></td>
      <td>87.3</td>
      <td>86.1</td>
      <td>86.8</td>
    </tr>
    <tr>
      <td>ChartQA_test</td>
      <td>–</td>
      <td>88.0</td>
      <td><b>89.5</b></td>
      <td>89.3</td>
    </tr>
    <tr>
      <td colspan="5">Counting</td>
    </tr>
    <tr>
      <td>CountBench</td>
      <td>88.6</td>
      <td>–</td>
      <td>88.6</td>
      <td><b>92.5</b></td>
    </tr>
    <tr>
      <td colspan="5">Video Understanding</td>
    </tr>
    <tr>
      <td>Video-MME</td>
      <td><b>79.6</b></td>
      <td>72.9</td>
      <td>69.7</td>
      <td>69.8</td>
    </tr>
    <tr>
      <td>LVBench</td>
      <td><b>64.5</b></td>
      <td>–</td>
      <td>49.0</td>
      <td>49.5</td>
    </tr>
    <tr>
      <td>MLVU</td>
      <td><b>82.1</b></td>
      <td>78.2</td>
      <td>72.9</td>
      <td>73.9</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>AudioVisual -&gt; Text</summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Datasets</th>
      <th>Previous Open-source SoTA</th>
      <th>Gemini-2.5-Flash</th>
      <th>Qwen2.5-Omni</th>
      <th>Qwen3-Omni-30B-A3B-Instruct</th>
      <th>Qwen3-Omni-Flash-Instruct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>WorldSense</td>
      <td>47.1</td>
      <td>50.9</td>
      <td>45.4</td>
      <td>54.0</td>
      <td><strong>54.1</strong></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Datasets</th>
      <th>Previous Open-source SoTA</th>
      <th>Gemini-2.5-Flash-Thinking</th>
      <th>Qwen3-Omni-30B-A3B-Thinking</th>
      <th>Qwen3-Omni-Flash-Thinking</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DailyOmni</td>
      <td>69.8</td>
      <td>72.7</td>
      <td>75.8</td>
      <td><b>76.2</b></td>
    </tr>
    <tr>
      <td>VideoHolmes</td>
      <td>55.6</td>
      <td>49.5</td>
      <td><b>57.3</b></td>
      <td><b>57.3</b></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>Zero-shot Speech Generation</summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Datasets</th>
      <th>Model</th>
      <th>Performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>&nbsp;</td>
      <td colspan="2"><em>Content Consistency</em></td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td rowspan="10"><strong>SEED</strong><br><em>test-zh</em> | <em>test-en</em></td>
      <td>Seed-TTS<sub>ICL</sub></td>
      <td>1.11 | 2.24</td>
    </tr>
    <tr>
      <td>Seed-TTS<sub>RL</sub></td>
      <td>1.00 | 1.94</td>
    </tr>
    <tr>
      <td>MaskGCT</td>
      <td>2.27 | 2.62</td>
    </tr>
    <tr>
      <td>E2 TTS</td>
      <td>1.97 | 2.19</td>
    </tr>
    <tr>
      <td>F5-TTS</td>
      <td>1.56 | 1.83</td>
    </tr>
    <tr>
      <td>Spark TTS</td>
      <td>1.20 | 1.98</td>
    </tr>
    <tr>
      <td>CosyVoice 2</td>
      <td>1.45 | 2.57</td>
    </tr>
    <tr>
      <td>CosyVoice 3</td>
      <td><strong>0.71</strong> | 1.45</td>
    </tr>
    <tr>
      <td>Qwen2.5-Omni-7B</td>
      <td>1.42 | 2.33</td>
    </tr>
    <tr>
      <td>Qwen3-Omni-30B-A3B</td>
      <td>1.07 | <strong>1.39</strong></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>Multilingual Speech Generation </summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th rowspan="2">Language</th>
      <th colspan="3">Content Consistency</th>
      <th colspan="3">Speaker Similarity</th>
    </tr>
    <tr>
      <th>Qwen3-Omni-30B-A3B</th>
      <th>MiniMax</th>
      <th>ElevenLabs</th>
      <th>Qwen3-Omni-30B-A3B</th>
      <th>MiniMax</th>
      <th>ElevenLabs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Chinese</td>
      <td><strong>0.716</strong></td>
      <td>2.252</td>
      <td>16.026</td>
      <td>0.772</td>
      <td><strong>0.780</strong></td>
      <td>0.677</td>
    </tr>
    <tr>
      <td>English</td>
      <td><strong>1.069</strong></td>
      <td>2.164</td>
      <td>2.339</td>
      <td><strong>0.773</strong></td>
      <td>0.756</td>
      <td>0.613</td>
    </tr>
    <tr>
      <td>German</td>
      <td>0.777</td>
      <td>1.906</td>
      <td><strong>0.572</strong></td>
      <td><strong>0.738</strong></td>
      <td>0.733</td>
      <td>0.614</td>
    </tr>
    <tr>
      <td>Italian</td>
      <td><strong>1.067</strong></td>
      <td>1.543</td>
      <td>1.743</td>
      <td><strong>0.742</strong></td>
      <td>0.699</td>
      <td>0.579</td>
    </tr>
    <tr>
      <td>Portuguese</td>
      <td>1.872</td>
      <td>1.877</td>
      <td><strong>1.331</strong></td>
      <td>0.770</td>
      <td><strong>0.805</strong></td>
      <td>0.711</td>
    </tr>
    <tr>
      <td>Spanish</td>
      <td>1.765</td>
      <td><strong>1.029</strong></td>
      <td>1.084</td>
      <td>0.744</td>
      <td><strong>0.762</strong></td>
      <td>0.615</td>
    </tr>
    <tr>
      <td>Japanese</td>
      <td>3.631</td>
      <td><strong>3.519</strong></td>
      <td>10.646</td>
      <td>0.763</td>
      <td><strong>0.776</strong></td>
      <td>0.738</td>
    </tr>
    <tr>
      <td>Korean</td>
      <td><strong>1.670</strong></td>
      <td>1.747</td>
      <td>1.865</td>
      <td><strong>0.778</strong></td>
      <td>0.776</td>
      <td>0.700</td>
    </tr>
    <tr>
      <td>French</td>
      <td><strong>2.505</strong></td>
      <td>4.099</td>
      <td>5.216</td>
      <td><strong>0.689</strong></td>
      <td>0.628</td>
      <td>0.535</td>
    </tr>
    <tr>
      <td>Russian</td>
      <td>3.986</td>
      <td>4.281</td>
      <td><strong>3.878</strong></td>
      <td>0.759</td>
      <td><strong>0.761</strong></td>
      <td>0.676</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>Cross-Lingual Speech Generation </summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Language</th>
      <th>Qwen3-Omni-30B-A3B</th>
      <th>CosyVoice3</th>
      <th>CosyVoice2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>en-to-zh</td>
      <td>5.37</td>
      <td><strong>5.09</strong></td>
      <td>13.5</td>
    </tr>
    <tr>
      <td>ja-to-zh</td>
      <td>3.32</td>
      <td><strong>3.05</strong></td>
      <td>48.1</td>
    </tr>
    <tr>
      <td>ko-to-zh</td>
      <td><strong>0.99</strong></td>
      <td>1.06</td>
      <td>7.70</td>
    </tr>
    <tr>
      <td>zh-to-en</td>
      <td><strong>2.76</strong></td>
      <td>2.98</td>
      <td>6.47</td>
    </tr>
    <tr>
      <td>ja-to-en</td>
      <td><strong>3.31</strong></td>
      <td>4.20</td>
      <td>17.1</td>
    </tr>
    <tr>
      <td>ko-to-en</td>
      <td><strong>3.34</strong></td>
      <td>4.19</td>
      <td>11.2</td>
    </tr>
    <tr>
      <td>zh-to-ja</td>
      <td>8.29</td>
      <td><strong>7.08</strong></td>
      <td>13.1</td>
    </tr>
    <tr>
      <td>en-to-ja</td>
      <td>7.53</td>
      <td><strong>6.80</strong></td>
      <td>14.9</td>
    </tr>
    <tr>
      <td>ko-to-ja</td>
      <td>4.24</td>
      <td><strong>3.93</strong></td>
      <td>5.86</td>
    </tr>
    <tr>
      <td>zh-to-ko</td>
      <td><strong>5.13</strong></td>
      <td>14.4</td>
      <td>24.8</td>
    </tr>
    <tr>
      <td>en-to-ko</td>
      <td><strong>4.96</strong></td>
      <td>5.87</td>
      <td>21.9</td>
    </tr>
    <tr>
      <td>ja-to-ko</td>
      <td><strong>6.23</strong></td>
      <td>7.92</td>
      <td>21.5</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setting for Evaluation</h3><a id="user-content-setting-for-evaluation" aria-label="Permalink: Setting for Evaluation" href="#setting-for-evaluation"></a></p>
<ul dir="auto">
<li><strong>Decoding Strategy</strong>: For the Qwen3-Omni series across all evaluation benchmarks, <code>Instruct</code> models use greedy decoding during generation without sampling. For <code>Thinking</code> models, the decoding parameters should be taken from the <code>generation_config.json</code> file in the checkpoint.</li>
<li><strong>Benchmark-Specific Formatting</strong>: For the majority of evaluation benchmarks, they come with their own ChatML formatting to embed the question or prompt. It should be noted that all video data are set to <code>fps=2</code> during evaluation.</li>
<li><strong>Default Prompts</strong>: For tasks in certain benchmarks that do not include a prompt, we use the following prompt settings:</li>
</ul>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Task Type</th>
<th>Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Auto Speech Recognition (ASR) for Chinese</td>
<td>请将这段中文语音转换为纯文本。</td>
</tr>
<tr>
<td>Auto Speech Recognition (ASR) for Other languages</td>
<td>Transcribe the  audio into text.</td>
</tr>
<tr>
<td>Speech-to-Text Translation (S2TT)</td>
<td>Listen to the provided &lt;source_language&gt; speech and produce a translation in &lt;target_language&gt; text.</td>
</tr>
<tr>
<td>Song Lyrics Recognition</td>
<td>Transcribe the song lyrics into text without any punctuation, separate lines with line breaks, and output only the lyrics without additional explanations.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<ul dir="auto">
<li><strong>System Prompt</strong>: No <code>system prompt</code> should be set for any evaluation benchmark.</li>
<li><strong>Input Sequence</strong>: The question or prompt should be input as user text. Unless otherwise specified by the benchmark, the text should come <strong>after</strong> multimodal data in the sequence. For example:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;/path/to/audio.wav&quot;},
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;/path/to/image.png&quot;},
            {&quot;type&quot;: &quot;video&quot;, &quot;video&quot;: &quot;/path/to/video.mp4&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe the audio, image and video.&quot;},
        ],
    },
]"><pre><span>messages</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"/path/to/audio.wav"</span>},
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"/path/to/image.png"</span>},
            {<span>"type"</span>: <span>"video"</span>, <span>"video"</span>: <span>"/path/to/video.mp4"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"Describe the audio, image and video."</span>},
        ],
    },
]</pre></div>

<br>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California issues fine over lawyer's ChatGPT fabrications (156 pts)]]></title>
            <link>https://calmatters.org/economy/technology/2025/09/chatgpt-lawyer-fine-ai-regulation/</link>
            <guid>45335774</guid>
            <pubDate>Mon, 22 Sep 2025 16:30:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://calmatters.org/economy/technology/2025/09/chatgpt-lawyer-fine-ai-regulation/">https://calmatters.org/economy/technology/2025/09/chatgpt-lawyer-fine-ai-regulation/</a>, See on <a href="https://news.ycombinator.com/item?id=45335774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		
				
		<div>
				
				<p><strong>In summary</strong></p>
				

				
				<p>The court of appeals said 21 of 23 quotes in an opening brief were fake. State authorities are scrambling to grapple with widespread use of artificial intelligence.</p>
				
							</div>
		
		

<p>A California attorney must pay a $10,000 fine for filing a state court appeal full of fake quotations generated by the artificial intelligence tool ChatGPT.</p>

<p>The fine appears to be the largest issued over AI fabrications by a California court and came with a <a href="https://www4.courts.ca.gov/opinions/documents/B331918.PDF" target="_blank" rel="noreferrer noopener">blistering opinion</a> stating that 21 of 23 quotes from cases cited in the attorney’s opening brief were made up. It also noted that numerous out-of-state and federal courts have confronted attorneys for citing fake legal authority.</p>

<p>“We therefore publish this opinion as a warning,” it continued. “Simply stated, no brief, pleading, motion, or any other paper filed in any court should contain any citations— whether provided by generative AI or any other source—that the attorney responsible for submitting the pleading has not personally read and verified.”</p>

<p>The opinion, issued 10 days ago in California’s 2nd District Court of Appeal, is a clear example of why the state’s legal authorities are scrambling to regulate the use of AI in the judiciary. The state’s Judicial Council two weeks ago <a href="https://courts.ca.gov/cms/rules/index/standards/Standard10_80" target="_blank" rel="noreferrer noopener">issued guidelines requiring judges and court staff</a> to either ban generative AI or adopt a generative AI use policy by Dec. 15. Meanwhile, the California Bar Association is considering whether to strengthen its code of conduct to account for various forms of AI following a request by the California Supreme Court last month.</p>


<p>The Los Angeles-area attorney fined last week, Amir Mostafavi, told the court that he did not read text generated by the AI model before submitting the appeal in July 2023, months after OpenAI marketed ChatGPT as capable of <a href="https://law.stanford.edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificial-intelligence-tools-in-the-legal-industry/" target="_blank" rel="noreferrer noopener">passing the bar exam</a>. A three-judge panel fined him for filing a frivolous appeal, violating court rules, citing fake cases, and wasting the court’s time and the taxpayers money, according to the opinion.</p>

<p>Mostafavi told CalMatters he wrote the appeal and then used ChatGPT to try and improve it. He said that he didn’t know it would add case citations or make things up.&nbsp;</p>

<p>He thinks it is unrealistic to expect lawyers to stop using AI. It’s become an important tool just as online databases largely replaced law libraries and, until AI systems stop hallucinating fake information, he suggests lawyers who use AI to proceed with caution.</p>

<p>“In the meantime we’re going to have some victims, we’re going to have some damages, we’re going to have some wreckages,” he said. “I hope this example will help others not fall into the hole. I’m paying the price.”</p>


<p>The fine issued to Mostafavi is the most costly penalty issued to an attorney by a California state court and one of the highest fines ever issued over attorney use of&nbsp; AI, according to Damien Charlotin, who teaches a class on AI and the law at a business school in Paris. He <a href="https://www.damiencharlotin.com/hallucinations/">tracks</a> instances of attorneys citing fake cases, primarily in Australia, Canada, the United States, and the United Kingdom.</p>

<p>In a widely-publicized case in May, a U.S. district court judge in California <a href="https://websitedc.s3.amazonaws.com/documents/Lacey_v._State_Farm_General_Insurances_Co._D._Cal._May_6_2025.pdf">ordered two law firms to pay</a> $31,100 in fees to defense counsel and the court for costs associated with using “bogus AI-generated research.” In that ruling, the judge described feeling misled, said they almost cited fake material in a judicial order and said “Strong deterrence is needed to make sure that attorneys don’t succumb to this easy shortcut.”</p>

<figure><blockquote><p>“We’re going to have some wreckages.”</p><cite>Amir Mostafavi, lawyer fined $10,000 after submitting opening brief filled with quotes fabricated by ChatGPT</cite></blockquote></figure>

<p>Charlotin thinks courts and the public should expect to see an exponential rise in these cases in the future. When he started tracking court filings involving AI and fake cases earlier this year, he encountered a few cases a month. Now he sees a few cases a day. Large language models confidently state falsehoods as facts, particularly when there are no supporting facts.</p>

<p>“The harder your legal argument is to make, the more the model will tend to hallucinate, because they will try to please you,” he said. “That’s where the confirmation bias kicks in.”</p>

<p>A <a href="https://hai.stanford.edu/news/ai-trial-legal-models-hallucinate-1-out-6-or-more-benchmarking-queries">May 2024 analysis</a> by Stanford University’s RegLab found that although three out of four lawyers plan to use generative AI in their practice, some forms of AI generate hallucinations in one out of three queries. Detecting fake material cited in legal filings <a href="https://www.nytimes.com/2025/05/05/technology/ai-hallucinations-chatgpt-google.html">could get harder as models grow in size</a>.</p>

<p><a href="https://www.ailawlibrarians.com/2025/08/10/coming-soon-the-interactive-genai-legal-hallucination-tracker-sneak-peek-today/">Another tracker</a> of cases where lawyers cite nonexistent legal authority due to use of AI identifies 52 such cases in California and more than 600 nationwide.<strong> </strong>That amount is expected to increase in the near future because AI innovation is outpacing the education of attorneys, said Nicholas Sanctis, a law student at Capital University Law School in Ohio.</p>
	<div data-posts="" data-current-post-id="476080">
							<h2>
					<span>More on AI in government</span>
				</h2>
						
	<article data-post-id="466445">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

				</div>
	
<p>Jenny Wondracek, who leads the tracker project, said she expects this trend to get worse because she still regularly encounters lawyers who don’t know that AI makes things up or believe that legal tech tools can eliminate all fake or false material generated by language models.&nbsp;</p>

<p>“I think we’d see a reduction if (lawyers) just understood the basics of the technology,” she said.</p>

<p>Like Charlotin, she suspects there are more instances of made up cases generated by AI in state court filings than in federal courts, but a lack of standard filing methods makes it difficult to verify that. She said she encounters fake cases most often among overburdened attorneys or people who choose to represent themselves in family court.</p>

<p>She suspects the number of arguments filed by attorneys that use AI and cite fake cases will continue to go up, but added that not just attorneys engage in the practice. In recent weeks, she’s documented three <a href="https://www.ailawlibrarians.com/2025/07/03/first-known-court-order-with-fabricated-cases-and-a-test-run-of-citecheck-ai/">instances of judges</a> citing fake legal authority in their decisions.</p>

<figure><blockquote><p>She’s documented three judges citing fake legal authority in their decisions.</p></blockquote></figure>

<p>As California considers how to treat generative AI and fake case citations, Wondracek said they can consider approaches taken by other states, such as temporary suspensions, requiring attorneys who get caught to take courses to better understand how to ethically use AI, or requiring them to <a href="https://www.lawnext.com/2025/09/nevada-judge-takes-creative-and-unusual-approach-to-combat-ai-generated-fictitious-citations.html">teach law students how they can avoid making the same mistake</a>.</p>

<p>Mark McKenna, codirector of the UCLA Institute of Technology, Law &amp; Policy praised fines like the one against Mostafavi as punishing lawyers for “an abdication of your responsibility as a party representing someone.” He thinks the problem “will get worse before it gets better,” because there’s been a rush among law schools and private firms to adopt AI without thinking through the appropriate way to use them.</p>

<p>UCLA School of Law professor Andrew Selbst agrees, pointing out that clerks that work for judges are recent law school graduates, and students are getting bombarded with the message that they must use AI or get left behind. Educators and other professionals <a href="https://calmatters.org/economy/technology/2024/06/teachers-ai-grading/">report feeling similar pressures</a>.</p>

<p>“This is getting shoved down all our throats,” he said. “It’s being pushed in firms and schools and a lot of places and we have not yet grappled with the consequences of that.”</p>

<p><strong>For the record</strong>: The fine issued to Mostafavi was for $10,000. Due to an editing error, an earlier version of this article had an incorrect figure.</p>
	<div data-posts="" data-current-post-id="476080">
							<h2>
					<span>READ NEXT</span>
				</h2>
						
	<article data-post-id="475357">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

		
	<article data-post-id="467979">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

				</div>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Testing is better than data structures and algorithms (161 pts)]]></title>
            <link>https://nedbatchelder.com/blog/202509/testing_is_better_than_dsa.html</link>
            <guid>45335635</guid>
            <pubDate>Mon, 22 Sep 2025 16:21:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nedbatchelder.com/blog/202509/testing_is_better_than_dsa.html">https://nedbatchelder.com/blog/202509/testing_is_better_than_dsa.html</a>, See on <a href="https://news.ycombinator.com/item?id=45335635">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><article id="e20250922T120408"><header><p><a href="https://nedbatchelder.com/blog/archive/date0922.html">Monday 22</a> <a href="https://nedbatchelder.com/blog/archive/year2025.html#month202509">September 2025</a></p><p>People should spend less time learning DSA, more time learning testing.</p></header><p>I see new learners asking about “DSA” a lot.  Data Structures and Algorithms
are of course important: considered broadly, they are the two ingredients that
make up all programs.  But in my opinion, “DSA” as an abstract field of study
is over-emphasized.</p><p>I understand why people focus on DSA: it’s a concrete thing to learn about,
there are web sites devoted to testing you on it, and most importantly, because
job interviews often involve DSA coding questions.</p><p>Before I get to other opinions, let me make clear that anything you can do to
help you get a job is a good thing to do.  If grinding
<a rel="external noopener" href="https://leetcode.com/">leetcode</a> will land you a position, then do it.</p><p>But I hope companies hiring entry-level engineers aren’t asking them to
reverse linked lists or balance trees.  Asking about techniques that can be
memorized ahead of time won’t tell them anything about how well you can work.
The stated purpose of those interviews is to see how well you can figure out
solutions, in which case memorization will defeat the point.</p><p>The thing new learners don’t understand about DSA is that actual software
engineering almost never involves implementing the kinds of algorithms that
“DSA” teaches you.  Sure, it can be helpful to work through some of these
puzzles and see how they are solved, but writing real code just doesn’t involve
writing that kind of code.</p><p>Here is what I think in-the-trenches software engineers should know about
data structures and algorithms:</p><ul><li>Data structures are ways to organize data. Learn some of the basics: linked
list, array, hash table, tree.  By “learn” I mean understand what it does
and why you might want to use one.</li><li>Different data structures can be used to organize the same data in different
ways.  Learn some of the trade-offs between structures that are similar.</li><li>Algorithms are ways of manipulating data.  I don’t mean named algorithms
like Quicksort, but algorithms as any chunk of code that works on data and
does something with it.</li><li>How you organize data affects what algorithms you can use to work with the
data.  Some data structures will be slow for some operations where another
structure will be fast.</li><li>Algorithms have a “time complexity” (Big O): <a rel="external noopener" href="https://nedbatchelder.com/text/bigo.html">how the code
slows as the data grows</a>.  Get a sense of what this means.</li><li>Python has a number of built-in data structures.  Learn how they work, and
the time complexity of their operations.</li><li>Learn how to think about your code to understand its time complexity.</li><li>Read a little about more esoteric things like <a rel="external noopener" href="https://systemdesign.one/bloom-filters-explained/">Bloom
filters</a>, so you can find them later in the unlikely case you need them.</li></ul><p>Here are some things you don’t need to learn:</p><ul><li>The details of a dozen different sorting algorithms.  Look at two to see
different ways of approaching the same problem, then move on.</li><li>The names of “important” algorithms.  Those have all been implemented for
you.</li><li>The answers to all N problems on some quiz web site.  You won’t be asked
these exact questions, and they won’t come up in your real work.  Again: try a
few to get a feel for how some algorithms work.  The exact answers are not what
you need.</li></ul><p>Of course some engineers need to implement hash tables, or sorting algorithms
or whatever.  We love those engineers: they write libraries we can use off the
shelf so we don’t have to implement them ourselves.</p><p>There have been times when I implemented something that felt like An
Algorithm (for example, <a href="https://nedbatchelder.com/blog/201707/finding_fuzzy_floats.html">Finding fuzzy floats</a>), but it was
more about considering another perspective on my data, looking at the time
complexity, and moving operations around to avoid quadratic behavior. It wasn’t
opening a textbook to find the famous algorithm that would solve my problem.</p><p>Again: if it will help you get a job, deep-study DSA. But don’t be
disappointed when you don’t use it on the job.</p><p>If you want to prepare yourself for a career, and also stand out in job
interviews, learn how to write tests:</p><ul><li>This will be a skill you use constantly. Real-world software means writing
tests much more than school teaches you to.</li><li>In a job search, testing experience will stand out more than DSA depth.  It
shows you’ve thought about what it takes to write high-quality software instead
of just academic exercises.</li><li>It’s not obvious how to test code well. It’s a puzzle and a problem to
solve.  If you like figuring out solutions to tricky questions, focus on how to
write code so that it can be tested, and how to test it.</li><li>Testing not only gives you more confidence in your code, it helps you write
better code in the first place.</li><li>Testing applies everywhere, from tiny bits of code to entire architectures,
assisting you in design and implementation at all scales.</li><li>If pursued diligently, testing is an engineering discipline in its own
right, with a fascinating array of tools and techniques.</li></ul><p>Less DSA, more testing.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI and Nvidia announce partnership to deploy 10GW of Nvidia systems (442 pts)]]></title>
            <link>https://openai.com/index/openai-nvidia-systems-partnership/</link>
            <guid>45335474</guid>
            <pubDate>Mon, 22 Sep 2025 16:10:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/openai-nvidia-systems-partnership/">https://openai.com/index/openai-nvidia-systems-partnership/</a>, See on <a href="https://news.ycombinator.com/item?id=45335474">Hacker News</a></p>
Couldn't get https://openai.com/index/openai-nvidia-systems-partnership/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[UK Millionaire exodus did not occur, study reveals (245 pts)]]></title>
            <link>https://taxjustice.net/press/millionaire-exodus-did-not-occur-study-reveals/</link>
            <guid>45335135</guid>
            <pubDate>Mon, 22 Sep 2025 15:48:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taxjustice.net/press/millionaire-exodus-did-not-occur-study-reveals/">https://taxjustice.net/press/millionaire-exodus-did-not-occur-study-reveals/</a>, See on <a href="https://news.ycombinator.com/item?id=45335135">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="s_content"><p><span><em>30 news pieces a day in 2024 on non-existent exodus</em></span></p><p>A millionaire exodus widely reported by news outlets around the world in 2024, and credited for the UK Labour government’s decision to weaken tax reforms, did not occur, the Tax Justice Network reveals.</p><p>The media reporting – consisting of over 10,900 news pieces across print, broadcast and online news in 2024 – was primarily based on a report published by Henley &amp; Partners<sup>1</sup>, a firm that sells golden passports to the superrich and advises governments on setting up such schemes. The European Court of Justice recently ruled one such scheme, that of Malta, to be unlawful.<sup>2</sup></p><p>The Tax Justice Network’s review – co-published with Patriotic Millionaires UK and Tax Justice UK – of the Henley report finds that the number of millionaires claimed by Henley &amp; Partners to be leaving countries in “exodus” in 2024 represented near-0% of those countries’ millionaire populations.<sup>3</sup> For example, the 9500 millionaires widely reported to be leaving the UK in 2024 represented 0.3% of the UK’s 3.06 million millionaires.<sup>4</sup></p><p>Media reporting widely blamed the alleged millionaire exodus on tax policies in the same year that calls for a wealth tax on the superrich gained unprecedented momentum globally.<sup>5</sup> The media reporting was equivalent to 30 news pieces a day on the non-existent millionaire exodus across 2024.</p><p>Reviewing the full period from 2013 to 2024 for which the Henley report presents estimates on millionaire migration, the Tax Justice Network finds that millionaire migration rates consistently stood at near-0% for every year.<sup>6</sup> Academic studies consistently show that the tax responses of the wealthy involves minimal levels of migration.<sup>7</sup></p><p>Henley’s estimates, when put into perspective, reveal a picture that is at complete odds with the report’s narrative and media coverage: millionaires are highly immobile, and nearly 100% of millionaires have <em>not</em> relocated to a new country since 2013, if Henley’s estimates are to be taken at face value.</p><p>Henley &amp; Partners was accused in a 2018 UK Parliamentary inquiry of meddling in the elections of Caribbean nations in return for the exclusive rights to sell golden passports.<sup>8</sup> Henley &amp; Partners told The Guardian it “fundamentally rejects any allegation of wrongdoing”.<sup>9</sup> A recent Financial Times article identified an EU-sanctioned Russian businessman with links to the Ukraine invasion who could more easily circumvent travel restrictions due to a Maltese golden passport Henley helped him acquire in the past.<sup>10</sup> A spokesperson for Henley &amp; Partners told the Financial Times that while she “could not comment on individual cases because of missing information and data protection… an individual ‘may pass all the stringent due diligence tests imposed, but still go on to engage in criminal activity.’”<sup>11 </sup></p><p>Golden passports are now illegal in the EU following a successful court challenge brought by the European Commission against Malta’s scheme, on which Henley &amp; Partners had advised. The Commission said such schemes pose serious risks for money laundering, tax evasion and corruption.<sup>12</sup> Henley &amp; Partners told media in response that it was “disappointed” but that the decision “will only increase the demand for specialized advisors”.<sup>13</sup></p><p><span><strong>Findings behind Labour climbdown riddled with problems</strong></span></p><p>The UK Labour government’s decision in January 2025 to weaken non-dom tax reform was widely reported to be a result of concerns about the Henley report’s findings.<sup>14</sup></p><p>The Tax Justice Network’s review of the Henley report flags several issues with the report’s methodology as well as contradictions in Henley &amp; Partner’s reporting, and particularly in its claims on the UK exodus.</p><p>Strikingly, the report’s methodology<sup>15</sup> states that its estimates are primarily a measure of where millionaires <em>say they work on social media</em> and not of where they live or reside, meaning the report does not track actual, physical migration – contrary to the presenting of the estimates in the press.</p><p>Moreover, the report uses a far narrower definition of ‘millionaires’ that does not include all dollar millionaires like the standard definition (people with net worth of 1 million dollars or more), but rather only individuals with liquid assets worth 1 million dollars or more, who are thus richer and more mobile on average than a standardly defined millionaire.<sup>16</sup> In the case of the UK, the ‘millionaires’ identified by the report represent just a fifth (20%) of the UK millionaire population.<sup>17 </sup>Even then, the report is based on a small sample from within these narrowly defined millionaires and the sample is skewed towards centi-millionaires and billionaires, who are also likely to be the most easily mobile.<sup>18</sup></p><p>Just as striking, the use of the term “exodus” has been inconsistent in the analysis. In 2021, Henley described 2000 millionaires leaving the UK as “insignificant” but in 2023 described 1600 millionaires leaving the UK an “exodus”. In 2023, the 6500 millionaires claimed to be leaving India were described as “not particularly concerning” but redescribed in 2024 as a “wealth exodus”.<sup>19</sup></p><p>The Tax Justice Network wrote to Henley &amp; Partners and New World Wealth (who prepared the Henley report’s estimates) with questions for each ahead of the publication of its review. The response received said<sup>20</sup>:</p><p><em>“It seems this entire debate is over that one word. The dictionary definition is just ‘mass migration’, and HMRC’s own data shows that the number of non-doms in the UK is decreasing year on year – which seems to be a mass migration. If you are looking to the biblical definition, then to use the term ‘exodus’ would of course mean that all non-doms are leaving, but I don’t think many people take biblical interpretations quite so literally?”</em></p><p>Furthermore, Henley &amp; Partners labelled the UK’s alleged exodus a <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2023/private-wealth-insights/transformative-era-investment-migration">“Brexodus”</a> in 2023, claiming that the exodus was largely an impact of Brexit.<sup>21 </sup>In October 2024, Henley relabelled the exodus a <a href="https://www.henleyglobal.com/newsroom/press-releases/uk-wealth-exodus">“Wexit”</a> in a press release framing the UK exodus as a reaction to tax hikes that might be announced in the UK Labour government’s upcoming budget statement.<sup>22</sup></p><p>Henley &amp; Partners’ specified in October 2024 that the “Wexit” is a “wealth exodus” that includes centi-millionaires and billionaires, and the report’s author emphasised that “[t]he large number of centi-millionaires leaving the UK is particularly concerning”.<sup>23</sup> These claims appear inconsistent with Henley’s forecast made the month prior in September 2024 that the UK centi-millionaire population is growing and will continue to grow from 2024 to 2040.<sup>24</sup></p><p>The press release highlighted that the UK Labour government’s budget statement as a main reason for this alleged “wealth exodus”:</p><p><em>“The UK’s high tax rates and concerns about additional tax hikes that could be announced at the end of the month in the Labour Party’s first budget in 14 years, are highlighted as being among the main reasons for the wealth exodus.”<sup>25</sup></em></p><p>A response sent by Henley &amp; Partners to the Tax Justice Network said:</p><p><em>“We have never claimed that Labour tax policies were the sole or root cause.&nbsp; If papers such as the Telegraph, Times, Mail, decide to add their own layer on to that, and deliberately exclude from their story our standard reminder to them that these were the Conservatives’ tax changes, then I think your argument is with them not with us.”</em></p><p>Moreover, it is unclear whether the forecast of centi-millionaires and billionaires leaving the UK that Henley reported in October 2024 was different from the forecast it initially made in June 2024 when the Labour party was not in power.</p><p><span><strong>Media adds fuel to the fictional fire</strong></span></p><p>The Tax Justice Network’s analysis of media coverage of the Henley report finds that coverage often went far beyond any claims made in the report itself, contributing to an entirely unfounded narrative about the role of tax and government policies in causing a millionaire exodus which itself did not occur.</p><p>Tax was mentioned in half of global media coverage of the exodus and far more often than any other exodus drivers discussed in the Henley report.</p><p>The UK Labour party, which was not in power when the report was published in June 2024, was mentioned more than twice as much as the UK Conservative party in global media coverage, and nearly four times as much as Brexit in UK media coverage.</p><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4MDAiIGhlaWdodD0iNDc0IiB2aWV3Qm94PSIwIDAgODAwIDQ3NCI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" data-src="https://taxjustice.net/wp-content/uploads/2025/06/Graph-drivers-in-the-media.png" alt="" width="800" height="474"><br>
<span>Note: Percentages show the number of mentions as a share of all global media coverage &nbsp;</span></p><p>The picture is more skewed in UK media coverage, where tax was mentioned in 71% of coverage and Labour mentioned in 43% of coverage.<sup>26</sup></p><p>Seven high-profile millionaires leaving the UK were mentioned nearly three times more often in global media coverage than pro-tax millionaire campaign groups representing hundreds of millionaires.<sup>27</sup></p><p>In contrast to the media narrative, 81% of UK millionaires agree with the statement that it is patriotic to pay a fair share of tax, according to a poll published on 5 June 2025 by Patriotic Millionaires UK. 80% of UK millionaires said they would support a wealth tax of 2% on wealth over £10 million.<sup>28</sup></p><p>The Tax Justice Network’s review of the Henley report raised a number of other questions, including the statistical credibility of drawing any conclusions from a self-admittedly unrepresentative sample; and the degree of extrapolation necessary to make any findings about smaller groups such as billionaires.</p><p>On the report’s sample, the response sent by Henley &amp; Partners said:</p><p><em>“Statistically, if it is consistent year by year, then laws of statistical sampling mean that it can be used to draw a conclusion.”</em></p><p>The report is published by Henley &amp; Partners but prepared by New World Wealth<sup>29</sup>, which describes itself as a “wealth intelligence firm” on its website. New World Wealth appears to have only one staff member and has not made the data behind its calculations public.</p><p>New World Wealth has been publishing estimates on millionaire migrations for at least a decade, and first began to publish its estimates with Henley &amp; Partners in 2022, which was the first time the estimates on millionaire migrations underway were described as an “exodus”.</p><p>More specific questions about the sample sent to New World Wealth, including a question asking how many real persons in the sample were observed to have “migrated” in 2024, were not responded to.</p><p><strong>Fariya Mohiuddin</strong>, Deputy Director: External Affairs at Tax Justice UK said:</p><p><em>“Taxing the super-rich to revitalise key services like the NHS and education, that we all rely on, is more urgent than ever. Taxing the wealth of the richest is simply not going to cause a mass exodus. This is scaremongering and statistical obfuscation by companies that represent the interests of billionaires and multi-millionaires. In fact, when the numbers are crunched properly, rather than using dodgy stats and figures, tax is an inconsequential factor in the decision-making of the vanishingly small percentage of millionaires that do decide to move. In fact, many wealthy people want to pay more tax. They know that when public services are well-funded, people are healthy, and the country works better, they will benefit – alongside everyone else.”</em></p><p>Member of Patriotic Millionaires UK and legal consultant, <strong>Stephen Kinsella</strong> said:</p><p><em>“As this excellent report from the Tax Justice Network shows, millionaires like me aren’t going anywhere.&nbsp;We want to build a better&nbsp;Britain so we’re proud to pay&nbsp;and here to stay.&nbsp;When nearly three quarters of UK millionaires think taxes should be raised on the richest to reduce the strain on everyone else, and 81%&nbsp;think it’s patriotic to pay their fair share in tax, what on earth is stopping our Government from doing their duty and taxing extreme wealth?”</em></p><p><strong>Alex Cobham</strong>, chief executive at the Tax Justice Network, said:</p><p><em>“The majority of people want taxes on the superrich, the majority of millionaires are saying tax us, and practically all credible studies say you should do it. But &nbsp;what the media reported, and the government listened to, was a fictional millionaire ‘exodus’ based on questionable data published by a firm that helps the superrich buy their way out of the rules that apply to everybody else. Tax is our most powerful tool for creating more equal societies, but scare stories like these are used to talk down to people and to block positive change.” &nbsp;</em></p><p><em>“This is a wakeup call for media professionals and governments alike. Do your homework when it comes to tax. Treat the Henley report and any such claims about fleeing millionaires with extreme caution, and make sure your stories and your policy decisions are based on robust evidence.”</em></p><p>-ENDS-</p><p><strong>Read </strong><a href="https://taxjustice.net/reports/the-millionaire-exodus-myth/"><strong>our review of the Henley report</strong></a></p><p><strong>Notes to Editor</strong></p><ol><li>The <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2024/">Henley Private Wealth Migration Report 2024</a> was published on 18 June 2024 by <a href="https://www.henleyglobal.com/">Henley &amp; Partners</a>. The report’s estimates are prepared by <a href="http://newworldwealth.com/">New World Wealth</a>.</li><li>See this <a href="https://www.ft.com/content/7bc66138-a5b4-4fd6-948c-78bb65d6d492">FT article</a> for more information about the European Court of Justice’s ruling and Henley &amp; Partners role in the Maltese scheme. See the press release from the Court <a href="https://curia.europa.eu/jcms/upload/docs/application/pdf/2025-04/cp250052en.pdf">here</a>.</li><li>The Tax Justice Network’s review of the Henley report is available <a href="https://taxjustice.net/reports/the-millionaire-exodus-myth/">here.</a></li><li>Countries’ reported migrating millionaires represented less than 1% of their millionaires, and was closer to 0% for most countries, including the UK.<br>
<img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI2NTkiIGhlaWdodD0iMzA2IiB2aWV3Qm94PSIwIDAgNjU5IDMwNiI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" data-src="https://taxjustice.net/wp-content/uploads/2025/06/Table-national-global-migrations.png" alt="" width="659" height="306"><br>
<span>Source: <em>The millionaire exodus myth</em>, Tax Justice Network, June 2025</span></li><li>Some examples of media coverage:<br>
– <a href="https://www.reuters.com/world/uk/globally-mobile-millionaires-threaten-desert-britain-over-tax-2024-07-19/"><strong>Reuters:</strong></a> Ultra-rich entrepreneurs threaten to desert Britain over tax<br>
– <a href="https://www.bloomberg.com/news/articles/2024-07-05/britain-s-ultra-rich-map-out-plans-to-escape-non-dom-taxes-after-labour-victory"><strong>Bloomberg UK:</strong></a> Britain’s Ultra-Rich Map Out Routes to Escape ‘Non-Dom’ Taxes After Labour Victory<br>
– <a href="http://edition.cnn.com/2024/06/18/business/uk-millionaires-loss-record"><strong>CNN:</strong></a> Millionaires fleeing Britain in their thousands<br>
– <a href="https://www.telegraph.co.uk/business/2024/07/10/millennials-inheritance-boomer-wealth/"><strong>The Telegraph:</strong></a> Britain to suffer world’s biggest exodus of millionaires as Labour takes power<br>
– <a href="https://www.cnbc.com/2024/06/18/millionaires-are-abandoning-the-uk-in-their-droves-new-research-shows.html"><strong>CNBC:</strong></a> Millionaires are abandoning the UK in droves, new research shows</li><li>The total number of millionaires reported on <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2024/global-wealth-migration">the Henley &amp; Partners website</a> to have migrated every year since 2013 to 2023 consistently represented around 0.2% of millionaires annually. Moreover, while the global millionaire population has grown since 2013, the millionaire migration rate, however small, is marginally lower now than it was in the middle of the previous decade – even after bouncing back from the enforced immobility of the pandemic years. The “<a href="https://www.henleyglobal.com/newsroom/press-releases/henley-private-wealth-migration-report-2024">unprecedented</a>”, “<a href="https://www.henleyglobal.com/newsroom/press-releases/henley-private-wealth-migration-report-2024">record numbers</a>” of migrating millionaires Henley reported in 2024 are proportionally smaller than the migration numbers reported for 2016, 2017 and 2018.<br>
<img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4MDAiIGhlaWdodD0iNDQ1IiB2aWV3Qm94PSIwIDAgODAwIDQ0NSI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" data-src="https://taxjustice.net/wp-content/uploads/2025/06/Graph-rate-vs-population-2013-2024.png" alt="" width="800" height="445"><br>
<span>Source: <em>The millionaire exodus myth</em>, Tax Justice Network, June 2025</span></li><li>See the literature on migration surveyed in our report, <a href="https://taxjustice.net/reports/taxing-extreme-wealth-what-countries-around-the-world-could-gain-from-progressive-wealth-taxes/"><em>Taxing extreme wealth: what countries around the world could gain from progressive wealth taxes</em></a> (Alison Schulz &amp; Miroslav Palanský), 2024, Tax Justice Network. Meanwhile, a <a href="https://www.lse.ac.uk/News/Latest-news-from-LSE/2024/a-January-2024/super-rich-unlikely-to-leave-uk-for-boring-and-culturally-barren-tax-havens">London School of Economics study</a> found that the vast majority of Britain’s extremely wealthy people would never leave the country for tax reasons, partly due to the stigma involved in doing so, and partly because they think lower-tax jurisdictions are “boring”.</li><li>More information about the inquiry in this <a href="https://www.ft.com/content/7bc66138-a5b4-4fd6-948c-78bb65d6d492">FT article</a>. Read The Guardian’s 2018 investigation <a href="https://www.theguardian.com/world/2018/oct/16/the-passport-king-who-markets-citizenship-for-cash">here</a>.</li><li>See Henley’s response to the Guardian <a href="https://www.theguardian.com/world/2018/oct/16/henley-partners-statement">here</a>.</li><li>Read the FT’s investigation on Maltese golden passports sold to Russians <a href="https://www.ft.com/content/b6accfc5-eeca-4a83-a39d-1dc54a7688dd">here</a>.</li><li>See note 10 for Henley’s response.</li><li>See note 2.</li><li>See Henley’s statement on the European Court of Justice’s ruling <a href="https://www.henleyglobal.com/newsroom/press-releases/ecj-malta-ruling-2025">here</a>.</li><li>Some examples of media reporting:<br>
– <a href="https://news.sky.com/story/rachel-reeves-to-soften-non-dom-tax-crackdown-after-listening-to-concerns-13295016"><strong>Sky News</strong></a><strong>: </strong>“Rachel Reeves is to water down her crackdown on the non-dom tax status after analysis showed it had prompted an exodus of millionaires.”<br>
– <a href="https://www.cnbc.com/2025/01/24/uk-to-soften-tax-rules-for-wealthy-foreigners-after-millionaire-exodus-rachel-reeves-says.html"><strong>CNBC</strong></a><strong>: </strong>“The U.K. is to soften some planned changes to its controversial non-dom tax rule following concerns of a millionaire exodus, the Treasury has confirmed.”<br>
– <a href="https://www.independent.co.uk/news/uk/politics/reeves-davos-non-dom-tax-millionaire-b2684926.html"><strong>The Independent</strong></a><strong>: </strong>“Reeves to water down tax raid on non-doms amid exodus of millionaires”<br>
– <a href="https://www.thetimes.com/business-money/economics/article/rachel-reeves-to-amend-non-dom-tax-rules-after-millionaire-exodus-s6fjr2tj9"><strong>The Times</strong></a><strong>:</strong> “Rachel Reeves to relax non-dom tax rule amid millionaire exodus”</li><li>The Henley report’s methodology states: “The firm [New World Wealth] uses various public sources to check city locations, including LinkedIn and other business portals. Its stats are therefore mainly based on the work locations of the individuals.” The methodology is available at the bottom of this <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2024/">webpage</a>.</li><li>See the Henley report’s methodology at the bottom of this <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2024/">webpage</a>.</li><li>The Henley report’s author stated in a <a href="https://www.bbc.co.uk/sounds/play/m002304z">BBC interview</a> that the group of UK ‘millionaires’ as defined in the report totalled 602,000, which is around one fifth of the UK’s millionaire population, which the UBS Global Wealth Report 2024 estimates to stand at 3.06 million millionaires.</li><li>The Henley report’s author Andrew Amolis acknowledged in an interview with <a href="https://www.bbc.co.uk/sounds/play/m002304z">BBC More or Less</a> that the report’s sample is skewed. Presenter Tim Harford challenged Mr Amolis further (emphasis added):<strong>Andrew Amoils</strong>: Most of the database – I’d say between twenty and a hundred million dollars in assets, that would be the bulk of the database. So our data is skewed to the top end, so there will be the billionaires and the centimillionaires with over a hundred million, with some of the people lower down it’s more difficult to know if they are a high net worth.<strong>Tim Harford</strong>: But wait, aren’t these super-rich more easily able to skip off to Monaco or Dubai than your run of a mill dollar millionaire?<strong>AA</strong>: No, you’re right, 100%, that would be an issue. Though I would argue that the super-wealthy leaving is obviously the most important, because if you’ve got a banker at Goldman Sachs who’s making five hundred thousand pounds a year and they leave, that has very little impact whereas if somebody with over a hundred million who’s got a business leaves, the impact’s much greater.<strong>TH</strong>: Sure, but the headlines are not about a few people controlling a huge amount of money leaving, the headlines are about 9000 millionaires leaving. So the headlines imply that there is some kind of representative sample, and there’s some kind of reasonable extrapolation, but from <strong>based on what you’ve told me I don’t think we really can reasonably extrapolate</strong>, given the methods you’re describing.<strong>AA</strong>: Well how else would you do it? I mean, we’ve got a sample of 150000 high net worths globally, and we’re tracking them in terms of their movements. How else would you do it? How else would you work out where people are going, apart from the way we’re doing it?<strong>TH</strong>: Well I think <strong>if you don’t have a representative sample, you don’t have any basis to make the claim at all</strong>.<strong>AA</strong>: Well I would argue it is a representative sample. 150000 people, that’s a lot!<strong>TH</strong>: <strong>But you just told us it wasn’t representative.</strong> The sheer number of people doesn’t make it representative.</li><li>The term “exodus” has been used inconsistently, as this table shows.<br>
<img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI2MDIiIGhlaWdodD0iMTkzIiB2aWV3Qm94PSIwIDAgNjAyIDE5MyI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" data-src="https://taxjustice.net/wp-content/uploads/2025/06/Table-use-of-exodus.png" alt="" width="602" height="193"><br>
<span>Source: <em>The millionaire exodus myth</em>, Tax Justice Network, June 2025</span><br>
<span><em>Note</em>: The table presents migration numbers for the three reports New World Wealth published with the AfrAsia bank from 2018 to 2020 and three reports it published with Henley &amp; Partners from 2022 to 2024. New World Wealth’s calculation of migration as a percentage of millionaire population are provided in parentheses where available. New World Wealth provided the percentages in 2019 and 2020, and provided percentages for some countries in 2022. No percentages were provided in 2023 and 2024 that we could find. </span><br>
<span>*Retroactively called a “wealth exodus” in Henley’s 2024 <a href="https://www.henleyglobal.com/newsroom/press-releases/henley-private-wealth-migration-report-2024">press release</a>.</span></li><li>The response the Tax Justice Network received is reproduced in full at the end of the Tax Justice Network’s <a href="https://taxjustice.net/reports/the-millionaire-exodus-myth/">review of the Henley report</a>.</li><li>See Henley &amp; Partners’ use of the term “Brexodus” in this <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2023/private-wealth-insights/transformative-era-investment-migration">article </a>from the Henley 2023 report.</li><li>See Henley &amp; Partner’s October 2024 press release using the term “Wexit” <a href="https://www.henleyglobal.com/newsroom/press-releases/uk-wealth-exodus">here</a>. The press release was syndicated at least 400 times across the media landscape in large part due to the PR Newswire service. While Henley’s original press release did list in its notes to editor Brexit as possible driver of exodus, the notes to editor were cut off in the <a href="https://www.prnewswire.co.uk/news-releases/wexit-wealthy-brits-exit-uk-for-eu-ahead-of-budget-302280346.html">PR Newswire version</a> of the press release that was widely syndicated. Nonetheless, Henley’s October 2024 press release did refer to Brexit in the body of the release, but when doing so contrasted Brexit as a positive phenomenon that is separate from the wealth exodus. The press release stated: “Based on data over the past nine months, the UK’s wealth exodus or WEXIT is expected to include 85 centi-millionaires and 10 billionaires, and in an ironic reversal of Brexit fortunes, 68% are heading for Europe, with favored destinations being Italy, Malta, Greece, Portugal, Switzerland, Monaco, Cyprus, France, Spain, and the Netherlands.”</li><li>Henley’s <a href="https://www.henleyglobal.com/publications/centi-millionaire-report-2024"><em>The Centi-Millionaire Report 2024</em></a> was published on 17 September 2024.</li><li>See note 22.</li><li>Looking specifically at UK media coverage, we find the mentions of themes and drivers to be far more skewed towards tax and Labour.<br>
<img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4MDAiIGhlaWdodD0iNDc0IiB2aWV3Qm94PSIwIDAgODAwIDQ3NCI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" data-src="https://taxjustice.net/wp-content/uploads/2025/06/Graph-drivers-in-UK-media.png" alt="" width="800" height="474"><br>
<span>Source: <em>The millionaire exodus myth</em>, Tax Justice Network, June 2025</span></li><li>The seven high-profile millionaires reportedly moving away from the UK – Charlie Mullins, Christian Angermayer, Alan Howard, Nassef Sawiris, Asif Aziz and Bassim Haidar – were mentioned in 199 articles. In contrast, Patriotic Millionaires, Patriotic Millionaires UK, Millionaires for Humanity, Tax Me Now and Proud to Pay More – campaigning groups representing over 300 millionaires calling on governments to tax them more – were mentioned 73 times. The seven migrating millionaires were mentioned 2.7 times more than the pro-tax millionaires groups.</li><li>See Patriotic Millionaire UK’s polling <a href="https://patrioticmillionaires.uk/latest-news/uk-millionaire-poll-2025">here</a>.</li><li>See New World Wealth’s <a href="http://newworldwealth.com/">website</a>.</li></ol><p><strong>About Patriotic Millionaires UK</strong></p><p><a href="https://patrioticmillionaires.uk/">Patriotic Millionaires UK</a> is a nonpartisan network of British millionaires, from multiple industries and backgrounds from across the UK. It delivers a single mission – to leverage the voice of wealth to build a better Britain by changing the system to end extreme wealth and make those with it make their fair and proper contribution.</p><p><strong>About Tax Justice UK</strong></p><p>The UK’s approach to tax isn’t working. Our government fails to raise enough money to support high quality public services and wealth is desperately under-taxed. We campaign for a fairer tax system that takes more from the very wealthy. A tax system that actively redistributes wealth to tackle inequality; and that funds high quality public services. Our mission is to ensure that everyone in the UK benefits from a fair and effective tax system. <a href="https://taxjustice.uk/">Tax Justice UK</a> is a partner of – but independent from – the Tax Justice Network.</p></div></div>]]></description>
        </item>
    </channel>
</rss>