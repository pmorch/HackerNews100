<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 12 Dec 2024 08:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A ChatGPT clone, in 3000 bytes of C, backed by GPT-2 (2023) (136 pts)]]></title>
            <link>https://nicholas.carlini.com/writing/2023/chat-gpt-2-in-c.html</link>
            <guid>42396372</guid>
            <pubDate>Thu, 12 Dec 2024 05:01:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicholas.carlini.com/writing/2023/chat-gpt-2-in-c.html">https://nicholas.carlini.com/writing/2023/chat-gpt-2-in-c.html</a>, See on <a href="https://news.ycombinator.com/item?id=42396372">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>
              This program is a dependency-free implementation of GPT-2. It loads
              the weight matrix and BPE file out of the original TensorFlow files,
              tokenizes the input with a simple byte-pair encoder,
              implements a basic linear algebra package with matrix math operations,
              defines the transformer architecture, performs transformer inference,
              and un-tokenizes the output with the BPE decoder.
              All in ~3000 bytes of C.
            </p>
            
      
            <p>
              It's optimized efficiently enough so that GPT-2 Small takes a few
              seconds per reply on any modern machine. To do this I've implemented
              KV caching and an efficient matrix multiplication algorithm,
              with optional OMP parallelism.
            </p>
      
            <p>
              You can then use this to create something like Chat GPT---just so long
              as you don't care about the quality of the output. (It's actually
              pretty terrible output, objectively speaking... But it does run.)
              There are a
              few quirks (especially with handling UTF-8 characters), and running
              the XL size model at long context length can require ~100GB of RAM.
              But if you're just typing with ASCII using GPT2-Small it should run
              just about anywhere.
            </p>
      
            <p>
              I've uploaded <a href="https://github.com/carlini/c-chat-gpt-2">the code to GitHub</a>, so feel free to try and use it there.
            </p>
            
      
            
        
            
            <div id="mine"><p>
            This program is made up of the following main blocks (hover over each to see the coresponding code):        
                <a href="#linalg" id="show0">Basic matrix math library (700 bytes)</a>
                <a href="#matmul" id="show1">Fast matrix multiplication (300 bytes)</a>
                <a href="#nn" id="show2">Neural network layers (300 bytes)</a>
                <a href="#gpt" id="show3">Transformer model (600 bytes)</a>
                <a href="#bpe" id="show5">Byte pair encoding (400 bytes)</a>
                <a href="#z" id="show6">I/O (200 bytes)</a>
                <a href="#loadweight" id="show8">Weight loading (300 bytes)</a>
                <a href="#loadbpe" id="show7">Byte pair encoding loading (300 bytes)</a></p><div id="main">
      <p><span>#include</span><span>&lt;stdio.h&gt;</span></p>
      <p><span>#include</span><span>&lt;stdlib.h&gt;</span></p>
      <p><span>#include</span><span>&lt;string.h&gt;</span></p>
      <p><span>#include</span><span>&lt;math.h&gt;</span></p>
      <p><span>int</span><span> </span><span>U</span><span>,</span><span>C</span><span>,</span><span>K</span><span>,</span><span>c</span><span>,</span><span>d</span><span>,</span><span>S</span><span>,</span><span>zz</span><span>;</span><span>char</span><span>*</span><span>bpe</span><span>;</span><span>typedef</span><span> </span><span>struct</span><span>{</span><span>float</span><span>*</span><span>i</span><span>;</span><span>int</span><span> </span><span>j</span><span>,</span><span>k</span><span>;} </span><span>A</span><span>;</span><span>void</span><span>*</span><span>E</span><span>,*</span><span>n</span><span>;</span><span>A</span><span>*</span><span>f</span><span>;</span><span>FILE</span><span>*</span><span>fp</span><span>;</span></p>
      <p><span>#define</span><span> </span><span>N</span><span>(</span><span>i</span><span>,</span><span>j</span><span>)</span><span>for</span><span>(</span><span>int</span><span> </span><span>i</span><span>=0; i&lt;j; i++)</span></p>
      <p><span>
      <p><span>A</span><span> </span><span>o</span><span>(</span><span>int</span><span> </span><span>j</span><span>,</span><span>int</span><span> </span><span>k</span><span>,</span><span>int</span><span> </span><span>i</span><span>){</span><span>float</span><span>*</span><span>a</span><span>=E;E+=S=4*j*k;memset(a,0,S*i);</span><span>A</span><span> </span><span>R</span><span>={ a,j,k} ;</span><span>return</span><span> R;}</span></p>
      <p><span>#define</span><span> </span><span>I</span><span>(</span><span>R</span><span>,</span><span>B</span><span>)</span><span>A</span><span> </span><span>R</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>float</span><span> </span><span>k</span><span>){ N(i,a.j*a.k){ </span><span>float</span><span> </span><span>b</span><span>=a.i[i]; a.i[i]=B; } </span><span>return</span><span> a; }</span></p>
      <p><span>I</span><span>(l,b/k)</span><span>I</span><span>(q,b+k)</span><span>I</span><span>(u,1./sqrt(</span><span>b</span><span>))</span><span>I</span><span>(z,</span><span>exp</span><span>(</span><span>b</span><span>))</span><span>I</span><span>(r,a.i[(i/a.k)*a.k])</span><span>I</span><span>(P,(i/k&lt;i%(</span><span>int</span><span>)k)?0:exp(b/8))</span><span>I</span><span>(Q,b/2*(1+tanh(.7978845*(b+.044715*b*b*b))))</span></p>
      <p><span>#define</span><span> </span><span>F</span><span>(</span><span>R</span><span>,</span><span>B</span><span>)</span><span>A</span><span> </span><span>R</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>A</span><span> </span><span>b</span><span>){ N(i,a.j*a.k){ a.i[i]=a.i[i]B b.i[i]; } </span><span>return</span><span> a; }</span></p>
      <p><span>F</span><span>(V,+)</span><span>F</span><span>(v,*)</span><span>F</span><span>(H,/)</span><span>F</span><span>(at,+b.i[i%a.k];)</span><span>F</span><span>(mt,*b.i[i%a.k];)</span><span>A</span><span> </span><span>X</span><span>(</span><span>A</span><span> </span><span>a</span><span>){</span><span>A</span><span> </span><span>R</span><span>=o(a.j,a.k,1);N(i,a.j*a.k)R.i[(i/a.k)*a.k]+=a.i[i];r(R,0);</span><span>return</span><span> R;}</span><span>A</span><span> </span><span>p</span><span>(</span><span>A</span><span> </span><span>a</span><span>){</span><span>A</span><span> </span><span>R</span><span>=o(a.k,a.j,1);N(i,a.j*a.k)R.i[i%a.k*a.j+i/a.k]=a.i[i];</span><span>return</span><span> R;}</span></p>
      </span>
      <span>
        <p><span>A</span><span> </span><span>g</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>A</span><span> </span><span>b</span><span>){</span><span>A</span><span> </span><span>R</span><span>=o(a.j,b.j,!c);</span><span>{</span><span>for</span><span>(</span><span>int</span><span> </span><span>i</span><span>=c;i&lt;d;i++){</span><span>for</span><span>(</span><span>int</span><span> </span><span>j</span><span>=0;j&lt;b.j;j+=4){</span><span>for</span><span>(</span><span>int</span><span> </span><span>k</span><span>=0;k&lt;a.k;k+=4){N(k2,4)N(j2,4)R.i[i*b.j+j+j2]+=a.i[i*a.k+k+k2]*b.i[(j+j2)*b.k+k+k2];}}}}</span><span>return</span></p>
      </span>
      <span>
      <p><span> V(o(R.j,R.k,1),R);}</span><span>A</span><span> </span><span>J</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>int</span><span> </span><span>b</span><span>,</span><span>int</span><span> </span><span>j</span><span>,</span><span>int</span><span> </span><span>k</span><span>){</span><span>A</span><span> </span><span>R</span><span>={ a.i+b*j,j,k} ;</span><span>return</span><span> R;}</span><span>A</span><span> </span><span>s</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>int</span><span> </span><span>i</span><span>){</span><span>A</span><span> </span><span>b</span><span>=V(a,l(X(a),-a.k));</span><span>A</span><span> </span><span>k</span><span>=l(X(v(V(o(b.j,b.k,1),b),b)),b.k-1);</span><span>A</span><span> </span><span>R</span><span>=at(mt(v(V(o(b.j,b.k,1),b),u(q(k,1e-5),0)),f[i+1]),f[i]);</span><span>return</span><span> R;}</span></p>
      <p><span>#define</span><span> </span><span>G</span><span>(</span><span>a</span><span>,</span><span>i</span><span>)at(g(a,f[i+1]),f[i])</span></p>
      </span>
      <span>
        <p><span>A</span><span> </span><span>m</span><span>(</span><span>int</span><span> </span><span>j</span><span>,</span><span>int</span><span> </span><span>k</span><span>){j+=!j;k+=!k;</span><span>A</span><span> </span><span>a</span><span>=o(j,k,1);fread(a.i,S,1,fp);</span><span>return</span><span> p(a);}</span></p>
      </span>
      <span>
        <p><span>int</span><span> </span><span>t</span><span>;</span><span>int</span><span> </span><span>Y</span><span>(</span><span>char</span><span>*</span><span>R</span><span>){</span><span>if</span><span>(!*R)</span><span>return</span><span> 0;</span><span>int</span><span> </span><span>B</span><span>=1e9,</span><span>r</span><span>;N(i,5e4){</span><span>if</span><span>(bpe[999*i]&amp;&amp;strncmp(bpe+999*i,R,S=strlen(bpe+999*i))==0){</span><span>int</span><span> </span><span>k</span><span>=Y(R+S)+i+1e7;</span><span>if</span><span>(k&lt;B){B=k;r=i;}}}t=r;</span><span>return</span><span> B;}</span><span>int</span><span> *</span><span>w</span><span>(</span><span>char</span><span>*</span><span>q</span><span>,</span><span>int</span><span>*</span><span>B</span><span>){</span><span>char</span><span> </span><span>R</span><span>[1000];</span><span>int</span><span> </span><span>i</span><span>=0;</span><span>while</span><span>(q[i]){</span><span>int</span><span> </span><span>j</span><span>=i++;</span><span>while</span><span>(47&lt;q[i]&amp;&amp;q[i]&lt;58||64&lt;q[i]){fflush(stdout);i++;}strcpy(R,q+j);R[i-j]=0;fflush(stdout);</span><span>int</span><span> </span><span>k</span><span>=0;</span><span>while</span><span>(R[k]){Y(R+k);</span><span>char</span><span>*</span><span>M</span><span>=bpe+t*999;k+=strlen(M);*B++=t;}}</span><span>return</span><span> B;}</span></p>
      </span>
      <span>
        <p><span>int</span><span> </span><span>main</span><span>(</span><span>int</span><span> </span><span>S</span><span>,</span><span>char</span><span>**</span><span>D</span><span>){S=D[1][5]+3*D[1][7]+3&amp;3;K=12+4*S+(S&gt;2);U=K*64;C=12*S+12;zz=atoi(D[4]);E=malloc(2LL*U*U*C*zz);</span></p>
      </span>
      <span>
        <p><span>bpe=malloc(1e9);fp=fopen(D[2],</span><span>"r"</span><span>);</span><span>unsigned</span><span> </span><span>char</span><span> </span><span>a</span><span>[S=999],</span><span>b</span><span>[S];N(i,5e4){</span><span>int</span><span> </span><span>k</span><span>=i*S;</span><span>if</span><span>(i&lt;93){bpe[k]=i+33;bpe[k+1]=0;} </span><span>else</span><span> </span><span>if</span><span>(i&gt;254){fscanf(fp,</span><span>"%s %s"</span><span>,a,b);strcat((</span><span>char</span><span>*)a,(</span><span>char</span><span>*)b);</span><span>int</span><span> </span><span>j</span><span>=0;N(i,a[i])bpe[k+j++]=a[i]^196?a[i]:a[++i]-128;bpe[k+j++]=0;} </span><span>else</span><span> </span><span>if</span><span>(i&gt;187){bpe[k]=i-188;bpe[k+1]=0;}}</span><span>int</span><span> </span><span>e</span><span>[1024];d=w(D[3],e)-e;</span><span>int</span><span> </span><span>h</span><span>;N(i,d){</span><span>if</span><span>(e[i]==18861)h=i+1;}printf(</span><span>"AI"</span><span>);N(i,d-h)printf(</span><span>"%s"</span><span>,bpe+e[i+h]*999);</span></p>
      </span>
      <span>
        <p><span>fp=fopen(D[1],</span><span>"r"</span><span>);</span><span>A</span><span>\</span></p>
      <p><span><span>&nbsp;</span></span><span>x</span><span>[999];</span><span>A</span><span>*</span><span>R</span><span>=x;N(i,C){N(j,12)*R++=m(U+U*(j?j^8?j^11?0:3:3:2),U*((j%8==3)+3*(j%8==1)+(j==9)));}*R++=m(U,1);*R++=m(U,1);</span><span>A</span><span> </span><span>QA</span><span>=m(1024,U),</span><span>Z</span><span>=p(m(5e4,U));</span></p>
      </span>
      <span>
        <p><span>while</span><span>(1){</span><span>char</span><span> </span><span>W</span><span>[1000]={ 0} ;</span><span>int</span><span> </span><span>T</span><span>;strcat(W,</span><span>"\nAlice: "</span><span>);printf(</span><span>"\n%s: "</span><span>,bpe+20490*999);fflush(stdout);fgets(W+8,1000,stdin);printf(</span><span>"AI:"</span><span>);strcat(W,</span><span>"\nBob:"</span><span>);d=w(W,e+d)-e;n=E;c=0;</span></p>
      </span>
      <span>
        <p><span>while</span><span>(1){E=n;T=d+32-d%32;c*=!!(d%32);</span><span>A</span><span> </span><span>O</span><span>=o(T,U,1);N(i,d){N(j,U)O.i[i*U+j]=Z.i[e[i]*U+j]+QA.i[j*1024+i];}N(i,C){</span><span>int</span><span> </span><span>y</span><span>;S=0;N(j,10){</span><span>if</span><span>(j==i)y=S;S++;N(k,10*(j&gt;0)){</span><span>if</span><span>(j*10+k&lt;C&amp;&amp;S++&amp;&amp;i==j*10+k)y=S;}}f=x+12*y;</span><span>A</span><span> </span><span>QB</span><span>=p(J(G(s(O,4),0),0,T*3,U));</span><span>A</span><span> </span><span>B</span><span>=o(U,T,1);N(k,K){</span><span>A</span><span> </span><span>L</span><span>=p(J(QB,k*3,64*T,3)),</span><span>a</span><span>=P(g(p(J(L,0,64,T)),p(J(L,T,64,T))),T),</span><span>R</span><span>=p(g(H(a,X(a)),J(L,T*2,64,T)));memcpy(B.i+64*T*k,R.i,64*T*4);}O=V(O,G(p(B),2));O=V(O,G(Q(G(s(O,6),8),0),10));}f=x;O=s(O,12*C);c=0;</span><span>int</span><span> </span><span>S</span><span>=d;d=1;</span><span>A</span><span> </span><span>B</span><span>=g(p(J(O,S-1,U,1)),Z);c=d=S;S=0;N(i,5e4){</span><span>if</span><span>(B.i[i]&gt;B.i[S])S=i;}</span><span>if</span><span>(d==zz){memcpy(e,e+zz/2,S*2);d-=zz/2;c=0;}e[d++]=S;</span></p>
      </span>
      <span>
        <p><span>if</span><span>(bpe[S*999]==10)</span><span>break</span><span>;printf(</span><span>"%s"</span><span>,bpe+S*999);fflush(stdout);}}}</span></p>
        </span>
            </p></div>
      
            </div>
      
            
            
      
            <br>
            <h2>Background: ChatGPT and transformers</h2>
      
            <p>
              In case you've been living under a rock for the past few months,
              ChatGPT is an application where you can talk to a type of machine learning
              model called a "language model" as if it was another person. It responds remarkably well,
              and GPT-4, the latest model that powers ChatGPT, is incredibly impressive.
            </p>
      
            <p>
              This C program implements the behavior of ChatGPT using a much
              weaker model from 2019: GPT-2. Despite being just 2 smaller than GPT-4,
              it has no where near the same capabilities---but it is open source.
              So it has that going for it.
            </p>
      
            <p>
              <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>
              is a type of machine learning model called a <a href="https://arxiv.org/abs/1706.03762">"transformer"</a>.
              These neural networks take a fixed-size sequence of words as input,
              and predict the next word that will occur. By repeating the procedure
              over and over, you can use them to generate arbitrary-length sequences.
            </p>
      
            <p>
              This post isn't meant to be an introduction to all the machine learning
              you'll need to know <i>why</i> a transformer is designed the way it is,
              but the rest of this post will be dedicated to describing how the above
              C code works.
            </p>
      
            <br>
            <h2>Walkthrough the C Code</h2>
      
            <h3 id="linalg">Getting started: Matrix Math (700 bytes)</h3>
      
            <p>
              Seeing as neural networks are just matrix operations. So we're going to need to get
              started by building a matrix library in as few bytes as possible.
            </p>
      
            <p>
              My definition of a matrix is completely minimal:
            </p>
      
      
            <div>
            <p><span>typedef</span><span> </span><span>struct</span><span> {</span></p>
      <p><span><span>&nbsp; </span></span><span>float</span><span>* </span><span>dat</span><span>;</span></p>
            <p><span><span>&nbsp; </span></span><span>int</span><span> </span><span>rows</span><span>, </span><span>cols</span><span>;</span></p>
      <p><span>} Matrix;</span></p>
            </div>
      
            <p>
              We'll begin by by observing that while there are a bunch of different operations
              we'll need to implement, there are basically two "types" of operations"
              </p><ol>
                <li>
                  Matrix-constant operations (e.g., add 7 to each entry of a matrix)
                </li>
                <li>
                  Matrix-matrix operations (e.g., add corresponding matrix entries)
                </li>
              </ol>
            
      
            <p>
              This similarity allows us to use macros to pull out a bunch of the common logic
              into a meta-routine that knows how to operate on, for example, pairs
              of matrices and just leaves the specific operator implementation defined.
            </p>
      
            <p>
              To do this in C, I'll define the function
            </p>
      
      
            <p><span>#define</span><span> </span><span>BINARY</span><span>(</span><span>function</span><span>, </span><span>operation</span><span>)</span></p>
      
            <p>
              as the following:
            </p>
            
            <div>
              <p><span>Matrix FUNCTION(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>Matrix</span><span> </span><span>b</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; a.rows; i++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; a.cols; j++) {</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>a[i*a.cols + j] = a[i*a.cols + j] OPERATION b[i*a.cols+j];</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span></span><span>return</span><span> a;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              And so for example this lets us just write
            </p>
      
            <div>
              <p><span>BINARY(matrix_elementwise_add, +);</span></p>
      <p><span>BINARY</span><span>(matrix_elementwise_multiply, *);</span></p>
            </div>
      
            <p>
            and have it automatically expand to the full operation that perform
            elementwise addition or multiplication of two matrices. I define a few
            other easy to understand operations as well:
            </p>
      
            <p>
            Now the thing about C's #defines is they're basically just glorified regexs.
            So when we actually run this, what's going to happen is we're going to take
            the line
            </p>
      
            <p><span>a[i*a.cols + j] = a[i*a.cols + j] OPERATION b[i*a.cols+j];</span></p><p>
            
            and expand for the case of multiplication expand it to
            
            </p><p><span>a[i*a.cols + j] = a[i*a.cols + j] * b[i*a.cols+j];</span></p>
      
            <p>
              But this replacement is almost literally just a regular expression replace.
              We could have put anything in place of OPERATION.
              This allows us to define a function like
            </p>
      
            <p><span>BINARY(add_tile, + b.dat[i%a.cols] ; )</span></p>
      
            <p>
              Which at first glance looks rather confusing---what is that semi-colon doing there?---but
              if you just do a regular expression replace on it, you'll see that it expands to
            </p>
      
            <p><span><span>&nbsp; </span>a[i*a.cols + j] = a[i*a.cols + j] + b.dat[i%a.cols] ; </span><span>b</span><span>[i*a.cols+j];</span></p>
      
            <p>
              where because the second expression doesn't do anything this is just equivalent to
            </p>
            
            <p><span>a[i*a.cols + j] = a[i*a.cols + j] + b.dat[i%a.cols] ; </span><span>b</span><span>[i*a.cols+j];</span></p>
      
            <p>
              (TAKE THAT LANGUAGES WITH PROPER MACROS. LISP ISN'T ALWAYS BETTER THAN C!)
            </p>
            
            <h3 id="matmul">Fast matrix multiplication (300 bytes)</h3>
      
            <p>
              The basic implementation of matrix multiplication is entirely straightforward:
              we just implement the naive cubic-time three loops:
              (There's nothing intelligent about my matrix multiplication. If you know how to make
              matrix multiplication fast you can just move along.)
            </p>
      
      
            <div>
              <p><span>Matrix matmul(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>Matrix</span><span> </span><span>b</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>out</span><span> = NewMatrix(a.rows, b.rows);</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; a.rows; i++)</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; b.rows; j++)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; a.cols; k++)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>out.dat[i * b.rows + j] += a.dat[i * a.cols + k+k2] * b.dat[(j+j2) * b.cols + k];</span></p>
      
      <p><span><span>&nbsp; </span></span><span>return</span><span> out;</span></p>
      <p><span>}</span></p>
            </div>
            
            <p>
              Fortunately we can make it much faster with just a few bits of intelligence.
              Because of the way memory and caches work on most computers, it's (much!) faster
              to read and write to the same piece of memory over and over.
            </p>
            
            <div>
              <p><span>Matrix matmul_t_fast(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>Matrix</span><span> </span><span>b</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>out</span><span> = NewMatrix(a.rows, b.rows);</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; a.rows; i++)</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; b.rows; j += 4)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; a.cols; k += 4)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k2</span><span> = 0; k2 &lt; 4; k2 += 1)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j2</span><span> = 0; j2 &lt; 4; j2 += 1)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>out.dat[i * b.rows + j+j2] += a.dat[i * a.cols + k+k2] * b.dat[(j+j2) * b.cols + k+k2];</span></p>
      
      <p><span><span>&nbsp; </span></span><span>return</span><span> out;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              Later we're going to make one more change to the way we do inference and add a new
              parameter to the matrix multiply that instead allows us to only multiply part of Matrix A
              by Matrix B, which is useful when we've already pre-computed part of the product.
            </p>
            
            <h3 id="nn">Neural network layers (300 bytes)</h3>
      
            <p>
              In order to write a transformer I'll need to define a few special neural-network specific layers.
              One of these is the <a href="https://arxiv.org/abs/1606.08415">GELU</a> activation function,
              which you can just think of as magic.
            </p>
            <p><span>UNARY(GELU, b / 2 * (1 + tanh(.7978845 * (b + .044715 * b * b * b))))</span></p>
      
            <p>
              I also implement a function to set the lower-diagonal of a matrix
              (after exponentiating the values). This is useful for what's called <i>causal attention</i>:
              we only want to attend to the past, not the future, and so we set
              the lower diagonal of the attention matrix to zero with this function.
            </p>
      
            <p><span>UNARY(tril, (i/k&lt;i%(</span><span>int</span><span>)k) ? 0 : exp(b/8))</span></p>
      
            <p>
              And finally we need a layer normalization function.
              (Again another piece of magic that you can look up if you want.
              Basically what it does is normalize the mean and variance of each layer.)
            </p>
            <div>
              <p><span>Matrix LayerNorm(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>int</span><span> </span><span>i</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>b</span><span> = add(a, divide_const(sum(a), -a.cols));</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>k</span><span> = divide_const(sum(multiply(</span></p>
      <p><span><span>&nbsp; &nbsp; </span>add(NewMatrix(b.rows,b.cols,1),b), b)), b.cols-1);</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>out</span><span> = add_tile(multiply_tile(</span></p>
      <p><span><span>&nbsp; &nbsp; </span>multiply(add(NewMatrix(b.rows,b.cols,1),b),</span></p>
      <p><span><span>&nbsp; &nbsp; </span>mat_isqrt(add_const(k, 1e-5),0)), layer_weights[i+1]),</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>layer_weights[i]);</span></p>
      
      <p><span><span>&nbsp; </span></span><span>return</span><span> out;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              The final piece of the model is the Linear function that
              just performs a matrix multiplication and adds (with tiling) a bias.
            </p>
      
            <p><span>#define</span><span> </span><span>Linear</span><span>(</span><span>a</span><span>, </span><span>i</span><span>) add_tile(matmul_t_fast(a, layer_weights[i+1]), layer_weights[i])</span></p>
      
            <h3 id="gpt">Transformer architecture (600 bytes)</h3>
      
            <p>
              With all of this out of the way, we can finally implement our transformer
              in just 600 bytes. 
              
            </p>
            
            <div>
              <p><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; NLAYER; i++) {</span></p>
      <p><span><span>&nbsp; </span>layer_weights = weights + 12*permute;</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Compute the keys, queries, and values all at once with a big multiply </span><span></span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>qkv</span><span> = transpose(slice(Linear(LayerNorm(line, 4), 0), 0, T*3, DIM));</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Make space for the output of the computation<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>result</span><span> = NewMatrix(DIM, T, 1);</span></p>
      
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; NHEAD; k++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// Split the qkv into each of the heads<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>Matrix</span><span> </span><span>merge</span><span> = transpose(slice(qkv, k*3, 64*T, 3)),</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>// perform the product of the queries and keys and then exponentiate </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>a</span><span> = tril(matmul_t_fast(transpose(slice(merge, 0, 64, T)),</span></p>
      <p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>transpose(slice(merge, T, 64, T))), T),</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>// finally multiply the softmax output (a/sum(a)) with the values matrix </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>out</span><span> = transpose(matmul_t_fast(divide(a, sum(a)), slice(merge, T*2, 64, T)));</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// and copy the output to the proper location in the result matrix </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; </span>memcpy(result.dat+64*T*k, out.dat, 64*T*4);</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Residual connection </span><span></span></p>
      <p><span><span>&nbsp; </span>line = add(line,Linear(transpose(result), 2));</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Activation function and residual connection </span><span></span></p>
      <p><span><span>&nbsp; </span>line = add(line, Linear(GELU(Linear(LayerNorm(line, 6), 8), 0), 10));</span></p>
      <p><span>}</span></p>
      
      <p><span>// Reset layer weights so we can do the last layer norm<span>&nbsp;</span></span><span> </span></p>
      <p><span>layer_weights = weights;</span></p>
      <p><span>line = LayerNorm(line, 12*NLAYER);</span></p>
      
      <p><span>Matrix</span><span> </span><span>result</span><span> = matmul_t_fast(transpose(slice(line, tmp-1, DIM, 1)), wte);</span></p>
      
            </div>
            
            <p>
              Now here's a fact that might not be completely obvious about transformer
              inference: once you've called the model to generate one token, you don't
              actually have to re-compute the entire function to generate the next token.
              In fact, you only need to do a small amount of additional work to generate
              each additional token.
            </p>
      
            <p>
              This is because once you've computed the output of the transformer on the
              output of all the tokens up to the Nth token, you can re-use almost all of
              this output to compute the N+1st token (with a little bit more work.)
            </p>
      
            <p>
              To actually implement this, I make all allocations in my code occur
              sequentially within the same block of memory, to guarantee that each
              matrix multiply will always use exactly the same memory. Then, at each
              iteration of the loop, I can just not zero-out the memory before using
              it for the next iteration, and the memory will already contain the
              results of the previous iteration. I just need to run the computation
              for the N+1st row.
            </p>
            
            
            <h3 id="bpe">Byte pair encoding (400 bytes)</h3>
      
            <p>          
              The simplest way to build a language model is on a sequence of words.
              But because the total number of words is essentially unbounded,
              and language models need to have fixed-size inputs,
              it would be necessary to replace sufficiently rare words with a special
              [OUT OF DISTRIBUTION] token. This is no good.
            </p>
      
            <p>
              While a simple “fix” for this would be to just use character-level
              language models that only know about individual letters, this would
              be a problem because it would mean that the model would have to learn
              the meaning of every word from scratch, and also reduces the effective
              context size of the language model by a factor of the average word length.
            </p>
      
            <p>
              So to fix this, language models like GPT-2 work by creating tokens out
              of "word pieces". Some words might be tokens all by them-self, but
              rare words are broken up into smaller pieces. For example, the word
              “nicholas” might be broken up into “nich” “o” “las”.
            </p>
            
            <p>
              The general algorithm for this is rather easy to implement:
              given a word we want to tokenize, we first split it into individual
              characters. Then, we look for pairs of adjacent tokens that should
              be merged, and merge them together. We repeat this until there are
              no more possible merges.
            </p>
      
            <p>
              This algorithm is simple but unfortunately rather hard to implement
              in C because it requires a bunch of allocations, and requires keeping
              track of a tree-like structure of the tokens.
            </p>
      
            <p>
              So instead, we'll turn the rather simple linear time algorithm into a
              potentially exponential time algorithm but save a bunch of code.
              Our basic idea will work like this in C-like pseudocode:
            </p>
      
            <div>
              <p><span>word_tokenize(word) {</span></p>
      <p><span><span>&nbsp; </span></span><span>if</span><span> len(word) == 0 { </span><span>return</span><span> (0, 0); }</span></p>
      <p><span><span>&nbsp; </span>result = (1e9, -1);</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; VOCAB_LEN; i++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>if</span><span> (is_prefix(bpe[i]), word) {</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>sub_cost = word_tokenize(word+len(bpe[i]))[0] + i + 1e7;</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>result = min(result, (sub_cost, i));</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span></span><span>return</span><span> result;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              That is, to tokenize a word, we try each possible word in the vocabulary
              to see if it's a prefix of the current word. If so, we try to use this
              as the first token, and then recursively try to tokenize the rest of the
              word. We keep track of the best tokenization we've seen so far (as judged
              by the length, breaking ties by the index of the token in the vocab), and
              return that.
            </p>
              
            
            <h3 id="loadweight">Weight loading (300 bytes)</h3>
      
            <p>
              We're almost done! The last thing we need to do is load the actual weights
              of the neural network off disk. This is actually pretty easy, because
              the weights are stored in a simple binary format that's easy to read
              in C: it's just a completely flat serialization of 32-bit floats.
            </p>
      
            <p>
              The only thing we need to know is how big the various matrices are.
              And fortunately, this is also easy to figure out. Each of the GPT-2
              model sizes have the same architecture, and the weights are saved in the
              same order, so all we need to do is read read the correctly-shaped
              matrices off of disk.
            </p>
      
            <p>
              There's one final annoying thing. The layers of the neural network are
              not stored on disk in the order you might expect, with layer 0 first,
              then layer 1, then layer 2. Instead, the first layer is layer 0, then
              layer 1, and then layer .... TEN! (and then layer 11, and then layer 12.)
              This is because weights are stored when sorted lexicographically.
              And lexicographically, “0” comes before “1”, but “10” comes before
              “2”. So we have to do a bit of work to permute the weights into the
              correct order with the following code
            </p>
      
            <div>
              <p><span>int</span><span> permute;</span></p>
      <p><span>tmp=0;</span></p>
      <p><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; 10; j++) {</span></p>
      <p><span><span>&nbsp; </span></span><span>if</span><span> (j == i) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span>permute = tmp;</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>tmp++;</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; 10*(j&gt;0); k++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>if</span><span> (j*10+k &lt; NLAYER &amp;&amp; tmp++ &amp;&amp; i == j*10+k) {</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>permute = tmp;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span>}</span></p>
            </div>
      
            
            <h3 id="loadbpe">Byte pair encoding loading (300 bytes)</h3>
      
            <p>
              In order to actually perform byte-pair encoding, we need to first load
              the byte-pair encoding vocabulary off disk. In an ideal world we'd
              actually just have a list of all the words in the vocabulary stored
              in some reasonable C-readable format, but because the original file
              was (a) meant for reading in Python, and (b) not meant to make it
              easy to parse in as few bytes as possible, we'll have to do some work here.
            </p>
      
            <p>
              You might expect the file format to just be a list of words one after
              the other, but it's actually instead a list of the byte-pair encodings.
              What this means is instead of being able to read “Hello” as one token,
              the line is “H” “ello” which means we should be merging the tokens
              “H” and “ello” into a single token “Hello”.
            </p>
      
            <p>
              The other challenge is that the file is encoded in smoothing-like
              UTF-8 (but not quite exactly that) for ... reasons.
              All of the printable ascii characters are encoded as themselves,
              but the non-printable characters from 0-31 are encoded as the value
              188+the character. So for example, a space is encoded as the token “Ġ”.
              But now the problem is that the UTF8 encoding of “Ġ” is 0xc4 0xa0
              when on disk, and so when reading it we have to do just some ugly work
              to convert this back to a space.
            </p>
      
            <p>
              And while none of this is actually that hard to do, it still requires
              a fair amount of code which is annoying when you're trying to compress
              everything to be small.
            </p>
      
            <div>
      <p><span>unsigned</span><span> </span><span>char</span><span> a[tmp=999],b[tmp];</span></p>
      <p><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; 5e4; i++) {</span></p>
      <p><span><span>&nbsp; </span></span><span>int</span><span> </span><span>k</span><span> = i*tmp;</span></p>
      <p><span><span>&nbsp; </span></span><span>if</span><span> (i &lt; 93) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// The first 92 tokens are just the printable ascii characters </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k] = i + 33;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k+1] = 0;</span></p>
      <p><span><span>&nbsp; </span>} </span><span>else</span><span> </span><span>if</span><span> (i &gt; 254) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// Ones above 254 are from the BPE file. Load those<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; &nbsp; </span>fscanf(fp, </span><span>"%s %s"</span><span>, a, b);</span></p>
      <p><span><span>&nbsp; &nbsp; </span>strcat((</span><span>char</span><span>*)a, (</span><span>char</span><span>*)b);</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>int</span><span> </span><span>j</span><span> = 0;</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; a[i]; i++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// UTF8 encoding makes life hard so handle that here </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>bpe[k+j++] = a[i] ^ 196 ? a[i] : a[++i]-128;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k+j++] = 0;</span></p>
      <p><span><span>&nbsp; </span>} </span><span>else</span><span> </span><span>if</span><span> (i &gt; 187) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// Tokens above 187 are the nonprintable asii character from 0-32<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k] = i-188;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k+1] = 0;</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span>}</span></p>
      </div>        
                        
            
      
            <h2>Conclusion</h2>
            
            <p>
              It's really remarkable how you can distill so many
              decades of progress in machine learning to just a few thousand bytes.
              There is essentially nothing missing here from everything you need to run
              any state-of-the-art neural network (except for the actual model weights).
              While I mostly put this together for fun,
              it's a nice demonstration how <i>simple</i> neural networks actually are.
            </p>
      
            
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[QEMU with VirtIO GPU Vulkan Support (179 pts)]]></title>
            <link>https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f</link>
            <guid>42392802</guid>
            <pubDate>Wed, 11 Dec 2024 20:48:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f">https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f</a>, See on <a href="https://news.ycombinator.com/item?id=42392802">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-qemu-vulkan-virtio-md">
    <article itemprop="text">
<p dir="auto">With its latest reales qemu added the Venus patches so that virtio-gpu now support venus encapsulation for vulkan. This is one more piece to the puzzle towards full Vulkan support.</p>
<p dir="auto">An outdated blog post on <a href="https://www.collabora.com/news-and-blog/blog/2021/11/26/venus-on-qemu-enabling-new-virtual-vulkan-driver/" rel="nofollow">clollabora</a> described in 2021 how to enable 3D acceleration of Vulkan applications in QEMU through the Venus experimental Vulkan driver for VirtIO-GPU with a local development environment. Following up on the outdated write up, this is how its done today.</p>
<p dir="auto"><h2 dir="auto">Definitions</h2><a id="user-content-definitions" aria-label="Permalink: Definitions" href="#definitions"></a></p>
<p dir="auto">Let's start with the brief description of the projects mentioned in the post &amp; extend them:</p>
<ul dir="auto">
<li>QEMU is a machine emulator</li>
<li>VirGL is an OpenGL driver for VirtIO-GPU, available in Mesa.</li>
<li>Venus is an experimental Vulkan driver for VirtIO-GPU, also available in Mesa.</li>
<li>Virglrenderer is a library that enables hardware acceleration to VM guests, effectively translating commands from the two drivers just mentioned to either OpenGL or Vulkan.</li>
<li>libvirt is an API for managing platform virtualization</li>
<li>virt-manager is a desktop user interface for managing virtual machines through libvirt</li>
</ul>
<p dir="auto">Merged Patches:</p>
<ul dir="auto">
<li>2024-08-14 <a href="https://gitlab.freedesktop.org/mesa/mesa/-/commit/087e9a96d13155e26987befae78b6ccbb7ae242b" rel="nofollow">venus: make cross-device optional</a> merged in <a href="https://www.phoronix.com/news/Mesa-24.2-Released" rel="nofollow">mesa 24.2</a></li>
<li>2024-11-25 <a href="https://lore.kernel.org/all/20240726235234.228822-1-seanjc@google.com/" rel="nofollow">KVM: Stop grabbing references to PFNMAP'd pages</a> merged in <a href="https://www.phoronix.com/news/Linux-6.13-KVM" rel="nofollow">linux 6.13</a></li>
<li>2024-11-12 <a href="https://lists.gnu.org/archive/html/qemu-devel/2024-08/msg03288.html" rel="nofollow">Support blob memory and venus on qemu</a> merged in <a href="https://www.phoronix.com/news/QEMU-9.2-Released" rel="nofollow">qemu 9.2.0</a></li>
</ul>
<p dir="auto">Work in progress:</p>
<ul dir="auto">
<li>libvirt <a href="https://gitlab.com/libvirt/libvirt/-/issues/638" rel="nofollow">Add support for more virtio-vga-gl arguments #638</a></li>
<li>virt-manager <a href="https://github.com/virt-manager/virt-manager/issues/362" data-hovercard-type="issue" data-hovercard-url="/virt-manager/virt-manager/issues/362/hovercard">Add support for Venus / Vulkan VirtIO-GPU driver #362</a></li>
</ul>
<p dir="auto"><h2 dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<p dir="auto">Make sure you have the proper version installed on the host:</p>
<ul dir="auto">
<li>linux kernel &gt;= 6.13 built with CONFIG_UDMABUF</li>
<li>working Vulkan and kvm setup</li>
<li>qemu &gt;= 9.2.0</li>
</ul>
<p dir="auto">You can verify this like so:</p>
<pre><code>$ uname -r
6.13.0
$ ls /dev/udmabuf
/dev/udmabuf
$ ls /dev/kvm
/dev/kvm
$ qemu-system-x86_64 --version
QEMU emulator version 9.2.0
Copyright (c) 2003-2024 Fabrice Bellard and the QEMU Project developers
</code></pre>
<p dir="auto">For Vulkan to work you need the proper drivers to be installed for your graphics card. To verfiy your setup, install <code>vulkan-tools</code>:</p>
<pre><code>$ vulkaninfo --summary
==========
VULKANINFO
==========

Vulkan Instance Version: ...
...
$ vkcube
Selected GPU x: ..., type: ...
</code></pre>
<p dir="auto"><h4 dir="auto">Building qemu</h4><a id="user-content-building-qemu" aria-label="Permalink: Building qemu" href="#building-qemu"></a></p>
<p dir="auto">If your distro doesn't (yet) ship and updated version of qemu, you can build it yourself from source:</p>
<pre><code>wget https://download.qemu.org/qemu-9.2.0.tar.xz
tar xvJf qemu-9.2.0.tar.xz
cd qemu-9.2.0
mkdir build &amp;&amp; cd build
../configure --target-list=x86_64-softmmu  \
  --enable-kvm                 \
  --enable-opengl              \
  --enable-virglrenderer       \
  --enable-gtk                 \
  --enable-sdl
make -j4
</code></pre>
<p dir="auto">The configuration step will throgh errors if packages are missing. Check the qemu wiki for further info what to install: <a href="https://wiki.qemu.org/Hosts/Linux" rel="nofollow">https://wiki.qemu.org/Hosts/Linux</a></p>
<p dir="auto"><h2 dir="auto">Create and run an image for QEMU</h2><a id="user-content-create-and-run-an-image-for-qemu" aria-label="Permalink: Create and run an image for QEMU" href="#create-and-run-an-image-for-qemu"></a></p>
<p dir="auto">Create an image &amp; fetch the distro of your choice:</p>
<p dir="auto"><h3 dir="auto">Host</h3><a id="user-content-host" aria-label="Permalink: Host" href="#host"></a></p>
<div dir="auto"><pre>ISO=ubuntu-24.10-desktop-amd64.iso  
wget https://releases.ubuntu.com/oracular/ubuntu-24.10-desktop-amd64.iso  

IMG=ubuntu-24-10.qcow2
qemu-img create -f qcow2 <span>$IMG</span> 16G</pre></div>
<p dir="auto">Run a live version or install the distro</p>
<pre><code>qemu-system-x86_64                                               \
    -enable-kvm                                                  \
    -M q35                                                       \
    -smp 4                                                       \
    -m 4G                                                        \
    -cpu host                                                    \
    -net nic,model=virtio                                        \
    -net user,hostfwd=tcp::2222-:22                              \
    -device virtio-vga-gl,hostmem=4G,blob=true,venus=true        \
    -vga none                                                    \
    -display gtk,gl=on,show-cursor=on                            \
    -usb -device usb-tablet                                      \
    -object memory-backend-memfd,id=mem1,size=4G                 \
    -machine memory-backend=mem1                                 \
    -hda $IMG                                                    \
    -cdrom $ISO                                                  
</code></pre>
<p dir="auto">Adjust the parameters accordingly:</p>
<ul dir="auto">
<li>smp: number of cpu cores</li>
<li>m: RAM</li>
<li>hostmem,size: VRAM</li>
</ul>
<p dir="auto"><h3 dir="auto">Guest</h3><a id="user-content-guest" aria-label="Permalink: Guest" href="#guest"></a></p>
<p dir="auto">Install <code>mesa-utilites</code> and <code>vulkan-tools</code> to test the setup:</p>
<pre><code>$ glxinfo -B
</code></pre>
<pre><code>$ vkcube
Selected GPU x: ..., type: ...
</code></pre>
<p dir="auto">If the deive is <code>llvmpipe</code> somehting is wrong. The device should be <code>virgl (...)</code>.</p>
<p dir="auto"><h4 dir="auto">Troubleshooting</h4><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<ul dir="auto">
<li>(host) add <code>-d guest_errors</code> to show error messages from the guest</li>
<li>(guest) try installing vulkan virtio drivers and mesa</li>
<li>check the original <a href="https://www.collabora.com/news-and-blog/blog/2021/11/26/venus-on-qemu-enabling-new-virtual-vulkan-driver/" rel="nofollow">blog post</a></li>
</ul>
<p dir="auto"><h2 dir="auto">virt-manager</h2><a id="user-content-virt-manager" aria-label="Permalink: virt-manager" href="#virt-manager"></a></p>
<p dir="auto">-- work in progress --</p>
<p dir="auto">Currently this is work in progress, so there is no option to add vulkan support in virt-manager. There are no fields to configure this. Also xml doesnt work, because libvirt doesn't know about these options either, so xml validation fails. There is however an option for <a href="https://libvirt.org/kbase/qemu-passthrough-security.html" rel="nofollow">QEMU command-line passthrough</a> which bypasses the validation.</p>
<p dir="auto">If you setup a default machine with 4G of memory, you can do this:</p>
<div dir="auto"><pre>  &lt;<span>qemu</span><span>:</span><span>commandline</span>&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-device<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>virtio-vga-gl,hostmem=4G,blob=true,venus=true<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-object<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>memory-backend-memfd,id=mem1,size=4G<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-machine<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>memory-backend=mem1<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-vga<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>none<span>"</span></span>/&gt;
  &lt;/<span>qemu</span><span>:</span><span>commandline</span>&gt;</pre></div>
<p dir="auto">Which gives this error:</p>
<pre><code>qemu-system-x86_64: virgl could not be initialized: -1
</code></pre>
<p dir="auto">Changing the number from 4G to 4194304k (same as memory) leds to this error:</p>
<pre><code>qemu-system-x86_64: Spice: ../spice-0.15.2/server/red-qxl.cpp:435:spice_qxl_gl_scanout: condition `qxl_state-&gt;gl_draw_cookie == GL_DRAW_COOKIE_INVALID' failed
</code></pre>
<p dir="auto">to be further investigated.</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Instant macOS install on Proxmox including AMD patches (127 pts)]]></title>
            <link>https://github.com/luchina-gabriel/OSX-PROXMOX</link>
            <guid>42392660</guid>
            <pubDate>Wed, 11 Dec 2024 20:36:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/luchina-gabriel/OSX-PROXMOX">https://github.com/luchina-gabriel/OSX-PROXMOX</a>, See on <a href="https://news.ycombinator.com/item?id=42392660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">OSX-PROXMOX - Run macOS on ANY Computer - AMD &amp; Intel</h2><a id="user-content-osx-proxmox---run-macos-on-any-computer---amd--intel" aria-label="Permalink: OSX-PROXMOX - Run macOS on ANY Computer - AMD &amp; Intel" href="#osx-proxmox---run-macos-on-any-computer---amd--intel"></a></p>
<p dir="auto">Install <code>** FRESH/CLEAN **</code> Proxmox VE v7.0.XX ~ 8.2.XX - Next, Next &amp; Finish (NNF).</p>
<p dir="auto">Open Proxmox Web Console -&gt; Datacenter &gt; NAME OF YOUR HOST &gt; Shell.</p>
<p dir="auto">Copy, paste and execute (code below).</p>
<p dir="auto">Voilà, install macOS! This is really and magic <strong>easiest way</strong>!
<a target="_blank" rel="noopener noreferrer" href="https://github.com/luchina-gabriel/OSX-PROXMOX/blob/main/Artefacts/proxmox-screen.png"><img src="https://github.com/luchina-gabriel/OSX-PROXMOX/raw/main/Artefacts/proxmox-screen.png" alt="overview"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">COPY &amp; PASTE - in shell of Proxmox (for Install or Update this solution)</h2><a id="user-content-copy--paste---in-shell-of-proxmox-for-install-or-update-this-solution" aria-label="Permalink: COPY &amp; PASTE - in shell of Proxmox (for Install or Update this solution)" href="#copy--paste---in-shell-of-proxmox-for-install-or-update-this-solution"></a></p>
<div data-snippet-clipboard-copy-content="/bin/bash -c &quot;$(curl -fsSL https://install.osx-proxmox.com)&quot;"><pre><code>/bin/bash -c "$(curl -fsSL https://install.osx-proxmox.com)"
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">For install EFI Package in macOS, first disable Gatekeeper</h2><a id="user-content-for-install-efi-package-in-macos-first-disable-gatekeeper" aria-label="Permalink: For install EFI Package in macOS, first disable Gatekeeper" href="#for-install-efi-package-in-macos-first-disable-gatekeeper"></a></p>
<div data-snippet-clipboard-copy-content="sudo spctl --master-disable"><pre><code>sudo spctl --master-disable
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Versions of macOS Supported</h2><a id="user-content-versions-of-macos-supported" aria-label="Permalink: Versions of macOS Supported" href="#versions-of-macos-supported"></a></p>
<ul dir="auto">
<li>macOS High Sierra - 10.13</li>
<li>macOS Mojave - 10.14</li>
<li>macOS Catalina - 10.15</li>
<li>macOS Big Sur - 11</li>
<li>macOS Monterey - 12</li>
<li>macOS Ventura - 13</li>
<li>macOS Sonoma - 14</li>
<li>macOS Sequoia - 15</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Versions of Proxmox VE Supported</h2><a id="user-content-versions-of-proxmox-ve-supported" aria-label="Permalink: Versions of Proxmox VE Supported" href="#versions-of-proxmox-ve-supported"></a></p>
<ul dir="auto">
<li>v7.0.XX ~ 8.2.XX</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Opencore version</h2><a id="user-content-opencore-version" aria-label="Permalink: Opencore version" href="#opencore-version"></a></p>
<ul dir="auto">
<li>Oct/2024 - 1.0.2 Added support to macOS Sequoia</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cloud Support (Yes, install your Hackintosh in Cloud Environment)</h2><a id="user-content-cloud-support-yes-install-your-hackintosh-in-cloud-environment" aria-label="Permalink: Cloud Support (Yes, install your Hackintosh in Cloud Environment)" href="#cloud-support-yes-install-your-hackintosh-in-cloud-environment"></a></p>
<ul dir="auto">
<li><a href="https://www.vultr.com/?ref=9035565-8H" rel="nofollow">VultR</a></li>
<li><a href="https://youtu.be/8QsMyL-PNrM" rel="nofollow">Vídeo/Tutorial</a>, please activate captions!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<ul dir="auto">
<li>FOR DEV/STUDENT/TEST ONLY PURPOSES.</li>
<li>I'm not responsible for any problem and/or equipment damage or loss of files.</li>
<li>Always back up everything before any changes to your computer.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<p dir="auto">Since Monterey, your host must have a working TSC (timestamp counter), because otherwise if you give the VM more than one core, macOS will observe the skew between cores and <strong>kernel/memory panic</strong> when it sees time ticking backwards. To check this, on Proxmox run:</p>
<div data-snippet-clipboard-copy-content="dmesg | grep -i -e tsc -e clocksource
...
# for working host must be:
...
clocksource: Switched to clocksource tsc
...

# for broken host could be:
tsc: Marking TSC unstable due to check_tsc_sync_source failed
clocksource: Switched to clocksource hpet"><pre><code>dmesg | grep -i -e tsc -e clocksource
...
# for working host must be:
...
clocksource: Switched to clocksource tsc
...

# for broken host could be:
tsc: Marking TSC unstable due to check_tsc_sync_source failed
clocksource: Switched to clocksource hpet
</code></pre></div>
<p dir="auto">Below is a possible workaround from here: <a href="https://www.nicksherlock.com/2022/10/installing-macos-13-ventura-on-proxmox/comment-page-1/#comment-55532" rel="nofollow">https://www.nicksherlock.com/2022/10/installing-macos-13-ventura-on-proxmox/comment-page-1/#comment-55532</a></p>
<ol dir="auto">
<li>Try to turn off “ErP mode” or any C state power saving modes your BIOS supports and poweroff/poweron device (including physical cable). It could help host OS to init TSC correctly, but no guarantee.</li>
<li>Or try to activate TSC force in GRUB by adding boot flags <code>clocksource=tsc tsc=reliable</code> in the <code>GRUB_CMDLINE_LINUX_DEFAULT</code> and call <code>update-grub</code>. In this case host OS probably could work unstable in some cases.</li>
<li>Check the current TSC by call <code>cat /sys/devices/system/clocksource/clocksource0/current_clocksource</code> must be <code>tsc</code>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">High Siearra and below installation issues</h3><a id="user-content-high-siearra-and-below-installation-issues" aria-label="Permalink: High Siearra and below installation issues" href="#high-siearra-and-below-installation-issues"></a></p>
<p dir="auto">To solve error <em>The Recovery Server Could Not Be Contacted</em> you need to change the protocol from <code>https://</code> to <code>http://</code>. To do this, follow:</p>
<ul dir="auto">
<li>start installation and get error <em>The Recovery Server Could Not Be Contacted</em>, hold the window with error opened</li>
<li>open Window -&gt; Installer Log</li>
<li>search for the line "Failed to load catalog" -&gt; select line in log windows -&gt; Edit -Copy</li>
<li>close the error message and return to <code>macOS Utilities</code> window</li>
<li>open Utilities -&gt; Terminal, right click -&gt; paste</li>
<li>edit the pasted data, remove everything except URL, like <code>https://blablabla.sucatalog</code></li>
<li>change https -&gt; http</li>
<li>adjust the command to be like: nvram IASUCatalogURL=""</li>
<li>press enter, quit Terminal and try to start installation again</li>
</ul>
<p dir="auto">After this, no additional ISO needed, HighSierra must be installed well from recovey.</p>
<p dir="auto">Here a sample how need to change the error message to the final URL:</p>
<p dir="auto"><code>nIUvram IASUCatalogURL="http://swscan.apple.com/content/catalogs/others/index-10.13-10.12-10.11-10.10-10.9-mountainlion-lion-snowleopard-leopard.merged-1.sucatalog"</code></p>
<p dir="auto">The solution took from here: <a href="https://mrmacintosh.com/how-to-fix-the-recovery-server-could-not-be-contacted-error-high-sierra-recovery-is-still-online-but-broken/" rel="nofollow">https://mrmacintosh.com/how-to-fix-the-recovery-server-could-not-be-contacted-error-high-sierra-recovery-is-still-online-but-broken/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demonstration (in Portuguese/Brazil)</h2><a id="user-content-demonstration-in-portuguesebrazil" aria-label="Permalink: Demonstration (in Portuguese/Brazil)" href="#demonstration-in-portuguesebrazil"></a></p>
<p dir="auto"><a href="https://youtu.be/dil6iRWiun0" rel="nofollow">https://youtu.be/dil6iRWiun0</a></p>
<p dir="auto">* Please use CC with Auto Translate to English for your convenience.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li>Opencore/Acidanthera Team</li>
<li>Corpnewt for Applications (ProperTree, genSMBIOS, etc)</li>
<li>Apple for macOS</li>
<li>Proxmox - Excelent and better documentation for Virtualization</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Discord - Universo Hackintosh</h2><a id="user-content-discord---universo-hackintosh" aria-label="Permalink: Discord - Universo Hackintosh" href="#discord---universo-hackintosh"></a></p>
<ul dir="auto">
<li><a href="https://discord.universohackintosh.com.br/" rel="nofollow">Discord</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Guesses Your Accent (178 pts)]]></title>
            <link>https://start.boldvoice.com/accent-guesser</link>
            <guid>42392088</guid>
            <pubDate>Wed, 11 Dec 2024 19:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://start.boldvoice.com/accent-guesser">https://start.boldvoice.com/accent-guesser</a>, See on <a href="https://news.ycombinator.com/item?id=42392088">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><div><p><img src="https://start.boldvoice.com/build/_assets/OracleWaveAudioFuzzy-OOCGQGMB.png" alt="OracleWaveAudioFuzzy" width="100%" height="100%"></p></div><div><p>The <span>Accent</span> Oracle</p><p>Do you have an accent when speaking English? I bet I can guess your native language in less than 30 seconds.</p></div></div><div><p><img src="https://start.boldvoice.com/build/_assets/GlobeLanguages-6JQ4ADYP.svg" alt="Globe Languages"></p></div></div><div><p>© 2024 BoldVoice. All rights reserved.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mysterious New Jersey drone sightings prompt call for 'state of emergency' (298 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency</link>
            <guid>42391443</guid>
            <pubDate>Wed, 11 Dec 2024 19:02:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency">https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency</a>, See on <a href="https://news.ycombinator.com/item?id=42391443">Hacker News</a></p>
Couldn't get https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[2400 phone providers may be shut down by the FCC for failing to stop robocalls (318 pts)]]></title>
            <link>https://docs.fcc.gov/public/attachments/DOC-408083A1.txt</link>
            <guid>42391203</guid>
            <pubDate>Wed, 11 Dec 2024 18:41:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.fcc.gov/public/attachments/DOC-408083A1.txt">https://docs.fcc.gov/public/attachments/DOC-408083A1.txt</a>, See on <a href="https://news.ycombinator.com/item?id=42391203">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[X41 Reviewed Mullvad VPN (321 pts)]]></title>
            <link>https://x41-dsec.de/news/2024/12/11/mullvad/</link>
            <guid>42390768</guid>
            <pubDate>Wed, 11 Dec 2024 18:08:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://x41-dsec.de/news/2024/12/11/mullvad/">https://x41-dsec.de/news/2024/12/11/mullvad/</a>, See on <a href="https://news.ycombinator.com/item?id=42390768">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <h2 id="review-of-mullvad-vpn">Review of Mullvad VPN</h2>

<p>X41 performed a white box penetration test with source code access against the <a href="https://mullvad.net/">Mullvad</a> VPN Application. The efforts included formulating a light threat model.</p>

<p>The targets of this test were challenging for the team because of its size, the fact that they run on five different platforms (Linux, Windows, macOS, Android, and iOS), and the regular audits performed by Mullvad VPN. The fact that new vulnerabilities were found in existing code shows that the efforts taken regularly by Mullvad are justified and appropriate for product of such complexity.</p>

<p>It also shows that in mature targets the findings tend to move into domains not under direct control or in direct focus of the application development as can be seen in the findings rooting from specifics of the operating system’s behavior or the interplay of different network layers and protocols.</p>

<p>This is what keeps security audits and tests of mature and hard targets interesting for the team at X41 as well.</p>

<h2 id="results">Results</h2>

<p>A total of six vulnerabilities were discovered during the test by X41.</p>

<p>Overall, the Mullvad VPN Applications appear to have a high security level and are well positioned to protect from the threat model proposed in our report. The use of safe coding and design patterns in combination with regular audits and penetration tests led to a very hardened environment.</p>

<p>The most serious vulnerabilities are considered to be race conditions and temporal safety violations leading to memory corruption issues in the signal handler code. While exploitation of the signal handler code once triggered seems not unlikely, the fact that an attacker first needs to trigger a signal via another fault reduces the severity of the issues. Other vulnerabilities allow leaking information about the identity of a user by network adjacent attackers and to perform side channel attacks that could in specific circumstances reveal which site a client is currently accessing.</p>

<p>The aspect of side channel attacks is mitigated in most parts, except for protocol level attacks that are not within the control of Mullvad VPN AB because they root from a combination of different technologies such as NAT and modern variants of the HTTP protocol. The introduction of obfuscation technologies and proxy  services within the protected VPN is an option for users with higher security and privacy demands.</p>

<p>In conclusion, the client applications exposed a limited number of relevant vulnerabilities. Mullvad VPN AB addressed them swiftly and the fixes were audited to be working properly.</p>

<p>X41 would like to thank Mullvad VPN AB for the nice collaboration and smooth communication throughout the audit!</p>

<h3 id="findings">Findings</h3>

<p>Mullvad’s <a href="https://mullvad.net/en/blog/the-report-for-the-2024-security-audit-of-the-app-is-now-available">announcement</a> about the audit covers each of the findings and their mitigations. The technical details can be found in our <a href="https://x41-dsec.de/static/reports/X41-Mullvad-Audit-Public-Report-2024-12-10.pdf">report</a>, which we are releasing today.</p>

<h3 id="links">Links</h3>

<p>Full report:<br>
<a href="https://x41-dsec.de/static/reports/X41-Mullvad-Audit-Public-Report-2024-12-10.pdf">https://x41-dsec.de/static/reports/X41-Mullvad-Audit-Public-Report-2024-12-10.pdf</a></p>

<p>Mullvad announcement:<br>
<a href="https://mullvad.net/en/blog/the-report-for-the-2024-security-audit-of-the-app-is-now-available">https://mullvad.net/en/blog/the-report-for-the-2024-security-audit-of-the-app-is-now-available</a></p>

<p>Mullvad’s previous audits:<br>
<a href="https://github.com/mullvad/mullvadvpn-app/tree/main/audits">https://github.com/mullvad/mullvadvpn-app/tree/main/audits</a></p>

<hr>

<p>If you are interested in working with us on such projects in the future, remote or in-office, have a look at our <a href="https://x41-dsec.de/jobs/">jobs</a> page!</p>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Azalea Robotics (YC S24) – Baggage-handling robots for airports (102 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42390761</link>
            <guid>42390761</guid>
            <pubDate>Wed, 11 Dec 2024 18:07:11 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42390761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="42390761">
      <td><span></span></td>      <td><center><a id="up_42390761" href="https://news.ycombinator.com/vote?id=42390761&amp;how=up&amp;goto=item%3Fid%3D42390761"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=42390761">Launch HN: Azalea Robotics (YC S24) – Baggage-handling robots for airports</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_42390761">101 points</span> by <a href="https://news.ycombinator.com/user?id=dmillard">dmillard</a> <span title="2024-12-11T18:07:11 1733940431"><a href="https://news.ycombinator.com/item?id=42390761">13 hours ago</a></span> <span id="unv_42390761"></span> | <a href="https://news.ycombinator.com/hide?id=42390761&amp;goto=item%3Fid%3D42390761">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Azalea%20Robotics%20%28YC%20S24%29%20%E2%80%93%20Baggage-handling%20robots%20for%20airports&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=42390761&amp;auth=e0d246a6e8016d06b58f3062ec36f765256d3508">favorite</a> | <a href="https://news.ycombinator.com/item?id=42390761">61&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN! We’re David and John B, cofounders of Azalea Robotics (<a href="https://www.azalearobotics.com/" rel="nofollow">https://www.azalearobotics.com</a>). We build robots to handle passenger baggage in airports. Here are some videos to give you the idea:</p><p>Unedited autonomous ops: <a href="https://www.youtube.com/watch?v=DuJ3ZORnO1o" rel="nofollow">https://www.youtube.com/watch?v=DuJ3ZORnO1o</a></p><p>Teleoperated (sped up, so no sound): <a href="https://www.youtube.com/watch?v=LeK8NQLnYgA" rel="nofollow">https://www.youtube.com/watch?v=LeK8NQLnYgA</a></p><p>The marketing version: <a href="https://www.youtube.com/watch?v=k0SDPm09U6s" rel="nofollow">https://www.youtube.com/watch?v=k0SDPm09U6s</a></p><p>Robotics is in an interesting place right now, with many warehouse automation companies humming along for almost a decade, and a lot of new effort going to full general purpose hardware with humanoids and software via generalist robotics foundation models. We love these efforts (David used to work on one at Google X with Everyday Robots), but we also see a lot of utility in the current wave of robotics planning and perception tech that can enable new use cases today.</p><p>Airlines in the US compete primarily on efficiency and customer loyalty, and baggage handling hits both (John B. has first-hand experience from working on baggage optimization projects at United Airlines). 2% of flights are delayed by baggage errors, leading to downstream network delays. Baggage handling is also a major complaint in customer experience—almost everyone has a horror story of a missing bag, and sometimes people vow never to fly an airline again for losing their belongings. Furthermore, it’s a really dangerous job for employees from a repetitive stress standpoint. EU regulation is coming to reflect this, protecting workers with a maximum number of bags transferred per shift to alleviate back and tendon injuries that are inherent to this job.</p><p>Unfortunately for airlines, passengers don’t package their luggage in nicely uniform cardboard boxes. If they did, then the airlines could benefit directly from the recent takeoff in manipulator tech for warehouses. But airline luggage is way more wacky and irregular. If robots are going to handle it, they need to reason about how to grasp each item, handle its deformability, stack it in a stable way, and do all of this quickly, safely, and reliably.</p><p>This is what we’re tackling at Azalea. We’re bringing our expertise in deformable object manipulation, perception, robot learning, and planning, to this logistical problem.</p><p>We have a few strong bets behind what we’re working on: (1) The hardware to solve this problem has been available or manufacturable for decades, what’s been missing is perception, planning, and control. (2) Cobots, robots designed to operate alongside humans, aren’t enough for safety. To do this task efficiently, you need to move up to 50 kg bags very quickly, which can be dangerous no matter how well the cobots are designed. Light curtains (arrays of lasers that stop a machine when interrupted) and machine cages are the current industrial standard and remain the way to go. (3) Software for generalist robots needs more data than most people today believe, and it will be at least 15 years before deployment: we should focus on specialized problems of economic value.</p><p>Our core technical developments are in a few areas:</p><p>- Grasp synthesis and selection: From visual data only, how can we identify good candidate grasp points and rank them? For this, we use a mix of physical reasoning, heuristics, and a lot of learning from previous data, combined in a single objective function. Furthermore, success must be evaluated as both a successful grasp and continual hold throughout the transfer.</p><p>- Placement planning: How do we lay out luggage in the module we’re loading? There’s a nice ramp-up in difficulty for this problem, from open-loop “divide the world into a grid” approaches, to 3d bin-packing optimization, to reinforcement learning. An interesting aspect of this problem for us is that the bags should be physically stable when the cart starts driving, and lighter, deformable objects shouldn’t be underneath heavy, hard objects. We use a similar mix of physics and learning to model this problem.</p><p>- Fast collision-free planning: Off the shelf planners work great for the most part but can fail in heavily cluttered areas or dynamic scenes. We leverage the fact that we’re always solving a series of similar problems to provide initial guesses for downstream trajectory optimization algorithms. Since each problem is so similar, we can use techniques similar to generative models to propose these initial plans.</p><p>- Mechanical design: The perfect tool to pick up everything checked down a conveyor belt isn’t an easy thing to design. We’re building tools with multiple modes of grasping to handle wide varieties of objects. The videos we linked to are all with suction only – which can be surprisingly powerful! An interesting aspect of autonomy becomes choosing which mode to use when, and how to use it.</p><p>These problems can be deeply interlinked: where you grasp an object depends on what your tooling looks like and informs where you can put it– so a perfect solution would jointly reason about both problems simultaneously. We’re looking forward to getting there as we collect more data and continue our efforts.</p><p>Check out our demo videos above! We have a brand new hardware stack coming soon (and we’ve added a new end effector that we’re keeping hush), but it’s amazing what you can do with pure suction.</p><p>We’re proud of our progress so far but would love to hear your thoughts and feedback. Let us know if you’ve had a particularly bad baggage horror story and/or have personal experience with the industry.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WordPress CEO quits community Slack after court injunction (133 pts)]]></title>
            <link>https://www.404media.co/wordpress-wp-engine-preliminary-injunction/</link>
            <guid>42390709</guid>
            <pubDate>Wed, 11 Dec 2024 18:02:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/wordpress-wp-engine-preliminary-injunction/">https://www.404media.co/wordpress-wp-engine-preliminary-injunction/</a>, See on <a href="https://news.ycombinator.com/item?id=42390709">Hacker News</a></p>
Couldn't get https://www.404media.co/wordpress-wp-engine-preliminary-injunction/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[FCC Opens Entire 6 GHz Band to Low Power Device Operations (565 pts)]]></title>
            <link>https://www.fcc.gov/document/fcc-opens-entire-6-ghz-band-very-low-power-device-operations</link>
            <guid>42390344</guid>
            <pubDate>Wed, 11 Dec 2024 17:35:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fcc.gov/document/fcc-opens-entire-6-ghz-band-very-low-power-device-operations">https://www.fcc.gov/document/fcc-opens-entire-6-ghz-band-very-low-power-device-operations</a>, See on <a href="https://news.ycombinator.com/item?id=42390344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-off-canvas-main-canvas="">
        
				
	
        <header>
  


<nav aria-labelledby="main-navigation">
  <div id="main-navigation">

          <div>
          
  <p><a href="https://www.fcc.gov/" aria-label="" id="logo">
            <img src="https://www.fcc.gov/themes/custom/fcc/logo.svg" width="175" height="auto" alt="Federal Communications Commission logo">

        
  </a>

  
  </p>


      </div>
    
          <div aria-label="Main Navigation">
        <nav aria-labelledby="browse-by">
          <div role="tablist" id="mainNavbar">
              <ul id="browse-by" role="tablist">
                <li>
                  <a id="nav-category-tab" data-bs-toggle="tab" href="#nav-category" role="tab" aria-controls="nav-category" aria-selected="true" aria-haspopup="true" aria-expanded="true">
                    <p>Browse by</p>
                    <p>category</p>
                    
                  </a>
                </li>
                <li>
                  <a id="nav-bureaus-and-offices-tab" data-bs-toggle="tab" href="#nav-bureaus-and-offices" role="tab" aria-controls="nav-bureaus-and-offices" aria-selected="false" aria-haspopup="true" aria-expanded="false">
                    <p>Browse by</p>
                    <p>bureaus &amp; offices</p>
                    
                  </a>
                </li>
              </ul>
              </div>
        </nav>
      </div>
    
                  
          
  </div>
</nav>
</header>
  
        
  

  <main role="main">
    <div>
          <article>
  
  

    <div>
      <div>
        <ul>
                      <li>
  <div>
    <p>
      Full Title<span>:</span>    </p>
                  <p>FCC Opens Entire 6 GHz Band To Very Low Power Device Operations</p>
              </div>
</li>
                                <li>
        
      
  </li>
                                <li>
  
</li>
                                <li>
  <div>
    <p>
      Description    </p>
                  <p>FCC Opens Entire 6 GHz Band To Very Low Power Device Operations</p>
              </div>
</li>
                                                                                                    </ul>

        
                  <div>
  <h3>Files</h3>
    
  

  </div>
              </div>
      <div>
            <h4>Document Dates</h4>
            <ul>
                              <li>
  <div>
    <p>
      Released On<span>:</span>    </p>
                  <p><time datetime="2024-12-11T12:00:00Z">Dec 11, 2024</time>
</p>
              </div>
</li>
                                                          <li>
  <div>
    <p>
      Adopted Date<span>:</span>    </p>
                  <p><time datetime="2024-12-11T12:00:00Z">Dec 11, 2024</time>
</p>
              </div>
</li>
                                            <li>
  
</li>
                                                                                                </ul>
          </div>
    </div>
</article>


      </div>
  </main>

        
  
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Estimated concentrations of atrazine in agricultural groundwater (140 pts)]]></title>
            <link>https://water.usgs.gov/nawqa/pnsp/features/feature.php</link>
            <guid>42390236</guid>
            <pubDate>Wed, 11 Dec 2024 17:25:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://water.usgs.gov/nawqa/pnsp/features/feature.php">https://water.usgs.gov/nawqa/pnsp/features/feature.php</a>, See on <a href="https://news.ycombinator.com/item?id=42390236">Hacker News</a></p>
Couldn't get https://water.usgs.gov/nawqa/pnsp/features/feature.php: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[OnlyFans models are using AI impersonators to keep up with their DMs (225 pts)]]></title>
            <link>https://www.wired.com/story/onlyfans-models-are-using-ai-impersonators-to-keep-up-with-their-dms/</link>
            <guid>42390210</guid>
            <pubDate>Wed, 11 Dec 2024 17:23:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/onlyfans-models-are-using-ai-impersonators-to-keep-up-with-their-dms/">https://www.wired.com/story/onlyfans-models-are-using-ai-impersonators-to-keep-up-with-their-dms/</a>, See on <a href="https://news.ycombinator.com/item?id=42390210">Hacker News</a></p>
Couldn't get https://www.wired.com/story/onlyfans-models-are-using-ai-impersonators-to-keep-up-with-their-dms/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[RISC-V HiFive Premier P550 Development Boards with Ubuntu Now Available (142 pts)]]></title>
            <link>https://www.sifive.com/blog/hifive-premier-p550-development-boards-with-ubuntu</link>
            <guid>42389532</guid>
            <pubDate>Wed, 11 Dec 2024 16:33:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sifive.com/blog/hifive-premier-p550-development-boards-with-ubuntu">https://www.sifive.com/blog/hifive-premier-p550-development-boards-with-ubuntu</a>, See on <a href="https://news.ycombinator.com/item?id=42389532">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>With strong initial reviews, the  <a href="https://www.sifive.com/boards/hifive-premier-p550">HiFive Premier P550 Development Boards</a> with pre-installed Ubuntu are in stock and available for purchase at <a href="https://www.arrow.com/en/products/hf106-001/sifive-inc" target="_blank" rel="noopener">Arrow.com</a>. First launched in October, the Yocto-based Early Access edition of the HiFive Premier P550 sold out rapidly as developers eagerly embraced it to test their RISC-V designs on real silicon. Today, we’re excited to announce the general availability of the Premier P550 in both <a href="https://www.arrow.com/en/products/hf106/sifive-inc" target="_blank" rel="noopener">16GB</a> and <a href="https://www.arrow.com/en/products/hf106-001/sifive-inc" target="_blank" rel="noopener">32GB</a> configurations. This release offers even more flexibility and power, featuring pre-installed Ubuntu 24.04 to help developers make the most of their RISC-V projects.</p>
<p><strong>Lower Price, Greater Accessibility</strong>
Due to the boards' initial popularity, we worked closely with our manufacturing partner, ESWIN to ramp up production. As a result of increased production and economies of scale, we’re excited to announce we are able to lower the price to just $399 for the 16GB version and $499 for the 32GB version, making this powerful development board more accessible to a wider range of developers, enthusiasts, and designers.</p>
<p>For those who acted quickly and purchased a board before the price drop, we want to thank you for your support. We’ll be issuing a refund for the price difference to show our appreciation. You will receive an email from Arrow explaining next steps.</p>
<p><strong>Rave Reviews and Feedback</strong>
Early feedback from users has been overwhelmingly positive. It’s been exciting to see how developers and build farms are leveraging the board’s capabilities—from testing software to running video games—pushing the board to its limits. We’d love to hear your thoughts as well. Feel free to send your feedback to HighFiveboards@sifive.com, or share your project with us on social media. Tag us or send in a video to showcase how you're using the HiFive Premier P550.</p>
<p>As we mentioned during the product launch, our goal is to get these boards into the hands of as many developers as possible to help accelerate the growth of the RISC-V ecosystem. Early users have praised the board’s smooth out-of-the-box experience, thorough testing and certification, and premium features. The boards are built with high quality components including powerful Samsung and Micron LPDDR memory, System on Module (SOM) for modularity and upgradeability, and an onboard Baseboard Management Controller ( BMC) offering remote management and control without having physical access to the board, and much more.</p>
<p><strong>Built on Strong Engineering Collaboration</strong>
The HiFive Premier P550 Development Board’s uniqueness stems from the close collaboration between SiFive, ESWIN, and Canonical to ensure the board performs to specification. This strong partnership is helping drive the RISC-V ecosystem forward, and we’re thrilled to see these boards gaining traction.</p>
<p>“We’re excited to see these boards rapidly proliferate into the market,” said Bo Wang, Vice Chairman of <a href="https://www.eswincomputing.com/en/" target="_blank" rel="noopener">ESWIN Computing</a>. “We are pleased to be able to collaborate with SiFive, the industry leader in RISC-V, and we look forward to future products.”</p>
<p>“<a href="https://canonical.com/" target="_blank" rel="noopener">Canonical</a> is deeply committed to RISC-V and creating the best software environment possible for developers coming into this ecosystem. The HiFive Premier P550 is set to be the de facto development platform, and with Ubuntu coming pre-installed on the board we are excited to see the platform used by innovators and developers,” said Gordan Markus, Silicon Alliances Director, Canonical.</p>
<p><strong>A Strategic Investment in RISC-V’s Future</strong>
These boards represent a long-term investment in accelerating the global adoption of RISC-V  and are not intended as a revenue driver.  As the RISC-V ecosystem grows at an exponential pace, having access to actual silicon for hands-on development is essential. We’re committed to supporting this growth, and you can expect many more boards to follow in the months ahead.</p>
<p>We believe this investment benefits you too. With the new, lower price, Ubuntu support, and availability through Arrow.com, there’s never been a better time to dive into RISC-V development. Whether you're building a prototype, exploring new software, or experimenting with hardware, the HiFive Premier P550 offers an unparalleled platform. And with the confidence that it’s built on SiFive’s proven IP, you can be sure you’re working with the best.</p>
<p>We look forward to hearing how you’re using these boards and can’t wait to share more exciting stories from the growing RISC-V community.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pgroll – Zero-downtime, reversible, schema changes for PostgreSQL (new website) (206 pts)]]></title>
            <link>https://pgroll.com/</link>
            <guid>42388973</guid>
            <pubDate>Wed, 11 Dec 2024 15:51:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pgroll.com/">https://pgroll.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42388973">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Trillium TPU Is GA (145 pts)]]></title>
            <link>https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga</link>
            <guid>42388901</guid>
            <pubDate>Wed, 11 Dec 2024 15:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga">https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga</a>, See on <a href="https://news.ycombinator.com/item?id=42388901">Hacker News</a></p>
Couldn't get https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Dear OAuth Providers (224 pts)]]></title>
            <link>https://pilcrowonpaper.com/blog/dear-oauth-providers/</link>
            <guid>42388870</guid>
            <pubDate>Wed, 11 Dec 2024 15:42:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pilcrowonpaper.com/blog/dear-oauth-providers/">https://pilcrowonpaper.com/blog/dear-oauth-providers/</a>, See on <a href="https://news.ycombinator.com/item?id=42388870">Hacker News</a></p>
Couldn't get https://pilcrowonpaper.com/blog/dear-oauth-providers/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini 2.0: our new AI model for the agentic era (741 pts)]]></title>
            <link>https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/</link>
            <guid>42388783</guid>
            <pubDate>Wed, 11 Dec 2024 15:33:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/">https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=42388783">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
  }">
  
  <div>
          
            <p>Dec 11, 2024</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
  
  <div data-summary-id="ai_summary_2" data-component="uni-ai-generated-summary" data-analytics-module="{
    &quot;event&quot;: &quot;module_impression&quot;,
    &quot;module_name&quot;: &quot;ai_summary&quot;,
    &quot;section_header&quot;: &quot;CTA&quot;
  }">
          <h2>Bullet points</h2>
          <ul>
<li>Google DeepMind introduces Gemini 2.0, a new AI model designed for the "agentic era."</li>
<li>Gemini 2.0 is more capable than previous versions, with native image and audio output and tool use.</li>
<li>Gemini 2.0 Flash is available to developers and trusted testers, with wider availability planned for early next year.</li>
<li>Google is exploring agentic experiences with Gemini 2.0, including Project Astra, Project Mariner, and Jules.</li>
<li>Google is committed to building AI responsibly, with safety and security as key priorities.</li>
</ul>
          
          <p><small>
            Summaries were generated by Google AI. Generative AI is experimental.
          </small>
        </p></div>
</div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1200.format-webp.webp" fetchpriority="high" alt="Text &quot;Gemini 2.0&quot; in front of a futuristic blue and black abstract background">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to A message from our CEO" href="#ceo-message" id="ceo-message-anchor">A message from our CEO</a>
        </li>
        
        <li>
          <a aria-label="link to Introducing Gemini 2.0" href="#gemini-2-0" id="gemini-2-0-anchor">Introducing Gemini 2.0</a>
        </li>
        
        <li>
          <a aria-label="link to Gemini 2.0 Flash" href="#gemini-2-0-flash" id="gemini-2-0-flash-anchor">Gemini 2.0 Flash</a>
        </li>
        
        <li>
          <a aria-label="link to Project Astra" href="#project-astra" id="project-astra-anchor">Project Astra</a>
        </li>
        
        <li>
          <a aria-label="link to Project Mariner" href="#project-mariner" id="project-mariner-anchor">Project Mariner</a>
        </li>
        
        <li>
          <a aria-label="link to Agents for developers" href="#agents-for-developers" id="agents-for-developers-anchor">Agents for developers</a>
        </li>
        
        <li>
          <a aria-label="link to Agents in games" href="#ai-game-agents" id="ai-game-agents-anchor">Agents in games</a>
        </li>
        
        <li>
          <a aria-label="link to Building responsibly" href="#building-responsibly" id="building-responsibly-anchor">Building responsibly</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
              


<google-read-aloud-player data-analytics-module="{
        &quot;event&quot;: &quot;module_impression&quot;,
        &quot;module_name&quot;: &quot;ai_audio&quot;,
        &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
    }" data-date-modified="2024-12-11T15:30:20.983616+00:00" data-progress-bar-style="half-wave" data-api-key="AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac" data-article-style="style9" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-layout-style="style1" data-highlight-mode="word-over-paragraph" data-highlight-text-color="#000000" data-highlight-word-background="#8AB4F8" data-highlight-paragraph-background="#D2E3FC" data-background="linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)" data-foreground-color="#202124" data-font="600 16px Google Sans, sans-serif" data-box-shadow="0px 1px 3px 1px rgba(60, 64, 67, 0.15)">
</google-read-aloud-player>




            

            
            
<!--article text-->

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="99isd"><b>A note from Google and Alphabet CEO Sundar Pichai:</b></p><p data-block-key="efphd">Information is at the core of human progress. It’s why we’ve focused for more than 26 years on our mission to organize the world’s information and make it accessible and useful. And it’s why we continue to push the frontiers of AI to organize that information across every input and make it accessible via any output, so that it can be truly useful for you.</p><p data-block-key="4oebp">That was our vision when <a href="https://blog.google/technology/ai/google-gemini-ai/">we introduced Gemini 1.0 last December</a>. The first model built to be natively multimodal, Gemini 1.0 and 1.5 drove big advances with multimodality and long context to understand information across text, video, images, audio and code, and process a lot more of it.</p><p data-block-key="2huik">Now millions of developers are building with Gemini. And it’s helping us reimagine all of our products — including all 7 of them with 2 billion users — and to create new ones. <a href="https://notebooklm.google/">NotebookLM</a> is a great example of what multimodality and long context can enable for people, and why it’s loved by so many.</p><p data-block-key="60rf2">Over the last year, we have been investing in developing more agentic models, meaning they can understand more about the world around you, think multiple steps ahead, and take action on your behalf, with your supervision.</p><p data-block-key="ejiii">Today we’re excited to launch our next era of models built for this new agentic era: introducing Gemini 2.0, our most capable model yet. With new advances in multimodality — like native image and audio output — and native tool use, it will enable us to build new AI agents that bring us closer to our vision of a universal assistant.</p><p data-block-key="bh7ok">We’re getting 2.0 into the hands of developers and trusted testers today. And we’re working quickly to get it into our products, leading with Gemini and Search. Starting today our Gemini 2.0 Flash experimental model will be available to all Gemini users. We're also launching a new feature called <a href="https://blog.google/products/gemini/google-gemini-deep-research/">Deep Research</a>, which uses advanced reasoning and long context capabilities to act as a research assistant, exploring complex topics and compiling reports on your behalf. It's available in Gemini Advanced today.</p><p data-block-key="eqmfh">No product has been transformed more by AI than Search. Our AI Overviews now reach 1 billion people, enabling them to ask entirely new types of questions — quickly becoming one of our most popular Search features ever. As a next step, we’re bringing the advanced reasoning capabilities of Gemini 2.0 to AI Overviews to tackle more complex topics and multi-step questions, including advanced math equations, multimodal queries and coding. We started limited testing this week and will be rolling it out more broadly early next year. And we’ll continue to bring AI Overviews to more countries and languages over the next year.</p><p data-block-key="aaa8b">2.0’s advances are underpinned by decade-long investments in our differentiated full-stack approach to AI innovation. It’s built on custom hardware like Trillium, our sixth-generation TPUs. TPUs powered 100% of Gemini 2.0 training and inference, and today Trillium is <a href="https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga">generally available</a> to customers so they can build with it too.</p><div data-block-key="7gbvh"><p>If Gemini 1.0 was about organizing and understanding information, Gemini 2.0 is about making it much more useful. I can’t wait to see what this next era brings.</p><p>-Sundar</p></div><hr></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="99isd">Introducing Gemini 2.0: our new AI model for the agentic era</h2><p data-block-key="7o7kh"><i>By Demis Hassabis, CEO of Google DeepMind and Koray Kavukcuoglu, CTO of Google DeepMind on behalf of the Gemini team</i></p><p data-block-key="ecj6n">Over the past year, we have continued to make incredible progress in artificial intelligence. Today, we are releasing the first model in the Gemini 2.0 family of models: an experimental version of Gemini 2.0 Flash. It’s our workhorse model with low latency and enhanced performance at the cutting edge of our technology, at scale.</p><p data-block-key="6o7oe">We are also sharing the frontiers of our agentic research by showcasing prototypes enabled by Gemini 2.0’s native multimodal capabilities.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="99isd">Gemini 2.0 Flash</h2><p data-block-key="7lrrf">Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
          }">
    

    <p><img alt="A chart comparing Gemini models and their capabilities" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_benchmarks_narrow_light2x.gif">
          
        
      
      </p>
      
    
      </div>
    
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="99isd">Our goal is to get our models into people’s hands safely and quickly. Over the past month, we’ve been sharing early, experimental versions of Gemini 2.0, getting great feedback from developers.</p><p data-block-key="964c">Gemini 2.0 Flash is available now as an experimental model to developers via the Gemini API in <a href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-exp">Google AI Studio</a> and <a href="https://cloud.google.com/generative-ai-studio">Vertex AI</a> with multimodal input and text output available to all developers, and text-to-speech and native image generation available to early-access partners. General availability will follow in January, along with more model sizes.</p><p data-block-key="a4e0l">To help developers build dynamic and interactive applications, we’re also releasing a new Multimodal Live API that has real-time audio, video-streaming input and the ability to use multiple, combined tools. More information about 2.0 Flash and the Multimodal Live API can be found in our <a href="https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/">developer blog</a>.</p><h3 data-block-key="dsvak">Gemini 2.0 available in Gemini app, our AI assistant</h3><p data-block-key="en32v">Also starting today, <a href="https://gemini.google.com/">Gemini</a> users globally can access a chat optimized version of 2.0 Flash experimental by selecting it in the model drop-down on desktop and mobile web and it will be available in the Gemini mobile app soon. With this new model, users can experience an even more helpful Gemini assistant.</p><p data-block-key="ea033">Early next year, we’ll expand Gemini 2.0 to more Google products.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="ju7ol">Unlocking agentic experiences with Gemini 2.0</h2><p data-block-key="78rfe">Gemini 2.0 Flash’s native user interface action-capabilities, along with other improvements like multimodal reasoning, long context understanding, complex instruction following and planning, compositional function-calling, native tool use and improved latency, all work in concert to enable a new class of agentic experiences.</p><p data-block-key="3k97r">The practical application of AI agents is a research area full of exciting possibilities. We’re exploring this new frontier with a series of prototypes that can help people accomplish tasks and get things done. These include an update to Project Astra, our research prototype exploring future capabilities of a universal AI assistant; the new Project Mariner, which explores the future of human-agent interaction, starting with your browser; and Jules, an AI-powered code agent that can help developers.</p><p data-block-key="c8gij">We’re still in the early stages of development, but we’re excited to see how trusted testers use these new capabilities and what lessons we can learn, so we can make them more widely available in products in the future.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="Fs0t6SdODd8" data-index-id="10" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Gemini 2.0 supercut video" src="https://i.ytimg.com/vi_webp/Fs0t6SdODd8/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/Fs0t6SdODd8/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/Fs0t6SdODd8/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="ju7ol">Project Astra: agents using multimodal understanding in the real world</h2><p data-block-key="fdk7b">Since we introduced <a href="https://deepmind.google/technologies/gemini/project-astra/">Project Astra</a> at I/O, we’ve been learning from trusted testers using it on Android phones. Their valuable feedback has helped us better understand how a universal AI assistant could work in practice, including implications for safety and ethics. Improvements in the latest version built with Gemini 2.0 include:</p><ul><li data-block-key="4arno"><b>Better dialogue:</b> Project Astra now has the ability to converse in multiple languages and in mixed languages, with a better understanding of accents and uncommon words.</li><li data-block-key="1f3oh"><b>New tool use:</b> With Gemini 2.0, Project Astra can use Google Search, Lens and Maps, making it more useful as an assistant in your everyday life.</li><li data-block-key="9f826"><b>Better memory:</b> We’ve improved Project Astra’s ability to remember things while keeping you in control. It now has up to 10 minutes of in-session memory and can remember more conversations you had with it in the past, so it is better personalized to you.</li><li data-block-key="68bh5"><b>Improved latency:</b> With new streaming capabilities and native audio understanding, the agent can understand language at about the latency of human conversation.</li></ul><p data-block-key="4qful">We’re working to bring these types of capabilities to Google products like <a href="http://gemini.google.com/">Gemini</a> app, our AI assistant, and to other form factors like glasses. And we’re starting to expand our trusted tester program to more people, including a small group that will soon begin testing Project Astra on prototype glasses.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="hIIlJt8JERI" data-index-id="13" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Project Astra demo video" src="https://i.ytimg.com/vi_webp/hIIlJt8JERI/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/hIIlJt8JERI/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/hIIlJt8JERI/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Project Mariner: agents that can help you accomplish complex tasks</h2><p data-block-key="b0b0b">Project Mariner is an early research prototype built with Gemini 2.0 that explores the future of human-agent interaction, starting with your browser. As a research prototype, it’s able to understand and reason across information in your browser screen, including pixels and web elements like text, code, images and forms, and then uses that information via an experimental Chrome extension to complete tasks for you.</p><p data-block-key="d0nh0">When evaluated against the <a href="https://arxiv.org/abs/2401.13919">WebVoyager benchmark</a>, which tests agent performance on end-to-end real world web tasks, Project Mariner <a href="http://deepmind.google/technologies/project-mariner">achieved a state-of-the-art result of 83.5%</a> working as a single agent setup.</p><p data-block-key="8mvv5">It’s still early, but Project Mariner shows that it’s becoming technically possible to navigate within a browser, even though it’s not always accurate and slow to complete tasks today, which will improve rapidly over time.</p><p data-block-key="2n0oq">To build this safely and responsibly, we’re conducting active research on new types of risks and mitigations, while keeping humans in the loop. For example, Project Mariner can only type, scroll or click in the active tab on your browser and it asks users for final confirmation before taking certain sensitive actions, like purchasing something.</p><p data-block-key="ch96g">Trusted testers are starting to test Project Mariner using an experimental Chrome extension now, and we’re beginning conversations with the web ecosystem in parallel.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="2XJqLPqHtyo" data-index-id="16" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Mariner demo video" src="https://i.ytimg.com/vi_webp/2XJqLPqHtyo/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/2XJqLPqHtyo/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/2XJqLPqHtyo/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Jules: agents for developers</h2><p data-block-key="216k6">Next, we’re exploring how AI agents can assist developers with Jules — an experimental AI-powered code agent that integrates directly into a GitHub workflow. It can tackle an issue, develop a plan and execute it, all under a developer’s direction and supervision. This effort is part of our long-term goal of building AI agents that are helpful in all domains, including coding.</p><p data-block-key="acjnm">More information about this ongoing experiment can be found in our <a href="https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/">developer blog post</a>.</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
          }">
    

    <p>

        
        
          
            <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Jules_GIF_3D_10_1.mp4" type="video/mp4" title="Animation of Jules coding assistant" alt="Jules">
              Video format not supported
            </video>
          
        
      
      </p>
      
    
      </div>
    
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Agents in games and other domains</h2><p data-block-key="384g8">Google DeepMind has a <a href="https://deepmind.google/discover/blog/agent57-outperforming-the-human-atari-benchmark/">long</a> <a href="https://deepmind.google/research/breakthroughs/alphago/">history</a> of using games to help AI models become better at following rules, planning and logic. Just last week, for example, we introduced <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2</a>, our AI model that can create an endless variety of playable 3D worlds — all from a single image. Building on this tradition, we’ve built agents using Gemini 2.0 that can help you navigate the virtual world of video games. It can reason about the game based solely on the action on the screen, and offer up suggestions for what to do next in real time conversation.</p><p data-block-key="b3sa9">We're collaborating with leading game developers like Supercell to explore how these agents work, testing their ability to interpret rules and challenges across a diverse range of games, from strategy titles like “Clash of Clans” to farming simulators like “Hay Day.”</p><p data-block-key="2p0a">Beyond acting as virtual gaming companions, these agents can even tap into Google Search to connect you with the wealth of gaming knowledge on the web.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="IKuGNHJBGsc" data-index-id="22" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Navi demo video" src="https://i.ytimg.com/vi_webp/IKuGNHJBGsc/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/IKuGNHJBGsc/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/IKuGNHJBGsc/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="of3wa">In addition to exploring agentic capabilities in the virtual world, we’re experimenting with agents that can help in the physical world by applying Gemini 2.0's spatial reasoning capabilities to robotics. While it’s still early, we’re excited about the potential of agents that can assist in the physical environment.</p><p data-block-key="ba0kt">You can learn more about these research prototypes and experiments at <a href="http://labs.google/">labs.google</a>.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Building responsibly in the agentic era</h2><p data-block-key="6j36d">Gemini 2.0 Flash and our research prototypes allow us to test and iterate on new capabilities at the forefront of AI research that will eventually make Google products more helpful.</p><p data-block-key="fvsj8">As we develop these new technologies, we recognize the responsibility it entails, and the many questions AI agents open up for safety and security. That is why we are taking an exploratory and gradual approach to development, conducting research on multiple prototypes, iteratively implementing safety training, working with trusted testers and external experts and performing extensive risk assessments and safety and assurance evaluations.</p><p data-block-key="angf4">For example:</p><ul><li data-block-key="crrpj">As part of our safety process, we’ve worked with our Responsibility and Safety Committee (RSC), our longstanding internal review group, to identify and understand potential risks.</li><li data-block-key="2uku6">Gemini 2.0's reasoning capabilities have enabled major advancements in our AI-assisted red teaming approach, including the ability to go beyond simply detecting risks to now automatically generating evaluations and training data to mitigate them. This means we can more efficiently optimize the model for safety at scale.</li><li data-block-key="81t7m">As Gemini 2.0’s multimodality increases the complexity of potential outputs, we’ll continue to evaluate and train the model across image and audio input and output to help improve safety.</li><li data-block-key="clba0">With Project Astra, we’re exploring potential mitigations against users unintentionally sharing sensitive information with the agent, and we’ve already built in privacy controls that make it easy for users to delete sessions. We’re also continuing to research ways to ensure AI agents act as reliable sources of information and don’t take unintended actions on your behalf.</li><li data-block-key="7r7pa">With Project Mariner, we’re working to ensure the model learns to prioritize user instructions over 3rd party attempts at prompt injection, so it can identify potentially malicious instructions from external sources and prevent misuse. This prevents users from being exposed to fraud and phishing attempts through things like malicious instructions hidden in emails, documents or websites.</li></ul><p data-block-key="f9e42">We firmly believe that the only way to build AI is to be responsible from the start and we'll continue to prioritize making safety and responsibility a key element of our model development process as we advance our models and agents.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Gemini 2.0, AI agents and beyond</h2><p data-block-key="35lej">Today’s releases mark a new chapter for our Gemini model. With the release of Gemini 2.0 Flash, and the series of research prototypes exploring agentic possibilities, we have reached an exciting milestone in the Gemini era. And we’re looking forward to continuing to safely explore all the new possibilities within reach as we build towards AGI.</p></div>
  

  
    














<uni-related-content-tout title="Gemini 2.0: Our latest, most capable AI model yet" cta="See more" summary="See how Gemini 2.0 and our research prototypes work — and how they’ll help make our Google products more helpful." hideimage="False" eyebrow="Collection" fullurl="https://blog.google/products/gemini/google-gemini-ai-collection-2024/" pagetype="collectiondetailpage" isarticlepage="">
  
    <div slot="rct-image-slot">
      
    <figure>
        <picture>
            


    

    
        <source media="(max-resolution: 1.5dppx)" sizes="300px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-300.format-webp.webp 300w">
    
        <source media="(min-resolution: 1.5dppx)" sizes="600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp 600w">
    

    <img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp" alt="gemini social share collection" sizes=" 300px,  600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp 600w" data-target="image" loading="lazy">
    


        </picture>
    </figure>


    </div>
  
</uni-related-content-tout>

  

  
    

  
    





<div role="form" aria-label="Sign up to receive weekly news and stories from Google." data-component="uni-subscribe" data-analytics-module="{
    &quot;module_name&quot;: &quot;Newsletter&quot;,
    &quot;section_header&quot;: &quot;Get more stories from Google in your inbox.&quot;
  }">
        
        
        <p>You are already subscribed to our newsletter.</p>
      </div>

  

  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dioxus 0.6 – Crossplatform apps with Rust (217 pts)]]></title>
            <link>https://dioxuslabs.com/blog/release-060/</link>
            <guid>42388665</guid>
            <pubDate>Wed, 11 Dec 2024 15:24:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dioxuslabs.com/blog/release-060/">https://dioxuslabs.com/blog/release-060/</a>, See on <a href="https://news.ycombinator.com/item?id=42388665">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-node-hydration="103"><div><a href="https://dioxuslabs.com/blog/" data-node-hydration="104,click:1"><p data-node-hydration="105"><svg viewBox="0 0 16 16" width="16" style="width: 12px; height: 12px; color: currentcolor;" data-testid="geist-icon" height="16" stroke-linejoin="round"><path d="M10.5 14.0607L9.96966 13.5303L5.14644 8.7071C4.75592 8.31658 4.75592 7.68341 5.14644 7.29289L9.96966 2.46966L10.5 1.93933L11.5607 2.99999L11.0303 3.53032L6.56065 7.99999L11.0303 12.4697L11.5607 13L10.5 14.0607Z" clip-rule="evenodd" fill="currentColor" fill-rule="evenodd"></path></svg>Back to blog</p></a><h2><!--node-id106-->Dioxus 0.6<!--#--></h2><p><!--node-id107-->December 9, 2024<!--#--> - <!--node-id108-->Jonathan Kelley<!--#--></p><h3><!--node-id109-->Massive Tooling Improvements: Mobile Simulators, Magical Hot-Reloading, Interactive CLI, and more!<!--#--></h3></div><div><p data-node-hydration="110">Today we're releasing Dioxus 0.6!</p><p data-node-hydration="111">Dioxus is a framework for building fullstack web, desktop, and mobile apps with a single codebase. Our goal is to build a "Flutter but better." Dioxus focuses on first-class fullstack web support, type-safe server/client communication, and blazing fast performance.</p><p data-node-hydration="112">With this release, we focused on making Dioxus easier to use, improving the developer experience, and fixing bugs.</p><p data-node-hydration="113">Headlining the release is a complete overhaul of the Dioxus CLI:</p><ul data-node-hydration="114"><li><strong><a href="#android-and-ios-support-for"><code>dx serve</code> for mobile</a></strong>: Serve your app on Android and iOS simulators and devices.</li><li><strong><a href="#completely-revamped-hot-reloading">Magical Hot-Reloading</a></strong>: Hot-Reloading of formatted strings, properties, and nested <code>rsx!{}</code>.</li><li><strong><a href="#interactive-command-line-tools">Interactive CLI</a></strong>: Rewrite of the Dioxus CLI with a new, interactive UX inspired by Astro.</li><li><strong><a href="#inline-wasm-stacktraces-and">Inline Stack Traces</a></strong>: Capture WASM panics and logs directly into your terminal.</li><li><strong><a href="#fullstack-desktop-and-mobile">Server Functions for Native</a></strong>: Inline Server RPC for Desktop and Mobile apps.</li></ul><p data-node-hydration="115">We also improved the developer experience across the entire framework, fixing long standing bugs and improving tooling:</p><ul data-node-hydration="116"><li><strong><a href="#toasts-and-loading-screens">Toasts and Loading Screens</a></strong>: New toasts and loading screens for web apps in development.</li><li><strong><a href="#completely-revamped-autocomplete">Improved Autocomplete</a></strong>: Massively improved autocomplete of RSX.</li><li><strong><a href="#stabilizing-manganis"><code>asset!</code> Stabilization</a></strong>: Stabilizing our linker-based asset system integrated for native apps.</li><li><strong><a href="#suspense-and-html-streaming-for-the-web">Streaming HTML</a></strong>: Stream <code>Suspense</code> and <code>Error</code> Boundaries from the server to the client.</li><li><strong><a href="#static-site-generation-and-isg">SSG and ISG</a></strong>: Support for Static Site Generation and Incremental Static Regeneration.</li><li><strong><a href="#question-mark-error-handling">Error Handling with  <code>?</code></a></strong>: Use <code>?</code> to handle errors in event handlers, tasks, and components.</li><li><strong><a href="#document-elements">Meta Elements</a></strong>: New <code>Head</code>, <code>Title</code>, <code>Meta</code>, and <code>Link</code> elements for setting document attributes.</li><li><strong><a href="#synchronous">Synchronous  <code>prevent_default</code></a></strong>: Handle events synchronously across all platforms.</li><li><strong><a href="#tracking-size-with"><code>onresize</code> Event Handler</a></strong>: Track an element's size without an IntersectionObserver.</li><li><strong><a href="#tracking-visibility-with"><code>onvisible</code> Event Handler</a></strong>: Track an element's visibility without an IntersectionObserver.</li><li><strong><a href="#hybrid-wgpu-overlays">WGPU Integration</a></strong>: Render Dioxus as an overlay on top of WGPU surfaces and child windows.</li><li><strong><a href="#web-ios-and-android-bundle-support"><code>dx bundle</code> for Web, iOS, and Android</a></strong>: Complete <code>dx bundle</code> support for every platform.</li><li><strong><a href="#json-output-for-ci--cli"><code>json</code> mode</a></strong>: Emit CLI messages as JSON for use by 3rd party tools and CI/CD pipelines.</li><li><strong><a href="#new-starter-templates">New Templates</a></strong>: Three new starter templates for cross-platform apps.</li><li><strong><a href="#nightly-docs-tutorials-and-new-guides">Nightly Tutorial and Guides</a></strong>: New tutorials and guides for Dioxus 0.6 and beyond.</li><li><strong><a href="#preview-of-in-place-binary-patching">Binary Patching Prototype</a></strong>: Prototype of our new pure Rust hot-reloading engine.</li></ul><h2 id="about-this-release" data-node-hydration="117"><a href="#about-this-release">About this Release</a></h2><p data-node-hydration="118">Dioxus 0.6 is our biggest release ever: over 350 pull requests merged and hundreds of issues closed. We shipped 0.6 with a few goals:</p><ul data-node-hydration="119"><li>Dramatically improve the quality of hot-reloading, autocomplete, and asset bundling.</li><li>Make the Dioxus CLI more robust and easier to use.</li><li>Inline our mobile tooling into the dioxus CLI for 1st-class mobile support.</li></ul><p data-node-hydration="120">Since this post is quite long, we made a quick video highlighting new features, bugs fixed, and a quick tour of everything you can do with Dioxus now:</p><p data-node-hydration="121"><iframe height="500px" src="https://www.youtube.com/embed/WgAjWPKRVlQ" title="Dioxus 0.6" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p><p data-node-hydration="123">Dioxus 0.6 is shipping with a completely overhauled CLI experience! We’ve completely rewritten the CLI to support a ton of new features and improve stability:</p><p data-node-hydration="124"><img src="https://dioxuslabs.com/assets/image1-5eec3673e8f5fbfe.avif" alt="new-cli.png" title="" data-node-hydration="125"></p><p data-node-hydration="126">The new CLI sports live progress bars, animations, an interactive filter system, the ability to change log levels on the fly, and more.</p><p data-node-hydration="129">The CLI rewrite alone took more than half this release cycle. We went through several different design iterations and solved tons of bugs along the way. A few of the highlights:</p><ul data-node-hydration="130"><li>You can manually rebuild your app by pressing <code>r</code></li><li>You can toggle the log level of the CLI output on the fly and even inspect Cargo internal logs</li><li>We output all internal logs of the CLI so you can debug any issues</li><li>We capture logs for WASM tracing and panics</li><li>We dropped the <code>outdir</code> concept and instead use <code>target/dx</code> for all output.</li><li>Inline support for iOS and Android emulators.</li></ul><p data-node-hydration="131">You can install the new CLI using <a href="https://github.com/cargo-bins/cargo-binstall">cargo binstall</a> with <code>cargo binstall dioxus-cli@0.6.0 --force</code>.</p><h2 id="android-and-ios-support-for" data-node-hydration="132"><a href="#android-and-ios-support-for">Android and iOS support for </a><code>dx serve</code></h2><p data-node-hydration="133">With Dioxus 0.6, the dioxus CLI supports  <code>dx serve --platform ios/android</code> out of the box! 🎉</p><p data-node-hydration="134">While Dioxus has always had mobile support, the Rust tooling for mobile has been extremely unstable. Users constantly ran into issues with tools like <a href="https://github.com/BrainiumLLC/cargo-mobile"><code>cargo-mobile</code></a> and <a href="https://github.com/tauri-apps/cargo-mobile2"><code>cargo-mobile2</code></a>. These tools, while useful, take a different architectural approach than what is a good fit for Dioxus.</p><p data-node-hydration="135">With this release, we wrote our entire mobile tooling system from scratch. Now, you can go from  <code>dx new</code> to  <code>dx serve --platform ios</code> in a matter of seconds.</p><p data-node-hydration="136"><img src="https://dioxuslabs.com/assets/image-66cb6ea50fe694ec.avif" alt="Dioxus Mobile Support" title="" data-node-hydration="137"></p><p data-node-hydration="138">The Android and iOS simulator targets support all the same features as desktop: hot-reloading, fast rebuilds, asset bundling, logging, etc. Dioxus is also the only Rust framework that supports  <code>main.rs</code> for mobile - no other tools have supported the same  <code>main.rs</code> for every platform until now.</p><p data-node-hydration="139">Our inline mobile support requires no extra configurations, no manual setup for Gradle, Java, Cocoapods, and no other 3rd party tooling. If you already have the Android NDK or iOS Simulator installed, you currently are less than 30 seconds away from a production-ready mobile app written entirely in Rust.</p><p data-node-hydration="142">The simplest Dioxus 0.6 Mobile app is tiny:</p><p data-node-hydration="149">Especially, when compared to v0.5 which required you to migrate your app to a  <code>cdylib</code> and manually set up the binding layer:</p><p data-node-hydration="156">While 1st-class support for mobile platforms is quite exciting, there are certainly many limitations: the Rust mobile ecosystem is nascent, we don’t have great ways of configuring the many platform-specific build flags, and there isn’t a particularly great Rust/Java interop story.</p><p data-node-hydration="157">If you're interested in helping us build out mobile support, please join us on <a href="https://discord.gg/XgGxMSkvUM">Discord</a>.</p><h2 id="completely-revamped-hot-reloading" data-node-hydration="158"><a href="#completely-revamped-hot-reloading">Completely Revamped Hot-Reloading</a></h2><p data-node-hydration="159">We shipped massive improvements to the hot-reloading engine powering Dioxus. Our internal goal was to iterate on the Dioxus Docsite with zero full rebuilds.</p><p data-node-hydration="160">This means we needed to add support for a number of new hot-reloading engine changes:</p><ul data-node-hydration="161"><li>Hot-reload formatted strings</li><li>Hot-reload nested rsx blocks</li><li>Hot-reload component properties and simple Rust expressions</li><li>Hot-reload mobile platforms and their bundled assets</li></ul><p data-node-hydration="162">The new hot-reloading engine almost feels like magic - you can quickly iterate on new designs - and even modify simple Rust code! - without waiting for full rebuilds:</p><p data-node-hydration="165">The new engine allows you to modify formatted strings anywhere in your  <code>rsx</code>: in text blocks, element attributes, and even on component properties.</p><p data-node-hydration="172">The same tooling that enables component props reloading also works with <em>any Rust literal!</em> You can hot-reload numbers, booleans, and strings on component prop boundaries.</p><p data-node-hydration="181">The new hot-reloading engine also brings nested rsx hot-reloading support. The contents of  <code>for</code> loops,  <code>if</code> statements, and component bodies all now participate in hot-reloading:</p><p data-node-hydration="188">You can now move and clone Rust expressions between contexts, allowing you to re-use components and formatted strings between element properties without a full rebuild.</p><p data-node-hydration="195">These changes are supported in all platforms: web, desktop, and mobile.</p><p data-node-hydration="196">You can now hot-reload RSX and Assets on iOS and Android apps in addition to the classic web and desktop platforms.</p><p data-node-hydration="199">The new hot-reloading feels like magic and we encourage you to try it out!</p><h2 id="completely-revamped-autocomplete" data-node-hydration="200"><a href="#completely-revamped-autocomplete">Completely Revamped Autocomplete</a></h2><p data-node-hydration="201">Another huge overhaul in Dioxus 0.6: greatly improved autocomplete of  <code>rsx! {}</code>.  Our old implementation of  <code>rsx! {}</code> suffered from poor integration with tools like Rust-analyzer which provide language-server integration for your code. If the input to the macro wasn’t perfectly parsable, we failed to generate any tokens at all, meaning rust-analyzer couldn’t jump in to provide completions.</p><p data-node-hydration="202">The work to fix this was immense. Macro parsing libraries like  <code>syn</code> don’t provide great facilities for “partial parsing” Rust code which is necessary for implementing better errors and autocomplete. We had to rewrite the entire internals of  <code>rsx! {}</code> to support partial parsing of  <code>rsx! {}</code> , but finally, in 0.6, we’re able to provide stellar autocomplete. Not only can we autocomplete Rust code in attribute positions, but with a few tricks, we’re able to automatically insert the appropriate braces next to element names:</p><p data-node-hydration="203"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_9.55.12_PM-da711de8a0f26852.avif" alt="Screenshot 2024-11-14 at 9.55.12 PM.png" title="" data-node-hydration="204"></p><p data-node-hydration="205">The autocomplete experience is much nicer now, with all attributes, elements, components, and inline Rust code benefiting from the overhauled experience. All Rust expressions participate in proper Rust-analyzer autocomplete and we're even able to provide warnings when  <code>rsx!{}</code> input is malformed instead of panicking.</p><h2 id="inline-wasm-stacktraces-and" data-node-hydration="208"><a href="#inline-wasm-stacktraces-and">Inline WASM stacktraces and </a><code>tracing</code> integration</h2><p data-node-hydration="209">Along with the rewrite of the CLI, we shipped a  <code>tracing</code> integration for WASM apps that captures panics and logs and sends them  <code>dx</code> in your terminal. When you build your app with debug symbols, stack traces directly integrate with your editor, allowing you to jump directly to the troublesome files from within your terminal.</p><p data-node-hydration="210"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_8.52.18_PM-09c4ac2afae8d38e.avif" alt="Inline Stack Traces" title="" data-node-hydration="211"></p><p data-node-hydration="212">Thanks to this integration, we now have much nicer logging around fullstack apps, showing status codes, fetched assets, and other helpful information during development. With the toggle-able verbosity modes, you can now inspect the internal logs of the CLI itself, making it easier to debug issues with tooling to understand what exactly  <code>dx</code> is doing when it builds your app. Simply type  <code>v</code> to turn on “verbose” mode and  <code>t</code> to turn on “trace” mode for more helpful logs:</p><p data-node-hydration="213"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_9.06.05_PM-fa7be0c82414b119.avif" alt="Screenshot 2024-11-14 at 9.06.05 PM.png" title="" data-node-hydration="214"></p><h2 id="toasts-and-loading-screens" data-node-hydration="215"><a href="#toasts-and-loading-screens">Toasts and Loading Screens</a></h2><p data-node-hydration="216">As part of our CLI overhaul, we wanted to provide better feedback for developers when building web apps. Dioxus 0.6 will now show Popup Toasts and Loading Screens for web apps in development mode.</p><p data-node-hydration="217">Now, when your app is building, Dioxus will render a loading screen with the current progress of the build:</p><p data-node-hydration="218"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_9.41.38_PM-5776d82a4fca6b2b.avif" alt="Screenshot 2024-11-14 at 9.41.38 PM.png" title="" data-node-hydration="219"></p><p data-node-hydration="220">Additionally, once the app is rebuilt, you’ll receive a toast indicating the status of the build:</p><p data-node-hydration="221"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_9.42.33_PM-985e43ad24eac448.avif" alt="Screenshot 2024-11-14 at 9.42.33 PM.png" title="" data-node-hydration="222"></p><h2 id="fullstack-desktop-and-mobile" data-node-hydration="223"><a href="#fullstack-desktop-and-mobile">Fullstack Desktop and Mobile</a></h2><p data-node-hydration="224">Additionally, we properly integrated server functions with native apps. Server functions finally work out-of-the-box when targeting desktop and mobile:</p><p data-node-hydration="227">By default, in development, we set the server function endpoint to be localhost, so in production you need to make sure to point the functions to your deployed server:</p><h2 id="stabilizing-manganis" data-node-hydration="234"><a href="#stabilizing-manganis">Stabilizing Manganis </a><code>asset!()</code> system</h2><p data-node-hydration="235">We introduced our new asset system,&nbsp;<a href="https://github.com/DioxusLabs/manganis">Manganis</a>, in an alpha state with the 0.5 release. Dioxus 0.6 stabilizes the asset system and fixes several bugs and performance issues. You can try out the new&nbsp;<a href="https://github.com/DioxusLabs/manganis/pull/30">linker based asset system</a>&nbsp;by including an&nbsp;<code>asset!</code>&nbsp;anywhere in your code. It will automatically be optimized and bundled across all platforms:</p><p data-node-hydration="242">Manganis is a crucial step in supporting assets cross-platform, and specifically, through dependencies. Previously, if an upstream library wanted to export an asset like an image or a stylesheet, your app would need to manually add those assets in your  <code>assets</code> folder. This gets complex and messy when libraries generate CSS: many classes are duplicated and might even conflict with each other. Now, all CSS collected by the  <code>asset!()</code> macro is processed via our build pipeline, benefiting from minification and deduplication. Libraries can include their stylesheets and images and components and you can be guaranteed that those assets make it bundled into your app:</p><p data-node-hydration="249">Even better, assets like images are automatically optimized to generate thumbnails and more optimized formats. This can cut huge amounts of data from your site - AVIF and Webp can reduce file sizes by up to 90%. A funny note - platforms like Vercel actually <a href="https://vercel.com/docs/image-optimization">provide paid products for image optimization</a> while Manganis can do this for you, for free, at build time!</p><p data-node-hydration="250"><img src="https://dioxuslabs.com/assets/manganis-opt-09461e92e1507282.avif" alt="manganis-opt" title="" data-node-hydration="251"></p><p data-node-hydration="252">Additionally, manganis automatically hashes the images and modifies the generated asset name, allowing for better integration with CDNs and browser caching.</p><p data-node-hydration="253"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_10.22.48_PM-bdb45daf44c2cae9.avif" alt="Screenshot 2024-11-14 at 10.22.48 PM.png" title="" data-node-hydration="254"></p><p data-node-hydration="255">Manganis can handle a wide variety of formats - applying optimizations to assets like CSS, JavaScript, images, videos, and more.</p><p data-node-hydration="256">In Dioxus 0.5, we released Manganis in “alpha” status - and in 0.6 we’re stabilizing it. We’ve adjusted the API, so you’ll need to update any existing code that already uses it. Our new implementation is much more reliable, solving many of the bugs users were running into after the 0.5 release.</p><p data-node-hydration="257">Our new system leverages <em>the linker</em> to extract asset locations from the compiled binary. This is a rather advanced technique and took a while to get right, but we believe it’s a more robust solution in the long term. If you’re interested in integrating Manganis into your libraries and apps (like say, Bevy!), we have a guide just for that.</p><h2 id="suspense-and-html-streaming-for-the-web" data-node-hydration="258"><a href="#suspense-and-html-streaming-for-the-web">Suspense and HTML Streaming for the Web</a></h2><p data-node-hydration="259">Async is a core component of any UI framework. Dioxus provides hooks to handle async state. You can start a future and handle the loading and resolved states within the component:</p><p data-node-hydration="266">This works ok if you have a single future, but it quickly gets messy when combining many futures into one UI:</p><p data-node-hydration="273">In addition to hooks, we need a way to display a different state when async is loading. Dioxus 0.6 introduces a new core primitive for async UI called suspense boundaries. A suspense boundary is a component that renders a placeholder when any child component is loading:</p><p data-node-hydration="280">In any child component, you can simply bubble up the pending state with&nbsp; <code>?</code>&nbsp;to pause rendering until the future is finished:</p><p data-node-hydration="287">Along with suspense boundaries, dioxus fullstack also supports streaming each suspense boundary in from the server. Instead of waiting for the whole page to load, dioxus fullstack streams in each chunk with the resolved futures as they finish:</p><p data-node-hydration="290">Many of these features are quite cutting-edge and are just now being rolled out in frameworks in the JavaScript ecosystem. Getting the details right for Dioxus was quite difficult. We wanted to support both the fullstack web as well as native desktop and mobile apps. These two platforms often have competing design considerations. Fortunately, suspense also works for desktop and mobile, allowing you to emulate web-like data fetching patterns for native apps.</p><h2 id="static-site-generation-and-isg" data-node-hydration="291"><a href="#static-site-generation-and-isg">Static Site Generation and ISG</a></h2><p data-node-hydration="292">As part of our work on streaming, we also wanted to support another cutting-edge web feature: incremental static generation (ISG) and its cousin static site generation (SSG).</p><p data-node-hydration="293">Static site generation is a technique used by many web frameworks like Jekyll, Hugo, or Zola, to emit static  <code>.html</code> not reliant on JavaScript. Sites like blogs and portfolios typically use static site generation since platforms like GitHub Pages allow hosting static sites for free. In fact, this very docsite uses Dioxus SSG deployed to GitHub Pages! SSG helps improve SEO and speed up load times for your users.</p><p data-node-hydration="294">In Dioxus 0.6, we now support static-site-generation out of the box for all fullstack projects. Simply add a server function to your app called  <code>static_routes</code> that returns the list of routes that  <code>dx</code> should generate:</p><p data-node-hydration="301">Now, when you want to emit your static  <code>.html</code>, add the  <code>--ssg</code>  flag to  <code>dx build</code>:</p><p data-node-hydration="308">Static-site-generation is built on a new feature in Dioxus called incremental-site-generation (ISG). ISG is a technique similar to static-site-generation where the server generates pages on demand and caches them on the system filesystem. This allows the server to cache huge amounts of pages (for something like a school’s facebook directory or an e-commerce site with thousands of products) that get periodically invalidated. ISG is a somewhat advanced technique but is required to enable when using static-site-generation:</p><p data-node-hydration="315">We will likely be changing these APIs in future releases, but we are eager to let users experiment with these new features to simplify the existing static site setup.</p><h2 id="document-elements" data-node-hydration="316"><a href="#document-elements">Document Elements: </a><code>Title {}</code> , <code>Link {}</code> , <code>Stylesheet</code> , and <code>Meta {}</code></h2><p data-node-hydration="317">To date, it’s been rather cumbersome to do seemingly simple JavaScript operations in Dioxus. Due to our cross-platform nature, we need to find solutions to simple problems in ways that work for web, desktop, and mobile with a single abstraction.</p><p data-node-hydration="318">With Dioxus 0.6, we’re providing special elements under the  <code>document</code> namespace that make it possible to interact with the HTML  <code>document</code> object without needing to write extra JavaScript.</p><p data-node-hydration="319">Now, to set the  <code>title</code> of your HTML document, simply use the  <code>document::Title {}</code> component:</p><p data-node-hydration="326">And accordingly, the title of the page will update:</p><p data-node-hydration="327"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_11.28.42_PM-b6b3d19eb8871209.avif" alt="Screenshot 2024-11-14 at 11.28.42 PM.png" title="" data-node-hydration="328"></p><p data-node-hydration="329">Similarly, with  <code>Link</code> ,  <code>Stylesheet</code> , and  <code>Style</code>, you can include elements that automatically get merged into the document’s  <code>&lt;head&gt;</code> element. During server side rendering, these links get collected, deduplicated, and minified. With these built-in  <code>document</code> components, you’re now guaranteed that your  <code>&lt;head&gt;</code> element is properly set for pre-loading heavy assets like stylesheets and external JavaScript.</p><p data-node-hydration="336"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_11.31.18_PM-60417025a9cec40b.avif" alt="Screenshot 2024-11-14 at 11.31.18 PM.png" title="" data-node-hydration="337"></p><h2 id="question-mark-error-handling" data-node-hydration="338"><a href="#question-mark-error-handling">Question Mark Error Handling</a></h2><p data-node-hydration="339">With this release, we’ve made the transition where  <code>Element</code> is no longer an  <code>Option&lt;Node&gt;</code> but rather a  <code>Result&lt;Node&gt;</code>. This means we’re <em>finally</em> able to open up the use of typical Rust error handling in components:</p><p data-node-hydration="346">The new  <code>RenderError</code> acts like anyhow’s  <code>Error</code> type that can take in any  <code>dyn std::Error</code> type and propagate it upwards to the nearest error boundary.</p><p data-node-hydration="353">What’s even better: the  <code>?</code> syntax also works in EventHandlers, so you can quickly add things like server functions to your app without worrying about manual error handling:</p><p data-node-hydration="360">This new syntax lets Suspense and HTML-streaming return errors while rendering that don’t bring down the entire page.</p><h2 id="synchronous" data-node-hydration="361"><a href="#synchronous">Synchronous </a><code>prevent_default</code></h2><p data-node-hydration="362">In addition to being able to access the native event type, Dioxus 0.6 also makes all event handling synchronous. Previously, all event handling in Dioxus had to occur outside the normal browser event handling flow to support platforms like  <code>dioxus-desktop</code> which need to communicate over an interprocess communication (IPC) layer with the host webview. With this release, we’ve finally figured out how to enable blocking communication for  <code>dioxus-desktop</code> and can finally make event handling synchronous!</p><p data-node-hydration="363">As such, we no longer need the special  <code>dioxus_prevent_default</code> attribute and you can directly call  <code>event.prevent_default()</code>.</p><p data-node-hydration="370">This now makes it possible to implement  <code>prevent_default</code> conditionally which has previously been a limitation with Dioxus. Components like  <code>Link {}</code> now exhibit behavior exactly aligned with their native counterparts, solving long-standing issues with Dioxus apps.</p><h2 id="tracking-size-with" data-node-hydration="371"><a href="#tracking-size-with">Tracking size with </a><code>onresize</code></h2><p data-node-hydration="372">Thanks to the community, we now have two special handlers <em>not</em> found in the HTML spec: <code>onvisible</code> and <code>onresize</code>. These handlers are “special” dioxus handlers that automatically sets up an <code>IntersectionObserver</code> which previously required JavaScript.</p><p data-node-hydration="373">You can now implement rich interactions with little hassle:</p><h2 id="tracking-visibility-with" data-node-hydration="380"><a href="#tracking-visibility-with">Tracking visibility with </a><code>onvisible</code></h2><p data-node-hydration="381">In addition to  <code>onresize</code>, we now have a special handler <em>not</em> found in the HTML spec: <code>onvisible</code>.</p><p data-node-hydration="388">This makes it possible to add rich animations to your app without needing to write custom JavaScript.</p><h2 id="hybrid-wgpu-overlays" data-node-hydration="391"><a href="#hybrid-wgpu-overlays">Hybrid WGPU Overlays</a></h2><p data-node-hydration="392">This release also brings the "child window" feature for Dioxus desktop which lets you overlay native Dioxus apps on existing windows. This makes it simple to integrate Dioxus as an overlay over other renderers like WGPU and OpenGL:</p><h2 id="web-ios-and-android-bundle-support" data-node-hydration="395"><a href="#web-ios-and-android-bundle-support">Web, iOS, and Android bundle support</a></h2><p data-node-hydration="396">We added support for web and mobile with  <code>dx bundle</code>. Previously,  <code>dx bundle</code> only worked for desktop apps. Now you can bundle for a wide variety of targets:</p><ul data-node-hydration="397"><li>macOS (.app, .dmg)</li><li>Windows (.exe, .msi)</li><li>Linux (.deb, .rpm, .appimage)</li><li>Android (.apk)</li><li>iOS (.ipa, .app)</li><li>Web (.appimage, /public folder)</li></ul><h2 id="json-output-for-ci--cli" data-node-hydration="398"><a href="#json-output-for-ci--cli">JSON Output for CI / CLI</a></h2><p data-node-hydration="399">As part of our overhaul with the CLI, we’re also shipping a  <code>json-output</code> mode. Now, when you pass  <code>--json-output</code> to Dioxus commands, you will receive the logging in json format:</p><p data-node-hydration="400"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_10.38.33_PM-8328473ee184927b.avif" alt="Screenshot 2024-11-14 at 10.38.33 PM.png" title="" data-node-hydration="401"></p><p data-node-hydration="402">This is particularly important for users of  <code>dx bundle</code> who want to automatically upload the their bundles to their hosting provider of choice. You can easily combine the output of  <code>dx</code> with a tool like  <code>jq</code> to extract important information like bundle outputs with a simple one-liner:</p><p data-node-hydration="403"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_10.40.56_PM-51046f38000e601e.avif" alt="Screenshot 2024-11-14 at 10.40.56 PM.png" title="" data-node-hydration="404"></p><h2 id="new-starter-templates" data-node-hydration="405"><a href="#new-starter-templates">New Starter Templates</a></h2><p data-node-hydration="406">Dioxus 0.6 ships with three new starter templates for cross-platform apps. Each template is a fully-featured, production-ready app that you can use as a starting point for your own Dioxus apps.</p><ul data-node-hydration="407"><li>Bare-Bones: A bare-bones starter template with no styling, assets, or structure.</li><li>Jumpstart: A starter template with a basic structure, components, and a few pages.</li><li>Workspace: A starter template with separate crates for web, desktop, and mobile.</li></ul><p data-node-hydration="408">These are baked directly into the  <code>dx new</code> command - simply run  <code>dx new</code> and follow the prompts to select the template you want.</p><h2 id="nightly-docs-tutorials-and-new-guides" data-node-hydration="409"><a href="#nightly-docs-tutorials-and-new-guides">Nightly Docs, Tutorials, and New Guides</a></h2><p data-node-hydration="410">As usual with these large releases, Dioxus 0.6 features a rather sizable overhaul to the documentation. We’ve completely overhauled the tutorial to be less heavy on code. The new tutorial focuses on basics like including assets and deploying to production.</p><p data-node-hydration="411"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_11.35.23_PM-5896b8f2b94e25ea.avif" alt="Screenshot 2024-11-14 at 11.35.23 PM.png" title="" data-node-hydration="412"></p><p data-node-hydration="413">The docsite now includes all “modern” versions of Dioxus inline: 0.3, 0.4, 0.5, and 0.6 are all accessible under the same top-level website. Previously, we linked out to different MDbooks which eventually became a hassle. Now, you can simply switch between each version inline:</p><p data-node-hydration="414"><img src="https://dioxuslabs.com/assets/version_switch_shadow-2d19108722c8fa50.avif" alt="Screenshot 2024-11-15 at 1.02.23 AM.png" title="" data-node-hydration="415"></p><p data-node-hydration="416">The inline version switcher means we’ll now be able to publish documentation for alpha releases of Dioxus, hopefully making your life easier as we ship new features for the future. The new docs also feature small quality-of-life upgrades like breadcrumbs:</p><p data-node-hydration="417"><img src="https://dioxuslabs.com/assets/breadcrumbs_shadow-33da77df40ec45f0.avif" alt="Screenshot 2024-11-15 at 1.04.13 AM.png" title="" data-node-hydration="418"></p><p data-node-hydration="419">as well as new codeblocks with interactive examples:</p><p data-node-hydration="420"><img src="https://dioxuslabs.com/assets/interacitve_widget_shadow-be1d0998201ca923.avif" alt="Screenshot 2024-11-15 at 1.05.03 AM.png" title="" data-node-hydration="421"></p><h2 id="preview-of-in-place-binary-patching" data-node-hydration="422"><a href="#preview-of-in-place-binary-patching">Preview of In-Place Binary Patching</a></h2><p data-node-hydration="423">While working on the new hot-reloading engine, we experimented with adding proper hot-reloading of Rust code to Dioxus apps. The work here was inspired by Andrew Kelley’s “in-place-binary-patching” goal for Zig. Unfortunately, we didn’t have a chance to productionize the prototype for this release (way too many features already!) but we did put together a <a href="http://github.com/jkelleyrtp/ipbp">small prototype</a>:</p><p data-node-hydration="426">We likely won’t have the time to ship true Rust hot-reloading in 0.7, but stay tuned for early next year!</p><h2 id="smaller-changes" data-node-hydration="427"><a href="#smaller-changes">Smaller changes:</a></h2><p data-node-hydration="428">Not every change gets a particularly large section in the release notes, but we did land several new features and refactors.</p><ul data-node-hydration="429"><li>System tray support: we now have proper support for System Trays again, thanks to a wonderful community contribution.</li><li>Custom event loops: you can provide your own event loop, making it possible to use Dioxus in contexts where you already have other windows.</li><li><code>dioxus-document</code>: we split out our <code>document</code> abstraction so any renderer can implement the <code>Document</code> trait to integrate with <code>Title {}</code>, <code>Script {}</code> , and <code>eval</code></li><li><code>dioxus-history</code>: we also split out our <code>history</code> abstraction so other renderers can benefit from <code>Link</code> and <code>Router</code> without needing a dedicated feature flag on <code>dioxus-router</code></li><li><code>eval</code> API was simplified to allow <code>.recv::&lt;T&gt;().await</code> on evals, making interoperating with JavaScript easier.</li><li><code>dx fmt</code> now supports <code>#[rustfmt::skip]</code> attributes, respects <code>rustfmt.toml</code> settings, and is generally more reliable</li></ul><h2 id="upgrading-from-05-to-06" data-node-hydration="430"><a href="#upgrading-from-05-to-06">Upgrading from 0.5 to 0.6</a></h2><p data-node-hydration="431">Generally there are few huge breaking changes in this release. However, we did change a few APIs that might break your existing apps but are easy to fix.</p><ul data-node-hydration="432"><li><code>asset!()</code> syntax changes</li><li><code>eval()</code> API small changes</li><li>migrating to <code>prevent_default()</code></li><li>migrating from VNode::None to <code>rsx! {}</code> for empty nodes</li></ul><p data-node-hydration="433">We’ve assembled a <a href="https://dioxuslabs.com/learn/0.6/migration/">migration guide</a> to help.</p><h2 id="conclusion" data-node-hydration="434"><a href="#conclusion">Conclusion</a></h2><p data-node-hydration="435">That’s it for this release! We addressed countless issues including bundling bugs, spurious hot-reloads, and compatibility with unusual platforms and editors.</p><p data-node-hydration="436">Dioxus 0.6 has been in alpha for quite a while, and we’re very thankful for all the testing the community has done to make this the most polished release yet. It’s quite difficult to run a large open source project such a wide scope. This release took <em>much</em> longer to get out than we wanted - consuming two release cycles instead of just one.</p><p data-node-hydration="437">We focused hard this release to polish up as many rough edges as possible. Our continuous integration and deployment is in a much nicer place. We’re finally able to release nightly versions of documentation and the alpha release system has worked well for users eager to test out new features and bug fixes.</p><p data-node-hydration="438">Unfortunately, this release contained many connected pieces which made it hard to release incrementally. Systems like assets integrate tightly with CLI tooling and cross-platform support: to get one configuration right you need to test them all. With 0.6 behind us, the future seems much more “incremental” which should let us release major versions with faster cadence.</p><p data-node-hydration="439">We plan to keep 0.6 around for a while. Instead of shipping new features for a while, we're excited to make tutorial videos, write documentation, fix bugs, improve performance, and work with the community. The Dioxus team wants to spend time building our own apps!</p><p data-node-hydration="440">That being said, we do have a few major items planned for Dioxus 0.7 and beyond:</p><ul data-node-hydration="441"><li>Rust hot-reloading with binary patching</li><li>Integrating wasm bundle splitting with the router</li><li><code>dx deploy</code> to a hosted deploy platform (Fly.io, AWS, Cloudflare, etc.)</li></ul><p data-node-hydration="442">We’re also hiring - if you want to come build Dioxus with me in San Francisco (or remote) please reach out!</p><p data-node-hydration="444">We want to extend a huge thank-you to everyone who helped test and improve this release. We saw an incredible number of contributors fix bugs and add features. Special thanks to:</p><p data-node-hydration="445"><a href="https://github.com/ASR-ASU">@ASR-ASU</a> - <a href="https://github.com/Aandreba">@Aandreba</a> - <a href="https://github.com/Andrew15-5">@Andrew15-5</a> - <a href="https://github.com/DogeDark">@DogeDark</a> - <a href="https://github.com/Klemen2">@Klemen2</a> - <a href="https://github.com/LeWimbes">@LeWimbes</a> - <a href="https://github.com/LeoDog896">@LeoDog896</a> - <a href="https://github.com/MrGVSV">@MrGVSV</a> - <a href="https://github.com/Rahul721999">@Rahul721999</a> - <a href="https://github.com/Septimus">@Septimus</a> - <a href="https://github.com/Tahinli">@Tahinli</a> - <a href="https://github.com/WilliamRagstad">@WilliamRagstad</a> - <a href="https://github.com/ahqsoftwares">@ahqsoftwares</a> - <a href="https://github.com/airblast-dev">@airblast-dev</a> - <a href="https://github.com/alilosoft">@alilosoft</a> - <a href="https://github.com/azamara">@azamara</a> - <a href="https://github.com/chungwong">@chungwong</a> - <a href="https://github.com/d3rpp">@d3rpp</a> - <a href="https://github.com/daixiwen">@daixiwen</a> - <a href="https://github.com/dependabot">@dependabot</a> - <a href="https://github.com/ealmloff">@ealmloff</a> - <a href="https://github.com/hackartists">@hackartists</a> - <a href="https://github.com/hardBSDk">@hardBSDk</a> - <a href="https://github.com/houseme">@houseme</a> - <a href="https://github.com/i123iu">@i123iu</a> - <a href="https://github.com/ilaborie">@ilaborie</a> - <a href="https://github.com/imgurbot12">@imgurbot12</a> - <a href="https://github.com/jacklund">@jacklund</a> - <a href="https://github.com/jingchanglu">@jingchanglu</a> - <a href="https://github.com/luveti">@luveti</a> - <a href="https://github.com/marc2332">@marc2332</a> - <a href="https://github.com/matthunz">@matthunz</a> - <a href="https://github.com/nayo0513">@nayo0513</a> - <a href="https://github.com/opensource-inemar-net">@opensource-inemar-net</a> - <a href="https://github.com/oskardotglobal">@oskardotglobal</a> - <a href="https://github.com/panglars">@panglars</a> - <a href="https://github.com/pyrrho">@pyrrho</a> - <a href="https://github.com/ribelo">@ribelo</a> - <a href="https://github.com/rogusdev">@rogusdev</a> - <a href="https://github.com/ryo33">@ryo33</a> - <a href="https://github.com/samtay">@samtay</a> - <a href="https://github.com/sknauff">@sknauff</a> - <a href="https://github.com/srid">@srid</a> - <a href="https://github.com/tigerros">@tigerros</a> - <a href="https://github.com/tpoliaw">@tpoliaw</a> - <a href="https://github.com/uzytkownik">@uzytkownik</a></p></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PeerTube mobile app: discover videos while caring for your attention (370 pts)]]></title>
            <link>https://joinpeertube.org/news/peertube-app</link>
            <guid>42388488</guid>
            <pubDate>Wed, 11 Dec 2024 15:09:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joinpeertube.org/news/peertube-app">https://joinpeertube.org/news/peertube-app</a>, See on <a href="https://news.ycombinator.com/item?id=42388488">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today, at Framasoft (bonjour!), we publish the very first version of the PeerTube Mobile app for android and iOS. A lot of care went into its conception, to help a wider audience watch videos and discover platforms, while not getting their attention (and data) exploited.</p>
<h4>Another step into PeerTube growth</h4>
<p>Even though we have been developing and maintaining the PeerTube software for 7 years, we, <a href="https://framasoft.org/" target="_blank" rel="noopener noreferrer">at Framasoft, are far from being an IT company</a>. First because <strong>we are a not-for-profit</strong> (funded through donations, you can support us <a href="https://support.joinpeertube.org/" target="_blank" rel="noopener noreferrer">here</a>), and then because <strong>our goal is, actually, to help others educate themselves on digital issues, surveillance capitalism</strong>, etc. and to give them tools that helps them get digitally emancipated.</p>
<p><strong>Developing PeerTube has been, to us, an (happy) accident</strong>. We wanted to show that with one paid developer (for the first six years, then two), very little means (~ €650,000 over 7 years) and lots of community contributions, we can create a radical alternative to YouTube and Twitch. It also took a lot of patience. From the get go, <strong>we knew we needed to aim for a slow but steady pace of growth</strong> for the software, the network of video platforms it federates, the whole ecosystem and the audiences it reached.</p>

<p>Videos and live-streams are increasingly watched on mobile devices. We knew <strong>the next step to widen the audience of the PeerTube network of platforms was to develop a mobile client</strong>. Last year, we decided to hire <a href="https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/" target="_blank" rel="noopener noreferrer">Wicklow (who completed his last internship, before graduating, here with us)</a>, to train him on mobile technologies, develop a mobile app, while continuing to get familiar with PeerTube's core code.</p>
<h4>Getting funded and getting help</h4>
<p>This was (and still is) a big decision: a new hire needs to be funded (our huge thanks to <a href="https://nlnet.nl/" target="_blank" rel="noopener noreferrer">NLnet</a> and the <a href="https://nlnet.nl/entrust/" target="_blank" rel="noopener noreferrer">NGI0 Entrust program</a>!), and we want to stay a small structure, so we don't have lots of room in our team. In hindsight, though, we believe it was the right one.</p>
<p>We surrounded ourselves with <a href="https://www.zenika.com/" target="_blank" rel="noopener noreferrer">Zenika</a>, to get help on architecture and experience on mobile strategy. We soon realized that peer-to-peer video sharing wouldn't be a wise strategy on mobile devices. After benchmarking different technologies, Wicklow picked Flutter for the development.</p>
<p><a href="https://www.lacooperativedesinternets.fr/" target="_blank" rel="noopener noreferrer">La Coopérative des Internets (French design workers-owed-company)</a>, helped us pinpoint the relevant user experience and design an app fit for videos on the fediverse. <strong>We decided, for the first release, to limit the scope of the app to the "spectator use-case"</strong>: browsing and watching videos.</p>
<p>We plan to share all their reports soon (early 2025), as soon as we put in the final touches. We hope that sharing this expertise and experience will help other FLOSS initiatives in their endeavor.</p>
<p>In the meanwhile, the PeerTube Mobile app is (as always with us) Free-libre and open-source, and you can <a href="https://framagit.org/framasoft/peertube/mobile-application" target="_blank" rel="noopener noreferrer">find the source code here on our repository</a>.</p>
<p><img src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-welcome.jpg" alt="image welcome page">
  <img src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-video-player.jpg" alt="image player">
</p>
<h4>Fediverse complexities made simple</h4>
<p>This preparatory work helped us realize that a mobile client was <strong>an amazing opportunity to simplify the PeerTube experience</strong>. PeerTube is not a video platform: it's a network of video platforms, each with their own rules, means and focus, that can choose to federate with others (or not).</p>
<p>It is, by design, more complex than a centralized platform. One of the main feedback we got from video enthusiasts was</p>
<blockquote>
<p>"I don't know where to get an account. I don't know where to search &amp; find videos" (even though we maintain <a href="https://sepiasearch.org/" target="_blank" rel="noopener noreferrer">SepiaSearch</a>).</p>
</blockquote>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/2024-12-sepia-search-screenshot-EN.jpg" title="" alt="">  </figure>

<h5>Local account</h5>
<p>Within a mobile client, we can create some kind of local account, directly on your device, so you get your watch-list, playlists, faves, etc. <strong>It saves you the hassle of finding a platform where you'd need to create an account</strong> if you just want to enjoy video content.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-watch-later.jpg" title="" alt="">  </figure>

<h5>Explore platforms</h5>
<p>We can also include a search engine and an interface to explore the federation of PeerTube platforms and find videos suited to your interest. Not everyone knows <a href="https://sepiasearch.org/" target="_blank" rel="noopener noreferrer">SepiaSearch</a> (and other fediverse search engines) exists: <strong>you get it from the get go, in your pocket</strong>.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-explore.jpg" title="" alt="">  </figure>

<h5>Highlighting platforms' diversity</h5>
<p>Finally, we can present content in a way that highlights the platforms, and show you where the videos/channels you watch are hosted. Differentiating platforms is <strong>a practical, visual way of introducing the concept of federation</strong> to a wider audience.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-explore-2.jpg" title="" alt="">  </figure>

<h4>Designing out dark patterns</h4>
<p>Humility check: a small French nonprofit will never have Google's workforce nor Amazon's money (and vice versa). But <strong>we have an edge: we are not constrained by surveillance capitalism rules</strong>, and its captology models.</p>
<blockquote>
<p>Neither PeerTube nor the mobile app have any interest into grabbing your attention, forcefeeding you ads and milking behavioural and personal data from you.</p>
</blockquote>
<p>That is how <strong>we freed the design from toxic design patterns such as doom scrolling, curated feeds, needy notifications and so on</strong>.</p>
<p>It might sound obvious, but it takes real effort to concieve an interface cleaned from what has unfortunately became the new normal. Even more if you need to keep it familiar enough so it says easy to use.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-show-more.jpg" title="" alt="">  </figure>

<h4>A very first build, limited by (play &amp; i) stores</h4>
<p>We knew beforehand that <strong>fitting into Google's PlayStore and Apple AppStore would be a challenge</strong>. They clearly weren't ready to host a client for (not-a-platform but) a network of autonomous video-sharing platforms, published by a small French nonprofit, funded through its independent donation website.</p>
<p>We knew about the <a href="https://github.com/sschueller/peertube-android/issues/302" target="_blank" rel="noopener noreferrer">issues encountered by Thorium</a> (another PeerTube mobile client). We got help and advices from Gabe, who develops <a href="https://owncast.online/" target="_blank" rel="noopener noreferrer">the streaming tool Owncast</a> (may your keyboard always repel crumbs and click smoothly), and <a href="https://laurenshof.online/owncast-and-the-app-store/" target="_blank" rel="noopener noreferrer">encountered many obstacles</a>... We knew about all that but, oh my Tux, it was a wild ride.</p>
<p>After jumping though hoops, here we are, you can download the PeerTube mobile app here:</p>

<p>
  <a target="_blank" rel="noopener noreferrer" href="https://asso.framasoft.org/dolo/h/peertube-apk-latest">Download the lastest apk (Android/Advanced)</a>
</p>
<h4>(un-)Limiting the federation</h4>
<p>To get through Apple's (and, in a lesser way, Google's) validation processes, we had to present the mobile app with a curated "allowlist" of PeerTube platforms that meet their standards.</p>
<p>Here is the state of those limitations right now:</p>
<ul>
<li><strong>Apple AppStore</strong>: limited to a very strict allowlist. Truth be told, a week before release, we are still unsure of being validated. Once we manage it, we'll see how to widen the list &amp; let users add platforms they want</li>
<li><strong>Google Play Store</strong>: limited allowlist, but users can already add the platforms they want. We plan to widen the allowlist next</li>
<li><strong>F-Droid</strong> (coming soon) and direct download apk: all PeerTube platforms we have indexed on <a href="https://sepiasearch.org/" target="_blank" rel="noopener noreferrer">SepiaSearch</a> are available. If an instance isn't declared to our index or is moderated, you can add it manually.</li>
</ul>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-plaforms.jpg" title="" alt="">  </figure>

<p>We cannot stress enough how <strong>their stores are not ready for independent solidarity-oriented networks</strong>. For exemple, a small "support us" donation link in our website footer or even on one of the allowed platforms triggered a "nope" from Apple.</p>
<p>And that's consistent: as seen in <a href="https://en.wikipedia.org/wiki/Epic_Games_v._Apple" target="_blank" rel="noopener noreferrer">their fight with Epic</a> (owners of Fortnite) Apple take their share in every in-app purchases. They have an economic interest to keep your expenses enclosed in their ecosystem. Please, please: consider getting your freedom back ;).</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/expected-nothing.jpg" title="" alt="">  </figure>

<h4>Coming soon, in the PeerTube App</h4>
<p>Fitting into Apple's (and Google's) very small boxes took time and energy, more than what we expected. We decided to release a first (incomplete) version of the app in December anyway, and gradually improve on it.</p>
<p>Here are the <strong>features we plan to develop and share for the PeerTube app</strong>:</p>
<ul>
<li>Soon (early 2024)
<ul>
<li>Finalize and publish design and mobile strategy reports</li>
<li>Publish documentation</li>
<li>Play video in background</li>
<li>Log in to one's account, gets subscriptions, comment videos</li>
<li>next video recommandation</li>
<li>improve on the limited platforms list situation</li>
</ul>
</li>
<li>Then (mid 2024 (if funded))
<ul>
<li>adapt to tablets</li>
<li>adapt to TVs (AndroidTV... AppleTV will depend on their limitations)</li>
<li>Watch offline (for downloadable content)</li>
</ul>
</li>
</ul>
<p>Right now, we are still waiting to secure funding for those mid-2024 features (for which we have requested a NLnet grant).</p>
<p>Depending on the app success and usage, <strong>we would love to add the content creator usecase to the app</strong>. But that's a big one: upload and publish a video, manage one's content, create a livestream, etc. We are still wondering <strong>where, when and how to get funds for this undertaking</strong>.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/PeerTube-app-CC-BY-David-Revoy.jpeg" title="" alt="">  </figure>

<h4>Care, Share and Contribute!</h4>
<p><strong>This is the part where we need you</strong>.</p>
<p>We hope you will <strong>enjoy this app, download and use it, and share it</strong> with your friends. This is a new gateway to promote PeerTube content, get audience to fabulous content creators, entice them to share more and boost that virtious loop.</p>
<p>This app is also <strong>a way of showcasing how media could be presented</strong>, when they are made with care for your agency and attention. More than ever: <strong>sharing is caring</strong>.</p>
<p>You can also <strong>contribute by reporting bugs</strong> (within the app), helping on the code (<a href="https://framagit.org/framasoft/peertube/mobile-application" target="_blank" rel="noopener noreferrer">here is the git repository</a>), and translating the interface. This is an important one: right now, the App is only available in English and French. <strong><a href="https://weblate.framasoft.org/projects/peertube-app/peertube-app/" target="_blank" rel="noopener noreferrer">Your language contributions are welcomed</a> here on our translation platform</strong>.</p>
<p>Obviously, we plan to maintain the app, add translations, implement bugfixes and security updates when needed: but this has a cost. <strong>We need to secure Framasoft's 2025 budget</strong> to make Wicklow's position permanent in our team (which is a priority to us). <strong>Our donation campaign is active right now</strong>, you can add your support <a href="https://support.joinpeertube.org/" target="_blank" rel="noopener noreferrer">here</a> (and thanks!).</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/20-ans-CC-BY-David-Revoy.jpeg" title="" alt="">  </figure>

<hr>
<p>You can help us continue to improve PeerTube by sharing this information, <a href="https://ideas.joinpeertube.org/" target="_blank" rel="noopener noreferrer">suggesting improvements</a> and, if you can afford it, making <a href="https://support.joinpeertube.org/" target="_blank" rel="noopener noreferrer">a donation to Framasoft</a>, the association that develops PeerTube.</p>
<p>Thanks in advance for your support!<br>
Framasoft</p>

<p>
  <a target="_blank" rel="noopener noreferrer" href="https://asso.framasoft.org/dolo/h/peertube-apk-latest">Download the lastest apk (Android/Advanced)</a>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't Get Distracted (195 pts)]]></title>
            <link>https://calebhearth.com/dont-get-distracted</link>
            <guid>42388354</guid>
            <pubDate>Wed, 11 Dec 2024 14:57:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://calebhearth.com/dont-get-distracted">https://calebhearth.com/dont-get-distracted</a>, See on <a href="https://news.ycombinator.com/item?id=42388354">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    
      
<section>
  <iframe src="https://www.youtube.com/embed/UBdBoWAtLNI" frameborder="0" allowfullscreen=""></iframe>
</section>

<p><em>Be sure to check out these <a href="https://calebhearth.com/images/dont-get-distracted-sketchnotes--stephanie-nemeth.jpg">Sketchnotes</a> from a 15 minute version of this talk from <a href="http://codelandconf.com/">Codeland</a> by <a href="https://twitter.com/stephaniecodes">Stephanie Nemeth</a></em></p>

<p>I’m going to tell you about how I took a job building software to kill people.</p>

<p>But don’t get distracted by that; I didn’t know at the time.</p>

<p>Even before I’d walked across the stage for graduation, I accepted an offer for an internship. It paid half again as much as the most I’d ever gotten in the highest paying job up to that point. Not to mention that I’d spent years in college with low paid student jobs or living only on student loans.</p>

<p>I’d be joining a contracting company for the Department of Defense. The Department of Defense, or DOD, is the part of the government made up by the military in the United States. The DOD outsources all sorts of things, from <a href="https://www.defense.gov/News/Contracts/Contract-View/Article/1177766/">P-8A Multi-mission Maritime aircraft to blue, shade 451, Poly/wool cloth</a>.</p>

<p>At the time, I thought nothing of the fact that I’d be supporting the military. Besides, they’re the good folks right? My dad was in the military. So was my grandfather. It was good money, a great opportunity, and a close friend of mine had gotten me the gig. Life was good.</p>

<p>I showed up for my first day of work in North Virginia, or NoVA as the industry likes to call it. I met the team of other interns, then learned what the team would be building: a tool to use phones to find WiFi signals.</p>

<p>It seemed pretty cool compared to what I’d built to up that point. The most complicated thing was an inventory management system. It didn’t concern itself overmuch with persistence. Who needs to stop and restart a program anyhow? The data was right there in memory and if you forgot how many Aerosmith CDs you had, who cares? It got me an A on the assignment, and that was all that mattered.</p>

<p>Honestly, the idea of finding WiFi routers based on the signal strength seemed pretty intimidating at the time. The idea impressed me.</p>

<p>But don’t get distracted by all this; the software was intended to kill people.</p>

<p>I joined the team after they’d already gotten started on the project. The gist of the tool was that it would look at how WiFi signal strength changed as your phone moved around. If the signal strength got stronger, you were getting closer. If it got weaker, you were moving away. To find this information, we’d collect two pieces of information for each WiFi access point in range. The phone’s geolocation information and the WiFi signal strength.</p>

<p>To predict the actual location of the WiFi signal, we used a convolution of two algorithms. Both of them relied on the Free Space Path Loss equation. For our purposes, FSPL calculates how far away a phone is from a WiFi signal based on loss in signal strength. It assumes that there is only empty air, or “free space”, between the access point and the phone.</p>

<p>The first algorithm was R^2. It measured the difference between the signal strength we’d observed and the expected signal strength at each distance in a search grid based on Free Space Path Loss. Locations with the lowest R^2 error rate were the most likely location.</p>

<p>We’d combine that calculation with a Gaussian estimate. Now I spent two or three days last week trying to understand Gaussian estimates for this talk and couldn’t. The best documentation on them are still research papers. I do know that it creates a probability curve. The high points of the curve are distances where the access point is likely to be, and low points are unlikely. The curve started with a probability hole of low values. They represented low likelihood that the phone is standing right next to the signal. The curve then increased to high probabilities further out. It decreased to near zero even further. The algorithm adjusted the width and height of these two curves by consulting past measurements. It created a heat map of probabilities for the signal source.</p>

<p>We’d normalize the probabilities for each location in the search grid. A combination of probabilities was more correct than either algorithm itself.</p>

<p>We stored this probability matrix for each location a phone collected from. Using these, if we collected readings while moving in a straight line, we could tell you how far away the WiFi was. If you turned a corner, we could also add direction, so you could find it in 2D space. If you climbed some stairs, we’d show you altitude as well. The technology was the most interesting project I’d ever worked on.</p>

<p>But don’t let that distract you; it was designed to kill people.</p>

<p>I mentioned that I had a software engineering degree at this point. My teammates were earlier in their education careers. Most of them were a year or two into their four year programs, also a mix of Computer Science and Math majors. My expertise was in the design and process of building software, while theirs was more in high level mathematics or the theory of computer use.</p>

<p>I helped to translate the working algorithms they’d designed in MatLab to the Java code we needed to run on the phones.</p>

<p>And let’s be honest, we spent plenty of time deciding whether we preferred Eclipse or NetBeans. Can I say how happy I am that as a Ruby developer, I’ve not had to figure out where to put a .jar file in over half a decade?</p>

<p>One such example is calculating distance between two points. As these things go, it was in the deepest level of each loop. We were using great circle distance, which is the way you measure the shortest distance between two points on a sphere, such as Earth. Did we need to use this complicated measurement over the meager distances a WiFi network can operate over? Absolutely not. Did I mention this was over half a decade ago? We weren’t always making the best choices. Anyhow, we needed to calculate these distances. The function doing it was being hit hundreds of thousands of time for each collection point, often with the same two locations. It was a very slow process.</p>

<p>We solved that by implementing a dictionary of latitude/longitude pairs to distances. This at least meant we didn’t re-do those calculations. This and other optimizations we made sped up the performance from seven minutes to a few seconds.</p>

<p>But don’t get distracted; that performance increase made it faster to kill people.</p>

<p>The accuracy of the locations wasn’t fantastic. I don’t remember exactly what it was before we focused on improving this, but an average error about 45 feet sticks in my head.</p>

<p>That’s a little longer than a Tyrannosaurus Rex nose to tail, or more concretely the length of a shipping container.</p>

<p>That’s significant when the WiFi range for 802.11n is only about 100 feet. That means we could be up to almost half the range of the router off from where it was.</p>

<p>I talked about the Gaussian estimation, the two curves from the second algorithm. We hard-coded numbers that defined this curve. They were only starting points, but they were starting points every time we made the calculation.</p>

<p>A Genetic Algorithm is a type of program that produces a set of values that optimize for a desired result. It’s a perfect fit for tuning these hard coded values to get more accurate results.</p>

<p>Each of the Gaussian estimation values will is a gene. The set of values is a genome in Genetic Algorithm parlance. The genes were our 3 constants.</p>

<p>A fitness function is what Genetic Algorithms use to measure performance of a genome. For the dataset of readings I was using in the GA, I knew the actual location of the access points. That meant I could run the geolocation algorithm with each genome’s values in place as the fitness function. The result would be the distance between the actual location and the one calculated by the GA-derived values.</p>

<p>Genetic Algorithms take a set of genomes, called a generation, and keep a certain percentage of top performers.</p>

<p>These top performers “survive” to the next generation as copies. Sometimes the algorithm mutates these copies by adding a Gaussian random value to each gene. This means that there was a chance that any of the copied genomes would have each gene changed slightly. That way they would have a chance of performing better or worse. New random genomes are created for the remaining spots in the new generation.</p>

<p>We saved the top performers across all populations. When the GA ended I could take a look at the values and select the best performer.</p>

<p>I let this genetic algorithm run over the weekend. It was able to increase the accuracy from 40-odd feet to about 10.3 feet, 25% of the error from the original. That is less than the GPS accuracy on the smartphones collecting data (which is about 16 feet according to GPS.gov). This is too accurate, so it may have over-optimized against the test data and might not be as accurate against other data sets. This is called overfitting and the way around it is to have separate sets of training and test data. Did I mention this was five years ago? I didn’t even know what overfitting was back then.</p>

<p>I loved this. Genetic algorithms, R^2, and Gaussian estimation are the kind of thing that they tell you you’ll never need to use again once you graduate. But we were using them for a real world project! It was great.</p>

<p>But don’t let that distract you; this accuracy made it easier for the software to help kill people.</p>

<p>The tracking now worked accurately and quickly. The next feature was to add tracking a moving WiFi access point. I briefly wondered why an access point would be moving, but that question wasn’t as interesting as figuring out how to track it.</p>

<p>We made use of Kalman Filters to observe state variables: the position, velocity, and acceleration of the WiFi signal. Given these and the time since the last measurement, a Kalman Filter is able to improve the current prediction with surprising accuracy, filtering out “noise” data automatically.</p>

<p>Each time we ran the real time algorithm, we’d also feed this to the Kalman Filter.  With only that information, it was able to produce an estimate that is a weighted average of previous data. A new predicted location that was more accurate than the calculated value.</p>

<p>At the same time, we added the ability to track more than one WiFi signal. We’d filter our collection of readings by the unique identifier of each access point. The filtered datasets each went through the full algorithm to produce predictions.</p>

<p>We used the APIs on the phone to read the signal strength of all WiFi access points in range. We were able to track multiple hotspots, basically whatever we could see in the WiFi network list. It was all very exciting. These seem like academic problems, but we were getting to use them in a real project! Being a programmer was going to be great.</p>

<p>But don’t let that distract you; this meant we could kill multiple more accurately people.</p>

<p>We’d been working with the project owner throughout this process.</p>

<p>He was laissez-faire about most things. He might check in once a day then going back to his main job in the area of the building dedicated to classified work.</p>

<p>Whenever we hit one of these milestones, we’d tell him. He’d be happy about it, but a question always came up. He wanted it to sniff for the signals put out by phones in addition to WiFi hotspots. This is a much harder problem from a technical perspective. The functionality necessary to do this is “promiscuous mode”, a setting on the wireless network controller. Neither iPhone nor Android supported that option. We’d need to jailbreak or root the phone regardless of the platform. We looked for packages that we could use that would let us do this to “sniff” the packets that devices sent back to routers. The closest we ever found was a SourceForge project that seemed promising. We didn’t fully understand its use and it wasn’t well documented.</p>

<p>We told the project owner that we’d get to it later. None of us thought it was that important: we had the technology to find WiFi Access Points working. That was the goal right? Each time we’d demonstrate the new exciting tech we’d built though, the same question came up.</p>

<ul>
  <li>We got WiFi hotspots located! Great, does it find phones?</li>
  <li>It’s taking seconds instead of minutes! Great, does it find phones?</li>
  <li>We looked into it finding phones, it seems unlikely but maybe! Ok, we’ll come back to it.</li>
  <li>We got moving targets working! Great, does it find phones?</li>
</ul>

<p>I had been distracted.</p>

<p>All of the cool problems we were solving: finding nodes, speeding things up, making more accurate predictions. It was all so cool, so much fun. I hadn’t thought about why we were putting all this work into finding a better place to sit and get good WiFi. That doesn’t even make sense if you look at it for more than a few seconds.</p>

<p><em>Does it find phones.</em></p>

<p>This was never about finding better WiFi. We were always finding phones. Phones carried by people. Remember I said I was working for a Department of Defense contractor? The DoD is the military. I was building a tool for the military to find people based on where their phones where, and shoot them.</p>

<p>I tried to rationalize this then. The military is in place to protect Truth, Justice, and the American Way. But this was the same time that we found out the government had been spying on Americans in the US with drones. They’d also lent out that technology to federal, state, and local law enforcement agencies nearly 700 times to run missions. The military and government do things that I know I don’t agree with pretty often. I didn’t want to be a part of building something used to kill people, especially since I knew I’d never know who it was killing, let alone have a say.</p>

<p>I rationalize it now too. We were interns, and we didn’t even have clearance. The projects this company did for the government were classified Top Secret. I wasn’t allowed to know what they were. My code probably got thrown away and forgotten. Probably.</p>

<p>This was an extreme example of code used in a way that the creator did not intend it. The project owner conveniently left out its purpose was when explaining the goals. I conveniently didn’t focus too much on that part. It was great pay for me at the time. It was a great project. Maybe I just didn’t want to know what it would be used for. I got distracted.</p>

<p>There are other examples of when code is used in ways it wasn’t intended, and of code that does bad things.</p>

<p>A year and a day ago, a developer named Bill Sourour <a href="https://medium.freecodecamp.org/the-code-im-still-ashamed-of-e4c021dff55e" title="The Code I'm Still Ashamed Of">wrote a blog post</a>. It opened with the line: “If you write code for a living, there’s a chance that at some point in your career, someone will ask you to code something a little deceitful – if not outright unethical.”</p>

<p>Bill had been asked to create a quiz that would almost always give a result that benefitted his client. Bill worked in Canada, and in Canada there are laws in place that limit how pharmaceutical companies can advertise prescription drugs to customers. Anyone could learn about the general symptoms a given drug addressed, but only patients with prescriptions could get specific information about the drug.</p>

<p>Because of this law, the quiz was posing as a general information site and not an advertisement for a specific drug. If the user didn’t answer that either they were allergic to the drug or already taking it, every quiz result suggested this specific drug. That’s what the requirements said to do, and that’s what Bill coded up.</p>

<p>The project manager did a quick test before submitting the website to the client. She told Bill that the quiz was broken: it always had the same answer. “Those were the requirements,” Bill responded. “Oh. Ok.”</p>

<p>A little while later, Bill got an email from a colleague that had a link to a news article. A young woman had taken the drug that Bill had built this quiz for. She had killed herself. It turns out that one of the main side effects of the drug were severe depression and suicidal thoughts.</p>

<p>Nothing Bill did was illegal. Like me, Bill was a young developer making great money. The purpose of the site was to push a particular drug - that’s why it was being built. He chalked it up to marketing. He never intended for this to happen. Maybe Bill got distracted too.</p>

<p>As his conclusion, Bill writes:</p>

<figure>
  <blockquote>
  <p>As developers, we are often one of the last lines of defense against potentially dangerous and unethical practices.</p>
  <p>We’re approaching a time where software will drive the vehicle that transports your family to soccer practice. There are already AI programs that help doctors diagnose disease. It’s not hard to imagine them recommending prescription drugs soon, too.</p>
  <p>The more software continues to take over every aspect of our lives, the more important it will be for us to take a stand and ensure that our ethics are ever-present in our code.</p>
  <p>Since that day, I always try to think twice about the effects of my code before I write it. I hope that you will too.</p>
  </blockquote>
  </figure>

<p>Bill’s story isn’t that far off from mine, but there are still other examples.</p>

<p>Earlier this year, a story came out that Uber had built into its ridesharing app code they call “greyball”. It’s a feature of their VTOS (or violation of terms of service) tool that can populate the screen with fake cars when the app is opened by users in violation of the terms of service.</p>

<p>In a statement, Uber said, “This program denies ride requests to users who are violating our terms of service — whether that’s people aiming to physically harm drivers, competitors looking to disrupt our operations, or opponents who collude with officials on secret ‘stings’ meant to entrap drivers.”</p>

<p>In practice, as <a href="https://www.nytimes.com/2017/03/03/technology/uber-greyball-program-evade-authorities.html" title="How Uber Deceives the Authorities Worldwide">The New York Times reports</a>, it was used in Portland to avoid code enforcement officers working to build a case against Uber for operating without a license. When triggered by Uber’s logic, it populates the app with cars that don’t exist, with fake drivers who quickly cancel after accepting a ride.</p>

<p>I am not a lawyer, but it seems like this is likely an obstruction of justice, itself a crime outside of Uber’s unlawful operations in Portland. Greyball is used even today, though mostly outside the United States. I’m a huge fan of ridesharing - though I use a competitor in Austin and Boston called Fasten<sup id="fnref:fasten" role="doc-noteref"><a href="#fn:fasten" rel="footnote">1</a></sup> rather than the much larger Uber or Lyft. But it’s not uncommon to see in the news these days articles about heinous things these drivers are doing. Greyball may have enabled some of those.</p>

<p>Again, it’s an unintended consequence of a tool built. Maybe the greyball internal pitch was to “greyball” users who were in violation of the terms of service. People who were under 18, or who didn’t pay to clean up their late night explosive accidents one too many times for example. Rather than block them, probably causing them to create a new account, they could be put into an alternate dimension where for some reason they just couldn’t ever get a ride. That’s fine, right?</p>

<p>If these developers had thought about the worst possible case for how this could be used, maybe obstruction of an investigation into Uber’s shady dealings would have come up in that conversation and it could have been addressed early on. Maybe they were distracted by the face value of the request from looking deeper at the purpose and uses.</p>

<p>There’s all sorts of things as well that aren’t as black and white (if you’ll excuse the pun). <a href="http://www.deidrariggs.com/2016/11/30/is-instagram-listening-in-on-you/" title="Is Instagram Listening In On You?">Apps that always listen to the microphone to tailor ads to you based on what you say near your phone</a>, <a href="https://www.axios.com/sean-parker-unloads-on-facebook-2508036343.html" title="Sean Parker Unloads on Facebook">websites designed to exploit psychology to take up as much of your time and attention as possible</a>, and any number of apps that opt you into mailing lists when you sign up or purchase something. These aren’t nearly as obviously bad, but at least in my opinion they’re still kind of shady.</p>

<p>This value system is different for others. We don’t always agree as individuals what is right and wrong, or even with what should be legal or illegal.</p>

<p>There are actually words for things that society decides are good or bad versus what you or I individually believe: ethics and morals. While modern philosophy more or less uses these terms interchangeably, a common understanding at least between us will be important later.</p>

<p>Ethics are imposed by an outside group. A society, a profession, a community such as ours or even where you live. Religions provide ethical systems, as do groups of friends. Societies in whatever form define right and wrong, good and bad, and imposes those on its members. Ethics in societies such as local, state, and national groups are often, but not always, coded into laws.</p>

<p>Morals are a more personal version of the same thing. Society as a whole imposes its mores on smaller communities, and all of that trickles down to the individual level. That’s not to say that your morals can’t conflict with the ethics of society. For example, you might think that freedom of speech is a basic human right, but live somewhere that defacing religious or political objects is considered wrong.</p>

<p>Let’s not get distracted by morals and ethics yet, though. We’ll come back to them.</p>

<p>The unifying factor in all of the stories I’ve told is that a developer wrote the code that did these unethical or immoral things. As a profession, we have a superpower: we can make computers do things. We build tools, and ultimately some responsibility lies with us to think through how those tools will be used. Not just what their intention is, but also what misuses might come out of them. None of us wants to build things that will be used for evil.</p>

<p>The Association for Computing Machinery is a society dedicated to advancing computing as a science &amp; profession. ACM includes this in their <a href="https://www.acm.org/about-acm/acm-code-of-ethics-and-professional-conduct#imp1.2">Code of Ethics and Professional Conduct</a>:</p>

<figure>
  <blockquote><p>
  Well-intended actions, including those that accomplish assigned duties, may lead to harm unexpectedly. In such an event the responsible person or persons are obligated to undo or mitigate the negative consequences as much as possible. One way to avoid unintentional harm is to carefully consider potential impacts on all those affected by decisions made during design and implementation.</p></blockquote></figure>

<p>So how can we “carefully consider potential impacts”? Honestly, I don’t have any answers to this. I don’t think that there really is a universal answer yet, because if we had it I have to believe we’d not be building these dangerous pieces of software.</p>

<p>I do have a couple of ideas though. One I got from my friend Schneems is to add to the planning process a step where we come up with the worst possible uses of our software. In opting in folks to an email list by default, the worst case might be that we send them a bunch of unwanted email and they unsubscribe. Maybe they even stop being a customer. As Schneems said: “Am I willing to sell my hypothetical startup’s soul for a bigger mailing list, when that might be all that keeps the company afloat? Yeah, no problem.” That makes sense to me. I don’t think it’s the best practice, but in the end it’s not physically hurting anyone. If I had sat down and thought about what the WiFi location app could be used for in the worst case, I would have come to a very different conclusion.</p>

<p>Actually, thinking about the worst possible uses of code could probably be a fun exercise. You might come up with some pretty wacky examples like “If we send Batman an email and he happens to have notifications on his iPhone for new emails, he might be looking at the notification when the Riddler drives by in the Riddler Car and he might not catch him before he gets off his witty one liner at the crime scene. Riddle me this, riddle me that, who’s afraid of the big, black bat?.” This isn’t so plausible, but it shows that these exercises can go down all sorts of different paths that aren’t obvious at a glance.</p>

<p>Another, the thing that I think I should have done, and that we can all do more of, is to simply not take requests at face value. The project owner at the Defense contractor I worked at didn’t spell out what the reason for the code was. But at least in retrospect, it wasn’t a big leap of logic. “We’re going to build an app to find WiFi signals” is all true, but it’s not the whole truth. Asking them, or myself, “why” enough times probably would have led me to a much earlier understanding. Why? To find the sources. Why? To go to them. Why? Why? Why?</p>

<p>Comedian Kumail Nanjiani, best known for the TV show Silicon Valley and his recent film The Big Sick, took to Twitter recently on this subject.</p>

<figure>
  <blockquote>
  <p>I know there's a lot of scary stuff in the world right now, but this is something I've been thinking about that I can't get out of my head.</p>
  <p>As a cast member on a show about tech, our job entails visiting tech companies, conferences, etc. We meet people eager to show off new tech.</p>
  <p>Often we'll see tech that is scary. I don't mean weapons. I mean altering video, tech that violates privacy, stuff with obvious ethical issues.</p>
  <p>And we'll bring up our concerns to them. We are realizing that ZERO consideration seems to be given to the ethical implications of tech.</p>
  <p>They don't even have a pat rehearsed answer. They are shocked at being asked. </p><p>Which means nobody is asking those questions.</p>
  <p>"We're not making it for that reason but the way people choose to use it isn't our fault. Safeguards will develop." But tech is moving so fast.</p>
  <p>That there is no way humanity or laws can keep up. We don't even know how to deal with open death threats online.</p>
  <p>Only "Can we do this?" Never "should we do this?" We've seen that  same blasé attitude in how Twitter or Facebook deal with abuse and fake news.</p>

  <p>Tech has the capacity to destroy us. We see the negative effect of  social media. No ethical considerations are going into dev of  tech.</p>

  <p>You can't put this stuff back in the box. Once it's out there, it's out there. And there are no guardians. It's terrifying. The end.</p>
  </blockquote>
  <figcaption>
  Kumail Nanjiani <a href="https://twitter.com/i/moments/930118384225800192?ref_src=twsrc%5Etfw">November 1, 2017</a>
  </figcaption>
  </figure>

<p>It’s a major problem when we’re given so much power in tech, but we’re not doing anything to ensure that we use it safely. Thinking about what we’re doing and being careful not to build things that can be used maliciously is really important.</p>

<p>Make your own decisions. Make your own choices. Make your own judgement.</p>

<figure>
  <blockquote>
  <p>For engineers in particular, we develop systems. But the systems we develop can be used for different things. The software that I was using in Iraq is the same you’d use in marketing. It’s the same tools. It’s the same analysis.</p>
  <p>I guess technologists should realize that we have an ethical obligation to make decisions that go beyond just meeting deadlines for creating a product. Let’s actually take some chunks of time time and think ‘what are the consequences of this system? How can this be used? How can it be misused?’ Let’s try to figure out how we can mitigate a software system from being misused, or decide whether or not you want to implement it at all. There are systems that if misused can be very dangerous.</p>
  </blockquote>
  <figcaption>
  Chelsea Manning in
  <cite>
  <a href="https://www.wnyc.org/story/chelsea-manning-life-after-prison/" title="Chelsea Manning on Life After Prison fron The New Yorker Radio Hour">
  The New Yorker Radio Hour
</a> (31:55-33:40)
  </cite>
  </figcaption>
  </figure>

<p>Don’t get distracted by deadlines and feature requests. Think about the consequences of what you’re building. Build in safeguards to prevent misuse, or don’t build it at all because it’s too dangerous.</p>

<p>I’m asking you to do something about this. Well, I guess I’m asking you to not do something because of this. It’s only fair that we talk a bit about how and when to take a stand.</p>

<p>Let’s say I had a time machine and could go back in time to 2011 and do it all over again. I already have the foreknowledge that this tool is unethical. I’ve accepted this job. I’ve moved across the country from Tempe, Arizona to Brookville, Maryland. I’ve driven the two hour commute to Sterling, Virginia, home of <a href="https://www.governmentcontractswon.com/department/defense/sterling_va_virginia.asp">395 defense contractors awarded 16.8 trillion dollars in contracts over the past 16 years</a>. It’s my first job out of school, and it’s my first day. I don’t have my clearance so I’m an intern. My new project owner introduces me to the team then pulls me into a side room to give me an overview of the project. What do I say?</p>

<p>I think the first thing is to establish a mutual understanding of the task. It’s entirely possible at this point that I don’t understand what the actual thing is, and that I’m overreacting. I ask “Why are we finding these signals” and the project owner says “We want to find people’s cell phones.” “Who’s finding them, and why?” I ask. “I don’t know, probably some soldiers in the Middle East.” “Why?” I repeat. “I can’t tell you that.”</p>

<p>“I can’t tell you that” is something I got a lot from this project owner. It’s code for “I have clearance and I know things about this project. I know what you’re asking and I know the answer but I am not allowed to tell you.”</p>

<p>At this point, I think we have a mutual understanding. The task is to help soldiers find people’s phones, probably attached to those people. The reason is left unsaid but we both know.</p>

<p>This organization is a defense contractor. They build things for the military. It is their core competency. They’re not not going to do this…. On the other hand, I care a lot about not killing people. The company’s goal is to build things for the military. If my goal is not to let this happen, then there isn’t a good fit for me at this company. This probably means that the worst case here is that I’m going to leave today without a job. Either I’ll say no and they’ll fire me, or I’ll say “that’s not something I’m comfortable with, best of luck” and quit. These are the worst case scenarios, not necessarily what will happen.</p>

<p>Before saying no then, I need to consider: Can I afford to leave here without a job financially? Am I likely to be able to rely on my network to get me another job? Have I built up a trust with my employer where I can go to them with this type of thing and feel confident that I’ll be heard out? The answer to these questions was no for me in 2011. Sometimes, something is important enough that you should still do something, but there’s a lot that goes into these decisions. I’d like to think that I would still say no.</p>

<p>Let’s look at another situation, where someone did the ethical thing. A developer we’ll call Alice received a strange request. We want to identify weak passwords in the system to notify users to change them. We’d like you to run a password cracking program on the very, very, large password database.</p>

<p>This was a long time ago, before aged passwords were common. Expiring old passwords wasn’t a straightforward option. Alice thought this was a weird request, but said that if the appropriate paperwork was completed she would be willing.</p>

<p>Alice received the completed paperwork and ran the password crack. The next request was “We’d like the list of users along with their weak passwords”. Alice knew that her coworkers had a valid desire to help customers improve their passwords. She also knew that users often re-used passwords. Combining the email and password into one report could allow someone to log into the customers’ accounts on other websites.</p>

<p>Alice pointed this out to her manager, and together they worked with the CSA team to design an email that didn’t include the password. Customers received notifications about their weak passwords, and there was less risk of the report falling into malicious hands. No one was fired and Alice built up trust within her team.</p>

<p>Different scenarios need different ways of analyzing what you should do. In some cases,  the right thing to do say nothing and build the product. It isn’t a simple thing to make this decision.</p>

<p>But don’t get distracted by having to think through it. Sometimes your code can kill people.</p>

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><a href="https://www.theatlantic.com/video/index/541797/anil-dash-tech-ethics">Does Technology Need to Be Ethical?</a>
Anil Dash briefly talks to The Atlantic about tech ethics.</li>
  <li><a href="http://www.bbc.com/news/technology-35639549">Is your smartphone listening to you?</a>
The BBC addresses whether tech companies use your phone to listen to what you’re saying while not using their apps, and if they even can.</li>
  <li><a href="http://www.hcpro.com/HOM-236942-5728/know-your-ethical-obligations-regarding-coding-and-documentation">Know your ethical obligations regarding coding and documentation</a>
A blog post on how to define your ethical obligations as a programmer, some ways of dealing with them, and some real world examples</li>
  <li><a href="https://www.nytimes.com/2018/04/04/technology/google-letter-ceo-pentagon-project.html">‘The Business of War’: Google Employees Protest Work for the Pentagon</a>
Google employees ask Google’s CEO not to build software to help the military build warfare technology</li>
  <li><a href="https://www.theverge.com/2015/10/8/9481651/volkswagen-congressional-hearing-diesel-scandal-fault">Volkswagen America’s CEO blames software engineers for emissions cheating scandal</a>
VW’s CEO blames software engineers for changing how diesel emissions are reported during tests. Even if they were doing what they were told, someone up the chain can throw the blame back onto the programmers.</li>
</ul>

<h2 id="glossary-of-terms">Glossary of Terms</h2>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Kernel_density_estimation#A_rule-of-thumb_bandwidth_estimator">Gaussian Estimation</a>
Used in the WiFi geolocation algorithm to estimate free space path loss using probability density estimation. It said “it’s less likely to be nearby if you’re looking for it and more likely to be further out”.</li>
  <li><a href="https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3">Genetic Algorithm</a>
A type of machine learning or searching that is inspired by the theory of evolution. It uses randomization to create populations of individuals and tests their fitness to determine which ones will reproduce. It was used to increase accuracy in the geolocation algorithm.</li>
  <li><a href="https://heroku.com/">Heroku</a>
A cloud platform as a service (PaaS) supporting several programming languages, that is used as a web application deployment model. It supports Java, Node.js, Scala, Clojure, Python, PHP, and Go.</li>
  <li><a href="https://schneems.com/2017/06/12/bayes-is-bae/">Kalman Filter</a>
A Kalman Filter can be used any time you have a model of motion and some noisy data that you want to produce a more accurate prediction.</li>
  <li><a href="https://stackoverflow.com/a/1988826/218211">Memoization</a>
Remembering the result of a calculation based on its arguments, and then using that result instead of re-calculating if the same method call is made with the same arguments. Caleb’s team did this with a hash/dictionary/map (different names for the same thing) that contained the location pairs as keys and the distance between them as values.</li>
  <li><a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">R2 Algorithm</a>
Used in the WiFi geolocation algorithm to measure the difference between expected and actual signal strength for each point in a search grid. The smallest difference is the most likely to be the correct distance from the source.</li>
</ul>


    
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Django and Postgres for the Busy Rails Developer (148 pts)]]></title>
            <link>https://andyatkinson.com/django-python-postgres-busy-rails-developer</link>
            <guid>42388340</guid>
            <pubDate>Wed, 11 Dec 2024 14:56:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andyatkinson.com/django-python-postgres-busy-rails-developer">https://andyatkinson.com/django-python-postgres-busy-rails-developer</a>, See on <a href="https://news.ycombinator.com/item?id=42388340">Hacker News</a></p>
Couldn't get https://andyatkinson.com/django-python-postgres-busy-rails-developer: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Overweight overtakes tobacco smoking as the leading disease risk factor in 2024 (252 pts)]]></title>
            <link>https://www.scimex.org/newsfeed/being-overweight-overtakes-tobacco-smoking-as-the-leading-disease-risk-factor-in-2024</link>
            <guid>42388273</guid>
            <pubDate>Wed, 11 Dec 2024 14:51:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scimex.org/newsfeed/being-overweight-overtakes-tobacco-smoking-as-the-leading-disease-risk-factor-in-2024">https://www.scimex.org/newsfeed/being-overweight-overtakes-tobacco-smoking-as-the-leading-disease-risk-factor-in-2024</a>, See on <a href="https://news.ycombinator.com/item?id=42388273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Living with overweight or obesity has overtaken tobacco smoking as the leading risk factor contributing to disease burden in 2024, according to a new report from the Australian Institute of Health and Welfare. The Australian Burden of Disease Study 2024 estimates the millions of years of healthy life Australians lose because of injury, illness or premature death – measuring over 200 diseases and injuries. This report also provides estimates of how much of this disease burden can be attributed to 20 individual risk factors such as alcohol use, physical inactivity, poor diet, overweight or obesity and tobacco smoking. Overweight, including obesity, overtook tobacco use as the leading risk factor in 2024, driven by a substantial fall (41%) in the burden attributable to tobacco use since 2003.</p>
                </div><div>
                                            <h2>Media release</h2>
                    
<p><strong>From:</strong> Australian Institute of Health and Welfare (AIHW)

                                        
                    </p><p><strong>Living with overweight or obesity overtakes tobacco as the new leading risk factor contributing to burden of disease</strong></p><p>Living with overweight or obesity has overtaken tobacco smoking as the leading risk factor contributing to disease burden in 2024.</p><p>The Australian Burden of Disease Study 2024, released today by the Australian Institute of Health and Welfare, estimates the millions of years of healthy life Australians lose because of injury, illness or premature death – measuring over 200 diseases and injuries.</p><p>This report also provides estimates of how much of this disease burden can be attributed to 20 individual risk factors such as alcohol use, physical inactivity, poor diet, overweight or obesity and tobacco smoking.</p><p>‘Australians lost an estimated 5.8 million years of healthy life due to living with disease and dying prematurely in 2024,’ said AIHW spokesperson Ms Michelle Gourley.</p><p>‘Over one-third of the total burden of disease and injury in Australia in 2024 could have been avoided or reduced due to modifiable risk factors included in the study.</p><p>‘Overweight, including obesity, overtook tobacco use as the leading risk factor in 2024, driven by a substantial fall (41%) in the age-standardised rate of total burden attributable to tobacco use since 2003.</p><p>‘This fall is likely due to declines in smoking prevalence and burden rates from some of the major linked diseases, such as lung cancer and chronic obstructive pulmonary disease (COPD).’</p><p>An estimated 8.3% of total disease burden in 2024 was due to overweight (including obesity) and 7.6% was due to tobacco use (excluding vaping). This was followed by dietary risks (4.8%) and high blood pressure (4.4%).</p><p>Alcohol use and illicit drug use were the leading risk factors contributing to disease burden for young males aged 15–24, while child abuse and neglect was the leading risk factor contributing to burden for young females of the same age.</p><p>When looking at rates of total disease burden, there was a 10% decrease between 2003 and 2024 after adjusting for population ageing. This decrease was driven by a 26% decrease in the rate of fatal burden, as the non-fatal burden rate increased by 7%.</p><p>‘While Australians are living longer on average, years lived in ill health are also growing, resulting in little change in the proportion of life spent in full health. This contributes to the growing demand and pressures on the health system and services,’ said Ms Gourley.</p><p>In line with previous years, cancer was the leading group of diseases causing burden in 2024 (16.4%), with 91.3% of this burden fatal and 8.7% non-fatal.</p><p>The leading specific causes of disease burden were coronary heart disease (5.5%), dementia (4.5%), back pain and problems (4.3%), anxiety disorders (3.9%) and COPD (3.7%).</p><p>Males experienced more total disease burden than females across all age groups, driven by males having higher rates of fatal burden. The leading individual causes of burden also differed between males and females. Coronary heart disease was the leading cause of burden among males, and dementia was the leading cause of burden among females.</p><p>For young people, mental health conditions and suicide and self-inflicted injuries were the leading contributors of disease burden.</p><p>For males aged 15–24, suicide and self-inflicted injuries caused the most burden (12%), followed by anxiety disorders (10%) and depression (7%). For young females of the same age, the leading causes of burden were anxiety disorders (17%), depression (12%) and eating disorders (7%).</p><p>In the 5–14 age group, the leading causes of disease burden were autism spectrum disorders and asthma for males, and asthma and anxiety disorders for females.</p><p>‘Burden of disease is the gold standard approach for measuring the impact of illness, injury and death, and this information provides an important evidence base to inform health policy and service planning,’ said Ms Gourley.</p>

                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The GPU is not always faster (106 pts)]]></title>
            <link>https://cowfreedom.de/#dot_product/introduction/</link>
            <guid>42388009</guid>
            <pubDate>Wed, 11 Dec 2024 14:28:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cowfreedom.de/#dot_product/introduction/">https://cowfreedom.de/#dot_product/introduction/</a>, See on <a href="https://news.ycombinator.com/item?id=42388009">Hacker News</a></p>
Couldn't get https://cowfreedom.de/#dot_product/introduction/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The PayPal Mafia is taking over America's government (140 pts)]]></title>
            <link>https://www.economist.com/business/2024/12/10/the-paypal-mafia-is-taking-over-americas-government</link>
            <guid>42387549</guid>
            <pubDate>Wed, 11 Dec 2024 13:38:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/business/2024/12/10/the-paypal-mafia-is-taking-over-americas-government">https://www.economist.com/business/2024/12/10/the-paypal-mafia-is-taking-over-americas-government</a>, See on <a href="https://news.ycombinator.com/item?id=42387549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main role="main" id="content"><article data-test-id="Article" id="new-article-template"><div data-test-id="standard-article-template"><section><p><span><a href="https://www.economist.com/business" data-analytics="sidebar:section"><span>Business</span></a></span><span> | <!-- -->All-in on Donald Trump</span></p><h2>America’s right-wing tech bros are celebrating Donald Trump’s victory</h2></section><section><figure><img alt="Palace of Fine Arts at night with the Golden Gate Bridge in the background, San Francisco, California, USA. " fetchpriority="high" width="1280" height="720" decoding="async" data-nimg="1" sizes="(min-width: 960px) 700px, 95vw" srcset="https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg"><figcaption><span>Photograph: Getty Images</span></figcaption></figure></section><div><div><p><time datetime="2024-12-10T17:55:20.458Z"> <!-- -->Dec 10th 2024</time><span>|</span><span>SAN FRANCISCO</span></p></div><section data-body-id="cp2"><p data-component="paragraph"><span data-caps="initial">O</span><small>n the night</small> of December 7th San Francisco’s Palace of Fine Arts, with its lakeside colonnade echoing a Roman ruin, turned into Mar-a-Lago, as <a href="https://www.economist.com/united-states/2024/11/21/how-donald-trump-could-win-the-future">Silicon Valley’s newly emboldened right-wingers</a> gathered for a Christmas bash organised by the All-In podcast. The festive good cheer did not extend to everyone;&nbsp;<i>The Economist</i> was made to feel most unwelcome. But not before being privy to a riotous celebration of how a clique of billionaires—the so-called PayPal Mafia—helped clinch Donald Trump’s election victory and has taken Washington by storm.</p></section><p><h3 id="article-tags">Explore more</h3><nav aria-labelledby="article-tags"><a href="https://www.economist.com/topics/elon-musk" data-analytics="tags:elon_musk"><span>Elon Musk</span></a><a href="https://www.economist.com/topics/donald-trump" data-analytics="tags:donald_trump"><span>Donald Trump</span></a><a href="https://www.economist.com/topics/united-states" data-analytics="tags:united_states"><span>United States</span></a></nav></p></div></div></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making memcpy(NULL, NULL, 0) well-defined (193 pts)]]></title>
            <link>https://developers.redhat.com/articles/2024/12/11/making-memcpynull-null-0-well-defined</link>
            <guid>42387013</guid>
            <pubDate>Wed, 11 Dec 2024 12:19:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.redhat.com/articles/2024/12/11/making-memcpynull-null-0-well-defined">https://developers.redhat.com/articles/2024/12/11/making-memcpynull-null-0-well-defined</a>, See on <a href="https://news.ycombinator.com/item?id=42387013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>Undefined behavior (UB) in the C programming language is a regular source of heated discussions among programmers. On the one hand, UB can be important for compiler optimizations. On the other hand, it makes is easy to introduce bugs that lead to security issues.</p><p>The good news is that <a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3322.pdf">N3322</a> has been accepted for C2y, which will remove undefined behavior from one particular corner of the C language, making all of the following well-defined:</p><pre><code>memcpy(NULL, NULL, 0);
memcmp(NULL, NULL, 0);
(int *)NULL + 0;
(int *)NULL - 0;
(int *)NULL - (int *)NULL;</code></pre><p>This only applies when a null pointer is combined with a "zero-length" operation. The following are still undefined:</p><pre><code>memcpy(NULL, NULL, 4);
(int *)NULL + 4;</code></pre><p>The removal of this undefined behavior is not expected to have any negative impact on performance. In fact, the reverse is true.</p><h2>Motivation</h2><p>The examples above are somewhat silly because they hard-code a <code>NULL</code>/<code>nullptr</code> constant. However, it is easy to run into this situation with a pointer that is only sometimes null. For example, consider a typical representation for a string with a known length:</p><pre><code>struct str {
   char *data;
   size_t len;
};</code></pre><p>An empty string would usually be represented as <code>(struct str) { .data = NULL, .len = 0 }</code>, with the <code>data</code> pointer being <code>NULL</code>. Now, consider a function that checks if two strings are equal:</p><pre><code>bool str_eq(const struct str *str1, const struct str *str2) {
   return str1-&gt;len == str2-&gt;len &amp;&amp;
          memcmp(str1-&gt;data, str2-&gt;data, str1-&gt;len) == 0;
}</code></pre><p>This implementation looks very reasonable at first glance. However, it exhibits undefined behavior if both of the inputs are empty strings. In that case, we will call <code>memcmp(NULL, NULL, 0)</code>, which is undefined behavior according to the C standard.</p><p>This kind of UB introduces the risk that the compiler will optimize away following null pointer checks. For example, GCC will happily remove the <code>dest == NULL</code> branch in the following code, while Clang deliberately does not perform this optimization:</p><pre><code>int test(char *dest, const char *src, size_t len) {
   memcpy(dest, src, len);
   if (dest == NULL) {
       // This branch will be removed by GCC due to undefined behavior.
   }
}</code></pre><p>The correct way to write the <code>str_eq</code> function is as follows:</p><pre><code>bool str_eq(const struct str *str1, const struct str *str2) {
   return str1-&gt;len == str2-&gt;len &amp;&amp;
          (str1-&gt;len == 0 ||
           memcmp(str1-&gt;data, str2-&gt;data, str1-&gt;len) == 0);
}</code></pre><p>The new code is correct, but worse in every other way:</p><ul><li>It increases code size, by requiring an extra check at each inlined call-site.</li><li>It decreases performance, by redundantly checking something <code>memcmp</code> has to handle anyway.</li><li>It increases code complexity.</li></ul><p>At the same time, there is no useful way in which the C library can make use of this undefined behavior to provide a more efficient implementation. This is the kind of UB that benefits nobody, and should be removed from the language.</p><h2>Null pointer arithmetic</h2><p>The original proposal was focused on removing UB for memory library calls, but an early reviewer pointed out that this is not sufficient. After all, we also need to take into account how these library functions are implemented.</p><p>For example, let's consider a typical implementation for a <code>memcpy</code>-like function:</p><pre><code>void copy(char *dst, const char *src, size_t n) {
   for (const char *end = src + n; src &lt; end; src++) {
       *dst++ = *src;
   }
}</code></pre><p>This function exhibits undefined behavior when called as <code>copy(NULL, NULL, 0)</code>, because <code>NULL + 0</code> is undefined behavior in C.</p><p>To avoid this, and make the overall language self-consistent, we need to define <code>NULL + 0</code> as returning <code>NULL</code> and <code>NULL - NULL</code> as returning 0. This also aligns C with C++ semantics, where this was already well-defined.</p><h2>Opposition</h2><p>When this proposal was discussed at two WG14 meetings, the opposition didn't come from the direction I expected.</p><p>The most broadly controversial part of the proposal was to define <code>NULL - NULL</code> as returning 0. The reason for this is that when address spaces get involved (which are not part of standard C, but may be implemented as an extension), there may be multiple representations of a null pointer. Making sure that subtracting two "different" nulls still results in zero might require the generation of additional code, breaking the premise that this change is entirely free.</p><p>However, the most vocal opposition came from a static analysis perspective: Making null pointers well-defined for zero length means that static analyzers can no longer unconditionally report <code>NULL</code> being passed to functions like <code>memcpy</code>—they also need to take the length into account now. If an <a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3089.pdf"><code>_Optional</code> qualifier</a> is introduced in the future, <code>memcpy</code> arguments would have to be qualified with it. GCC is considering the introduction of a <a href="https://gcc.gnu.org/pipermail/gcc-patches/2024-November/668505.html"><code>nonnull_if_nonzero</code></a> attribute to represent the new pre-condition.</p><p>After the seemingly negative discussion, I was somewhat surprised that the vote not only went strongly in favor of the change, but also came with a recommendation to implementers to apply the change <a href="https://www.open-std.org/jtc1/sc22/wg14/www/previous.html">retroactively</a> to old standard versions. This means that, once compilers and C libraries have implemented the change, it should apply even without specifying the <code>-std=c2y</code> flag.</p><h2>Compiler builtins</h2><p>I work on the middle-end of the <a href="https://llvm.org/">LLVM</a> compiler toolchain. Being far removed from any "user-facing" parts of the compiler, I am generally not involved with standardization efforts.</p><p>The reason I got involved here at all is the specification for LLVM's internal memcpy intrinsic:</p><blockquote><div><p>The <code>llvm.memcpy.*</code> intrinsics copy a block of memory from the source location to the destination location, which must either be equal or non-overlapping. [...]</p><p>If <code>&lt;len&gt;</code> is 0, it is no-op modulo the behavior of attributes attached to the arguments. [...]</p></div></blockquote><p>The <code>llvm.memcpy</code> intrinsic may lower to a call to the <code>memcpy</code> function, which is treated as a "compiler runtime builtin" here, even though it is ultimately also provided by the C library.</p><p>When used as a builtin, LLVM requires that both <code>memcpy(x, x, s)</code> and <code>memcpy(NULL, NULL, 0)</code> are well-defined, even though the C standard says they are UB. GCC and MSVC have similar assumptions.</p><p>Making <code>memcpy(NULL, NULL, 0)</code> officially well-defined removes one of the assumptions, while the <code>memcpy(x, x, s)</code> case remains for now. Allowing this was originally also part of the proposal, but was later dropped, because it didn't fit well with the other changes.</p><p>In a weird turn of events, this change to the C standard came about because Rust developers kept nagging me about the mismatch between LLVM and C semantics.</p><h2>Acknowledgements</h2><p>This paper was a collaboration with Aaron Ballman, who also drove the discussion during the actual WG14 meetings. Special thanks go to David Stone, whose early feedback radically changed the direction of the proposal from memory library calls in particular to "zero-length" operations in general.</p>
          
                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Helium: Lighter Web Automation with Python (135 pts)]]></title>
            <link>https://github.com/mherrmann/helium</link>
            <guid>42386971</guid>
            <pubDate>Wed, 11 Dec 2024 12:11:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mherrmann/helium">https://github.com/mherrmann/helium</a>, See on <a href="https://news.ycombinator.com/item?id=42386971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Lighter web automation with Python</h2><a id="user-content-lighter-web-automation-with-python" aria-label="Permalink: Lighter web automation with Python" href="#lighter-web-automation-with-python"></a></p>
<p dir="auto">Helium is a Python library for automating browsers such as Chrome and Firefox.
For example:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mherrmann/helium/blob/master/docs/helium-demo.gif"><img src="https://github.com/mherrmann/helium/raw/master/docs/helium-demo.gif" alt="Helium Demo" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To get started with Helium, you need Python 3 and Chrome or Firefox.</p>
<p dir="auto">I would recommend creating a virtual environment. This lets you install Helium
for just your current project, instead of globally on your whole computer.</p>
<p dir="auto">To create and activate a virtual environment, type the following commands into
a command prompt window:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m venv venv
# On Mac/Linux:
source venv/bin/activate
# On Windows:
call venv\scripts\activate.bat"><pre>python3 -m venv venv
<span><span>#</span> On Mac/Linux:</span>
<span>source</span> venv/bin/activate
<span><span>#</span> On Windows:</span>
call venv<span>\s</span>cripts<span>\a</span>ctivate.bat</pre></div>
<p dir="auto">Then, you can install Helium with <code>pip</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m pip install helium"><pre>python -m pip install helium</pre></div>
<p dir="auto">Now enter <code>python</code> into the command prompt and (for instance) the commands in
the animation at the top of this page (<code>from helium import *</code>, ...).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Your first script</h2><a id="user-content-your-first-script" aria-label="Permalink: Your first script" href="#your-first-script"></a></p>
<p dir="auto">I've compiled a <a href="https://github.com/mherrmann/helium/blob/master/docs/cheatsheet.md">cheatsheet</a> that quickly teaches you all
you need to know to be productive with Helium.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Connection to Selenium</h2><a id="user-content-connection-to-selenium" aria-label="Permalink: Connection to Selenium" href="#connection-to-selenium"></a></p>
<p dir="auto">Under the hood, Helium forwards each call to Selenium. The difference is that
Helium's API is much more high-level. In Selenium, you need to use HTML IDs,
XPaths and CSS selectors to identify web page elements. Helium on the other hand
lets you refer to elements by user-visible labels. As a result, Helium scripts
are typically 30-50% shorter than similar Selenium scripts. What's more, they
are easier to read and more stable with respect to changes in the underlying web
page.</p>
<p dir="auto">Because Helium is simply a wrapper around Selenium, you can freely mix the two
libraries. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# A Helium function:
driver = start_chrome()
# A Selenium API:
driver.execute_script(&quot;alert('Hi!');&quot;)"><pre><span># A Helium function:</span>
<span>driver</span> <span>=</span> <span>start_chrome</span>()
<span># A Selenium API:</span>
<span>driver</span>.<span>execute_script</span>(<span>"alert('Hi!');"</span>)</pre></div>
<p dir="auto">So in other words, you don't lose anything by using Helium over pure Selenium.</p>
<p dir="auto">In addition to its more high-level API, Helium simplifies further tasks that are
traditionally painful in Selenium:</p>
<ul dir="auto">
<li><strong>iFrames:</strong> Unlike Selenium, Helium lets you interact with elements inside
nested iFrames, without having to first "switch to" the iFrame.</li>
<li><strong>Window management.</strong> Helium notices when popups open or close and focuses /
defocuses them like a user would. You can also easily switch to a window by
(parts of) its title. No more having to iterate over Selenium window handles.</li>
<li><strong>Implicit waits.</strong> By default, if you try click on an element with Selenium
and that element is not yet present on the page, your script fails. Helium by
default waits up to 10 seconds for the element to appear.</li>
<li><strong>Explicit waits.</strong> Helium gives you a much nicer API for waiting for a
condition on the web page to become true. For example: To wait for an element
to appear in Selenium, you would write:
<div dir="auto" data-snippet-clipboard-copy-content="element = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.ID, &quot;myDynamicElement&quot;))
)"><pre><span>element</span> <span>=</span> <span>WebDriverWait</span>(<span>driver</span>, <span>10</span>).<span>until</span>(
    <span>EC</span>.<span>presence_of_element_located</span>((<span>By</span>.<span>ID</span>, <span>"myDynamicElement"</span>))
)</pre></div>
With Helium, you can write:
<div dir="auto" data-snippet-clipboard-copy-content="wait_until(Button('Download').exists)"><pre><span>wait_until</span>(<span>Button</span>(<span>'Download'</span>).<span>exists</span>)</pre></div>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status of this project</h2><a id="user-content-status-of-this-project" aria-label="Permalink: Status of this project" href="#status-of-this-project"></a></p>
<p dir="auto">I have too little spare time to maintain this project for free. If you'd like
my help, please go to my <a href="http://herrmann.io/" rel="nofollow">web site</a> to ask about my
consulting rates. Otherwise, unless it is very easy for me, I will usually not
respond to emails or issues on the issue tracker. I will however accept and
merge PRs. So if you add some functionality to Helium that may be useful for
others, do share it with us by creating a Pull Request. For instructions, please
see <a href="#Contributing">Contributing</a> below.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How you can help</h2><a id="user-content-how-you-can-help" aria-label="Permalink: How you can help" href="#how-you-can-help"></a></p>
<p dir="auto">I find Helium extremely useful in my own projects and feel it should be more
widely known. Here's how you can help with this:</p>
<ul dir="auto">
<li>Star this project on GitHub.</li>
<li>Tell your friends and colleagues about it.</li>
<li><a href="https://twitter.com/intent/tweet?text=I%20find%20Helium%20very%20useful%20for%20web%20automation%20with%20Python%3A%20https%3A//github.com/mherrmann/helium" rel="nofollow">Share it on Twitter with one click</a></li>
<li>Share it on other social media</li>
<li>Write a blog post about Helium.</li>
</ul>
<p dir="auto">With this, I think we can eventually make Helium the de-facto standard for web
automation in Python.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Pull Requests are very welcome. Please follow the same coding conventions as the
rest of the code, in particular the use of tabs over spaces. Also, read through my
<a href="https://gist.github.com/mherrmann/5ce21814789152c17abd91c0b3eaadca">PR guidelines</a>.
Doing this will save you (and me) unnecessary effort.</p>
<p dir="auto">Before you submit a PR, ensure that the tests still work:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -Ur requirements/test.txt
python setup.py test"><pre>pip install -Ur requirements/test.txt
python setup.py <span>test</span></pre></div>
<p dir="auto">This runs the tests against Chrome. To run them against Firefox, set the
environment variable <code>TEST_BROWSER</code> to <code>firefox</code>. Eg. on Mac/Linux:</p>
<div dir="auto" data-snippet-clipboard-copy-content="TEST_BROWSER=firefox python setup.py test"><pre>TEST_BROWSER=firefox python setup.py <span>test</span></pre></div>
<p dir="auto">On Windows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="set TEST_BROWSER=firefox
python setup.py test"><pre><span>set</span> TEST_BROWSER=firefox
python setup.py <span>test</span></pre></div>
<p dir="auto">If you do add new functionality, you should also add tests for it. Please see
the <a href="https://github.com/mherrmann/helium/blob/master/tests"><code>tests/</code></a> directory for what this might look like.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">History</h2><a id="user-content-history" aria-label="Permalink: History" href="#history"></a></p>
<p dir="auto">I (Michael Herrmann) originally developed Helium in 2013 for a Polish IT startup
called BugFree software. (It could be that you have seen Helium before at
<a href="https://heliumhq.com/" rel="nofollow">https://heliumhq.com</a>.) We shut down the company at the end of 2019 and I felt it
would be a shame if Helium simply disappeared from the face of the earth. So I
invested some time to modernize it and bring it into a state suitable for open
source.</p>
<p dir="auto">Helium used to be available for both Java and Python. But because I now only
use it from Python, I didn't have time to bring the Java implementation up to
speed as well. Similarly for Internet Explorer: Helium used to support it, but
since I have no need for it, I removed the (probably broken) old implementation.</p>
<p dir="auto">The name Helium was chosen because it is also a chemical element like Selenium,
but it is lighter.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>