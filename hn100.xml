<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 10 Jan 2025 10:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Visualizing All ISBNs (140 pts)]]></title>
            <link>https://annas-archive.org/blog/all-isbns.html</link>
            <guid>42652577</guid>
            <pubDate>Fri, 10 Jan 2025 04:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://annas-archive.org/blog/all-isbns.html">https://annas-archive.org/blog/all-isbns.html</a>, See on <a href="https://news.ycombinator.com/item?id=42652577">Hacker News</a></p>
Couldn't get https://annas-archive.org/blog/all-isbns.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Gleam v1.7 (128 pts)]]></title>
            <link>https://gleam.run/news/improved-performance-and-publishing/</link>
            <guid>42652329</guid>
            <pubDate>Fri, 10 Jan 2025 04:04:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gleam.run/news/improved-performance-and-publishing/">https://gleam.run/news/improved-performance-and-publishing/</a>, See on <a href="https://news.ycombinator.com/item?id=42652329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">
  <p>
    Published 05 Jan, 2025 by Louis Pilfold
  </p>
<p>Gleam is a type-safe and scalable language for the Erlang virtual machine and
JavaScript runtimes. Today Gleam <a href="https://github.com/gleam-lang/gleam/releases/tag/v1.7.0">v1.7.0</a> has been published, featuring
an array of wonderful improvements. Let’s take a look!</p>

<h2 id="faster-record-updates">Faster record updates</h2>

<p>Gleam is a language with immutable data, and it has a syntax for creating a new
record from an old one with some updated fields.</p>

<pre><code>/// Create a new version of the user with `admin` set to true.
pub fn make_admin(person: User) {
  User(..person, admin: True)
}
</code></pre>

<p>If you’re familiar with JavaScript this is similar to the object spread update
syntax, and similarly it is fast, only copying the references to the fields,
not the data itself.</p>

<p>The code that the Gleam compiler would generate would also be similar to how
JavaScript’s update works, using a small amount of dynamic code at runtime to
construct the new record with the new fields. This runtime conditional logic
had a small performance cost at runtime.</p>

<p>The compiler now instead <em>monomorphises</em> record updates, meaning it generates
exactly the most efficient code to construct the new record on a case-by-case
basis, removing the runtime conditional logic and its associated cost entirely.</p>

<p>The optimisation is for both the Erlang and the JavaScript targets, has no
additional compile speed cost or increase in code size, so it’s an improvement
in every way!</p>

<p>Another benefit of record update monomorphisation is that you can now change
the parameterised types of a generic record with the update syntax.</p>

<pre><code>pub type Named(element) {
  Named(name: String, value: element)
}

pub fn replace_value(data: Named(a), replacement: b) -&gt; Named(b) {
  Named(..data, value: replacement)
}
</code></pre>

<p>Previously this would not compile as the type parameter changed, and the
compiler wasn’t able to infer it was always done safely. Now it can tell, so
this compiles!</p>

<p>Thank you <a href="https://github.com/joshi-monster">yoshi</a> for this excellent change!</p>

<h2 id="generate-decoder-code-action">Generate decoder code action</h2>

<p>Gleam has a very robust type system, it won’t let you unsafely cast values
between different types. This results in a programming experience where the
compiler and language server can offer lots help to the programmer, especially
in unfamiliar or large codebases.</p>

<p>One drawback of this sound type system is that converting untyped input from
the outside world into data of known types requires some additional code which
would not be required in unsound systems. This decoder code can be unfamiliar
and confusing to those new to Gleam, and in simple cases it can seem a chore.</p>

<p>To aid with this the Gleam language server now includes code action to
generate a dynamic decoder for a custom type. For example, if you have this code:</p>

<pre><code>pub type Person {
  Person(name: String, age: Int)
}
</code></pre>

<p>If you place your cursor on the type header and select the code action in your
editor, then it’ll be updated to this:</p>

<pre><code>import gleam/dynamic/decode

pub type Person {
  Person(name: String, age: Int)
}

fn person_decoder() -&gt; decode.Decoder(Person) {
  use name &lt;- decode.field("name", decode.string)
  use age &lt;- decode.field("age", decode.int)
  decode.success(Person(name:, age:))
}
</code></pre>

<p>Thank you <a href="https://github.com/GearsDatapacks">Surya Rose</a>! I know this will be
a very popular addition.</p>

<h2 id="more-secure-package-manager-credential-handling">More secure package manager credential handling</h2>

<p>Gleam is part of the Erlang ecosystem, so it uses the <a href="https://hex.pm/">Hex package manager</a>.
To publish a package to Hex the build tool needs the credentials for your Hex
account, and you would type them into the command line to supply them.
We make this as secure as possible, but there’s always some risk when typing in
credentials. No amount of in-computer security can save you from someone
sitting behind you, watching your fingers on the keyboard.</p>

<p>Gleam now only asks for your Hex credentials once, and uses that to create a
long-lived API token, which will be stored on your filesystem and encrypted
using a local password of your choosing. For all future interactions with Hex
Gleam will ask for the local password, use that to decrypt the API key, and
then use it to authenticate with the Hex APIs.</p>

<p>With this if someone manages to learn the password you use to Hex they would
not be able to do anything with it unless they can also get access to your
computer and the encrypted file stored on it.</p>

<h2 id="package-namespace-checking">Package namespace checking</h2>

<p>The Erlang virtual machine has a single namespace for modules. It does not have
isolation of modules between different packages, so if two packages define
modules with the same name they can collide and cause a build failure or
other undesired behaviour.</p>

<p>To avoid this packages place their modules within their own namespace. For
example, if I am writing a package named <code>pumpkin</code> I would place my modules
within the <code>src/pumpkin/</code> directory.</p>

<p>Sometimes people from other ecosystems with per-package isolation may not
understand this convention and place all their code in the top-level namespace,
using generic names, which results in problems for any users of their package. To
avoid this the <code>gleam publish</code> command will now check for top-level namespace
pollution, explaining the problem and asking for confirmation if it is present.</p>

<p>Thank you <a href="https://github.com/guria">Aleksei Gurianov</a>!</p>

<h2 id="core-team-package-name-checking">Core team package name checking</h2>

<p>The Hex package manager system doesn’t have namespaces, so we can’t publish
packages maintained by the Gleam core team as <code>@gleam/*</code> or such. Instead Hex
users have to rely on adding a prefix to the names of their packages, and in
the Gleam core team we use the prefix <code>gleam_</code>.</p>

<p>These prefixes are unchecked, so one can use anyone else’s prefix without
issue. This is a problem for us as people occasionally publish packages using
the core team’s prefix, and then other people get confused as to why this
seemingly official package is of a low quality. To try and remedy this Gleam
will ask for confirmation when a package is published with the <code>gleam_</code> prefix.
Unfortunately this was not enough and another unofficial package was
accidentally published, so Gleam now asks for a much longer confirmation to be
typed in, to force the publisher to read the message.</p>

<h2 id="semantic-versioning-encouragement">Semantic versioning encouragement</h2>

<p>Sometimes people like to publish packages that are unfinished or unsuitable for
use by others, publishing them as version 0.*. Other people publish code that
is good to use, but shy away from semantic versioning and publish them as
v0.*. In both of these cases the users of these packages have an inferior
experience, unable to take advantage of the benefits that semantic versioning
is designed to bring, which can lead to irritating build errors.</p>

<p>Gleam will now ask for confirmation if a package is published with a v0.*
version, as it does not respect semantic versioning. The fewer zero-version
packages published the better experience users of the package ecosystem will
have.</p>

<h2 id="variant-deprecation">Variant deprecation</h2>

<p>In Gleam one can deprecate functions and types using the <code>@deprecated</code>
attribute, which causes the compiler to emit a warning if they are used. With
this release it is also possible to deprecate individual custom type variants
too!</p>

<pre><code>pub type HashAlgorithm {
  @deprecated("Please upgrade to another algorithm")
  Md5
  Sha224
  Sha512
}

pub fn hash_password(input: String) -&gt; String {
  hash(input:, algorithm: Md5) // Warning: Deprecated value used
}
</code></pre>

<p>Thank you <a href="https://github.com/wilbert-mad">Iesha</a> for this!</p>

<h2 id="canonical-documentation-links">Canonical documentation links</h2>

<p>When packages are published to Hex Gleam will also generate HTML documentation
and upload it to <a href="https://hexdocs.pm/">HexDocs</a>, the documentation hosting site
for the BEAM ecosystem.</p>

<p>Currently we have a problem where Google is returning documentation for very
old versions of Gleam libraries instead of the latest version, which can result
in confusion as people try to use functions that no longer exist, etc. To
prevent this from happening with future versions Gleam now adds a canonical
link when publishing, which should help search engines return the desired version.
In the near future we will write a tool that will update historical
documentation to add these links too.</p>

<p>Thank you <a href="https://github.com/rockerBOO">Dave Lage</a> for this improvement!</p>

<h2 id="custom-messages-for-pattern-assertions">Custom messages for pattern assertions</h2>

<p>Gleam’s <code>let assert</code> allows you to pattern match with a <em>partial pattern</em>, that
is: one that doesn’t match all possible values a type could be. When the value
does not match the program it crashes the program, which is most often used in
tests or in quick scripts or prototypes where one doesn’t care to implement
proper error handling.</p>

<p>With this version the <code>as</code> syntax can be used to add a custom error message for
the crash, which can be helpful for debugging when the unexpected does occur.</p>

<pre><code>let assert Ok(regex) = regex.compile("ab?c+") as "This regex is always valid"
</code></pre>

<p>Thank you <a href="https://github.com/GearsDatapacks">Surya Rose</a>!</p>

<h2 id="javascript-bit-array-compile-time-evaluation">JavaScript bit array compile time evaluation</h2>

<p>Gleam’s bit array literal syntax is a convenient way to build up and to pattern
match on binary data. When targeting JavaScript the compiler now generates
faster and smaller code for int values in these bit array expressions and
patterns by evaluating them at compile time where possible.</p>

<p>Thank you <a href="https://github.com/richard-viney">Richard Viney</a> for this!</p>

<h2 id="javascript-bit-array-slicing-optimisation">JavaScript bit array slicing optimisation</h2>

<p>Continuing on from his previous bit array optimisation, <a href="https://github.com/richard-viney">Richard Viney</a>
has made taking a sub-slice of a bit array a constant time operation on
JavaScript, to match the behaviour on the Erlang target. This is a significant
improvement to performance.</p>

<p>Thank you Richard! Our bit array magician!</p>

<h2 id="empty-blocks-are-valid">Empty blocks are valid!</h2>

<p>In Gleam one can write an empty function body, and it is considered a
not-yet-implemented function, emitting a warning when compiled. This is useful
for when writing new code, where you want to check some things about your
program but have not yet finished writing it entirely.</p>

<pre><code>pub fn wibble() {} // warning: unimplemented function
</code></pre>

<p>You can now also do the same for blocks, leaving them empty will result in a
compile warning but permit you to compile the rest of your code.</p>

<pre><code>pub fn wibble() {
  let x = {
     // warning: incomplete block
  }
  io.println(x)
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a>!</p>

<h2 id="external-modules-in-subdirectories">External modules in subdirectories</h2>

<p>Gleam has excellent interop with Erlang, Elixir, JavaScript, and other
languages running on its target platforms. Modules in these languages can be
added to your project and imported using Gleam’s <a href="https://tour.gleam.run/advanced-features/externals/">external functions</a>
feature.</p>

<p>Previously these external modules had to be at the top level of the <code>src/</code> or
<code>test/</code> directories, but now they can reside within subdirectories of them too.</p>

<p>Thank you <a href="https://github.com/PgBiel">PgBiel</a> for this long-awaited feature!</p>

<h2 id="installation-hints">Installation hints</h2>

<p>To run Gleam on the BEAM an Erlang installation is required, and to run it on
JavaScript a suitable runtime such as NodeJS is required. To initialise a
repository git is required. To compile Elixir code Elixir must be installed.
You get the idea- to use various external tools they need to be installed.</p>

<p>If there’s a particular recommended way to install a missing component for your
operating system the error message for its absence will now direct you to
install it that way.</p>

<pre><code>error: Shell command failed

The program `elixir` was not found. Is it installed?

You can install Elixir via homebrew: brew install elixir

Documentation for installing Elixir can be viewed here:
https://elixir-lang.org/install.html
</code></pre>

<p>Thank you <a href="https://github.com/enkerewpo">wheatfox</a> for this helpful improvement!</p>

<h2 id="faster-erlang-dependency-compilation">Faster Erlang dependency compilation</h2>

<p>You can add packages written in Erlang or Elixir to your Gleam projects, and
the Gleam build tool will compile them for you. To compile Erlang packages
rebar3, the Erlang build tool, is used.</p>

<p>Gleam now sets the <code>REBAR_SKIP_PROJECT_PLUGINS</code> environment variable
when using rebar3. With future versions of rebar3 this will cause it to skip
project plugins, significantly reducing the amount of code it’ll need to
download and compile, improving compile times.</p>

<p>Thank you to <a href="https://github.com/tsloughter">Tristan Sloughter</a> for this
contribution to both Gleam and rebar3! Elixir’s Mix build tool will also
benefit from this new rebar3 feature.</p>

<h2 id="sugar-and-desugar-use-expressions">Sugar and desugar <code>use</code> expressions</h2>

<p>Gleam’s <code>use</code> expression is a much loved and very useful bit of syntactic
sugar, good for making nested higher-order-functions easy to work with. It is
by-far Gleam’s most unusual feature, so it can take a little time to get a good
understanding of it.</p>

<p>To help with this, and to make refactoring easier, Jak has added two new code
actions to the language server, to convert to and from the <code>use</code> expression
syntax and the equivalent using the regular function call syntax.</p>

<p>Here’s the same code in each syntax, so you can get an idea of what the code
actions will convert to and from for you.</p>

<pre><code>pub fn main() {
  use profile &lt;- result.try(fetch_profile(user))
  render_welcome(user, profile)
}
</code></pre>

<pre><code>pub fn main() {
  result.try(fetch_profile(user), fn(profile) {
    render_welcome(user, profile)
  })
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a> for these!</p>

<h2 id="yet-more-language-server-hover-information">Yet more language server hover information</h2>

<p><a href="https://github.com/GearsDatapacks">Surya Rose</a> has been adding more hover
information to the language server. If you hover over patterns or function
labels in your editor then type and documentation information will be shown.
Thank you Surya!</p>

<h2 id="inexhaustive-let-to-case-code-action">Inexhaustive <code>let</code> to <code>case</code> code action</h2>

<p>Using a partial pattern that does not match all possible values with a <code>let</code>
binding is a compile error in Gleam.</p>

<pre><code>pub fn unwrap_result(result: Result(a, b)) -&gt; a {
  let Ok(inner) = result // error: inexhaustive
  inner
}
</code></pre>

<p>The language server now suggests a code action to convert this <code>let</code> into a
<code>case</code> expression with the missing patterns added, so you can complete the
code.</p>

<pre><code>pub fn unwrap_result(result: Result(a, b)) -&gt; a {
  let inner = case result {
    Ok(inner) -&gt; inner
    Error(_) -&gt; todo
  }
  inner
}
</code></pre>

<p>Thanks again <a href="https://github.com/GearsDatapacks">Surya Rose</a>!</p>



<p>The language server now provides an action to extract a value into a variable.
Given this code:</p>

<pre><code>pub fn main() {
  list.each(["Hello, Mike!", "Hello, Joe!"], io.println)
}
</code></pre>

<p>If you place your cursor on the list and trigger the code action in your editor
the code will be updated to this:</p>

<pre><code>pub fn main() {
  let value = ["Hello, Mike!", "Hello, Joe!"]
  list.each(value, io.println)
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a> for this!</p>

<h2 id="expand-function-capture-code-action">Expand function capture code action</h2>

<p>Gleam has a short-hand syntax for a function that takes a single argument and
passes it to another function, along with some other arguments. Here you can
see it being used with the <code>int.add</code> function to create a function that always
adds 11.</p>

<pre><code>pub fn main() {
  let add_eleven = int.add(_, 11)
  list.map([1, 2, 3], add_eleven)
}
</code></pre>

<p>Triggering the code action results in the function-capture being expanded to the
full anonymous function syntax:</p>

<pre><code>pub fn main() {
  list.map([1, 2, 3], fn(value) { int.add(value, 11) })
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a> for the
final code action of the release!</p>

<h3 id="and-the-rest">And the rest</h3>

<p>And thank you to the bug fixers and error message improvers
<a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a>,
<a href="https://github.com/ivanjermakov">Ivan Ermakov</a>,
<a href="https://github.com/Frank-III">Jiangda Wang</a>,
<a href="https://github.com/jrstrunk">John Strunk</a>,
<a href="https://github.com/PgBiel">PgBiel</a>,
<a href="https://github.com/richard-viney">Richard Viney</a>,
<a href="https://github.com/sbergen">Sakari Bergen</a>,
<a href="https://github.com/GearsDatapacks">Surya Rose</a>, and
<a href="https://github.com/joshi-monster">yoshi</a></p>

<p>For full details of the many fixes and improvements they’ve implemented see <a href="https://github.com/gleam-lang/gleam/blob/main/changelog/v1.7.md">the
changelog</a>.</p>

<h2 id="its-my-birthday-">It’s my birthday 🎁</h2>

<p>Today is my birthday! If you’d like to give me a gift please consider
<a href="https://github.com/sponsors/lpil">supporting Gleam on GitHub Sponsors</a>.</p>

<p>Gleam is not owned by a corporation; instead it is entirely supported by
sponsors, most of which contribute between $5 and $20 USD per month, and Gleam
is my sole source of income.</p>

<p><a href="https://github.com/sponsors/giacomocavalieri">Giacomo Cavalieri</a> is also
deserving of your support. He has been doing amazing work on Gleam without any
pay for nearly two years, but now he has GitHub sponsors, so show him some love!</p>

<p><a href="https://github.com/sponsors/lpil" rel="noopener" target="_blank">
  <img src="https://gleam.run/images/community/github.svg" alt="GitHub Sponsors">
</a></p>

<p>Thank you to all our sponsors, especially our top sponsor: Lambda.</p>



<ul>
  <li><a href="https://github.com/00bpa">00bpa</a></li>
  <li><a href="https://github.com/agundy">Aaron Gunderson</a></li>
  <li><a href="https://github.com/zeroows">Abdulrhman Alkhodiry</a></li>
  <li><a href="https://github.com/abeljim">Abel Jimenez</a></li>
  <li><a href="https://github.com/ad-ops">ad-ops</a></li>
  <li><a href="https://github.com/AdamBrodzinski">Adam Brodzinski</a></li>
  <li><a href="https://github.com/adjohnston">Adam Johnston</a></li>
  <li><a href="https://github.com/adam-wyluda">Adam Wyłuda</a></li>
  <li><a href="https://github.com/thebugcatcher">Adi Iyengar</a></li>
  <li><a href="https://github.com/amouat">Adrian Mouat</a></li>
  <li><a href="https://github.com/JitPackJoyride">Ajit Krishna</a></li>
  <li><a href="https://github.com/Guria">Aleksei Gurianov</a></li>
  <li><a href="https://alembic.com.au/">Alembic</a></li>
  <li><a href="https://github.com/eelmafia">Alex</a></li>
  <li><a href="https://github.com/ahouseago">Alex Houseago</a></li>
  <li><a href="https://github.com/rawhat">Alex Manning</a></li>
  <li><a href="https://github.com/aexvir">Alex Viscreanu</a></li>
  <li><a href="https://github.com/akoutmos">Alexander Koutmos</a></li>
  <li><a href="https://github.com/muonoum">Alexander Stensrud</a></li>
  <li><a href="https://github.com/defgenx">Alexandre Del Vecchio</a></li>
  <li><a href="https://github.com/Acepie">Ameen Radwan</a></li>
  <li><a href="https://github.com/abueide">Andrea Bueide</a></li>
  <li><a href="https://github.com/AndreHogberg">AndreHogberg</a></li>
  <li><a href="https://github.com/antharuu">Antharuu</a></li>
  <li><a href="https://github.com/anthony-khong">Anthony Khong</a></li>
  <li><a href="https://github.com/Illbjorn">Anthony Maxwell</a></li>
  <li><a href="https://github.com/amscotti">Anthony Scotti</a></li>
  <li><a href="https://github.com/aweagel">Arthur Weagel</a></li>
  <li><a href="https://github.com/aryairani">Arya Irani</a></li>
  <li><a href="https://github.com/azureflash">Azure Flash</a></li>
  <li><a href="https://github.com/chiroptical">Barry Moore</a></li>
  <li><a href="https://github.com/bartekgorny">Bartek Górny</a></li>
  <li><a href="https://github.com/requestben">Ben Martin</a></li>
  <li><a href="https://github.com/bgmarx">Ben Marx</a></li>
  <li><a href="https://github.com/benmyles">Ben Myles</a></li>
  <li><a href="https://github.com/bbkane">Benjamin Kane</a></li>
  <li><a href="https://github.com/bcpeinhardt">Benjamin Peinhardt</a></li>
  <li><a href="https://github.com/bentomas">Benjamin Thomas</a></li>
  <li><a href="https://github.com/bgwdotdev">bgw</a></li>
  <li><a href="https://github.com/bigtallbill">Bill Nunney</a></li>
  <li><a href="https://github.com/bjartelund">Bjarte Aarmo Lund</a></li>
  <li><a href="https://github.com/bmehder">Brad Mehder</a></li>
  <li><a href="https://github.com/brettkolodny">brettkolodny</a></li>
  <li><a href="https://github.com/brian-dawn">Brian Dawn</a></li>
  <li><a href="https://github.com/bglusman">Brian Glusman</a></li>
  <li><a href="https://github.com/bruce">Bruce Williams</a></li>
  <li><a href="https://github.com/nono">Bruno Michel</a></li>
  <li><a href="https://github.com/bucsi">bucsi</a></li>
  <li><a href="https://github.com/camray">Cam Ray</a></li>
  <li><a href="https://github.com/cameronpresley">Cameron Presley</a></li>
  <li><a href="https://github.com/carlomunguia">Carlo Munguia</a></li>
  <li><a href="https://github.com/csaltos">Carlos Saltos</a></li>
  <li><a href="https://github.com/chadselph">Chad Selph</a></li>
  <li><a href="https://github.com/ctdio">Charlie Duong</a></li>
  <li><a href="https://github.com/charlie-n01r">Charlie Govea</a></li>
  <li><a href="https://github.com/chazwatkins">Chaz Watkins</a></li>
  <li><a href="https://github.com/choonkeat">Chew Choon Keat</a></li>
  <li><a href="https://github.com/ceedon">Chris Donnelly</a></li>
  <li><a href="https://github.com/Morzaram">Chris King</a></li>
  <li><a href="https://github.com/chrislloyd">Chris Lloyd</a></li>
  <li><a href="https://github.com/utilForever">Chris Ohk</a></li>
  <li><a href="https://github.com/Chriscbr">Chris Rybicki</a></li>
  <li><a href="https://github.com/christophershirk">Christopher David Shirk</a></li>
  <li><a href="https://github.com/devries">Christopher De Vries</a></li>
  <li><a href="https://github.com/cdaringe">Christopher Dieringer</a></li>
  <li><a href="https://github.com/christopherhjung">Christopher Jung</a></li>
  <li><a href="https://github.com/christhekeele">Christopher Keele</a></li>
  <li><a href="https://github.com/specialblend">CJ Salem</a></li>
  <li><a href="https://github.com/clangley">clangley</a></li>
  <li><a href="https://github.com/CliffordAnderson">Clifford Anderson</a></li>
  <li><a href="https://github.com/codecrafters-io">CodeCrafters</a></li>
  <li><a href="https://github.com/coder">Coder</a></li>
  <li><a href="https://github.com/colelawrence">Cole Lawrence</a></li>
  <li><a href="https://github.com/insanitybit">Colin</a></li>
  <li><a href="https://github.com/Comamoca">Comamoca</a></li>
  <li><a href="https://github.com/Lucostus">Constantin (Cleo) Winkler</a></li>
  <li><a href="https://github.com/jcorentin">Corentin J.</a></li>
  <li><a href="https://github.com/ccarvalho-eng">Cristiano Carvalho</a></li>
  <li><a href="https://github.com/sdaigo">Daigo Shitara</a></li>
  <li><a href="https://github.com/dvic">Damir Vandic</a></li>
  <li><a href="https://github.com/ddresselhaus">Dan Dresselhaus</a></li>
  <li><a href="https://github.com/DanielleMaywood">Danielle Maywood</a></li>
  <li><a href="https://github.com/pinnet">Danny Arnold</a></li>
  <li><a href="https://github.com/despairblue">Danny Martini</a></li>
  <li><a href="https://github.com/davydog187">Dave Lucia</a></li>
  <li><a href="https://github.com/dbernheisel">David Bernheisel</a></li>
  <li><a href="https://github.com/davidcornu">David Cornu</a></li>
  <li><a href="https://github.com/davesnx">David Sancho</a></li>
  <li><a href="https://github.com/dangdennis">Dennis Dang</a></li>
  <li><a href="https://github.com/dennistruemper">dennistruemper</a></li>
  <li><a href="https://github.com/diemogebhardt">Diemo Gebhardt</a></li>
  <li><a href="https://github.com/dmmulroy">Dillon Mulroy</a></li>
  <li><a href="https://github.com/gothy">Dima Utkin</a></li>
  <li><a href="https://github.com/poroh">Dmitry Poroh</a></li>
  <li><a href="https://github.com/DoctorCobweb">DoctorCobweb</a></li>
  <li><a href="https://github.com/floodfx">Donnie Flood</a></li>
  <li><a href="https://github.com/ds2600">ds2600</a></li>
  <li><a href="https://github.com/ducdetronquito">ducdetronquito</a></li>
  <li><a href="https://github.com/gdcrisp">Dylan Carlson</a></li>
  <li><a href="https://github.com/edongashi">Edon Gashi</a></li>
  <li><a href="https://github.com/eeeli24">eeeli24</a></li>
  <li><a href="https://github.com/enoonan">Eileen Noonan</a></li>
  <li><a href="https://github.com/dropwhile">eli</a></li>
  <li><a href="https://github.com/Emma-Fuller">Emma</a></li>
  <li><a href="https://github.com/EMRTS">EMR Technical Solutions</a></li>
  <li><a href="https://github.com/yellowsman">Endo Shogo</a></li>
  <li><a href="https://github.com/ekosz">Eric Koslow</a></li>
  <li><a href="https://github.com/eterps">Erik Terpstra</a></li>
  <li><a href="https://liberapay.com/erikareads/">erikareads</a></li>
  <li><a href="https://github.com/ErikML">ErikML</a></li>
  <li><a href="https://github.com/erlend-axelsson">erlend-axelsson</a></li>
  <li><a href="https://github.com/oberernst">Ernesto Malave</a></li>
  <li><a href="https://github.com/EthanOlpin">Ethan Olpin</a></li>
  <li><a href="https://github.com/evaldobratti">Evaldo Bratti</a></li>
  <li><a href="https://github.com/evanj2357">Evan Johnson</a></li>
  <li><a href="https://github.com/evanasse">evanasse</a></li>
  <li><a href="https://github.com/fabridamicelli">Fabrizio Damicelli</a></li>
  <li><a href="https://github.com/fmesteban">Fede Esteban</a></li>
  <li><a href="https://github.com/yerTools">Felix Mayer</a></li>
  <li><a href="https://github.com/nandofarias">Fernando Farias</a></li>
  <li><a href="https://github.com/ffigiel">Filip Figiel</a></li>
  <li><a href="https://github.com/floriank">Florian Kraft</a></li>
  <li><a href="https://github.com/francishamel">Francis Hamel</a></li>
  <li><a href="https://github.com/Frank-III">frankwang</a></li>
  <li><a href="https://github.com/gvrooyen">G-J van Rooyen</a></li>
  <li><a href="https://github.com/gabrielvincent">Gabriel Vincent</a></li>
  <li><a href="https://github.com/janag">Ganesan Janarthanam (Jana)</a></li>
  <li><a href="https://github.com/gahjelle">Geir Arne Hjelle</a></li>
  <li><a href="https://github.com/hagenek">Georg H. Ekeberg</a></li>
  <li><a href="https://github.com/brasilikum">Georg Hartmann</a></li>
  <li><a href="https://github.com/george-grec">George</a></li>
  <li><a href="https://github.com/ggobbe">ggobbe</a></li>
  <li><a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a></li>
  <li><a href="https://github.com/giovannibonetti">Giovanni Kock Bonetti</a></li>
  <li><a href="https://github.com/obmarg">Graeme Coupar</a></li>
  <li><a href="https://github.com/grottohub">grotto</a></li>
  <li><a href="https://github.com/nirev">Guilherme de Maio</a></li>
  <li><a href="https://github.com/guillheu">Guillaume Heu</a></li>
  <li><a href="https://github.com/ghivert">Guillaume Hivert</a></li>
  <li><a href="https://github.com/hammad-r-javed">Hammad Javed</a></li>
  <li><a href="https://github.com/kwando">Hannes Nevalainen</a></li>
  <li><a href="https://github.com/ildorn">Hannes Schnaitter</a></li>
  <li><a href="https://github.com/oderwat">Hans Raaf</a></li>
  <li><a href="https://github.com/jhundman">Hayes Hundman</a></li>
  <li><a href="https://github.com/hayleigh-dot-dev">Hayleigh Thompson</a></li>
  <li><a href="https://github.com/hibachrach">Hazel Bachrach</a></li>
  <li><a href="https://github.com/hdahlheim">Henning Dahlheim</a></li>
  <li><a href="https://github.com/h14h">Henry Firth</a></li>
  <li><a href="https://github.com/henrysdev">Henry Warren</a></li>
  <li><a href="https://github.com/losfair">Heyang Zhou</a></li>
  <li><a href="https://github.com/human154">human154</a></li>
  <li><a href="https://github.com/hpiaia">Humberto Piaia</a></li>
  <li><a href="https://github.com/iainh">Iain H</a></li>
  <li><a href="https://github.com/Ian-GL">Ian González</a></li>
  <li><a href="https://github.com/ianmjones">Ian M. Jones</a></li>
  <li><a href="https://github.com/igordsm">Igor Montagner</a></li>
  <li><a href="https://github.com/irumiha">Igor Rumiha</a></li>
  <li><a href="https://github.com/nilliax">ILLIA NEGOVORA</a></li>
  <li><a href="https://github.com/intarga">Ingrid</a></li>
  <li><a href="https://github.com/inoas">inoas</a></li>
  <li><a href="https://github.com/graphiteisaac">Isaac</a></li>
  <li><a href="https://github.com/isaacharrisholt">Isaac Harris-Holt</a></li>
  <li><a href="https://github.com/imcquee">Isaac McQueen</a></li>
  <li><a href="https://github.com/ismaelga">Ismael Abreu</a></li>
  <li><a href="https://github.com/ivarvong">Ivar Vong</a></li>
  <li><a href="https://github.com/m-rinaldi">J. Rinaldi</a></li>
  <li><a href="https://github.com/jacobdalamb">Jacob Lamb</a></li>
  <li><a href="https://github.com/jakecleary">Jake Cleary</a></li>
  <li><a href="https://github.com/jamesbirtles">James Birtles</a></li>
  <li><a href="https://github.com/jamesmacaulay">James MacAulay</a></li>
  <li><a href="https://github.com/janpieper">Jan Pieper</a></li>
  <li><a href="https://github.com/monzool">Jan Skriver Sørensen</a></li>
  <li><a href="https://github.com/jlgeering">Jean-Luc Geering</a></li>
  <li><a href="https://github.com/okkdev">Jen Stehlik</a></li>
  <li><a href="https://github.com/jiangplus">jiangplus</a></li>
  <li><a href="https://github.com/hunkyjimpjorps">Jimpjorps™</a></li>
  <li><a href="https://github.com/joeykilpatrick">Joey Kilpatrick</a></li>
  <li><a href="https://github.com/joeytrapp">Joey Trapp</a></li>
  <li><a href="https://github.com/johan-st">Johan Strand</a></li>
  <li><a href="https://github.com/JohnBjrk">John Björk</a></li>
  <li><a href="https://github.com/johngallagher">John Gallagher</a></li>
  <li><a href="https://github.com/jmpavlick">John Pavlick</a></li>
  <li><a href="https://github.com/xjojorx">Jojor</a></li>
  <li><a href="https://github.com/jonlambert">Jon Lambert</a></li>
  <li><a href="https://github.com/igern">Jonas E. P</a></li>
  <li><a href="https://github.com/JonasHedEng">Jonas Hedman Engström</a></li>
  <li><a href="https://github.com/jooaf">jooaf</a></li>
  <li><a href="https://github.com/joseph-lozano">Joseph Lozano</a></li>
  <li><a href="https://github.com/joshocalico">Joshua Steele</a></li>
  <li><a href="https://liberapay.com/d2quadra/">Julian Lukwata</a></li>
  <li><a href="https://github.com/schurhammer">Julian Schurhammer</a></li>
  <li><a href="https://github.com/justinlubin">Justin Lubin</a></li>
  <li><a href="https://github.com/Neofox">Jérôme Schaeffer</a></li>
  <li><a href="https://github.com/jkbrinso">Kemp Brinson</a></li>
  <li><a href="https://github.com/keroami">Kero van Gelder</a></li>
  <li><a href="https://github.com/kevinschweikert">Kevin Schweikert</a></li>
  <li><a href="https://github.com/hamptokr">Kramer Hampton</a></li>
  <li><a href="https://github.com/Bearfinn">Kritsada Sunthornwutthikrai</a></li>
  <li><a href="https://github.com/krystofrezac">Kryštof Řezáč</a></li>
  <li><a href="https://github.com/krzysztofgb">Krzysztof G.</a></li>
  <li><a href="https://github.com/leostera">Leandro Ostera</a></li>
  <li><a href="https://github.com/leejarvis">Lee Jarvis</a></li>
  <li><a href="https://github.com/leonqadirie">Leon Qadirie</a></li>
  <li><a href="https://github.com/LeartS">Leonardo Donelli</a></li>
  <li><a href="https://github.com/defp">lidashuang</a></li>
  <li><a href="https://github.com/LighghtEeloo">LighghtEeloo</a></li>
  <li><a href="https://github.com/LilyRose2798">Lily Rose</a></li>
  <li><a href="https://github.com/wowi42">Loïc Tosser</a></li>
  <li><a href="https://github.com/lucaspellegrinelli">Lucas Pellegrinelli</a></li>
  <li><a href="https://github.com/lbjarre">Lukas Bjarre</a></li>
  <li><a href="https://github.com/lukasmeihsner">Lukas Meihsner</a></li>
  <li><a href="https://github.com/lamdor">Luke Amdor</a></li>
  <li><a href="https://github.com/2kool4idkwhat">Luna</a></li>
  <li><a href="https://github.com/manuel-rubio">Manuel Rubio</a></li>
  <li><a href="https://github.com/ideaMarcos">Marcos</a></li>
  <li><a href="https://github.com/marcusandre">marcusandre</a></li>
  <li><a href="https://github.com/AYM1607">Mariano Uvalle</a></li>
  <li><a href="https://github.com/mariuskalvo">Marius Kalvø</a></li>
  <li><a href="https://github.com/markholmes">Mark Holmes</a></li>
  <li><a href="https://github.com/markmark206">Mark Markaryan</a></li>
  <li><a href="https://github.com/datayja">Markéta Lisová</a></li>
  <li><a href="https://github.com/Janiczek">Martin Janiczek</a></li>
  <li><a href="https://github.com/rechsteiner">Martin Rechsteiner </a></li>
  <li><a href="https://github.com/martonkaufmann">martonkaufmann</a></li>
  <li><a href="https://github.com/han-tyumi">Matt Champagne</a></li>
  <li><a href="https://github.com/mhheise">Matt Heise</a></li>
  <li><a href="https://github.com/m">Matt Mullenweg</a></li>
  <li><a href="https://github.com/matthewrobinsondev">Matt Robinson</a></li>
  <li><a href="https://github.com/matt-savvy">Matt Savoia</a></li>
  <li><a href="https://github.com/mattvanhorn">Matt Van Horn</a></li>
  <li><a href="https://github.com/mwhitworth">Matthew Whitworth</a></li>
  <li><a href="https://github.com/maxmcd">Max McDonnell</a></li>
  <li><a href="https://github.com/max-tern">max-tern</a></li>
  <li><a href="https://github.com/metame">metame</a></li>
  <li><a href="https://github.com/metatexx">METATEXX GmbH</a></li>
  <li><a href="https://github.com/amiroff">Metin Emiroğlu</a></li>
  <li><a href="https://github.com/stunthamster">Michael Duffy</a></li>
  <li><a href="https://github.com/michaeljones">Michael Jones</a></li>
  <li><a href="https://github.com/monocursive">Michael Mazurczak</a></li>
  <li><a href="https://github.com/michallepicki">Michał Łępicki</a></li>
  <li><a href="https://github.com/karlsson">Mikael Karlsson</a></li>
  <li><a href="https://liberapay.com/Daybowbow/">Mike</a></li>
  <li><a href="https://github.com/mroach">Mike Roach</a></li>
  <li><a href="https://liberapay.com/mikej/">Mikey J</a></li>
  <li><a href="https://github.com/MoeDevelops">MoeDev</a></li>
  <li><a href="https://github.com/MoritzBoehme">Moritz Böhme</a></li>
  <li><a href="https://github.com/rykawamu">MzRyuKa</a></li>
  <li><a href="https://github.com/n8nio">n8n - Workflow Automation</a></li>
  <li><a href="https://github.com/natanaelsirqueira">Natanael Sirqueira</a></li>
  <li><a href="https://github.com/nathanielknight">Nathaniel Knight</a></li>
  <li><a href="https://github.com/Kuuuuuuuu">Nayuki</a></li>
  <li><a href="https://github.com/NFIBrokerage">NFIBrokerage</a></li>
  <li><a href="https://github.com/arcanemachine">Nicholas Moen</a></li>
  <li><a href="https://github.com/nchapman">Nick Chapman</a></li>
  <li><a href="https://github.com/ndreynolds">Nick Reynolds</a></li>
  <li><a href="https://github.com/NicklasXYZ">Nicklas Sindlev Andersen</a></li>
  <li><a href="https://github.com/NicoVIII">NicoVIII</a></li>
  <li><a href="https://github.com/mrniket">Niket Shah</a></li>
  <li><a href="https://github.com/ninanomenon">Ninaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa</a></li>
  <li><a href="http://www.ninefx.com/">NineFX</a></li>
  <li><a href="https://github.com/nomio">Nomio</a></li>
  <li><a href="https://github.com/oceanlewis">Ocean</a></li>
  <li><a href="https://github.com/osebelin">Olaf Sebelin</a></li>
  <li><a href="https://github.com/OldhamMade">OldhamMade</a></li>
  <li><a href="https://github.com/CanadaHonk">Oliver Medhurst</a></li>
  <li><a href="https://github.com/otosky">Oliver Tosky</a></li>
  <li><a href="https://github.com/optizio">optizio</a></li>
  <li><a href="https://github.com/daslaf">Osman Cea</a></li>
  <li><a href="https://github.com/PastMoments">PastMoments</a></li>
  <li><a href="https://github.com/Davorak">Patrick Wheeler</a></li>
  <li><a href="https://github.com/giddie">Paul Gideon Dann</a></li>
  <li><a href="https://github.com/pguse">Paul Guse</a></li>
  <li><a href="https://github.com/biernacki">Pawel Biernacki</a></li>
  <li><a href="https://github.com/Tulkdan">Pedro Correa</a></li>
  <li><a href="https://github.com/petejodo">Pete Jodo</a></li>
  <li><a href="https://github.com/pvsr">Peter Rice</a></li>
  <li><a href="https://github.com/philpax">Philpax</a></li>
  <li><a href="https://github.com/pierrot-lc">Pierrot</a></li>
  <li><a href="https://github.com/sz-piotr">Piotr Szlachciak</a></li>
  <li><a href="https://github.com/qdentity">Qdentity</a></li>
  <li><a href="https://github.com/raquentin">Race Williams</a></li>
  <li><a href="https://github.com/stoft">Rasmus</a></li>
  <li><a href="https://github.com/ray-delossantos">Ray</a></li>
  <li><a href="https://github.com/chouzar">Raúl Chouza </a></li>
  <li><a href="https://github.com/renatillas">re.natillas</a></li>
  <li><a href="https://github.com/redmar">Redmar Kerkhoff</a></li>
  <li><a href="https://github.com/reillysiemens">Reilly Tucker Siemens</a></li>
  <li><a href="https://github.com/renatomassaro">Renato Massaro</a></li>
  <li><a href="https://github.com/renovatorruler">Renovator</a></li>
  <li><a href="https://github.com/richard-viney">Richard Viney</a></li>
  <li><a href="https://github.com/rico">Rico Leuthold</a></li>
  <li><a href="https://github.com/ripta">Ripta Pasay</a></li>
  <li><a href="https://github.com/robertwayne">Rob</a></li>
  <li><a href="https://github.com/TanklesXL">Robert Attard</a></li>
  <li><a href="https://github.com/rellen">Robert Ellen</a></li>
  <li><a href="https://github.com/malkomalko">Robert Malko</a></li>
  <li><a href="https://github.com/Papipo">Rodrigo Álvarez</a></li>
  <li><a href="https://liberapay.com/Karakunai/">Ronan Harris</a></li>
  <li><a href="https://github.com/rotabull">Rotabull</a></li>
  <li><a href="https://github.com/reinefjord">Rupus Reinefjord</a></li>
  <li><a href="https://github.com/ustitc">Ruslan Ustitc</a></li>
  <li><a href="https://github.com/samaaron">Sam Aaron</a></li>
  <li><a href="https://github.com/metruzanca">Sam Zanca</a></li>
  <li><a href="https://github.com/soulsam480">sambit</a></li>
  <li><a href="https://github.com/samifouad">Sami Fouad</a></li>
  <li><a href="https://github.com/bkspace">Sammy Isseyegh</a></li>
  <li><a href="https://github.com/castletaste">Savva</a></li>
  <li><a href="https://github.com/sasa1977">Saša Jurić</a></li>
  <li><a href="https://github.com/scotttrinh">Scott Trinh</a></li>
  <li><a href="https://github.com/smweber">Scott Weber</a></li>
  <li><a href="https://github.com/scottwey">Scott Wey</a></li>
  <li><a href="https://github.com/seanjensengrey">Sean Jensen-Grey</a></li>
  <li><a href="https://github.com/SeanRoberts">Sean Roberts</a></li>
  <li><a href="https://github.com/sporto">Sebastian Porto</a></li>
  <li><a href="https://github.com/sekunho">sekun</a></li>
  <li><a href="https://github.com/tehprofessor">Seve Salazar</a></li>
  <li><a href="https://github.com/codemonkey76">Shane Poppleton</a></li>
  <li><a href="https://github.com/honsq90">Shuqian Hon</a></li>
  <li><a href="https://github.com/simonewebdesign">Simone Vittori</a></li>
  <li><a href="https://github.com/star-szr">star-szr</a></li>
  <li><a href="https://github.com/bytesource">Stefan</a></li>
  <li><a href="https://github.com/sthagen">Stefan Hagen</a></li>
  <li><a href="https://github.com/Qard">Stephen Belanger</a></li>
  <li><a href="https://github.com/stvpwrs">Steve Powers</a></li>
  <li><a href="https://github.com/Strandinator">Strandinator</a></li>
  <li><a href="https://github.com/threepointone">Sunil Pai</a></li>
  <li><a href="https://github.com/slafs">Sławomir Ehlert</a></li>
  <li><a href="https://github.com/Theosaurus-Rex">Theo Harris</a></li>
  <li><a href="https://github.com/thomaswhyyou">Thomas</a></li>
  <li><a href="https://github.com/tcoopman">Thomas Coopman</a></li>
  <li><a href="https://github.com/ernstla">Thomas Ernst</a></li>
  <li><a href="https://github.com/tmbrwn">Tim Brown</a></li>
  <li><a href="https://github.com/timgluz">Timo Sulg</a></li>
  <li><a href="https://github.com/modellurgist">Tom Calloway</a></li>
  <li><a href="https://github.com/tomjschuster">Tom Schuster</a></li>
  <li><a href="https://github.com/tomekowal">Tomasz Kowal</a></li>
  <li><a href="https://github.com/tommaisey">tommaisey</a></li>
  <li><a href="https://github.com/ThisGuyCodes">Travis Johnson</a></li>
  <li><a href="https://github.com/TristanCacqueray">Tristan de Cacqueray</a></li>
  <li><a href="https://github.com/tsloughter">Tristan Sloughter</a></li>
  <li><a href="https://github.com/tymak">tymak</a></li>
  <li><a href="https://github.com/upsidedownsweetfood">upsidedowncake</a></li>
  <li><a href="https://github.com/vvzen">Valerio Viperino</a></li>
  <li><a href="https://github.com/sandsower">Vic Valenzuela</a></li>
  <li><a href="https://github.com/rodrigues">Victor Rodrigues</a></li>
  <li><a href="https://github.com/PerpetualPossum">Viv Verner</a></li>
  <li><a href="https://github.com/yelps">Volker Rabe</a></li>
  <li><a href="https://github.com/weizhliu">Weizheng Liu</a></li>
  <li><a href="https://github.com/enkerewpo">wheatfox</a></li>
  <li><a href="https://github.com/Willyboar">Willyboar</a></li>
  <li><a href="https://github.com/wilsonsilva">Wilson Silva</a></li>
  <li><a href="https://github.com/HymanZHAN">Xucong Zhan</a></li>
  <li><a href="https://github.com/yamen">Yamen Sader</a></li>
  <li><a href="https://github.com/Yasuo-Higano">Yasuo Higano</a></li>
  <li><a href="https://github.com/joshi-monster">yoshi~ </a></li>
  <li><a href="https://github.com/Yuri2b">Yuriy Baranov</a></li>
  <li><a href="https://github.com/gasparinzsombor">Zsombor Gasparin</a></li>
  <li><a href="https://liberapay.com/~1814730/">~1814730</a></li>
  <li><a href="https://liberapay.com/~1847917/">~1847917</a></li>
  <li><a href="https://liberapay.com/~1867501/">~1867501</a></li>
  <li><a href="https://github.com/eberfreitas">Éber Freitas Dias</a></li>
</ul>

<p>Thanks for reading, I hope you have fun with Gleam! 💜</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[41% of Employers Worldwide Say They'll Reduce Staff by 2030 Due to AI (114 pts)]]></title>
            <link>https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131</link>
            <guid>42652076</guid>
            <pubDate>Fri, 10 Jan 2025 03:12:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131">https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131</a>, See on <a href="https://news.ycombinator.com/item?id=42652076">Hacker News</a></p>
Couldn't get https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[TikTok tells staff impacted by wildfires to use sick hours if they can't work (164 pts)]]></title>
            <link>https://techcrunch.com/2025/01/09/tiktok-tells-la-staff-impacted-by-wildfires-to-use-personal-sick-hours-if-they-cant-work-from-home/</link>
            <guid>42652056</guid>
            <pubDate>Fri, 10 Jan 2025 03:08:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/01/09/tiktok-tells-la-staff-impacted-by-wildfires-to-use-personal-sick-hours-if-they-cant-work-from-home/">https://techcrunch.com/2025/01/09/tiktok-tells-la-staff-impacted-by-wildfires-to-use-personal-sick-hours-if-they-cant-work-from-home/</a>, See on <a href="https://news.ycombinator.com/item?id=42652056">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Wildfires are currently <a href="https://www.cbsnews.com/live-updates/california-windstorm-fuels-pacific-palisades-wildfire-as-residents-flee-live-updates/" target="_blank" rel="noreferrer noopener nofollow">devastating the greater Los Angeles area</a>, burning over 45 square miles, torching over 1,300 structures, and putting nearly 180,000 people under evacuation orders as of Thursday. And yet, TikTok’s LA-based employees are being told to either continue their work from home or use their personal/sick days if that’s not possible, while the company’s LA office remains closed due to power outages caused by high winds.</p>

<p>Already, the Palisades Fire is close enough to TikTok’s office that smoke can be seen out the windows. But although the Culver City-based office itself is <a href="https://www.culvercity.org/News/LA-Wildfire-Update" target="_blank" rel="noreferrer noopener nofollow">not under mandatory evacuation orders</a> at this time, both it and many employees’ homes are impacted by the fires, windstorms, and related problems.</p>







<p>TikTok’s employees in the region hail from all over the broader LA area — some even commuting hours into work — and many of their homes are currently without power, Wi-Fi, or both, we understand from employee sources at TikTok. Some could even be under evacuation orders of their own (though we have not directly confirmed this at this time).</p>

<p>Unfortunately for staff dealing with this large-scale natural disaster, TikTok is telling them to use their personal or sick time to account for the days they need to take off due to these conditions.</p>

<p>In messages from TikTok leadership to LA staff, the company informed employees that the LA office would be closed on January 8 and would remain closed through Sunday, January 12, as the fires continued to ravage the area and the office itself is without power. The days the office is closed are being made Work From Home days as opposed to days off, however — unless an individual team leader decides otherwise. </p>

<p>In one message, an HR representative shared links to other company resources for those impacted by the fires, including a Mental Wellbeing Portal, a way to sign up for free mental health sessions with <a href="https://www.lyrahealth.com/" target="_blank" rel="noreferrer noopener nofollow">Lyra</a>, and a link to TikTok’s “PSSL” policy. The latter refers to TikTok’s paid sick and safe leave program — essentially, sick time and personal days.</p>

<p>TikTok’s LA employees have 10 paid sick/personal (PSSL) days per year in addition to 15 PTO (paid time off/vacation) days, if they were hired before June. These sick/personal days are highly coveted, too, as TikTok’s strict return-to-office policy requires employees to work from the office a minimum of three days per week. (The days of the week are chosen by the team and can’t be swapped for other days if needed.)</p>


<p>That means if an employee is feeling unwell, like with a simple cold or flu, and they don’t want to spread their illness to coworkers, they do have the option to stay home. But because they’re required to be in the office for three days each week, they would still have to use their PSSL hours and take the day off on those working-from-home-while-sick days (rather than being allowed to work from home with no penalty).</p>

<p>This week, TikTok’s LA staff are being asked to use their personal/sick days if&nbsp;they cannot work from home due to power or Wi-Fi outages, or if they’re under evacuation orders (unless their entire team has been given time off, which is not the case for many impacted by the fires). This leaves them fewer days later in the year to use in case of an actual illness or other personal emergency, like staying home to care for a sick child. If they don’t have enough PSSL hours available, they can either borrow from next year or use their PTO time instead, we understand.</p>

<p>Employees who can work from home still must go into their “My RTO” portal, where they manage their sick time, and change their work-from-home status to “natural disaster” to not be penalized. This won’t subtract from their PSSL hours, though. </p>







<p>Meanwhile, TikTok’s PSSL policy documentation doesn’t specifically state that the time can be used for natural disasters, such as these massive wildfires.</p>

<p>Instead, the policy says employees can use the time for either a physical or mental health condition, to take care of a family member with a health condition, or if the office is closed by the “order of public officials” due to a public health emergency, including exposures to an infectious agent, biological toxin, or hazardous material. (While, arguably, smoke in the area could be “hazardous,” not every TikTok LA employee facing poor air quality is also under an evacuation order enacted by a public official.)</p>

<p>In several internal messages shared with us, employees are reporting their home has no power, or their city overall has no power. (News reports indicate that some <a href="https://ktla.com/news/california/wildfires/millions-without-power-in-southern-california-map-shows-latest-outages/" target="_blank" rel="noreferrer noopener nofollow">4 million people are without power</a> due to the wildfires as of yesterday.) Some employees are worried about how bad their air quality is getting. Others are worried about using up their precious battery power or generator fuel just to work at home, as it’s unclear how long these power outages will last.</p>

<p>Given the pressure TikTok is under due to the <a href="https://techcrunch.com/2024/12/28/trump-asks-supreme-court-to-pause-imminent-tiktok-ban/">upcoming ban in the U.S.</a>, which is probably already impacting U.S. employees’ mental health and stress levels, being told to keep working through a disaster of this scale comes across as a little tone-deaf. In fact, some internal messages reviewed by TechCrunch have very much a “business-as-usual” vibe to them despite the scale of the disaster at hand. One lead, for example, reached out to an employee without power for a status update on some of their work, messages show. </p>

<p>Employees have been told to contact the EAP (Employee Assistance Program) or their HR rep if they are told they need to evacuate. Though there are many messages from leaders stressing that employees should put their own safety and well-being first, asking staff to worry about using personal days if they can’t work from home seems to counter that narrative.</p>

<p>TikTok was asked for comment but didn’t offer a response ahead of publication. After publication, the company issued a statement, shared with TechCrunch. (See below). </p>

<p>TikTok claims that any communications to LA employees telling them to use personal time if they can’t work from home due to fires, power outages or internet issues must be a misunderstanding. (We should note that we’ve seen screenshots of TikTok HR’s communications to staff that contradict these claims. Additionally, after the story was published, TikTok enabled a feature that now alerts everyone in a company-wide Lark channel — <a rel="nofollow" href="https://en.wikipedia.org/wiki/Lark_(software)">a Slack competitor from TikTok parent ByteDance</a> — when a screenshot is taken.)</p>

<p>“The safety and well-being of our employees is our highest priority,” a TikTok spokesperson said. “In light of current circumstances, our offices have been closed since Tuesday and will remain so for as long as necessary. While employees who can work from home safely are encouraged to do so, we also recognize the unique challenges this situation may present and are committed to supporting our team with flexibility if they are unable to work remotely at this time.”</p>







<p><em>Sarah Perez can be reached via email at sarahp@techcrunch.com or @sarahperez.01 on Signal.</em> <em>Note that this article was updated after publication with TikTok’s statement.</em></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Predictions Scorecard, 2025 January 01 (166 pts)]]></title>
            <link>https://rodneybrooks.com/predictions-scorecard-2025-january-01/</link>
            <guid>42651275</guid>
            <pubDate>Fri, 10 Jan 2025 00:27:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rodneybrooks.com/predictions-scorecard-2025-january-01/">https://rodneybrooks.com/predictions-scorecard-2025-january-01/</a>, See on <a href="https://news.ycombinator.com/item?id=42651275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-inner">
		<main id="main" role="main">
			<!-- .page-header -->
		

<article id="post-1698">
	
	<!-- .entry-header -->

	<div>
		<p>[You can follow me on social media: @rodneyabrooks.bsky.social and see my publications etc., at <a href="https://people.csail.mit.edu/brooks" target="_blank" rel="noopener">https://people.csail.mit.edu/brooks</a>]</p>
<p>This is my seventh annual update on how <a href="https://rodneybrooks.com/my-dated-predictions/" target="_blank" rel="noopener">my dated predictions</a> from January 1<sup>st</sup>, 2018&nbsp;concerning (1) <em>self driving cars</em>, (2) <em>robotics, AI , and machine learning</em>, and (3) <em>human space travel</em>, have held up. I promised then to review them at the start of the year every year until 2050 (right after my 95<sup>th</sup> birthday), thirty two years in total. The idea is to hold myself accountable for those predictions. How right or wrong was I?</p>
<p>I have decided to change my rules for myself a little bit after this year, in response to the many many people who have said how much they enjoy seeing my updates.</p>
<p>My predictions were mostly for the first few years, and by next year the density of due dates will be very low. &nbsp;So, on the eight anniversary of my first set of predictions, i.e., a year from today, I will be making a new set of predictions centered on the period January&nbsp;1<sup>st&nbsp;</sup>2026 to January&nbsp;1<sup>st</sup>&nbsp;2036, and that will give a new density of predictions where there will be real meat to see how accurately they turned out.</p>
<p><span><em>What I Want to Achieve and a Changing Hype-driven Landscape</em></span></p>
<p>The level of hype about AI, Machine Learning and Robotics completely distorts people’s understanding of reality. It distorts where VC money goes, always to something that promises impossibly large payoffs–it seems it is better to have an untested idea that would have an enormous payoff than a tested idea which can get to a sustainable business, but does not change the world for ever. It distorts what young researchers work on as they do not want to be seen as old fashioned even when the current hyped topic is sort of dumb–soon the dumbness is forgotten and the heat of the chase becomes all. It distorts what people think they need to get a degree in at college in order to have good career prospects.</p>
<p>I want people to use rational thought processes when they hear about hyped ideas and be able to assess what is really going on, and what is just plain (to use the technical term) bullshit.</p>
<p><span><em>My Color Scheme and Past Analysis</em></span></p>
<p>The acronyms I used for predictions in my original post were as follows.</p>
<p><strong>NET <em>year</em></strong> means it will not happen before that year (No Earlier Than)<br>
<strong>BY <em>year</em></strong> means I predict that it will happen by that year.<br>
<strong>NIML</strong>, Not In My Lifetime, i.e., not before 2050.</p>
<p>As time passes mentioned years I color then as <span>accurate</span>, <span>too pessimistic</span>, or&nbsp;<span>too optimistic</span>.</p>
<p>This year I have added <span>hemming and hawing</span>. This is for when something looks just like what I said would take a lot longer has happened, but the underlying achievement is not what everyone expected, and is not what was delivered. This is mostly for things that were talked about as being likely to happen with no human intervention and it now appears to happen that way, but in reality there are humans in the loop that the companies never disclose. So the technology that was promised to be delivered hasn’t actually been delivered but everyone thinks it has been.</p>
<p>I have not changed any of the text of the first three columns of the prediction tables since their publication on the first day of 2018. I only change the text in the fourth column to say what actually happened. &nbsp;This meant that by two years ago that fourth column was getting very long and skinny, so I removed them and started with fresh comments last year. I have kept last year’s comments and added new ones, with yellow backgrounds, for this year. If you want to see the previous five years of comments you can go back to&nbsp;&nbsp;<a href="https://rodneybrooks.com/predictions-scorecard-2023-january-01/" target="_blank" rel="noopener">the 2023 scorecard</a>.</p>
<h5>Overview of changes this year</h5>
<p>There has been a lot of activity in both self driving cars (the demise of Cruise a big push by Waymo to scale human assisted deployments, and lots of smoke and mirrors from an electric car company) and in AI, where robotics has been pulled into the ultra hyposphere while in generative AI the end of scaling and the introduction of inference mechanisms (!!) have been hotly announced and disputed. &nbsp;The human spaceflight endeavor, as it did last year, has crawled along and again has stretched out dates that were probably too optimistic in the first place.</p>
<h2><span>But First.</span></h2>
<p><span>&lt;rant&gt;</span></p>
<p>We all know about FOMO, Fear Of Missing Out. In late 2023, for a talk on generative AI that I gave at MIT,&nbsp;<a href="https://www.youtube.com/watch?v=pgrzEHJTPPM" target="_blank" rel="noopener">I coined another acronym</a>, &nbsp;FOBAWTPALSL, Fear Of Being A Wimpy Techno-Pessimist And Looking Stupid Later. Perhaps that one is a little bit too much of a mouthful to catch on. These two human insecurities lead people to herd-like behavior in establishing and propagating the zeitgeist on almost any topic.</p>
<p>They lead to people piling on the hype fiestas, rushing to invest (money, effort, or hope) in marginal ideas once they have become a little bit popular, or <a href="https://www.nytimes.com/2024/12/24/nyregion/new-jersey-new-york-drones.html" target="_blank" rel="noopener">believing our airspace is being invaded by foreign drones</a>.</p>
<p>“Mounting evidence, <a title="" href="https://www.nytimes.com/2024/12/19/video/new-jersey-drones-planes-videos.html" target="_blank" rel="noopener">and lack thereof</a>, suggests that perhaps the whole craze has been a sort of communal fever dream fueled by crowd mentality, confirmation bias and a general distrust in all things official.”</p>
<p>That quote is from the drone story linked to above, but it could well as been about the hype that we are moving towards AGI (Artificial General Intelligence).</p>
<p><span>I want to be clear, as there has been for almost seventy years now, there has been significant progress in Artificial Intelligence over the last decade. There are new tools and they are being applied widely in science and technology, and are changing the way we think about ourselves, and how to make further progress.</span></p>
<p>That being said, we are not on the verge of replacing and eliminating humans in either white collar jobs or blue collar jobs. Their tasks may shift in both styles of jobs, but the jobs are not going away. We are not on the verge of a revolution in medicine and the role of human doctors. We are not on the verge of the elimination of coding as a job. We are not on the verge of replacing humans with humanoid robots to do jobs that involve physical interactions in the world. We are not on the verge of replacing human automobile and truck drivers world wide. We are not on the verge of replacing scientists with AI programs.</p>
<p>Breathless predictions such as these have happened for seven decades in a row, and each time people have thought the end is in sight and that it is all over for humans, that we have figured out the secrets of intelligence and it will all just scale. &nbsp;The only difference this time is that these expectations have leaked out into the world at large. I’ll analyze why this continues to happen below in the section on AI and ML.</p>
<p>Here is a list of some of those hype cycles that I, personally, have perceived and lived through, as taken from my presentation at MIT in late 2023 that I referenced above re FOBAWTPALSL.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/AIhypecycles.jpg" alt="" width="2842" height="1590"></p>
<p>Really, was there really hype about all these things? &nbsp;Yes, there was, within the circles that cared. Those circles have gotten wider and wider and when reigning world chess champion Garry Kasparov was beaten by I.B.M.’s Deep Blue computer under tournament conditions in 1997 it was widely reported in the popular press, And it was declared that it was all over for humans.</p>
<p>Back in February 2011 a computer program named Watson <a href="https://www.pbs.org/wgbh/nova/article/watson-and-jeopardy/" target="_blank" rel="noopener">played on the television game show Jeopardy against all time human champions</a>. John Markoff, legendary technology reporter at the New York Times, wrote stories about this <a href="https://www.nytimes.com/2011/02/15/science/15essay.html" target="_blank" rel="noopener">the day before</a> the competition, and <a href="https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html" target="_blank" rel="noopener">the day after</a>, when Watson had indeed beaten the humans, with the same questions (fed as text to it as the same time as the humans heard the questions) all running on a cluster of machines not connected to an outside network. Here are three successive paragraphs from the second of those stories.</p>
<p><span><span>For I.B.M., the future will happen very quickly, company executives said. On Thursday it plans to announce that it will collaborate with Columbia University and the University of Maryland to create a physician’s assistant service that will allow doctors to query a cybernetic assistant. The company also plans to work with Nuance Communications Inc. to add voice recognition to the physician’s assistant, possibly making the service available in as little as 18 months.</span></span></p>
<p><span>“I have been in medical education for 40 years and we’re still a very memory-based curriculum,” said Dr. Herbert Chase, a professor of clinical medicine at Columbia University who is working with I.B.M. on the physician’s assistant. “The power of Watson- like tools will cause us to reconsider what it is we want students to do.”</span></p>
<p><span>I.B.M. executives also said they are in discussions with a major consumer electronics retailer to develop a version of Watson, named after I.B.M.’s founder, Thomas J. Watson, that would be able to interact with consumers on a variety of subjects like buying decisions and technical support.</span></p>
<p>My personal experience at that time was people I did not know, but who had heard about my role at MIT (as director of the MIT AI Lab, and then founding director of MIT CSAIL, the Computer Science and Artificial Intelligence Lab) would come up to me and ask about the future of medicine. The people were variously doctors or health industry executives. I reassured them that medicine as we knew it then would stay much the same and was not about to be rendered obsolete.</p>
<p>And then in 2016 Geoff Hinton, one of the key architects of Deep Learning (which has had undeniable impact on the world) said:</p>
<p>“People should stop training radiologists now. It is just completely obvious that within five years deep learning is going to be better than radiologists.”</p>
<p>More people asking me whether this was true. It wasn’t in five years and it isn’t now. We need more radiologists than ever. And yes they do use deep learning tools to help them see some things they wouldn’t otherwise see. But they also understand anomalies using causal reasoning and we would be in a sorry state if all radiology was done by programs today.</p>
<p>Now look at those plum colored paragraphs above again as you take yourself way back in time to a year or so ago when ChatGPT was just a baby AGI, You can find stories just like this one if you substitute “ChatGPT” for “Watson” and “Microsoft” for “I.B.M.”</p>
<p>The things confidently predicted in 2011 (and in 1979, and in 2016) about the end of doctors didn’t happen then and it is not happening now. Nor are all the other jobs ending.</p>
<p>Today I get asked about humanoid robots taking away people’s jobs. In March 2023 I was at a cocktail party and there was a humanoid robot behind the bar making jokes with people and shakily (in a bad way) mixing drinks. A waiter was standing about 20 feet away silently staring at the robot with mouth hanging open. I went over and told her it was tele-operated. “Thank God” she said. (And I didn’t need to explain what “tele-operated” meant). Humanoids are not going to be taking away jobs anytime soon (and by that I mean not for decades).</p>
<p>You, you people!, are all making fundamental errors in understanding the technologies and where their boundaries lie. Many of them will be useful technologies but their imagined capabilities are just not going to come about in the time frames the majority of the technology and prognosticator class, deeply driven by FOBAWTPALSL, think.</p>
<p>But this time it is different you say. This time it is really going to happen. You just don’t understand how powerful AI is now, you say. All the early predictions were clearly wrong and premature as the AI programs were clearly not as good as now and we had much less computation back then. This time it is all different and it is for sure now.</p>
<p>Yeah, well, I’ve got a <a href="https://en.wikipedia.org/wiki/Predictions_and_claims_for_the_Second_Coming" target="_blank" rel="noopener">Second Coming</a> to sell you…</p>
<p><span>&lt;/rant&gt;</span></p>
<h5>Self Driving Cars</h5>
<p>As with <em>flying cars</em> the definition, or common understanding, of what <em>self driving cars</em>&nbsp;really means has changed since my post on predictions seven years ago. &nbsp;At that time self driving cars meant that the cars would drive themselves to wherever they were told to go with no further human control inputs.</p>
<p>Now self driving cars means that there is no one in the drivers seat, but there may well be, and in all cases so far deployed, <a href="https://www.nytimes.com/2024/09/11/insider/when-self-driving-cars-dont-actually-drive-themselves.html" target="_blank" rel="noopener">humans monitoring those cars from a remote location</a>, and occasionally sending control inputs to the cars. The companies do not advertise this feature out loud too much, but they do acknowledge it, and the reports are that it happens somewhere between every one to two miles traveled. These inputs are not direct control of the normal human mechanism of control the steering wheel, the brakes, and the accelerator. &nbsp;Rather they are advice that overrides some of the algorithms. &nbsp;For instance, “steer out into the next lane and go around this truck” as the human realizes that the truck is just not going to move (see an anecdote below on the first night I took the new Waymo taxis in San Francisco (I had previously last ridden a Waymo in 2012 in Mountain View)).</p>
<p>Why is this difference important? &nbsp;One of the motivations for self driving cars was that the economics of taxis, cars that people hire at any time for a short ride of a few miles from where they are to somewhere else of their choosing, would be radically different as there would be no driver. Systems which do require remote operations assistance to get full reliability cut into that economic advantage and have a higher burden on their ROI calculations to make a business case for their adoption and therefore their time horizon to scaling across geographies.</p>
<p>But wait, you might say, isn’t that electric car company that used to be based in California and is now based in Texas going to roll this out imminently and have a fully digital taxi service. They demoed it on a Hollywood movie studio lot just this year, and the cars were painted gold. Hmm. The location of the demo and the fact that the cars, even down to the tires, were painted gold tells you everything you need to know. Both the cars and the humanoid robots at that event were presented as autonomous but in reality they were all tele-operated directly by people (see below in the humanoid section for more details). And <a href="https://gizmodo.com/tesla-is-looking-to-hire-a-team-to-remotely-control-its-self-driving-robotaxis-2000530600" target="_blank" rel="noopener">that same electric car company is actively hiring people into paying jobs as remote operators</a>.</p>
<p>There was a reasonably balanced appraisal from <a href="https://www.reuters.com/technology/teslas-musk-unveil-robotaxis-amid-fanfare-skepticism-2024-10-10/" target="_blank" rel="noopener">Reuters</a> just after the event, though it does not go into details of the demos. Here is a direct quote from the story:</p>
<p>“We do expect to start fully autonomous unsupervised FSD in Texas and California next year.” Musk said.</p>
<p>The astute reader will note that this is the 11<sup>th</sup> year in a row that the CEO of Tesla has made this prediction of the same milestone happening the next year. We can admire the consistency.</p>
<p><em>Actual</em> <a href="https://www.slashgear.com/1605007/autonomous-driving-plateau-engineer-expert-explains/" target="_blank" rel="noopener">self-driving is now generally accepted to be much harder than every one believed</a>.</p>
<p>The reason that this bait and switch is important to understand is that the promise of inevitable fully self driving technology upended a historical way that new transportation systems have been adopted.</p>
<p>In the past whenever we have introduced new transportation mechanisms there have been large investments in infrastructure and that infrastructure is shared and used by everyone. The Romans built roads so soldiers and traded goods could travel long distances–in Europe those road networks are still the basis of today’s road networks. When steam engine driven trains were the new transportation technology vast networks of rails were built allowing goods to move long distances in mere hours or days. When Ford started mass production of automobiles he built roads and the local governments followed and the the Federal government followed, and those roads are what we use today.</p>
<p>Actual fully self driving cars promised that no infrastructure changes would be needed to revolutionize how vehicles would be controlled. Each individual vehicle would do what was needed all by itself. As sensors and networks got better there was no need for expensive new infrastructure because of this promise.</p>
<p>The promise was false. If government and private partnerships in building smart roads, which was a hot topic in the 1990s. had continued, every one of us would now have smarter safer cars, but still with onboard human drivers taking over in many situations. But we would have had smart freeways where once you were on it your car would be self driving. The road would have had lots of sensors effectively shared across all cars, as that data would have been transmitted to all passing cars. It would have been a fraction of the cost per car compared to the sensing on today’s almost but not really self driving cars like those of Waymo. And we would have had much more accurate congestion data where the root causes of local congestion would have been sensed with semantic understanding rather than just inferring it from the aggregate collection of location data from phones, individual cars, and historical data from roadside sensors.</p>
<p>Instead we now have individual corporate actors using a mixture of partial self driving and remote human supervision. The big question is whether the economics of this works at scale, and whether the fake promises will drive out the human drivers in cheaper services and we’ll all end up paying more. Will the level of hype we saw push our decentralized transportation system into the hands of a few wealthy companies, and in effect make it a centralized system where everybody has to pay private companies to be part of it?</p>
<p>As a reminder of how strong the hype was and the certainty of promises that it was just around the corner here is a snapshot of a whole bunch of predictions by major executives from 2017.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Screenshot-2024-12-21-at-4.02.30%E2%80%AFPM.png" alt="" width="2470" height="1280"></p>
<p>I have shown this many times before but there is one new annotation here for 2024. The years in parentheses are when the predictions were made. The years in blue are the years are the predicted years of achievement. When a blue year is shaded pink it means that it did not come to pass by then. The predictions with orange arrows are those that I had noticed had later been retracted.</p>
<p>The prediction that Jaguar and Land-Rover made that they would have fully autonomous cars by 2024 did not come to pass, so I have shaded it pink,</p>
<p>Note that every single blue year up until now is shaded pink, and that every one that is shaded pink has still not come to pass. None of the predictions that were out there in 2017 for the next few years have happened. &nbsp;None. There are three more for 2025, and I am sure that a year from now they will all be shaded pink also.</p>
<p>One of the big selling points of self driving cars was that they would be safer than cars driven by humans. So far that is not holding up with real data. One electric car maker with self driving software had it disengage when it sensed there would be an accident, supposedly so that the human could take over in a split second. And then the company did not report the incident as the fault of the software as it was no longer controlling the car when the impact occurred. It was reported, and I had this experience myself in my last ride in a Cruise in 2023, that Cruise vehicles would freeze when an accident looked likely, and then not report it as their software’s fault as the car was stationary and was hit by another car. In many reported cases, and in my case, simply continuing to move forward would avert any likely accident (fortunately for me the human driver of the other car slammed on the brakes and did not hit my robot vehicle).</p>
<p>In&nbsp;<a href="https://www.washingtonpost.com/business/2024/05/24/all-major-robotaxi-firms-are-facing-federal-safety-investigations/" target="_blank" rel="noopener">this story from the Washington Post</a>&nbsp;about Federal investigations into the safety incidents with self driving cars, they report that the companies involved claim they have vast amounts of driving on our roads under their belt. Not so.</p>
<p>An industry association says autonomous vehicles have logged a total of 70 million miles, a figure that it compares to 293 trips to the moon and back. But it’s a tiny fraction of the almost 9 billion miles that Americans drive every day. The relatively small number of miles the vehicles have driven makes it difficult to draw broad conclusions about their safety.</p>
<p>To put that into perspective, the total number of miles driven by all autonomous (sort of) vehicles over the last decade is less than 1% of the miles driven by humans every day in the United States. It is a tiny, tiny portion.</p>
<p>Take a look at this embedded video from the Wall Street Journal about investigations of crashes (many of which have been fatal) involving autonomous driving systems.</p>
<p><iframe width="660" height="371" src="https://www.youtube.com/embed/mPUGh0qAqWA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="The Hidden Autopilot Data That Reveals Why Teslas Crash | WSJ"></iframe></p>
<p>From the audio:&nbsp;“The kinds of things that tend to go wrong with these systems are things like it was not trained on, pictures of an overturned double trailer. It just didn’t know what it was. There were some lights there, but the lights were in unusual positions. A person would have clearly said something big is in the middle of the road. But the way machine learning works is it trains it on a bunch of examples and if it encounters something it doesn’t have a bunch of examples for it may have no idea what’s going on.”</p>
<p>[[My own take is that the fetish of end to end learning leads people to leave out well known algorithms that might solve many of &nbsp;these problems (e.g,, the incredibly <a href="https://www.sciencedirect.com/science/article/pii/S0042698911003646" target="_blank" rel="noopener">simple time to collision algorithms based on looming</a>). Yes, end to end learning made speech understanding systems better, but that does not mean it is the appropriate fetish to apply everywhere.]]</p>
<p><strong><span>Pro tip:</span></strong> Think about this history of industry prognostications about fully autonomous driving being just around the corner when you read today’s prognostications about LLMs taking jobs, en masse, in the next couple of years, or humanoid robots being dirt cheap and being able to learn how to do any human manual task real real soon now. You know you have seen this movie before…</p>
<p><span><em>My own experiences with Waymo in 2024</em></span></p>
<p>I have two sorts of experiences with Waymo vehicles. First, as a driver of my own vehicle and sharing road space with them every single time that I drive. And second, as a user of their ride service.</p>
<p>The streets of San Francisco had been thick with Waymo vehicles with no driver in them especially in the second half of 2024. As I drive across the city every morning to head down to my robotics/AI startup half way down the peninsula I see them everywhere until I get on to 101. &nbsp;I see them in front of me and behind me and in adjacent lanes as I drive on multilane one way streets. Sometimes I see four of them in a single block. Twice I’ve seen four of them in a line, in my block and could see four of them in a line in the block ahead of me. &nbsp;When I am at four way intersections with no traffic lights I see them participating in the social ritual of taking your turn to drive through the intersection in the order you stopped, except when a pedestrian is crossing in front of you. They do that pretty well. They do less well when they accidentally get into a line of parents’ cars snaking around a corner for school drop off or pickup.</p>
<p>Over the last few months I have noticed that in general they are getting more aggressive about stretching the rules, just like people do. Otherwise human drivers (including me) take advantage of their politeness. That aggression is not always welcomed. One morning I saw a workman with a group doing some digging on a road, and holding a sign with SLOW on one side and STOP on the other side have to jump in front of a Waymo to get it to do what he was trying to tell it to do with the sign. STOP. It wasn’t stopping for no stinking sign!</p>
<p>The only time I have seen a Waymo go into reverse, ever, was when I was illegally driving the wrong way down a single lane street and we were heading straight at each other.</p>
<p>As a rider I feel they are not quite aggressive enough with human drivers some time, so a ride in a Waymo takes longer than with an Uber or Lyft.</p>
<p>It is hit and miss where they drop me off. Sometimes they take a place to pull over half a block from my house, even when it is raining. There is no way to adjust what they happen to decide that day, even though I know that they will always be able to pull in right in front of my house.</p>
<p>The first time I took a Waymo this year, on the way home it picked me up at a restaurant and then was about to make a right turn. But at that corner there was an 18 wheeler with its lights flashing and surrounded by green cones. It pulled right in behind that truck and waited a long time before it drove forward. I am guessing a remote operator intervened told it to go around because eventually it pulled around it in the lane just to the left. Based on seeing Waymos interact with orange cones I suspect it would have done better if the cones had been orange rather than green. &nbsp;This easily illustrates that the learning that this robot does, and indeed any robot does, is nothing like the learning that people do (see my rant about the seven deadly sins and mistaking performance for competence in the section below on advances in AI and ML).</p>
<p>I mostly feel safe when I am a passenger in a Waymo. &nbsp;Sometimes I don’t feel that my driver of an Uber that I am taking rides with Uber that are not as safe as I would prefer.</p>
<p><span><em>Self Driving Taxi Services</em></span></p>
<p>There have been three self driving taxi services in the US in various stages of play over the last handful of years, though it turns out, as pointed out above that all of them have remote operators. They are Waymo, Cruise, and Zoox.</p>
<p>Waymo and Cruise are similar in that they use conventional cars adorned with lots of sensors. Zoox has purpose built vehicles that have no steering wheel or pedals for brake or accelerator.</p>
<p>Waymo and Cruise went for deployments in large parts of two or more cities and have had ride services callable by apps, just as one can do with Uber or Lyft. Zoox is smaller scale, much more restricted in geography, and really not comparable.</p>
<p>At this time last year Cruise was in trouble has it had suspended all of its San Francisco operations under pressure from regulators after some bad accidents that happened in a way that never would happen for human driven cars. &nbsp;Briefly, their cars were getting hit at night by emergency vehicles with lights flashing as the Cruise cars crossed intersections. Human drivers see the reflections of lights from such vehicles flashing even if they don’t see the vehicles themselves. The Cruise vehicles were only reacting to flashing lights that they could perceive directly. But the accident that tipped the scales was when a pedestrian crossing in front of a human driven vehicle was hit and went flying in the air landing right in front of a Cruise. The Cruise hit the person (who now disappeared from sight) as a human driver would most likely have done. But then it proceeded to drive 20 feet with the human underneath the vehicle being dragged along as it went into a mode where it was supposed to get off the road. A human driver would not have reacted that way to having been in a collision, even if it was not their fault.</p>
<p>The hammer finally fell in December of 2024. General Motors shut down Cruise. The leading paragraphs from this <a href="https://www.wsj.com/business/autos/general-motors-scraps-cruise-robotaxi-program-ea3298a8" target="_blank" rel="noopener">linked story</a>&nbsp;from the Wall Street Journal&nbsp;are:</p>
<div>
<p>General Motors has scrapped its Cruise robotaxi program after nearly a decade and $10 billion in development, citing the time and costs needed to scale the business and rising competition.</p>
<p>GM on Tuesday said it plans to realign its autonomous driving strategy and give priority to development of advanced driver assistance systems, which take over steering and other functions in certain situations and are common on new vehicles today.</p>
<p>The automaker said it would continue to develop fully autonomous technology for personal vehicles, and build on the progress of its Super Cruise system, a hands-off, eyes-on driving feature that the company introduced several years ago.</p>
<p data-type="paragraph">GM said it owns about 90% of Cruise and intends to buy out the remaining investors. It plans to combine the technical teams from Cruise and GM into a single effort to advance autonomous and assisted driving.</p>
<p>“We want to leverage what already has been done as we go forward in this,” Chief Executive Mary Barra told analysts on a call Tuesday.</p>
<p>The Detroit automaker said it expects the restructuring to reduce spending by more than $1 billion annually after the proposed plan is completed, which is expected in the first half of next year.</p>
</div>
<p>While there are 40 companies that have permits to test autonomous driving in California, alone, the demise of Cruise leaves just one company, Waymo, trying to make an actual go of a digital taxi service in the United States. They have an enormous significant lead over anyone else who wants get into this business and have spent billions of dollars (probably very much north of $10 billion) on this endeavor over the last 15 years. In an email they sent me a couple of weeks ago as a user of their services they reported that they provided 4 million customer rides in 2024. That is approximately 4 million more than any other company in the United States.</p>
<p><span><span><i>Waymo</i></span></span></p>
<p>Despite being so far out in front it has not been all smooth sailing for Waymo.</p>
<p>Early in the year the operations center for Waymo somehow neglected to realize it was Chinese New Year in Chinatown in San Francisco. So Waymo vehicles were routed through that area on the biggest night of celebration. Any human driver would have realized that the streets, i.e., the street surfaces where cars usually drive, were completely packed with humans, no doubt some of whom were intoxicated as well as just being out having a good time. Not so the Waymo vehicles. They tried pushing through the very very dense crowds, no doubt annoying many people. And what do people have at Chinese New Year? &nbsp;Fireworks. So some revelers decided to push back on this robot car invading their space. Here are a couple of pictures of the results.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Waymo1.jpg" alt="" width="1924" height="1110"></p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Waymo2.jpg" alt="" width="1916" height="1440"></p>
<p>Not pretty. &nbsp;And an example of how taking away people’s agency is never a good idea for robots (<a href="https://rodneybrooks.com/rodney-brooks-three-laws-of-robotics/" target="_blank" rel="noopener">see my second law of robotics</a>).</p>
<p>Throughout 2024 Waymo has been investigates for various accidents such as those described in <a href="https://www.wsj.com/business/autos/regulators-probe-alphabets-waymo-after-22-self-driving-car-incidents-996fde65" target="_blank" rel="noopener">this Wall Street Journal article</a>. “Reports included collisions with stationary or semistationary objects, such as gates, chains or parked vehicles, according to the regulator.”</p>
<p>In the middle of the summer Waymo added a feature where they would honk their horns at cars in their way. But this backfired when hundreds of Waymos were coming back to their parking lot in the very early hours of the morning, and they started honking at each other and <a href="https://www.designnews.com/automotive-engineering/neighbors-hate-waymo-horn-hassles" target="_blank" rel="noopener">waking up human neighbors</a>. Eventually that got fixed.</p>
<p>In late September a motorcade for Kamala Harris in San Francisco was brought to a halt by a Waymo that <a href="https://sfstandard.com/2024/09/28/waymo-halts-kamala-harris-san-francisco-motorcade/" target="_blank" rel="noopener">stopped in the middle of California Street</a> doing a U-turn in front of it. I’m sure this incident was of great concern to the Secret Service. Eventually a San Francisco police officer got into the car and drove it out of the way–this is shown in a video included with the story above. I do not know how the officer got access to the vehicle and whether Waymo remote operations were cooperating.</p>
<p>More disturbingly humans outside the Waymos started harrassing humans inside them. The most concerning cases come from the realization that if a woman is in a Waymo at night she will be dropped off, outside, on a public road at the end of her journey with no option but to get out of the car where it has stopped. So groups of men have followed Waymos with women in them and then harassing the woman when she gets out. If she was driving her own car she might be heading to an off road parking space or she might choose not to stop if she knows she is being followed. There are no such options in a Waymo so taking a Waymo at night is less safe than other means of transportation–just follow it and eventually the preyed upon woman will have to get out. Here is a <a href="https://www.washingtonpost.com/technology/2024/12/22/waymo-robotaxi-passengers-harassment/" target="_blank" rel="noopener">very recent disturbing story</a>&nbsp;about this practice.</p>
<p>Meanwhile Waymo <a href="https://electrek.co/2024/10/25/waymo-secures-5-6-billion-funding-expanding-robotaxis-new-cities-2025/" target="_blank" rel="noopener">managed to raise $5.6B to expand to new cities in 2025</a>. It already operates in parts of San Francisco, Los Angeles, and Phoenix. The new money will let it expand to Austin and Atlanta in the United States and to start operating in parts of Tokyo in Japan. That is expensive expansion.</p>
<p>Here is the question for the future of watered down remote monitored “autonomous” driving systems (let’s call it “watered down autonomy”), and it is up to Waymo now. Can Waymo expand fast enough in these new markets in 2025 and take enough business from what is left of traditional taxi operators, along with those operating under the Uber and Lyft models, and do it in a way which is in sight of profitability, so that it has a case to raise the stupendous amounts of money needed to operate in all large cities in the US in the next 10 t0 20 years?</p>
<p>If Waymo can not succeed at this in the next two years I think the idea of large scale use of watered down autonomy will be dead for at least a decade or two. Right now full autonomy everywhere is already dead.</p>

<table id="tablepress-30">
<thead>
<tr>
	<th>Prediction<br>
[Self Driving Cars]</th><th>Date</th><th>2018 Comments</th><th>Updates</th>
</tr>
</thead>
<tbody>
<tr>
	<td>A flying car can be purchased by any US resident if they have enough money.</td><td>NET 2036</td><td>There is a real possibility that this will not happen at all by 2050.<br>
</td><td></td>
</tr>
<tr>
	<td>Flying cars reach 0.01% of US total cars.</td><td>NET 2042</td><td>That would be about 26,000 flying cars given today's total.</td><td></td>
</tr>
<tr>
	<td>Flying cars reach 0.1% of US total cars.</td><td>NIML</td><td></td><td></td>
</tr>
<tr>
	<td>First dedicated lane where only cars in truly driverless mode are allowed on a public freeway.<br>
</td><td><p>NET 2021</p></td><td>This is a bit like current day HOV lanes. My bet is the left most lane on 101 between SF and Silicon Valley (currently largely the domain of speeding Teslas in any case). People will have to have their hands on the wheel until the car is in the dedicated lane.</td><td></td>
</tr>
<tr>
	<td>Such a dedicated lane where the cars communicate and drive with reduced spacing at higher speed than people are allowed to drive</td><td><p>NET 2024</p></td><td></td><td><span>20240101</span> <p>This didn't happen in 2023 so I can call it now. But there are no plans anywhere for infrastructure to communicate with cars, though some startups are finally starting to look at this idea--it was investigated and prototyped by academia 20 years ago.</p></td>
</tr>
<tr>
	<td>First driverless "taxi" service in a major US city, with dedicated pick up and drop off points, and restrictions on weather and time of day.</td><td><p>NET 2021</p></td><td>The pick up and drop off points will not be parking spots, but like bus stops they will be marked and restricted for that purpose only.</td><td><span>20240101</span> <p>People may think this happened in San Francisco in 2023, but it didn't. Cruise has now admitted that there were humans in the loop intervening a few percent of the time. THIS IS NOT DRIVERLESS. Without a clear statement from Waymo to the contrary, one must assume the same for them. Smoke and mirrors.</p></td>
</tr>
<tr>
	<td>Such "taxi" services where the cars are also used with drivers at other times and with extended geography, in 10 major US cities</td><td><p>NET 2025</p></td><td>A key predictor here is when the sensors get cheap enough that using the car with a driver and not using those sensors still makes economic sense.</td><td><span>20250101</span> <p>Imminent dual use of personal cars was the carrot that got lots of people to pay cash when buying a Tesla for the software subscription that would allow thei car to operate in this way. Shockingly the CEO of Tesla announced in smoke and mirrors roll out of Cyber Cab in 2024, that the service would use specially built vehicles to be produced at some indeterminate late date. I got suckered by his hype. This is unlikely to happen in the first half of this century.</p></td>
</tr>
<tr>
	<td>Such "taxi" service as above in 50 of the 100 biggest US cities.</td><td>NET 2028</td><td>It will be a very slow start and roll out. The designated pick up and drop off points may be used by multiple vendors, with communication between them in order to schedule cars in and out.<br>
</td><td><span>20250101</span> <p>Even the watered down version of this with remote operators is not gong to happen in 50 cities by 2028. Waymo has it in 3 cities and is currently planning on 2 more in the US in 2025.</p></td>
</tr>
<tr>
	<td>Dedicated driverless package delivery vehicles in very restricted geographies of a major US city.</td><td><p>NET 2023</p></td><td>The geographies will have to be where the roads are wide enough for other drivers to get around stopped vehicles.<br>
</td><td></td>
</tr>
<tr>
	<td>A (profitable) parking garage where certain brands of cars can be left and picked up at the entrance and they will go park themselves in a human free environment.</td><td><p>NET 2023</p></td><td>The economic incentive is much higher parking density, and it will require communication between the cars and the garage infrastructure.</td><td></td>
</tr>
<tr>
	<td>A driverless "taxi" service in a major US city with arbitrary pick and drop off locations, even in a restricted geographical area.<br>
</td><td>NET 2032
<p>NET 2032</p></td><td>This is what Uber, Lyft, and conventional taxi services can do today.</td><td><span>20240101</span> <p>Looked like it was getting close until the dirty laundry came out.</p><span>20250101</span> <p>Waymo now has a service that looks and feels like this in San Francisco, 8 years earlier than I predicted. But it is not what every one was expecting. There are humans in the loop. And for those of us who use it regularly we know it is not as general  case on drop off and pick up as it is with human drivers.</p></td>
</tr>
<tr>
	<td>Driverless taxi services operating on all streets in Cambridgeport, MA, and Greenwich Village, NY. </td><td>NET 2035</td><td>Unless parking and human drivers are banned from those areas before then.</td><td></td>
</tr>
<tr>
	<td>A major city bans parking and cars with drivers from a non-trivial portion of a city so that driverless cars have free reign in that area.</td><td>NET 2027<br>
BY 2031</td><td>This will be the starting point for a turning of the tide towards driverless cars.</td><td></td>
</tr>
<tr>
	<td>The majority of US cities have the majority of their downtown under such rules.</td><td>NET 2045</td><td></td><td></td>
</tr>
<tr>
	<td>Electric cars hit 30% of US car sales.</td><td>NET 2027</td><td></td><td><span>20240101</span> <p>This one looked pessimistic last year, but now looks at risk. There was a considerable slow down in the second derivative of adoption this year in the US.</p><span>20250101</span> <p>Q3 2024 had the rate 8.9% so there is no way it can reach 30% in 2027. I was way too optimistic at a time when EV enthusiasts thought I was horribly pessimistic.</p></td>
</tr>
<tr>
	<td>Electric car sales in the US make up essentially 100% of the sales.</td><td>NET 2038<br>
</td><td></td><td></td>
</tr>
<tr>
	<td>Individually owned cars can go underground onto a pallet and be whisked underground to another location in a city at more than 100mph.</td><td>NIML</td><td>There might be some small demonstration projects, but they will be just that, not real, viable mass market services.<br>
</td><td></td>
</tr>
<tr>
	<td>First time that a car equipped with some version of a solution for the trolley problem is involved in an accident where it is practically invoked.</td><td>NIML</td><td>Recall that a variation of this was a key plot aspect in the movie "I, Robot", where a robot had rescued the Will Smith character after a car accident at the expense of letting a young girl die.</td><td></td>
</tr>
</tbody>
</table>
<!-- #tablepress-30 from cache -->
<p><span><em>Electric Cars</em></span></p>
<p>Last year US manufacturers pulled back on their planned production of EVs. In data from <a href="https://caredge.com/guides/electric-vehicle-market-share-and-sales" target="_blank" rel="noopener">this report</a> we can see that sales dropped at the start of 2024 but have now picked up again.</p>

<table id="tablepress-34">
<tbody>
<tr>
	<td>2022</td><td>2022</td><td>2022</td><td>2022</td><td>2023</td><td>2023</td><td>2023</td><td>2023</td><td>2024</td><td>2024</td><td>2024</td>
</tr>
<tr>
	<td>Q1</td><td>Q2</td><td>Q3</td><td>Q4</td><td>Q1</td><td>Q2</td><td>Q3</td><td>Q4</td><td>Q1</td><td>Q2</td><td>Q3</td>
</tr>
<tr>
	<td>5.3%</td><td>5.6%</td><td>6.1%</td><td>6.5%</td><td>7.3%</td><td>7.2%</td><td>7.9%</td><td>8.1%</td><td>7.3%</td><td>8.0%</td><td>8.9%</td>
</tr>
</tbody>
</table>
<!-- #tablepress-34 from cache -->
<p>There is steady growth in sales but my prediction of 30% of US car sales being electric by 2027 now seems wildly optimistic. We need two doublings to get there in three years and the doubling rate seems more like one doubling in four to five years.</p>
<p>Note that some sources include hybrids and hydrogen powered cars in electric vehicles but I am using the battery electric vehicle (BEV) numbers.</p>
<p>To see how the trends are across brands you can see a breakout for Q2 of 2024 <a href="https://www.coxautoinc.com/wp-content/uploads/2024/07/Q2-2024-Kelley-Blue-Book-Electric-Vehicle-Sales-Report.pdf" target="_blank" rel="noopener">here</a>.</p>
<p>There appear to be two main headwinds for BEV adoption. Firstly, if one doesn’t have on property residential parking it is hard work in the US to find a place to recharge, and it takes hours for the charging to finish. This will stop many city dwellers from adopting. Secondly the increased tire wear adds up to real money. The maintenance requirements for BEVs are much less than for cars with an internal combustion engine. On the other hand tires do not last as long (I have had to buy four new tires in less than two years owning my first BEV), <a href="https://www.telegraph.co.uk//money/consumer-affairs/my-electric-car-heavy-had-change-tyres-after-7500-miles/" target="_blank" rel="noopener">apparently due to the increased weight of the car</a>.</p>
<p><span><em>Flying Cars</em></span></p>
<p>Flying cars are another category where the definitions have changed. Back when I made my predictions it meant a vehicle that could both drive on roads and fly through the air. &nbsp;Now it has come to mean an electric multi-rotor helicopter than can operate like a taxi between various fixed landing locations. Often touted are versions that have no human pilot. These are known as eVTOLs, for “electric vertical take off &amp; landing”.</p>
<p>Large valuations have been given to start ups who make nice videos of their electric air taxis flying about. But on inspection one sees that they don’t have people in them. Often, you might notice, even those flights are completely over water rather than land. I wrote about the <a href="https://rodneybrooks.com/where-are-the-crewed-evtol-videos/" target="_blank" rel="noopener">lack of videos of viable prototypes</a> back in November 2022.</p>
<p>Nevertheless there have been wild predictions. &nbsp;I ended a longer version of this component in <a href="https://rodneybrooks.com/predictions-scorecard-2024-january-01/" target="_blank" rel="noopener">last year’s annual review</a> with:</p>
<p>Also note the size of this vehicle. There are many fossil fuel powered helicopters that are much smaller. This is not going to be a personally owned vehicle for the masses.</p>
<p>Don’t hold your breath. They are not here. They are not coming soon.</p>
<p>Nothing has changed. Billions of dollars have been spent on this fantasy of personal flying cars. &nbsp;It is just that, a fantasy, largely fueled by spending by billionaires.</p>
<h6>Robotics, AI, and Machine Learning</h6>
<p>So what happened in Robotics, AI, and Machine Learning this year?</p>
<p>Many, many, many people got just a little bit over excited. That’s what happened.</p>
<p>There have been a lot of party tricks and it is the researchers who often play the tricks on themselves without realizing it. This is not new, none of it is new. But there are orders of magnitude more people watching it now, and more people are out to make a buck by being hypesters, promising riches to those who will invest in their irrationally overpriced companies.</p>
<p>How could this be?</p>
<p>We are seeing mass sinning, lots and lots of people committing some of the seven deadly sins of predicting the future of AI &nbsp;which I wrote about back in 2017 <a href="https://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/" target="_blank" rel="noopener">here</a> (or <a href="https://www.technologyreview.com/2017/10/06/241837/the-seven-deadly-sins-of-ai-predictions/" target="_blank" rel="noopener">here</a> you can see a professionally edited version of that blog post of mine).</p>
<p>Four of those seven sins seem most relevant to today’s hyped up atmosphere around robotics, AI, and machine learning.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/PerformCompet-1.jpg" alt="" width="314" height="166">&nbsp;<img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Magic-1.jpg" alt="" width="287" height="165">&nbsp;<img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Exponentialism-1.jpg" alt="" width="277" height="173">&nbsp; <img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Deployment-1.jpg" alt="" width="320" height="181"></p>
<p>Here now are short descriptions of these particular four sins, edited down from my earlier much more detailed descriptions. Then I will weave them together to explain how it is still pretty much business as usual, and I mean that in a good way, with steady progress on both the science and engineering of AI.</p>
<p><span><em>Performance versus Competence</em></span></p>
<p>One of the social skills that we all develop is an ability to estimate the capabilities of individual people with whom we interact. We use cues from how a person performs any particular task to estimate how well they might perform some different task. We are able to generalize from observing performance at one task to a guess at competence over a much bigger set of tasks.</p>
<p>These estimators that we have all inherited or learned do not generalize well to other creatures or machines. We are not good at guessing which smart things other species might be able to do, and we are not good at guessing what an AI system can do when we have seen it do a few tasks in a limited domain. We get it wrong all the time.</p>
<p><span><em>Indistinguishable from Magic</em></span></p>
<p>When people cannot explain how something works they cannot know its limits as they do not have any sort of model (nor have they seen enough examples of it before). Arthur C. Clarke said that any sufficiently advanced technology is indistinguishable from magic.</p>
<p>In our minds UFOs can do all sorts of amazing things as we have no way of knowing their limits–they may as well be magic, And that is what they become in speculation about them.</p>
<p>Isaac Newton spent half his working life on alchemy as he did not know that the nucleus of atoms were not subject to mere chemistry. He would have been just as ignorant of the limitations of an iPhone screen (different sort of apple…), despite his own ground breaking work in optics. Remember, he was a really really smart dude. But even he was not able to develop all the theories needed to understand the world around him, despite his successes with calculus and gravity and the makeup of white light. He attributed properties to chemistry that were way beyond its limits.</p>
<p><span><em>Exponentialism</em></span></p>
<p>We have just lived through sixty years of the most phenomenal growth of a technology in the history of humankind. It is the story of silicon-based computation. Everyone has some idea about Moore’s Law, at least as much to sort of know that computers get better and better on a clockwork like schedule.</p>
<p>This reality has trained people to think that probably a lot of other things in tech will change exponentially, especially when that thing has a strong computational component. The sin of exponentialism is to argue that some other process is going to follow a Moore’s-like law when it is unwarranted to so argue.</p>
<p>Moore’s law worked for so long because in the starting technology of the 1960s the currents used to represent digital information were many many orders of magnitude beyond the minimal physical limit needed to determine whether they &nbsp;were present or not, and hence distinguish a 1 from a 0. Those currents could be halved many times without breaking physics limits.</p>
<p><em><span>Speed of Deployment</span></em></p>
<p>New technologies get deployed much more slowly than people imagine. Even software technologies.</p>
<p>The old internet protocol, IPv4, can only address two billion, or&nbsp;2×10<sup>9</sup>, devices, which is way less than the number of people on our planet. A new protocol, IPv6, which can address more than&nbsp;3×10<sup>38</sup>&nbsp;devices was meant to replace it over a two year period of dual use by about 2003. But in 2024 IPv4 was still there and carrying over half the world’s internet traffic despite its inadequacies.</p>
<p>Must functioning businesses that operate in the physical world are very averse to taking up new technology as it dramatically increases existential risk to their business. They must foresee immediate and incredibly high return on investment (ROI) to be tempted to move to new technologies.</p>
<p>Even the military is slow to adopt new technologies. The US Air Force still flies the B-52H variant of the B-52 bomber. This version was introduced in 1961, making it 63 years old. The last one was built in 1963, a mere 61 years ago. Currently these planes are expected to keep flying until at least 2040, and perhaps longer–there is talk of extending their life out to 100 years.</p>
<p><em><span>What does this all mean?</span></em></p>
<p>Right now there is incredible hype for both Large Language Models (LLMs), and all their variations, and for humanoid robots, especially humanoid robots that are going to learn how to do things.</p>
<p>The hype is driven by the four sins above.</p>
<p><em><span>LLMs</span></em></p>
<p>LLMs have proved amazing facile with language. They have been trained on pretty much all the text that is available on the Web and all the digitized historical books that exist. Miraculously LLMs seem to be able to infer a representation of some sort, that is somewhat independent of the particular human language that they read. So they are able to translate between human languages, and when you ask them just about anything they produce text in the language that you asked in, and that text often seems entirely reasonable and informative.</p>
<p>I used the word “miraculously” as we do not really understand why they are able to do what they do. We, of course, know that the architecture for them is built around noticing correlations in vast amounts of text &nbsp;that connect some tens of thousands of tokens which are the components of words in each language that is digested. It is a surprise that they work as well as they they do, and produce coherent sounding language on just about any topic.</p>
<p>Here is the original architectural diagram from the 2017 <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">Attention Is All You Need</a> paper:</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/attention.jpg" alt="" width="1618" height="1600"></p>
<p>Each column from bottom to top is a pure feed forward network, with no search, no iteration, no conventional algorithm at all. There are inputs at the bottom and then layer upon layer of linear neurons that have numbers or weights stored in them that multiply and add their inputs and threshold that sum to provide an output. The detail in the architectural diagram is how the connections between layers are organized.</p>
<p>On the left is an input or question, in a linear string of words, from a user. That gets injected half way up the network on the right and remains constant while a single iteration process runs. The stack on the right outputs a word (or token) and that gets fed back to the bottom of that stack, and a new token pops out the top. All the output tokens that have so far been produced remain in the right bottom input buffer as ordered input.</p>
<p>What the network has been trained to do, is given the user input on the left, and what the network has output so far, choose a very likely next word, given the billions of examples it has seen in training. Some randomness is used to choose among a small number of very likely next words at each stage.</p>
<p>There are hundreds of billions of weights that get learned and stored in the layers of network to act as multipliers for each individual input to each layer.</p>
<p>So now us humans are faced with looking at this system running and our human nature just makes us commit the first two sins from above. &nbsp;It is in our nature and we cannot help ourselves.</p>
<p>First, we see really impressive examples of responses to input questions, and if a human was giving those answers we would estimate that person to be quite clever and able to reason. Often though, because they have so many billions of examples on which they were trained LLMs are essentially looking up the question in the weights. The weight if gained from all of human knowledge that is out there on the network in language form. Invisibly the network is perhaps (but not in any intentional way) merging some similar questions, and then merging the answers which were already in the vast data that it has seen.</p>
<p>But us dumb humans just think the damn thing is really really smart.</p>
<p>Then, since we don’t have a real explanation in our heads for what it is doing we start thinking it is magic, and that there is no real limit to what it is extracting from all that data (that it used a significant portion of the energy budget for many different countries to compute) and how general its capabilities will be. It becomes magic. And then researchers try to show that it can reason, that it has inferred a spatial understanding of the world, that language can be used to do all sorts of things that Moravec’s paradox tells us it can’t. There is a lot of magical thinking that humans do about LLMs.</p>
<p>Of course it can diagnose diseases like a doctor talking about them. Of course it can teach a student as well as a human teacher. Of course it can program as well as a human computer programmer. It is magic after all.</p>
<p>But in reality the fact that it is just picking likely next words means that in fact we can’t trust its output. Some outputs are great. Some are pure confabulations (most people use the word “hallucinations” for this, but I prefer “confabulations”). And we do not know which we will get ahead of time, or more perniciously how much of each we will get, trustworthy pieces of output and confabulated pieces of output all jumbled together.</p>
<p>Not to worry say the proponents, More learning will fix it. Fire up a nuclear power plant (I am not making this up–the tech companies are getting more nuclear power built or activated so that their LLMs can learn what a human learns using just 20 watts powering their brain; I am not confabulating this!!), and we’ll feed it more data and it will become more trustworthy. &nbsp;It is magic after all. But the magic is not going as well as the proponents imagined and promised as this Wall Street Journal <a href="https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693?mod=hp_lead_pos8" target="_blank" rel="noopener">story</a> explains. Their imaginations were definitely encourage by exponentialism, but in fact all they knew was that when the went from smallish to largish networks following the architectural diagram above, the performance got much better. So the inherent reasoning was that if <strong>more</strong> made things better then <em>more</em> <strong>more</strong> would make things <em>more</em> better. Alas for them it appears that this is probably not the case. But rabid exponentialists have not yet given up. Expect a bunch of VCs to adversely affect the growth of pension funds around the world as pension funds are a prime source of capital that VCs spend.</p>
<p>More serious academics are working on boxing in the LLMs with more external mechanism beyond just feeding the output tokens back in as a linear string of input. Many of these mechanisms look a lot like more conventional AI mechanisms, and we will see where these additions prove to be useful, how much of the wheel will be reinvented, and how long (months?, years?, decades?) to get there.</p>
<p>And the answers to those last questions will tell us how much sinning has been done by companies in predicting fast deployments. Back in rant at the beginning of this post I gave the example of I.B.M. and Watson and their completely optimistic predictions of how any problems of applying Watson (which seemed extremely competent based on its performance on live TV) to the real world would be solvable. The areas that it was predicted to be applicable came from magical thinking.</p>
<p>Surely no one today could be as dumb as that big company was back in 2011. Surely not. No, not us smart inhabitants of 2025. Its us. We are nowhere near as dumb as them!!</p>
<p><em><span>Humanoid Robots</span></em></p>
<p>The other thing that has gotten over hyped in 2024 is humanoids robots. &nbsp;The rationale for humanoid robots being a thing is a product of the four sins above and I think way less rooted in reality than the hype about LLMs. In fact I think it is pretty dumb. [[I suspect many people will reason that I cannot have a valid opinion about this precisely because I happen to have built more humanoid robots than anyone else on the planet. So read ahead with caution.]]</p>
<p><a href="https://rodneybrooks.com/rodney-brooks-three-laws-of-robotics/" target="_blank" rel="noopener">My first law of robotics</a> states:</p>
<p><em>The visual appearance of a robot makes a promise about what it can do and how smart it is. It needs to deliver or slightly over deliver on that promise or it will not be accepted.</em></p>
<p>The first sentence describes, I think, what is sucking people into believing that humanoid robots have a big future. It looks like a human, so its performance will be like a human, so it will be competent like a human. &nbsp;It’s the performance/competence sin without even waiting for the performance part!</p>
<p>The second sentence describes how the humanoid fever will break, and how the hundreds of millions of dollars put into many of these companies (billions of dollars overall) will disappear. The puppets will not perform at acceptable levels. It is easy to see this as you hear all the things investors and CEOs of humanoid robots say they will be able to do. They have hardly even got to the lab demonstration phase. &nbsp;My third law of robotics is:</p>
<p><em>Technologies for robots need 10+ years of steady improvement beyond lab demos of the target tasks to mature to low cost and to have their limitations characterized well enough that they can deliver 99.9% of the time. Every 10 more years gets another 9 in reliability.</em></p>
<p>For real work, robots need to operate with four, five, or six nines. We are a long way from that. The zeitgeist is that we will simply teach the robots to do stuff and then they will be able to do it.</p>
<p>BUT, we do not know yet whether that is going to work. In order for it to work you have to both collect the <strong>right sort of data</strong> and then <strong>learn the right things</strong> from that data. It is not at all clear to me that we know the answers to make either of those things true. I think it will be an active place for lots of good research for many years to come.</p>
<p>There is an excellent survey paper of current research state of the art called <a href="https://arxiv.org/abs/2408.03539">Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes</a>. Unfortunately I think the title of the paper is going to confuse many people. “Real-World Successes” to someone like me, who these days deploys robots that people pay for and that provide real ROI, sounds like it is about systems that have been deployed. But on reading the paper it turns out that they mean that it is learning and demonstrations done in a lab setting on physical hardware rather than just in simulations and simulators. &nbsp;And, to me the lab demonstrations are shakier (literally) than I imagined in my third law above.</p>
<p>I think we are a long way off from being able to for-real deploy humanoid robots which have even minimal performance to be useable and even further off from ones that have enough ROI for people want to use them for anything beyond marketing the forward thinking outlook of the buyer.</p>
<p>Despite this, many people have predicted that the cost of humanoid robots will drop exponentially as their numbers grow, and so they will get dirt cheap. I have seen people refer to the cost of integrated circuits having dropped so much over the last few decades as proof. Not so.</p>
<p>They are committing the sin of exponentialism in an obviously dumb way. As I explained above the first integrated circuits were far from working at the limits of physics of representing information. But today’s robots use mechanical components and motors that are not too far at all from physics based limits, about mass, force, and energy. You can’t just halve the size of a motor and have a robot lift the same sized payload. Perhaps you can halve it once to get rid of inefficiencies in current designs. Perhaps. But you certainly can’t do it twice. Physical robots are not ripe for exponential cost reduction by burning wastes in current designs. And it won’t happen just because we start (perhaps) mass producing humanoid robots (oh, but the way, I already did this a decade ago–see my parting shot below). We know that from a century of mass producing automobiles. They did not get exponentially cheaper, except in the computing systems. Engines still have mass and still need the same amount of energy to accelerate good old fashioned mass.</p>
<p><em><span>This Year’s Prediction Update</span></em></p>
<div class="page" title="Page 1">
<p>There is only one new comment in my robotics, AI and ML predictions table this year. There are a bunch of well funded new companies in the home robot space, and perhaps they will come up with new mobility solutions, which in my experience is the big blocker for home robots.</p>
</div>

<table id="tablepress-31">
<thead>
<tr>
	<th>Prediction<br>
[AI and ML]</th><th>Date</th><th>2018 Comments</th><th>Updates</th>
</tr>
</thead>
<tbody>
<tr>
	<td>Academic rumblings about the limits of Deep Learning</td><td><p>BY 2017</p></td><td>Oh, this is already happening... the pace will pick up.</td><td></td>
</tr>
<tr>
	<td>The technical press starts reporting about limits of Deep Learning, and limits of reinforcement learning of game play.</td><td><p>BY 2018</p></td><td></td><td></td>
</tr>
<tr>
	<td>The popular press starts having stories that the era of Deep Learning is over.</td><td><p>BY 2020</p></td><td></td><td></td>
</tr>
<tr>
	<td>VCs figure out that for an investment to pay off there needs to be something more than "X + Deep Learning".</td><td><p>NET 2021</p></td><td>I am being a little cynical here, and of course there will be no way to know when things change exactly.</td><td></td>
</tr>
<tr>
	<td>Emergence of the generally agreed upon "next big thing" in AI beyond deep learning.</td><td><p>NET 2023</p><p>BY 2027</p></td><td>Whatever this turns out to be, it will be something that someone is already working on, and there are already published papers about it. There will be many claims on this title earlier than 2023, but none of them will pan out.</td><td><span>20240101</span> <p>It definitely showed up in 2023. It was in the public mind in December 2022, but was not yet the big thing that it became during 2023. A year ago I thought it would perhaps be neuro-symbolic AI, but clearly it is LLMs, and ChatGPT and its cousins. And, as I predicted in 2018 it was something already being worked on as the "attention is all you need" paper, the key set of ideas, was published in 2017.</p></td>
</tr>
<tr>
	<td>The press, and researchers, generally mature beyond the so-called "Turing Test" and Asimov's three laws as valid measures of progress in AI and ML.</td><td><p>NET 2022</p></td><td>I wish, I really wish.</td><td><span>20230101</span> <p>The Turing Test was missing from all the breathless press coverage of ChatGPT and friends in 2022. Their performance, though not consistent, pushes way past the old comparisons.</p> <span>20240101</span> <p>The Turing Test was largely missing from the press in 2024 also, and there was a <a href="https://www.nature.com/articles/d41586-023-02361-7" rel="noopener">story in Nature</a> commenting on that. So yes, this has now happened.</p> </td>
</tr>
<tr>
	<td>Dexterous robot hands generally available.</td><td>NET 2030<br>
BY 2040 (I hope!)</td><td>Despite some impressive lab demonstrations we have not actually seen any improvement in widely deployed robotic hands or end effectors in the last 40 years.</td><td></td>
</tr>
<tr>
	<td>A robot that can navigate around just about any US home, with its steps, its clutter, its narrow pathways between furniture, etc.</td><td>Lab demo: NET 2026<br>
Expensive product: NET 2030<br>
Affordable product: NET 2035</td><td>What is easy for humans is still very, very hard for robots.</td><td> <span>20250101</span> <p>A bunch of startups in the home robot space got significant funding in 2024. Two of them are run by ex-CEOs of large companies: iRobot and Cruise (and he was also an intern at iRobot after we were already a public company). So this one may be in play for a lab demo in the next few years if they have this as one of their goals..</p></td>
</tr>
<tr>
	<td>A robot that can provide physical assistance to the elderly over multiple tasks (e.g., getting into and out of bed, washing, using the toilet, etc.) rather than just  a point solution.</td><td>NET 2028</td><td>There may be point solution robots before that. But soon the houses of the elderly will be cluttered with too many robots.</td><td></td>
</tr>
<tr>
	<td>A robot that can carry out the last 10 yards of delivery, getting from a vehicle into a house and putting the package inside the front door.</td><td>Lab demo: NET 2025<br>
Deployed systems: NET 2028<br>
</td><td></td><td></td>
</tr>
<tr>
	<td>A conversational agent that both carries long term context, and does not easily fall into recognizable and repeated patterns.</td><td><p>Lab demo: NET 2023</p>Deployed systems: 2025</td><td>Deployment platforms already exist (e.g., Google Home and Amazon Echo) so it will be a fast track from lab demo to wide spread deployment.</td><td><span>20240101</span> <p>One half of this happened this year. ChatGPT has been connected to microphones and speakers so you can now talk to it. and It does not fall into recognizable patterns. BUT the other half is the half it does not have;  it has no updatable memory apart from its token buffer of what it has just said. Long term context may be long term in coming.</p></td>
</tr>
<tr>
	<td>An AI system with an ongoing existence (no day is the repeat of another day as it currently is for all AI systems) at the level of a mouse.</td><td>NET 2030</td><td>I will need a whole new blog post to explain this...</td><td></td>
</tr>
<tr>
	<td>A robot that seems as intelligent, as attentive, and as faithful, as a dog.</td><td>NET 2048</td><td>This is so much harder than most people imagine it to be--many think we are already there; I say we are not at all there.</td><td></td>
</tr>
<tr>
	<td>A robot that has any real idea about its own existence, or the existence of humans in the way that a six year old understands humans.</td><td>NIML</td><td></td><td></td>
</tr>
</tbody>
</table>
<!-- #tablepress-31 from cache -->
<p><span><em>A Parting Shot</em></span></p>
<p>I recently read a research paper on humanoid robots working in built for human environments. It was based on the argument that the best form for a robot that is to operate in human environments is something tallish and skinny-ish, and probably dynamically balancing, with arms that can reach down to table tops etc., and with a sensor system that can look down from above, as that is what our human environments are optimized for. Here is the first paragraph of the paper:</p>
<p>The past decade has seen an explosion of research in humanoid robotics. The stated motivations for this work have varied widely. Many teams have concentrated on bipedal locomotion, some have been interested in human level social interactions, understanding human intelligence, modeling human learning capabilities and others have been more interested in entertainment. Some humanoid robots have had manipulation capabilities on static humanoid platforms and some of that work is aimed at dexterity, plus there has been simple two armed grasping on mobile humanoid platforms. Overall there has been very little work combining dexterous manipulation with humanoid robots, static or mobile–much of that which has appeared, has been concerned with dynamic tasks like pole balancing and juggling rather than manipulation, or has used teleoperated manipulation.</p>
<p>Apart from the weird references to pole balancing and juggling this all sounds pretty reasonable and consistent with what is happening today, and with recent history. &nbsp;In fact this is the very first paragraph of the very first paper in the very first issue of the very first volume of&nbsp;the <a href="https://www.worldscientific.com/worldscinet/IJHR" target="_blank" rel="noopener">International Journal of Humanoid Robotics</a>.</p>
<p>And it was published in 2004, with me as first author. &nbsp;Let me spell that out in case you thought there was a typo in the year. This is from a paper that I and my students and post-docs wrote in the year <em>two thousand and four</em>. Here is the beginning of the contents page for that first issue.</p>
<div class="page" title="Page 1">
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/IJHR.jpg" alt="" width="1663" height="858"></p>
<p>You can download the text of that paper <a href="https://people.csail.mit.edu/brooks/papers/brooks04sensing.pdf" target="_blank" rel="noopener">here</a>. The journal&nbsp;is now in its 21<sup>st</sup> year of operation, an on its 21<sup>st</sup> volume of issues and papers.</p>
<p>By the time this paper was written my research group at MIT had been working on and building humanoid robots for twelve years. This paper, about a robot named Cardea, was probably our sixth or seventh humanoid robot. [[In 2008 I started a company that built and shipped thousands of humanoid robots. The picture at the top of this post was taken in China with a line up of humanoids that we had built in Massachusetts and New Hampshire and sold to people in China (before a US initiated trade war with China <a href="https://www.wired.com/story/a-long-goodbye-to-baxter-a-gentle-giant-among-robots/" target="_blank" rel="noopener">put an end to it in 2018</a>…irony can be personally hard to take at times…).]]</p>
<p>The robot&nbsp;Cardea (Cardea was an ancient Roman goddess of door hinges and handles; these are still a challenge for modern robots…) was a two wheeled dynamically balancing robot &nbsp;that lived in a built-for-humans office environment. Cardea was able to open doors using existing door handles and then make its way through doors it had opened.</p>
<p><strong><span>Pro tip:</span></strong>&nbsp;Just because you heard about a new idea this last year or two doesn’t mean that people haven’t been working on that very same idea for decades. So temper your expectations that it must be about to transform the world. Ideas that transform the world take decades, or centuries of development, and plenty of people long before you have been just as excited about the idea and had thought it was on the verge of taking off. And none of us, including you and me, are likely to be special enough or lucky enough to come along at just the right time to see it all happen.</p>
<p>Like all modern humanoid robots Cardea did not walk in a way that used passive dynamics to store energy, and basically modulate the behavior of a passive mechanism that had only low energy input, which is how all animals walk. So, like all modern mobile humanoid robots (and legged robots in general) when things were going awry its control algorithms tried to recover by pumping in large amounts of energy very quickly and sometimes that didn’t quite work and the energy needed to go somewhere.</p>
<p>Cardea could be a little dangerous in those circumstances, if it fell on you having just increased its kinetic energy. Even the spring based deployment system for its stick-like legs that were engaged when it realized it was going to fall could be dangerous.</p>
<p>This is still a problem with all modern humanoid robots. That is why the tele-operated humanoids that were in the Tesla movie lot theater show a couple of months ago operated in two modes. When they all walked out the human guests were kept away from them. Once they stopped walking and were operating in a very different mode people were allowed to approach them, and then get fooled into thinking they were talking to an AI powered robot when they were really talking to a remote human operator. But the robot was no longer moving its feet, and no longer a source of physical danger as a result.</p>
</div>
<p><strong><span>Another pro tip:</span></strong>&nbsp;Don’t stand anywhere near a walking or balancing wheeled humanoid when they are moving or doing any task. I have had some near misses for myself with my own humanoids twenty years ago and more recently with some of the humanoids from new start ups. And more generally never be below any sort of walking robot, no matter how many legs it has, when it is walking up stairs.</p>
<h5>HUMAN SpaceFLIGHT</h5>
<p>The numbers of flights in 2024 was not much different from those in 2023 (I neglected to include the flights by China last year). &nbsp;It does not feel like a golden age of human spaceflight, though there were other highlights from SpaceX.</p>
<p><span><em>Orbital Crewed Flights</em></span></p>
<p>Three countries put 28 people into orbit in 2024, the United States launched 16 people on five flights and Russia and China launched 6 people each with two launches. So there were nine crewed orbital flights total. Two were private and seven were government flights.</p>
<p><span><em>The United States:</em></span>&nbsp;There were four US flights to the International Space Station, starting with the private Axion-3 mission with a crew of four on January 18<sup>th</sup>. The launch vehicle for this was a SpaceX Falcon 9, and the crew vehicle was a SpaceX Dragon. The remaining US flights to the ISS were paid for by NASA. Two of them were SpaceX flights, with four people on March 4<sup>th</sup>, the Crew-8 mission, and two people on board Crew-9 on October 25<sup>th</sup>. The remaining US flight to the ISS was the inaugural crewed flight of Boeing’s Starliner, launched on June 5<sup>th</sup> atop an Atlas V rocket with two people aboard. They are still stuck in space and will be for a few more months–see the section on Boeing below.</p>
<p>The other US mission was also a SpaceX launch and vehicle flight, this time known as Polaris Dawn. It was the second mission paid for by billionaire Jared Isaacman, with him as commander. There was a former US Air Force fighter pilot as mission pilot and two SpaceX employees as mission specialists, giving a total crew size of four. They stayed aloft for five days, launching on September 10<sup>th</sup>, This mission flew higher above Earth than any mission since Apollo 17, the last lunar landing mission, in 1972. Two of the crew “spacewalked” with their feet inside the Dragon capsule but with their bodies outside. This was the first private spacewalk ever. Now Isaacman has been tapped by the incoming US President to be the administrator of NASA.</p>
<p><span><em>Russia:</em></span>&nbsp;There were two Soyuz launches, each with three people, up and down, but different people coming back. The launch dates were March 23<sup>rd</sup> and September 11<sup>the</sup>. The six people that launched on Soyuz in 2024 were 3 Russian Cosmonauts 2 NASA Astronauts and one Belarusian commercial airline flight attendant who won a national competition with 3,000 applications. She was the only one not set for a long duration mission and was off the ground for slightly less than 14 days. So there were no space tourists per so, but the Belarusian flyer was most likely included as part of Russia’s efforts to keep in good favor with Belarus which has aided it in its war in Ukraine, and was certainly not part of the regular scientific program of the ISS.</p>
<p><span><em>China:</em></span>&nbsp;There were two flights of &nbsp;Shenzhou (a larger more modern version of Soyuz) that were crewed in 2024. &nbsp;Both flights were to the Tiangong Space Station and both took along three Taikonauts, first on April 25<sup>th</sup> and then on October 9<sup>th</sup>. &nbsp;Both crews were assigned long duration missions and now the crews are overlapping previous crews at Tiangong so it is now being continuously occupied. The first handover this year took about five days and the second about three and a half weeks. &nbsp;Both times there were six Taikonauts onboard Tiangong at the same time.</p>
<p><span><em>Suborbital Crewed Flights</em></span></p>
<p>There have been two companies providing space tourism flights on suborbital flights. Blue Origin launches a capsule on top of a reusable rocket, New Shepard, and the capsule lands using a parachute and a brief rocket blast right before hitting the ground (similar to how Soyuz lands). Virgin Galactic has a winged craft which is carried aloft by a bigger a jet engined airplane, it separates at high altitude within the atmosphere and rockets into space. It flies back and lands on a runway.</p>
<p>Both companies are run by billionaires who made their money in other businesses. &nbsp;Both billionaires have flown to space on their own craft.</p>
<p>Both companies have aimed to have regular launches with lots of tourists, but neither has gotten to that scale and so far only a very small number of the many people who have paid a substantial deposit have been able to fly.</p>
<p>Blue Origin had a failure with an uncrewed version of the vehicle in 2022 and only flew one flight in 2023 which was also uncrewed. This year they flew three crewed flights on May 19<sup>th</sup>, August 29<sup>th</sup>, and November 22<sup>nd</sup>, each with six passengers (the system is automated and requires no pilots). In 2021 and 2022 they also had three flights, so there has now been nine crewed flights total. The first two took four passengers and the remaining seven have had six passengers, so altogether they have flown 50 people above the Karman line, 100 kilometers above Earth. &nbsp;This is not yet a regular cadence, nor a large scale tourist business.</p>
<p>In 2024 Virgin Galactic had two flights, each with two crew from the company and four passengers. These flights were on January 26<sup>th</sup> and June 8<sup>th</sup>. Virgin Galactic flights are now on hiatus, awaiting a new bigger and better vehicle in about two years. &nbsp;Virgin Galactic has had a total of twelve flights since December 13th in 2018. &nbsp;Three have had two people on board and nine have had six people on board, for a total of sixty filled seats that have crossed the Karman line. The total number of different people is smaller as the two pilot seats on each flight have been occupied by a small number of people who have flown multiple times.</p>
<p>So, in 2024 thirty people went on suborbital flights, and altogether there have been 110 people on these commercial suborbital flights. Space tourism on suborbital flights has yet to take off in a regular or scaled way.</p>

<table id="tablepress-32">
<thead>
<tr>
	<th>Prediction<br>
[Space]</th><th>Date</th><th>2018 Comments</th><th>Updates</th>
</tr>
</thead>
<tbody>
<tr>
	<td>Next launch of people (test pilots/engineers) on a sub-orbital flight by a private company.</td><td><p>BY 2018</p></td><td></td><td></td>
</tr>
<tr>
	<td>A few handfuls of customers, paying for those flights.</td><td><p>NET 2020</p></td><td></td><td></td>
</tr>
<tr>
	<td>A regular sub weekly cadence of such flights.</td><td><p>NET 2022</p>BY 2026</td><td></td><td><span>20240101</span> <p>There were four flights in 2021, three in 2022, and seven, five with customers on board, in 2023--all of them by Virgin Glactic. Blue Origin did not fly in 2023. At this point 2026 is looking doubtful for regular flights every week.</p><span>20250101</span> <p>Now 2026 is looking impossible given the data from 2023 and 2024, and one of the two companies being on hiatus for all of 2025, and well into 2026.</p></td>
</tr>
<tr>
	<td>Regular paying customer orbital flights.</td><td>NET 2027</td><td>Russia offered paid flights to the ISS, but there were only 8 such flights (7 different tourists). They are now suspended indefinitely. </td><td><span>20240101</span><p>There were three paid flights in 2021, and one each in 2022, and 2023, with the latter being the Axiom 2 mission using SpaceX hardware. So not regular yet, and certainly not common.</p><span>20250101</span> <p>There were two paid flights in 2024.</p></td>
</tr>
<tr>
	<td>Next launch of people into orbit on a US booster.</td><td><p>NET 2019</p><p>BY 2021</p><p>BY 2022 (2 different companies)</p><br>
</td><td>Current schedule says 2018.</td><td><span>20240101</span><p>Both SpaceX and Boeing were scheduled to have crewed flights in 2018. SpaceX pulled it off in 2020, Boeing's Starliner did not fly at all in 2023, but is scheduled to launch with people onboard for the first time in April 2024.</p><span>20250101</span> <p>The second company did finally launch humans into orbit in June 2024, so it has happened three years later than I predicted and six years later than what had been promised when my prediction was made. Of course, everyone implicitly assumed that along with getting humans into space the companies would also be able to bring them back. Not so for Boeing.</p></td>
</tr>
<tr>
	<td>Two paying customers go on a loop around the Moon, launch on Falcon Heavy.</td><td><p>NET 2020</p></td><td>The most recent prediction has been 4th quarter 2018. That is not going to happen.</td><td><span>20240101</span><p>Starship launched twice in 2023 but didn't get to orbit either time. This is going to be well over six years later than the original  prediction by the CEO of SpaceX.</p><span>20250101</span> <p>The billionaire who signed up for this and paid a hefty deposit in 2017 gave up waiting and cancelled the contract in 2024. This fantasy is over, for now at least.</p></td>
</tr>
<tr>
	<td>Land cargo on Mars for humans to use at a later date<br>
</td><td><p>NET 2026</p></td><td>SpaceX has said by 2022. I think 2026 is optimistic but it might be pushed to happen as a statement that it can be done, rather than for an pressing practical reason.</td><td><span>20240101</span><p>I was way too optimistic, and bought into the overoptimistic hype of the CEO of SpaceX even though I added four years, doubling his estimated time frame.</p><span>20250101</span> <p>I can now call this as orbital mechanics and Hohmann transfer windows dictate that the cargo would need to have been launched a few months ago for it to get to Mars in 2025. It has not been launched.</p></td>
</tr>
<tr>
	<td>Humans on Mars make use of cargo previously landed there.</td><td>NET 2032</td><td>Sorry, it is just going to take longer than every one expects.</td><td></td>
</tr>
<tr>
	<td>First "permanent" human colony on Mars.</td><td>NET 2036</td><td>It will be magical for the human race if this happens by then. It will truly inspire us all.<br>
</td><td></td>
</tr>
<tr>
	<td>Point to point transport on Earth in an hour or so (using a BF rocket).</td><td>NIML</td><td>This will not happen without some major new breakthrough of which we currently have no inkling.<br>
</td><td></td>
</tr>
<tr>
	<td>Regular service of Hyperloop between two cities.</td><td><p>NIML</p></td><td>I can't help but be reminded of when Chuck Yeager described the Mercury program as "Spam in a can".<br>
</td><td><span>20240101</span><p>Calling this one 26 years early. As of today no-one is still working on this in an operating company.</p></td>
</tr>
</tbody>
</table>
<!-- #tablepress-32 from cache -->
<p><span><em>Boeing’s Starliner</em></span></p>
<p>First announced in 2010 Boeing’s Starliner was originally scheduled to fly a human crew in 2018. It carried out its second uncrewed flight in May 2022, and finally did make its first crewed flight on June 5<sup>th</sup>. The crew of two docked with the ISS, but there were problems with multiple gas thrusters for fine motion during the docking. The original plan was that the crew would stay on the ISS for about a week and then return to Earth for a touchdown on to hard soil (as all Russian and Chinese crewed missions end along with all Blue Origin sub-orbital flights).</p>
<p>The option of that return was considered, but the thrusters were on a section of the vehicle which is discarded along the way before the landing so there was no possibility of getting a look at the hardware back on Earth. &nbsp;So a program of tests while docked to the ISS was started delaying the crew return.</p>
<p>Eventually it was decided that it was too risky for the crew to return on the craft and so it returned empty on &nbsp;September&nbsp;7<sup>th</sup>, landing in New Mexico. As it happened, although there were more anomalies with the thrusters the crew would have landed safely had they been on board.</p>
<p>Now the crew was stranded in space with no designated ride home. It was decided to remove two crew from the Crew-9 launch and have the Starliner astronauts, Barry Wilmore and Sunita Williams, fly back on that SpaceX Dragon with the other two, which after additional delays is now scheduled to happen some time in March 2025. Their one week visit to the ISS will have stretched out to nine months by then.</p>
<p>Boeing has committed to fixing the problems with Starliner. The boosters that it uses are no longer being built, but there are five existing ones reserved for the five additional contracted flights that Boeing has with NASA. They are supposed to happen once per year.</p>
<p>We do not know at this point, but I think it would not be a huge surprise if Starliner never flies again.</p>
<p><span><span><i>SpaceX Falcon 9&nbsp;</i></span></span></p>
<p>Once again the Falcon 9 launch system has broken all sorts of records for number of launches and reuse.</p>
<p>During 2024 there were 132 single booster launches. &nbsp;For two of those flights no attempt was made to recover the first stage (there is a performance penalty for the primary payload in order to recover the first stage). One attempted recovery failed when the booster (on its 23<sup>rd</sup> flight) caught fire as it landed on the recovery barge. Another booster has since flown a total of 24 times.</p>
<p>In terms of mission success all but one of these flights succeeded; one failed when the second stage failed during re-ignition for adjusting the orbit.</p>
<p>There were also two Falcon Heavy, the three booster version, launches, both of which succeeded. One of the had successful landings for the two side boosters, but there was no attempt to recoer the central booster on that flight and no attempt to recover any of the three boosters on the other Heavy flight.</p>
<p>This brings the total number of launches of the single booster version to 417 along with 11 launches of the three booster Heavy version. &nbsp;These numbers are way beyond the number of launches for any other orbital booster. &nbsp;Additionally it is the only flying orbital system that is reusable at the moment, though &nbsp;Blue Origin and Rocket Lab both plan on joining the club soon.</p>
<p>It is worth, once again, looking at how long it has taken to get to a total (across both single booster and Heavy triple booster versions) of 428 launches, with only three failures to deliver the payload to where it was intended to go.</p>
<p>The first launch occured in June 2010, and there were a total of 4 launches in the first three years. &nbsp;The first successful booster recover happened on the 20th flight, in December 2015, five and a half years in. The first reuse of a booster occured in 2017, in the 8<sup>th</sup> year of the program.</p>
<p>Since 2021 there has been a steady increase in the number of launches per year,</p>

<table id="tablepress-33">
<thead>
<tr>
	<th>Year</th><th># of launches</th>
</tr>
</thead>
<tbody>
<tr>
	<td>2010</td><td>2</td>
</tr>
<tr>
	<td>2011</td><td>0</td>
</tr>
<tr>
	<td>2012</td><td>2</td>
</tr>
<tr>
	<td>2013</td><td>3</td>
</tr>
<tr>
	<td>2014</td><td>6</td>
</tr>
<tr>
	<td>2015</td><td>7</td>
</tr>
<tr>
	<td>2016</td><td>8</td>
</tr>
<tr>
	<td>2017</td><td>18<br>
</td>
</tr>
<tr>
	<td>2018</td><td>21</td>
</tr>
<tr>
	<td>2019</td><td>13</td>
</tr>
<tr>
	<td>2020</td><td>26</td>
</tr>
<tr>
	<td>2021</td><td>31</td>
</tr>
<tr>
	<td>2022</td><td>61</td>
</tr>
<tr>
	<td>2023</td><td>96</td>
</tr>
<tr>
	<td>2024</td><td>134</td>
</tr>
</tbody>
</table>
<!-- #tablepress-33 from cache -->
<p>SpaceX had previously gotten satellites to orbit with its first rocket, the Falcon 1. &nbsp;Falcon 9 has been a spectacular success. &nbsp;But it was not instantaneous. &nbsp;It took time to build from the cadence of launches, about 10 years before the hockey stick curve showed up. &nbsp;Deployment is never sudden but comes after a long build.</p>
<p><em><span>SpaceX Starship</span></em></p>
<p>Starship is SpaceX’s superheavy two stage rocket, designed to put 150 tons of payload into orbit, but also be able to go to the Moon or Mars. There is the booster which is designed only to work in Earth atmosphere with 33 Raptor engines both to get the second stage high enough and fast enough and to let the first stage have a controlled return to the launch site. The second stage, called Starship, is both a booster and the payload. &nbsp;It has three Raptor engines and three Raptor vacuum engines. The Raptor engines are designed to get the Starship into orbit after the first stage drops away, and to guide the Starship as it returns to its Earth launch site. The Raptor vacuum engines are meant for breaking out of Earth orbit and going to the Moon or Mars, and to do soft landings on those two bodies where there is no or almost no atmosphere.</p>
<p>In 2024 SpaceX made steady progress with four launches of the two stages coupled together. &nbsp;The first two launches lead to both stages blowing up.</p>
<p>The third and fourth launches were a big improvement. &nbsp;As with earlier flights they launched from the coast of Texas. In both cases the second stage did a reentry burn on it first orbit and then did a soft landing in a target zone in the Indian Ocean. &nbsp;In the third flight the main booster returned to the launch site and hovered next to the launch tower betweeen two giant arms which then captured it and the engines shot down successfully. It was sifficiently damaged during flight however, that it was not reusable. In the fourth flight there were health anomalies to the first stage was ditched in the Gulf of Mexico.</p>
<p>On the fourth flight there was both less heat shielding and much less damage from heat during reentry. This is definite forward progress. But it is still quite a long way from both being operational and both stages being reusable. And it is even further away from being human rated.</p>
<p>This is the vehicle that the CEO of SpaceX recently said would be launched to Mars and attempt a soft landing there. &nbsp;He also said that if successful the humans would fly to Mars on it in 2030. These are enormously ambitious goals just from a maturity of technology standpoint. The real show stopper however may be human physiology as evidence accumulates that humans would not survive three years (the minimum duration of a Mars mission, due to orbital mechanics) in space with current shielding practices and current lack of gravity on board designs. Those two challenges may take decades, or even centuries to overcome (recall that Leonardo Da Vinci had designs for flying machines that took centuries to be developed…).</p>
<p>The President of SpaceX may be taking a leaf out of the CEO’s always overly optimistic predictions. In November she said <a href="https://www.dailystar.co.uk/news/us-news/spacex-president-says-could-easily-34143695" target="_blank" rel="noopener">“I would not be surprised if we fly 400 Starship launches in the next four years”</a>. Looking at the success of Falcon 9 it is certainly plausible that I may live to see 400 Starship launches in a four year period, but I am quite confident that it will not happen in the&nbsp;<strong>next</strong> four years (2025 through 2028).</p>
<p><em><strong>One more thing.</strong></em> Back when I first made the predictions there had been an announcement by the CEO of SpaceX that in 2018 the company was under contract to send a very rich paying customer in a trip around the moon in 2018, launched on a Falcon Heavy. I was completely skeptical. Over the years the date got pushed back and pushed back, and the proposed flight vehicle was changed to be Starship. As we all know the flight of the Japanese billionaire around the Moon still hasn’t happened. In 2024 Yusaku Maezawa finally gave up waiting and <a href="https://www.space.com/japanese-billionaire-cancels-spacex-starship-moon-dearmoon-flight" target="_blank" rel="noopener">cancelled the contract</a>.</p>
<p><span><em>NASA Artemis</em></span></p>
<p>NASA’s plan is that the second Artemis mission, using the Orion Capsule, Artemis II, will fly to the Moon with four people aboard, the first crewed Artemis flight. An uncrewed flight of Orion around the Moon flew in 2022. &nbsp;The crewed flight was scheduled to launch in May 2024, but it was first delayed by six months and then a little more and in the last year it has slipped another full year. It is now scheduled to fly in April 2026.</p>
<p>Artemis III was scheduled to launch in 2025 with a return to the surface of the Moon. However that relied on using a Starship (itself refueled in LEO by 14 (yes, <em><strong>fourteen</strong></em>!!) other Starship launches) to land there. &nbsp;No one any longer believes that schedule, and willlikely delay a few years, given where Starship is in its development and current capability. &nbsp;The officieal schedule says mid 2027, but that seems unlikely.</p>
<p>You can find the architecture of the Artemis III mission at this <a href="https://www.nasa.gov/missions/artemis/artemis-iii/" target="_blank" rel="noopener">website</a>.</p>
<p><span><em>Blue Origin Orbital BE-4 Engines and New Glenn</em></span></p>
<p>The suborbital tourist flights that Blue Origin operates are not its main business. It has ambitions to compete head to head with SpaceX. Another billionaire vs billionaire competition.</p>
<p>It has developed the BE-4 engine designed to fly 100 times, and to power the first stage of its massive New Glenn rocket (see below). &nbsp;But in the meantime it has started selling the BE-4 to ULA (United Launch Alliance) to power their Vulcan Centaur heavy launch vehicle. It’s first stage uses two BE-4 engines, along with a variable number of solid fuel strap ons.</p>
<p>Vulcan Centaur flew two times in 2024 and the BE-4 engines worked perfectly both times, on January 8<sup>th</sup> and again on October 4<sup>th</sup>. This is a solid validation of the engine’s capabilities.</p>
<p>Blue Origin’s own first orbital class rocket, New Glenn, is massive, and comparable to the Flacon Heavy (three boosters) rather than the Falcon 9 in capability. &nbsp; It has been in development for a long time, but saw its first visits to a launch pad, fully stacked in 2024. The first stage uses seven BE-4 engines, and is intended to land on a barge and be fully reusable. The second stage uses two BE-3U engines, a variant of the single engine used on their New Shepard sub-orbital space tourism vehicle. There is a project underway to make a fully reusable version of the second stage.</p>
<p>Launch seems imminent. &nbsp;Here it is at the launch pad in November 2024.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/image.jpeg" alt="" width="571" height="876"></p>
<p>On Friday December 27<sup>th</sup>, 2024, it was fully fueled in both stages and <a href="https://www.nytimes.com/2024/12/27/science/new-glenn-blue-origin.html" target="_blank" rel="noopener">went through a countdown and fired its seven BE-4 engines for 24 seconds</a>. Now it will leave the pad to have its payload installed. The launch could be as early January 6<sup>th</sup>. &nbsp;The very first launch will be an all up affair, attempting to get something to orbit and land the booster on its first flight. This is a very different development approach to that used by SpaceX.</p>
<p><span><em>Let’s Continue a Noble Tradition!</em></span></p>
<p>The billionaire founders of both Virgin Galactic and Blue Origin had faith in the systems they had created. They both personally flew on the first operational flights of their sub-orbital launch systems. They went way beyond simply talking about how great their technology was, they believed in it, and flew in it.</p>
<p>Let’s hope this tradition continues. Let’s hope the billionaire founder/CEO of SpaceX will be onboard the first crewed flight of Starship to Mars, and that it happens sooner than I expect. We can all cheer for that.</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article><!-- #post-## -->

<!-- .comments-area -->

	
		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Candy Crush, Tinder, MyFitnessPal: Apps hijacked to spy on location (150 pts)]]></title>
            <link>https://www.wired.com/story/gravy-location-data-app-leak-rtb/</link>
            <guid>42651115</guid>
            <pubDate>Fri, 10 Jan 2025 00:00:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/gravy-location-data-app-leak-rtb/">https://www.wired.com/story/gravy-location-data-app-leak-rtb/</a>, See on <a href="https://news.ycombinator.com/item?id=42651115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Some of the world’s most popular apps are likely being co-opted by rogue members of the advertising industry to harvest sensitive location data on a massive scale, with that data ending up with a location data company whose subsidiary has previously sold global location data to US law enforcement.</p><p>The thousands of apps, <a data-offer-url="https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/&quot;}" href="https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/" rel="nofollow noopener" target="_blank">included in hacked files</a> from location data company Gravy Analytics, include everything from games like <em>Candy Crush</em> and dating apps like Tinder to pregnancy tracking and religious prayer apps across both Android and iOS. Because much of the collection is occurring through the advertising ecosystem—not code developed by the app creators themselves—this data collection is likely happening without users’ or even app developers’ knowledge.</p><p>“For the first time publicly, we seem to have proof that one of the largest data brokers selling to both commercial and government clients appears to be acquiring their data from the online advertising ‘bid stream,’” rather than code embedded into the apps themselves, Zach Edwards, senior threat analyst at cybersecurity firm Silent Push and who has followed the location data industry closely, tells 404 Media after reviewing some of the data.</p><p>The data provides a rare glimpse inside the world of real-time bidding (RTB). Historically, location data firms <a data-offer-url="https://www.vice.com/en/article/us-military-location-data-xmode-locate-x/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/us-military-location-data-xmode-locate-x/&quot;}" href="https://www.vice.com/en/article/us-military-location-data-xmode-locate-x/" rel="nofollow noopener" target="_blank">paid app developers</a> to include bundles of code that collected the location data of their users. Many companies have turned instead to <a data-offer-url="https://www.wsj.com/tech/cybersecurity/how-ads-on-your-phone-can-aid-government-surveillance-943bde04?ref=404media.co" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.wsj.com/tech/cybersecurity/how-ads-on-your-phone-can-aid-government-surveillance-943bde04?ref=404media.co&quot;}" href="https://www.wsj.com/tech/cybersecurity/how-ads-on-your-phone-can-aid-government-surveillance-943bde04?ref=404media.co" rel="nofollow noopener" target="_blank">sourcing location information through the advertising ecosystem</a>, where companies bid to place ads inside apps. But a side effect is that data brokers can listen in on that process and harvest the location of peoples’ mobile phones.</p><p>“This is a nightmare scenario for privacy, because not only does this data breach contain data scraped from the RTB systems, but there's some company out there acting like a global honey badger, doing whatever it pleases with every piece of data that comes its way,” Edwards says.</p><p>Included in the hacked Gravy data are tens of millions of mobile phone coordinates of devices inside the US, Russia, and Europe. Some of those files also reference an app next to each piece of location data. 404 Media extracted the app names and built a list of mentioned apps.</p><p>The list includes dating sites Tinder and Grindr; massive games such as <em>Candy Crush</em>, <em>Temple Run</em>, <em>Subway Surfers</em>, and <em>Harry Potter: Puzzles &amp; Spells</em>; transit app Moovit; My Period Calendar &amp; Tracker, a period-tracking app with more than 10 million downloads; popular fitness app MyFitnessPal; social network Tumblr; Yahoo’s email client; Microsoft’s 365 office app; and flight tracker Flightradar24. The list also mentions multiple religious-focused apps such as Muslim prayer and Christian Bible apps, various pregnancy trackers, and many VPN apps, which some users may download, ironically, in an attempt to protect their privacy.</p><p>The full list can be found <a href="https://docs.google.com/spreadsheets/d/1Ukgd0gIWd9gpV6bOx2pcSHsVO6yIUqbjnlM4ewjO6Cs/edit?usp=sharing">here</a>. Multiple security researchers <a data-offer-url="https://pastejustit.com/atnbotturr" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://pastejustit.com/atnbotturr&quot;}" href="https://pastejustit.com/atnbotturr" rel="nofollow noopener" target="_blank">have published</a> <a data-offer-url="https://gist.github.com/fs0c131y/f498b21cba9ee23956fc7d7629262e9d" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://gist.github.com/fs0c131y/f498b21cba9ee23956fc7d7629262e9d&quot;}" href="https://gist.github.com/fs0c131y/f498b21cba9ee23956fc7d7629262e9d" rel="nofollow noopener" target="_blank">other lists</a> of apps included in the data, of varying sizes. Our version is relatively larger because it includes both Android and iOS apps, and we decided to keep duplicate instances of the same app that had slight name variations to make it easier for readers to search for apps they have installed.</p><p>Although this dataset came from an apparent hack of Gravy, it is not clear whether Gravy collected this location data itself or sourced it from another company, or which location company ultimately owns it or is licensed to use it.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Much of the location data attached to these app names does not have a time stamp. But there are indications it dates from 2024. One of the apps listed is <em>Call of Duty: Mobile</em>, and specifically its Season 5 iteration, <a data-offer-url="https://www.callofduty.com/blog/2024/05/call-of-duty-mobile-season-5-digital-dusk-operators-map-challenge-announcement" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.callofduty.com/blog/2024/05/call-of-duty-mobile-season-5-digital-dusk-operators-map-challenge-announcement&quot;}" href="https://www.callofduty.com/blog/2024/05/call-of-duty-mobile-season-5-digital-dusk-operators-map-challenge-announcement" rel="nofollow noopener" target="_blank">which launched in May 2024</a>.</p><p>Gravy is a company that powers much of the rest of the location data industry. It collates mobile phone location data from various sources, then sells that to commercial companies or, through its subsidiary Venntel, to US government agencies. Norwegian outlet NRK and I <a data-offer-url="https://www.vice.com/en/article/ice-dhs-fbi-location-data-venntel-apps/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/ice-dhs-fbi-location-data-venntel-apps/&quot;}" href="https://www.vice.com/en/article/ice-dhs-fbi-location-data-venntel-apps/" rel="nofollow noopener" target="_blank">previously revealed the flow of location data</a> from a handful of ordinary apps to Gravy and then to Venntel. Venntel’s clients have included <a data-offer-url="https://www.wsj.com/articles/federal-agencies-use-cellphone-location-data-for-immigration-enforcement-11581078600" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.wsj.com/articles/federal-agencies-use-cellphone-location-data-for-immigration-enforcement-11581078600&quot;}" href="https://www.wsj.com/articles/federal-agencies-use-cellphone-location-data-for-immigration-enforcement-11581078600" rel="nofollow noopener" target="_blank">Immigration and Customs Enforcement, Customs and Border Protection</a>, <a data-offer-url="https://www.vice.com/en/article/irs-investigation-location-data-no-warrant-venntel/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/irs-investigation-location-data-no-warrant-venntel/&quot;}" href="https://www.vice.com/en/article/irs-investigation-location-data-no-warrant-venntel/" rel="nofollow noopener" target="_blank">the IRS</a>, <a data-offer-url="https://theintercept.com/2020/06/24/fbi-surveillance-social-media-cellphone-dataminr-venntel/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://theintercept.com/2020/06/24/fbi-surveillance-social-media-cellphone-dataminr-venntel/&quot;}" href="https://theintercept.com/2020/06/24/fbi-surveillance-social-media-cellphone-dataminr-venntel/" rel="nofollow noopener" target="_blank">the FBI</a>, <a data-offer-url="https://www.vice.com/en/article/dea-venntel-location-data/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/dea-venntel-location-data/&quot;}" href="https://www.vice.com/en/article/dea-venntel-location-data/" rel="nofollow noopener" target="_blank">and the Drug Enforcement Administration</a>.</p><p>Venntel has also provided the underlying data for another government-bought surveillance tool called Locate X. <a data-offer-url="https://www.404media.co/inside-the-u-s-government-bought-tool-that-can-track-phones-at-abortion-clinics/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/inside-the-u-s-government-bought-tool-that-can-track-phones-at-abortion-clinics/&quot;}" href="https://www.404media.co/inside-the-u-s-government-bought-tool-that-can-track-phones-at-abortion-clinics/" rel="nofollow noopener" target="_blank">404 Media and a group of other outlets</a> showed last year how that tool, made by a company called Babel Street, could be used to monitor visitors to out-of-state abortion clinics.</p><p>But the newly hacked data shows for the first time just how many apps could be part of a location data supply chain, even if their developers are not aware of it. Most app developers and companies included in the list did not respond to a request for comment. Flightradar24 said in an email that it had never heard of Gravy, but that it does display ads, which “help keep Flightradar24 free.”</p><p>Tinder said in an email that “Tinder takes safety and security very seriously. We have no relationship with Gravy Analytics and have no evidence that this data was obtained from the Tinder app” but did not answer questions about ads inside the app.</p><p>Muslim Pro, one of the Muslim prayer apps included in the list, said in an email that it was not aware of Gravy. “Yes, we display ads through several ad networks to support the free version of the app. However, as mentioned above, we do not authorize these networks to collect location data of our users,” the email said. That does not necessarily mean that a member of the advertising ecosystem can’t extract such data, though. (In 2020 <a data-offer-url="https://www.vice.com/en/article/muslim-pro-location-data-military-xmode/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/muslim-pro-location-data-military-xmode/&quot;}" href="https://www.vice.com/en/article/muslim-pro-location-data-military-xmode/" rel="nofollow noopener" target="_blank">I revealed Muslim Pro was selling its users’ location data</a> to a company called X-Mode, whose clients included US military contractors; Muslim Pro stopped the practice after my reporting.)</p><p>A Grindr spokesperson told 404 Media in an email that “Grindr has never worked with or provided data to Gravy Analytics. We do not share data with data aggregators or brokers and have not shared geolocation with ad partners for many years. Transparency is at the core of our privacy program, therefore the third parties and service providers we work with are listed here <a data-offer-url="https://www.grindr.com/privacy-policy/third-parties/en-us" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.grindr.com/privacy-policy/third-parties/en-us&quot;}" href="https://www.grindr.com/privacy-policy/third-parties/en-us" rel="nofollow noopener" target="_blank">on our website</a>.” Grindr <a data-offer-url="https://lamag.com/news/grindr-sold-its-user-data-for-years-revealing-personal-information" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://lamag.com/news/grindr-sold-its-user-data-for-years-revealing-personal-information&quot;}" href="https://lamag.com/news/grindr-sold-its-user-data-for-years-revealing-personal-information" rel="nofollow noopener" target="_blank">was previously found to have allowed data brokers</a> to obtain its users’ location data.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>It’s important that the data appears to be sourced through real-time bidding, because that dictates who is responsible (rogue members of the advertising industry and the tech giants that facilitate that industry), how users can protect themselves (attempting to block ads), and the fact that massive app publishers may not even be aware their users’ data is being harvested and therefore might not know how to stop it. An app developer will know if it implemented location-data-gathering code itself. It might not know that some company, somewhere, is silently listening in on the advertising process and siphoning data from their app.</p><p>Surveillance firms can obtain RTB data by <a data-offer-url="https://www.bloomberg.com/news/articles/2023-05-11/surveillance-company-turns-ad-data-into-government-tracking-tool?ref=404media.co" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.bloomberg.com/news/articles/2023-05-11/surveillance-company-turns-ad-data-into-government-tracking-tool?ref=404media.co&quot;}" href="https://www.bloomberg.com/news/articles/2023-05-11/surveillance-company-turns-ad-data-into-government-tracking-tool?ref=404media.co" rel="nofollow noopener" target="_blank">acquiring ad tech companies</a> and posing as prospective advertisers. The spy-run location data company doesn’t need to successfully place an ad; instead, it is able to gather data on devices by simply being plugged into that industry. Location data in this case can also include a users’ IP address, which is then geolocated to give their coarse location.</p><p>Last January, <a data-offer-url="https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/&quot;}" href="https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/" rel="nofollow noopener" target="_blank">404 Media reported</a> on an Israeli surveillance company called Patternz, which was sourcing masses of location data through the RTB process.</p><p>In an exposed training video, Patternz showed some of the popular apps it got location data from: 9GAG, Kik, sports app FUTBIN, caller ID apps such as CallApp and Truecaller; and various word, sudoku, and solitaire puzzle games. Every one of these apps are also in the Gravy data. This suggests that Gravy, or wherever Gravy got that data from, also sourced it from interacting with the advertising system rather than location-tracking code baked into the apps.</p><p>404 Media shared some of the location data with another security researcher with knowledge of the advertising and location data industries. “It appears that at least some of this data would likely have been sourced from advertising related, real-time bidding,” Krzysztof Franaszek, founder of Adalytics, a digital forensics firm, told 404 Media after reviewing the data. He pointed out some of the user-agents in the file, which show how a user’s device connected to a service, referenced “afma-sdk.” That is a string <a href="https://ads-developers.googleblog.com/2022/07/use-new-google-mobile-ads-sdk.html">used by Google’s Mobile Ads SDK</a> (software development kit). In other words, in some cases, it is Google’s advertising platform that is delivering the ads that are eventually leading to this tracking by outside companies and potentially government contractors.</p><p>Google did not respond to multiple requests for comment for this article. Neither did Apple.</p><p>Franaszek also says that “a significant amount of this geolocation dataset appears to be inferred by IP address to geolocation lookups, meaning the vendor or their source is deriving the user's geolocation by checking their IP address rather than by using GNSS [Global Navigation Satellite System]/GPS data. That would suggest that the data is not being sourced entirely from a location data SDK.”</p><p>“What we’re seeing here in this data appears to me to be a huge diversity of apps,” Edwards says. “That’s not what you see from an SDK ingestion; that’s what you see from bulk RTB ingestions.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In December <a href="https://www.ftc.gov/news-events/news/press-releases/2024/12/ftc-takes-action-against-mobilewalla-collecting-selling-sensitive-location-data">the Federal Trade Commission banned another location data company</a> called Mobilewalla from collecting consumer data “from online advertising auctions for purposes other than participating in those auctions.” In other words, the agency banned Mobilewalla from participating in the RTB process for building datasets on peoples’ devices. The <a data-offer-url="https://www.404media.co/ftc-bans-location-data-company-that-powers-the-surveillance-ecosystem/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/ftc-bans-location-data-company-that-powers-the-surveillance-ecosystem/&quot;}" href="https://www.404media.co/ftc-bans-location-data-company-that-powers-the-surveillance-ecosystem/" rel="nofollow noopener" target="_blank">FTC also said Venntel and Gravy collected data</a> without obtaining user consent, ordered the company to delete historical location data, and banned it from selling data related to sensitive areas like health clinics and places of worship, except in “limited circumstances” involving national security or law enforcement.</p><p>404 Media has verified the hacked Gravy data in various ways. Some files include credentials for Gravy’s Snowflake instances, a data warehousing tool. 404 Media checked that the URLs in the hacked files do correspond to real Snowflake instances. One file called “users” contains a long list of companies. Some of these firms denied having any relationship with Gravy; Cuebiq, another location data firm mentioned in the file, told 404 Media it “routinely evaluates available data in the market to determine if they are an appropriate fit for our business. Most do not make it past the evaluation stage to production, as was the case here. Cuebiq tested a limited data sample, which was never made available to our customers, and the data was deleted at the end of the limited trial.”</p><p>404 Media also sent a section of the data to another data broker called Datonics. “We investigated the matter described in your email, and the segment IDs in those files are those of Gravy, not Datonics,” the company said in an email.</p><p>Unacast, <a data-offer-url="https://www.prnewswire.com/news-releases/gravy-analytics-and-unacast-merge-to-become-leader-in-location-data-and-insights-302000184.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.prnewswire.com/news-releases/gravy-analytics-and-unacast-merge-to-become-leader-in-location-data-and-insights-302000184.html&quot;}" href="https://www.prnewswire.com/news-releases/gravy-analytics-and-unacast-merge-to-become-leader-in-location-data-and-insights-302000184.html" rel="nofollow noopener" target="_blank">which merged with Gravy in 2023</a>, did not respond to multiple requests for comment, both on the hack and whether it or any of its suppliers have derived location data from the real-time bidding process.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disappointed with the TVs at CES 2025 (149 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/</link>
            <guid>42650855</guid>
            <pubDate>Thu, 09 Jan 2025 23:20:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/">https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/</a>, See on <a href="https://news.ycombinator.com/item?id=42650855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2069712">
  
  <header>
  <div>
    <div>
      <div>
        <p><span>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="section-gadgets_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath><clipPath id="section-gadgets_svg__b"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g clip-path="url(#section-gadgets_svg__a)"><g fill="currentColor" clip-path="url(#section-gadgets_svg__b)"><path d="M38 22c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2V4h-4V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2S8 .9 8 2v2H4v4H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v4h4v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h4v-4h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6zm-6 10H8V8h24z"></path><path d="M24.7 17.3 20 12h-7.1c-.6 0-1 .4-1 1s.4 1 1 1h6.3l4.1 4.7L20 22h8v-8z"></path><path d="m15.2 22.7 4.7 5.3H27c.6 0 1-.4 1-1s-.4-1-1-1h-6.3l-4.1-4.7 3.3-3.3h-8v8z"></path></g></g></svg>
  </span>
  <span>
    Won't someone please think of the viewer?
  </span>
</p>
      </div>

      

      <p>
        Op-ed: TVs miss opportunity for real improvement by prioritizing corporate needs.
      </p>

      
    </div>

    <div>
    
    <p>
      The TV industry is hitting users over the head with AI and other questionable gimmicks 

              <span>
          Credit:

          
          Getty

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>If you asked someone what they wanted from <a href="https://arstechnica.com/gadgets/2024/12/buying-a-tv-in-2025-expect-lower-prices-more-ads-and-an-os-war/">TVs released in 2025</a>, I doubt they'd say "more software and AI." Yet, if you look at what TV companies have planned for this year, which is being primarily promoted at the CES technology trade show in Las Vegas this week, software and AI are where much of the focus is.</p>
<p>The trend reveals the implications of TV brands increasingly viewing themselves as software rather than hardware companies, with their products being customer data rather than TV sets. This points to an alarming future for smart TVs, where even premium models sought after for top-end image quality and hardware capabilities are stuffed with unwanted gimmicks.</p>
<h2>LG’s remote regression</h2>
<p>LG has long made some of the best—and most expensive—TVs available. Its OLED lineup, in particular, has appealed to people who use their TVs to watch Blu-rays, enjoy HDR, and the like. However, some features that LG is introducing to high-end TVs this year seem to better serve LG’s business interests than those users' needs.</p>
<p>Take the new remote. Formerly known as the Magic Remote, LG is calling the 2025 edition the AI Remote. That is already likely to dissuade people who are skeptical about AI marketing in products (<a href="https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2368040">research suggests</a> there are many such people). But the more immediately frustrating part is that the new remote doesn’t have a dedicated button for switching input modes, as previous remotes from LG and countless other remotes do.</p>
<figure>
    <p><img width="640" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-640x360.jpg" alt="LG AI remote" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1440x810.jpg 1440w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote.jpg 1920w" sizes="auto, (max-width: 640px) 100vw, 640px">
                  </p>
          <figcaption>
        <div><p>
      LG's AI Remote.

              <span>
          Credit:

                      <a href="https://www.youtube.com/watch?v=z_KexjTLETo" target="_blank">
          
          Tom's Guide/YouTube

                      </a>
                  </span>
          </p></div>
      </figcaption>
      </figure>

<p>To use the AI Remote to change the TV’s input—a common task for people using their sets to play video games, watch Blu-rays or DVDs, connect their PC, et cetera—you have to long-press the Home Hub button. Single-pressing that button brings up a dashboard of webOS (the operating system for LG TVs) apps. That functionality isn't immediately apparent to someone picking up the remote for the first time and detracts from the remote’s convenience.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>By overlooking other obviously helpful controls (play/pause, fast forward/rewind, and numbers) while including buttons dedicated to things like LG's free ad-supported streaming TV (FAST) channels and Amazon Alexa, LG missed an opportunity to update its remote in a way centered on how people frequently use TVs. That said, it feels like user convenience didn't drive this change. Instead, LG seems more focused on getting people to use webOS apps. LG can monetize app usage through, i.e., getting a cut of streaming subscription sign-ups, selling <a href="https://arstechnica.com/gadgets/2024/09/lg-tvs-continue-down-advertising-rabbit-hole-with-new-screensaver-ads/">ads on webOS</a>, and selling and<a href="https://arstechnica.com/gadgets/2024/10/streaming-industry-has-unprecedented-surveillance-manipulation-capabilities/">&nbsp;leveraging user data</a>.</p>
<h2>Moving from hardware provider to software platform</h2>
<p>LG, like many other TV OEMs, has been growing its ads and data business. Deals with data analytics firms like Nielsen give it more incentive to acquire customer data. Declining TV margins and rock-bottom prices from budget brands (like Vizio and Roku, which sometimes lose money on TV hardware sales and make up for the losses through ad sales and data collection) are also pushing LG's software focus. In the case of the AI Remote, software prioritization comes at the cost of an oft-used hardware capability.</p>
<p>Further demonstrating its motives, in September 2023, LG announced intentions to "become a media and entertainment platform company" by offering "services" and a "collection of curated content in products, including LG OLED and LG QNED TVs." At the time, the South Korean firm said it would invest 1 trillion KRW (about $737.7 million) into its webOS business through 2028.</p>
<p>Low TV margins, improved TV durability, market saturation, and broader economic challenges are all serious challenges for an electronics company like LG and have pushed LG to explore alternative ways to make money off of TVs. However, after paying four figures for TV sets, LG customers shouldn't be further burdened to help LG accrue revenue.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<h2>Google TVs gear up for subscription-based features</h2>
<p>There are numerous TV manufacturers, including Sony, TCL, and Philips, relying on Google software to power their TV sets. Numerous TVs announced at CES 2025 will come with what Google calls Gemini Enhanced Google Assistant. The idea that this is something that people using Google TVs have requested is somewhat contradicted by Google Assistant interactions with TVs thus far being “somewhat limited,” per a <a href="https://www.lowpass.cc/p/google-tv-far-field-microphones-ces-gemini">Lowpass</a> report.</p>
<p>Nevertheless, these TVs are adding far-field microphones so that they can hear commands directed at the voice assistant. For the first time, the voice assistant will include Google’s generative AI chatbot, Gemini, this year—another feature that TV users don’t typically ask for. Despite the lack of demand and the privacy concerns associated with microphones that can pick up audio from far away even when the TV is off, companies are still loading 2025 TVs with far-field mics to support Gemini. Notably, these TVs will likely allow the mics to be disabled, like you can with <a href="https://arstechnica.com/gadgets/2022/09/amazons-self-branded-tvs-get-fancier-with-quantum-dots-local-dimming/">other TVs using far-field mics.</a> But I still ponder about features/hardware that could have been implemented instead.</p>
<p>Google is also working toward having people pay a subscription fee to use Gemini on their TVs, <a href="https://www.pcworld.com/article/2568513/google-hopes-youll-pay-for-an-ai-tv-assistant-someday.html">PCWorld</a> reported.</p>
<p>“For us, our biggest goal is to create enough value that yes, you would be willing to pay for [Gemini],” Google TV VP and GM Shalini Govil-Pai told the publication.</p>
<p>The executive pointed to future capabilities for the Gemini-driven Google Assistant on TVs, including asking it to “suggest a movie like <em>Jurassic Park </em>but suitable for young children” or to show “Bollywood movies that are similar to <em>Mission: Impossible</em>.”</p>
<p>She also pointed to future features like showing weather, top news stories, and upcoming calendar events when someone is near the TV, showing AI-generated news briefings, and the ability to respond to questions like “explain the solar system to a third-grader” with text, audio, and YouTube videos.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>But when people have desktops, laptops, tablets, and phones in their homes already, how helpful are these features truly? Govil-Pai admitted to PCWorld that “people are not used to” using their TVs this way “so it will take some time for them to adapt to it.” With this in mind, it seems odd for TV companies to implement new, more powerful microphones to support features that Google acknowledges aren't in demand. I’m not saying that tech companies shouldn’t get ahead of the curve and offer groundbreaking features that users hadn’t considered might benefit them. But already planning to monetize those capabilities—with a subscription, no less—suggests a prioritization of corporate needs.</p>
<h2>Samsung is hungry for AI</h2>
<p>People who want to use their TV for cooking inspiration often turn to cooking shows or online cooking videos. However, Samsung wants people to use its TV software to identify dishes they want to try making.</p>
<p>During CES, Samsung announced Samsung Food for TVs. The feature leverages Samsung TVs’ AI processors to identify food displayed on the screen and recommend relevant recipes. Samsung introduced the capability in 2023 as an iOS and Android app after buying the app Whisk in 2019. As noted by <a href="https://techcrunch.com/2025/01/05/samsungs-new-tvs-can-find-recipes-for-dishes-in-shows/">TechCrunch</a>, though, <a href="https://www.tomsguide.com/ai/i-gave-claude-chatgpt-and-gemini-a-photo-of-some-ingredients-to-see-which-came-up-with-the-best-recipe-heres-the-results">other AI tools</a> for providing recipes based on food images <a href="https://www.cnet.com/tech/services-and-software/tired-of-eating-out-i-tried-this-recipe-generating-ai-tool-to-create-a-restaurant-meal-at-home/">are flawed</a>.</p>
<p>So why bother with such a feature? You can get a taste of Samsung’s motivation from its CES-announced deal with Instacart that lets people order off Instacart from Samsung smart fridges that support the capability. Samsung Food on TVs can show users the progress of food orders placed via the Samsung Food mobile app on their TVs. Samsung Food can also create a shopping list for recipe ingredients based on what it knows (using cameras and AI) is in your (supporting) Samsung fridge. The feature also requires a Samsung account, which allows the company to gather more information on users.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Other software-centric features loaded into Samsung TVs this year include a dedicated AI button on the new TVs’ remotes, the ability to use gestures to control the TV but only if you’re wearing a Samsung Galaxy Watch, and AI Karaoke, which lets people sing karaoke using their TVs by stripping vocals from music playing and using their phone as a mic.</p>
<p>Like LG, Samsung has shown growing interest in ads and data collection. In May, for example, it expanded its automatic content recognition tech to track ad exposure on streaming services viewed on its TVs. It also has an ads analytics partnership with Experian.</p>

<h2>Large language models on TVs</h2>
<p>TVs are mainstream technology in most US homes. Generative AI chatbots, on the other hand, are emerging technology that many people have yet to try. Despite these disparities, LG and Samsung are incorporating Microsoft’s Copilot chatbot into 2025 TVs.</p>
<p>LG claims that Copilot will help its TVs “understand conversational context and uncover subtle user intentions,” adding: “Access to Microsoft Copilot further streamlines the process, allowing users to efficiently find and organize complex information using contextual cues. For an even smoother and more engaging experience, the AI chatbot proactively identifies potential user challenges and offers timely, effective solutions.”</p>
<p>Similarly, Samsung, which is also adding Copilot to some of its smart monitors, said in its announcement that Copilot will help with “personalized content recommendations.” Samsung has also said that Copilot will help its TVs understand strings of commands, like increasing the volume and changing the channel, <a href="https://www.cnet.com/tech/home-entertainment/samsungs-2025-tvs-get-all-the-ai-extras-nobody-asked-for/">CNET</a> noted. Samsung said it intends to work with additional AI partners, namely Google, but it's unclear why it needs multiple AI partners, especially when it hasn’t yet seen how people use large language models on their TVs.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<h2>TV-as-a-platform</h2>
<p>To be clear, this isn't a condemnation against new, unexpected TV features. This also isn't a censure against new TV apps or the usage of AI in TVs.</p>
<p><a href="https://arstechnica.com/gadgets/2024/04/ai-marketing-hype-is-coming-for-your-favorite-gadgets/">AI marketing hype </a>is real and misleading regarding the demand, benefits, and possibilities of AI in consumer gadgets. However, there are some cases when innovative software, including AI, can improve things that TV users not only care about but actually want or need. For example, some TVs use AI for things like trying to optimize sound, color, and/or brightness, including based on current environmental conditions or upscaling. This week, Samsung announced AI Live Translate for TVs. The feature is supposed to be able to translate foreign language closed captions in real time, providing a way for people to watch more international content. It's a feature I didn't ask for but can see being useful and changing how I use my TV.</p>
<p>But a lot of this week's TV announcements underscore an alarming TV-as-a-platform trend where TV sets are sold as a way to infiltrate people's homes so that apps, <a href="https://arstechnica.com/gadgets/2024/12/tcl-tvs-will-use-films-made-with-generative-ai-to-push-targeted-ads/">AI</a>, and <a href="https://arstechnica.com/gadgets/2024/11/an-ad-giant-wants-to-control-your-next-tvs-operating-system/">ads</a> can be pushed onto viewers. Even high-end TVs are moving in this direction and amplifying features with questionable usefulness, effectiveness, and privacy considerations. Again, I can't help but wonder what better innovations could have come out this year if more R&amp;D was directed toward hardware and other improvements that are more immediately rewarding for users than karaoke with AI.</p>
<p>The TV industry is facing economic challenges, and, understandably, TV brands are seeking creative solutions for making money. But for consumers, that means paying for features that you’re likely to ignore. Ultimately, many people just want a TV with amazing image and sound quality. Finding that without having to sift through a bunch of fluff is getting harder.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/scharonharding/"><img src="https://arstechnica.com/wp-content/uploads/2021/09/scharon-harding-headshot.jpg" alt="Photo of Scharon Harding"></a></p>
  </div>

  <div>
    

    <p>
      Scharon is a Senior Technology Reporter at Ars Technica writing news, reviews, and analysis on consumer gadgets and services. She's been reporting on technology for over 10 years, with bylines at Tom’s Hardware, Channelnomics, and CRN UK.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/#comments" title="117 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    117 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/ai/2025/01/how-i-program-with-llms/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/ai-code-buddy-768x432.jpg" alt="Listing image for first story in Most Read: How I program with LLMs" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Soldering the Tek way (158 pts)]]></title>
            <link>https://hackaday.com/2025/01/09/retrotechtacular-soldering-the-tek-way/</link>
            <guid>42650561</guid>
            <pubDate>Thu, 09 Jan 2025 22:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackaday.com/2025/01/09/retrotechtacular-soldering-the-tek-way/">https://hackaday.com/2025/01/09/retrotechtacular-soldering-the-tek-way/</a>, See on <a href="https://news.ycombinator.com/item?id=42650561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        <main id="main" role="main">

        
            
<article itemscope="" itemtype="http://schema.org/Article" id="post-752809">
    <!-- .entry-header -->

    <div itemprop="articleBody">
        <p>For a lot of us, soldering just seems to come naturally. But if we’re being honest, none of us was born with a soldering iron in our hand — ouch! — and if we’re good at soldering now, it’s only thanks to good habits and long practice. But what if you’re a company that lives and dies by the quality of the solder joints your employees produce? How do you get them to embrace the dark art of soldering?</p>
<p>If you’re Tektronix in the late 1970s and early 1980s, the answer is simple: make <a href="https://youtu.be/yZSveVpgmIM" target="_blank">in-depth training videos that teach people to solder the Tek way</a>. The first video below, from 1977, is aimed at workers on the assembly line and as such concentrates mainly on the practical aspects of making solid solder joints on PCBs and mainly with through-hole components. The video does have a bit of theory on soldering chemistry and the difference between eutectic alloys and other tin-lead mixes, as well as a little about the proper use of silver-bearing solders. But most of the time is spent discussing the primary tool of the trade: the iron. Even though the film is dated and looks like a multi-generation dupe from VHS, it still has a lot of valuable tips; we’ve been soldering for decades and somehow never realized that cleaning a tip on a wet sponge is so effective because the sudden temperature change helps release oxides and burned flux. The more you know.</p>
<p><a href="https://youtu.be/jMchFqu3Jx0" target="_blank">The second video below</a> is aimed more at the Tek repair and rework technicians. It reiterates a lot of the material from the first video, but then veers off into repair-specific topics, like effective desoldering. Pro tip: Don’t use the “Heat and Shake” method of desoldering, and wear those safety glasses. There’s also a lot of detail on how to avoid damaging the PCB during repairs, and how to fix them if you do manage to lift a trace. They put a fair amount of emphasis on the importance of making repairs look good, especially with bodge wires, which should be placed on the back of the board so they’re not so obvious. It makes sense; Tek boards from the era are works of art, and you don’t want to mess with that.</p>

<p><iframe title="Tektronix Solder And Its Application In Electrical Assembly 1977" width="800" height="450" src="https://www.youtube.com/embed/yZSveVpgmIM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p><iframe title="Tektronix Making Quality Circuit Board Repairs 1980" width="800" height="450" src="https://www.youtube.com/embed/jMchFqu3Jx0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
	            </div><!-- .entry-content -->
    
    <!-- .entry-footer -->
</article><!-- #post-## -->

            	<!-- .navigation -->
	
            

            
<!-- #comments -->

        
        

        
        

        
        </main><!-- #main -->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: A friend has brain cancer: any bio hacks that worked? (113 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42649996</link>
            <guid>42649996</guid>
            <pubDate>Thu, 09 Jan 2025 21:33:06 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42649996">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42649996: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[How to delete your Facebook account (102 pts)]]></title>
            <link>https://www.theverge.com/22231495/delete-facebook-page-account-how-to</link>
            <guid>42649887</guid>
            <pubDate>Thu, 09 Jan 2025 21:19:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/22231495/delete-facebook-page-account-how-to">https://www.theverge.com/22231495/delete-facebook-page-account-how-to</a>, See on <a href="https://news.ycombinator.com/item?id=42649887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><div><p>You may be wondering how to delete your Facebook account now that <a href="https://www.theverge.com/24339131/meta-content-moderation-fact-check-zuckerberg-texas">fact-checking is no longer considered important</a>, and Meta’s changing its <a href="https://www.theverge.com/2025/1/7/24338471/meta-hate-speech-hateful-conduct-policy-moderation">definition of what constitutes Hateful Conduct</a>. It’s easy to do, and we’ll show you how. But you should download all your stuff first.</p></div><p>The following instructions are for the web version of Facebook, but you can follow pretty much the same sequence on the mobile app.</p><p><h3>Download your archives</h3></p><p>Your Facebook archives contain just about <a href="https://www.facebook.com/help/405183566203254?helpref=faq_content">all of the pertinent information related to your account</a>, including your photos, active sessions, chat history, IP addresses, facial recognition data, and which ads you clicked. That’s personal information you should save.</p><div><ul><li>Click on your personal icon in the upper-right corner. </li><li>Go to <strong>Settings &amp; Privacy</strong> &gt; <strong>Settings</strong>. </li><li>Click on the <strong>Accounts Center</strong> box on the left.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Facebook “Settings and privacy” page showing Account Center on the left." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>The Accounts Center is where you can both download your info and delete your account.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>Go to <strong>Your information and permissions</strong> on the left, and then <strong>Download Your Information &gt;  Download or transfer information.</strong></li><li>You can choose to transfer information from your Facebook or Instagram account (or both). </li><li>You now have another choice. You can select <strong>Available information</strong>, which includes everything but data logs. (Meta defines these as “additional details we collect and store that can be associated with you.”) Or you can select <strong>Specific types of information</strong>, which allows you to determine exactly what you want to download, including those data logs.</li><li>If you choose the latter, you can then select from the variety of data you’ve accumulated, including posts, friends, logged information, saved items and collections, etc. (You can also get the data logs, although Facebook warns it could take 15 days for them to show.) Click on <strong>Select all</strong> — but be aware you have to click it for each category.  When you’re ready, select <strong>Next</strong>.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A “Select information” pop-up box showing a list of types of Facebook data with checkboxes next to them." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>You can download various types of Facebook data, or all of it. The latter will take longer.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>Choose <strong>Download to device</strong> or <strong>Transfer to destination</strong>. According to Meta, the typical download is about 2.5GB.</li><li>You’ll now be able to select the date range of the info you want to download (or you can simply download all of it), the format (usually HTML or JSON), and the media quality (low, medium, or high). Enter an email address for a notification when the download is ready.</li><li>Finally, select <strong>Create files</strong>. You’ll receive an email when your file is ready, and it will be available for a few days. If you’ve been waiting a while and want to know the status of your download (or want to cancel it), go back to the <strong>Download your information</strong> tab.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up headed “Download your information” with a section labeled “In progress” halfway down." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>You’ll be notified when your data is ready for download.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><p><h3>Delete your account</h3></p><p>You’re ready to delete your account once you’ve finished downloading your archive.</p><div><ul><li>When you are ready, go back to the <strong>Accounts Center</strong> and click on <strong>Personal Details &gt; Account ownership and control &gt; Deactivation or deletion</strong>. </li><li>Click&nbsp;<strong>Deactivation and Deletion</strong>.</li><li>If you have both Facebook and Instagram accounts, you will be asked to choose one.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up window headed Deactivating or deleting your Facebook account, with the choice of doing either underneath." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Facebook gives you the option of temporarily deactivating your account — just in case you might change your mind.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>If you only want to deactivate your account temporarily (maybe you hope CEO Mark Zuckerberg <a href="https://www.theverge.com/24339131/meta-content-moderation-fact-check-zuckerberg-texas">will change his mind</a>?), you can choose to do so. Otherwise, select <strong>Delete account</strong> and click <strong>Continue</strong>.</li><li>You’ll be informed of any other accounts you have with Meta and given several options to explain why you’re leaving. Just keep hitting <strong>Continue</strong>.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up with check boxes so people can choose why they’re leaving Facebook." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Why do you want to leave? Choose your reason.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>You’ll see an option to deactivate your account instead or save the posts in your archive, download your info, and review the apps you’re logged into. When you’re ready, hit <strong>Continue</strong>.</li><li>You’ll be asked for your password for confirmation. Enter it. </li><li>Finally ready? Hit <strong>Delete account</strong>.</li><li>Once you click <strong>Delete account</strong>, your account will be marked for termination and inaccessible to others using Facebook. </li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up headed Confirm permanent account deletion with explanatory text beneath and a blue Delete account button." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>It takes a few pages, but you will finally get to the point where you can delete your account. And even after that, you have time to log in again.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><p>Meta <a href="https://www.facebook.com/help/125338004213029?helpref=uf_permalink">notes</a> that it delays termination for a few days after the request has gone through.  The deletion will be canceled if you log back in during that period. So don’t sign on, or you’ll be forced to start the process over again. </p><p>Certain things, like comments you’ve made on a friend’s post, may still appear even after you delete your account. Facebook also says that copies of certain items like log records will remain in its database, but it notes that those are disassociated with personal identifiers. </p><p>If you’re really serious about quitting anything associated with Meta, remember that the company owns several other popular services as well, like Instagram, WhatsApp, and Threads, so you should delete your accounts there, too.</p><p><em><strong>Update January 9th, 2025:</strong> This article was originally published on September 28th, 2018, and has been updated several times to allow for changes in the Facebook interface.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Britain got its first internet connection (2015) (118 pts)]]></title>
            <link>https://theconversation.com/how-britain-got-its-first-internet-connection-by-the-late-pioneer-who-created-the-first-password-on-the-internet-45404</link>
            <guid>42649340</guid>
            <pubDate>Thu, 09 Jan 2025 20:02:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theconversation.com/how-britain-got-its-first-internet-connection-by-the-late-pioneer-who-created-the-first-password-on-the-internet-45404">https://theconversation.com/how-britain-got-its-first-internet-connection-by-the-late-pioneer-who-created-the-first-password-on-the-internet-45404</a>, See on <a href="https://news.ycombinator.com/item?id=42649340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p><em>British computer scientist and <a href="https://www.internethalloffame.org/inductee/peter-kirstein">Internet Hall of Fame inductee</a> Peter Kirstein died in January 2020 at the age of 86, after a nearly 50-year career at UCL. A few years before he died, he was commissioned by then Conversation technology editor Michael Parker (now director of operations) to write an in-depth piece originally intended as part of a special series on the internet. It wasn’t published at the time, as the series was postponed, but now to mark Professor Kirsten’s contributions we are delighted to be able to publish his reflections on the challenges he faced connecting the UK in the early 1970s to the forerunner of what would become the modern internet. The article was edited by Michael with oversight kindly provided by <a href="https://theconversation.com/profiles/jon-crowcroft-143812">Professor Jon Crowcroft</a>, a colleague of Professor Kirstein’s.</em></p>

<p>The internet has become the most prevalent communications technology the world has ever seen. Though there are more fixed and mobile telephone connections, even they use internet technology in their core. For all the many uses the internet allows for today, its origins lie in the cold war and the need for a defence communications network that could survive a nuclear strike. But that defence communications network quickly became used for general communications and within only a few years of the first transmission, traffic on the predecessor to today’s internet was already 75% email.</p>

<h2>In the beginning</h2>

<p><a href="https://www.britannica.com/topic/ARPANET">Arpanet</a> was the vital precursor of today’s internet, commissioned by the US Defence Advanced Research Projects Agency (Darpa) in 1969. In his interesting account of <a href="https://www.computer.org/csdl/magazine/an/2011/03/man2011030004/13rRUxly9fL">why Arpanet came about</a>, Stephen Lukasic, Director of Darpa from 1970-75, wrote that if its true nature and impact had been realised it would never have been permitted under the US government structure of the time. The concept for a decentralised communications technology that would survive a nuclear attack would have placed it outside Darpa’s remit (as defence communications specifically were assigned to a different agency), so the focus changed to how to connect computers together so that major applications could be run on the most appropriate system available.</p>

<p>This was in the era of <a href="https://www.ibm.com/history/time-sharing">time-sharing computers</a>. Today’s familiar world of the ubiquitous “personal computer” on each desk was decades away. Computers of this time were generally very large, filling entire rooms, and comparatively rare. Users working at connected terminals would submit jobs to the computer which would allocate processing time for the job when available. The idea went that if these computers were networked together, an available remote computer could process a job even when the computers closer to the users were full. The resulting network was called Arpanet and the first packets of data traversed the network in September 1969. </p>

<figure>
            <a href="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>A CDC 7600 mainframe computer fills an entire room at Lawrence Livermore National Laboratory, California, mid-1970s.</span>
              <span><a href="https://www.flickr.com/photos/llnl/3094299714/">Lawrence Livermore National Laboratory</a>, <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a></span>
            </figcaption>
          </figure>

<p>At this time the computing industry was dominated by a few large companies, which produced products that would work only with others from the same company. However the Arpanet concept included a vital decision on how the network would function: <a href="https://twobithistory.org/2021/03/08/arpanet-protocols.html">it sharply distinguished and separated</a> the technology and medium that would carry the communications (satellite link, copper cable, fibre optic), the network layer (the software that manages communications between different computers), and applications (the programs that users run over the network to do work) from one another. </p>

<p>This contrasted with the vertical “stove-pipe” philosophy that persisted among computer manufacturers at the time, where any networking that existed worked only in specific situations and for specific computer systems. For example, IBM computers could communicate using IBM’s <a href="https://www.ibm.com/docs/en/zos-basic-skills?topic=implementation-what-is-systems-network-architecture-sna">SNA protocol</a>, but not with non-IBM equipment. The direction Arpanet took was manufacturer-agnostic, where different types of computers could be networked together.</p>

<h2>First footprint in Europe</h2>

<p>In 1970, the leading network research outside the US was a group at the <a href="https://www.npl.co.uk/getattachment/about-us/History/11408-History-of-NPL-May-2023.pdf.aspx?lang=en-GB">National Physical Laboratory</a> (NPL) in London led by <a href="https://www.internethalloffame.org/inductee/donald-davies/">Donald Davies</a>. Davies had built a network with similar concepts to Arpanet, and as one of the inventors of <a href="https://www.npl.co.uk/getattachment/about-us/History/Famous-faces/Donald-Davies/UK-role-in-Packet-Switching-(1).pdf.aspx?lang=en-GB">packet-switching</a> his work had influenced the direction of Arpanet. But despite his plans for a national digital network, he was prevented from extending his project outside the lab by pressure from the British Post Office, which then held a monopoly on telecommunications. </p>

<p>Around this time the director of the Arpanet project, <a href="https://www.nytimes.com/2018/12/30/obituaries/lawrence-g-roberts-dies-at-81.html">Larry Roberts</a>, proposed connecting Arpanet to Davies’ NPL network in the UK. This would be possible because a few years previously a large seismic array in Norway run by Norwegian researchers for Darpa had been connected to Arpanet via a dedicated 2.4 Kbps connection to Washington. Due to the transatlantic technology of the time, this was by satellite link via the only earth station for satellite communications in Europe, in <a href="https://www.bbc.co.uk/news/uk-england-cornwall-62277946">Goonhilly, Cornwall</a>, and thence by cable to Oslo. Larry proposed to interrupt the connection in London, connect the NPL network, and then continue to Norway. </p>

<p>Since the international communications were the main cost, this seemed straightforward. Unfortunately Britain was at this point negotiating to join the Common Market, and the UK government was afraid that closer links with the US would jeopardise the talks. When the government refused NPL permission to participate, as I was doing relevant research at the University of London’s Institute of Computer Science and subsequently at <a href="https://www.ucl.ac.uk/computer-science/about-0">UCL</a>, I was the obvious alternative.  </p>

<h2>Vaulting many non-technical hurdles</h2>

<p>From the beginning I proposed a twin approach. I would connect the large computers at the University of London and the <a href="https://www.chilton-computing.org.uk/acl/pdfs/davies.pdf">Rutherford and Appleton laboratories</a> (RAL) in Oxfordshire, which were hubs for other UK computer networks, and I would provide services to allow UK researchers to use the networks to collaborate with colleagues in the US.</p>

<p>This novel approach would mean the IBM System 360/195 at RAL, then the most powerful computer in the UK, would be made available as a remote host – available to those in the US on the other side of the transatlantic link, without being directly connected to the interface message processor – the equipment which sent and received messages between Arapanet nodes, which would be installed in UCL.</p>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=1242&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=1242&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=1242&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=1561&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=1561&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=1561&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p>
            <figcaption>
              <span>An interface message processor used to connect Arpanet nodes. About the size of a wardrobe, it is the type that would have been impounded by customs.</span>
              <span><a href="https://commons.wikimedia.org/wiki/File:ARPANET_first_router_2.jpg">Steve Jurvetson</a>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<p>Unfortunately there then came <a href="https://www.researchgate.net/publication/3330677_Early_experiences_with_the_Arpanet_and_Internet_in_the_UnitedKingdom">many non-technical hurdles</a>. I attempted to get other universities’ computer science departments to back the project, but this foundered because the Science Research Council did not consider the opportunity worth funding. The UK Department of Industry wanted a statement of interest from industry before funding, but even though I knew executives at ICL, the UK’s principal computer manufacturer, after months of agonising it declined stating that “one would gain more from a two-week visit to the US than from a physical link”. Consequently after a year of back and forth I had nothing.</p>

<p>However by 1973 the project was becoming a reality. By now the Norwegian siesmic array, <a href="https://www.norsar.no/about-us/history/arpanet">Norsar</a>, was connected to Arpanet via a newly opened satellite earth station at Tanum in Sweden, and so there was no longer a link via the UK at all. Now what was required was a link from UCL to Oslo. With a small grant of £5,000 from Donald Davies at the NPL, and the provision by the British Post Office of a 9.6 Kbps link to Oslo without charge for one year, we had the resources to proceed. </p>

<p>Darpa duly shipped its message processor with which to connect the new London node to Arpanet. It was promptly impounded at Heathrow Airport for import duty and the newly introduced Value Added Tax. I managed to avoid paying the duty by declaring it an “instrument on loan”, but it took all my available funds to provide a guarantee that would allow me to get hold of the equipment pending an appeal. With the equipment finally installed, in July 1973 I connected the first computers outside the US to the Arpanet, sending a transmission from London, via Norway, through the Arpanet to the Information Science Institute at the University of Southern California.</p>

<h2>First password on the internet</h2>

<p><a href="https://datatracker.ietf.org/doc/html/rfc588">Within three months</a> my group was able to implement the Arpanet network protocols and translate them to the IBM protocols necessary to communicate with computers at RAL. And so, once connected to the wider network through our gateway at UCL, the IBM computer at RAL became one of the most powerful on the Arpanet. </p>

<figure>
            <a href="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=430&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=430&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=430&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=540&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=540&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=540&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>Arpanet map in 1977. The satellite connection from SDAC to NORSAR and then to London visible bottom right, with the large box bottom right representing the computers available at the Rutherford and Appleton Laboratories, Royal Signals and Radar Establishment and elsewhere.</span>
              <span><a href="https://en.wikipedia.org/wiki/ARPANET#/media/File:Arpanet_logical_map,_march_1977.png">The Computer History Museum</a></span>
            </figcaption>
          </figure>

<p>When I gave a talk stating this fact, RAL staff first did not believe me; they still saw only my small minicomputer, without understanding that it was the gateway to the rest of the Arpanet on the other side of the link. On realising they became very concerned that access to their computer services would be available not only to me, but with my complicity to the whole research community in the US. </p>

<p>However, I had been concerned that I would, in exactly this way, be criticised for improper use of both UK and US facilities. So from the beginning I put password protection on my gateway. This had been done in such a way that even if UK users telephoned directly into the communications computer provided by Darpa in UCL, they would require a password. </p>

<p>In fact this was the first password on Arpanet. It proved invaluable in satisfying authorities on both sides of the Atlantic for the 15 years I ran the service – during which no security breach occurred over my link. I also put in place a system of governance that any UK users had to be approved by a committee which I chaired but which also had UK government and British Post Office representation. </p>

<hr>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=112&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=112&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=112&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=140&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=140&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=140&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p>
            <figcaption>
              <span></span>
              
            </figcaption>
          </figure>

<p><em>The <a href="https://theconversation.com/uk/insights">Insights section</a> is committed to high-quality <a href="https://theconversation.com/insights-the-conversations-long-reads-section-240155">longform journalism</a>. Our editors work with academics from many different backgrounds who are tackling a wide range of societal and scientific challenges.</em></p>

<hr>

<p>The transatlantic connection included terminal services (which connected users to remote computers to run jobs), file access and later email services. It was immediately very popular. Within a couple of years, I was supported by half a dozen government ministries, with leased line links (a dedicated line) to five remote sites – some of which allowed access through their own networks. Other users could telephone into my UCL site, or use the fledgling post office data network to which I also provided access. </p>

<p>Indeed, its profile had become so prominent that when the Queen opened a building at the Ministry of Defence’s Royal Radar Establishment at Malvern in Worcestershire in 1976 (which had taken over funding the leased line to Oslo), this was accompanied by her inaugurating the connection by <a href="https://www.internethalloffame.org/2012/12/31/how-queen-england-beat-everyone-internet/">sending an email</a> – the first to be sent by a head of state.</p>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=396&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=396&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=396&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=498&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=498&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=498&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p>
            <figcaption>
              <span>Her Majesty Queen Elizabeth II sends her first email, and the first email sent by a head of state, at the Royal Radar Establishment in 1976.</span>
              <span><span>Peter Kirstein</span></span>
            </figcaption>
          </figure>

<p>As the UK side of Arpanet continued growing, additional message processors had to be imported, each one racking up additional VAT and duty to be paid, pending the outcome of the appeal. Finally in 1976 the appeal was refused. But a meeting with senior treasury officials subsequently led to an agreement that my research group would be permitted to import equipment free of VAT and duty. The importance of this ruling cannot be overemphasised for ensuring the independence of our operation: over the following decade many government bodies considered trying take it over, and each time would be discouraged by the magnitude of the VAT and duty bill they would incur.</p>

<h2>Agreeing the language of Arpanet</h2>

<p>In their 1975 paper <a href="https://www.internethalloffame.org/inductee/robert-kahn/">Bob Kahn</a> at Darpa and <a href="https://www.internethalloffame.org/inductee/vint-cerf/">Vint Cerf</a> at Stanford University made the next vital contribution towards building the internet of today when they formulated the concept of connecting together different network technologies – such as those defined by different computer manufacturers, or designed for different communications media such as cable, satellite link or radio waves – with a <a href="https://www.cs.princeton.edu/courses/archive/fall06/cos561/papers/cerf74.pdf">common inter-network layer</a>, which would come to be known as TCP/IP. </p>

<p>Transport Control Protocol (TCP) managed the packaging and unpacking of data sent between computers, while Internet Protocol (IP) provided the pathfinding to ensure the data packets reached the intended destination. One of the important aspects of IP was that it allowed <a href="https://www.juniper.net/documentation/us/en/software/junos/interfaces-security-devices/topics/topic-map/security-interface-ipv4-ipv6-protocol.html">scalability</a>: the 8-bit number previously used to identify a computer on the network that allowed just 256 devices suddenly increased to a 32-bit number, which allowed 4 billion devices. </p>

<p>I misjudged how successful TCP/IP would be. In one of the <a href="https://archive.org/details/IssuesInPacketNetworkInterconnection/mode/1up?view=theater">first papers on network interconnection</a> Cerf argued that all computers should adopt TCP/IP, but I felt that this was unrealistic, and that gateways like the interface message processors were needed to “translate” communications between networks. While for the first 15 years my view prevailed, eventually in the long run Cerf’s view was the right one.</p>

<figure>
            <a href="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=388&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=388&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=388&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=488&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=488&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=488&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p></a>
            <figcaption>
              <span>Stanford Research Institute’s Packet Radio Van, used in the first TCP/IP internet experiments. The van drove across the Golden Gate Bridge while transmitting, and the steel girders interrupted the signal. But when it exited the bridge, the transmission picked up where it left off.</span>
              <span><a href="https://en.wikipedia.org/wiki/Packet_Radio_Van#/media/File:SRI_Packet_Radio_Van.jpg">SRI International</a>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<p>At UCL, my group participated in the first independent TCP/IP implementations, connecting in 1977 for the first time networks using a different technology to Arpanet. This saw three different types of network, Arpanet, the satellite network Satnet, and PRNET, a packet-radio network using <a href="https://computerhistory.org/blog/born-in-a-van-happy-40th-birthday-to-the-internet/">radio transmissions from mobile vans</a>, all connected using the same common “language”, TCP/IP. This was in essence the first demonstration of the internet – a network of networks.</p>

<p>Later, we connected the first multi-service heterogeneous network outside the US (<a href="https://www.jisc.ac.uk/janet">Janet</a>, the UK’s academic network connecting universities) to Arpanet, and then to the internet in the early 1980s. Indeed, UCL was the first organisation on Arpanet to adopt TCP/IP as standard.</p>

<figure>
            <a href="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=416&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=416&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=416&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=522&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=522&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=522&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>Schematic of the first internet demonstration, connecting three different networks, PRNET, ARPANET and SATNET, with TCP/IP. This was the first connection that created a ‘network of networks’, as the internet would become.</span>
              <span><a href="https://en.wikipedia.org/wiki/History_of_the_Internet#/media/File:First_Internet_Demonstration,_1977.jpg">Computer History Museum</a></span>
            </figcaption>
          </figure>

<p>During the 1980s the internet approach took over, where computers used TCP/IP to manage their own connections to the network. Darpa provided funding to add TCP/IP into its <a href="https://klarasystems.com/articles/history-of-freebsd-part-4-bsd-and-tcp-ip/">chosen operating system of the time, BSD</a>, and this was later made available to the public. </p>

<p>After the release of the IBM PC microcomputer in 1981 there was a rapid growth of cheap (relatively speaking) personal computers in offices connected to each other by <a href="https://www.wired.com/story/what-is-ethernet/">ethernet</a> networks. And routers (small devices to connect networks) were developed that made the huge, outdated interface message processors used with the original Arpanet obsolete. </p>

<p>The universal adoption of common protocols that provided useful services like virtual terminal (telnet), file transfer (FTP), directory (LDAP) and email (SMTP) made the internet an invaluable tool for researchers. As fibre optic installations became more economical it allowed networks to scale up to very large numbers of interconnected computers. The internet’s most widespread and largest use by volume was still email, but a number of shared data repositories and resources developed. </p>

<p>Then in 1989, with the development of the <a href="https://home.cern/science/computing/birth-web/short-history-web">World Wide Web</a>, Tim Berners-Lee provided the killer application that would make the internet essential to all types of commercial and government use. The simplicity and ease of use of the web and web browsers, together with the internet as the distribution mechanism underpinning it, laid the basis for the universal use of the internet we have today. </p>

<h2>The little black book of the internet</h2>

<p>Back when there were even only a few hundred computers, discovering their addresses and maintaining a directory of them had become impractical. Bob Kahn, then director of the relevant office at Darpa, remedied this problem by commissioning the <a href="https://www.cs.cornell.edu/courses/cs6411/2018sp/papers/mockapetris.pdf">Domain Name Service</a>. This mapped IP addresses to names organised in hierarchical structure. The effect was a sort of directory of internet-connected computers, where top level domains (such as .com, .org, .uk, .fr) lay above second level domains (such as .ac.uk, .co.uk, or microsoft.com, wikipedia.org), which in turn lay above domains below them (such as www.microsoft.com or www.wikipedia.org, where the www. represents a subdomain below the domain). This domain model forms the basis of the URLs that we type into our browser address bars today.</p>

<p>Although four billion addresses seemed near infinite in 1974, by the early 1990s it was already evident that the internet would soon run out of IP (IPv4) addresses, necessary for computers to be connected to the internet. Work on the next generation of IP, IPv6, was to increase the number of routable network addresses from 32-bit (2<sup>32,</sup> or 4 billion) to 128-bit (or 2<sup>128</sup> or 3.4x10<sup>29</sup> billion) addresses. Technical fixes managed to extend the lifetime of IPv4, but over the last few years the need to move to IPv6 has become pressing, and adoption is now happening faster. </p>

<figure>
            <p><iframe data-src="https://www.youtube.com/embed/JLilyBJeYgQ?wmode=transparent&amp;start=0" frameborder="0" allowfullscreen="" width="100%" height="400"></iframe></p>
            
          </figure>

<h2>Growth and change</h2>

<p>Over the last two decades, the emergence of social networks, the increasing availability of internet streaming media and the integration of mobile telephone networks with the internet have hugely increased demand for internet capacity. Such demand will require large investments to meet, but probably without any radical rethink of the internet’s architecture. The number of internet-connected devices is growing significantly, but we can assume that it would increase only to a small multiple of the world’s population. So even if the protocols that govern how devices connect to the internet had to change to cope with demand, this could be achieved within only a few years.</p>

<p>The ability to monitor the activities of people – with or without their knowledge – is one important outcome of so many people so frequently connected to the network. The ability by unauthorised individuals to hack into private systems, to obtain private data or damage operations, are very worrying developments. The advances in computer and network security needed require massive research and development, and new legal and regulatory powers. And an even more disruptive development now looms, <a href="https://direct.mit.edu/daed/article/145/1/33/27105/Edge-Networks-amp-Devices-for-the-Internet-of">the Internet of Things</a>.</p>

<p>Increasingly devices and equipment found in all aspects of our life may incorporate sensors and actuators that can be operated remotely. The estimated numbers of devices to be network-connected is much larger: as many as hundreds of billions within ten years. Cars (for navigation or automated driving), home appliances (for automation, security), devices on the national power grid (monitoring and error correction), smart buildings (temperature or humidity control, security), smart cities (traffic control, services supply, waste management), wearable and implanted medical devices, and so on. </p>

<p>The characteristics of such devices are often quite different from today’s computers on the internet. The data rate may be very low, and often but not necessarily the data may be required only for local networks, rather than full internet availability. The devices or their controllers may have internet interfaces, but they may not obey other internet protocols, and would possibly need to be left in place for years, or decades.</p>

<p>They may not be able to carry out sophisticated security operations themselves, yet ensuring they are secure will be crucial if they are not to become a vast vulnerable network of potential points of entry for hostile actors. It is the Things on the internet of the future, rather than typical computing devices, that may prompt a radical re-think of the ways the internet works. </p>

<p>The impact of the internet on our way of life in its first 40 years has been immeasurable. It has expanded and developed in a way none of us envisaged in 1975. While we may have a better idea of what to expect over the next couple of decades, I am sure most of us will be mistaken.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Twelfth Night Till Candlemas" – a 40-year book-quest and its remarkable ending (141 pts)]]></title>
            <link>https://davidallengreen.com/2024/12/twelfth-night-till-candlemas-the-story-of-a-forty-year-book-quest-and-of-its-remarkable-ending/</link>
            <guid>42647633</guid>
            <pubDate>Thu, 09 Jan 2025 17:07:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://davidallengreen.com/2024/12/twelfth-night-till-candlemas-the-story-of-a-forty-year-book-quest-and-of-its-remarkable-ending/">https://davidallengreen.com/2024/12/twelfth-night-till-candlemas-the-story-of-a-forty-year-book-quest-and-of-its-remarkable-ending/</a>, See on <a href="https://news.ycombinator.com/item?id=42647633">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
<article>

<div dir="auto">
<p>This post is about finally finding a book from one’s youth forty years later – and after nearly thirty years of searching.</p>
<p>It is also a tale about goblins and Christmas decorations; about the perils of ChatGPT and Artificial Intelligence; and about the real value of librarians, cataloguers, indexers, and archivists – what should be called the Noble Professions.</p>
<p>And it is an account that ends with not one but two wonderful events.</p>
<p>So if you are sitting comfortably, with a suitable seasonal drink, we will start with a bit of background and with a historical excursion.</p>
<p>*</p>
<p>Once upon a time there was a story.</p>
<p>And the story was in a book – a child’s anthology: the sort of book that one used to get in school bookshops and advertised in the special catalogues that were common in English schools (and elsewhere) in the 1970s and 1980s.</p>
<p>All the books I had at the time got lost – house moves and so on – and since the world wide web made searching for second-hand books easy I have replaced the books one-by-one.</p>
<p>When you re-read such books, sometimes what one thinks are one’s own original ideas and expressions stare back at you and you realise where you got them from.</p>
<p>What the economist J. M. Keynes once said –&nbsp;<em>“Practical men who believe themselves to be quite exempt from any intellectual influence, are usually the slaves of some defunct economist”</em>&nbsp;– has a far wider application.</p>
<p>Many of us are the slaves of what we read when very young.</p>
<p>*</p>
<p>But there was one book what eluded me, every time it was searched for.</p>
<p>What I could remember (or believed I could remember) was as follows:</p>
<p>– it was a story in an anthology;</p>
<p>– the story was about what will happen if you do not take your decorations down by Twelfth Night – for goblins and other ne’er-do-wells will go through your town and hide behind any remaining decorations and cause you mischief all year round;</p>
<p>– but there was a cure to this mischief if a certain thing was done on Candlemas – 2 February – and this was because of an esoteric rule which could be applied surreptitiously by those with special knowledge;</p>
<p>– the book was purple;</p>
<p>– the title or sub-title of the book, or of the story, was <em>“from Michelmas to Candlemas”&nbsp;</em>– the use of “<em>Candlemas</em>” was obvious from the story, and the&nbsp;<em>“Michaelmas”</em>&nbsp;I was certain about because it was a word I would again encounter in my late teens as a student, as it reminded me of the story/book.</p>
<p>(One of these memories, however, was false and another only semi-reliable.)</p>
<p>*</p>
<p>The story was important to me because it led to my passion for lore.</p>
<p>For me as a legal commentator, law (in its technical, black-letter sense) is practically far less important than what people – including lawyers and even judges – believe the law to be.</p>
<p>(Long-term followers may also recall my original blogging name was of&nbsp;<a href="https://en.wikipedia.org/wiki/Jack_o%27_Kent" rel="">a folklore hero</a>&nbsp;who bested the devil by careful attention to what was actually agreed.)</p>
<p>And so this remembered Candlemas story had everything for a lover of lore and law: a predicament, an obscure rule, the skilled application of that rule, and a remedy.</p>
<p>*</p>
<p>How I searched for this story – usually every year in November or December.</p>
<p>At first, I searched the web generally – with text and then, as Google developed, for the book cover.</p>
<p>I searched sites which had pictures of the book catalogues of the time.</p>
<p>I searched the British Library, the Bodleian Library, and every library I could think of.</p>
<p>Nil-return.</p>
<p>*</p>
<p>It was a mini-exercise&nbsp;<a href="https://www.youtube.com/watch?v=r2TilNclT8k" rel="">in being J. R. Hartley</a>&nbsp;year after year.</p>
<p>After a while certain results became familiar – and I probably know more about <a href="https://www.google.com/search?sca_esv=187175ea13837ba9&amp;sxsrf=ADLYWIIVO6N79MGGuSSGU9PrVvM2PfL4mQ:1734603763244&amp;q=Candlemas+books+devotional&amp;udm=2&amp;fbs=AEQNm0Aa4sjWe7Rqy32pFwRj0UkWde_MVagR57NIrd96T8bhwqEHeQ8jxei7F1s0aV23iWdi741Y3wNj5M6YQHR2X6laQ2bG_-lQAEb0rBAh8HRP8QnwCon_R8sFRzqEefr32Ob9swxFwdMawgPopA8MJ3jT9WeCgU-iUIUXZtKdtqKHtMWi_SZ41t31ikJ6wkUr5mZT0z82cmGy5mWpAaSfdIqbKHiMSw&amp;sa=X&amp;ved=2ahUKEwj6ionJzrOKAxUcWkEAHS_TK8IQtKgLegQIFhAB&amp;biw=1920&amp;bih=958&amp;dpr=1" rel="">devotional texts about</a>&nbsp;– and&nbsp;<a href="https://www.google.com/search?q=Candlemas+adventure+stories&amp;sca_esv=187175ea13837ba9&amp;udm=2&amp;biw=1920&amp;bih=958&amp;sxsrf=ADLYWIKkKlYh0g7MhfVOIAghQnYkSQZRFg%3A1734603765270&amp;ei=9fNjZ8ObEJ6VhbIPsMWz6QE&amp;ved=0ahUKEwiD6oTKzrOKAxWeSkEAHbDiLB0Q4dUDCBE&amp;uact=5&amp;oq=Candlemas+adventure+stories&amp;gs_lp=EgNpbWciG0NhbmRsZW1hcyBhZHZlbnR1cmUgc3Rvcmllc0j2LVDqBFiKLHABeACQAQCYAUGgAfAJqgECMjS4AQPIAQD4AQGYAgmgAoAEwgIEECMYJ8ICBBAAGB7CAgYQABgIGB7CAgUQABiABJgDAIgGAZIHATmgB_Iq&amp;sclient=img" rel="">adventure stories set</a>&nbsp;at – Candlemas than many other people.</p>
<p>And it was always a pleasure to renew contact with&nbsp;<a href="https://www.british-history.ac.uk/letters-papers-hen8/vol10/pp1-12" rel="">texts like</a>&nbsp;<em>“[i]t is a very old enactment that no Gascon wines or Toulouse woad be brought into England in strange bottoms, and nothing which has been done affects them but was devised to restrain the folly of English merchants who ventured to Bordeaux at unseasonable times, and the restraint&nbsp;<strong>from Michaelmas to Candlemas</strong>, by avoiding dangerous times, will rather augment the traffic…” (emphasis added.)</em></p>
<p>I bought books of Christmas stories on the off-chance they would reprint the story I was looking for – a disconcerting number of which appear to have been edited by Gyles Brandreth.</p>
<p>Nil-return.</p>
<p>*</p>
<p>When social media came along, I would then appeal from time-to-time for any information.</p>
<p>Those who responded were often very helpful – and so yet more Christmas anthologies were bought, and further lines of enquiry followed.</p>
<p>I made direct contact with experts in folklore and fairy tales, but they were as non-plussed as me.</p>
<p>Still nil-return.</p>
<p>*</p>
<p>Along the way though, I found out a great deal about the lores of the twelve days of Christmas and Candlemas which contextualised what I could remember.</p>
<p>For example, both Twelfth Night and&nbsp;<a href="https://en.wikipedia.org/wiki/Candlemas" rel="">Candlemas</a>&nbsp;have historically been the ends of the Christmas period – the latter being the fortieth day after Christmas.</p>
<p>And I discovered that Candlemas – which is also marked the purification (or what became known in England as ‘<a href="https://en.wikipedia.org/wiki/Churching_of_women" rel="">churching</a>’) of Mary and the presentation of Jesus at the Temple – was once an annual event that was very important in English culture.</p>
<p>Indeed Charles I arranged&nbsp;<a href="https://historyofparliament.com/2023/05/11/1626-coronation-charles-i/" rel="">his coronation to be held on Candlemas</a>.</p>
<p>And royalists made a point of celebrating Candlemas as part of what we would now call&nbsp;<em>“culture wars”</em>&nbsp;of the 1600s.</p>
<p>One once-famous poet, the loyalist clergyman&nbsp;<a href="https://en.wikipedia.org/wiki/Robert_Herrick_(poet)" rel="">Robert Herrick</a>&nbsp;<a href="https://gutenberg.org/cache/epub/22421/pg22421-images.html#id_2.p892" rel="">published three poems</a>&nbsp;about Candlemas, one of which urged the burning of decorations on that day, else bad things would follow:</p>
<p><em>Kindle the Christmas brand, and then</em></p>
<p><em>Till sunset let it burn;</em></p>
<p><em>Which quench’d, then lay it up again</em></p>
<p><em>Till Christmas next return.</em></p>
<p><em>Part must be kept wherewith to teend</em></p>
<p><em>The Christmas log next year,</em></p>
<p><em>And where ’tis safely kept, the fiend</em></p>
<p><em>Can do no mischief there.</em></p>
<p>(This ritual burning of decorations is a tradition that&nbsp;<a href="https://www.dailymail.co.uk/columnists/article-12930745/BORIS-JOHNSON-burn-Christmas-tree.html" rel="">still has echoes today</a>.)</p>
<p>After the culture wars of the 1600s, however, Candlemas became less popular – and soon it was all-but forgotten culturally, outside the annual blessing of candles at certain churches.</p>
<p>(On Candlemas in particular, see chapter 13 of&nbsp;<em>The Stations of the Sun</em>&nbsp;by Ronald Hutton, and on the place of Candlemas in the politics and religion of early modern England generally, see Eamon Duffy’s&nbsp;<em>The Stripping of the Altars: Traditional Religion in England, 1400-1580</em>.)</p>
<p>*</p>
<p>This was all fascinating, but it was not getting me any closer to the book or the story.</p>
<p>A couple of years ago, after the usual social media appeal, someone suggested I try the&nbsp;<a href="https://www.reddit.com/r/whatsthatbook/" rel="">r/whatsthatbook</a>&nbsp;thread on Reddit, where very clever and generous people spend time trying to identify books from the scantiest of details.</p>
<p>So&nbsp;<a href="https://www.reddit.com/r/whatsthatbook/comments/zdbnk3/book_with_candlemas_story/" rel="">I did</a>.</p>
<p>And someone there corresponded with a suggestion which actually covered each of the data points I could recall about the book – and it had the right title, and the book even had a well-known editor.</p>
<p>This was an extraordinary find – how could I have missed this in all the years of searching?</p>
<p>Well.</p>
<p>The reason it had never been uncovered before was because the impressive looking account had been generated – entirely fabricated – by ChatGPT.</p>
<p>This false account has now been deleted, but the correspondent remarked when I said this looked like it had been auto-generated:&nbsp;<em>“You’re right, I’ve tried chatGPT on some descriptions around here and it worked pretty well. However sometimes it has a propensity to spew random bullshit. I forgot because it’s so good in other areas. I’ll check better.”</em></p>
<p>I had never come across ChatGPT before – and so I have distrusted it ever since.</p>
<p>*</p>
<p>So this year – a couple of weeks ago – I did the annual appeal – but this time on BlueSky and Mastodon, and not on Twitter.</p>
<p>And yet again, people were helpful – anthologies were suggested and bought (though no further ones by Gyles Brandreth).</p>
<p>Someone again used ChatGPT, and they came up with:</p>
<p><em>“The book you’re describing sounds like “From Michaelmas to Candlemas” by Ruth Ainsworth. It was published in the 1970s and features seasonal stories aimed at children, including the one about the need to take down Christmas decorations by Candlemas to avoid goblins hiding behind them. The title references the traditional English calendar, marking the time from the feast of Michaelmas (September 29) to Candlemas (February 2). The story you mentioned aligns with themes found in folklore and poetry, including those by Robert Herrick. If this is the book you’re thinking of, it was indeed popular in school book clubs during that era.”</em></p>
<p>Again, like the account offered by the Reddit correspondent, this passage looks authoritative and plausible.</p>
<p>You will even notice how it neatly covers everything I could remember – giving equal weight to each data point and deftly joining them all together.</p>
<p>And again, what ChatGPT here had to offer was utterly – absolutely – false.</p>
<p>Like a fluent and practised (but unwise) liar it had contrived an account that fitted only the available information.</p>
<p>It was fake.</p>
<p>This year looked like another nil-return.</p>
<p>*</p>
<p>And then, something remarkable happened.</p>
<p>The appeal got&nbsp;<a href="https://bsky.app/profile/tambourine.bsky.social/post/3lc25w54a3k2k" rel="">this response</a>:</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" fetchpriority="high" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png?resize=589%2C948&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 1456w" alt="" width="589" height="948" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:948,&quot;width&quot;:589,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:505803,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>Wow.</p>
<p>It was the same story, now looking up at me from a computer screen forty years later.</p>
<p>I remember the stylised first letter, the imagery, the pacing, the tone.</p>
<p>It did mention goblins as part of the ne’er-do-wells, but it was about a demon – not a goblin – who hid behind a sprig of holly.</p>
<p>(My insistence that it was a goblin was a semi-unreliable memory.)</p>
<p>And there was (who I now know was) Granny Hawkins being the holder of the all-important esoteric knowledge.</p>
<p>*</p>
<p>What had happened was this: Charlotte was far from a ChatGPT bot but instead a trained and experienced librarian.</p>
<p>(You can and should&nbsp;<a href="https://bsky.app/profile/tambourine.bsky.social" rel="">follow her here</a>&nbsp;– she is a genius and a treasure, and she has found other odd things out for other people.)</p>
<p>She sensibly assumed some of the things I could recall would have more weight – be more reliable – than others.</p>
<p>(The&nbsp;<em>“Michaelmas”</em>&nbsp;was, it turned out, a false memory – and this had undermined my searches.)</p>
<p>She then used various permutations of my memory points until she found a match, and she then found a book which someone had scanned onto internet archive.</p>
<p>You can see the book <a href="https://archive.org/details/ghostsshadows0000unse" rel="">here</a>.</p>
<p>The details there found could then be cross-referenced against&nbsp;<a href="http://philsp.com/" rel="">this truly amazing catalogue of fantasy short stories</a>&nbsp;-and it was indeed in an anthology – alongside the Herrick poem!</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png?resize=660%2C377&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 1456w" alt="" width="660" height="377" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:543,&quot;width&quot;:951,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:209299,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>The story had been found – because of a librarian using critical skills (and thereby not giving equal weight to each factor), an archive, and a catalogue/index.</p>
<p>Verily: librarians, archivists, cataloguers, and indexers are the Noble Professions.</p>
<p>For they organise information in a manner in which humans actually think – unlike ChatGPT and Artificial Intelligence.</p>
<p>They are the holders of the old knowledge and skills.</p>
<p>*</p>
<p>So here is the book:</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png?resize=414%2C659&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 1456w" alt="" width="414" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:414,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:606327,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>*</p>
<p>My initial reaction was that Charlotte had certainly uncovered the same story – but it was perhaps in a different edition.</p>
<p>The cover of what Charlotte had found was black – and I distinctly remembered the book being purple.</p>
<p>Nonetheless I ordered the book online – so I could read all the story again in physical form (I refused to read it on the archive – I could wait one more week after so many years).</p>
<p>And when the book arrived I noticed something.</p>
<p>The back of the book was purple.</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:482521,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>Never judge a book by its front cover.</p>
<p>*</p>
<p>Before we come to the second wonderful event of this book-quest, here is the story of&nbsp;<em>“Twelfth Night Till Candlemas”</em>&nbsp;in full.</p>
<p>Note as you read how the old knowledge is used and the necessary rule are applied – and how the vicar ensures that a suppressed, secret ceremony can take place – there seems to be a great deal of cultural and religious knowledge behind this simple-looking children’s story.</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:335613,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:347152,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:385282,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:378668,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:383785,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a0736730-8772-48b3-b369-fadafefc7a97_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:271792,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>*</p>
<p><em>“Of course!”</em>&nbsp;said Granny Hawkins.</p>
<p>Of course.</p>
<p>*</p>
<p>But who was Ruth C. Paine?</p>
<p>It was certainly not this&nbsp;<a href="https://en.wikipedia.org/wiki/Ruth_Paine" rel="">Ruth Paine</a>&nbsp;(which made internet searches very difficult).</p>
<p>The editor&nbsp;<a href="https://en.wikipedia.org/wiki/Dorothy_Edwards_(children%27s_writer)" rel="">Dorothy Edwards</a>&nbsp;was a prolific author and editor – many of her books are still in print – and she was also involved with&nbsp;<em><a href="https://en.wikipedia.org/wiki/Listen_with_Mother" rel="">Listen with Mother</a></em>.</p>
<p>(I said you should be sitting comfortably.)</p>
<p>But Ruth C. Paine was more elusive.</p>
<p>*</p>
<p>What I was then able to find out was that Ruth C. Paine had published stories in a number of Dorothy Edwards’ anthologies.</p>
<p>Here is another example, about the changing of the seasons, with a splendid line from a frog about how to deal with winter</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png?resize=393%2C649&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 1456w" alt="" width="393" height="649" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:649,&quot;width&quot;:393,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:408742,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png?resize=660%2C544&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 1456w" alt="" width="660" height="544" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:655,&quot;width&quot;:794,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:841143,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>*</p>
<p><em>‘I’m just off to the pond. I shall dive to the bottom and cover myself with mud and stay there. That is the only proper way to spend the winter,’ said Frog, and he hopped away.</em></p>
<p>*</p>
<p>I can also recommend the story about old Mother Merriweather in her&nbsp;<em>Cuckoo Fair</em>&nbsp;story, which deals with summer, in&nbsp;<a href="https://www.amazon.co.za/Mists-Magic-Dorothy-Edwards/dp/0718825373" rel="">this other Dorothy Edwards anthology</a>.</p>
<p>*</p>
<p>A bit more research showed that Ruth C. Paine had contributed&nbsp;<a href="https://genome.ch.bbc.co.uk/schedules/service_bbc_radio_fourfm/1971-07-09" rel="">a story for broadcast for&nbsp;</a><em><a href="https://genome.ch.bbc.co.uk/schedules/service_bbc_radio_fourfm/1971-07-09" rel="">Listen with Mother</a></em>&nbsp;(thank you to the kind person who put the&nbsp;<em>Radio Times</em>&nbsp;listings archive online).</p>
<p>But otherwise it was really not surprising that an author from the 1970s and 1980s had so little online trace.</p>
<p>It crossed my mind that Ruth C. Paine could be a pseudonym of Dorothy Edwards – such things are not uncommon with busy editors who need to fill spaces in books and broadcasts.</p>
<p>Yet there was something about the distinctive depth of knowledge behind the Candlemas story which made it unlikely to be a throwaway pseudonym of someone else. And Dorothy Edwards often included her own stories in her anthologies.</p>
<p>Anyway, no matter: I had the book and the story.</p>
<p>That is where I thought this story would end.</p>
<p>*</p>
<p>And then the second remarkable event occurred.</p>
<p>Charlotte and I got&nbsp;<a href="https://bsky.app/profile/vickymackenzie.bsky.social/post/3lclu2ovlpc2h" rel="">this reply</a>&nbsp;from the novelist&nbsp;<a href="https://victoriamackenzie.net/" rel="">Victoria MacKenzie</a>:</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png?resize=596%2C666&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 1456w" alt="" width="596" height="666" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:666,&quot;width&quot;:596,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:133344,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>And so Ruth C. Paine certainly did exist – and, as the Candlemas story indicated, she did have an extensive knowledge of religion and cultural history.</p>
<p>Her great niece has now kindly provided the following fascinating details:</p>
<p><em>“Ruth Cecilia Paine was my great aunt – my grandfather’s twin sister – and although she passed away in 2001, when I was just twenty-one, she was a big influence on my life. We wrote to each other regularly (I still remember her postcode, all these years later) and she was very encouraging to me about my education; at the start of each school year she sent me a little money for stationery, which I found incredibly exciting!</em></p>
<p><em>“I always knew that she had written stories for children, but that only a handful had been published – mostly in anthologies edited by Dorothy Edwards. As far as I knew, writing was a hobby, but I sensed it was one that meant a great deal to her. She often sent me books as gifts and occasionally I visited her in her flat in Canterbury where she lived with her lifelong companion, Lillian.</em></p>
<p><em>“She was a Christian and a church-goer all her life, latterly giving tours of Canterbury Cathedral. My dad told me that she’d been a missionary in India earlier in her life and I seem to half-remember a story she told me of travelling through a monsoon in a small aeroplane – understandably a terrifying experience!</em></p>
<p><em>“When she returned to in Britain she became a Religious Studies teacher, and her last job was at Hastings High School, before her retirement around the time I was born in 1980. Apparently she was regarded as quite formidable by my dad’s generation, but she was always very kind to me. I wish so much that she could have known her great niece would become a writer!”</em></p>
<p>*</p>
<p>The formidable Ruth C. Paine had indeed been a former missionary in her youth – Birmingham University records attest this.</p>
<p>This is Ruth Cecilia Paine in her teaching days:</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg?resize=268%2C297&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 1456w" alt="" width="268" height="297" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:297,&quot;width&quot;:268,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:21283,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>And not only did Vicky MacKenzie provide this information and this photo, she also had a box of papers from her late great aunt, and in that box of papers were the original amended proofs of the personally influential story I had spent years looking for!</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg?w=317&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 1456w" alt="" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:317,&quot;bytes&quot;:441348,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg?w=319&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 1456w" alt="" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:319,&quot;bytes&quot;:774750,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg?w=319&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 1456w" alt="" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:319,&quot;bytes&quot;:827104,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg?w=318&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 1456w" alt="" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:318,&quot;bytes&quot;:827104,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg?w=320&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 1456w" alt="" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:320,&quot;bytes&quot;:556761,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>*</p>
<p><em>“Of course!”</em>&nbsp;said Granny Hawkins.</p>
<p>Of course.</p>
<p>*</p>
<p>In a matter of days I has gone from the ritual despair of an annual fruitless, futile search, to not only having the story and the book – but to also seeing the actual manuscript of the story I had spent forty years thinking about and about thirty years searching for.</p>
<p>This was a wonderful, extraordinary turn-of-events.</p>
<p>*</p>
<p>Two things can perhaps be said by way of a conclusion to this story.</p>
<p>The first is that we should be wary of the mischievous demons of our own age – that is ChatGPT and Artificial Intelligence – and to renew our trust in the Noble Professions who hold the old knowledge and skills: librarians, archivists, cataloguers, and indexers.</p>
<p>The second is that nowadays the real problem perhaps is not with Christmas decorations staying up too late, but with them going up too early, and with shops selling Christmas wares and playing Christmas music well before Advent, let alone Christmas.</p>
<p>We need new cautionary tales about when such things should be done and not done.</p>
<p>We are going to need some new goblins.</p>
<p>*******</p>
<p>I am very grateful to the heirs and holders of the literary estate of Ruth C. Paine for their kind permission for me to publish&nbsp;<em>“Twelfth Night Till Candlemas”</em>&nbsp;and&nbsp;<em>“How Nip spent the winter”</em>.</p>
<p>Editors would do well to contact Vicky MacKenzie to arrange permission to put her great aunt’s stories in new anthologies.</p>
<p>I am also grateful to Vicky MacKenzie for her kind permission to publish the unpublished corrected manuscript of&nbsp;<em>“Twelfth Night Till Candlemas”&nbsp;</em>and the unpublished photograph of her aunt..</p>
<p>Many thanks to Charlotte who not only found the story, but dealt with many follow-on queries.</p>
<p>Many thanks also to my friends who listened to previous versions of this post.</p>
<p>This post is dedicated to one of these friends, Nick, who is currently dealing with a challenging time – and who has also listened to me go on about story-telling for over thirty years. Poor sod.</p>
</div>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: TabPFN v2 – A SOTA foundation model for small tabular data (110 pts)]]></title>
            <link>https://www.nature.com/articles/s41586-024-08328-6/link</link>
            <guid>42647343</guid>
            <pubDate>Thu, 09 Jan 2025 16:38:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/s41586-024-08328-6/link">https://www.nature.com/articles/s41586-024-08328-6/link</a>, See on <a href="https://news.ycombinator.com/item?id=42647343">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                        <div id="Sec1-section" data-title="Main"><h2 id="Sec1">Main</h2><div id="Sec1-content"><p>Throughout the history of artificial intelligence, manually created algorithmic components have been replaced with better-performing end-to-end learned ones. Hand-designed features in computer vision, such as SIFT (Scale Invariant Feature Transform)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Lowe, D. G. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis. 60, 91–110 (2004)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR10" id="ref-link-section-d83722773e547">10</a></sup> and HOG (Histogram of Oriented Gradients)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Dalal, N. &amp; Triggs, B. Histograms of oriented gradients for human detection. In Proc. 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) 886–893 (IEEE, 2005)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR11" id="ref-link-section-d83722773e551">11</a></sup>, have been replaced by learned convolutions; grammar-based approaches in natural language processing have been replaced by learned transformers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Vaswani, A. et al. Attention is all you need. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR12" id="ref-link-section-d83722773e555">12</a></sup>; and the design of customized opening and end-game libraries in game playing has been superseded by end-to-end learned strategies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Silver, D. et al. Mastering the game of go with deep neural networks and tree search. Nature 529, 484–489 (2016)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR3" id="ref-link-section-d83722773e559">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Silver, D. et al. Mastering the game of go without human knowledge. Nature 550, 354–359 (2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR13" id="ref-link-section-d83722773e562">13</a></sup>. Here we extend this end-to-end learning to the ubiquitous domain of tabular data.</p><p>The diversity of tabular data&nbsp;sets them apart from unprocessed modalities such as text and images. While in language modelling&nbsp;for example the meaning of a word is consistent across documents, in tabular datasets the same value can mean fundamentally different things. A drug discovery dataset, for example, might record chemical properties, whereas another dataset in materials science might document thermal and electric properties. This specialization leads to a proliferation of smaller, independent datasets and associated models. To illustrate, on the popular tabular benchmarking website openml.org, 76% of the datasets contain less than 10,000 rows at the time of writing.</p><p>Deep learning methods have traditionally struggled with tabular data, because of the heterogeneity between datasets and the heterogeneity of the raw data itself: Tables contain columns, also called features, with various scales and types (Boolean, categorical, ordinal, integer, floating point), imbalanced or missing data, unimportant features, outliers and so on. This made non-deep-learning methods, such as tree-based models, the strongest contender so far<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e572">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e575">15</a></sup>.</p><p>However, these traditional machine learning models have several drawbacks. Without substantial modifications, they yield poor out-of-distribution predictions and poor transfer of knowledge from one dataset to another<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goodfellow, I., Bengio, Y. &amp; Courville, A. Deep Learning (MIT Press, 2016)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR16" id="ref-link-section-d83722773e582">16</a></sup>. Finally, they are hard to combine with neural networks, as they do not propagate gradients.</p><p>As a remedy, we introduce TabPFN, a foundation model for small- to medium-sized tabular data. This new supervised tabular learning method can be applied to any small- to moderate-sized dataset and yields dominant performance for datasets with up to 10,000 samples and 500 features. In a single forward pass, TabPFN significantly outperforms state-of-the-art baselines on our benchmarks, including gradient-boosted decision trees, even when these are allowed 4 h of tuning, a speedup of 5,140× (classification) and 3,000× (regression). Finally, we demonstrate various foundation model characteristics of TabPFN, including fine-tuning, generative abilities and density estimation.</p></div></div><div id="Sec2-section" data-title="Principled in-context learning"><h2 id="Sec2">Principled in-context learning</h2><div id="Sec2-content"><p>TabPFN leverages in-context learning (ICL)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Brown, T. et al. Language models are few-shot learners. In Proc. Advances in Neural Information Processing Systems (eds Larochelle, H. et al.) Vol. 33, 1877–1901 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR17" id="ref-link-section-d83722773e598">17</a></sup>, the same mechanism that led to the astounding performance of large language models, to generate a powerful tabular prediction algorithm that is fully learned. Although ICL was first observed in large language models, recent work has shown that transformers can learn simple algorithms such as logistic regression through ICL<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Garg, S., Tsipras, D., Liang, P. S. &amp; Valiant, G. What can transformers learn in-context? A case study of simple function classes. In Proc. Advances in Neural Information Processing Systems Vol. 35, 30583–30598 (ACM, 2022)." href="#ref-CR18" id="ref-link-section-d83722773e602">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. &amp; Zhou, D. What learning algorithm is in-context learning? Investigations with linear models. In Proc. The Eleventh International Conference on Learning Representations (ICLR, 2023)." href="#ref-CR19" id="ref-link-section-d83722773e602_1">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Von Oswald, J. et al. Transformers learn in-context by gradient descent. In Proc. 40th International Conference on Machine Learning 35151–35174 (PMLR, 2023)." href="#ref-CR20" id="ref-link-section-d83722773e602_2">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Zhou, H. et al. What algorithms can transformers learn? A study in length generalization. In Proc. The Twelfth International Conference on Learning Representations (ICLR, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR21" id="ref-link-section-d83722773e605">21</a></sup>. Prior-data Fitted Networks (PFNs) have shown that even complex algorithms, such as Gaussian Processes and Bayesian Neural Networks, can be approximated with ICL<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e609">22</a></sup>. ICL enables us to learn a wider space of possible algorithms, including cases for which a closed-form solution does not exist.</p><p>We build on a preliminary version of TabPFN<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hollmann, N., Müller, S., Eggensperger, K. &amp; Hutter, F. TabPFN: a transformer that solves small tabular classification problems in a second. In Proc. The Eleventh International Conference on Learning Representations (ICLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR23" id="ref-link-section-d83722773e616">23</a></sup>, which demonstrated the applicability of in-context-learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Brown, T. et al. Language models are few-shot learners. In Proc. Advances in Neural Information Processing Systems (eds Larochelle, H. et al.) Vol. 33, 1877–1901 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR17" id="ref-link-section-d83722773e620">17</a></sup> for tabular data in principle but had many limitations that rendered it inapplicable in most cases. Based on a series of improvements, the new TabPFN scales to 50× larger datasets; supports regression tasks, categorical data and missing values; and is robust to unimportant features and outliers.</p><p>The key idea behind TabPFN is to generate a large corpus of synthetic tabular datasets and then train a transformer-based<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Vaswani, A. et al. Attention is all you need. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR12" id="ref-link-section-d83722773e627">12</a></sup> neural network to learn to solve these synthetic prediction tasks. Although traditional approaches require hand-engineered solutions for data challenges such as missing values, our method autonomously learns effective strategies by solving synthetic tasks&nbsp;that include these challenges. This approach leverages ICL as a framework for exemplar-based declarative programming of algorithms. We design desired algorithmic behaviour by generating diverse synthetic datasets that demonstrate the desired behaviour and then train a model to encode an algorithm that satisfies it. This shifts the algorithm design process from writing explicit instructions to defining input–output examples, opening up possibilities for creating algorithms in various domains. Here, we apply this approach to the high-impact field of tabular learning, generating a powerful tabular prediction algorithm.</p><p>Our ICL approach differs fundamentally from standard supervised deep learning. Usually, models are trained per dataset, updating model parameters on individual samples or batches according to hand-crafted weight-updating algorithms, such as Adam<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Kingma, D. &amp; Ba, J. Adam: A method for stochastic optimization. In Proc. International Conference on Learning Representations (ICLR, 2015)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR24" id="ref-link-section-d83722773e634">24</a></sup>. At inference time, the learned model is applied to test samples. By contrast, our approach is trained across datasets and is applied to entire datasets at inference time rather than individual samples. Before being applied to real-world datasets, the model is once pre-trained on millions of synthetic datasets representing different prediction tasks. At inference time, the model receives an unseen dataset with both labelled training and unlabelled test samples and performs training and prediction on this dataset in a single neural network forward pass.</p><p>Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig1">1</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig2">2</a> outline our approach:</p><ol>
                <li>
                  <span>1.</span>
                  
                    <p>Data generation: we define a generative process (referred to as our prior) to synthesize diverse tabular datasets with varying relationships between features and targets, designed to capture a wide range of potential scenarios that our model might encounter. We sample millions of datasets from the generative process. For each dataset, a subset of samples has their target values masked, simulating a supervised prediction problem. Further details of our prior design are shown in the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec4">Synthetic data based on causal models</a>’.</p>
                  
                </li>
                <li>
                  <span>2.</span>
                  
                    <p>Pre-training: we train a transformer model, our PFN, to predict the masked targets of all synthetic datasets, given the input features and the unmasked samples as context. This step is done only once during model development, learning a generic learning algorithm that can be used to predict any dataset.</p>
                  
                </li>
                <li>
                  <span>3.</span>
                  
                    <p>Real-world prediction: the resulting trained model can now be applied to arbitrary unseen real-world datasets. The training samples are provided as context to the model, which predicts the labels of these unseen datasets through ICL.</p>
                  
                </li>
              </ol><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Overview of the proposed method."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Overview of the proposed method.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="365"></picture></a></div><p><b>a</b>, The high-level overview of TabPFN pre-training and usage. <b>b</b>, The TabPFN architecture. We train a model to solve more than 100 million synthetic tasks. Our architecture is an adaptation of the standard transformer encoder that is adapted for the two-dimensional data encountered in tables.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Our approach also has a theoretical foundation as described in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e714">22</a></sup>. It can be viewed as approximating Bayesian prediction for a prior defined by the synthetic datasets. The trained PFN will approximate the posterior predictive distribution <span>\(p({\widehat{{\bf{y}}}}_{{\rm{test}}}| {{\bf{X}}}_{{\rm{test}}},{{\bf{X}}}_{{\rm{train}}},{{\bf{y}}}_{{\rm{train}}})\)</span>  and thus return a Bayesian prediction for the specified distribution over artificial datasets used during PFN pre-training.</p></div></div><div id="Sec3-section" data-title="An architecture designed for tables"><h2 id="Sec3">An architecture designed for tables</h2><div id="Sec3-content"><p>The transformer architecture is currently the favoured architecture for flexible deep learning and foundation models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Jumper, J. M. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583 – 589 (2021)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR4" id="ref-link-section-d83722773e834">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="OpenAI. GPT-4 Technical Report. Preprint at 
                  https://arxiv.org/abs/2303.08774
                  
                 (2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR5" id="ref-link-section-d83722773e837">5</a></sup>. Transformer models work on sequences and combine information between sequence items using so-called attention mechanisms<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. In Proc. 3rd International Conference on Learning Representations (eds Bengio, Y. &amp; LeCun, Y.) (ICLR, 2015)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR25" id="ref-link-section-d83722773e841">25</a></sup>, allowing them to effectively capture long-range dependencies and learn complex relationships in data. Although transformer-based models can be applied to tabular data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Gorishniy, Y., Rubachev, I., Khrulkov, V. &amp; Babenko, A. Revisiting deep learning models for tabular data. In Proc. Advances in Neural Information Processing Systems 34 (eds Ranzato, M. et al.) 18932–18943 (NeurIPS, 2021)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR26" id="ref-link-section-d83722773e845">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Zhu, B. et al. XTab: cross-table pretraining for tabular transformers. In Proc. 40th International Conference on Machine Learning (eds Krause, A. et al.) 43181–43204 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR27" id="ref-link-section-d83722773e848">27</a></sup>, TabPFN addresses two key limitations inherent to them. First, as transformers are designed for sequences, they treat the input data as a single sequence, not using the tabular structure. Second, machine learning models are often used in a fit-predict model, in which a model is fitted on the training set once and then reused for multiple test datasets. Transformer-based ICL algorithms, however, receive train and test data in a single pass and thus perform training and prediction at once. Thus, when a fitted model is reused, it has to redo computations for the training set.</p><p>To better use the tabular structure, we propose an architecture that assigns a separate representation to each cell in the table, inspired by refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e855">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Lorch, L., Sussex, S., Rothfuss, J., Krause, A. &amp; Schölkopf, B. Amortized inference for causal structure learning. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) Vol. 35, 13104–13118 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR28" id="ref-link-section-d83722773e858">28</a></sup>. Our architecture, visualized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig1">1b</a>, uses a two-way attention mechanism, with each cell attending to the other features in its row (that is, its sample) and then attending to the same feature across its column (that is, all other samples). This design enables the architecture to be invariant to the order of both samples and features and enables more efficient training and extrapolation to larger tables than those encountered during training, in terms of both the number of samples and features.</p><p>To mitigate repeating computations on the training set for each test sample in a fit-predict setting, our model can separate the inference on the training and test samples. This allows us to perform ICL on the training set once, save the resulting state and reuse it for multiple test set inferences. On datasets with 10,000 training samples and 10 features, our optimized train-state caching results in inference speedups of around 300× on CPU (from 32 s to 0.1 s) and 6× on GPU. With 10× more features (100), the speedups increase to 800× on CPU and 30× speedup on GPU. These measurements focus solely on the core inference process, excluding pre-processing and ensembling steps detailed in the section ‘Inference details’. The lower speedups on GPUs are because of an underutilization of their massively parallel architecture.</p><p>We further optimize the memory and compute requirements of the architecture by computing layer norms in half-precision, using flash attention<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Dao, T., Fu, D., Ermon, S., Rudra, A. &amp; Ré, C. Flashattention: fast and memory-efficient exact attention with io-awareness. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) Vol. 35, 16344–16359 (2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR29" id="ref-link-section-d83722773e871">29</a></sup>, activation checkpointing and sequential computation of the state. Our optimizations reduce the memory requirements by a factor of four, resulting in less than 1,000 bytes per cell. This enables the prediction on datasets with up to 50 million cells (for example, 5 million rows × 10 features) on a single H100 GPU.</p><p>For regression tasks, we use a piece-wise constant output distribution, following refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e879">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Torgo, L. &amp; Gama, J. Regression using classification algorithms. Intell. Data Anal. 1, 275–292 (1997)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR30" id="ref-link-section-d83722773e882">30</a></sup>, which allows our models to predict a probability distribution of target values instead of a single value, including, for example, bimodal distributions.</p></div></div><div id="Sec4-section" data-title="Synthetic data based on causal models"><h2 id="Sec4">Synthetic data based on causal models</h2><div id="Sec4-content"><p>The performance of TabPFN relies on generating suitable synthetic training datasets that capture the characteristics and challenges of real-world tabular data. To generate such datasets, we developed an approach based on structural causal models (SCMs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Pearl, J. Causality 2nd edn (Cambridge Univ. Press, 2009)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR31" id="ref-link-section-d83722773e894">31</a></sup>. SCMs provide a formal framework for representing causal relationships and generative processes underlying the data. By relying on synthetic data instead of large collections of public tabular data, we avoid common problems of foundational models, such as privacy and copyright infringements, contaminating our training data with test data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Jiang, M. et al. Investigating Data Contamination for Pre-training Language Models. Preprint at 
                  https://arxiv.org/abs/2401.06059
                  
                 (2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR32" id="ref-link-section-d83722773e898">32</a></sup> or limited data availability.</p><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig2">2</a>, our generative pipeline first samples high-level hyperparameters, such as dataset size, number of features and difficulty level, to govern the overall properties of each synthetic dataset. Guided by these hyperparameters, we construct a directed acyclic graph specifying the causal structure underlying the dataset.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Overview of the TabPFN prior."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: Overview of the TabPFN prior.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="318"></picture></a></div><p><b>a</b>, For each dataset, we first sample high-level hyperparameters. <b>b</b>, Based on these hyperparameters, we construct a structural causal model that encodes the computational function generating the dataset. Each node holds a vector and each edge in the computational graph implements a function according to one of the connection types. In step 1, using random noise variables we generate initialization data, which is fed into the root nodes of the graphs and propagated through the computational graph for each to-be-generated sample. In step 2, we randomly sample feature and target node positions in the graph, labelled F and T, respectively. In step 3, we extract the intermediate data representations at the sampled feature and target node positions. In step 4, we post-process the extracted data. <b>c,</b> We retrieve the final datasets. We plot interactions of feature pairs and the node colour represents the class of the sample.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>To generate each sample within a dataset, we propagate randomly generated noise, called our initialization data, through the root nodes of the causal graph. This initialization data are generated by sampling from a random normal or uniform distribution with varying degrees of non-independence between samples, see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec23">Initialization data sampling</a>’. As these data traverse the edges of the computational graph, we apply a diverse set of computational mappings: small neural networks with linear or nonlinear activations (for example, sigmoid, ReLU (rectified linear unit), modulo, sine), discretization mechanisms for generating categorical features and decision tree structures to encode local, rule-based dependencies. At each edge, we add Gaussian noise, introducing uncertainty into the generated data. We save the intermediate data representations at each node to be retrieved later. See section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec22">Computational edge mappings</a>’ for details.</p><p>After traversing the causal graph, we extract the intermediate representations at the sampled feature and target nodes, yielding a sample consisting of feature values and an associated target value.</p><p>By incorporating various data challenges and complexities into the synthetic datasets, we create a training ground that allows TabPFN to develop strategies for handling similar issues in real-world datasets. For instance, consider the case of missing values, commonly present in tabular data. By exposing TabPFN to synthetic datasets with varying patterns and fractions of missing values in our synthetic data generation process, the model learns effective ways of handling missing values that generalize to real-world datasets. We apply post-processing techniques to further enhance the realism and challenge the robustness of the learned prediction algorithms. This includes warping with the Kumaraswamy distribution<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Kumaraswamy, P. A generalized probability density function for double-bounded random processes. J. Hydrol. 46, 79–88 (1980)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR33" id="ref-link-section-d83722773e952">33</a></sup>, introducing complex nonlinear distortions and quantization mimicking discretized features. See section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec24">Post-processing</a>’ for details.</p><p>Through this generative process, we created a massive corpus of around 100 million synthetic datasets per model training, each with a unique causal structure, feature types and functional characteristics.</p></div></div><div id="Sec5-section" data-title="Qualitative analysis"><h2 id="Sec5">Qualitative analysis</h2><div id="Sec5-content"><p>We first analyse the behaviour of TabPFN on toy problems to build intuition and disentangle the impact of various dataset characteristics. As regression problems are easier to visualize, we focus on these in our qualitative analysis. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig3">3a</a>, we compare TabPFN with a diverse set of standard predictors, with all methods using default settings.</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="The behaviour of TabPFN and a set of baselines on simple functions."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: The behaviour of TabPFN and a set of baselines on simple functions.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="319"></picture></a></div><p>In all plots, we use orange for the ground truth and blue for model predictions. <b>a</b>, Each column represents a different toy function, each having a single feature (along the <i>x</i>-axis) and a target (along the <i>y</i>-axis). TabPFN can model a lot of different functions, including noisy functions. <b>b</b>, TabPFN can model distributions over outputs out of the box, which is exemplified by predicting the light intensity pattern in a double-slit experiment after observing the positions of 1,000 photons.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Linear (ridge) regression can naturally model only linear functions, leading to simple and interpretable predictions but catastrophic failure on many of the toy functions. Multilayer perceptrons (MLPs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Rosenblatt, F. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Report No. 1196-0-8 (Cornell Aeronautical Lab, 1961)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR34" id="ref-link-section-d83722773e1008">34</a></sup> perform worse on datasets with highly non-smooth patterns<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e1012">14</a></sup>. This is especially apparent for the step function. TabPFN, by contrast, models either function type, smooth or non-smooth, out of the box. This includes a good approximation to step functions despite TabPFN being a neural network. CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d83722773e1016">9</a></sup>, representative of tree-based methods, fits only piece-wise constant functions. Although this leads to approximation errors and unintuitive predictions, it avoids catastrophic failures.</p><p>The main advantage of TabPFN over all baselines is its inherent ability to model uncertainty at no extra cost. Whereas classical regression methods output a single real-valued prediction, TabPFN returns a target distribution, capturing the uncertainty of predictions. These uncertainty modelling abilities of TabPFN extend beyond simple distributions and can handle complex, multi-modal distributions. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig3">3b</a> shows this by modelling the density of light reaching a detector screen in a double-slit experiment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Young, T. I. The bakerian lecture. experiments and calculations relative to physical optics. Philos. Trans. R. Soc. Lond. 94, 1–16 (1804)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR35" id="ref-link-section-d83722773e1026">35</a></sup> for different slit distances and widths. In this classic experiment, photons are sent through two slits creating a multi-modal intensity pattern because of the wave-like interference behaviour of light. TabPFN predicts these intricate patterns in just a single forward pass, requiring only 1.2 s. By contrast, traditional methods such as CatBoost require training multiple quantile models at different quantiles and reconstructing the distribution from these predictions. Even after tuning CatBoost specifically for this task, it produced substantially worse predictions compared with TabPFN, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig3">3b</a>. With default settings, CatBoost requires 169.3 s and yields further deteriorated results. Qualitatively, we observe that TabPFN is more accurate in predicting very low densities and has fewer artefacts compared with CatBoost.</p></div></div><div id="Sec6-section" data-title="Quantitative analysis"><h2 id="Sec6">Quantitative analysis</h2><div id="Sec6-content"><p>We quantitatively evaluate TabPFN on two dataset collections: the AutoML Benchmark<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gijsbers, P. et al. AMLB: an AutoML benchmark. J. Mach. Learn. Res. 25, 1–65 (2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR36" id="ref-link-section-d83722773e1042">36</a></sup> and OpenML-CTR23<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Fischer, S. F., Feurer, M. &amp; Bischl, B. OpenML-CTR23 – a curated tabular regression benchmarking suite. In Proc. AutoML Conference 2023 (Workshop) (AutoML, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR37" id="ref-link-section-d83722773e1046">37</a></sup>. These benchmarks comprise diverse real-world tabular datasets, curated for complexity, relevance and domain diversity. From these benchmarks, we use&nbsp;the 29 classification datasets and 28 regression datasets that have up to 10,000 samples, 500 features and 10 classes. We further evaluated additional benchmark suites from refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e1050">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e1053">15</a></sup>, as well as five Kaggle competitions from the Tabular Playground Series.</p><p>We compared TabPFN against state-of-the-art baselines, including tree-based methods (random forest<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Breimann, L. Random forests. Mach. Learn. 45, 5–32 (2001)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR38" id="ref-link-section-d83722773e1060">38</a></sup>, XGBoost (XGB)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR7" id="ref-link-section-d83722773e1064">7</a></sup>, CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d83722773e1068">9</a></sup>, LightGBM<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 3149–3157 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR8" id="ref-link-section-d83722773e1072">8</a></sup>), linear models, support vector machines (SVMs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Cortes, C. &amp; Vapnik, V. Support-vector networks. Mach. Learn. 20, 273–297 (1995)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR39" id="ref-link-section-d83722773e1076">39</a></sup> and MLPs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Rosenblatt, F. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Report No. 1196-0-8 (Cornell Aeronautical Lab, 1961)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR34" id="ref-link-section-d83722773e1081">34</a></sup>.</p><p>Evaluation metrics include ROC AUC (area under the receiver operating characteristic curve; One-vs-Rest) and accuracy for classification, and <i>R</i><sup>2</sup> (coefficient of determination) and negative RMSE (root mean squared error) for regression. Scores were normalized per dataset, with 1.0 representing the best and 0.0 the worst performance with respect to all baselines.</p><p>For each dataset and method, we ran 10 repetitions with different random seeds and train–test splits (90% train, 10% test). We tuned hyperparameters using random search with five-fold cross-validation, with time budgets ranging from 30 s to 4 h. All methods were evaluated using eight CPU cores, with TabPFN additionally using a consumer-grade GPU (RTX 2080 Ti;&nbsp;other methods did not benefit from this, see Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig8">2d</a>). TabPFN was pre-trained once using eight NVIDIA RTX 2080 GPUs over 2 weeks, allowing for ICL on all new datasets in a single forward pass. These modest computational requirements make similar research accessible to academic labs. For details, refer to the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec35">Detailed evaluation protocol</a>’.</p><h3 id="Sec7">Comparison with state-of-the-art baselines</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig4">4a</a> demonstrates the strong out-of-the-box performance of TabPFN compared with tuned and default configurations of XGBoost, CatBoost and a random forest. For classification tasks, TabPFN surpasses CatBoost, the strongest default baseline, by 0.187 (0.939 compared with 0.752) in normalized ROC AUC in the default setting and by 0.13 (0.952 compared with 0.822) in the tuned setting. For regression, TabPFN outperforms CatBoost in normalized RMSE by 0.051 (0.923 compared with 0.872) in the default setting and by 0.093 (0.968 compared with 0.875) in the tuned setting. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig4">4b</a>, we show per-dataset comparisons. Although for some datasets CatBoost outperforms TabPFN, TabPFN wins on most of the datasets.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Comparison of TabPFN on our test benchmarks, containing datasets with up to 10,000 samples and 500 features."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Comparison of TabPFN on our test benchmarks, containing datasets with up to 10,000 samples and 500 features.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="304"></picture></a></div><p>Performance was normalized per dataset before aggregation using all baselines; intervals represent the 95% confidence interval. Wilcoxon <i>P</i> refers to the two-sided Wilcoxon signed-rank test <i>P</i> value<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Wilcoxon, F. in Breakthroughs in Statistics: Methodology and Distribution (eds Kotz, S. &amp; Johnson, N. L.) 196–202 (Springer, 1992)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR54" id="ref-link-section-d83722773e1133">54</a></sup>. <b>a</b>, Average performance of the default as well as the tuned versions of TabPFN and our baselines. All methods are tuned for ROC AUC or RMSE, respectively, thus decreasing the representativeness of the secondary metrics. LGBM, LightGBM; MLP, multilayer perceptron; SVM, support vector machines; RF, random forest; CB, CatBoost; XGB, XGBoost; Lin, logistic regression for classification and ridge regression for regression tasks. Plots on the right-hand side show a magnified analysis of the strongest baselines considered. <b>b</b>, A per-dataset comparison of TabPFN with its strongest baseline, CatBoost. Each dot is the average score on one dataset. <b>c</b>, The impact of hyperparameter tuning for the considered methods. The <i>x</i>-axis shows the average time required to fit and predict with the algorithm.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig4">4c</a> shows how the performance of TabPFN and the baselines improve with more time spent on hyperparameter search. The default of TabPFN, taking 2.8 s on average for classification and 4.8 s for regression, outperforms all baselines, even when tuning them for 4 h—a speedup of 5,140× and 3,000×, respectively. We show comparisons on a larger number of metrics in Extended Data Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab1">1</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab2">2</a>.</p><p>As shown in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig8">2</a>, similar to our primary benchmarks, TabPFN substantially outperformed all baselines on the benchmarks of refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e1176">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e1179">15</a></sup>. The benchmark of ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e1183">14</a></sup> is particularly noteworthy because on this benchmark, tree-based methods were previously found to excel. Moreover, we show in Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab6">6</a> that default TabPFN outperforms default CatBoost on all five Kaggle competitions with less than 10,000 training samples from the latest completed Tabular Playground Series.</p><h3 id="Sec8">Evaluating diverse data attributes</h3><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig5">5a,b</a>, we show the robustness of TabPFN to dataset characteristics that are traditionally hard to handle for neural-network-based approaches<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e1201">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hollmann, N., Müller, S., Eggensperger, K. &amp; Hutter, F. TabPFN: a transformer that solves small tabular classification problems in a second. In Proc. The Eleventh International Conference on Learning Representations (ICLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR23" id="ref-link-section-d83722773e1204">23</a></sup>.</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Robustness across datasets and performance comparison with tuned ensembles."><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Fig. 5: Robustness across datasets and performance comparison with tuned ensembles.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="352"></picture></a></div><p><b>a</b>, A comparison of modified datasets. We can see that TabPFN is not more vulnerable to the modifications compared with baselines. We also see that TabPFN reproduces the accuracy of CatBoost (default) with only half the training samples provided. Here we normalize scores per dataset (sharing one normalization across all modifications of one experiment) to avoid negative outliers. <b>b</b>, We split the test datasets by data characteristics and analyse the performance per subgroup. <b>c</b>, Classification performance. Left, the win rate of TabPFN (PHE) against AutoGluon (with one tie excluded); right, the ROC AUC score over time for tuning each method, with the first marker representing the default configuration for the non-ensembling methods. <b>d</b>, Regression performance presented as in <b>c</b> but using the RMSE metric. Intervals represent the 95% confidence interval and Wilcoxon <i>P</i> refers to the two-sided Wilcoxon signed-rank test <i>P</i> value<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Wilcoxon, F. in Breakthroughs in Statistics: Methodology and Distribution (eds Kotz, S. &amp; Johnson, N. L.) 196–202 (Springer, 1992)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR54" id="ref-link-section-d83722773e1241">54</a></sup>.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig5">5a</a> provides an analysis of the performance of TabPFN across various dataset types. First, we add uninformative features (randomly shuffled features from the original dataset) and outliers (multiply each cell with 2% probability with a random number between 0 and the outlier factor). The results show that TabPFN is very robust to uninformative features and outliers, something typically hard for neural networks, as can be seen with the MLP baseline. Second, although dropping either samples or features hurts the performance of all methods, with half the samples TabPFN still performs as well as the next best method using all samples.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig5">5b</a>, we split our test datasets into subgroups and perform analyses per subgroup. We create subgroups based on the presence of categorical features, missing values, number of samples and number of features in the datasets. The sample- and feature-number subgroups are split such that a third of the datasets fall into each group. We can see that none of these characteristics strongly affect the performance of TabPFN relative to the other methods. However, we note that these results should not be taken as evidence that TabPFN scales well beyond the 10,000 samples and 500 features considered here. We show four further ablations in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig7">1</a>.</p><h3 id="Sec9">Comparison with tuned ensemble methods</h3><p>We compare the performance of TabPFN with AutoGluon 1.0 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d83722773e1276">40</a></sup>), which combines various machine learning models, including our baselines, into a stacked ensemble<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Wolpert, D. Stacked generalization. Neural Netw. 5, 241–259 (1992)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR41" id="ref-link-section-d83722773e1280">41</a></sup>, tunes their hyperparameters and then generates the final predictions using post hoc ensembling (PHE)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Caruana, R., Niculescu-Mizil, A., Crew, G. &amp; Ksikes, A. Ensemble selection from libraries of models. In Proc. 21st International Conference on Machine Learning (ed. Greiner, R.) (Omnipress, 2004)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR42" id="ref-link-section-d83722773e1284">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In Proc. International Conference on Automated Machine Learning Vol. 224 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR43" id="ref-link-section-d83722773e1287">43</a></sup>. It thus represents a different class of methods compared with individual baselines.</p><p>To assess whether TabPFN can also be improved by a tuned ensemble approach, we introduce TabPFN (PHE). TabPFN (PHE) automatically combines only TabPFN models with PHE and tunes their hyperparameters using a random portfolio from our search space. We detail this approach in the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec30">TabPFN (PHE)</a>’.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig5">5c–d</a> compares the performance of TabPFN, TabPFN (PHE), AutoGluon and CatBoost. For TabPFN (PHE) and AutoGluon, we start with a minimal budget of 300 s for tuning because AutoGluon otherwise does not reliably return results. In just 2.8 s, TabPFN (default) outperforms AutoGluon for classification tasks, even if AutoGluon is allowed up to 4 h, a 5.140× speedup. TabPFN (PHE) further improves performance leading to an average normalized ROC AUC score of 0.971, compared with 0.939 for TabPFN (default) and 0.914 for AutoGluon. For regression tasks, tuning hyperparameters is more important. Here, TabPFN (PHE) outperforms AutoGluon (allowed 4 h) after its minimal tuning budget of 300 s, a 48× speedup.</p></div></div><div id="Sec10-section" data-title="Foundation model with interpretability"><h2 id="Sec10">Foundation model with interpretability</h2><div id="Sec10-content"><p>Apart from its strong predictive performance, TabPFN exhibits key foundation model abilities, such as data generation, density estimation, learning reusable embeddings and fine-tuning. We showcase these abilities through proof-of-concept experiments on the German Credit Dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Hofmann, H. Statlog (German Credit Data). UCI Machine Learning Repository 
                  https://doi.org/10.24432/C5NC77
                  
                 (1994)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR44" id="ref-link-section-d83722773e1312">44</a></sup>, which contains credit risk information and the mfeat-factors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Duin, R. Multiple Features. UCI Machine Learning Repository 
                  https://doi.org/10.24432/C5HC70
                  
                 (1998)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR45" id="ref-link-section-d83722773e1316">45</a></sup> dataset classifying handwritten digits based on a tabular representation.</p><p>TabPFN can estimate the probability density function of numerical features, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig6">6a</a>, and the probability mass function of categorical features. Computing the sample densities enables anomaly detection to identify issues such as fraud, equipment failures, medical emergencies or low-quality data.</p><div data-test="figure" data-container-section="figure" id="figure-6" data-title="Showcase of the application of TabPFN as tabular foundation model."><figure><figcaption><b id="Fig6" data-test="figure-caption-text">Fig. 6: Showcase of the application of TabPFN as tabular foundation model.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig6_HTML.png?as=webp"><img aria-describedby="Fig6" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="180"></picture></a></div><p><b>a</b>,<b>b</b>, On the German Credit Dataset, we perform data density estimation (<b>a</b>) and generation of new synthetic samples (<b>b</b>). <b>c</b>, We show our learned embeddings are useful representations of each sample on the handwritten digits dataset (mfeat-factors) with different classes forming different clusters. <b>d</b>, We demonstrate fine-tuning TabPFN for a specific set of tasks. Fine-tuned on a dataset containing various sine curves (top), we see the model makes more accurate predictions on another sine curve dataset.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>TabPFN also allows synthesizing new tabular data samples that mimic real-world dataset characteristics as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig6">6b</a>. This enables applications such as data augmentation or privacy-preserving data sharing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Rajotte, J.-F. et al. Synthetic data as an enabler for machine learning applications in medicine. iScience 25, 105331 (2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR46" id="ref-link-section-d83722773e1369">46</a></sup>.</p><p>The architecture of TabPFN yields meaningful feature representations that can be reused for downstream tasks such as data imputation and clustering. We extract and visualize learned embeddings from the mfeat-factors dataset in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig6">6c</a>, showing improved class separation compared with the raw data on the first two principal components.</p><p>Furthermore, we demonstrate the ability of TabPFN to improve performance through fine-tuning on related datasets. Unlike tree-based methods, the neural architecture of TabPFN enables fine-tuning on specific dataset classes. We conduct proof-of-concept experiments using sine curve datasets with varying offsets between fine-tuning and test data. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig6">6d</a> shows an example fine-tuning result. Our analysis across 50 runs (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig10">4</a>) shows that TabPFN successfully transfers knowledge even when labels differ significantly between fine-tuning and test tasks, with performance improving as distributions become more similar. This could, for example, enable fine-tuning for a range of datasets from medical studies to obtain an improved general model for medical diagnosis tasks. For details, refer to section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec31">Foundation model abilities</a>’.</p><p>Finally, we have developed a methodology to easily interpret the predictions of TabPFN. Interpretability is crucial for building trust and accountability when deploying models in high-stakes domains. We support the computation of feature importance through SHAP<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Lundberg, S. M. &amp; Lee, S.-I. A unified approach to interpreting model predictions. In Proc. Advances in Neural Information Processing Systems (eds Guyon, I. et al.) Vol. 30, 4765–4774 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR47" id="ref-link-section-d83722773e1395">47</a></sup> (Shapley Additive Explanations), a game-theoretic approach to explain predictions. SHAP values represent the contribution of each feature to the output of the model. Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig9">3</a> compares the feature importance and impact for logistic regression, CatBoost and TabPFN. TabPFN achieves high accuracy while learning simple, interpretable feature relationships. By contrast, logistic regression is interpretable but less accurate, whereas CatBoost is accurate but qualitatively less interpretable because of complex, non-smooth decision boundaries.</p></div></div><div id="Sec11-section" data-title="Conclusion"><h2 id="Sec11">Conclusion</h2><div id="Sec11-content"><p>TabPFN represents a major change in tabular data modelling, leveraging ICL to autonomously discover a highly efficient algorithm that outperforms traditional human-designed approaches on datasets with up to 10,000 samples and 500 features. This shift towards foundation models trained on synthetic data opens up new possibilities for tabular data analysis across various domains.</p><p>Potential future directions include scaling to larger datasets<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Feuer, B. et al. TuneTables: context optimization for scalable prior-data fitted networks. In Proc. 38th Conference on Neural Information Processing Systems (NeurIPS, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR48" id="ref-link-section-d83722773e1413">48</a></sup>, handling data drift<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Helli, K., Schnurr, D., Hollmann, N., Müller, S. &amp; Hutter, F. Drift-resilient tabPFN: In-context learning temporal distribution shifts on tabular data. In Proc. 38th Conference on Neural Information Processing Systems (NeurIPS, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR49" id="ref-link-section-d83722773e1417">49</a></sup>, investigating fine-tuning abilities across related tabular tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Thomas, V. et al. Retrieval &amp; fine-tuning for in-context tabular models. In Proc. 1st Workshop on In-Context Learning at the 41st International Conference on Machine Learning (ICML, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR50" id="ref-link-section-d83722773e1421">50</a></sup> and understanding the theoretical foundations of our approach<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Nagler, T. Statistical foundations of prior-data fitted networks. In Proc. 40th International Conference on Machine Learning (eds Krause, A. et al.) Vol. 202, 25660–25676 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR51" id="ref-link-section-d83722773e1425">51</a></sup>. Future work could also explore creating specialized priors to handle data types such as time series<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Dooley, S., Khurana, G. S., Mohapatra, C., Naidu, S. V. &amp; White, C. ForecastPFN: synthetically-trained zero-shot forecasting. In Proc. 37th Conference on Advances in Neural Information Processing Systems (eds Oh, A. et al.) (NeurIPS, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR52" id="ref-link-section-d83722773e1429">52</a></sup> and multi-modal data, or specialized modalities such as ECG, neuroimaging data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Czolbe, S. &amp; Dalca, A. V. Neuralizer: General neuroimage analysis without re-training. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 6217–6230 (IEEE, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR53" id="ref-link-section-d83722773e1434">53</a></sup> and genetic data. As the field of tabular data modelling continues to evolve, we believe that foundation models, such as TabPFN, will play a key part in empowering researchers. To facilitate the widespread use of TabPFN, in the section ‘User guide’ we discuss how to use it effectively.</p></div></div><div id="Sec12-section" data-title="Methods"><h2 id="Sec12">Methods</h2><div id="Sec12-content"><h3 id="Sec13">User guide</h3><h4 id="Sec14">When to use TabPFN</h4><p>TabPFN excels in handling small- to medium-sized datasets with up to 10,000 samples and 500 features (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig4">4</a> and Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab1">1</a>). For larger datasets and highly non-smooth regression datasets, approaches such as CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d83722773e1460">9</a></sup>, XGB<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR7" id="ref-link-section-d83722773e1464">7</a></sup> or AutoGluon<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d83722773e1468">40</a></sup> are likely to outperform TabPFN.</p><p>Although TabPFN provides a powerful drop-in replacement for traditional tabular data models such as CatBoost, similar to these models, it is intended to be only one component in the toolkit of a data scientist. Achieving top performance on real-world problems often requires domain expertise and the ingenuity of data scientists. As for other modelling approaches, data scientists should continue to apply their skills and insights in feature engineering, data cleaning and problem framing to get the most out of TabPFN. We hope that the training speed of TabPFN will facilitate faster iterations in the data science workflow.</p><h4 id="Sec15">Limitations of TabPFN</h4><p>The limitations of TabPFN are as follows: (1) the inference speed of TabPFN may be slower than highly optimized approaches such as CatBoost; (2) the memory usage of TabPFN scales linearly with dataset size, which can be prohibitive for very large datasets; and (3) our evaluation focused on datasets with up to 10,000 samples and 500 features; scalability to larger datasets requires further study.</p><h4 id="Sec16">Computational and time requirements</h4><p>TabPFN is computationally efficient and can run on consumer hardware for most datasets. However, training on a new dataset is recommended to run on a (consumer) GPU as this speeds it up by one to three orders of magnitude. Although TabPFN is very fast to train, it is not optimized for real-time inference tasks. For a dataset with 10,000 rows and 10 columns, our model requires 0.2 s (0.6 s without GPU) to perform a prediction for one sample, whereas CatBoost (default) can do the same in 0.0002 s. In ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Müller, A., Curino, C. &amp; Ramakrishnan, R. Mothernet: a foundational hypernetwork for tabular classification. Preprint at 
                  https://arxiv.org/abs/2312.08598
                  
                 (2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR55" id="ref-link-section-d83722773e1491">55</a></sup>, further optimizing TabPFN specifically for inference tasks has already been explored, resulting in four times faster inference performance compared with even XGBoost, but so far also reducing predictive quality. Refer to the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec19">Details on the neural architecture</a>’ for details on the memory usage and runtime complexity of TabPFN.</p><h4 id="Sec17">Data preparation</h4><p>TabPFN can handle raw data with minimal pre-processing. If we simply provide the data in a tabular format (NumPy matrix), TabPFN will automatically handle missing values, encode categorical variables and normalize features. Although TabPFN works well out of the box, we can further improve the performance using dataset-specific pre-processing. This can also be partly done automatically with our PHE technique or manually by modifying the default settings. When manually pre-processing data, we should keep in mind that the neural network of TabPFN expects roughly normally distributed features and targets after all pre-processing steps. If we, for example, know that a feature follows a log distribution, it might help to exponentiate it before feeding it to TabPFN. As TabPFN does <i>z</i>-normalization of all inputs, scaling does not affect the predictions. As for all algorithms, however, using domain knowledge to combine or remove features can increase performance.</p><h4 id="Sec18">Hyperparameter tuning</h4><p>TabPFN provides strong performance out of the box without extensive hyperparameter tuning (see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec7">Comparison with state-of-the-art baselines</a>’). If we have additional computational resources, we can further optimize the performance of TabPFN using hyperparameter optimization (HPO) or the PHE technique described in the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec30">TabPFN (PHE)</a>’. Our implementation directly provides HPO with random search and PHE.</p><h3 id="Sec19">Details on the neural architecture</h3><p>Our architecture is a variation of the original transformer encoder<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Vaswani, A. et al. Attention is all you need. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR12" id="ref-link-section-d83722773e1533">12</a></sup> and the original PFN architecture<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e1537">22</a></sup>, but it treats each cell in the table as a separate time position, similar to that in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Lorch, L., Sussex, S., Rothfuss, J., Krause, A. &amp; Schölkopf, B. Amortized inference for causal structure learning. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) Vol. 35, 13104–13118 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR28" id="ref-link-section-d83722773e1541">28</a></sup>. Therefore, it can generalize to more training samples as well as features than seen during training.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig1">1b</a> details our new architecture. All features that go into our architecture are first mapped to floating point values, that is, categoricals are transformed to integers. These values are subjected to <i>z</i>-normalization using the mean and standard deviation for each feature separately across the whole training set. These values are now encoded with simple linear encoders. Each layer first has an attention over features, followed by an attention over samples, both of which operate separately on each column or row, respectively. These two sub-layers are followed by an MLP sublayer. Each sublayer is followed by a residual addition and a half-precision layer norm.</p><p>We found that encoding groups of features can be even more effective compared with encoding one value per representation. For our hyperparameter search space, we selected six architectures for classification and five for regression. In three of the six classification models and four of the five regression models, including the TabPFN default, a transformer position encodes two features of one example; in others, it represents one value.</p><p>Although the inter-feature attention is a classical fully connected attention, our inter-sample attention does not allow the test samples to attend to each other but only to the training data. Therefore, we make sure that the test samples do not influence each other or the training set representations. To allow our model to differentiate features more easily that have the same statistics, for example, two features that have the same entries just in different orders, we use random feature embeddings that we add to all embeddings before the first layer. We generate one embedding per feature by projecting a random vector of one-fourth the size of our embeddings through a learned linear layer and add this to all embeddings representing an instance of that feature.</p><p>As the representations of training samples are not influenced by the test set, we cache the keys and values of the training samples to allow splitting training and inference. We use a special variant of multi-query attention for our inter-sample attention from test samples<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Shazeer, N. Fast transformer decoding: one write-head is all you need. Preprint at 
                  https://arxiv.org/abs/1911.02150
                  
                 (2019)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR56" id="ref-link-section-d83722773e1564">56</a></sup> to save memory when caching representations. In our variant, we use all keys and values for the attention between samples of the training set, but repeatedly use the first key and value for attention from the test samples. This allows caching only one key or value vector pair per cell in the training set that is fed into our inter-sample attention of new test samples.</p><p>The compute requirements of this architecture scale quadratically with the number of samples (<i>n</i>) and the number of features (<i>m</i>), that is <i>O</i>(<i>n</i><sup>2</sup> + <i>m</i><sup>2</sup>), and the memory requirements scale linearly in the dataset size, <i>O</i>(<i>n</i> <span>⋅</span> <i>m</i>).</p><p>Finally, we found that pre-processing inputs can help performance, thus we can perform <i>z</i>-normalization of all inputs across the sample dimension and add an extra input for each cell that indicates whether the input was missing; the input itself is set to 0 in these cases. All inputs are finally linearly encoded into the embedding dimension of TabPFN.</p><h3 id="Sec20">Details on the causal generative process</h3><p>An SCM <span>\({\mathcal{G}}:= (Z,{\epsilon })\)</span> consists of a collection <i>Z</i> <span>≔</span> (<i>z</i><sub>1</sub>, …, <i>z</i><sub><i>k</i></sub>) of structural assignments (called mechanisms): <span>\({z}_{i}={f}_{i}({z}_{{\rm{PA}}{\mathcal{G}}(i)},{{\epsilon }}_{i})\,,\)</span> where <span>\({\rm{PA}}\,{\mathcal{G}}(i)\)</span> is the set of parents of node <i>i</i> (its direct causes) in the underlying directed acyclic graph (DAG) <span>\({\mathcal{G}}\)</span> (the causal graph), <i>f</i><sub><i>i</i></sub> is a (potentially nonlinear) deterministic function and <i>ϵ</i><sub><i>i</i></sub> is a noise variable. Causal relationships in <span>\({\mathcal{G}}\)</span> are represented by edges pointing from causes to effects<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Pearl, J. Causality 2nd edn (Cambridge Univ. Press, 2009)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR31" id="ref-link-section-d83722773e1864">31</a></sup>. As our prior is a sampling procedure, we can make a lot of choices on, for example, the graph size or complexity. By defining a probability distribution over these hyperparameters in the prior, the posterior predictive distribution approximated by TabPFN at inference time implicitly represents a Bayesian ensemble, jointly integrating over a weighted hyperparameter space. The specific hyperparameter ranges and sampling strategies are chosen to cover a diverse set of scenarios that we expect to encounter in real-world tabular data.</p><h4 id="Sec21">Graph structure sampling</h4><p>The structural causal models underlying each dataset are based on a DAG <span>\({\mathcal{G}}\)</span>. We sample these graphs using the growing network with redirection sampling method<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Krapivsky, P. L. &amp; Redner, S. Organization of growing random networks. Phys. Rev. E 63, 066123 (2001)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR57" id="ref-link-section-d83722773e1893">57</a></sup>, a preferential attachment process that generates random scale-free networks. We either sample a single connected component or merge multiple disjoint subgraphs. Disjoint subgraphs lead to features that are marginally independent of the target if they are not connected to the target node, reflecting real-world scenarios with uninformative predictors.</p><p>To control the complexity of the sampled DAGs, we use two hyperparameters: the number of nodes <i>N</i> and the redirection probability <i>P</i>. <i>N</i> is sampled from a log-uniform distribution, <span>\(\log N \sim {\mathcal{U}}(a,b)\)</span>, where <i>a</i> and <i>b</i> are hyperparameters controlling the range of the graph size. The redirection probability <i>P</i> is sampled from a gamma distribution, <i>P</i> ~ <i>Γ</i>(<i>α</i>, <i>β</i>), where <i>α</i> and <i>β</i> are shape and rate parameters, respectively. Larger values of <i>N</i> yield graphs with more nodes, whereas smaller values of <i>P</i> lead to denser graphs with more edges on average<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Krapivsky, P. L. &amp; Redner, S. Organization of growing random networks. Phys. Rev. E 63, 066123 (2001)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR57" id="ref-link-section-d83722773e1987">57</a></sup>.</p><h4 id="Sec22">Computational edge mappings</h4><p>In our implementation, each SCM node and sample is represented as a vector in <span>\({{\mathbb{R}}}^{d}\)</span>. When propagating data through the SCM, the deterministic functions <i>f</i><sub><i>i</i></sub> at each edge map the input vectors to an output vector using four types of computational modules:</p><ol>
                    <li>
                      <span>1.</span>
                      
                        <p>Small neural networks: here we initialize weight matrices <span>\(W\in {{\mathbb{R}}}^{d\times d}\)</span> using Xavier initialization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Glorot, X. &amp; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. 13th International Conference on Artificial Intelligence and Statistics 249–256 (JMLR, 2010)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR58" id="ref-link-section-d83722773e2084">58</a></sup> and apply a linear transformation <i>W</i><i>x</i> + <i>b</i> to the input vectors <span>\(x\in {{\mathbb{R}}}^{d}\)</span>, where <span>\(b\in {{\mathbb{R}}}^{d}\)</span> is a bias vector. After the linear projection, we apply element-wise nonlinear activation functions <span>\(\sigma :{{\mathbb{R}}}^{d}\to {{\mathbb{R}}}^{d}\)</span>, randomly sampled from a set, including identity, logarithm, sigmoid, absolute value, sine, hyperbolic tangent, rank operation, squaring, power functions, smooth ReLU<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Nair, V. &amp; Hinton, G. Rectified linear units improve restricted Boltzmann machines. In Proc. 27th International Conference on Machine Learning (eds Fürnkranz, J. &amp; Joachims, T.) 807–814 (Omnipress, 2010)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR59" id="ref-link-section-d83722773e2221">59</a></sup>, step function and modulo operation.</p>
                      
                    </li>
                    <li>
                      <span>2.</span>
                      
                        <p>Categorical feature discretization: to generate categorical features from the numerical vectors at each node, we map the vector to the index of the nearest neighbour in a set of per node randomly sampled vectors {<i>p</i><sub>1</sub>, …, <i>p</i><sub><i>K</i></sub>} for a feature with <i>K</i> categories. This discrete index will be observed in the feature set as a categorical feature. We sample the number of categories <i>K</i> from a rounded gamma distribution with an offset of 2 to yield a minimum number of classes of 2. To further use these discrete class assignments in the computational graph, they need to be embedded as continuous values. We sample a second set of embedding vectors <span>\(\{{p}_{1}^{{\prime} },\ldots ,{p}_{K}^{{\prime} }\}\)</span> for each class and transform the classes to these embeddings.</p>
                      
                    </li>
                    <li>
                      <span>3.</span>
                      
                        <p>Decision trees: to incorporate structured, rule-based dependencies, we implement decision trees in the SCMs. At certain edges, we select a subset of features and apply decision boundaries on their values to determine the output<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Quinlan, J. R. Induction of decision trees. Mach. Learn. 1, 81–106 (1986)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR60" id="ref-link-section-d83722773e2331">60</a></sup>. The decision tree parameters (feature splits, thresholds) are randomly sampled per edge.</p>
                      
                    </li>
                    <li>
                      <span>4.</span>
                      
                        <p>Noise injection: at each edge, we add random normal noise from the normal distribution <span>\({\mathcal{N}}(0,{\sigma }^{2}I)\)</span>.</p>
                      
                    </li>
                  </ol><h4 id="Sec23">Initialization data sampling</h4><p>For each to-be-generated sample, we randomly generate initialization data <i>ϵ</i> that is inserted at the DAG root nodes and then propagated through the computational graph. The noise variables <i>ϵ</i> are generated according to one of three sampling mechanisms:</p><ol>
                    <li>
                      <span>1.</span>
                      
                        <p>Normal: <span>\({\epsilon } \sim {\mathcal{N}}(0,{\sigma }_{{\epsilon }}^{2})\)</span>, where <span>\({\sigma }_{{\epsilon }}^{2}\)</span> is a hyperparameter.</p>
                      
                    </li>
                    <li>
                      <span>2.</span>
                      
                        <p>Uniform: <span>\({\epsilon } \sim {\mathcal{U}}(-a,a)\)</span>, where <i>a</i> is a hyperparameter.</p>
                      
                    </li>
                    <li>
                      <span>3.</span>
                      
                        <p>Mixed: for each root node, we randomly select either a normal or uniform distribution to sample the initialization noise <i>ϵ</i> from.</p>
                      
                    </li>
                  </ol><p>Furthermore, we sample input data with varying degrees of non-independence for some datasets. Here we first sample a random fraction <i>ρ</i> of samples to serve as prototypes <span>\({x}_{1}^{* },\ldots ,{x}_{M}^{* }\)</span>, where <i>M</i> = <i>ρ</i><i>n</i> and <i>n</i> is the dataset size. Then, for each input vector <i>x</i><sub><i>i</i></sub> to be sampled, we assign weights <i>α</i><sub><i>i</i><i>j</i></sub> to the prototypes and linearly mix the final input as</p><div id="Equ1"><p><span>$${x}_{i}=\mathop{\sum }\limits_{j=1}^{M}{\alpha }_{ij}{x}_{j}^{* },$$</span></p><p>
                    (1)
                </p></div><p>where ∑<sub><i>j</i></sub><i>α</i><sub><i>i</i><i>j</i></sub> = 1. The weights <i>α</i><sub><i>i</i><i>j</i></sub> are sampled from a multinomial distribution, <i>α</i><sub><i>i</i></sub> ~ Multinomial(<i>β</i>), where <i>β</i> is a temperature hyperparameter controlling the degree of non-independence: larger <i>β</i> yields more uniform weights, whereas smaller <i>β</i> concentrates the weights on fewer prototypes per sample.</p><h4 id="Sec24">Post-processing</h4><p>Each dataset is post-processed randomly with one or more of the following post-processings: (1) For some datasets, we use the Kumaraswamy feature warping, introducing nonlinear distortions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Kumaraswamy, P. A generalized probability density function for double-bounded random processes. J. Hydrol. 46, 79–88 (1980)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR33" id="ref-link-section-d83722773e2824">33</a></sup> to features as done in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Müller, S., Feurer, M., Hollmann, N. &amp; Hutter, F. PFNS4BO: in-context learning for Bayesian optimization. In Proc. 40th International Conference on Machine Learning 25444–25470 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR61" id="ref-link-section-d83722773e2828">61</a></sup>. (2) We quantize some continuous features into buckets of randomly sampled cardinality <i>K</i>, mimicking binned or discretized features commonly encountered in datasets. We map a feature value <i>x</i> to the index of the bucket it falls into, determined by <i>K</i> + 1 bin edges sampled from the set of values this feature takes. (3) To introduce scenarios for dynamic imputation and handling of incomplete datasets, a common challenge in data science, we randomly designate a fraction <i>ρ</i><sub>miss</sub> of the data as missing according to the missing completely at random strategy. Each value is masked as missing with probability <i>ρ</i><sub>miss</sub>, independently of the data values.</p><h4 id="Sec25">Target generation</h4><p>To generate target labels for regression tasks, we select a randomly chosen continuous feature without post-processing. For classification labels, we select a random categorical feature that contains up to 10 classes. Thus, natively our method is limited to predicting at most 10 classes. This number can be increased by pre-training on datasets with a larger number of classes or by using approaches such as building a one-vs-one classifier, one-vs-rest classifier or building on approaches such as error-correcting output codes (ECOC)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Dietterich, T. G. &amp; Bakiri, G. Solving multiclass learning problems via error-correcting output codes. J. Artif. Intell. Res. 2, 263–286 (1994)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR62" id="ref-link-section-d83722773e2858">62</a></sup>.</p><h3 id="Sec26">Training details</h3><p>The training loss of any PFN is the cross-entropy between the targets of held-out samples of synthetic datasets and the model prediction. For a test set (<b>X</b><sub>test</sub>, <b><i>y</i></b><sub>test</sub>) = <i>D</i><sub>test</sub>, the training loss is given by <span>\({{\mathcal{L}}}_{{\rm{P}}{\rm{F}}{\rm{N}}}={{\bf{E}}}_{(({{\boldsymbol{X}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}},{{\boldsymbol{y}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}})\cup {D}_{{\rm{t}}{\rm{r}}{\rm{a}}{\rm{i}}{\rm{n}}})\sim p(D)}[-\log {q}_{\theta }({{\boldsymbol{y}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}}|{{\boldsymbol{X}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}},{D}_{{\rm{t}}{\rm{r}}{\rm{a}}{\rm{i}}{\rm{n}}})]\)</span>. By minimizing this loss, the PFN learns to approximate the true Bayesian posterior predictive distribution for a chosen prior over datasets (and potentially their latent variables) <i>D</i>, as shown in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e3210">22</a></sup>.</p><p>We trained our final models for approximately 2,000,000 steps with a batch size of 64 datasets. That means the models used for TabPFN are trained on around 130,000,000 synthetically generated datasets each. One training run requires around 2 weeks on one node with eight Nvidia RTX 2080 Ti GPUs. We sample the number of training samples for each dataset uniformly up to 2,048 and use a fixed validation set size of 128. We sample the number of features using a beta distribution (<i>k</i> = 0.95,&nbsp;<i>b</i> = 8.0) that we linearly scale to the range 1–160. To avoid peaks in memory usage, the total size of each table was restricted to be below 75,000 cells by decreasing the number of samples for large numbers of features.</p><p>We chose the hyperparameters for the prior based on random searches, in which we use only a single GPU per training and evaluate on our development set, see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec6">Quantitative analysis</a>’. We used the Adam optimizer<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Kingma, D. &amp; Ba, J. Adam: A method for stochastic optimization. In Proc. International Conference on Learning Representations (ICLR, 2015)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR24" id="ref-link-section-d83722773e3229">24</a></sup> with linear warmup and cosine annealing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Loshchilov, I. &amp; Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In Proc. 5th International Conference on Learning Representations (ICLR, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR63" id="ref-link-section-d83722773e3233">63</a></sup> and tested a set of learning rates in [0.0001, 0.0005], using the one with the lowest final training loss.</p><h3 id="Sec27">Inference details</h3><p>To get the most performance out of TabPFN, it is crucial to optimize its inference pipeline. We generally always apply TabPFN in a small ensemble, in which we perform pre-processing or post-processing of the data differently for each ensemble member.</p><p>As our models are not fully permutation invariant, for each ensemble member, we shuffle the feature order, approximating order invariance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Murphy, R. L., Srinivasan, B., Rao, V. A. &amp; Ribeiro, B. Janossy pooling: learning deep permutation-invariant functions for variable-size inputs. In Proc. 7th International Conference on Learning Representations (ICLR, 2019)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR64" id="ref-link-section-d83722773e3249">64</a></sup>. For classification tasks, we additionally randomly permute the labels. We also apply a temperature to the softmax distribution of our model outputs for calibration.</p><p>Apart from the above, we use a subset of the following for each of our default ensemble members:</p><ol>
                  <li>
                    <span>1.</span>
                    
                      <p>Quantile + Id: we quantize the inputs to equally spaced values between 0 and 1, but keep a copy of each original feature. This effectively doubles the number of features passed to TabPFN.</p>
                    
                  </li>
                  <li>
                    <span>2.</span>
                    
                      <p>Category shuffling: the labels of categorical features with low cardinality are shuffled.</p>
                    
                  </li>
                  <li>
                    <span>3.</span>
                    
                      <p>SVD: an SVD compression of the features is appended to the features.</p>
                    
                  </li>
                  <li>
                    <span>4.</span>
                    
                      <p>Outlier removal: all outliers, more than 12 standard deviations from the mean, are removed.</p>
                    
                  </li>
                  <li>
                    <span>5.</span>
                    
                      <p>Power transform: each feature (or the label for regression) is transformed using a Yeo–Johnson transformation to stabilize the variance and make the data more normally distributed.</p>
                    
                  </li>
                  <li>
                    <span>6.</span>
                    
                      <p>One-hot encoding: categorical features are encoded using one-hot encoding, in which each category is represented as a binary vector.</p>
                    
                  </li>
                </ol><p>For PHE and hyperparameter tuning of TabPFN, we use a larger set of pre-processing techniques that additionally include a logarithmic, an exponential and a KDI transformation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="McCarter, C. The kernel density integral transformation. Transact. Mach. Learn. Res. 
                  https://openreview.net/pdf?id=6OEcDKZj5j
                  
                 (2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR65" id="ref-link-section-d83722773e3328">65</a></sup>. These transformations help address nonlinear relationships, skewed distributions and varying scales among features.</p><p>To calibrate prediction uncertainty, we apply a softmax temperature (default <i>T</i> = 0.9) by dividing logits before the softmax calculation:</p><div id="Equ2"><p><span>$$P({y}_{i}| x)=\frac{\exp ({z}_{i}/T)}{{\sum }_{j}\exp ({z}_{j}/T)},$$</span></p><p>
                    (2)
                </p></div><p>where <i>z</i><sub><i>i</i></sub> are the logits, <i>T</i> is the temperature and <i>P</i>(<i>y</i><sub><i>i</i></sub><span>∣</span><i>x</i>) is the calibrated probability. We offer the option to generate second-order polynomial features by multiplying up to 50 randomly selected feature pairs:</p><div id="Equ3"><p><span>$${f}_{ij}={x}_{i}\cdot {x}_{j},\quad \,{\rm{for}}\,(i,j)\in {\mathcal{S}},$$</span></p><p>
                    (3)
                </p></div><p>where <span>\({\mathcal{S}}\)</span> is the set of randomly chosen feature pairs. This can capture nonlinear interactions between features. This option is disabled by default. To ensure proper handling of duplicate samples given the sample permutation invariance of our architecture, we add a unique sample identifier feature. This is a random number drawn from a standard normal distribution, ensuring each sample is treated distinctly in the attention mechanism. We also provide an option for subsampling in each estimator, to increase ensemble diversity, which performs random sampling without replacement. This option is disabled by default.</p><h4 id="Sec28">Regression details</h4><p>To enable our model to do classification on a large range of scales and target distributions, we use the following approach. During pre-training, we rescale our regression targets to have zero mean and a standard deviation of 1 (<i>z</i>-score). To decide where the borders between our features lie, we draw a large sample of datasets from our prior and choose the 1/5,000 quantiles from this distribution. At inference time, we bring the real-world data to a similar range by again applying <i>z</i>-score normalization. Furthermore, we allow applying a range of transforms, including a power transform as part of our default. All of the transforms, including the <i>z</i>-score are inverted at prediction time by applying the inverse of the transform to the borders between buckets. This is equivalent to applying the inverse of the transform to the random variable represented by our output distribution but for the half-normals used on the sides for full support<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e3630">22</a></sup>. This is because all transforms are strictly monotone and the borders represent positions on the cumulative distribution function.</p><h4 id="Sec29">Data grouping based on random forest</h4><p>To perform well on very heterogeneous datasets, we also propose to use random trees to split the training data into smaller more homogeneous datasets. This technique is used only when performing HPO or PHE for TabPFN. It is especially useful for TabPFN as our model performs best on small datasets.</p><p>The pre-processing for a single ensemble member, that is, a single tree, works as follows: we use a standard random tree with feature and sample bootstrapping and Gini impurity loss. For each leaf node of the decision tree, we store the subset of training samples that fall into that node and train a TabPFN on these. To predict the class label for a test sample <i>x</i>, we determine the TabPFN to use by passing <i>x</i> through the decision tree. We set the minimal leaf size to be large (500–2,000) such that the resulting data groups are large enough to train a strong model.</p><h3 id="Sec30">TabPFN (PHE)</h3><p>To further enhance the inference performance of TabPFN, in TabPFN (PHE), we use PHE for a fixed portfolio of TabPFN configurations from our search space detailed in Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab5">5</a>. For TabPFN (PHE), we first use holdout validation to sequentially evaluate models from the portfolio until a time limit is reached. After all models are evaluated once, we repeat holdout validation with new data splits until the time limit is reached. Then, we ensemble all evaluated TabPFN models by aggregating their predictions with a weighted arithmetic mean. We learn the weights using greedy ensemble selection (GES)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Caruana, R., Niculescu-Mizil, A., Crew, G. &amp; Ksikes, A. Ensemble selection from libraries of models. In Proc. 21st International Conference on Machine Learning (ed. Greiner, R.) (Omnipress, 2004)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR42" id="ref-link-section-d83722773e3663">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Caruana, R., Munson, A. &amp; Niculescu-Mizil, A. Getting the most out of ensemble selection. In Proc. 6th IEEE International Conference on Data Mining (eds Clifton, C. et al.) 828–833 (IEEE, 2006)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR66" id="ref-link-section-d83722773e3666">66</a></sup> with 25 iterations on prediction data from holdout validation. Finally, we prune each zero-weighted model, refit all remaining models on all data and return the weighted average of their predictions.</p><p>Following standard practice in AutoML, we use GES because its predictive performance is often superior to the best individual model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In Proc. International Conference on Automated Machine Learning Vol. 224 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR43" id="ref-link-section-d83722773e3673">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Feurer, M. et al. in Automated Machine Learning: Methods, Systems, Challenges (eds Hutter, F. et al.) Ch. 6 (Springer, 2019)." href="#ref-CR67" id="ref-link-section-d83722773e3676">67</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Purucker, L. &amp; Beel, J. Assembled-OpenML: creating efficient benchmarks for ensembles in AutoML with OpenML. In Proc. First International Conference on Automated Machine Learning (AutoML, 2022)." href="#ref-CR68" id="ref-link-section-d83722773e3676_1">68</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Purucker, L. &amp; Beel, J. CMA-ES for post hoc ensembling in AutoML: a great success and salvageable failure. In Proc. International Conference on Automated Machine Learning Vol. 224, 1–23 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR69" id="ref-link-section-d83722773e3679">69</a></sup>. Owing to its ICL, we expect TabPFN to overfit the training data less than predictions of traditionally trained algorithms; thus, we opt for (repeated) holdout validation (as in Auto-Sklearn 1; ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Feurer, M. et al. in Automated Machine Learning: Methods, Systems, Challenges (eds Hutter, F. et al.) Ch. 6 (Springer, 2019)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR67" id="ref-link-section-d83722773e3683">67</a></sup>) instead of (repeated) cross-validation (as in AutoGluon<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d83722773e3687">40</a></sup>). Moreover, as GES usually produces sparse weight vectors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In Proc. International Conference on Automated Machine Learning Vol. 224 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR43" id="ref-link-section-d83722773e3691">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Purucker, L. &amp; Beel, J. CMA-ES for post hoc ensembling in AutoML: a great success and salvageable failure. In Proc. International Conference on Automated Machine Learning Vol. 224, 1–23 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR69" id="ref-link-section-d83722773e3694">69</a></sup>, we expect the final ensemble after pruning each zero-weighted model to consist of a smaller number of models than for other ensembling approaches, such as bagging. Consequently, PHE can also improve the inference efficiency of a TabPFN ensemble compared with other ensembling approaches.</p><h3 id="Sec31">Foundation model abilities</h3><h4 id="Sec32">Density estimation</h4><p>The combination of a regression and a classification TabPFN can be used as a generative model for tabular data, not only modelling targets but features as well. Let <span>\({\mathcal{D}}={\{({{\bf{x}}}_{i},{y}_{i})\}}_{i=1}^{N}\)</span> denote the original dataset, where <span>\({{\bf{x}}}_{i}\in {{\mathbb{R}}}^{d}\)</span> is a <i>d</i>-dimensional feature vector and <i>y</i><sub><i>i</i></sub> is the corresponding target value, and let <i>q</i><sub><i>θ</i></sub> represent our trained TabPFN model, either a regression or classification model depending on the target type. We aim to approximate the joint distribution of a new example and its label <span>\(p({\bf{x}},y| {\mathcal{D}})\)</span>. To do this, we factorize the joint distribution as</p><div id="Equ4"><p><span>$$p({\bf{x}},y| {\mathcal{D}})=\mathop{\prod }\limits_{j=1}^{d}p({x}_{j}| {{\bf{x}}}_{ &lt; j},{\mathcal{D}})\cdot p(\,y| {\bf{x}},{\mathcal{D}})$$</span></p><p>
                    (4)
                </p></div><div id="Equ5"><p><span>$$\approx \mathop{\prod }\limits_{j=1}^{d}{q}_{\theta }({x}_{j}| {{\boldsymbol{x}}}_{ &lt; j},{{\mathcal{D}}}_{:, &lt; j})\cdot {q}_{\theta }(\,y| {\boldsymbol{x}},{\mathcal{D}}),$$</span></p><p>
                    (5)
                </p></div><p>where we only condition on a subset of the features in the training set (<span>\({{\mathcal{D}}}_{:, &lt; j}\)</span>). The feature order of the joint density factorization influences the estimated densities. To reduce variance from this source, we apply a permutation sampling approximation of Janossy Pooling at inference time, in which we average the outputs of <i>N</i><sub><i>j</i></sub> feature permutations, with <i>N</i><sub><i>j</i></sub> = 24 in our experiments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Murphy, R. L., Srinivasan, B., Rao, V. A. &amp; Ribeiro, B. Janossy pooling: learning deep permutation-invariant functions for variable-size inputs. In Proc. 7th International Conference on Learning Representations (ICLR, 2019)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR64" id="ref-link-section-d83722773e4262">64</a></sup>.</p><p>As we cannot condition on an empty feature set for technical reasons, we condition the prediction of the first feature <i>x</i><sub>1</sub>, on a feature with random noise, that is, no information.</p><p>The above factorization of the density of a sample (equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Equ5">5</a>)) is completely tractable and we thus use it to estimate the likelihood for data points. This enables tasks such as anomaly detection and outlier identification.</p><h4 id="Sec33">Synthetic data generation</h4><p>We can leverage the generative abilities of TabPFN (see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec32">Density estimation</a>’) to synthesize new tabular data samples that mimic the characteristics of a given real-world dataset, by simply following the factorization in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Equ5">5</a>) and sampling each feature step by step. The generated synthetic samples (<b>x</b><sup>*</sup>, <i>y</i><sup>*</sup>) can be used for various purposes, such as data augmentation, privacy-preserving data sharing and scenario simulation.</p><h4 id="Sec34">Embeddings</h4><p>TabPFN can be used to retrieve meaningful feature representations or embeddings. Given a dataset <span>\({\mathcal{D}}={\{({{\bf{x}}}_{i},{y}_{i})\}}_{i=1}^{N}\)</span>, the goal is to learn a mapping <span>\({f}_{\theta }:{{\mathbb{R}}}^{d}\to {{\mathbb{R}}}^{k}\)</span> that transforms the original <i>d</i>-dimensional feature vectors <b>x</b><sub><i>i</i></sub> into an embedding space of dimension <i>k</i>. The resulting embeddings <span>\({f}_{\theta }({{\bf{x}}}_{i})\in {{\mathbb{R}}}^{k}\)</span> capture the learned relationships between features and can be used for downstream tasks. To use TabPFN for this problem, we simply use the target-column representations of its final layer as embeddings.</p><h3 id="Sec35">Detailed evaluation protocol</h3><p>To rigorously assess the performance and robustness of TabPFN, we conduct a comprehensive quantitative evaluation on standard tabular dataset benchmarks, comparing against state-of-the-art baselines under a standardized protocol.</p><h4 id="Sec36">Default configuration of TabPFN</h4><p>Unlike traditional algorithms, in-context-learned algorithms do not have hyperparameters that directly control their training procedure. Instead, hyperparameters for inference of TabPFN only control the pre-processing of data and post-processing of predictions (for example, feature scaling or softmax temperature). Our default configuration (TabPFN (default)) for both classification and regression is optimized for accurate predictions with minimal fitting time. Here, we apply the same model multiple times with different pre- and post-processors and take the average over the predictions, yielding a four-way (eight-way for regression) ensemble. The settings for our data processing were obtained through a hyperparameter search optimized on our development datasets. The exact settings chosen are listed in Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab5">5</a>. We emphasize that, as for other foundation models (such as GPT), we trained our TabPFN model once and used the same model to perform ICL in a forward pass on all new datasets.</p><h4 id="Sec37">Baselines</h4><p>We compare with tree-based methods, such as random forests<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Breimann, L. Random forests. Mach. Learn. 45, 5–32 (2001)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR38" id="ref-link-section-d83722773e4575">38</a></sup>, XGBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR7" id="ref-link-section-d83722773e4579">7</a></sup>, CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d83722773e4583">9</a></sup> and LightGBM<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 3149–3157 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR8" id="ref-link-section-d83722773e4587">8</a></sup>, the state of the art for experts to perform predictions on tabular data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e4591">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e4594">15</a></sup>. We also compare with simpler methods, such as ridge regression<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Hoerl, A. E. &amp; Kennard, R. W. Ridge regression: biased estimation for nonorthogonal problems. Technometrics 12, 55–67 (1970)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR70" id="ref-link-section-d83722773e4599">70</a></sup>, logistic regression and SVMs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Cortes, C. &amp; Vapnik, V. Support-vector networks. Mach. Learn. 20, 273–297 (1995)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR39" id="ref-link-section-d83722773e4603">39</a></sup>. Although standard neural networks, which unlike TabPFN do not use ICL, were shown to underperform for small (&lt;10,000 samples) tabular data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Borisov, V. et al. Deep neural networks and tabular data: a survey. IEEE Trans. Neural Netw. Learn. Syst. 35, 7499–7519 (2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR1" id="ref-link-section-d83722773e4607">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e4610">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Shwartz-Ziv, R. &amp; Armon, A. Tabular data: deep learning is not all you need. Inf. Fusion 81, 84–90 (2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR71" id="ref-link-section-d83722773e4613">71</a></sup>, as a point of reference, we still consider a simple neural network, the MLP.</p><h4 id="Sec38">Tabular dataset benchmarks</h4><p>We perform our analysis on two widely used and publicly available benchmark suites: the standard AutoML benchmark<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gijsbers, P. et al. AMLB: an AutoML benchmark. J. Mach. Learn. Res. 25, 1–65 (2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR36" id="ref-link-section-d83722773e4625">36</a></sup> and the recent regression benchmark OpenML-CTR23 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Fischer, S. F., Feurer, M. &amp; Bischl, B. OpenML-CTR23 – a curated tabular regression benchmarking suite. In Proc. AutoML Conference 2023 (Workshop) (AutoML, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR37" id="ref-link-section-d83722773e4629">37</a></sup>). Both benchmarks comprise a diverse set of real-world tabular datasets, carefully curated to be representative of various domains and data characteristics. The authors of the benchmark suite selected these datasets based on criteria such as sufficient complexity, real-world relevance, absence of free-form text features and diversity of problem domains.</p><p>For our quantitative analysis of TabPFN for classification tasks, we use a set of test datasets comprising all 29 datasets from the AutoML benchmark with up to 10,000 samples, 500 features and 10 classes. For regression tasks, the AutoML benchmark contains only 16 datasets matching these constraints. To increase statistical power, we augmented this set with all datasets matching our constraints from the recent OpenML-CTR23 benchmark, yielding a test set of 28 unique regression datasets in total. Extended Data Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab3">3</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab4">4</a> provide full details for our test sets of classification and regression datasets, respectively.</p><p>We further evaluated additional benchmark suites from refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e4645">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e4648">15</a></sup>. In ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e4652">14</a></sup>, there are 22 tabular classification datasets selected based on criteria such as heterogeneous columns, moderate dimensionality and sufficient difficulty. In ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e4656">15</a></sup>, there is a collection of 176 classification datasets, representing one of the largest tabular data benchmarks. However, the curation process for these datasets may not be as rigorous or quality controlled as for AutoML Benchmark and OpenML-CTR23. We also evaluated five Kaggle competitions with less than 10,000 training samples from the latest completed Tabular Playground Series.</p><h4 id="Sec39">Development datasets</h4><p>To decide on the hyperparameters of TabPFN, as well as our hyperparameter search spaces, we considered another set of datasets, our development datasets. We carefully selected datasets to be non-overlapping with our test datasets described above. The list of development datasets can be found in Supplementary Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-08328-6#MOESM2">5</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-08328-6#MOESM2">6</a>. We considered the mean of normalized scores (ROC/RMSE) and rank quantiles and chose the best model configurations on these development datasets.</p><h4 id="Sec40">Metrics and cross-validation</h4><p>To obtain scores for classification tasks, we use two widely adopted evaluation metrics: ROC AUC (One-vs-Rest) and accuracy. ROC AUC averages performance over different sensitivity–specificity trade-offs, and&nbsp;accuracy measures the fraction of samples labelled correctly.</p><p>For regression tasks, we use <i>R</i><sup>2</sup> and negative RMSE as evaluation metrics. <i>R</i><sup>2</sup> represents the proportion of variance in the target column that the model can predict. RMSE is the root of the average squared magnitude of the errors between the predicted and actual values. As we use negative RMSE, for all our four metrics higher values indicate a better fit.</p><p>To increase statistical validity, for each dataset and method in our test datasets, we evaluated 10 repetitions, each with a different random seed and train–test split (90% train and 10% test samples; all methods used the same cross-validation splits, defined by OpenML<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Vanschoren, J., van Rijn, J. N., Bischl, B. &amp; Torgo, L. OpenML: networked science in machine learning. SIGKDD Explor. 15, 49–60 (2014)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR72" id="ref-link-section-d83722773e4697">72</a></sup>). We average the scores of all repetitions per dataset. Then, to average scores across datasets, we normalize per dataset following previous benchmarks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gijsbers, P. et al. AMLB: an AutoML benchmark. J. Mach. Learn. Res. 25, 1–65 (2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR36" id="ref-link-section-d83722773e4701">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d83722773e4704">40</a></sup>. The absolute scores are linearly scaled such that a score of 1.0 corresponds to the highest value achieved by any method on that dataset, whereas a score of 0 represents the lowest result. This normalization allows for building meaningful averages across datasets with very different score ranges. We provide absolute performance numbers in Supplementary Data Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-08328-6#MOESM1">1</a>–<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-08328-6#MOESM1">2</a>. All confidence intervals shown are 95% confidence intervals.</p><p>We tuned all methods with a random search using five-fold cross-validation with ROC AUC/RMSE up to a given time budget, ranging from half a minute to 4 h. The first candidate in the random search was the default setting supplied in the implementation of the method and was also used if not a single cross-validation run finished before the time budget was consumed. See the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec5">Qualitative analysis</a>’ for the used search spaces per method. All methods were evaluated using 8 CPU cores. Moreover, TabPFN makes use of a 5-year-old consumer-grade GPU (RTX 2080 Ti). We also tested GPU acceleration for the baselines. However, as Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig8">2</a> shows, this did not improve performance, probably because of the small dataset sizes.</p></div></div>
                    
                </div><div>
                <div id="data-availability-section" data-title="Data availability"><h2 id="data-availability">Data availability</h2><p>All datasets evaluated are publicly available on <a href="https://openml.org/">openml.org</a> or <a href="https://kaggle.com/">kaggle.com</a>. We have provided scripts in our code repository that automate the process of downloading and evaluating the datasets. These scripts contain dataset identifiers, as well as exact data splitting and processing procedures.</p></div><div id="code-availability-section" data-title="Code availability"><h2 id="code-availability">Code availability</h2><p>Our code is available at <a href="https://priorlabs.ai/tabpfn-nature/">https://priorlabs.ai/tabpfn-nature/</a> (<a href="https://doi.org/10.5281/zenodo.13981285">https://doi.org/10.5281/zenodo.13981285</a>). We also provide an API that allows users to run TabPFN with minimal coding experience or without the availability of specific computing hardware such as a GPU. The code is designed to be modular and easily installable in a standard Python environment. The code to generate synthetic pre-training data has not been released with our models. We aim to enable researchers and practitioners to easily integrate TabPFN into their workflows and apply it to their specific tabular data tasks. We encourage users to provide feedback, report issues, and contribute to the further development of TabPFN. This open release aims to facilitate collaboration and accelerate the adoption and advancement of TabPFN in various research and application domains.</p></div><div id="MagazineFulltextArticleBodySuffix" aria-labelledby="Bib1" data-title="References"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Borisov, V. et al. Deep neural networks and tabular data: a survey. <i>IEEE Trans. Neural Netw. Learn. Syst.</i> <b>35</b>, 7499–7519 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TNNLS.2022.3229161" data-track-item_id="10.1109/TNNLS.2022.3229161" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTNNLS.2022.3229161" aria-label="Article reference 1" data-doi="10.1109/TNNLS.2022.3229161">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37015381" aria-label="PubMed reference 1">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1543.93250" aria-label="MATH reference 1">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20neural%20networks%20and%20tabular%20data%3A%20a%20survey&amp;journal=IEEE%20Trans.%20Neural%20Netw.%20Learn.%20Syst.&amp;doi=10.1109%2FTNNLS.2022.3229161&amp;volume=35&amp;pages=7499-7519&amp;publication_year=2024&amp;author=Borisov%2CV">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">van Breugel, B. &amp; van der Schaar, M. Position: why tabular foundation models should be a research priority. In <i>Proc. 41st International Conference on Machine Learning</i> 48976–48993 (PMLR, 2024).</p></li><li data-counter="3."><p id="ref-CR3">Silver, D. et al. Mastering the game of go with deep neural networks and tree search. <i>Nature</i> <b>529</b>, 484–489 (2016).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nature16961" data-track-item_id="10.1038/nature16961" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature16961" aria-label="Article reference 3" data-doi="10.1038/nature16961">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016Natur.529..484S" aria-label="ADS reference 3">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28Xhs12is7w%3D" aria-label="CAS reference 3">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26819042" aria-label="PubMed reference 3">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0153.49402" aria-label="MATH reference 3">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search&amp;journal=Nature&amp;doi=10.1038%2Fnature16961&amp;volume=529&amp;pages=484-489&amp;publication_year=2016&amp;author=Silver%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="4."><p id="ref-CR4">Jumper, J. M. et al. Highly accurate protein structure prediction with AlphaFold. <i>Nature</i> <b>596</b>, 583 – 589 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-021-03819-2" data-track-item_id="10.1038/s41586-021-03819-2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-021-03819-2" aria-label="Article reference 4" data-doi="10.1038/s41586-021-03819-2">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34265844" aria-label="PubMed reference 4">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8371605" aria-label="PubMed Central reference 4">PubMed Central</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0849.90143" aria-label="MATH reference 4">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Highly%20accurate%20protein%20structure%20prediction%20with%20AlphaFold&amp;journal=Nature&amp;doi=10.1038%2Fs41586-021-03819-2&amp;volume=596&amp;publication_year=2021&amp;author=Jumper%2CJM">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="5."><p id="ref-CR5">OpenAI. GPT-4 Technical Report. Preprint at <a href="https://arxiv.org/abs/2303.08774" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a> (2023).</p></li><li data-counter="6."><p id="ref-CR6">Friedman, J. H. Greedy function approximation: a gradient boosting machine. <i>Ann. Stat</i>. 1189–1232 (2001).</p></li><li data-counter="7."><p id="ref-CR7">Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In <i>Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016).</p></li><li data-counter="8."><p id="ref-CR8">Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In <i>Proc. 30th International Conference on Advances in Neural Information Processing Systems</i> (eds Guyon, I. et al.) 3149–3157 (Curran Associates, 2017).</p></li><li data-counter="9."><p id="ref-CR9">Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In <i>Proc. 30th International Conference on Advances in Neural Information Processing Systems</i> (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018).</p></li><li data-counter="10."><p id="ref-CR10">Lowe, D. G. Distinctive image features from scale-invariant keypoints. <i>Int. J. Comput. Vis.</i> <b>60</b>, 91–110 (2004).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1023/B:VISI.0000029664.99615.94" data-track-item_id="10.1023/B:VISI.0000029664.99615.94" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" aria-label="Article reference 10" data-doi="10.1023/B:VISI.0000029664.99615.94">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1066.68568" aria-label="MATH reference 10">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Distinctive%20image%20features%20from%20scale-invariant%20keypoints&amp;journal=Int.%20J.%20Comput.%20Vis.&amp;doi=10.1023%2FB%3AVISI.0000029664.99615.94&amp;volume=60&amp;pages=91-110&amp;publication_year=2004&amp;author=Lowe%2CDG">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="11."><p id="ref-CR11">Dalal, N. &amp; Triggs, B. Histograms of oriented gradients for human detection. In <i>Proc. 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR</i>’<i>05)</i> 886–893 (IEEE, 2005).</p></li><li data-counter="12."><p id="ref-CR12">Vaswani, A. et al. Attention is all you need. In <i>Proc. 30th International Conference on Advances in Neural Information Processing Systems</i> (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017).</p></li><li data-counter="13."><p id="ref-CR13">Silver, D. et al. Mastering the game of go without human knowledge. <i>Nature</i> <b>550</b>, 354–359 (2017).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nature24270" data-track-item_id="10.1038/nature24270" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature24270" aria-label="Article reference 13" data-doi="10.1038/nature24270">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2017Natur.550..354S" aria-label="ADS reference 13">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXhs12ltLvM" aria-label="CAS reference 13">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29052630" aria-label="PubMed reference 13">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1443.97024" aria-label="MATH reference 13">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Mastering%20the%20game%20of%20go%20without%20human%20knowledge&amp;journal=Nature&amp;doi=10.1038%2Fnature24270&amp;volume=550&amp;pages=354-359&amp;publication_year=2017&amp;author=Silver%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="14."><p id="ref-CR14">Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In <i>Proc. 36th International Conference on Neural Information Processing Systems</i> Vol. 35, 507–520 (ACM, 2022).</p></li><li data-counter="15."><p id="ref-CR15">McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In <i>Proc. 37th International Conference on Neural Information Processing System</i> Vol. 36, 76336–76369 (ACM, 2024).</p></li><li data-counter="16."><p id="ref-CR16">Goodfellow, I., Bengio, Y. &amp; Courville, A. <i>Deep Learning</i> (MIT Press, 2016).</p></li><li data-counter="17."><p id="ref-CR17">Brown, T. et al. Language models are few-shot learners. In <i>Proc. Advances in Neural Information Processing Systems</i> (eds Larochelle, H. et al.) Vol. 33, 1877–1901 (Curran Associates, 2020).</p></li><li data-counter="18."><p id="ref-CR18">Garg, S., Tsipras, D., Liang, P. S. &amp; Valiant, G. What can transformers learn in-context? A case study of simple function classes. In <i>Proc. Advances in Neural Information Processing Systems</i> Vol. 35, 30583–30598 (ACM, 2022).</p></li><li data-counter="19."><p id="ref-CR19">Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. &amp; Zhou, D. What learning algorithm is in-context learning? Investigations with linear models. In <i>Proc. The Eleventh International Conference on Learning Representations</i> (ICLR, 2023).</p></li><li data-counter="20."><p id="ref-CR20">Von Oswald, J. et al. Transformers learn in-context by gradient descent. In <i>Proc. 40th International Conference on Machine Learning</i> 35151–35174 (PMLR, 2023).</p></li><li data-counter="21."><p id="ref-CR21">Zhou, H. et al. What algorithms can transformers learn? A study in length generalization. In <i>Proc. The Twelfth International Conference on Learning Representations</i> (ICLR, 2024).</p></li><li data-counter="22."><p id="ref-CR22">Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In <i>Proc.</i> <i>The Tenth International Conference on Learning Representations</i> (ICLR, 2022).</p></li><li data-counter="23."><p id="ref-CR23">Hollmann, N., Müller, S., Eggensperger, K. &amp; Hutter, F. TabPFN: a transformer that solves small tabular classification problems in a second. In <i>Proc. The Eleventh International Conference on Learning Representations</i> (ICLR, 2023).</p></li><li data-counter="24."><p id="ref-CR24">Kingma, D. &amp; Ba, J. Adam: A method for stochastic optimization. In <i>Proc. International Conference on Learning Representations</i> (ICLR, 2015).</p></li><li data-counter="25."><p id="ref-CR25">Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. In <i>Proc. 3rd International Conference on Learning Representations</i> (eds Bengio, Y. &amp; LeCun, Y.) (ICLR, 2015).</p></li><li data-counter="26."><p id="ref-CR26">Gorishniy, Y., Rubachev, I., Khrulkov, V. &amp; Babenko, A. Revisiting deep learning models for tabular data. In <i>Proc. Advances in Neural Information Processing Systems 34</i> (eds Ranzato, M. et al.) 18932–18943 (NeurIPS, 2021).</p></li><li data-counter="27."><p id="ref-CR27">Zhu, B. et al. XTab: cross-table pretraining for tabular transformers. In <i>Proc. 40th International Conference on Machine Learning</i> (eds Krause, A. et al.) 43181–43204 (PMLR, 2023).</p></li><li data-counter="28."><p id="ref-CR28">Lorch, L., Sussex, S., Rothfuss, J., Krause, A. &amp; Schölkopf, B. Amortized inference for causal structure learning. In <i>Proc. Advances in Neural Information Processing Systems</i> (eds Koyejo, S. et al.) Vol. 35, 13104–13118 (ACM, 2022).</p></li><li data-counter="29."><p id="ref-CR29">Dao, T., Fu, D., Ermon, S., Rudra, A. &amp; Ré, C. Flashattention: fast and memory-efficient exact attention with io-awareness. In <i>Proc. Advances in Neural Information Processing Systems</i> (eds Koyejo, S. et al.) Vol. 35, 16344–16359 (2022).</p></li><li data-counter="30."><p id="ref-CR30">Torgo, L. &amp; Gama, J. Regression using classification algorithms. <i>Intell. Data Anal.</i> <b>1</b>, 275–292 (1997).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3233/IDA-1997-1405" data-track-item_id="10.3233/IDA-1997-1405" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3233%2FIDA-1997-1405" aria-label="Article reference 30" data-doi="10.3233/IDA-1997-1405">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1032.62065" aria-label="MATH reference 30">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Regression%20using%20classification%20algorithms&amp;journal=Intell.%20Data%20Anal.&amp;doi=10.3233%2FIDA-1997-1405&amp;volume=1&amp;pages=275-292&amp;publication_year=1997&amp;author=Torgo%2CL&amp;author=Gama%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="31."><p id="ref-CR31">Pearl, J. <i>Causality</i> 2nd edn (Cambridge Univ. Press, 2009).</p></li><li data-counter="32."><p id="ref-CR32">Jiang, M. et al. Investigating Data Contamination for Pre-training Language Models. Preprint at <a href="https://arxiv.org/abs/2401.06059" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2401.06059">https://arxiv.org/abs/2401.06059</a> (2024).</p></li><li data-counter="33."><p id="ref-CR33">Kumaraswamy, P. A generalized probability density function for double-bounded random processes. <i>J. Hydrol.</i> <b>46</b>, 79–88 (1980).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/0022-1694(80)90036-0" data-track-item_id="10.1016/0022-1694(80)90036-0" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2F0022-1694%2880%2990036-0" aria-label="Article reference 33" data-doi="10.1016/0022-1694(80)90036-0">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=1980JHyd...46...79K" aria-label="ADS reference 33">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0253.73040" aria-label="MATH reference 33">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20generalized%20probability%20density%20function%20for%20double-bounded%20random%20processes&amp;journal=J.%20Hydrol.&amp;doi=10.1016%2F0022-1694%2880%2990036-0&amp;volume=46&amp;pages=79-88&amp;publication_year=1980&amp;author=Kumaraswamy%2CP">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="34."><p id="ref-CR34">Rosenblatt, F. <i>Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms</i>. Report No. 1196-0-8 (Cornell Aeronautical Lab, 1961).</p></li><li data-counter="35."><p id="ref-CR35">Young, T. I. The bakerian lecture. experiments and calculations relative to physical optics. <i>Philos. Trans. R. Soc. Lond.</i> <b>94</b>, 1–16 (1804).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=1804RSPT...94....1Y" aria-label="ADS reference 35">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1032.68554" aria-label="MATH reference 35">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=I.%20The%20bakerian%20lecture.%20experiments%20and%20calculations%20relative%20to%20physical%20optics.&amp;journal=Philos.%20Trans.%20R.%20Soc.%20Lond.&amp;volume=94&amp;pages=1-16&amp;publication_year=1804&amp;author=Young%2CT">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="36."><p id="ref-CR36">Gijsbers, P. et al. AMLB: an AutoML benchmark. <i>J. Mach. Learn. Res.</i> <b>25</b>, 1–65 (2024).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=AMLB%3A%20an%20AutoML%20benchmark&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=25&amp;pages=1-65&amp;publication_year=2024&amp;author=Gijsbers%2CP">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="37."><p id="ref-CR37">Fischer, S. F., Feurer, M. &amp; Bischl, B. OpenML-CTR23 – a curated tabular regression benchmarking suite. In <i>Proc. AutoML Conference 2023 (Workshop)</i> (AutoML, 2023).</p></li><li data-counter="38."><p id="ref-CR38">Breimann, L. Random forests. <i>Mach. Learn.</i> <b>45</b>, 5–32 (2001).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1023/A:1010933404324" data-track-item_id="10.1023/A:1010933404324" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1023%2FA%3A1010933404324" aria-label="Article reference 38" data-doi="10.1023/A:1010933404324">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1007.68152" aria-label="MATH reference 38">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20forests&amp;journal=Mach.%20Learn.&amp;doi=10.1023%2FA%3A1010933404324&amp;volume=45&amp;pages=5-32&amp;publication_year=2001&amp;author=Breimann%2CL">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="39."><p id="ref-CR39">Cortes, C. &amp; Vapnik, V. Support-vector networks. <i>Mach. Learn.</i> <b>20</b>, 273–297 (1995).</p><p><a data-track="click_references" rel="noopener" data-track-label="10.1007/BF00994018" data-track-item_id="10.1007/BF00994018" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/BF00994018" aria-label="Article reference 39" data-doi="10.1007/BF00994018">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0831.68098" aria-label="MATH reference 39">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Support-vector%20networks&amp;journal=Mach.%20Learn.&amp;doi=10.1007%2FBF00994018&amp;volume=20&amp;pages=273-297&amp;publication_year=1995&amp;author=Cortes%2CC&amp;author=Vapnik%2CV">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="40."><p id="ref-CR40">Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at <a href="https://arxiv.org/abs/2003.06505" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2003.06505">https://arxiv.org/abs/2003.06505</a> (2020).</p></li><li data-counter="41."><p id="ref-CR41">Wolpert, D. Stacked generalization. <i>Neural Netw.</i> <b>5</b>, 241–259 (1992).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/S0893-6080(05)80023-1" data-track-item_id="10.1016/S0893-6080(05)80023-1" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2FS0893-6080%2805%2980023-1" aria-label="Article reference 41" data-doi="10.1016/S0893-6080(05)80023-1">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0792.68144" aria-label="MATH reference 41">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Stacked%20generalization&amp;journal=Neural%20Netw.&amp;doi=10.1016%2FS0893-6080%2805%2980023-1&amp;volume=5&amp;pages=241-259&amp;publication_year=1992&amp;author=Wolpert%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="42."><p id="ref-CR42">Caruana, R., Niculescu-Mizil, A., Crew, G. &amp; Ksikes, A. Ensemble selection from libraries of models. In <i>Proc. 21st International Conference on Machine Learning</i> (ed. Greiner, R.) (Omnipress, 2004).</p></li><li data-counter="43."><p id="ref-CR43">Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In <i>Proc. International Conference on Automated Machine Learning</i> Vol. 224 (PMLR, 2023).</p></li><li data-counter="44."><p id="ref-CR44">Hofmann, H. Statlog (German Credit Data). UCI Machine Learning Repository <a href="https://doi.org/10.24432/C5NC77" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.24432/C5NC77">https://doi.org/10.24432/C5NC77</a> (1994).</p></li><li data-counter="45."><p id="ref-CR45">Duin, R. Multiple Features. UCI Machine Learning Repository <a href="https://doi.org/10.24432/C5HC70" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.24432/C5HC70">https://doi.org/10.24432/C5HC70</a> (1998).</p></li><li data-counter="46."><p id="ref-CR46">Rajotte, J.-F. et al. Synthetic data as an enabler for machine learning applications in medicine. <i>iScience</i> <b>25</b>, 105331 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.isci.2022.105331" data-track-item_id="10.1016/j.isci.2022.105331" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.isci.2022.105331" aria-label="Article reference 46" data-doi="10.1016/j.isci.2022.105331">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2022iSci...25j5331R" aria-label="ADS reference 46">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB38XivVSitb%2FM" aria-label="CAS reference 46">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36325058" aria-label="PubMed reference 46">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9619172" aria-label="PubMed Central reference 46">PubMed Central</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1471.91288" aria-label="MATH reference 46">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Synthetic%20data%20as%20an%20enabler%20for%20machine%20learning%20applications%20in%20medicine&amp;journal=iScience&amp;doi=10.1016%2Fj.isci.2022.105331&amp;volume=25&amp;publication_year=2022&amp;author=Rajotte%2CJ-F">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="47."><p id="ref-CR47">Lundberg, S. M. &amp; Lee, S.-I. A unified approach to interpreting model predictions. In <i>Proc.</i> <i>Advances in Neural Information Processing Systems</i> (eds Guyon, I. et al.) Vol. 30, 4765–4774 (Curran Associates, 2017).</p></li><li data-counter="48."><p id="ref-CR48">Feuer, B. et al. TuneTables: context optimization for scalable prior-data fitted networks. In <i>Proc.</i> <i>38th Conference on Neural Information Processing Systems</i> (NeurIPS, 2024).</p></li><li data-counter="49."><p id="ref-CR49">Helli, K., Schnurr, D., Hollmann, N., Müller, S. &amp; Hutter, F. Drift-resilient tabPFN: In-context learning temporal distribution shifts on tabular data. In <i>Proc. 38th Conference on Neural Information Processing Systems</i> (NeurIPS, 2024).</p></li><li data-counter="50."><p id="ref-CR50">Thomas, V. et al. Retrieval &amp; fine-tuning for in-context tabular models. In <i>Proc. 1st Workshop on In-Context Learning at the 41st International Conference on Machine Learning</i> (ICML, 2024).</p></li><li data-counter="51."><p id="ref-CR51">Nagler, T. Statistical foundations of prior-data fitted networks. In <i>Proc. 40th International Conference on Machine Learning</i> (eds Krause, A. et al.) Vol. 202, 25660–25676 (PMLR, 2023).</p></li><li data-counter="52."><p id="ref-CR52">Dooley, S., Khurana, G. S., Mohapatra, C., Naidu, S. V. &amp; White, C. ForecastPFN: synthetically-trained zero-shot forecasting. In <i>Proc. 37th Conference on Advances in Neural Information Processing Systems</i> (eds Oh, A. et al.) (NeurIPS, 2023).</p></li><li data-counter="53."><p id="ref-CR53">Czolbe, S. &amp; Dalca, A. V. Neuralizer: General neuroimage analysis without re-training. In <i>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> 6217–6230 (IEEE, 2023).</p></li><li data-counter="54."><p id="ref-CR54">Wilcoxon, F. in <i>Breakthroughs in Statistics: Methodology and Distribution</i> (eds Kotz, S. &amp; Johnson, N. L.) 196–202 (Springer, 1992).</p></li><li data-counter="55."><p id="ref-CR55">Müller, A., Curino, C. &amp; Ramakrishnan, R. Mothernet: a foundational hypernetwork for tabular classification. Preprint at <a href="https://arxiv.org/abs/2312.08598" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2312.08598">https://arxiv.org/abs/2312.08598</a> (2023).</p></li><li data-counter="56."><p id="ref-CR56">Shazeer, N. Fast transformer decoding: one write-head is all you need. Preprint at <a href="https://arxiv.org/abs/1911.02150" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/1911.02150">https://arxiv.org/abs/1911.02150</a> (2019).</p></li><li data-counter="57."><p id="ref-CR57">Krapivsky, P. L. &amp; Redner, S. Organization of growing random networks. <i>Phys. Rev. E</i> <b>63</b>, 066123 (2001).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1103/PhysRevE.63.066123" data-track-item_id="10.1103/PhysRevE.63.066123" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1103%2FPhysRevE.63.066123" aria-label="Article reference 57" data-doi="10.1103/PhysRevE.63.066123">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2001PhRvE..63f6123K" aria-label="ADS reference 57">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD38%2FhsF2isA%3D%3D" aria-label="CAS reference 57">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1109.92301" aria-label="MATH reference 57">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Organization%20of%20growing%20random%20networks&amp;journal=Phys.%20Rev.%20E&amp;doi=10.1103%2FPhysRevE.63.066123&amp;volume=63&amp;publication_year=2001&amp;author=Krapivsky%2CPL&amp;author=Redner%2CS">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="58."><p id="ref-CR58">Glorot, X. &amp; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In <i>Proc. 13th International Conference on Artificial Intelligence and Statistics</i> 249–256 (JMLR, 2010).</p></li><li data-counter="59."><p id="ref-CR59">Nair, V. &amp; Hinton, G. Rectified linear units improve restricted Boltzmann machines. In <i>Proc. 27th International Conference on Machine Learning</i> (eds Fürnkranz, J. &amp; Joachims, T.) 807–814 (Omnipress, 2010).</p></li><li data-counter="60."><p id="ref-CR60">Quinlan, J. R. Induction of decision trees. <i>Mach. Learn.</i> <b>1</b>, 81–106 (1986).</p><p><a data-track="click_references" rel="noopener" data-track-label="10.1007/BF00116251" data-track-item_id="10.1007/BF00116251" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/BF00116251" aria-label="Article reference 60" data-doi="10.1007/BF00116251">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0887.73077" aria-label="MATH reference 60">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Induction%20of%20decision%20trees&amp;journal=Mach.%20Learn.&amp;doi=10.1007%2FBF00116251&amp;volume=1&amp;pages=81-106&amp;publication_year=1986&amp;author=Quinlan%2CJR">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="61."><p id="ref-CR61">Müller, S., Feurer, M., Hollmann, N. &amp; Hutter, F. PFNS4BO: in-context learning for Bayesian optimization. In <i>Proc. 40th International Conference on Machine Learning</i> 25444–25470 (PMLR, 2023).</p></li><li data-counter="62."><p id="ref-CR62">Dietterich, T. G. &amp; Bakiri, G. Solving multiclass learning problems via error-correcting output codes. <i>J. Artif. Intell. Res.</i> <b>2</b>, 263–286 (1994).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1613/jair.105" data-track-item_id="10.1613/jair.105" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1613%2Fjair.105" aria-label="Article reference 62" data-doi="10.1613/jair.105">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0900.68358" aria-label="MATH reference 62">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Solving%20multiclass%20learning%20problems%20via%20error-correcting%20output%20codes&amp;journal=J.%20Artif.%20Intell.%20Res.&amp;doi=10.1613%2Fjair.105&amp;volume=2&amp;pages=263-286&amp;publication_year=1994&amp;author=Dietterich%2CTG&amp;author=Bakiri%2CG">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="63."><p id="ref-CR63">Loshchilov, I. &amp; Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In <i>Proc. 5th International Conference on Learning Representations</i> (ICLR, 2017).</p></li><li data-counter="64."><p id="ref-CR64">Murphy, R. L., Srinivasan, B., Rao, V. A. &amp; Ribeiro, B. Janossy pooling: learning deep permutation-invariant functions for variable-size inputs. In <i>Proc. 7th International Conference on Learning Representations</i> (ICLR, 2019).</p></li><li data-counter="65."><p id="ref-CR65">McCarter, C. The kernel density integral transformation. <i>Transact. Mach. Learn. Res.</i> <a href="https://openreview.net/pdf?id=6OEcDKZj5j" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/pdf?id=6OEcDKZj5j">https://openreview.net/pdf?id=6OEcDKZj5j</a> (2023).</p></li><li data-counter="66."><p id="ref-CR66">Caruana, R., Munson, A. &amp; Niculescu-Mizil, A. Getting the most out of ensemble selection. In <i>Proc. 6th IEEE International Conference on Data Mining</i> (eds Clifton, C. et al.) 828–833 (IEEE, 2006).</p></li><li data-counter="67."><p id="ref-CR67">Feurer, M. et al. in <i>Automated Machine Learning: Methods, Systems, Challenges</i> (eds Hutter, F. et al.) Ch. 6 (Springer, 2019).</p></li><li data-counter="68."><p id="ref-CR68">Purucker, L. &amp; Beel, J. Assembled-OpenML: creating efficient benchmarks for ensembles in AutoML with OpenML. In <i>Proc. First International Conference on Automated Machine Learning</i> (AutoML, 2022).</p></li><li data-counter="69."><p id="ref-CR69">Purucker, L. &amp; Beel, J. CMA-ES for post hoc ensembling in AutoML: a great success and salvageable failure. In <i>Proc. International Conference on Automated Machine Learning</i> Vol. 224, 1–23 (PMLR, 2023).</p></li><li data-counter="70."><p id="ref-CR70">Hoerl, A. E. &amp; Kennard, R. W. Ridge regression: biased estimation for nonorthogonal problems. <i>Technometrics</i> <b>12</b>, 55–67 (1970).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1080/00401706.1970.10488634" data-track-item_id="10.1080/00401706.1970.10488634" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F00401706.1970.10488634" aria-label="Article reference 70" data-doi="10.1080/00401706.1970.10488634">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0202.17205" aria-label="MATH reference 70">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=Ridge%20regression%3A%20biased%20estimation%20for%20nonorthogonal%20problems&amp;journal=Technometrics&amp;doi=10.1080%2F00401706.1970.10488634&amp;volume=12&amp;pages=55-67&amp;publication_year=1970&amp;author=Hoerl%2CAE&amp;author=Kennard%2CRW">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="71."><p id="ref-CR71">Shwartz-Ziv, R. &amp; Armon, A. Tabular data: deep learning is not all you need. <i>Inf. Fusion</i> <b>81</b>, 84–90 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.inffus.2021.11.011" data-track-item_id="10.1016/j.inffus.2021.11.011" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.inffus.2021.11.011" aria-label="Article reference 71" data-doi="10.1016/j.inffus.2021.11.011">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 71" href="http://scholar.google.com/scholar_lookup?&amp;title=Tabular%20data%3A%20deep%20learning%20is%20not%20all%20you%20need&amp;journal=Inf.%20Fusion&amp;doi=10.1016%2Fj.inffus.2021.11.011&amp;volume=81&amp;pages=84-90&amp;publication_year=2022&amp;author=Shwartz-Ziv%2CR&amp;author=Armon%2CA">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="72."><p id="ref-CR72">Vanschoren, J., van Rijn, J. N., Bischl, B. &amp; Torgo, L. OpenML: networked science in machine learning. <i>SIGKDD Explor.</i> <b>15</b>, 49–60 (2014).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1145/2641190.2641198" data-track-item_id="10.1145/2641190.2641198" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1145%2F2641190.2641198" aria-label="Article reference 72" data-doi="10.1145/2641190.2641198">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1505.62090" aria-label="MATH reference 72">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 72" href="http://scholar.google.com/scholar_lookup?&amp;title=OpenML%3A%20networked%20science%20in%20machine%20learning&amp;journal=SIGKDD%20Explor.&amp;doi=10.1145%2F2641190.2641198&amp;volume=15&amp;pages=49-60&amp;publication_year=2014&amp;author=Vanschoren%2CJ&amp;author=Rijn%2CJN&amp;author=Bischl%2CB&amp;author=Torgo%2CL">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="73."><p id="ref-CR73">Fix, E. &amp; Hodges, J. L. Discriminatory analysis. Nonparametric discrimination: consistency properties. <i>Int. Stat. Rev.</i> <b>57</b>, 238–247 (1989).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.2307/1403797" data-track-item_id="10.2307/1403797" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.2307%2F1403797" aria-label="Article reference 73" data-doi="10.2307/1403797">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0715.62080" aria-label="MATH reference 73">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 73" href="http://scholar.google.com/scholar_lookup?&amp;title=Discriminatory%20analysis.%20Nonparametric%20discrimination%3A%20consistency%20properties&amp;journal=Int.%20Stat.%20Rev.&amp;doi=10.2307%2F1403797&amp;volume=57&amp;pages=238-247&amp;publication_year=1989&amp;author=Fix%2CE&amp;author=Hodges%2CJL">
                    Google Scholar</a>&nbsp;
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-024-08328-6?format=refman&amp;flavour=references">Download references</a></p></div></div><div id="Ack1-section" data-title="Acknowledgements"><h2 id="Ack1">Acknowledgements</h2><p>We express our gratitude to the following individuals for their valuable contributions and support. We thank E. Bergman for his assistance with the evaluation of TabPFN, for helping implement the random forest pre-processing, and for his efforts in improving the code quality and documentation. His contributions were instrumental in benchmarking TabPFN and ensuring the reproducibility of our results. We thank A. Gupta and D. Otte for their work on the Inference Server, which enables the fast deployment of TabPFN without the need for a local GPU. Their efforts have greatly enhanced the accessibility and usability of TabPFN. We thank L. Schweizer for his work on exploring the random forest pre-processing for TabPFN further. We thank D. Schnurr and K. Helli for their work on visualization, and D. Schnurr for his specific contributions related to handling missing values. We thank S. M. Lundberg for the collection of visualization methods for feature attribution that we adapted for our work. We thank A. Müller for the insightful discussions related to TabPFN training and for his guidance on identifying and mitigating biases in the prior. His expertise has been invaluable in refining the TabPFN methodology. We are very grateful to C. Langenberg and M. Pietzner for providing insights on medical applications, interpreting model results and offering general advice. Their continued support has been instrumental in shaping this work. We thank S. Stäglich for his outstanding maintenance and support with the cluster infrastructure. We thank B. Lake for his general paper writing advice. We are grateful for the computational resources that were available for this research. Specifically, we acknowledge support by the state of Baden-Württemberg through bwHPC and the German Research Foundation (DFG) through grant no INST 39/963-1 FUGG (bwForCluster NEMO), and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant no. 417962828. We acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under SFB 1597 (SmallData), grant no. 499552394, and by the European Union (through ERC Consolidator Grant DeepLearning 2.0, grant no. 101045765). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. F.H. acknowledges the financial support of the Hector Foundation.</p></div><div id="author-information-section" aria-labelledby="author-information" data-title="Author information"><h2 id="author-information">Author information</h2><div id="author-information-content"><p><span id="author-notes">Author notes</span></p><ol><li id="na1"><p>These authors contributed equally: Noah Hollmann, Samuel Müller</p></li></ol><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Machine Learning Lab, University of Freiburg, Freiburg, Germany</p><p>Noah Hollmann,&nbsp;Samuel Müller,&nbsp;Lennart Purucker,&nbsp;Arjun Krishnakumar,&nbsp;Max Körfer,&nbsp;Shi Bin Hoo&nbsp;&amp;&nbsp;Frank Hutter</p></li><li id="Aff2"><p>Computational Medicine, Berlin Institute of Health at Charité, Universitätsmedizin Berlin, Berlin, Germany</p><p>Noah Hollmann</p></li><li id="Aff3"><p>Prior Labs, Freiburg, Germany</p><p>Noah Hollmann&nbsp;&amp;&nbsp;Frank Hutter</p></li><li id="Aff4"><p>Neuromedical AI Lab, Department of Neurosurgery, Medical Center - University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany</p><p>Robin Tibor Schirrmeister</p></li><li id="Aff5"><p>Medical Physics, Department of Diagnostic and Interventional Radiology, Medical Center - University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany</p><p>Robin Tibor Schirrmeister</p></li><li id="Aff6"><p>ELLIS Institute Tübingen, Tübingen, Germany</p><p>Frank Hutter</p></li></ol><div data-test="author-info"><p><span>Authors</span></p><ol><li id="auth-Noah-Hollmann-Aff1-Aff2-Aff3"><span>Noah Hollmann</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Noah%20Hollmann" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Noah%20Hollmann%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Samuel-M_ller-Aff1"><span>Samuel Müller</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Samuel%20M%C3%BCller" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Samuel%20M%C3%BCller%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Lennart-Purucker-Aff1"><span>Lennart Purucker</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Lennart%20Purucker" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Lennart%20Purucker%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Arjun-Krishnakumar-Aff1"><span>Arjun Krishnakumar</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Arjun%20Krishnakumar" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Arjun%20Krishnakumar%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Max-K_rfer-Aff1"><span>Max Körfer</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Max%20K%C3%B6rfer" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Max%20K%C3%B6rfer%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Shi_Bin-Hoo-Aff1"><span>Shi Bin Hoo</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shi%20Bin%20Hoo" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shi%20Bin%20Hoo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Robin_Tibor-Schirrmeister-Aff4-Aff5"><span>Robin Tibor Schirrmeister</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Robin%20Tibor%20Schirrmeister" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Robin%20Tibor%20Schirrmeister%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Frank-Hutter-Aff1-Aff3-Aff6"><span>Frank Hutter</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Frank%20Hutter" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Frank%20Hutter%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li></ol></div><h3 id="contributions">Contributions</h3><p>N.H. improved the prior of the model; added regression support, unsupervised capabilities and inference optimizations; and contributed to the experiments and wrote the paper. S.M. improved the neural network architecture, training and efficiency; added inference optimizations; and contributed to experiments and wrote the paper. L.P. improved the inference interface of the model; contributed to hyperparameter tuning; added post hoc ensembling of TabPFN models; contributed to benchmarking; and wrote the paper. A.K. added inference optimizations and Kaggle experiments. M.K. contributed to inference optimizations. S.B.H. contributed to the usability of our code. R.T.S. contributed to preliminary architectural experiments to speed up inference and helped revise the first draft of the paper. F.H. contributed technical advice and ideas, contributed to the random forest pre-processing, managed collaborations and funding, and wrote the paper.</p><h3 id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:noah@priorlabs.ai">Noah Hollmann</a>, <a id="corresp-c2" href="mailto:samuelgabrielmuller@gmail.com">Samuel Müller</a> or <a id="corresp-c3" href="mailto:fh@cs.uni-freiburg.de">Frank Hutter</a>.</p></div></div><div id="ethics-section" data-title="Ethics declarations"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar2">Competing interests</h3>
                <p>The following patent applications invented by S.M. and F.H. and filed by R. Bosch are related to this work: DE202021105192U1 and DE102021210775A1. The authors do not have any ownership rights to these patent applications. F.H. and N.H. are affiliated with PriorLabs, a company focused on developing tabular foundation models. The authors declare no other competing interests.</p>
              
            </div></div><div id="peer-review-section" data-title="Peer review"><h2 id="peer-review">Peer review</h2><div id="peer-review-content">
              
              
                <h3 id="FPar1">Peer review information</h3>
                <p><i>Nature</i> thanks Duncan McElfresh, Oleksandr Shchur and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p>
              
            </div></div><div id="additional-information-section" data-title="Additional information"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div><div id="Sec42-section" data-title="Extended data figures and tables"><h2 id="Sec42">Extended data figures and tables</h2><div data-test="supplementary-info" id="Sec42-content"><div data-test="supp-item" id="Fig7"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 1 performance comparison across" href="https://www.nature.com/articles/s41586-024-08328-6/figures/7" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig7_ESM.jpg">Extended Data Fig. 1 Performance comparison across additional dataset characteristics, extending Fig. </a><a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig5">5</a>.</h3><p>This figure shows the relative performance of different methods when datasets are split based on specific attributes. Error bars represent 95% confidence intervals. While performance differences are generally subtle across these splits, the most notable variation is observed for datasets with outliers in the target variable, though confidence intervals still overlap.</p></div><div data-test="supp-item" id="Fig8"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 2 performance comparisons of ta" href="https://www.nature.com/articles/s41586-024-08328-6/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig8_ESM.jpg">Extended Data Fig. 2 Performance comparisons of TabPFN and baselines on additional benchmark datasets&nbsp;and with GPU support.</a></h3><p>(a) Classification performance on the Grinsztajn medium-sized benchmark with categorical features, across 7 datasets. (b) Classification performance on the Grinsztajn medium-sized benchmark with numerical features, across its 15 datasets. (c) Classification performance on the TabZilla benchmark, consisting of 102 datasets with fewer than 10,000 rows of data, 500 features, and 10 classes. Duplicated datasets and those with fewer than 5 samples per class were removed to enable 5-fold cross-validation. (d) Performance Over Time Comparison with CPU vs. GPU Hardware: The performance over time when running our strongest baselines with eight CPUs (CPU) vs. eight CPUs and on one GPU (+GPU) on our classification test benchmark. AutoGluon automatically decides which models to train with what resources. For CatBoost and XGB, we specified that the models should train with GPU. Intervals represent 95% CI.</p></div><div data-test="supp-item" id="Fig9"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 3 comparing shap (shapley addit" href="https://www.nature.com/articles/s41586-024-08328-6/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig9_ESM.jpg">Extended Data Fig. 3 Comparing SHAP (SHapley Additive exPlanations) summary plots between TabPFN and baselines.</a></h3><p>We compare SHAP feature importance and impact for Logistic Regression, TabPFN, and CatBoost on the “Default of Credit Card Clients” dataset. The top features visualized are credit amount, age, and duration. Each point represents a single instance, with the color indicating the value of the checking status feature (blue for low, red for high), illustrating its interaction with the respective feature on the x-axis. We see that Logistic Regression is most interpretable due to the simple underlying functions. However, Logistic Regression has poor predictive accuracy, and the learned functions are unintuitive when looking at the outer bounds of features. TabPFN has good predictive accuracy and learns simple, interpretable functions. CatBoost is the least interpretable, with unclear patterns and wide variation in SHAP values per sample. This figure is adapted from Lundberg et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Lundberg, S. M. &amp; Lee, S.-I. A unified approach to interpreting model predictions. In Proc. Advances in Neural Information Processing Systems (eds Guyon, I. et al.) Vol. 30, 4765–4774 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR47" id="ref-link-section-d83722773e4935">47</a></sup>.</p></div><div data-test="supp-item" id="Fig10"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 4 finetuning tabpfn on 2-dimens" href="https://www.nature.com/articles/s41586-024-08328-6/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig10_ESM.jpg">Extended Data Fig. 4 Finetuning TabPFN on 2-dimensional sine curve datasets.</a></h3><p>(a) Examples of 2D sine curve datasets with different offsets. (b) Finetuning loss curves for 50 runs with random train-test offsets. Colors indicate the offset between train and test. TabPFN shows positive transfer, with better performance for more similar distributions. For a dataset shift of <i>π</i>, the inverse label needs to be predicted in the test set, compared to the finetuning data. However, TabPFN still generalizes when finetuned on this data.</p></div><div data-test="supp-item" data-container-section="table" id="table-1"><figure><figcaption><b id="Tab1" data-test="table-caption">Extended Data Table 1 Aggregated results on the 29 AMLB classification Benchmark datasets</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/1" aria-label="Full size table 1"><span>Full size table</span></a></p></figure></div><div data-test="supp-item" data-container-section="table" id="table-2"><figure><figcaption><b id="Tab2" data-test="table-caption">Extended Data Table 2 Aggregated results on the 28 AMLB and OpenML-CTR23 regression Benchmark datasets</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/2" aria-label="Full size table 2"><span>Full size table</span></a></p></figure></div><div data-test="supp-item" data-container-section="table" id="table-3"><figure><figcaption><b id="Tab3" data-test="table-caption">Extended Data Table 3 List of test datasets used for primary evaluation of classification tasks</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/3" aria-label="Full size table 3"><span>Full size table</span></a></p></figure></div><div data-test="supp-item" data-container-section="table" id="table-4"><figure><figcaption><b id="Tab4" data-test="table-caption">Extended Data Table 4 List of test datasets used for primary evaluation of regression tasks</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/4" aria-label="Full size table 4"><span>Full size table</span></a></p></figure></div><div data-test="supp-item" data-container-section="table" id="table-5"><figure><figcaption><b id="Tab5" data-test="table-caption">Extended Data Table 5 Hyperparameter defaults and search space for TabPFN and our baselines</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/5" aria-label="Full size table 5"><span>Full size table</span></a></p></figure></div><div data-test="supp-item" data-container-section="table" id="table-6"><figure><figcaption><b id="Tab6" data-test="table-caption">Extended Data Table 6 Performance on Kaggle Data Science Challenges</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/6" aria-label="Full size table 6"><span>Full size table</span></a></p></figure></div></div></div><div id="Sec43-section" data-title="Supplementary information"><h2 id="Sec43">Supplementary information</h2><div data-test="supplementary-info" id="Sec43-content"><div data-test="supp-item" id="MOESM1"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary tables 1–4" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Tables 1–4</a></h3><p> Unnormalized per dataset results: per dataset ROC AUC scores for our model and baselines on the four evaluated benchmarks.</p></div><div data-test="supp-item" id="MOESM2"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary tables 5 and 6" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_MOESM2_ESM.pdf" data-supp-info-image="">Supplementary Tables 5 and 6</a></h3><p>Meta-information on development datasets: meta-information of the development dataset is used to validate the performance of our models for regression and classification.</p></div></div></div><div id="rightslink-section" data-title="Rights and permissions"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Accurate%20predictions%20on%20small%20data%20with%20a%20tabular%20foundation%20model&amp;author=Noah%20Hollmann%20et%20al&amp;contentID=10.1038%2Fs41586-024-08328-6&amp;copyright=The%20Author%28s%29&amp;publication=0028-0836&amp;publicationDate=2025-01-08&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div><div id="article-info-section" aria-labelledby="article-info" data-title="About this article"><h2 id="article-info">About this article</h2><div id="article-info-content"><p><a data-crossmark="10.1038/s41586-024-08328-6" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41586-024-08328-6" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></p><div><h3 id="citeas">Cite this article</h3><p>Hollmann, N., Müller, S., Purucker, L. <i>et al.</i> Accurate predictions on small data with a tabular foundation model.
                    <i>Nature</i> <b>637</b>, 319–326 (2025). https://doi.org/10.1038/s41586-024-08328-6</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-024-08328-6?format=refman&amp;flavour=citation">Download citation</a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2024-05-17">17 May 2024</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2024-10-31">31 October 2024</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2025-01-08">08 January 2025</time></span></p></li><li><p>Issue Date<span>: </span><span><time datetime="2025-01-09">09 January 2025</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41586-024-08328-6</span></p></li></ul></div></div></div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WorstFit: Unveiling Hidden Transformers in Windows ANSI (291 pts)]]></title>
            <link>https://blog.orange.tw/posts/2025-01-worstfit-unveiling-hidden-transformers-in-windows-ansi/</link>
            <guid>42647101</guid>
            <pubDate>Thu, 09 Jan 2025 16:19:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.orange.tw/posts/2025-01-worstfit-unveiling-hidden-transformers-in-windows-ansi/">https://blog.orange.tw/posts/2025-01-worstfit-unveiling-hidden-transformers-in-windows-ansi/</a>, See on <a href="https://news.ycombinator.com/item?id=42647101">Hacker News</a></p>
Couldn't get https://blog.orange.tw/posts/2025-01-worstfit-unveiling-hidden-transformers-in-windows-ansi/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A playable game of Tetris inside a PDF (955 pts)]]></title>
            <link>https://th0mas.nl/downloads/pdftris.pdf</link>
            <guid>42645218</guid>
            <pubDate>Thu, 09 Jan 2025 13:31:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://th0mas.nl/downloads/pdftris.pdf">https://th0mas.nl/downloads/pdftris.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=42645218">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[My Favorite Self-Hosted Apps Launched in 2024 (109 pts)]]></title>
            <link>https://selfh.st/2024-favorite-new-apps/</link>
            <guid>42645119</guid>
            <pubDate>Thu, 09 Jan 2025 13:22:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://selfh.st/2024-favorite-new-apps/">https://selfh.st/2024-favorite-new-apps/</a>, See on <a href="https://news.ycombinator.com/item?id=42645119">Hacker News</a></p>
Couldn't get https://selfh.st/2024-favorite-new-apps/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[SQL nulls are weird (115 pts)]]></title>
            <link>https://jirevwe.github.io/sql-nulls-are-weird.html</link>
            <guid>42645110</guid>
            <pubDate>Thu, 09 Jan 2025 13:21:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jirevwe.github.io/sql-nulls-are-weird.html">https://jirevwe.github.io/sql-nulls-are-weird.html</a>, See on <a href="https://news.ycombinator.com/item?id=42645110">Hacker News</a></p>
Couldn't get https://jirevwe.github.io/sql-nulls-are-weird.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Rational or Not? This Basic Math Question Took Decades to Answer (125 pts)]]></title>
            <link>https://www.quantamagazine.org/rational-or-not-this-basic-math-question-took-decades-to-answer-20250108/</link>
            <guid>42644896</guid>
            <pubDate>Thu, 09 Jan 2025 13:01:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/rational-or-not-this-basic-math-question-took-decades-to-answer-20250108/">https://www.quantamagazine.org/rational-or-not-this-basic-math-question-took-decades-to-answer-20250108/</a>, See on <a href="https://news.ycombinator.com/item?id=42644896">Hacker News</a></p>
Couldn't get https://www.quantamagazine.org/rational-or-not-this-basic-math-question-took-decades-to-answer-20250108/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[My Amazon TV Now Unmutes Itself During Prime Video Commercial Breaks (134 pts)]]></title>
            <link>https://old.reddit.com/r/mildlyinfuriating/comments/1hx5wkq/my_amazon_tv_now_unmutes_itself_during_prime/</link>
            <guid>42644834</guid>
            <pubDate>Thu, 09 Jan 2025 12:54:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/mildlyinfuriating/comments/1hx5wkq/my_amazon_tv_now_unmutes_itself_during_prime/">https://old.reddit.com/r/mildlyinfuriating/comments/1hx5wkq/my_amazon_tv_now_unmutes_itself_during_prime/</a>, See on <a href="https://news.ycombinator.com/item?id=42644834">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/mildlyinfuriating/comments/1hx5wkq/my_amazon_tv_now_unmutes_itself_during_prime/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft should be terrified of SteamOS (105 pts)]]></title>
            <link>https://www.pcworld.com/article/2571541/microsoft-should-be-terrified-of-steamos.html</link>
            <guid>42644434</guid>
            <pubDate>Thu, 09 Jan 2025 12:06:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcworld.com/article/2571541/microsoft-should-be-terrified-of-steamos.html">https://www.pcworld.com/article/2571541/microsoft-should-be-terrified-of-steamos.html</a>, See on <a href="https://news.ycombinator.com/item?id=42644434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="link_wrapped_content">




<p>Ten years ago PC gamers were eagerly awaiting <a href="https://www.pcworld.com/article/423974/steam-machines-unleashed-impressions-from-one-week-in-a-steam-powered-living-room.html">Steam Machines</a>, console-style Linux boxes built from the ground up to play PC games. They flopped, due in no small part to Steam operator Valve’s lack of experience working with hardware partners. But in 2025, both Valve and its home-built gaming operating system are different beasts. And Microsoft should be afraid of them. </p>



<p><strong>Further reading: </strong><a href="https://www.pcworld.com/article/1981383/how-to-use-steam-deck-as-a-desktop-pc.html">How to use Steam Deck as a desktop PC</a></p>



<h2 id="the-steam-deck-dominates-pc-gaming">The Steam Deck dominates PC gaming</h2>



<p>The big story in PC gaming for the last three years has been the Steam Deck. This low-power, portable, relatively inexpensive machine is clearly something the market has been waiting for, exciting gamers and energizing PC makers to pump out imitators, like the Asus ROG Ally and the Lenovo Legion Go. </p>



<p>But all of these machines lack a crucial component, despite copying the Steam Deck’s hardware to a greater or lesser degree. They rely on Windows, as do almost all consumer PCs not made by Apple. And <a href="https://www.pcworld.com/article/2229777/why-lenovos-legion-go-isnt-my-go-to-handheld.html">Windows just isn’t a good experience</a> in this form factor.</p>

		
			
			


<p>That’s why Lenovo turned to Valve for its second-gen Legion Go S. Or perhaps more precisely, the Legion Go S Powered By SteamOS (its full and cumbersome title). It’s <a href="https://www.pcworld.com/article/2567576/lenovo-legion-go-s-is-the-first-non-valve-steamos-gaming-handheld.html">the first handheld PC officially powered by Valve’s Linux-based operating system</a>, but probably not the last. </p>



<p>Lenovo is also making new Windows-based versions of the same hardware, but we’ve already heard that Asus is working on a similar Steam-powered handheld, and Valve itself will <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://www.theverge.com/2025/1/7/24338405/valve-steamos-beta-other-handhelds-beyond-steam-deck&amp;xcust=2-1-2571541-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2571541/microsoft-should-be-terrified-of-steamos.html" rel="nofollow" data-subtag="2-1-2571541-1-0-0-0-0" data-domain-name="theverge" target="_blank">let you download and install builds of SteamOS later in 2025</a>. Some tech heads aren’t even waiting, and are already <a href="https://www.pcworld.com/article/2480004/run-steamos-on-your-desktop-pc-or-gaming-handheld-with-bazzite.html">building their own quasi-SteamOS-powered PCs</a>.</p>


<div><figure data-wp-context="{&quot;uploadedSrc&quot;:false,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-2567614&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:&quot;none&quot;,&quot;targetHeight&quot;:&quot;none&quot;,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: legion go S steam &quot;,&quot;alt&quot;:&quot;legion go S steam &quot;}" data-wp-interactive="core/image"><img decoding="async" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://b2c-contenthub.com/wp-content/uploads/2025/01/legion-go-S-steam.jpg?quality=50&amp;strip=all&amp;w=1200" alt="legion go S steam " width="1200" height="675" loading="lazy"></figure><p>Lenovo/Valve</p></div>



<p>Despite fumbling its initial debut on console-style Steam Machines, SteamOS has quietly and steadily improved over the last decade, benefitting both from the Linux market’s maturity and Valve’s endless investment into the Steam store and community as a quasi-platform of its own. </p>



<p>The X factor in the Steam Deck’s explosive popularity is <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://www.theverge.com/2025/1/7/24338405/valve-steamos-beta-other-handhelds-beyond-steam-deck&amp;xcust=2-1-2571541-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2571541/microsoft-should-be-terrified-of-steamos.html" rel="nofollow" data-subtag="2-1-2571541-1-0-0-0-0" data-domain-name="theverge" target="_blank">the Proton compatibility layer</a>, which allows games made only for Windows to run on the low-powered AMD hardware with minimal fuss. It can’t run everything — non-Steam games like <em>Fortnite </em>and the latest AAA polygon-pushers can’t run optimally on the Steam Deck. But it’s good enough for the vast majority of PC games and on a device that starts at $400, you get a lot of grace from gamers who also need to pay for rent and groceries.</p>



<p>Contrast this with Windows, the current de facto standard for PC gaming. Yes, Linux fans, I know you’ve been playing some of the same games as Windows users for years, ditto for Mac. But when you think “gaming PC,” you think of a Windows-powered desktop or laptop. Or do you? It’s possible — though hard to pin down, since Valve hasn’t released any numbers — that in terms of single-device volume, the Steam Deck is now <a href="https://www.pcworld.com/article/2519400/stop-waiting-for-an-xbox-handheld-and-just-buy-a-steam-deck.html">the most popular gaming PC in the world</a>. </p>



<h2 id="windows-wobbles-from-10-to-11">Windows wobbles from 10 to 11</h2>



<p>But I digress. Windows <em>is </em>the home of PC gaming, at least for now and the foreseeable future, but it’s not a happy home. As I said previously, handheld gaming PCs that ape the Steam Deck’s hardware but run Windows 11 often find that last point is the biggest pain point for users.</p>



<p>They complain of inefficient use of the limited hardware, to say nothing of how Windows just isn’t easy to use on those smaller screens. And companies like Asus, Lenovo, MSI, et cetera don’t have the software chops to make an effective go-between layer for users, even if these devices could spare the performance overhead (they can’t).</p>


<div><figure data-wp-context="{&quot;uploadedSrc&quot;:false,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-2351680&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:&quot;none&quot;,&quot;targetHeight&quot;:&quot;none&quot;,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Asus ROG Ally X header image&quot;,&quot;alt&quot;:&quot;Asus ROG Ally X header image&quot;}" data-wp-interactive="core/image"><img decoding="async" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://b2c-contenthub.com/wp-content/uploads/2024/05/ROG-Ally-X-in-the-hand.jpg?quality=50&amp;strip=all&amp;w=1200" alt="Asus ROG Ally X header image" width="1200" height="676" loading="lazy"></figure><p>Michael Crider/Foundry</p></div>



<p>Windows isn’t looking so hot in general, in fact. The transition from Windows 10 to Windows 11 hasn’t quite been the disaster that the initial Windows 7 to 8 transition was. But it hasn’t been great, either. </p>



<p>Those big yearly updates seem to reliably bork at least some portion of the userbase’s machines, <a href="https://www.pcworld.com/article/2532669/ubisoft-games-are-crashing-on-pcs-with-the-windows-11-24h2-update.html">disproportionately affecting gamers</a> and Microsoft is still struggling to get people to give up Windows 10. Even with a well-publicized end of support coming in under a year, Windows 11 is struggling — and <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://www.theregister.com/2025/01/02/windows_10_grows/&amp;xcust=2-1-2571541-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2571541/microsoft-should-be-terrified-of-steamos.html" rel="nofollow" data-subtag="2-1-2571541-1-0-0-0-0" data-domain-name="theregister" target="_blank">sometimes flat-out failing</a> — to gain market share over its previous incarnation.</p>



<p>Microsoft’s has larger general woes in the gaming market, <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://www.levelup.com/en/originals/799325/Xbox-Series-XS-is-a-disaster-and-Microsoft-caused-it/&amp;xcust=2-1-2571541-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2571541/microsoft-should-be-terrified-of-steamos.html" rel="nofollow" data-subtag="2-1-2571541-1-0-0-0-0" data-domain-name="levelup" target="_blank">watching the Xbox platform and brand apparently drown</a> even as its Game Pass subscription grows. Game Pass is pretty clearly the company’s attempt at a cross-platform rebirth, the culmination of hundreds of billions invested in buying up developers and publishers to own games as diverse as <em>Minecraft, Call of Duty, </em>and <em>WoW. </em></p>



<p>But you can’t spend-money-to-make-money forever, and gaming isn’t Microsoft’s only business. It’s also desperate to sell Windows machines (2025 is “The year of the Windows 11 PC refresh,” <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://blogs.windows.com/windowsexperience/2025/01/06/ces-2025-the-year-of-the-windows-11-pc-refresh/&amp;xcust=2-1-2571541-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2571541/microsoft-should-be-terrified-of-steamos.html" rel="nofollow" data-subtag="2-1-2571541-1-0-0-0-0" data-domain-name="windows" target="_blank">allegedly</a>), Office subscriptions, and AI services to the enterprise. There might be too many cooks in the kitchen and too many mouths to feed, all at once, in one of the tech industry’s oldest and most reliable megacorps.</p>



<h2 id="steamos-reminds-me-of-android">SteamOS reminds me of Android</h2>



<p>So look at a wobbling Windows platform on one hand, and an ascendant and suddenly spreading SteamOS on the other. Valve has committed to offering SteamOS to manufacturing partners via the <a href="https://www.pcworld.com/article/2547938/valve-new-powered-by-steamos-branding-hints-at-new-hardware.html">“Powered by SteamOS” branding initiative</a>. </p>



<p>With the open source Linux as a foundation and relative hardware agnosticism, it’s starting to look a lot like the relationship that Google developed with smartphone makers to proliferate Android across the mobile market. It’s not a complete one-to-one comparison, but <a href="https://www.pcworld.com/article/2570630/interview-valve-talks-steamos-on-third-party-handhelds.html">Valve told us in an interview</a> that it’s not charging for SteamOS. Huh.</p>



<p>Microsoft tried to compete with Android. It failed, miserably, and the company essentially had to abandon the mobile space entirely and settle for providing backend services through apps. Even when Microsoft tried to get an early foothold in the folding device segment with the Surface Duo (<a href="https://www.pcworld.com/article/2483990/microsofts-surface-duo-2-gets-one-last-update-before-its-send-off.html">also failing</a>), it did so using Android as a basis. </p>



<figure><p>
<iframe title="Valve Lays Out Plans For SteamOS" width="500" height="281" src="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" data-src="https://www.youtube.com/embed/UI-C-nZnDE8?feature=oembed"></iframe>
</p></figure>



<p>My colleague Adam Patrick Murray waxed philosophical about SteamOS powering gaming laptops when he spoke with a Valve engineer at CES. And I think that’s a definite possibility, even if it isn’t Valve’s immediate focus with SteamOS as it moves to conquer the handheld form factor first. </p>



<p>But we’re talking about a “free” operating system (those quotes are because you’ll need to partner with Valve in some capacity to get the branding), built from the ground up for PC gaming, and flexible enough to run on some of the lowest-power hardware on the market or potentially the most cutting-edge gaming devices.</p>



<p>The parallels to Android are hard to ignore, at least for me, a journalist who cut my teeth on the smartphone boom. But the prospects don’t stop at gaming. With <a href="https://www.pcworld.com/article/2500763/why-i-switched-from-windows-to-chromebooks-and-havent-looked-back.html">Chromebooks and ChromeOS</a>, Google has proven that regular consumers and even some bigger customers like education aren’t as committed to Windows as they were back in the 90s. </p>






<p>ChromeOS is still seen as a “budget” laptop solution (<a href="https://www.pcworld.com/article/2525709/what-could-google-do-to-finally-make-a-winning-laptop.html">much to Google’s chagrin</a>). But a year or two from now, you could see Chrome-powered budget laptops next to mid-range and high-end SteamOS-powered gaming laptops, all sitting next to Windows 11 machines on a Best Buy shelf. And that’ll be <em>after </em>Microsoft has forced an upgrade upon lots of people who didn’t want to give up Windows 10. </p>



<h2 id="consumers-are-ready-for-a-future-beyond-windows">Consumers are ready for a future beyond Windows</h2>



<p>Let me be clear: The odds of a massive, immediate shift away from Windows PCs aren’t great. This isn’t a “year of the Linux desktop” rallying cry. But if there <em>is </em>a Linux desktop that exists today, it’s the Steam Deck. And that makes SteamOS a bellwether for greater proliferation of non-Windows devices (if not necessarily “Linux” specifically) in a huge range of form factors.</p>



<p>At the start of 2025, Microsoft still has its comfortable stranglehold on the consumer side of the PC market. It weathered “the death of the desktop” predicted during the smartphone and tablet boom — people aren’t getting rid of their personal machines. But Windows’ never-ending dominance as the de facto PC operating system is, if not in doubt, then certainly in question. </p>



<p>Microsoft’s attempts to ameliorate the issues that Windows-powered handheld PCs, lacking as they are, shows that the company is aware of the problem it has in that form factor. I wonder what it’ll do if it sees SteamOS jump to gaming laptops…or desktops. SteamOS isn’t necessarily a harbinger of doom for Windows. But it <em>could </em>be. And that should make Microsoft very, very frightened. </p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stay Gold, America (114 pts)]]></title>
            <link>https://blog.codinghorror.com/stay-gold-america/</link>
            <guid>42644291</guid>
            <pubDate>Thu, 09 Jan 2025 11:42:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.codinghorror.com/stay-gold-america/">https://blog.codinghorror.com/stay-gold-america/</a>, See on <a href="https://news.ycombinator.com/item?id=42644291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        <article>

            <header>

                
                    <p>We are at an unprecedented point in American history, and I'm concerned we may lose sight of the American Dream.</p>

                <div>
                    <p><a href="https://blog.codinghorror.com/author/jeff-atwood/">
                                    <img src="https://blog.codinghorror.com/content/images/size/w160/2025/01/coding-horror-logo-transparency.png" alt="Jeff Atwood">
                                </a>
                    </p>
                    
                </div>

                
            </header>

            <section>
                <p>We are at an unprecedented point in American history, and I'm concerned we may lose sight of the <a href="https://en.wikipedia.org/wiki/American_Dream?ref=blog.codinghorror.com" rel="noreferrer">American Dream</a>:</p><ul><li>The costs of housing, healthcare, and education have soared <a href="https://uvaro.com/blog/college-costs-vs-inflation-1982-present?ref=blog.codinghorror.com" rel="noreferrer">far beyond the pace of inflation and wage growth</a>.</li><li>We are a democracy, but 144 million Americans – 42% of the adults who live here – do not vote and have no say in what happens.</li><li><a href="https://youtu.be/QPKKQnijnsM?ref=blog.codinghorror.com" rel="noreferrer">Wealth concentration</a> has reached <a href="https://americancompass.org/economic-inequality-guide/?ref=blog.codinghorror.com" rel="noreferrer">historic levels</a>. The top 1% of households control 32% of all wealth, while the bottom 50% only have 2.6%.</li></ul><p>We must act now to keep the dream alive. Our family made <strong>eight $1 million donations</strong> to nonprofit groups working to support those most currently in need:</p><ul><li><a href="https://teamrubiconusa.org/?ref=blog.codinghorror.com" rel="noreferrer">Team Rubicon</a> – Mobilizing veterans to continue their service, leveraging their skills and experience to help Americans prepare, respond, and recover from natural disasters.</li><li><a href="https://childrenshungerfund.org/?ref=blog.codinghorror.com" rel="noreferrer">Children's Hunger Fund</a> – Provides resources to local churches in the United States and around the world to meet the needs of impoverished community members.</li><li><a href="https://pen.org/?ref=blog.codinghorror.com">PEN America</a> – Defends writers against censorship and abuse, supports writers in need of emergency assistance, and amplifies the writing of incarcerated prisoners. (One of my personal favorites; I've seen the power of writing transform our world <a href="https://www.weforum.org/stories/2016/03/10-novels-that-changed-the-world/?ref=blog.codinghorror.com" rel="noreferrer">many times</a>.)</li><li><a href="https://www.thetrevorproject.org/?ref=blog.codinghorror.com" rel="noreferrer">The Trevor Project</a> – Working to change hearts, minds, and laws to support the lives of young adults seeking acceptance as fellow Americans.</li><li><a href="https://www.naacpldf.org/?ref=blog.codinghorror.com" rel="noreferrer">NAACP Legal Defense and Educational Fund</a> – Legal organization with a historic record of advancing racial justice and reducing inequality.</li><li><a href="https://www.firstgenerationinvestors.com/?ref=blog.codinghorror.com" rel="noreferrer">First Generation Investors</a> –<strong> </strong>Introduces high school students in low-income areas to the fundamentals of investing, providing them real money to invest, encouraging long-term wealth accumulation and financial literacy among underserved youth.</li><li><a href="https://www.globalrefuge.org/?ref=blog.codinghorror.com" rel="noreferrer">Global Refuge</a> – Supporting migrants and refugees from around the globe, in partnership with community-based legal and social service providers nationwide, helping rebuild lives in America.</li><li><a href="https://www.plannedparenthood.org/?ref=blog.codinghorror.com" rel="noreferrer">Planned Parenthood</a> – Provides essential healthcare services and resources that help individuals and families lead healthier lives.</li></ul><p>I encourage every American to<strong> contribute soon, however you can, to organizations you feel are </strong><a href="https://www.charitynavigator.org/?ref=blog.codinghorror.com" rel="noreferrer"><strong>effectively helping</strong></a><strong> </strong>those most currently in need here in America.</p><p>We must also<em> </em>work toward deeper changes that will take decades to achieve. Over the next five years,<strong> my family pledges half our remaining wealth </strong>towards long term efforts ensuring that all Americans continue to have access to the American Dream. </p><figure><img src="https://blog.codinghorror.com/content/images/2025/01/share-vertical-1.png" alt="" loading="lazy" width="328" height="500"></figure><div><p>I never thought my family would be able to do this. My parents are of hardscrabble rural West Virginia and rural North Carolina origins. They barely managed to claw their way to the bottom of the middle class by the time they ended up in Virginia. Unfortunately, due to the demons passed on to them by their parents, my father was an alcoholic and my mother participated in the drinking. She ended up divorcing my father when I was 16 years old. It was only after the divorce that my parents were able to heal themselves, heal their only child, and stop the drinking, which was so destructive to our family. If the divorce hadn't forced the issue, alcohol would have inevitably destroyed us all. </p><p>My parents may not have done everything right, but they both unconditionally loved me. <strong>They taught me how to fully, deeply receive love, and the profound joy of reflecting that love upon everyone around you. </strong></p></div><p>I went on to attend public school in Chesterfield County, Virginia. In 1992 I graduated from the University of Virginia, founded by <a href="https://en.wikipedia.org/wiki/Thomas_Jefferson?ref=blog.codinghorror.com">Thomas Jefferson</a>.</p><p>During college, I worked at Safeway as a part-time cashier, earning the <a href="https://www.dol.gov/agencies/whd/minimum-wage/history?ref=blog.codinghorror.com" rel="noreferrer">federal minimum wage</a>, scraping together whatever money I could through government Pell grants, scholarships, and other part-time work to pay my college tuition. Even with <a href="https://www.salliemae.com/blog/in-state-vs-out-of-state-tuition/?ref=blog.codinghorror.com" rel="noreferrer">lower in-state tuition</a>, it was rocky. Sometimes I could barely manage tuition payments. And that was in 1992, when tuition was only $3,000 per year. It is now $23,000 per year. <a href="https://educationdata.org/average-cost-of-college-by-year?ref=blog.codinghorror.com" rel="noreferrer">College tuition</a> at a state school increased by 8 times over the last 30 years. These <a href="https://uvaro.com/blog/college-costs-vs-inflation-1982-present?ref=blog.codinghorror.com" rel="noreferrer">huge cost increases</a> for healthcare, education, and housing are not compatible with the American Dream.</p><figure><a href="https://uvaro.com/blog/college-costs-vs-inflation-1982-present?ref=blog.codinghorror.com"><img src="https://blog.codinghorror.com/content/images/2025/01/image-6.png" alt="" loading="lazy" width="675" height="700" srcset="https://blog.codinghorror.com/content/images/size/w600/2025/01/image-6.png 600w, https://blog.codinghorror.com/content/images/2025/01/image-6.png 675w"></a></figure><div><p>Programmers all over the world helped make an American Dream happen <a href="https://web.archive.org/web/20080703183923/http://stackoverflow.com/" rel="noreferrer">in 2008</a> when we built <a href="https://stackoverflow.com/?ref=blog.codinghorror.com" rel="noreferrer">Stack Overflow</a>, a Q&amp;A website for programmers creating a shared <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en?ref=blog.codinghorror.com" rel="noreferrer">Creative Commons</a> knowledge base for the world. We did it democratically, because that's the <a href="https://en.wikipedia.org/wiki/American_way?ref=blog.codinghorror.com" rel="noreferrer">American way</a>. We voted to rank questions and answers, and held elections for community moderators using <a href="https://en.m.wikipedia.org/wiki/Ranked-choice_voting_in_the_United_States?ref=blog.codinghorror.com" rel="noreferrer">ranked choice voting</a>. We built a <strong>digital democracy</strong> – of the programmers, by the programmers, for the programmers. <a href="https://www.forbes.com/sites/vijaygurbaxani/2021/06/08/the-18-billion-acquisition-of-stack-overflow-aims-to-turbocharge-the-worlds-software-knowhow/?ref=blog.codinghorror.com" rel="noreferrer">It worked</a>. </p><p>With the guidance of my co-founder <a href="https://en.wikipedia.org/wiki/Joel_Spolsky?ref=blog.codinghorror.com" rel="noreferrer">Joel Spolsky</a>, I came to understand that the digital democracy of Stack Overflow was not enough. We must be brave enough to actively, openly share love with each other. That became the foundation for <a href="https://discourse.org/?ref=blog.codinghorror.com">Discourse</a>, a free, open source tool for constructive, empathetic community discussions that are also Creative Commons. We can disagree in those discussions because Discourse empowers communities to <a href="https://frameshiftconsulting.com/2017/09/10/the-intolerable-speech-rule-the-paradox-of-tolerance-for-tech-companies?ref=blog.codinghorror.com" rel="noreferrer">set boundaries the community agrees on</a>, providing tools to democratically govern and strongly moderate by enforcing these boundaries. <strong>Digital democracy <em>and empathy</em>,</strong> for everyone.</p></div><p>In order for digital democracy to work, we need to see each other through our screens.</p><figure><a href="https://xkcd.com/438/?ref=blog.codinghorror.com"><img src="https://blog.codinghorror.com/content/images/2025/01/xkcd438_condensed_b.png" alt="" loading="lazy" width="382" height="253"></a></figure><p><a href="https://blog.codinghorror.com/they-have-to-be-monsters/"><u>We often behave online in ways we never would in the real world</u></a> because we cannot see the person on the other side of the screen. But as our world becomes more digital, we must extend our kindness through that screen.</p><p>I've always felt Stack Overflow and Discourse are <a href="https://www.bcorporation.net/en-us/?ref=blog.codinghorror.com" rel="noreferrer">projects for the public good</a> that happen to be <a href="https://en.wikipedia.org/wiki/Company?ref=blog.codinghorror.com" rel="noreferrer">corporations</a>. I probably couldn't have accomplished this in any other country, and I was rewarded handsomely for a combination of hard work and good luck. That's what the <a href="https://www.britannica.com/topic/American-Dream?ref=blog.codinghorror.com" rel="noreferrer">American Dream</a> promises us. </p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/mXBMqbWcqzg?start=165&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Field of Dreams (James Earl Jones Scene) | People Will Come in 4K HDR"></iframe></figure><p><a href="https://blog.codinghorror.com/the-field-of-dreams-strategy/"><u>We built it, and<em> people came</em></u></a>. I earned millions of dollars. I thought that was the final part of the American Dream. But it wasn't.</p><p>I recently attended a theater performance of <a href="https://en.wikipedia.org/wiki/The_Outsiders_(novel)?ref=blog.codinghorror.com">The Outsiders</a> at my son's public high school. All I really knew was the famous <a href="https://www.youtube.com/watch?v=f46JMzVzSB4&amp;ref=blog.codinghorror.com" rel="noreferrer">"stay gold" line</a> from the 1983 movie adaptation. But as I sat there in the audience among my neighbors, watching the complete story acted out in front of me by these teenagers, I slowly realized what staying gold actually meant: <a href="http://blog.asjournal.org/the-american-dream-reconsidered-the-outsiders-1967/?ref=blog.codinghorror.com" rel="noreferrer"><em>sharing</em> the American Dream</a>. </p><p>In the printed program, the director wrote:</p><blockquote>This play is a reminder that strength lies not just in overcoming hardships but in staying true to ourselves and lifting up those around us. <p>We hope you feel the raw emotions, sense the camaraderie, and <strong>connect with the enduring themes of resilience, empathy, and unity</strong>. Whether you've read this story recently, long ago, or not at all, I hope you are able to find inspiration in the strength and passion of youth. Thank you for being part of this journey with us.</p><p><a href="https://poets.org/poem/nothing-gold-can-stay?ref=blog.codinghorror.com">Stay gold</a>.</p></blockquote><p>I <a href="https://blog.codinghorror.com/im-loyal-to-nothing-except-the-dream/" rel="noreferrer">believe deeply</a> in sharing The American Dream. It is the foundation of our country, the second paragraph in our <a href="https://www.archives.gov/founding-docs/declaration-transcript?ref=blog.codinghorror.com" rel="noreferrer">Declaration of Independence</a>, written by the founder of the public university I attended:</p><blockquote>We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.</blockquote><p>But the American Dream is not always available to every American. Its meaning can be distorted. Jimi Hendrix <a href="https://www.newyorker.com/culture/cultural-comment/rewinding-jimi-hendrixs-national-anthem?ref=blog.codinghorror.com" rel="noreferrer">captured this distortion</a> so eloquently in his rendition of our national anthem. </p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/sjzZh6-h9fM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Jimi Hendrix - The Star Spangled Banner [ National Anthem ] ( Live at Woodstock 1969 )"></iframe></figure><p>We are still trying to live up to those ideals today. In November 2024, <a href="https://en.wikipedia.org/wiki/2024_United_States_presidential_election?ref=blog.codinghorror.com" rel="noreferrer">enough of us voted</a> for people who interpret the dream in a way that I don't understand.</p><figure><a href="https://election.lab.ufl.edu/2024-general-election-turnout/?ref=blog.codinghorror.com"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfe_wqbzOe_elPxK-9TqRLDAxtI2Qrd8GHAtxqtJ8DvQ87d69tFasQnClpWuRYvDP-Iofd-PDkfpQU5RayMlyDo2rlVZWzfacm-kr1aM8fZprnh_1XXkpbr63MbGUpiNNaibPmxVQ?key=gIYEBDEiC9EgZC24CSOXGvAl" alt="" loading="lazy" width="661" height="573"></a></figure><p>34% of adults in America <a href="https://apnews.com/projects/election-2024-our-very-complicated-democracy/election-2024-why-americans-dont-vote-episode-6.html?ref=blog.codinghorror.com" rel="noreferrer">did not exercise their right to vote</a>. Why? Is it <a href="https://en.wikipedia.org/wiki/Voter_suppression_in_the_United_States?ref=blog.codinghorror.com" rel="noreferrer">voter suppression</a>, <a href="https://en.wikipedia.org/wiki/Gerrymandering?ref=blog.codinghorror.com" rel="noreferrer">gerrymandering</a> causing indifference, or people who felt their vote didn't matter? The 7.6% that are ineligible to vote are mostly adults living in America who have not managed to attain citizenship, or people <a href="https://www.ncsl.org/elections-and-campaigns/felon-voting-rights?ref=blog.codinghorror.com" rel="noreferrer">convicted of a felony</a>. Whatever the reasons, 42% of adults living in America had no say in the 2024 election. The vote <a href="https://election.lab.ufl.edu/2024-general-election-turnout/?ref=blog.codinghorror.com" rel="noreferrer">failed to represent everyone</a>.</p><div><p>I think many of the Americans who did vote are telling us they no longer believe our government is effectively <em>keeping America fair for everyone</em>. Our status as the <a href="https://thefulcrum.us/ethics-leadership/democracy-index?ref=blog.codinghorror.com" rel="noreferrer">world's leading democracy</a> is in question. We should make it easier for more eligible Americans to vote, such as <strong>making election day a </strong><a href="https://www.actforamerica.org/act-now/Make-Election-Day-a-National-Holiday?ref=blog.codinghorror.com" rel="noreferrer"><strong>national holiday</strong></a><strong>, universal </strong><a href="https://tracker.votingrightslab.org/issues/universal-mailed-ballots?ref=blog.codinghorror.com#issues_map" rel="noreferrer"><strong>mail in voting</strong></a><strong>, and adopting </strong><a href="https://fairvote.org/our-reforms/ranked-choice-voting/?ref=blog.codinghorror.com" rel="noreferrer"><strong>ranked choice voting</strong></a> so all votes carry more weight. We should also strengthen institutions keeping democracy fair for everyone, such as state and local election boards, as well as the Federal Election Commission.</p><p>It was only after I attained the dream that I was able to <a href="https://www.youtube.com/watch?v=QPKKQnijnsM&amp;ref=blog.codinghorror.com" rel="noreferrer">fully see how many Americans have so very little</a>. This much wealth starts to unintentionally distance my family from other Americans. I no longer bother to look at how much items cost, because <em>I don't have to</em>. We don't have to think about all these things that are challenging or unreachable for so many others. The more wealth you attain, the more unmistakably clear it becomes how unequal life is for so many of us.</p><p>Even with the wealth I have, I can't imagine what it would feel like <a href="https://www.theatlantic.com/politics/archive/2024/11/democrats-harris-billionaire-mistake/680779/?ref=blog.codinghorror.com" rel="noreferrer">to be a billionaire</a><em>.</em> <strong>It is, for lack of a better word, </strong><a href="https://www.pewresearch.org/social-trends/2020/01/09/trends-in-income-and-wealth-inequality/?ref=blog.codinghorror.com" rel="noreferrer"><strong>unamerican</strong></a><strong>. </strong></p></div><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/QPKKQnijnsM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Wealth Inequality in America"></iframe></figure><div><p>In 2012, the top 1% of Americans held 24% of our country's wealth. By 2021, the top 1% of Americans held 30%.  So many have so little, while a tiny few have massive, wildly disproportionate wealth, which keeps growing. Now the global top 1% hold nearly twice as much wealth <a href="https://www.oxfam.org/en/press-releases/richest-1-bag-nearly-twice-much-wealth-rest-world-put-together-over-past-two-years?ref=blog.codinghorror.com" rel="noreferrer">as the rest of the world combined</a>.</p><p>I grew up poor in America, inspired by the promise of the American Dream that I could better myself and my family by building things <a href="https://credo.library.umass.edu/view/full/mums312-b203-i031?ref=blog.codinghorror.com" rel="noreferrer">that <em>mattered</em></a>:</p></div><blockquote>Work is service, not gain. The object of work is life, not income. The reward of production is plenty, not private fortune. We should measure the prosperity of a nation not by the number of millionaires, but by<strong> the absence of poverty, the prevalence of health, the efficiency of the public schools, </strong>and the number of people who can and do read worthwhile books<strong>.</strong></blockquote><p>Our <a href="https://en.wikipedia.org/wiki/Capitalism?ref=blog.codinghorror.com#Types" rel="noreferrer">version of capitalism</a> delivered so much wealth to my family for my hard work in co-founding two successful companies. My partner and I gladly paid our <a href="https://en.wikipedia.org/wiki/Taxation_in_the_United_States?ref=blog.codinghorror.com" rel="noreferrer">full taxes</a>, and we always planned to give most of our remaining wealth to charities when we pass, following the <a href="https://givingpledge.org/pledger?pledgerId=177&amp;ref=blog.codinghorror.com" rel="noreferrer">Warren Buffet Philanthropic Pledge</a>:</p><blockquote>More than 99% of my wealth will go to philanthropy during my lifetime or at death.</blockquote><p>I admire Buffett, but even having only a tiny fraction of his $325 billion fortune, to me this pledge was incomplete. When would this wealth be transferred? </p><p>Last year he <a href="https://amp.cnn.com/cnn/2024/06/28/investing/warren-buffett-gates-donation?ref=blog.codinghorror.com" rel="noreferrer">amended the pledge</a>, giving all his wealth at death to a charitable trust run by his children, aged 71, 69, and 66, who <a href="https://www.axios.com/2024/07/01/warren-buffett-pledge-100-billion?ref=blog.codinghorror.com" rel="noreferrer">do not make for natural charitable bedfellows</a>. I am only holding back enough wealth for my children so they can afford college educations and buy a home. I am compelled to, because being a parent is <a href="https://blog.codinghorror.com/on-parenthood/" rel="noreferrer">the toughest job I've ever had</a>, and I am concerned about their future.</p><p>November 5th <a href="https://en.wikipedia.org/wiki/2024_United_States_presidential_election?ref=blog.codinghorror.com" rel="noreferrer">raised the stakes</a>. It is now time to allocate <strong>half the wealth</strong> I was so fortunate to be dealt within the next five years<strong>,</strong> not just for my own family, but for all my fellow Americans. </p><p>Our government seems to be slower and slower at delivering change due to the <a href="https://carnegieendowment.org/research/2023/09/polarization-democracy-and-political-violence-in-the-united-states-what-the-research-says?lang=en&amp;ref=blog.codinghorror.com" rel="noreferrer">increased polarization of our two party system</a>. The last meaningful constitutional amendment we've managed to pass in the last 60 years was the 26th amendment in 1971, lowering the voting age to 18 and giving more people a voice in our democracy. </p><p>Political polarization is at <a href="https://en.wikipedia.org/wiki/Political_polarization_in_the_United_States?ref=blog.codinghorror.com#Gilded_Age" rel="noreferrer">historically high levels</a> and rising. In a two party system, this <a href="https://blogs.cfainstitute.org/investor/2018/03/13/red-states-blue-states-two-economies-one-nation/?ref=blog.codinghorror.com" rel="noreferrer">level of polarization</a> is counterproductive and even dangerous.<strong> Do we all still believe in the same American Dream? </strong></p><figure><a href="https://www.facinghistory.org/resource-library/political-polarization-united-states?ref=blog.codinghorror.com"><img src="https://blog.codinghorror.com/content/images/2025/01/image-9.png" alt="" loading="lazy" width="739" height="500" srcset="https://blog.codinghorror.com/content/images/size/w600/2025/01/image-9.png 600w, https://blog.codinghorror.com/content/images/2025/01/image-9.png 739w" sizes="(min-width: 720px) 720px"></a></figure><p>I've always loved <a href="https://blog.codinghorror.com/im-loyal-to-nothing-except-the-dream/" rel="noreferrer">the ideals</a> behind the American Dream, though we continually struggle to live up to them. They are worth fighting for, even if it means making <a href="https://blogs.loc.gov/loc/2020/07/remembering-john-lewis-the-power-of-good-trouble/?ref=blog.codinghorror.com" rel="noreferrer">"good trouble"</a>. We must come together and believe in our shared American Dream so deeply that we can improve our democracy... but <a href="https://www.rogerebert.com/reviews/great-movie-the-night-of-the-hunter-1955?ref=blog.codinghorror.com" rel="noreferrer">which dream?</a></p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/QC6gI4gXYPw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Do the Right Thing | Radio Raheem's Story of LOVE and HATE"></iframe></figure><p>The American Dream contains the <a href="https://www.goodreads.com/quotes/376797-returning-hate-for-hate-multiplies-hate-adding-deeper-darkness-to?ref=blog.codinghorror.com" rel="noreferrer">path of hate</a>, and the <a href="https://kinginstitute.stanford.edu/king-papers/documents/loving-your-enemies-sermon-delivered-dexter-avenue-baptist-church?ref=blog.codinghorror.com" rel="noreferrer">path of love</a>. Throughout our history, one hand is always fighting the other. Which path are we choosing?</p><blockquote>Our family pledges <strong>half our remaining wealth</strong> toward an <strong>American Dream founded on love</strong>.</blockquote>
<!--kg-card-begin: html-->
<!-- CSS for BlockQuote -->

<!--kg-card-end: html-->
<p>Here are some starting points for longer term efforts:</p><ul><li>We can support organizations making it easier for Americans to vote for a new Congress in two years and a new president in four years. My concern is damage to our democratic institutions may happen so quickly that our votes could matter even less within the coming years.</li><li>We could fund nonprofits that have a <a href="https://www.charitynavigator.org/about-us/our-methodology/ratings/?ref=blog.codinghorror.com" rel="noreferrer">proven track record</a> of protecting democratic institutions.</li><li>We could found a new organization loosely based on the original <a href="https://en.wikipedia.org/wiki/RAND_Corporation?ref=blog.codinghorror.com" rel="noreferrer">RAND Corporation</a>, but modernized like <a href="https://www.leverforchange.org/?ref=blog.codinghorror.com" rel="noreferrer">Lever for Change</a>. We can empower the best and brightest to determine a realistic, achievable path toward preserving the American Dream for <em>everyone,</em> working within the current system or outside it.</li><li>All states are shades of purple, not fully red or blue. We have <a href="https://today.yougov.com/politics/articles/50343-national-policy-proposals-with-bipartisan-support?ref=blog.codinghorror.com" rel="noreferrer">more in common on specific policies</a> than we realize. It would be very difficult to draw borders if we split. I know what divorce feels like, and we don't want this. Let's come together <a href="https://carnegieendowment.org/research/2023/09/polarization-democracy-and-political-violence-in-the-united-states-what-the-research-says?lang=en&amp;ref=blog.codinghorror.com">through our shared American Dream</a>. </li><li>We can start with change in our local communities. Vote in your own city, county, and state elections. Support local independent journalism and media. Find a local organization doing work you admire, ask what they need, and help them meet those needs. Listen to the stories of fellow volunteers, listen to the stories of the people you’re serving – that is the heart of Democracy.</li></ul><p>We've already completed the eight $1 million donations <a href="#" rel="noreferrer">listed above</a> to help those most immediately in need. Within the next five years, half of our family wealth will support longer term efforts. There is no single solution, so let's work together. I will gladly advise and <strong>empower others </strong>working towards the same goal.</p><figure><img src="https://blog.codinghorror.com/content/images/2025/01/share-landscape.png" alt="" loading="lazy" width="1235" height="690" srcset="https://blog.codinghorror.com/content/images/size/w600/2025/01/share-landscape.png 600w, https://blog.codinghorror.com/content/images/size/w1000/2025/01/share-landscape.png 1000w, https://blog.codinghorror.com/content/images/2025/01/share-landscape.png 1235w" sizes="(min-width: 720px) 720px"></figure><p>Please join us in <strong>Sharing the American Dream</strong>:</p><ol><li>Support organizations you feel are <a href="https://www.charitynavigator.org/?ref=blog.codinghorror.com" rel="noreferrer">effectively helping</a> those most in need across America right now.</li><li>Within the next five years, also contribute <strong>public dedications of time or funds towards longer term efforts </strong>to keep the American Dream fair and attainable for all our children.</li></ol><p><a href="https://poets.org/poem/nothing-gold-can-stay?ref=blog.codinghorror.com" rel="noreferrer">Stay gold</a>, America.</p>
<!--kg-card-begin: html-->
<small>(I could not have done this without the support of my partner Betsy Burton and the rest of my family. I'd also like to thank <a href="https://en.wikipedia.org/wiki/Steve_McConnell?ref=blog.codinghorror.com">Steve McConnell</a>, whose writing inspired me to start this blog in 2004. So many people from all walks of life generously shared their feedback to improve this post. <a href="https://blog.codinghorror.com/on-the-meaning-of-coding-horror/">We wrote it together</a>. Thank you all.)</small>
<!--kg-card-end: html-->

            </section>

            
        </article>

        

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists uncover how the brain washes itself during sleep (306 pts)]]></title>
            <link>https://www.science.org/content/article/scientists-uncover-how-brain-washes-itself-during-sleep</link>
            <guid>42644204</guid>
            <pubDate>Thu, 09 Jan 2025 11:29:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/scientists-uncover-how-brain-washes-itself-during-sleep">https://www.science.org/content/article/scientists-uncover-how-brain-washes-itself-during-sleep</a>, See on <a href="https://news.ycombinator.com/item?id=42644204">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/scientists-uncover-how-brain-washes-itself-during-sleep: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Factorio Blueprint Visualizer (406 pts)]]></title>
            <link>https://github.com/piebro/factorio-blueprint-visualizer</link>
            <guid>42644168</guid>
            <pubDate>Thu, 09 Jan 2025 11:23:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/piebro/factorio-blueprint-visualizer">https://github.com/piebro/factorio-blueprint-visualizer</a>, See on <a href="https://news.ycombinator.com/item?id=42644168">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Factorio Blueprint Visualizer</h2><a id="user-content-factorio-blueprint-visualizer" aria-label="Permalink: Factorio Blueprint Visualizer" href="#factorio-blueprint-visualizer"></a></p>
<p dir="auto">I love the game Factorio and I really like the look of factories after growing for many hours or blueprints after tweaking them for perfection. That's way I created a <a href="https://piebro.github.io/factorio-blueprint-visualizer/" rel="nofollow">website</a> to artfully visualize Factorio blueprints.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/piebro/factorio-blueprint-visualizer/blob/master/examples/Rocket_Ship_1.jpg"><img src="https://github.com/piebro/factorio-blueprint-visualizer/raw/master/examples/Rocket_Ship_1.jpg" width="58%"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/piebro/factorio-blueprint-visualizer/blob/master/examples/Rocket_Ship_1_Visualization.png"><img src="https://github.com/piebro/factorio-blueprint-visualizer/raw/master/examples/Rocket_Ship_1_Visualization.png" width="37%"></a>
</p>
<p dir="auto">With the website you can import Factorio blueprints as text and visualize them. You can tweak the drawing settings or create random ones.</p>
<p dir="auto">All buildings and tiles with their bounding boxes and belt, pipe, rail, inserter, wire and electricity connections can be visualized. Everything is drawn in vector graphics (SVG) to be able to view it in any resolution.</p>
<p dir="auto">With the latest update, Blueprints from Factorio before version 2.0 might not work correctly. You can import older blueprints to factorio and export them again to update them.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<ol dir="auto">
<li>Open Factorio and create a blueprint.</li>
<li>Export the blueprint as text (in the upper left corner, next to the "Delete" button).</li>
<li>Go the the <a href="https://piebro.github.io/factorio-blueprint-visualizer/" rel="nofollow">website</a>.</li>
<li>Click the "Upload Blueprint" button and paste the text into the text area.</li>
<li>Now you can test new random drawing settings (using the buttons or the arrow keys) or edit the current drawing settings.</li>
</ol>
<p dir="auto">Documentation of the drawing settings can be found <a href="https://github.com/piebro/factorio-blueprint-visualizer/blob/master/drawing_settings_documentation.md">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/piebro/factorio-blueprint-visualizer/blob/master/examples/random_0077.svg"><img src="https://github.com/piebro/factorio-blueprint-visualizer/raw/master/examples/random_0077.svg" width="47%"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/piebro/factorio-blueprint-visualizer/blob/master/examples/example_06.svg"><img src="https://github.com/piebro/factorio-blueprint-visualizer/raw/master/examples/example_06.svg" width="47%"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/piebro/factorio-blueprint-visualizer/blob/master/examples/example_12.svg"><img src="https://github.com/piebro/factorio-blueprint-visualizer/raw/master/examples/example_12.svg" width="47%"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/piebro/factorio-blueprint-visualizer/blob/master/examples/example_23.svg"><img src="https://github.com/piebro/factorio-blueprint-visualizer/raw/master/examples/example_23.svg" width="47%"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/piebro/factorio-blueprint-visualizer/blob/master/examples/example_24.svg"><img src="https://github.com/piebro/factorio-blueprint-visualizer/raw/master/examples/example_24.svg" width="47%"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/piebro/factorio-blueprint-visualizer/blob/master/examples/example_21.svg"><img src="https://github.com/piebro/factorio-blueprint-visualizer/raw/master/examples/example_21.svg" width="47%"></a>
</p>
<p dir="auto">The last three blueprints are by Josh Ventura and can be found <a href="https://factorioprints.com/user/6QrnfqXIffQcWgHC6Xs4uHv1BGg2" rel="nofollow">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Update [2025-01-08]</h2><a id="user-content-update-2025-01-08" aria-label="Permalink: Update [2025-01-08]" href="#update-2025-01-08"></a></p>
<ul dir="auto">
<li>Factorio 2.0 and Factorio: Space Age are supported (Blueprints from earlier version might only work partially)</li>
<li>Ported everything from Python to Javascript for simplicity and performance.</li>
<li>Added ability to modify drawing settings.</li>
<li>Added support for tiles.</li>
<li>Lots of quality-of-life improvements.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Ways to use this tool</h2><a id="user-content-ways-to-use-this-tool" aria-label="Permalink: Ways to use this tool" href="#ways-to-use-this-tool"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Text to Image</h3><a id="user-content-text-to-image" aria-label="Permalink: Text to Image" href="#text-to-image"></a></p>
<p dir="auto">I created a <a href="https://huggingface.co/datasets/piebro/factorio-blueprint-visualizations" rel="nofollow">dataset</a> with images generated using this tool to finetune <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0" rel="nofollow">SDXL</a> (a text-to-image neural network). The model with examples can be found here: <a href="https://huggingface.co/piebro/factorio-blueprint-visualizations-sdxl-lora" rel="nofollow">https://huggingface.co/piebro/factorio-blueprint-visualizations-sdxl-lora</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pen Plotting</h3><a id="user-content-pen-plotting" aria-label="Permalink: Pen Plotting" href="#pen-plotting"></a></p>
<p dir="auto">I have a pen plotter, and one of my initial ideas was also to be able to plot my factories. You can create visualizations you can easily draw. I recommend using <a href="https://github.com/abey79/vpype">https://github.com/abey79/vpype</a> for merging lines together before plotting. An example of a visualization for plotting is here:</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/piebro/factorio-blueprint-visualizer/blob/master/examples/example_25.svg"><img src="https://github.com/piebro/factorio-blueprint-visualizer/raw/master/examples/example_25.svg" width="70%"></a>
</p>
<p dir="auto">Another way to create plots from your factories is to use: <a href="https://github.com/drawscape-labs/factorio-cli">https://github.com/drawscape-labs/factorio-cli</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Factorio Tools to create blueprints</h2><a id="user-content-factorio-tools-to-create-blueprints" aria-label="Permalink: Factorio Tools to create blueprints" href="#factorio-tools-to-create-blueprints"></a></p>
<ul dir="auto">
<li><a href="https://github.com/R-O-C-K-E-T/Factorio-SAT">Factorio SAT</a> - Create optimal belt layouts with a SAT solver</li>
<li><a href="https://github.com/redcrafter/verilog2factorio">Factorio Verilog</a> - Convert Verilog code to factorio blueprints</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><a href="https://docs.astral.sh/uv/getting-started/installation/" rel="nofollow">uv</a> is used for linting and formatting the python code with <code>uv run ruff check --fix</code> and <code>uv run ruff format</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contribute</h2><a id="user-content-contribute" aria-label="Permalink: Contribute" href="#contribute"></a></p>
<p dir="auto">Contributions to this project are welcome. Feel free to report bugs or post ideas.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Statistics</h2><a id="user-content-statistics" aria-label="Permalink: Statistics" href="#statistics"></a></p>
<p dir="auto">There is lightweight tracking with <a href="https://plausible.io/about" rel="nofollow">Plausible</a> for the <a href="https://piebro.github.io/factorio-blueprint-visualizer/" rel="nofollow">website</a> to get infos about how many people are visiting. Everyone who is interested can look at these stats here: <a href="https://plausible.io/piebro.github.io%2Ffactorio-blueprint-visualizer?period=all" rel="nofollow">https://plausible.io/piebro.github.io%2Ffactorio-blueprint-visualizer?period=all</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VLC tops 6B downloads, previews AI-generated subtitles (155 pts)]]></title>
            <link>https://techcrunch.com/2025/01/09/vlc-tops-6-billion-downloads-previews-ai-generated-subtitles/</link>
            <guid>42644015</guid>
            <pubDate>Thu, 09 Jan 2025 10:56:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/01/09/vlc-tops-6-billion-downloads-previews-ai-generated-subtitles/">https://techcrunch.com/2025/01/09/vlc-tops-6-billion-downloads-previews-ai-generated-subtitles/</a>, See on <a href="https://news.ycombinator.com/item?id=42644015">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<div>
			<div>
<p id="speakable-summary">VLC media player, the popular open-source software developed by nonprofit VideoLAN, has topped 6 billion downloads worldwide and teased an AI-powered subtitle system.</p>

<p>The new feature automatically generates real-time subtitles — which can then also be translated in many languages — for any video using open-source AI models that run locally on users’ devices, eliminating the need for internet connectivity or cloud services, VideoLAN demoed at CES.</p>







<p>The firm didn’t say when it planned to rollout the feature.</p>

<p>VideoLAN started in 1996 as a project by students at Ecole Centrale Paris trying to stream videos across campus. Unlike many open-source projects that have struggled to survive on donations alone, VLC has maintained its free, ad-free model while expanding across multiple operating systems. The media player continues to operate without advertising, data collection, or commercial revenue streams.</p>

<p>“The number of active users of VLC is actually growing, even in this age of streaming services,” VideoLAN president Jean-Baptiste Kempf <a href="https://www.linkedin.com/feed/update/urn:li:activity:7282533937812258816/" target="_blank" rel="noreferrer noopener nofollow">wrote</a> in a LinkedIn post.</p>

<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">VLC automatic subtitles generation and translation based on local and open source AI models running on your machine working offline, and supporting numerous languages!<br>Demo can be found on our <a rel="nofollow" href="https://twitter.com/hashtag/CES2025?src=hash&amp;ref_src=twsrc%5Etfw">#CES2025</a> booth in Eureka Park. <a rel="nofollow" href="https://t.co/UVmgT6K4ds">pic.twitter.com/UVmgT6K4ds</a></p>— VideoLAN (@videolan) <a rel="nofollow" href="https://twitter.com/videolan/status/1877072497146781946?ref_src=twsrc%5Etfw">January 8, 2025</a></blockquote>
</div></figure>
</div>

			

			


			
			
			

			




			
			
			

			



			
<div>
	
	
	
	

	
<div>
	<p>
		Manish Singh is a senior reporter at TechCrunch, covering India’s startup scene and venture capital investments. He also reports on global tech firms’ India play. Before joining TechCrunch in 2019, Singh wrote for about a dozen publications, including CNBC and VentureBeat. He graduated in Computer Science and Engineering in 2015. He is reachable on manish(at)techcrunch(dot)com.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/manish-singh/" data-event="button" href="https://techcrunch.com/author/manish-singh/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div>


			


		</div>
		

		
		<div id="wp-block-techcrunch-most-popular-posts__heading">
<h2 id="h-most-popular">Most Popular</h2>

</div>
		
	</div><div>
		<div>
	<div>
		<div>
			<h3>Newsletters</h3>
			
		</div>
		<p>Subscribe for the industry’s biggest tech news</p>
	</div>
	<form method="POST" action="/">
		
	</form>
	
</div>


		
		<h2>Related</h2>
		

		
		
		

		
		<div>

<h2>Latest in Media &amp; Entertainment</h2>




</div>
		

	</div></div>]]></description>
        </item>
    </channel>
</rss>