<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 12 Oct 2024 09:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Google is preparing to let you run Linux apps on Android, just like Chrome OS (117 pts)]]></title>
            <link>https://www.androidauthority.com/android-linux-terminal-app-3489887/</link>
            <guid>41816756</guid>
            <pubDate>Sat, 12 Oct 2024 05:49:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.androidauthority.com/android-linux-terminal-app-3489887/">https://www.androidauthority.com/android-linux-terminal-app-3489887/</a>, See on <a href="https://news.ycombinator.com/item?id=41816756">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><picture><source sizes="(min-width: 64rem) 51.25rem, 80vw" srcset="https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone.jpg.webp 1919w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-1536w-864h.jpg.webp 1536w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-675w-380h.jpg.webp 675w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-64w-36h.jpg.webp 64w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-1000w-562h.jpg.webp 1000w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-300w-170h.jpg.webp 300w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-1280w-720h.jpg.webp 1280w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-840w-472h.jpg.webp 840w" type="image/webp"><img decoding="async" loading="eager" sizes="(min-width: 64rem) 51.25rem, 80vw" title="Linux Terminal app on a Google Pixel 9" srcset="https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone.jpg 1919w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-1536w-864h.jpg 1536w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-675w-380h.jpg 675w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-64w-36h.jpg 64w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-1000w-562h.jpg 1000w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-300w-170h.jpg 300w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-1280w-720h.jpg 1280w, https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone-840w-472h.jpg 840w" alt="Linux Terminal app on a Google Pixel 9" src="https://www.androidauthority.com/wp-content/uploads/2024/10/Terminal_app_on_Android_phone.jpg"></picture><div><p>Mishaal Rahman / Android Authority</p></div></div><div data-container-type="content"><p>TL;DR</p>
<ul>
<li>Google is developing a Linux terminal app for Android.</li>
<li>The Terminal app can be enabled via developer options and will install Debian in a virtual machine.</li>
<li>This app is likely intended for Chromebooks but might also be available for mobile devices, too.</li>
</ul>
</div><p>Although a lot of work can be done in a web browser these days, some tasks still require installing apps that aren’t available on <a href="https://www.androidauthority.com/what-is-chrome-os-1137371/">Chrome OS</a>, which is why Google lets you install <a href="https://www.androidauthority.com/linux-on-chromebook-1139944/">Linux apps on your Chromebook</a>. While there are ways to run some Linux apps on Android devices, all of those methods have some limitations and aren’t officially supported by Google. Fortunately, though, Google is finally working on an official way to run Linux apps on Android.</p><p>You're reading an&nbsp;<b data-stringify-type="bold">Authority Insights</b>&nbsp;story. Discover&nbsp;<a href="https://www.androidauthority.com/tag/authority-insights/" rel="noopener noreferrer" data-stringify-link="https://www.androidauthority.com/tag/authority-insights/" data-sk="tooltip_parent">Authority Insights</a><b data-stringify-type="bold">&nbsp;</b>for more exclusive reports, app teardowns, leaks, and in-depth tech coverage you won’t find anywhere else.</p><p>Engineers at Google started work on a new Terminal app for Android a couple of weeks ago. This Terminal app is part of the Android Virtualization Framework (AVF) and contains a WebView that connects to a Linux virtual machine via a local IP address, allowing you to run Linux commands from the Android host. Initially, you had to manually enable this Terminal app using a shell command and then configure the Linux VM yourself. However, in recent days, Google began work on integrating the Terminal app into Android as well as turning it into an all-in-one app for running a Linux distro in a VM.</p><p>A set of patches under the tag “<code>ferrochrome-dev-option</code>” was recently submitted to <a href="https://www.androidauthority.com/aosp-explained-1093505/">AOSP</a> that adds a new developer option called <strong>Linux terminal</strong> under <strong>Settings &gt; System &gt; Developer options</strong>. This new option will enable a “Linux terminal app that runs inside the VM,” according to its proposed description. Toggling this option enables the Terminal app that’s bundled with AVF.</p><div><picture><source sizes="(min-width: 64rem) 51.25rem, 80vw" srcset="https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app.jpeg.webp 1116w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-879w-1440h.jpeg.webp 879w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-659w-1080h.jpeg.webp 659w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-527w-864h.jpeg.webp 527w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-232w-380h.jpeg.webp 232w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-39w-64h.jpeg.webp 39w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-1000w-1639h.jpeg.webp 1000w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-675w-1106h.jpeg.webp 675w" type="image/webp"><img decoding="async" loading="lazy" sizes="(min-width: 64rem) 51.25rem, 80vw" title="Android Linux Terminal app" srcset="https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app.jpeg 1116w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-879w-1440h.jpeg 879w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-659w-1080h.jpeg 659w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-527w-864h.jpeg 527w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-232w-380h.jpeg 232w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-39w-64h.jpeg 39w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-1000w-1639h.jpeg 1000w, https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app-675w-1106h.jpeg 675w" alt="Android Linux Terminal app" src="https://www.androidauthority.com/wp-content/uploads/2024/10/Android_Linux_Terminal_app.jpeg"></picture></div><p>Currently, Android’s Terminal app still requires you to manually configure the Linux VM by providing a Debian image and creating a <code>vm_config.json</code> file, but Google plans to upgrade the Terminal app to take care of that for you. In one of the patches under the “<code>ferrochrome-dev-option</code>” tag, Google says that the existing “LinuxInstaller” app, which downloads and configures Debian to run in a VM through AVF, will be “merged to [the] terminal app soon.” This suggests that the Terminal app will become an all-in-one app that downloads, configures, runs, and interfaces with an instance of Debian running in a VM.</p><div><picture><source sizes="(min-width: 64rem) 51.25rem, 80vw" srcset="https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_Terminal_app_to_developer_options.jpg.webp 1497w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_Terminal_app_to_developer_options-675w-203h.jpg.webp 675w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_Terminal_app_to_developer_options-64w-19h.jpg.webp 64w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_Terminal_app_to_developer_options-1000w-301h.jpg.webp 1000w" type="image/webp"><img decoding="async" loading="lazy" sizes="(min-width: 64rem) 51.25rem, 80vw" title="AOSP patch to add Terminal app to developer options" srcset="https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_Terminal_app_to_developer_options.jpg 1497w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_Terminal_app_to_developer_options-675w-203h.jpg 675w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_Terminal_app_to_developer_options-64w-19h.jpg 64w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_Terminal_app_to_developer_options-1000w-301h.jpg 1000w" alt="AOSP patch to add Terminal app to developer options" src="https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_Terminal_app_to_developer_options.jpg"></picture><div><p>Mishaal Rahman / Android Authority</p></div></div><p>Google is still working on improving the Terminal app as well as AVF before shipping this feature. AVF already supports graphics and some input options, but it’s preparing to add support for backing up and restoring snapshots, nested virtualization, and devices with an x86_64 architecture. It’s also preparing to add some settings pages to the Terminal app, which is pretty barebones right now apart from a menu to copy the IP address and stop the existing VM instance. The settings pages will let you resize the disk, configure port forwarding, and potentially recover partitions.</p><div><picture><source sizes="(min-width: 64rem) 51.25rem, 80vw" srcset="https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_settings_to_Terminal_app.jpg.webp 1531w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_settings_to_Terminal_app-675w-224h.jpg.webp 675w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_settings_to_Terminal_app-64w-21h.jpg.webp 64w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_settings_to_Terminal_app-1000w-331h.jpg.webp 1000w" type="image/webp"><img decoding="async" loading="lazy" sizes="(min-width: 64rem) 51.25rem, 80vw" title="AOSP patch to add settings to Terminal app" srcset="https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_settings_to_Terminal_app.jpg 1531w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_settings_to_Terminal_app-675w-224h.jpg 675w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_settings_to_Terminal_app-64w-21h.jpg 64w, https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_settings_to_Terminal_app-1000w-331h.jpg 1000w" alt="AOSP patch to add settings to Terminal app" src="https://www.androidauthority.com/wp-content/uploads/2024/10/AOSP_patch_to_add_settings_to_Terminal_app.jpg"></picture><div><p>Mishaal Rahman / Android Authority</p></div></div><p>What’s particularly interesting about the patch that adds these settings is that it was tested on “tangorpro” and “komodo,” the codenames for the Pixel Tablet and Pixel 9 Pro XL respectively. This suggests that the Terminal app won’t be limited to Chromebooks like the new <a href="https://www.androidauthority.com/desktop-chrome-android-extensions-3488455/">desktop versions of Chrome for Android</a>. I don’t know when the Terminal app will land, but it’s possible we could see it arrive in next year’s <a href="https://www.androidauthority.com/android-16-features-3484159/">Android 16</a> update.</p><p>Here’s an early look at how this could work, courtesy of an anonymous developer. What you see below features the current version of the Terminal app, which lacks some of the automations we’re expecting. That’s why you’ll see LinuxInstaller involved at the beginning here, to help set things up:</p><p>If you’re wondering why you’d want to run Linux apps on Android, then this feature is probably not for you. Google added Linux support to Chrome OS so developers with Chromebooks can run Linux apps that are useful for development. For example, Linux support on Chrome OS allows developers to run the Linux version of Android Studio, the recommended IDE for Android app development, on Chromebooks. It also lets them run Linux command line tools safely and securely in a container.</p><p>Most Chromebooks have x86-based processors, as opposed to most Android devices which have ARM-based ones, so a lot of Linux apps that work on most Chromebooks (like Android Studio) might not work on most Android devices. Still, adding support for Linux apps will greatly improve the utility of Android to developers and make it more viable as a desktop-class platform in the future, especially as more and more apps add support for ARM. Plus, it’s necessary for Google to do as <a href="https://www.androidauthority.com/chrome-os-adopts-android-stack-3451177/">Chrome OS is becoming more like Android</a>, which means it’s adopting more and more of Android’s architecture and frameworks.</p><p><strong>Got a tip? Talk to us!</strong>&nbsp;Email our staff at <a href="mailto:news@androidauthority.com" rel="noopener noreferrer" data-stringify-link="mailto:tips@androidauthority.com" data-sk="tooltip_parent" aria-haspopup="menu">news@androidauthority.com</a>. You can stay anonymous or get credit for the info, it's your choice.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My WordPress Slack Ban (116 pts)]]></title>
            <link>https://linuxjedi.co.uk/my-wordpress-slack-ban/</link>
            <guid>41815614</guid>
            <pubDate>Sat, 12 Oct 2024 01:37:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linuxjedi.co.uk/my-wordpress-slack-ban/">https://linuxjedi.co.uk/my-wordpress-slack-ban/</a>, See on <a href="https://news.ycombinator.com/item?id=41815614">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A couple of days ago, I got banned from the WordPress community Slack. As this is about WordPress, I figure I should use my WordPress blog to talk about it.</p><h2>The Checkbox</h2><p>Everything that day revolved around a small, mandatory checkbox which had been added to the wordpress.org sign-in page, declaring that you are not affiliated in any way. This login is pretty much the portal to contribute to the WordPress ecosystem.</p><figure><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzOTciIGhlaWdodD0iNTM3IiB2aWV3Qm94PSIwIDAgMzk3IDUzNyI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" decoding="async" width="397" height="537" data-src="https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_072042.png" alt="" data-srcset="https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_072042.png 397w, https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_072042-222x300.png 222w" data-sizes="(max-width: 397px) 100vw, 397px"></figure><p>I, and several others, immediately had questions as to how broad this was and the legal ramifications. This is important to me because my work on WordPress represents the MariaDB Foundation, my login uses my Foundation email address. Not only do I not want to be tied-up in legal issues, but I do not want to be responsible for dragging the MariaDB Foundation into them.</p><p>It is also very difficult for me to know if the MariaDB Foundation has a loose affiliation with WP Engine. Sure, I know they aren’t a direct sponsor, but they may be financially tied to one of our sponsors. As I am paid by the MariaDB Foundation, this could financially tie me.</p><p>I needed clarification on this before I could proceed. Others asking questions have clients who host on WP Engine, or use paid-for plugins created by WP Engine. They also needed similar clarification.</p><h2>The Discussion</h2><p>The discussion happened on Slack, and a majority of it that I’m aware about can be seen in the Twitter thread below.</p><figure></figure><p>It continued on a little bit afterwards. Matt emphasised that the value of the checkbox is not stored, and that many people had logged in since the checkbox was added. I pointed out that although the value may not be directly stored, by his own admission, it has been logged who has signed in since the mandatory checkbox was added, so it is effectively stored.</p><p>Shortly after this, my Slack session reset to another instance, and I was confused for a minute as to what happened. I then realised I had been kicked and banned from the server. I attempted to log in to Slack today just to confirm it was still in place. The system sent me this email when I tried.</p><figure><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMDI0IiBoZWlnaHQ9IjUyOSIgdmlld0JveD0iMCAwIDEwMjQgNTI5Ij48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBzdHlsZT0iZmlsbDojY2ZkNGRiO2ZpbGwtb3BhY2l0eTogMC4xOyIvPjwvc3ZnPg==" decoding="async" width="1024" height="529" data-src="https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_062231-1024x529.png" alt="" data-srcset="https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_062231-1024x529.png 1024w, https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_062231-300x155.png 300w, https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_062231-768x397.png 768w, https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_062231-1536x794.png 1536w, https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_062231.png 1879w" data-sizes="(max-width: 1024px) 100vw, 1024px"></figure><h2>My Thoughts</h2><p>I don’t think we had a resolution to our questions, and the checkbox remains in place. Several people have stated that they will not sign in until there has been some proper legal clarification. WP Engine said this:</p><figure><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI1ODciIGhlaWdodD0iMjA2IiB2aWV3Qm94PSIwIDAgNTg3IDIwNiI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" decoding="async" width="587" height="206" data-src="https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_073923.png" alt="" data-srcset="https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_073923.png 587w, https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_073923-300x105.png 300w" data-sizes="(max-width: 587px) 100vw, 587px"></figure><p>I’m not fully convinced that this protects everyone. The WordPress Twitter account responded with:</p><figure><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI1OTIiIGhlaWdodD0iNjkzIiB2aWV3Qm94PSIwIDAgNTkyIDY5MyI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" decoding="async" width="592" height="693" data-src="https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_073937.png" alt="" data-srcset="https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_073937.png 592w, https://linuxjedi.co.uk/wp-content/uploads/2024/10/Screenshot_20241011_073937-256x300.png 256w" data-sizes="(max-width: 592px) 100vw, 592px"></figure><p>I have to say, I love that reader context is still a thing, I’m very surprised the current owner of Twitter did not kill it.</p><p>The MariaDB Foundation is a non-profit, any legal representation would be prohibitively expensive for us. So, I’d rather err on the side of not logging in until it is clear I am safe.</p><h2>Does This Matter?</h2><p>I am a small fry in this, not very important. My contributions mostly go as far as helping out with WordPress database questions, and often via other communication methods. I meet a lot of people face to face in the community and advise on how things could be better, things like that. My personal ban is not that important.</p><p>Several people who were banned at the same time are more important to the community. Having their voices taken away in this way is more damaging. I guess this is the start of the fallout from going nuclear that was talked about at the beginning of the fight.</p><p>I think clarification matters, this Tweet from Anita, quoting Uncle Chuck, says perfectly what I was thinking at the time:</p><figure><div><blockquote data-width="500" data-dnt="true"><div lang="en" dir="ltr"><p>Having coffee with Uncle Chuck (CGPT) 😂☕️ and he said:</p><p>You raise some valid points about the ambiguity of the checkbox. If people are asking Matt Mullenweg for clarification, it indicates a broader concern within the WordPress community about the implications of this statement,…</p></div>— Anita (@TheCre8tiveDiva) <a href="https://twitter.com/TheCre8tiveDiva/status/1843997862943666607?ref_src=twsrc%5Etfw">October 9, 2024</a></blockquote> </div></figure><h2>Wider Thoughts</h2><p>I think I speak for many in the WordPress community / ecosystem when I say that we don’t want to take sides in this battle. We don’t want to be forced to take sides via a checkbox. We just want to get work done, to improve WordPress for everyone.</p><p>The WordPress community is one of the most welcoming communities I’ve ever been a part of. It has brought friendship, meaning and value to many people. I want to see it succeed.</p><p>One of the things I have said since the beginning of me becoming Chief Contributions Officer is that contributions aren’t just code. Documentation, events, translations, community building, testing, bug reports, just so many things make up important contributions in an open source project. You don’t have to be a coder to contribute. I have seen these values expressed more in the WordPress community than anywhere else.</p><p>I cannot take a side in this battle, for several reasons. Not least because I have friends employed by both sides of it. But I would love for the collateral damage to stop. So that the community can heal.</p><p>I’ve said publicly, I would much rather my ban be permanent for asking the difficult questions and restore the voice to others who are more important. I’m not sure how I can help the community, but in some small way, I will be there for it.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's Turin: 5th Gen EPYC Launched (172 pts)]]></title>
            <link>https://chipsandcheese.com/p/amds-turin-5th-gen-epyc-launched</link>
            <guid>41815268</guid>
            <pubDate>Sat, 12 Oct 2024 00:22:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/amds-turin-5th-gen-epyc-launched">https://chipsandcheese.com/p/amds-turin-5th-gen-epyc-launched</a>, See on <a href="https://news.ycombinator.com/item?id=41815268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Hello you fine Internet folks, for today we have a video and an article for y’all.</p><p><span>Unlike our prior Granite Rapids coverage where we just had a video, we have had hands-on with Turin, specifically the AMD EPYC 9575F, thanks to </span><a href="https://x.com/SoundTechTen" rel="">Jordan</a><span> from </span><a href="https://www.storagereview.com/" rel="">StorageReview</a><span>. </span></p><div id="youtube2-O_NRYy59eEA" data-attrs="{&quot;videoId&quot;:&quot;O_NRYy59eEA&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/O_NRYy59eEA?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><span>This article is going to be a little different from our usual. It’s going to be shorter than usual because we have already covered the Zen 5 core both</span><a href="https://chipsandcheese.com/p/amds-strix-point-zen-5-hits-mobile" rel=""> in mobile</a><span> and </span><a href="https://chipsandcheese.com/p/amds-ryzen-9950x-zen-5-on-desktop" rel="">in desktop</a><span> and the </span><a href="https://chipsandcheese.com/p/zen-5-variants-and-more-clock-for-clock" rel="">differences between them</a><span>, so this article will be focused on the memory subsystem changes that Turin has.</span></p><p><a href="https://www.servethehome.com/amd-epyc-9005-turin-turns-transcendent-performance-solidigm-broadcom/" rel="">Serve the Home</a><span> has an excellent article that has the slides that AMD has put out for the launch of Turin. But because we have our own data, I thought that our data would be more interesting to dive into.</span></p><p>First, looking at the 1T results, we see that the 9575F can pull around 52 GB/s of memory read bandwidth, 48 GB/s of memory write bandwidth, and 95 GB/s of memory add (Read-Modify-Write) bandwidth. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png" width="1055" height="475" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:475,&quot;width&quot;:1055,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:33342,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ff6b56-81ca-4188-8c3c-b0cc24070f37_1055x475.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>And looking at the results for how much memory bandwidth a single CCD can get, we can see that a single core can use just under half the total CCD memory read bandwidth, about 55% the total CCD memory write bandwidth, and over two-thirds the total CCD memory add bandwidth.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png" width="1055" height="475" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:475,&quot;width&quot;:1055,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:36216,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b5957b-1996-474a-8a54-006dac3b1969_1055x475.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Looking a bit closer at these results, you’ll notice that the 9575F has significantly higher bandwidth to a CCD compared to the desktop Zen 5 parts. And the reason for this is the 9575F has GMI3-W which means that it has 2 GMI links to the IO die instead of the single GMI link that the 9950X gets.</p><p>And this is not only the only change to the GMI links on server. The GMI write link is now 32B per GMI link instead of the 16B per GMI link that you’d see on desktop Zen 5.</p><p>Before moving to the full socket memory performance for the 9575F, let’s clear up the memory speeds that Turin supports. Turin has 12 channels of memory that can run up to DDR5-6400MT/s, however 6400MT/s is only supported on specific validated systems and only for 1 DIMM per channel. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png" width="675" height="724" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:724,&quot;width&quot;:675,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:275928,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d1550a-b7b7-4888-bb36-bf54901e2823_675x724.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The system we had access to was running 6000MT/s for its memory, and DDR5-6000 MT/s is what most systems will support in a 1 DIMM per channel configuration. Should you want to run 2 DIMMs per channel, then your memory speeds drop to 4400 MT/s; and if you run 1 DIMM per channel in a motherboard with 2 DIMMs per channel then expect 5200 MT/s for your memory speed.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png" width="1055" height="475" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:475,&quot;width&quot;:1055,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:35864,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8964ab8-f489-4ac3-83d4-543a4155fd91_1055x475.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Now, actually diving into the memory of the full 9575F and we see that we can get nearly 99% of the theoretical 576 GB/s of memory bandwidth using reads. Writes and adds are still an impressive 435 GB/s and 453 GB/s respectively.</p><p>We also tested the socket to socket bandwidth on AMD’s Volcano Platform which only has 3 GMI links between the two 9575Fs. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png" width="983" height="495" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:495,&quot;width&quot;:983,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:35062,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F144c313e-973d-4bc1-b22f-a3e30072c9f0_983x495.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>These results are very similar to our </span><a href="https://chipsandcheese.com/p/testing-amds-bergamo-zen-4c-spam" rel="">Bergamo</a><span> results, which isn’t surprising because that system also had the same 3 GMI link setup.</span></p><p>Moving to memory latency, we see that Turin’s unloaded memory latency is very similar to Genoa’s unloaded memory latency.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66403d22-bb80-4098-8ed7-131b51be6520_983x493.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66403d22-bb80-4098-8ed7-131b51be6520_983x493.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66403d22-bb80-4098-8ed7-131b51be6520_983x493.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66403d22-bb80-4098-8ed7-131b51be6520_983x493.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66403d22-bb80-4098-8ed7-131b51be6520_983x493.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66403d22-bb80-4098-8ed7-131b51be6520_983x493.png" width="983" height="493" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/66403d22-bb80-4098-8ed7-131b51be6520_983x493.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:493,&quot;width&quot;:983,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:30653,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66403d22-bb80-4098-8ed7-131b51be6520_983x493.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66403d22-bb80-4098-8ed7-131b51be6520_983x493.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66403d22-bb80-4098-8ed7-131b51be6520_983x493.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66403d22-bb80-4098-8ed7-131b51be6520_983x493.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>At Hot Chips 2024, </span><a href="https://chipsandcheese.com/p/ampereone-at-hot-chips-2024-maximizing-density" rel="">Ampere Computing showed a graph</a><span> demonstrating the loaded memory latency of an AmpereOne chip and AMD’s Genoa CPU. Now Chester wanted to make a test similar to this, so he made a loaded latency test.</span></p><p>The way that this test works is that it runs our memory bandwidth benchmark on either 7 cores on a CCD or 7 CCDs on the 9575F. This ensures that the IOD to CCD link or the whole memory system is fully loaded. Then, with the 8th core or 8th CCD, we run the memory latency test to see what the latency of the fully loaded system is.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png" width="501" height="295" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:295,&quot;width&quot;:501,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:16435,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdecee13-5587-41d0-a173-f6ebe5ee3989_501x295.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>When a single CCD is loaded up on the 9575F, we see about a 39 nanosecond increase between the unloaded and loaded latency.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5340197-b348-4754-8796-68593252063a_500x296.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5340197-b348-4754-8796-68593252063a_500x296.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5340197-b348-4754-8796-68593252063a_500x296.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5340197-b348-4754-8796-68593252063a_500x296.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5340197-b348-4754-8796-68593252063a_500x296.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5340197-b348-4754-8796-68593252063a_500x296.png" width="500" height="296" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d5340197-b348-4754-8796-68593252063a_500x296.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:296,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:17479,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5340197-b348-4754-8796-68593252063a_500x296.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5340197-b348-4754-8796-68593252063a_500x296.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5340197-b348-4754-8796-68593252063a_500x296.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5340197-b348-4754-8796-68593252063a_500x296.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>When the whole system is loaded, we see about a 31 nanosecond increase between unloaded and loaded system latency.</p><p>Looking at the single CCD results compared to the fully loaded system results, the 9575F has very similar memory latency behavior regardless if a single CCD is loaded or if the whole system is loaded.</p><p>And lastly, everyone’s favorite graph, the core to core latency graph.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png" width="1200" height="602.4725274725274" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:731,&quot;width&quot;:1456,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:14446108,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf5acc-e03e-463a-a95e-84d48a7b9e1a_10281x5161.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>For convenience, the list below are the numbers from the chart because a chart this big can be hard to read. </p><ul><li><p>Intra-CCD latency: ~45ns</p></li><li><p>Inter-CCD latency: ~150ns</p></li><li><p>Socket to Socket latency: ~260ns</p></li></ul><p>This is a latency increase compared to Genoa, especially within a CCD.</p><p>Now, a note about the clock speeds we saw with the 9575F. All 64 cores could hit up to 5GHz in single threaded test. This is quite impressive, but we were able to get all 8 cores in a CCD to run at 5GHz in our memory bandwidth testing.</p><p><span>And with all 128 threads chugging away on Cinebench 2024, we saw the 9575F sticking around the 4.3GHz range. Wendell from </span><a href="https://www.youtube.com/@Level1Techs" rel="">Level1Techs</a><span> saw about 4.9GHz all core on a web server/TLS transaction workload, which is a less vectorized workload.</span></p><p>Realistically, AMD’s Turin is the generational update you’d normally expect. Not only does AMD have high core count SKUs (9755, 9965), which the hyperscalers will be picking up, they now also have lower core count, very high frequency SKUs (9575F) which the traditional enterprise market will appreciate. Apparently we now think 64 cores is ‘lower core count’. What a world we live in.</p><p>Turin isn’t the step-function revolution that Naples to Rome was; it’s more akin to the evolution we saw with Milan to Genoa, which was a memory bandwidth increase, a core increase, and a core update. Nonetheless, this generation is set to excite a lot of people, as there’s lots of value here in a very competitive ecosystem.</p><p><span>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;</span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span>&nbsp;or our&nbsp;</span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;</span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An exoskeleton let a paralyzed man walk, then its maker refused repairs (167 pts)]]></title>
            <link>https://www.washingtonpost.com/nation/2024/10/08/exoskeleton-paralyzed-repairs-michael-straight/</link>
            <guid>41813720</guid>
            <pubDate>Fri, 11 Oct 2024 21:03:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/nation/2024/10/08/exoskeleton-paralyzed-repairs-michael-straight/">https://www.washingtonpost.com/nation/2024/10/08/exoskeleton-paralyzed-repairs-michael-straight/</a>, See on <a href="https://news.ycombinator.com/item?id=41813720">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="76RRI2FS7VDNBBISNMCU7RSPBY" data-el="text" dir="null">An exoskeleton gave Michael Straight the ability to walk again after a horse racing accident left him a paraplegic. Over the course of 10 years, Straight walked more than a half-million steps while paralyzed, helping to pioneer a field.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="SCJKE5QQZ5DUHOCL77RKMO6FOA" data-el="text" dir="null">But in June, his machine stopped working, and the manufacturer refused to repair it. For the first time in a decade, Straight couldn’t walk.</p></div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="7QVFCJ4MVRDGJJKYCDEJEQ7ZEQ" data-el="text" dir="null">“It was like being paralyzed all over again,” he said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="3V624CTCSRF4REDUQ2SOJKXBVQ" data-el="text" dir="null">Straight, 38, said he spent three months pleading with exoskeleton manufacturer Lifeward, which last year changed its name from ReWalk Robotics, to replace a tiny component that connected the battery to a watch that controls his exoskeleton. Repeatedly, company employees told him they were no longer doing maintenance on machines more than five years old and then connected him with phone lines where he left voicemail messages that were never returned.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="E56SDYBYORCFHOSCLAYE7GWJUA" data-el="text" dir="null">Calling Straight “a real pioneer,” Lifeward CEO Larry Jasinski apologized Friday for the company’s response to the maintenance requests. He said that the company is no longer repairing exoskeletons older than five years because the Food and Drug Administration, which regulates the devices, has approved their usage for that timespan.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="LIBVQBWS4JENLGR4657FTJOHKU" data-el="text" dir="null">But, he added, when he realized that repairing Straight’s exoskeleton was minor and would not have a material impact on the machine’s structure, he overnighted him a replacement watch.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="FFJOX5WPMFDT3PKS37UENOLSWE" data-el="text" dir="null">“We didn’t do this one perfectly, and I’m sorry for that,” Jasinski said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="NIDQKWZQRVHB3IGCJLU6GD5T2Y" data-el="text" dir="null">Straight’s path to paralysis started in the 1990s at the Saratoga Race Course, one of the most famous thoroughbred horse racetracks in the country. His grandparents took Straight and his twin brother to the track in Saratoga Springs, N.Y., for the first time when they were about 7 years old, the start of a childhood surrounded by horse racing. The boys were obsessed with becoming jockeys.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="OBG4OCX5VJBHBPSMYLLGBO26OI" data-el="text" dir="null">“We were around it our whole life, so it’s definitely something we always dreamed about doing,” he said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="KBDDCGV5WZCTNBEDUWUVHFJ4JQ" data-el="text" dir="null">In the mid-2000s, Straight and his brother got a chance when they were accepted to the inaugural class of the North American Racing Academy, the first-ever jockey school in the United States. In March 2009, Straight parlayed that training into instant success, riding to a long-shot victory at Tampa Bay Downs in his first race.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="EYUPZXMTKZBX5DMBU7WXVRR5A4" data-el="text" dir="null">Then, on Aug. 26, 2009, he was riding I’m No Gentleman at Arlington Park in the Chicago suburbs when tragedy struck. Because of a brain injury, Straight can’t remember anything about the race that paralyzed him. But video shows that Straight’s horse’s front hoofs clipped the back legs of the mount in front of them. The two crashed to the track heads-first.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="PO2WCH2HTJG3PEBXLJJQBCV7ZQ" data-el="text" dir="null">The accident broke the horse’s neck, forcing authorities to euthanize it. For Straight, the fall caused bleeding on the brain and broke two vertebrae in his upper spine, paralyzing him from the chest down.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="MY5LZDAOOJELRM2SA5X6IZJVHA" data-el="text" dir="null">Recovery was slow. He says he remembers nothing about the weeks after his accident. But he spent seven weeks in the hospital, which included multiple surgeries. During inpatient rehabilitation, staff had him recite the alphabet and count out numbers to test the severity of his brain injury. They prodded him to build letters into words and words into sentences.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="U5BO6SFUNNB73ITF3OQRU57COU" data-el="text" dir="null">At one point, they strapped him to a bed and lifted it so that Straight was in a vertical position. After a minute, he felt lightheaded and dizzy.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="P4SFNPHOQ5GODD4KCLMGV6LPW4" data-el="text" dir="null">“That was the first time I was like, ‘Wow, this is going to be a long, long road to recovery.’”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="FPOF2TLFINGQJDBMEWTQC7IAUM" data-el="text" dir="null">But even though he knew it would be hard, Straight said he wanted walking to be part of that road. He asked the first doctor he met at rehab about the chances of walking again. The doctor told him he would be honest with him: 2 percent. As the doctor left, Straight turned to his mom and dad and said, “Two?” Then he turned toward the departing doctor’s back and held up his middle fingers.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="WIXJDWWZMRG3RDP7LZQOUUANUU" data-el="text" dir="null">“I’ll show him 2 percent,” he remembered saying.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="XBZULZ75NJCLZCJUDOF2RCHQWY" data-el="text" dir="null">Straight had settled into paralyzed life by 2012 when he was watching a TV show in which a character used a suit to stand and walk. At first, Straight dismissed it as the fictional embellishment of TV writers. But when he followed up with some research, he discovered there were organizations in the nascent stages. One of them was the University of Miami’s Project to Cure Paralysis, which was working with ReWalk Robotics’s exoskeleton.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="MAHOZNT55FA2TL7IRGBM2SSP2M" data-el="text" dir="null">Straight was instantly sold.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="UYSOITNZMZC6TKUGE3FV3B5PUI" data-el="text" dir="null">“I wanted it so bad,” he said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="AXTGYBUTEFADJPHXNDLG6EOL4Y" data-el="text" dir="null">The horse-racing community helped raise the money for the exoskeleton and, in 2014, he moved from Kentucky to South Florida so he could work with the Miami Project to Cure Paralysis. That summer, he was strapped into the exoskeleton for the first time and took his first post-paralysis steps. He was hooked.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="JXK47IB57FDBNBPACSMCM4LGGE" data-el="text" dir="null">But it was one small step in what turned out to be a marathon. Straight spent the next several months working through a checklist of actions he needed to get clearance to use the device at home. They included tests of balance and strength, tasks he repeatedly failed, leading him to get stronger and more skilled.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="36TXYBONTBCJXB3EEH44VWAHSY" data-el="text" dir="null">Finally, in November 2015, he took his exoskeleton home.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="FOMQABJMLNCEHPQAJ4OULDV6IA" data-el="text" dir="null">For nearly a decade, he has used the exoskeleton two to three times a week, doing about a 1,000 steps per session. Over that time, he has logged more than a half-million steps. Walking in the exoskeleton has provided him with workouts and physical therapy. It helps regulate his bowel and bladder functions and stops his legs from spasming, a common ailment of paraplegics.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="4VT4Y2A4UFGWJK3SSXGKF5QQNI" data-el="text" dir="null">“It’s been a game changer,” he said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="WNXCT2BY7BCLNIIT3UOGSHIMAQ" data-el="text" dir="null">Then, in June, the watch he uses to control the machine stopped working, making it useless.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="C66LIFTOTVEX7AS7O45VHRLRMY" data-el="text" dir="null">At first, Straight figured it would be an easy fix. He’d had problems with the machine before, ones that were easily solved by ReWalk through over-the-phone instructions or an in-person visit from a technician. This time, however, ReWalk’s employees said they wouldn’t fix the exoskeleton because they were no longer working on machines older than five years.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="AWXMICXHCRFTHD747LCWT2LWZY" data-el="text" dir="null">Straight resorted to Plan B — fixing it himself. But, he told his contact at ReWalk, he just needed him to tell him where he could find the parts. The contact told him to search the internet. Straight went online and discovered the battery he needed. But he couldn’t find the piece that connected the battery to the watch.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="DZQSWZYM6RGU3MC3LYAWMEHNM4" data-el="text" dir="null">Straight said he called ReWalk five times in total and left messages four of those times. He said he called from his wife’s phone another three or four times after that. He never got a response.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="QUDVMKCG2VBOHDUZTB2VK72NWU" data-el="text" dir="null">So he contacted local TV station WTLV, which <a href="https://www.firstcoastnews.com/article/news/investigations/ask-anthony/paralyzed-horse-jockey-in-st-johns-county-again-help-from-ask-anthony-team/77-1564e447-85f7-45ef-bb61-e009524f4110" target="_blank">published an article</a> about Straight’s struggles. Lifeward responded quickly after that.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="F262527MCFBUPHRCWI26FLCIVY" data-el="text" dir="null">Jasinski said that even though Straight’s exoskeleton is working again, he encouraged him to buy one of Lifeward’s newer models. Patients no longer have to foot the entire bill like they did when Straight got his machine, he noted. Earlier this year, Medicare said it would start paying for 80 percent of exoskeletons, which at Lifeward cost about $100,000.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="FTGHN6HYAZDX3NC4BX77QZMQOI" data-el="text" dir="null">And although that coverage doesn’t extend to paraplegics who have injuries to their spine as high up as Straight, Jasinski said that, given his track record of walking in the exoskeleton, he’s confident Medicare would approve Lifeward’s claim.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="FI4E3KMLDRAHDO37WUSMYGGXKA" data-el="text" dir="null">“We would take that battle forward,” Jasinski said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="2WPZKMWTYNDQPIUPLXDULRCCEY" data-el="text" dir="null">Straight said that his recent experience has left him wary of Lifeward and more prone to look to another company if he decides to buy another device. For now, he said, he’s happy with the machine that’s let him walk a half-million steps more than doctors ever thought he would.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="CO57Y2G3PNEXFIYHMB6LTEG6II" data-el="text" dir="null">“I’m a paralyzed guy who stands every day, stands or walks every day,” he said. “That’s how I want to put my foot forward.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kagi Snaps (145 pts)]]></title>
            <link>https://help.kagi.com/kagi/features/snaps.html</link>
            <guid>41813676</guid>
            <pubDate>Fri, 11 Oct 2024 20:58:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://help.kagi.com/kagi/features/snaps.html">https://help.kagi.com/kagi/features/snaps.html</a>, See on <a href="https://news.ycombinator.com/item?id=41813676">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-6b87e69f=""><br><video src="https://help.kagi.com/assets/snaps.c5bb4b7c.mp4" width="720" type="video/mp4" autoplay="" muted="" loop="" playsinline="" disablepictureinpicture=""></video><p><strong>Snaps</strong> is an exclusive Kagi Search feature that allows you to easily limit search results to a specific website by using the <code>@</code> symbol followed by a short code for the site and then your search query.</p><h2 id="example" tabindex="-1">Example <a href="#example" aria-label="Permalink to &quot;Example&quot;">​</a></h2><p>Typing <code>@r headphones</code> will search for "headphones" but limit the results to reddit.com (<code>r</code> is the short code for Reddit). This allows you to quickly find relevant content on a specific site using Kagi's powerful index.</p><p>Its relative, <a href="https://help.kagi.com/kagi/features/bangs.html">Bangs</a> feature, invoked by using "!r headphones", would redirect the user to reddit's internal search.</p><h2 id="how-to-use-snaps" tabindex="-1">How to Use Snaps <a href="#how-to-use-snaps" aria-label="Permalink to &quot;How to Use Snaps&quot;">​</a></h2><ol><li>Type the <code>@</code> symbol in the search bar.</li><li>Enter the short code for the site you want to search. <ul><li>If you don’t know the short code, type a few letters of the site’s name and autosuggest options will appear.</li><li>Navigate these options using your arrow keys or mouse.</li></ul></li><li>Enter your search query.</li><li>Press enter or click the search button.</li></ol><h2 id="contributing-to-snaps" tabindex="-1">Contributing to Snaps <a href="#contributing-to-snaps" aria-label="Permalink to &quot;Contributing to Snaps&quot;">​</a></h2><p>The short codes used in Snaps are the same as those used in our <a href="https://help.kagi.com/kagi/features/bangs.html">Bangs</a> feature. The list of bangs is <a href="https://github.com/kagisearch/bangs" target="_blank" rel="noreferrer">open source</a>, so anyone can contribute to it. If you don't see a short code for a site you want to search, feel free to submit a pull request to add it to the list.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Working from home is powering productivity (447 pts)]]></title>
            <link>https://www.imf.org/en/Publications/fandd/issues/2024/09/working-from-home-is-powering-productivity-bloom</link>
            <guid>41813304</guid>
            <pubDate>Fri, 11 Oct 2024 20:18:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.imf.org/en/Publications/fandd/issues/2024/09/working-from-home-is-powering-productivity-bloom">https://www.imf.org/en/Publications/fandd/issues/2024/09/working-from-home-is-powering-productivity-bloom</a>, See on <a href="https://news.ycombinator.com/item?id=41813304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>                 
                    
                    <!-- DOWNLOAD PDF LINK -->
                    <p>
                        <a href="https://www.imf.org/-/media/Files/Publications/Fandd/Article/2024/09/Bloom.ashx">Download PDF <i></i></a>
                    </p>
                    <!-- END DOWNLOAD PDF LINK -->
<!-- BEGIN INTRO TEXT -->
<p>A fivefold increase in remote work since the pandemic could boost economic growth and bring wider benefits</p>

<!-- END INTRO TEXT --><p>Economics is famous for being the dismal science. Sadly, <a href="https://web.stanford.edu/~chadj/IdeaPF.pdf">recent work</a> highlighting the slowdown in productivity growth stretching back to the 1950s is no exception. But I take a more cheerful view because of the great productivity gains promised by the pandemic-induced jump in working from home.&nbsp;&nbsp;</p>
<p>Working from home (WFH) increased about tenfold following the outbreak of the pandemic and has settled in at about five times its prepandemic level (see Chart 1). This could counter slowing productivity and deliver a surge in economic growth over the next few decades. If AI yields additional output, the era of slow growth could be over.&nbsp;&nbsp;</p>
<p><img src="https://www.imf.org/-/media/Images/IMF/FANDD/Charts/2024/09/bloom-chart1-v3.ashx?w=950" alt="Bloom 1" data-featherlight="/-/media/Images/IMF/FANDD/Charts/2024/09/bloom-chart1-v3.ashx"></p>
<p>The decomposition of economic growth by Nobel laureate Robert Solow, one of the most famous economists of all time, guides my analysis. Solow’s <a href="https://www.jstor.org/stable/1926047">1957 classic paper</a> highlights how growth comes from both the increase in factor inputs like labor and capital and from raw productivity growth. I hang my&nbsp;analysis on his framework by highlighting in turn how each of these factors will promote faster growth.&nbsp;</p>
<h6>Labor</h6>
<p>The easiest way to see labor’s impact is the survey evidence from across the United States, Europe, and Asia that shows hybrid work is worth about an <a href="https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.37.4.23">8 percent increase in salary</a>. Hybrid work is the typical pattern for office workers, managers, and other professionals, involving usually two or three days a week away from the office. To understand why employees would consider this to be worth 8 percent of their salary, note that typical workers spend about 45 hours a week in the office, yet they spend close to another 8 hours a week commuting. So working from home three days a week saves them about five hours a week, about 10 percent of their total weekly work and commute time.&nbsp;&nbsp;</p>
<p>Most people really dislike commuting, and so place even greater value on this time savings. See, for example, <a href="https://www.science.org/doi/10.1126/science.1103572">another famous paper</a>, by the Nobel Prize winner <a href="https://www.imf.org/external/pubs/ft/fandd/2009/09/people.htm">Daniel Kahneman</a>. This research found that commuting is the most detested activity in the day, disliked even more than work itself. This makes it easy to understand why the average employee values working from home so much—with its ability to save hours of painful weekly commuting, alongside the flexibility of being able to live farther from work.&nbsp;&nbsp;</p>
<p>This value of working from home has a powerful impact on labor supply. In the global economy there are tens of millions of people who are on the edge of the workforce. So small changes in the attractiveness of work can bring many millions of them into employment. This marginal labor force includes those with childcare or eldercare responsibilities, those close to retirement, and some folks in rural areas.&nbsp;&nbsp;</p>
<p>One example of this WFH impact on labor supply is the <a href="https://fred.stlouisfed.org/series/LNU02074597">approximately 2 million more employees</a> with a disability who are working in the US following the pandemic. These increases in disability employment have occurred primarily in high-WFH occupations. Employees with a disability benefit in two ways: first, by avoiding long commutes and second, by the ability to control their work environment at home.&nbsp;&nbsp;</p>
<p>Another example is prime-age female employment in the US, which has risen about 2 percent faster than prime-age male employment since the pandemic. Women’s larger role in childcare could be driving this rise in female labor force participation via WFH, according to <a href="https://drive.google.com/file/d/1Z1J2GHZjqkWzRV5ygA02yzSemwkAmzne/view">recent research</a>.&nbsp;</p>
<p>Collectively these effects could increase labor supply by several percent.&nbsp;&nbsp;</p>
<p>Of course, this calculation takes the current population as given. In the longer run, WFH could also increase fertility rates. One story I’ve heard repeatedly from talking to hundreds of employees and managers is how working remotely makes it easier to parent. This is perhaps most salient in East Asia, where long workdays, punishing commutes, and intense parenting pressures have led to rapidly dropping fertility. If parents are able to work two or three days a week at home, particularly with flexible schedules that allow them to share parenting responsibilities, this could increase birth rates. <a href="http://www.wfhresearch.com/">Preliminary analysis based on US survey data</a> suggests perhaps 0.3 to 0.5 more desired children per couple when both work from home one day or more a week.&nbsp;</p>
<h6>Capital</h6>
<p>The beneficial impact of WFH on capital comes from the longer-term release of office space for other uses, like residential and retail. If employees are based at home two or three days a week, society needs less office space, and that space can be used for other activities. It also reduces commuting traffic, curbing the need for additional transportation infrastructure. More intensive use of our home capital—the space and equipment in our houses and apartments—can allow society to save on the use of transportation and office capital, which can be redeployed to other uses. In major city centers about half of the land is covered in office space, and given that office occupancy is now 50 percent below prepandemic levels, there is great potential for office space reduction.&nbsp;&nbsp;</p>
<p><a href="https://nbloom.people.stanford.edu/sites/g/files/sbiybj24291/files/media/file/supercommuters_final.pdf">Recent data on driving speeds</a> show that traffic is now moving about 2 or 3 miles per hour faster during the morning commute, which reduces the need for additional transportation infrastructure and saves the typical commuter a few minutes a day.&nbsp;&nbsp;</p>
<p>Over the longer term, allowing employees to work partially or fully remotely also opens up currently underused land for housing, effectively <a href="https://www.kansascityfed.org/research/economic-review/hybrid-working-commuting-time-and-the-coming-long-term-boom-in-home-construction/">increasing the usable land supply</a>. Many major cities are heavily congested because most employees do not want to live more than a one-hour commute from the center. If they are required at work only a couple of days a week, longer commutes become possible, opening up space farther outside city centers for housing use.&nbsp;</p>
<p>Collectively, these capital contributions could also raise output a few percent over the coming decades.&nbsp;</p>
<h6>Productivity</h6>
<p>Classic firm and individual micro studies <a href="https://www.nature.com/articles/s41586-024-07500-2">typically find</a> that hybrid work, the usual pattern for about 30 percent of the US, European, and Asian labor forces, has a roughly flat impact on productivity. WFH benefits workers by saving them from exhausting commutes and typically provides a quieter working environment. But by reducing time at the office, it can also reduce employees’ ability to learn, to innovate, and to communicate. These positive and negative effects roughly offset each other, generating no net productivity impact of hybrid WFH, research suggests.&nbsp;</p>
<p>The impact of <a href="https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.37.4.23">fully remote working</a>, which has been adopted by about 10 percent of employees, is highly dependent on how well it’s managed. Some studies that examined fully remote working during the early days of the pandemic found large negative impacts, potentially because of the chaos of the early lockdowns. Other studies found large positive impacts, typically in more self-directed activities, such as call center or data entry work with well-managed firms.&nbsp;&nbsp;</p>
<p>In summary, the impact of fully remote work is perhaps neutral, because firms tend to adopt it only when such work arrangements match the work activity—often tasks such as coding or IT support, carried out by trained employees in a managed environment. But while the micro productivity impacts on any individual firm may be neutral, the huge power of labor market inclusion means that the aggregate macro impact is likely to be positive.&nbsp;&nbsp;</p>
<p>To explain the benefits of labor market inclusion, consider that fully in-person jobs can be filled only by nearby employees. A human resources or information technology position in New York can, for example, be filled only by a local resident. Even if there are people in Bulgaria, Brazil, or Belize who would be a better fit, they cannot do the job if they are not there in person. But as soon as positions can be filled remotely, employers go from taking the best local employee to taking the best regional employee for hybrid and the best global employee for fully remote work.&nbsp;&nbsp;</p>
<p><a href="https://web.stanford.edu/~chadj/HHJK.pdf">Recent studies</a> of work discrimination and reallocation highlight how expanding labor markets to a wider pool of potential employees can have massive productivity benefits. Going from 10 to 10,000 qualified candidates for a position allows a far more productive match, particularly if AI can help screen applicants. Remote work enables global matching between employees and firms, boosting labor productivity.&nbsp;</p>
<p>An additional macro productivity benefit from working from home is its positive impact on pollution from transportation. The WFH surge has curbed commuting traffic volumes across the US and Europe by an estimated 10 percent. This has <a href="https://www.pnas.org/doi/10.1073/pnas.2304099120">reduced pollution</a>, particularly emissions of low-level heavy particulates. Health studies have linked pollution to cognitive and productivity damage. Lowering pollution not only improves our quality of life but can also increase growth.&nbsp;</p>
<h6>Positive feedback loop</h6>
<p>A positive feedback loop—from working from home to faster growth and back—boosts these impacts. A long history of market-size effects in economics highlights how firms strive to innovate to serve larger, more lucrative markets. When you go from 5 million to 50 million people working from home every day, major hardware and software companies, start-ups, and funders take notice. This leads to an acceleration of new technologies to serve those markets, improving their productivity and growth.&nbsp;&nbsp;</p>
<p>That feedback loop has already begun. The share of <a href="https://drive.google.com/file/d/1ydRlxKc1ss17WuvlJSqXDj2vd1F12KeW/view">new patent applications</a> at the US Patent and Trademark Office that repeatedly use “remote work,” “working from home,” or similar words was flat until 2020 but has started to rise (see Chart 2). This highlights the improvement in technologies. Better cameras, screens, and software and technologies such as augmented and virtual reality and holograms will increase the productivity of hybrid and remote work in the future. This will generate a positive feedback loop between growth and working from home.&nbsp;</p>
<p><img src="https://www.imf.org/-/media/Images/IMF/FANDD/Charts/2024/09/bloom-chart2-v3.ashx?w=950" alt="Bloom 2" data-featherlight="/-/media/Images/IMF/FANDD/Charts/2024/09/bloom-chart2-v3.ashx"></p>
<p>One critique of the boom in working from home is the damage to city centers. It’s true that retail spending has fallen in city centers, but this activity has relocated to the suburbs, and overall consumption expenditure has resumed its prepandemic trend. Perhaps more problematic is the large reduction in valuations of commercial office space. Although this represents a loss of valuation for investors in the office sector, the release of city center space for residential use will in the long run make downtown living more affordable. The cost of living in the city rose dramatically in the 1990s and 2000s, pricing many middle- and lower-income employees out of city centers. This is especially problematic as many of these workers provide essential services, such as firefighting, policing, teaching, health care, food, transportation, and other work that can only be done in person. Cutting the amount of space for office use in city centers and converting it to residential use would make housing more affordable for these essential workers.&nbsp;</p>
<p>The 2020 surge in working from home has helped offset the prepandemic productivity slowdown overall and is boosting present and future growth. Being an economist usually means balancing winners and losers. Analyzing changes in technology, trade, prices, and regulations usually has mixed effects, with large groups of winners and losers. When it comes to working from home, the winners massively outweigh the losers. Firms, employees, and society in general have all reaped huge benefits. In my lifetime as an economist I have never seen a change that is so broadly beneficial.&nbsp;&nbsp;</p>
<p>This leaves me in the unusual place of being an optimistic “dismal scientist.” But it’s a place I’m happy to be as I write this while working from home. </p>
                <div>
                    <h5>Podcast</h5>
                    

                            <p><a href="https://www.imf.org/en/News/Podcasts/All-Podcasts/2024/09/03/nicholas-bloom">
                                <img src="https://www.imf.org/-/media/Images/IMF/News/Podcasts/2024/800x545-nick-bloom-sndwv.ashx?h=545&amp;w=800&amp;la=en" alt="">
                            </a></p>
                            <p>
                                Working from home was not an option for most people before March 11, 2020, when work and home life suddenly collided. Stanford University's Nicholas Bloom was studying the potential impact of remote work long before the pandemic launched it into the mainstream and now has data to suggest businesses should stick to the hybrid working model.
                            </p>



                </div>

            <!-- BEGIN AUTHORS -->
            
            <!-- END AUTHORS -->
<!-- BEGIN FOOTNOTE -->
<p>Opinions expressed in articles and other materials are those of the authors; they do not necessarily reflect IMF policy.</p>
<!-- END FOOTNOTE -->
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Machines of loving grace: How AI could transform the world for the better (113 pts)]]></title>
            <link>https://darioamodei.com/machines-of-loving-grace</link>
            <guid>41813268</guid>
            <pubDate>Fri, 11 Oct 2024 20:15:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://darioamodei.com/machines-of-loving-grace">https://darioamodei.com/machines-of-loving-grace</a>, See on <a href="https://news.ycombinator.com/item?id=41813268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
            <h2>Machines of <br>Loving Grace<sup id="fnref:1"><a href="#fn:1">1</a></sup></h2>
            <h4>How AI Could Transform the World for the Better</h4>

            
            <p>I think and talk a lot about the risks of powerful AI. The company I’m the CEO of, Anthropic, does a lot
                of research on how to reduce these risks. Because of this, people sometimes draw the conclusion that I’m
                a pessimist or “doomer” who thinks AI will be mostly bad or dangerous. I don’t think that at all. In
                fact, one of my main reasons for focusing on risks is that they’re the only thing standing between us
                and what I see as a fundamentally positive future. <strong>I think that most people are underestimating
                    just how radical the upside of AI could be</strong>, just as I think most people are underestimating
                how bad the risks could be.</p>
            <p>In this essay I try to sketch out what that upside might look like—what a world with powerful AI might
                look like if everything goes <i>right</i>. Of course no one can know the future with any certainty or
                precision, and the effects of powerful AI are likely to be even more unpredictable than past
                technological changes, so all of this is unavoidably going to consist of guesses. But I am aiming for at
                least educated and useful guesses, which capture the flavor of what will happen even if most details end
                up being wrong. I’m including lots of details mainly because I think a concrete vision does more to
                advance discussion than a highly hedged and abstract one.</p>
            <p>First, however, I wanted to briefly explain why I and Anthropic haven’t talked that much about powerful
                AI’s upsides, and why we’ll probably continue, overall, to talk a lot about risks. In particular, I’ve
                made this choice out of a desire to:</p>
            <ul>
                <li><strong>Maximize leverage</strong>. The basic development of AI technology and many (not all) of its
                    benefits seems inevitable (unless the risks derail everything) and is fundamentally driven by
                    powerful market forces. On the other hand, the risks are not predetermined and our actions can
                    greatly change their likelihood.</li>
                <li><strong>Avoid perception of propaganda</strong>. AI companies talking about all the amazing benefits
                    of AI can come off like propagandists, or as if they’re attempting to distract from downsides. I
                    also think that as a matter of principle it’s bad for your soul to spend too much of your time
                    “talking your book”.</li>
                <li><strong>Avoid grandiosity</strong>. I am often turned off by the way many AI risk public figures
                    (not to mention AI company leaders) talk about the post-AGI world, as if it’s their mission to
                    single-handedly bring it about like a prophet leading their people to salvation. I think it’s
                    dangerous to view companies as unilaterally shaping the world, and dangerous to view practical
                    technological goals in essentially religious terms.</li>
                <li><strong>Avoid “sci-fi” baggage</strong>. Although I think most people underestimate the upside of
                    powerful AI, the small community of people who do discuss radical AI futures often does so in an
                    excessively “sci-fi” tone (featuring e.g. uploaded minds, space exploration, or general cyberpunk
                    vibes). I think this causes people to take the claims less seriously, and to imbue them with a sort
                    of unreality. To be clear, the issue isn’t whether the technologies described are possible or likely
                    (the main essay discusses this in granular detail)—it’s more that the “vibe” connotatively smuggles
                    in a bunch of cultural baggage and unstated assumptions about what kind of future is desirable, how
                    various societal issues will play out, etc. The result often ends up reading like a fantasy for a
                    narrow subculture, while being off-putting to most people.</li>
            </ul>
            <p>Yet despite all of the concerns above, I really do think it’s important to discuss what a good world with
                powerful AI could look like, while doing our best to avoid the above pitfalls. In fact I think it is
                critical to have a genuinely inspiring vision of the future, and not <i>just</i> a plan to fight fires.
                Many of the implications of powerful AI are adversarial or dangerous, but at the end of it all, there
                has to be something we’re fighting <i>for</i>, some positive-sum outcome where everyone is better off,
                something to rally people to rise above their squabbles and confront the challenges ahead. Fear is one
                kind of motivator, but it’s not enough: we need hope as well.</p>
            <p>The list of positive applications of powerful AI is extremely long (and includes robotics, manufacturing,
                energy, and much more), but I’m going to focus on a small number of areas that seem to me to have the
                greatest potential to directly improve the quality of human life. The five categories I am most excited
                about are:</p>
            <ol>
                <li>Biology and physical health</li>
                <li>Neuroscience and mental health</li>
                <li>Economic development and poverty</li>
                <li>Peace and governance</li>
                <li>Work and meaning</li>
            </ol>
            <p>My predictions are going to be radical as judged by most standards (other than sci-fi “singularity”
                visions<sup id="fnref:2"><a href="#fn:2">2</a></sup>), but I mean them earnestly
                and sincerely. Everything I’m saying could very easily be wrong (to repeat my point from above), but
                I’ve at least attempted to ground my views in a semi-analytical assessment of how much progress in
                various fields might speed up and what that might mean in practice. I am fortunate to have professional
                experience in <a href="https://scholar.google.com/citations?user=6-e-ZBEAAAAJ" target="_blank">both
                    biology and neuroscience</a>, and I am an informed amateur in the field of economic development, but
                I am sure I will get plenty of things wrong. One thing writing this essay has made me realize is that it
                would be valuable to bring together a group of domain experts (in biology, economics, international
                relations, and other areas) to write a much better and more informed version of what I’ve produced here.
                It’s probably best to view my efforts here as a starting prompt for that group.</p>
            <h2>Basic assumptions and framework</h2>
            <p>To make this whole essay more precise and grounded, it’s helpful to specify clearly what we mean by
                powerful AI (i.e. the threshold at which the 5-10 year clock starts counting), as well as laying out a
                framework for thinking about the effects of such AI once it’s present.</p>
            <p>What powerful AI (I dislike the term AGI)<sup id="fnref:3"><a href="#fn:3">3</a></sup> will look like, and when (or if) it will arrive, is a huge topic in
                itself. It’s one I’ve discussed publicly and could write a completely separate essay on (I probably will
                at some point). Obviously, many people are skeptical that powerful AI will be built soon and some are
                skeptical that it will ever be built at all. I think it could come as early as 2026, though there are
                also ways it could take much longer. But for the purposes of this essay, I’d like to put these issues
                aside, assume it will come reasonably soon, and focus on what happens in the 5-10 years after that. I
                also want to assume a definition of what such a system <i>will look like,</i> what its capabilities are
                and how it interacts, even though there is room for disagreement on this.</p>
            <p>By <i>powerful AI</i>, I have in mind an AI model—likely similar to today’s LLM’s in form, though it
                might be based on a different architecture, might involve several interacting models, and might be
                trained differently—with the following properties:</p>
            <ul>
                <li>In terms of pure intelligence<sup id="fnref:4"><a href="#fn:4">4</a></sup>, it
                    is smarter than a Nobel Prize winner across most relevant fields –
                    biology, programming, math, engineering, writing, etc. This means it can prove unsolved mathematical
                    theorems, write extremely good novels, write difficult codebases from scratch, etc.</li>
                <li>In addition to just being a “smart thing you talk to”, it has all the “interfaces” available to a
                    human working virtually, including text, audio, video, mouse and keyboard control, and internet
                    access. It can engage in any actions, communications, or remote operations enabled by this
                    interface, including taking actions on the internet, taking or giving directions to humans, ordering
                    materials, directing experiments, watching videos, making videos, and so on. It does all of these
                    tasks with, again, a skill exceeding that of the most capable humans in the world.</li>
                <li>It does not just passively answer questions; instead, it can be given tasks that take hours, days,
                    or weeks to complete, and then goes off and does those tasks autonomously, in the way a smart
                    employee would, asking for clarification as necessary.</li>
                <li>It does not have a physical embodiment (other than living on a computer screen), but it can control
                    existing physical tools, robots, or laboratory equipment through a computer; in theory it could even
                    design robots or equipment for itself to use.</li>
                <li>The resources used to train the model can be repurposed to <i>run</i> millions of instances of it
                    (this matches projected cluster sizes by ~2027), and the model can absorb information and generate
                    actions at roughly 10x-100x human speed<sup id="fnref:5"><a href="#fn:5">5</a></sup>. It may however be limited by the response time of the physical
                    world or of software it interacts with.</li>
                <li>Each of these million copies can act independently on unrelated tasks, or if needed can all work
                    together in the same way humans would collaborate, perhaps with different subpopulations fine-tuned
                    to be especially good at particular tasks.</li>
            </ul>
            <p>We could summarize this as a “country of geniuses in a datacenter”.</p>
            <p>Clearly such an entity would be capable of solving very difficult problems, very fast, but it is not
                trivial to figure out how fast. Two “extreme” positions both seem false to me. First, you might think
                that the world would be instantly transformed on the scale of seconds or days (“<a href="https://en.wikipedia.org/wiki/Technological_singularity#:~:text=The%20technological%20singularity%E2%80%94or%20simply,unforeseeable%20consequences%20for%20Human%20civilization." target="_blank">the Singularity</a>”), as superior intelligence builds on itself and solves every
                possible scientific, engineering, and operational task almost immediately. The problem with this is that
                there are real physical and practical limits, for example around building hardware or conducting
                biological experiments. Even a new country of geniuses would hit up against these limits. Intelligence
                may be very powerful, but it isn’t magic fairy dust.</p>
            <p>Second, and conversely, you might believe that technological progress is saturated or rate-limited by
                real world data or by social factors, and that better-than-human intelligence will add very little<sup id="fnref:6"><a href="#fn:6">6</a></sup>. This seems equally implausible to
                me—I can think of hundreds of scientific or even social problems where a large group of really smart
                people would drastically speed up progress, especially if they aren’t limited to analysis and can make
                things happen in the real world (which our postulated country of geniuses can, including by directing or
                assisting teams of humans).</p>
            <p>I think the truth is likely to be some messy admixture of these two extreme pictures, something that
                varies by task and field and is very subtle in its details. I believe we need new frameworks to think
                about these details in a productive way.</p>
            <p>Economists often talk about “factors of production”: things like labor, land, and capital. The phrase
                “marginal returns to labor/land/capital” captures the idea that in a given situation, a given factor may
                or may not be the limiting one – for example, an air force needs both planes and pilots, and hiring more
                pilots doesn’t help much if you’re out of planes. I believe that in the AI age, we should be talking
                about <i>the marginal returns to intelligence<sup id="fnref:7"><a href="#fn:7">7</a></sup></i>, and trying to figure out what the other factors are that are
                complementary to intelligence and that become limiting factors when intelligence is very high. We are
                not used to thinking in this way—to asking “how much does being smarter help with this task, and on what
                timescale?”—but it seems like the right way to conceptualize a world with very powerful AI.</p>
            <p>My guess at a list of factors that limit or are complementary to intelligence includes:</p>
            <ul>
                <li><strong>Speed of the outside world</strong>. Intelligent agents need to operate interactively in the
                    world in order to accomplish things and also to learn<sup id="fnref:8"><a href="#fn:8">8</a></sup>. But the world only moves so fast. Cells and animals run at a fixed
                    speed so experiments on them take a certain amount of time which may be irreducible. The same is
                    true of hardware, materials science, anything involving communicating with people, and even our
                    existing software infrastructure. Furthermore, in science many experiments are often needed in
                    sequence, each learning from or building on the last. All of this means that the speed at which a
                    major project—for example developing a cancer cure—can be completed may have an irreducible minimum
                    that cannot be decreased further even as intelligence continues to increase.</li>
                <li><strong>Need for data</strong>. Sometimes raw data is lacking and in its absence more intelligence
                    does not help. Today’s particle physicists are very ingenious and have developed a wide range of
                    theories, but lack the data to choose between them because particle accelerator data <a href="https://www.technologyreview.com/2024/02/20/1088002/higgs-boson-physics-particle-collider-large-hadron-collider/" target="_blank">is so limited</a>. It is not clear that they would do drastically better if they
                    were superintelligent—other than perhaps by speeding up the construction of a bigger accelerator.
                </li>
                <li><strong>Intrinsic complexity</strong>. Some things are inherently unpredictable or chaotic and even
                    the most powerful AI cannot predict or untangle them substantially better than a human or a computer
                    today. For example, even incredibly powerful AI could predict only marginally further ahead in a
                    chaotic system (such as the <a href="https://en.wikipedia.org/wiki/Three-body_problem" target="_blank">three-body problem</a>) in the general case,<sup id="fnref:9"><a href="#fn:9">9</a></sup> as compared to today’s humans and computers.
                </li>
                <li><strong>Constraints from humans</strong>. Many things cannot be done without breaking laws, harming
                    humans, or messing up society. An aligned AI would not want to do these things (and if we have an
                    unaligned AI, we’re back to talking about risks). Many human societal structures are inefficient or
                    even actively harmful, but are hard to change while respecting constraints like legal requirements
                    on clinical trials, people’s willingness to change their habits, or the behavior of governments.
                    Examples of advances that work well in a technical sense, but whose impact has been substantially
                    reduced by regulations or misplaced fears, include nuclear power, <a href="https://en.wikipedia.org/wiki/Concorde" target="_blank">supersonic flight</a>, and <a href="https://www.nytimes.com/2024/07/08/opinion/elevator-construction-regulation-labor-immigration.html" target="_blank">even elevators</a>.</li>
                <li><strong>Physical laws</strong>. This is a starker version of the first point. There are certain
                    physical laws that appear to be unbreakable. It’s not possible to travel faster than light. <a href="https://en.wikipedia.org/wiki/Arcadia_(play)" target="_blank">Pudding does not unstir</a>.
                    Chips can only have so many transistors per square centimeter <a href="https://en.wikipedia.org/wiki/Quantum_tunnelling" target="_blank">before they become
                        unreliable</a>. Computation requires a <a href="https://en.wikipedia.org/wiki/Landauer%27s_principle" target="_blank">certain minimum
                        energy per bit</a> erased, limiting the density of computation in the world.</li>
            </ul>
            <p>There is a further distinction based on <i>timescales</i>. Things that are hard constraints in the short
                run may become more malleable to intelligence in the long run. For example, intelligence might be used
                to develop a new experimental paradigm that allows us to learn <i>in vitro</i> what used to require live
                animal experiments, or to build the tools needed to collect new data (e.g. the bigger particle
                accelerator), or to (within ethical limits) find ways around human-based constraints (e.g. helping to
                improve the clinical trial system, helping to create new jurisdictions where clinical trials have less
                bureaucracy, or improving the science itself to make human clinical trials less necessary or cheaper).
            </p>
            <p>Thus, we should imagine a picture where intelligence is initially heavily bottlenecked by the other
                factors of production, but over time intelligence itself increasingly routes around the other factors,
                even if they never fully dissolve (and some things like physical laws are absolute)<sup id="fnref:10"><a href="#fn:10">10</a></sup>. The key question is how fast it all happens and
                in what order.</p>
            <p>With the above framework in mind, I’ll try to answer that question for the five areas mentioned in the
                introduction.</p>
            <h2>1. Biology and health</h2>
            <p>Biology is probably the area where scientific progress has the greatest potential to directly and
                unambiguously improve the quality of human life. In the last century some of the most ancient human
                afflictions (such as smallpox) have finally been vanquished, but many more still remain, and defeating
                them would be an enormous humanitarian accomplishment. Beyond even curing disease, biological science
                can in principle improve the <i>baseline</i> quality of human health, by extending the healthy human
                lifespan, increasing control and freedom over our own biological processes, and addressing everyday
                problems that we currently think of as immutable parts of the human condition.</p>
            <p>In the “limiting factors” language of the previous section, the main challenges with directly applying
                intelligence to biology are data, the speed of the physical world, and intrinsic complexity (in fact,
                all three are related to each other). Human constraints also play a role at a later stage, when clinical
                trials are involved. Let’s take these one by one.</p>
            <p>Experiments on cells, animals, and even chemical processes are limited by the speed of the physical
                world: many biological protocols involve culturing bacteria or other cells, or simply waiting for
                chemical reactions to occur, and this can sometimes take days or even weeks, with no obvious way to
                speed it up. Animal experiments can take months (or more) and human experiments often take years (or
                even decades for long-term outcome studies). Somewhat related to this, data is often lacking—not so much
                in quantity, but quality: there is always a dearth of clear, unambiguous data that isolates a biological
                effect of interest from the other 10,000 confounding things that are going on, or that intervenes
                causally in a given process, or that directly measures some effect (as opposed to inferring its
                consequences in some indirect or noisy way). Even massive, quantitative molecular data, like the
                proteomics data that I collected while working on mass spectrometry techniques, is noisy and misses a
                lot (which types of cells were these proteins in? Which part of the cell? At what phase in the cell
                cycle?).</p>
            <p>In part responsible for these problems with data is intrinsic complexity: if you’ve ever seen a <a href="https://today.ucsd.edu/story/international_consortium_builds_google_map_of_human_metabolism" target="_blank">diagram showing the biochemistry of human metabolism</a>, you’ll know that it’s very
                hard to isolate the effect of any part of this complex system, and even harder to intervene on the
                system in a precise or predictable way. And finally, beyond just the intrinsic time that it takes to run
                an experiment on humans, actual clinical trials involve a lot of bureaucracy and regulatory requirements
                that (in the opinion of many people, including me) add unnecessary additional time and delay progress.
            </p>
            <p>Given all this, many biologists have long been <a href="https://www.nature.com/articles/d41586-024-00306-2" target="_blank">skeptical</a> of the value
                of AI and “big data” more generally in biology. Historically, mathematicians, computer scientists, and
                physicists who have applied their skills to biology over the last 30 years have been quite successful,
                but have not had the truly transformative impact initially hoped for. Some of the skepticism has been
                reduced by major and revolutionary breakthroughs like <a href="https://alphafold.ebi.ac.uk/" target="_blank">AlphaFold</a> (which has just deservedly won its creators the <a href="https://www.nobelprize.org/prizes/chemistry/2024/summary/" target="_blank">Nobel Prize in
                    Chemistry</a>) and <a href="https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/" target="_blank">AlphaProteo</a><sup id="fnref:11"><a href="#fn:11">11</a></sup>, but there’s still a perception that AI is (and will continue to be)
                useful in only a limited set of circumstances. A common formulation is “AI can do a better job analyzing
                your data, but it can’t produce more data or improve the quality of the data. Garbage in, garbage out”.
            </p>
            <p>But I think that pessimistic perspective is thinking about AI in the wrong way. If our core hypothesis
                about AI progress is correct, then the right way to think of AI is not as a method of data analysis, but
                as a virtual biologist who performs <i>all</i> the tasks biologists do, including designing and running
                experiments in the real world (by controlling lab robots or simply telling humans which experiments to
                run – as a Principal Investigator would to their graduate students), inventing new biological methods or
                measurement
                techniques, and so on. It is by speeding up <i>the whole research process</i> that AI can truly
                accelerate biology. <strong>I want to repeat this because it’s the most common misconception that comes
                    up when I talk about AI’s ability to transform biology: I am <i>not</i> talking about AI as merely a
                    tool to analyze data. In line with the definition of powerful AI at the beginning of this essay, I’m
                    talking about using AI to perform, direct, and improve upon nearly everything biologists
                    do.</strong></p>
            <p>To get more specific on where I think acceleration is likely to come from, a surprisingly large fraction
                of the progress in biology has come from a truly tiny number of discoveries, often related to broad
                measurement tools or techniques<sup id="fnref:12"><a href="#fn:12">12</a></sup>
                that allow precise but generalized or programmable intervention in biological systems. There’s perhaps
                ~1 of these major discoveries per year and collectively they arguably drive &gt;50% of progress in biology.
                These discoveries are so powerful precisely because they cut through intrinsic complexity and data
                limitations, directly increasing our understanding and control over biological processes. A few
                discoveries per decade have enabled both the bulk of our basic scientific understanding of biology, and
                have driven many of the most powerful medical treatments.</p>
            <p>Some examples include:</p>
            <ul>
                <li><a href="https://en.wikipedia.org/wiki/CRISPR" target="_blank">CRISPR</a>: a technique that allows
                    live editing of any gene in living organisms (replacement of any arbitrary gene sequence with any
                    other arbitrary sequence). Since the original technique was developed, there have been <a href="https://www.nature.com/articles/s41581-022-00636-2" target="_blank">constant
                        improvements</a> to target specific cell types, increasing accuracy, and reducing edits of the
                    wrong gene—all of which are needed for safe use in humans.</li>
                <li>Various kinds of microscopy for watching what is going on at a precise level: advanced light
                    microscopes (with various kinds of fluorescent techniques, special optics, etc), electron
                    microscopes, atomic force microscopes, etc.</li>
                <li>Genome sequencing and synthesis, which has <a href="https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost" target="_blank">dropped in cost</a> by several orders of magnitude in the last couple decades.
                </li>
                <li><a href="https://en.wikipedia.org/wiki/Optogenetics#:~:text=Optogenetics%20is%20a%20biological%20technique,specifically%20in%20the%20target%20cells." target="_blank">Optogenetic</a> techniques that allow you to get a neuron to fire by shining a
                    light on it.</li>
                <li><a href="https://pubmed.ncbi.nlm.nih.gov/34433919/" target="_blank">mRNA vaccines</a> that, in
                    principle, allow us to design a vaccine against anything and then quickly adapt it (mRNA vaccines of
                    course became famous during COVID).</li>
                <li>Cell therapies such as <a href="https://en.wikipedia.org/wiki/CAR_T_cell" target="_blank">CAR-T</a>
                    that allow immune cells to be taken out of the body and “reprogrammed” to attack, in principle,
                    anything.</li>
                <li>Conceptual insights like the germ theory of disease or the realization of a link between the immune
                    system and cancer<sup id="fnref:13"><a href="#fn:13">13</a></sup>.</li>
            </ul>
            <p>I’m going to the trouble of listing all these technologies because I want to make a crucial claim about
                them: <strong>I think their rate of discovery could be increased by 10x or more if there were a lot more
                    talented, creative researchers</strong><i>.</i> Or, put another way, <strong>I think the returns to
                    intelligence are high for these discoveries</strong>, and that everything else in biology and
                medicine mostly follows from them.</p>
            <p>Why do I think this? Because of the answers to some questions that we should get in the habit of asking
                when we’re trying to determine “returns to intelligence”. First, these discoveries are generally made by
                a tiny number of researchers, often the same people repeatedly, suggesting skill and not random search
                (the latter might suggest lengthy experiments are the limiting factor). Second, they often “could have
                been made” years earlier than they were: for example, CRISPR was a naturally occurring component of the
                immune system in bacteria that’s been <a href="https://link.springer.com/article/10.1134/S1062360422040075" target="_blank">known since the
                    80’s</a>, but it took another 25 years for people to realize it could be repurposed for general gene
                editing. They also are often delayed many years by lack of support from the scientific community for
                promising directions (see <a href="https://www.vox.com/future-perfect/2023/10/5/23903292/katalin-kariko-drew-weissman-nobel-prize-medicine-mrna-vaccines-covid-coronavirus" target="_blank">this profile</a> on the inventor of mRNA vaccines; similar stories abound). Third,
                successful projects are often scrappy or were afterthoughts that people didn’t initially think were
                promising, rather than massively funded efforts. This suggests that it’s not just massive resource
                concentration that drives discoveries, but ingenuity.</p>
            <p>Finally, although some of these discoveries have “serial dependence” (you need to make discovery A first
                in order to have the tools or knowledge to make discovery B)—which again might create experimental
                delays—many, perhaps most, are independent, meaning many at once can be worked on in parallel. Both
                these facts, and my general experience as a biologist, strongly suggest to me that there are hundreds of
                these discoveries waiting to be made if scientists were smarter and better at making connections between
                the vast amount of biological knowledge humanity possesses (again consider the CRISPR example). The
                success of <a href="https://alphafold.ebi.ac.uk/" target="_blank">AlphaFold</a>/<a href="https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/" target="_blank">AlphaProteo</a> at solving important problems much more effectively than humans,
                despite decades of carefully designed physics modeling, provides a proof of principle (albeit with a
                narrow tool in a narrow domain) that should point the way forward.</p>
            <p>Thus, it’s my guess that powerful AI could at least 10x the rate of these discoveries, giving us the next
                50-100 years of biological progress in 5-10 years.<sup id="fnref:14"><a href="#fn:14">14</a></sup> Why not 100x? Perhaps it is possible, but here both serial dependence
                and experiment times become important: getting 100 years of progress in 1 year requires a lot of things
                to go right the first time, including animal experiments and things like designing microscopes or
                expensive lab facilities. I’m actually open to the (perhaps absurd-sounding) idea that we could get
                <i>1000</i> years of progress in 5-10 years, but very skeptical that we can get 100 years in 1 year.
                Another way to put it is I think there’s an unavoidable constant delay: experiments and hardware design
                have a certain “latency” and need to be iterated upon a certain “irreducible” number of times in order
                to learn things that can’t be deduced logically. But massive parallelism may be possible on top of
                that<sup id="fnref:15"><a href="#fn:15">15</a></sup>.
            </p>
            <p>What about clinical trials? Although there is a lot of bureaucracy and slowdown associated with them, the
                truth is that a lot (though by no means all!) of their slowness ultimately derives from the need to
                rigorously evaluate drugs that barely work or ambiguously work. This is sadly true of most therapies
                today: the average cancer drug increases survival by a few months while having significant side effects
                that need to be carefully measured (there’s a similar story for Alzheimer’s drugs). This leads to huge
                studies (in order to achieve statistical power) and difficult tradeoffs which regulatory agencies
                generally aren’t great at making, again because of bureaucracy and the complexity of competing
                interests.</p>
            <p>When something works really well, it goes much faster: there’s an accelerated approval track and the ease
                of approval is much greater when effect sizes are larger. mRNA vaccines for COVID were approved in 9
                months—much faster than the usual pace. That said, even under these conditions clinical trials are still
                too slow—mRNA vaccines arguably <a href="https://www.1daysooner.org/" target="_blank"><i>should</i> have
                    been approved in ~2 months</a>. But these kinds of delays (~1 year end-to-end for a drug) combined
                with massive parallelization and the need for some but not too much iteration (“a few tries”) are very
                compatible with radical transformation in 5-10 years. Even more optimistically, it is possible that <a href="https://www.sciencedirect.com/science/article/pii/S135964462400134X" target="_blank">AI-enabled biological science</a> will reduce the need for iteration in clinical
                trials by developing better animal and cell experimental models (or even simulations) that are more
                accurate in predicting what will happen in humans. This will be particularly important in developing
                drugs against the aging process, which plays out over decades and where we need a faster iteration loop.
            </p>
            <p>Finally, on the topic of clinical trials and societal barriers, it is worth pointing out explicitly that
                in some ways biomedical innovations have an unusually <i>strong</i> track record of being successfully
                deployed, in contrast to some other technologies<sup id="fnref:16"><a href="#fn:16">16</a></sup>. As mentioned in the introduction, many technologies are hampered by
                societal factors despite working well technically. This might suggest a pessimistic perspective on what
                AI can accomplish<i>.</i> But biomedicine is unique in that although the process of developing drugs is
                overly cumbersome, once developed they generally are successfully deployed and used.</p>
            <strong>To summarize the above, my basic prediction is that AI-enabled biology and medicine will allow us to
                compress the progress that human biologists would have achieved over the next 50-100 years into 5-10
                years. I’ll refer to this as the “compressed 21st century”: the idea that after powerful AI is
                developed, we will in a few years make all the progress in biology and medicine that we would have made
                in the whole 21st century.</strong>
            <p>Although predicting what powerful AI can do in a few years remains inherently difficult and
                speculative,
                there is some concreteness to asking “what could humans do unaided in the next 100 years?”. Simply
                looking at what we’ve accomplished in the 20th century, or extrapolating from the first 2 decades of
                the
                21st, or asking what “10 CRISPR’s and 50 CAR-T’s” would get us, all offer practical, grounded ways
                to
                estimate the general level of progress we might expect from powerful AI.</p>
            <p>Below I try to make a list of what we might expect. This is not based on any rigorous methodology,
                and
                will almost certainly prove wrong in the details, but it’s trying to get across the general
                <i>level</i>
                of radicalism we should expect:
            </p>
            <ul>
                <li><strong>Reliable prevention and treatment of nearly all</strong><sup id="fnref:17"><a href="#fn:17">17</a></sup> <strong>natural infectious
                        disease.</strong>
                    Given the enormous advances against infectious disease in the 20th century, it is not radical to
                    imagine that we could more or less “finish the job” in a compressed 21st. mRNA vaccines and
                    similar
                    technology already point the way towards “<a href="https://www.nih.gov/news-events/news-releases/clinical-trial-mrna-universal-influenza-vaccine-candidate-begins" target="_blank">vaccines for anything</a>”. Whether infectious disease is <i>fully
                        eradicated
                        from the world</i> (as opposed to just in some places) depends on questions about poverty
                    and
                    inequality, which are discussed in Section 3.</li>
                <li><strong>Elimination of most cancer</strong>. Death rates from cancer <a href="https://www.bmj.com/content/384/bmj-2023-076962" target="_blank">have been dropping
                        ~2%
                        per year</a> for the last few decades; thus we are on track to eliminate most cancer in the
                    21st
                    century at the current pace of human science. Some subtypes have already been largely cured (for
                    example some types of leukemia with <a href="https://www.statnews.com/2022/02/02/cart-cancer-therapy-leukemia-treatment/" target="_blank">CAR-T therapy</a>), and I’m perhaps even more excited for very selective
                    drugs
                    that target cancer in its infancy and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10080017/" target="_blank">prevent it</a>
                    from ever growing. AI will also make possible treatment regimens very <a href="https://www.sciencedirect.com/science/article/pii/S0305737220300578" target="_blank">finely adapted</a> to the individualized genome of the cancer—these are
                    possible
                    today, but hugely expensive in time and human expertise, which AI should allow us to scale.
                    Reductions of 95% or more in both mortality and incidence seem possible. That said, cancer is
                    extremely varied and adaptive, and is likely the hardest of these diseases to fully destroy. It
                    would not be surprising if an assortment of rare, difficult malignancies persists.</li>
                <li><strong>Very effective prevention and effective cures for genetic disease</strong>. Greatly
                    improved
                    <a href="https://www.nature.com/articles/s41591-022-01735-0" target="_blank">embryo
                        screening</a>
                    will likely make it possible to prevent most genetic disease, and some safer, more reliable
                    descendant of CRISPR may cure most genetic disease in existing people. Whole-body afflictions
                    that
                    affect a large fraction of cells may be the last holdouts, however.
                </li>
                <li><strong>Prevention of Alzheimer’s</strong>. We’ve had a very hard time figuring out what causes
                    Alzheimer’s (it is somehow related to beta-amyloid protein, but the actual details seem to be <a href="https://www.nature.com/articles/s41380-021-01249-0" target="_blank">very complex</a>).
                    It
                    seems like exactly the type of problem that can be solved with better measurement tools that
                    isolate
                    biological effects; thus I am bullish about AI’s ability to solve it. There is a good chance it
                    can
                    eventually be prevented with relatively simple interventions, once we actually understand what
                    is
                    going on. That said, damage from already-existing Alzheimer’s may be very difficult to reverse.
                </li>
                <li><strong>Improved treatment of most other ailments</strong>. This is a catch-all category for
                    other
                    ailments including diabetes, obesity, heart disease, autoimmune diseases, and more. Most of
                    these
                    seem “easier” to solve than cancer and Alzheimer’s and in many cases are already in steep
                    decline.
                    For example, deaths from heart disease have already declined over 50%, and simple interventions
                    like
                    <a href="https://www.google.com/url?q=https://onlinelibrary.wiley.com/doi/full/10.1002/edm2.462&amp;sa=D&amp;source=docs&amp;ust=1726506125482285&amp;usg=AOvVaw1t12gKr6YA4RNeWMnhLtU6" target="_blank">GLP-1 agonists</a> have already made huge progress against obesity and
                    diabetes.
                </li>
                <li><strong>Biological freedom</strong>. The last 70 years featured advances in birth control,
                    fertility, <a href="https://www.astralcodexten.com/p/why-does-ozempic-cure-all-diseases" target="_blank">management of weight</a>, and much more. But I suspect AI-accelerated
                    biology
                    will greatly expand what is possible: weight, physical appearance, reproduction, and other
                    biological processes will be fully under people’s control. We’ll refer to these under the
                    heading of
                    <i>biological freedom:</i> the idea that everyone should be empowered to choose what they want
                    to
                    become and live their lives in the way that most appeals to them. There will of course be
                    important
                    questions about global equality of access; see Section 3 for these.
                </li>
                <li><strong>Doubling of the human lifespan<sup id="fnref:18"><a href="#fn:18">18</a></sup>.</strong>This might seem radical,
                    but <a href="https://ourworldindata.org/life-expectancy" target="_blank">life expectancy increased
                        almost 2x</a> in the 20th century (from ~40 years to ~75), so it’s “on trend” that the
                    “compressed 21st” would double it again to 150. Obviously the interventions involved in slowing
                    the
                    actual aging process will be different from those that were needed in the last century to
                    prevent
                    (mostly childhood) premature deaths from disease, but the magnitude of change is not
                    unprecedented<sup id="fnref:19"><a href="#fn:18">19</a></sup>. Concretely,
                    there already <a href="https://www.nature.com/articles/s41586-024-07701-9" target="_blank">exist
                        drugs that increase maximum lifespan in rats by 25-50%</a> with limited ill-effects. And
                    some
                    animals (e.g. some types of turtle) already live 200 years, so humans are manifestly not at some
                    theoretical upper limit. At a guess, the most important thing that is needed might be reliable,
                    <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" target="_blank">non-Goodhart-able</a>
                    biomarkers of human aging, as that will allow fast iteration on experiments and clinical trials.
                    Once human lifespan is 150, we may be able to reach “escape velocity”, buying enough time that
                    most
                    of those currently alive today will be able to live as long as they want, although there’s
                    certainly
                    no guarantee this is biologically possible.
                </li>
            </ul>
            <p>It is worth looking at this list and reflecting on how different the world will be if all of it is
                achieved 7-12 years from now (which would be in line with an aggressive AI timeline). It goes
                without
                saying that it would be an unimaginable humanitarian triumph, the elimination all at once of most of
                the
                scourges that have haunted humanity for millennia. Many of my friends and colleagues are raising
                children, and when those children grow up, I hope that any mention of disease will sound to them the
                way
                scurvy, <a href="https://ourworldindata.org/smallpox" target="_blank">smallpox</a>, or bubonic
                plague
                sounds to us. That generation will also benefit from increased biological freedom and
                self-expression,
                and with luck may also be able to live as long as they want.</p>
            <p>It’s hard to overestimate how surprising these changes will be to everyone except the small community
                of
                people who expected powerful AI. For example, thousands of economists and policy experts in the US
                currently debate <a href="https://en.wikipedia.org/wiki/Social_Security_debate_in_the_United_States" target="_blank">how to keep Social Security</a> and Medicare solvent, and more broadly how to
                keep
                down the cost of healthcare (which is mostly consumed by those over 70 and especially those with
                terminal illnesses such as cancer). The situation for these programs is likely to be radically
                improved
                if all this comes to pass<sup id="fnref:20"><a href="#fn:20">20</a></sup>, as
                the
                ratio of working age to retired population will change drastically. No doubt these challenges will
                be
                replaced with others, such as how to ensure widespread access to the new technologies, but it is
                worth
                reflecting on how much the world will change even if biology is the <i>only</i> area to be
                successfully
                accelerated by AI.</p>
            <h2>2. Neuroscience and mind</h2>
            <p>In the previous section I focused on <i>physical</i> diseases and biology in general, and didn’t
                cover
                neuroscience or mental health. But neuroscience is a subdiscipline of biology and mental health is
                just
                as important as physical health. In fact, if anything, mental health affects human well-being even
                more
                directly than physical health. Hundreds of millions of people have very low quality of life due to
                problems like addiction, depression, schizophrenia, low-functioning autism, PTSD, psychopathy<sup id="fnref:21"><a href="#fn:21">21</a></sup>, or intellectual disabilities.
                Billions more struggle with everyday problems that can often be interpreted as much milder versions
                of
                one of these severe clinical disorders. And as with general biology, it may be possible to go beyond
                addressing problems to improving the baseline quality of human experience.</p>
            <p>The basic framework that I laid out for biology applies equally to neuroscience. The field is
                propelled
                forward by a small number of discoveries often related to tools for measurement or precise
                intervention
                – in the list of those above, optogenetics was a neuroscience discovery, and more recently <a href="https://en.wikipedia.org/wiki/CLARITY" target="_blank">CLARITY</a> and <a href="https://en.wikipedia.org/wiki/Expansion_microscopy#:~:text=Expansion%20microscopy%20(ExM)%20is%20a,them%20using%20a%20polymer%20system." target="_blank">expansion microscopy</a> are
                advances
                in the same vein, in addition to many of the general cell biology methods directly carrying over to
                neuroscience. I think the rate of these advances will be similarly accelerated by AI and therefore
                that
                the framework of “100 years of progress in 5-10 years” applies to neuroscience in the same way it
                does
                to biology and for the same reasons. As in biology, the progress in 20th century neuroscience was
                enormous – for example we didn’t even understand how or why neurons fired <a href="https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model" target="_blank">until the
                    1950’s</a>. Thus, it seems reasonable to expect AI-accelerated neuroscience to produce rapid
                progress over a few years.</p>
            <p>There is one thing we should add to this basic picture, which is that some of the things we’ve
                learned
                (or are learning) about AI itself in the last few years are likely to help advance neuroscience,
                even if
                it continues to be done only by humans. <a href="https://www.anthropic.com/research/mapping-mind-language-model" target="_blank">Interpretability</a> is an obvious example: although biological neurons
                superficially operate in a completely different manner from artificial neurons (they communicate via
                spikes and often spike rates, so there is a time element not present in artificial neurons, and a
                bunch
                of details relating to cell physiology and neurotransmitters modifies their operation
                substantially),
                the basic question of “how do distributed, trained networks of simple units that perform combined
                linear/non-linear operations work together to perform important computations” is the same, and I
                strongly suspect the details of individual neuron communication will be abstracted away in most of
                the
                interesting questions about computation and circuits<sup id="fnref:22"><a href="#fn:22">22</a></sup>. As just one example of this, a <a href="https://distill.pub/2020/circuits/frequency-edges/" target="_blank">computational
                    mechanism</a> discovered by interpretability researchers in AI systems was recently <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10055119/" target="_blank">rediscovered</a>
                in
                the brains of mice.</p>
            <p>It is much easier to do experiments on artificial neural networks than on real ones (the latter often
                requires cutting into animal brains), so interpretability may well become a tool for improving our
                understanding of neuroscience. Furthermore, powerful AI’s will themselves probably be able to
                develop
                and apply this tool better than humans can.</p>
            <p>Beyond just interpretability though, what we have learned from AI about how intelligent systems are
                <i>trained</i> should (though I am not sure it <i>has</i> yet) cause a revolution in neuroscience.
                When
                I was working in neuroscience, a lot of people focused on what I would now consider the wrong
                questions
                about learning, because the concept of the <a href="https://arxiv.org/abs/2001.08361" target="_blank">scaling hypothesis</a> / <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf" target="_blank" rel="noopener noreferrer">bitter lesson</a> didn’t
                exist yet. The idea that a simple objective function plus a lot of data can
                drive
                incredibly complex behaviors makes it more interesting to understand the objective functions and
                architectural biases and less interesting to understand the details of the emergent computations. I
                have
                not followed the field closely in recent years, but I have a vague sense that computational
                neuroscientists have still not fully absorbed the lesson. My attitude to the scaling hypothesis has
                always been “aha – this is an explanation, at a high level, of how intelligence works and how it so
                easily evolved”, but I don’t think that’s the average neuroscientist’s view, in part because the
                scaling
                hypothesis as “the secret to intelligence” isn’t fully accepted even within AI.
            </p>
            <p>I think that neuroscientists should be trying to combine this basic insight with the particularities
                of
                the human brain (biophysical limitations, evolutionary history, topology, details of motor and
                sensory
                inputs/outputs) to try to figure out some of neuroscience’s key puzzles. Some likely are, but I
                suspect
                it’s not enough yet, and that AI neuroscientists will be able to more effectively leverage this
                angle to
                accelerate progress.</p>
            <p>I expect AI to accelerate neuroscientific progress along four distinct routes, all of which can
                hopefully
                work together to cure mental illness and improve function:</p>
            <ul>
                <li><strong>Traditional molecular biology, chemistry, and genetics</strong>. This is essentially the
                    same story as general biology in section 1, and AI can likely speed it up via the same
                    mechanisms.
                    There are many drugs that modulate neurotransmitters in order to alter brain function, affect
                    alertness or perception, change mood, etc., and AI can <a href="https://www.science.org/doi/10.1126/sciadv.adn1524" target="_blank">help us invent</a>
                    many more. AI can probably also accelerate research on the genetic basis of mental illness.</li>
                <li><strong>Fine-grained neural measurement and intervention</strong>. This is the ability to
                    measure
                    what a lot of individual neurons or neuronal circuits are doing, and intervene to change their
                    behavior. Optogenetics and neural probes are technologies capable of both measurement and
                    intervention in live organisms, and a number of very advanced methods (such as molecular ticker
                    tapes to read out the firing patterns of large numbers of individual neurons) <a href="https://arxiv.org/abs/1306.5709" target="_blank">have also been proposed</a> and seem
                    possible in principle.</li>
                <li><strong>Advanced computational neuroscience</strong>. As noted above, both the specific insights
                    and
                    the <i>gestalt</i> of modern AI can probably be applied fruitfully to questions in <a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(23)00099-2" target="_blank">systems neuroscience</a>, including perhaps uncovering the real
                    causes
                    and dynamics of complex diseases like psychosis or mood disorders.</li>
                <li><strong>Behavioral interventions</strong>. I haven’t much mentioned it given the focus on the
                    biological side of neuroscience, but psychiatry and psychology have of course developed <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/wps.21203" target="_blank">a wide
                        repertoire of behavioral interventions</a> over the 20th century; it stands to reason that
                    AI
                    could accelerate these as well, both the development of new methods and helping patients to
                    adhere
                    to existing methods. More broadly, the idea of an “AI coach” who always helps you to be the best
                    version of yourself, who studies your interactions and helps you learn to be more effective,
                    seems
                    very promising.</li>
            </ul>
            <p>It’s my guess that these four routes of progress working together would, as with physical disease, be
                on
                track to lead to the cure or prevention of most mental illness in the next 100 years even if AI was
                not
                involved – and thus might reasonably be completed in 5-10 AI-accelerated years. Concretely my guess
                at
                what will happen is something like:</p>
            <ul>
                <li><strong>Most mental illness can probably be cured</strong>. I’m not an expert in psychiatric
                    disease
                    (my time in neuroscience was spent building probes to study small groups of neurons) but it’s my
                    guess that diseases like PTSD, depression, schizophrenia, addiction, etc. can be figured out and
                    very effectively treated via some combination of the four directions above. The answer is likely
                    to
                    be some combination of “something went wrong biochemically” (although it could be very complex)
                    and
                    “something went wrong with the neural network, at a high level”. That is, it’s a systems
                    neuroscience question—though that doesn’t gainsay the impact of the behavioral interventions
                    discussed above. Tools for measurement and intervention, especially in live humans, seem likely
                    to
                    lead to rapid iteration and progress.</li>
                <li><strong>Conditions that are very “structural” may be more difficult, but not
                        impossible</strong>.
                    There’s <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7016047/" target="_blank">some
                        evidence</a> that psychopathy is associated with obvious neuroanatomical differences – that
                    some
                    brain regions are simply smaller or less developed in psychopaths. Psychopaths are also believed
                    to
                    lack empathy from a young age; whatever is different about their brain, it was probably always
                    that
                    way. The same may be true of some intellectual disabilities, and perhaps other conditions.
                    Restructuring the brain sounds hard, but it also seems like a task with high returns to
                    intelligence. Perhaps there is some way to coax the adult brain into an earlier or more plastic
                    state where it can be reshaped. I’m very uncertain how possible this is, but my instinct is to
                    be
                    optimistic about what AI can invent here.</li>
                <li><strong>Effective genetic prevention of mental illness seems possible</strong>. Most mental
                    illness
                    is <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9840515/" target="_blank">partially
                        heritable</a>, and genome-wide association studies are <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/wps.21034" target="_blank">starting
                        to
                        gain traction</a> on identifying the relevant factors, which are often many in number. It
                    will
                    probably be possible to prevent most of these diseases via embryo screening, similar to the
                    story
                    with physical disease. One difference is that psychiatric disease is more likely to be polygenic
                    (many genes contribute), so due to complexity there’s an increased risk of unknowingly selecting
                    against <a href="https://en.wikipedia.org/wiki/Antagonistic_pleiotropy_hypothesis" target="_blank">positive traits that are correlated with disease</a>. Oddly however, in
                    recent
                    years GWAS studies seem to suggest that these <a href="https://academic.oup.com/humupd/advance-article/doi/10.1093/humupd/dmae012/7684172" target="_blank">correlations might have been overstated.</a> In any case, AI-accelerated
                    neuroscience may help us to figure these things out. Of course, embryo screening for complex
                    traits
                    raises a number of societal issues and will be controversial, though I would guess that most
                    people
                    would support screening for severe or debilitating mental illness.</li>
                <li><strong>Everyday problems that we don’t think of as clinical disease will also be
                        solved</strong>.
                    Most of us have everyday psychological problems that are not ordinarily thought of as rising to
                    the
                    level of clinical disease. Some people are quick to anger, others have trouble focusing or are
                    often
                    drowsy, some are fearful or anxious, or react badly to change. Today, drugs already exist to
                    help
                    with e.g. alertness or focus (caffeine, modafinil, ritalin) but as with many other previous
                    areas,
                    much more is likely to be possible. Probably many more such drugs exist and have not been
                    discovered, and there may also be totally new modalities of intervention, such as targeted light
                    stimulation (see optogenetics above) or magnetic fields. Given how many drugs we’ve developed in
                    the
                    20th century that tune cognitive function and emotional state, I’m very optimistic about the
                    “compressed 21st” where everyone can get their brain to behave a bit better and have a more
                    fulfilling day-to-day experience.</li>
                <li><strong>Human baseline experience can be much better</strong>. Taking one step further, many
                    people
                    have experienced extraordinary moments of revelation, creative inspiration, compassion,
                    fulfillment,
                    transcendence, love, beauty, or meditative peace. The character and frequency of these
                    experiences
                    differs greatly from person to person and within the same person at different times, and can
                    also
                    sometimes be triggered by various drugs (though often with side effects). All of this suggests
                    that
                    the “space of what is possible to experience” is very broad and that a larger fraction of
                    people’s
                    lives could consist of these extraordinary moments. It is probably also possible to improve
                    various
                    cognitive functions across the board. This is perhaps the neuroscience version of “biological
                    freedom” or “extended lifespans”.</li>
            </ul>
            <p>One topic that often comes up in sci-fi depictions of AI, but that I intentionally haven’t discussed
                here, is “mind uploading”, the idea of capturing the pattern and dynamics of a human brain and
                instantiating them in software. This topic could be the subject of an essay all by itself, but
                suffice
                it to say that while I think uploading is almost certainly <a href="https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf" target="_blank">possible</a>
                in
                principle, in practice it faces significant technological and societal challenges, even with
                powerful
                AI, that likely put it outside the 5-10 year window we are discussing.</p>
            <p>In summary, AI-accelerated neuroscience is likely to vastly improve treatments for, or even cure,
                most
                mental illness as well as greatly expand “cognitive and mental freedom” and human cognitive and
                emotional abilities. It will be every bit as radical as the improvements in physical health
                described in
                the previous section. Perhaps the world will not be visibly different on the outside, but the world
                as
                experienced by humans will be a much better and more humane place, as well as a place that offers
                greater opportunities for self-actualization. I also suspect that improved mental health will
                ameliorate
                a lot of other societal problems, including ones that seem political or economic.</p>
            <h2>3. Economic development and poverty</h2>
            <p>The previous two sections are about <i>developing</i> new technologies that cure disease and improve
                the
                quality of human life. However an obvious question, from a humanitarian perspective, is: “will
                everyone
                have access to these technologies?”</p>
            <p>It is one thing to develop a cure for a disease, it is another thing to eradicate the disease from
                the
                world. More broadly, many existing health interventions have not yet been applied everywhere in the
                world, and for that matter the same is true of (non-health) technological improvements in general.
                Another way to say this is that living standards in many parts of the world are still desperately
                poor:
                <a href="https://data.worldbank.org/indicator/NY.GDP.PCAP.CD" target="_blank">GDP per capita</a> is
                ~$2,000 in Sub-Saharan Africa as compared to ~$75,000 in the United States. If AI further increases
                economic growth and quality of life in the developed world, while doing little to help the
                developing
                world, we should view that as a terrible moral failure and a blemish on the genuine humanitarian
                victories in the previous two sections. Ideally, powerful AI should help the developing world
                <i>catch
                    up to</i> the developed world, even as it revolutionizes the latter.
            </p>
            <p>I am not as confident that AI can address inequality and economic growth as I am that it can invent
                fundamental technologies, because technology has such obvious high returns to intelligence
                (including
                the ability to route around complexities and lack of data) whereas the economy involves a lot of
                constraints from humans, as well as a large dose of intrinsic complexity. I am somewhat skeptical
                that
                an AI could solve the famous “<a href="https://en.wikipedia.org/wiki/Socialist_calculation_debate#:~:text=The%20socialist%20calculation%20debate%2C%20sometimes,of%20the%20means%20of%20production." target="_blank">socialist calculation problem</a>”<sup id="fnref:23"><a href="#fn:23">23</a></sup> and I don’t think governments will (or should) turn over their
                economic policy to such an entity, even if it could do so. There are also problems like how to
                convince
                people to take treatments that are effective but that they may be suspicious of.</p>
            <p>The challenges facing the developing world are made even more complicated by <a href="https://economics.mit.edu/sites/default/files/publications/120224%20Corruption%20Review%20Final.pdf" target="_blank">pervasive corruption</a> in both
                private and public sectors. Corruption creates a vicious cycle: it <a href="https://pdf.usaid.gov/pdf_docs/pnacw645.pdf" target="_blank">exacerbates poverty</a>, and
                poverty in
                turn breeds more corruption. AI-driven plans for economic development need to reckon with corruption,
                weak institutions, and other very human challenges. </p>
            <p>Nevertheless, I do see significant reasons for optimism. Diseases <i>have</i> been eradicated and many
                countries
                <i>have</i> gone from poor to rich, and it is clear that the decisions involved in these tasks exhibit
                high
                returns to intelligence (despite human constraints and complexity). Therefore, AI can likely do them
                better than they are currently being done. There may also be targeted interventions that get around the
                human constraints and that AI could focus on. More importantly though, <i>we have</i> to try. Both AI
                companies
                and developed world policymakers will need to do their part to ensure that the developing world is not
                left out; the moral imperative is too great. So in this section, I’ll continue to make the optimistic
                case, but keep in mind everywhere that success is not guaranteed and depends on our collective efforts.
            </p>
            <p>Below I make some guesses about how I think things may go in the developing world over the 5-10 years
                after powerful AI is developed:</p>
            <ul>
                <li><strong>Distribution of health interventions</strong>. The area where I am perhaps most
                    optimistic
                    is distributing health interventions throughout the world. Diseases have actually been
                    eradicated by
                    top-down campaigns: smallpox was <a href="https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-smallpox-vaccination" target="_blank">fully eliminated</a> in the 1970’s, and polio and guinea worm are nearly
                    eradicated with less than 100 cases per year. <a href="https://en.wikipedia.org/wiki/Institute_for_Disease_Modeling" target="_blank">Mathematically sophisticated epidemiological modeling</a> plays an active
                    role
                    in disease eradication campaigns, and it seems very likely that there is room for
                    smarter-than-human
                    AI systems to do a better job of it than humans are. The logistics of distribution can probably
                    also
                    be greatly optimized. One thing I learned as an early donor to <a href="https://www.givewell.org/" target="_blank">GiveWell</a> is that some health charities
                    are way more effective than others;
                    the hope is that AI-accelerated efforts would be more effective still. Additionally, some
                    biological
                    advances actually make the logistics of distribution much easier: for example, malaria has been
                    difficult to eradicate because it requires treatment each time the disease is contracted; a
                    vaccine
                    that only needs to be administered once makes the logistics much simpler (and such vaccines for
                    malaria <a href="https://en.wikipedia.org/wiki/Malaria_vaccine" target="_blank">are in fact
                        currently being developed</a>). Even simpler distribution mechanisms are possible: some
                    diseases
                    could in principle be eradicated by targeting their animal carriers, for example releasing
                    mosquitoes infected with a bacterium that <a href="https://www.gavi.org/vaccineswork/south-american-cities-release-mosquitoes-stem-disease" target="_blank">blocks their ability</a> to carry a disease (who then infect all the other
                    mosquitos) or simply using <a href="https://www.nature.com/articles/s41576-021-00386-0" target="_blank">gene drives</a> to wipe out the mosquitos. This requires one or a few
                    centralized actions, rather than a coordinated campaign that must individually treat millions.
                    Overall, I think 5-10 years is a reasonable timeline for a good fraction (maybe 50%) of
                    AI-driven
                    health benefits to propagate to even the poorest countries in the world. A good goal might be
                    for
                    the developing world 5-10 years after powerful AI to at least be substantially healthier than
                    the
                    developed world is today, even if it continues to lag behind the developed world. Accomplishing
                    this
                    will of course require a huge effort in global health, philanthropy, political advocacy, and
                    many
                    other efforts, which both AI developers and policymakers should help with.</li>
                <li><strong>Economic growth</strong>. Can the developing world quickly catch up to the developed
                    world,
                    not just in health, but across the board economically? There is some precedent for this: in the
                    final decades of the 20th century, <a href="https://en.wikipedia.org/wiki/Four_Asian_Tigers" target="_blank">several East Asian economies</a> achieved sustained ~10% annual real GDP
                    growth
                    rates, allowing them to catch up with the developed world. Human economic planners made the
                    decisions that led to this success, not by directly controlling entire economies but by pulling
                    a
                    few key levers (such as an industrial policy of export-led growth, and resisting the temptation
                    to
                    rely on natural resource wealth); it’s plausible that “AI finance ministers and central bankers”
                    could replicate or exceed this 10% accomplishment. An important question is how to get
                    developing
                    world governments to adopt them while respecting the principle of self-determination—some may be
                    enthusiastic about it, but others are likely to be skeptical. On the optimistic side, many of
                    the
                    health interventions in the previous bullet point are likely to organically increase economic
                    growth: eradicating AIDS/malaria/parasitic worms would have a transformative effect on
                    productivity,
                    not to mention the economic benefits that some of the neuroscience interventions (such as
                    improved
                    mood and focus) would have in developed and developing world alike. Finally, non-health
                    AI-accelerated technology (such as energy technology, transport drones, improved building
                    materials,
                    better logistics and distribution, and so on) may simply permeate the world naturally; for
                    example,
                    even cell phones quickly permeated sub-Saharan Africa via market mechanisms, without needing
                    philanthropic efforts. On the more negative side, while AI and automation have many potential
                    benefits, they also pose challenges for economic development, particularly for countries that
                    haven't yet industrialized. Finding ways to ensure these countries can still develop and improve
                    their economies in an age of increasing automation is an important challenge for economists and
                    policymakers to address. Overall, a dream scenario—perhaps a goal to aim for—would be 20% annual
                    GDP
                    growth rate in the developing world, with 10% each coming from AI-enabled economic decisions and
                    the
                    natural spread of AI-accelerated technologies, including but not limited to health. If achieved,
                    this would bring sub-Saharan Africa to the current per-capita GDP of China in 5-10 years, while
                    raising much of the rest of the developing world to levels higher than the current US GDP.
                    Again,
                    this is a dream scenario, not what happens by default: it’s something all of us must work
                    together
                    to make more likely.</li>
                <li><strong>Food security <sup id="fnref:24"><a href="#fn:24">24</a></sup></strong>. Advances in crop technology like better
                    fertilizers and
                    pesticides, more automation, and more efficient land use drastically increased <a href="https://ourworldindata.org/crop-yields" target="_blank">crop yields</a> across the
                    20th
                    Century, saving millions of people from hunger. Genetic engineering is <a href="https://www.science.org/content/article/new-genetic-tricks-boosting-crop-yield-take-clues-ancient-farmers" target="_blank">currently improving</a> many crops even further. Finding even more ways to
                    do
                    this—as well as to make agricultural supply chains even more efficient—could give us an
                    AI-driven
                    second <a href="https://en.wikipedia.org/wiki/Green_Revolution" target="_blank">Green
                        Revolution</a>, helping close the gap between the developing and developed world.</li>
                <li><strong>Mitigating climate change</strong>. Climate change will be felt much more strongly in
                    the
                    developing world, hampering its development. We can expect that AI will lead to improvements in
                    technologies that slow or prevent climate change, from atmospheric <a href="https://www.nature.com/articles/s41558-023-01604-9" target="_blank">carbon-removal</a>
                    and
                    clean energy technology to <a href="https://www.annualreviews.org/content/journals/10.1146/annurev-animal-021022-055132" target="_blank">lab-grown meat</a> that reduces our reliance on carbon-intensive factory
                    farming. Of course, as discussed above, technology isn’t the only thing restricting progress on
                    climate change—as with all of the other issues discussed in this essay, human societal factors
                    are
                    important. But there’s good reason to think that AI-enhanced research will give us the means to
                    make
                    mitigating climate change far less costly and disruptive, rendering many of the objections moot
                    and
                    freeing up developing countries to make more economic progress.</li>
                <li><strong>Inequality within countries</strong>. I’ve mostly talked about inequality as a global
                    phenomenon (which I do think is its most important manifestation), but of course inequality also
                    exists <i>within</i> countries. With advanced health interventions and especially radical
                    increases
                    in lifespan or cognitive enhancement drugs, there will certainly be valid worries that these
                    technologies are “only for the rich”. I am more optimistic about within-country inequality
                    especially in the developed world, for two reasons. First, markets function better in the
                    developed
                    world, and markets are typically good at bringing down the cost of high-value technologies over
                    time<sup id="fnref:25"><a href="#fn:25">25</a></sup>. Second, developed
                    world
                    political institutions are more responsive to their citizens and have greater state capacity to
                    execute universal access programs—and I expect citizens to demand access to technologies that so
                    radically improve quality of life. Of course it’s not predetermined that such demands
                    succeed—and
                    here is another place where we collectively have to do all we can to ensure a fair society.
                    There is
                    a separate problem in inequality of <i>wealth</i> (as opposed to inequality of access to
                    life-saving
                    and life-enhancing technologies), which seems harder and which I discuss in Section 5.</li>
                <li><strong>The opt-out problem</strong>. One concern in both developed and developing world alike
                    is
                    people <i>opting out</i> of AI-enabled benefits (similar to the anti-vaccine movement, or
                    Luddite
                    movements more generally). There could end up being bad feedback cycles where, for example, the
                    people who are least able to make good decisions opt out of the very technologies that improve
                    their
                    decision-making abilities, leading to an ever-increasing gap and even creating a dystopian
                    underclass (some researchers have argued that this will <a href="https://benmgarfinkel.blog/2021/02/26/is-democracy-a-fad/" target="_blank">undermine
                        democracy</a>, a topic I discuss further in the next section). This would, once again, place
                    a
                    moral blemish on AI’s positive advances. This is a difficult problem to solve as I don’t think
                    it is
                    ethically okay to coerce people, but we can at least try to increase people’s scientific
                    understanding—and perhaps AI itself can help us with this. One hopeful sign is that historically
                    anti-technology movements have been more bark than bite: railing against modern technology is
                    popular, but most people adopt it in the end, at least when it’s a matter of individual choice.
                    Individuals tend to adopt most health and consumer technologies, while technologies that are
                    truly
                    hampered, like nuclear power, tend to be collective political decisions.</li>
            </ul>
            <p>Overall, I am optimistic about quickly bringing AI’s biological advances to people in the developing
                world. I am hopeful, though not confident, that AI can also enable unprecedented economic growth
                rates
                and allow the developing world to at least surpass where the developed world is now. I am concerned
                about the “opt out” problem in both the developed and developing world, but suspect that it will
                peter
                out over time and that AI can help accelerate this process. It won’t be a perfect world, and those
                who
                are behind won’t fully catch up, at least not in the first few years. But with strong efforts on our
                part, we may be able to get things moving in the right direction—and fast. If we do, we can make at
                least a downpayment on the promises of dignity and equality that we owe to every human being on
                earth.
            </p>
            <h2>4. Peace and governance</h2>
            <p>Suppose that everything in the first three sections goes well: disease, poverty, and inequality are
                significantly reduced and the baseline of human experience is raised substantially. It does not
                follow
                that all major causes of human suffering are solved. Humans are still a threat to each other<i>.</i>
                Although there is a trend of technological improvement and economic development <a href="https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature" target="_blank">leading to
                    democracy and peace</a>, it is a very loose trend, with frequent (and <a href="https://ourworldindata.org/grapher/countries-democracies-autocracies-row" target="_blank">recent</a>) backsliding. At the dawn of the 20th Century, people <a href="https://en.wikipedia.org/wiki/The_Great_Illusion" target="_blank">thought</a> they had put
                war
                behind them; then came the two world wars. Thirty years ago Francis Fukuyama wrote about “<a href="https://en.wikipedia.org/wiki/The_End_of_History_and_the_Last_Man" target="_blank">the End
                    of
                    History</a>” and a final triumph of liberal democracy; that hasn’t happened yet. Twenty years
                ago US
                policymakers believed that free trade with China would cause it to liberalize as it became richer;
                that
                very much didn’t happen, and we now seem <a href="https://www.noahpinion.blog/p/why-the-us-should-fight-cold-war" target="_blank">headed for
                    a
                    second cold war</a> with a resurgent authoritarian bloc. And plausible theories suggest that
                internet technology <a href="https://www.noahpinion.blog/p/the-super-scary-theory-of-the-21st-a3a" target="_blank">may actually advantage authoritarianism</a>, not democracy as initially believed
                (e.g. in the “Arab Spring” period). It seems important to try to understand how powerful AI will
                intersect with these issues of peace, democracy, and freedom.</p>
            <p>Unfortunately, I see no strong reason to believe AI will preferentially or structurally advance
                democracy
                and peace, in the same way that I think it will structurally advance human health and alleviate
                poverty.
                Human conflict is adversarial and AI can in principle help both the “good guys” and the “bad guys”.
                If
                anything, some structural factors seem worrying: AI seems likely to enable much better propaganda
                and
                surveillance, both major tools in the autocrat’s toolkit. It’s therefore up to us as individual
                actors
                to tilt things in the right direction: if we want AI to favor democracy and individual rights, we
                are
                going to have to fight for that outcome. I feel even more strongly about this than I do about
                international inequality: the triumph of liberal democracy and political stability is <i>not</i>
                guaranteed, perhaps not even likely, and will require great sacrifice and commitment on all of our
                parts, as it often has in the past.</p>
            <p>I think of the issue as having two parts: international conflict, and the internal structure of
                nations.
                On the international side, it seems very important that democracies have the upper hand on the world
                stage when powerful AI is created. AI-powered authoritarianism seems too terrible to contemplate, so
                democracies need to be able to set the terms by which powerful AI is brought into the world, both to
                avoid being overpowered by authoritarians and to prevent human rights abuses within authoritarian
                countries.</p>
            <p>My current guess at the best way to do this is via an “entente strategy”<sup id="fnref:26"><a href="#fn:26">26</a></sup>, in which a coalition of democracies seeks
                to
                gain a clear advantage (even just a temporary one) on powerful AI by securing its supply chain,
                scaling
                quickly, and <a href="https://www.csis.org/analysis/updated-october-7-semiconductor-export-controls" target="_blank">blocking or delaying</a> adversaries’ access to key resources like chips and
                semiconductor equipment. This coalition would on one hand use AI to achieve robust military
                superiority
                (the stick) while at the same time offering to distribute the benefits of powerful AI (the carrot)
                to a
                wider and wider group of countries in exchange for supporting the coalition’s strategy to promote
                democracy (this would be a bit analogous to “<a href="https://en.wikipedia.org/wiki/Atoms_for_Peace" target="_blank">Atoms for Peace</a>”). The coalition would aim to gain the support of more and
                more
                of the world, isolating our worst adversaries and eventually putting them in a position where they
                are
                better off taking the same bargain as the rest of the world: give up competing with democracies in
                order
                to receive all the benefits and not fight a superior foe.</p>
            <p>If we can do all this, we will have a world in which democracies lead on the world stage and have the
                economic and military strength to avoid being undermined, conquered, or sabotaged by autocracies,
                and
                may be able to parlay their AI superiority into a durable advantage. This could optimistically lead
                to
                an “eternal 1991”—a world where democracies have the upper hand and Fukuyama’s dreams are realized.
                Again, this will be very difficult to achieve, and will in particular require close cooperation
                between
                private AI companies and democratic governments, as well as extraordinarily wise decisions about the
                balance between carrot and stick.</p>
            <p>Even if all that goes well, it leaves the question of the fight between democracy and autocracy
                <i>within</i> each country. It is obviously hard to predict what will happen here, but I do have
                some
                optimism that <i>given</i> a global environment in which democracies control the most powerful AI,
                <i>then</i> AI may actually structurally favor democracy everywhere. In particular, in this
                environment
                democratic governments can use their superior AI to win the information war: they can counter
                influence
                and propaganda operations by autocracies and may even be able to create a globally free information
                environment by providing channels of information and AI services in a way that autocracies lack the
                technical ability to block or monitor. It probably isn’t necessary to deliver propaganda, only to
                counter malicious attacks and unblock the free flow of information. Although not immediate, a level
                playing field like this stands a good chance of gradually tilting global governance towards
                democracy,
                for several reasons.
            </p>
            <p>First, the increases in quality of life in Sections 1-3 should, all things equal, promote democracy:
                historically they have, to at least some extent. In particular I expect improvements in mental
                health,
                well-being, and education to increase democracy, as all three are <a href="https://link.springer.com/article/10.1007/s11482-022-10070-y" target="_blank">negatively</a>
                <a href="https://www.cambridge.org/core/journals/perspectives-on-politics/article/abs/who-is-open-to-authoritarian-governance-within-western-democracies/0ADCD5FFE5B7E9267E8283C7561FB6BE" target="_blank">correlated</a> with support for authoritarian leaders. In general people want
                more
                self-expression when their other needs are met, and democracy is among other things a form of
                self-expression. Conversely, authoritarianism thrives on fear and resentment.
            </p>
            <p>Second, there is a good chance free information really does undermine authoritarianism, as long as
                the
                authoritarians can’t censor it. And uncensored AI can also bring individuals powerful tools for
                undermining repressive governments. Repressive governments survive by denying people a certain kind
                of
                common knowledge, keeping them from realizing that “the emperor has no clothes”. For example <a href="https://en.wikipedia.org/wiki/Sr%C4%91a_Popovi%C4%87_(activist)" target="_blank">Srđa
                    Popović</a>, who helped to topple the Milošević government in Serbia, has written extensively
                about
                techniques for psychologically robbing authoritarians of their power, for breaking the spell and
                rallying support against a dictator. A superhumanly effective AI version of Popović (whose skills
                seem
                like they have high returns to intelligence) in everyone’s pocket, one that dictators are powerless
                to
                block or censor, could create a wind at the backs of dissidents and reformers across the world. To
                say
                it again, this will be a long and protracted fight, one where victory is not assured, but if we
                design
                and build AI in the right way, it may at least be a fight where the advocates of freedom everywhere
                have
                an advantage.</p>
            <p>As with neuroscience and biology, we can also ask how things could be “better than normal”—not just
                how
                to avoid autocracy, but how to make democracies better than they are today. Even within democracies,
                injustices happen all the time. Rule-of-law societies make a promise to their citizens that everyone
                will be equal under the law and everyone is entitled to basic human rights, but obviously people do
                not
                always receive those rights in practice. That this promise is even partially fulfilled makes it
                something to be proud of, but can AI help us do better?</p>
            <p>For example, could AI improve our legal and judicial system by making decisions and processes more
                impartial? Today people mostly worry in legal or judicial contexts that AI systems will be a <a href="https://www.vox.com/technology/23738987/racism-ai-automated-bias-discrimination-algorithm" target="_blank"><i>cause</i> of discrimination</a>, and these worries are important and need to
                be
                defended against. At the same time, the vitality of democracy depends on harnessing new technologies
                to
                improve democratic institutions, not just responding to risks. A truly mature and successful
                implementation of AI has the potential to <i>reduce</i> bias and be fairer for everyone.</p>
            <p>For centuries, legal systems have faced the dilemma that the law aims to be impartial, but is
                inherently
                subjective and thus must be interpreted by biased humans. Trying to make the law fully mechanical
                hasn’t
                worked because the real world is messy and can’t always be captured in mathematical formulas.
                Instead
                legal systems rely on notoriously imprecise criteria like “<a href="https://en.wikipedia.org/wiki/Cruel_and_unusual_punishment" target="_blank">cruel and
                    unusual
                    punishment</a>” or “<a href="https://en.wikipedia.org/wiki/Roth_v._United_States" target="_blank">utterly without redeeming social importance</a>”, which humans then
                interpret—and
                often do so in a manner that displays bias, favoritism, or arbitrariness. “<a href="https://en.wikipedia.org/wiki/Smart_contract" target="_blank">Smart contracts</a>” in
                cryptocurrencies haven’t revolutionized law because ordinary code isn’t smart enough to adjudicate
                all
                that much of interest. But AI might be smart enough for this: it is the first technology capable of
                making broad, fuzzy judgements in a repeatable and mechanical way.</p>
            <p>I am not suggesting that we literally replace judges with AI systems, but the combination of
                impartiality
                with the ability to understand and process messy, real world situations <i>feels</i> like it should
                have
                some serious positive applications to law and justice. At the very least, such systems could work
                alongside humans as an aid to decision-making. Transparency would be important in any such system,
                and a
                mature science of AI could conceivably provide it: the training process for such systems could be
                extensively studied, and <a href="https://transformer-circuits.pub/" target="_blank">advanced
                    interpretability techniques</a> could be used to see inside the final model and assess it for
                hidden
                biases, in a way that is simply not possible with humans. Such AI tools could also be used to
                monitor
                for violations of fundamental rights in a judicial or police context, making constitutions more
                self-enforcing.</p>
            <p>In a similar vein, AI could be used to both aggregate opinions and drive consensus among citizens,
                resolving conflict, finding common ground, and seeking compromise. Some early ideas in this
                direction
                have been undertaken by the <a href="https://compdemocracy.org/" target="_blank">computational
                    democracy
                    project</a>, including <a href="https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input" target="_blank">collaborations with Anthropic</a>. A more informed and thoughtful citizenry
                would
                obviously strengthen democratic institutions.</p>
            <p>There is also a clear opportunity for AI to be used to help provision government services—such as
                health
                benefits or social services—that are in principle available to everyone but in practice often
                severely
                lacking, and worse in some places than others. This includes health services, the DMV, taxes, social
                security, building code enforcement, and so on. Having a very thoughtful and informed AI whose job
                is to
                give you everything you’re legally entitled to by the government in a way you can understand—and who
                also helps you comply with often confusing government rules—would be a big deal. Increasing state
                capacity both helps to deliver on the promise of equality under the law, and strengthens respect for
                democratic governance. Poorly implemented services are currently a major driver of cynicism about
                government<sup id="fnref:27"><a href="#fn:27">27</a></sup>.</p>
            <p>All of these are somewhat vague ideas, and as I said at the beginning of this section, I am not
                nearly as
                confident in their feasibility as I am in the advances in biology, neuroscience, and poverty
                alleviation. They may be unrealistically utopian. But the important thing is to have an ambitious
                vision, to be willing to dream big and try things out. The vision of AI as a guarantor of liberty,
                individual rights, and equality under the law is too powerful a vision not to fight for. A 21st
                century,
                AI-enabled polity could be both a stronger protector of individual freedom, and a beacon of hope
                that
                helps make liberal democracy the form of government that the whole world wants to adopt.</p>
            <h2>5. Work and meaning</h2>
            <p>Even if everything in the preceding four sections goes well—not only do we alleviate disease,
                poverty,
                and inequality, but liberal democracy becomes the dominant form of government, and existing liberal
                democracies become better versions of themselves—at least one important question still remains.
                “It’s
                great we live in such a technologically advanced world as well as a fair and decent one”, someone
                might
                object, “but with AI’s doing everything, how will humans have meaning? For that matter, how will
                they
                survive economically?”.</p>
            <p>I think this question is more difficult than the others. I don’t mean that I am necessarily more
                pessimistic about it than I am about the other questions (although I do see challenges). I mean that
                it
                is fuzzier and harder to predict in advance, because it relates to macroscopic questions about how
                society is organized that tend to resolve themselves only over time and in a decentralized manner.
                For
                example, historical hunter-gatherer societies might have imagined that life is meaningless without
                hunting and various kinds of hunting-related religious rituals, and would have imagined that our
                well-fed technological society is devoid of purpose. They might also have not understood how our
                economy
                can provide for everyone, or what function people can usefully service in a mechanized society.</p>
            <p>Nevertheless, it’s worth saying at least a few words, while keeping in mind that the brevity of this
                section is not at all to be taken as a sign that I don’t take these issues seriously—on the
                contrary, it
                is a sign of a lack of clear answers.</p>
            <p>On the question of meaning, I think it is very likely a mistake to believe that tasks you undertake
                are
                meaningless simply because an AI could do them better. Most people are not the best in the world at
                anything, and it doesn’t seem to bother them particularly much. Of course today they can still
                contribute through comparative advantage, and may derive meaning from the economic value they
                produce,
                but people also greatly enjoy activities that produce no economic value. I spend plenty of time
                playing
                video games, swimming, walking around outside, and talking to friends, all of which generates zero
                economic value. I might spend a day trying to get better at a video game, or faster at biking up a
                mountain, and it doesn’t really matter to me that someone somewhere is much better at those things.
                In
                any case I think meaning comes mostly from human relationships and connection, not from economic
                labor.
                People do want a sense of accomplishment, even a sense of competition, and in a post-AI world it
                will be
                perfectly possible to spend years attempting some very difficult task with a complex strategy,
                similar
                to what people do today when they embark on research projects, try to become Hollywood actors, or
                found
                companies<sup id="fnref:28"><a href="#fn:28">28</a></sup>. The facts that (a)
                an AI
                somewhere could in principle do this task better, and (b) this task is no longer an economically
                rewarded element of a global economy, don’t seem to me to matter very much.</p>
            <p>The economic piece actually seems more difficult to me than the meaning piece. By “economic” in this
                section I mean the possible problem that <i>most or all</i> humans may not be able to contribute
                meaningfully to a sufficiently advanced AI-driven economy. This is a more macro problem than the
                separate problem of inequality, especially inequality in access to the new technologies, which I
                discussed in Section 3.</p>
            <p>First of all, in the short term I agree with arguments that comparative advantage will continue to
                keep
                <a href="https://www.noahpinion.blog/p/plentiful-high-paying-jobs-in-the" target="_blank">humans
                    relevant</a> and in fact increase their productivity, and may even in some ways <a href="https://www.nber.org/papers/w31161" target="_blank">level the playing field between
                    humans</a>. As long as AI is only better at 90% of a given job, the other 10% will cause humans
                to
                become highly leveraged, increasing compensation and in fact creating a bunch of new human jobs
                complementing and amplifying what AI is good at, such that the “10%” <a href="https://en.wikipedia.org/wiki/Lump_of_labour_fallacy" target="_blank">expands to continue
                    to
                    employ almost everyone</a>. In fact, even if AI can do 100% of things better than humans, but it
                remains inefficient or expensive at some tasks, or if the resource <i>inputs</i> to humans and AI’s
                are
                meaningfully different, then the logic of comparative advantage continues to apply. One area humans
                are
                likely to maintain a relative (or even absolute) advantage for a significant time is the physical
                world.
                Thus, I think that the human economy may continue to make sense even a little past the point where
                we
                reach “a country of geniuses in a datacenter”.
            </p>
            <p>However, I do think in the long run AI will become so broadly effective and so cheap that this will
                no
                longer apply. At that point our current economic setup will no longer make sense, and there will be
                a
                need for a broader societal conversation about how the economy should be organized.</p>
            <p>While that might sound crazy, the fact is that civilization has successfully navigated major economic
                shifts in the past: from hunter-gathering to farming, farming to feudalism, and feudalism to
                industrialism. I suspect that some new and stranger thing will be needed, and that it’s something no
                one
                today has done a good job of envisioning. It could be as simple as a large universal basic income
                for
                everyone, although I suspect that will only be a small part of a solution. It could be a capitalist
                economy of AI systems, which then give out resources (huge amounts of them, since the overall
                economic
                pie will be gigantic) to humans based on some secondary economy of what the AI systems think makes
                sense
                to reward in humans (based on some judgment ultimately derived from human values). Perhaps the
                economy
                runs on <a href="https://en.wikipedia.org/wiki/Down_and_Out_in_the_Magic_Kingdom" target="_blank">Whuffie points</a>. Or perhaps humans will continue to be economically valuable
                after all, in some way not anticipated by the usual economic models. All of these solutions have
                tons of
                possible problems, and it’s not possible to know whether they will make sense without lots of
                iteration
                and experimentation. And as with some of the other challenges, we will likely have to fight to get a
                good outcome here: exploitative or dystopian directions are clearly also possible and have to be
                prevented. Much more could be written about these questions and I hope to do so at some later time.
            </p>
            <h2>Taking stock</h2>
            <p>Through the varied topics above, I’ve tried to lay out a vision of a world that is both plausible
                <i>if</i> everything goes right with AI, and much better than the world today. I don’t know if this
                world is realistic, and even if it is, it will not be achieved without a huge amount of effort and
                struggle by many brave and dedicated people. Everyone (including AI companies!) will need to do
                their
                part both to prevent risks and to fully realize the benefits.
            </p>
            <p>But it is a world worth fighting for. If all of this really does happen over 5 to 10 years—the defeat
                of
                most diseases, the growth in biological and cognitive freedom, the lifting of billions of people out
                of
                poverty to share in the new technologies, a renaissance of liberal democracy and human rights—I
                suspect
                everyone watching it will be surprised by the effect it has on them. I don’t mean the experience of
                personally benefiting from all the new technologies, although that will certainly be amazing. I mean
                the
                experience of watching a long-held set of ideals materialize in front of us all at once. I think
                many
                will be literally moved to tears by it.</p>
            <p>Throughout writing this essay I noticed an interesting tension. In one sense the vision laid out here
                is
                extremely radical: it is not what almost anyone expects to happen in the next decade, and will
                likely
                strike many as an absurd fantasy. Some may not even consider it desirable; it embodies values and
                political choices that not everyone will agree with. But at the same time there is something
                blindingly
                obvious—something overdetermined—about it, as if many different attempts to envision a good world
                inevitably lead roughly here.</p>
            <p>In Iain M. Banks’ <a href="https://www.hachettebookgroup.com/titles/iain-m-banks/the-player-of-games/9780316005401/" target="_blank"><i>The Player of Games</i></a><sup id="fnref:29"><a href="#fn:29">29</a></sup>, the protagonist—a member of a society called the Culture, which
                is
                based on principles not unlike those I’ve laid out here—travels to a repressive, militaristic empire
                in
                which leadership is determined by competition in an intricate battle game. The game, however, is
                complex
                enough that a player’s strategy within it tends to reflect their own political and philosophical
                outlook. The protagonist manages to defeat the emperor in the game, showing that his values (the
                Culture’s values) represent a winning strategy even in a game designed by a society based on
                ruthless
                competition and survival of the fittest. <a href="https://slatestarcodex.com/2015/08/17/the-goddess-of-everything-else-2/" target="_blank">A
                    well-known post</a> by Scott Alexander has the same thesis—that competition is self-defeating
                and
                tends to lead to a society based on compassion and cooperation. The “<a href="https://www.si.edu/spotlight/mlk?page=4&amp;iframe=true" target="_blank">arc of the moral
                    universe</a>” is another similar concept.</p>
            <p>I think the Culture’s values are a winning strategy because they’re the sum of a million small
                decisions
                that have clear moral force and that tend to pull everyone together onto the same side. Basic human
                intuitions of fairness, cooperation, curiosity, and autonomy are hard to argue with, and are
                cumulative
                in a way that our more destructive impulses often aren’t. It is easy to argue that children
                shouldn’t
                die of disease if we can prevent it, and easy from there to argue that <i>everyone’s</i> children
                deserve that right equally. From there it is not hard to argue that we should all band together and
                apply our intellects to achieve this outcome. Few disagree that people should be punished for
                attacking
                or hurting others unnecessarily, and from there it’s not much of a leap to the idea that punishments
                should be consistent and systematic across people. It is similarly intuitive that people should have
                autonomy and responsibility over their own lives and choices. These simple intuitions, if taken to
                their
                logical conclusion, lead eventually to rule of law, democracy, and Enlightenment values. If not
                inevitably, then at least as a statistical tendency, this is where humanity was already headed. AI
                simply offers an opportunity to get us there more quickly—to make the logic starker and the
                destination
                clearer.</p>
            <p>Nevertheless, it is a thing of transcendent beauty. We have the opportunity to play some small role
                in
                making it real.</p>
            <hr>
            <p><i>Thanks to Kevin Esvelt, Parag Mallick, Stuart Ritchie, Matt Yglesias, Erik Brynjolfsson, Jim
                    McClave,
                    Allan Dafoe, and many people at Anthropic for reviewing drafts of this essay.</i></p>
            <p><i>To the winners of the <a href="https://www.nobelprize.org/prizes/chemistry/2024/press-release/" target="_blank">2024 Nobel prize in Chemistry</a>, for showing us all the way.</i></p>
            <section>
                <h3>Footnotes</h3>
                <ol>
                    <li id="fn:1">
                        <p><sup>1</sup><a href="https://allpoetry.com/All-Watched-Over-By-Machines-Of-Loving-Grace" target="_blank">https://allpoetry.com/All-Watched-Over-By-Machines-Of-Loving-Grace</a>
                            <a href="#fnref:1">↩</a>
                        </p>
                    </li>
                    <li id="fn:2">
                        <p><sup>2</sup>I do anticipate some minority of people’s reaction will be “this is pretty
                            tame”.
                            I think those people need to, in Twitter parlance, “touch grass”. But more importantly,
                            tame
                            is good from a societal perspective. I think there’s only so much change people can
                            handle
                            at once, and the pace I’m describing is probably close to the limits of what society can
                            absorb without extreme turbulence. <a href="#fnref:2">↩</a></p>
                    </li>
                    <li id="fn:3">
                        <p><sup>3</sup>I find AGI to be an imprecise term that has gathered a lot of sci-fi baggage
                            and
                            hype. I prefer "powerful AI" or "Expert-Level Science and Engineering" which get at what
                            I
                            mean without the hype. <a href="#fnref:3">↩</a></p>
                    </li>
                    <li id="fn:4">
                        <p><sup>4</sup>In this essay, I use "intelligence" to refer to a general problem-solving
                            capability that can be
                            applied across diverse domains. This includes abilities like reasoning, learning, planning,
                            and
                            creativity. While I use "intelligence" as a shorthand throughout this essay, I acknowledge
                            that the
                            nature of intelligence is a complex and debated topic in cognitive science and AI research.
                            Some
                            researchers argue that intelligence isn't a single, unified concept but rather a collection
                            of
                            separate cognitive abilities. Others contend that there's a general factor of intelligence
                            (g
                            factor) underlying various cognitive skills. That’s a debate for another time. <a href="#fnref:4">↩</a></p>
                    </li>
                    <li id="fn:5">
                        <p><sup>5</sup>This is roughly the current speed of AI systems – for example they can read a
                            page of text in a couple seconds and write a page of text in maybe 20 seconds, which is
                            10-100x the speed at which humans can do these things. Over time larger models tend to
                            make
                            this slower but more powerful chips tend to make it faster; to date the two effects have
                            roughly canceled out. <a href="#fnref:5">↩</a></p>
                    </li>
                    <li id="fn:6">
                        <p><sup>6</sup>This might seem like a strawman position, but careful thinkers like <a href="https://www.bloomberg.com/opinion/articles/2023-08-16/ai-won-t-supercharge-the-us-economy" target="_blank">Tyler Cowen</a> and <a href="https://www.slowboring.com/p/im-skeptical-that-powerful-ai-will" target="_blank">Matt Yglesias</a> have raised it as a serious concern (though I
                            don’t
                            think they fully hold the view), and I don’t think it is crazy. <a href="#fnref:6">↩</a></p>
                    </li>
                    <li id="fn:7">
                        <p><sup>7</sup>The closest economics work that I’m aware of to tackling this question is
                            work on
                            “general purpose technologies” and “<a href="https://www.aeaweb.org/articles?id=10.1257/jep.14.4.23" target="_blank">intangible
                                investments</a>” that <a href="https://www.aeaweb.org/articles?id=10.1257/mac.20180386" target="_blank">serve
                                as complements</a> to general purpose technologies. <a href="#fnref:7">↩</a></p>
                    </li>
                    <li id="fn:8">
                        <p><sup>8</sup>This learning can include temporary, in-context learning, or traditional
                            training; both will be rate-limited by the physical world. <a href="#fnref:8">↩</a></p>
                    </li>
                    <li id="fn:9">
                        <p><sup>9</sup>In a chaotic system, small errors compound exponentially over time, so that
                            even
                            an enormous increase in computing power leads to only a small improvement in how far
                            ahead
                            it is possible to predict, and in practice measurement error may degrade this further.
                            <a href="#fnref:9">↩</a>
                        </p>
                    </li>
                    <li id="fn:10">
                        <p><sup>10</sup>Another factor is of course that powerful AI itself can potentially be used
                            to
                            create even more powerful AI. My assumption is that this might (in fact, probably will)
                            occur, but that its effect will be smaller than you might imagine, precisely because of
                            the
                            “decreasing marginal returns to intelligence” discussed here. In other words, AI will
                            continue to get smarter quickly, but its effect will eventually be limited by
                            non-intelligence factors, and analyzing those is what matters most to the speed of
                            scientific progress outside AI. <a href="#fnref:10">↩</a></p>
                    </li>
                    <li id="fn:11">
                        <p><sup>11</sup>These achievements have been an inspiration to me and perhaps the most
                            powerful
                            existing example of AI being used to transform biology. <a href="#fnref:11">↩</a></p>
                    </li>
                    <li id="fn:12">
                        <p><sup>12</sup>“Progress in science depends on new techniques, new discoveries and new
                            ideas,
                            probably in that order.” - <a href="https://en.wikipedia.org/wiki/Sydney_Brenner" target="_blank">Sydney Brenner</a> <a href="#fnref:12">↩</a>
                        </p>
                    </li>
                    <li id="fn:13">
                        <p><sup>13</sup>Thanks to Parag Mallick for suggesting this point. <a href="#fnref:13">↩</a></p>
                    </li>
                    <li id="fn:14">
                        <p><sup>14</sup>I didn't want to clog up the text with speculation about what specific
                            future
                            discoveries AI-enabled science could make, but here is a brainstorm of some
                            possibilities:
                            <br>
                            — Design of better computational tools like AlphaFold and AlphaProteo — that is, a general
                            AI
                            system speeding up our ability to make specialized AI computational biology tools.<br>
                            — More efficient and selective CRISPR.<br>
                            — More advanced cell therapies.<br>
                            — Materials science and miniaturization breakthroughs leading to better implanted
                            devices.<br>
                            — Better control over stem cells, cell differentiation, and de-differentiation, and a
                            resulting ability to regrow or reshape tissue.<br>
                            — Better control over the immune system: turning it on selectively to address cancer and
                            infectious disease, and turning it off selectively to address autoimmune diseases.

                            <a href="#fnref:14">↩</a>
                        </p>
                    </li>
                    <li id="fn:15">
                        <p><sup>15</sup>AI may of course also help with being smarter about choosing what
                            experiments to
                            run: improving experimental design, learning more from a first round of experiments so
                            that
                            the second round can narrow in on key questions, and so on. <a href="#fnref:15">↩</a></p>
                    </li>
                    <li id="fn:16">
                        <p><sup>16</sup>Thanks to Matthew Yglesias for suggesting this point. <a href="#fnref:16">↩</a></p>
                    </li>
                    <li id="fn:17">
                        <p><sup>17</sup>Fast evolving diseases, like the multidrug resistant strains that <a href="https://www.nejm.org/doi/full/10.1056/NEJMoa1914433" target="_blank">essentially
                                use hospitals as an evolutionary laboratory</a> to continually improve their
                            resistance
                            to treatment, could be especially stubborn to deal with, and could be the kind of thing
                            that
                            prevents us from getting to 100%. <a href="#fnref:17">↩</a></p>
                    </li>
                    <li id="fn:18">
                        <p><sup>18</sup>Note it may be hard to know that we have doubled the human lifespan within
                            the
                            5-10 years. While we might have accomplished it, we may not know it yet within the study
                            time-frame. <a href="#fnref:18">↩</a></p>
                    </li>
                    <li id="fn:19">
                        <p><sup>19</sup>This is one place where I am willing, despite the obvious biological
                            differences
                            between curing diseases and slowing down the aging process itself, to instead look from
                            a
                            greater distance at the statistical trend and say “even though the details are
                            different, I
                            think human science would probably find a way to continue this trend; after all, smooth
                            trends in anything complex are necessarily made by adding up very heterogeneous
                            components.
                            <a href="#fnref:19">↩</a>
                        </p>
                    </li>
                    <li id="fn:20">
                        <p><sup>20</sup>As an example, I’m told that an increase in productivity growth per year of
                            1%
                            or even 0.5% would be transformative in projections related to these programs. If the
                            ideas
                            contemplated in this essay come to pass, productivity gains could be much larger than
                            this.
                            <a href="#fnref:20">↩</a>
                        </p>
                    </li>
                    <li id="fn:21">
                        <p><sup>21</sup>The media loves to portray <a href="https://en.wikipedia.org/wiki/American_Psycho_(film)" target="_blank">high
                                status
                                psychopaths</a>, but the average psychopath is probably a person with poor economic
                            prospects and poor impulse control who ends up spending significant time in prison. <a href="#fnref:21">↩</a></p>
                    </li>
                    <li id="fn:22">
                        <p><sup>22</sup>I think this is somewhat analogous to the fact that many, though likely not
                            all,
                            of the results we’re learning from interpretability would continue to be relevant even
                            if
                            some of the architectural details of our current artificial neural nets, such as the
                            attention mechanism, were changed or replaced in some way. <a href="#fnref:22">↩</a></p>
                    </li>
                    <li id="fn:23">
                        <p><sup>23</sup>I suspect it is a bit like a classical chaotic system – <a href="https://en.wikipedia.org/wiki/Friedrich_Hayek" target="_blank">beset by
                                irreducible complexity</a> that has to be managed in a mostly decentralized manner.
                            Though as I say later in this section, more modest interventions may be possible. A
                            counterargument, made to me by economist Erik Brynjolfsson, is that large companies
                            (such as
                            Walmart or Uber) are starting to have enough centralized knowledge to understand
                            consumers
                            better than any decentralized process could, perhaps forcing us to revise <a href="https://www.econlib.org/library/Essays/hykKnw.html" target="_blank">Hayek’s
                                insights</a> about who has the best local knowledge. <a href="#fnref:23">↩</a></p>
                    </li>
                    <li id="fn:24">
                        <p><sup>24</sup>Thanks to Kevin Esvelt for suggesting this point. <a href="#fnref:24">↩</a></p>
                    </li>
                    <li id="fn:25">
                        <p><sup>25</sup>For example, cell phones were initially a technology for the rich, but
                            quickly
                            became very cheap with year-over-year improvements happening so fast as to obviate any
                            advantage of buying a “luxury” cell phone, and today most people have phones of similar
                            quality. <a href="#fnref:25">↩</a></p>
                    </li>
                    <li id="fn:26">
                        <p><sup>26</sup>This is the title of a forthcoming paper from RAND, that lays out roughly
                            the
                            strategy I describe. <a href="#fnref:26">↩</a></p>
                    </li>
                    <li id="fn:27">
                        <p><sup>27</sup>When the average person thinks of public institutions, they probably think
                            of
                            their experience with the DMV, IRS, medicare, or similar functions. Making these
                            experiences
                            more positive than they currently are seems like a powerful way to combat undue
                            cynicism. <a href="#fnref:27">↩</a></p>
                    </li>
                    <li id="fn:28">
                        <p><sup>28</sup>Indeed, in an AI-powered world, the range of such possible challenges and
                            projects will be much vaster than it is today. <a href="#fnref:28">↩</a></p>
                    </li>
                    <li id="fn:29">
                        <p><sup>29</sup>I am breaking my own rule not to make this about science fiction, but I’ve
                            found
                            it hard not to refer to it at least a bit. The truth is that science fiction is one of
                            our
                            only sources of expansive thought experiments about the future; I think it says
                            something
                            bad that it’s entangled so heavily with a particular narrow subculture. <a href="#fnref:29">↩</a></p>
                    </li>
                </ol>
                <p>Back to top</p>
                
            </section>

            
        </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In SSRI withdrawal, brain zaps go from overlooked symptom to center stage (2023) (117 pts)]]></title>
            <link>https://www.psychiatrist.com/news/brain-zaps-go-from-overlooked-symptom-to-center-stage-in-ssri-withdrawal/</link>
            <guid>41812876</guid>
            <pubDate>Fri, 11 Oct 2024 19:42:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psychiatrist.com/news/brain-zaps-go-from-overlooked-symptom-to-center-stage-in-ssri-withdrawal/">https://www.psychiatrist.com/news/brain-zaps-go-from-overlooked-symptom-to-center-stage-in-ssri-withdrawal/</a>, See on <a href="https://news.ycombinator.com/item?id=41812876">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-area">
					                        <div>
							<p><strong>Clinical Relevance: Brain zaps are a disconcerting symptom of antidepressant withdrawal that should be addressed</strong></p>
<ul>
<li>Brain zaps, a symptom of antidepressant discontinuation syndrome (ADS), are distressing and mysterious sensations experienced when stepping off SSRIs.</li>
<li>Physicians were initially unaware or dismissive of brain zaps due to limited information and a focus on downplaying the addictive nature of antidepressants.</li>
<li>Gradual tapering, switching to fluoxetine, and CBT during the discontinuation process can help mitigate brain zaps and other ADS symptoms.</li>
</ul>
                        </div>
					<p><span>A sensation unofficially known as “brain zaps” has caused concern in patients stepping off of their SSRIs. Until recently, physicians have widely been unaware or dismissive of the symptom.&nbsp;</span></p>
<p><a href="https://www.pointlomaclinic.com/providers"><span>Alexander Papp</span></a><span>, MD, who has led </span><a href="https://www.psychiatrist.com/pcc/brain-zaps/"><span>studies</span></a><span> published in <em>The Primary Care Companion for CNS Disorders</em> examining brain zaps, believes this may simply be due to the lack of available information.</span></p>
<p><span>“There was an emphasis both within the [psychiatric] profession and pharmacological companies to make these medications sound not addictive,” he told </span><i><span>Psychiatrist.com</span></i><span>. Therefore, when SSRIs first became popular, very few systematic studies were conducted on antidepressant discontinuation effects.&nbsp;</span></p>
<p><a href="https://www.psychiatrist.com/pcc/brain-zaps/">An Underappreciated Symptom of Antidepressant Withdrawal&nbsp;</a></p>
<p><a href="https://www.psychiatrist.com/news/study-tackles-the-mystery-of-brain-zaps-in-antidepressant-withdrawal/">Study Tackles the Mystery of Brain Zaps</a></p>
<p><a href="https://www.psychiatrist.com/jcp/functioning-and-life-engagement-treatment-goals-in-mdd/">Treatment Goals in MDD</a></p>
<p><span><br>
The tide seems to be shifting. Psychiatrists better understand a collection of&nbsp; symptoms that accompany stepping off of antidepressant use known as <a href="https://www.psychiatrist.com/jcp/antidepressant-discontinuation-syndrome-suicide-attempt/">antidepressant discontinuation syndrome</a> (ADS). But brain zaps still remain a bit of a mystery.</span></p>
<h3><b>An Off-Putting Sensation</b></h3>
<p><span>Brain zaps are a relatively uncommon ADS symptom. Those who have them say they can be quite distressing.</span></p>
<p><span>“The way it’s described, they feel like an electrical sensation in the brain as if you were shocked inside your head—which is unusual because people usually don’t feel anything inside their heads,” said Papp.&nbsp;</span></p>
<p><span>He clarified that while zaps, which typically last about one second each, </span><i><span>feel</span></i><span> like they’re coming from inside the brain tissue, they actually occur on the surface and around the nerves surrounding the lining of the brain.&nbsp;</span></p>
<p><span>As far as scientists have been able to tell, brain zaps aren’t dangerous. But because the experience is so unfamiliar and uncomfortable, it is highly unsettling. “Some people think that they are having <a href="https://www.psychiatrist.com/neurology/seizure/">seizures</a>. Some people think that they’re having a heart attack. Some people have no idea what’s happening. It can be a very scary state to be in,” Papp said.&nbsp;</span></p>
<p><span>Perhaps the most disconcerting feature of the zaps is the jumpy lateral eye movements. “People actually hear their eyes move when they move their eyes from left to right. They almost feel a faint ‘whoosh’ sound in their heads,” Papp explained. “Sometimes, people feel as if the brain stops for a moment and reboots like a computer.”</span></p>
<h3><b>Patient Reaction</b></h3>
<p><span>Zaps can cause <a href="https://www.psychiatrist.com/anxiety/">anxiety</a>. Some people also report vertigo, insomnia, vision changes and balance problems. </span><i><span>Psychiatrist.com </span></i><span>asked a group of Reddit users from various antidepressant-focused subreddits including, r/antidepressants and&nbsp; r/SSRIs, about what brain zaps feel like.</span></p>
<p><span>“Zaps are very similar to experiences I had after heavy weekends on ecstasy / MDMA in my younger years,” Reddit user u/heliskinki</span> <span>wrote in a direct message</span><i><span>.</span></i><span> “My brain ‘jumps’ forwards and I get a tingling feeling in my lips.”&nbsp;</span></p>
<p><span>“They feel like jolts of electricity that make me stutter and force me to slow down,” explained u/CosmosisJone5.&nbsp;</span></p>
<p><span>The sensation caused u/Remmy1319 to re-evaluate their decision to discontinue Zoloft, a sentiment shared by a number of Redditors. “Tried to come off Zoloft and was zapped out of my mind. I was on my meds for 14 months and I’m already thinking that I don’t have what it takes to face the withdrawals,” one anonymous Redditor commented.</span></p>
<p><span>Some tapered off too quickly after experiencing unpleasant reactions to medication only to have those side effects replaced with withdrawal symptoms including brain zaps.&nbsp;</span></p>
<p><span>“I was on 10 mg [Citalopram] for two weeks before deciding to stop as the depersonalisation was too much, plus the chest pains. I tapered down to 5mg for two days then came off completely over the course of a week, before starting to get brain zaps intermittently throughout the day,” u/FillPleasant wrote in a comment.&nbsp;</span></p>
<h3><b>Treatment Options</b></h3>
<p><span>The only thing that’s known to help prevent brain zaps is to stay on the <a href="https://www.psychiatrist.com/depression/">antidepressant</a>. Or at least avoid going cold turkey.</span></p>
<p><span>Taper off extremely slowly or, as Papp suggested, “You can switch from a short half-life medication to fluoxetine (Prozac) and then taper down from that.”&nbsp;</span></p>
<p><span>Gradually reducing the antidepressant agent with a long half-life has indeed been shown to reduce the severity of brain zaps and other kinds of </span><a href="https://www.psychiatrist.com/jcp/discontinuing-antidepressants-how-can-clinicians-guide-patients-and-drive-research/"><span>discontinuation effects</span></a><span>.&nbsp;</span></p>
<p><span>“It really seems to me that the speed of drop in blood level is the [factor] that is most likely responsible for these brain zaps,” Papp said. “The medications where blood level drops faster are the ones that are most likely to cause brain zaps.”&nbsp;</span></p>
<p><span>Undergoing cognitive behavioral therapy (CBT) during the tapering process may also help to decrease ADS symptoms, including the zaps. However, there’s no known cure, and for some people, they can be incredibly disruptive. Clinicians should note that they can become a barrier to successfully weaning off of medications, Papp advised.</span></p>
<p><span>Discontinuing or reducing antidepressant dosage or using these meds irregularly can cause a variety of other unpleasant symptoms, which can include flu-like symptoms, insomnia, nausea, imbalance, sensory disturbances, and hyperarousal.&nbsp;</span></p>
<p><span>“Eventually most people are able to get off their antidepressants, but there’s a small minority of people who are either never able to get off their antidepressants and continue to have these antidepressant discontinuation effects, including brain zaps, for years—sometimes even decades,” Papp said. </span></p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Valve says Steam users don't own a thing, GOG says its games can't be taken away (188 pts)]]></title>
            <link>https://www.gamesradar.com/games/valve-reminds-steam-users-they-dont-actually-own-a-darn-thing-they-buy-gog-pounces-and-says-its-games-cannot-be-taken-away-from-you-thanks-to-offline-installers/</link>
            <guid>41812813</guid>
            <pubDate>Fri, 11 Oct 2024 19:37:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamesradar.com/games/valve-reminds-steam-users-they-dont-actually-own-a-darn-thing-they-buy-gog-pounces-and-says-its-games-cannot-be-taken-away-from-you-thanks-to-offline-installers/">https://www.gamesradar.com/games/valve-reminds-steam-users-they-dont-actually-own-a-darn-thing-they-buy-gog-pounces-and-says-its-games-cannot-be-taken-away-from-you-thanks-to-offline-installers/</a>, See on <a href="https://news.ycombinator.com/item?id=41812813">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-320-80.jpg" alt="An orange car in The Crew." srcset="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Ubisoft)</span>
</figcaption>
</div>

<div id="article-body">
<p>A subtle change has arrived to the Steam shopping experience to drive home the fact that you're buying a game <em>license </em>rather than a copy of a game that you'll definitely own forever, and rival storefront GOG already seems to be weighing in on the matter.</p><p>As <a data-analytics-id="inline-link" href="https://www.engadget.com/gaming/steam-now-tells-gamers-up-front-that-theyre-buying-a-license-not-a-game-085106522.html" target="_blank" data-url="https://www.engadget.com/gaming/steam-now-tells-gamers-up-front-that-theyre-buying-a-license-not-a-game-085106522.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>reported by Engadget</u></a>, if you're about to buy a new game on Steam, you'll now notice a new message pop up, which reads: "A purchase of a digital product grants a license for the product on Steam." This disclaimer appears as though it's likely related to a California law set to come into effect next year, which'll stop digital storefronts from using words like "buy" in relation to things like game licenses unless it's obvious what people are spending their money on, as part of a move to make it clearer to consumers what they actually own (or rather, what they <em>don't</em>).&nbsp;</p><p>Needless to say, this new message on Steam has already caused quite a stir, so much so that it's seemingly reached the ears of rival storefront GOG. GOG is famously free of digital rights management (DRM), and offers its customers offline installers for the games it sells which you can download onto your PC where they can remain safe forever, so it's understandable that the site might have some thoughts on all this.&nbsp;</p><p>"Since checkout banners are trending, we're thinking of putting one up ourselves," a <a data-analytics-id="inline-link" href="https://x.com/GOGcom/status/1844752098145038435" target="_blank" data-url="https://x.com/GOGcom/status/1844752098145038435" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>tweet posted on the official GOG Twitter</u></a> account reads. "Thoughts on this one?" The concept banner in question says: "A purchase of a digital product on GOG grants you its Offline Installers, which cannot be taken away from you."</p><p>As was pointed out as recently as September when GOG weighed in on the upcoming California law, however, it's worth noting that GOG does, in fact, sell licenses to games much like other storefronts. With that said, <a data-analytics-id="inline-link" href="https://www.gamesradar.com/platforms/pc-gaming/as-california-forces-stores-to-admit-you-dont-own-digital-games-gog-reminds-pc-gamers-you-can-keep-drm-free-games-your-gaming-legacy-is-always-in-your-hands/" data-before-rewrite-localise="https://www.gamesradar.com/platforms/pc-gaming/as-california-forces-stores-to-admit-you-dont-own-digital-games-gog-reminds-pc-gamers-you-can-keep-drm-free-games-your-gaming-legacy-is-always-in-your-hands/"><u>it clarified at the time</u></a>: "When we said we let you 'own' your games, we meant that no matter what happens – whether it's licensing issues, storefronts shutting down, or even a zombie apocalypse cutting off your Internet – you'll still be able to play them thanks to our offline installers. We want to ensure your gaming legacy is always in your hands, not ours." With that in mind, there's no doubt many would argue that it's a better deal than what Steam offers.&nbsp;</p><p><em>Be sure to check out our recommendations for the </em><a data-analytics-id="inline-link" href="https://www.gamesradar.com/best-pc-games/" data-before-rewrite-localise="https://www.gamesradar.com/best-pc-games/"><u><em>best PC games</em></u></a><em>.</em></p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-amWrMVdqPwh7TxvGcdx7NS"><section><p>Weekly digests, tales from the communities you love, and more</p></section></div>
</div>
<div id="slice-container-authorBio-amWrMVdqPwh7TxvGcdx7NS"><p>I'm one of GamesRadar+'s news writers, who works alongside the rest of the news team to deliver cool gaming stories that we love. After spending more hours than I can count filling The University of Sheffield's student newspaper with Pokemon and indie game content, and picking up a degree in Journalism Studies, I started my career at GAMINGbible where I worked as a journalist for over a year and a half. I then became TechRadar Gaming's news writer, where I sourced stories and wrote about all sorts of intriguing topics. In my spare time, you're sure to find me on my Nintendo Switch or PS5 playing through story-driven RPGs like Xenoblade Chronicles and Persona 5 Royal, nuzlocking old Pokemon games, or going for a Victory Royale in Fortnite.</p></div>





</section>


<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>







</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs don't do formal reasoning (118 pts)]]></title>
            <link>https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and</link>
            <guid>41812523</guid>
            <pubDate>Fri, 11 Oct 2024 19:11:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and">https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and</a>, See on <a href="https://news.ycombinator.com/item?id=41812523">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>A superb </span><a href="https://arxiv.org/pdf/2410.05229" rel="">new article</a><span> on LLMs from six AI researchers at Apple who were brave enough to challenge the dominant paradigm has just come out.</span></p><p><em>Everyone</em><span> actively working with AI should read it, or at least this </span><a href="https://x.com/mfarajtabar/status/1844456880971858028?s=61" rel="">terrific X thread</a><span> by senior author, Mehrdad Farajtabar, that summarizes what they observed. One key passage: </span></p><blockquote><p>“we found no evidence of formal reasoning in language models …. Their behavior is better explained by sophisticated pattern matching—so fragile, in fact, that changing names can alter results by ~10%!” </p></blockquote><p>One particularly damning result was a new task the Apple team developed, called GSM-NoOp</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png" width="1456" height="967" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:967,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1517763,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>§</p><p><span>This kind of flaw, in which reasoning fails in light of distracting material, is not new. Robin Jia Percy Liang of Stanford ran a similar study, with similar results, back in 2017 (which Ernest Davis and I quoted in </span><em>Rebooting AI</em><span>, in 2019:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png" width="935" height="638" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:638,&quot;width&quot;:935,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:171487,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>§</p><p><strong>𝗧𝗵𝗲𝗿𝗲 𝗶𝘀 𝗷𝘂𝘀𝘁 𝗻𝗼 𝘄𝗮𝘆 𝗰𝗮𝗻 𝘆𝗼𝘂 𝗯𝘂𝗶𝗹𝗱 𝗿𝗲𝗹𝗶𝗮𝗯𝗹𝗲 𝗮𝗴𝗲𝗻𝘁𝘀 𝗼𝗻 𝘁𝗵𝗶𝘀 𝗳𝗼𝘂𝗻𝗱𝗮𝘁𝗶𝗼𝗻, where changing a</strong><span> word or two in irrelevant ways or adding a few bit of irrelevant info can give you a different answer.</span></p><p>§</p><p><span>Another manifestation of the lack of sufficiently abstract, formal reasoning in LLMs is the way in which performance often fall apart as problems are made bigger.  This comes from </span><a href="https://www.arxiv.org/pdf/2409.13373" rel="">a recent analysis of GPT o1</a><span> by Subbarao Kambhapati’s team:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png" width="888" height="532" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:532,&quot;width&quot;:888,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:127278,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Performance is ok on small problems, but quickly tails off.</p><p>§</p><p>We can see the same thing on integer arithmetic. Fall off on increasingly large multiplication problems has repeatedly been observed, both in older models and newer models. (Compare with a calculator which would be at 100%.)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg" width="1200" height="396" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:396,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:67405,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Even o1 suffers from this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png" width="1301" height="1078" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ad644058-8cbe-429a-9298-21318e200efe_1301x1078.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1078,&quot;width&quot;:1301,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1781277,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>§</p><p>Failure to follow the rules of chess is another continuing failure of formal reasoning:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg" width="1244" height="1248" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1248,&quot;width&quot;:1244,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:319035,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>§</p><p>Elon Musk’s putative robotaxis are likely to suffer from a similar affliction: they may well work safely for the most common situations, but are also likely struggle to reason abstractly enough in some circumstances. (We are, however, unlikely ever to get systematic data on this, since the company isn’t transparent about what it has done or what the results are.) </p><p>§</p><p>The refuge of the LLM fan is always to write off any individual error. The patterns we see here, in the new Apple study, and the other recent work on math and planning (which fits with many previous studies), and even the anecdotal data on chess, are too broad and systematic for that.</p><p>§</p><p><span>The inability of standard neural network architectures to reliably extrapolate — and reason formally — has been </span><em>the</em><span> central theme of my own work back to </span><a href="https://www.sciencedirect.com/science/article/pii/S0010028598906946" rel="">1998</a><span> and </span><a href="https://mitpress.mit.edu/9780262133791" rel="">2001</a><span>, and has been a theme in all of my challenges to deep learning, going back to 2012, and LLMs in 2019. </span></p><p><span>I strongly believe the current results are robust. After a quarter century of “</span><a href="http://wikibin.org/articles/real-soon-now.html" rel="">real soon now</a><span>” promissory notes I would want a lot more than hand-waving to be convinced than at an LLM-compatible solution is in reach. </span></p><p><span>What I argued in 2001, in </span><em>The Algebraic Mind</em><span>, still holds: </span><a href="https://mitpress.mit.edu/9780262632683/the-algebraic-mind/" rel="">symbol manipulation</a><span>, in which some knowledge is represented truly abstractly in terms of variables and operations over those variables, much as we see in algebra and traditional computer programming, must be part of the mix.  Neurosymbolic AI —  combining such machinery with neural networks – is likely a necessary condition for going forward. </span></p><p><em><strong>Gary Marcus</strong><span> is the author of The Algebraic Mind, a 2001 MIT Press Book that foresaw the Achilles’ Heel of current models. In his most recent book, Taming Silicon Valley (also MIT Press), in Chapter 17, he discusses the need for alternative research strategies.</span></em></p><p data-attrs="{&quot;url&quot;:&quot;https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gleam: A Basic Introduction (113 pts)]]></title>
            <link>https://peq42.com/blog/gleam-a-basic-introduction/</link>
            <guid>41812336</guid>
            <pubDate>Fri, 11 Oct 2024 18:57:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://peq42.com/blog/gleam-a-basic-introduction/">https://peq42.com/blog/gleam-a-basic-introduction/</a>, See on <a href="https://news.ycombinator.com/item?id=41812336">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
				
<p>Gleam is a statically-typed functional programming language, <a href="https://gleam.run/news/gleam-version-1/" target="_blank" rel="noreferrer noopener">which recently had its v1 released</a>,  designed for building scalable and maintainable software. It is inspired by languages like Elm and Rust and compiles down to Erlang, making it well-suited for building concurrent and distributed systems. In this tutorial, we’ll cover the basics of Gleam to help you get started with this expressive and powerful language.</p>



<h2>Preparing the environment</h2>



<p>Before diving into Gleam, you need to set up the development environment. The language <strong>compiles to Erlang and Javascript</strong>, and while the second doesn’t require anything but a browser and the gleam compiler to run, the first will need the compiler, Erlang and Rebar3. Let’s focus on that:</p>



<ol>
<li>First, grab a pre-compiled version of Gleam here: <a href="https://github.com/gleam-lang/gleam/releases" target="_blank" rel="noreferrer noopener">https://github.com/gleam-lang/gleam/releases</a></li>



<li>Then, install Erlang from the <a href="https://www.erlang-solutions.com/downloads/">Erlang-Solutions website</a>.</li>



<li>Finally, go to <a href="https://rebar3.org/docs/getting-started/#installing-from-the-rebar3-escript" target="_blank" rel="noreferrer noopener">rebar3 “getting started” page</a> and follow their instructions. This tool will be necessary in certain cases, such as Web Servers and HTTP clients.</li>
</ol>



<p>With everything in place, <a href="https://gleam.run/getting-started/installing/#editor-plugins" target="_blank" rel="noreferrer noopener">install a plugin to support syntax highlighting</a> in your editor of choice.</p>



<h2>Your First Gleam Program</h2>



<p>Let’s create a simple “Hello, World!” program to get started. Open your favorite text editor and create a file named <code>hello.gleam</code> with the following content:</p>



<pre><code>// hello.gleam
import gleam/io

pub fn main() {
  io.println("Hello, Gleam!")
}</code></pre>



<p>Save the file and run it using the Gleam compiler:</p>



<pre><code>gleam run hello.gleam</code></pre>



<p>You should see the output:</p>



<pre><code>Hello, Gleam!</code></pre>



<p>Congratulations! You’ve just written and executed your first Gleam program.</p>



<h2>Variables and Types</h2>



<p>Gleam is a statically-typed language, meaning you must declare the type of a variable before using it. Here’s an example demonstrating variable declaration and basic types:</p>



<pre><code>// variables.gleam
import gleam/io

pub fn main() {
  let message: String = "Hello, Gleam!"
  let number: Int = 42
  let is_true: Bool = True
  let ints = [1, 2, 3]

  io.debug(ints)
  io.debug(message)
  io.debug(number)
  io.debug(is_true)
}</code></pre>



<p>Run the program to see the output:</p>



<pre><code>gleam run variables.gleam</code></pre>



<p>As you may have noticed, this time we used <code>io.debug</code> instead of <code>io.println</code> . That is because the later only works with strings, while the first will print any type.</p>



<h2>Arithmetic Operations</h2>



<p>Gleam supports standard arithmetic operations for numerical values. Here’s a concise example demonstrating addition:</p>



<pre><code>import gleam/io
import gleam/int

pub fn main() {
  // Int arithmetic
  io.debug(1 + 1)
  io.debug(5 - 1)
  io.debug(5 / 2)
  io.debug(3 * 3)
  io.debug(5 % 2)

  // Int comparisons
  io.debug(2 &gt; 1)
  io.debug(2 &lt; 1)
  io.debug(2 &gt;= 1)
  io.debug(2 &lt;= 1)

  // Equality works for any type
  io.debug(1 == 1)
  io.debug(2 == 1)

  // Standard library int functions
  io.debug(int.max(42, 77)) //highest number
  io.debug(int.clamp(5, 10, 20))
}
</code></pre>



<p>In this program, the <code>+</code> operator adds the numbers 3 and 5. Run the program:</p>



<pre><code>gleam run arithmetic.gleam</code></pre>



<p>This should output:</p>



<pre><code>2
4
2
9
1
True
False
True
False
True
False
77
10</code></pre>



<h2>Functions</h2>



<p>Functions are a fundamental part of Gleam. Here’s an example of defining and calling a function:</p>



<pre><code>// functions.gleam
import gleam/io

pub fn main() {
  let result = add(3, 5)
  io.debug(result) //8
}

fn add(x: Int, y: Int) -&gt; Int {
  x + y
}</code></pre>



<p>Run the program and observe the output:</p>



<pre><code>gleam run functions.gleam</code></pre>



<h2>Case expressions</h2>



<p>In Gleam, case expressions offer a concise and expressive way to handle multiple conditions. Here’s a simple example demonstrating how to use case expressions for pattern matching:</p>



<pre><code>import gleam/io
import gleam/int

pub fn main() {
  let x = int.random(5)
  io.debug(x)

  let result = case x {
    // Match specific values
    0 -&gt; "Zero"
    1 -&gt; "One"

    // Match any other value
    _ -&gt; "Other"
  }
  io.debug(result)
}</code></pre>



<p>This code  generates a random integer between 0 and 4 (inclusive) and then uses a case expression to match and print a corresponding message based on the generated value.</p>



<h2>Conclusion</h2>



<p>In conclusion, Gleam emerges as a compelling language for developers seeking a balance between the expressiveness of functional programming and the performance of concurrent and distributed systems. Drawing inspiration from Elm and Rust, Gleam’s statically-typed nature ensures robustness and maintainability in software development, while being simple and easy to learn over an afternoon.</p>



<p>If you wish to learn more, <a href="https://tour.gleam.run/" target="_blank" rel="noreferrer noopener">Gleam has a language tour</a> available on their website!</p>
<br><center><a href="https://get.surfshark.net/SH3vn" target="_blank"><img decoding="async" src="https://peq42.com/wp-content/uploads/2024/08/surfshark-antivirus-1.webp"></a></center><br><center><strong>Get articles on your E-mail</strong>
</center>			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Started a guide to writing FUSE filesystems in Python (200 pts)]]></title>
            <link>https://gwolf.org/2024/10/started-a-guide-to-writing-fuse-filesystems-in-python.html</link>
            <guid>41811983</guid>
            <pubDate>Fri, 11 Oct 2024 18:29:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gwolf.org/2024/10/started-a-guide-to-writing-fuse-filesystems-in-python.html">https://gwolf.org/2024/10/started-a-guide-to-writing-fuse-filesystems-in-python.html</a>, See on <a href="https://news.ycombinator.com/item?id=41811983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
	
	
<!-- Blog entry follows -->

<p>As DebConf22 was coming to an end, in Kosovo, talking with <em>Eeveelweezel</em> they
invited me to prepare a talk to give for the <a href="https://chipy.org/">Chicago Python User
Group</a>. I replied that I’m not really that much of a Python
guy… But would think about a topic. Two years passed. I meet <em>Eeveelweezel</em>
again for DebConf24 in Busan, South Korea. And the topic came up again. I had
thought of some ideas, but none really pleased me. Again, I do write some Python
when needed, and I <em>teach</em> using Python, as it’s the language I find my students
can best cope with. But <em>delivering a talk</em> to ChiPy?</p>

<p>On the other hand, I have long used a very simplistic and limited filesystem
I’ve designed as an implementation project at class:
<a href="https://github.com/unamfi/sistop-2024-2/blob/main/proyectos/1/README.org">FIUnamFS</a>
(for “Facultad de Ingeniería, Universidad Nacional Autónoma de México”: the
Engineering Faculty for Mexico’s National University, where I teach. Sorry, the
link is in Spanish — but you will find several implementations of it from the
students 😉). It is a toy filesystem, with as many bad characteristics you can
think of, but easy to specify and implement. It is based on contiguous file
allocation, has no support for sub-directories, and is often limited to the size
of a 1.44MB floppy disk.</p>

<p>As I give this filesystem as a <em>project</em> to my students (and not as a mere
<em>homework</em>), I always ask them to try and provide a good, polished, professional
interface, not just the simplistic menu I often get. And I tell them the best
possible interface would be if they provide support for FIUnamFS transparently,
usable by the user without thinking too much about it. With high probability,
that would mean: Use FUSE.</p>

<p><a href="https://gwolf.org/files/2024-10/python-fuse.png"><img src="https://gwolf.org/files/2024-10/python-fuse.400.png" alt="Python FUSE"></a></p>

<p>But, in the six semesters I’ve used this project (with 30-40 students per
semester group), <em>only one student</em> has bitten the bullet and presented a FUSE
implementation.</p>

<p>Maybe this is because it’s not easy to understand how to build a FUSE-based
filesystem from a high-level language such as Python? Yes, I’ve seen several
implementation examples and even nice web pages (i.e. <a href="https://github.com/libfuse/python-fuse/tree/master/example">the examples shipped with
the<code>python-fuse</code>
module</a> <a href="https://www.stavros.io/posts/python-fuse-filesystem/">Stavros’
passthrough filesystem</a>,
<a href="https://thepythoncorner.com/posts/2017-02-27-writing-a-fuse-filesystem-in-python/">Dave Filesystem based upon, and further explaining,
Stavros’</a>,
and several others) explaining how to provide basic functionality. I found a
<a href="https://speakerdeck.com/matteobertozzi/python-fuse-pycon4">particularly useful presentation by Matteo
Bertozzi</a> presented
~15 years ago at PyCon4… But none of those is IMO followable enough by
itself. Also, most of them are <em>very</em> old (maybe the world is telling me
something that I refuse to understand?).</p>

<p>And of course, there isn’t a single interface to work from. In Python only, we
can find
<a href="https://github.com/libfuse/python-fuse/tree/master/example">python-fuse</a>,
<a href="https://github.com/nrclark/pyfuse">Pyfuse</a>,
<a href="https://github.com/fusepy/fusepy">Fusepy</a>… Where to start from?</p>

<p>…So I setup to try and help.</p>

<p>Over the past couple of weeks, I have been slowly working on my own version, and
presenting it as a <em>progressive set of tasks</em>, adding filesystem calls, and
being careful to thoroughly document what I write (but… maybe my documentation
ends up <em>obfuscating</em> the intent? I hope not — and, read on, I’ve provided some
remediation).</p>

<p>I registered a GitLab project for <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide">a hand-holding guide to writing FUSE-based
filesystems in Python</a>. This
is a project where I present several working FUSE filesystem implementations,
some of them RAM-based, some passthrough-based, and I intend to add to this also
filesystems backed on pseudo-block-devices (for implementations such as my
FIUnamFS).</p>

<p>So far, I have added five stepwise pieces, starting from the barest possible
<a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/1._emptyfs.py">empty
filesystem</a>,
and adding system calls (and functionality) until (so far) either a <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/4._basic_stat_info.py">read-write
filesystem in RAM with basic<code>stat()</code>
support</a>
or a <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/5._read_only_passthrough.py">read-only passthrough
filesystem</a>.</p>

<p>I think providing fun or useful examples is also a good way to get students to
use what I’m teaching, so I’ve added some ideas I’ve had: <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/dnsfs.py">DNS
Filesystem</a>,
<a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/markdown_compiling_fs.py">on-the-fly markdown compiling
filesystem</a>,
<a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/unzipfs.py">unzip
filesystem</a>
and <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/uncommentfs.py">uncomment
filesystem</a>.</p>

<p>They all provide something that could be seen as useful, in a way that’s easy to
teach, in just some tens of lines. And, in case my comments/documentation are
too long to read, <code>uncommentfs</code> will happily strip all comments and whitespace
automatically! 😉</p>

<p>So… I will be delivering my talk <a href="https://www.chipy.org/meetings/288/">tomorrow (2024.10.10, 18:30 GMT-6) at
ChiPy</a> (virtually). I am also presenting
this talk virtually at <a href="https://eventol.flisol.org.ar/events/jrsl-2024-santa-fe/">Jornadas Regionales de Software
Libre</a> in Santa Fe,
Argentina, next week (virtually as well). And also in November, in person, at
<a href="https://nerdear.la/">nerdear.la</a>, that will be held in Mexico City for the
first time.</p>

<p>Of course, I will also share this project with my students in the next couple of
weeks… And hope it manages to lure them into implementing FUSE in Python. At
some point, I shall report!</p>




      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How long til we're all on Ozempic? (348 pts)]]></title>
            <link>https://asteriskmag.com/issues/07/how-long-til-were-all-on-ozempic</link>
            <guid>41811263</guid>
            <pubDate>Fri, 11 Oct 2024 17:06:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asteriskmag.com/issues/07/how-long-til-were-all-on-ozempic">https://asteriskmag.com/issues/07/how-long-til-were-all-on-ozempic</a>, See on <a href="https://news.ycombinator.com/item?id=41811263">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<div data-mode="add-marker">
		<p><img id="marker" src="https://asteriskmag.com/assets/img/asterisk_mark.png" title="save highlight"></p><!-- <a href="https://asteriskmag.com/about/#highlights"><img id="help" src="https://asteriskmag.com/assets/img/asterisk_help.png" title="about highlights"></a> -->
		
	</div>

	<section>
					<p>forecast</p>
				
		 			<h2>
				   
					<span>Greg Justice</span>
							</h2>
			</section>
	
			<section id="rangyscope">
					<p>Over 100 million Americans, and possibly many more, could benefit from GLP-1 drugs. When can they expect to get them?</p>
				<div>
											<div><p>Obesity medication has something of a troubled past. Fen-phen, a weight-loss drug combination popular in the 1990s, was pulled after it was found to cause heart valve problems. Sibutramine, sold under the brand name Meridia, was prescribed until it was discovered to lead to adverse cardiovascular events including strokes in 2010. &nbsp;</p><p>But the market for an effective weight-loss drug is too big and the potential profits too high for pharmaceutical companies to give up. More than one in eight people around the world live with obesity. In the United States, it’s more than two in five. Though many clinical trials of weight-loss drugs over the past decade ended in failure, it was only a matter of time until a successful drug emerged.&nbsp;</p><p>GLP-1 medications<sup>
    <!-- <a id="fnref-1" href="#fn-1"> -->
    <span id="fnref-1">
        1    </span>
    <!-- </a> -->
</sup>
 like Ozempic appear to be that drug. Estimates suggest GLP-1s can reduce body weight by at least 15% when taken regularly — and perhaps even more as newer drugs come to market. And though evidence is still being gathered, they may have benefits beyond weight loss: potentially curbing drinking, treating sleep apnea, and reducing risk of stroke. They’ve been called, in many places, a miracle drug, and as such, the category is poised for massive growth. Gallup&nbsp;<a href="https://news.gallup.com/poll/644861/injectable-weight-loss-drugs-uses-work.aspx" rel="noopener noreferrer">estimated</a> that 15.5 million Americans have tried them, and half as many are currently using them.</p><p>But to date, the supply has been plagued by shortages so severe that there’s a website that tracks which pharmacies have it in stock. Though new products continue to come to market, and companies are doing what they can to increase supply, it’s extremely unlikely that demand will be met in the near term. So what does growth for GLP-1s really look like, and how many people stand to benefit?&nbsp;</p><p>In this forecast, I’m going to look at this over the next six years.</p><p><strong>Question: How many GLP-1 agonist medications will be sold in 2030 in the United States? </strong>Since a substantial portion of people discontinue their use, we’ll measure this by the number of one-year supplies sold.</p></div>
											<p><h2><strong>Estimating current usage</strong></h2>
</p>
											<div><p>Our projections for the future will be based on growth relative to the present, so it’s important to have a good idea of what GLP-1 sales look like today, which we’ll approach in a few different ways.</p><p>Polling is one approach. A Gallup poll from March estimated that 3% of American adults (approximately 7.75 million people) are currently taking GLP-1s. The poll has some issues for our purposes,<sup>
    <!-- <a id="fnref-2" href="#fn-2"> -->
    <span id="fnref-2">
        2    </span>
    <!-- </a> -->
</sup>
 but its flaws bias the results in opposing directions, so approximately eight million is a fine starting point.<sup>
    <!-- <a id="fnref-3" href="#fn-3"> -->
    <span id="fnref-3">
        3    </span>
    <!-- </a> -->
</sup>
</p><p>Another approach is to take company revenue for GLP-1s and divide that by the <a href="https://www.aei.org/wp-content/uploads/2023/09/Estimating-the-Cost-of-New-Treatments-for-Diabetes-and-Obesity.pdf?x91208" rel="noopener noreferrer">estimated net price</a> to get monthly supply. Almost the entire supply of GLP-1s in the United States is manufactured by either Eli Lilly or Novo Nordisk. Looking at their revenue for 2023 implies enough supply for 5.4 million patients each month: 3.5 million for Novo Nordisk and 1.9 million for Eli Lilly.&nbsp;</p><p>But sales are growing fast, and we want to base our estimate as much as we can on 2024 data. Novo Nordisk expects approximately <a href="https://www.morningstar.co.uk/uk/news/248948/novo-nordisk-earnings-beat-estimates-on-weight-loss-sales.aspx" rel="noopener noreferrer">23% sales growth for 2024</a>, or $7.8 billion at current exchange rates. Lilly expects revenue growth of about $8.9 billion. If those projections are right, and we assume 100% of Novo’s growth and 90% of Lilly’s<sup>
    <!-- <a id="fnref-4" href="#fn-4"> -->
    <span id="fnref-4">
        4    </span>
    <!-- </a> -->
</sup>
 is from GLP-1s, that gives us growth of $15.8 billion (54%) in the GLP-1 market, for a total of $45 billion.&nbsp;</p><p>The United States represented 72% of GLP-1 sales globally in 2023 — if that stays roughly constant, we should expect $32 billion in US sales for 2024. If, for simplicity, growth is also evenly spread between brands, then implied supply for 2024 would be 5.4 million * 154% = 8.3 million — similar to the poll results.&nbsp;</p><p>So enough supply for eight million people on average in 2024 seems like a safe base to work from.</p></div>
											<p><h2><strong>The outside view of obesity drugs</strong></h2>
</p>
											<div><p>I’ll begin by taking an “outside view” approach — that is, ignoring the details of specific GLP-1 drugs like Ozempic and coming up with an estimate based on other drugs in roughly the same reference class. This approach is less accurate than a more detailed “inside view” model — which we’ll get to— but it also requires much less information. Taking the outside view allows me to come up with a rough estimate that I can adjust as I do additional research. It also helps me account for factors I might not explicitly think to include when building a detailed model of GLP-1s in particular. From there, I can tweak only the factors that I think are significant and different for Ozempic, rather than trying to establish each and every one from scratch.&nbsp;</p><p>So what’s the right reference class for GLP-1 medications? I chose to look at the uptake of other recent multibillion-dollar “blockbuster” pharmaceutical drug categories. These are new classes of drugs created in the past 40 years or so that, like GLP-1s, experienced rapid expansions in sales and production. The drug categories I used are:</p></div>
											<div><p>Statins (cholesterol-lowering medications) — Lipitor, Crestor, etc.</p><p>TNF-α inhibitors (treatments for inflammatory conditions like rheumatoid arthritis) — Humira, Enbrel, Remicade.</p><p>PD-1/PD-L1 inhibitors (treatments for specific types of cancer) — Keytruda, Opdivo, Tecentriq.</p><p>Direct oral anticoagulants (DOACs, which prevent blood clotting) — Eliquis, Xarelto, Pradaxa.</p></div>
											<div><p>There are hundreds of other types of drugs I could’ve used. I want to address some drugs I didn’t use, and why.</p><p>The first omitted drug is insulin. Insulin is similar to obesity drugs in many ways. It addresses a large patient population, it’s a biologic (or drug made from a living organism), it’s an injection used on an ongoing basis, and it treats similar indications — Ozempic was initially approved as a treatment for diabetes. But market conditions now are very different from those of 1923, when commercial insulin was first introduced. Furthermore, when insulin was discovered and commercialized, it had to be extracted from pig and cow pancreases. Scaling production meant building an operation to collect animal pancreases and developing processes to better refine the pancreatic extract — a much different process than building a modern factory.&nbsp;</p><p>I also left out Hepatitis C drugs like sofosbuvir, which came out in the 2010s. Though this is another blockbuster release that had several billion dollars in sales, it is a cure, rather than an ongoing treatment, and so its sales trend downward rather than up.&nbsp;</p><p>That leaves the four drug classes listed above, which have their own limitations. First, obesity drugs have a larger potential market than any of them. Almost 150 million American adults have diabetes or obesity. About 45 million are eligible for statins,<sup>
    <!-- <a id="fnref-5" href="#fn-5"> -->
    <span id="fnref-5">
        5    </span>
    <!-- </a> -->
</sup>
 the next largest population, and the markets for DOACs and TNF-α inhibitors are likely in the single-digit millions. The US government also has more incentive to make GLP-1 drugs available — both because promoting weight loss would lead to savings for Medicare down the line and because it would (probably) please voters. Both factors suggest faster growth.</p><p>On the other hand, GLP-1s currently receive different insurance treatment in the United States when prescribed for weight loss, and they currently have lower patient adherence rates when compared with comparable drugs. Those factors point in the other direction.</p><p>Finally, and most importantly, obesity drugs have already shown a different growth path than our base rate drugs. The other drugs gained popularity very quickly after launch. But GLP-1s were initially prescribed as a treatment for diabetes and then gained popularity for weight loss later on thanks to off-label use. Novo Nordisk then underestimated demand for Wegovy and ran into supply problems, which meant sales didn’t truly take off until late 2022 or 2023.&nbsp;</p><p>There are two different benchmarks we can get from the reference class: sales and prescriptions. Each approach has its benefits and drawbacks, but both should hopefully give similar estimates. If they do, then we’ll be more confident that our base rate forecast is robust.</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/07/how-long-til-were-all-on-ozempic/2592186f22-1720207926/web_2.svg" alt="">
  </p>
  </figure>
</div>
											<p><h2><strong>Calculating growth in sales</strong></h2>
</p>
											<div><p>For each category, I collected annual net sales data for each drug from published research and drugmakers’ annual reports.<sup>
    <!-- <a id="fnref-6" href="#fn-6"> -->
    <span id="fnref-6">
        6    </span>
    <!-- </a> -->
</sup>
 I then calculated year-over-year changes in sales relative to the drug class’s first full year of significant sales. The growth rates over time for the drugs selected follow&nbsp; similar exponential decay patterns. Growth rates start high when sales are small. PD-1/PD-L1 sales grew by about 75% in their second year, and DOACs more than doubled. But by their eighth year on the market, sales for all classes grew between only 9% and 18%.</p><p>Looking at cumulative growth relative to the fourth year of sales, we can see very roughly the type of growth we should expect for GLP-1s over the next few years. If 2024 is year four, we expect somewhere between 180% and 240% growth in total sales by 2030.</p><p>To implement the sales model, I built a Monte Carlo model and ran it 10,000 times. The goal of a Monte Carlo simulation is to model future sales while accounting for uncertainty and variability in the possible growth trajectory of GLP-1s. This works by setting some parameters ourselves and randomly sampling others for each run. The cumulative result of this process should tell us which outcomes are most likely. To get a forecast for GLP-1s using that information, we’ll need to first answer one question: What year of sales are we in for GLP-1s?</p><p>This matters because, while sales grow each year, the rate at which they grow slows down. Picking the wrong year would give us the wrong rate of growth, and ultimately very different results. We could take the simple approach and define year one as the first full year in which a modern GLP-1 drug, Ozempic, was widely sold (2019),<sup>
    <!-- <a id="fnref-7" href="#fn-7"> -->
    <span id="fnref-7">
        7    </span>
    <!-- </a> -->
</sup>
 making 2024 what we call “index year” six. This isn’t a bad approach. In 2019, Ozempic was approved only as a treatment for diabetes, but that’s still a patient population of 38 million Americans. But GLP-1s have exploded into the larger obesity market much more recently. Indeed, if we look at growth in revenue for Novo Nordisk and Eli Lilly from 2023 to 2024 and compare it with our growth rate model, it suggests that 2024 should really be seen as approximately index year 3.5. I handled this discrepancy by making it one of the parameters I randomly varied between runs of my model.</p><p>After converting sales numbers to patient supply, this produces the following distribution:</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/07/how-long-til-were-all-on-ozempic/c44a55175b-1720207979/web_2-20.svg" alt="">
  </p>
  </figure>
</div>
											<div><p>Our median estimate at this point suggests a patient-year supply of around 18 million.</p><p>That’s what looking at sales gets us, but we can also look at prescription volume directly. Data on prescription fills is not widely publicly available, especially going back multiple decades. However, there is some limited data from the long-running Medical Expenditure Panel Survey. The survey measures 18 respondents reporting a TNF-α prescription in year four and 38 in year 10. In the same period, DOAC respondents went from 132 to 527. Data doesn’t go back far enough to include the early years of statins.</p><p>Still, this is a data point worth considering. Our outside view using sales predicts approximately 100% growth in US supply. Prescription growth is higher than those estimates, so we may want to adjust them upward.&nbsp;</p></div>
											<p><h2><strong>Incorporating the inside view</strong></h2>
</p>
											<div><p>The outside view approaches above are a useful starting point. Now we’ll consider information specific to GLP-1s.&nbsp;</p><p>We should start with supply constraints. After that, there are three other considerations that, in my opinion, are both significant to and different from the reference class: insurance treatment, patient adherence, and pipeline developments.&nbsp;</p><p><strong>Supply constraints and manufacturing expansion</strong></p><p>Demand for GLP-1s is sky-high. A major limiting factor for sales right now, according to both Eli Lilly and Novo Nordisk, is supply. So why is supply so hard to scale up?&nbsp;</p><p>Right now, the constraint appears to be in production facilities. Entirely new facilities need to be bought and repurposed or even built to meet demand. At baseline, much of pharmaceutical manufacturing is contracted out to third-party companies. Normally, this system would provide some flexibility for Novo and Lilly to expand production, but current capacity is too small to meet the historic demand for GLP-1s. Novo Nordisk recently agreed to acquire three factories by purchasing Catalent, a contract manufacturer which also produces products for Eli Lilly, in order to maximize production of its own products. Additionally, injector pens for Mounjaro are apparently unusually complex and may not be able to use existing assembly lines from other factories, which <a href="https://www.bloomberg.com/news/articles/2024-05-02/why-weight-loss-drugs-wegovy-zepbound-are-facing-shortages" rel="noopener noreferrer">further limits</a> Lilly’s options.&nbsp;</p><p>There are also geopolitical concerns. The United States is <a href="https://www.bloomberg.com/news/articles/2024-03-06/weight-loss-drugs-threatened-by-us-effort-to-contain-china" rel="noopener noreferrer">actively cracking down</a> on Chinese contract manufacturers, including WuXi AppTec, who reportedly produces much of the active ingredient in Mounjaro. All of this suggests that Novo and Lilly will need to invest in building new facilities or expanding existing ones, rather than utilizing existing sites. This isn’t like building a new Costco. Such advanced manufacturing facilities are multiyear, multibillion-dollar projects.&nbsp;</p><p>What’s especially notable about the current situation is that it’s very distinct from the reference class drugs. We didn’t see large spikes in capital expenditures after the launch of drug classes in our base rate. However, we see it very clearly already for Novo and Lilly. Lilly recently committed at least $13 billion for factories in North Carolina, Wisconsin, Indiana, and Germany, which are expected to come online at varying points by 2028. That’s a nearly 70% increase relative to their gross plant, property, and equipment (PP&amp;E) holdings in 2021 of about $19 billion. Novo Nordisk has similarly announced at least $19 billion in investments. $11 billion of that is for Catalent’s factories, with the rest going to facilities in Kalundborg, Denmark, and Chartres, France. This would more than double their gross PP&amp;E holdings of about $14 billion in 2021.</p><p>How much should we update based on this information?&nbsp;</p><p>Not as much it may seem we should at first glance. First, some investments would have happened regardless. Lilly averaged approximately $1.2 billion annually in PP&amp;E purchases from 2015 to 2021, and Novo Nordisk averaged approximately $1.1 billion. The large sums announced recently likely include some investments they would have made anyway.&nbsp;</p><p>It’s also already partly priced in. Our model predicts doubling sales, and while it doesn’t explain how, that capacity has to come from somewhere. We already expected that sales would be much larger than TNF-α inhibitors and DOACs, so unusually large investments are reasonable.</p><p>There are a lot of unknowns here that make it very difficult to rigorously factor this information in. It’s clearly important, but I don’t know, for example, to what degree the base rate factors this type of growth in, and I don’t know how much these investments actually increase production, whether they’ll be completed on time or in budget, what level of growth would be possible without them, or what additional investments are yet to come.&nbsp;</p><p>In the end, I tried to estimate the GLP-1-relevant manufacturing assets Novo and Lilly have now and then estimated how much of an increase their new investments might represent.&nbsp;</p><p>The announced investments across both companies total $32 billion.<sup>
    <!-- <a id="fnref-8" href="#fn-8"> -->
    <span id="fnref-8">
        8    </span>
    <!-- </a> -->
</sup>
 GLP-1s were 71% of Novo’s revenue in 2023, 16% of Lilly’s in 2023, and 26% of Lilly’s in 2024Q1. If these sales are proportional to the manufacturing capacity used to create those drugs, then about 40% of Novo and Lilly’s combined estimate of $45 billion in gross PP&amp;E is for GLP-1s, for a total of $18 billion; $25 billion would then mean a 140% increase in GLP-1-relevant PP&amp;E. However, a 140% increase in company-owned PP&amp;E does not necessarily mean a 140% increase in production capacity, because much capacity currently is contracted out. If company-owned production grows 140% but contracted production is unchanged, then total growth will be much lower. Contractors may scale up in parallel, or the new company-owned facilities may displace them, which seems likely for WuXi AppTec. Lastly, we’re also likely to see even more investment by Novo and Lilly announced between now and 2030.</p><p>Ultimately, the companies are investing more in PP&amp;E than I would’ve expected, which means I need to update. Increased investment is significant both in itself and for what it tells us about the companies’ future intentions. In light of this, I increase the base rate estimates for US volume by a pretty sizable 30%, with a standard deviation of 5%. This means I’m estimating most likely a 20%–40% increase in volume in light of this information.</p><p>This would tentatively bring median estimated patient-year supply up from 18 million to around 24 million.</p><p><strong>Insurance treatment</strong></p><p>Next let’s consider insurance treatment. In the United States, insurers are typically required to cover all FDA-approved medications that are “medically necessary.” While diabetes falls into that category, obesity does not. Medicare is actually prohibited by law from covering medication prescribed for weight loss. This makes the drugs much more expensive for patients when used only for weight loss, which should lead to lower use.&nbsp;</p><p>About 140 million Americans are obese. 38 million have diabetes, but most of them — approximately 31 million — are obese as well. This makes the total patient pool for GLP-1s approximately 147 million people, a bit over a quarter of whom can get GLP-1s prescribed for diabetes and covered through their insurance.</p><p>And insurers have a strong incentive to keep coverage limitations in place. Right now, an annual supply of Ozempic or Mounjaro costs roughly <a href="https://www.aei.org/wp-content/uploads/2023/09/Estimating-the-Cost-of-New-Treatments-for-Diabetes-and-Obesity.pdf?x91208" rel="noopener noreferrer">$3,000</a>. If our supply estimate of 22 million patient-years is accurate, that would mean a total annual spend of $66 billion — more than 10% of current spending on&nbsp;<a href="https://pubmed.ncbi.nlm.nih.gov/37094296/" rel="noopener noreferrer">all pharmaceuticals in the United States</a> combined. Insurance companies and Medicare may seek to restrict access to these drugs to keep premiums down.</p><p>However, expanding coverage for these drugs is a popular idea. In one poll, <a href="https://www.kff.org/health-costs/poll-finding/kff-health-tracking-poll-may-2024-the-publics-use-and-views-of-glp-1-drugs/" rel="noopener noreferrer">61%</a> of people stated Medicare should cover GLP-1s for weight loss. There’s also the chance that the government repeals Medicare’s ban on weight-loss drugs, which increasingly feels antiquated now that we have safe and effective treatments. And finally, we shouldn’t underestimate pharma’s famously strong lobby — or advocacy from the millions of patients who stand to benefit.&nbsp;</p><p>So currently, insurance coverage is quite restricted for GLP-1s, which should limit sales even as shortages become less severe. However, it seems like the desires of patients and most lobbyists are aligned in wanting to remove those limits. Medicare could also be able to negotiate the price of Ozempic as soon as 2025, and lower prices may make coverage more palatable. However, it’s still possible that prices will remain stubbornly high in the commercial (non-Medicare) market and that insurers will be able to restrict coverage enough to slow down demand.&nbsp;</p><p>Combining all of those pieces, I’d estimate (very roughly) that there’s a 50% chance that nothing changes, which reduces volume by 10% relative to the base rate drugs with no restrictions, a 40% chance of GLP-1s getting coverage parity, so no change from the base rate, and 10% chance that insurers restrict coverage even more, reducing volume 20%. On average, this is a 7% adjustment downward relative to the base rate.</p><p><strong>Patient adherence</strong></p><p>The second consideration is that so far, GLP-1 drugs have had low adherence rates. One study showed <a href="https://www.primetherapeutics.com/news/real-world-analysis-of-glp-1a-drugs-for-weight-loss-finds-low-adherence-and-increased-cost-in-first-year/" rel="noopener noreferrer">68%</a> of patients who started taking a GLP-1 drug for weight loss weren’t taking it within a year. Some percentage of this is due to supply shortages — there’s simply not enough to go around. But there are concerns that people are discontinuing the drugs once their weight loss has plateaued, once they’re no longer able to afford it, or due to side effects like nausea, diarrhea, vomiting, and loss of muscle mass. This discontinuation rate is much higher than those of the base rate drugs. <a href="https://pubmed.ncbi.nlm.nih.gov/28958039/" rel="noopener noreferrer">Statins</a> and <a href="https://pubmed.ncbi.nlm.nih.gov/26490106/" rel="noopener noreferrer">TNFa inhibitors</a> have one-year adherence rates of 59% and 73% respectively.&nbsp;</p><p>GLP-1s’ net turnover rate<sup>
    <!-- <a id="fnref-9" href="#fn-9"> -->
    <span id="fnref-9">
        9    </span>
    <!-- </a> -->
</sup>
 may be considerably higher than that of comparable drugs, meaning it may be harder to reach new GLP-1 patients. However, if turnover is driven by seasonality or people hitting weight-loss plateaus, then lapsed patients may become eligible again in the near future. On the other hand, if side effects become more well known as time goes on, there’s a chance that puts a dent in demand. Still, demand is so high relative to supply, though, that this probably won’t be a major issue. I’d say there’s a 20% chance of an 8% decrease in volume resulting from these concerns.</p><p><strong>Pipeline drugs</strong></p><p>The last factor I considered is other obesity drugs currently in development. There are several <a href="https://www.fiercebiotech.com/biotech/late-breaking-obesity-glp-1-wegovy-zepbound-novo-lilly-pipeline-rd-landscape" rel="noopener noreferrer">ongoing phase 3 clinical trials</a> for obesity drugs that could lead to subsequent FDA approval if successful. It’s likely though not certain that these will be roughly comparable to existing treatments in terms of efficacy. And most of them are produced by Eli Lilly or Novo Nordisk, so we should expect their introduction to mostly cannibalize sales and manufacturing capacity from existing drugs, rather than widen the market.&nbsp;</p><p>However, innovations in the pipeline might help raise adherence or improve production efficiencies. Orforglipron, for example, is a daily oral pill, as opposed to a weekly injection. Rybelsus, which is daily oral semaglutide made by Novo Nordisk, showed up to <a href="https://www.novonordisk.com/news-and-media/news-and-ir-materials/news-details.html?id=166110" rel="noopener noreferrer">17.4% weight loss</a> at 68 weeks in a Phase 3 trial but is <a href="https://www.reuters.com/business/healthcare-pharmaceuticals/novo-nordisk-says-obesity-pill-leads-15-weight-loss-availability-be-determined-2023-06-26/" rel="noopener noreferrer">not yet approved</a> for weight loss. Orals don’t depend on the complex injection pen manufacturing process, but they also use much larger doses of active ingredients than injectables do. A typical maintenance dose of the injectable Ozempic is 0.5 mg per week. The equivalent dose for Rybelsus is 7 mg per <em>day</em>. This means a manufacturer can create many more doses of injectables than orals with the same ingredients. Daily oral alternatives are likely to be available by 2030. But given their comparable efficacy, higher ingredient requirements, and the fact that the manufacturers overlap, I don’t believe that the current pipeline merits an adjustment.</p><p>In sum, there’s three adjustments to be added. Supply investments: a 30% increase with standard deviation of 5%. Insurance coverage: 50% chance of 10% decrease, and a&nbsp; 10% chance of a 20% decrease. Side effects and adherence: a 20% chance of an 8% decrease.&nbsp;</p><p>Applying those adjustments to the original estimate, we get our final prediction:</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/07/how-long-til-were-all-on-ozempic/4899f3d74b-1720208054/web_2-21.svg" alt="">
  </p>
  </figure>
</div>
											<div><p><strong>My forecast predicts that the supply of GLP-1s will increase from eight million patient-years to roughly enough for approximately 23 million Americans by 2030. </strong>Still, this is only enough supply for about 15% of the 147 million Americans with diabetes or obesity.&nbsp;</p><p>Most patients who want them will not have access to these drugs in the near future. The need is vast, especially abroad, where rollouts are being delayed until demand in the United States can be met. And as indications continue to be added for conditions ranging from heart disease to <a href="https://www.bloomberg.com/news/newsletters/2023-12-08/can-ozempic-wegovy-treat-alcoholism">potentially even alcoholism</a>, demand will only grow. It will likely be many years before these drugs become widely available to the patients who need them. It takes a long time to make a miracle.</p></div>
										 
				</div>
		
	</section>
	 	<section>
		 		 <p><strong>Greg Justice</strong> is a member of the Samotsvety forecasting group. He has worked as an analyst and project manager in the healthcare industry, and is completing his MBA at Chicago Booth.</p>		 		 		 </section>
	 	<section>            
		<p>
			Published July 2024		</p>
		
		<p>Have something to say? Email us at <a href="mailto:letters@asteriskmag.com">letters@asteriskmag.com</a>.</p>		                        
	</section>	
	
	
	<!--end published content, not coming soon-->

	<!--tags-->
		<section>
		<h4>Further Reading</h4>                
			<p>
				More:  
									<span data-no="tag-1">health</span>
									<span data-no="tag-2">forecasting</span>
							</p>
			<!--related articles-->
			             
	</section>
	 
	
	  
	<p>Subscribe</p>
	<div id="signup-article-popup">
			
<p><img src="https://asteriskmag.com/assets/img/asterisk_x.png">
		</p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lm.rs: Minimal CPU LLM inference in Rust with no dependency (248 pts)]]></title>
            <link>https://github.com/samuel-vitorino/lm.rs</link>
            <guid>41811078</guid>
            <pubDate>Fri, 11 Oct 2024 16:46:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/samuel-vitorino/lm.rs">https://github.com/samuel-vitorino/lm.rs</a>, See on <a href="https://news.ycombinator.com/item?id=41811078">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
    <img alt="lmrs logo" src="https://github.com/samuel-vitorino/lm.rs/raw/main/repo_cover.svg">
</picture></themed-picture>
<p dir="auto">lm.rs: run inference on Language Models locally on the CPU with Rust</p>

</div>
<hr>
<p dir="auto"><strong>🌃 Now supporting multimodality with PHI-3.5-vision model! PHI-3.5-mini text-only model also now supported.</strong></p>
<p dir="auto">Inspired by Karpathy's <a href="https://github.com/karpathy/llama2.c">llama2.c</a> and <a href="https://github.com/karpathy/llm.c">llm.c</a> I decided to create the most minimal code (not so minimal atm) that can perform full inference on Language Models on the CPU without ML libraries. Previously only Google's Gemma 2 models were supported, but I decided to add support for the new Llama 3.2 models, and more recently the option to use images with PHI-3.5. Image processing/encoding currently takes a bit, so it slows the first response, working on optimization now.</p>
<p dir="auto">Disclaimer: some of the code could be optimized and improved. This is just an excuse for me to write Rust for the first time. Isn't it incredible that in a few years, we could have AGI running in a few lines of poorly written Rust code?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prepared models</h2><a id="user-content-prepared-models" aria-label="Permalink: Prepared models" href="#prepared-models"></a></p>
<p dir="auto">Some benchmarks and download links for the models and tokenizers. I recommend using Q8_0, Q4_0 quantization still being improved. Speed measured on a 16-core AMD Epyc.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-2b-it-q4_0-LMRS" rel="nofollow">Gemma 2 2B IT Q4_0</a></td>
<td>1.39G</td>
<td>20 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-2b-it-q8_0-LMRS" rel="nofollow">Gemma 2 2B IT Q8_0</a></td>
<td>2.66GB</td>
<td>18 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-9b-it-q4_0-LMRS" rel="nofollow">Gemma 2 9B IT Q4_0</a></td>
<td>4.91GB</td>
<td>7 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-9b-it-q8_0-LMRS" rel="nofollow">Gemma 2 9B IT Q8_0</a></td>
<td>9.53GB</td>
<td>8 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-LMRS" rel="nofollow">Llama 3.2 1B IT</a></td>
<td>4.94GB</td>
<td>20 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-Q8_0-LMRS" rel="nofollow">Llama 3.2 1B IT Q8_0</a></td>
<td>1.27GB</td>
<td>35 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-3B-Instruct-Q4_0-LMRS" rel="nofollow">Llama 3.2 3B IT Q4_0</a></td>
<td>1.71GB</td>
<td>17 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-3B-Instruct-Q8_0-LMRS" rel="nofollow">Llama 3.2 3B IT Q8_0</a></td>
<td>3.31GB</td>
<td>16 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Phi-3.5-vision-instruct-q8_0-LMRS" rel="nofollow">PHI 3.5 IT Vision Q8_0</a></td>
<td>4.28GB</td>
<td>15 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Phi-3.5-mini-instruct-q8_0-LMRS" rel="nofollow">PHI 3.5 IT Mini Q8_0</a></td>
<td>3.94GB</td>
<td>16 tok/s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Instructions</h2><a id="user-content-instructions" aria-label="Permalink: Instructions" href="#instructions"></a></p>
<p dir="auto">You can download the prepared quantized model and tokenizer model files in the lmrs format from huggingface. If you'd prefer to convert the models published by Google/Meta on huggingface yourself, please refer to the following section. Otherwise, you can skip ahead to the build section.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Model Conversion</h3><a id="user-content-model-conversion" aria-label="Permalink: Model Conversion" href="#model-conversion"></a></p>
<p dir="auto">Install additional python dependencies (assuming you already have pytorch installed) used in export.py and tokenizer.py:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<p dir="auto">Download the <strong>.safetensors</strong> and <strong>config.json</strong> files from the original model's page on huggingface (So we don't have to clone the pytorch repo). For multimodal models (PHI3.5 Vision), we also need the CLIP <strong>.config</strong> <a href="https://huggingface.co/openai/clip-vit-large-patch14-336/blob/main/config.json" rel="nofollow">file</a>.</p>
<p dir="auto">Use the export.py script to convert the model bfloat16 weights into the LMRS format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python export.py --files [ordered .safetensor files] --config [model config.json] --save-path [name and path to save] --type [model type (GEMMA/LLAMA/PHI)]"><pre>python export.py --files [ordered .safetensor files] --config [model config.json] --save-path [name and path to save] --type [model type (GEMMA/LLAMA/PHI)]</pre></div>
<p dir="auto">To export the quantized version use the <strong>--quantize</strong> and <strong>--quantize-type</strong> flags. The int8 quantized model size should be 4X smaller (from ~9.8G to ~2.5G, depending on the group size). For multimodal models include the <strong>--vision-config</strong> argument.</p>
<p dir="auto">Use the tokenizer.py script to convert the tokenizer model into the LMRS tokenizer format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python tokenizer.py --model-id [huggingface model_id] --tokenizer-type [type of the tokenizer (GEMMA/LLAMA/PHI)]"><pre>python tokenizer.py --model-id [huggingface model_id] --tokenizer-type [type of the tokenizer (GEMMA/LLAMA/PHI)]</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build</h3><a id="user-content-build" aria-label="Permalink: Build" href="#build"></a></p>
<p dir="auto">Compile the rust code with cargo (make sure to pass the target-cpu flag):</p>
<div dir="auto" data-snippet-clipboard-copy-content="RUSTFLAGS=&quot;-C target-cpu=native&quot; cargo build --release --bin chat"><pre><span>RUSTFLAGS</span>=<span><span>"</span>-C target-cpu=native<span>"</span></span> cargo build --release --bin chat</pre></div>
<p dir="auto">To enable multimodality, include the multimodal feature by passing the <strong>--features multimodal</strong> argument.</p>
<p dir="auto">And you are good to go:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./target/release/chat --model [model weights file]"><pre>./target/release/chat --model [model weights file]</pre></div>
<p dir="auto">Other arguments include tokenizer, temperature, top-p, show-metrics etc. To check available arguments run with --help. For multimodal models use the <strong>--image</strong> argument with the image path.</p>
<hr>
<p dir="auto">To run the backend for the <a href="https://github.com/samuel-vitorino/lm.rs-webui">WebUI</a>, first compile:</p>
<div dir="auto" data-snippet-clipboard-copy-content="RUSTFLAGS=&quot;-C target-cpu=native&quot; cargo build --release --features backend --bin backend"><pre><span>RUSTFLAGS</span>=<span><span>"</span>-C target-cpu=native<span>"</span></span> cargo build --release --features backend --bin backend</pre></div>
<p dir="auto">For multimodality enable the <strong>backend-multimodal</strong> feature.</p>
<p dir="auto">Then run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./target/release/backend --model [model weights file]"><pre>./target/release/backend --model [model weights file]</pre></div>
<p dir="auto">You can change the ip and port with --ip and --port. Other flags such as temperature, etc. are also available. For multimodal compatibility use the <strong>--multimodal</strong> flag. You can now connect via the web interface.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">TODOs</h2><a id="user-content-todos" aria-label="Permalink: TODOs" href="#todos"></a></p>
<p dir="auto">Some things to do in the future:</p>
<ul>
<li> Add other sampling methods.</li>
<li> Test the 9B and 27B models (tested the 9B, 27B would be too slow).</li>
<li> Parallelize the multi head attention loop.</li>
<li> Add performance metrics.</li>
<li> Ability to give a system prompt</li>
<li> Quantization support (int8, int4).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Dead man's switch without reliance on your infra (125 pts)]]></title>
            <link>https://github.com/adamdecaf/deadcheck</link>
            <guid>41809879</guid>
            <pubDate>Fri, 11 Oct 2024 14:40:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/adamdecaf/deadcheck">https://github.com/adamdecaf/deadcheck</a>, See on <a href="https://news.ycombinator.com/item?id=41809879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">deadcheck</h2><a id="user-content-deadcheck" aria-label="Permalink: deadcheck" href="#deadcheck"></a></p>
<p dir="auto"><a href="https://godoc.org/github.com/adamdecaf/deadcheck" rel="nofollow"><img src="https://camo.githubusercontent.com/50f96480adc71d0679655bcdeaaf1d17bd1a1a1d200cf0912e0cb575fff6200c/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f6164616d64656361662f64656164636865636b3f7374617475732e737667" alt="GoDoc" data-canonical-src="https://godoc.org/github.com/adamdecaf/deadcheck?status.svg"></a>
<a href="https://github.com/adamdecaf/deadcheck/actions"><img src="https://github.com/adamdecaf/deadcheck/workflows/Go/badge.svg" alt="Build Status"></a>
<a href="https://codecov.io/gh/adamdecaf/deadcheck" rel="nofollow"><img src="https://camo.githubusercontent.com/075df6433861f25f7b1de1d009bb28cc1b46383fda6243f46f81bf154f8bd598/68747470733a2f2f636f6465636f762e696f2f67682f6164616d64656361662f64656164636865636b2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage Status" data-canonical-src="https://codecov.io/gh/adamdecaf/deadcheck/branch/master/graph/badge.svg"></a>
<a href="https://goreportcard.com/report/github.com/adamdecaf/deadcheck" rel="nofollow"><img src="https://camo.githubusercontent.com/769326ffc5972c7cde59a002222232ad9a33639bf3ab487e510dda34d38a1257/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6164616d64656361662f64656164636865636b" alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/adamdecaf/deadcheck"></a>
<a href="https://raw.githubusercontent.com/adamdecaf/deadcheck/master/LICENSE" rel="nofollow"><img src="https://camo.githubusercontent.com/29ba86e33d6c73f556fd517b07ad76acf76b1352e87e1e681e9c51c9501e7dba/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865322d626c75652e737667" alt="Apache 2 License" data-canonical-src="https://img.shields.io/badge/license-Apache2-blue.svg"></a>
<a href="https://hub.docker.com/r/adamdecaf/deadcheck" rel="nofollow"><img src="https://camo.githubusercontent.com/f4b22cf0558a58274c54e8f8684097ff975c5a640ed87029fd4ce13844d5fccd/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6164616d64656361662f64656164636865636b" alt="Docker Pulls" data-canonical-src="https://img.shields.io/docker/pulls/adamdecaf/deadcheck"></a></p>
<p dir="auto">Deadcheck is a versatile <a href="https://en.wikipedia.org/wiki/Dead_man's_switch" rel="nofollow">dead man's switch</a> designed to be independent of the infrastructure hosting it. The project allows users to set up checks that must be periodically "confirmed" to indicate that everything is fine. If a check isn't confirmed within the specified time, Deadcheck triggers a set of actions, such as sending alerts or executing tasks to ensure that the necessary steps are taken in the event you're no longer able to do so.</p>
<p dir="auto">Deadcheck is an automated dead man's switch that <strong>doesn't rely on its own uptime</strong>. Instead, it uses external services for final triggers, ensuring alerts and actions occur even if the hosting infrastructure is down. Deadcheck relies on third parties (e.g., PagerDuty) to handle alerts when a check is missed.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Decoupled from Hosting Infrastructure: Deadcheck ensures that the actual execution of alerts or incidents happens independently from the infrastructure hosting it. Even if the Deadcheck server goes down, the check-in process will still trigger events using external services.</li>
<li>Configurable Check Intervals: Flexible check intervals allow you to set up switches ranging from short-term (hours) to long-term (months).</li>
<li>Provider-Agnostic Setup: Deadcheck is designed to integrate with a variety of external systems, allowing for a wide range of customization in how your dead man's switch operates.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/adamdecaf/deadcheck/blob/master/docs/images/timeline.png"><img src="https://github.com/adamdecaf/deadcheck/raw/master/docs/images/timeline.png" alt=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto">Download the <a href="https://github.com/adamdecaf/deadcheck/releases/latest">latest release for your architecture</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="checks:
  - id: &quot;hourly-sync&quot;
    name: &quot;Upload data every hour&quot;
    description: &quot;<string>&quot;
    schedule:
      every:
        interval: &quot;1h&quot;
        start: &quot;14:00&quot;
        end: &quot;18:00&quot;
    # Override alert for one check
    alert:
      pagerduty:
        apiKey: &quot;<string>&quot;
        escalationPolicy: &quot;<string>&quot;

  - id: &quot;2pm-checkin&quot;
    name: &quot;Reports Finalized&quot;
    schedule:
      weekdays:
        timezone: &quot;America/New_York&quot;
        times:
          - &quot;14:00&quot;
        # Only allow check-ins between 13:55 and 14:05
        tolerance: &quot;5m&quot;

  - id: &quot;5pm-close&quot;
    name: &quot;Close out for the day&quot;
    schedule:
      bankingDays:
        timezone: &quot;America/New_York&quot;
        times:
          - &quot;17:00&quot;
        # Only allow check-ins between 16:55 and 17:05
        tolerance: &quot;5m&quot;

# Global alert configuration
alert:
  pagerduty:
    apiKey: &quot;<string>&quot;
    escalationPolicy: &quot;<string>&quot;"><pre><span>checks</span>:
  - <span>id</span>: <span><span>"</span>hourly-sync<span>"</span></span>
    <span>name</span>: <span><span>"</span>Upload data every hour<span>"</span></span>
    <span>description</span>: <span><span>"</span>&lt;string&gt;<span>"</span></span>
    <span>schedule</span>:
      <span>every</span>:
        <span>interval</span>: <span><span>"</span>1h<span>"</span></span>
        <span>start</span>: <span><span>"</span>14:00<span>"</span></span>
        <span>end</span>: <span><span>"</span>18:00<span>"</span></span>
    <span><span>#</span> Override alert for one check</span>
    <span>alert</span>:
      <span>pagerduty</span>:
        <span>apiKey</span>: <span><span>"</span>&lt;string&gt;<span>"</span></span>
        <span>escalationPolicy</span>: <span><span>"</span>&lt;string&gt;<span>"</span></span>

  - <span>id</span>: <span><span>"</span>2pm-checkin<span>"</span></span>
    <span>name</span>: <span><span>"</span>Reports Finalized<span>"</span></span>
    <span>schedule</span>:
      <span>weekdays</span>:
        <span>timezone</span>: <span><span>"</span>America/New_York<span>"</span></span>
        <span>times</span>:
          - <span><span>"</span>14:00<span>"</span></span>
        <span><span>#</span> Only allow check-ins between 13:55 and 14:05</span>
        <span>tolerance</span>: <span><span>"</span>5m<span>"</span></span>

  - <span>id</span>: <span><span>"</span>5pm-close<span>"</span></span>
    <span>name</span>: <span><span>"</span>Close out for the day<span>"</span></span>
    <span>schedule</span>:
      <span>bankingDays</span>:
        <span>timezone</span>: <span><span>"</span>America/New_York<span>"</span></span>
        <span>times</span>:
          - <span><span>"</span>17:00<span>"</span></span>
        <span><span>#</span> Only allow check-ins between 16:55 and 17:05</span>
        <span>tolerance</span>: <span><span>"</span>5m<span>"</span></span>

<span><span>#</span> Global alert configuration</span>
<span>alert</span>:
  <span>pagerduty</span>:
    <span>apiKey</span>: <span><span>"</span>&lt;string&gt;<span>"</span></span>
    <span>escalationPolicy</span>: <span><span>"</span>&lt;string&gt;<span>"</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div data-snippet-clipboard-copy-content="PUT /v1/checks/{id}/check-in"><pre><code>PUT /v1/checks/{id}/check-in
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="{&quot;nextExpectedCheckIn&quot;:&quot;2024-10-09T21:05:00Z&quot;}"><pre>{<span>"nextExpectedCheckIn"</span>:<span><span>"</span>2024-10-09T21:05:00Z<span>"</span></span>}</pre></div>
<p dir="auto">Successful response, or failure in the response.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Integrations</h2><a id="user-content-integrations" aria-label="Permalink: Integrations" href="#integrations"></a></p>
<ul dir="auto">
<li>PagerDuty: A service is used and incident created but snoozed preventing notifications. Each successful check-in pushes the snooze out into the future until the next expected check-in.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported and tested platforms</h2><a id="user-content-supported-and-tested-platforms" aria-label="Permalink: Supported and tested platforms" href="#supported-and-tested-platforms"></a></p>
<ul dir="auto">
<li>64-bit Linux (Ubuntu, Debian), macOS, and Windows</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Apache License 2.0 - See <a href="https://github.com/adamdecaf/deadcheck/blob/master/LICENSE">LICENSE</a> for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Manifest v2 is now removed from Chrome canary (430 pts)]]></title>
            <link>https://developer.chrome.com/docs/extensions/develop/migrate/mv2-deprecation-timeline</link>
            <guid>41809698</guid>
            <pubDate>Fri, 11 Oct 2024 14:20:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.chrome.com/docs/extensions/develop/migrate/mv2-deprecation-timeline">https://developer.chrome.com/docs/extensions/develop/migrate/mv2-deprecation-timeline</a>, See on <a href="https://news.ycombinator.com/item?id=41809698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
    




<p>Understand when Manifest V2 will stop working for extensions</p>

<h2 id="latest" data-text="Latest" tabindex="-1">Latest</h2>

<h3 id="october_9th_2024_an_update_on_manifest_v2_phase-out" data-text="October 9th 2024: an update on Manifest V2 phase-out." tabindex="-1">October 9th 2024: an update on Manifest V2 phase-out.</h3>

<p>Over the last few months, we have continued with the Manifest V2 phase-out.
Currently the chrome://extensions page displays a warning banner for all users
of Manifest V2 extensions. Additionally, we have started disabling Manifest V2
extensions on pre-stable channels.</p>

<p>We will now begin disabling installed extensions still using Manifest V2 in
Chrome stable. This change will be slowly rolled out over the following weeks.
Users will be directed to the Chrome Web Store, where they will be recommended
Manifest V3 alternatives for their disabled extension. For a short time, users
will still be able to turn their Manifest V2 extensions back on. Enterprises
using the
<a href="https://chromeenterprise.google/policies/#ExtensionManifestV2Availability">ExtensionManifestV2Availability</a>
policy will be exempt from any browser changes until June 2025. See our <a href="https://blog.chromium.org/2024/05/manifest-v2-phase-out-begins.html">May
2024 blog</a>
for more context.</p>

<h3 id="june_3rd_2024_the_manifest_v2_phase-out_begins" data-text="June 3rd 2024: the Manifest V2 phase-out begins." tabindex="-1">June 3rd 2024: the Manifest V2 phase-out begins.</h3>

<p>Starting on June 3rd on the Chrome Beta, Dev and Canary channels, if users still
have Manifest V2 extensions installed, some will start to see a warning banner
when visiting their extension management page - chrome://extensions - informing
them that some (Manifest V2) extensions they have installed will soon no longer
be supported. At the same time, extensions with the Featured badge that are
still using Manifest V2 will lose their badge.</p>

<h2 id="upcoming" data-text="Upcoming" tabindex="-1">Upcoming</h2>

<h3 id="june_2025_chrome_mv2_deprecation_enterprise_rollout" data-text="June 2025: Chrome MV2 deprecation enterprise rollout" tabindex="-1">June 2025: Chrome MV2 deprecation enterprise rollout</h3>

<p>Enterprises using the
<a href="https://chromeenterprise.google/policies/#ExtensionManifestV2Availability">ExtensionManifestV2Availability</a>
policy to ensure the continued functioning of Manifest V2 extensions in their
organization will have one additional year - until June 2025 - to migrate the
Manifest V2 extensions in their organization. Browsers with the policy enabled
won't be impacted by the rollout of the deprecation until that time.</p>

<h2 id="past" data-text="Past" tabindex="-1">Past</h2>

<h3 id="june_2022_chrome_web_store_-_no_new_private_extensions" data-text="June 2022: Chrome Web Store -  no new private extensions" tabindex="-1">June 2022: Chrome Web Store -  no new private extensions</h3>

<p>Chrome Web Store stopped accepting new Manifest V2 extensions with visibility
set to "Private".</p>

<h3 id="january_2022_chrome_web_store_-_no_new_public_unlisted_extensions" data-text="January 2022: Chrome Web Store - no new public / unlisted extensions" tabindex="-1">January 2022: Chrome Web Store - no new public / unlisted extensions</h3>

<p>Chrome Web Store stopped accepting new Manifest V2 extensions with visibility
set to "Public" or "Unlisted". The ability to change Manifest V2 extensions from
"Private" to "Public" or "Unlisted" was removed.</p>

  

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: NotesHub: cross-platform, Markdown-based note-taking app (133 pts)]]></title>
            <link>https://about.noteshub.app</link>
            <guid>41808943</guid>
            <pubDate>Fri, 11 Oct 2024 12:37:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://about.noteshub.app">https://about.noteshub.app</a>, See on <a href="https://news.ycombinator.com/item?id=41808943">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>If you like the sharing and security advantages of storing your notes in GitHub repositories, you likely will feel at home in this versatile notetaker/organizer/keeper. From there you can use your notes to feed a public website or blog. For those who prefer iCloud Drive, that option is available as well.</p><p>You’ll find yourself quickly and easily setting up a hierarchical folder structure and navigating it by simply tapping oval icons in a horizontal row at the top of your screen. Couldn’t be more intuitive. In that structure you can store almost anything: text, images and photos, names, addresses, phone numbers, geolocations, dates and times, etc. You could even set up a zettelkasten, a knowledge management database in hierarchical form—for business record keeping, research, study, or just personal reference.</p><p>The markdown editor is fully featured and included is a primer that you can refer to as you write. Tables, code blocks, html tags, even footnotes are all supported.</p><p>What attracted me initially was the kanban board feature. I experimented by setting up an archplot structure “template” for novel plotting using the “Save the Cat” method. I simply brainstorm and add event summaries or setting descriptions or dialogue or whatever in vertical columns below each horizontal plot element. Don’t know how much that will help me yet, but so far I’m pleased with the results.</p><p>Lots of potential with this NotesHub app, and the one-time purchase price can’t be beat.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Play killed my game and won't tell me why (129 pts)]]></title>
            <link>https://antiidlereborn.com/news/</link>
            <guid>41808917</guid>
            <pubDate>Fri, 11 Oct 2024 12:34:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antiidlereborn.com/news/">https://antiidlereborn.com/news/</a>, See on <a href="https://news.ycombinator.com/item?id=41808917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hello everyone – this is Tukkun (of course) and this is my first real post on this website! I know you guys are all excited for the release of my upcoming game “Anti-Idle: Reborn” and it is my intention to keep this website updated with contents related to the game – its development, gameplay sneak peeks, and so on. But before that, today I’d like to write this post to explain the current situation.</p>







<h2>In short…</h2>



<ul>
<li>I have submitted the game “Anti-Idle: Reborn” to both Google and Apple for review. Both Google and Apple have reviewed the game and approved it for production.</li>



<li>Closed Beta of the game has been ongoing for about a month. People have found lots of weird bugs, and I have fixed many of them.</li>



<li>However, on October 7, 2024, without any prior warnings, Google decided to terminate my account due to “prior violations” and “High Risk Behavior”. I’ve re-read the policies, I’ve checked everything I can think of, and I still can’t figure out why.</li>



<li>I sent an appeal but it seems like they haven’t looked into it yet (as of October 11, 2024). I have gathered any information I can think of and sent it to the Google Play Team, but I only receive the same (possibly automated) response.</li>



<li>I have developed games for many years, and “Anti-Idle: Reborn” is more than just my passion. It’s over one year of full time work and dedication, and it is my future source of income. With the Google Play Developer account terminated, I can no longer continue work.</li>



<li>A quick search showed me that I am not the only one in this situation. Lots of other developers have had their account terminated for vague reasons, possibly by bots and automated algorithms, and received nothing other than automated messages when appealing.</li>



<li>I would like to emphasize that I understand the need for thorough app reviews and termination of accounts that violate the rules. However, this shouldn’t come at a cost of many good faith developers’ accounts being wrongly terminated.</li>
</ul>







<h2>The full story</h2>



<p>While I think most of you are familiar with my works and are just here to check for new information about my game <strong>Anti-Idle: Reborn</strong>, I understand that some of you might have gotten here from other pages and have no idea who I am. In that case, or in the rare case that a Google employee is somehow reading this, let me introduce myself.</p>



<p>I am <strong>Tukkun</strong>, an indie game developer making games since 2008. My most significant work is a PC Flash game I made back in 2009 called <a href="http://www.kongregate.com/games/tukkun/anti-idle-the-game" data-type="link" data-id="http://www.kongregate.com/games/tukkun/anti-idle-the-game"><strong>Anti-Idle: The Game</strong></a>, uploaded to the website Kongregate. As of the writing of this post, the game has been played 16,392,188 times (and this is not counting plays of the offline version). I know I shouldn’t say too much good stuff about my own projects (just like anything, my game had imperfections), but <strong>Anti-Idle: The Game</strong> is often said to have pioneered the idle game genre. It is one of the first games to popularize many mechanics often seen in modern idle games and is a source of inspiration for the development of many popular idle games, including several games on the Play Store (Android) and App Store (iOS). It is even mentioned in the <a href="https://en.wikipedia.org/wiki/Incremental_game">Wikipedia page for Incremental game</a>: </p>



<p><em>The early pioneers of idle games also saw some games parodying the genre, such as Anti-Idle (2009, by tukkun) which has elements of both active and idle games. The game was extremely complicated, content-rich, and constantly updated, and it helped popularize the genre.</em></p>


<div>
<figure><img decoding="async" loading="lazy" width="646" height="646" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image.png 646w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-300x300.png 300w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-150x150.png 150w" sizes="(max-width: 646px) 100vw, 646px"></figure></div>


<p><em>A screenshot of “Anti-Idle: The Game”</em> </p>



<p>Following the success of <strong>Anti-Idle: The Game</strong>, I decided to develop the mobile sequel <strong>Anti-Idle: Reborn</strong>. I started doing serious design works on the game since 2023, and started programming it in Unity since the beginning of 2024, with the target of releasing it to Android and iOS late 2024.</p>



<p>The idea of developing a mobile sequel started as early as around 2020, with Flash no longer being supported by browsers, and lots of people in the community asked for a “mobile version” of the game. However, like many grown up adults, I had a day job and didn’t have enough free time to develop a mobile game. Despite that, I’ve released a few updates for the original Anti-Idle: The Game – 13 years after its initial release, the game still has a nice active community.</p>



<p><strong>That’s why I decided to follow my passion</strong> – I went as far as quitting my day job some time ago to fully dedicate myself to game development, and to make Anti-Idle live on. I’ve decided to announce and work on <strong>Anti-Idle: Reborn</strong> as my first mobile game. Creating a game from scratch feels great – when I managed to get the game design work on paper, when I opened Unity and created a simple loading screen that worked, when I got a prototype running… everything felt like a huge milestone.</p>


<div>
<figure><img decoding="async" loading="lazy" width="188" height="300" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-188x300.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-188x300.png 188w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-640x1024.png 640w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-768x1229.png 768w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-960x1536.png 960w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-1280x2048.png 1280w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-1.png 1440w" sizes="(max-width: 188px) 100vw, 188px"></figure></div>


<p><em>A screenshot of “Anti-Idle: Reborn” (under development)</em></p>



<p>The day eventually came when I created enough features to launch the first version of the game and decided that <strong>Closed Beta</strong> can finally begin. I made a Google Forms so people can voluntarily register for Closed Beta and shared it with the community that still played my original game after over 14 years. To my surprise, they shared it to many other communities, including <a href="https://www.reddit.com/r/incremental_games/comments/1etrj2e/antiidles_mobile_sequel_closed_beta_opening_and/">this post on Reddit r/incremental_games</a>, and in total <strong>over 1000 people have applied</strong>!</p>



<p>Then, of course, in order to start Closed Beta, I would have to upload my game to the stores: Play Store (Android) and App Store (iOS). Little did I know, this is only the beginning of the story.</p>







<h3>Uploading the game to iOS</h3>



<p>I’ve always been under the impression that it is really difficult to upload a game to the App Store of iOS. They have always set high quality standards and from what I’ve heard, they seem to review apps really thoroughly.</p>



<p>And I guess I was right. Within a few hours of uploading my game to the App Store for review, it got rejected.</p>


<div>
<figure><img decoding="async" loading="lazy" width="1024" height="626" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image-2-1024x626.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image-2-1024x626.png 1024w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-2-300x183.png 300w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-2-768x470.png 768w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-2.png 1248w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<p><em>Apple’s initial rejection</em></p>



<p>Oh, great, I knew it. Of course “Anti-Idle” is a world-renowned intellectual property and it is natural for Apple to think I have no permission to use it (sigh).</p>



<p>I attempted to convince Apple that I am the real Tukkun (because that’s who I am), and I submitted some proof to the Apple review team, including a screenshot of the source code of the original <strong>Anti-Idle: The Game</strong> and a link from my profile on Kongregate to this website of <strong>Anti-Idle: Reborn</strong>. I told Apple that I would provide any other information if necessary. A few hours later, they reviewed the game again, pointed out a bug and even sent screenshots as evidence. I fixed it, and they approved the game. All of the subsequent updates also passed through Apple’s review just fine. All good.</p>


<div>
<figure><img decoding="async" loading="lazy" width="836" height="279" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image-5.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image-5.png 836w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-5-300x100.png 300w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-5-768x256.png 768w" sizes="(max-width: 836px) 100vw, 836px"></figure></div>


<p><em>iOS version has been approved! Hooray!</em></p>







<h3>Uploading the game to Android</h3>



<p>Onto the next fun part – I entered the necessary information and uploaded the game to the Play Store. It seemed to get through review pretty quickly. I was able to get the Closed Beta up and running in no time. I think initial review for my first closed testing version took around half a day. My first impressions with the Google Play Console were pretty good, it’s easy and intuitive to use.</p>



<p>However, for the Play Store, there is a policy that before applying for production (which is required to start open testing and put the game on pre-registration) you need to run closed testing for at least 14 days with at least 20 testers (this seems like a new policy since 2023). All good, that sounds like it will increase the quality of apps uploaded to the store. And I didn’t have problem finding testers at all – as I said, I had over 1000 people applying so I just randomly picked 40 of them for the first phase of testing.</p>



<p>During testing, the testers have found a lot of bugs, ranging from minor ones to game-breaking ones, like microtransactions not working correctly, user data sometimes being rolled back and every action within the game causing serious lag. I fixed the game-breaking ones pretty quickly (as a result though, I didn’t have much time creating new features or writing progress updates). And with the game-breaking bugs gone, I have also fulfilled the requirements of “20 testers in 14 days” so I figured I should apply for production. So I did.</p>



<p>There was a questionnaire about how I found testers, what the testers did, how I incorporated the testers’ feedback, what makes the game stand out, and why I think the game is ready for production. I just answered the questions truthfully. And after Google’s review, on October 4, they approved my request for production! Look, I even received a congratulations email.</p>


<div>
<figure><img decoding="async" loading="lazy" width="631" height="386" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image-3.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image-3.png 631w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-3-300x184.png 300w" sizes="(max-width: 631px) 100vw, 631px"></figure></div>


<p><em>My game has been granted Google Play production access!</em></p>



<p>Great! Now all that’s left is testing some more, improving the game quality and then publishing to production, right? That’s Google’s recommendation as well.</p>



<p>Unfortunately, before I could do that, three days after the above email, on October 7, testers started reporting that in-app purchases suddenly stopped working and the URL to download the game doesn’t work anymore. In a hurry, I checked and was shocked to find out that <strong>my account has been terminated</strong>.</p>


<div>
<figure><img decoding="async" loading="lazy" width="998" height="870" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image-4.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image-4.png 998w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-4-300x262.png 300w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-4-768x669.png 768w" sizes="(max-width: 998px) 100vw, 998px"></figure></div>


<p><em>My account has been terminated… Wait, what?</em></p>



<p>I did a quick search for the issue and learned that a termination for this reason is most likely related to <strong>prior violations, possibly prior violations of associated accounts</strong>. Which is weird. My app got two policy warnings from Google Play before, but both times I fixed it promptly, and according to Google’s <a href="https://support.google.com/googleplay/android-developer/answer/9899234?hl=en" data-type="link" data-id="https://support.google.com/googleplay/android-developer/answer/9899234?hl=en">Enforcement Process</a>, app rejections or violations of this level shouldn’t affect the standing of my account. And my game was even approved for production. Which leads me think of prior violations of associated accounts. But what’s an associated account exactly? I am an indie developer and this is my first and only account. Well, I did add some trusted developers and testers into the internal testing track, but I’ve checked with them and they insist that their accounts are in good standing. Do the 40 testers I added for closed testing count? And why should I be responsible for their prior violations (how do I even know whether they made any violations in the first place)?</p>



<p>I even took extra precaution steps: I used my Google Play Console account on only one device that I use for releasing the game, I didn’t use VPN, I didn’t use the same network with other people, and ensured I didn’t accidentally connect to some public Wi-Fi. And yet Google still decided that my account has a “high risk of abuse” and terminated it.</p>



<p>I have heard a lot of stories about other Android developers having their accounts terminated, but I never thought it would happen to me. I re-read their policies once again just to be sure (by now, I think I’ve read through Google’s policies at least five times). Believing that I did nothing wrong, I sent an appeal.</p>







<h3>Appealing</h3>



<p>The appeal form only allowed me to enter 1000 characters, so this is what I wrote.</p>



<hr>



<p><em>After thoroughly checking the Developer Program Policies and Developer Distribution Agreement, as well as the Policy Coverage policy, I don’t believe I have made any violations that could have led to account termination. I have promptly resolved violations in the past, and my app was even approved for production a few days ago. I am also working closely with my testers in Closed Testing to fix bugs, improve app performance, and ensure that my game “Anti-Idle: Reborn” meets all of Google’s standards and meets user expectations prior to production. I am new to the Developer Program, this is my first account and my first app. I don’t know about “associated accounts” but if this includes the testers’ emails I have added, they are users who volunteered to test my game and I’m not associated with any of their violations (if any). I am always thriving to improve app quality and would greatly appreciate it if you could tell me what is wrong with my app or account so that I could resolve it.</em></p>



<hr>







<p>In hindsight, that was probably not the most useful information that could have fit into 1000 characters, but that’s all I could think of at the time.</p>



<p>I received a system email saying that my appeal would be reviewed by a specialist, subsequently followed by an email with the name of a person at Google (presumably the “specialist”).</p>



<hr>



<p><em>Hi developers at Tukkun,</em></p>



<p><em>Thanks for contacting the Google Play team.</em></p>



<p><em>I’ve received your appeal and I appreciate your patience while I look into it.</em></p>



<p><em>I’ll let you know as soon as I have any additional information to share. Please let me know if you have any questions in the meantime.</em></p>



<p><em>[…]</em></p>



<p><em>Regards,<br>[Name of Google specialist]</em></p>



<hr>







<p>I subsequently sent some additional information in relatively lengthy emails, basically everything that I can think of.</p>



<ul>
<li>Any information I know about my prior violations (I’ve promptly resolved them anyway)</li>



<li>How I am “the real Tukkun” and have rights to the things I’m using within the game (basically the same things I’ve sent Apple)</li>



<li>How I’ve put my heart and soul into the development of Anti-Idle: Reborn and that it is a very anticipated release. I’ve even sent screenshots of the game’s design files</li>



<li>Anything I know about what’s possibly considered “associated accounts”</li>



<li>Anything else that I think might be the problem</li>
</ul>



<p>I just said everything I can know of with all of my honesty, and said that whatever the problem is I am committed to resolving it. I still have no idea what the exact problem is though. Of course, in my emails, I tried asking for more information too.</p>



<p>However, both times I contacted them, they responded with the exact same email:</p>



<hr>



<p><em>Hi developers at Tukkun,</em></p>



<p><em>Thanks again for contacting the Google Play team.</em></p>



<p><em>I’ve received your appeal and I appreciate your patience while I look into it.</em></p>



<p><em>I’ll let you know as soon as I have any additional information to share.</em></p>



<p><em>Regards,<br>[Name of Google specialist]</em></p>



<hr>







<p>They didn’t respond instantly, but several hours after I sent the information. And to be fair they did say “thanks <span>again</span>” (they know it’s not the first time I contacted them), but there’s no other useful information in the email. At this point I’m not even sure if that’s an actual human or just an automated email delayed to feel human. I wouldn’t even be surprised if Google started giving AI unique “names” to make them sound like human specialists.</p>



<p>According to Google, it can take up to 7 days for them to make a decision. As of the writing of this post, it is the 4th day. There’s still time and I guess “I appreciate your patience” is still better than a rejection, but I am beginning to get impatient and this is affecting my future plans for the development of Anti-Idle: Reborn. And the Closed Beta testers are just as impatient as I am.</p>



<p>I still believe that I have done nothing wrong, and I hoped it would be easy to show my good faith (just like how Apple immediately re-reviewed my app after I sent the evidence that I am indeed Tukkun), but I’m starting to get more and more worried as each day passes without any new information. And from what I’ve read about these appeals, most of the time they just get rejected for vague reasons.</p>



<p>And that’s it. Over 15 years of game development, first app on Android with over 1 year of development, and my career as an Android game developer is at stake for no reason, even before the game is released.</p>



<p>Dear everyone who is looking forward to the release of Anti-Idle: Reborn on Android, thank you for your continued support and your interest in the game. While I can’t make any promises under the current situation, I will keep you updated with any new information.</p>



<p>Dear Google, thank you for providing a trustworthy place for app developers to provide apps to billions of users. Once again, I would like to emphasize that I understand the need for thorough app reviews and termination of accounts that violate the rules. It is what allows users to trust the Google Play Store to download and use apps. However, it also needs trust from developers so they can confidently develop great apps without the fear of everything being erased without prior warning and for no reason.</p>



<p>Please, tell me what I am doing wrong and what I can do to have my account and my game restored. Anti-Idle: Reborn is my hopes and dreams, and a large community is waiting for it to become a reality.</p>



<hr>



<p><em>In case anyone at Google is reading this, my appeal ticket number is <strong>6-1733000037134</strong>, and my game’s package ID (before it was removed from Google Play) is <strong>com.tukkun.anti.idle.reborn</strong></em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Lisp compiler to RISC-V written in Lisp (231 pts)]]></title>
            <link>http://www.ulisp.com/show?4Y20</link>
            <guid>41808696</guid>
            <pubDate>Fri, 11 Oct 2024 11:56:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.ulisp.com/show?4Y20">http://www.ulisp.com/show?4Y20</a>, See on <a href="https://news.ycombinator.com/item?id=41808696">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

<p>11th October 2024</p>
<p>This is a simple experimental Lisp compiler, written in uLisp, that will compile a Lisp function into RISC-V machine code. You can run the compiler on the RISC-V core of a Raspberry Pi Pico 2 (or another RP2350-based board):</p>
<p><img src="http://www.ulisp.com/pictures/3j/raspberrypipico2.jpg" alt="RaspberryPiPico2.jpg" width="600" height="275"></p>
<p>It's based on my earlier project&nbsp;<a href="http://www.ulisp.com/show?4W2I">A Lisp compiler to ARM written in Lisp</a>.</p>
<h4><span>Introduction</span></h4>
<p>When I added the facility of executing machine code to uLisp I had in mind the eventual goal of being able to compile uLisp functions into machine code, and this is a first step in that direction.</p>
<p>The nice thing about compiling Lisp is that you don't have to write a tokeniser or parser, because Lisp programs are already in a consistent structure that can be processed by another Lisp program.</p>
<p>The compiler program is written in the subset of Common Lisp supported by uLisp, and will run on the RISC-V core of a RP2350-based board; I used a Raspberry Pi Pico 2. You can also run it using Common Lisp on a laptop or desktop computer, and display the code it generates, but of course you won't be able to run the RISC-V machine code because Common Lisp doesn't have uLisp's <strong>defcode</strong> command.</p>
<p>I got my initial inspiration for this compiler from Peter Norvig's book "Paradigms of Artificial Intelligence Programming" <sup id="cite_ref1"><a href="#cite_note1">[1]</a></sup>.</p>
<h4>Resources</h4>
<p>To use the compiler you first need to load the RISC-V assembler from:&nbsp;<a href="http://www.ulisp.com/list?31OE">RISC-V assembler in uLisp</a>.</p>
<p>Get the full source of the compiler here:&nbsp;<a href="http://www.ulisp.com/list?4Y4Q">Lisp compiler for RISC-V</a>.</p>
<p>Or from GitHub here:&nbsp;<a href="https://github.com/technoblogy/lisp-arm-compiler" target="_blank">https://github.com/technoblogy/lisp-arm-compiler</a>.</p>
<p>For information about setting up uLisp on a Raspberry Pi Pico 2 see:&nbsp;<a href="http://www.ulisp.com/show?4X21">Raspberry Pi Pico 2</a>.</p>
<h4>Using the compiler</h4>
<p>To run the compiler you simply call <strong>compile</strong> on a Lisp function; for example:</p>
<pre>(compile 'fibonacci)</pre>
<p>The function will be compiled into a machine code function, replacing the original Lisp code, so that calling <strong>fibonacci</strong> will now execute the RISC-V machine-code version.</p>
<p>You can also display the code generated for an expression by calling <strong>comp</strong> on the expression; for example:</p>
<pre>(pprint (comp '(* 13 17)))

(:integer
  ($li 'a0 13)
  ($addi 'sp 'sp -4)
  ($sw 'a0 0 '(sp))
  ($li 'a0 17)
  ($lw 'a1 0 '(sp))
  ($addi 'sp 'sp 4)
  ($mul 'a0 'a1 'a0))</pre>
<p>The <strong>:integer</strong> prefix shows that the result is an integer; see below.</p>
<p>For examples of several simple Lisp programs that it will successfully compile see <a href="#examples">Examples</a> below. These also give a comparison of the speed of the Lisp and machine-code versions.</p>
<h4>Specification</h4>
<p>The compiler understands the following Lisp objects:</p>
<p><strong>Defining variables and functions:</strong> defun, setq</p>
<p><strong>Symbols:</strong> nil, t</p>
<p><strong>List functions:</strong> car, cdr</p>
<p><strong>Arithmetic functions:</strong> +, -, *, /, mod, 1+, 1-</p>
<p><strong>Arithmetic comparisons:</strong> =, &lt;, &lt;=, &gt;, &gt;=, /=</p>
<p><strong>Conditionals:</strong> if, and, or</p>
<h4>Tail-call optimisation</h4>
<p>Although the compiler doesn't include any iteration constructs, it does provide tail-call optimisation which can make recursive programs as efficient as iterative ones. Consider this recursive program to add two positive numbers:</p>
<pre>(defun add (a b)
  (if (= b 0) a
    (add (+ a 1) (- b 1))))</pre>
<p>On a system without tail-call optimisation, evaluating:</p>
<pre>(add 10000 10000)</pre>
<p>will probably fail, because it requires 10000 stack frames to store the intermediate results. This compiler recognises that the recursive call to <strong>add</strong> can be replaced by a jump to the start of the program, and so it has no problem evaluating it. For a more sensible example see <strong><a href="#factor">factor</a></strong> below.</p>
<h3>How the compiler works</h3>
<h4>Register usage</h4>
<p>To avoid needing to keep track of register usage the compiler makes use of the stack to pass values to an expression, and store the value returned by an expression.</p>
<p>The following table shows how the RISC-V registers are used within the compiler:</p>
<table>
<thead>
<tr>
<td><strong>Registers</strong></td>
<td><strong>Use</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td>a0 a1 a2 a3</td>
<td>Used to pass the parameters to the main function's arguments.</td>
</tr>
<tr>
<td>a0</td>
<td>Contains the value returned by the main function.</td>
</tr>
<tr>
<td>a4 a5 a6 a7</td>
<td>Contain copies of the function arguments within the function.</td>
</tr>
<tr>
<td>a0 a1</td>
<td>Used to pass the arguments to each operator.</td>
</tr>
<tr>
<td>a0</td>
<td>Used to return the value from each operator.</td>
</tr>
<tr>
<td>s0 to s11</td>
<td>Local variables.</td>
</tr>
</tbody>
</table>
<h4>Compiling an expression</h4>
<p>The following steps show the sequence of compiling an expression, such as:</p>
<pre>(* x 13)</pre>
<ul>
<li>Code is generated to evaluate each of the arguments, in this case <strong>x</strong> and 13, and each result is pushed onto the stack, apart from the last which is left in <strong>a0</strong>.</li>
<li>The first value is popped from the stack into register&nbsp;<strong>a1.</strong></li>
<li>The function, in this case *, is then evaluated for <strong>a1</strong> and <strong>a0</strong>, with the result in <strong>a0</strong>.</li>
</ul>
<p>This stack-based approach ensures that a more complex expression, such as:</p>
<pre>(* (- x 1) (+ x 13))</pre>
<p>will also compile into correct code, without conflicts between registers.</p>
<h4>Calling the function recursively</h4>
<p>The compiler supports calling a function recursively from within the function itself. Because the registers corresponding to the parameters and local variables would be overwritten by the recursive call they are stored on the stack around the function call.</p>
<p>There are several recursive functions in the examples below.</p>
<h4>Types</h4>
<p>For boolean operations I decided to represent <strong>nil</strong> as 0, and <strong>t</strong> as 1. A problem I hadn't anticipated was that I would need to keep track of what type of object each function returned, integer or boolean. For example, consider the problem of compiling the statement:</p>
<pre>(and x y)</pre>
<p>If <strong>x</strong> has the value 0 and <strong>y</strong> has the value 7 this should return 7. However, if <strong>x</strong> has the value <strong>nil</strong>&nbsp;and&nbsp;<strong>y</strong>&nbsp;has the value 7 this should return <strong>nil</strong>. Representing <strong>nil</strong> as zero leads to an ambiguity.</p>
<p>I solved this by returning a type, <strong>:integer</strong> or <strong>:boolean</strong>, with each compiled expression, according to the following rules:</p>
<ul>
<li>Predicates, and <strong>t</strong> or <strong>nil</strong>, always return a <strong>:boolean</strong>.</li>
<li>Arithmetic operations always return an <strong>:integer</strong>.</li>
<li>An <strong>if</strong> form requires a <strong>:boolean</strong> test form and returns an <strong>:integer</strong>.</li>
<li>A <strong>progn</strong>&nbsp;or <strong>let</strong> block returns the type of its last expression.</li>
</ul>
<p>An item with an ambiguous type returns the type <strong>nil</strong>.</p>
<h4>Running the examples</h4>
<p>I used the following simple examples to test the compiler. Before compiling a new function you might want to remove the previous one from memory using&nbsp;<strong>makunbound</strong>&nbsp;to free up the code memory before compiling the next function; for example:</p>
<pre>(makunbound 'fibonacci)</pre>
<p>Alternatively you could increase the amount of memory available for machine code by editing the directive such as:</p>
<pre>#define CODESIZE 256&nbsp;</pre>
<p>before uploading uLisp to your board.</p>
<h3 id="examples">Examples</h3>
<p>The following examples take integer arguments and return an integer result.</p>
<h4 id="factor">Factor</h4>
<p>This function takes a simple approach to finding the least prime factor of a number:</p>
<pre>(defun factor (n d)
  (if (&gt; (* d d) n) n
   (if (= 0 (mod n d)) d
     (factor n (1+ d)))))</pre>
<p>It should be called with n equal to the number to be factorized, and d=2. It takes advantage of the compiler's tail-call optimisation, which makes it as efficient as an iterative solution. If the number is prime,&nbsp;factor&nbsp;will print the number itself.</p>
<p>To find the least prime factor of&nbsp;2146654199 (46327 x 46337):</p>
<p>Lisp version:</p>
<pre>&gt; (time (factor 2146654199 2))
46327
Time: 5.4 s</pre>
<p>Compiled version:</p>
<pre>&gt; (time (factor 2146654199 2))
46327
Time: 19 ms</pre>
<p>You can use the above&nbsp;function as the basis for a simple recursive routine to factorize a number into a list of its prime factors:</p>
<pre>(defun factorize (n)
  (let ((f (factor n 2)))
    (if (= n f) (list n) (cons f (factorize (/ n f))))))</pre>
<p>For example:</p>
<pre>&gt; (factorize 731731731)
(3 17 43 333667)</pre>
<h4>Takeuchi function</h4>
<p>This is a version of the highly-recursive benchmark I use for comparing versions of Lisp <sup id="cite_ref2"><a href="#cite_note2">[2]</a></sup>:</p>
<pre>(defun tak (x y z)
  (if (&gt;= y x) z
    (tak
     (tak (1- x) y z)
     (tak (1- y) z x)
     (tak (1- z) x y))))</pre>
<p>Lisp version:</p>
<pre>&gt; (time (tak 18 12 6))
7
Time: 4.1 s
</pre>
<p>Compiled version</p>
<pre>&gt; (time (tak 18 12 6))
7
Time: 16 ms</pre>
<h4>Factorial</h4>
<p>This is a recursive implementation of the factorial function:</p>
<pre>(defun fact (n)
  (if (&lt;= n 1) 1
    (* n (fact (- n 1)))))</pre>
<p>Lisp version:</p>
<pre>&gt; (time (fact 12))
479001600
Time: 1 ms
</pre>
<p>Compiled version</p>
<pre>&gt; (time (fact 12))
479001600
Time: 0 ms</pre>
<h4>Fibonacci</h4>
<p>This is a recursive implementation of the Fibonacci series:</p>
<pre>(defun fibonacci (n)
  (if (&lt; n 3) 1
    (+ (fibonacci (- n 1)) (fibonacci (- n 2)))))</pre>
<p>Lisp version:</p>
<pre>&gt; (time (fibonacci 27))<br>196418
Time: 50.5 s
</pre>
<p>Compiled version</p>
<pre>&gt; (time (fibonacci 27))
196418
Time: 80 ms</pre>
<h4>Greatest Common Divisor</h4>
<p>A recursive algorithm to calculate the greatest common divisor of two integers.</p>
<pre>(defun gcd (a b)
  (if (= b 0) a
   (gcd b (mod a b))))</pre>
<p>Lisp version:</p>
<pre>&gt; (time (gcd 2032460032 2056252672))
256
Time: 1 ms
</pre>
<p>Compiled version</p>
<pre>&gt; (time (gcd 2032460032 2056252672))
256
Time: 0 ms</pre>
<h4>Hofstadter Q sequence</h4>
<p>This is one of several recursive sequences described in Douglas Hofstadter's book "Gödel, Escher, Bach: an Eternal Golden Braid". It is defined as follows:</p>
<pre>(defun q (n)
  (if (&lt;= n 2) 1
    (+
     (q (- n (q (- n 1))))
     (q (- n (q (- n 2)))))))</pre>
<p>It is related to the Fibonacci sequence, except that in this case&nbsp;the two preceding terms specify how far to go back in the sequence to find the two terms to be summed.</p>
<p>Lisp version:</p>
<pre>&gt; (time (q 21))
12
Time: 8.6 s
</pre>
<p>Compiled version</p>
<pre>&gt; (time (q 21))
12
Time: 25 ms</pre>
<h4>Two-dimensional recursive function Q2</h4>
<p>This function Q2 is my two-dimensional extension of the Hofstadter Q sequence <sup id="cite_ref3"><a href="#cite_note3">[3]</a></sup>:</p>
<pre>(defun q2 (x y)
  (if (or (&lt; x 1) (&lt; y 1)) 1
    (+ (q2 (- x (q2 (1- x) y)) y)
       (q2 x (- y (q2 x (1- y)))))))</pre>
<p>Lisp version:</p>
<pre>&gt; (time (q2 7 8))
31
Time: 13.8 s
</pre>
<p>Compiled version</p>
<pre>&gt; (time (q2 7 8))
31
Time: 50 ms</pre>
<h4>Number of combinations - nCr</h4>
<p>This is a simple but very inefficient way of recursively calculating nCr, based on Pascal's Triangle:</p>
<pre>(defun ncr (n r)
  (if (or (= r 0) (= r n)) 1
    (+ (ncr (1- n) (1- r)) (ncr (1- n) r))))</pre>
<p>For example, to calculate the number of possible poker hands from a pack of cards:</p>
<p>Lisp version:</p>
<pre>&gt; (time (ncr 52 5))
2598960
Time: 615.5 s
</pre>
<p>Compiled version</p>
<pre>&gt; (time (ncr 52 5))
2598960
Time: 1.7 s</pre>
<h3 id="listexamples">List examples</h3>
<p>Any of the arguments to a machine-code function can be a list, in which case the address of the list is passed to the routine in the corresponding parameter. You can then use the functions <strong>car</strong> and <strong>cdr</strong> to process the elements in the list.</p>
<h4>Dot product</h4>
<p>This recursive function calculates the dot product of two vectors:</p>
<pre>(defun dot-product (a b)
  (if (and a b)
      (+ (* (car a) (car b)) (dot-product (cdr a) (cdr b)))<br>    0))</pre>
<p>It can handle two vectors of arbitrary length provided they are the same length.</p>
<p>For example, to calculate the following dot product:</p>
<p>(987 654 321)&nbsp;•&nbsp;(963 852 741) = 987&nbsp;×&nbsp;963 + 654&nbsp;×&nbsp;852 + 321&nbsp;×&nbsp;741 = 1745550</p>
<p>Lisp version:</p>
<pre>&gt; (time (dot-product '(987 654 321) '(963 852 741)))
1745550
Time: 0 ms
</pre>
<p>Compiled version</p>
<pre>&gt; (time (dot-product '(987 654 321) '(963 852 741)))
1745550
Time: 0 ms</pre>
<h3>Compiler source</h3>
<p>Here's a description of the source of the compiler.</p>
<h4>Invoking the compiler</h4>
<p>To compile a Lisp function you simply give the command compile followed by the name of the function; for example, to compile the <strong>fibonacci</strong> function:</p>
<pre>(compile 'fibonacci)</pre>
<p>Here's the definition of the command <strong>compile</strong>:</p>
<pre>(defun compile (name)
  (if (eq (car (eval name)) 'lambda)
    (eval (comp (cons 'defun (cons name (cdr (eval name))))))
 "Not a Lisp function"))</pre>
<h4>Main compiler function</h4>
<p>The main function&nbsp;<strong>comp</strong> returns the compiled code for an expression or form, as a list of assembler instructions prefixed by the type, <strong>:integer</strong> or <strong>:boolean</strong>:</p>
<pre>(defun comp (x &amp;optional env tail)
  (cond
   ((null x) (type-code :boolean '(($li 'a0 0))))
   ((eq x t) (type-code :boolean '(($li 'a0 1))))
   ((symbolp x) (comp-symbol x env tail))
   ((atom x) (type-code :integer (list (list '$li ''a0 x))))
   (t (let ((fn (first x)) (args (rest x)))
        (case fn
          (defun (setq *label-num* 0)
                 (setq env (mapcar #'(lambda (x y) (cons x y)) (second args) *locals*))
                 (comp-defun (first args) (second args) (cddr args) env))
          (progn (comp-progn args env tail))
          (if    (comp-if (first args) (second args) (third args) env tail))
          (setq  (comp-setq args env tail))
          (t     (comp-funcall fn args env tail)))))))</pre>
<p>The function <strong>comp</strong> takes the item <strong>x</strong> to be compiled,&nbsp;the current environment <strong>env</strong> associating each local variable with a register, and a flag <strong>tail</strong>&nbsp;which is true if the item has no successors.</p>
<p>Each of the different types of form are handled by separate functions such as <strong>comp-defun</strong>, <strong>comp-if</strong>, and <strong>comp-progn</strong>.</p>
<h4>Utilities</h4>
<p>The compiler uses the following utility functions:</p>
<p>The functions <strong>push-regs</strong> and <strong>pop-regs</strong> generate instructions to push a list of registers to the stack, and pop a list of registers from the stack:</p>
<pre>(defun push-regs (&amp;rest regs)
  (let ((n -4))
  (append
   (list (list '$addi ''sp ''sp (* -4 (length regs))))
   (mapcar #'(lambda (reg) (list '$sw (list 'quote reg) (incf n 4) ''(sp))) regs))))

(defun pop-regs (&amp;rest regs)
  (let ((n (* 4 (length regs))))
  (append
   (mapcar #'(lambda (reg) (list '$lw (list 'quote reg) (decf n 4) ''(sp))) regs)
   (list (list '$addi ''sp ''sp (* 4 (length regs)))))))</pre>
<p>The function <strong>mappend</strong> applies a function to the elements of a list, and the results, which should be lists, are appended together:</p>
<pre>(defun mappend (fn lst)
  (apply #'append (mapcar fn lst)))</pre>
<p>The function <strong>type-code</strong> adds a code type label to the front of a list of assembler instructions, and the functions code-type and code return the code type label, and the code list, respectively:</p>
<pre>(defun type-code (type code) (cons type code))

(defun code-type (type-code) (car type-code))

(defun code (type-code) (cdr type-code))</pre>
<p>The function <strong>checktype</strong> gives an error if the value returned is not the correct type:</p>
<pre>(defun checktype (fn type check)
  (unless (or (null type) (null check) (eq type check))
    (error "Argument to '~a' must be ~a not ~a~%" fn check type)))</pre>
<p>The lists <strong>*params*</strong> and <strong>*locals*</strong> list the registers available for use in the compiler:</p>
<pre>(defvar *params* '(a0 a1 a2 a3))

(defvar *locals* '(a4 a5 s0 s1 a6 a7 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11))</pre>
<p>Finally, <strong>gen-label</strong> generates a new label for use in branches and jumps:</p>
<pre>(defvar *label-num* 0)

(defun gen-label ()
  (read-from-string (format nil "lab~d" (incf *label-num*))))</pre>
<p>The remaining functions handle the compiling of specific types of Lisp form:</p>
<h4>Symbols</h4>
<p>The environment is represented by an association list giving the register associated with each variable, such as:</p>
<pre>((y . r5) (x . r4))</pre>
<p>The function <strong>comp-symbol</strong> looks up a symbol in the association list and returns the appropriate register:</p>
<pre>(defun comp-symbol (x env)
  (let ((reg (cdr (assoc x env))))
    (type-code nil (list (list '$mv ''a0 (list 'quote reg))))))</pre>
<h4>Assignment</h4>
<p>The function <strong>comp-setq</strong> handles assignment to a variable:</p>
<pre>(defun comp-setq (args env tail)
  (let ((value (comp (second args) env tail))
        (reg (cdr (assoc (first args) env))))
    (type-code 
     (code-type value) 
     (append (code value) (list (list '$mv (list 'quote reg) ''a0))))))</pre>
<h4>Function definition</h4>
<p>The definition of the function being compiled is handled by <strong>comp-defun</strong>:</p>
<pre>(defun comp-defun (name args body env)
  (setq *used-params* (subseq *locals* 0 (length args)))
  (append 
   (list 'defcode name args)
   (list name)
   (apply #'append 
          (mapcar #'(lambda (x y) (list (list '$mv (list 'quote x) (list 'quote y))))
                  *used-params* *params*))
   (code (comp-progn body env t))))</pre>
<h4>Progn special form</h4>
<p>The function <strong>comp-progn</strong> compiles a <strong>progn</strong> form:</p>
<pre>(defun comp-progn (exps env tail)
  (let* ((len (1- (length exps)))
         (nlast (subseq exps 0 len))
         (last1 (nth len exps))
         (start (mappend #'(lambda (x) (append (code (comp x env t)))) nlast))
         (end (comp last1 env tail)))
    (type-code (code-type end) (append start (code end)))))</pre>
<p>It compiles code to evaluate each expression in the body of the <strong>progn</strong>, discarding all but the last results, and returns the type of the last form as the type of the whole block.</p>
<h4>If special form</h4>
<p>The function <strong>comp-if</strong> compiles the code for an <strong>if</strong> special form:</p>
<pre>(defun comp-if (pred then else env tail)
  (let ((lab1 (gen-label))
        (lab2 (gen-label))
        (test (comp pred env nil)))
    (checktype 'if (car test) :boolean)
    (type-code :integer
               (append
                (code test) (list (list '$beqz ''a0 lab1))
                (code (comp then env t)) (list (list '$j lab2) lab1)
                (code (comp else env tail)) (list lab2)
                (when tail '(($ret)))))))</pre>
<h4>Function calls</h4>
<p>Finally, <strong>comp-funcall</strong> compiles code for function calls to the built-in functions, or a recursive call to the main function:</p>
<pre>(defun comp-funcall (f args env tail)
  (let ((test (assoc f '((&lt; . $slt) (&gt; . $sgt))))
        (teste (assoc f '((= . $seqz) (/= . $snez))))
        (testn (assoc f '((&gt;= . $slt) (&lt;= . $sgt))))
        (logical (assoc f '((and . $and) (or . $or))))
        (arith1 (assoc f '((1+ . 1) (1- . -1))))
        (arith (assoc f '((+ . $add) (- . $sub) (* . $mul) (/ . $div) (mod . $rem)))))
    (cond
     ((or test teste testn)
      (type-code :boolean
                   (append
                    (comp-args f args 2 :integer env)
                    (pop-regs 'a1)
                    (cond
                     (test (list (list (cdr test) ''a0 ''a1 ''a0)))
                     (teste (list '($sub 'a0 'a1 'a0) (list (cdr teste) ''a0 ''a0)))
                     (testn (list (list (cdr testn) ''a0 ''a1 ''a0) '($xori 'a0 'a0 1))))
                    (when tail '(($ret))))))
     (logical 
      (type-code :boolean
                 (append
                  (comp-args f args 2 :boolean env)
                  (pop-regs 'a1)
                  (list (list (cdr logical) ''a0 ''a0 ''a1))
                  (when tail '(($ret))))))
     (arith1
      (type-code :integer
                 (append
                  (comp-args f args 1 :integer env)
                  (list (list '$addi ''a0 ''a0 (cdr arith1)))
                  (when tail '(($ret))))))
     (arith
      (type-code :integer 
                 (append
                  (comp-args f args 2 :integer env)
                  (pop-regs 'a1)
                  (list (list (cdr arith) ''a0 ''a1 ''a0))
                  (when tail '(($ret))))))
     ((member f '(car cdr))
      (type-code :integer
                 (append
                  (comp-args f args 1 :integer env)
                  (if (eq f 'cdr) (list '($lw 'a0 4 '(a0)))
                    (list '($lw 'a0 0 '(a0)) '($lw 'a0 4 '(a0))))
                  (when tail '(($ret))))))
     (t ; function call
      (type-code :integer 
                 (append
                  (comp-args f args nil :integer env)
                  (when (&gt; (length args) 1)
                    (append
                     (list (list '$mv (list 'quote (nth (1- (length args)) *params*)) ''a0))
                     (apply #'pop-regs (subseq *params* 0 (1- (length args))))))
                  (cond
                   (tail (list (list '$j f)))
                   (t (append
                       (apply #'push-regs (cons 'ra (reverse *used-params*)))
                       (list (list '$jal f))
                       (apply 'pop-regs (append *used-params* (list 'ra))))))))))))</pre>
<p>The arithmetic comparisons take advantage of the RISC-V instructions such as <strong>slt</strong>&nbsp;(Set if less than), which set the destination register to 0 if the comparison is false, and to 1 if it's true; this provides the required boolean result without needing a branch.</p>
<p>The function <strong>comp-funcall</strong> uses the routine&nbsp;<strong>comp-args</strong>&nbsp;to generate code to compile each of the arguments to a function:</p>
<pre>(defun comp-args (fn args n type env)
  (unless (or (null n) (= (length args) n))
    (error "Incorrect number of arguments to '~a'" fn))
  (let ((n (length args)))
    (mappend #'(lambda (y)
                 (let ((c (comp y env nil)))
                   (decf n)
                   (checktype fn type (code-type c))
                   (if (zerop n) (code c) (append (code c) (push-regs 'a0)))))
             args)))</pre>
<h3>Appendix</h3>
<p>The following example shows the code generated by a simple function,&nbsp;<strong>rec</strong>, a recursive function related to the factorial function:</p>
<pre>(defun rec (n)
  (1+ (* n (if (= n 0) 0 (rec (1- n))))))</pre>
<p>Compiling this gives the following RISC-V machine code:</p>
<pre>&gt; (compiler 'rec)
0000      rec
0000 872a ($mv 'a4 'a0)
0002 853a ($mv 'a0 'a4)
0004 1171 ($addi 'sp 'sp -4)
0006 c02a ($sw 'a0 0 '(sp))
0008 853a ($mv 'a0 'a4)
000a 1171 ($addi 'sp 'sp -4)
000c c02a ($sw 'a0 0 '(sp))
000e 4501 ($li 'a0 0)
0010 4582 ($lw 'a1 0 '(sp))
0012 0111 ($addi 'sp 'sp 4)
0014 8533 ($sub 'a0 'a1 'a0)
0016 40a5 
0018 3513 ($seqz 'a0 'a0)
001a 0015 
001c c119 ($beqz 'a0 lab1)
001e 4501 ($li 'a0 0)
0020 a819 ($j lab2)
0022      lab1
0022 853a ($mv 'a0 'a4)
0024 157d ($addi 'a0 'a0 -1)
0026 1161 ($addi 'sp 'sp -8)
0028 c006 ($sw 'ra 0 '(sp))
002a c23a ($sw 'a4 4 '(sp))
002c f0ef ($jal rec)
002e fd5f 
0030 4712 ($lw 'a4 4 '(sp))
0032 4082 ($lw 'ra 0 '(sp))
0034 0121 ($addi 'sp 'sp 8)
0036      lab2
0036 4582 ($lw 'a1 0 '(sp))
0038 0111 ($addi 'sp 'sp 4)
003a 8533 ($mul 'a0 'a1 'a0)
003c 02a5 
003e 0505 ($addi 'a0 'a0 1)
0040 8082 ($ret)</pre>
<p>Trying it out:</p>
<pre>&gt; (rec 12)
1302061345</pre>
<p>This example demonstrates how the RISC-V Lisp assembler takes advantage of 16-bit compressed instructions where possible, instead of the equivalent full 32-bit instructions.</p><hr>
<ol>
<li id="cite_note1"><a href="#cite_ref1">^</a> Norvig, Peter "Paradigms of Artificial Intelligence Programming" Morgan Kaufmann Publishers, Inc, San Francisco, 1992, pp 784-833, available as a PDF <a href="https://github.com/norvig/paip-lisp" target="_blank">paip-lisp</a> on GitHub.</li>
<li id="cite_note2"><a href="#cite_ref2">^</a> <a href="http://www.ulisp.com/show?1EO1#tak">Benchmarks - Takeuchi function</a>.</li>
<li id="cite_note3"><a href="#cite_ref3">^</a> <a href="http://www.ulisp.com/show?1EO1#q2">Benchmarks - Two-dimensional recursive function Q2</a>.</li>
</ol>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding the Limitations of Mathematical Reasoning in Large Language Models (200 pts)]]></title>
            <link>https://arxiv.org/abs/2410.05229</link>
            <guid>41808683</guid>
            <pubDate>Fri, 11 Oct 2024 11:55:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.05229">https://arxiv.org/abs/2410.05229</a>, See on <a href="https://news.ycombinator.com/item?id=41808683">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.05229">View PDF</a>
    <a href="https://arxiv.org/html/2410.05229v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of <a href="http://models.our/" rel="external noopener nofollow">this http URL</a> findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Seyed Iman Mirzadeh [<a href="https://arxiv.org/show-email/d6cde35e/2410.05229">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 7 Oct 2024 17:36:37 UTC (5,949 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fundamental physics is dying? [video] (104 pts)]]></title>
            <link>https://www.youtube.com/watch?v=cBIvSGLkwJY</link>
            <guid>41808127</guid>
            <pubDate>Fri, 11 Oct 2024 10:21:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=cBIvSGLkwJY">https://www.youtube.com/watch?v=cBIvSGLkwJY</a>, See on <a href="https://news.ycombinator.com/item?id=41808127">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Initial CUDA Performance Lessons (153 pts)]]></title>
            <link>https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/</link>
            <guid>41808013</guid>
            <pubDate>Fri, 11 Oct 2024 10:01:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/">https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/</a>, See on <a href="https://news.ycombinator.com/item?id=41808013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>I am somehow very late to learning CUDA. I didn’t even know until recently that CUDA is just C++ with a small amount of extra stuff. If I had known that there is so little friction to learning it, I would have checked it out much earlier. But if you come in with C++ habits, you’ll write suboptimal code, so here are some lessons I had to learn to get things to run fast.</p>



<h2>Memory Coalescing</h2>



<p>If you have multiple threads operating on an array in C++, you probably want to iterate like this:</p>


<div><pre title="">std::vector&lt;T&gt; vec = ...;
size_t per_thread = vec.size() / num_threads;
T * my_slice = vec.data() + per_thread * my_thread_i;
for (size_t i = 0; i &lt; per_thread; ++i) {
    do_something(my_slice[i]);
}
</pre></div>


<p>Meaning each thread iterates over a contiguous chunk of memory. In CUDA this is going to be slow because you want the threads to load memory together. So if thread 0 loads bytes 0 to 15, then you want thread 1 to load bytes 16 to 31 and thread 2 to load bytes 32 to 47 etc. So the loop instead has to look like this:</p>


<div><pre title="">T * data = ...;
size_t num_elements = ...;
for (int i = my_thread_i; i &lt; num_elements; i += num_threads) {
    do_something(data[i]);
}
</pre></div>


<p>This is called “memory coalescing” where adjacent threads use adjacent memory. On a loop with a small body (dot product) this is 3x faster.</p>



<h2>Most of the Performance is now in Specialized Hardware</h2>



<p>Many years ago Sean Parent presented a graph that breaks down where the performance is in a modern PC. I’m reproducing it with current numbers here:</p>



<figure><a href="https://probablydance.com/wp-content/uploads/2024/10/pc_performance-2.png"><img data-attachment-id="11747" data-permalink="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/pc_performance-2/" data-orig-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance-2.png" data-orig-size="1480,290" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pc_performance" data-image-description="" data-image-caption="" data-medium-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance-2.png?w=300" data-large-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance-2.png?w=650" tabindex="0" role="button" width="1480" height="290" src="https://probablydance.com/wp-content/uploads/2024/10/pc_performance-2.png" alt=""></a></figure>



<p>What we see here is the breakdown of theoretical performance in a PC with a Ryzen 9950X and a RTX 4090. The overall theoretical performance is ~95 TFLOPS. These are theoretical, so for example the single-threaded CPU performance is just “5.7 Ghz * 4 instructions per cycle = 22.8 GFLOPS”. That’s the blue line that you can’t see because it’s such a tiny fraction. If you use all 32 threads and AVX 512 you can multiply that performance by 32*16 = 512 to fill up the red and yellow parts of the graph. But if you really want performance, you need to use the GPU which gives you the green part of the graph.</p>



<p>But while these are current numbers, it’s missing most of the GPU performance. The GPU now has specialized hardware for machine learning and for raytracing. When you add those to the graph you get current performance.</p>



<figure><a href="https://probablydance.com/wp-content/uploads/2024/10/pc_performance_with_specialized_hardware-1.png"><img data-attachment-id="11750" data-permalink="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/pc_performance_with_specialized_hardware-2/" data-orig-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance_with_specialized_hardware-1.png" data-orig-size="1472,340" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pc_performance_with_specialized_hardware" data-image-description="" data-image-caption="" data-medium-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance_with_specialized_hardware-1.png?w=300" data-large-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance_with_specialized_hardware-1.png?w=650" tabindex="0" role="button" width="1472" height="340" src="https://probablydance.com/wp-content/uploads/2024/10/pc_performance_with_specialized_hardware-1.png" alt=""></a></figure>



<p>This is the same graph plus specialized hardware. For the tensor core I chose the TFLOPS when doing BF16 matrix multiplies. Meaning it’s not exactly a fair comparison because it operates on lower precision (the output is in 32 bits though) but everyone uses this for matrix multiplies and thinks it’s fine.</p>



<p>The point is that now most of the performance in your PC is in specialized chips. If you’re just writing straightforward CUDA code, you’re leaving most of the performance on the table. The graph gets even more lopsided when looking at a deep learning GPU like the H100:</p>



<figure><a href="https://probablydance.com/wp-content/uploads/2024/10/h100_performance.png"><img data-attachment-id="11755" data-permalink="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/h100_performance/" data-orig-file="https://probablydance.com/wp-content/uploads/2024/10/h100_performance.png" data-orig-size="1440,347" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_performance" data-image-description="" data-image-caption="" data-medium-file="https://probablydance.com/wp-content/uploads/2024/10/h100_performance.png?w=300" data-large-file="https://probablydance.com/wp-content/uploads/2024/10/h100_performance.png?w=650" tabindex="0" role="button" width="1440" height="347" src="https://probablydance.com/wp-content/uploads/2024/10/h100_performance.png" alt=""></a></figure>



<p>Note how the x-axis now goes above 2000 TFLOPS. If you’re not using tensor cores, the GPU is sitting &gt;90% idle. This is changing the algorithms that are used in deep learning. If algorithm A can just do bigger matrix multiplications to get higher quality results, and algorithm B can achieve better quality results by cleverly doing lots of little pieces of work, people will choose algorithm A.</p>



<h2>Different Kinds of Memory</h2>



<p>Memory is more complicated in CUDA, but with my limited understanding so far I think of CUDA as having three different types of memory:</p>



<ol>
<li>Normal memory</li>



<li>Shared memory (faster)</li>



<li>Registers (fastest)</li>
</ol>



<p>Registers are particularly weird. One thread block has 65536 registers, meaning you can store 256k bytes of data in registers. Which is more than you can store in shared memory. I was trying to understand how some cuDNN kernel could possibly be as fast as it was, when I realized that they keep a particular matrix entirely in registers where each thread holds a small part of the matrix.</p>



<p>You get some control over how many registers you have. You can have up to 1024 threads per thread block, meaning you get 64 registers per thread by default. But you could launch fewer threads and get proportionally more registers per thread. If you need, say 150 registers because you want to cache some data, you divide 65536/150 which tells you that you can use 436 threads.</p>



<p>But you’re still just writing in C++ which doesn’t make it easy to say “keep this data in registers.” The best way I found to do this is to keep a fixed-size array on the stack and then use “#pragma unroll” in every single loop that uses that array. The loop needs to be unrolled because every unrolled iteration of the loop needs to refer to different registers.</p>



<p>Shared memory was straightforward in comparison. It allows you to dedicate some cache space for a specific purpose, and the data is shared between threads. So you can use it for two purposes:</p>



<ol>
<li>To communicate between threads</li>



<li>To load data more quickly: If you want to load 512 floats and you have 512 threads, every thread can load one float into shared memory. So you don’t even have to loop.</li>
</ol>



<h2>Sharing is ~Free Within a Warp</h2>



<p>This one was a delight when I saw code doing this for the first time: A warp is 32 threads that share one instruction pointer. They all do the same thing at the same time. So if you e.g. parallelize a dot product, the 32 threads of the warp can sum their results to get the overall result in five steps, using a parallel sum algorithm:</p>



<figure><a href="https://probablydance.com/wp-content/uploads/2024/09/parallel_sum.png"><img data-attachment-id="11736" data-permalink="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/parallel_sum/" data-orig-file="https://probablydance.com/wp-content/uploads/2024/09/parallel_sum.png" data-orig-size="2298,804" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="parallel_sum" data-image-description="" data-image-caption="" data-medium-file="https://probablydance.com/wp-content/uploads/2024/09/parallel_sum.png?w=300" data-large-file="https://probablydance.com/wp-content/uploads/2024/09/parallel_sum.png?w=650" tabindex="0" role="button" loading="lazy" width="2298" height="804" src="https://probablydance.com/wp-content/uploads/2024/09/parallel_sum.png" alt=""></a></figure>



<p>On a CPU this algorithm is impractical because the overhead of keeping the threads in sync is too high. But on a GPU they just are in sync, so sharing is literally five steps:</p>


<div><pre title="">__device__ float add_warp(float x) {
    static constexpr const unsigned all = 0xffffffff;
    x += __shfl_xor_sync(all, x, 1);
    x += __shfl_xor_sync(all, x, 2);
    x += __shfl_xor_sync(all, x, 4);
    x += __shfl_xor_sync(all, x, 8);
    x += __shfl_xor_sync(all, x, 16);
    return x;
}
</pre></div>


<p>I verified that this compiles down to two instructions each. This compiles to 5 SHFL.BFLY instructions plus 5 FADD instructions for the addition. There are no secret locks or barriers here.</p>



<p>This only works within a warp (32 threads). For a thread block, up to 1024 threads, you can use shared memory, which requires using barriers because the threads won’t automatically be in sync. If you need more threads than that and want to share data between them, don’t. (you’ll often want many more threads, you just can’t share data. You need to write out the result to memory and then launch a new thread to work on the new data)</p>



<h2>Parallelism First</h2>



<p>My intuition for how many threads to use was wrong by a lot. If you’re iterating over some data and have to do several non-trivial things to it, it’s probably best to launch one thread for each of the things you want to do. It’s tempting to say “this thread already loaded all the relevant data, it can just do a bit of extra work” but in CUDA it’s better to launch a separate thread for that extra work, even if they both have to load the same data. It’s much cheaper for them to synchronize and share their data than it would be on a CPU.</p>



<p>When I ran Nsight Compute on the first couple versions of my code, the feedback that came back could always be summarized as “you’re barely using the GPU, make it more parallel.”</p>



<p>This also means that you often want to pull your algorithm apart. If there is one part that can run massively parallel (across tens of thousands of threads) and one part that has limited parallelism (say only a few hundred threads) then it’s probably worth to launch those as separate kernels to benefit from the massive parallelism on part of your problem, even if that part is only a small part.</p>



<p>So whenever you try to solve a problem, the first question should not be “how can I make this fast?” but “how can I run this in parallel?” After you solve that, worry about making the parallel code fast.</p>



<h2>Conclusion</h2>



<p>Writing CUDA definitely has a different feeling. It feels more puzzly because it’s so easy to accidentally only use 1% of your GPU. It actually reminds me of TIS-100, especially the trick of distributing data in the registers of multiple threads. But instead of managing a small number of chips you have to figure out how to generate work for tens of thousands of threads. My mental model is that you’ve got a bunch of container ships that can travel at 10% of the speed of light. You’re using them to ship goods around the world. They’re very fast so most of the work is in setting up your harbors so that you can load and unload these container-ships in fractions of a second so that it can sail to do the next thing. It’s not easy to feed these beasts, but if you do it right you can do huge chunks of work in almost no time.</p>



<figure><a href="https://probablydance.com/wp-content/uploads/2024/10/comfyui_00091_.png"><img data-attachment-id="11765" data-permalink="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/comfyui_00091_/" data-orig-file="https://probablydance.com/wp-content/uploads/2024/10/comfyui_00091_.png" data-orig-size="1024,1024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ComfyUI_00091_" data-image-description="" data-image-caption="" data-medium-file="https://probablydance.com/wp-content/uploads/2024/10/comfyui_00091_.png?w=300" data-large-file="https://probablydance.com/wp-content/uploads/2024/10/comfyui_00091_.png?w=650" tabindex="0" role="button" loading="lazy" width="1024" height="1024" src="https://probablydance.com/wp-content/uploads/2024/10/comfyui_00091_.png" alt=""></a></figure>
					</div></div>]]></description>
        </item>
    </channel>
</rss>