<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 09 Nov 2024 06:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[It's legal for police to use deception in interrogations. Some want that to end (204 pts)]]></title>
            <link>https://text.npr.org/nx-s1-4974964</link>
            <guid>42091423</guid>
            <pubDate>Fri, 08 Nov 2024 23:57:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://text.npr.org/nx-s1-4974964">https://text.npr.org/nx-s1-4974964</a>, See on <a href="https://news.ycombinator.com/item?id=42091423">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Ted Bradford says the worst day of his life was when detectives took him into a tiny room to question him about a rape.</p><p>“The whole day it was like accusation after accusation,” he says. “I kept telling them over and over, ‘I didn't do this.’”</p><p>Bradford says the officers in Yakima, Wash., <a href="https://wainnocenceproject.org/stories/ted-bradford/"><u>claimed they had biological evidence</u></a> that would prove he did it, and they weren't going to let him leave until he admitted it.</p><p>“I knew I didn’t do it,” he said. “So I'm thinking, ‘In order to get out of this situation, I could just give them a statement. They’ll test that evidence. It’ll show that I didn’t do it, and then this will all be done with.’”</p><p>After<em> </em>hours of questioning, Bradford confessed to the crime. But the evidence police had – a mask left at the scene – could not be DNA-tested. This was the late nineties and the technology wasn’t there yet.</p><p>Bradford recanted his confession, but was convicted anyway. He was 22 with two small children when he went to prison.</p><p>“Every day I woke up and knew that I shouldn't be there,” he says.</p><p>Advancements in DNA testing helped lead to his exoneration in 2010.</p><p>What happened to Bradford might seem extreme, but nearly 30 years later, the tactic used on him is not. In every state, police officers are allowed to lie to adults during an interrogation. The hope, in many cases, is that they’ll get a person to confess to committing a crime.</p><p>When it comes to children and teenagers, a growing number of states are stopping that practice: Ten have passed laws in recent years effectively banning police from lying to juveniles during interrogations, starting with Illinois in 2021. But some legal advocates are pushing for a deception ban that would apply to everyone, not just kids.</p><h2>‘A quick and relatively straightforward way to close a case’</h2><p>Deception is a powerful law enforcement tool in eliciting confessions, says wrongful convictions attorney Laura Nirider.</p><p>“Police are trained around the country in all 50 states to use deception during interrogation, to lie both about the evidence against a suspect and to lie about the consequences of confessing in order to make it seem not so bad if you just say that you did these things,” she says.</p><p>Police can go into an interrogation room with a suspect, Nirider says, and emerge with “one of the most believable pieces of evidence imaginable, a confession.”</p><p>“It's a quick and relatively straightforward way to close a case,” she says.</p><p>But Nirider says using deception can also draw false confessions.</p><p>According to the Innocence Project, a national organization that works to overturn wrongful convictions, nearly a third of DNA exonerations from 1989 to 2020 <a href="https://innocenceproject.org/dna-exonerations-in-the-united-states/"><u>involved a false confession</u></a>.</p><p>Legal experts say the deception bans passed in recent years fail to protect other vulnerable groups: young adults, people with intellectual disabilities, even just people who are naturally compliant.</p><p>“Children are one category that makes you more vulnerable, but it's certainly not the only category,” says Lara Zarowsky, executive and policy director at the Washington Innocence Project. “It's something that all of us are vulnerable to.”</p><h2>‘Law enforcement is the biggest impediment’</h2><p>In Washington state, where Bradford was convicted, Democratic lawmakers want to set a higher bar: A bill that would make incriminating statements made in police custody – by adults or children – largely inadmissible in court if obtained using deception.</p><p>State Rep. Strom Peterson has introduced the bill twice, but it hasn’t gone anywhere.</p><p>“Law enforcement is the biggest impediment to the bill. They believe that the system in which they work is effective,” he says.</p><p>The Washington Association of Sheriffs and Police Chiefs declined NPR’s request for an interview, but said in a statement that it opposes such a measure, because banning deception would take away a tactic that yields “many more true confessions” than false ones.</p><p>“We fear that it will negatively impact our ability to solve crimes and would result in less accountability for those who victimize others,” the association’s policy director, James McMahan, <a href="https://tvw.org/video/house-appropriations-2024021057/?eventID=2024021057"><u>said at a hearing</u></a> for the bill in February.</p><p>“Criminals often conduct elaborate stories to conceal their crimes,” McMahan said at the hearing. “Sometimes the use of deception is required to locate the truth both to convict and to exonerate people. Such deceptions include telling a person that abuse was discovered during a routine medical exam rather than reported by a family member.”</p><p>In its statement, the association added that judges assess whether confessions are given voluntarily before they can be introduced as evidence, and convictions based solely on confessions are rare.</p><p>Even with other evidence, however, confessions carry a lot of weight. Research indicates that people who confess <a href="https://core.ac.uk/reader/81748492?utm_source=linkout"><u>are treated differently</u></a> afterwards: They’re more likely to be charged, face more charges, and receive a harsher punishment when convicted.</p><p>“A confession will trump everything,” says Jim Trainum, a retired homicide detective in Washington, D.C.</p><p>In his experience, there is pressure to move on after a suspect confesses because a detective’s measure of success is often tied to closure rates.</p><p>“Let's say that I get a confession and I get all the stuff that I want to go out and corroborate. I want to make sure that this is an accurate confession,” Trainum says. “I'm sitting there at my desk working very, very hard on it. And my sergeant comes up and says, ‘What are you doing? That's a confession. That's closed. Move on. You got other ones to take care of.’”</p><h2>‘Trying to give the police new tools’</h2><p>Those against deception bans see them as an attack on police, says Mark Fallon, a consultant on interrogation practices and former federal agent. In fact, he says, it’s the opposite.</p><p>“It is actually trying to give the police new tools, better tools,” he says.</p><p>There’s another way for police to question people, Fallon says, that relies on building rapport and asking open-ended questions, and where the primary goal is information, rather than a confession.</p><p>That technique is used in other countries, including much of Europe. In England, France, Germany, Australia, Japan and elsewhere, for instance, the police are generally <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3669413"><u>not allowed to deceive suspects</u></a>.</p><p>Trainum says interrogation methods that don’t rely on deception ultimately make the police more trustworthy to communities.</p><p>“Today’s suspect is tomorrow's witness,” he says.</p><p>When a suspect or witness has been lied to, he says, “that radiates out. And no wonder people don't trust us. Why should they trust us?”</p><p>That is why Peterson, the lawmaker, plans to introduce the bill in Washington again. He says the public is<em> </em>better off when police use the best tools available to convict the right people.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Delta: A syntax-highlighting pager for Git, diff, grep, and blame output (248 pts)]]></title>
            <link>https://github.com/dandavison/delta</link>
            <guid>42091365</guid>
            <pubDate>Fri, 08 Nov 2024 23:46:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dandavison/delta">https://github.com/dandavison/delta</a>, See on <a href="https://news.ycombinator.com/item?id=42091365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/52205/147996902-9829bd3f-cd33-466e-833e-49a6f3ebd623.png"><img width="400px" src="https://user-images.githubusercontent.com/52205/147996902-9829bd3f-cd33-466e-833e-49a6f3ebd623.png" alt="image"></a>
</p>
<p dir="auto">
  <a href="https://github.com/dandavison/delta/actions">
    <img src="https://github.com/dandavison/delta/workflows/Continuous%20Integration/badge.svg" alt="CI">
  </a>
  <a href="https://coveralls.io/github/dandavison/delta?branch=main" rel="nofollow">
    <img src="https://camo.githubusercontent.com/e8785ac8ede00f6ec8ad672e5031d27eb6eb5a599d56b232a469b9824f76753c/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f64616e64617669736f6e2f64656c74612f62616467652e7376673f6272616e63683d6d61696e" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/dandavison/delta/badge.svg?branch=main">
  </a>
  <a href="https://gitter.im/dandavison-delta/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge" rel="nofollow">
    <img src="https://camo.githubusercontent.com/6c92914f6e39c859372cd85d6c7676c73d524f994663f1ae0e2b0b566a0e1361/68747470733a2f2f6261646765732e6769747465722e696d2f64616e64617669736f6e2d64656c74612f636f6d6d756e6974792e737667" alt="Gitter" data-canonical-src="https://badges.gitter.im/dandavison-delta/community.svg">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Started</h2><a id="user-content-get-started" aria-label="Permalink: Get Started" href="#get-started"></a></p>
<p dir="auto"><a href="https://dandavison.github.io/delta/installation.html" rel="nofollow">Install it</a> (the package is called "git-delta" in most package managers, but the executable is just <code>delta</code>) and add this to your <code>~/.gitconfig</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[core]
    pager = delta

[interactive]
    diffFilter = delta --color-only

[delta]
    navigate = true    # use n and N to move between diff sections

    # delta detects terminal colors automatically; set one of these to disable auto-detection
    # dark = true
    # light = true

[merge]
    conflictstyle = zdiff3"><pre>[<span>core</span>]
    <span>pager</span> <span>=</span> <span>delta</span>

[<span>interactive</span>]
    <span>diffFilter</span> <span>=</span> <span>delta</span> <span>--color-only</span>

[<span>delta</span>]
    <span>navigate</span> <span>=</span> <span>true</span>    <span><span>#</span> use n and N to move between diff sections</span>

    <span><span>#</span> delta detects terminal colors automatically; set one of these to disable auto-detection</span>
    <span><span>#</span> dark = true</span>
    <span><span>#</span> light = true</span>

[<span>merge</span>]
    <span>conflictstyle</span> <span>=</span> <span>zdiff3</span></pre></div>
<p dir="auto">Delta has many features and is very customizable; please see the <a href="https://dandavison.github.io/delta/" rel="nofollow">user manual</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Language syntax highlighting with the same syntax-highlighting themes as <a href="https://github.com/sharkdp/bat#readme">bat</a></li>
<li>Word-level diff highlighting using a Levenshtein edit inference algorithm</li>
<li>Side-by-side view with line-wrapping</li>
<li>Line numbering</li>
<li><code>n</code> and <code>N</code> keybindings to move between files in large diffs, and between diffs in <code>log -p</code> views (<code>--navigate</code>)</li>
<li>Improved merge conflict display</li>
<li>Improved <code>git blame</code> display (syntax highlighting; <code>--hyperlinks</code> formats commits as links to hosting provider etc. Supported hosting providers are: GitHub, GitLab, SourceHut, Codeberg)</li>
<li>Syntax-highlights grep output from <code>rg</code>, <code>git grep</code>, <code>grep</code>, etc</li>
<li>Support for Git's <code>--color-moved</code> feature.</li>
<li>Code can be copied directly from the diff (<code>-/+</code> markers are removed by default).</li>
<li><code>diff-highlight</code> and <code>diff-so-fancy</code> emulation modes</li>
<li>Commit hashes can be formatted as terminal <a href="https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda">hyperlinks</a> to the hosting provider page (<code>--hyperlinks</code>).
File paths can also be formatted as hyperlinks for opening in your OS.</li>
<li>Stylable box/line decorations to draw attention to commit, file and hunk header sections.</li>
<li>Style strings (foreground color, background color, font attributes) are supported for &gt;20 stylable elements, using the same color/style language as git</li>
<li>Handles traditional unified diff output in addition to git output</li>
<li>Automatic detection of light/dark terminal background</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">A syntax-highlighting pager for git, diff, and grep output</h2><a id="user-content-a-syntax-highlighting-pager-for-git-diff-and-grep-output" aria-label="Permalink: A syntax-highlighting pager for git, diff, and grep output" href="#a-syntax-highlighting-pager-for-git-diff-and-grep-output"></a></p>
<p dir="auto">Code evolves, and we all spend time studying diffs. Delta aims to make this both efficient and enjoyable: it allows you to make extensive changes to the layout and styling of diffs, as well as allowing you to stay arbitrarily close to the default git/diff output.</p>
<markdown-accessiblity-table><p>
      <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/52205/86275526-76792100-bba1-11ea-9e78-6be9baa80b29.png"><img width="400px" src="https://user-images.githubusercontent.com/52205/86275526-76792100-bba1-11ea-9e78-6be9baa80b29.png" alt="image"></a>
      <br>
      <sub>delta with <code>line-numbers</code> activated</sub>
    </p></markdown-accessiblity-table>
<markdown-accessiblity-table><p>
      <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/52205/87230973-412eb900-c381-11ea-8aec-cc200290bd1b.png"><img width="800px" src="https://user-images.githubusercontent.com/52205/87230973-412eb900-c381-11ea-8aec-cc200290bd1b.png" alt="image"></a>
      <br>
      <sub>delta with <code>side-by-side</code> and <code>line-numbers</code> activated</sub>
    </p></markdown-accessiblity-table>
<p dir="auto">Here's what <code>git show</code> can look like with git configured to use delta:</p>
<br>
<markdown-accessiblity-table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Syntax-highlighting themes</h3><a id="user-content-syntax-highlighting-themes" aria-label="Permalink: Syntax-highlighting themes" href="#syntax-highlighting-themes"></a></p>
<p dir="auto"><strong>All the syntax-highlighting color themes that are available with <a href="https://github.com/sharkdp/bat/">bat</a> are available with delta:</strong></p>
<br>
<markdown-accessiblity-table></markdown-accessiblity-table>

<p dir="auto"><h3 tabindex="-1" dir="auto">Side-by-side view</h3><a id="user-content-side-by-side-view" aria-label="Permalink: Side-by-side view" href="#side-by-side-view"></a></p>
<p dir="auto">[<a href="https://dandavison.github.io/delta/side-by-side-view.html" rel="nofollow">User manual</a>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="[delta]
    side-by-side = true"><pre>[<span>delta</span>]
    <span>side-by-side</span> <span>=</span> <span>true</span></pre></div>
<p dir="auto">By default, side-by-side view has line-numbers activated, and has syntax highlighting in both the left and right panels: [<a href="#side-by-side-view-1">config</a>]</p>
<markdown-accessiblity-table><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/52205/87230973-412eb900-c381-11ea-8aec-cc200290bd1b.png"><img width="800px" src="https://user-images.githubusercontent.com/52205/87230973-412eb900-c381-11ea-8aec-cc200290bd1b.png" alt="image"></a></p></markdown-accessiblity-table>
<p dir="auto">Side-by-side view wraps long lines automatically:</p>
<markdown-accessiblity-table><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/52205/139064537-f8479504-16d3-429a-b4f6-d0122438adaa.png"><img width="600px" src="https://user-images.githubusercontent.com/52205/139064537-f8479504-16d3-429a-b4f6-d0122438adaa.png" alt="image"></a></p></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Line numbers</h3><a id="user-content-line-numbers" aria-label="Permalink: Line numbers" href="#line-numbers"></a></p>
<p dir="auto">[<a href="https://dandavison.github.io/delta/line-numbers.html" rel="nofollow">User manual</a>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="[delta]
    line-numbers = true"><pre>[<span>delta</span>]
    <span>line-numbers</span> <span>=</span> <span>true</span></pre></div>
<markdown-accessiblity-table><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/52205/86275526-76792100-bba1-11ea-9e78-6be9baa80b29.png"><img width="400px" src="https://user-images.githubusercontent.com/52205/86275526-76792100-bba1-11ea-9e78-6be9baa80b29.png" alt="image"></a></p></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Merge conflicts</h3><a id="user-content-merge-conflicts" aria-label="Permalink: Merge conflicts" href="#merge-conflicts"></a></p>
<p dir="auto">[<a href="https://dandavison.github.io/delta/merge-conflicts.html" rel="nofollow">User manual</a>]</p>
<markdown-accessiblity-table><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/52205/144783121-bb549100-69d8-41b8-ac62-1704f1f7b43e.png"><img width="500px" src="https://user-images.githubusercontent.com/52205/144783121-bb549100-69d8-41b8-ac62-1704f1f7b43e.png" alt="image"></a></p></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Git blame</h3><a id="user-content-git-blame" aria-label="Permalink: Git blame" href="#git-blame"></a></p>
<p dir="auto">[<a href="https://dandavison.github.io/delta/git-blame.html" rel="nofollow">User manual</a>]</p>
<markdown-accessiblity-table><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/52205/141891376-1fdb87dc-1d9c-4ad6-9d72-eeb19a8aeb0b.png"><img width="600px" src="https://user-images.githubusercontent.com/52205/141891376-1fdb87dc-1d9c-4ad6-9d72-eeb19a8aeb0b.png" alt="image"></a></p></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Ripgrep, git grep</h3><a id="user-content-ripgrep-git-grep" aria-label="Permalink: Ripgrep, git grep" href="#ripgrep-git-grep"></a></p>
<p dir="auto">[<a href="https://dandavison.github.io/delta/grep.html" rel="nofollow">User manual</a>]</p>
<markdown-accessiblity-table><p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/52205/242993705-d203d380-5acb-4296-aeb9-e38c73d6c27f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzExMTk3MDMsIm5iZiI6MTczMTExOTQwMywicGF0aCI6Ii81MjIwNS8yNDI5OTM3MDUtZDIwM2QzODAtNWFjYi00Mjk2LWFlYjktZTM4YzczZDZjMjdmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTA5VDAyMzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTM0YTI4NDcxNzJhNjgwZjYyZmU0YzAzMzU0NDRlNWVlOWYwN2ZhMzYwYzUyMzg0MzgxZmIwYTcwYzFmNGY5OTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.KIvs6cUnqPH2O6y8UH16_sX9nhVwj9vdzsY6x6HmdJY"><img width="600px" alt="image" src="https://private-user-images.githubusercontent.com/52205/242993705-d203d380-5acb-4296-aeb9-e38c73d6c27f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzExMTk3MDMsIm5iZiI6MTczMTExOTQwMywicGF0aCI6Ii81MjIwNS8yNDI5OTM3MDUtZDIwM2QzODAtNWFjYi00Mjk2LWFlYjktZTM4YzczZDZjMjdmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTA5VDAyMzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTM0YTI4NDcxNzJhNjgwZjYyZmU0YzAzMzU0NDRlNWVlOWYwN2ZhMzYwYzUyMzg0MzgxZmIwYTcwYzFmNGY5OTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.KIvs6cUnqPH2O6y8UH16_sX9nhVwj9vdzsY6x6HmdJY"></a>
</p></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation and usage</h3><a id="user-content-installation-and-usage" aria-label="Permalink: Installation and usage" href="#installation-and-usage"></a></p>
<p dir="auto">Please see the <a href="https://dandavison.github.io/delta/" rel="nofollow">user manual</a> and <code>delta --help</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Maintainers</h3><a id="user-content-maintainers" aria-label="Permalink: Maintainers" href="#maintainers"></a></p>
<ul dir="auto">
<li><a href="https://github.com/dandavison">@dandavison</a></li>
<li><a href="https://github.com/th1000s">@th1000s</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude AI to process secret government data through new Palantir deal (135 pts)]]></title>
            <link>https://arstechnica.com/ai/2024/11/safe-ai-champ-anthropic-teams-up-with-defense-giant-palantir-in-new-deal/</link>
            <guid>42091043</guid>
            <pubDate>Fri, 08 Nov 2024 22:42:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/ai/2024/11/safe-ai-champ-anthropic-teams-up-with-defense-giant-palantir-in-new-deal/">https://arstechnica.com/ai/2024/11/safe-ai-champ-anthropic-teams-up-with-defense-giant-palantir-in-new-deal/</a>, See on <a href="https://news.ycombinator.com/item?id=42091043">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<h2>An ethical minefield</h2>
<p>Since its founders started Anthropic in 2021, the company has <a href="https://www.youtube.com/watch?v=UMF1nf3Iy3Q">marketed itself</a> as one that takes an ethics- and safety-focused approach to AI development. The company differentiates itself from competitors like OpenAI by adopting what it calls responsible development practices and self-imposed ethical constraints on its models, such as its "<a href="https://arstechnica.com/information-technology/2023/05/ai-with-a-moral-compass-anthropic-outlines-constitutional-ai-in-its-claude-chatbot/">Constitutional AI</a>" system.</p>
<p>As Futurism <a href="https://futurism.com/the-byte/ethical-ai-anthropic-palantir">points out</a>, this new defense partnership appears to conflict with Anthropic's public "good guy" persona, and pro-AI pundits on social media are noticing. <span>Frequent AI commentator Nabeel S. Qureshi <a href="https://x.com/nabeelqu/status/1854574146283618521">wrote</a> on X, </span><span>"Imagine telling the safety-concerned, effective altruist founders of Anthropic in 2021 that a mere three years after founding the company, they'd be signing partnerships to deploy their ~AGI model straight to the military frontlines.</span>"</p>
<figure>
    <div>
              <p><a data-pswp-width="1200" data-pswp-height="675" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red.jpg 1200w" data-cropped="true" href="https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red.jpg" target="_blank">
                <img decoding="async" width="1200" height="675" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red.jpg" alt="Anthropic's &quot;Constitutional AI&quot; logo." srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red.jpg 1200w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-980x551.jpg 980w" sizes="(max-width: 1200px) 100vw, 1200px">
              </a></p><div id="caption-2061278"><p>
                Anthropic's "Constitutional AI" logo.
                                  </p><p>
                    Credit:
                                          Anthropic / Benj Edwards
                                      </p>
                              </div>
            </div>
                  <figcaption>
          <div>
    
    <p>
      Anthropic's "Constitutional AI" logo.

              <span>
          Credit:

          
          Anthropic / Benj Edwards

                  </span>
          </p>
  </div>
        </figcaption>
            </figure>

<p>Aside from the implications of working with defense and intelligence agencies, the deal connects Anthropic with Palantir, a <a href="https://amp.theguardian.com/commentisfree/2020/sep/04/palantir-ipo-ice-immigration-trump-administration">controversial company</a> which <a href="https://defensescoop.com/2024/05/29/palantir-480-million-army-contract-maven-smart-system-artificial-intelligence/">recently won</a> a $480 million contract to develop an AI-powered target identification system called Maven Smart System for the US Army. Project Maven has <a href="https://www.reuters.com/article/business/media-telecom/google-to-scrub-us-military-deal-protested-by-employees-source-idUSL2N1T320P/">sparked criticism</a> within the tech sector over military applications of AI technology.</p>
<p>It's worth noting that Anthropic's terms of service <a href="https://www.anthropic.com/news/expanding-access-to-claude-for-government">do outline</a> specific rules and limitations for government use. These terms permit activities like foreign intelligence analysis and identifying covert influence campaigns, while prohibiting uses such as disinformation, weapons development, censorship, and domestic surveillance. Government agencies that maintain regular communication with Anthropic about their use of Claude may receive broader permissions to use the AI models.</p>
<p>Even if Claude is never used to target a human or as part of a weapons system, other issues remain. While its Claude models are highly regarded in the AI community, they (like all LLMs) have the tendency to <a href="https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/">confabulate</a>, potentially generating incorrect information in a way that is difficult to detect.</p>
<p>That's a huge potential problem that could impact Claude's effectiveness with secret government data, and that fact, along with the other associations, has Futurism's Victor Tangermann worried. As he puts it, "It's a disconcerting partnership that sets up the AI industry's growing ties with the US military-industrial complex, a worrying trend that should raise all kinds of alarm bells given the tech's many inherent flaws—and even more so when lives could be at stake."</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Is a Staff Engineer? (105 pts)]]></title>
            <link>https://nishtahir.com/what-is-a-staff-engineer/</link>
            <guid>42090771</guid>
            <pubDate>Fri, 08 Nov 2024 21:55:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nishtahir.com/what-is-a-staff-engineer/">https://nishtahir.com/what-is-a-staff-engineer/</a>, See on <a href="https://news.ycombinator.com/item?id=42090771">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <!--kg-card-begin: markdown--><blockquote>
<p><em>"I've worked with a couple of Staff Engineers on different teams in the past and I've seen them do different things, but I've not been able to pin down exactly what they do."</em></p>
</blockquote>
<p>I get this question quite frequently. Sometimes from engineers looking to elevate their roles. At other times, team members reach out looking to learn how they could get the most value from Staff Engineers on the teams. It is a complicated question because a lot of ambiguity exists in the role. Different engineers have distinct interpretations, so you may get a significantly different answer depending on who you ask. With that in mind, I wanted to capture my thoughts on the subject. It's deep, complex, and nuanced. As a result, I'm likely to be as successful as someone attempting to unravel the mysteries of Engineering Management <sup><a href="#fn1" id="fnref1">[1]</a></sup> in a single blog post.</p>
<p>To lay some foundation, I'll be describing a class of engineers as <em>Staff Plus</em> (Staff+). These engineers operate above the Senior level. However, they do not assume the role of an Engineering Manager. These engineers often aim to stay within the technical track of an organization's career ladder. While there is currently no universally accepted title for this role <sup><a href="#fn2" id="fnref2">[2]</a></sup>, successful individuals I've seen in this role tend to share notable common traits</p>
<ul>
<li>They are proven experts in their area of expertise</li>
<li>They have a lot of experience leading teams toward shipping products</li>
</ul>
<h2 id="characterizing-a-staff-engineer">Characterizing a Staff+ Engineer</h2>
<p>One of my favorite ways to characterize the Staff+ role is by using the "4 key skills <sup><a href="#fn3" id="fnref3">[3]</a></sup> every job needs". It provides a solid framework that we can use to determine the distribution of skills one would need to be successful in the role.</p>
<p><img src="https://nishtahir.com/content/images/2023/01/four_skills_every_job_needs.png" alt="four_skills_every_job_needs" loading="lazy"></p>
<h2 id="core-technical-skill">Core Technical Skill</h2>
<p>This is the foundational skill needed to execute the role effectively and one that the Staff+ Engineer should be highly proficient in. In my experience, this level requires deep technical knowledge in some specialty<sup><a href="#fn4" id="fnref4">[4]</a></sup>, and it is often accompanied by a wide breadth of knowledge and experience working with multiple different systems within multiple different environments<sup><a href="#fn5" id="fnref5">[5]</a></sup>. This is the wealth of experience that the Staff+ Engineer reaches into to solve complex technical problems that contribute toward furthering their team's objectives. While I cannot overstate the importance of this skill as a foundational element of the role, it's not enough to be successful on its own. It must be fluidly combined with other skills to empower the Staff+ Engineer to fluidly adapt to different roles on the team, some of which I will cover below.</p>
<h2 id="product-management">Product Management</h2>
<p>A Staff+ Engineer relies on this skill to determine what should be built as well as why. At this level, the Staff+ Engineer should be capable of looking at a team, project, and/or organization's objectives, gaining some understanding of its history, and developing a technical vision<sup><a href="#fn6" id="fnref6">[6]</a></sup> to meet those objectives<sup><a href="#fn7" id="fnref7">[7]</a></sup>. A skilled Staff+ Engineer should be able to communicate this vision to their stakeholders, as well as other parties that may have a stake in the outcome, and get buy-in from all parties, especially the engineering team that will be responsible for building the solution. This role may sometimes manifest as a Technical Architect<sup><a href="#fn8" id="fnref8">[8]</a></sup>.</p>
<h2 id="project-management">Project Management</h2>
<p>This skill helps the Staff+ engineer break down large work items into smaller more manageable tasks for more junior members of the team, create a plan/timeline for completion that can be tracked, as well as manage uncertainties/risks that may deter completion of the work. Proficiency in this skill requires a mastery of basic project management fundamentals <sup><a href="#fn9" id="fnref9">[9]</a></sup>. This does not mean that Staff+ Engineers should be expected to replace project managers; rather these roles should be seen as complementary.</p>
<h2 id="people-management">People Management</h2>
<p>This includes the ability to rally and lead a team toward completing a set of objectives. I've heard this fondly described as "herding cats"<sup><a href="#fn10" id="fnref10">[10]</a></sup>. While I don't think a Staff+ engineer in this role is required to assume full people management responsibilities (that's what Managers are for), there is notably a lot of overlap. For example, I would expect an engineer operating at this level to be an effective mentor, able to provide technical and a reasonable extent of career guidance. This skill also requires having a solid awareness of the team's composition. This includes the skills makeup, strengths as well as growth areas. At this level, the Staff+ engineer should be able to use this awareness to elevate the effectiveness of the team through coaching and mentoring.</p>
<h2 id="youre-rubber-im-glue">You're rubber, I'm glue</h2>
<blockquote>
<p><em>"I feel like they do a little bit of everything. They seem to be the go-to on the team when there's an issue. They are like a rock with all the answers!"</em></p>
</blockquote>
<p>I've found that a key aspect of my day-to-day is autonomously combining these skills to fill roles that may find difficult to fill. It's often the less glamorous but high-value work that is required to build or maintain team momentum. This is sometimes described as "glue" work.</p>
<blockquote>
<p><em>"Every senior person in an organisation should be aware of the less glamorous - and often less-promotable - work that needs to happen to make a team successful. Managed deliberately, glue work demonstrates and builds strong technical leadership skills. Left unconscious, it can be career limiting."</em><sup><a href="#fn11" id="fnref11">[11]</a></sup></p>
</blockquote>
<p>Doing glue work often requires a cross-functional grasp of how the team operates as well as deep insight into areas of the team that may require optimization. Here are a couple of scenarios that exemplify glue work,</p>
<ol>
<li>You notice that a couple of email threads between your engineers and a 3rd party vendor have been running long. They seem to be talking past each other without making any headway. You decide to help improve the situation by scheduling a couple of meetings to help foster alignment and develop a culture of partnership between the teams.</li>
<li>You notice an up tick in the number of bug tickets being written about a feature in the product. After a brief investigation, you find that area of the code lacks automated tests because of a dependency on a third-party framework and will require some rework to make it testable. The development team needs some coaching on how to handle these sorts of problems in the future and a plan needs to be drafted and communicated to the leadership team. There's some upfront cost but will pay for itself in fewer bug tickets down the road.</li>
<li>A team member has been struggling with a new aspect of their assignment. They are unsure of what specific skills they need to learn to be most effective. So you help by offering some light coaching by offering some resources that help them get up to speed quickly as well as setting up 1:1s where they can ask questions and get feedback.</li>
<li>Your team was asked to build a tool that aggregates data for marketing and Business Intelligence (BI). The requirements were vague but enough for the engineering team to work on. Noticing the potential for improvement, you schedule meetings with representatives from the marketing and BI team to better understand how the aggregated data will be used to provide a better product.</li>
</ol>
<p>While one could argue that this work has a high-value impact on the team, it may be tough to justify having the Staff+ Engineer function doing any one of those things in the long term. As a result, a crucial part of the role is leveling up the team such that they may take over such responsibilities such that the Staff+ Engineer may shift their focus towards tackling other priorities. It may be by coaching an existing team member to own one of those tasks or working with the leadership team to staff a new permanent owner.</p>
<h2 id="conclusion">Conclusion</h2>
<blockquote>
<p><em>"I forget what was said exactly, but [Staff+ Engineer] spoke up and said something with clarity and confidence that changed the conversation to be much more productive. They were thoughtful with their comments and have a keen ability to drive toward clarity in a room of swirling indecision."</em></p>
</blockquote>
<p>I've only scratched the surface here, but hope I've captured some specific nuances in the role. Ultimately I think a Staff+ Engineer should be able to use their autonomy and influence within an organization and turn that into meaningful impact and value in service of a team or organization's objectives.</p>
<p>Here are a couple of great resources that I recommend if you are interested in learning more.</p>
<ol>
<li><a href="https://learning.oreilly.com/library/view/the-staff-engineers/9781098118723/?ref=nishtahir.com">The Staff Engineers Path</a> by Tanya Rielly</li>
<li><a href="https://staffeng.com/book?ref=nishtahir.com">Staff Engineer: Leadership beyond the management track</a> by Will Larson</li>
</ol>
<p>To wrap things up, I'm adding an assorted collection of questions I've gotten recently. This is either because I couldn't figure out a way to answer it directly within the context of this blog post or because I thought it may add additional perspective to address the question directly.</p>
<h2 id="faq">FAQ</h2>
<ol>
<li>
<blockquote>
<p>Does Staff+ Engineering require mentoring responsibilities?</p>
</blockquote>
</li>
</ol>
<p>Yes. I think this is a non-negotiable part of the role. The ability to elevate a team's capability is predicated on being a good mentor. In essence, the ability to identify strengths and growth areas on the team. Additionally, creating opportunities for team members to learn and grow.</p>
<ol start="2">
<li>
<blockquote>
<p>What kinds of teams need a Staff+ Engineer?</p>
</blockquote>
</li>
</ol>
<p>A Staff+ Engineer can exist on any team in theory. However, their role will depend on the specific team composition. The Staff+ Engineer may be the main Individual Contributor (IC) on a small team working on a proof of concept for some experimental technology, while a Staff+ Engineer may act as a technical lead on a larger team trying to build long-term momentum. The opportunity cost is the Staff+ Engineer's time and must be considered when making staffing decisions. Could a Senior Engineer be sourced to fill the IC role? This would free up the Staff+ Engineer to work on more complex or higher value problems for the project.</p>
<ol start="3">
<li>
<blockquote>
<p>What differentiates senior levels of Staff Engineers?</p>
</blockquote>
</li>
</ol>
<p>The main differentiator is their scope of impact. More senior Staff+ Engineers should be able to have and manage an impact on an organization or company, in some cases an industry at large. Being able to build and leverage their influence to guide a technical direction is a skill in and of itself</p>
<ol start="4">
<li>
<blockquote>
<p>Hmm... It looks like you went over a lot of general points but didn't address a lot of specific expectations of the role</p>
</blockquote>
</li>
</ol>
<p>This is because the nature of the role changes with each individual and circumstance. This means that being able to adapt to each circumstance is important. That being said, I think the most important thing is that the Staff+ Engineer can turn autonomy into meaningful impact at a scale proportional to their role/level.</p>
<ol start="5">
<li>
<blockquote>
<p>I stayed on the technical track with I got promoted because I wanted to continue to write code. How do I balance leadership responsibilities but still retain coding in my day-to-day?</p>
</blockquote>
</li>
</ol>
<p>I would argue that at this level, your leadership skills are likely your most valuable asset. Trying to keep hands-on-keyboard writing code a major part of your role may not be using your talents to their full potential. However, your day-to-day should be determined by the team/project needs while considering that time dedicated towards work that an IC would typically do comes at the cost of glue work and other higher-level work that may require your attention. This is not to say that you should be completely removed from the code avenues such as working on future work such as PoCs or lower priority features as you have availability might be great ways to keep you engaged in writing code.</p>
<hr>
<section>
<ol>
<li id="fn1"><p><a href="https://dzone.com/articles/the-art-of-engineering-management?ref=nishtahir.com">Chegini, A. (2022) The art of engineering management, dzone.com. DZone. Available at: https://dzone.com/articles/the-art-of-engineering-management (Accessed: January 24, 2023)</a> <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>Some alternate titles I've encountered include Lead Engineer/Developer, Principal/Distinguished Engineer, Technical Fellow, etc... <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p><a href="https://leaddev.com/leaddev-live/role-and-influence-ic-trajectory-beyond-staff?ref=nishtahir.com">Zunger, Y. (no date) Role and Influence: The IC trajectory beyond Staff, Leaddev.com. Available at: https://leaddev.com/leaddev-live/role-and-influence-ic-trajectory-beyond-staff (Accessed: January 24, 2023).</a> <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p><a href="https://hired.com/blog/candidates/balance-breadth-depth-learning-software-development/?ref=nishtahir.com">Woodhams, B. (2018) Balance between breadth and depth of learning software development, candidates. Available at: https://hired.com/blog/candidates/balance-breadth-depth-learning-software-development/ (Accessed: January 24, 2023).</a> <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>This may be different in your organization and your specific personality <a href="#fnref5">↩︎</a></p>
</li>
<li id="fn6"><p><a href="https://lethain.com/what-do-staff-engineers-actually-do/?ref=nishtahir.com">What do Staff engineers actually do? (2020) Lethain.com. Available at: https://lethain.com/what-do-staff-engineers-actually-do/ (Accessed: January 24, 2023).</a> <a href="#fnref6">↩︎</a></p>
</li>
<li id="fn7"><p><a href="https://www.eventbrite.com/engineering/writing-our-3-year-technical-vision/?ref=nishtahir.com">Micol, D. (2021) Writing our 3-year technical vision, Engineering Blog. Available at: https://www.eventbrite.com/engineering/writing-our-3-year-technical-vision/ (Accessed: January 24, 2023).</a> <a href="#fnref7">↩︎</a></p>
</li>
<li id="fn8"><p><a href="https://www.lucidchart.com/blog/defining-technical-architects?ref=nishtahir.com">Rethinking the role of the technical architect (2021) Lucidchart. Available at: https://www.lucidchart.com/blog/defining-technical-architects (Accessed: January 24, 2023).</a> <a href="#fnref8">↩︎</a></p>
</li>
<li id="fn9"><p><a href="https://www.northeastern.edu/graduate/blog/essential-project-management-skills/?ref=nishtahir.com">Joubert, S. (2019) Project management skills, Northeastern University Graduate Programs. Available at: https://www.northeastern.edu/graduate/blog/essential-project-management-skills/ (Accessed: January 24, 2023).</a> <a href="#fnref9">↩︎</a></p>
</li>
<li id="fn10"><p><a href="https://www.frontendhappyhour.com/episodes/tech-lead-engineer-herding-cats-&amp;-drinks/?ref=nishtahir.com">Tech lead engineer - herding cats &amp; drinks - Front End Happy Hour (no date) Frontendhappyhour.com. Available at: https://www.frontendhappyhour.com/episodes/tech-lead-engineer-herding-cats-&amp;-drinks/ (Accessed: January 25, 2023).</a> <a href="#fnref10">↩︎</a></p>
</li>
<li id="fn11"><p><a href="https://noidea.dog/glue?ref=nishtahir.com">Being Glue — (no date) No Idea Blog. Available at: https://noidea.dog/glue (Accessed: January 25, 2023).</a> <a href="#fnref11">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I quit Google to work for myself (2018) (159 pts)]]></title>
            <link>https://mtlynch.io/why-i-quit-google/</link>
            <guid>42090430</guid>
            <pubDate>Fri, 08 Nov 2024 20:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mtlynch.io/why-i-quit-google/">https://mtlynch.io/why-i-quit-google/</a>, See on <a href="https://news.ycombinator.com/item?id=42090430">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For the past four years, I’ve worked as a software developer at Google. On February 1st, I quit. It was because they refused to buy me a Christmas present.</p><p>Well, I guess it’s a little more complicated than that.</p><h2 id="the-first-two-years">The first two years<a href="#the-first-two-years" arialabel="Anchor"> 🔗︎</a></h2><p>Two years in, I loved Google.</p><p>When the annual employee survey asked me whether I expected to be at Google in five years, it was a no-brainer.</p><p>Of <em>course</em> I’d still be at Google in five years. I was surrounded by the best engineers in the world, using the most advanced development tools in the world, and eating the free-est food in the world.</p><p><a href="https://mtlynch.io/why-i-quit-google/spoiled-coder.png"><img sizes="(min-width: 768px) 750px, 98vw" srcset="https://mtlynch.io/why-i-quit-google/spoiled-coder_hu4f194abb4e8e6858d1fbc287ed8a6d8e_188016_300x0_resize_lanczos_3.png 300w,
https://mtlynch.io/why-i-quit-google/spoiled-coder_hu4f194abb4e8e6858d1fbc287ed8a6d8e_188016_600x0_resize_lanczos_3.png 600w,
https://mtlynch.io/why-i-quit-google/spoiled-coder_hu4f194abb4e8e6858d1fbc287ed8a6d8e_188016_800x0_resize_lanczos_3.png 800w,
https://mtlynch.io/why-i-quit-google/spoiled-coder.png 1024w" src="https://mtlynch.io/why-i-quit-google/spoiled-coder.png" alt="My typical day at Google" loading="lazy"></a></p><p>My most recent performance rating was “Strongly Exceeds Expectations.” If I just kept going, I’d soon be promoted to the next level, Senior Software Engineer. What a great title! Forever after in my career, I’d be able to say, “Yes, I was a <em>Senior</em> Software Engineer. At <em>Google</em>.” People would be so impressed.</p><p>My manager assured me that my promotion was close. He felt that I was already capable of senior-level work. I just needed the right project to prove it to the promotion committee.</p><p>No, managers at Google can’t promote their direct reports. They don’t even get a vote.</p><p>Instead, promotion decisions come from small committees of upper-level software engineers and managers who have never heard of you until the day they decide on your promotion.</p><p>You apply for promotion by assembling a “promo packet”: a collection of written recommendations from your teammates, design documents you’ve created, and mini-essays you write to explain why your work merits a promotion.</p><p>A promotion committee then reviews your packet with a handful of others, and they spend the day deciding who gets promoted and who doesn’t.</p><p>During my two-year honeymoon phase, this system sounded great to me. Of <em>course</em> my fate should be in the hands of a mysterious committee who’s never met me. They wouldn’t be tainted by any sort of favoritism or politics. They’d see past all that and recognize me for my high-quality code and shrewd engineering decisions.</p><h2 id="thats-not-really-how-it-works">That’s not really how it works<a href="#thats-not-really-how-it-works" arialabel="Anchor"> 🔗︎</a></h2><p>Before I put together my first promo packet, I never thought about the logistics of how it all worked.</p><p>In my head, the promotion committee was this omniscient and fair entity. If I spent each day choosing the right problems to solve, making the codebase better, and helping my team execute efficiently, the promotion committee would magically know this and reward me for it.</p><p>Unsurprisingly, it doesn’t work like that. It took me two years to figure that out.</p><h2 id="working-naïvely">Working naïvely<a href="#working-naïvely" arialabel="Anchor"> 🔗︎</a></h2><p>My main responsibility until that point was a legacy data pipeline. It had been in maintenance mode for years, but load had increased, and the pipeline was buckling under the pressure. It frequently died silently or produced incorrect output. Its failures took days to diagnose because nobody had written documentation for it since its original design spec.</p><p>I proudly and lovingly nursed the pipeline back to health. I fixed dozens of bugs and wrote automated tests to make sure they wouldn’t reappear. I deleted thousands of lines of code that were either dead or could be replaced by modern libraries. I documented the pipeline as I learned it so that the institutional knowledge was available to my teammates instead of siloed in my head.</p><p>The problem, as I discovered at promotion time, was that none of this was quantifiable. I couldn’t prove that anything I did had a positive impact on Google.</p><h2 id="metrics-or-it-didnt-happen">Metrics or it didn’t happen<a href="#metrics-or-it-didnt-happen" arialabel="Anchor"> 🔗︎</a></h2><p>The pipeline didn’t record many metrics. The ones it did have made it look like things had gotten worse. My bug discoveries caused the overall bug count to increase. The pipeline’s failures increased because I made it fail fast on anomalies instead of silently passing along bad data. I drastically reduced the time developers spent repairing those failures, but there were no metrics that tracked developer time.</p><p>My other work didn’t look so good on paper either. On several occasions, I put my projects on hold for weeks or even months at a time to help a teammate whose launch was at risk. It was the right decision for the team, but it looked unimpressive in a promo packet. To the promotion committee, my teammate’s project was the big, important work that demanded coordination from multiple developers. If they hornswoggled me into helping them, it’s evidence of their strong leadership qualities. I was just the mindless peon whose work was so irrelevant that it could be pre-empted at a moment’s notice.</p><p>I submitted my first promo packet, and the results were what I feared: the promotion committee said that I hadn’t proven I could handle technical complexity, and they couldn’t see the impact I had on Google.</p><p><a href="https://mtlynch.io/why-i-quit-google/promo-committee.png"><img sizes="(min-width: 768px) 800px, 98vw" srcset="https://mtlynch.io/why-i-quit-google/promo-committee_hu7b22fcc95e2f40d7ada24e83ce6de553_523971_300x0_resize_lanczos_3.png 300w,
https://mtlynch.io/why-i-quit-google/promo-committee_hu7b22fcc95e2f40d7ada24e83ce6de553_523971_600x0_resize_lanczos_3.png 600w,
https://mtlynch.io/why-i-quit-google/promo-committee_hu7b22fcc95e2f40d7ada24e83ce6de553_523971_800x0_resize_lanczos_3.png 800w,
https://mtlynch.io/why-i-quit-google/promo-committee.png 1024w" src="https://mtlynch.io/why-i-quit-google/promo-committee.png" alt="Arguing my case to the promotion committee" loading="lazy"></a></p><h2 id="learning-from-rejection">Learning from rejection<a href="#learning-from-rejection" arialabel="Anchor"> 🔗︎</a></h2><p>The rejection was a difficult blow, but I wasn’t discouraged. I felt I was performing above my level, but the promotion committee couldn’t see it. That was solvable.</p><p>I decided that I had been too naïve in my first couple years. I didn’t do enough planning up front to make sure the work I was doing left a paper trail. Now that I understood how the process worked, I could keep doing the same good work, just with better record-keeping.</p><p>For example, my team was receiving tons of distracting email alerts due to false alarms. Old me would have just fixed these alerts. But now I knew that for this work to appear in my promo packet, I should first set up metrics so that we’d have historical records of alert frequency. At promotion time, I’d have an impressive-looking graph of the alerts trending downward.</p><p>Shortly after, I was assigned a project that seemed destined for promotion. It depended heavily on machine-learning, which was and still is the hot thing at Google. It would automate a task that hundreds of human operators were doing manually, so it had a clear, objective impact on Google. It also required me to lead a junior developer throughout the project, which generally won points with promotion committees.</p><h2 id="the-holiday-gift-wake-up-call">The holiday gift wake up call<a href="#the-holiday-gift-wake-up-call" arialabel="Anchor"> 🔗︎</a></h2><p>A few months later, Google <a href="http://fortune.com/2016/12/09/alphabet-donated-its-employees-holiday-gifts-to-charity/">made headlines</a> when they ended their long-standing tradition of giving lavish holiday gifts to all of their employees. Instead, they used the gift budget to buy <del>advertising disguised as charity</del> Chromebooks for underprivileged schoolchildren.</p><p>Shortly after this, I witnessed the following conversation between two employees:</p><blockquote><p><strong>Employee A</strong>: You effectively <strong>are</strong> still getting the gift. Cuts like these increase the value of Google’s stock. You can sell your stock grants and buy any present you choose.</p><p><strong>Employee B</strong>: What if I told my wife that I wasn’t buying her a Christmas gift, but she could use the money in our bank account to buy any present she wants?</p><p><strong>Employee A</strong>: You’re in a <strong>business</strong> relationship with Google. If you’re disappointed that Google isn’t “romancing” you with gifts like you do for your wife, you have a misguided notion of the relationship.</p></blockquote><p>Wait a second. <em>I</em> was in a business relationship with Google.</p><p>It may sound strange that it took me two and a half years to realize it, but Google does a good job of building a sense of community within the organization. To make us feel that we’re not just employees, but that we <em>are</em> Google.</p><p>That conversation made me realize that I’m <em>not</em> Google. I provide a service to Google in exchange for money.</p><p>So if Google and I have a business relationship that exists to serve each side’s interests, why was I spending time on all these tasks that served Google’s interests instead of my own? If the promotion committee doesn’t reward bugfixing or team support work, why was I doing that?</p><p>My first denied promotion taught me the wrong lesson. I thought I could keep doing the same work but package it to look good for the promotion committee. I should have done the opposite: figure out what the promotion committee wants, and do that work exclusively.</p><p>I adopted a new strategy. Before starting any task, I asked myself whether it would help my case for promotion. If the answer was no, I didn’t do it.</p><p>My quality bar for code dropped from, “Will we be able to maintain this for the next 5 years?” to, “Can this last until I’m promoted?” I didn’t file or fix any bugs unless they risked my project’s launch. I wriggled out of all responsibilities for maintenance work. I stopped volunteering for campus recruiting events. I went from conducting one or two interviews per week to zero.</p><h2 id="then-my-project-was-canceled">Then my project was canceled<a href="#then-my-project-was-canceled" arialabel="Anchor"> 🔗︎</a></h2><p>Priorities shifted. Management traded my project away to our sister team in India. In exchange, that team gave us one of their projects. It was an undocumented system, built on deprecated infrastructure, but it was nevertheless a critical component in production. I was assigned to untangle it from our sister team’s code and migrate it to a new framework, all while keeping it running in production and hitting its performance metrics.</p><p>As far as my promotion was concerned, this was a setback of several months. Because I hadn’t released anything for my canceled project, the two months I spent on it were worthless. It would take me weeks just to get up to speed on the system I was inheriting, and I was liable to lose several more in the gruntwork of keeping it operational.</p><h2 id="what-am-i-even-doing">What am I even doing?<a href="#what-am-i-even-doing" arialabel="Anchor"> 🔗︎</a></h2><p>It was the third time in six months that my manager had reassigned me midway through a project. Each time, he assured me that it had nothing to do with the quality of my work, but rather some shift in upper management strategy or team headcount.</p><p>At this point, I took a step back to assess what was happening from a high level. Forget my manager, forget his managers, forget the promotion committee. What if I boiled it down to just me and just Google? What was happening in our “business relationship?”</p><p>Well, Google kept telling me that it couldn’t judge my work until it saw me complete a project. Meanwhile, I couldn’t complete any projects because Google kept interrupting them midway through and assigning me new ones.</p><p>The dynamic felt absurd.</p><p><a href="https://mtlynch.io/why-i-quit-google/book-publisher.png"><img sizes="(min-width: 768px) 750px, 98vw" srcset="https://mtlynch.io/why-i-quit-google/book-publisher_hu67c1fce3e1f8685743944e5c14f41bdf_378594_300x0_resize_lanczos_3.png 300w,
https://mtlynch.io/why-i-quit-google/book-publisher_hu67c1fce3e1f8685743944e5c14f41bdf_378594_600x0_resize_lanczos_3.png 600w,
https://mtlynch.io/why-i-quit-google/book-publisher_hu67c1fce3e1f8685743944e5c14f41bdf_378594_800x0_resize_lanczos_3.png 800w,
https://mtlynch.io/why-i-quit-google/book-publisher.png 1024w" src="https://mtlynch.io/why-i-quit-google/book-publisher.png" alt="The Google promotion committee approach to book publishing" loading="lazy"></a></p><p>My career was being dictated by a shifting, anonymous committee who thought about me for an hour of their lives. Management decisions that I had no input into were erasing months of my career progress.</p><p>Worst of all, I wasn’t proud of my work. Instead of asking myself, “How can I solve this challenging problem?” I was asking, “How can I make this problem <em>look</em> challenging for promotion?” I hated that.</p><p>Even if I got the promotion, what then? Popular wisdom said that each promotion was exponentially harder than the last. To continue advancing my career, I’d need projects that were even larger in scope and involved collaboration with more partner teams. But that just meant the project could fail due to even more factors outside my control, wasting months or years of my life.</p><h2 id="whats-the-alternative">What’s the alternative?<a href="#whats-the-alternative" arialabel="Anchor"> 🔗︎</a></h2><p>Around this time, I discovered Indie Hackers.</p><p><a href="https://mtlynch.io/why-i-quit-google/indie-hackers.png"><img sizes="(min-width: 768px) 550px, 98vw" srcset="https://mtlynch.io/why-i-quit-google/indie-hackers_hu41c2d18a1506b17864c6f0bbd92fea7f_91019_300x0_resize_lanczos_3.png 300w,
https://mtlynch.io/why-i-quit-google/indie-hackers_hu41c2d18a1506b17864c6f0bbd92fea7f_91019_600x0_resize_lanczos_3.png 600w,
https://mtlynch.io/why-i-quit-google/indie-hackers_hu41c2d18a1506b17864c6f0bbd92fea7f_91019_800x0_resize_lanczos_3.png 800w,
https://mtlynch.io/why-i-quit-google/indie-hackers_hu41c2d18a1506b17864c6f0bbd92fea7f_91019_1200x0_resize_lanczos_3.png 1200w,
https://mtlynch.io/why-i-quit-google/indie-hackers.png 1545w" src="https://mtlynch.io/why-i-quit-google/indie-hackers.png" alt="Screenshot of Indie Hackers website" loading="lazy"></a></p><p>It’s an online community for founders of small software businesses. Emphasis on small. These weren’t Zuckerberg hopefuls, but rather people who wanted to build modest, profitable businesses that pay their bills.</p><p>I had always been interested in starting my own software company, but I only knew of the Silicon Valley startup path. I thought being a software founder meant spending most of my time fundraising and the rest of it worrying about how to attract my next million users.</p><p>Indie Hackers presented an attractive alternative. Most members built their businesses with their own savings or as side projects to their full-time jobs. They didn’t answer to investors, and they certainly didn’t have to prove themselves to anonymous committees.</p><p>There were downsides, of course. Their income was less steady, and they faced more numerous catastrophic risks. If I ever made a mistake at Google that cost the company $10 million, I would suffer no consequences. I’d be asked to write a post-mortem, and everyone would celebrate the learning opportunity. For most of these founders, a $10 million mistake would mean the end of their business and several lifetimes of debt.</p><p>Founders on Indie Hackers captivated me because they were in control. Whether their business became a runaway success or stagnated for years, they were calling the shots. At Google, I didn’t feel in control of my own projects, much less my career growth or my team’s direction.</p><p>I thought about it for months and finally decided. I wanted to be an Indie Hacker.</p><h2 id="one-last-thing-before-i-leave">One last thing before I leave<a href="#one-last-thing-before-i-leave" arialabel="Anchor"> 🔗︎</a></h2><p>I still had unfinished business at Google. After investing three years into my promotion, I hated the idea of leaving with nothing to show for it. There were only a few months left until I could reapply for promotion, so I decided to give it one last shot.</p><p>Six weeks before the performance period ended, my project was canceled. Again.</p><p>Actually, my whole team was canceled. This was a common enough occurrence at Google that there was a euphemism for it: a defrag. Management transferred my team’s projects to our sister team in India. My teammates and I all had to start over in different areas of the company.</p><p>I applied for the promotion anyway. Weeks later, my manager read me the results. My performance rating was “Superb,” the highest possible score, given to around 5% of employees each cycle. The promotion committee noted that in the past six months, I clearly demonstrated senior-level work. These were, uncoincidentally, the months when I was optimizing for promotion.</p><p><em>But</em> they felt that six months wasn’t a long enough track record, so… better luck next time.</p><p>My manager told me I had a strong chance at promotion if I did the same quality work for another six months. I can’t say I wasn’t tempted, but by that point, I’d been hearing, “great shot at promotion in six months,” for the past two years.</p><p>It was time to go.</p><h2 id="whats-next">What’s next?<a href="#whats-next" arialabel="Anchor"> 🔗︎</a></h2><p>When I tell people I left Google, they assume I must have some brilliant startup idea. Only an <em>idiot</em> would leave a job as cushy as Google Software Engineer.</p><p>But I am indeed an idiot with no idea.</p><p>My plan is to try different projects for a few months each to see if any of them catch on, for example:</p><ul><li>Continue working on <a href="https://mtlynch.io/tags/ketohub">KetoHub</a> to see if I can make it profitable</li><li>Build a business on top of Sia, a distributed storage technology I’ve <a href="https://mtlynch.io/tags/sia">written about frequently</a></li><li>Spend more time writing, and look for ways to earn money from it</li></ul><p>Google was a great place to work, and I learned valuable skills during my time there. Leaving was difficult because I had more to learn, but there will always be employers like Google. I won’t always have the freedom to start my own company, so I look forward to seeing where this takes me.</p><h2 id="updates">Updates<a href="#updates" arialabel="Anchor"> 🔗︎</a></h2><ul><li><strong>Update (Feb. 1, 2019)</strong>: <a href="https://mtlynch.io/solo-developer-year-1/">My First Year as a Solo Developer</a></li><li><strong>Update (Jan. 31, 2020)</strong>: <a href="https://mtlynch.io/solo-developer-year-2/">My Second Year as a Solo Developer</a></li><li><strong>Update (Feb. 1, 2021)</strong>: <a href="https://mtlynch.io/solo-developer-year-3/">My Third Year as a Solo Developer</a></li><li><strong>Update (Feb. 1, 2022)</strong>: <a href="https://mtlynch.io/solo-developer-year-4/">My Fourth Year as a Bootstrapped Founder</a></li><li><strong>Update (Feb. 10, 2023)</strong>: <a href="https://mtlynch.io/solo-developer-year-5/">My Fifth Year as a Bootstrapped Founder</a></li><li><strong>Update (Feb. 10, 2024)</strong>: <a href="https://mtlynch.io/solo-developer-year-6/">My Sixth Year as a Bootstrapped Founder</a></li></ul><hr><p><em>Illustrations by <a href="https://www.loraineyow.com/">Loraine Yow</a>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mitochondria Are Alive (461 pts)]]></title>
            <link>https://www.asimov.press/p/mitochondria</link>
            <guid>42088758</guid>
            <pubDate>Fri, 08 Nov 2024 17:39:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.asimov.press/p/mitochondria">https://www.asimov.press/p/mitochondria</a>, See on <a href="https://news.ycombinator.com/item?id=42088758">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><strong><span>An opinion essay by </span><a href="https://www.hertzfoundation.org/person/liyam-chitayat/" rel="">Liyam Chitayat</a></strong></p><p>The cells within our body are the remnants of an ancient alliance.&nbsp;</p><p><span>In a 1967 paper called “</span><a href="https://doi.org/10.1016/0022-5193(67)90079-3" rel="">On the Origin of Mitosing Cells</a><span>,” American evolutionary biologist Lynn Margulis proposed an idea that, upon first hearing, seems ludicrous. Her paper, in fact, was rejected by 12 different journals before it was published.</span></p><p>Margulis argued that one-and-a-half billion years ago, a primitive eukaryotic cell engulfed an oxygen-utilizing bacterium. But rather than digesting this bacterium — or conversely, the bacterium destroying its newfound host — the two cells gradually entered into an endosymbiotic relationship; the host provided nutrients and protection to the bacterium, and the bacterium supplied energy to the host. Margulis argued that this endosymbiosis event was a seminal “innovation engine” for biological systems, ultimately leading to the modern mitochondrion and chloroplast.</p><p>Margulis’ theory was attacked and ridiculed, igniting academic hostilities that lasted for decades. Over time, though, biologists began to accept her ideas because the membrane structure and molecular machinery within mitochondria closely resemble that of extant bacteria. Most biologists today, however, also believe that mitochondria have “devolved” into little more than membrane-bound organelles, similar to inanimate components like the endoplasmic reticulum or Golgi apparatus.</p><p><span>But a swelling tide of scientific evidence about mitochondrial functions and dynamics suggests otherwise — </span><em>mitochondria are not just organelles, but their own life forms.</em><strong>&nbsp;&nbsp;&nbsp;</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png" width="1200" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:309045,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d79ce7-9f19-4a32-8ad2-13c18f831ad8_1200x800.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>An image from L. Margulis’ 1967 paper, depicting the origins of modern mitochondria.</figcaption></figure></div><p>This distinction between “life” on the one hand and “mere membranous structure” on the other may seem trivial, but it’s a symptom of a deeper problem. Defining mitochondria as “nonliving” isn’t just a classification mistake, nor a question of word choice. Rather, it is a fundamental misunderstanding of the nature and role of mitochondria. It inherently undermines our understanding of biological systems and deeply influences the tools we build to study them. </p><p>If we think of mitochondria as non-living organelles, how will we ever harness their full potential?</p><p>The precise definition of “life” has been debated since the inception of biology as a scientific field. Even today, researchers offer overlapping, but distinct, criteria. Molecular biologists tend to focus on characteristics like metabolism, growth and development, response to stimuli, reproduction, and the ability to process information or evolve. This definition uses “checklists” to determine whether or not an organism is alive.</p><p>Biophysicists often take a more rigorous approach, defining life by means of energetic terms. Physicists Erwin Schrödinger and Ilya Prigogine said that living organisms maintain order despite the universe's tendency towards increasing entropy, a measure of how dispersed or disordered the energy within a system is. Living systems maintain far-from-equilibrium states, constantly exchanging matter and energy with their environment to sustain highly organized structures. Cells take in low-entropy inputs, such as food or sunlight, and expel high-entropy outputs, including waste.</p><p>Regardless of which definition one chooses, mitochondria are clearly alive.</p><p>Mitochondria carry their own genomes and express their own genes within their lumens, an internal pocket of watery space, using biomolecules distinct from the cell’s nucleus. Mitochondria also replicate and divide through binary fission, much like bacteria. If one considers bacteria as living entities — and all biologists seem to — then it is impossible to explain why mitochondria are not.</p><p>From a thermodynamic perspective, mitochondria take in low-entropy inputs from their host cell, such as glucose or fatty acids, and expel high-entropy outputs, including carbon dioxide and water. Mitochondria also pump out protons through their inner membrane to maintain an out-of-equilibrium thermodynamic balance, using the resulting gradient to produce the ATP molecules that fuel cellular functions, from DNA replication to protein synthesis.</p><p><span>From the molecular biologist’s perspective, a mitochondrion’s role is not limited to simple energy generation, either. Mitochondria also process</span><em> </em><span>information and interact with their environment, much like a human cell. They monitor steroid hormones, oxidative stress, heat, ATP levels, secondary metabolites, and </span><a href="https://journals.physiology.org/doi/full/10.1152/physrev.00058.2021#" rel="">many more molecules</a><span> floating through their environment, the cell’s cytoplasm. Mitochondria then use this information to precisely control cellular functions. For example, when a virus invades a cell, the mitochondria are critical in sensing the intrusion and signaling a host cell to undergo programmed cell death to halt its spread.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png" width="1200" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/abab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:327767,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabab6c4f-4f66-44a9-8234-cc8eaddc975e_1200x800.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Mitochondria are intimately involved in many cellular processes; not just energy production. Image by the author.</figcaption></figure></div><p>And finally, mitochondria grow and reproduce in a manner distinct from the host’s replication process. Mitochondria independently copy their circular genomes, known as mitochondrial DNA, and divide through binary fission. Notably, mitochondrial replication has several distinct properties from those observed during human cellular replication. Mitochondrial DNA mutates 100-1,000 times faster than the human genome and these mutations can significantly alter a mitochondrion’s fitness, thereby changing the fitness of its host cell. Mitochondria are thus agents of — and subject to — the forces of evolution.</p><p><span>Despite all this evidence, the main case made against mitochondria being alive is that they do not perform all of these functions </span><em>independently</em><span>, as they must be embedded within the cytoplasm of a host cell to function. However, such an argument is logically inconsistent because, by this same logic, most organisms on Earth would not be considered “living.” After all, nothing in biology lives in isolation from its environment.&nbsp;</span></p><p><span>Human life begins inside of another human, with a zygote requiring many months in the uterus to develop into an infant. Many other organisms — not just mitochondria — also live inside other cells. For example, the bacteria </span><em><a href="https://www.ncbi.nlm.nih.gov/books/NBK7624/" rel="">rickettsiae</a><span> </span></em><span>occupy the cytoplasm of cells of ticks, lice, fleas, and mites. Other bacteria, such as </span><em><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5911502/" rel="">Holospora spp.</a></em><span>, also live within the nucleus of various protists. All living creatures have evolved and live embedded within an environment or biological system, with different organisms embedded in different layers.</span></p><p>It seems like scientists have decided what is living based on whether or not an organism exists in certain, arbitrarily chosen layers of our biosphere. But this is a logical fallacy. Every living organism grows and adapts to occupy a specific context in the universe. We refer to this as the “effective niche” of the lifeform, which could be both inside and outside of another living system. Just because an organism has evolved to live in one niche does not mean that the organism cannot survive in another. Therefore, the so-called “potential niche” of a lifeform is often much larger than its effective niche.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png" width="1200" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:404460,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6de7af-2472-42ef-8e1d-cfad5738af17_1200x800.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The “potential” niche of an organism is typically much larger than its “effective” niche. Image by the author.</figcaption></figure></div><p><span>Consider, for example, that free-living bacteria have been artificially implanted into the cytoplasms of different fungi. Researchers at ETH Zurich </span><a href="https://www.nature.com/articles/s41586-024-08010-x" rel="">recently implanted</a><span> “bacteria into the filamentous fungus </span><em>Rhizopus microsporus</em><span> to follow the fate of artificially induced endosymbioses.” It is clear that the insertion of bacteria into other cells does not suddenly make those bacteria non-living.</span></p><p><span>Similarly, a mitochondrion’s effective niche is a host cell’s cytoplasm, but its potential niche is likely far greater. Mitochondria are not bound to their host cell; they can </span><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tra.12951" rel="">travel between different cells</a><span>. Although different species carry distinct mitochondria, experiments show that mitochondria from one species can be transferred to another.&nbsp;</span></p><p><span>In 1997, scientists isolated mitochondria from chimpanzees and gorillas and showed that they are naturally internalized and integrated </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC23071/" rel="">into human cells</a><span>. Notably, the addition of external mitochondria even showed therapeutic benefits in </span><a href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.800883/full" rel="">heart failure and spinal cord injury</a><span>. Thus, the potential niche that mitochondria can live in is greater than their effective niche.&nbsp;</span></p><p>When Margulis fought to overturn widely-held ideas in evolutionary biology, it allowed biologists to understand how complexity emerges in biological systems with the creation of eukaryotes and the rise of multicellularity. By revisiting our understanding of mitochondria, we will similarly break down long-held scientific dogmas.</p><p><span>In the early 20th century, Albert Einstein and Claude Shannon laid out the three pillars of the physical world: matter, information, and energy. When Francis Crick and James Watson published their model of the DNA double helix, they created a paradigm shift in our ability to understand and control the first two: matter and information. In the 70 years since then, we’ve developed powerful tools to study genes, decode how information moves through cells, and manipulate DNA using tools such as CRISPR-based gene editing. However, we have not yet reached an equal level of understanding of, or tools to manipulate, biological </span><em>energy</em><span>. Just as CRISPR enabled scientists to rewrite the code of life, we need similar tools to engineer mitochondria and control bioenergetics across the eukaryotic tree of life.</span></p><p>Despite more than a billion years of evolution, mitochondria still play critical roles within cells; they have not been displaced or rendered obsolete. This means that, as humans evolved, so too did the role of mitochondria in shaping our health and longevity. Mitochondrial dysfunction has long been linked to cardiovascular disorders, diabetes, Alzheimers, Parkinsons, amyotrophic lateral sclerosis, and other age-related diseases. In patients with these conditions, the mitochondria adopt abnormal and fragmented morphologies, failing to make enough energy for cells or sending improper communication signals. The diseased mitochondria gradually make toxic compounds that accelerate cell death.&nbsp;</p><p>Perhaps one of the paths to solving energy-related diseases, extending lifespan, or even engineering processes like photosynthesis lies in the complex interaction between our cells and the other lifeforms so actively inhabiting them. To find out, let’s embrace these eons-old alliances.</p><p><em>Thanks to Kate Adamala, Zeno Fox, Michael Retchin, Niko McCarty, and Ed Boyden for helpful feedback on this essay.</em></p><p><strong>Liyam Chitayat </strong><span>is a Hertz Fellow and PhD student at MIT working on synthetic endosymbiosis and building an initiative to integrate and accelerate the field. Liyam is also a Fellow of The Council on Strategic Risk.</span></p><p><strong>Cite: </strong><span>Liyam Chitayat. “Mitochondria Are Alive” </span><em>Asimov Press </em><span>(2024). DOI: https://doi.org/10.62211/38pe-75hu</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making Electronic Calipers (106 pts)]]></title>
            <link>https://kevinlynagh.com/calipertron/</link>
            <guid>42087560</guid>
            <pubDate>Fri, 08 Nov 2024 15:29:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kevinlynagh.com/calipertron/">https://kevinlynagh.com/calipertron/</a>, See on <a href="https://news.ycombinator.com/item?id=42087560">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://kevinlynagh.com/">← Back to Kevin's homepage</a><span>Published: 2024 November 2</span></p><p>Have you ever wished for a 500 Hz, millimeter-precise linear position sensing system?
Well you’re in luck — all you need is some circuit board, a basic microcontroller, and a wee bit of maths!</p>

<video src="https://kevinlynagh.com/calipertron/2024_11_02_caliper_demo.mp4" controls="true" loop="true" autoplay="true"></video>

<p>See also the <a href="https://github.com/lynaghk/calipertron/">full source code and my research log</a> for this project.</p>

<h2 id="why-make-calipers">Why make calipers?</h2>

<p>Electronic calipers are awesome.
<a href="https://amzn.to/3BVZhvB">This \$30 pair</a> has served me well for years, reading far more precision than my skills justify:</p>

<p><img src="https://kevinlynagh.com/calipertron/calipers.png" alt="calipers"></p>

<p>Such calipers work via capacitive coupling between a PCB on the powered slidey display and a passive PCB “scale” in the stationary spine.</p>

<p>Back in March, I idly wondered if the same working principle could be used for a cheap and cheerful “maker-friendly” positioning system.
E.g., slide some passive PCB scales into an aluminum extrusion rail, add a capacitive pickup to the bottom of whatever carriage you’ve got riding along, and <em>tada</em> — you’ve got sub-mm closed-loop positioning.
All for the cost of some PCB, a few GPIO pins, and some firmware (so free, basically).</p>

<p>I figured someone else must’ve done this before, but I wasn’t able to find any “open source caliper” projects.
The closest was <a href="https://hackaday.io/project/194778-diy-digital-caliper-calipatron/log/227428-research">this hackaday.io project page</a>, started literally a month before.</p>

<p>I reached out to the author, Mitko, and offered to implement the firmware if they sent me a PCB.
My main motivation was to learn some digital signal processing, as I’d never studied it beyond a passing undergrad mention of the Fourier transform.</p>

<p>If you just want some off-the-shelf precision measurements and don’t want to go on <em>An Adventure</em>, you may want to consider instead:</p>

<ul>
<li>reading measurement data directly out of cheap calipers via their <a href="https://hackaday.com/2016/05/17/improved-digital-caliper-interfacing-including-3d-printed-connector/">secret data interface</a></li>
<li>searching for “digital read out” (DRO) kits, which is the generic term for all sorts of capacitive, optical, and magnetic precision linear and angular measurement schemes (typically for retrofitting manual machine equipment with a digital readout, so <em>you</em> can be the “<a href="https://en.wikipedia.org/wiki/Numerical_control">NC</a>” of “CNC”). E.g., this  <a href="https://www.sra-measurement.com/digital-readout-systems/magnetic-linear-encoder-reading-head-1-micron-resolution">\$200 magnetic encoder</a> with some <a href="https://www.sra-measurement.com/high-accuracy-magnetic-linear-tape">\$1/cm linear tape</a>.)</li>
</ul>

<h2 id="caliper-theory">Caliper theory</h2>

<p>Here’s a photo of calipers disassembled by my collaborator Mitko (annotations mine):</p>

<p><img src="https://kevinlynagh.com/calipertron/book_calipers.jpg" alt=""></p>

<p>The left half is the caliper’s stationary metal spine, which contains a passive PCB with a pattern of “reflectors” (which look sorta like the capital letter “T” rotated 90 degrees clockwise).</p>

<p>The right half is the powered display part of the calipers, which slides up and down along the metal stem; this PCB has a long receiver pad and a bunch of emitter pads that look like the keys of a piano.</p>

<p>When assembled (folded together like a book), the reflector “T” stems are over top the signal-emitting piano keys and the reflector “T” crossbars are over the receiver pad.</p>

<p>(Pedant note: There’s not actually any “reflection” going on, the plates are capacitively coupled. I just find the term “reflector” conveys the right vibe.)</p>

<p>Here’s a close up, taken from Big Clive’s excellent caliper <a href="https://youtu.be/fKSSY1gzCEs?t=588">teardown video</a> (annotations mine):</p>

<p><img src="https://kevinlynagh.com/calipertron/diagram.jpg" alt=""></p>

<p>The top half is the stationary part and the bottom half is slidey part.</p>

<p>Note the following details of the geometry:</p>

<ul>
<li>The slidey part emits 8 signals through the little piano keys (labeled), which repeat along the entire length.</li>
<li>The reflector plate stems are exactly 4 keys wide.</li>
</ul>

<p>In essence, the reflector plate “adds up” the signals of the piano keys underneath it (in this photo, signals 0, 1, 2, and 3).</p>

<p>Imagine sliding the caliper display 0.5 keys to the right.
Then the reflectors would be above <em>half</em> of signal 0, all of signals 1, 2, 3, and half of signal 4.
Sliding another 0.5 keys to the right, the reflectors would then be on top of signals 1, 2, 3, and 4.</p>

<p>The reflectors are just passive pieces of metal; all they can do is sum together the signals coupled to them.
This summed signal is then reflected back to the slidey part’s single receiver pad.</p>

<p>So what are the 8 signals you should emit?</p>

<p>If you use sine waves at the same frequency but different phases, then their reflected sum will always be a sine wave of the original frequency, with some combined phase and amplitude (<a href="https://www.johndcook.com/blog/2020/08/17/adding-phase-shifted-sine-waves/">proof</a>).</p>

<p>That is: <strong>as you move the slidey part, the phase offset of the reflected signal changes</strong>.</p>

<p>Since we have 8 signals, if we evenly divide the unit circle so that the nth signal is:</p>

<p>$$\sin\left( 2\pi f + 2\pi\frac{n}{8} \right)$$</p>

<p>then we can track the cumulative phase offset of the received signal (relative to some initial position) and know that every $2\pi$ moved in phase space corresponds to a linear movement 8 emitter keys wide.</p>

<h2 id="microcontroller-implementation">Microcontroller implementation</h2>

<p>Mitko mailed me version 1.1 of his PCB (<a href="https://github.com/MitkoDyakov/Calipatron/blob/444c72c3e81eab0a2e7ee198f5574062dc1fc510/Hardware/V1.1/Schematics%20V1.1.pdf">schematic</a>), which is built around an stm32f103 microcontroller.</p>

<p>I wrote the firmware using the <a href="https://github.com/embassy-rs/embassy">Embassy Rust framework</a>, which worked reasonably well.
(“Well” as far as embedded goes — there was a side quest tracking down an intermittent freeze that locked out the debugger, which seems to be a genuine hardware bug triggered only on <a href="https://github.com/lynaghk/repro-stm32f103-rust-embassy-freeze/">older ARM core silicon revisions</a>.)</p>

<p>The firmware needs to:</p>

<ul>
<li>emit 8 sinusoidal waves</li>
<li>measure the reflected sum calculate the phase offset</li>
</ul>

<p>Let’s take these in turn.</p>

<h2 id="emitting-sinusoidal-waves">Emitting sinusoidal waves</h2>

<p>The stm32f103 doesn’t have a digital to analog converter, but we can emit a sinusoidal wave using “pulse density modulation” (PDM).
The technique is similar to PWM (“pulse width modulation”) in that an analog signal level is approximated by having a digital signal stay “on” for the appropriate fraction of time.
But while PWM has its “on” fraction all at once, the PDM signal spaces it out across the sample window.</p>

<p>For example, to represent an analog level of 50% using 8 pulses:</p>
<div><pre><span></span>PWM: X X X X . . . .
PDM: X . X . X . X .
</pre></div>
<p>the PWM signal stays high (<code>x</code>) for the first half of the period, whereas the PDM signal alternates.
This is preferable for our use case, since it means the switching noise is at a higher frequency, further away from our lower frequency sinusoidal wave.</p>

<p>We need all of the waves to move in lockstep with each other, so rather than updating the pins one-by-one, we update all of them with a single 32-bit write to the GPIO’s “bit set reset register” (BSRR).</p>

<p>Furthermore, since we know how many PDM samples we want in advance, we can take pity on our lil’ stm32f103 (which doesn’t even have a hardware floating point unit) and calculate all of the BSRR values at compile-time.
Rust’s formal compile-time machinery <a href="https://users.rust-lang.org/t/constant-trigonometry/58565">doesn’t support trigonometry</a>, so we use a <code>build.rs</code> script to generate a string of code at compile-time:</p>
<div><pre><span></span><span>fn</span> <span>generate_pdm_bsrr</span><span>(</span><span>n_samples</span>: <span>usize</span><span>)</span><span> </span>-&gt; <span>String</span> <span>{</span><span></span>
<span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>output</span><span> </span><span>=</span><span> </span><span>String</span>::<span>new</span><span>();</span><span></span>
<span>    </span><span>output</span><span>.</span><span>push_str</span><span>(</span><span>"pub const PDM_SIGNAL: [u32; "</span><span>);</span><span></span>
<span>    </span><span>output</span><span>.</span><span>push_str</span><span>(</span><span>&amp;</span><span>n_samples</span><span>.</span><span>to_string</span><span>());</span><span></span>
<span>    </span><span>output</span><span>.</span><span>push_str</span><span>(</span><span>"] = [</span><span>\n</span><span>"</span><span>);</span><span></span>

<span>    </span><span>let</span><span> </span><span>n_waves</span><span> </span><span>=</span><span> </span><span>8</span><span>;</span><span></span>

<span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>errors</span><span> </span><span>=</span><span> </span><span>vec!</span><span>[</span><span>0.0</span><span>;</span><span> </span><span>n_waves</span><span>];</span><span></span>
<span>    </span><span>for</span><span> </span><span>sample</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>n_samples</span><span> </span><span>{</span><span></span>
<span>        </span><span>let</span><span> </span><span>mut</span><span> </span><span>bsrr</span><span> </span><span>=</span><span> </span><span>0</span><span>u32</span><span>;</span><span></span>
<span>        </span><span>for</span><span> </span><span>wave</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>n_waves</span><span> </span><span>{</span><span></span>
<span>            </span><span>let</span><span> </span><span>phase_offset</span><span> </span><span>=</span><span> </span><span>2.0</span><span> </span><span>*</span><span> </span><span>PI</span><span> </span><span>*</span><span> </span><span>(</span><span>wave</span><span> </span><span>as</span><span> </span><span>f64</span><span>)</span><span> </span><span>/</span><span> </span><span>(</span><span>n_waves</span><span> </span><span>as</span><span> </span><span>f64</span><span>);</span><span></span>
<span>            </span><span>let</span><span> </span><span>angle</span><span> </span><span>=</span><span> </span><span>2.0</span><span> </span><span>*</span><span> </span><span>PI</span><span> </span><span>*</span><span> </span><span>(</span><span>sample</span><span> </span><span>as</span><span> </span><span>f64</span><span> </span><span>/</span><span> </span><span>n_samples</span><span> </span><span>as</span><span> </span><span>f64</span><span>)</span><span> </span><span>+</span><span> </span><span>phase_offset</span><span>;</span><span></span>
<span>            </span><span>let</span><span> </span><span>cosine</span><span> </span><span>=</span><span> </span><span>angle</span><span>.</span><span>cos</span><span>()</span><span> </span><span>as</span><span> </span><span>f32</span><span>;</span><span></span>
<span>            </span><span>let</span><span> </span><span>normalized_signal</span><span> </span><span>=</span><span> </span><span>(</span><span>cosine</span><span> </span><span>+</span><span> </span><span>1.0</span><span>)</span><span> </span><span>/</span><span> </span><span>2.0</span><span>;</span><span></span>

<span>            </span><span>if</span><span> </span><span>normalized_signal</span><span> </span><span>&gt;</span><span> </span><span>errors</span><span>[</span><span>wave</span><span>]</span><span> </span><span>{</span><span></span>
<span>                </span><span>bsrr</span><span> </span><span>|=</span><span> </span><span>1</span><span> </span><span>&lt;&lt;</span><span> </span><span>wave</span><span>;</span><span> </span><span>// set bit</span>
<span>                </span><span>errors</span><span>[</span><span>wave</span><span>]</span><span> </span><span>+=</span><span> </span><span>1.0</span><span> </span><span>-</span><span> </span><span>normalized_signal</span><span>;</span><span></span>
<span>            </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span><span></span>
<span>                </span><span>bsrr</span><span> </span><span>|=</span><span> </span><span>1</span><span> </span><span>&lt;&lt;</span><span> </span><span>(</span><span>wave</span><span> </span><span>+</span><span> </span><span>16</span><span>);</span><span> </span><span>// reset bit</span>
<span>                </span><span>errors</span><span>[</span><span>wave</span><span>]</span><span> </span><span>-=</span><span> </span><span>normalized_signal</span><span>;</span><span></span>
<span>            </span><span>}</span><span></span>
<span>        </span><span>}</span><span></span>
<span>        </span><span>output</span><span>.</span><span>push_str</span><span>(</span><span>&amp;</span><span>format!</span><span>(</span><span>"    {:#034b},</span><span>\n</span><span>"</span><span>,</span><span> </span><span>bsrr</span><span>));</span><span></span>
<span>    </span><span>}</span><span></span>

<span>    </span><span>output</span><span>.</span><span>push_str</span><span>(</span><span>"];</span><span>\n</span><span>"</span><span>);</span><span></span>
<span>    </span><span>output</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>This emitted string is then written to a file, which we <code>import</code> as usual from our main code namespace.
The <code>PDM_SIGNAL</code> const slice is then baked into the firmware, and at runtime a hardware timer and a DMA task is used copy each value directly to BSRR at a fixed rate.
This prevents any jitter in the emitted signal, as after starting the transmission the CPU is no longer involved.</p>

<h2 id="measuring-phase-offset">Measuring phase offset</h2>

<p>The reflected composite wave is measured by the stm32f103’s ADC.
A DMA task is triggered at the same time as the emitted PDM signals, and it reads a fixed number of samples into a buffer.</p>

<p>So how do we get the phase offset?</p>

<p>If you’re like me, the first thing you need to do is read some textbooks to figure out what’s what.
I recommend <a href="https://amzn.to/3MDLdZN">Understanding Digital Signal Processing</a> by Richard Lyons, as the book has a casual friendly style and is clearly written by an experienced engineer — the final chapter is simply 150 pages of “Digital Signal Processing Tricks”!</p>

<p>Anyway, we know from earlier that our reflected signal $s(t)$ is the sum of the sinusoidal signals that we emitted, so it must also be a sinusoid with some phase offset; let’s call it $A \cos( \omega t + \phi)$ (with $A$ some constant representing a change in amplitude due to our capacitive coupling, amplification, etc. compared to our original emitted signal).</p>

<p>Then as I’m sure you recall from <a href="https://mathworld.wolfram.com/TrigonometricAdditionFormulas.html">trigonometric addition formula</a> from grade school, we can rewrite this as:</p>

<p>$$
\begin{align}
s(t) &amp;= A \cos( \omega t + \phi)\newline
     &amp;= A \left[ \cos( \omega t)\cos(\phi) - \sin( \omega t)\sin(\phi) \right]
\end{align}
$$</p>

<p>If we correlate our signal with $\cos(\omega t)$ then we’re left with $A \cos(\phi)$, and similarly for sin.
Thus:</p>

<p>$$
\begin{align}
\frac{\mathrm{Corr}\left(s(t), \sin(\omega t)\right)}{\mathrm{Corr}\left(s(t), \cos(\omega t)\right)} &amp;= \frac{A \sin(\phi)} {A \cos(\phi)} \newline
\arctan\left(\frac{\mathrm{Corr}\left(s(t), \sin(\omega t)\right)}{\mathrm{Corr}\left(s(t), \cos(\omega t)\right)}\right) &amp;= \phi
\end{align}
$$</p>

<p>The correlation operator itself is simple: it’s the sum of the product of the two signals at matching points in time.</p>

<p>All we need to do is figure out the exact times of our $s(t)$ samples, which can be derived by the sampling rate of the ADC.</p>

<p>All of the terms besides our measured signal samples $s(t)$ are knowable at compile-time, so we can again generate a lookup table for our microcontroller to use:</p>
<div><pre><span></span><span>fn</span> <span>generate_sine_cosine_table</span><span>(</span><span></span>
<span>    </span><span>signal_frequency</span>: <span>f64</span><span>,</span><span></span>
<span>    </span><span>sampling_frequency</span>: <span>f64</span><span>,</span><span></span>
<span>    </span><span>num_samples</span>: <span>usize</span><span>,</span><span></span>
<span>)</span><span> </span>-&gt; <span>String</span> <span>{</span><span></span>
<span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>output</span><span> </span><span>=</span><span> </span><span>String</span>::<span>new</span><span>();</span><span></span>
<span>    </span><span>output</span><span>.</span><span>push_str</span><span>(</span><span>"pub const SINE_COSINE_TABLE: [(f32, f32); "</span><span>);</span><span></span>
<span>    </span><span>output</span><span>.</span><span>push_str</span><span>(</span><span>&amp;</span><span>num_samples</span><span>.</span><span>to_string</span><span>());</span><span></span>
<span>    </span><span>output</span><span>.</span><span>push_str</span><span>(</span><span>"] = [</span><span>\n</span><span>"</span><span>);</span><span></span>

<span>    </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>num_samples</span><span> </span><span>{</span><span></span>
<span>        </span><span>let</span><span> </span><span>angle</span><span> </span><span>=</span><span> </span><span>2.0</span><span> </span><span>*</span><span> </span><span>PI</span><span> </span><span>*</span><span> </span><span>signal_frequency</span><span> </span><span>*</span><span> </span><span>(</span><span>i</span><span> </span><span>as</span><span> </span><span>f64</span><span> </span><span>*</span><span> </span><span>(</span><span>1.0</span><span> </span><span>/</span><span> </span><span>sampling_frequency</span><span>));</span><span></span>
<span>        </span><span>let</span><span> </span><span>sine</span><span> </span><span>=</span><span> </span><span>angle</span><span>.</span><span>sin</span><span>()</span><span> </span><span>as</span><span> </span><span>f32</span><span>;</span><span></span>
<span>        </span><span>let</span><span> </span><span>cosine</span><span> </span><span>=</span><span> </span><span>angle</span><span>.</span><span>cos</span><span>()</span><span> </span><span>as</span><span> </span><span>f32</span><span>;</span><span></span>
<span>        </span><span>output</span><span>.</span><span>push_str</span><span>(</span><span>&amp;</span><span>format!</span><span>(</span><span>"    ({:?}, {:?}),</span><span>\n</span><span>"</span><span>,</span><span> </span><span>sine</span><span>,</span><span> </span><span>cosine</span><span>));</span><span></span>
<span>    </span><span>}</span><span></span>

<span>    </span><span>output</span><span>.</span><span>push_str</span><span>(</span><span>"];</span><span>\n</span><span>"</span><span>);</span><span></span>
<span>    </span><span>output</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>The build.rs script is then:</p>
<div><pre><span></span><span>fn</span> <span>main</span><span>()</span><span> </span><span>{</span><span></span>

<span>    </span><span>// lol, compile-time-programming by literally writing code to a file that we import</span>
<span>    </span><span>let</span><span> </span><span>out_dir</span><span> </span><span>=</span><span> </span><span>std</span>::<span>env</span>::<span>var</span><span>(</span><span>"OUT_DIR"</span><span>).</span><span>unwrap</span><span>();</span><span></span>
<span>    </span><span>let</span><span> </span><span>dest_path</span><span> </span><span>=</span><span> </span><span>std</span>::<span>path</span>::<span>Path</span>::<span>new</span><span>(</span><span>&amp;</span><span>out_dir</span><span>).</span><span>join</span><span>(</span><span>"constants.rs"</span><span>);</span><span></span>
<span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>f</span><span> </span><span>=</span><span> </span><span>File</span>::<span>create</span><span>(</span><span>&amp;</span><span>dest_path</span><span>).</span><span>unwrap</span><span>();</span><span></span>

<span>    </span><span>let</span><span> </span><span>pdm_frequency</span>: <span>u32</span> <span>=</span><span> </span><span>100_000</span><span>;</span><span> </span><span>// 100 kHz</span>
<span>    </span><span>f</span><span>.</span><span>write_all</span><span>(</span><span>format!</span><span>(</span><span>"pub const PDM_FREQUENCY: u32 = {:?};</span><span>\n</span><span>"</span><span>,</span><span> </span><span>pdm_frequency</span><span>).</span><span>as_bytes</span><span>())</span><span></span>
<span>        </span><span>.</span><span>unwrap</span><span>();</span><span></span>

<span>    </span><span>let</span><span> </span><span>pdm_length</span><span> </span><span>=</span><span> </span><span>128</span><span>;</span><span></span>

<span>    </span><span>let</span><span> </span><span>num_samples</span><span> </span><span>=</span><span> </span><span>512</span><span>;</span><span></span>
<span>    </span><span>let</span><span> </span><span>signal_frequency</span><span> </span><span>=</span><span> </span><span>pdm_frequency</span><span> </span><span>as</span><span> </span><span>f64</span><span> </span><span>/</span><span> </span><span>pdm_length</span><span> </span><span>as</span><span> </span><span>f64</span><span>;</span><span></span>
<span>    </span><span>let</span><span> </span><span>adc_frequency</span><span> </span><span>=</span><span> </span><span>12_000_000.</span><span>;</span><span></span>
<span>    </span><span>let</span><span> </span><span>adc_sample_cycles</span><span> </span><span>=</span><span> </span><span>71.5</span><span>;</span><span></span>
<span>    </span><span>let</span><span> </span><span>adc_sample_overhead_cycles</span><span> </span><span>=</span><span> </span><span>12.5</span><span>;</span><span> </span><span>// see reference manual section 11.6</span>
<span>    </span><span>let</span><span> </span><span>sampling_frequency</span><span> </span><span>=</span><span> </span><span>adc_frequency</span><span> </span><span>/</span><span> </span><span>(</span><span>adc_sample_cycles</span><span> </span><span>+</span><span> </span><span>adc_sample_overhead_cycles</span><span>);</span><span></span>

<span>    </span><span>f</span><span>.</span><span>write_all</span><span>(</span><span></span>
<span>        </span><span>generate_sine_cosine_table</span><span>(</span><span>signal_frequency</span><span>,</span><span> </span><span>sampling_frequency</span><span>,</span><span> </span><span>num_samples</span><span>).</span><span>as_bytes</span><span>(),</span><span></span>
<span>    </span><span>)</span><span></span>
<span>    </span><span>.</span><span>unwrap</span><span>();</span><span></span>

<span>    </span><span>f</span><span>.</span><span>write_all</span><span>(</span><span>generate_pdm_bsrr</span><span>(</span><span>pdm_length</span><span>).</span><span>as_bytes</span><span>())</span><span></span>
<span>        </span><span>.</span><span>unwrap</span><span>();</span><span></span>

<span>    </span><span>/// ...</span>
<span>}</span><span></span>
</pre></div>
<p>Finally, at runtime we’re left with this loop:</p>
<div><pre><span></span><span>loop</span><span> </span><span>{</span><span></span>

<span>  </span><span>// start PDM emission via DMA</span>
<span>  </span><span>// start ADC via DMA</span>
<span>  </span><span>// wait for ADC to read NUM_SAMPLES</span>
<span>  </span><span>// then...</span>

<span>  </span><span>let</span><span> </span><span>mut</span><span> </span><span>sum_sine</span>: <span>f32</span> <span>=</span><span> </span><span>0.0</span><span>;</span><span></span>
<span>  </span><span>let</span><span> </span><span>mut</span><span> </span><span>sum_cosine</span>: <span>f32</span> <span>=</span><span> </span><span>0.0</span><span>;</span><span></span>

<span>  </span><span>let</span><span> </span><span>adc_buf</span><span> </span><span>=</span><span> </span><span>unsafe</span><span> </span><span>{</span><span> </span><span>&amp;</span><span>ADC_BUF</span><span>[</span><span>..</span><span>]</span><span> </span><span>};</span><span></span>

<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>NUM_SAMPLES</span><span> </span><span>{</span><span></span>
<span>      </span><span>let</span><span> </span><span>(</span><span>sine</span><span>,</span><span> </span><span>cosine</span><span>)</span><span> </span><span>=</span><span> </span><span>SINE_COSINE_TABLE</span><span>[</span><span>i</span><span>];</span><span></span>
<span>      </span><span>sum_sine</span><span> </span><span>+=</span><span> </span><span>adc_buf</span><span>[</span><span>i</span><span>]</span><span> </span><span>as</span><span> </span><span>f32</span><span> </span><span>*</span><span> </span><span>sine</span><span>;</span><span></span>
<span>      </span><span>sum_cosine</span><span> </span><span>+=</span><span> </span><span>adc_buf</span><span>[</span><span>i</span><span>]</span><span> </span><span>as</span><span> </span><span>f32</span><span> </span><span>*</span><span> </span><span>cosine</span><span>;</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>let</span><span> </span><span>phase</span><span> </span><span>=</span><span> </span><span>sum_sine</span><span>.</span><span>atan2</span><span>(</span><span>sum_cosine</span><span>);</span><span></span>

<span>  </span><span>// add latest phase reading to position estimation.</span>
<span>  </span><span>// this object also handles wraparound and hysteresis.</span>
<span>  </span><span>position_estimator</span><span>.</span><span>update</span><span>(</span><span>phase</span><span>);</span><span></span>
<span>  </span><span>info</span><span>!</span><span>(</span><span>"Phase: {} Position: {}"</span><span>,</span><span> </span><span>phase</span><span>,</span><span> </span><span>position_estimator</span><span>.</span><span>position</span><span>);</span><span></span>

<span>  </span><span>if</span><span> </span><span>user_button</span><span>.</span><span>is_low</span><span>()</span><span> </span><span>{</span><span></span>
<span>      </span><span>info</span><span>!</span><span>(</span><span>"Button pressed, zeroing"</span><span>);</span><span></span>
<span>      </span><span>position_estimator</span><span>.</span><span>position</span><span> </span><span>=</span><span> </span><span>0.</span><span>;</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>
</pre></div>
<h2 id="precision">Precision</h2>

<p>There are several parameters we need to select in the firmware to implement the measurement:</p>

<ul>
<li>The frequency at which we emit the PDM pulses</li>
<li>The number of PDM pulses across which we divide a single sinusoid period</li>
<li>The ADC sampling time</li>
<li>The number of ADC samples we take for each phase calculation</li>
</ul>

<p>Rather than try to calculate the ideal parameters from first principles (which would probably depend on all sorts of specifics like the PCB soldermask thickness and trace resistances), let’s just try them all and see what works best.
More specifically: for each parameter configuration, which has the lowest standard deviation for multiple measurements taken from the same physical slide position?</p>

<p>Instead of reflashing the firmware for each set of parameters, we can write a special <a href="https://github.com/lynaghk/calipertron/blob/2ceedc0e477498601dcbd6a6f257012657070bf9/firmware/src/bin/recorder.rs">recorder firmware</a> that can be controlled by a laptop to try different parameters configurations.
Then we can stream  the raw ADC readings back to the laptop, which lets us try a much wider variety parameters.</p>

<p>You can look at the <a href="https://github.com/lynaghk/calipertron/blob/2ceedc0e477498601dcbd6a6f257012657070bf9/analysis/parameter_sweep.ipynb">parameter sweep notebook</a> for the gory details, but here’s the summary in a plot:</p>

<p><img src="https://kevinlynagh.com/calipertron/parameter_sweep.jpg" alt=""></p>

<p>There’s a lot going on here, let’s break it down.
Schematically:</p>

<ul>
<li>The Y-axis corresponds to the standard deviation of recorded phases (lower is better — after all, the slide is never moving, so ideally all measurements would return the same value)</li>
<li>The X-axis corresponds to the frequency for which we’re sending out the PDM pulses of the emitted sinusoidal signal. All data here are for 128 PDM segments and an X-axis spacing of 2 kHz.</li>
<li>Each sub-plot corresponds to a different “window size” — i.e., how many ADC samples we correlate to derive a single phase measurement.</li>
<li>Each colored line is a different ADC sampling frequency (the stm32f103’s ADC can sample over 8 different periods, and we’re trying all of them)</li>
</ul>

<p>What stands out to me in the data itself:</p>

<ul>
<li>The higher the ADC frequency, the higher the PDM frequency needs to be before we start to pick up the signal (i.e., for the standard deviation to drop).
This makes sense to me, as if the ADC frequency is much higher than the signal’s, our window of samples won’t see the signal change much at all, so it’s going to be dominated by noise and we’ll have no idea what the phase is.</li>
<li>Increasing the window size tends to improve precision — makes sense, as it means we’re looking at more samples and (presumably) reducing the effect of noise.</li>
<li>The lowest ADC sampling frequency (i.e., the longest ADC sampling period) tends to have the best performance.</li>
<li>There’s a strange cat at 250 kHz; this is probably the point at which our ADC sampling rate isn’t fast enough to keep up with the signal itself. With 250 kHz / 128 PDM segments implies our emitted sinusoid is at about 1950 Hz</li>
</ul>

<p>While in this static test increasing the window size and ADC sampling period looks best, that does mean it’ll reduce the rate at which we can actually calculate the phase, which limits how fast the slider can move before it loses track of its absolute position.</p>

<p>So, based on this survey I decided to set the parameters for the local, in-firmware calculation (as seen in the demo video) to be the point indicated by the red arrow:</p>

<ul>
<li>window size = 128</li>
<li>PDM frequency = 222 kHz</li>
<li>ADC sampling frequency = 222.2 kHz</li>
</ul>

<p>It actually makes sense that the phase deviation is a minimum here: At this ADC sampling frequency and a window size of 128, we’re pretty much matching a full period of the emitted 128-segment PDM signal.</p>

<p>The timestamps in the demo video show about 1.5 ms between readings (0.6 kHz), which is about three times the ideal limit (222.2 kHz / 128 samples =&gt; 1.7 kHz), probably due to the time it takes to do correlation math, print to the host computer, and cycle through the Embassy async machinery.
I’m sure a more optimized implementation could hit the limit by, e.g., calculating the correlations while the samples are being collected rather than afterwards.</p>

<p>As for the precision, taking 200ms worth of phase measurements from the printed logs (n = 124) while the slide isn’t moving, the standard deviation of the phase is 0.039 radians, which (for my PCB with 8 emitter keys = 9.4 mm) is a position error of about $ 0.039 * 9.4\,\mathrm{mm} / 2\pi = 0.6\,\mathrm{mm}$.</p>

<p>Honestly, this is way better than I expected — especially since the stm32f103 came out in 2007, the drive signal is created by just banging on GPIO, and only conditioning for the received signal is a fixed-gain amplifier (we’re not even filtering out the 50 Hz line noise).</p>

<h2 id="misc-tips-lessons-learned">Misc. tips / lessons learned</h2>

<ul>
<li>I’m very happy with the workflow of streaming raw data to my computer over USB and then doing analysis in Python notebooks. This was also helpful for letting continue work on the project while I was traveling and away from the working hardware.</li>
<li>I’m not super-well versed in the Python ecosystem, and it was spectacular asking LLMs like Claude stuff like, “Can you please plot an FFT of these data samples”, “Run these bad boys through a low pass filter at with cutoff at 1000 Hz”, “Can you please write an a phase accumulator that handles wraparound correctly”, etc. Being able to use <a href="https://kevinlynagh.com/newsletter/2024_10_transcription_app_art_wall/">my lightweight dictation app</a> to just ramble out thoughts/ideas at an LLM was particularly satisfying.</li>
<li>Even with LLM-assistance, plotting was more difficult than I expected:

<ul>
<li>The JS-based interactive plots (Plotly, etc.) blew up when I tried to visualize raw data with just ~100k samples.</li>
<li>The matplotlib-based static plotting libraries didn’t make it easy to read exact coordinates from a point on the plot interactively.</li>
<li>Doing data aggregation with <a href="https://pola.rs/">Polars</a> was pretty good, but it wasn’t obvious to me how to aggregate + plot, e.g., both underlying data and their standard deviation across dimensions.</li>
<li>Likely I just need to pick a Python plotting library and spend 20 hours getting fluent.</li>
</ul></li>
<li>The proliferation of complex type signatures in the Rust Embedded ecosystem still fails to spark joy — I’m stuck in the local optima of “literally just write everything in <code>fn main()</code> with a loop at the bottom” so I never have to write out the type signatures.</li>
</ul>

<h2 id="future-improvements">Future improvements</h2>

<p>I probably won’t go further on this until I have some sort of robotics context-of-use.
But for anyone who’s in the market for a project, here are a few ideas:</p>

<ul>
<li>Figure out how to make a parametric caliper emitter/reflector designs in PCB tools like <a href="https://atopile.io/">atopile</a> or some kind of KiCAD footprint generation script.</li>
<li>Design something specifically to work with aluminum extrusion motion systems and see how cheap/accurate you can make it.</li>
<li>Compare to more “off the shelf” positioning systems like magnetic tape or <a href="https://blog.adafruit.com/2024/02/29/eye-on-npi-triad-semiconductor-ts4631-light-to-digital-converter-eyeonnpi-adafruit-digikey-digikey-triadsemi-adafruit/">SteamVR trackers</a>.</li>
<li>Actually make this into a functional caliper by designing more suitable housing (as you can see from the video, my 3d printed base is just clamped to my desk).</li>
</ul>

<h2 id="thanks">Thanks</h2>

<ul>
<li>Mitko for designing the PCB hardware. See the details on the <a href="https://hackaday.io/project/194778-diy-digital-caliper-calipatron">hackday.io project page</a>.</li>
<li><a href="https://blog.npry.dev/">Nathan Perry</a> and <a href="https://jeffmcbride.net/">Jeff McBride</a> for helping me track down the <a href="https://github.com/lynaghk/repro-stm32f103-rust-embassy-freeze/">intermittent stm32f103 freeze up</a> to a bug in the CPU silicon. (Let’s all do our best to never run into that again!)</li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elwood Edwards, Voice of AOL's 'You've Got Mail ' Alert, Dies at 74 (145 pts)]]></title>
            <link>https://www.nytimes.com/2024/11/07/technology/elwood-edwards-aol-dead.html</link>
            <guid>42087087</guid>
            <pubDate>Fri, 08 Nov 2024 14:29:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/11/07/technology/elwood-edwards-aol-dead.html">https://www.nytimes.com/2024/11/07/technology/elwood-edwards-aol-dead.html</a>, See on <a href="https://news.ycombinator.com/item?id=42087087">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/11/07/technology/elwood-edwards-aol-dead.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[How to self-host all of Bluesky except the AppView (for now) (112 pts)]]></title>
            <link>https://alice.bsky.sh/post/3laega7icmi2q</link>
            <guid>42086596</guid>
            <pubDate>Fri, 08 Nov 2024 13:13:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alice.bsky.sh/post/3laega7icmi2q">https://alice.bsky.sh/post/3laega7icmi2q</a>, See on <a href="https://news.ycombinator.com/item?id=42086596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p node="[object Object]">Did you know? You can self-host and/or mirror almost all of Bluesky's infrastructure today!</p>
<blockquote node="[object Object]">
<p node="[object Object]">This article assumes familiarity with the architecture of Bluesky, general *nix knowledge and basic knowledge of TypeScript, Go and Rust toolchains</p>
</blockquote>
<h2 node="[object Object]">PDS</h2>
<p node="[object Object]">The most obvious one: having your own <a href="https://docs.bsky.app/docs/advanced-guides/federation-architecture#personal-data-server-pds" node="[object Object]">PDS</a> and owning your data. You can find the GitHub repository with instructions <a href="https://github.com/bluesky-social/pds?tab=readme-ov-file#self-hosting-pds" node="[object Object]">here</a>; you can migrate your existing account with the <a href="https://github.com/bluesky-social/indigo/tree/main/cmd/goat" node="[object Object]">GOAT tool</a>, for which <a href="https://bsky.app/profile/bnewbold.net" node="[object Object]">Bryan Newbold</a> has written an <a href="https://whtwnd.com/bnewbold.net/entries/Migrating%20PDS%20Account%20with%20%60goat%60" node="[object Object]">excellent howto post</a>.</p>
<h2 node="[object Object]">Relay</h2>
<p node="[object Object]">Bryan has a great blog post on how to set up your own <a href="https://docs.bsky.app/docs/advanced-guides/federation-architecture#relay" node="[object Object]">Relay</a> <a href="https://whtwnd.com/bnewbold.net/entries/Notes%20on%20Running%20a%20Full-Network%20atproto%20Relay%20(July%202024)" node="[object Object]">right here</a>. Things have grown since, so make sure you have at least ~4.5 TB of disk space (maybe more). Two important comments, not in the post: make sure to use the <code node="[object Object]">--disk-persister-dir=/data/events</code> flag as well as <a href="https://github.com/bluesky-social/indigo/blob/d1686e9b80835948407fd7ad2428e92379298dac/cmd/bigsky/main.go#L136-L141" node="[object Object]">enable compaction</a>.</p>
<h2 node="[object Object]">Jetstream</h2>
<p node="[object Object]"><a href="https://github.com/bluesky-social/jetstream" node="[object Object]">Jetstream</a> is like having your own Relay firehose, but uses a fraction of the bandwidth, storage, and gives you friendly JSON instead of <a href="https://en.wikipedia.org/wiki/CBOR" node="[object Object]">CBOR</a>-encoded <a href="https://en.wikipedia.org/wiki/Merkle_tree" node="[object Object]">MST</a> blocks. After cloning the GitHub repository, you can either use the included <code node="[object Object]">docker-compose.yaml</code> with <code node="[object Object]">make up</code> or build it directly with <code node="[object Object]">make build</code> and use your favorite keep-this-process-running-pretty-please tool (systemd, pm2 etc.) Don't forget to take a look at the <a href="https://github.com/bluesky-social/jetstream/blob/0ab10bd041fe1fdf682d3964b20d944905c4862d/cmd/jetstream/main.go#L36-L103" node="[object Object]">available CLI arguments/env variables</a>, which let you do things like change the default retention (24 hours) or override the Firehose cursor and backfill it with 3 days of data—which is what's available from the official relays.</p>
<h2 node="[object Object]">plc.directory mirror</h2>
<p node="[object Object]">To get a mirror of <a href="https://plc.directory/" node="[object Object]">plc.directory</a>, with almost all users on Bluesky (they <a href="https://atproto.com/specs/did#blessed-did-methods" node="[object Object]">do support did:web</a>, but they are few and far between), clone <a href="https://bsky.app/profile/str4d.xyz" node="[object Object]">str4d</a>'s <a href="https://github.com/str4d/plc" node="[object Object]"><code node="[object Object]">plc</code> repo</a>, switch to the <code node="[object Object]">mirror</code> branch, run <code node="[object Object]">cargo run --features mirror -- mirror run mirror.db</code>, then sit back and wait 5-10 hours. Once it's done and up-to-date, you have a full replica in a neat sqlite3 DB. You can monitor its status with something like <code node="[object Object]">watch -n60 'sqlite3 mirror.db "select count(*) from identity;"'</code> in a <a href="https://github.com/tmux/tmux/wiki" node="[object Object]">tmux</a> pane.</p>
<h2 node="[object Object]">The official web/mobile app, also known as <code node="[object Object]">social-app</code></h2>
<p node="[object Object]">Clone the <a href="https://github.com/bluesky-social/social-app" node="[object Object]">repo</a>, run <code node="[object Object]">yarn &amp;&amp; yarn web</code>, and you have a fully-functional copy of it in mere minutes that you can modify to your heart's desire. Doing this on mobile is a lot more involved and <a href="https://github.com/bluesky-social/social-app/blob/main/docs/build.md#iosandroid-build" node="[object Object]">documented here</a>.</p>
<h2 node="[object Object]">AppView</h2>
<p node="[object Object]">The elephant in the room is, of course, running your own Bluesky <a href="https://atproto.com/guides/glossary#app-view" node="[object Object]">AppView</a>. If you're interested, DM me on Bluesky to join my working group to make it happen!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LoRA vs. Full Fine-Tuning: An Illusion of Equivalence (204 pts)]]></title>
            <link>https://arxiv.org/abs/2410.21228</link>
            <guid>42085665</guid>
            <pubDate>Fri, 08 Nov 2024 09:58:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.21228">https://arxiv.org/abs/2410.21228</a>, See on <a href="https://news.ycombinator.com/item?id=42085665">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.21228">View PDF</a>
    <a href="https://arxiv.org/html/2410.21228v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Fine-tuning is a crucial paradigm for adapting pre-trained large language models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA) have been shown to match the performance of fully fine-tuned models on various tasks with an extreme reduction in the number of trainable parameters. Even in settings where both methods learn similarly accurate models, \emph{are their learned solutions really equivalent?} We study how different fine-tuning methods change pre-trained models by analyzing the model's weight matrices through the lens of their spectral properties. We find that full fine-tuning and LoRA yield weight matrices whose singular value decompositions exhibit very different structure; moreover, the fine-tuned models themselves show distinct generalization behaviors when tested outside the adaptation task's distribution. More specifically, we first show that the weight matrices trained with LoRA have new, high-ranking singular vectors, which we call \emph{intruder dimensions}. Intruder dimensions do not appear during full fine-tuning. Second, we show that LoRA models with intruder dimensions, despite achieving similar performance to full fine-tuning on the target task, become worse models of the pre-training distribution and adapt less robustly to multiple tasks sequentially. Higher-rank, rank-stabilized LoRA models closely mirror full fine-tuning, even when performing on par with lower-rank LoRA models on the same tasks. These results suggest that models updated with LoRA and full fine-tuning access different parts of parameter space, even when they perform equally on the fine-tuned distribution. We conclude by examining why intruder dimensions appear in LoRA fine-tuned models, why they are undesirable, and how their effects can be minimized.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Reece Shuttleworth [<a href="https://arxiv.org/show-email/8c29db1a/2410.21228" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 28 Oct 2024 17:14:01 UTC (9,438 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Λ-2D: An Exploration of Drawing as Programming Language (188 pts)]]></title>
            <link>https://www.media.mit.edu/projects/2d-an-exploration-of-drawing-as-programming-language-featuring-ideas-from-lambda-calculus/overview/</link>
            <guid>42085273</guid>
            <pubDate>Fri, 08 Nov 2024 08:30:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.media.mit.edu/projects/2d-an-exploration-of-drawing-as-programming-language-featuring-ideas-from-lambda-calculus/overview/">https://www.media.mit.edu/projects/2d-an-exploration-of-drawing-as-programming-language-featuring-ideas-from-lambda-calculus/overview/</a>, See on <a href="https://news.ycombinator.com/item?id=42085273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
                <div>
                            

                            <p>&nbsp;An Exploration of Drawing as Programming Language, Featuring Ideas from Lambda Calculus</p>
                        </div>

                
                    <div>
    
        

    
        



    
        <div>
        <p><span>How can we code through drawing?</span>
        </p>
    </div>

    
        <div>
            <p>The area of non-verbal programming languages has not been unexplored. There are ASCII-based languages such as <a href="https://en.wikipedia.org/wiki/Befunge">Befunge</a> and <a href="https://github.com/aaronjanse/asciidots">asciidots</a>, as well as image-based ones such as <a href="https://esolangs.org/wiki/Piet">Piet</a>, just to name a few. Both inspired and challenged by these work, I set the following goals for my new language:</p><ul><li>To take advantage of the fact that the program is drawn, to include features that is otherwise unfeasible with text-based programming languages.</li><li>To have neither too few instructions, nor too many: For the former it becomes laborious to construct even the simplest programs, and for the latter it becomes non-minimalistic and difficult to do computer vision on.</li><li>To be able to draw programs that look visually appealing themselves, such that someone would want to put a frame around them and hang them on the wall.</li></ul>
        </div>

    
        <div>
        <p>
                    Though I'm normally an imperative and low-level person, I was compelled to use lambda calculus, or functional programming in its most primitive form, as the basis for the language, for it seemed to me that the ideas bear a lot of resemblance to a typical drawing. It has no concept of "execution" but only that of "evaluation", much like how the viewers' eyes can linger on any part of a drawing in no particular order, and by following many dots they see the line, and lines the form, and forms the composition.
                </p>
    </div>

    
        <div>
        <p>Conceptual blabber aside, I started with a grid-based system. While the user can draw continuous lines across many grids, each grid end up as one of a finite set of symbols. It seems like a good compromise between the ease of drawing for the human and that of parsing for the computer.
                    
                </p>
    </div>

    
        



    
        <div>
        <p>
                    Lambda calculus is such a concise language that it only has two instructions: that of function application and that of function definition. I quickly came up with working symbols for each: a "cup" shape for the former (for the silly intuition that applying the function is like putting the argument into the "cup"), and the eponymous greek letter for the latter (which is a bit unimaginative and arguably overused, but at least it's clear). Just like vanilla lambda calculus, functions always take one argument and produce one output; to get more, you can chain multiple functions together, known as "currying".
                </p>
    </div>

    
        



    
        <div>
        <p>Then came the wires to connect the symbols and through which data can flow. Technically the language is Turing complete at this point, but will be excruciatingly laborious to use, violating my design rule #2. Therefore, I added a lot more other symbols you would expect from your favorite programming languages, things like numbers and math operators. Consider them mere syntactic sugars: you can still stick to Church numerals (they're pretty cool) and other "pure" lambda calculus constructs if you'd like.
                    
                </p>
    </div>

    
        



    
        <div>
        <p>
                    I used to enjoy how easy it is in Scratch, to draw some sprites in the same editor and immediately use it for the program. Since my new language is entirely drawn, it should be even more natural to incorporate such kind of feature. Additionally, I wanted to be able to sketch the shape of a mathematical function and use it (e.g. for animating stuff), without the extra step of figuring out the equation (manually or otherwise). So I introduced the idea of "frames": fence any area of the canvas with wires, and put an indicating symbol at upper left; anything doodled in the area can be used as data. In a similar spirit, I wanted to be able to draw sliders (and perhaps other GUI elements in the future), which can be dragged at run-time to parametrically control the program.
                </p>
    </div>

    
        



    
        <div>
        <p>Initially I sketched my ideas on a dotted notebook, trying to construct some example programs in my (then imaginary) language. (I found this approach useful in my previous programming-language-design experiences). Then I figured it was time to code a parser for it, to see if it really "works". Since the computer vision part to scan the program from the paper was not ready yet, I decided to first make a simple editor software to let the user digitally draw programs.
                    
                </p>
    </div>

    
        <div>
        <p>Each symbol is made to 5x5 pixels, conveniently stamp-able on the gridded canvas, while the user can also draw "free hand" with a "pencil"-like tool. What initially started as a temporary measure grew to an almost full-fledged editor with many features.&nbsp;
                    
                </p>
    </div>

    
        



    
        <div>
        <p>One interesting problem that I did not anticipate while imagining the language was that it turned out so purely functional and absolutely state-less, that it becomes impossible to implement a "print" statement, for to print is to change state, to expect some things to be printed in some particular order is to assume that some expressions will be evaluated in some order. The solution was a functional re-thinking of the definition of "printing" as passing a piece of empty canvas to some function and receiving a new canvas with altered pixels resembling text (or whatever scribble desired) on it. (Replace "canvas" with "string" and "pixels" with "characters", if you wish, but you probably figured having read thus far that this is an anti-strings and pro-pixels language).
                    
                </p>
    </div>

    
        <div>
        <p>The baseline parser works by transpiling (translating) an entire λ-2d program to a javascript equivalence. The resultant javascript one-liner is a single horrendously inscrutable mega-expression that contains enough parentheses to make a lisper shudder. But the coolest part about it is that it works (albeit inefficiently)!
                    
                </p>
    </div>

    
        



    
        <div>
        <p>I work to improve the language by constructing more example programs, and as I do I discover design flaws to be corrected. It was a lot of fun, for I am myself unfamiliar with my own creation: I only know the base rules, and that in theory it should work, but as to how to actually program in it I am as clueless as any other new learner of programming languages. Gradually I start to grasp its temperaments, of what it is like to program in this very strange, drawing-based language. In the beginning I was using the syntax clumsily, trying to bend things to my will; later I become more artful and expressive in it. Coding in λ-2D is somewhat like playing Minecraft or Factorio, but it's even better because I can call it research.
                    
                </p>
    </div>

    
        <div>
        <p>
                    Below you can the comparison of two fractal tree programs, one in notebook doodles, and the other in refined digital form. (Please forgive the copious bugs and logical inconsistencies in the former, for I know people who can program on a piece of paper and get it right in one shot, unfortunately I'm not one of them and learned programming the "rogue" way: by running stuff over and over again and see if I can get any errors to pop up).
                </p>
    </div>

    
        



    
        



    
        <div>
        <p>I thought it must be cool and wondered what it would look like, to visualize the execution of a program written in this unusual language. The current parser spits out javascript and your browser's super-optimized javascript engine takes it over from there, so it is difficult to visualize the actual execution. However I can easily visualize the parsing, which should look similar to the path taken by a tree-walk interpreter executing the program.
                    
                </p>
    </div>

    
        



    
        <div>
        <p>And the animation did turn out quite fun to watch. But what if it makes sounds when going over different symbols? We can then "listen" to a program as it is being run, as if it were a song! I'm no musician myself but theoretically it should be possible to compose something musical with this kind of system.
                    
                </p>
    </div>

    
        <div>
        <p>It ended up sounding like a whacky computer game from 8-bit era. You can check it out in the <a href="https://l-2d.glitch.me/">online demo</a>&nbsp;(Menu &gt; Program &gt; Animated Run).</p>
    </div>

    
        <div>
        <p>λ-2D started as a part of a larger research to design a system where the user draw programs with pen and paper, and receive interactive feedback through augmented reality. However,&nbsp;it grew increasingly interesting that it became a full project on its own. &nbsp;Though I'm quite proud of this neat little language, it is yet to fully meet some of the initial goals. For instance, the programs look too much like circuit diagrams and not enough like, well, drawings. Also, I'm not too optimistic about how easy it is for a human (excluding myself) to learn it, and for a computer vision system to scan it without error.</p>
    </div>

    
        <div>
        <p>Therefore, after I refine λ-2D, I plan to design more potential programming languages that can be incorporated into the drawing-as-computation system I am developing, using knowledge and experience I've since gained.
                    
                </p>
    </div>

    
        <div>
        <p>
                    You can try out a beta version of λ-2D <a href="https://l-2d.glitch.me/">online here</a>. The source code for the parser and editor will shortly be available on GitHub.
                </p>
    </div>

    
        



    
</div>

                

                

                
                    


    


 




                

                
                
                
            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Asterogue, my sci-fi roguelike, is now playable on the web (265 pts)]]></title>
            <link>https://asterogue.com</link>
            <guid>42085036</guid>
            <pubDate>Fri, 08 Nov 2024 07:43:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asterogue.com">https://asterogue.com</a>, See on <a href="https://news.ycombinator.com/item?id=42085036">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="story-text"><div><p>One thousand years of peace and prosperity in the galaxy have come to an end.</p><p>You, a humble lightswords-person, lost everything you loved in The Great War.</p></div><div><p>Late one night in the midst of the Dark Times you drowned your sorrows at a dive bar.</p><p>An old man mumbled a mythical tale. "Ancient aliens have returned".</p></div><div><p>He told of their dark arts and strange ambitions.</p><p>To suck away all that is good in the galaxy and store it in their Cursed Orb.</p></div><p><span></span> The Orb of the ancients is the source of all this misery, and whoever finds it shall bring peace.</p><div><p>He sketched a space-map on the back of a napkin, and was suddenly gone.</p><p>From your haze you stared at the map and knew it to be your destiny.</p></div><div><p>Days later, wheeling through the outer limits of the spiral arm, you crash land upon an asteroid.</p><p>Is this the one on the map? Is that an entrance you spy?</p></div><div><p>Faced with sure death on the surface or the chilling unknown within, the choice is clear.</p><p>You descend into the heart of the asteroid...</p></div><p id="story-next">Next</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perceptually lossless (talking head) video compression at 22kbit/s (194 pts)]]></title>
            <link>https://mlumiste.com/technical/liveportrait-compression/</link>
            <guid>42084977</guid>
            <pubDate>Fri, 08 Nov 2024 07:30:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mlumiste.com/technical/liveportrait-compression/">https://mlumiste.com/technical/liveportrait-compression/</a>, See on <a href="https://news.ycombinator.com/item?id=42084977">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
        <header>
          
          

  <p>
    
      
      <span>
        
        
        <time datetime="2024-11-07T00:00:00+02:00">November 7, 2024</time>
      </span>
    

    <span></span>

    
      
      

      <span>
        
        
          8 minute read
        
      </span>
    
  </p>


        </header>
      

      <section itemprop="text">
        
        <p>Update: <a href="https://news.ycombinator.com/item?id=42084977">Discussion on Hacker News</a></p>

<p>I’ve been having quite a bit of fun with the fairly recent <a href="https://github.com/KwaiVGI/LivePortrait">LivePortrait</a> model, generating deepfakes of my friends for some cheap laughs.</p>

<p><img src="https://mlumiste.com/assets/images/compression/elon.gif" alt=""></p>

<p><em>The inevitable Elon Musk deepfake, picture by <a href="https://www.debbierowe.com/corporate">Debbie Rowe</a></em></p>

<p>The emerging field of 2D avatar/portrait animation (being able to animate any still image, avoiding the need to render cumbersome 3D models that would struggle with small facial details) is a harbinger of things to come. In the best case, it will be ubiquitous on social media (the authors have already added an extension to animate cute animal faces) and in the worst, trust on the internet will be heavily undermined. But one overlooked use case of the technology is (talking head) video compression. After all, <a href="http://prize.hutter1.net/hfaq.htm#compai">prediction is compression</a>, so a sufficiently powerful face generator should be able to compress frame information into an extremely sparse set of cues to reconstruct the same frame from.</p>

<p>This was briefly explored in Nvidia’s seminal <a href="https://nvlabs.github.io/face-vid2vid/">facevid2vid</a> paper that compared their models’ compression ratio to the classical H.264 codec. The main idea is quite simple: given a source image that is shared between the sending and receiving side, the only information that needs to be transmitted is the change in expression, pose and facial keypoints. The receiving side then simply animates the source frame into the new one, using these motion parameters.</p>

<p><img src="https://mlumiste.com/assets/images/compression/facevid2vid.png" alt=""></p>

<p>The main upside is that this method achieves pretty reasonable perceptual quality at an extremely low bitrate, while at a comparable level a traditional video codec will show heavy artifacts. There are, of course, downsides as well:</p>

<ul>
  <li>there is no longer a natural lever to trade-off between quality and bitrate, like the <a href="https://trac.ffmpeg.org/wiki/Encode/H.264">CRF</a> for H.264.</li>
  <li>as a model with large generative capacity, there’s essentially no limit to how bad the worst case reconstruction can be. It could in theory render a completely different person, or distort your face into a monstrous gremlin.</li>
  <li>the impressive bitrate does not come for free, as e.g. LivePortrait needs to run on an RTX 4090 for real-time processing. In the space of possible learned compression models, compared to something like <a href="https://github.com/microsoft/DCVC">DCVC</a>, it is a further improvement in compression rate, at the cost of having a 10x+ slower model.</li>
</ul>

<p>Anyway, LivePortrait is a beefed up version of facevid2vid, so let’s look at how good it is for video compression. I extracted the first frame of the above driving video as a key frame, simulating a scenario where instead of a high quality enrolled image, key frames are extracted on-demand in the background. This means that in addition to being same identity animation, this is also same video animation - by far the simplest scenario to work on for the model, as you have very good alignment between the source and driving frames. It’s also the closest to a drop-in replacement of the current video call experience. Here are the results of a quick try:</p>

<p><img src="https://mlumiste.com/assets/images/compression/self.gif" alt=""></p>

<p><em>Self-animation, i.e. driving a keyframe of the video with the motion</em></p>

<p>It’s possible to uncover discrepancies in a side by side analysis:</p>

<ul>
  <li>the head tends to be a bit shaky in all my experiments, probably because LivePortrait processes frames in isolation, without any motion prior. Or maybe my driving video is low quality. 🤷</li>
  <li>since the eye gaze is off camera in the key frame (“neutral mode”), the model seems to map it incorrectly in every frame after that.</li>
  <li>teeth are generally hallucinated, but this is only noticeable in smiling videos.</li>
</ul>

<p>As expected, the discrepancies are much more obvious if we provide a driving video with shoulder movement and difficult head angles. Also, the further the inference setup is from the training one (which is same video animation), the worse the results.</p>

<p>Nonetheless, it’s clear that there is a set of frames, arguably a large proportion of video-conferencing, where the model manages to produce subjectivelly distinguishable reconstructions. Sure, in a side by side analysis we might be able to tell which is the original and which is the reconstruction. However, if you are only looking at the generated output, it works very well.</p>

<p>So how small is the bitrate of this reconstruction? The model equation for transforming the face keypoints is:</p>

\[x_d = s_d \times (x_{c, s} R_d + \delta_d) + t_d\]

<p>where $x_{c, s} \in \mathbb{R}^{K \times 3}$ are the “canonical” implicit 3D facial keypoints of the source image, $R_d \in \mathbb{R}^{3 \times 3}$ is a 3D <a href="https://en.wikipedia.org/wiki/Rotation_matrix#In_three_dimensions">rotation matrix</a> (relative to the canonical keypoints), $\delta_d \in \mathbb{R}^{K \times 3}$ denotes the expression deformations, $t_d \in \mathbb{R}^3$ is a translation vector and $s_d$ is just a scaling coefficient and $K$, the number of keypoints, is a hyperparameter. The intuition of this equation is provided in the Nvidia paper:</p>

<p><img src="https://mlumiste.com/assets/images/compression/rotation.png" alt=""></p>

<p>Of course, $x_d$ is not yet the final reconstruction of the image, only the transformed keypoints. There are some flow field estimations and warp operations remaining to actually turn the source image into the driving one. Nonetheless, the sender only needs to transmit $s_d$, $R_d$, $\delta_d$ and $t_d$ for a lifelike reconstruction to happen on the receiver’s side.</p>

<p>And since we know their shapes, we can also infer the bitrate: $3 \times 3 +  K \times 3 + 3 = 75 $ numbers at $ K = 21 $, the default LivePortrait setting. At half precision floats, that’s $ 16 \times 75 \times 30 $ bits per second for a 30FPS video, or 36kbit/s. This could be compressed further - note that each frame is processed in isolation. This could be alleviated with entropy coding and having a temporal prior. In facevid2vid, simple entropy coding reduced the baseline model’s bitrate by nearly 40%, while using an adaptive number of keypoints reduced it by 60%. Using the first figure, we should be able to bring down LivePortrait’s bitrate to about 22kbit/s. For reference, the low bitrate challenge in <a href="https://www.compression.cc/leaderboard/video_0_05/test/">CLIC 2024</a> featured video compression at 50kbit/s, but as expected, the models showed significantly worse subjective quality scores than at 500kbit/s.</p>

<p>LivePortrait has roughly the same bitrate as the facevid2vid had (model transmits similar information), but achieving better results. Looking at the evaluation results for the latter method, we see that as expected, their model only provides a single point on the bitrate-quality curve. So without any evaluation results at hand, I would expect LivePortrait to move strictly downwards, matching a lower H.264 CRF for equal preference. Extrapolating ahead, a future model might achieve the same perceptual quality as a visually lossless CRF (FFmpeg suggests 17 or 18). Then, it is up to the user whether they want to squeeze bitrate to near zero at the cost of compute.</p>

<p><img src="https://mlumiste.com/assets/images/compression/bdrate.png" alt=""></p>

<h2 id="how-does-it-work-what-is-the-magic">How does it work (what is the magic?)</h2>

<p>The main problem of frame animation is that we are projecting a 3D object to a 2D image. So our model needs to understand the rotation and deformation of the underlying object. The good thing about faces is that they are rigid, i.e. tends to have limited degrees of freedom in movement and nearby pixels move together in predictable ways. Nonetheless, this has proven to be a hard problem.</p>

<p>The main innovation of facevid2vid, that also powers LivePortrait, was realising that this can be formed as a 3D rotation problem. By rotating a set of abstract 3D tensors enough times, the model learns to actually map these to keypoints of the face, as if someone would have painstakingly labelled them for each frame. Up until then, models like <a href="https://aliaksandrsiarohin.github.io/first-order-model-website/">First Order Motion Model</a> had also used the implicit keypoints approach, but only with 2D keypoints.</p>

<p>The second thing that seems to work is quite humdrum: compared to facevid2vid, LivePortrait has seriously scaled up the training dataset to 69 million high quality frames, and added regional GAN losses that focus only on the local regions like the face or the lips. So rather than any architectural breakthrough, it seems to have been a lot of iterative improvements on dataset and losses.</p>

<p>While being able to learn facial keypoints self-supervisedly is a testament to why deep learning is cool, it also allows direct controllability of the avatar. Since the rotation matrix has a direct geometric interpretation, you can input parameters for a required pose. LivePortrait adds on top of this by training small neural networks to control lip and eye movement. This is a big step ahead in terms of avatar controllability which generally has not been a strong suit of many generative approaches (I’m looking at you, diffusion).</p>

<p>LivePortrait methodology is quite different from SotA learned video compression models like <a href="https://github.com/microsoft/DCVC">DCVC</a>, which need to encode spatial information with a great degree of fidelity targeting pixel-aligned distortion losses such as MSE. A generative model unencumbered by pixel-alignment and optimised for various GAN based perceptive losses, only tries to generate something plausible. On a spectrum of model architectures, it achieves higher compression efficiency at the cost of model complexity. Indeed, the full LivePortrait model has 130m parameters compared to DCVC’s 20 million. While that’s tiny compared to LLMs, it currently requires an Nvidia RTX 4090 to run it in real time (in addition to parameters, a large culprit is using expensive warping operations). That means deploying to edge runtimes such as Apple Neural Engine is still quite a ways ahead.</p>

<p>Nonetheless, models and hardware become faster reliably quickly. Also, the same identity animation problem is significantly easier than animating Elon Musk or your cat, so probably a model optimised for teleconferencing could be remarkably smaller. That’s why it might not be that much of a moonshot. Publicly, Zoom seems to have played with the idea of <a href="https://www.business-standard.com/technology/tech-news/zoom-expands-ai-features-introduces-custom-avatars-and-upgraded-companion-124101000315_1.html">avatar technology</a>. I’ll let the precise use cases be determined by product people, but off the top of my head:</p>

<ul>
  <li>having a more formal version of yourself avatar for days when youre working in your underwear</li>
  <li>animating a 4k studio quality avatar from a driving video from my terrible webcam</li>
  <li>using pose and gaze connection to seat the avatars in some kind of more immersive virtual meeting room</li>
  <li>letting your avatar attend meetings / send messages as a digital twin. If all the driving keypoints are directly manipulatable, you could programmatically control a photorealistic video.</li>
</ul>

<p>Of course it’s possible that none of these will be useful or socially normalised, yet its fun to theorise.</p>

        
      </section>

      

      
  

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Methodology is bullshit: principles for product velocity (216 pts)]]></title>
            <link>https://ssoready.com/blog/from-the-founders/methodology-is-bullshit/</link>
            <guid>42084753</guid>
            <pubDate>Fri, 08 Nov 2024 06:48:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ssoready.com/blog/from-the-founders/methodology-is-bullshit/">https://ssoready.com/blog/from-the-founders/methodology-is-bullshit/</a>, See on <a href="https://news.ycombinator.com/item?id=42084753">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
            Building the right thing shouldn't take very long -- doing away with nonsense makes product development really fast"
        </p><div>
            <p>We’ve begun preparations for a major product launch. For us, it’s a big deal; it’s exciting. It’s a really big commitment. It’s also the kind of thing that threatens to become a quagmire, a set of features perpetually in-development.</p>
<p>We’ve found that making the right thing – something people want – is <em>intrinsically</em> quite fast. By contrast, it’s all the other stuff that slows down product teams. It’s process, it’s distance between deciders and doers, it’s bloated specifications.</p>
<p>With this in mind, I’ve realized that our company has developed – largely by accident – some general principles for achieving product velocity. Some of these may be wrong, and we’ll likely change our perspectives over time, but I thought I’d share them here nonetheless.</p>
<h2 id="principles-for-product-velocity">Principles for product velocity</h2>
<h2 id="we-should-probably-do-less">We should probably do less</h2>
<p>All else being equal, there’s usually a trade-off between speed and quality. For the most part, doing something faster usually requires a bit of compromise. There’s a corner getting cut somewhere.</p>
<p>But all else need not be equal. We can often eliminate requirements … and just do less stuff. With sufficiently limited scope, it’s usually feasible to build something quickly and to a high standard of quality.</p>
<p>Most companies assign requirements, assert a deadline, and treat quality as an output. We tend to do the opposite. Given a standard of quality, what can we ship in 60 days?</p>
<p>Recent escapades notwithstanding, Elon Musk has a similar <a href="https://medium.com/@cclark.osi/use-teslas-five-step-production-algorithm-to-improve-your-own-processes-d3c5bd427f37">thought process here</a>. Before anything else, an engineer should <em>make the requirements less dumb.</em></p>
<h2 id="idiot-mode-usually-works">Idiot mode usually works</h2>
<p>We’re big fans of the midwit meme. Put simply, the midwit meme usually shows an idiot and a genius agreeing on a simple solution, while a person of average intelligence flails around complaining about complicated stuff.</p>
<figure>
<p><img src="https://ssoready.com/blog/from-the-founders/methodology-is-bullshit/midwit.png">
</p>
</figure>
<p>Early in our company’s history, we challenged ourselves to operate in idiot mode as often as possible. When we’ve made mistakes, we’ve usually been overthinking things. We often arrive at a workable solution by asking ourselves, <em>how would I do this if I were an idiot</em>.</p>
<h2 id="some-problems-arent-important">Some problems aren’t important</h2>
<p>A small number of problems matter a lot. For example, we have to take security extremely seriously. It’s really not okay for us to cut corners there.</p>
<p>But we can choose to ignore certain other things. For example, we know that our front-end doesn’t look very good on mobile devices. For the foreseeable future, we’ll just need our customers not to use mobile devices. We’d obviously like for our software to look great everywhere, but we’re choosing not to spend time on our mobile layout. No one really seems to mind.</p>
<p>I’ll be damned if we ship dark mode any time soon.</p>
<h2 id="just-make-the-thing">Just make the thing</h2>
<p>We don’t have a process for product development. We don’t do Figma mocks. We don’t write PRDs. We don’t really have a design system. We don’t do <em>agile</em>. We don’t have OKRs. We don’t even have a firm product roadmap. We don’t have any A/B testing or growth hacks.</p>
<p>Our customers are engineers, so we generally expect that our engineers can handle product, design, and all the rest. We don’t need to have a whole committee weighing in.</p>
<figure>
<p><img src="https://ssoready.com/blog/from-the-founders/methodology-is-bullshit/peopleskills.jpg">
</p>
</figure>
<p>We just make things and see whether people like them.</p>
<h2 id="rewrites-need-to-happen-sometimes">Rewrites need to happen sometimes</h2>
<p>Companies often think they’ll move faster if they defer technical debt as long as possible. That’s sometimes fine, but we’re comfortable doing major rewrites when appropriate.</p>
<p>Sometimes the fastest path to building the right thing looks like this:</p>
<ol>
<li>Build the wrong thing</li>
<li>Realize it’s the wrong thing</li>
<li>Replace the wrong thing with the right thing</li>
</ol>
<p>If it seems reasonably useful to eliminate technical debt, we’ll do it.</p>
<h2 id="pay-vendors-to-do-it">Pay vendors to do it</h2>
<p>When possible, we buy solutions from vendors instead of building things in-house. For example, we use a vendor called Fern to generate our SDKs. They do a pretty good job.</p>
<p>Of course, using a vendor has significant upfront costs – these things are usually pretty expensive. It also restricts our freedom a bit.</p>
<p>But using a vendor is typically the right move. We have very limited engineering resources, and our engineering resources are really expensive. A week of a single engineer’s time costs about $5,000 in cash. It’s worth <em>vastly</em> more than that when we factor in opportunity costs (i.e. how much better off we’d be if we spent that engineer’s time on something else).</p>
<p>Relatively few things are actually worth building.</p>
<h2 id="dont-hire-people">Don’t hire people</h2>
<p>We don’t expect that adding headcount would increase our team’s output. Hiring is slow and hard. Onboarding and people management consume time. Even for a ramped hire, collaboration’s expensive.</p>
<p>It’s especially hard to bring on <em>strong</em> people, the kind of people that can contribute without a lot of support.</p>
<p>So although we have the resources to build a large engineering team, we do everything possible to stay small. It just makes life a lot easier.</p>


<h2 id="closing-thoughts">Closing thoughts</h2>
<p>To an extent that wasn’t obvious to us before, we’ve realized that product development shouldn’t take very long. If you know what your customers need, have a strong team, and avoid distracting nonsense, velocity is pretty close to inevitable.</p>

        </div></div>]]></description>
        </item>
    </channel>
</rss>