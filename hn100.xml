<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 18 May 2025 01:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Mystical (149 pts)]]></title>
            <link>https://suberic.net/~dmm/projects/mystical/README.html</link>
            <guid>44016037</guid>
            <pubDate>Sat, 17 May 2025 18:21:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://suberic.net/~dmm/projects/mystical/README.html">https://suberic.net/~dmm/projects/mystical/README.html</a>, See on <a href="https://news.ycombinator.com/item?id=44016037">Hacker News</a></p>
<div id="readability-page-1" class="page">


<p><img src="https://suberic.net/~dmm/projects/mystical/images/quicksort_example.png" alt="quicksort example"></p>
<p>I wanted to make a programming language that resembled magical circles. This is more like a way to write PostScript that looks like a magical circle, but I will refer to it as Mystical in this document.</p>
<h2>Rings</h2>
<p>The structure of Mystical is based on rings. These are circular bands of text and sigils, with an inner and outer border. The content of the main ring of a program starts at the rightmost (3:00) point and flow continues widdershins (counter-clockwise) both to respect postscript's angles and to reflect the assumption that these rings should be written from the outside.  Subsidiary rings start from their attachment point to their caller.</p>
<p>There are three types of rings in Mystical:</p>
<ul>
<li>executable arrays, written in <code>{</code> <code>}</code> in PostScript, are represented with simple circular borders on the inside and outside of the ring, with a star of some sort inside. The start/end point is marked by a symbol based on the "work complete" symbol from alchemy.</li>
<li>non-executable arrays, written in <code>[</code> <code>]</code> in PostScript, are the same but without the star. The start/end point is marked with a simple triangle.</li>
<li>dictionaries, written in <code>&lt;&lt;</code> <code>&gt;&gt;</code> in PostScript, are polygons with a double outer border and a single inner border. The start/end point is marked the same as the array.</li>
</ul>
<table>
<thead>
<tr>
<th>xarray</th>
<th>array</th>
<th>dict</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/xarray_example.png" alt="xarray example"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/array_example.png" alt="array example"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/dict_example.png" alt="dict example"></td>
</tr>
<tr>
<td><code>{ 0 0 currentlinewidth 1.5 mul 0 360 arc fill }</code></td>
<td><code>[ 0 1 2 1.5 40 360 (Hooray World) ]</code></td>
<td><code>&lt;&lt; /longname (Mystical) /w 45 /h 8 /x 23 &gt;&gt;</code></td>
</tr>
</tbody>
</table>
<p>(Note that the entries in the dict image are in a different order than the PostScript text since dict insertion order is not preserved in PostScript.)</p>
<p>When one of these structures appear inside a different structure, a small circle or dot at the inclusion point is connected to a line which leads to the subsidiary ring's start/end sigil.</p>
<p><img src="https://suberic.net/~dmm/projects/mystical/images/link_example.png" alt="link example"></p>
<pre><code>[
    0 1 2 1.5 40 360 &lt;&lt;
        /longname (Mystical) /w 45 /h 8 /x 23
    &gt;&gt;
]
</code></pre>
<p>It is theoretically possible to use <code>[ ]</code> and <code>&lt;&lt; &gt;&gt;</code> in PostScript in ways that Mystical can't handle:</p>
<pre><code>[ 1 2 3 split { ] /first exch def [ } if 4 5 6 ] /final exch def
</code></pre>
<p>so don't do that.</p>
<p>Other commands like <code>gsave/grestore</code> and <code>begin/end</code> are more likely to be used in non-balanced or loop-crossing ways so those are treated as normal sigils below.</p>
<h2>Text and Sigils</h2>
<p>The rings' rims contain text or sigils.  Sigils are symbols that stand in for operators, variables, or other keywords. Any name, written in PostScript as <code>/name</code>, is instead written with a triangle surrounding or superimposing the text of the name or its sigil.  Any strings, written in () in Postscript, are cartouche-like shapes containing the string text.</p>
<table>
<thead>
<tr>
<th>array</th>
<th>/array</th>
<th>(array)</th>
<th>foo</th>
<th>/foo</th>
<th>/foobar</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/operator_array.png" alt="array sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/name_array.png" alt="/array sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/string_array.png" alt="array string"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/operator_foo.png" alt="foo"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/name_foo.png" alt="foo name"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/name_foobar.png" alt="foobar name"></td>
</tr>
</tbody>
</table>
<h3>Standard Sigils</h3>
<p>Many built-in operators have been given their own sigils.  These are used in place of the text of the operator if it appears as a name or operator (but not if it appears as a string).  I have generally made these sigils based on the initial of the command and an illustration of the concept, though in some cases I have taken a more fully illustrative route or created some standard visual language.  Some examples are below - see <a href="https://suberic.net/~dmm/projects/mystical/docs/operators.html">Standard Sigils</a> for a full list.</p>
<h4>Sample sigils</h4>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_dup.png" alt="dup sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_copy.png" alt="copy sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_add.png" alt="add sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_mul.png" alt="mul sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_neg.png" alt="neg sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_for.png" alt="for sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_forall.png" alt="forall sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_repeat.png" alt="repeat sigil"></td>
</tr>
<tr>
<td>dup</td>
<td>copy</td>
<td>add</td>
<td>mul</td>
<td>neg</td>
<td>for</td>
<td>forall</td>
<td>repeat</td>
</tr>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_if.png" alt="if sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_ifelse.png" alt="ifelse sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_eq.png" alt="eq sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_ne.png" alt="ne sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_ge.png" alt="ge sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_gt.png" alt="gt sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_le.png" alt="le sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_lt.png" alt="lt sigil"></td>
</tr>
<tr>
<td>if</td>
<td>ifelse</td>
<td>eq</td>
<td>ne</td>
<td>ge</td>
<td>gt</td>
<td>le</td>
<td>lt</td>
</tr>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_moveto.png" alt="moveto sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_lineto.png" alt="lineto sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_arc.png" alt="arc sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_arcn.png" alt="arcn sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_curveto.png" alt="curveto sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_closepath.png" alt="closepath sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_stroke.png" alt="stroke sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_fill.png" alt="fill sigil"></td>
</tr>
<tr>
<td>moveto</td>
<td>lineto</td>
<td>arc</td>
<td>arcn</td>
<td>curveto</td>
<td>closepath</td>
<td>stroke</td>
<td>fill</td>
</tr>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_gsave.png" alt="gsave sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_grestore.png" alt="grestore sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_translate.png" alt="translate sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_scale.png" alt="scale sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_rotate.png" alt="rotate sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_setmatrix.png" alt="setmatrix sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_currentmatrix.png" alt="currentmatrix sigil"></td>
<td></td>
</tr>
<tr>
<td>gsave</td>
<td>grestore</td>
<td>translate</td>
<td>scale</td>
<td>rotate</td>
<td>setmatrix</td>
<td>currentmatrix</td>
<td></td>
</tr>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_setrgbcolor.png" alt="setrgbcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_currentrgbcolor.png" alt="currentrgbcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_setcmykcolor.png" alt="setcmykcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_currentcmykcolor.png" alt="currentcmykcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_sethsbcolor.png" alt="sethsbcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_currenthsbcolor.png" alt="currenthsbcolor sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_setgray.png" alt="setgray sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_currentgray.png" alt="currentgray sigil"></td>
</tr>
<tr>
<td>setrgbcolor</td>
<td>currentrgbcolor</td>
<td>setcmykcolor</td>
<td>currentcmykcolor</td>
<td>sethsbcolor</td>
<td>currenthsbcolor</td>
<td>setgray</td>
<td>currentgray</td>
</tr>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_dict.png" alt="dict sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_begin.png" alt="begin sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_end.png" alt="end sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_def.png" alt="def sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_get.png" alt="get sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_put.png" alt="put sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_length.png" alt="length sigil"></td>
<td></td>
</tr>
<tr>
<td>dict</td>
<td>begin</td>
<td>end</td>
<td>def</td>
<td>get</td>
<td>put</td>
<td>length</td>
<td></td>
</tr>
</tbody>
</table>
<h3>User Sigils</h3>
<p>Sigils for new functions or names can be added to <code>sigil_bank</code> at runtime.  They should fit into the 1-unit square centered on the origin, so no coordinate should be more than 0.5 (of course, you can transform your coordinate system for convenience).  If you use <code>nstroke</code> instead of <code>stroke</code> you will get the same calligraphic effect as the standard sigils.</p>
<p>Sigils for user variables can be designed with any sigil system.  My examples mostly use letter collision, inspired by Spare's Chaos Magick system, but anything that turns a word into a symbol will work - kameas, wheels, Square Word Calligraphy, Circular Gallifreyan, sitelen sitelen, illustration, puns, etc.  New names based on official operators can incorporate the standard sigils for those operators.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_arg.png" alt="arg sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_dot.png" alt="dot sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_softscale.png" alt="softscale sigil"></td>
<td><img src="https://suberic.net/~dmm/projects/mystical/images/sigil_nstroke.png" alt="nstroke sigil"></td>
</tr>
<tr>
<td>arg</td>
<td>dot</td>
<td>softscale</td>
<td>nstroke</td>
</tr>
</tbody>
</table>
<h2>Ligature for <code>/name { ring } def</code></h2>
<p>There is a sigil for <code>def</code> but a very common pattern is to push a name, push a function, and def the name to the function. To save space and to emphasize this definition, there is special syntax for this case consisting of the usual name triangle with the end of the link line directly below it, and the def sigil is omitted entirely. This is extended to the other two ring types for simplicity. Any other use of <code>def</code> will just use the def sigil as normal.</p>
<table>
<thead>
<tr>
<th><img src="https://suberic.net/~dmm/projects/mystical/images/ligature_example.png" alt="ligature example"></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>{ ...  /even { 2 mod 0 eq } def ...  }</code></td>
</tr>
</tbody>
</table>
<p>This only applies inside of executable arrays. I considered a similar ligature for /name { ring } in dictionaries but there's too much chance of getting it wrong.</p>
<h2>Sample Algorithms</h2>
<p>Quicksort is the illustration at the top of this page.</p>
<p>Euclid's GCD algorithm (using my <code>/arg {exch def} def</code> function from dmmlib):</p>
<p><img src="https://suberic.net/~dmm/projects/mystical/images/gcd_example.png" alt="gcd example"></p>
<h2>Functions to generate Mystical images</h2>
<p>All of these are defined in "mystical.ps".</p>
<p><code>mystical</code>: takes an array, xarray, or dict and renders it in mystical, descending into substructures as necessary.  The entire image will be scaled to fit into a unit circle.</p>
<p><code>mystical_evoke</code>: The same as <code>mystical</code> but it takes a name that is looked up in the current dictionary.</p>
<p><code>mystical_evoke_label</code>:  Like <code>mystical_evoke</code> but adds a name-def ligature with the name at the top and orients the image so that the name sigil is right-side-up.</p>
<p>All of these have versions with <code>_unscaled</code> appended to them that skip the scaling step.  The rings will be 1 unit thick so the image will be quite large.</p>
<h3>layout issues</h3>
<p>Currently the code figures out the layout of the subcircles so that nothing collides, but it's overly safe so most programs will be very spread out.  For the examples on this page I ran the parsing/layout functions (<code>mystical_get_spell</code> and <code>mystical_make_evocation_ligature</code>) and then adjusted the results before calling the draw functions <code>draw_sigil</code> and <code>draw_link</code>.  I'm intending to improve the default layout somewhat.</p>
<h2>Is this a programming language?</h2>
<p>At the moment it's a way to draw a PostScript program - there's no interpreter that will ingest a Mystical image and perform the appropriate computation.  It could be run and interpreted by a human, or (more likely) a human could read it and turn it into a PostScript program and run that.  I'll leave further philosophical arguments to other people for now.</p>
<h2>Could this work for other languages?</h2>
<p>This approach seems applicable to other language with just operators, such as Forth. Languages with more complicated statements might be more difficult, and I don't know if a new ring for every brace or indent will be overly busy.</p>

<hr><p><a href="https://github.com/denismm/mystical_ps">Download on github</a></p><hr><p><a href="https://codeberg.org/yomikoma/mystical_ps">Download on codeberg</a></p><hr>
<address>This page generated 2025-05-16 by <a href="https://suberic.net/~dmm/">Denis</a>.</address>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dead Stars Don't Radiate (166 pts)]]></title>
            <link>https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/</link>
            <guid>44015872</guid>
            <pubDate>Sat, 17 May 2025 17:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/">https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/</a>, See on <a href="https://news.ycombinator.com/item?id=44015872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
				<p><a href="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg"><img data-attachment-id="39886" data-permalink="https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/end_of_universe_coming_sooner_2/" data-orig-file="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg" data-orig-size="984,815" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="end_of_universe_coming_sooner_2" data-image-description="" data-image-caption="" data-medium-file="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=300" data-large-file="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=450" src="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=450" alt="" width="450" height="373" srcset="https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=450 450w, https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=900 900w, https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=150 150w, https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=300 300w, https://johncarlosbaez.wordpress.com/wp-content/uploads/2025/05/end_of_universe_coming_sooner_2.jpg?w=768 768w" sizes="(max-width: 450px) 100vw, 450px"></a></p>
<p>Three guys claim that any heavy chunk of matter emits Hawking radiation, even if it’s not a black hole:</p>
<p>• Michael F. Wondrak, Walter D. van Suijlekom and Heino Falcke, <a href="https://arxiv.org/abs/2305.18521">Gravitational pair production and black hole evaporation</a>, <i>Phys. Rev. Lett.</i> <b>130</b> (2023), 221502.</p>
<p>Now they’re getting more publicity by claiming this means that the universe will fizzle out sooner than we expected.  <a href="https://iopscience.iop.org/article/10.1088/1475-7516/2025/05/023">They’re claiming</a>, for example, that a dead, cold star will emit Hawking radiation, and thus slowly lose mass and eventually disappear!</p>
<p>They admit that this would violate baryon conservation: after all, the protons and neutrons in the star would have to go away somehow!  They admit they don’t know how this would work.  They just say that the gravitational field of the star will create particle-antiparticle pairs that will slowly radiate away, forcing the dead star to lose mass <em>somehow</em> to conserve energy.</p>
<p>If experts thought this had even a chance of being true, it would be the biggest thing since sliced bread—at least in the field of quantum gravity.  Everyone would be writing papers about it, because if true it would be revolutionary.   It would overturn calculations by experts which say that a stationary chunk of matter doesn’t emit Hawking radiation.  It would also mean that quantum field theory in curved spacetime can only be consistent if baryon number fails to be conserved!  This would be utterly shocking.</p>
<p>But in fact, these new papers have had almost zero effect on physics.  There’s a short rebuttal, here:</p>
<p>• Antonio Ferreiro José Navarro-Salas and Silvia Pla, <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.133.229001">Comment on “Gravitational pair production and black hole evaporation”</a>, <i>Phys. Rev. Lett.</i> <b>133</b> (2024), 229001.</p>
<p>It explains that these guys used a crude approximation that gives wrong results even in a simpler problem.   Similar points are made here:</p>
<p>• E. T. Akhmedov, D. V. Diakonov and C. Schubert, <a href="https://link.aps.org/pdf/10.1103/PhysRevD.110.105011">Complex effective actions and gravitational pair creation</a>, <i>Phys. Rev. D.</i> <b>110</b>, 105011.</p>
<p>Unfortunately, it seems the real experts on quantum field theory in curved spacetime have not come out and mentioned the <i>correct</i> way to think about this issue, which has been known at least since 1975.  To them—or maybe I should dare to say “us”—it’s just <em>well known</em> that the gravitational field of a static mass does not cause the creation of particle-antiparticle pairs.</p>
<p>Of course, the referees should have rejected Wondrak, van Suijlekom and Falcke’s papers.  But apparently none of those referees were experts on the subject at hand.  So you can’t trust a paper just because it appears in a supposedly reputable physics journal.  You have to actually understand the subject and assess the paper yourself, or talk to some experts you trust.</p>
<p>If I were a science journalist writing an article about a supposedly shocking development like this, I would email some experts and check to see if it’s for real.  But plenty of science journalists don’t bother with that anymore: they just believe the press releases.  So now we’re being bombarded with lazy articles like these:</p>
<p>• <a href="https://www.cbsnews.com/news/universe-end-much-sooner-than-expected-researchers-say/">Universe will die “much sooner than expected,” new research says</a>, <i>CBS News</i>, May 13, 2025.</p>
<p>• Sharmila Kuthunur, <a href="https://www.space.com/astronomy/scientists-calculate-when-the-universe-will-end-its-sooner-than-expected">Scientists calculate when the universe will end—it’s sooner than expected</a>, <i>Space.com</i>, 15 May 2025.</p>
<p>• Jamie Carter, <a href="https://www.forbes.com/sites/jamiecartereurope/2025/05/16/the-universe-will-end-sooner-than-thought-scientists-say/">The universe will end sooner than thought, scientists say</a>, <i>Forbes</i>, 16 May 2025.</p>
<p>The list goes on; these are just three.   There’s no way what I say can have much effect against such a flood of misinformation.  As Mark Twain said, “A lie can travel around the world and back again while the truth is lacing up its boots.”  Actually he probably didn’t say that—but everyone keeps saying he did, illustrating the point perfectly.</p>
<p>Still, there might be a few people who both care and don’t already know this stuff.  Instead of trying to give a mini-course here, let me simply point to an explanation of how things really work:</p>
<p>• Abhay Ashtekar and Anne Magnon, <a href="https://www.researchgate.net/publication/243683929_Quantum_Fields_in_Curved_Space-Times">Quantum fields in curved space-times</a>, <i>Proceedings of the Royal Society</i>, <b>346</b> (1975), 375–394.</p>
<p>It’s technical, so it’s not easy reading if you haven’t studied quantum field theory and general relativity, but that’s unavoidable.  It shows that in a static spacetime there is a well-defined concept of ‘vacuum’, and the vacuum is stable.  Jorge Pullin pointed out the key sentence for present purposes:</p>
<blockquote><p>
  Thus, if the underlying space-time admits a everywhere time-like Killing field, the vacuum state is indeed stable and phenomena such as the spontaneous creation of particles do not occur.
</p></blockquote>
<p>This condition of having an “everywhere time-like Killing field” says that a spacetime has time translation symmetry.  Ashtekar and Magnon also assume that spacetime is globally hyperbolic and that the wave equation for a massive spin-zero particle has a smooth solution given smooth initial data.  All this lets us define a concept of energy for solutions of this equation.  It also lets us split solutions into positive-frequency solutions, which correspond to particles, and negative-frequency ones, which correspond to antiparticles.  We can thus set up quantum field theory in way we’re used to on Minkowski spacetime, where there’s a well-defined vacuum which does not decay into particle-antiparticle pairs.</p>
<p>The Schwarzschild solution, which describes a static black hole, also has a Killing field.  But this ceases to be timelike at the event horizon, so this result does not apply to that!</p>
<p>I could go into more detail if required, but you can find a more pedagogical treatment in this standard textbook:</p>
<p>• Robert Wald, <i>Quantum Field Theory in Curved Spacetime and Black Hole Thermodynamics</i>, University of Chicago Press, Chicago, 1994.</p>
<p>In particular, go to Section 4.3, which is on quantum field theory in stationary spacetimes.</p>
<p>I also can’t resist citing this thesis by a student of mine:</p>
<p>• Valeria Michelle Carrión Álvarez, <a href="https://www.arxiv.org/abs/math-ph/0412032"><i>Loop Quantization versus Fock Quantization of p-Form Electromagnetism on Static Spacetimes</i></a>, Ph.D. thesis, U. C. Riverside, 2004.</p>
<p>This thesis covers the case of electromagnetism, while Ashtekar and Magnon, and also Wald, focus on a massive scalar field for simplicity.</p>
<p>So: it’s been rigorously shown that the gravitational field of a static object does not create particle-antiparticle pairs.  This has been known for decades.  Now some people have done a crude approximate calculation that seems to show otherwise.  Some flaws in the approximation have been pointed out.  Of course the authors of the calculation <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.133.229002">don’t believe their approximation is flawed</a>.    We could argue about that for a long time.  But it’s scarcely worth thinking about, because no approximations were required to settle this issue.  It was settled over 50 years ago, and the new work is not shedding new light on the issue: it’s much more hand-wavy than the old work.</p>

				
				<p>
					<small>
					This entry was posted  on Saturday, May 17th, 2025 at 2:44 pm and is filed under <a href="https://johncarlosbaez.wordpress.com/category/physics/" rel="category tag">physics</a>.					You can follow any responses to this entry through the <a href="https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/feed/">RSS 2.0</a> feed.
											You can <a href="#respond">leave a response</a>, or <a href="https://johncarlosbaez.wordpress.com/2025/05/17/dead-stars-dont-radiate-and-shrink/trackback/" rel="trackback">trackback</a> from your own site.
					
					</small>
				</p>

				<nav id="nav-below">
					<h3>Post navigation</h3>
					<span><a href="https://johncarlosbaez.wordpress.com/2025/05/16/meteor-burst-communications/" rel="prev">« Previous Post</a></span>
					<span></span>
				</nav><!-- #nav-below -->

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to have the browser pick a contrasting color in CSS (138 pts)]]></title>
            <link>https://webkit.org/blog/16929/contrast-color/</link>
            <guid>44015367</guid>
            <pubDate>Sat, 17 May 2025 16:26:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webkit.org/blog/16929/contrast-color/">https://webkit.org/blog/16929/contrast-color/</a>, See on <a href="https://news.ycombinator.com/item?id=44015367">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-16929">
            
            

            <div>
                                
                <p>Have you ever wished you could write simple CSS to declare a color, and then have the browser figure out whether black or white should be paired with that color? Well, now you can, with <code>contrast-color()</code>. Here’s how it works.</p>
<p>Imagine we’re building a website or a web app, and the design calls for a bunch of buttons with different background colors. We can create a variable named <code>--button-color</code>  to handle the background color. And then assign that variable different values from our design system in different situations.</p>
<p>Sometimes the button background will be a dark color, and the button text should be white to provide contrast. Other times, the background will be a lighter color, and the text should be black. Like this:</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/cc-button-1-dark-scaled.png" type="image/png" media="(prefers-color-scheme: dark)"><img fetchpriority="high" decoding="async" src="https://webkit.org/wp-content/uploads/cc-button-1-light-scaled.png" alt="Two buttons side by side. White text on dark purple for the first, black text on pink background for the second. " width="2560" height="546" srcset="https://webkit.org/wp-content/uploads/cc-button-1-light-scaled.png 2560w, https://webkit.org/wp-content/uploads/cc-button-1-light-300x64.png 300w, https://webkit.org/wp-content/uploads/cc-button-1-light-1024x218.png 1024w, https://webkit.org/wp-content/uploads/cc-button-1-light-768x164.png 768w, https://webkit.org/wp-content/uploads/cc-button-1-light-1536x327.png 1536w, https://webkit.org/wp-content/uploads/cc-button-1-light-2048x436.png 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></picture></figure>
<p>Now, of course, we could use a second variable for the text color and carefully define the values for <code>--button-color</code> and <code>--button-text-color</code> at the same time, in pairs, to ensure the choice for the text color is the right one. But, on a large project, with a large team, carefully managing such details can become a really hard task to get right. Suddenly a dark button has unreadable black text, and users can’t figure out what to do.</p>
<p>It’d be easier if we could just tell our CSS to make the text black/white, and have the browser pick which to use — whichever one provides more contrast with a specific color. Then we could just manage our many background colors, and not worry about the text color.</p>
<p>That’s exactly what the <code>contrast-color()</code> function will let us do.</p>
<h2>contrast-color()</h2>
<p>We can write this in our CSS:</p>
<pre><code><span>color</span>: <span>contrast-color</span>(<span>purple</span>);
</code></pre>
<p>And the browser will set <code>color</code> to either black or white, whichever choice provides better contrast with <code>purple</code>.</p>
<p>Let’s style our button. We’ll set the button background color to our variable. And we’ll define the text color to be the contrasting black/white choice that pairs with that variable.</p>
<pre><code><span>button</span> {
  <span>background-color</span>: <span>var</span>(<span>--button-color</span>);
  <span>color</span>: <span>contrast-color</span>(<span>var</span>(<span>--button-color</span>));
}
</code></pre>
<p>Now we only need to define one color, and the other follows! When we change the button color, the browser will reconsider whether the text should be black or white, and choose fresh the option with more contrast.</p>
<p>For fun, let’s also define a hover color using <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_colors/Relative_colors">Relative Color Syntax</a>, and now one variable determines four colors — the default button color &amp; the text to go with it, plus the hover color &amp; the text to go with that.</p>
<pre><code><span>:root</span> {
  <span>--button-color</span>: <span>purple</span>;
  <span>--hover-color</span>: <span>oklch</span>(<span>from</span> <span>var</span>(<span>--button-color</span>) <span>calc</span>(<span>l</span> + <span>.2</span>) <span>c</span> <span>h</span>);
}
<span>button</span> {
  <span>background-color</span>: <span>var</span>(<span>--button-color</span>);
  <span>color</span>: <span>contrast-color</span>(<span>var</span>(<span>--button-color</span>));
  <span>text-box</span>: <span>cap</span> <span>alphabetic</span>; <span>/* vertically centers the text */</span>
}
<span>button</span><span>:hover</span> {
  <span>background-color</span>: <span>var</span>(<span>--hover-color</span>);
  <span>color</span>: <span>contrast-color</span>(<span>var</span>(<span>--hover-color</span>));
}
</code></pre>
<p><a href="https://codepen.io/jensimmons/pen/XJJjKMO?editors=1100">Here’s a demo of the result</a>. Try it in <a href="https://developer.apple.com/safari/technology-preview/">Safari Technology Preview</a>, where you can change the button color dynamically.</p>

<h2>Accessibility considerations and contrast algorithms</h2>
<p>Now, it might be tempting to believe that <code>contrast-color()</code> will magically solve all contrast accessibility concerns all by itself, and your team will never have to think about color contrast again. Nope, that’s not the case. At all.</p>
<p>Using the <code>contrast-color()</code> function does not guarantee that the resulting pair of colors will be accessible. It’s quite possible to pick a color (in this case a background color) that will not have enough contrast with either black or white. It’s still up to the humans involved — designers, developers, testers, and more — to ensure there’s enough contrast.</p>
<p>In fact, if you try out <a href="https://codepen.io/jensimmons/pen/XJJjKMO?editors=1100">our demo</a> in Safari Technology Preview now (as this article is published in May 2025), you’ll find many of the pairings with mid-tone background colors don’t result in enough contrast. It often seems like the wrong choice is being made. For example, this #317CFF blue returns a contrast-color of black.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/cc-button-2-dark-scaled.png" type="image/png" media="(prefers-color-scheme: dark)"><img decoding="async" src="https://webkit.org/wp-content/uploads/cc-button-2-light-scaled.png" alt="Medium dark blue button with black text. The text is hard to see." width="2560" height="545" srcset="https://webkit.org/wp-content/uploads/cc-button-2-light-scaled.png 2560w, https://webkit.org/wp-content/uploads/cc-button-2-light-300x64.png 300w, https://webkit.org/wp-content/uploads/cc-button-2-light-1024x218.png 1024w, https://webkit.org/wp-content/uploads/cc-button-2-light-768x164.png 768w, https://webkit.org/wp-content/uploads/cc-button-2-light-1536x327.png 1536w, https://webkit.org/wp-content/uploads/cc-button-2-light-2048x436.png 2048w" sizes="(max-width: 2560px) 100vw, 2560px">
</picture></figure>
<p>When white is clearly the better choice for perceptual contrast.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/cc-button-3-dark-scaled.png" type="image/png" media="(prefers-color-scheme: dark)"><img decoding="async" src="https://webkit.org/wp-content/uploads/cc-button-3-light-scaled.png" alt="Same dark medium blue button, now with white text. Much easier to see what it says." width="2560" height="545" srcset="https://webkit.org/wp-content/uploads/cc-button-3-light-scaled.png 2560w, https://webkit.org/wp-content/uploads/cc-button-3-light-300x64.png 300w, https://webkit.org/wp-content/uploads/cc-button-3-light-1024x218.png 1024w, https://webkit.org/wp-content/uploads/cc-button-3-light-768x164.png 768w, https://webkit.org/wp-content/uploads/cc-button-3-light-1536x327.png 1536w, https://webkit.org/wp-content/uploads/cc-button-3-light-2048x436.png 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></picture></figure>
<p>What is happening here? Why is the less-contrasting choice being made?</p>
<p>Well, the current implementation in Safari Technology Preview is using the contrast algorithm officially defined in <a href="https://www.w3.org/WAI/standards-guidelines/wcag/">WCAG 2</a> (Web Content Accessibility Guidelines version 2). If we put this color blue through a well-respected <a href="https://webaim.org/resources/contrastchecker/">color contrast checker at WebAIM</a>, it does clearly recommend using black for the text color, not white. WCAG 2 is the current authoritative standard for accessibility on the web, required by law in many places.</p>
<p>The WCAG 2 algorithm calculates black-on-#317CFF as having a contrast ratio of 5.45:1, while white-on-#317CFF has 3.84:1. The <code>contrast-color()</code> function is simply choosing the option with the bigger number — and 5.45 is bigger than 3.84.</p>
<figure><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/WCAG2-test-blue-scaled.png" alt="Screenshots of the WCAG 2 color contrast checker, showing results of white on blue and black on blue. Black passes. White fails. But black is hard to read while white is easy to read." width="2560" height="1361" srcset="https://webkit.org/wp-content/uploads/WCAG2-test-blue-scaled.png 2560w, https://webkit.org/wp-content/uploads/WCAG2-test-blue-300x160.png 300w, https://webkit.org/wp-content/uploads/WCAG2-test-blue-1024x545.png 1024w, https://webkit.org/wp-content/uploads/WCAG2-test-blue-768x408.png 768w, https://webkit.org/wp-content/uploads/WCAG2-test-blue-1536x817.png 1536w, https://webkit.org/wp-content/uploads/WCAG2-test-blue-2048x1089.png 2048w" sizes="auto, (max-width: 2560px) 100vw, 2560px"><figcaption>Testing black versus white on a medium-dark blue in the <a href="https://webaim.org/resources/contrastchecker/">WCAG 2 color contrast checker at Web AIM</a>.</figcaption></figure>
<p>When machines run the WCAG 2 algorithm, the black text has higher contrast mathematically. But when humans look at these combinations, the black text has lower contrast perceptually. If you find this odd, well, you aren’t the only one. The WCAG 2 color contrast algorithm has long been a subject of criticism. In fact, one of the major driving forces for updating WCAG to level 3 is a desire to improve the contrast algorithm.</p>
<p>The <a href="https://github.com/Myndex/SAPC-APCA">Accessible Perceptual Contrast Algorithm (APCA)</a> is one possible candidate for inclusion in <a href="https://www.w3.org/WAI/standards-guidelines/wcag/wcag3-intro/">WCAG 3</a>. You can try out this algorithm today by using the APCA Contrast Calculator at <a href="https://apcacontrast.com/">apcacontrast.com</a>. Let’s look at what it thinks about black vs white text on this particular shade of blue background.</p>
<figure><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/APCA-test-blue.png" alt="Screenshot of APCA Contrast Calculator, showing the same tests of black on blue vs white on blue. White clearly wins." width="1924" height="2442" srcset="https://webkit.org/wp-content/uploads/APCA-test-blue.png 1924w, https://webkit.org/wp-content/uploads/APCA-test-blue-236x300.png 236w, https://webkit.org/wp-content/uploads/APCA-test-blue-807x1024.png 807w, https://webkit.org/wp-content/uploads/APCA-test-blue-768x975.png 768w, https://webkit.org/wp-content/uploads/APCA-test-blue-1210x1536.png 1210w, https://webkit.org/wp-content/uploads/APCA-test-blue-1614x2048.png 1614w" sizes="auto, (max-width: 1924px) 100vw, 1924px"><figcaption>Testing the same black versus white on a medium-dark blue in the <a href="https://apcacontrast.com/">APCA Contrast Calculator</a>.</figcaption></figure>
<p>This contrast algorithm evaluates black-on-blue as having a score of Lc 38.7, while white-on-blue scores Lc -70.9. To know which has more contrast, ignore the negative sign for a moment, and compare 38.7 to 70.9. The bigger the number, the more contrast. The APCA test results say that white text is clearly better than black. Which feels exactly right.</p>
<p>(In the APCA scoring system, the negative number simply signifies that the text is lighter than the background. Think light mode = positive numbers, dark mode = negative numbers.)</p>
<p>Why is APCA giving such better results than WCAG 2? Because its algorithm calculates contrast perceptually instead of with simple mathematics. This takes into consideration the fact humans do not perceive contrast linearly across hue and lightness. If you’ve learned about LCH vs HSL color models, you’ve probably heard about how newer approaches to color mathematics do a better job of understanding our perception of lightness, and knowing which colors seem to be the same luminance or tone. The “Lc” marking the APCA score stands for “Lightness contrast”, as in “Lc 75”.</p>
<p>Luckily, the algorithm behind the <code>contrast-color</code> function can be swapped out. Support for this feature first shipped in March 2021, in <a href="https://webkit.org/blog/11577/release-notes-for-safari-technology-preview-122/">Safari Technology Preview 122</a>. (Also, at that time it was named <code>color-contrast</code>.) Back then, it was too early to choose a better algorithm.</p>
<p>The CSS standard still calls for browsers to use the older algorithm, but contains a note about the future: “Currently only WCAG 2.1 is supported, however this algorithm is known to have problems, particularly on dark backgrounds. Future revisions of this module will likely introduce additional contrast algorithms.” Debates over which algorithm is best for WCAG 3 are still ongoing, including discussion of licensing of the algorithms under consideration.</p>
<p>Meanwhile, your team should still take great care in choosing color palettes, keeping accessibility in mind. If you are choosing clearly-light or clearly-dark colors for the contrasting color,  <code>contrast-color()</code> will work great even when backed by the WCAG 2 algorithm. It’s in evaluating contrast with mid-tones where the algorithms start to differ in their results.</p>
<p>Plus, the <code>contrast-color()</code> function alone will never guarantee accessibility, even when updated with a better algorithm. “This one has <em>more</em> contrast” is not the same thing as “this one has <em>enough</em> contrast”. There are plenty of colors that never have enough contrast with either black or white, especially at smaller text sizes or thinner font weights.</p>
<h2>Providing enough contrast in the real world</h2>
<p>While thinking about color contrast, we should remember another tool in our arsenal to ensure we provide good contrast for everyone — the<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@media/prefers-contrast"></a><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@media/prefers-color-scheme"><code>prefers-contrast</code></a> media query. It lets us offer alternative styling to those who want more contrast.</p>
<pre><code><span>@media</span> (<span>prefers-contrast</span>: <span>more</span>) {
  <span>/* styling with more contrast */</span>
}
</code></pre>
<p>Let’s think through how to use these tools in a real world situation. Imagine we are creating a website for a tree nursery whose main brand color is a particular shade of bright medium green. Our design team really wants to use #2DAD4E as the main button background.</p>
<p>To keep things simple, let’s also pretend we live in a future when the APCA algorithm has replaced the WCAG 2 algorithm in CSS. This change will mean <code>contrast-color()</code> will return white for our text color against this medium green, not black.</p>
<p>But looking up <a href="https://apcacontrast.com/?BG=2dad4e&amp;TXT=ffffff&amp;DEV=G4g&amp;BUF=A22">this color combination</a>, we see there might not be enough contrast for some users, especially if the text is small. This is where good design is important.</p>
<p><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/APCA-text-green-scaled.png" alt="Testing white on medium green in the APCA contrast calculator. The interface has lots of options for adjusting the colors. And it's got a panel across the bottom with six sections of examples of white text on this color green, in various sizes and weights of fonts. " width="2560" height="1341" srcset="https://webkit.org/wp-content/uploads/APCA-text-green-scaled.png 2560w, https://webkit.org/wp-content/uploads/APCA-text-green-300x157.png 300w, https://webkit.org/wp-content/uploads/APCA-text-green-1024x536.png 1024w, https://webkit.org/wp-content/uploads/APCA-text-green-768x402.png 768w, https://webkit.org/wp-content/uploads/APCA-text-green-1536x805.png 1536w, https://webkit.org/wp-content/uploads/APCA-text-green-2048x1073.png 2048w" sizes="auto, (max-width: 2560px) 100vw, 2560px"></p>
<p>When using this shade of green as the background for white text, the APCA score is Lc -60.4.</p>
<p>You might remember that WCAG 2 evaluates contrast with a ratio (like “2.9:1”). However, APCA scores are a single number, ranging from Lc -108 to 106. Whether or not Lc -60.4 has enough contrast depends on how big the text is — and, new in APCA, how thick the font weight is.</p>
<p>There’s information about what’s considered a good target for Bronze, Silver, and Gold level conformance in the <a href="https://readtech.org/ARC/tests/visual-readability-contrast/?tn=criterion">APCA Readability Criterion</a>. These recommendations can really help guide designers to select the size and weight of text to ensure enough contrast, while allowing a range of beautiful color combinations. In fact, the WCAG 3 itself is being designed to provide flexible guidance to help you understand how to support all users, rather than binary judgments the way WCAG 2 does. Good accessibility isn’t about simply meeting a magical metric to check off a box on a list. It’s about understanding what works for real people, and designing for them. And what people need is complex, not binary.</p>
<p>You’ll notice that this particular <a href="https://apcacontrast.com/">APCA Contrast Calculator</a> not only provides a score, but also evaluates the success of dynamic examples showing combinations of font size and font weight. In our case, for “Usage” it says “fluent text okay”. (For the black on blue example above, it instead says “Usage: spot &amp; non text only”.) The Calculator is showing that white text on #2DAD4E works at 24px text if the font weight is 400 or bolder. If we want to use a font-weight of 300, then the text should be at least 41px. Of course, this will depend on which font-face we use, and we aren’t using the same font as that Contrast Calculator does, but there’s far more nuance in this guidance than tools for the WCAG 2 algorithm. And it helps our team come up with a plan for a beautiful design.</p>
<p>Our tree nursery website supports both light and dark mode, and our designers determined that #2DAD4E works as a button color for both light and dark mode for many users, as long as they carefully designed our buttons considering how font size and weight impacts contrast. But even with those considerations, Lc -60.4 is not quite enough contrast for all users, so for anyone who has set their accessibility preferences to ask for more contrast, we’ll replace the button background color with two options — a darker #3B873E green for light mode (with white text, scoring Lc -76.1), and a lighter #77e077 green for dark mode (with black text, scoring Lc 75.2).</p>
<p>Here’s the color palette our fictional design team wants us to accomplish in CSS:</p>
<p><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/green-design-2-scaled.png" alt="A diagram of our color palette, explaining when to use which color combination. (All information is also articulated in the text of this article.)" width="2560" height="1436" srcset="https://webkit.org/wp-content/uploads/green-design-2-scaled.png 2560w, https://webkit.org/wp-content/uploads/green-design-2-300x168.png 300w, https://webkit.org/wp-content/uploads/green-design-2-1024x575.png 1024w, https://webkit.org/wp-content/uploads/green-design-2-768x431.png 768w, https://webkit.org/wp-content/uploads/green-design-2-1536x862.png 1536w, https://webkit.org/wp-content/uploads/green-design-2-2048x1149.png 2048w" sizes="auto, (max-width: 2560px) 100vw, 2560px"></p>
<p>When we define colors in variables, it’s incredibly easy to swap out color values for these various conditions. And by using <code>contrast-color()</code>, we only need to worry about the background colors, not the text color pairings. We’ll make the browser do the work, and get the paired colors for free.</p>
<p>To accomplish all of these things at once, we can just write this code (because, remember, we are pretending to live in a future when a better algorithm has replaced the WCAG 2 algorithm in CSS):</p>
<pre><code><span>--button-color</span>: #2<span>DAD4E</span>;  <span>/* brand green background */</span> 

<span>@media</span> (<span>prefers-contrast</span>: <span>more</span>) {
  @<span>media</span> (<span>prefers-color-scheme</span>: <span>light</span>) {
    <span>--button-color</span>: <span>#419543</span>;  <span>/* darker green background */</span>
  }
  <span>@media</span> (<span>prefers-color-scheme</span>: <span>dark</span>) {
    <span>--button-color</span>: <span>#77CA8B</span>;  <span>/* lighter green background */</span>
  }
}

<span>button</span> {
  <span>background-color</span>: <span>var</span>(<span>--button-color</span>);
  <span>color</span>: <span>contrast-color</span>(<span>var</span>(<span>--button-color</span>));
  <span>font-size</span>: <span>1.5</span><span>rem</span>;  <span>/* 1.5 * 16 = 24px at normal zoom */</span>
  <span>font-weight</span>: <span>500</span>;
}
</code></pre>
<p>In reality, since the WCAG 2 algorithm is the one driving <code>contrast-color()</code>, we probably couldn’t use it on this website. But if we had another project where the brand color was a darker green, and the choice between white/black was the correct one, it could be quite helpful today.</p>
<p>Using <code>contrast-color()</code> is especially helpful when defining colors for multiple states or options like enabled/disabled, light/dark mode, prefers-contrast, and more.</p>
<h2>Beyond black &amp; white</h2>
<p>You might be wondering, “but what if I want the browser to choose a color beyond just black/white?” If you read about or tried out our original implementation in Safari Technology Preview 122 four years ago, you might remember that the original feature did much more. The newer <code>contrast-color()</code> function is greatly simplified from the original <code>color-contrast()</code>.</p>
<p>Because a decision on which color-contrast algorithm to use for WCAG 3 is still being debated, the CSS Working Group decided to move forward with a tool that simply chooses black or white to contrast with the first color. Keeping it simple makes it possible to swap out the algorithm later. By hardcoding the list of options to be black/white, websites are far less likely to break when the WCAG 2 algorithm is replaced, giving the CSSWG the flexibility it needs to keep making needed changes, even as <code>contrast-color</code> ships into the hands of users.</p>
<p>In the future, more complex tools will come along to support more powerful options. Perhaps you’ll be able to list a set of custom color options and have the browser pick from those, instead of picking from black/white. Perhaps you’ll list a set of options, plus specify a contrast level that you want the browser to aim for, instead of having it picking the choice that yields maximum contrast.</p>
<p>In the meantime, often a simple choice between black and white is all you need. We wanted to get the simple version into your hands sooner, rather than waiting for a process that will take years.</p>
<p>And while all of the examples above show black/white text on a color background, <code>contrast-color</code> can be used for much more. You can use a custom color for your text, and make the background be black/white. Or not involve text at all, and define colors for borders, background — anything. There’s a lot you can do.</p>
<h2>Continue the conversation</h2>
<p>You can learn more about the APCA (Accessible Perceptual Contrast Algorithm) by reading documentation from the folks <a href="https://github.com/Myndex/SAPC-APCA/?tab=readme-ov-file">creating it</a>. Including:</p>
<ul>
<li><a href="https://git.apcacontrast.com/documentation/APCAeasyIntro">The Easy Intro to the APCA Contrast Method</a> — a plain-language introduction to perceptually uniform contrast</li>
<li><a href="https://readtech.org/ARC/tests/bronze-simple-mode/?tn=criterion">Bronze Simple Mode</a> — a “most basic” design guideline, intended for users migrating from WCAG 2 contrast</li>
</ul>
<p>We’d love to hear your thoughts about <code>contrast-color()</code>. Your feedback on this tool can help shape its future. You can find me, Jen Simmons, on <a href="https://bsky.app/profile/jensimmons.bsky.social">Bluesky</a> / <a href="https://front-end.social/@jensimmons">Mastodon</a>. Or follow our other web evangelists —  Saron Yitbarek on <a href="https://bsky.app/profile/saron.bsky.social">BlueSky</a>, and Jon Davis on <a href="https://bsky.app/profile/jondavis.bsky.social">Bluesky</a> / <a href="https://mastodon.social/@jondavis">Mastodon</a>. You can also follow WebKit <a href="https://www.linkedin.com/in/apple-webkit/">on LinkedIn</a>.</p>

                            </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[If nothing is curated, how do we find things (154 pts)]]></title>
            <link>https://tadaima.bearblog.dev/if-nothing-is-curated-how-do-we-find-things/</link>
            <guid>44015144</guid>
            <pubDate>Sat, 17 May 2025 15:51:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tadaima.bearblog.dev/if-nothing-is-curated-how-do-we-find-things/">https://tadaima.bearblog.dev/if-nothing-is-curated-how-do-we-find-things/</a>, See on <a href="https://news.ycombinator.com/item?id=44015144">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-05-15T16:55Z">
                    May 15, 2025
                </time>
            </i>
        </p>
    

    <p>Bjork is currently promoting a new concert film being released called <em>Cornucopia</em>. She's been releasing new photoshoots and interviews almost every day for the past two weeks. For a musician who normally goes into hiding and only emerges when it's time to promote something, it's been a pretty exciting time to be a fan. However, all the information being put out, promoted on social media and reposted on places like Reddit, have all been a little confusing.</p>
<p>I saw a post from someone on Reddit asking to "explain to me like I'm five" what exactly is being released and what is it. Someone did reply to them with information, but the comment thread turned into a short argument as another commenter disagreed that the film would have a documentary attached to it. "Where did you get that info from?" a commenter asked. "I think I saw it somewhere in another interview that I translated into English," they replied, "but I might have gotten that wrong."</p>
<p>While reading this I thought, "It's times like these where an old-fashioned website would come in handy." Because, quite frankly, it would.<sup id="fnref-1"><a href="#fn-1">1</a></sup> As convenient as social media is, it scatters the information like bread being fed to ducks. You then have to hunt around for the info or hope the magical algorithm gods read your mind and guide the information to you.</p>
<p>I always felt like social media creates an illusion of convenience. Think of how much time it takes to stay on top of things. To stay on top of music or film. Think of how much time it takes these days, how much hunting you have to do. Although technology has made information vast and reachable, it's also turned the entire internet into a sludge pile. And now, instead of relying on professional curators to sort through things for us, now <em>we</em> have to do the sorting.</p>
<p>Think of the old days. When I was a kid, I lived in a podunk, suburban town in the middle of nowhere. It wasn't a major city or even a major state, but I was able to quite easily stay on top of everything pop culture related, even things that weren't mainstream or even super popular in my country.</p>
<p>I discovered interesting music like Aphex Twin, Squarepusher, Portishead, Tricky, Orbital, Takako Minekawa, Hooverphonic, Poe, Veruca Salt all from sporadically listening to one college radio station in my hometown and, once a week, watching one music program on MTV (usually <em>120 Minutes</em> or <em>AMP</em>). Then, once a month, I would sometimes flip through a music magazine while at the hair salon (usually <em>Rolling Stone</em> or <em>Spin</em>). And that was literally it.</p>
<p>Same with movies. Once a week I would watch <em>Ebert and Roeper</em>, who would discuss and review all the releases of that week, including indie and foreign ones. I would also sometimes flip through film magazines or randomly stumble across something cool being aired on the IFC channel or Bravo<sup id="fnref-2"><a href="#fn-2">2</a></sup>. That was how I discovered indie films like <em>Welcome to the Dollhouse</em> and <em>Run, Lola, Run</em>. I was a "cool," knowledgeable cultural teen with next to no Internet access.</p>
<p>The rise of social media has killed the art of curation because, these days, things are rarely curated. Criticism is dead (with Fantano<sup id="fnref-3"><a href="#fn-3">3</a></sup> being the one exception) and Gen Alpha doesn't know how to find music through anything but TikTok. Relying on algorithms puts way too much power in technology's hands. And algorithms can only predict content that you've seen before. It'll never surprise you with something different. It keeps you in a little bubble. Oh, you like shoegaze? Well, that's all the algorithm is going to give you until you intentionally start listening to something else.</p>
<p>It makes art (music, film, tv, etc.) seem like one big sludge pile. It makes it feel vast and exhausting, like an endless list of things that you'll never get to the end of. I've been noticing this sentiment with society, this feeling of always being mentally exhausted. How many times have we had a discussion with a friend who was recommending a show and our response was, "Oh yeah, I'll have to see it, but my list of shows is so long!" The reality is we're not going to watch it because we feel like we have no time to get through everything and we don't fully trust other people's recommendations.</p>
<p>And that's where curation comes in. We need critics who devote their lives to browsing through the pile and telling us what is worth our time and what isn't.</p>
<p>There are still some critics out there (Vulture, Pitchfork), but these sites are dying. They publish dozens of articles a day, trying to get as many clicks and pageviews as possible, adding to the volume of content we view daily. Before, you could reach for a magazine once a month or a watch a show once a week, but now you have to browse Vulture every day and read all 20+ articles they publish, even on the weekends. Who has time to read all that? Who has the time for any of this? Technology is making our lives harder, not easier.</p>
<p>So I guess the next question is, "How do I fix this?" Like most people, I've been pulling back. Less time relying on algorithms to predict what I like and more time just making notes and lists in Obsidian. Any time I stumble across something that looks interesting or something I don't want to forget, I make a note of it so I can retrieve it later.</p>
<p>It's honestly not much of a solution as it still makes "staying on top of things" feel like a job. But I'm struggling to find a better way to wrap up this post. This might just be society's new normal. The ones who prioritize comfort will stay in their algorithmic bubbles, while those who care about broadening their horizons will prioritize finding things on their own. Search long enough and eventually you <em>will</em> find what you're looking for. Eventually.</p>
<section>
<ol>
<li id="fn-1"><p>There actually is a <a href="https://bjorkcornucopia.com/">website now</a>, but when I wrote this a month ago there wasn't. Oh well. 🤷‍♀️<a href="#fnref-1">↩</a></p></li>
<li id="fn-2"><p>Pre-<em>Real Housewives</em> Bravo was a <em>very</em> different channel.<a href="#fnref-2">↩</a></p></li>
<li id="fn-3"><p>Uproxx recently had an <a href="https://uproxx.com/music/anthony-fantano-interview-music-criticism-needle-drop/">interesting convo</a> with him about this exact topic.<a href="#fnref-3">↩</a></p></li>
</ol>
</section>


    

    
        
            <p>
                
                    <a href="https://tadaima.bearblog.dev/blog/?q=internet">#internet</a>
                
                    <a href="https://tadaima.bearblog.dev/blog/?q=music">#music</a>
                
            </p>
        

        
            


        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["We would be less confidential than Google" Proton threatens to quit Switzerland (251 pts)]]></title>
            <link>https://www.techradar.com/vpn/vpn-privacy-security/we-would-be-less-confidential-than-google-proton-threatens-to-quit-switzerland-over-new-surveillance-law</link>
            <guid>44014808</guid>
            <pubDate>Sat, 17 May 2025 14:59:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techradar.com/vpn/vpn-privacy-security/we-would-be-less-confidential-than-google-proton-threatens-to-quit-switzerland-over-new-surveillance-law">https://www.techradar.com/vpn/vpn-privacy-security/we-would-be-less-confidential-than-google-proton-threatens-to-quit-switzerland-over-new-surveillance-law</a>, See on <a href="https://news.ycombinator.com/item?id=44014808">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R.jpg" alt="Proton CEO and founder Andy Yen poses next to the Proton logo at the headquarters of the encrypted email and VPN services company in Geneva." srcset="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Photo by FABRICE COFFRINI/AFP via Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<hr><ul><li><strong>Proton CEO confirmed the company will leave Switzerland if new controversial surveillance rules pass</strong></li><li><strong>An amendment to the current surveillance law would require VPNs and messaging apps to identify and retain user data</strong></li><li><strong>Another Swiss company, NymVPN, is also ready to leave the country instead of undermining its privacy and security infrastructure</strong></li></ul><hr><p>Proton confirms the company will leave Switzerland if new controversial surveillance rules pass.</p><p>Switzerland is <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/vpn-privacy-security/secure-encryption-and-online-anonymity-are-now-at-risk-in-switzerland-heres-what-you-need-to-know" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/secure-encryption-and-online-anonymity-are-now-at-risk-in-switzerland-heres-what-you-need-to-know">considering amending its surveillance law</a>, with experts warning against the risk to secure encryption and online anonymity in the country. Specifically, the amendment could require all VPN services, messaging apps, and social networks to identify and retain user data – an obligation that is now limited to mobile networks and internet service providers.</p><p>The firm behind one of the <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/best-vpn" data-before-rewrite-localise="https://www.techradar.com/vpn/best-vpn">best VPN</a> and encrypted email services, Proton, is ready to fight back on behalf of the privacy of its over 100 million users. Other Swiss-based companies, like <a data-analytics-id="inline-link" href="https://www.techradar.com/pro/vpn/nymvpn" data-before-rewrite-localise="https://www.techradar.com/pro/vpn/nymvpn">NymVPN</a>, are also doing the same.</p><h2 id="no-choice-but-to-leave-3">No choice but to leave</h2><p>In an <a data-analytics-id="inline-link" href="https://www.rts.ch/info/suisse/2025/article/proton-menace-de-quitter-la-suisse-face-aux-nouvelles-regles-de-surveillance-28883036.html" target="_blank" data-url="https://www.rts.ch/info/suisse/2025/article/proton-menace-de-quitter-la-suisse-face-aux-nouvelles-regles-de-surveillance-28883036.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">interview with RTS</a> (Radio Télévision Suisse) on May 13, 2025, Proton CEO Andy Yen slammed the proposed amendment as a "major violation of the right to privacy" that will also harm the country's reputation and its ability to compete on an international level.</p><p>"This revision attempts to implement something that has been deemed illegal in the EU and the United States. The only country in Europe with a roughly equivalent law is Russia," said Yen.</p><p><a data-analytics-id="inline-link" href="https://www.news.admin.ch/fr/nsb?id=103968" target="_blank" data-url="https://www.news.admin.ch/fr/nsb?id=103968" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">The amendment</a> aims to expand the number of service providers targeted to include so-called "derived service providers". Crucially, the new provisions will introduce three new types of information and two types of monitoring.</p><p>If the changes pass, Proton will be forced to modify how <a data-analytics-id="inline-link" href="https://www.techradar.com/reviews/protonmail-secure-email" data-before-rewrite-localise="https://www.techradar.com/reviews/protonmail-secure-email">Proton Mail</a> and <a data-analytics-id="inline-link" href="https://www.techradar.com/reviews/protonvpn" data-before-rewrite-localise="https://www.techradar.com/reviews/protonvpn">Proton VPN</a> handle encryption, alongside its strict no-log policies – something the company isn't willing to do.</p><p>"I think we would have no choice but to leave Switzerland," said Yen. "The law would become almost identical to the one in force today in Russia. It's an untenable situation. We would be less confidential as a company in Switzerland than Google, based in the United States. So it's impossible for our business model."</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">In Switzerland, the new version of the surveillance law aims to make it impossible for Proton, Threema and@nymproject to operate from Switzerland. We are in the consultation phase. We will fight. https://t.co/BcMBxzIPFC<a href="https://twitter.com/cantworkitout/status/1904483355812377045" data-url="https://twitter.com/cantworkitout/status/1904483355812377045" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">March 25, 2025</a></p></blockquote></div><p>Proton is not alone in feeling this way, though.</p><p>A new player in the VPN world, <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/vpn-services/nymvpn-is-now-live-heres-everything-you-need-to-know" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-services/nymvpn-is-now-live-heres-everything-you-need-to-know">NymVPN</a> has also been publicly fighting Swiss government plans since the beginning.</p><p>Talking to TechRadar, Nym's co-founder and COO, Alexis Roussel, confirmed that Nym will do the same and leave Switzerland if the new surveillance rules are enforced.</p><h2 id="what-s-next-3">What's next?</h2><p>As public consultations ended on May 6, 2025, we will now have to wait and see what the Swiss government decides.</p><p>Nonetheless, Roussel confirmed to TechRadar that there has been significant push-back from political parties and Swiss companies.</p><p>Some Cantons, <a data-analytics-id="inline-link" href="https://www.ge.ch/document/39174/telecharger" target="_blank" data-url="https://www.ge.ch/document/39174/telecharger" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">including Geneva</a>, have even called on the right to digital integrity as an argument against these rules. Roussel was the main originator of the initiative that <a data-analytics-id="inline-link" href="https://www.swissinfo.ch/eng/democracy/how-swiss-federalism-is-helping-the-rise-of-a-new-digital-right/89023201" target="_blank" data-url="https://www.swissinfo.ch/eng/democracy/how-swiss-federalism-is-helping-the-rise-of-a-new-digital-right/89023201" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">introduced this new right</a> to protect citizens' online privacy and data – in Geneva in 2023 and Neuchâtel in 2024 – with over 90% consensus.</p><p>Yen also told RTS to be more optimistic, despite pointing out how this matter shows the need for a more balanced approach when it comes to crafting new laws.</p><p>"If we can get Bern to adopt common-sense rules that allow companies like Proton to be competitive in Switzerland and around the world, I will stay, take my passport, and continue to invest in Switzerland," he added.</p><h3 id="section-you-might-also-like"><span>You might also like</span></h3><ul><li><a href="https://www.techradar.com/vpn/vpn-services/once-you-have-the-data-you-have-to-cooperate-windscribe-ceo-speak-out-against-global-threats-to-no-log-vpns" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-services/once-you-have-the-data-you-have-to-cooperate-windscribe-ceo-speak-out-against-global-threats-to-no-log-vpns">Windscribe CEO speaks out against global threats to no-log VPNs</a></li><li><a href="https://www.techradar.com/vpn/vpn-privacy-security/encryption-backdoors-privacy-can-be-misused-but-the-cost-of-a-world-without-is-so-much-higher" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/encryption-backdoors-privacy-can-be-misused-but-the-cost-of-a-world-without-is-so-much-higher">Encryption backdoors: privacy can be misused, "but the cost of a world without is so much higher"</a></li><li><a href="https://www.techradar.com/vpn/vpn-privacy-security/a-win-for-privacy-florida-rejects-the-encryption-backdoor-law-for-social-media" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/a-win-for-privacy-florida-rejects-the-encryption-backdoor-law-for-social-media">"A win for privacy" – Florida rejects the encryption backdoor law for social media</a></li></ul>
</div>

<div id="slice-container-authorBio-QWbBN83AJHY8NHNgbHUYZL"><p>Chiara is a multimedia journalist committed to covering stories to help promote the rights and denounce the abuses of the digital side of life – wherever cybersecurity, markets, and politics tangle up. She believes an open, uncensored, and private internet is a basic human need and wants to use her knowledge of VPNs to help readers take back control. She writes news, interviews, and analysis on data privacy, online censorship, digital rights, tech policies, and security software, with a special focus on VPNs, for TechRadar and TechRadar Pro. Got a story, tip-off, or something tech-interesting to say? Reach out to chiara.castro@futurenet.com</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Palette lighting tricks on the Nintendo 64 (179 pts)]]></title>
            <link>https://30fps.net/pages/palette-lighting-tricks-n64/</link>
            <guid>44014587</guid>
            <pubDate>Sat, 17 May 2025 14:28:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://30fps.net/pages/palette-lighting-tricks-n64/">https://30fps.net/pages/palette-lighting-tricks-n64/</a>, See on <a href="https://news.ycombinator.com/item?id=44014587">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="text">

<p><em>This article is a continuation to <a href="https://bsky.app/profile/pekkavaa.bsky.social/post/3lnnwax4vxk2v">my Bluesky thread</a> from April.</em></p>
<!-- ![](castello_screenshot.jpg) -->
<p>We made a Nintendo 64 demo for <a href="https://2025.revision-party.net/">Revision 2025</a>!</p>
<center>
<iframe width="100%" src="https://www.youtube.com/embed/v3wYV6gxJII" title="Real-time tech demo Castello (N64)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
</iframe>
</center>
<p>It has baked lighting with normal mapping and real-time specular shading, ahem, well sort of.
More on that later.
The beautiful song was made by <a href="https://bsky.app/profile/did:plc:svoizwy5mp6q4ol6j2pu74we">noby</a> with guitar performed by Moloko (<a href="https://soundcloud.com/sou_andrade">https://soundcloud.com/sou_andrade</a>).</p>
<p>Below I have some notes on the directional ambient and normal mapping techniques I developed.
They are both pretty simple in the end but I haven’t seen them used elsewhere.</p>
<h2 id="but-wait-normal-mapping-on-the-n64">But wait, normal mapping on the N64?</h2>
<p>I knew normal mapping on the N64 was possible due to earlier experiments by fellow homebrew developers WadeTyhon and <a href="https://www.youtube.com/@SpookyIluha">Spooky Iluha</a>. I had also done <a href="https://www.youtube.com/watch?v=UOHdDllyqOU">some emboss bump mapping hacks</a> myself.</p>
<p>The approach explained in this article is not new: <strong>the renderer computes lighting directly to textures at runtime</strong>.
It’s great because no specialized hardware support is needed and you can run arbitrary shading code on the CPU.
Too bad it’s so slow…</p>
<!-- The Nintendo 64 supports no shaders but it has a bunch of registers you can program for different combinations of textures and interpolated vertex colors. This kind of graphics hardware is known as a "register combiner" and it's also how early GeForce chips worked. -->
<h2 id="shading-a-palette-instead">Shading a palette instead</h2>
<p>So the idea is to do texture-space shading on the CPU.
But what if it’s a palette texture we’re shading?
Those are very common on the N64 anyway.
In that case it’s enough to update <em>only the palette</em> and the texture will respond as if we computed lighting for each texel.
Instant savings!</p>
<!-- In this case it's optimized by fitting a 256-color palette to the normal map, and then computing lighting only for each entry in the palette for speed. -->
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/palettes_example.jpg" alt="A demonstration of “palette-space” shading. When the palettes update, the full textures update too. When mapped to an object, it looks like the shading changed.">

</figure>
<p>The original palette is replaced with a shaded palette and the palette texture is applied as a regular texture to an object.
With just a diffuse “dot(N,L)” lighting the results look pretty good:
<!-- The shading model is basically `color=basecolor*dot(normal, light)`, but since this is on the CPU side, you could use any formula. --></p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/potatorock.png" alt="Another view of the above potato-shaped rock mesh.">

</figure>
<!-- The result looks pretty convincing if you don't see any UV map seams. -->
<p>In the above example I also did shading in linear space by undoing the gamma correction of the color texture :) In the final demo it wasn’t possible because I split the ambient and direct light terms to be combined by N64’s RDP unit in hardware.</p>
<h3 id="object-space-normal-mapping">Object-space normal mapping</h3>
<p>Usually normal mapping is done in tangent space.
This is way you can use repeating textures and the fine normals can modulate smoothly varying vertex normals.
A tangent-space normal map of a single color represents a smooth surface.</p>
<p>An object-space normal is simpler but more constrained.
Now the normal map’s texels don’t represent deviations from the vertex normals but absolute surface normals instead.
The runtime math becomes simpler – just read a color from a texture – but all surface points now need a unique texel, like in lightmaps.</p>
<!-- ![An early object-space normal mapping experiment in Blender. I reduced the color count of a baked normal map in an image editor and reapplied it back on the object. The results validated that the approach might work.](2025-01-21-object-space-normal-map-compression-comparison.jpg) -->
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/baseline_vs_32_palette_objectspace_normalmap.png" alt="An early experiment to validate the approach on a high-res normal map. Left: The original object-space normal map. Right: Compressed to a 32-color palette.">

</figure>

<p>The objects have both a diffuse texture (basecolor * ao) and a normal map.
Both textures actually share the same palette indices that I generated with scikit-learn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">K-means clustering</a>.
The images were interpreted as a single six-channel image for that to work.
<!-- It took a lot of tweaking to persuade k-means to behave and weight both textures fairly. --></p>
<p>Below is an example how the compression looks with a tangent-space normal map.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/compression_diagram.png" alt="A roof tile texture compression example. An RGB diffuse texture and a normal map are both compressed to a 16-color palette image in a way that palette indices are shared. Therefore the actual image data has to be stored just once at 4 bits per pixel.">

</figure>
<!-- The objects still need a varying surface color, even though they are shaded with a normal map. -->
<!-- I achieved this by combining a diffuse map and an object-space normal map to a single six-channel image, and fit a palette to *that*. -->
<!-- I had to tweak the colors-vs-normals weights for each texture though. -->
<p>At shading time, which can happen on load or on each frame, each palette color is processed in a for loop.
A single index is used to fetch a normal and a surface diffuse color.
The CPU-side shader code then produces a new RGB color for that index.
The result of the loop is a new palette but with shading applied.</p>
<p>Unfortunately this approach only really works with directional lights.
It’s also difficult to represent any kind of shadows with just the palette alone.
That’s why I started looking into how baked lighting could fit in the to the equation.</p>
<h2 id="baked-directional-ambient-and-sun-light">Baked directional ambient and sun light</h2>
<p>I wanted the demo to have a building with realistic lighting.
Perhaps it was a bit too ambitious😅
After a lot of deliberation, I put ambient and direct sun lighting in vertex color RGB and alpha channels, respectively.
The ambient term is further split into a directional intensity (a greyscale environment map) and color (vertex RGB with a saturation boost).
The sun is a directional light whose visibility is transmitted in vertex alpha.</p>
<p>The shading formula is therefore this:</p>
<pre><code>ambient = vertex_rgb      * grey_irradiance_map(N) 
direct  = vertex_alpha    * sun_color * dot(N, sun_dir)
color   = diffuse_texture * (ambient + direct)</code></pre>
<!-- Ambient color is stored in `vertex_rgb` and sun visibility in `vertex_alpha`. -->
<p>Here’s how the different terms look:</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/lighting_comparison.jpg" alt="Lighting decomposition.">

</figure>
<p>Note how the messy “Sun visibility” vertex colors get neatly masked out by the sun (N.L) computation in the bottom right corner.
In the end the ambient and direct terms are summed to get the shaded result below.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/shaded.jpg" alt="Shaded result.">

</figure>
<p>The thing about directional ambient is that even the baked lighting is rough, the details in the textures still make it look pretty high end.
Consider this scene that has just a colored blurred environment map and per-vertex ambient occlusion:</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/ibl.png" alt="Image-based ambient lighting. In this image only an ambient sky light is enabled. Also shows the palettes used (top left corner).">

</figure>
<p>It really pops!
I love image-based lighting.</p>
<p>For the blurred environment maps, I used an equirectangular projection for simplicity.
Polyhaven’s HDRIs already use the projection.
Since I precomputed the shading at load time, the complex sampling math wasn’t an issue.</p>
<!-- Consider these messy vertex colors:

![Direct light intensity only.](pillars_vertex_alpha.png)

When these vertex colors are modulated by the surface texture, the shaded normal map, and a directional ambient term, the shading looks pretty convincing:

![Combined ambient and direct light.](pillars_shaded.png)

Here's using a colored environment and per-vertex ambient occlusion: -->
<!-- Conceptually, on each texture texel the renderer samples the irradiance map with the surface normal, multiplies the resulting sky color with the surface color. In this case the texture is then modulated by per-vertex ambient occlusion but in the final demo I also had other bounce light in the vertex color RGB channels. -->
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/hdri_plot.png" alt="Visualization of an 64x32 environment map (right) before it gets blurred to an irradiance map. The dot sphere on the left shows the image pixels mapped to unit sphere directions.">

</figure>
<!-- Both ambient and direct light respect normal maps. Ambient uses image-based lighting and the direct light is just a single directional light. Environment lighting is provided by a greyscale irradiance map (think of a blurred cubemap) and is later modulated by vertex colors. -->
<!-- ![The original 3D reconstruction geometry was way too dense.](zumaglia_wireframe2.png)

![A cleaned-up model. I used Instant Meshes for this step. It's really good!](zumaglia_mesh2.png)

The castle model in the demo is based on [a 3D reconstruction by Sketchfab user andxet](https://sketchfab.com/3d-models/zumaglias-castle-bi-italy-8ef740a8ca31498c9e8f73b1c27a3298).
I retopo'd and textured it myself. I'm still very slow working in Blender so the model was left in a pretty rough state in the end.
[Instant Meshes](https://github.com/wjakob/instant-meshes) was a great help in this process. -->
<h2 id="shading-a-larger-model-with-repeating-textures">Shading a larger model with repeating textures</h2>
<p>I designed the original shading algorithm for single objects and only tested it with the <code>potato_rock.obj</code> you saw in the beginning.
For the demo, the castle mesh’s repeating textures posed a problem.
As a workaround, I split the large mesh into submeshes that each conceptually share the same object-space normal map.</p>
<p>The task was done primarily by yours truly manually in Blender, by grouping geometry by material and surface direction.
The computer did its part by calculating a world-to-model matrix based on polygon normals for each group.
That is a pretty much an approximate tangent space.
So I couldn’t escape them in the end!</p>
<p>Each of these groups shares a palette so as a whole their lighting will be correct only in the average sense.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/cube_tangents.png" alt="Tangent-space basis vector visualization for a simple cube. In the final model many polygons that point roughly in the same direction have to share the same tangent space.">

</figure>
<p>The tangent spaces are <em>not</em> interpolated at runtime, which shows up as faceted lighting.
This is perhaps the biggest downside of this technique.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/lighting_facet.png" alt="Lighting isn’t interpolated smoothly on this arch because the tangent spaces are constant over polygons, unlike in proper tangent-space normal mapping.">

</figure>
<!-- The skydome consists of a 32x64 texture that repeats horizontally and a cut sphere with some vertex coloring. I think it looked alright in the end, even though it is blurry.

![The skydome model.](skydome.jpg)

Regarding bloom, it's done on the CPU and composited back as a white quad with an alpha texture. A bit slow and buggy though🙂

![The sky dome with some bloom.](skydomebloom.jpg)

Finally, the white "egg" was supposed to be just a test model that I replace with something else. Well, that didn't happen!

![How the ending of the demo could have been.](cat_statue.jpg)

It's a perfect sphere but due to projection precision issues (!) it got stretched. Later I wanted to put a cat sit on top of it but sadly couldn't make it look right on time.😿 -->
<h2 id="specular-shading">Specular shading</h2>
<!-- Unfortunately many surface points now share a single shaded color. -->
<p>Since many surface points now share the same shaded color, computing point light or specular shading correctly is not possible.
The “palette-space” approach only really works for diffuse directional lights because the shading formulas don’t need a “to camera” vector <span><em>V</em></span>, which depends on the position of the shaded surface point.
Yet still I tried to hack it for the speculars :)</p>
<p>If we approximate the object to be shaded as a sphere, then the point <em>p</em> being shaded is simply <code>p=radius*normal</code>.
We must also accept that the result will look faceted since many surface points share the same palette index.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/lion.jpg" alt="Fresnel shading. The sculpt was approximated as a stretched sphere in lighting calculations.">

</figure>
<p>In the demo, the specular highlights looked a bit funny but still they seemed to fool most people. I count this as a success.</p>
<h2 id="is-this-the-future">Is this the future?</h2>
<p>In the demo I tried to hide the main limitations of the technique: shading discontinuities, only greyscale textures supported (!), no point lights.
So it really only works with elaborate preprocessing.
I’d love to see the shading discontinuity issue solved somehow (Spooky Iluha’s techniques don’t have it) without losing support for both ambient and direct lighting.
I don’t know if it’s possible but that’s what makes this hobby so fun :)</p>
<p>A <a href="https://files.scene.org/view/parties/2025/revision25/wild/castello.zip">PAL-compatible N64 ROM</a> is available but note that it crashes a lot.</p>
<hr>
<p><em>I’m also thinking of writing a book. <a href="https://30fps.net/book">Sign up here</a> if you’re interested.</em></p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[O2 VoLTE: locating any customer with a phone call (174 pts)]]></title>
            <link>https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/</link>
            <guid>44014046</guid>
            <pubDate>Sat, 17 May 2025 13:08:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/">https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/</a>, See on <a href="https://news.ycombinator.com/item?id=44014046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article id="blog-article"><div><p role="doc-subtitle">Privacy is dead: For multiple months, any O2 customer has had their location exposed to call initiators without their knowledge.</p></div><div><nav aria-label="Breadcrumbs"><ol><li><a href="https://mastdatabase.co.uk/">Home</a></li><li><a href="https://mastdatabase.co.uk/blog/1/">Blog articles</a></li><li><a aria-current="page" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/">O2 VoLTE: locating any customer wit…</a></li></ol></nav></div><div id="blog-article-content"><nav><h2 id="table-of-contents">Contents</h2><ul><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#introduction">Introduction</a></li><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#o2-uk">O2 UK</a></li><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#signalling-messages">Signalling messages</a></li><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#what-id-like-to-see-change">What I'd like to see change</a></li><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#conclusion">Conclusion</a></li><li><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#notes">Notes</a></li></ul></nav>
<h2 id="introduction"><a aria-label="introduction permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#introduction"><span></span></a>Introduction</h2>
<p>Voice over LTE (VoLTE) is a way to make calls via an internet-based protocol on mobile networks using a standard called IP Multimedia Subsystem
(IMS). IMS implementations have historically caused trouble due to their increased complexity and device interdependence. This increase in
complexity has traditionally only externally manifested with device incompatibility problems. In the past, it wasn't uncommon to find devices
that required special firmware to utilise VoLTE and WiFi Calling.</p>
<p>However, I have always been interested in another risk to this increased complexity. <strong>Security.</strong></p>
<p>With an IMS implementation, it is up to the mobile network to choose how they want to implement the services, and what configurations they want
to use. Your phone then talks directly with these servers. Mobile networks have a responsibility to ensure that these servers are kept up to date
and secure, and that their configurations do not lead to unnecessary data exposure.</p>
<p>Unfortunately, today we will be looking at a great example of a mobile network that has validated my concerns.</p>
<h2 id="o2-uk"><a aria-label="o2 uk permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#o2-uk"><span></span></a>O2 UK</h2>
<p>On 27 March 2017, O2 UK launched their first IMS service.<sup id="note-1-link"><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#note-1"><span>[1]</span></a></sup> Dubbed "4G Calling" by the network, it
provided improved voice quality and a better in-call data experience as customers did not drop down to 3G when making a call.</p>
<p>As someone who had recently moved to O2, I was interested in the network's IMS implementation, particularly which voice codecs were supported by
the network for calls made on 4G/WiFi Calling.</p>
<p>Using an application known as Network Signal Guru (NSG) on my rooted Google Pixel 8, I called another O2 customer (with a 4G VoLTE compatible
device) to try and determine audio quality.</p>
<p>A bug within NSG on modern Google Pixel devices with Samsung Modems means that the VoLTE section of the the app doesn't automatically populate
the codec used for the current call, meaning that I instead had to look at the raw IMS signalling messages sent between my device and the network
to find this out.</p>
<h2 id="signalling-messages"><a aria-label="signalling messages permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#signalling-messages"><span></span></a>Signalling messages</h2>
<p>Quite quickly I realised something was wrong. The responses I got from the network were extremely detailed and long, and were unlike anything I
had seen before on other networks. The messages contained information such as the IMS/SIP server used by O2
(<a href="https://www.mavenir.com/portfolio/mavcore/cloud-native-ims/" rel="noopener">Mavenir UAG</a>) along with version numbers, occasional error messages raised by the
C++ services processing the call information when something went wrong, and other debugging information. However, most notable were a set of five
headers near the bottom of the message:</p>
<figure><pre><code>SIP Msg
...
  P-Mav-Extension-IMSI: 23410123456789
  P-Mav-Extension-IMSI: 23410987654321
  P-Mav-Extension-IMEI: 350266809828927
  P-Mav-Extension-IMEI: 350266806365261
  ...
  Cellular-Network-Info: 3GPP-E-UTRAN-FDD;utran-cell-id-3gpp=2341010037A60773;cell-info-age=26371
</code></pre><figcaption><p>Synthesised excerpt of IMS signalling message for demonstration; not a genuine IMEI/IMSI/cell ID.</p></figcaption></figure>
<p>Two sets of IMSIs, two sets of IMEIs, and a Cell ID header. How curious…</p>
<p>Sure enough, when comparing both the IMSIs and IMEIs in the message to those of my own devices, I had been given both the IMSI and IMEI of my
phone which initiated the call, <strong>but also the call recipient's</strong>.</p>
<p>Curious, I looked into the <code>Cellular-Network-Info</code> header. I had never seen this SIP header before but a quick bit of research led me to learn
how to decode it. The start of the value, <code>3GPP-E-UTRAN-FDD</code> indicates that the cell data is for 4G (officially known as E-UTRAN) FDD (frequency
division duplex). The folllowing section starting with <code>utran-cell-id-3gpp</code> is broken down into 3 parts:</p>
<figure><img draggable="false" src="https://mastdatabase.co.uk/static/cell-header-breakdown-ee1811a0608846ee73b1ff754a072b07.svg" alt="" loading="lazy"></figure>
<ul>
<li>the first 5–6 digits are the network PLMN <em>of the recipient</em></li>
<li>the following 4 characters are the <em>recipient's</em> Location Area Code (LAC) in hexadecimal</li>
<li>the final 7 characters are the <em>recipient's</em> Cell ID, again in hexadecimal</li>
</ul>
<p>The final section represents how old the data is in seconds. This is present when the device isn't currently connected to a network, such as when
you have no signal or are relying on WiFi Calling.</p>
<p>That means for the above example, we are able to work out that the recipient:</p>
<ul>
<li>is currently connected to the O2 network (234-10)</li>
<li>is within LAC 0x1003 (decimal: 4009) on Cell ID 0x7a60773 (decimal: 128321395)</li>
<li>uses a Google Pixel 9 (IMEI: 350266806365261)</li>
<li>has an O2 SIM (IMSI: 23410987654321)</li>
</ul>
<p><strong>This is bad.</strong></p>
<p>With all this information, we can make use of publicly crowdsourced data, collected by tools such as <a href="https://cellmapper.net/" rel="noopener">cellmapper.net</a>, to
cross-reference this information to work out a general location of the user.</p>
<p>First, we just throw the Cell ID into a tool that calculates what site ID this corresponds to:</p>
<figure><span><span>
      <a href="https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/2c964/cell-id-calculator.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/d5f3f/cell-id-calculator.avif 144w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/a1ffd/cell-id-calculator.avif 288w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/9565c/cell-id-calculator.avif 575w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/6cb26/cell-id-calculator.avif 863w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/75fe3/cell-id-calculator.avif 1150w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/23c81/cell-id-calculator.avif 1312w" sizes="(max-width: 575px) 100vw, 575px" type="image/avif"><source srcset="https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/3506a/cell-id-calculator.webp 144w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/5f1c3/cell-id-calculator.webp 288w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/d2a82/cell-id-calculator.webp 575w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/754d3/cell-id-calculator.webp 863w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/e0a21/cell-id-calculator.webp 1150w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/a43fd/cell-id-calculator.webp 1312w" sizes="(max-width: 575px) 100vw, 575px" type="image/webp">
          <source srcset="https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/c4366/cell-id-calculator.png 144w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/672f0/cell-id-calculator.png 288w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/4c1b5/cell-id-calculator.png 575w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/e3d9b/cell-id-calculator.png 863w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/5c19b/cell-id-calculator.png 1150w,
https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/2c964/cell-id-calculator.png 1312w" sizes="(max-width: 575px) 100vw, 575px" type="image/png">
          <img src="https://mastdatabase.co.uk/static/0c46fc340693c4cba3fe4977ca63b5ee/4c1b5/cell-id-calculator.png" alt="cell id calculator" title="" loading="lazy" decoding="async">
        </picture>
  </a>
    </span></span><figcaption><p><a href="https://www.cellmapper.net/enbid?net=LTE&amp;cellid=128321395" rel="noopener">Cellmapper's cell ID calculator</a></p></figcaption></figure>
<p>Then we just have to search for that site on Cellmapper's map:</p>
<figure><span><span>
      <a href="https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/28810/cellmapper-sector.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/d5f3f/cellmapper-sector.avif 144w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/a1ffd/cellmapper-sector.avif 288w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/9565c/cellmapper-sector.avif 575w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/6cb26/cellmapper-sector.avif 863w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/75fe3/cellmapper-sector.avif 1150w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/9e767/cellmapper-sector.avif 2502w" sizes="(max-width: 575px) 100vw, 575px" type="image/avif"><source srcset="https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/3506a/cellmapper-sector.webp 144w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/5f1c3/cellmapper-sector.webp 288w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/d2a82/cellmapper-sector.webp 575w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/754d3/cellmapper-sector.webp 863w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/e0a21/cellmapper-sector.webp 1150w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/a9c27/cellmapper-sector.webp 2502w" sizes="(max-width: 575px) 100vw, 575px" type="image/webp">
          <source srcset="https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/c4366/cellmapper-sector.png 144w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/672f0/cellmapper-sector.png 288w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/4c1b5/cellmapper-sector.png 575w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/e3d9b/cellmapper-sector.png 863w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/5c19b/cellmapper-sector.png 1150w,
https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/28810/cellmapper-sector.png 2502w" sizes="(max-width: 575px) 100vw, 575px" type="image/png">
          <img src="https://mastdatabase.co.uk/static/c94b088bbfd53670b8e29124ec998380/4c1b5/cellmapper-sector.png" alt="cellmapper sector" title="" loading="lazy" decoding="async">
        </picture>
  </a>
    </span></span><figcaption><p><a href="https://www.cellmapper.net/map?MCC=234&amp;MNC=10&amp;type=LTE&amp;latitude=51.52666039304819&amp;longitude=-0.6526597015225719&amp;zoom=16&amp;showTowers=true&amp;showIcons=true&amp;showTowerLabels=true&amp;clusterEnabled=true&amp;tilesEnabled=true&amp;showOrphans=false&amp;showNoFrequencyOnly=false&amp;showFrequencyOnly=false&amp;showBandwidthOnly=false&amp;DateFilterType=Last&amp;showHex=false&amp;showVerifiedOnly=false&amp;showUnverifiedOnly=false&amp;showLTECAOnly=false&amp;showENDCOnly=false&amp;showBand=0&amp;showSectorColours=true&amp;mapType=roadmap&amp;darkMode=false&amp;imperialUnits=false" rel="noopener">O2 UK eNB 501255 on Cellmapper</a></p></figcaption></figure>
<p>Here, you can see the macro cell the user was on at the time of the call. While this has a relatively sizeable coverage area, shown by the
selected polygon with the blue outline, dense urban areas will make use of very many sites (such as small cells, which are often fitted directly
to streetlamps) with small coverage areas. Each site in these areas can often cover areas as small as 100m<sup>2</sup>. In a city, this becomes
an <em>extremely</em> accurate measure of location.</p>
<p>I also tested the attack with another O2 customer who was roaming abroad, and the attack worked perfectly with me being able to pinpoint them to
the city centre of Copenhagen, Denmark.</p>
<figure><span><span>
      <a href="https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/e93db/dk-site.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/d5f3f/dk-site.avif 144w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/a1ffd/dk-site.avif 288w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/9565c/dk-site.avif 575w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/6cb26/dk-site.avif 863w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/75fe3/dk-site.avif 1150w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/905d8/dk-site.avif 1708w" sizes="(max-width: 575px) 100vw, 575px" type="image/avif"><source srcset="https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/3506a/dk-site.webp 144w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/5f1c3/dk-site.webp 288w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/d2a82/dk-site.webp 575w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/754d3/dk-site.webp 863w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/e0a21/dk-site.webp 1150w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/69203/dk-site.webp 1708w" sizes="(max-width: 575px) 100vw, 575px" type="image/webp">
          <source srcset="https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/c4366/dk-site.png 144w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/672f0/dk-site.png 288w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/4c1b5/dk-site.png 575w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/e3d9b/dk-site.png 863w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/5c19b/dk-site.png 1150w,
https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/e93db/dk-site.png 1708w" sizes="(max-width: 575px) 100vw, 575px" type="image/png">
          <img src="https://mastdatabase.co.uk/static/ea5fb317370b28f32edf666f3f8a9b3e/4c1b5/dk-site.png" alt="dk site" title="" loading="lazy" decoding="async">
        </picture>
  </a>
    </span></span><figcaption>Location of eNB 107258 on 3DK, near Vesterport St., Copenhagen</figcaption></figure>
<p>I think it's important to note here that my device is <strong>in no way special</strong>. It's not doing anything odd to the network and hasn't behaved any
differently. All it is doing is allowing me to see the information being sent to it. This effectively means that every O2 device that is making a
phone call on IMS (4G Calling / WiFi Calling) is receiving information that can be used to trivially geolocate the recipient of the call.</p>
<h2 id="what-id-like-to-see-change"><a aria-label="what id like to see change permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#what-id-like-to-see-change"><span></span></a>What I'd like to see change</h2>
<p>O2 must remove the highlighted headers from all IMS / SIP messages to protect the privacy and safety of customers. It would be logical to also
disable debug headers, as I imagine a scenario where those unintentionally leak further information could occur. There is no reason for any
device outside of the network core to see those headers.</p>
<p>I'm extremely disappointed as an O2 customer to see a lack of any escalation route to report these kind of potential vectors for attack. EE, a
rival network, have a clear and well defined escalation route (see <a href="https://www.bt.com/about/contact-bt/responsible-disclosure" rel="noopener">https://www.bt.com/about/contact-bt/responsible-disclosure</a>) that I imagine
would have prevented the need for this information to have been publicly shared. I don't want to be the enemy, I simply want to feel comfortable
using my phone.</p>
<h2 id="conclusion"><a aria-label="conclusion permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#conclusion"><span></span></a>Conclusion</h2>
<p>Any O2 customer can be trivially located by an attacker with even a basic understanding of mobile networking.</p>
<p>There is also <strong>no way to prevent this attack</strong> as an O2 customer. Disabling 4G Calling does <em>not</em> prevent these headers from being revealed, and
if your device is ever unreachable these internal headers will still reveal the last cell you were connected to and how long ago this was.</p>
<p>Attempts were made to reach out to O2 via email (to both Lutz Schüler, CEO and <a href="https://mastdatabase.co.uk/cdn-cgi/l/email-protection#5526303620273c212c3c3b363c31303b212615233c27323c3b3830313c347b363a7b203e" rel="noopener"><span data-cfemail="1360767066617a676a7a7d707a77767d676053657a61747a7d7e76777a723d707c3d6678">[email&nbsp;protected]</span></a>) on the 26 and 27 March 2025
reporting this behaviour and privacy risk, but I have yet to get any response or see any change in the behaviour.</p>
<hr>
<h2 id="notes"><a aria-label="notes permalink" href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#notes"><span></span></a>Notes</h2>
<p><sup id="note-1"><a href="https://mastdatabase.co.uk/blog/2025/05/o2-expose-customer-location-call-4g/#note-1-link"><span>[1]</span></a></sup> <a href="https://www.engadget.com/2017-03-29-o2-wifi-4g-calling.html" rel="noopener">https://www.engadget.com/2017-03-29-o2-wifi-4g-calling.html</a></p>
<p>Article edited by David Wheatley.</p></div><hr><nav><a href="https://mastdatabase.co.uk/blog/1/">Back to article list</a></nav></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pyrefly: A new type checker and IDE experience for Python (158 pts)]]></title>
            <link>https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/</link>
            <guid>44013913</guid>
            <pubDate>Sat, 17 May 2025 12:47:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/">https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/</a>, See on <a href="https://news.ycombinator.com/item?id=44013913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<p><span>Today we are announcing an alpha version of </span><a href="https://pyrefly.org/" target="_blank" rel="noopener"><span>Pyrefly</span></a><span>, an open source Python type checker and IDE extension crafted in <a href="https://engineering.fb.com/2021/04/29/developer-tools/rust/" target="_blank" rel="noopener">Rust</a>. Pyrefly is a static type checker that analyzes Python code to ensure type consistency and help you catch errors throughout your codebase before your code runs. It also supports IDE integration and CLI usage to give you flexibility in how you incorporate it into your workflow.&nbsp;</span></p>
<p><span>The open source community is the backbone of the Python language. We are eager to collaborate on Pyrefly with the community and improve Python’s type system and the many libraries that we all rely on.&nbsp;&nbsp;</span></p>
<h2><span>Get started</span></h2>
<p><span>Ready to dive in? </span><a href="https://pyrefly.org/" target="_blank" rel="noopener"><span>The official Pyrefly website</span></a><span> has all the details, but to quickly get started:</span></p>
<ul>
<li aria-level="1"><a href="https://pyrefly.org/en/docs/installation/" target="_blank" rel="noopener"><span>Install</span></a><span> Pyrefly on the command-line: </span><span>pip install pyrefly</span><span>.</span></li>
<li aria-level="1"><span><a href="https://pyrefly.org/en/docs/migrating-to-pyrefly/" target="_blank" rel="noopener">Migrate your existing type checker configuration to Pyrefly</a>.</span></li>
<li aria-level="1"><span>Enhance Your IDE: Download the </span><a href="https://marketplace.visualstudio.com/items?itemName=meta.pyrefly" target="_blank" rel="noopener"><span>Pyrefly extension for VSCode</span></a><span> and enjoy a lightning fast IDE experience from starter projects to monorepos.</span></li>
<li aria-level="1"><span>Leave feedback for us on </span><a href="https://github.com/facebook/pyrefly/issues" target="_blank" rel="noopener"><span>GitHub</span></a><span>.</span></li>
</ul>
<h2><span>Why we built Pyrefly</span></h2>
<p><span>Back in 2017, we embarked on a mission to create a type checker that could handle </span><a href="https://instagram-engineering.com/web-service-efficiency-at-instagram-with-python-4976d078e366" target="_blank" rel="noopener"><span>Instagram’s massive codebase</span></a><span> of typed Python. This mission led to the birth of the </span><a href="https://github.com/facebook/pyre-check" target="_blank" rel="noopener"><span>Pyre</span></a><span> type checker, inspired by the robust designs of </span><a href="https://hacklang.org/" target="_blank" rel="noopener"><span>Hack</span></a><span> and </span><a href="https://flow.org/"><span>Flow</span></a><span>, and written in OCaml to deliver scalable performance.&nbsp;</span></p>
<p><span>Over the years, Pyre served us well, but as the type system evolved and the need for typechecking to drive responsive IDE emerged, it was clear that we needed to take a new approach. We explored alternate solutions and leveraged community tools like </span><a href="https://github.com/Microsoft/pyright" target="_blank" rel="noopener"><span>Pyright</span></a><span> for code navigation. But the need for an extensible type checker that can bring code navigation, checking at scale, and exporting types to other services drove us to start over, creating Pyrefly.&nbsp;</span></p>
<h2><span>The principles behind Pyrefly</span></h2>
<p><span>Today, we’re excited to unveil Pyrefly, a project <a href="https://github.com/facebook/pyrefly" target="_blank" rel="noopener">we’ve been developing openly on </a></span><span>GitHub</span><span>. We invite you to explore our work and try it out on your own project. While a project like Pyrefly is the sum of thousands of technical choices, a few notable principles we’ve followed are:</span></p>
<h3>Performance</h3>
<p><span>We want to shift checks that used to happen later on CI to happening on every single keystroke. That requires checking code at speed (on large codebases we can check 1.8 million lines of code per second!) and careful thought to incrementality and updates. Pyrefly is implemented in Rust and designed for high performance on codebases of all sizes.</span></p>
<h3>IDE first</h3>
<p><span>We want the IDE and command line to share a consistent view of the world, which means crafting abstractions that capture the differences without incurring unnecessary costs. Designing these abstractions from the beginning is much easier than retrofitting them, which we tried with Pyre.</span></p>
<h3>Inference</h3>
<p><span>Some </span><a href="https://engineering.fb.com/2024/12/09/developer-tools/typed-python-2024-survey-meta/" target="_blank" rel="noopener"><span>Python programs are typed</span></a><span>, but many aren’t. We want users to benefit from types even if they haven’t annotated their code – so automatically infer types for returns and local variables and display them in the IDE. What’s more, in the IDE you can even double click to insert these inferred types if you think that would make the program better.</span></p>
<h3>Open source</h3>
<p><span>Python is open source, and hugely popular. The </span><a href="https://typing.python.org/en/latest/spec/" target="_blank" rel="noopener"><span>Python typing specification</span></a><span> is open source, which made Pyrefly vastly easier to develop. Many of the libraries Meta contributes to are open source,( e.g., </span><a href="https://pytorch.org/" target="_blank" rel="noopener"><span>PyTorch</span></a><span>).</span></p>
<p><span>Pyrefly is also open source, </span><a href="https://github.com/facebook/pyrefly/" target="_blank" rel="noopener"><span>available on GitHub</span></a><span> under the </span><a href="https://github.com/facebook/pyrefly/blob/main/LICENSE" target="_blank" rel="noopener"><span>MIT license</span></a><span>, and we encourage </span><a href="https://github.com/facebook/pyrefly/pulls" target="_blank" rel="noopener"><span>pull requests</span></a><span> and </span><a href="https://github.com/facebook/pyrefly/issues" target="_blank" rel="noopener"><span>issue reports</span></a><span>. We also have a </span><a href="https://discord.gg/Cf7mFQtW7W" target="_blank" rel="noopener"><span>Discord channel</span></a><span> for more free flowing discussions. We would love to build a community around Pyrefly.</span></p>
<h2><span>The future of Pyrefly</span></h2>
<p><span>We will work with the Python community to drive the language forward and improve the developer experience. Since the beginning of Pyre, we open sourced our code and contributed a number of PEPs alongside the community of type checker maintainers. We feel we can do more with Pyrefly to help Python developers leverage the benefits of types for developers, library authors, and folks just learning the language.&nbsp;</span></p>
<p><span>Meta has leveraged types in dynamic languages from the beginning and knows the significant benefits it brings to developer productivity and security. We plan to share more of our learnings and tooling with </span><a href="https://engineering.fb.com/2024/12/09/developer-tools/typed-python-2024-survey-meta/" target="_blank" rel="noopener"><span>blogs</span></a><span>, better types in the ecosystem and language enhancements.&nbsp;</span></p>
<p><span>Today we’re releasing Pyrefly as an alpha. At the same time, we’re busy burning down the long-tail of bugs and features aiming to remove the alpha label this Summer. Your feedback is invaluable to get there, so please give it a try and </span><a href="https://github.com/facebook/pyrefly/issues" target="_blank" rel="noopener"><span>report your bugs</span></a><span> or things you think can be improved. Even if Pyrefly isn’t right for your project, we would love to hear how you use types and what you would like to see improved in your editor.</span></p>
<p><span>Join us on the journey as we help illuminate your bugs with Pyrefly. Happy coding! 🐍✨</span></p>
<h2><span>Hear more about Pyrefly&nbsp;</span></h2>
<p><span>Check out the <a href="https://engineering.fb.com/2025/05/15/developer-tools/open-sourcing-pyrefly-a-faster-python-type-checker-written-in-rust" target="_blank" rel="noopener">episode of the Meta Tech Podcast</a> where several team members share their experience developing Pyrefly and technical details for how it works. We also just </span><a href="https://us.pycon.org/2025/schedule/presentation/118/" target="_blank" rel="noopener"><span>talked at PyCon US</span></a><span> about high-performance Python through faster type checking and free threaded execution.</span></p>
<p><span>To learn more about Meta Open Source, visit our </span><a href="https://opensource.fb.com/" target="_blank" rel="noopener"><span>open source site</span></a><span>, subscribe to our </span><a href="https://www.youtube.com/channel/UCCQY962PmHabTjaHv2wJzfQ" target="_blank" rel="noopener"><span>YouTube channel</span></a><span>, or follow us on </span><a href="https://www.facebook.com/MetaOpenSource" target="_blank" rel="noopener"><span>Facebook</span></a><span>, </span><a href="https://www.threads.net/@metaopensource" target="_blank" rel="noopener"><span>Threads</span></a><span>, </span><a href="https://x.com/MetaOpenSource" target="_blank" rel="noopener"><span>X</span></a>,<span> and </span><a href="https://www.linkedin.com/showcase/meta-open-source?fbclid=IwZXh0bgNhZW0CMTEAAR2fEOJNb7zOi8rJeRvQry5sRxARpdL3OpS4sYLdC1_npkEy60gBS1ynXwQ_aem_mJUK6jEUApFTW75Emhtpqw" target="_blank" rel="noopener"><span>LinkedIn</span></a><span>.</span></p>
<h2><span>Acknowledgements&nbsp;</span></h2>
<p><i><span>Pyrefly was created By Meta’s Python Language Tooling Team: Jia Chen, Rebecca Chen, Sam Goldman, David Luo, Kyle Into, Zeina Migeed, Neil Mitchell, Maggie Moss, Conner Nilsen, Aaron Pollack, Teddy Sudol, Steven Troxler, Lucian Wischik, Danny Yang, and Sam Zhou.</span></i></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Push Ifs Up and Fors Down (362 pts)]]></title>
            <link>https://matklad.github.io/2023/11/15/push-ifs-up-and-fors-down.html</link>
            <guid>44013157</guid>
            <pubDate>Sat, 17 May 2025 09:31:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2023/11/15/push-ifs-up-and-fors-down.html">https://matklad.github.io/2023/11/15/push-ifs-up-and-fors-down.html</a>, See on <a href="https://news.ycombinator.com/item?id=44013157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <h2>
          Push Ifs Up And Fors Down <time datetime="2023-11-15">Nov 15, 2023</time>
        </h2>
        <p>A short note on two related rules of thumb.</p>
        <section id="Push-Ifs-Up">
          
          <p>
            If there’s an <code>if</code> condition inside a function, consider
            if it could be moved to the caller instead:
          </p>

          <figure>
            <pre><code><span><span>// GOOD</span></span>
<span><span>fn</span> <span>frobnicate</span>(walrus: Walrus) {</span>
<span>    ...</span>
<span>}</span>
<span></span>
<span><span>// BAD</span></span>
<span><span>fn</span> <span>frobnicate</span>(walrus: <span>Option</span>&lt;Walrus&gt;) {</span>
<span>  <span>let</span> <span>walrus</span> = <span>match</span> walrus {</span>
<span>    <span>Some</span>(it) =&gt; it,</span>
<span>    <span>None</span> =&gt; <span>return</span>,</span>
<span>  };</span>
<span>  ...</span>
<span>}</span></code></pre>
          </figure>
          <p>
            As in the example above, this often comes up with preconditions: a
            function might check precondition inside and “do nothing” if it
            doesn’t hold, or it could push the task of precondition checking to
            its caller, and enforce via types (or an assert) that the
            precondition holds. With preconditions especially, “pushing up” can
            become viral, and result in fewer checks overall, which is one
            motivation for this rule of thumb.
          </p>
          <p>
            Another motivation is that control flow and <code>if</code>s are
            complicated, and are a source of bugs. By pushing <code>if</code>s
            up, you often end up centralizing control flow in a single function,
            which has a complex branching logic, but all the actual work is
            delegated to straight line subroutines.
          </p>
          <p>
            <em>If</em> you have complex control flow, better to fit it on a
            screen in a single function, rather than spread throughout the file.
            What’s more, with all the flow in one place it often is possible to
            notice redundancies and dead conditions. Compare:
          </p>

          <figure>
            <pre><code><span><span>fn</span> <span>f</span>() {</span>
<span>  <span>if</span> foo &amp;&amp; bar {</span>
<span>    <span>if</span> foo {</span>
<span></span>
<span>    } <span>else</span> {</span>
<span></span>
<span>    }</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>g</span>() {</span>
<span>  <span>if</span> foo &amp;&amp; bar {</span>
<span>    <span>h</span>()</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>h</span>() {</span>
<span>  <span>if</span> foo {</span>
<span></span>
<span>  } <span>else</span> {</span>
<span></span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            For <code>f</code>, it’s much easier to notice a dead branch than
            for a combination of <code>g</code> and <code>h</code>!
          </p>
          <p>
            A related pattern here is what I call “dissolving enum” refactor.
            Sometimes, the code ends up looking like this:
          </p>

          <figure>
            <pre><code><span><span>enum</span> <span>E</span> {</span>
<span>  <span>Foo</span>(<span>i32</span>),</span>
<span>  <span>Bar</span>(<span>String</span>),</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>main</span>() {</span>
<span>  <span>let</span> <span>e</span> = <span>f</span>();</span>
<span>  <span>g</span>(e)</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>f</span>() <span>-&gt;</span> E {</span>
<span>  <span>if</span> condition {</span>
<span>    E::<span>Foo</span>(x)</span>
<span>  } <span>else</span> {</span>
<span>    E::<span>Bar</span>(y)</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>g</span>(e: E) {</span>
<span>  <span>match</span> e {</span>
<span>    E::<span>Foo</span>(x) =&gt; <span>foo</span>(x),</span>
<span>    E::<span>Bar</span>(y) =&gt; <span>bar</span>(y)</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            There are two branching instructions here and, by pulling them up,
            it becomes apparent that it is the exact same condition, triplicated
            (the third time reified as a data structure):
          </p>

          <figure>
            <pre><code><span><span>fn</span> <span>main</span>() {</span>
<span>  <span>if</span> condition {</span>
<span>    <span>foo</span>(x)</span>
<span>  } <span>else</span> {</span>
<span>    <span>bar</span>(y)</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
        </section>
        <section id="Push-Fors-Down">
          <h2>
            <a href="#Push-Fors-Down">Push Fors Down </a>
          </h2>
          <p>
            This comes from data oriented school of thought. Few things are few,
            many things are many. Programs usually operate with bunches of
            objects. Or at least the hot path usually involves handling many
            entities. It is the volume of entities that makes the path hot in
            the first place. So it often is prudent to introduce a concept of a
            “batch” of objects, and make operations on batches the base case,
            with a scalar version being a special case of a batched ones:
          </p>

          <figure>
            <pre><code><span><span>// GOOD</span></span>
<span><span>frobnicate_batch</span>(walruses)</span>
<span></span>
<span><span>// BAD</span></span>
<span><span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>  <span>frobnicate</span>(walrus)</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The primary benefit here is performance. Plenty of performance, <a href="http://venge.net/graydon/talks/VectorizedInterpretersTalk-2023-05-12.pdf">in extreme cases</a>.
          </p>
          <p>
            If you have a whole batch of things to work with, you can amortize
            startup cost and be flexible about the order you process things. In
            fact, you don’t even need to process entities in any particular
            order, you can do vectorized/struct-of-array tricks to process one
            field of all entities first, before continuing with other fields.
          </p>
          <p>
            Perhaps the most fun example here is <a href="https://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm">FFT-based polynomial multiplication</a>: turns out, evaluating a
            polynomial at a bunch of points simultaneously could be done faster
            than a bunch of individual point evaluations!
          </p>
          <p>
            The two pieces of advice about <code>for</code>s and <code>if</code>s even compose!
          </p>

          <figure>
            <pre><code><span><span>// GOOD</span></span>
<span><span>if</span> condition {</span>
<span>  <span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>    walrus.<span>frobnicate</span>()</span>
<span>  }</span>
<span>} <span>else</span> {</span>
<span>  <span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>    walrus.<span>transmogrify</span>()</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>// BAD</span></span>
<span><span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>  <span>if</span> condition {</span>
<span>    walrus.<span>frobnicate</span>()</span>
<span>  } <span>else</span> {</span>
<span>    walrus.<span>transmogrify</span>()</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The <code>GOOD</code> version is good, because it avoids repeatedly
            re-evaluating <code>condition</code>, removes a branch from the hot
            loop, and potentially unlocks vectorization. This pattern works on a
            micro level and on a macro level — the good version is the
            architecture of TigerBeetle, where in the data plane we operate on
            batches of objects at the same time, to amortize the cost of
            decision making in the control plane.
          </p>
          <p>
            While performance is perhaps the primary motivation for the <code>for</code> advice, sometimes it helps with expressiveness as well.
            <code>jQuery</code> was quite successful back in the day, and it
            operates on collections of elements. The language of abstract vector
            spaces is often a better tool for thought than bunches of
            coordinate-wise equations.
          </p>
          <p>
            To sum up, push the <code>if</code>s up and the <code>for</code>s
            down!
          </p>
        </section>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Catalog of Novel Operating Systems (151 pts)]]></title>
            <link>https://github.com/prathyvsh/os-catalog</link>
            <guid>44012615</guid>
            <pubDate>Sat, 17 May 2025 07:19:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/prathyvsh/os-catalog">https://github.com/prathyvsh/os-catalog</a>, See on <a href="https://news.ycombinator.com/item?id=44012615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Novel Operating Systems Catalog</h2><a id="user-content-novel-operating-systems-catalog" aria-label="Permalink: Novel Operating Systems Catalog" href="#novel-operating-systems-catalog"></a></p>
<p dir="auto">Catalogue of novel operating systems</p>
<p dir="auto">Somewhere after the fall in popularity of note-taking apps, perhaps recognizing that just note-taking is not enough and the deafening hype of LLMs, there was a sweet period of lull when a lot of people started boldly building new operating systems. This is a catalogue of such operating systems that I have come across. In the past, before the commercialization of computers, we had a plethora of operating systems with unique languages to interact with computers, like AmigaOS, Symbolics, SunOS, MULTICS, Burroughs, Meneut, BeOS PARC, Star, Oberon, Plan9, NeXTSTEP, OS/2, PL/8, Inferno, QNX, RISCOS etc. This spirit can only be glimpsed in pockets, and kudos to all those who keep the fire alive!</p>
<p dir="auto">A thread on it <a href="https://x.com/Prabros/status/1922915943631523899" rel="nofollow">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://100r.co/site/uxn.html" rel="nofollow">UXN</a></h2><a id="user-content-uxn" aria-label="Permalink: UXN" href="#uxn"></a></p>
<p dir="auto">Perhaps the best one to start off this catalog is the UXN/Varvara personal computing stack of <a href="https://100r.co/" rel="nofollow">100 Rabbits</a>. Such a great couple with such a radical vision!</p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/UXN%20logo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/uxn-logo.jpg" alt="./img/uxn-logo.jpg"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/UXN%20screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/uxn-screenshot.jpg" alt="./img/uxn-screenshot.jpg"></a></p>
<p dir="auto">They have documented their rationale in these two documents:</p>
<ul dir="auto">
  <li><a href="https://100r.co/site/tools_ecosystem.html" rel="nofollow">Tools Ecosystem</a></li>
  <li><a href="https://100r.co/site/weathering_software_winter.html" rel="nofollow">Weathering Software Winter</a></li>
</ul>
<p dir="auto">Documents related to UXN can be obtained here: <a href="https://github.com/hundredrabbits/awesome-uxn?tab=readme-ov-file">https://github.com/hundredrabbits/awesome-uxn?tab=readme-ov-file</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://playb.it/" rel="nofollow">Playbit</a></h2><a id="user-content-playbit" aria-label="Permalink: Playbit" href="#playbit"></a></p>
<p dir="auto">Daring effort from <a href="https://github.com/rsms">Rasmus Andersson</a> and team to reinvent the computer stack.</p>
<p dir="auto">And alpha version available <a href="https://playb.it/alpha/" rel="nofollow">here</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/playbit-screenshot.webp"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/playbit-screenshot.webp" alt="Screenshot of Playbit"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://folk.computer/" rel="nofollow">Folk.computer</a></h2><a id="user-content-folkcomputer" aria-label="Permalink: Folk.computer" href="#folkcomputer"></a></p>
<p dir="auto">Omar Rizwan and Andreas Cuérvo</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/folk-computer.gif"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/folk-computer.gif" alt="Video of with Folk.computer" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://nette.io/" rel="nofollow">Nette.io</a></h2><a id="user-content-netteio" aria-label="Permalink: Nette.io" href="#netteio"></a></p>
<p dir="auto">Nette.io by <a href="https://github.com/qazwsxpawel">Pawel Ceranka</a> positions itself as a research OS for the web.</p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Nette%20website%20screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/nette.png" alt="./img/nette.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/mntmn/interim">Interim</a></h2><a id="user-content-interim" aria-label="Permalink: Interim" href="#interim"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Interim%20Logo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/interim-logo.png" alt="./img/interim-logo.png"></a></p>
<p dir="auto">Something about Lisp draws people into construct OSes from ground up. Perhaps it is the simplicity of the language that acts as the foundation. Here‘s Interim, one of our favourite minimal OSes constructed with Lisp.</p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Interim%20Screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/interim-screenshot.jpg" alt="./img/interim-screenshot.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/froggey/Mezzano">Mezzano</a></h2><a id="user-content-mezzano" aria-label="Permalink: Mezzano" href="#mezzano"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Mezzano%20Screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/mezzano.png" alt="./img/mezzano.png"></a></p>
<p dir="auto">An OS written in CommonLisp</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/vygr/ChrysaLisp">ChrysalLisp</a></h2><a id="user-content-chrysallisp" aria-label="Permalink: ChrysalLisp" href="#chrysallisp"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/ChrysaLisp%20screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/chrysalisp-screenshot.png" alt="./img/chrysalisp-screenshot.png"></a></p>
<p dir="auto">ChrysaLisp is amulti-threaded, multi-core, multi-user parallel OS with features such as a GUI, terminal, OO Assembler, class libraries, C-Script compiler, Lisp interpreter, debugger, profiler, vector font engine, and more.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://ravynos.com/" rel="nofollow">RayvnOS</a></h2><a id="user-content-rayvnos" aria-label="Permalink: RayvnOS" href="#rayvnos"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://www.redox-os.org/" rel="nofollow">RedoxOS</a></h2><a id="user-content-redoxos" aria-label="Permalink: RedoxOS" href="#redoxos"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Ideas</h2><a id="user-content-ideas" aria-label="Permalink: Ideas" href="#ideas"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://desktopneo.com/" rel="nofollow">DesktopNeo</a></h2><a id="user-content-desktopneo" aria-label="Permalink: DesktopNeo" href="#desktopneo"></a></p>
<p dir="auto">DesktopNeo, a rethinking of the desktop interface by <a href="https://www.lennartziburski.com/" rel="nofollow">Lennart Ziburski</a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Screenshot%20of%20Desktop%20Neo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/desktop-neo.jpg" alt="./img/desktop-neo.jpg"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Another%20screenshot%20of%20Desktop%20Neo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/desktop-neo-screenshot.png" alt="./img/desktop-neo-screenshot.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://www.mercuryos.com/" rel="nofollow">MercuryOS</a></h2><a id="user-content-mercuryos" aria-label="Permalink: MercuryOS" href="#mercuryos"></a></p>
<p dir="auto">MercuryOS by Jason Yuan is an interesting rethink of the OS based on intensions:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/mercury-screenshot.png"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/mercury-screenshot.png" alt="./img/mercury-screenshot.png"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/mercury-dark-mode.png"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/mercury-dark-mode.png" alt="./img/mercury-dark-mode.png"></a></p>
<p dir="auto">Prototype by Rauno Freiberg. <a href="https://github.com/prathyvsh/os-catalog/blob/main/Source">https://x.com/raunofreiberg/status/1666122499401166873</a></p>

<p dir="auto">The team seems to be working on MercuryOS → Makespace.fun → <a href="https://new.computer/" rel="nofollow">New.computer</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://x.com/getFreezeframe" rel="nofollow">Freeze.app</a></h2><a id="user-content-freezeapp" aria-label="Permalink: Freeze.app" href="#freezeapp"></a></p>
<p dir="auto">Freeze the desktop interface and then thaw it at will: <a href="https://x.com/getFreezeframe/status/1358805285393948673" rel="nofollow">https://x.com/getFreezeframe/status/1358805285393948673</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://azlen.me/stories/worm-os/" rel="nofollow">WormOS</a></h2><a id="user-content-wormos" aria-label="Permalink: WormOS" href="#wormos"></a></p>
<p dir="auto">Interesting article on partitioned rooms by mental space with little bubbles on the edges that act as wormholes into things you want to achieve.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/wormos.png"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/wormos.png" alt="./img/wormos.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status Unknown</h2><a id="user-content-status-unknown" aria-label="Permalink: Status Unknown" href="#status-unknown"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://bedrock.computer/" rel="nofollow">Bedrock.computer</a></h2><a id="user-content-bedrockcomputer" aria-label="Permalink: Bedrock.computer" href="#bedrockcomputer"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other lists</h2><a id="user-content-other-lists" aria-label="Permalink: Other lists" href="#other-lists"></a></p>
<ul dir="auto">
  <li><a href="https://github.com/jubalh/awesome-os">AwesomeOS by @jubalh</a></li>
  <li><a href="https://1.anagora.org/node/os" rel="nofollow">Anagora List</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JavaScript's New Superpower: Explicit Resource Management (292 pts)]]></title>
            <link>https://v8.dev/features/explicit-resource-management</link>
            <guid>44012227</guid>
            <pubDate>Sat, 17 May 2025 05:23:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://v8.dev/features/explicit-resource-management">https://v8.dev/features/explicit-resource-management</a>, See on <a href="https://news.ycombinator.com/item?id=44012227">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p>The <em>Explicit Resource Management</em> proposal introduces a deterministic approach to explicitly manage the lifecycle of resources like file handles, network connections, and more. This proposal brings the following additions to the language: the <code>using</code> and <code>await using</code> declarations, which automatically calls dispose method when a resource goes out of scope; <code>[Symbol.dispose]()</code> and <code>[Symbol.asyncDispose]()</code> symbols for cleanup operations; two new global objects <code>DisposableStack</code> and <code>AsyncDisposableStack</code> as containers to aggregate disposable resources; and <code>SuppressedError</code> as a new type of error (contain both the error that was most recently thrown, as well as the error that was suppressed) to address the scenario where an error occurs during the disposal of a resource, and potientially masking an existing error thrown from the body, or from the disposal of another resource. These additions enable developers to write more robust, performant, and maintainable code by providing fine-grained control over resource disposal.</p><h2 id="using-and-await-using-declarations" tabindex="-1"><code>using</code> and <code>await using</code> declarations <a href="#using-and-await-using-declarations">#</a></h2><p>The core of the Explicit Resource Management proposal lies in the <code>using</code> and <code>await using</code> declarations. The <code>using</code> declaration is designed for synchronous resources, ensuring that the <code>[Symbol.dispose]()</code> method of a disposable resource is called when the scope in which it's declared exits. For asynchronous resources, the <code>await using</code> declaration works similarly, but ensures that the <code>[Symbol.asyncDispose]()</code> method is called and the result of this calling is awaited, allowing for asynchronous cleanup operations. This distinction enables developers to reliably manage both synchronous and asynchronous resources, preventing leaks and improving overall code quality. The <code>using</code> and <code>await using</code> keywords can be used inside braces <code>{}</code> (such as blocks, for loops and function bodies), and cannot be used in top-levels.</p><p>For example, when working with <a href="https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamDefaultReader"><code>ReadableStreamDefaultReader</code></a>, it's crucial to call <code>reader.releaseLock()</code> to unlock the stream and allow it to be used elsewhere. However, error handling introduces a common problem: if an error occurs during the reading process, and you forget to call <code>releaseLock()</code> before the error propagates, the stream remains locked. Let's start with a naive example:</p><pre><code><span>let</span> responsePromise <span>=</span> <span>null</span><span>;</span><p><span>async</span> <span>function</span> <span>readFile</span><span>(</span><span>url</span><span>)</span> <span>{</span>  <br>    <span>if</span> <span>(</span><span>!</span>responsePromise<span>)</span> <span>{</span><br>        <span>// Only fetch if we don't have a promise yet</span><br>        responsePromise <span>=</span> <span>fetch</span><span>(</span>url<span>)</span><span>;</span><br>    <span>}</span><br>    <span>const</span> response <span>=</span> <span>await</span> responsePromise<span>;</span><br>    <span>if</span> <span>(</span><span>!</span>response<span>.</span>ok<span>)</span> <span>{</span><br>      <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span><span>`</span><span>HTTP error! status: </span><span><span>${</span>response<span>.</span>status<span>}</span></span><span>`</span></span><span>)</span><span>;</span><br>    <span>}</span><br>    <span>const</span> processedData <span>=</span> <span>await</span> <span>processData</span><span>(</span>response<span>)</span><span>;</span></p><p>    <span>// Do something with processedData</span><br>    <span>...</span><br> <span>}</span></p><p><span>async</span> <span>function</span> <span>processData</span><span>(</span><span>response</span><span>)</span> <span>{</span><br>    <span>const</span> reader <span>=</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>;</span><br>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><br>    <span>let</span> processedData<span>;</span></p><p>        <span>while</span> <span>(</span><span>!</span>done<span>)</span> <span>{</span><br>        <span>(</span><span>{</span> done<span>,</span> value <span>}</span> <span>=</span> <span>await</span> reader<span>.</span><span>read</span><span>(</span><span>)</span><span>)</span><span>;</span><br>        <span>if</span> <span>(</span>value<span>)</span> <span>{</span><br>            <span>// Process data and save the result in processedData</span><br>            <span>...</span><br>            <span>// An error is thrown here!</span><br>        <span>}</span><br>    <span>}</span></p><p>        <span>// Because the error is thrown before this line, the stream remains locked.</span><br>    reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span></p><p>    <span>return</span> processedData<span>;</span><br>  <span>}</span></p><p>  <span>readFile</span><span>(</span><span>'https://example.com/largefile.dat'</span><span>)</span><span>;</span></p></code></pre><p>So it is crucial for developers to have <code>try...finally</code> block while using streams and put <code>reader.releaseLock()</code> in <code>finally</code>. This pattern ensures that <code>reader.releaseLock()</code> is always called.</p><pre><code><span>async</span> <span>function</span> <span>processData</span><span>(</span><span>response</span><span>)</span> <span>{</span><br>    <span>const</span> reader <span>=</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>;</span><br>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><br>    <span>let</span> processedData<span>;</span><p>        <span>try</span> <span>{</span><br>        <span>while</span> <span>(</span><span>!</span>done<span>)</span> <span>{</span><br>            <span>(</span><span>{</span> done<span>,</span> value <span>}</span> <span>=</span> <span>await</span> reader<span>.</span><span>read</span><span>(</span><span>)</span><span>)</span><span>;</span><br>            <span>if</span> <span>(</span>value<span>)</span> <span>{</span><br>                <span>// Process data and save the result in processedData</span><br>                <span>...</span><br>                <span>// An error is thrown here!</span><br>            <span>}</span><br>        <span>}</span><br>    <span>}</span> <span>finally</span> <span>{</span><br>        <span>// The reader's lock on the stream will be always released.</span><br>        reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>    <span>}</span></p><p>    <span>return</span> processedData<span>;</span><br>  <span>}</span></p><p>  <span>readFile</span><span>(</span><span>'https://example.com/largefile.dat'</span><span>)</span><span>;</span></p></code></pre><p>An alternative to write this code is to create a disposable object <code>readerResource</code>, which has the reader (<code>response.body.getReader()</code>) and the <code>[Symbol.dispose]()</code> method that calls <code>this.reader.releaseLock()</code>. The <code>using</code> declaration ensures that <code>readerResource[Symbol.dispose]()</code> is called when the code block exits, and remembering to call <code>releaseLock</code> is no longer needed because the using declaration handles it. Integration of <code>[Symbol.dispose]</code> and <code>[Symbol.asyncDispose]</code> in web APIs like streams may happen in the future, so developers do not have to write the manual wrapper object.</p><pre><code> <span>async</span> <span>function</span> <span>processData</span><span>(</span><span>response</span><span>)</span> <span>{</span><br>    <span>const</span> reader <span>=</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>;</span><br>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><p>    <span>// Wrap the reader in a disposable resource</span><br>    using readerResource <span>=</span> <span>{</span><br>        <span>reader</span><span>:</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span><br>        <span>[</span>Symbol<span>.</span>dispose<span>]</span><span>(</span><span>)</span> <span>{</span><br>            <span>this</span><span>.</span>reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>        <span>}</span><span>,</span><br>    <span>}</span><span>;</span><br>    <span>const</span> <span>{</span> reader <span>}</span> <span>=</span> readerResource<span>;</span></p><p>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><br>    <span>let</span> processedData<span>;</span><br>    <span>while</span> <span>(</span><span>!</span>done<span>)</span> <span>{</span><br>        <span>(</span><span>{</span> done<span>,</span> value <span>}</span> <span>=</span> <span>await</span> reader<span>.</span><span>read</span><span>(</span><span>)</span><span>)</span><span>;</span><br>        <span>if</span> <span>(</span>value<span>)</span> <span>{</span><br>            <span>// Process data and save the result in processedData</span><br>            <span>...</span><br>            <span>// An error is thrown here!</span><br>        <span>}</span><br>    <span>}</span><br>    <span>return</span> processedData<span>;</span><br>  <span>}</span><br> <span>// readerResource[Symbol.dispose]() is called automatically.</span></p><p> <span>readFile</span><span>(</span><span>'https://example.com/largefile.dat'</span><span>)</span><span>;</span></p></code></pre><h2 id="disposablestack-and-asyncdisposablestack" tabindex="-1"><code>DisposableStack</code> and <code>AsyncDisposableStack</code> <a href="#disposablestack-and-asyncdisposablestack">#</a></h2><p>To further facilitate managing multiple disposable resources, the proposal introduces <code>DisposableStack</code> and <code>AsyncDisposableStack</code>. These stack-based structures allow developers to group and dispose of multiple resources in a coordinated manner. Resources are added to the stack, and when the stack is disposed, either synchronously or asynchronously, the resources are disposed of in the reverse order they were added, ensuring that any dependencies between them are handled correctly. This simplifies the cleanup process when dealing with complex scenarios involving multiple related resources. Both structures provide methods like <code>use()</code>, <code>adopt()</code>, and <code>defer()</code> to add resources or disposal actions, and a <code>dispose()</code> or <code>asyncDispose()</code> method to trigger the cleanup. <code>DisposableStack</code> and <code>AsyncDisposableStack</code> have <code>[Symbol.dispose]()</code> and <code>[Symbol.asyncDispose]()</code>, respectively, so they can be used with <code>using</code> and <code>await using</code> keywords. They offer a robust way to manage the disposal of multiple resources within a defined scope.</p><p>Let’s take a look at each method and see an example of it:</p><p><code>use(value)</code> adds a resource to the top of the stack.</p><pre><code><span>{</span><br>    <span>const</span> readerResource <span>=</span> <span>{</span><br>        <span>reader</span><span>:</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span><br>        <span>[</span>Symbol<span>.</span>dispose<span>]</span><span>(</span><span>)</span> <span>{</span><br>            <span>this</span><span>.</span>reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>            console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>        <span>}</span><span>,</span><br>    <span>}</span><span>;</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>use</span><span>(</span>readerResource<span>)</span><span>;</span><br><span>}</span><br><span>// Reader lock released.</span></code></pre><p><code>adopt(value, onDispose)</code> adds a non-disposable resource and a disposal callback to the top of the stack.</p><pre><code><span>{</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>adopt</span><span>(</span><br>      response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span> reader <span>=</span> <span>&gt;</span> <span>{</span><br>        reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>        console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>      <span>}</span><span>)</span><span>;</span><br><span>}</span><br><span>// Reader lock released.</span></code></pre><p><code>defer(onDispose)</code> adds a disposal callback to the top of the stack. It's useful for adding cleanup actions that don't have an associated resource.</p><pre><code><span>{</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>defer</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> console<span>.</span><span>log</span><span>(</span><span>"done."</span><span>)</span><span>)</span><span>;</span><br><span>}</span><br><span>// done.</span></code></pre><p><code>move()</code> moves all resources currently in this stack into a new <code>DisposableStack</code>. This can be useful if you need to transfer ownership of resources to another part of your code.</p><pre><code><span>{</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>adopt</span><span>(</span><br>      response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span> reader <span>=</span> <span>&gt;</span> <span>{</span><br>        reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>        console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>      <span>}</span><span>)</span><span>;</span><br>    using newStack <span>=</span> stack<span>.</span><span>move</span><span>(</span><span>)</span><span>;</span><br><span>}</span><br><span>// Here just the newStack exists and the resource inside it will be disposed.</span><br><span>// Reader lock released.</span></code></pre><p><code>dispose()</code> in DisposableStack and <code>asyncDispose()</code> in AsyncDisposableStack dispose the resources within this object.</p><pre><code><span>{</span><br>    <span>const</span> readerResource <span>=</span> <span>{</span><br>        <span>reader</span><span>:</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span><br>        <span>[</span>Symbol<span>.</span>dispose<span>]</span><span>(</span><span>)</span> <span>{</span><br>            <span>this</span><span>.</span>reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>            console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>        <span>}</span><span>,</span><br>    <span>}</span><span>;</span><br>    <span>let</span> stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>use</span><span>(</span>readerResource<span>)</span><span>;</span><br>    stack<span>.</span><span>dispose</span><span>(</span><span>)</span><span>;</span><br><span>}</span><br><span>// Reader lock released.</span></code></pre><h2 id="availability" tabindex="-1">Availability <a href="#availability">#</a></h2><p>Explicit Resource Management is shipped in Chromium 134 and V8 v13.8.</p><ul><li><a href="https://chromestatus.com/feature/5071680358842368"><span>Chrome:</span> <span>supported since version <span>134</span></span></a></li><li><a href="https://v8.dev/features/(nightly)"><span>Firefox:</span> <span>supported since version <span>134</span></span></a></li><li><a href="https://bugs.webkit.org/show_bug.cgi?id=248707"><span>Safari:</span> <span>no support</span></a></li><li><span>Node.js:</span> <span>no support</span></li><li><a href="https://github.com/zloirock/core-js#explicit-resource-management"><span>Babel:</span> <span>supported</span></a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A kernel developer plays with Home Assistant (138 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/</link>
            <guid>44011669</guid>
            <pubDate>Sat, 17 May 2025 02:50:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/">https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/</a>, See on <a href="https://news.ycombinator.com/item?id=44011669">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
Those of us who have spent our lives playing with computers naturally see
the appeal of deploying them though the home for both data acquisition and
automation.  But many of us who have watched the evolution of the
technology industry are increasingly unwilling to entrust critical
household functions to cloud-based servers run by companies that may not
have our best interests at heart.  The Apache-licensed <a href="https://www.home-assistant.io/">Home Assistant</a> project offers a
welcome alternative: locally controlled automation with free software.
This two-part series covers roughly a year of Home Assistant use, starting
with a set of overall observations about the project.
</p><p>
This is not the first time that LWN has looked at this project, of course;
<a href="https://lwn.net/Articles/822350/">this review</a> gives a snapshot of what Home
Assistant looked like five years ago, while <a href="https://lwn.net/Articles/947843/">this 2023 article</a> gives a good overview of the
project's history, governance, and overall direction.  I will endeavor to
not duplicate that material here.
</p><h4>Project health</h4>
<p>
At a first glance, Home Assistant bears some of the hallmarks of a
company-owned project.  The company in question, <a href="https://www.nabucasa.com/">Nabu Casa</a>, was formed around the
project and employs a number of its key developers.  One of the ways in
which the company makes money is with a $65/year subscription service, providing
remote access to Home Assistant servers installed on firewalled residential
networks.  Home Assistant has support for that remote option, and no
others.  It would be interesting to see what would happen to a pull request
adding support for, say, <a href="https://opensprinklershop.de/en/2023/01/22/opensprinkler-fernzugriff-mit-openthings-cloud-otc-token/">OpenThings
Cloud</a> as an alternative.  The fate of that request would say a lot
about how open the project really is.
</p><blockquote>
<b>No slop, all substance: subscribe to LWN today</b>
<p>
LWN has always been about quality over quantity; we need your help
to continue publishing in-depth, reader-focused articles about Linux
and the free-software community. Please subscribe today to support our work
and keep LWN on the air; we are offering <a href="https://lwn.net/Promo/no-slop/claim">a free one-month trial subscription</a> to get you started.
</p></blockquote>
<p>
(For the record, I have bought the Nabu Casa subscription rather than, say,
using WireGuard to make a port available on an accessible system; it is a
hassle-free way to solve the problem and support the development of this
software).
</p><p>
That said, most of the warning signs that accompany a corporate-controlled
project are not present with Home Assistant.  The project's <a href="https://github.com/home-assistant/core/blob/dev/CLA.md">contributor
license agreement</a> is a derivative of the kernel's developer certificate
of origin; contributors retain their copyright on their work.  Since the <a href="https://www.home-assistant.io/blog/2024/04/03/release-20244/">2024.4
release</a>, the Home Assistant core repository has acquired over 17,000
changesets from over 900 contributors.  While a number of Nabu Casa
employees (helpfully listed on <a href="https://www.nabucasa.com/about/">this page</a>) appear in the top ten
contributors, they do not dominate that list.
</p><p>
Home Assistant is clearly an active project with a wide developer base.  In
2024, overall responsibility for this project was transferred to <a href="https://www.openhomefoundation.org/blog/announcing-the-open-home-foundation/">the
newly created Open Home Foundation</a>.  This project is probably here to
stay, and seems unlikely to take a hostile turn in the future.  For a
system that sits at the core of one's home, those are important
characteristics.
</p><h4>Installation and setup</h4>
<p>
Linux users tend to be somewhat spoiled; installing a new application is
typically a matter of a single package-manager command.  Home Assistant
does not really fit into that model.  The first three options on <a href="https://www.home-assistant.io/installation/">the installation
page</a> involve dedicated computers — two of which are sold by Nabu Casa.
For those wanting to install it on a general-purpose computer, the
recommended course is to install the <a href="https://github.com/home-assistant/operating-system">Home Assistant
Operating System</a>, a bespoke Linux distribution that runs Home Assistant
within a Docker container.  There is also a container-based method that can
run on another distribution, but this installation does not support <a href="https://www.home-assistant.io/addons/">the add-ons feature</a>.
</p><p>
Home Assistant, in other words, is not really set up to be just another
application on a Linux system.  If one scrolls far enough, though, one will
find, the instructions to install onto a "normal" Linux system, suitably
guarded with warnings about how it is an "<q>advanced</q>" method.
Of course, that is what I did, putting the software onto an existing system
running Fedora.  The whole thing subsequently broke when a
distribution upgrade replaced Python, but that was easily enough repaired.
As a whole, the installation has worked as expected.
</p><p>
Out of the box, though, a new Home Assistant installation does not do much.
Its job, after all, is to interface with the systems throughout the house,
and every house is different.  While Home Assistant can find some systems
automatically (it found the Brother printer and dutifully informed me that
the device was, inevitably, low on cyan toner), it usually needs to be
told about what is installed in the house.  Thus, the user quickly delves
into the world of "integrations" — the device drivers of Home Assistant.
</p><p>
For each remotely accessible device in the house, there is, hopefully, at
least one integration available that allows Home Assistant to work with it.
Many integrations are packaged with the system itself, and can be found by
way of a simple search screen in the Home Assistant web interface.  A much
larger set is packaged separately, usually in the <a href="https://www.hacs.xyz/">Home Assistant Community Store</a>, or HACS;
it is fair to say that most users will end up getting at least some
integrations from this source.  Setting up HACS requires a few steps and,
unfortunately, requires the user to have a GitHub account for full
integration.  It <i>is</i> possible to install HACS integrations without
that account, but it is a manual process that loses support for features
like update tracking.
</p><p>
Most integrations, at setup time, will discover any of the appropriate
devices on the network — if those devices support that sort of discovery,
of course.  Often, using an integration will require the credentials to log
into the cloud account provided by the vendor of the devices in question.
When possible, integrations mostly strive to operate entirely locally; some
only use the cloud connection for the initial device discovery.  When there
is no alternative, though, integrations will remain logged into the cloud
account and interact with their devices that way; this mode may or may not
be supported (or condoned) by the vendor.  There are, of course, some
vendors that <a href="https://www.bleepingcomputer.com/news/security/haier-hits-home-assistant-plugin-dev-with-takedown-notice/">are
actively hostile</a> to integration with Home Assistant.
</p><p>
As might be expected, the quality of integrations varies widely.  Most of
the integrations I have tried have worked well enough.  The OpenSprinkler
(<a href="https://lwn.net/Articles/940509/">reviewed here</a> in 2023) integration,
instead, thoroughly corrupted the device configuration, exposing me to the
shame of being seen with a less-than-perfect lawn; it was quickly
removed.  It is an especially nice surprise when a device comes with Home
Assistant support provided by the vendor, but that is still a relatively
rare occurrence.  Home Assistant now is in a position similar to Linux
25&nbsp;years ago; many devices are supported, but often in spite of their
vendor, and one has to choose components carefully.
</p><h4>Security</h4>
<p>
Home Assistant sits at the core of the home network; it has access to
sensors that can reveal a lot about the occupants of the home, and it
collects data in a single location.  An installation will be exposed to the
Internet if its owner needs remote access.  There is clearly potential for
a security disaster here.
</p><p>
The project has <a href="https://www.home-assistant.io/security/">a posted
security policy</a> describing the project's stance; it asks for a 90-day
embargo on the reporting of any security issues.  Authors writing about the
project's security are encouraged to run their work past the project "<q>so
we can ensure that all claims are correct</q>".  The security policy
explicitly excludes reports regarding third-party integrations (the core
project cannot fix those, after all).  The project is also uninterested in
any sort of privilege escalation by users who are logged into Home
Assistant, assuming that anybody who has an account is fully trusted.
</p><p>
The project has only issued <a href="https://github.com/home-assistant/core/security/advisories/GHSA-m3pm-rpgg-5wj6">one
security advisory</a> since the beginning of 2024.  There were several in
2023, mostly as the result of <a href="https://github.blog/security/vulnerability-research/securing-our-home-labs-home-assistant-code-review/">a
security audit</a> performed by GitHub.
</p><p>

There is no overall vetting of third-party integrations, which are, in the
end, just more Python code.  So loading an unknown integration is similar
to importing an unknown module from PyPI; it will probably work, but the
potential for trouble is there.  The project has occasionally <a href="https://www.home-assistant.io/blog/2021/01/23/security-disclosure2/">reported
security problems in third-party integrations</a>, but such reports are
rare.  I am unable to find any reports of actively malicious integrations
in the wild, but one seems destined to appear sooner or later.
</p><h4>Actually doing something with Home Assistant</h4>
<p>
The first step for the owner of a new Home Assistant installation is,
naturally, to seek out integrations for the devices installed in the home.
On successful installation and initialization, an integration will add one
or more "devices" to the system, each of which has some number of "sensors"
for data it reports, and possible "controls" to change its operating state.
A heat-pump head, for example, may have sensors for the current temperature
and humidity, and controls for its operating mode, fan speed, vane
direction, and more.
</p><p>
It is worth noting that the setup of these entities seems a bit
non-deterministic at times.  My solar system has 22&nbsp;panels with
inverters, each of which reports nearly a dozen parameters (voltage,
current, frequency, temperature, etc.).  There is no easy way to determine
which panel is reporting, for example, <tt>sensor_amps_12</tt>, especially
since <tt>sensor_frequency_12</tt> almost certainly corresponds to a
<i>different</i> panel.  My experience is that Home Assistant is a system
for people who are willing to spend a lot of time fiddling around with
things to get them to a working state.  Dealing with these sensors was an
early introduction to that; it took some time to figure out the mapping
between names and rooftop positions, then to rename each sensor to
something more helpful.
</p><p>
The next level of fiddling around is setting up dashboards.  Home Assistant
offers a great deal of flexibility in the information and controls it
provides to the user; it is possible to set up screens focused on, say,
energy production or climate control.  Happily, the days when this
configuration had to be done by writing YAML snippets are mostly in the
past at this point; one occasionally still has to dip into YAML, but it
does not happen often.  The interface is not always intuitive,
but it is fairly slick, interactive, and functional.
</p><p>
Another part of Home Assistant that I have not yet played with much
is automations and scenes.  Automations are simple rule-triggered programs
that make changes to some controls.  They can carry out actions like
"turn on the front light when it gets dark" or "play scary music if
somebody rings the doorbell and nobody is home".  Scenes are sets of canned
device configurations.  One might create a scene called "in-laws visiting"
that plays loud punk music, sets the temperature to just above freezing,
disables all voice control, and tunes all of the light bulbs to 6000K, for
example.
</p><p>
The good news is that, unless the fiddling itself is the point (and it can
be a good one), there comes a time when things just work and the fiddling
can stop.  A well-configured Home Assistant instance provides detailed
information about the state of the home — and control where the devices
allow it — to any web browser that can reach it and log in.  There are
(open-source) apps that bring this support to mobile devices in a way that
is nearly indistinguishable from how the web interface works.
</p><p>
All told, it is clear why Home Assistant has a strong and growing
following.  It is an open platform that brings control to an industry that
is doing its best to keep a firm grasp on our homes and the data they
create.  Home Assistant shows that we can do nicely without all of these
fragile, non-interoperable, rug-pull-susceptible cloud systems.  Just like
Linux proved that we can have control over our computers, Home Assistant
shows that we do not have to surrender control over our homes.
</p><p>
This article has gotten long, and is remarkably short on interesting things
that one can actually <i>do</i> with Home Assistant.  There are some
interesting stories to be told along those lines; they will appear shortly
in <a href="https://lwn.net/Articles/1017945/">the second, concluding part</a> of this series.<br clear="all"></p>
               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Behind Silicon Valley and the GOP’s campaign to ban state AI laws (109 pts)]]></title>
            <link>https://www.bloodinthemachine.com/p/de-democratizing-ai</link>
            <guid>44011654</guid>
            <pubDate>Sat, 17 May 2025 02:46:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloodinthemachine.com/p/de-democratizing-ai">https://www.bloodinthemachine.com/p/de-democratizing-ai</a>, See on <a href="https://news.ycombinator.com/item?id=44011654">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Greetings all,</p><p>Today, we dive deep into the GOP’s radical campaign to ban US states from passing any laws that govern AI. Even in a political moment as fraught as ours, this one stands out. We’ll get into:</p><ul><li><p>How a proposal to ban AI lawmaking wound up in the budget reconciliation bill the same week that AI execs took a trip with Trump to Saudi Arabia</p></li><li><p>How the GOP plans to try to sell its state AI ban, according to the GOP</p></li><li><p>A look at what the implications are for AI in general</p></li><li><p>An interview at the end with California Assemblyman Isaac Bryan, author and co-sponsor of some of the AI bills Silicon Valley wants dead</p></li></ul><p>I’m not going to lie, this was a dark week, and a tough one to report through—I meant to publish this on Tuesday, then Wednesday, and the new twists and revelations in how this campaign came together just kept piling up. It took many hours to research, investigate, and write this story. To that end, Blood in the Machine is 100% reader supported, and made possible by paid subscribers. If you find value in this work, and if you can, your support would be immensely appreciated. </p><p><span>On Sunday, May 11th, Republicans added </span><a href="https://www.404media.co/republicans-try-to-cram-ban-on-ai-regulation-into-budget-reconciliation-bill/" rel="">a sweeping amendment</a><span> to the 2025 budget reconciliation bill that would ban all US states from enacting any laws regulating AI for ten years. Reconciliation is a common way for a party to try to push through controversial or unpopular legislation that might not survive a regular Senate vote (budgets can’t be filibustered, and need just a simple majority to pass). Even so, this amendment, put forward by the Kentucky congressman and energy and commerce committee chair Brett Guthrie, managed to shock.</span></p><p><span>The amendment drew admonitory </span><a href="https://www.washingtonpost.com/opinions/2025/05/14/artificial-intelligence-regulation-congress-reconciliation/" rel="">headlines</a><span>, consternation in Democrats, and anger and disbelief on social media. The outcry is well deserved. The bill’s language is not ambiguous. It </span><a href="https://d1dth6e84htgma.cloudfront.net/Subtitle_C_Communications_4e3fbcc3bc.pdf?ref=404media.co" rel="">says</a><span> that “no State or political subdivision thereof may enforce any law or regulation regulating artificial intelligence models, artificial intelligence systems, or automated decision systems during the 10-year period beginning on the date of the enactment of this Act.”</span></p><p><span>Take a minute to absorb what’s being proposed here: No state</span><em> </em><span>may enforce </span><em>any law or regulation</em><span> of AI. A total ban of state lawmaking on what is routinely touted as the most transformative commercial technology of our generation. And because we can safely assume there will be no serious efforts to regulate AI by the GOP-controlled Congress or by a Trump administration intent on </span><a href="https://www.bloodinthemachine.com/p/ai-is-in-its-empire-era" rel="">helping the US AI industry dominate</a><span>, this is, in effect, an effort to ban any lawmaking around AI whatsoever, for the next two to four years, while Republicans have a stranglehold on power.</span></p><p><span>All of this is, needless to say, profoundly undemocratic. Both in approach—the act of sliding a bill with such severe repercussions into the reconciliation process, where it won’t receive a proper public hearing—and intent: to prevent the public from having a vote on how pervasive Silicon Valley technologies are impacting their lives. Worse still, Gutherie’s amendment is the culmination of </span><a href="https://www.politico.com/news/2025/05/12/how-big-tech-is-pitting-washington-against-california-00336484" rel="">a multi-pronged lobbying effort</a><span> from the major AI companies. That effort’s aim, as reported by </span><a href="https://www.politico.com/news/2025/05/12/how-big-tech-is-pitting-washington-against-california-00336484" rel="">Politico</a><span>, was to shut down state laws that might constrain AI firms’ and investors’ ability to profit off of AI products—especially California’s. </span></p><p>AI industry pitchmen are fond of saying that AI is a powerful tool for “democratization.” It has instead become a force for the opposite.</p><p><span>On Tuesday, at about the same time that the proposed language seeking to ban state AI regulation was officially being introduced in Congress, a bevy of tech billionaires including Sam Altman, Elon Musk, Nvidia CEO Jensen Huang and Amazon CEO Andrew Jassy were at lunch with president Trump in Saudi Arabia. There, the tech titans cut billion dollar deals with Gulf State royalty and the Trump Administration. Trump announced a $142 billion defense and AI services sale to Saudi Arabia. DataVolt, a Saudi Arabian company will </span><a href="https://www.ft.com/content/5302d5d2-d375-4327-905c-7b1ad5173105" rel="">spend $20 billion on data centers in the US</a><span>. Amazon is </span><a href="https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/" rel="">investing $5 billion in Humain</a><span>, Mohammed bin Salman’s AI startup. Nvidia is selling billions of dollars worth of chips to Humain. Meanwhile, OpenAI is mulling a StarGate project in the United Arab Emirates; MGX, the Emirati investment firm, is already a backer of its fledgling Texas data megacenter. </span></p><p>And on and on it goes. I hope this fact escapes no one: While the executives of AI firms are abroad in Saudi Arabia, cutting billion dollar deals to expand their operations with nations boasting some of the worst human rights records in the world, their lobbyists and partners back home are trying to make it impossible to pass any laws governing their AI products at all. </p><p>With states’ rights to legislate AI under assault, I reached out to lawmakers to see how the move in DC was reverberating back home. </p><p>“The tech industry was incubated, cultivated, and continues to grow and innovate here in California,” says Isaac Bryan, a California assemblyman who has authored a state bill that limits the ways AI can be used for surveillance in the workplace—one of the bills that the GOP amendment would ban. (Bryan also happens to represent my district in the CA assembly.) “California deserves the right, and has the expertise, to lead. We’ve been establishing meaningful guardrails and regulations around these advancements so that we center people as we continue to innovate.”</p><p><span>But now there’s a gulf between who gets a say in AI policy, Bryan says, and who doesn’t. “There's the needs that everyday folks have,” he says, “and there's the needs that our tech billionaire class has—and </span><em>those</em><span> are the only ones being addressed.”</span></p><p>Samantha Gordon, a program director at TechEquity, a nonprofit group of tech workers that advocates for housing and labor issues, and that has backed a number of California AI bills, tells me that widening gulf is by design. “This amendment is the direct result of a campaign by Google, Meta, OpenAI, and venture capitalists like Andreessen Horowitz—and their dozens of trade associations—to bulldoze through the public's safety in order to continue to make risky bets on a precarious and potentially hazardous technology,” Gordon says. </p><p><a href="https://www.politico.com/newsletters/digital-future-daily/2024/05/06/exclusive-poll-americans-favor-ai-data-regulation-00156350" rel="">Public polling</a><span> shows </span><a href="https://www.forbes.com/sites/cio/2024/04/18/ai-regulation-has-strong-bipartisan-approval/" rel="">bipartisan support</a><span> for </span><em>more </em><span>regulation of AI, after all, not less. And yet, as Gordon puts it, “if this amendment passes, not a single state in America could protect people from AI systems that unfairly deny their medical care, keep their nursing homes understaffed, revoke their unemployment benefits, or inflate their rent.” It’s part of what she says is a “cynical campaign” the tech industry is waging “to override the will of the public.”</span></p><p><span>Now, there’s a good possibility that this aggressive language won’t survive the Byrd Rule—</span><a href="https://www.congress.gov/crs-product/RL30862" rel="">a law that restricts</a><span> what can be included in the reconciliation process to measures that affect spending levels and revenue—but it might. And GOP leadership, which now counts Silicon Valley insiders and AI bulls like Musk, Andreessen, and David Sacks among its inner circle, may deem it worth the legal challenges. And even if the language does gets stripped we cannot afford to ignore what it tells us: Top Republicans and top players in the AI industry can now move as a united front. The time of AI industry leaders paying lip service to AI as a technology that “benefits all of humanity,” a line that has been withering on the vine for a while now, is gone. In its place is a cold calculus bent on using the technology and </span><a href="https://www.bloodinthemachine.com/p/whats-really-behind-elon-musk-and" rel="">its logic</a><span> to accumulate as much power as possible. </span></p><p>So let’s run down why it is the AI companies are so intent on stopping these state-level bills, why the GOP is so interested in helping them, and how this changes the very way we should think about AI as a technology. </p><p><span>You may have noticed in the above language in the bill goes beyond “AI” and also includes “automated decision systems.” That’s likely because there are two California bills currently under consideration in the state legislature that use the term; AB 1018, </span><a href="https://techequity.us/the-automated-decisions-safety-act-ab-1018/" rel="">the Automated Decisions Safety Act</a><span> and SB7, the </span><a href="https://sd05.senate.ca.gov/news/mcnerney-introduces-no-robo-bosses-act-ensure-human-oversight-ai-workplace" rel="">No Robo Bosses Act</a><span>, which would seek to prevent employers from relying on “automated decision-making systems, to make hiring, promotion, discipline, or termination decisions without human oversight.”</span></p><p><span>The GOP’s new amendments would ban both outright, along with the other </span><a href="https://calmatters.org/economy/technology/2025/03/ai-regulation-after-trump-election/" rel="">30 proposed bills that address AI</a><span> in California. Three of the proposed bills are backed by the California Federation of Labor Unions, including AB 1018, which aims to eliminate algorithmic discrimination and to ensure companies are transparent about how they use AI in workplaces. It requires workers to be told if AI is used in the hiring process, allows them to opt out of AI systems, and to appeal decisions made by AI. The Labor Fed also backs Bryan’s bill, AB 1221, which seeks to prohibit discriminatory surveillance systems like facial recognition, establish worker data protections, and compels employers to notify workers when they introduce new AI surveillance tools. </span></p><p><span>It should be getting clearer why Silicon Valley is intent on halting these bills: One of the key markets—if not </span><em>the </em><span>key market—for AI is as enterprise and workplace software. A top promise is that companies can automate jobs and labor; restricting surveillance capabilities or carving out worker protections promise to put a dent in the AI companies’ bottom lines. Furthermore, AI products and automation software promise a way for managers to </span><em>evade</em><span> accountability—laws that force them to stay accountable defeat the purpose.</span></p><p><span>OpenAI already won a major victory in beating back state level policy earlier this year, after Assemblywoman Diane Papan, who had proposed a bill aimed at preventing nonprofits from restructuring as for-profit companies—which OpenAI was in the process of trying to do—</span><a href="https://www.sfexaminer.com/news/technology/california-bill-barring-openai-for-profit-transition-dead/article_27250d39-7577-47cc-a414-a7b13e5f6ce0.html" rel="">gutted the language of her own bill</a><span> and replaced it with </span><a href="https://garymarcus.substack.com/p/breaking-bill-that-would-have-blocked" rel="">essentially an entirely new one</a><span>. The strange move came after pushback from OpenAI, and just three days after OpenAI closed its deal with SoftBank for $40 billion, a large portion of which is contingent on the removal of that nonprofit structure. It’s almost quaint to think back to 2023, when Sam Altman made a performative show of </span><a href="https://www.nytimes.com/2023/05/16/technology/openai-altman-artificial-intelligence-regulation.html" rel="">asking Congress to regulate his company</a><span>—he’s spent the two years since fighting tooth and nail against every meaningful regulation that would affect his business.</span></p><p><span>The Trump administration, meanwhile, has adopted the industry’s zeal for deregulation; in part, of course, because there’s significant overlap between the industry and the administration. One of Trump’s first actions was to dissolve Biden’s framework for governing AI, and to institute a new set of priorities aimed not at safe, equitable AI but at helping the US AI industry achieve dominance. Vice president, and former venture capitalist, JD Vance used his first speech abroad to call for </span><a href="https://www.bloodinthemachine.com/p/ai-is-in-its-empire-era" rel="">an end to international AI regulations</a><span>. Marc Andreessen, who’s </span><a href="https://www.washingtonpost.com/politics/2025/01/13/andreessen-tech-industry-trump-administration-doge/" rel="">advising the administration on tech policy</a><span>, and who wields nearly as much influence as Musk, has long advocated for less regulation—his fingerprints are all over the Guthrie amendment.</span></p><p><span>In an effort to try to better understand the GOP’s aims here, I called up Guthrie’s office, and spoke on background with a rep on the energy and commerce committee. </span><a href="https://energycommerce.house.gov/posts/chairman-guthrie-delivers-opening-statement-at-full-committee-markup-of-budget-reconciliation-text" rel="">Evidently</a><span>, their plan is to argue that because the Trump administration is modernizing agencies like the Department of Commerce and the Federal Trade Commission with AI, banning states’ ability to regulate AI is a spending-related matter. If, for instance, California passes a law that, say, requires an AI company to comply with transparency laws, and it becomes more expensive as a result, then the federal government will have to spend more on AI services. </span></p><p>This strikes me as an enormous stretch, as such logic could be deployed to ban state lawmaking around just about anything. You could, say, ban states from making laws that seek to regulate the housing market, on the grounds that they might effect the price of maintaining federal buildings, or ban statewide labor laws because they impact the cost of paying federal employees, and so on.</p><p>It seems that the talking points around promoting this amendment will roughly be: </p><p><span>-It will encourage innovation and efficiency, preventing AI companies from having to deal with a patchwork of state laws</span><br><span>-States like Colorado and California that have passed or are preparing to pass AI regulations are not truly prepared to do so</span><br><span>-This effort is actually intended to benefit little tech, not big tech, because any regulations would harm little tech more</span><br><span>-A “light touch” is imperative so we can beat China in the AI race</span></p><p><span>A lot of these ideas can be traced back to Congressional </span><a href="https://web.archive.org/web/20250410155751/https://www.washingtonpost.com/politics/2025/04/10/ai-race-china-energy-congress/" rel="">committee hearings</a><span> held by Gutherie and Ted Cruz in recent months, which were attended by Altman, former Google chief Eric Schmidt, Scale AI CEO Alexandr Wang and others. The notion that the US must “beat” China in the AI race at any cost was a frequent theme, and this was where Schmidt’s now-infamous declaration that AI needs to be given as much energy as possible (and not to worry, AI will solve the climate crisis) was made. It left an impression.</span></p><p><span>“Eric Schmidt said we need to use energy [to develop AI] because it’s going to produce the solutions to climate change,” Brett Gutherie told the </span><em>Washington Post</em><span>, weeks before he introduced his amendment banning state lawmaking on AI. </span></p><div id="youtube2-Ffak4ngCvgE" data-attrs="{&quot;videoId&quot;:&quot;Ffak4ngCvgE&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/Ffak4ngCvgE?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><span>In the interview, Guthrie argues that “the most existential threat to America” is “losing the battle for AI” to China.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-163495084" href="https://www.bloodinthemachine.com/p/de-democratizing-ai#footnote-1-163495084" target="_self" rel="">1</a></span><span> That, Guthrie says, is why we can’t “go down the path some other continents have,” as Europe has, and adopt even modest regulations on AI or the tech sector. </span></p><p>It’s unclear whether Guthrie and the GOP—or Sam Altman and Eric Schmidt, for that matter—believe there’s an AI race with China of existential proportions, or if it’s simply a useful line to justify calling for limitless investment, and placing AI outside the democratic process. Ultimately it doesn’t matter. It’s serving GOP and Silicon Valley interests in providing the imperative for unfettered AI development, even halfheartedly. </p><p>What’s clear is that the GOP, AI executives, and Gulf State princes all have a common belief in AI—as a means of accumulating capital, undercutting labor, and concentrating power. And the terms of AI development and deployment are on the cusp of being set entirely by oligarchs, billionaires, and their allies in the ruling party. And those parties are intent removing any impediments—like the democratic process—from their pursuit of power and profit. </p><p>“Politicians are letting billionaires call the shots and all of us will be the ones who pay the price,” as TechEquity’s Samantha Gordon put it. Bryan says it might come to people taking to the streets; there’s so much at stake. “As goes California, as goes the country. And so they're trying to get ahead of us,” he says. “But I don't think they've got the expertise, and they certainly don't have the American people behind them in an effort like this.”</p><p><span>Over in the Senate, Ted Cruz </span><a href="https://www.washingtonpost.com/opinions/2025/05/14/artificial-intelligence-regulation-congress-reconciliation/" rel="">has announced</a><span> that he will introduce an amendment like Guthrie’s, where he’ll make the push for the ban on states making their own AI rules to become law. </span></p><p><em>This story was edited by Mike Pearl. Eliza McCullough contributed research.</em></p><p><span>In reporting the above piece, I spoke at length with California state assemblyman Isaac Bryan, the author of the state bill </span><a href="https://calmatters.digitaldemocracy.org/bills/ca_202520260ab1221" rel="">AB 1221: Workplace surveillance tools</a><span>, the co-sponsor of a number of other AI-focused bills, and who also happens to represent my district in Los Angeles. </span></p><p>I thought the full conversation worth sharing, so I’m sharing a lightly edited and abridged version of it below. </p><p><strong>BLOOD IN THE MACHINE: Thanks for taking the time to talk —&nbsp;so, you’ve sponsored some of the bills that are being considered in the California state legislature, to prevent employers from using AI unethically in the workplace, for one. How are you thinking about these bills now, as the GOP is taking aim at your capacity to even pass such laws?</strong></p><p>The reality is we still have a preservation of states' rights and the ability for states to set policy guidelines and regulations, particularly on issues that impact residents of their state disproportionately. The tech industry was incubated, cultivated, and continues to grow and innovate here in California. And I think many of us, all of us, most of us, believe in that innovation, believe in that creativity, believe in that advancement. It's also why California deserves the right, and has the expertise, to lead. We’ve been establishing meaningful guardrails and regulations around these advancements so that we center people as we continue to innovate.</p><p>The challenges of this administration in Washington—they often shoot from the hip and misfire as they have rapidly on several different occasions with several different policy fronts and especially on things related to the economy. I can't think of how many executive orders and how many tariff ideas and how many other things have come from this administration only to be kind of rolled back or changed when they ran into the pragmatism of reality.</p><p>The budget reconciliation process certainly isn't done yet and I know we continue all to an ever-encroaching minority in the House. And so I expect for California's voices to be heard. But I think all states should be deeply concerned about new levels of preemption. That should be a bipartisan kind of conversation about where the federal government can step in, should step in, and where it absolutely should not.</p><p>What we're going to do in California is continue to lead in the ways that we have. Balancing the needs of everyday people, the needs of the emerging industries, the desires of those of us in the state house, along with the goals of the governor, and strike those balances where we can. And if the federal government continues to encroach on that, we'll continue to file lawsuits as we have and have done successfully, both in the past Trump administration and currently.</p><p><strong>There are two levels of audacity here. First, that they would even attempt such a thing — this is a party that has, in the past, quite loudly expressed their belief in states' rights; as recently as last summer. And now to try to do away with them on such a key issue altogether. But then, secondly, to do this in the budget reconciliation bill, to have such a far-reaching and potentially impactful measure put through in reconciliation. That really, to me, without giving this bill a proper hearing, it really underscores how undemocratic this maneuver is, around such an important topic.</strong></p><p>I couldn't agree further. I mean, if there's one thing that the GOP has been consistent about, it has been their hypocrisy. They are for any type of legislative maneuvers, any type of distribution of checks and balances and powers that favor them in any given moment. There's very limited consistency, and we've seen some strict constitutionalists in the GOP absolutely flipped the script to justify the actions of the current administration and leaders in Congress. This is no exception.</p><p>But, you know, this is a moment for those in the federal government who value the preservation of our civic institutions over the preservation of self and self-interest to rise and stand up. And I think you're seeing that kind of consistency from the left. You're seeing that kind of measured and steady hand from the last two Democratic presidents, and hopefully that kind of longer term view of balancing powers and making sure that the American people are heard, the people who are most impacted by these kinds of changes. And in this particular space, on AI, that is Californians. </p><p>Whether the Trump administration likes it or not, I recognize that he took a far-reaching group of billionaire CEOs and particular tech CEOs to Saudi Arabia just the other day to meet with the prince. And I think all of that has important diplomatic motivations, but it's a very strange thing to have people struggling to keep a roof over their head and watching the exorbitant wealth being generated by tech billionaires, and the preservation of that wealth by this administration, supersede the needs of everyday people.</p><p>The richest man in the world, a tech billionaire himself, was serving as a surrogate president for the last several months. I think it's a scary time for folks.</p><p><strong>I could not ignore that irony either. The same day that the amendment to try to effectively wipe out AI regulation in the United States, the CEOs of these companies are in Saudi Arabia inking billion dollar deals.</strong><span> </span></p><p>It's interesting, too. It seems like the only bridges that Trump can build are between tech billionaire CEOs who don't like each other, right? It's not a well-kept secret that Musk and Altman don't like each other. Their views around OpenAI have spilled out into the public, and yet they both seem to find comfort, security, and safety in this current president.</p><p>It's a shame that the everyday folks across this country who are struggling right now and deeply afraid of how these kind of economic decisions will impact or limit their choices as they try to provide food for their kids and keep a roof over their heads and buy new school clothes and backpacks. There's the needs that everyday folks have, and there's the needs that our tech billionaire class has—and those are the only ones being addressed. </p><p><strong>I know it's always a tricky line in California, because the tech sector is largely based here. And it's a key constituency. Does it concern you at all that these tech companies, that this is essentially what they have been lobbying for? That Sam Altman and Google and IBM have been pushing for an exemption to state lawmaking, specifically because I think that they worry about having to comply with rules that might be put forward in places like California?</strong></p><p>Yeah, it's deeply concerning because I think there's no place better positioned to understand the tremendous positive things, both for society, for social living, but also for wealth generation in an industry that pays taxes to the state. Nobody understands that better than California. But we also deal with the harsh and very real realities that as these kinds of innovations take place in a way that displaces workers, with an intentionality of increasing productivity through the laying off of everyday people trying to earn a living, that there's a balance that's got to be struck there. </p><p>And even as we generate new forms of state revenue, and are able to increase the state's wealth through this new industry, we will also have a disproportionate growth in liabilities, as people will need unemployment and health care and other social safety nets because their ability to earn a living has drastically changed during this spike of innovation. So we've got to be mindful of that balance—we also want to make sure that tools don't become predatory, or increase the opportunity for data and information to be leaked.</p><p>It's unconscionable to me the way that the current federal administration has treated people's private and sensitive federal data like some sort of play thing for Elon Musk and Big Balls—only because I can't remember the guy's actual name.</p><p><strong>On the DOGE team.</strong></p><p>Exactly. To me, how much we've allowed for our lives to be captured through these algorithmic systems, and through these innovations, and then to have that data decisively unprotected in this present moment—and so we've got to do almost all of the above in California. And I think when we lead in this sector the kind of decisions we'll make will strike the appropriate balances that allow for others to follow.</p><p>And that's the real fear: As goes California, as goes the country. And so they're trying to get ahead of us. But I don't think they've got the expertise, and they certainly don't have the American people behind them in an effort like this.</p><p><strong>Yeah. These are proposals aimed at limiting some of the harms of AI, making sure that workers don't get steamrolled and can't be surveilled at will, and giving some power back to workers when these tools are used in their workplaces. Now, I think that when a tech company sees that, they see something that’s going to be inconvenient and costly. But can you talk a little bit about why it's important to have things like SB7 or AB 1221, which, I would describe them as hardly radical, but more like common sense proposals in the era of AI, as the technology that stands to affect more workers.</strong></p><p>They're absolutely common sense proposals, and they're working through the legislative process, taking amendments through the process, learning, bringing stakeholders to the table.</p><p><span>It's interesting too, if I was some of these tech CEOs, I actually wouldn't </span><em>want</em><span> this power to, the ability to regulate and make thoughtful decisions in the hands of somebody [Trump] who's decided that thoughtfulness is not a characteristic that they want to exhibit through their leadership.</span></p><p>I mean, he has haphazardly taken a sledgehammer and swung from left to right on a range of issues. And even in these issues, I think he's going to wake up one day and realize that the Teamsters, the only labor union that backed him at the federal level, have a deep interest in not being pushed out by automated trucks, which is a conversation that's been going on here in California, and there's been some back and forth between legislators and the governor on how to land this correctly, even here. But that's another base of a constituency that he will eventually hear from. They probably don't get in through the door as quickly as the billionaire tech CEOs.</p><p>So I think this is California's responsibility. These are the kinds of state rights that we can and should have. We are allowed to govern in the interests that protect our economy and the people who rely on us. To have that preempted or suggested to be preempted in this way surely has got to be unconstitutional and we should do what we can to find out.</p><p><strong>Let’s talk for a second about why you co-sponsored these bills in the first place, and what stands to be lost. </strong></p><p>Our bill, AB 1221, we offered this bill because we don't want to lose the humanity in the workplace.</p><p>There are some AI tools now that, register your emotional feeling for the day, your gait, movement and the way that you walk. They make corrective decisions, recommendations, disciplinary recommendations, all without human intervention. This has gone far beyond cameras in the store to make sure theft doesn't occur. It is very invasive. It has increasing ability to show biased attitudes, biased behaviors that can be harmful to both protected classes and workers more broadly. </p><p>We just want to make sure that these tools are being used responsibly, that workers know which ones are being used on them and that any kind of disciplinary activity or things that impact somebody's ability to keep their job that those decisions are ultimately made by a person—which doesn't sound unreasonable to me. Like I said, it's about preserving the humanity in the workplace.</p><p><strong>You mentioned legal challenges.</strong><span> </span><strong>Hopefully it gets struck down and doesn't pass the Byrd Rule in the Senate, and hopefully. But now we've also seen their colors, their intent, through this maneuver. That they're willing to even attempt this route; something that even just a couple years ago would have been considered audacious and extreme. How do you push back on this?</strong></p><p>We stand up. We make sure our elected officials hear from us. We, hold rallies, hold town halls, hold people accountable, take to the streets when necessary, and defend states that are willing to step up and buck this administration for the good of the American people.</p><p>You know, this is not a partisan issue. It's about putting people first. And that used to be a shared value. But it's not everyday people who met with Saudi oligarchs a week ago, right? You needed a certain net worth to be invited on that trip.And you can't imagine that any kind of conversations that take place in that setting are good for everyday workers trying to keep a roof over their head and food on their tables. But there are more everyday people trying to earn an honest living in this moment than there are tech billionaires, and it's time for those folks to be heard.</p><p><strong>Well, I think that's a great place to leave it. Thanks for your time.</strong></p><p>Absolutely. Thank you.</p><p><strong>OTHER BLOODY STUFF</strong></p><ul><li><p><span>Columbia Journalism Review asked me to </span><a href="https://www.cjr.org/feature-2/how-were-using-ai-tech-gina-chua-nicholas-thompson-emilia-david-zach-seward-millie-tran.php#Tristan%20Lee" rel="">participate in a roundup</a><span> of how journalists are using AI—Spoiler: I am not. The whole thing is worth a read, with smart takes from fellow travelers like Jason Koebler, Khari Johnson, Susie Cagle, and others. </span></p></li><li><p>I was on the Majority Report with Emma Vigeland to talk about my piece on the AI jobs crisis:</p><div id="youtube2-oezqSzpb-h8" data-attrs="{&quot;videoId&quot;:&quot;oezqSzpb-h8&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/oezqSzpb-h8?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div></li><li><p><span>Friend of the blog </span><a href="https://x.com/tigerbeat" rel="">Steve Rhodes</a><span> sent over this poem, which is worth a read:</span></p><div data-attrs="{&quot;instagram_id&quot;:&quot;DJo6P84OH4J&quot;,&quot;title&quot;:&quot;A post shared by @ssyjuco&quot;,&quot;author_name&quot;:&quot;ssyjuco&quot;,&quot;thumbnail_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/__ss-rehost__IG-meta-DJo6P84OH4J.jpg&quot;,&quot;timestamp&quot;:null,&quot;belowTheFold&quot;:true}" data-component-name="InstagramToDOM"><p><a href="https://instagram.com/p/DJo6P84OH4J" target="_blank" rel=""><img src="https://substackcdn.com/image/fetch/w_640,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F__ss-rehost__IG-meta-DJo6P84OH4J.jpg" loading="lazy"></a></p></div></li><li><p>Tune into System Crash this week, where we discuss the above, as well as the new Luddite pope, and a lot more. </p></li><li><p><span>Tune into a live chat on Friday, May 16th, at 10 AM EST / 1 PM PST with Karen Hao, where we’ll talk about her fantastic new book, </span><a href="https://www.penguinrandomhouse.com/books/743569/empire-of-ai-by-karen-hao/" rel="">Empire of AI</a><span>.</span></p></li></ul><p>That’s it for this week. Until next time, thanks for reading —&nbsp;and hammers up. Way up. </p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XTool – Cross-platform Xcode replacement (185 pts)]]></title>
            <link>https://github.com/xtool-org/xtool</link>
            <guid>44011515</guid>
            <pubDate>Sat, 17 May 2025 02:10:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/xtool-org/xtool">https://github.com/xtool-org/xtool</a>, See on <a href="https://news.ycombinator.com/item?id=44011515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">xtool</h2><a id="user-content-xtool" aria-label="Permalink: xtool" href="#xtool"></a></p>
<p dir="auto">Cross-platform Xcode replacement. Build and deploy iOS apps with SwiftPM on Linux, Windows, and macOS.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">xtool is a cross-platform (Linux/WSL/macOS) tool that replicates Xcode functionality with open standards.</p>
<p dir="auto">✅ Build a SwiftPM package into an iOS app</p>
<p dir="auto">✅ Sign and install iOS apps</p>
<p dir="auto">✅ Interact with Apple Developer Services programmatically</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<ol dir="auto">
<li>Follow the guide to install <code>xtool</code>
<ul dir="auto">
<li><a href="https://swiftpackageindex.com/xtool-org/xtool/documentation/xtool/installation-linux" rel="nofollow">Installation (Linux/Windows)</a></li>
<li><a href="https://swiftpackageindex.com/xtool-org/xtool/documentation/xtool/installation-macos" rel="nofollow">Installation (macOS)</a></li>
</ul>
</li>
<li>Create and run your first xtool-powered app by following the <a href="https://swiftpackageindex.com/xtool-org/xtool/tutorials/xtool/first-app" rel="nofollow">tutorial</a>!</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Screenshot</h3><a id="user-content-screenshot" aria-label="Permalink: Screenshot" href="#screenshot"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/xtool-org/xtool/blob/main/Sources/xtool/Documentation.docc/Resources/Cover.png"><img src="https://github.com/xtool-org/xtool/raw/main/Sources/xtool/Documentation.docc/Resources/Cover.png" alt="A screenshot of xtool being invoked from VSCode"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Command line interface</h3><a id="user-content-command-line-interface" aria-label="Permalink: Command line interface" href="#command-line-interface"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ xtool --help
OVERVIEW: Cross-platform Xcode replacement

USAGE: xtool <subcommand>

OPTIONS:
  -h, --help              Show help information.

CONFIGURATION SUBCOMMANDS:
  setup                   Set up xtool for iOS development
  auth                    Manage Apple Developer Services authentication
  sdk                     Manage the Darwin Swift SDK

DEVELOPMENT SUBCOMMANDS:
  new                     Create a new xtool SwiftPM project
  dev                     Build and run an xtool SwiftPM project
  ds                      Interact with Apple Developer Services

DEVICE SUBCOMMANDS:
  devices                 List devices
  install                 Install an ipa file to your device
  uninstall               Uninstall an installed app
  launch                  Launch an installed app

  See 'xtool help <subcommand>' for detailed help."><pre>$ xtool --help
OVERVIEW: Cross-platform Xcode replacement

USAGE: xtool <span>&lt;</span>subcommand<span>&gt;</span>

OPTIONS:
  -h, --help              Show <span>help</span> information.

CONFIGURATION SUBCOMMANDS:
  setup                   Set up xtool <span>for</span> iOS development
  auth                    Manage Apple Developer Services authentication
  sdk                     Manage the Darwin Swift SDK

DEVELOPMENT SUBCOMMANDS:
  new                     Create a new xtool SwiftPM project
  dev                     Build and run an xtool SwiftPM project
  ds                      Interact with Apple Developer Services

DEVICE SUBCOMMANDS:
  devices                 List devices
  install                 Install an ipa file to your device
  uninstall               Uninstall an installed app
  launch                  Launch an installed app

  See <span><span>'</span>xtool help &lt;subcommand&gt;<span>'</span></span> <span>for</span> detailed help.</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Library</h3><a id="user-content-library" aria-label="Permalink: Library" href="#library"></a></p>
<p dir="auto">xtool includes a library that you can use to interact with Apple Developer Services, iOS devices, and more from your own app. You can use this by adding <code>XKit</code> as a SwiftPM dependency.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// package dependency:
.package(url: &quot;https://github.com/xtool-org/xtool&quot;, .upToNextMinor(from: &quot;1.2.0&quot;))
// target dependency:
.product(name: &quot;XKit&quot;, package: &quot;xtool&quot;)"><pre>// package dependency:
<span>.</span><span>package</span><span>(</span>url<span>:</span> <span>"</span><span>https://github.com/xtool-org/xtool</span><span>"</span><span>,</span> <span>.</span>upToNextMinor<span>(</span>from<span>:</span> <span>"</span><span>1.2.0</span><span>"</span><span>)</span><span>)</span>
// target dependency:
<span>.</span><span>product</span><span>(</span>name<span>:</span> <span>"</span><span>XKit</span><span>"</span><span>,</span> <span>package</span><span>:</span> <span>"</span><span>xtool</span><span>"</span><span>)</span></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wow@Home – Network of Amateur Radio Telescopes (184 pts)]]></title>
            <link>https://phl.upr.edu/wow/outreach</link>
            <guid>44011489</guid>
            <pubDate>Sat, 17 May 2025 02:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phl.upr.edu/wow/outreach">https://phl.upr.edu/wow/outreach</a>, See on <a href="https://news.ycombinator.com/item?id=44011489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" dir="ltr"><div id="h.f6d8ee3d198bc6f_1" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p role="main" tabindex="0"><h2 id="h.m56w2pmswrzx" dir="ltr"><span>Wow@Home</span></h2></p></div><div id="h.f6d8ee3d198bc6f_27" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>A </span><span>network of small</span><span> </span><span>radio telescopes</span><span> offers several distinct advantages compared to large professional observatories. These systems are low</span><span>-</span><span>cost an</span><span>d</span><span> can operate autonomously around the clock, making them ideal for continuous monitoring of transient events or long-duration signals that professional telescopes cannot commit to observing full-time.</span></p><p dir="ltr"><span>Their geographic distribution enables global sky coverage and coordinated observations across different time zones, which is especially valuable for </span><span>validating repeating or time-variable signals</span><span>. Coincidence detection across multiple stations helps </span><span>reject local radio frequency interference (RFI)</span><span>, increasing confidence in true astrophysical or technosignature candidates.</span></p><p dir="ltr"><span>These networks are also highly scalable, resilient to single-point failures, and capable of </span><span>rapid response to external alerts</span><span>. Furthermore, they are cost-effective, engaging, and accessible, </span><span>ideal for education, citizen science, and expanding participation in radio astronomy</span><span>.</span></p><p dir="ltr"><span>However, these systems also come with notable limitations when compared to professional telescopes. They have </span><span>significantly lower sensitivity</span><span>, limiting their ability to detect faint or distant sources. Their angular resolution is poor due to smaller dish sizes and wide beamwidths, making </span><span>precise source localization difficult</span><span>.</span></p><p dir="ltr"><span>Calibration can be inconsistent across stations</span><span>, and frequency stability or dynamic range may not match the performance of professional-grade equipment. Additionally, without standardized equipment and protocols, data quality and interoperability can vary across the network. Despite these constraints, when thoughtfully coordinated, such networks can </span><span>provide valuable complementary observations to professional facilities</span><span>.</span></p></div><div id="h.3e7c17c5694c8634_48" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><h2 id="h.xu5ifbv830af_l" dir="ltr"><div jscontroller="Ae65rd" jsaction="touchstart:UrsOsc; click:KjsqPd; focusout:QZoaZ; mouseover:y0pDld; mouseout:dq0hvd;fv1Rjc:jbFSOd;CrfLRd:SzACGe;"><p><span>The </span><span>Wow@Home Radio Telescope</span></p></div></h2></div><div id="h.3e7c17c5694c8634_44" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>This page presents a test of our first </span><span>Wow@Home Radio Telescope</span><span> hardware and software configuration (Figure 1). The system is tested for a network of small radio telescopes designed to emulate, as closely as possible, the observation protocol of the meridian radio telescope </span><span>Big Ear</span><span> used by the </span><span>Ohio SETI project</span><span> in the 1970s. As in the </span><a href="http://www.bigear.org/Wow30th/wow30th.htm" target="_blank"><span>original setup</span></a><span>, we use a </span><span>10 kHz channel width</span><span> and a </span><span>12-second integration time</span><span>. However, our system differs in several ways: it features 256 channels instead of 50, a much larger beam size, but significantly lower sensitivity.</span></p><p dir="ltr"><span>The telescope is </span><span>fixed at a constant elevation</span><span>, pointed south, and scans a specific celestial declination over the course of one or more days using a wide field of view of approximately 25° (HPBW or its beamwidth). As the Earth rotates, this configuration allows the telescope to capture a </span><span>continuous 360° strip of the sky</span><span> at that declination. After completing three or more full-sky passes, the telescope is adjusted to a new elevation to begin scanning a different declination, gradually building up </span><span>full-sky coverage over time</span><span>.</span></p><p dir="ltr"><span>While optimized for </span><span>educational use</span><span>, this configuration also yields valuable data on RFI near the H I line in urban environments, helping us assess the likelihood of RFI mimicking a Wow!-like signal. Additionally, it serves as a practical platform for a </span><span>wide-field search for strong transient events</span><span>, whether of astrophysical origin or potential technosignatures.</span></p><p dir="ltr"><span>For events that persist longer than a day, </span><span>multiple observing passes</span><span> can be used to validate their presence, detect weaker features, improve overall sensitivity, and help distinguish them from RFI. Additionally, </span><span>simultaneous observations</span><span> by two or more telescopes pointed at the same location can further aid in rejecting local interference and confirming the reality of signals that last less than 24 hours.</span></p><p dir="ltr"><span>The Wow@Home Radio Telescope operates autonomously, 24/7, as a meridian-style instrument, conducting a continuous </span><span>all-sky survey</span><span> </span><span>for transient events</span><span>. The hardware required to build these telescopes is both </span><span>inexpensive and widely accessible</span><span>, relying on readily available components. </span><span>The critical element lies in the software</span><span>, which must be capable of analyzing data effectively, whether from a single station or across a coordinated network of telescopes.</span></p><p dir="ltr"><span>Future expansions could include the integration of </span><span>multibeam systems</span><span> to enable simultaneous ON–OFF observations </span><span>to improve sensitivity</span><span>, </span><span>tracking capability</span><span> to perform targeted observations of specific sources, </span><span>multi-site detection</span><span> for signal validation, higher sensitivity, and RFI discrimination, </span><span>interferometric capabilities</span><span> for improved angular resolution, and </span><span>phased array configurations</span><span> to enhance sensitivity and enable electronic beam steering.</span></p></div><div id="h.3e7c17c5694c8634_3" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p><img src="https://lh3.googleusercontent.com/A5qS8iP4yHHZob04FJu8ja9Yf1wfndGkZ_jZD5ng32OkIaWIn0i2yyo47SUkBpoWr64Mxka8U4MbdyZMRsQGyIkk_K8z5--u0LU3PokF8Y1snW6TSvEkKvvducYt9A4RMw=w1280" role="img"></p></div><div id="h.3e7c17c5694c8634_13" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>Figure 1: </span><span>Components of our first Wow@Home Radio Telescope. The </span><a href="https://github.com/tedcline/ezRA" target="_blank"><span>Easy Radio Astronomy (ezRA)</span></a><span> software is an excellent starter package for getting this configuration up and running for radio astronomy. We plan to test additional configurations in the coming months, including the </span><a href="https://www.crowdsupply.com/krakenrf/discovery-dish" target="_blank"><span>Discovery Dish</span></a><span>, which integrates the frontend into the antenna, and the </span><a href="https://airspy.com/airspy-mini/" target="_blank"><span>Airspy Mini</span></a><span> as the backend, offering a 12-bit ADC for improved dynamic range.</span></p></div><div id="h.3e7c17c5694c8634_30" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>The </span><span>Wow@Home Software</span><span> is the core of our project. It serves as the data acquisition and analysis platform designed to search for transient events caused by astrophysical phenomena, potential technosignatures, and RFI characterization, using data from any small radio telescope. The software is built on the analysis methods we are developing to detect Wow-like signals in the archive data of professional observatories, as part of our </span><a href="https://phl.upr.edu/wow"><span>Arecibo Wow! Project</span></a><span>. We are currently developing the software in </span><a href="https://www.nv5geospatialsoftware.com/Products/IDL" target="_blank"><span>IDL</span></a><span>, with example outputs shown in Figures 2, 3, and 4. It will later be translated to Python to ensure cross-platform compatibility and broader accessibility.</span></p></div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1" id="h.f6d8ee3d198bc6f_8"><div id="h.331927cc8f5c07b8_4"><p><img src="https://lh4.googleusercontent.com/zhgcn2e7vYwMCacemRJOlqkF5VcZcPjS6d8s5sf80ZYdLPNsoSpT0vNAZs7RIVnrGzhbHlboR0S0chVQrL1nGlPG6eynsKj6hH73NCSuuGDFrYGboTSbP8jtSwohUJC2vw=w1280" role="img"></p></div><div id="h.3e7c17c5694c8634_4"><p dir="ltr"><span>Figure </span><span>2</span><span>: </span><span>This is a test run of the Wow@Home Radio Telescope</span><span>. </span><span>The top panel shows the </span><span>relative</span><span> power as a function of time. The </span><span>next</span><span> panel </span><span>is</span><span> the signal-to-noise ratio (SNR). Most RFI here originates from continuum sources, which are relatively easy to filter out. </span><span>The following dynamic spectra images show three different ways to analyze the data, depending on the type of signal of interest. The broadband SNR is suitable for detecting continuum sources, but RFI heavily contaminates it. A second telescope at a different location could be used to cross-correlate astronomical signals. The mediumband SNR is good for highlighting the</span><span> Galactic center transiting </span><span>after</span><span> 6 hours and the Galactic anticenter </span><span>about</span><span> </span><span>12</span><span> hours later. The narrowband SNR is more sensitive to signals oc</span><span>curring in only one channel. </span><span>The horizontal line at channel 224 </span><span>is an injected</span><span> test signal spanning the telescope’s beamwidth. An actual narrowband RFI event is visible near channel 0 after 15 hours.</span></p></div></div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1" id="h.f6d8ee3d198bc6f_23"><div id="h.5ad76efa4068bcdf_14"><p><img src="https://lh4.googleusercontent.com/wurJ-qrx1IZw_zkvp20ykcD736bQVpzjvXIlsXw0zhCGxcojIRQMiB-Fv8DJtt_9LSaYWyN1IN8daAZTjlNhwWg905MYGhiU7TKfwLVL3GZ7zC_mIz-T7OOS0t9ro8Qi=w1280" role="img"></p></div><div id="h.f6d8ee3d198bc6f_20"><p dir="ltr"><span>Figure 3: </span><a href="https://www.britannica.com/science/hydrogen-cloud" target="_blank"><span>Neutral Hydrogen</span></a><span> (H I) spectral profile of the Galactic center, extracted from the data in Figure 2 at 6.5 hours. Error bars represent the 1σ uncertainty in each frequency channel.</span></p></div></div><div id="h.3e7c17c5694c8634_36" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p><img src="https://lh6.googleusercontent.com/hMIAxkutySlccvKGun3Pd3CByn7EsvLmj8TkK5o3QYFmBoSQwug-dYthHbmg3DS0Dtn1e9C_INbNOE4iyTW3kj_czt590rnQ-rgksTYY7dY1bQPvL-8StNajaFB2AIkL7Q=w1280" role="img"></p></div><div id="h.3e7c17c5694c8634_40" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>Figure 4: </span><span>In addition to the modern analysis tools available with today’s radio telescopes, we also aim to incorporate into our software the ability to generate a live preview of the data in the style of the original Ohio State SETI project printouts. This feature is intended to provide historical context and connect current efforts to the legacy of early SETI research. Above is an example using the original Wow! Signal data.</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A kernel developer plays with Home Assistant (164 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/</link>
            <guid>44011381</guid>
            <pubDate>Sat, 17 May 2025 01:36:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/">https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/</a>, See on <a href="https://news.ycombinator.com/item?id=44011381">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
Those of us who have spent our lives playing with computers naturally see
the appeal of deploying them though the home for both data acquisition and
automation.  But many of us who have watched the evolution of the
technology industry are increasingly unwilling to entrust critical
household functions to cloud-based servers run by companies that may not
have our best interests at heart.  The Apache-licensed <a href="https://www.home-assistant.io/">Home Assistant</a> project offers a
welcome alternative: locally controlled automation with free software.
This two-part series covers roughly a year of Home Assistant use, starting
with a set of overall observations about the project.
</p><p>
This is not the first time that LWN has looked at this project, of course;
<a href="https://lwn.net/Articles/822350/">this review</a> gives a snapshot of what Home
Assistant looked like five years ago, while <a href="https://lwn.net/Articles/947843/">this 2023 article</a> gives a good overview of the
project's history, governance, and overall direction.  I will endeavor to
not duplicate that material here.
</p><h4>Project health</h4>
<p>
At a first glance, Home Assistant bears some of the hallmarks of a
company-owned project.  The company in question, <a href="https://www.nabucasa.com/">Nabu Casa</a>, was formed around the
project and employs a number of its key developers.  One of the ways in
which the company makes money is with a $65/year subscription service, providing
remote access to Home Assistant servers installed on firewalled residential
networks.  Home Assistant has support for that remote option, and no
others.  It would be interesting to see what would happen to a pull request
adding support for, say, <a href="https://opensprinklershop.de/en/2023/01/22/opensprinkler-fernzugriff-mit-openthings-cloud-otc-token/">OpenThings
Cloud</a> as an alternative.  The fate of that request would say a lot
about how open the project really is.
</p><blockquote>
	<b>Like what you are reading?</b>
    		<a href="https://lwn.net/Promo/slink-trial-terse/claim">Try LWN for free</a> for 1 month,
    		no credit card required.
</blockquote>
<p>
(For the record, I have bought the Nabu Casa subscription rather than, say,
using WireGuard to make a port available on an accessible system; it is a
hassle-free way to solve the problem and support the development of this
software).
</p><p>
That said, most of the warning signs that accompany a corporate-controlled
project are not present with Home Assistant.  The project's <a href="https://github.com/home-assistant/core/blob/dev/CLA.md">contributor
license agreement</a> is a derivative of the kernel's developer certificate
of origin; contributors retain their copyright on their work.  Since the <a href="https://www.home-assistant.io/blog/2024/04/03/release-20244/">2024.4
release</a>, the Home Assistant core repository has acquired over 17,000
changesets from over 900 contributors.  While a number of Nabu Casa
employees (helpfully listed on <a href="https://www.nabucasa.com/about/">this page</a>) appear in the top ten
contributors, they do not dominate that list.
</p><p>
Home Assistant is clearly an active project with a wide developer base.  In
2024, overall responsibility for this project was transferred to <a href="https://www.openhomefoundation.org/blog/announcing-the-open-home-foundation/">the
newly created Open Home Foundation</a>.  This project is probably here to
stay, and seems unlikely to take a hostile turn in the future.  For a
system that sits at the core of one's home, those are important
characteristics.
</p><h4>Installation and setup</h4>
<p>
Linux users tend to be somewhat spoiled; installing a new application is
typically a matter of a single package-manager command.  Home Assistant
does not really fit into that model.  The first three options on <a href="https://www.home-assistant.io/installation/">the installation
page</a> involve dedicated computers — two of which are sold by Nabu Casa.
For those wanting to install it on a general-purpose computer, the
recommended course is to install the <a href="https://github.com/home-assistant/operating-system">Home Assistant
Operating System</a>, a bespoke Linux distribution that runs Home Assistant
within a Docker container.  There is also a container-based method that can
run on another distribution, but this installation does not support <a href="https://www.home-assistant.io/addons/">the add-ons feature</a>.
</p><p>
Home Assistant, in other words, is not really set up to be just another
application on a Linux system.  If one scrolls far enough, though, one will
find, the instructions to install onto a "normal" Linux system, suitably
guarded with warnings about how it is an "<q>advanced</q>" method.
Of course, that is what I did, putting the software onto an existing system
running Fedora.  The whole thing subsequently broke when a
distribution upgrade replaced Python, but that was easily enough repaired.
As a whole, the installation has worked as expected.
</p><p>
Out of the box, though, a new Home Assistant installation does not do much.
Its job, after all, is to interface with the systems throughout the house,
and every house is different.  While Home Assistant can find some systems
automatically (it found the Brother printer and dutifully informed me that
the device was, inevitably, low on cyan toner), it usually needs to be
told about what is installed in the house.  Thus, the user quickly delves
into the world of "integrations" — the device drivers of Home Assistant.
</p><p>
For each remotely accessible device in the house, there is, hopefully, at
least one integration available that allows Home Assistant to work with it.
Many integrations are packaged with the system itself, and can be found by
way of a simple search screen in the Home Assistant web interface.  A much
larger set is packaged separately, usually in the <a href="https://www.hacs.xyz/">Home Assistant Community Store</a>, or HACS;
it is fair to say that most users will end up getting at least some
integrations from this source.  Setting up HACS requires a few steps and,
unfortunately, requires the user to have a GitHub account for full
integration.  It <i>is</i> possible to install HACS integrations without
that account, but it is a manual process that loses support for features
like update tracking.
</p><p>
Most integrations, at setup time, will discover any of the appropriate
devices on the network — if those devices support that sort of discovery,
of course.  Often, using an integration will require the credentials to log
into the cloud account provided by the vendor of the devices in question.
When possible, integrations mostly strive to operate entirely locally; some
only use the cloud connection for the initial device discovery.  When there
is no alternative, though, integrations will remain logged into the cloud
account and interact with their devices that way; this mode may or may not
be supported (or condoned) by the vendor.  There are, of course, some
vendors that <a href="https://www.bleepingcomputer.com/news/security/haier-hits-home-assistant-plugin-dev-with-takedown-notice/">are
actively hostile</a> to integration with Home Assistant.
</p><p>
As might be expected, the quality of integrations varies widely.  Most of
the integrations I have tried have worked well enough.  The OpenSprinkler
(<a href="https://lwn.net/Articles/940509/">reviewed here</a> in 2023) integration,
instead, thoroughly corrupted the device configuration, exposing me to the
shame of being seen with a less-than-perfect lawn; it was quickly
removed.  It is an especially nice surprise when a device comes with Home
Assistant support provided by the vendor, but that is still a relatively
rare occurrence.  Home Assistant now is in a position similar to Linux
25&nbsp;years ago; many devices are supported, but often in spite of their
vendor, and one has to choose components carefully.
</p><h4>Security</h4>
<p>
Home Assistant sits at the core of the home network; it has access to
sensors that can reveal a lot about the occupants of the home, and it
collects data in a single location.  An installation will be exposed to the
Internet if its owner needs remote access.  There is clearly potential for
a security disaster here.
</p><p>
The project has <a href="https://www.home-assistant.io/security/">a posted
security policy</a> describing the project's stance; it asks for a 90-day
embargo on the reporting of any security issues.  Authors writing about the
project's security are encouraged to run their work past the project "<q>so
we can ensure that all claims are correct</q>".  The security policy
explicitly excludes reports regarding third-party integrations (the core
project cannot fix those, after all).  The project is also uninterested in
any sort of privilege escalation by users who are logged into Home
Assistant, assuming that anybody who has an account is fully trusted.
</p><p>
The project has only issued <a href="https://github.com/home-assistant/core/security/advisories/GHSA-m3pm-rpgg-5wj6">one
security advisory</a> since the beginning of 2024.  There were several in
2023, mostly as the result of <a href="https://github.blog/security/vulnerability-research/securing-our-home-labs-home-assistant-code-review/">a
security audit</a> performed by GitHub.
</p><p>

There is no overall vetting of third-party integrations, which are, in the
end, just more Python code.  So loading an unknown integration is similar
to importing an unknown module from PyPI; it will probably work, but the
potential for trouble is there.  The project has occasionally <a href="https://www.home-assistant.io/blog/2021/01/23/security-disclosure2/">reported
security problems in third-party integrations</a>, but such reports are
rare.  I am unable to find any reports of actively malicious integrations
in the wild, but one seems destined to appear sooner or later.
</p><h4>Actually doing something with Home Assistant</h4>
<p>
The first step for the owner of a new Home Assistant installation is,
naturally, to seek out integrations for the devices installed in the home.
On successful installation and initialization, an integration will add one
or more "devices" to the system, each of which has some number of "sensors"
for data it reports, and possible "controls" to change its operating state.
A heat-pump head, for example, may have sensors for the current temperature
and humidity, and controls for its operating mode, fan speed, vane
direction, and more.
</p><p>
It is worth noting that the setup of these entities seems a bit
non-deterministic at times.  My solar system has 22&nbsp;panels with
inverters, each of which reports nearly a dozen parameters (voltage,
current, frequency, temperature, etc.).  There is no easy way to determine
which panel is reporting, for example, <tt>sensor_amps_12</tt>, especially
since <tt>sensor_frequency_12</tt> almost certainly corresponds to a
<i>different</i> panel.  My experience is that Home Assistant is a system
for people who are willing to spend a lot of time fiddling around with
things to get them to a working state.  Dealing with these sensors was an
early introduction to that; it took some time to figure out the mapping
between names and rooftop positions, then to rename each sensor to
something more helpful.
</p><p>
The next level of fiddling around is setting up dashboards.  Home Assistant
offers a great deal of flexibility in the information and controls it
provides to the user; it is possible to set up screens focused on, say,
energy production or climate control.  Happily, the days when this
configuration had to be done by writing YAML snippets are mostly in the
past at this point; one occasionally still has to dip into YAML, but it
does not happen often.  The interface is not always intuitive,
but it is fairly slick, interactive, and functional.
</p><p>
Another part of Home Assistant that I have not yet played with much
is automations and scenes.  Automations are simple rule-triggered programs
that make changes to some controls.  They can carry out actions like
"turn on the front light when it gets dark" or "play scary music if
somebody rings the doorbell and nobody is home".  Scenes are sets of canned
device configurations.  One might create a scene called "in-laws visiting"
that plays loud punk music, sets the temperature to just above freezing,
disables all voice control, and tunes all of the light bulbs to 6000K, for
example.
</p><p>
The good news is that, unless the fiddling itself is the point (and it can
be a good one), there comes a time when things just work and the fiddling
can stop.  A well-configured Home Assistant instance provides detailed
information about the state of the home — and control where the devices
allow it — to any web browser that can reach it and log in.  There are
(open-source) apps that bring this support to mobile devices in a way that
is nearly indistinguishable from how the web interface works.
</p><p>
All told, it is clear why Home Assistant has a strong and growing
following.  It is an open platform that brings control to an industry that
is doing its best to keep a firm grasp on our homes and the data they
create.  Home Assistant shows that we can do nicely without all of these
fragile, non-interoperable, rug-pull-susceptible cloud systems.  Just like
Linux proved that we can have control over our computers, Home Assistant
shows that we do not have to surrender control over our homes.
</p><p>
This article has gotten long, and is remarkably short on interesting things
that one can actually <i>do</i> with Home Assistant.  There are some
interesting stories to be told along those lines; they will appear shortly
in <a href="https://lwn.net/Articles/1017945/">the second, concluding part</a> of this series.<br clear="all"></p>
               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
    </channel>
</rss>