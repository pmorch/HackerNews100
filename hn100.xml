<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 02 Oct 2025 05:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Cormac McCarthy's personal library (146 pts)]]></title>
            <link>https://www.smithsonianmag.com/arts-culture/two-years-cormac-mccarthys-death-rare-access-to-personal-library-reveals-man-behind-myth-180987150/</link>
            <guid>45444694</guid>
            <pubDate>Wed, 01 Oct 2025 23:06:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/arts-culture/two-years-cormac-mccarthys-death-rare-access-to-personal-library-reveals-man-behind-myth-180987150/">https://www.smithsonianmag.com/arts-culture/two-years-cormac-mccarthys-death-rare-access-to-personal-library-reveals-man-behind-myth-180987150/</a>, See on <a href="https://news.ycombinator.com/item?id=45444694">Hacker News</a></p>
Couldn't get https://www.smithsonianmag.com/arts-culture/two-years-cormac-mccarthys-death-rare-access-to-personal-library-reveals-man-behind-myth-180987150/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Edge264 – Minimalist, high-performance software decoder for H.264/AVC video (104 pts)]]></title>
            <link>https://github.com/tvlabs/edge264</link>
            <guid>45443462</guid>
            <pubDate>Wed, 01 Oct 2025 21:00:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tvlabs/edge264">https://github.com/tvlabs/edge264</a>, See on <a href="https://news.ycombinator.com/item?id=45443462">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">edge264</h2><a id="user-content-edge264" aria-label="Permalink: edge264" href="#edge264"></a></p>
<p dir="auto">Minimalist software decoder with state-of-the-art performance for the H.264/AVC video format.</p>
<p dir="auto"><em>Please note this is a work in progress and will be ready for use after making GStreamer/VLC plugins.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Supports <strong>Progressive High</strong> and <strong>MVC 3D</strong> profiles, up to level 6.2</li>
<li>Any resolution up to 8K UHD</li>
<li>8-bit 4:2:0 planar YUV output</li>
<li>Slices and Arbitrary Slice Order</li>
<li>Slice and frame multi-threading</li>
<li>Per-slice reference picture list</li>
<li>Memory Management Control Operations</li>
<li>Long-term reference frames</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported platforms</h2><a id="user-content-supported-platforms" aria-label="Permalink: Supported platforms" href="#supported-platforms"></a></p>
<ul dir="auto">
<li>Windows: x86, x64</li>
<li>Linux: x86, x64, ARM64</li>
<li>Mac OS: x64</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compiling and testing</h2><a id="user-content-compiling-and-testing" aria-label="Permalink: Compiling and testing" href="#compiling-and-testing"></a></p>
<p dir="auto">edge264 is entirely developed in C using 128-bit <a href="https://gcc.gnu.org/onlinedocs/gcc/Vector-Extensions.html" rel="nofollow">vector extensions</a> and vector intrinsics, and can be compiled with GNU GCC or LLVM Clang. <a href="https://www.libsdl.org/" rel="nofollow">SDL2</a> runtime library may be used (optional) to enable display with <code>edge264_test</code>.</p>
<p dir="auto">Here are the <code>make</code> options for tuning the compiled library file:</p>
<ul dir="auto">
<li><code>CC</code> - C compiler used to convert source files to object files (default <code>cc</code>)</li>
<li><code>CFLAGS</code> - additional compilation flags passed to <code>CC</code> and <code>TARGETCC</code></li>
<li><code>TARGETCC</code> - C compiler used to link object files into library file (default <code>CC</code>)</li>
<li><code>LDFLAGS</code> - additional compilation flags passed to <code>TARGETCC</code></li>
<li><code>TARGETOS</code> - resulting file naming convention among <code>Windows</code>|<code>Linux</code>|<code>Darwin</code> (default host)</li>
<li><code>VARIANTS</code> - comma-separated list of additional variants included in the library and selected at runtime (default <code>logs</code>)
<ul dir="auto">
<li><code>x86-64-v2</code> - variant compiled for x86-64 microarchitecture level 2 (SSSE3, SSE4.1 and POPCOUNT)</li>
<li><code>x86-64-v3</code> - variant compiled for x86-64 microarchitecture level 3 (AVX2, BMI, LZCNT, MOVBE)</li>
<li><code>logs</code> - variant compiled with logging support in YAML format (headers and slices)</li>
</ul>
</li>
<li><code>BUILD_TEST</code> - toggles compilation of <code>edge264_test</code> (default <code>yes</code>)</li>
<li><code>FORCEINTRIN</code> - enforce the use of intrinsics among <code>x86</code>|<code>ARM64</code> (for WebAssembly)</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="$ make CFLAGS=&quot;-march=x86-64&quot; VARIANTS=x86-64-v2,x86-64-v3 BUILD_TEST=no # example x86 build"><pre>$ make CFLAGS=<span><span>"</span>-march=x86-64<span>"</span></span> VARIANTS=x86-64-v2,x86-64-v3 BUILD_TEST=no <span><span>#</span> example x86 build</span></pre></div>
<p dir="auto">The automated test program <code>edge264_test</code> can browse files in a given directory, decoding each <code>&lt;video&gt;.264</code> file and comparing its output with each sibling file <code>&lt;video&gt;.yuv</code> if found. On the set of AVCv1, FRExt and MVC <a href="https://www.itu.int/wftp3/av-arch/jvt-site/draft_conformance/" rel="nofollow">conformance bitstreams</a>, 109/224 files are decoded without errors, the rest using yet unsupported features.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ make
$ ./edge264_test --help # prints all options available
$ ffmpeg -i vid.mp4 -vcodec copy -bsf h264_mp4toannexb -an vid.264 # optional, converts from MP4 format
$ ./edge264_test -d vid.264 # replace -d with -b to benchmark instead of display"><pre>$ make
$ ./edge264_test --help <span><span>#</span> prints all options available</span>
$ ffmpeg -i vid.mp4 -vcodec copy -bsf h264_mp4toannexb -an vid.264 <span><span>#</span> optional, converts from MP4 format</span>
$ ./edge264_test -d vid.264 <span><span>#</span> replace -d with -b to benchmark instead of display</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example code</h2><a id="user-content-example-code" aria-label="Permalink: Example code" href="#example-code"></a></p>
<p dir="auto">Here is a complete example that opens an input file in Annex B format from command line, and dumps its decoded frames in planar YUV order to standard output. See <a href="https://github.com/tvlabs/edge264/blob/master/edge264_test.c">edge264_test.c</a> for a more complete example which can also display frames.</p>
<div dir="auto" data-snippet-clipboard-copy-content="#include <fcntl.h>
#include <unistd.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <sys/types.h>

#include &quot;edge264.h&quot;

int main(int argc, char *argv[]) {
	int fd = open(argv[1], O_RDONLY);
	struct stat st;
	fstat(fd, &amp;st);
	uint8_t *buf = mmap(NULL, st.st_size, PROT_READ, MAP_SHARED, fd, 0);
	const uint8_t *nal = buf + 3 + (buf[2] == 0); // skip the [0]001 delimiter
	const uint8_t *end = buf + st.st_size;
	// auto threads, no logs, auto allocs
	Edge264Decoder *dec = edge264_alloc(-1, NULL, NULL, 0, NULL, NULL, NULL);
	Edge264Frame frm;
	int res;
	do {
		res = edge264_decode_NAL(dec, nal, end, 0, NULL, NULL, &amp;nal);
		while (!edge264_get_frame(dec, &amp;frm, 0)) {
			for (int y = 0; y < frm.height_Y; y++)
				write(1, frm.samples[0] + y * frm.stride_Y, frm.width_Y);
			for (int y = 0; y < frm.height_C; y++)
				write(1, frm.samples[1] + y * frm.stride_C, frm.width_C);
			for (int y = 0; y < frm.height_C; y++)
				write(1, frm.samples[2] + y * frm.stride_C, frm.width_C);
		}
	} while (res == 0 || res == ENOBUFS);
	edge264_free(&amp;dec);
	munmap(buf, st.st_size);
	close(fd);
	return 0;
}"><pre><span>#include</span> <span>&lt;fcntl.h&gt;</span>
<span>#include</span> <span>&lt;unistd.h&gt;</span>
<span>#include</span> <span>&lt;sys/mman.h&gt;</span>
<span>#include</span> <span>&lt;sys/stat.h&gt;</span>
<span>#include</span> <span>&lt;sys/types.h&gt;</span>

<span>#include</span> <span>"edge264.h"</span>

<span>int</span> <span>main</span>(<span>int</span> <span>argc</span>, <span>char</span> <span>*</span><span>argv</span>[]) {
	<span>int</span> <span>fd</span> <span>=</span> <span>open</span>(<span>argv</span>[<span>1</span>], <span>O_RDONLY</span>);
	<span>struct</span> <span>stat</span> <span>st</span>;
	<span>fstat</span>(<span>fd</span>, <span>&amp;</span><span>st</span>);
	<span>uint8_t</span> <span>*</span><span>buf</span> <span>=</span> <span>mmap</span>(<span>NULL</span>, <span>st</span>.<span>st_size</span>, <span>PROT_READ</span>, <span>MAP_SHARED</span>, <span>fd</span>, <span>0</span>);
	<span>const</span> <span>uint8_t</span> <span>*</span><span>nal</span> <span>=</span> <span>buf</span> <span>+</span> <span>3</span> <span>+</span> (<span>buf</span>[<span>2</span>] <span>==</span> <span>0</span>); <span>// skip the [0]001 delimiter</span>
	<span>const</span> <span>uint8_t</span> <span>*</span><span>end</span> <span>=</span> <span>buf</span> <span>+</span> <span>st</span>.<span>st_size</span>;
	<span>// auto threads, no logs, auto allocs</span>
	<span>Edge264Decoder</span> <span>*</span><span>dec</span> <span>=</span> <span>edge264_alloc</span>(<span>-1</span>, <span>NULL</span>, <span>NULL</span>, <span>0</span>, <span>NULL</span>, <span>NULL</span>, <span>NULL</span>);
	<span>Edge264Frame</span> <span>frm</span>;
	<span>int</span> <span>res</span>;
	<span>do</span> {
		<span>res</span> <span>=</span> <span>edge264_decode_NAL</span>(<span>dec</span>, <span>nal</span>, <span>end</span>, <span>0</span>, <span>NULL</span>, <span>NULL</span>, <span>&amp;</span><span>nal</span>);
		<span>while</span> (!<span>edge264_get_frame</span>(<span>dec</span>, <span>&amp;</span><span>frm</span>, <span>0</span>)) {
			<span>for</span> (<span>int</span> <span>y</span> <span>=</span> <span>0</span>; <span>y</span> <span>&lt;</span> <span>frm</span>.<span>height_Y</span>; <span>y</span><span>++</span>)
				<span>write</span>(<span>1</span>, <span>frm</span>.<span>samples</span>[<span>0</span>] <span>+</span> <span>y</span> <span>*</span> <span>frm</span>.<span>stride_Y</span>, <span>frm</span>.<span>width_Y</span>);
			<span>for</span> (<span>int</span> <span>y</span> <span>=</span> <span>0</span>; <span>y</span> <span>&lt;</span> <span>frm</span>.<span>height_C</span>; <span>y</span><span>++</span>)
				<span>write</span>(<span>1</span>, <span>frm</span>.<span>samples</span>[<span>1</span>] <span>+</span> <span>y</span> <span>*</span> <span>frm</span>.<span>stride_C</span>, <span>frm</span>.<span>width_C</span>);
			<span>for</span> (<span>int</span> <span>y</span> <span>=</span> <span>0</span>; <span>y</span> <span>&lt;</span> <span>frm</span>.<span>height_C</span>; <span>y</span><span>++</span>)
				<span>write</span>(<span>1</span>, <span>frm</span>.<span>samples</span>[<span>2</span>] <span>+</span> <span>y</span> <span>*</span> <span>frm</span>.<span>stride_C</span>, <span>frm</span>.<span>width_C</span>);
		}
	} <span>while</span> (<span>res</span> <span>==</span> <span>0</span> <span>||</span> <span>res</span> <span>==</span> <span>ENOBUFS</span>);
	<span>edge264_free</span>(<span>&amp;</span><span>dec</span>);
	<span>munmap</span>(<span>buf</span>, <span>st</span>.<span>st_size</span>);
	<span>close</span>(<span>fd</span>);
	<span>return</span> <span>0</span>;
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">API reference</h2><a id="user-content-api-reference" aria-label="Permalink: API reference" href="#api-reference"></a></p>
<p dir="auto"><code>const uint8_t * <b>edge264_find_start_code(buf, end, four_byte)</b></code></p>
<p dir="auto">Return a pointer to the next three or four byte (0)001 start code prefix, or <code>end</code> if not found.</p>
<ul dir="auto">
<li><code>const uint8_t * buf</code> - first byte of buffer to search into</li>
<li><code>const uint8_t * end</code> - first invalid byte past the buffer that stops the search</li>
<li><code>int four_byte</code> - if 0 seek a 001 prefix, otherwise seek a 0001</li>
</ul>
<hr>
<p dir="auto"><code>Edge264Decoder * <b>edge264_alloc(n_threads, log_cb, log_arg, log_mbs, alloc_cb, free_cb, alloc_arg)</b></code></p>
<p dir="auto">Allocate and initialize a decoding context.</p>
<ul dir="auto">
<li><code>int n_threads</code> - number of background worker threads, with 0 to disable multithreading and -1 to detect the number of logical cores at runtime</li>
<li><code>void (* log_cb)(const char * str, void * log_arg)</code> - if not NULL, a <code>fputs</code>-compatible function pointer that <code>edge264_decode_NAL</code> will call to log every header, SEI or macroblock (requires the <code>logs</code> variant otherwise fails at runtime, called from the same thread except macroblocks in multithreaded decoding)</li>
<li><code>void * log_arg</code> - custom value passed to <code>log_cb</code></li>
<li><code>int log_mbs</code> - set to 1 to enable logging of macroblocks</li>
<li><code>void (* alloc_cb)(void ** samples, unsigned samples_size, void ** mbs, unsigned mbs_size, int errno_on_fail, void * alloc_arg)</code> - if not NULL, a function pointer that <code>edge264_decode_NAL</code> will call (on the same thread) instead of malloc to request allocation of samples and macroblock buffers for a frame (<code>errno_on_fail</code> is ENOMEM for mandatory allocations, or ENOBUFS for allocations that may be skipped to save memory but reduce playback smoothness)</li>
<li><code>void (* free_cb)(void * samples, void * mbs, void * alloc_arg)</code> - if not NULL, a function pointer that <code>edge264_decode_NAL</code> and <code>edge264_free</code> will call (on the same thread) to free buffers allocated through <code>alloc_cb</code></li>
<li><code>void * alloc_arg</code> - custom value passed to <code>alloc_cb</code> and <code>free_cb</code></li>
</ul>
<hr>
<p dir="auto"><code>int <b>edge264_decode_NAL(dec, buf, end, non_blocking, free_cb, free_arg, next_NAL)</b></code></p>
<p dir="auto">Decode a single NAL unit containing any parameter set or slice.</p>
<ul dir="auto">
<li><code>Edge264Decoder * dec</code> - initialized decoding context</li>
<li><code>const uint8_t * buf</code> - first byte of NAL unit (containing <code>nal_unit_type</code>)</li>
<li><code>const uint8_t * end</code> - first byte past the buffer (max buffer size is 2<sup>31</sup>-1 on 32-bit and 2<sup>63</sup>-1 on 64-bit)</li>
<li><code>int non_blocking</code> - set to 1 if the current thread has other processing thus cannot block here</li>
<li><code>void (* free_cb)(void * free_arg, int ret)</code> - callback that may be called from another thread when multithreaded, to signal the end of parsing and release the NAL buffer</li>
<li><code>void * free_arg</code> - custom value that will be passed to <code>free_cb</code></li>
<li><code>const uint8_t ** next_NAL</code> - if not NULL and the return code is <code>0</code>|<code>ENOTSUP</code>|<code>EBADMSG</code>, will receive a pointer to the next NAL unit after the next start code in an Annex B stream</li>
</ul>
<p dir="auto">Return codes are:</p>
<ul dir="auto">
<li><code>0</code> on success</li>
<li><code>ENOTSUP</code> on unsupported stream (decoding may proceed but could return zero frames)</li>
<li><code>EBADMSG</code> on invalid stream (decoding may proceed but could show visual artefacts, if you can check with another decoder that the stream is actually flawless, please consider filling a bug report 🙏)</li>
<li><code>EINVAL</code> if the function was called with <code>dec == NULL</code> or <code>dec-&gt;buf == NULL</code></li>
<li><code>ENODATA</code> if the function was called while <code>dec-&gt;buf &gt;= dec-&gt;end</code></li>
<li><code>ENOMEM</code> if <code>malloc</code> failed to allocate memory</li>
<li><code>ENOBUFS</code> if more frames should be consumed with <code>edge264_get_frame</code> to release a picture slot</li>
<li><code>EWOULDBLOCK</code> if the non-blocking function would have to wait before a picture slot is available</li>
</ul>
<hr>
<p dir="auto"><code>int <b>edge264_get_frame(dec, out, borrow)</b></code></p>
<p dir="auto">Fetch the next frame ready for output.</p>
<ul dir="auto">
<li><code>Edge264Decoder * dec</code> - initialized decoding context</li>
<li><code>Edge264Frame *out</code> - a structure that will be filled with data for the frame returned</li>
<li><code>int borrow</code> - if 0 the frame may be accessed until the next call to <code>edge264_decode_NAL</code>, otherwise the frame should be explicitly returned with <code>edge264_return_frame</code>. Note that access is not exclusive, it may be used concurrently as reference for other frames.</li>
</ul>
<p dir="auto">Return codes are:</p>
<ul dir="auto">
<li><code>0</code> on success (one frame is returned)</li>
<li><code>EINVAL</code> if the function was called with <code>dec == NULL</code> or <code>out == NULL</code></li>
<li><code>ENOMSG</code> if there is no frame to output at the moment</li>
</ul>
<p dir="auto">While reference frames may be decoded ahead of their actual display (ex. B-Pyramid technique), all frames are buffered for reordering before being released for display:</p>
<ul dir="auto">
<li>Decoding a non-reference frame releases it and all frames set to be displayed before it.</li>
<li>Decoding a key frame releases all stored frames (but not the key frame itself which might be reordered later).</li>
<li>Exceeding the maximum number of frames held for reordering releases the next frame in display order.</li>
<li>Lacking an available frame buffer releases the next non-reference frame in display order (to salvage its buffer) and all reference frames displayed before it.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct Edge264Frame {
	const uint8_t *samples[3]; // Y/Cb/Cr planes
	const uint8_t *samples_mvc[3]; // second view
	const uint8_t *mb_errors; // probabilities (0..100) for each macroblock to be erroneous, NULL if there are no errors, values are spaced by stride_mb in memory
	int8_t pixel_depth_Y; // 0 for 8-bit, 1 for 16-bit
	int8_t pixel_depth_C;
	int16_t width_Y;
	int16_t width_C;
	int16_t height_Y;
	int16_t height_C;
	int16_t stride_Y;
	int16_t stride_C;
	int16_t stride_mb;
	uint32_t FrameId;
	uint32_t FrameId_mvc; // second view
	int16_t frame_crop_offsets[4]; // {top,right,bottom,left}, useful to derive the original frame with 16x16 macroblocks
	void *return_arg;
} Edge264Frame;"><pre><span>typedef</span> <span>struct</span> <span>Edge264Frame</span> {
	<span>const</span> <span>uint8_t</span> <span>*</span><span>samples</span>[<span>3</span>]; <span>// Y/Cb/Cr planes</span>
	<span>const</span> <span>uint8_t</span> <span>*</span><span>samples_mvc</span>[<span>3</span>]; <span>// second view</span>
	<span>const</span> <span>uint8_t</span> <span>*</span><span>mb_errors</span>; <span>// probabilities (0..100) for each macroblock to be erroneous, NULL if there are no errors, values are spaced by stride_mb in memory</span>
	<span>int8_t</span> <span>pixel_depth_Y</span>; <span>// 0 for 8-bit, 1 for 16-bit</span>
	<span>int8_t</span> <span>pixel_depth_C</span>;
	<span>int16_t</span> <span>width_Y</span>;
	<span>int16_t</span> <span>width_C</span>;
	<span>int16_t</span> <span>height_Y</span>;
	<span>int16_t</span> <span>height_C</span>;
	<span>int16_t</span> <span>stride_Y</span>;
	<span>int16_t</span> <span>stride_C</span>;
	<span>int16_t</span> <span>stride_mb</span>;
	<span>uint32_t</span> <span>FrameId</span>;
	<span>uint32_t</span> <span>FrameId_mvc</span>; <span>// second view</span>
	<span>int16_t</span> <span>frame_crop_offsets</span>[<span>4</span>]; <span>// {top,right,bottom,left}, useful to derive the original frame with 16x16 macroblocks</span>
	<span>void</span> <span>*</span><span>return_arg</span>;
} <span>Edge264Frame</span>;</pre></div>
<hr>
<p dir="auto"><code>void <b>edge264_return_frame(dec, return_arg)</b></code></p>
<p dir="auto">Give back ownership of the frame if it was borrowed from a previous call to <code>edge264_get_frame</code>.</p>
<ul dir="auto">
<li><code>Edge264Decoder * dec</code> - initialized decoding context</li>
<li><code>void * return_arg</code> - the value stored inside the frame to return</li>
</ul>
<hr>
<p dir="auto"><code>void <b>edge264_flush(dec)</b></code></p>
<p dir="auto">For use when seeking, stop all background processing, flush all delayed frames while keeping them allocated, and clear the internal decoder state.</p>
<ul dir="auto">
<li><code>Edge264Decoder * dec</code> - initialized decoding context</li>
</ul>
<hr>
<p dir="auto"><code>void <b>edge264_free(pdec)</b></code></p>
<p dir="auto">Deallocate the entire decoding context, and unset the pointer.</p>
<ul dir="auto">
<li><code>Edge264Decoder ** pdec</code> - pointer to a decoding context, initialized or not</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<ul dir="auto">
<li>Stress testing (in progress)</li>
<li>Multithreading (in progress)</li>
<li>Error recovery (in progress)</li>
<li>Integration in VLC/ffmpeg/GStreamer</li>
<li>ARM32</li>
<li>PAFF and MBAFF</li>
<li>4:0:0, 4:2:2 and 4:4:4</li>
<li>9-14 bit depths with possibility of different luma/chroma depths</li>
<li>Transform-bypass for macroblocks with QP==0</li>
<li>SEI messages</li>
<li>AVX-2 optimizations</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Programming techniques</h2><a id="user-content-programming-techniques" aria-label="Permalink: Programming techniques" href="#programming-techniques"></a></p>
<p dir="auto">I use edge264 to experiment on new programming techniques to improve performance and code size over existing decoders, and presented a few of these techniques at <a href="https://fosdem.org/2024/schedule/event/fosdem-2024-2931-innovations-in-h-264-avc-software-decoding-architecture-and-optimization-of-a-block-based-video-decoder-to-reach-10-faster-speed-and-3x-code-reduction-over-the-state-of-the-art-/" rel="nofollow">FOSDEM'24</a> and <a href="https://fosdem.org/2025/schedule/event/fosdem-2025-5455-more-innovations-in-h-264-avc-software-decoding/" rel="nofollow">FOSDEM'25</a>.</p>
<ol dir="auto">
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_internal.h">Single header file</a> - It contains all struct definitions, common constants and enums, SIMD aliases, inline functions and macros, and exported functions for each source file. To understand the code base you should look at this file first.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_slice.c">Code blocks instead of functions</a> - The main decoding loop is a forward pipeline designed as a DAG loosely resembling hardware decoders, with nodes being non-inlined functions and edges being tail calls. It helps mutualize code branches wherever possible, thus reduces code size to help fit in L1 cache.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_intra.c">Tree branching</a> - Directional intra modes are implemented with a jump table to the leaves of a tree then unconditional jumps down to the trunk. It allows sharing the bottom code among directional modes, to reduce code size.</li>
<li><del>Global context register - The pointer to the main structure holding context data is assigned to a register when supported by the compiler (GCC).</del> This technique was dropped as Clang eventually reached on-par performance, so there is little incentive to maintain this hack.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_internal.h">Default neighboring values</a> (search <code>unavail_mb</code>) - Tests for availability of neighbors are replaced with fake neighboring macroblocks around each frame. It reduces the number of conditional tests inside the main decoding loop, thus reduces code size and branch predictor pressure.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_internal.h">Relative neighboring offsets</a> (look for <code>A4x4_int8</code> and related variables) - Access to left/top macroblock values is done with direct offsets in memory instead of copying their values to a buffer beforehand. It helps to reduce the reads and writes in the main decoding loop.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_slice.c">Parsing uneven block shapes</a> (look at function <code>parse_P_sub_mb</code>) - Each Inter macroblock paving specified with mb_type and sub_mb_type is first converted to a bitmask, then iterated on set bits to fetch the correct number of reference indices and motion vectors. This helps to reduce code size and number of conditional blocks.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_internal.h">Using vector extensions</a> - GCC's vector extensions are used along vector intrinsics to write more compact code. All intrinsics from Intel are aliased with shorter names, which also provides an enumeration of all SIMD instructions used in the decoder.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_deblock.c">Register-saturating SIMD</a> - Some critical SIMD algorithms use more simultaneous vectors than available registers, effectively saturating the register bank and generating stack spills on purpose. In some cases this is more efficient than splitting the algorithm into smaller bits, and has the additional benefit of scaling well with later CPUs.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_bitstream.c">Piston cached bitstream reader</a> - The bitstream bits are read in a size_t[2] intermediate cache with a trailing set bit to keep track of the number of cached bits, giving access to 32/64 bits per read from the cache, and allowing wide refills from memory.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_bitstream.c">On-the-fly SIMD unescaping</a> - The input bitstream is unescaped on the fly using vector code, avoiding a full preprocessing pass to remove escape sequences, and thus reducing memory reads/writes.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_internal.h">Multiarch SIMD programming</a> - Using vector extensions along with aliased intrinsics allows supporting both Intel SSE and ARM NEON with around 80% common code and few #if #else blocks, while keeping state-of-the-art performance for both architectures.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_internal.h">The Structure of Arrays pattern</a> - The frame buffer is stored with arrays for each distinct field rather than an array of structures, to express operations on frames with bitwise and vector operators (see <a href="https://en.wikipedia.org/wiki/AoS_and_SoA" rel="nofollow">AoS and SoA</a>). The task buffer for multithreading also relies on it partially.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_headers.c">Deferred error checking</a> - Error detection is performed once in each type of NAL unit (search for <code>return</code> statements), by clamping all input values to their expected ranges, then expecting <code>rbsp_trailing_bit</code> afterwards (with <em>very high</em> probability of catching an error if the stream is corrupted). This design choice is discussed in <a href="https://traffaillac.github.io/parsing.html" rel="nofollow">A case about parsing errors</a>.</li>
</ol>
<p dir="auto">Other yet-to-be-presented bits:</p>
<ul dir="auto">
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264.h">Minimalistic API</a> with FFI-friendly design (7 functions and 1 structure).</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_internal.h">The bitstream caches</a> for CAVLC and CABAC (search for <code>rbsp_reg</code>) are stored in two size_t variables each, which may be mapped to Global Register Variables in the future.</li>
<li><a href="https://github.com/tvlabs/edge264/blob/master/edge264_slice.c">The decoding of input symbols</a> is interspersed with their parsing (instead of parsing to a <code>struct</code> then decoding the data). It deduplicates branches and loops that are present in both parsing and decoding, and even eliminates the need to store some symbols (e.g. mb_type, sub_mb_type, mb_qp_delta).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Testing (work in progress)</h2><a id="user-content-testing-work-in-progress" aria-label="Permalink: Testing (work in progress)" href="#testing-work-in-progress"></a></p>
<p dir="auto">With the help of a <a href="https://github.com/tvlabs/edge264/blob/master/tools/gen_avc.py">custom bitstream writer</a> using the same YAML format edge264 outputs, a set of extensive tests are being created in <a href="https://github.com/tvlabs/edge264/blob/master/tools/raw_tests">tools/raw_tests</a> to stress the darkest corners of this decoder. The following table lists them all, along with the files implementing them.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>General tests</th>
<th>Expected</th>
<th>Test files</th>
</tr>
</thead>
<tbody>
<tr>
<td>All supported types of NAL units</td>
<td>All OK</td>
<td>supp-nals</td>
</tr>
<tr>
<td>All unsupported types of NAL units</td>
<td>All unsupp</td>
<td>unsupp-nals</td>
</tr>
<tr>
<td>Maximal header log-wise</td>
<td>All OK</td>
<td>max-logs</td>
</tr>
<tr>
<td>All conditions (incl. ignored) for detecting the start of a new frame</td>
<td>All OK</td>
<td>finish-frame</td>
</tr>
<tr>
<td>nal_ref_idc=0 on a IDR</td>
<td>All OK</td>
<td>non-ref-idr</td>
</tr>
<tr>
<td>Missing rbsp_trailing_bit for all supported NAL types</td>
<td>All OK</td>
<td>no-trailing-bit</td>
</tr>
<tr>
<td>NAL of less than 11 bytes starting/ending at page boundary</td>
<td>All OK</td>
<td>tiny-nal</td>
</tr>
<tr>
<td>SEI/slice referencing an uninitialized SPS/PPS</td>
<td>1 OK, 4 errors</td>
<td>missing-ps</td>
</tr>
<tr>
<td>Two non-ref frames with decreasing POC</td>
<td>All OK, any order</td>
<td>non-ref-dec-poc</td>
</tr>
<tr>
<td>Horizontal/vertical cropping leaving zero space</td>
<td>All OK, 1x1 frames</td>
<td>zero-cropping</td>
</tr>
<tr>
<td>P/B slice with nal_unit_type=5 or max_num_ref_frames=0</td>
<td>4 OK, 2 errors</td>
<td>no-refs-P-B-slice</td>
</tr>
<tr>
<td>IDR slice with frame_num&gt;0</td>
<td>All OK, clamped to 0</td>
<td>pos-frame-num-idr</td>
</tr>
<tr>
<td>A ref that must bump out higher POCs to enter DPB (C.4.5.2)</td>
<td>All OK, check output order</td>
<td>poc-out-of-order</td>
</tr>
<tr>
<td>Two ref frames with the same frame_num but differing POC, then a third frame referencing both</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Gap in frame_num while gaps_in_frame_num_value_allowed_flag=0</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Stream starting with non-IDR I frame</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Stream starting with P/B frame</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Ref slice with delta_pic_order_cnt_bottom=-2**31, then a second frame referencing it</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Two frames A/B with intersecting top/bottom POC intervals in all possible intersections</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A 32-bit POC overflow between 2 frames</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A B-frame referencing frames with more than 2**16 POC diff</td>
<td></td>
<td></td>
</tr>
<tr>
<td>num_ref_idx_active&gt;15 in SPS then no override in slice for L0 and L1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A slice with more ref_pic_list_modifications than num_ref_idx_active/16 for L0 and L1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A slice with ref_pic_list_modifications duplicating a ref then referencing the second one</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A slice with insufficient ref frames with and without override of num_ref_idx_active for L0 and L1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A modification of RefPicList[0/1] to a non-existing short/long term frame, then referencing it in mb</td>
<td></td>
<td></td>
</tr>
<tr>
<td>33 IDR with long_term_reference_flag=0/1 while max_num_ref_frames=0 (8.2.5.1)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A new reference while max_num_ref_frames are already all long-term</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All combinations of mmco on all non-existing/short/long refs, with at least twice each mmco</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Two fields of the same frame being assigned different long-term frame indices then referenced</td>
<td></td>
<td></td>
</tr>
<tr>
<td>While all max_num_ref_frames are long-term, a ref_pic_list_modification that references all of them</td>
<td></td>
<td></td>
</tr>
<tr>
<td>An IDR picture with POC&gt;0</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A picture with mmco=5 decoded after a picture with greater POC (8.2.1)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A P/B frame with zero references before or received with a gap in frame_num equal to max_ref_frames</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A P/B frame referencing a non-existing/erroneous ref</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A B frame with colPic set to a non-existing frame</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A current frame mmco'ed to long-term while all max_num_ref_frames are already long-term</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A mmco marking a non-existing picture to long-term</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All combinations of IntraNxNPredMode with A/B/C/D unavailability with asserts for out-of-bounds reads</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A direct Inter reference from colPic that is not present in RefPicList0</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A residual block with all coeffs at maximum 32-bit values</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Two slices of the same frame separated by a currPic reset (ex. AUD)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Two frames with the same POC yet differing TopFieldOrderCnt/BottomFieldOrderCnt</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Differing mmcos on two slices of the same frame</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Sending 2 IDR, then reaching the lowest possible POC, then getting all frames</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Two slices with mmco=5 yet frame_num&gt;0 (to make it look like a new frame)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>POCs spaced by more than half max bits, such that relying on a stale prevPicOrderCnt yields wrong POC</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Filling the DPB with 16 refs then setting max_num_ref_frames=1 and adding a new ref frame</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Adding a frame cropping after decoding a frame</td>
<td>Crop should not apply retroactively</td>
<td></td>
</tr>
<tr>
<td>Making a Direct ref_pic be used after it has been unreferenced</td>
<td></td>
<td></td>
</tr>
<tr>
<td>poc_type=2 and non-ref frame followed by non-ref pic, and the opposite (7.4.2.1.1)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>direct_8x8_inference_flag=1 with frame_mbs_only_flag=0</td>
<td></td>
<td></td>
</tr>
<tr>
<td>checking that a gap in frame_num with poc_type==0 does not insert refs in B slices</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A SPS changing frame format while currPic&gt;=0</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A frame allocator putting all allocs at start/end of a page boundary</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Parameter sets tests</th>
<th>Expected</th>
<th>Test files</th>
</tr>
</thead>
<tbody>
<tr>
<td>Invalid profile_idc=0/255</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Highest level_idc=255</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All unsupported values of chroma_format_idc</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All unsupported values of bit_depth_luma/chroma</td>
<td></td>
<td></td>
</tr>
<tr>
<td>qpprime_y_zero_transform_bypass_flag=1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All scaling lists default/fallback rules and repeated values for all indices, with residual macroblock</td>
<td></td>
<td></td>
</tr>
<tr>
<td>log2_max_frame_num=4 and a frame referencing another with the same frame_num%4</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>CAVLC tests</th>
<th>Expected</th>
<th>Test files</th>
</tr>
</thead>
<tbody>
<tr>
<td>All valid total_zeros=0-8-prefix+3-bit-suffix for TotalCoeffs in [0;15] for 4x4 and 2x2</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Invalid total_zeros=31/63/127-prefix for TotalCoeffs in [0;15] for 4x4 and 2x2</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All valid coeff_token=0-14-prefix+4-bit-suffix for nC=0/2/4, and valid 6-bit-values for nC=8</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Invalid coeff_token=31/63/127-prefix for nC=0/2/4, and invalid 6-bit-values for nC=8</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All valid levelCode=25-prefix+suffixLength-bit-suffix for all values of suffixLength</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All valid run_before for all values of zerosLeft&lt;=7</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Invalid run_before=31/63/127 for zerosLeft=7</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Macroblock of maximal size for all values of mb_type</td>
<td></td>
<td></td>
</tr>
<tr>
<td>mb_qp_delta=-26/25 that overflows on both sides</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All valid inferences of nC for all values of nA/nB=unavail/other-slice/0-16</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All coded_block_pattern=[0;47] for I and P/B slices</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All combinations of intra_chroma_pred_mode and Intra4x4/8x8/16x16PredMode with A/B-unavailability</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All values of mb_type+sub_mb_types for I/P/B with ref_idx/mvds different than values from B_Direct</td>
<td></td>
<td></td>
</tr>
<tr>
<td>mvd=[-32768/0/32767,-32768/0/32767] in a single 16x16 macroblock</td>
<td></td>
<td></td>
</tr>
<tr>
<td>TotalCoeff=16 for a Intra16x16 AC block</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A residual block with run_length=14 making zerosLeft negative</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>CABAC tests</th>
<th>Expected</th>
<th>Test files</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mixing CAVLC and CABAC in a same frame</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Single slice with at least 8 cabac_zero_word</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>MVC tests</th>
<th>Expected</th>
<th>Test files</th>
</tr>
</thead>
<tbody>
<tr>
<td>All wrong combinations of non_idr_flag with nal_unit_type=1/5 and nal_ref_idc=0/1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>nal_unit_type=14 then filler unit then nal_unit_type=1/5</td>
<td></td>
<td></td>
</tr>
<tr>
<td>An nal_unit_type=5 view paired with a non_idr_flag=0 P view, or a non_idr_flag=1 view</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Missing a base or non-base view</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Receiving a SSPS yet only base views then</td>
<td></td>
<td></td>
</tr>
<tr>
<td>16 ref base views while non base are non-refs</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A SSPS with different pic_width_in_mbs/pic_height_in_mbs/chroma_format_idc than its SPS</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A SSPS with num_views=1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A non-base view with weighted_bipred_idc=2</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A non-base view with its base in RefPicList1[0] and direct_spatial_mv_pred_flag=0 (H.7.4.3)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A slice with num_ref_idx_l0_active&gt;8</td>
<td></td>
<td></td>
</tr>
<tr>
<td>svc_extension_flag=1 on a MVC stream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SSPS with additional_extension2_flag=1 and more trailing data</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Gap in frame_num of 16 frames on both views</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Specifying extra_frames=1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Receiving a non-base view before its base</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A stream sending non-base views after a few frames have been output</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Error recovery tests</th>
<th>Expected</th>
<th>Test files</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tests to implement</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A complete frame received twice</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A slice of a frame received twice</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Frame with correct and erroneous slice</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All combinations erroneous/correct and all interval intersections on 2 slices</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All failures of malloc</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All (dis-)allowed bit positions at the end without rbsp_trailing_bit</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DARPA project for automated translation from C to Rust (2024) (116 pts)]]></title>
            <link>https://www.darpa.mil/news/2024/memory-safety-vulnerabilities</link>
            <guid>45443368</guid>
            <pubDate>Wed, 01 Oct 2025 20:53:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.darpa.mil/news/2024/memory-safety-vulnerabilities">https://www.darpa.mil/news/2024/memory-safety-vulnerabilities</a>, See on <a href="https://news.ycombinator.com/item?id=45443368">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Memory safety vulnerabilities are the most prevalent type of disclosed software vulnerability<sup>1</sup> and affect a computer's memory in two primary ways. First, programming languages like C allow programmers to manipulate memory directly, making it easy to accidentally introduce errors in their program that would enable a seemingly routine operation to corrupt the state of memory. Second, memory safety issues can arise when a programming language exhibits an “undefined behavior.” Undefined behaviors happen when the programming language standard provides no specification or guidance on how the program should behave under conditions not explicitly defined in the standard.</p><p>After more than two decades of grappling with memory safety issues in C and C++, the software engineering community has reached a consensus. Relying on bug-finding tools is not enough. Even the Office of the National Cyber Director has called for more proactive approaches to eliminate memory safety vulnerabilities to reduce potential attacks<sup>2</sup>.</p><p>While it's been no secret that memory safe programming languages can eliminate memory safety vulnerabilities, the challenge has been rewriting legacy code at scale that matches the vastness of the problem. The C language was created in the 1970s and has become ubiquitous. It has been used to develop applications that run everything from modern smartphones to space vehicles and beyond. And the Department of Defense has long-lived systems that disproportionately depend on programming languages like C.</p><p>However, in recent years, a cultural shift toward the programming language Rust and recent breakthroughs in machine learning techniques, like large language models (LLMs), have created an environment that may lend itself to a new class of solutions.</p><p>DARPA’s Translating All C to Rust (TRACTOR) program wants to seize this opportunity by substantially automating the translation of the world’s legacy C code to Rust.</p><p>“You can go to any of the LLM websites, start chatting with one of the AI chatbots, and all you need to say is ‘here's some C code, please translate it to safe idiomatic Rust code,’ cut, paste, and something comes out, and it's often very good, but not always,” said Dr. Dan Wallach, DARPA program manager for TRACTOR. “The research challenge is to dramatically improve the automated translation from C to Rust, particularly for program constructs with the most relevance."</p><p>TRACTOR will strive to create the same quality and style that a skilled Rust developer would produce, thereby eliminating the entire class of memory safety security vulnerabilities in C programs.</p><p>Wallach anticipates proposals that include novel combinations of software analysis, such as static and dynamic analysis, and large language models. The program will host public competitions throughout the effort to test the capabilities of the LLM-powered solutions.</p><p>"Rust forces the programmer to get things right,” said Wallach. “It can feel constraining to deal with all the rules it forces, but when you acclimate to them, the rules give you freedom. They're like guardrails; once you realize they're there to protect you, you'll become free to focus on more important things."</p><p>DARPA will sponsor a Proposers Day on Aug. 26, 2024, which attendees can attend in person or virtually. Participants must register by Aug. 19, 2024. Details and registration info are available at <a href="https://sam.gov/opp/1e45d648886b4e9ca91890285af77eb7/view" target="_blank">SAM.Gov</a>.</p><p>[1]<a href="https://www.cisa.gov/sites/default/files/2023-12/The-Case-for-Memory-Safe-Roadmaps-508c.pdf" target="_blank">https://www.cisa.gov/sites/default/files/2023-12/The-Case-for-Memory-Safe-Roadmaps-508c.pdf</a></p><p>[2]<a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-fact-sheet/" target="_blank">https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-fact-sheet/</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft declares bring your Copilot to work day, usurping IT authority (123 pts)]]></title>
            <link>https://www.theregister.com/2025/10/01/microsoft_consumer_copilot_corporate/</link>
            <guid>45443304</guid>
            <pubDate>Wed, 01 Oct 2025 20:48:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/10/01/microsoft_consumer_copilot_corporate/">https://www.theregister.com/2025/10/01/microsoft_consumer_copilot_corporate/</a>, See on <a href="https://news.ycombinator.com/item?id=45443304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Your job may not support BYOD, but how about BYOC? Microsoft has declared that people can bring their personal Microsoft 365 subscriptions to work to access various Copilot features at companies that fail to provide an AI fix.</p>
<p>Redmond has done so unilaterally, effectively endorsing "shadow IT" – the practice of bringing unapproved software and devices into the workplace.</p>
<p>Earlier this year, Microsoft said it had adopted a new approach to shadow IT. "While earlier eras of our IT history focused on trying to prevent shadow IT, we are now concentrating on managing it," the biz said in a <a href="https://www.microsoft.com/insidetrack/blog/digitally-transforming-microsoft-our-it-journey/">blog post</a>. By "managing," Microsoft also means "enabling."</p>

    

<p>Samer Baroudi, senior product marketing manager at Microsoft, insists this is for your own good.</p>

        


        

<p>"This offers a safer alternative to other bring-your-own-AI scenarios, and empowers users with Copilot in their daily jobs while keeping IT firmly in control and all enterprise data protections intact," Baroudi explained in a <a href="https://techcommunity.microsoft.com/blog/microsoft365copilotblog/employees-can-bring-copilot-from-their-personal-microsoft-365-plans-to-work---wh/4458212">blog post</a>.</p>
<p>Makers of competing AI products might disagree.</p>

        

<p>Microsoft says that employees can sign into Microsoft 365 apps using both personal and work accounts and now can use Copilot features from their personal plan (Personal, Family, or Premium) for business documents – even if their work account lacks a Copilot license.</p>
<ul>

<li><a href="https://www.theregister.com/2025/10/01/raspberry_pi_price_hikes/">Raspberry Pi prices hiked as AI gobbles all the memory</a></li>

<li><a href="https://www.theregister.com/2025/10/01/ai_isnt_taking_people_jobs/">AI has had zero effect on jobs so far, says Yale study</a></li>

<li><a href="https://www.theregister.com/2025/10/01/us_air_force_investigates_breach/">Air Force admits SharePoint privacy issue as reports trickle out of possible breach</a></li>

<li><a href="https://www.theregister.com/2025/10/01/hundreds_businesses_urge_microsoft_not_end_win10_support/">Hundreds of orgs urge Microsoft: don't kill off free Windows 10 updates</a></li>
</ul>
<p>IT admins miffed at having their authority usurped by a diktat from Redmond can console themselves with the knowledge that Copilot's level of access "is strictly governed by the user’s work account permissions, ensuring enterprise data remains protected." The user's Entra (work) identity governs file permissions and access controls.</p>
<p>Also, "IT retains full control and oversight" – apart from the bit about allowing this to happen in the first place.</p>
<p>Admins have the ability to disallow personal Copilot usage on work documents using cloud policy controls. And they can audit personal Copilot interactions and can apply enterprise identity, permission, and compliance policies.</p>
<p>Government tenants (GCC/DoD) for some reason don't support this capability, the one that Baroudi insists "does not create new data exposure risks."</p>

        

<p>Meanwhile, employees who decide to fire up their personal Copilot accounts within the workplace should be mindful that their prompts and responses will be captured by their employer.</p>
<p>As to why Microsoft would bother, Baroudi provides a hint in the FAQs detailing the bring-your-own-Copilot-to-work initiative that accompanies his post.</p>
<blockquote>Can use of Copilot from personal Microsoft 365 subscriptions help drive AI adoption? &nbsp;
<p>

Yes. It allows users to experience AI productivity benefits while IT retains control.</p></blockquote>
<p>Of course, when Microsoft next cites enterprise adoption statistics for its AI products, it will be worth asking whether the company is counting personal usage of Copilot. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Company Man (133 pts)]]></title>
            <link>https://www.lesswrong.com/posts/JH6tJhYpnoCfFqAct/the-company-man</link>
            <guid>45443298</guid>
            <pubDate>Wed, 01 Oct 2025 20:47:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lesswrong.com/posts/JH6tJhYpnoCfFqAct/the-company-man">https://www.lesswrong.com/posts/JH6tJhYpnoCfFqAct/the-company-man</a>, See on <a href="https://news.ycombinator.com/item?id=45443298">Hacker News</a></p>
Couldn't get https://www.lesswrong.com/posts/JH6tJhYpnoCfFqAct/the-company-man: Error: Request failed with status code 429]]></description>
        </item>
        <item>
            <title><![CDATA[Evaluating the impact of AI on the labor market: Current state of affairs (133 pts)]]></title>
            <link>https://budgetlab.yale.edu/research/evaluating-impact-ai-labor-market-current-state-affairs</link>
            <guid>45442743</guid>
            <pubDate>Wed, 01 Oct 2025 20:07:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://budgetlab.yale.edu/research/evaluating-impact-ai-labor-market-current-state-affairs">https://budgetlab.yale.edu/research/evaluating-impact-ai-labor-market-current-state-affairs</a>, See on <a href="https://news.ycombinator.com/item?id=45442743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="block-budgetlab-content">
  
    
      

<article id="node-publication-1154" data-js="publication">
  

  <div>
          <section id="paragraph-key-takeaways-4326" data-js="key-takeaways">
        <h2>   Key Takeaways </h2>
  
      <ol>
              <li>
          <p>While the occupational mix is changing more quickly than it has in the past, it is not a large difference and predates the widespread introduction of AI in the workforce.</p>
        </li>
              <li>
          <p>Currently, measures of exposure, automation, and augmentation show no sign of being related to changes in employment or unemployment.</p>
        </li>
              <li>
          <p>Better data is needed to fully understand the impact of AI on the labor market.</p>
        </li>
              <li>
          <p>We plan on updating this analysis regularly moving forward to see how the impact of AI on the labor market changes over time.</p>
        </li>
          </ol>
  </section>

  <div id="paragraph-html-content-4287" data-js="html-content">
  <p>How has AI impacted the labor market? Since generative AI was first introduced nearly three years ago, surveys show widespread public anxiety about AI’s potential for job losses. While it is impossible to accurately predict the future, we can examine how U.S. employment has changed since ChatGPT’s release in November 2022.</p><p>Our analysis complements other recent studies that provide nascent evidence of possible AI impacts on specific occupations and sub-populations, such as early career workers. We took a broader lens, widening the aperture to the whole labor market, and asked two main questions.</p><p>First, is the pace of labor market change in this 33-month period of employment disruption different from past periods of early technological change? Second, is there evidence of economy-wide employment effects? To answer these questions, we compare how quickly the occupational mix has changed across a range of measures since ChatGPT’s launch, and compare this to past disruptions from computers and the internet.</p><p>Overall, our metrics indicate that the broader labor market has not experienced a discernible disruption since ChatGPT’s release 33 months ago, undercutting fears that AI automation is currently eroding the demand for cognitive labor across the economy.<sup>1</sup></p><p>While this finding may contradict the most alarming headlines, it is not surprising given past precedents. Historically, widespread technological disruption in workplaces tends to occur over decades, rather than months or years. Computers didn’t become commonplace in offices until nearly a decade after their release to the public, and it took even longer for them to transform office workflows. Even if new AI technologies will go on to impact the labor market as much, or more, dramatically, it is reasonable to expect that widespread effects will take longer than 33 months to materialize.</p><p>Of course, our analysis is not predictive of the future. We plan to continue monitoring these trends monthly to assess how AI’s job impacts might change. It is important to remember that the effects of new technologies are evolving and a simple snapshot in time is not enough to explicitly determine what the future holds.</p></div>

  <div aria-labelledby="heading-layout-4288">
                        <div id="paragraph-html-content-4289" data-js="html-content">
  <p>The current rhetoric about AI mirrors the anxiety over earlier generations of technological progress. So far, is this time different?</p><p>First, we look at how quickly the overall occupational mix changed in the first 33 months since ChatGPT’s relative to previous periods of technological change. Figure 1 compares the occupational mix by month to the mix in a baseline period at the start of a major technological development (e.g. January 1996 is the baseline month for the growing adoption of the internet). The job mix for AI appears to be changing faster than it has in the past, although not markedly so. (See the appendix for a discussion of pre-trends.)</p></div>
                                
                                <div id="paragraph-html-content-4290" data-js="html-content">
  <p>The occupational mix refers to the distribution of workers amongst all the jobs in the economy. In this context, a percentage point difference means that, relative to the start point, that percent of workers are in new occupations. This can occur by workers changing jobs, losing jobs, or unemployed people getting a new job. As such, this metric attempts to capture how different the sum of occupations that make up the labor force is relative to another point in time. By measuring this over the time generative AI has been publicly available, we can test the claim that AI is substantially changing the workforce by any of the methods mentioned above (pushing workers from one job to another, automating workers out of a job, or creating new jobs). Note that a change over this time period simply reflects <em>change</em> — it does not take a stance on the <em>cause</em> of that change. Note that if recent grads are not getting hired, that would show up in this measure in as much as recent grads (as discussed below) are often in different occupations than older workers.</p><p>With the development of the internet and the growing prevalence of computers, the turn of the 21st century gave rise to concerns over an imminent computerization of many jobs. Despite this automation anxiety, the occupational mix by 2002 was at most around 7 percentage points different than it was in 1996; i.e., only 7 percent of workers in 2002 would need to switch occupations to match the composition of the 1996 labor market.</p><p>Changes in the occupational mix since the advent of generative AI in 2022 seem to mirror the trends seen during the three comparison periods. The recent changes appear to be on a path only about 1 percentage point higher than it was at the turn of the 21st century with the adoption of the internet. Although recent trends seemingly outpace historical shifts in the occupational mix, the potential effects of AI on the labor market so far are not out of the ordinary. In fact, taking a closer look at recent years, the data suggests that this recent trend is not necessarily attributable to AI (Figure 2). Shifts in the occupational mix were well on their way during 2021, before the release of generative AI, and more recent changes do not seem any more pronounced, even as the use of AI continues to grow in popularity.</p></div>
                                
                                <div id="paragraph-html-content-4291" data-js="html-content">
      

<p>Repeating this analysis by industry similarly suggests a limited effect of AI. Figure 3 reports the change in the occupational mix from November 2022 within different industries. The Information, Financial Activities, and Professional and Business Services sectors have all seen larger shifts in the job mix compared to the shifts in the aggregate labor market, with the largest changes in the Information sector. (As a reminder the information sector includes things like newspapers, movies, and data processing.) These industries are among those with the highest exposure to generative AI. Although at first glance these changes may seem attributable to generative AI, the data again suggests that the trends within these industries started before the release of ChatGPT (Figures 4-6). In fact, over a broader time horizon, the large shifts in the Information Industry seem to be a feature of the industry itself rather than a consequence of any one technological development (Figure 7).</p>

    </div>
                                
                                
                                
                                
                                
                                <div id="paragraph-html-content-4292" data-js="html-content">
      

<p>Looking over an even longer horizon, labor market volatility appears rather low. As a chart Jed Kolko shows, the change in the occupational mix seen in Figure 1 is sluggish compared to the change seen in the 40s and 50s (which reflected mass labor market changes due to world events).&nbsp;<a href="https://hbr.org/2018/12/5-questions-we-should-be-asking-about-automation-and-jobs">Kolko cautioned</a> that “we simply don’t know for sure whether automation, algorithms, and AI will ultimately create more jobs than they destroy.”&nbsp;</p>

    </div>
                                
                                <div id="paragraph-html-content-4293" data-js="html-content">
      

<p>The dissimilarity data we have examined indicates that there is no substantial acceleration in the rate of change in the composition of the labor market since the introduction of ChatGPT. Lacking that, there is nothing meaningful we can either attribute or misattribute to AI.</p>

    </div>
                                <div id="paragraph-html-content-4294" data-js="html-content">
      

<p>Figure 9 compares the occupational mix for recent college graduates (ages 20-24) to that of their older counterparts (ages 25-34).<sup>2</sup> If generative AI were in fact substantially changing the labor market for recent college graduates, we would expect to see a growing dissimilarity in the occupational mix between these two groups. The dissimilarity has increased slightly faster in recent months than it did in a previous time period, which could be consistent with a recent&nbsp;<a href="https://digitaleconomy.stanford.edu/publications/canaries-in-the-coal-mine/">paper</a> from Brynjolfsson et al. showing a possible impact of AI on employment of early career workers. It could also simply reflect a slowing labor market. However, our results should be interpreted with caution particularly given small sample sizes.</p>

    </div>
                                
                                <div id="paragraph-html-content-4295" data-js="html-content">
      

<p>Taking a closer look at the trend since January 2021, the dissimilarity between older and more recent college graduates rarely deviates outside of the 30-33% range (Figure 10). This implies that these trends of growing dissimilarity may pre-date ChatGPT’s release and may not be attributable to AI. However, there is perhaps some slight upward momentum more recently, though this is consistent with both Brynjolfsson et al.’s work and the CPS’s noisiness (and a slowing labor market hitting younger workers). Further, the same caution in interpretation given the small sample sizes holds in this figure as well.</p>

    </div>
                                
                                </div>

  <div aria-labelledby="heading-layout-4296">
                        <div id="paragraph-html-content-4297" data-js="html-content">
  <p>To better understand whether AI is impacting the labor market, we would want to analyze whether the share of workers in occupations that are most impacted by AI usage is changing over time. If AI were automating jobs at scale, we would expect to see a smaller share of workers in some of the jobs that are most negatively impacted.</p><p>Unfortunately, comprehensive usage data is not publicly available. The best available data we have is from&nbsp;<a href="https://www.brookings.edu/articles/generative-ai-the-american-worker-and-the-future-of-work/">OpenAI</a> and&nbsp;<a href="https://www.anthropic.com/economic-index">Anthropic</a>, respectively, that detail the occupations that are most “exposed” to genAI tools (a theoretical, forward-looking metric across all jobs) and that have the highest actual usage of one specific AI tool, Claude (a more narrow, present-focused metric). While imperfect, these data are our best approximation of AI job “risk”. (See discussion of limitations in the next section and in the appendix.)</p><p>Importantly, OpenAI and Anthropic are measuring different things and we look at them separately.</p></div>
                                <div id="paragraph-html-content-4298" data-js="html-content">
  <p>We use data from OpenAI that shows a measure of “exposure” to ChatGPT technology. This refers, generally, to whether utilizing ChatGPT4 technology can help reduce the time it takes to complete the occupation’s tasks by at least 50%. (The data appendix describes this metric in more detail.)</p><p>We utilize the “Beta” exposure metric, which also accounts for if a model with additional software built on top of it can help reduce task completion time, though this capability is weighted half that of “direct” exposure. An exposure score is created on a scale from 0 to 1 based on a percent of an occupation’s tasks that are “exposed” to genAI. The data appendix describes this metric in more detail.</p><p>For our purposes, we categorize occupations into three groups using their exposure score quintile; an occupation has the lowest degree of exposure if it falls in the first two quintiles, a medium degree if in the 3rd and 4th quintiles, and the highest degree if in the top quintile. In other words, these metrics look at <em>relative</em> not <em>absolute</em> exposure. We provide a further discussion of this in the appendix.</p><p>We ask: has the share of workers in occupational exposure quintiles changed since ChatGPT’s launch? Our analysis shows that it has not (Figure 11). The share of workers in the lowest, middle, and highest occupational exposure groups stay stable at around 29%, 46% and 18%, respectively.</p></div>
                                
                                <div id="paragraph-html-content-4299" data-js="html-content">
      

<p>Even when specifically examining the unemployed population, there is no clear growth in exposure to generative AI. Figure 12 depicts the average percentage of tasks exposed amongst unemployed workers by duration of unemployment. AI-driven displacement might suggest a growth in the proportion of exposed tasks amongst recently unemployed workers. Irrespective of the duration of unemployment, however, unemployed workers were in occupations where about 25 to 35 percent of tasks, on average, could be performed by generative AI. Although there is some variation between months, the data demonstrate no clear upward trend and no clear difference by the duration of unemployment.&nbsp;</p>

    </div>
                                
                                
                                <div id="paragraph-html-content-4300" data-js="html-content">
  <p>Given Anthropic’s usage data’s novelty and uniqueness, there is no established standard for how to aggregate it. The usage Anthropic observes does not contain every single task in the O*NET task database, so the question of how to handle the missing tasks remains ambiguous. We proceed in two ways:</p><p>First, we ignore tasks that are not included in the Anthropic data and aggregate following the method detailed in the appendix. This method makes no assumption about how workers are using or could utilize Claude to perform the task, or how workers in occupations with tasks like those observed would use Claude. However, this method significantly limits the number of tasks and therefore occupations we can include. Further, an occupation may appear to have very high usage while in fact only a single of that occupation’s tasks were observed in the data.</p><p>Alternatively, we include all of the missing tasks and assume their usage is zero. There is some truth to this, as those tasks were not observed in Claude during the period in which the data was collected. However, it makes strong assumptions about those tasks’ potential usage. Two comparable tasks, where one appears in the data and one does not, would have totally different usage values. Given how the observed Claude data is a representation of the users who happened to utilize the model over a sample period, the following period could have had that similar task included.</p><p>In pursuit of a balanced and well-informed exploration of this data, we include results from both methods below.</p><p>Anthropic’s data on AI usage shows similar trends of stability over time, rather than disruption. The proportion of employment in occupations with high levels of task AI usage, whether automation or augmentation (as defined as more than half of AI usage), is stable at around 70% or 11%, respectively (Figure 15). When assuming that unobserved tasks indicate zero usage, however, these proportions drop to 3% and 0%, respectively. Repeating a similar analysis as above, Figures 16 and 18 report the occupation-level share of tasks that are automation or augmentation, respectively, amongst unemployed workers by duration of unemployment. Note: for this analysis, we use Anthropic’s most recent data on AI usage, which was released in mid-September. We discuss Anthropic’s data vintages more fully in the Appendix.</p></div>
                                </div>

  

  

  

  

  

  

  <div aria-labelledby="heading-layout-4301">
                        <div id="paragraph-html-content-4302" data-js="html-content">
  <p>As previously noted, the metrics from OpenAI and Anthropic are imperfect proxies for AI risk and usage, while still being the best available.</p><p>A key limitation of OpenAI’s “exposure” data is that it is not based on actual usage, and should therefore be interpreted as a theoretical estimate of the jobs and sectors that <em>could</em>, in theory, be impacted. In reality, actual AI usage and workplace diffusion has varied dramatically between sectors and occupations with similar levels of “exposure.” For instance, generative AI tools were adopted extremely quickly and at mass scale among coders and software developers, who are in the top quintile of exposure. Meanwhile, adoption has lagged considerably in clerical sectors, despite a similar level of exposure. Thus analyzing occupations by exposure alone likely <em>under-</em>estimates potential labor market disruption, as the top quintiles of exposure will include occupations that are theoretically exposed but not actively using AI at a meaningful scale, and thus unlikely to see AI impacts.</p><p>A comparison of the OpenAI “exposure” data with the Anthropic usage data makes this limitation clear. The two measures appear to have only a limited correlation with one another (Figure 20).</p></div>
                                
                                <div id="paragraph-html-content-4303" data-js="html-content">
      

<p>Figure 20 consists of occupations that have data for both the OpenAI and Anthropic measures, which amounts to about 80% of CPS occupations in our sample. Figure 21 splits the data from figure 20 into quadrants along the axes of low-high exposure and low-high usage and then groups the data into its SOC job categories. Particularly striking is the greater range of different job categories in the quadrants with low usage (the left side of figure 20) and conversely the concentration in just a handful of categories in the high usage quadrants. Across both high and low exposure, high usage occupations are dominated by scientific and quantitative professions and general business occupations. The occupations clustered in the low usage/exposure (bottom left of figure 20) tend to be production occupations with little computerization.</p>

    </div>
                                
                                
                                <div id="paragraph-html-content-4304" data-js="html-content">
  <p>Just as the OpenAI metric has limitations, so too does Anthropic’s usage data. Figure 22 shows the occupational shares of all “conversations” with Claude (the AI chatbot), and illustrates the occupation groups that are over- and under- represented in this usage, compared to their exposure ranking and employment share.</p><p>It is clear from the data that Claude’s usage is heavily dominated by one occupational group — computer and mathematical, which includes coders — and that arts and media (including writers) is also considerably overrepresented. While certainly coding is among the most prominent use cases of AI, it is likely that Claude’s userbase skews more heavily to these tasks due to Claude’s stand-out reputation among LLMs as being particularly good at writing and coding. <a href="https://cdn.openai.com/pdf/3c7f7e1b-36c4-446b-916c-11183e4266b7/chatgpt-usage-and-adoption-patterns-at-work.pdf">New data recently published by OpenAI</a> shows a broad pattern of usage among ChatGPT customers across a range of industries, including not only information services (including software development) but also professional services and even manufacturing. It is entirely possible, and even likely, that usage data from other AI models like Google’s Gemini or Microsoft’s Copilot would show different and more varied patterns of usage. Thus data from Claude usage alone is not representative of how workers across the economy are using AI chatbots and tools.</p><p>To accurately measure AI’s impact on the labor force, the most important data needed is comprehensive usage data from all the leading AI companies at the individual and enterprise level, including APIs.&nbsp;<a href="https://www.linkedin.com/company/anthropicresearch/">Anthropic</a> has led the way in transparently sharing Claude usage data, including a new release of enterprise data. To further our understanding of AI’s impact, it is important that all leading AI labs do the same in a similarly transparent and privacy-protected way.&nbsp;</p></div>
                                </div>

  <div id="paragraph-html-content-4306" data-js="html-content" aria-labelledby="heading-layout-4305">
      

<p>While anxiety over the effects of AI on today’s labor market is widespread, our data suggests it remains largely speculative. The picture of AI’s impact on the labor market that emerges from our data is one that largely reflects stability, not major disruption at an economy-wide level. While generative AI looks likely to join the ranks of transformative, general purpose technologies, it is too soon to tell how disruptive the technology will be to jobs. The lack of widespread impacts at this early stage is not unlike the pace of change with previous periods of technological disruption. Preregistering areas where we would expect to see the impact and continuing to monitor monthly impacts will help us distinguish rumor from fact.&nbsp;</p>

    </div>

  <div aria-labelledby="heading-layout-4307">
                        <div id="paragraph-html-content-4308" data-js="html-content">
  <p>Code used to create the data can be found <a href="https://github.com/Budget-Lab-Yale/AI-Employment-Model">here</a>.</p><p>We adapt&nbsp;<a href="https://inequality.stanford.edu/sites/default/files/media/_media/pdf/Classic_Media/Dudley_1955_Measurement.pdf">Duncan and Duncan’s</a> methodology to construct a dissimilarity index of the change in the occupational mix over time using monthly CPS data. Going month by month, we measure each occupation’s constituent percentage of the workforce and compare it to the starting month. To address noise in the data, we take a 12 month moving average for each month. We then sum up the absolute differences in percentage of workforce across all occupations to get our dissimilarity index for that given month. This captures both the advent of new occupations and the expansion or contraction of existing ones.</p><p>To examine generative AI’s impact, we begin in November 2022 and continue into the latest monthly CPS release in July, as this lines up with AI’s public introduction and the beginning of its adoption. We compare AI’s dissimilarity index to three other time periods:</p><ol><li>1984-1989: Capturing the popularization of PCs and the start of the computer revolution.</li><li>1996-2002: Capturing mass adoption of the internet in public life and the workplace.</li><li>2016-2019: A control period following the 2008 Recession recovery during which there was little change to the occupational mix.</li></ol><p>Figure A1 adds pre-trends in the occupational mix for the prior 12 months to the data from Figure 1. In all but one period, the pre-trends show a similar degree of dissimilarity as compared to the trend observed over the first year. This lack of a substantial difference suggests the advent of new technologies has minimal immediate effects; i.e., shifts in the labor market take time to develop. Note that the large pre-trends in the “Computers” period are likely due to issues with the quality of data prior to 1981.</p></div>
                                
                                <div id="paragraph-html-content-4309" data-js="html-content">
  <p>Using&nbsp;<a href="https://arxiv.org/pdf/2303.10130">OpenAI’s occupational exposure data</a>, we classify occupations as mildly, moderately, or highly exposed to AI provided that generative AI can reduce the time to complete at least one task (or a greater portion of tasks given the exposure level) by 50%. The raw occupational exposure data allocates tasks into one of three categories:</p><ol><li>No exposure: Generative AI cannot reduce the time to complete the task or lowers the quality of the output.</li><li>Direct exposure: Generative AI can reduce the time to complete a task by at least 50%.</li><li>LLM+ Exposed: Generative AI alone cannot reduce task completion time, but a piece of software built on top of a model could.</li></ol><p>We utilize the GPT4 categorized “Beta” exposure, which gives a task a score of 1 if it is directly exposed, or a score of .5 if it is LLM+ Exposed. An occupation’s exposure is the weighted average of the exposure of each individual task.</p><p>Following the methods in OpenAI’s paper presenting these results, when aggregating exposure to the occupational level, a job’s “core” tasks have a weight of 1 while their “supplemental” tasks have a weight of .5. After exposure and usage are aggregated to the SOC code level, they are weighted by that occupation’s OES labor count. This weight is used when aggregating from SOC code to CPS code.</p><p>Given the exposure data was created a few years and thus does not include the more advanced capabilities of today’s large language models, a relative comparison of exposure appears more appropriate. Rather than measuring the now aged direct exposure, we broke GPT4 Beta exposure (after our aggregation and weighting) into quintiles and bucketed them as described in the text above. The average exposure, rounded to the second digit, for each quintile is as follows:</p></div>
                                
                                <div id="paragraph-html-content-4310" data-js="html-content">
      

<p>Figure A2 replicates the prior analysis in Figure 12 with occupation-level exposure groups instead defined using the absolute measure of exposure. The lowest and medium exposure groups (scores less than 0.4 and between 0.4 and 0.8, respectively) stably comprise around 45% of workers each, while the highest exposure group represents only around 2% of workers (scores greater than 0.8). Like in Figure 12, there is no observable trend over time.</p>

    </div>
                                
                                <div id="paragraph-html-content-4327" data-js="html-content">
  <p>Anthropic’s usage metrics are aggregated similarly to the OpenAI exposure data. We follow the method their researchers use <a href="https://huggingface.co/datasets/Anthropic/EconomicIndex/tree/main/release_2025_03_27">here</a>. We aggregate the individual kinds of usage into their respective categories for each task:</p><ul><li>Augmentation = Validation + Task Iteration + Learning</li><li>Automation = Directive + Feedback Loop.</li></ul><p>We then weight the tasks using the same core/supplemental weighting method described above. Following that, the aggregation to CPS code level is identical to the OpenAI aggregation.&nbsp;<br>Throughout this piece we use Anthropic’s AI usage data from their most recent release in August. This data, however, only provides a static snapshot of AI usage. As more data is made available in future releases, the Budget Lab will continue to update this analysis to provide a more comprehensive picture of how changes in AI usage may be affecting the labor market. Figure A3 shows the trend in the automation/augmentation of tasks using data from a prior March release until August 2025 (note that to ensure consistency between the two releases, the August usage data excludes enterprise usage).</p></div>
                                
                                
                                </div>

  <div id="paragraph-html-content-4311" data-js="html-content">
  <ol><li>Much of the press focuses on <a href="https://www.cnbc.com/2025/08/28/generative-ai-reshapes-us-job-market-stanford-study-shows-entry-level-young-workers.html">early career</a> and <a href="https://fortune.com/2025/08/10/ai-unemployment-white-collar-knowledge-workers-jobless-recovery-recession/">white collar</a> workers being in <a href="https://www.cnbc.com/2025/08/05/ai-labor-market-young-tech-workers-goldman-economist.html">lower demand</a>.&nbsp;</li><li>In any given year, recent and older college graduates account for around 1,100 and 5,600 observations (3% and 12% of the overall sample), respectively.</li></ol></div>


    </div>

  </article>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Lost 32,000 Private-Sector Jobs in September, Says Payroll Processor (205 pts)]]></title>
            <link>https://www.wsj.com/economy/jobs/u-s-lost-32-000-jobs-in-september-says-payroll-processor-06528340</link>
            <guid>45442185</guid>
            <pubDate>Wed, 01 Oct 2025 19:30:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/economy/jobs/u-s-lost-32-000-jobs-in-september-says-payroll-processor-06528340">https://www.wsj.com/economy/jobs/u-s-lost-32-000-jobs-in-september-says-payroll-processor-06528340</a>, See on <a href="https://news.ycombinator.com/item?id=45442185">Hacker News</a></p>
Couldn't get https://www.wsj.com/economy/jobs/u-s-lost-32-000-jobs-in-september-says-payroll-processor-06528340: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[ICE Is Buying a Tool to Track Phones, Without Warrants (130 pts)]]></title>
            <link>https://olgalautman.substack.com/p/ice-is-buying-a-tool-to-track-hundreds</link>
            <guid>45441983</guid>
            <pubDate>Wed, 01 Oct 2025 19:15:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://olgalautman.substack.com/p/ice-is-buying-a-tool-to-track-hundreds">https://olgalautman.substack.com/p/ice-is-buying-a-tool-to-track-hundreds</a>, See on <a href="https://news.ycombinator.com/item?id=45441983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!xTUX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!xTUX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg 424w, https://substackcdn.com/image/fetch/$s_!xTUX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg 848w, https://substackcdn.com/image/fetch/$s_!xTUX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!xTUX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!xTUX!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg" width="1200" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:400,&quot;width&quot;:600,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Several agents wearing green uniforms and heavy-duty face masks are seen up close. One of them has a patch that says “HSI,” a reference to Homeland Security Investigations.&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:&quot;center&quot;,&quot;offset&quot;:false}" alt="Several agents wearing green uniforms and heavy-duty face masks are seen up close. One of them has a patch that says “HSI,” a reference to Homeland Security Investigations." title="Several agents wearing green uniforms and heavy-duty face masks are seen up close. One of them has a patch that says “HSI,” a reference to Homeland Security Investigations." srcset="https://substackcdn.com/image/fetch/$s_!xTUX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg 424w, https://substackcdn.com/image/fetch/$s_!xTUX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg 848w, https://substackcdn.com/image/fetch/$s_!xTUX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!xTUX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F513e54a8-0c1a-40ab-a994-6f04fdba9089_600x400.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Federal agents stand guard on a road outside an agricultural facility where an immigration raid occurred in Camarillo, Calif., in July.Credit...Daniel Cole/Reuters</figcaption></figure></div><p data-attrs="{&quot;url&quot;:&quot;https://olgalautman.substack.com/p/ice-is-buying-a-tool-to-track-hundreds?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://olgalautman.substack.com/p/ice-is-buying-a-tool-to-track-hundreds?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><span>While putting together my </span><a href="https://trumptyrannytracker.substack.com/" rel="">Trump Tyranny Tracker</a><span>, I came across alarming news that has gone largely unnoticed but needs our attention—ASAP. Documents reviewed by </span><em><a href="https://www.404media.co/ice-to-buy-tool-that-tracks-locations-of-hundreds-of-millions-of-phones-every-day/" rel="">404 Media</a></em><span> reveal that ICE is purchasing access to a powerful surveillance tool that harvests billions of pieces of location data daily from hundreds of millions of phones. This move reverses Biden-era curbs and marks a dramatic expansion of ICE’s ability to track people inside the U.S. without a warrant.</span></p><p>The contract has been awarded to PenLink, a little-known surveillance company headquartered in Nebraska that has quietly spent decades perfecting tools for geolocation data mining, mass communications interception, and real-time tracking, operating largely outside public scrutiny while steadily expanding its reach into the law enforcement and intelligence space. </p><p><span>This is not ICE’s first engagement with the company: in 2018, the agency signed a </span><a href="https://www.newsweek.com/ice-just-signed-24m-contract-secretive-data-surveillance-company-can-track-you-962493" rel="">$2.4 million contract</a><span> with PenLink, granting it access to the firm’s proprietary telecommunications analysis and intercept software suite, which was used to collect and analyze massive amounts of internet and social media communications data in real time. ICE has now deliberately selected PenLink over its competitors once again because it offers a comprehensive “all-in-one” platform capable of merging immense repositories of location data with sophisticated social media monitoring capabilities, thereby giving the agency an unprecedented ability to track, map, and analyze individuals’ movements and networks. In doing so, ICE is not simply reverting to its previous practice of warrantless location tracking; it is escalating this surveillance to a level of precision and integration that blurs the line between targeted investigation and dragnet monitoring.</span></p><p>Under the Biden administration, the Department of Homeland Security had suspended the purchase of commercial location data after the Inspector General found the agency had violated federal law, but that temporary safeguard has now been dismantled. Trump’s federal agencies are resurrecting these programs with renewed vigor, tapping into massive datasets assembled by surveillance contractors that systematically harvest and monetize the movements of hundreds of millions of people through their smartphones, effectively creating a parallel data-acquisition pipeline that allows federal agencies to sidestep judicial oversight and bypass the warrant requirements that govern direct requests to telecommunications providers. These practices were controversial even before Trump returned to power, and now, under his direction, they are being normalized, expanded, and institutionalized.</p><p>The threat posed by these capabilities is very real and happening now. These datasets can locate an individual within a single city block, construct detailed social graphs based on patterns of physical proximity, and generate real-time alerts when specific targets move, meet, or communicate. ICE has already tested similar analytical systems to assign “gang membership” through algorithmic inference — a program so deeply flawed that it infamously misidentified toddlers as gang members — and under Trump, these same technologies are being scaled up and turned loose on entire communities, dramatically increasing their potential for error, abuse, and political weaponization.</p><p>The latest developments around ICE’s surveillance powers should be setting off alarms everywhere. Trump is quietly building the machinery of a domestic surveillance state using tools that, so far, have faced almost no scrutiny. Shortly after returning to power, he and Musk created unauthorized DOGE, a shadow structure created without congressional approval. Under the banner of “efficiency,” DOGE quietly began consolidating massive amounts of federal data. It functioned as a bureaucratic Trojan horse, allowing Trump’s inner circle to merge agency datasets, centralize information flows, and bypass the oversight mechanisms that typically govern intelligence operations.</p><p>That initial framework is now being supercharged. ICE is being armed with a growing list of commercial surveillance tools: Graphite, an invasive spyware platform that can penetrate Signal and turn your phone into a surveillance tool using the camera and microphone; PenLink, which merges mass location tracking with social media monitoring; Flock’s license plate reader network; Clearview AI’s facial recognition database; and Palantir’s powerful data-integration and analysis systems. These are just a few examples. Together, they can weave DOGE’s centralized data stores with private-sector capabilities into a single, flexible, and largely unaccountable surveillance apparatus.</p><blockquote><p><strong>More on Graphite spyware….</strong></p></blockquote><p>This is exactly how Russia built its surveillance state. It began with the quiet centralization of telecommunications data, followed by the rollout of SORM, which gave the government mass interception and geolocation powers, and ended with the targeted use of those capabilities against political opponents, journalists, and civil society. Immigrants, ethnic minorities, and marginalized groups were the first targets—people with little power to resist. Once the system was normalized, the net widened, and tools designed for “security” quickly became instruments of political control.</p><p>And Trump is following the same blueprint. Immigrants are the test case, but the surveillance architecture being built is meant to reach far beyond them. ICE is being transformed into a personal security force equipped with spyware, dragnet tracking, and powers that bypass the courts and Congress. This is a deliberate, methodical effort to fuse state power with commercial surveillance markets, creating a system that operates with no transparency or accountability. </p><p>Russia offers a chilling preview of where this leads. Once surveillance infrastructure is in place, legal limits become irrelevant. The tools designed for “security” end up targeting opposition politicians, journalists, NGOs, and eventually ordinary citizens. If this trajectory continues unchecked, the same will happen here.</p><p>This story should be front-page news, but it isn’t. So it is up to us to make noise.</p><ul><li><p><strong>Spread this story</strong><span> widely. Most Americans have no idea ICE is buying tools that let them track phones in real time without warrants.</span></p></li><li><p><strong>Contact your representatives</strong><span> in Congress and at the state level. Demand hearings, oversight, and legal restrictions on warrantless location tracking.</span></p></li><li><p><strong>Support watchdog groups</strong><span> like the </span><a href="https://www.aclu.org/" rel="">ACLU</a><span>, </span><a href="https://www.eff.org/" rel="">EFF</a><span>, and </span><a href="https://citizenlab.ca/" rel="">Citizen Lab</a><span> that investigate these surveillance programs.</span></p></li><li><p><strong>Harden your devices</strong><span>: keep phones updated, consider Lockdown Mode, and reduce app location sharing.</span></p></li></ul><div data-attrs="{&quot;url&quot;:&quot;https://olgalautman.substack.com/p/ice-is-buying-a-tool-to-track-hundreds?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="CaptionedButtonToDOM"><p>Thanks for reading Unmasking Russia! This post is public, so feel free to share it.</p><p data-attrs="{&quot;url&quot;:&quot;https://olgalautman.substack.com/p/ice-is-buying-a-tool-to-track-hundreds?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="ButtonCreateButton"><a href="https://olgalautman.substack.com/p/ice-is-buying-a-tool-to-track-hundreds?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Announcing Tinker (123 pts)]]></title>
            <link>https://thinkingmachines.ai/blog/announcing-tinker/</link>
            <guid>45441219</guid>
            <pubDate>Wed, 01 Oct 2025 18:20:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thinkingmachines.ai/blog/announcing-tinker/">https://thinkingmachines.ai/blog/announcing-tinker/</a>, See on <a href="https://news.ycombinator.com/item?id=45441219">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">
    










    
<main id="main">
  
  
  
  
  
  
  
  

  

  
  
  

  

  <article>
    
    <nav id="left-toc" aria-label="Table of contents"></nav>
    
    <p>
  <a href="https://www.computerhistory.org/collections/catalog/X39.81/" target="_blank" rel="noopener">TinkerToy Computer</a> invented by <a href="https://en.wikipedia.org/wiki/Danny_Hillis" target="_blank" rel="noopener">Daniel Hillis</a> and <a href="https://en.wikipedia.org/wiki/Brian_Silverman" target="_blank" rel="noopener">Brian Silverman</a>
</p>
<p>Today, we are launching <a href="https://thinkingmachines.ai/tinker">Tinker</a>, a flexible API for fine-tuning language models. It empowers researchers and hackers to experiment with models by giving them control over the algorithms and data while we handle the complexity of distributed training. Tinker advances our mission of enabling more people to do research on cutting-edge models and customize them to their needs.</p>
<p>Tinker lets you fine-tune a range of large and small open-weight models, including large mixture-of-experts models such as Qwen-235B-A22B. Switching from a small model to a large one is as simple as changing a single string in your Python code.</p>
<p>Tinker is a managed service that runs on our internal clusters and training infrastructure. We handle scheduling, resource allocation, and failure recovery. This allows you to get small or large runs started immediately, without worrying about managing infrastructure. We use LoRA so that we can share the same pool of compute between multiple training runs, lowering costs.</p>
<p>Tinker’s API gives you low-level primitives like <code>forward_backward</code> and <code>sample</code>, which can be used to express most common post-training methods. Even so, achieving good results requires getting many details right. That’s why we’re releasing an open-source library, the <a href="http://github.com/thinking-machines-lab/tinker-cookbook">Tinker Cookbook</a>, with modern implementations of post-training methods that run on top of the Tinker API.</p>
<p>Groups at Princeton, Stanford, Berkeley, and Redwood Research have already been using Tinker:</p>
<ul>
<li>The <a href="https://blog.goedel-prover.com/">Princeton Goedel Team</a> trained mathematical theorem provers</li>
<li>The <a href="https://statmech.stanford.edu/">Rotskoff Chemistry group</a> at Stanford fine-tuned a model to complete chemistry reasoning tasks</li>
<li><a href="https://sky.cs.berkeley.edu/project/skyrl/">Berkeley’s SkyRL group</a> ran experiments on a custom async off-policy RL training loop with multi-agents and multi-turn tool-use.</li>
<li><a href="https://www.redwoodresearch.org/">Redwood Research</a> used Tinker to RL Qwen3-32B on difficult AI control tasks</li>
</ul>
<p>Tinker is now in private beta for researchers and developers. You can sign up for the Tinker waitlist <a href="https://thinkingmachines.ai/tinker">here</a>. We will be onboarding users to the platform starting today.</p>
<p>If you’re an organization interested in using Tinker, please contact us <a href="https://thinkingmachines.ai/cdn-cgi/l/email-protection#691d0007020c1b291d0100070200070e04080a0100070c1a470800">here</a>.</p>
<p>Tinker will be free to start. We will introduce usage-based pricing in the coming weeks.</p>
<p>We’re excited to see what you discover and make with Tinker!</p>

    
  </article>

  

  
  
  
  
  
  

  
  
  
  

  
  
</main>

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jane Goodall has died (1061 pts)]]></title>
            <link>https://www.latimes.com/obituaries/story/2025-10-01/jane-goodall-chimpanzees-dead</link>
            <guid>45441069</guid>
            <pubDate>Wed, 01 Oct 2025 18:10:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latimes.com/obituaries/story/2025-10-01/jane-goodall-chimpanzees-dead">https://www.latimes.com/obituaries/story/2025-10-01/jane-goodall-chimpanzees-dead</a>, See on <a href="https://news.ycombinator.com/item?id=45441069">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-element="story-body" data-subscriber-content=""> <p>Jane Goodall, the trailblazing naturalist whose intimate observations of chimpanzees in the African wild produced powerful insights that transformed basic conceptions of humankind, has died. She was 91.</p><p>A tireless advocate of preserving chimpanzees’ natural habitat, Goodall died on Wednesday morning in California of natural causes, the Jane Goodall Institute announced on its <a href="https://www.instagram.com/p/DPRn2HTCFYt/?igsh=NTc4MTIwNjQ2YQ%3D%3D" target="_blank">Instagram page</a>. </p><p>“Dr. Goodall’s discoveries as an ethologist revolutionized science,” the Jane Goodall Institute said in a statement. </p><p>A protege of anthropologist Louis S.B. Leakey, Goodall made history in 1960 when she discovered that chimpanzees, humankind’s closest living ancestors, made and used tools, characteristics that scientists had long thought were exclusive to humans.</p><p>She also found that chimps hunted prey, ate meat, and were capable of a range of emotions and behaviors similar to those of humans, including filial love, grief and violence bordering on warfare.</p><p>In the course of establishing one of the world’s longest-running studies of wild animal behavior at what is now Tanzania’s Gombe Stream National Park, she gave her chimp subjects names instead of numbers, a practice that raised eyebrows in the male-dominated field of primate studies in the 1960s. But within a decade, the trim British scientist with the tidy ponytail was a National Geographic heroine, whose books and films educated a worldwide audience with stories of the apes she called David Graybeard, Mr. McGregor, Gilka and Flo.</p><p>“When we read about a woman who gives funny names to chimpanzees and then follows them into the bush, meticulously recording their every grunt and groom, we are reluctant to admit such activity into the big leagues,” the late biologist Stephen Jay Gould wrote of the scientific world’s initial reaction to Goodall.</p><p>But Goodall overcame her critics and produced work that Gould later characterized as “one of the Western world’s great scientific achievements.”</p><p>Tenacious and keenly observant, Goodall paved the way for other women in primatology, including the late gorilla researcher Dian Fossey and orangutan expert Birutė Galdikas. She was honored in 1995 with the National Geographic Society’s Hubbard Medal, which then had been bestowed only 31 times in the previous 90 years to such eminent figures as North Pole explorer Robert E. Peary and aviator Charles Lindbergh.</p><p>In her 80s she continued to travel 300 days a year to speak to schoolchildren and others about the need to fight deforestation, preserve chimpanzees’ natural habitat and promote sustainable development in Africa. She was in California as part of her speaking tour in the U.S. at the time of her death.</p><div data-click="enhancement" data-align-center=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/4ac8360/2147483647/strip/true/crop/1024x680+0+0/resize/320x213!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg 320w,https://ca-times.brightspotcdn.com/dims4/default/482653b/2147483647/strip/true/crop/1024x680+0+0/resize/568x377!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg 568w,https://ca-times.brightspotcdn.com/dims4/default/2a1995f/2147483647/strip/true/crop/1024x680+0+0/resize/768x510!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg 768w,https://ca-times.brightspotcdn.com/dims4/default/169fbf0/2147483647/strip/true/crop/1024x680+0+0/resize/1024x680!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg 1024w,https://ca-times.brightspotcdn.com/dims4/default/f152c13/2147483647/strip/true/crop/1024x680+0+0/resize/1200x797!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg 1200w" sizes="100vw">       <img alt="Jane Goodall in Gombe National Park in Tanzania." srcset="https://ca-times.brightspotcdn.com/dims4/default/855f60f/2147483647/strip/true/crop/1024x680+0+0/resize/320x213!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg 320w,https://ca-times.brightspotcdn.com/dims4/default/2b83032/2147483647/strip/true/crop/1024x680+0+0/resize/568x377!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg 568w,https://ca-times.brightspotcdn.com/dims4/default/b66c829/2147483647/strip/true/crop/1024x680+0+0/resize/768x510!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg 768w,https://ca-times.brightspotcdn.com/dims4/default/7570df9/2147483647/strip/true/crop/1024x680+0+0/resize/1024x680!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg 1024w,https://ca-times.brightspotcdn.com/dims4/default/04e829c/2147483647/strip/true/crop/1024x680+0+0/resize/1200x797!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg 1200w" sizes="100vw" width="1200" height="797" src="https://ca-times.brightspotcdn.com/dims4/default/04e829c/2147483647/strip/true/crop/1024x680+0+0/resize/1200x797!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F9c%2F39%2F72e661ad406e8fbe8af7e121da8e%2Fhope-art-56.jpg" decoding="async" loading="lazy">   </picture>   <div>      <p>(Chase Pickering / Jane Goodall Institute)</p>   </div>   </figure> </div><p>Goodall was born April 3, 1934, in London and grew up in the English coastal town of Bournemouth. The daughter of a businessman and a writer who separated when she was a child and later divorced, she was raised in a matriarchal household that included her maternal grandmother, her mother, Vanne, some aunts and her sister, Judy.</p><p>She demonstrated an affinity for nature from a young age, filling her bedroom with worms and sea snails that she rushed back to their natural homes after her mother told her they would otherwise die.</p><p>When she was about 5, she disappeared for hours to a dark henhouse to see how chickens laid eggs, so absorbed that she was oblivious to her family’s frantic search for her. She did not abandon her study until she observed the wondrous event.</p><p>“Suddenly with a plop, the egg landed on the straw. With clucks of pleasure the hen shook her feathers, nudged the egg with her beak, and left,” Goodall wrote almost 60 years later. “It is quite extraordinary how clearly I remember that whole sequence of events.”</p><p>When finally she ran out of the henhouse with the exciting news, her mother did not scold her but patiently listened to her daughter’s account of her first scientific observation.</p><p>Later, she gave Goodall books about animals and adventure — especially the Doctor Dolittle tales and Tarzan. Her daughter became so enchanted with Tarzan’s world that she insisted on doing her homework in a tree.</p><p>“I was madly in love with the Lord of the Jungle, terribly jealous of his Jane,” Goodall wrote in her 1999 memoir, “Reason for Hope: A Spiritual Journey.” “It was daydreaming about life in the forest with Tarzan that led to my determination to go to Africa, to live with animals and write books about them.”</p><p>Her opportunity came after she finished high school. A week before Christmas in 1956 she was invited to visit an old school chum’s family farm in Kenya. Goodall saved her earnings from a waitress job until she had enough for a round-trip ticket.</p><div data-click="enhancement" data-align-center-expanded=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/48fc7d0/2147483647/strip/true/crop/1992x1334+0+0/resize/320x214!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/95b8306/2147483647/strip/true/crop/1992x1334+0+0/resize/568x381!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/17a7894/2147483647/strip/true/crop/1992x1334+0+0/resize/768x515!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/1c876f0/2147483647/strip/true/crop/1992x1334+0+0/resize/1024x686!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG 1024w,https://ca-times.brightspotcdn.com/dims4/default/30c89ba/2147483647/strip/true/crop/1992x1334+0+0/resize/1200x804!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG 1200w" sizes="100vw">       <img alt="Jane Goodall gives a little kiss to Tess, a 5- or 6-year-old female chimpanzee, in 1997." srcset="https://ca-times.brightspotcdn.com/dims4/default/ab5b58c/2147483647/strip/true/crop/1992x1334+0+0/resize/320x214!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/ef5412a/2147483647/strip/true/crop/1992x1334+0+0/resize/568x381!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/62ea93b/2147483647/strip/true/crop/1992x1334+0+0/resize/768x515!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/b8e4b6b/2147483647/strip/true/crop/1992x1334+0+0/resize/1024x686!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG 1024w,https://ca-times.brightspotcdn.com/dims4/default/d37bd3f/2147483647/strip/true/crop/1992x1334+0+0/resize/1200x804!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG 1200w" sizes="100vw" width="1200" height="804" src="https://ca-times.brightspotcdn.com/dims4/default/d37bd3f/2147483647/strip/true/crop/1992x1334+0+0/resize/1200x804!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F77%2Ff9%2Fe58d647d4f408ecab087c086fc4c%2Fla-photos-wgetty-kenya-jane-goodall.JPG" decoding="async" loading="lazy">   </picture>   <div>      <p>(Jean-Marc Bouju / Associated Press)</p>   </div>   </figure> </div><p>She arrived in Kenya in 1957, thrilled to be living in the Africa she had “always felt stirring in my blood.” At a dinner party in Nairobi shortly after her arrival, someone told her that if she was interested in animals, she should meet Leakey, already famous for his discoveries in East Africa of man’s fossil ancestors.</p><p>She went to see him at what’s now the National Museum of Kenya, where he was curator. He hired her as a secretary and soon had her helping him and his wife, Mary, dig for fossils at Olduvai Gorge, a famous site in the Serengeti Plains in what is now northern Tanzania.</p><p>Leakey spoke to her of his desire to learn more about all the great apes. He said he had heard of a community of chimpanzees on the rugged eastern shore of Lake Tanganyika where an intrepid researcher might make valuable discoveries.</p><p>When Goodall told him this was exactly the kind of work she dreamed of doing, Leakey agreed to send her there.</p><p>It took Leakey two years to find funding, which gave Goodall time to study primate behavior and anatomy in London. She finally landed in Gombe in the summer of 1960.</p><p>On a rocky outcropping she called the Peak, Goodall made her first important observation. Scientists had thought chimps were docile vegetarians, but on this day about three months after her arrival, Goodall spied a group of the apes feasting on something pink. It turned out to be a baby bush pig.</p><p>Two weeks later, she made an even more exciting discovery — the one that would establish her reputation. She had begun to recognize individual chimps, and on a rainy October day in 1960, she spotted the one with white hair on his chin. He was sitting beside a mound of red earth, carefully pushing a blade of grass into a hole, then withdrawing it and poking it into his mouth.</p><p>When he finally ambled off, Goodall hurried over for a closer look. She picked up the abandoned grass stalk, stuck it into the same hole and pulled it out to find it covered with termites. The chimp she later named David Graybeard had been using the stalk to fish for the bugs.</p><p>“It was hard for me to believe what I had seen,” Goodall later wrote. “It had long been thought that we were the only creatures on earth that used and made tools. ‘Man the Toolmaker’ is how we were defined...” What Goodall saw challenged man’s uniqueness.</p><p>When she sent her report to Leakey, he responded: “We must now redefine man, redefine tool, or accept chimpanzees as human!”</p><p>Goodall’s startling finding, published in Nature in 1964, enabled Leakey to line up funding to extend her stay at Gombe. It also eased Goodall’s admission to Cambridge University to study ethology. In 1965, she became the eighth person in Cambridge history to earn a doctorate without first having a bachelor’s degree.</p><p>In the meantime, she had met and in 1964 married Hugo Van Lawick, a gifted filmmaker who had traveled to Gombe to make a documentary about her chimp project. They had a child, Hugo Eric Louis — later nicknamed Grub — in 1967.</p><p>Goodall later said that raising Grub, who lived at Gombe until he was 9, gave her insights into the behavior of chimp mothers. Conversely, she had “no doubt that my observation of the chimpanzees helped me to be a better mother.”</p><p>She and Van Lawick were married for 10 years, divorcing in 1974. The following year she married Derek Bryceson, director of Tanzania National Parks. He died of colon cancer four years later.</p><p>Within a year of arriving at Gombe, Goodall had chimps literally eating out of her hands. Toward the end of her second year there, David Graybeard, who had shown the least fear of her, was the first to allow her physical contact. She touched him lightly and he permitted her to groom him for a full minute before gently pushing her hand away. For an adult male chimpanzee who had grown up in the wild to tolerate physical contact with a human was, she wrote in her 1971 book “In the Shadow of Man,” “a Christmas gift to treasure.”</p><div data-click="enhancement" data-align-center=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/fdcd33a/2147483647/strip/true/crop/1992x1344+0+0/resize/320x216!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/b6a5133/2147483647/strip/true/crop/1992x1344+0+0/resize/568x383!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/5652127/2147483647/strip/true/crop/1992x1344+0+0/resize/768x518!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/c9d1992/2147483647/strip/true/crop/1992x1344+0+0/resize/1024x691!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG 1024w,https://ca-times.brightspotcdn.com/dims4/default/0e0c1d4/2147483647/strip/true/crop/1992x1344+0+0/resize/1200x810!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG 1200w" sizes="100vw">       <img alt="Jane Goodall shares a play with Bahati, a 3 year-old female chimpanzee" srcset="https://ca-times.brightspotcdn.com/dims4/default/d73d494/2147483647/strip/true/crop/1992x1344+0+0/resize/320x216!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/aa710d0/2147483647/strip/true/crop/1992x1344+0+0/resize/568x383!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/6f21bf4/2147483647/strip/true/crop/1992x1344+0+0/resize/768x518!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/45da23e/2147483647/strip/true/crop/1992x1344+0+0/resize/1024x691!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG 1024w,https://ca-times.brightspotcdn.com/dims4/default/dd65665/2147483647/strip/true/crop/1992x1344+0+0/resize/1200x810!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG 1200w" sizes="100vw" width="1200" height="810" src="https://ca-times.brightspotcdn.com/dims4/default/dd65665/2147483647/strip/true/crop/1992x1344+0+0/resize/1200x810!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fd9%2Fa9%2Fca7199564ac18bd4fa065efdfd96%2Fla-photos-wgetty-kenya-jane-goodall1.JPG" decoding="async" loading="lazy">   </picture>   <div>   <p>Jane Goodall shares a play with Bahati, a 3 year-old female chimpanzee at the Sweetwaters Chimpanzee Sanctuary,  north of Nairobi Sunday December 6, 1997.</p>   <p>(Jean-Marc Bouju/Associated Press)</p>   </div>   </figure> </div><p>Her studies yielded a trove of other observations on behaviors, including etiquette (such as soliciting a pat on the rump to indicate submission) and the sex lives of chimps. She collected some of the most fascinating information on the latter by watching Flo, an older female with a bulbous nose and an amazing retinue of suitors who was bearing children well into her 40s.</p><p>Her reports initially caused much skepticism in the scientific community. “I was not taken very seriously by many of the scientists. I was known as a [National] Geographic cover girl,” she recalled in a CBS interview in 2012.</p><p>Her unorthodox personalizing of the chimps was particularly controversial. The editor of one of her first published papers insisted on crossing out all references to the creatures as “he” or “she” in favor of “it.” Goodall eventually prevailed.</p><p>Her most disturbing studies came in the mid-1970s, when she and her team of field workers began to record a series of savage attacks.</p><p>The incidents grew into what Goodall called the four-year war, a period of brutality carried out by a band of male chimpanzees from a region known as the Kasakela Valley. The marauders beat and slashed to death all the males in a neighboring colony and subjugated the breeding females, essentially annihilating an entire community.</p><p>It was the first time a scientist had witnessed organized aggression by one group of non-human primates against another. Goodall said this “nightmare time” forever changed her view of ape nature.</p><p>“During the first 10 years of the study I had believed ... that the Gombe chimpanzees were, for the most part, rather nicer than human beings,” she wrote in “Reason for Hope: A Spiritual Journey,” a 1999 book co-authored with Phillip Berman. “Then suddenly we found that the chimpanzees could be brutal — that they, like us, had a dark side to their nature.”</p><p>Critics tried to dismiss the evidence as merely anecdotal. Others thought she was wrong to publicize the violence, fearing that irresponsible scientists would use the information to “prove” that the tendency to war is innate in humans, a legacy from their ape ancestors. Goodall persisted in talking about the attacks, maintaining that her purpose was not to support or debunk theories about human aggression but to “understand a little better” the nature of chimpanzee aggression.</p><p>“My question was: How far along our human path, which has led to hatred and evil and full-scale war, have chimpanzees traveled?”</p><p>Her observations of chimp violence marked a turning point for primate researchers, who had considered it taboo to talk about chimpanzee behavior in human terms. But by the 1980s, much chimp behavior was being interpreted in ways that would have been labeled anthropomorphism — ascribing human traits to non-human entities — decades earlier. Goodall, in removing the barriers, raised primatology to new heights, opening the way for research on subjects ranging from political coalitions among baboons to the use of deception by an array of primates.</p><p>Her concern about protecting chimpanzees in the wild and in captivity led her in 1977 to found the <a href="https://www.janegoodall.org/" target="_blank">Jane Goodall Institute</a> to advocate for great apes and support research and public education. She also established Roots and Shoots, a program aimed at youths in 130 countries, and TACARE, which involves African villagers in sustainable development.</p><p>She became an international ambassador for chimps and conservation in 1986 when she saw a film about the mistreatment of laboratory chimps. The secretly taped footage “was like looking into the Holocaust,” she told interviewer Cathleen Rountree in 1998. From that moment, she became a globe-trotting crusader for animal rights. </p><p>In the 2017 documentary “Jane,” the producer poured through 140 hours of footage of Goodall that had been hidden away in the National Geographic archives. The film won a Los Angeles Film Critics Assn. Award, one of many honors it received.</p><div data-video-disable-history="" data-click="enhancement" data-align-center="">  <ps-youtubeplayer data-video-player="" data-player-id="f826de22fae514bee8031d59e313f3fbc" data-video-id="X6CW7dSXKWo" data-video-title="Jane Goodall discusses “The Book of Hope”" data-slot-name="/21787098806/web.latimes/obituaries/video" data-lazy-offset="1.0" data-autoplay-threshold="50" data-miniplayer="" data-internal-video-id="X6CW7dSXKWo" data-ad-slot-name="/21787098806/web.latimes/obituaries/video" data-ad-provider="ima" data-ima-sdk-url="https://imasdk.googleapis.com/js/sdkloader/ima3.js" data-ima-ad-tag-url="https://pubads.g.doubleclick.net/gampad/ads?sz=640x480&amp;gdfp_req=1&amp;env=vp&amp;output=vast&amp;unviewed_position_start=1&amp;cmsid=2652439&amp;ad_rule=0&amp;plcmt=1">  <picture> <source srcset="https://img.youtube.com/vi_webp/X6CW7dSXKWo/maxresdefault.webp" type="image/webp"> <source srcset="https://img.youtube.com/vi/X6CW7dSXKWo/maxresdefault.jpg"> <img id="yt-img-X6CW7dSXKWo" alt="" src="https://img.youtube.com/vi/X6CW7dSXKWo/hqdefault.jpg" loading="lazy" decoding="async"> </picture>       </ps-youtubeplayer> </div><p>In a <a href="https://www.latimes.com/la-oe-morrison18-2009jul18-column.html" target="_blank">ranging 2009 interview</a> with Times columnist Patt Morrison, Goodall mused on topics from traditional zoos — she said most captive environments should be abolished — to climate change, a battle she feared humankind was quickly losing, if not lost already. She also spoke about the power of what one human can accomplish.</p><p>“I always say, ‘If you would spend just a little bit of time learning about the consequences of the choices you make each day’ — what you buy, what you eat, what you wear, how you interact with people and animals — and start consciously making choices, that would be beneficial rather than harmful.”</p><p>As the years  passed, Goodall continued to track Gombe’s chimps, accumulating enough information to draw the arcs of their lives — from birth through sometimes troubled adolescence, maturity, illness and finally death.</p><p>She wrote movingly about how she followed Mr. McGregor, an older, somewhat curmudgeonly chimp, through his agonizing death from polio, and how the orphan Gilka survived to lonely adulthood only to have her babies snatched from her by a pair of cannibalistic female chimps.</p><div data-click="enhancement" data-align-left=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/fe7d253/2147483647/strip/true/crop/3960x2640+0+0/resize/320x213!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/097a3cb/2147483647/strip/true/crop/3960x2640+0+0/resize/568x379!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/2cf83f8/2147483647/strip/true/crop/3960x2640+0+0/resize/768x512!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/fed4802/2147483647/strip/true/crop/3960x2640+0+0/resize/1024x683!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG 1024w,https://ca-times.brightspotcdn.com/dims4/default/66b9217/2147483647/strip/true/crop/3960x2640+0+0/resize/1200x800!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG 1200w" sizes="100vw">       <img alt="Jane Goodall in San Diego." srcset="https://ca-times.brightspotcdn.com/dims4/default/e0c32f5/2147483647/strip/true/crop/3960x2640+0+0/resize/320x213!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG 320w,https://ca-times.brightspotcdn.com/dims4/default/7700788/2147483647/strip/true/crop/3960x2640+0+0/resize/568x379!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG 568w,https://ca-times.brightspotcdn.com/dims4/default/fd7c124/2147483647/strip/true/crop/3960x2640+0+0/resize/768x512!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG 768w,https://ca-times.brightspotcdn.com/dims4/default/f2aaab4/2147483647/strip/true/crop/3960x2640+0+0/resize/1024x683!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG 1024w,https://ca-times.brightspotcdn.com/dims4/default/4373298/2147483647/strip/true/crop/3960x2640+0+0/resize/1200x800!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG 1200w" sizes="100vw" width="1200" height="800" src="https://ca-times.brightspotcdn.com/dims4/default/4373298/2147483647/strip/true/crop/3960x2640+0+0/resize/1200x800!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F2f%2F63%2Fcffde3f94732b171d7b069c37f19%2F458667-sd-me-jane-goodall001.JPG" decoding="async" loading="lazy">   </picture>   <div>      <p>(Sam Hodgson/The San Diego Union-Tribune)</p>   </div>   </figure> </div><p>Her reaction in 1972 to the death of Flo, a prolific female known as Gombe’s most devoted mother, suggested the depth of feeling that Goodall had for the animals. Knowing that Flo’s faithful son Flint was nearby and grieving, Goodall watched over the body all night to keep marauding bush pigs from violating her remains.</p><p>“People say to me, thank you for giving them characters and personalities,” Goodall once told CBS’s “60 Minutes.” “I said I didn’t give them anything. I merely translated them for people.”</p><p><i>Woo is a former Times staff writer.</i></p><div data-list-id="00000192-be42-da32-a3db-ff76fc3b0000" data-module-id="00000192-be42-da32-a3db-ff76fc3b0000" data-click="enhancement" data-align-center="">  <p data-element="element-header" data-click="liZZListTitleCTA">  <h3 data-element="element-header-title" data-counter="3">More to Read </h3>  </p>      </div> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stop Avoiding Politics (363 pts)]]></title>
            <link>https://terriblesoftware.org/2025/10/01/stop-avoiding-politics/</link>
            <guid>45440571</guid>
            <pubDate>Wed, 01 Oct 2025 17:36:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terriblesoftware.org/2025/10/01/stop-avoiding-politics/">https://terriblesoftware.org/2025/10/01/stop-avoiding-politics/</a>, See on <a href="https://news.ycombinator.com/item?id=45440571">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Say the word “politics” to most engineers and watch their face scrunch up like they just bit into a lemon. We’ve all been conditioned to believe that workplace politics is this dirty game played by manipulative ladder-climbers while the “real” engineers focus on the code.</p>



<p>I used to think the same way. For years as an engineer, I wore my hatred of politics like a badge of honor. I was above all that nonsense. I just wanted to ship. Politics was for those other people, the ones who didn’t have what it takes technically.</p>



<p>Now I think the opposite: <strong>politics isn’t the problem; bad politics is.</strong> And pretending politics doesn’t exist? That’s how bad politics wins.</p>



<p>Politics is just how humans coordinate in groups. It’s the invisible network of relationships, influence, and informal power that exists in every organization. You can refuse to participate, but that doesn’t make it go away. It just means decisions get made without you.</p>



<p>Think about the last time a terrible technical decision got pushed through at your company. Maybe it was adopting some overcomplicated architecture, or choosing a vendor that everyone knew was wrong, or killing a project that was actually working. I bet if you dig into what happened, you’ll find it wasn’t because the decision-makers were stupid. It’s because the people with the right information weren’t in the room. They “didn’t do politics.”</p>



<p>Meanwhile, someone who understood how influence works was in that room, making their case, building coalitions, showing they’d done their homework. And their idea won. Not because it was better, but because they showed up to play while everyone else was “too pure” for politics.</p>



<p>Ideas don’t speak. People do. And the people who understand how to navigate organizational dynamics, build relationships, and yes, play politics? Their ideas get heard.</p>



<p>When you build strong relationships across teams, understand what motivates different stakeholders, and know how to build consensus, you’re doing politics. When you take time to explain your technical decisions to non-technical stakeholders in language they understand, that’s politics. When you grab coffee with someone from another team to understand their challenges, that’s politics too.</p>



<p><strong>Good politics is just being strategic about relationships and influence in the service of good outcomes.</strong></p>



<p>The best technical leaders are incredibly political. They just don’t call it that. They call it “stakeholder management” or “building alignment” or “organizational awareness.” But it’s politics, and they’re good at it.</p>



<p>The engineers who refuse to engage with politics often complain that their companies make bad technical decisions. But they’re not willing to do what it takes to influence those decisions. They want a world where technical merit alone determines outcomes. That world doesn’t exist and never has.</p>



<p>This isn’t about becoming a scheming backstabber. As I wrote in <a href="https://terriblesoftware.org/2025/03/31/your-strengths-are-your-weaknesses/">Your Strengths Are Your Weaknesses</a>, the same trait can be positive or negative depending on how you use it. Politics is the same way. You can use political skills to manipulate and self-promote, or you can use them to get good ideas implemented and protect your team from bad decisions.</p>



<p>Here’s what good politics looks like in practice:</p>



<ol>
<li><strong>Building relationships before you need them.</strong> That random coffee with someone from the data team? Six months later, they’re your biggest advocate for getting engineering resources for your data pipeline project.</li>



<li><strong>Understanding the real incentives.</strong> Your VP doesn’t care about your beautiful microservices architecture. They care about shipping features faster. Frame your technical proposals in terms of what they actually care about.</li>



<li><strong>Managing up effectively.</strong> Your manager is juggling competing priorities you don’t see. Keep them informed about what matters, flag problems early with potential solutions, and help them make good decisions. When they trust you to handle things, they’ll fight for you when it matters</li>



<li><strong>Creating win-win situations.</strong> Instead of fighting for resources, find ways to help other teams while getting what you need. It doesn’t have to be a zero-sum game.</li>



<li><strong>Being visible.</strong> If you do great work but nobody knows about it, did it really happen? Share your wins, present at all-hands, write those design docs that everyone will reference later.</li>
</ol>



<p>The alternative to good politics isn’t no politics. It’s bad politics winning by default. It’s the loud person who’s wrong getting their way because the quiet person who’s right won’t speak up. It’s good projects dying because nobody advocated for them. It’s talented people leaving because they couldn’t navigate the organizational dynamics.</p>



<p>Stop pretending you’re above politics. You’re not. Nobody is. The only question is whether you’ll get good at it or keep losing to people who already are.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenTSLM: Language models that understand time series (219 pts)]]></title>
            <link>https://www.opentslm.com/</link>
            <guid>45440431</guid>
            <pubDate>Wed, 01 Oct 2025 17:25:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.opentslm.com/">https://www.opentslm.com/</a>, See on <a href="https://news.ycombinator.com/item?id=45440431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>The Future of AI Delivered on Time</p><div><p>Paper Released Sep 30, 2025</p><p>Stanford Repo Released Oct 1, 2025</p></div></div><section><p>AI understands text, images, audio, and video.<br>But the real world runs on time.</p><p>Every heartbeat, price tick, sensor pulse, machine log, and user click is a temporal signal.<br>Current models can't reason about them.</p><p>We're changing that.</p></section><section><h2>A New Class of Foundation Models</h2><div><p><span>Time-Series Language Models (TSLMs)</span> are multimodal foundation models with time series as a native modality, next to text, enabling direct reasoning, explanation, and forecasting over temporal data in natural language.</p><p>Our research shows order-of-magnitude gains in temporal reasoning while running on smaller, faster backbones. TSLMs are not an add-on. They're a new modality for AI.</p></div></section><section><h2>Open Core, Frontier Edge</h2><div><p><span>OpenTSLM:</span> Lightweight base models trained on public data, released openly. They set the standard for temporal reasoning and power a global developer and research ecosystem.</p><p><span>Frontier TSLMs:</span> Advanced proprietary models trained on specialized data, delivering enterprise-grade performance and powering APIs, fine-tuning, and vertical solutions.</p></div></section><section><h2>Our Vision</h2><div><p>We're building the temporal interface for AI - the layer that connects continuous real-world signals to intelligent decisions and autonomous agents.</p><p>A universal TSLM will power proactive healthcare, adaptive robotics, resilient infrastructure, and new forms of human-AI collaboration.</p></div></section><section><h2>About Us</h2><p>OpenTSLM is a team of scientists, engineers, and builders from ETH, Stanford, Harvard, Cambridge, TUM, CDTM, Google, Meta, AWS, and beyond. We are the original authors of the OpenTSLM paper.</p></section><section><h2><p>Discover how TSLMs could transform</p></h2></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Solar leads EU electricity generation as renewables hit 54% (262 pts)]]></title>
            <link>https://electrek.co/2025/09/30/solar-leads-eu-electricity-generation-as-renewables-hit-54-percent/</link>
            <guid>45440387</guid>
            <pubDate>Wed, 01 Oct 2025 17:22:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/09/30/solar-leads-eu-electricity-generation-as-renewables-hit-54-percent/">https://electrek.co/2025/09/30/solar-leads-eu-electricity-generation-as-renewables-hit-54-percent/</a>, See on <a href="https://news.ycombinator.com/item?id=45440387">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="900" src="https://electrek.co/wp-content/uploads/sites/3/2025/09/pexels-photo-30762864.jpeg?quality=82&amp;strip=all&amp;w=1600" alt="EU electricity" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/09/pexels-photo-30762864.jpeg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/09/pexels-photo-30762864.jpeg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/09/pexels-photo-30762864.jpeg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/09/pexels-photo-30762864.jpeg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high">			<figcaption>
				Photo by Wolfgang Weiser on <a href="https://www.pexels.com/photo/historic-war-memorial-in-salzkotten-germany-30762864/" rel="nofollow">Pexels.com</a>			</figcaption>
			</figure>

<p>More than half of the European Union’s (EU) electricity came from renewables in the second quarter of 2025, and solar is leading from the front.</p>



<p>According to new data from Eurostat, renewable energy sources generated 54% of the EU’s net electricity in Q2 2025, up from 52.7% year-over-year. The growth came mainly from solar, which produced 122,317 gigawatt-hours (GWh) – nearly 20% of the total electricity generation mix.</p>



<p>June 2025 was a milestone month: Solar became the EU’s single largest electricity source for the first time ever. It supplied 22% of all power that month, edging out nuclear (21.6%), wind (15.8%), hydro (14.1%), and natural gas (13.8%).</p>



<p>Some countries are already nearly 100% renewable. Denmark led with an impressive 94.7% share of renewables in net electricity generated, followed by Latvia (93.4%), Austria (91.8%), Croatia (89.5%), and Portugal (85.6%). At the other end of the spectrum, Slovakia (19.9%), Malta (21.2%), and the Czech Republic (22.1%) lagged behind.</p>	
	



<p>In total, 15 EU countries saw their share of renewable generation rise year-over-year. Luxembourg (+13.5 percentage points) and Belgium (+9.1 pp) posted the most significant gains, driven largely by solar power growth.</p>



<p>Across the EU, solar made up 36.8% of renewable generation, followed by wind at 29.5%, hydro at 26%, biomass at 7.3%, and geothermal at 0.4%.</p>



<p><strong>Read more:</strong> <a href="https://electrek.co/2025/09/24/eia-solar-and-wind-crush-coal-with-20-percent-more-power-in-2025/">EIA: Solar and wind crush coal with 20% more power in 2025</a></p>



<figure><a href="https://www.energysage.com/landing/home-solar/p/electrek-rsm-ml/?utm_medium=Partner&amp;utm_source=Electrek" target="_blank" rel=" noreferrer noopener"><img decoding="async" width="750" height="150" src="https://electrek.co/wp-content/uploads/sites/3/2025/09/DES-1038_Electrek-Banners_Resiliency_b651c0.png" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/09/DES-1038_Electrek-Banners_Resiliency_b651c0.png 750w, https://electrek.co/wp-content/uploads/sites/3/2025/09/DES-1038_Electrek-Banners_Resiliency_b651c0.png?resize=150,30 150w, https://electrek.co/wp-content/uploads/sites/3/2025/09/DES-1038_Electrek-Banners_Resiliency_b651c0.png?resize=300,60 300w, https://electrek.co/wp-content/uploads/sites/3/2025/09/DES-1038_Electrek-Banners_Resiliency_b651c0.png?resize=350,70 350w, https://electrek.co/wp-content/uploads/sites/3/2025/09/DES-1038_Electrek-Banners_Resiliency_b651c0.png?resize=140,28 140w" sizes="(max-width: 750px) 100vw, 750px"></a></figure>



<hr>



<p><strong><em>The 30% federal solar tax credit is ending this year. If you’ve ever considered going solar, now’s the time to act. To make sure you find a trusted, reliable solar installer near you that offers competitive pricing, check out </em></strong><a href="https://c32b704.na1.hs-sales-engage.com/Ctc/P+23284/c32B704/JlF3crJ5W8wLKSR6lZ3p-W7s8QC84nlPDsW35x5rq5vFtfyW88cj9v1vtBqZVbfGXn1xzh51W8jvN5t8yhpLdW57_shc5Rp3MdMN0T8GbTR9LW5kbjcl41XSlfW1DWv0v4vFzCtW8zy2kd45l-lnW8TNZ_52QnSx-W54zS-f2SKtm5W5WK2df426XsKV7j3fd6CkxT7W7x6GCb20V3brW6qqHST3bthfYW6CpFKd7_yQ0XW2ysWt869bCphW6r8YtG4GrsVkW46V-MQ5bp2VwW5m6Bnn8b0H0_VsQ6Xw673G4GW2FfPnr6RDKb7W7dQjKN7Mqbk0W2D2_x791FrC9W84mN5P1JPzsPW5Ymmh58m7YHdW2RdHWF257Zzhf4hJ8JP04"><strong><em>EnergySage</em></strong></a><strong><em>, a free service that makes it easy for you to go solar. It has hundreds of pre-vetted solar installers competing for your business, ensuring you get high-quality solutions and save 20-30% compared to going it alone. Plus, it’s free to use, and you won’t get sales calls until you select an installer and share your phone number with them.&nbsp;</em></strong></p>



<p><strong><em>Your personalized solar quotes are easy to compare online and you’ll get access to unbiased Energy Advisors to help you every step of the way. </em></strong><a href="https://c32b704.na1.hs-sales-engage.com/Ctc/P+23284/c32B704/JlF3crJ5W8wLKSR6lZ3p-W7s8QC84nlPDsW35x5rq5vFtfyW88cj9v1vtBqZVbfGXn1xzh51W8jvN5t8yhpLdW57_shc5Rp3MdMN0T8GbTR9LW5kbjcl41XSlfW1DWv0v4vFzCtW8zy2kd45l-lnW8TNZ_52QnSx-W54zS-f2SKtm5W5WK2df426XsKV7j3fd6CkxT7W7x6GCb20V3brW6qqHST3bthfYW6CpFKd7_yQ0XW2ysWt869bCphW6r8YtG4GrsVkW46V-MQ5bp2VwW5m6Bnn8b0H0_VsQ6Xw673G4GW2FfPnr6RDKb7W7dQjKN7Mqbk0W2D2_x791FrC9W84mN5P1JPzsPW5Ymmh58m7YHdW2RdHWF257Zzhf4hJ8JP04"><strong><em>Get started here</em></strong></a><strong><em>.</em></strong></p>
	<p>
				<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
			</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Codeberg Reaches 300k Projects (197 pts)]]></title>
            <link>https://codeberg.org/</link>
            <guid>45439955</guid>
            <pubDate>Wed, 01 Oct 2025 16:48:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codeberg.org/">https://codeberg.org/</a>, See on <a href="https://news.ycombinator.com/item?id=45439955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<header>
		
	</header>

	<section id="home-section-about">
		<div>
				<div>
					<h3>NON-PROFIT</h3>
					<p>
						Codeberg is maintained by the non-profit organization <b>Codeberg e.V.</b>,
						based in <b>Berlin, Germany</b>. For us, supporting the commons comes <b>first</b>.
					</p>
					<p>
						<b>
							 Its future is in the hands of its users. You can help too!
						</b>
					</p>
				</div>
				<div>
					<h3>COMMUNITY</h3>
					<p>
						We are more than just Git hosting: Our community is comprised of like-minded
						developers, artists, academics, hobbyists and professionals.
					</p>
					<p>
						<b>
							We celebrate free culture, openness and creativity.
						</b>
					</p>
				</div>
				<div>
					<h3>RESPECT</h3>
					<p>
						No tracking. No third-party cookies. No profiteering.
						Everything runs on servers that we control. Your data is <b>not</b> for sale.
					</p>
					<p>
						<b>
							Hosted in Europe, we welcome the world.
						</b>
					</p>
				</div>
				
					
				
			</div>

		<div>
				<h4>POWERED BY</h4>
				
			</div>
	</section>

	<section id="home-section-support">
		<h3>Your support helps us grow!</h3>
		<div>
			<a href="https://join.codeberg.org/">
				
				Become a member
			</a>
			<p>
				Our non-profit structure reinforces our independence. Your donations and contributions sustain our community.
				Help us achieve our mission by joining Codeberg e.V. as a supporting or active member with full voting rights!
			</p>
		</div>
		<hr>
		<div>
			<a href="https://docs.codeberg.org/improving-codeberg/#donate-to-codeberg">
				
				Fund our project
			</a>
			<p>
				Free as in freedom, not as in beer! Maintaining our systems and developing our software has its costs, which
				are backed by optional donations. We appreciate them a lot, as they help provide a better service for everyone.
			</p>
		</div>
		<hr>
		<div>
			<a href="https://docs.codeberg.org/improving-codeberg/#contribute-to-codeberg">
				
				Develop Codeberg
			</a>
			<p>
				Powered by Free Software! Get involved with Codeberg and help improve your experience. We
				are always looking for contributions to our projects and services.
			</p>
		</div>
	</section>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DuckDuckGo Donates $25,000 to The Perl and Raku Foundation v2025 (145 pts)]]></title>
            <link>https://www.perl.com/article/duckduckgo-donates-25-000-to-the-perl-and-raku-foundation-v2025/</link>
            <guid>45439883</guid>
            <pubDate>Wed, 01 Oct 2025 16:42:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.perl.com/article/duckduckgo-donates-25-000-to-the-perl-and-raku-foundation-v2025/">https://www.perl.com/article/duckduckgo-donates-25-000-to-the-perl-and-raku-foundation-v2025/</a>, See on <a href="https://news.ycombinator.com/item?id=45439883">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
              
              <p>Oct 1, 2025 by
              
              
                
                
                <a href="#author-bio-olaf-alders">Olaf Alders</a>
              
              </p>
               <img alt="" src="https://www.perl.com/images/duck-duck-go/DuckDuckGo-Logo-1.png">
                <p>For the second consecutive year, The Perl and Raku Foundation (TPRF) is
overjoyed to announce <a href="https://spreadprivacy.com/2025-duckduckgo-charitable-donations/">a donation of USD 25,000 from
DuckDuckGo</a>.</p>
<blockquote>
<p>DuckDuckGo has demonstrated how Perl and its ecosystem can deliver power and
scale to drive the DuckDuckGo core systems, plug-in framework and Instant
Answers. The Foundation is grateful that DuckDuckGo recognises the importance
of Perl, and for their generous funding support for a second year through
their charitable donations programme.</p></blockquote>
<p>– Stuart J Mackintosh, President of The Perl and Raku Foundation</p>
<p><a href="https://www.perl.com/article/duckduckgo-donates-25-000-to-the-perl-and-raku-foundation/">Last year’s donation of USD 25,000 from
DuckDuckGo</a>
was instrumental in helping to fund the foundation’s Core Perl Maintenance Fund
and this year’s donation will help to fund more of the same crucial work that
keeps the Perl language moving forward.</p>
<p><img src="https://www.perl.com/images/duck-duck-go/fireworks.jpg" alt="Fireworks celebration"></p>
<p><a href="https://metacpan.org/author/PEVANS">Paul “LeoNerd” Evans</a> is one of the
developers who gets regular funding from the Perl Core Maintenance Fund. Here
is a short list of just some of the many contributions which Paul has made to
core Perl as part of the maintenance fund work:</p>
<hr>
<ul>
<li>The <a href="https://perldoc.perl.org/builtin">builtin</a> module (5.36), making
available many new useful language-level utilities that were previously
loaded from modules like <a href="https://metacpan.org/pod/Scalar::Util">Scalar::Util</a></li>
<li>The complete <a href="https://perldoc.perl.org/feature#The-'class'-feature">feature
‘class’</a> system
(5.38), adding proper object-orientation syntax and abilities</li>
<li>Lexical method support (5.42), adding <code>my method</code> and
the <code>$obj-&gt;&amp;method</code> invocation syntax for better object encapsulation</li>
<li>Stabilising some of the recent experiments - signatures (5.36),
try/catch (5.40), foreach on multiple vars (5.40)</li>
<li>Ability to use the //= and ||= operators in signatures (5.38),
performance improvements and named parameters (upcoming in next
release)</li>
<li>The new <a href="https://perldoc.perl.org/functions/any">any</a> and
<a href="https://perldoc.perl.org/functions/all">all</a> keywords (5.42)</li>
</ul>
<hr>
<p>We look forward to many more innovative contributions from Paul over the coming
year.</p>
<p>While TPRF never takes continued support for granted, when it does arrive, it
allows the foundation to plan for the future with much greater confidence.
Multi-year partnerships with our sponsors allow us to continue to prioritize
important work, knowing that we will have the runway that we need to fund the
work which helps to sustain the Perl Language and its associated communities.</p>
<p>For more information on how to become a sponsor, please contact:
<a href="mailto:olaf@perlfoundation.org">olaf@perlfoundation.org</a></p>
<hr>
<p><em>"<a href="https://www.flickr.com/photos/67458569@N00/7722577066">Fireworks</a>" by <a href="https://www.flickr.com/photos/67458569@N00">colink.</a> is licensed under <a href="https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse">CC BY-SA 2.0</a>.</em></p>

              </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No more "check mail from other accounts" in Gmail web (191 pts)]]></title>
            <link>https://support.google.com/mail/answer/16604719?hl=en</link>
            <guid>45439670</guid>
            <pubDate>Wed, 01 Oct 2025 16:25:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://support.google.com/mail/answer/16604719?hl=en">https://support.google.com/mail/answer/16604719?hl=en</a>, See on <a href="https://news.ycombinator.com/item?id=45439670">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="hcfe-content" role="main">                   <article class="page" sc-render-smart-button="false" itemscope=""> <div data-stats-ve="35"><p>Starting January 2026, Gmail will no longer provide support for the following:</p>

<ul>
  <li><strong>Gmailify:</strong> This feature allows you to get special features like spam protection or inbox organization applied to your third-party email account. <a href="https://support.google.com/mail/answer/6304825" rel="noopener">Learn more about Gmailify</a>.</li>
  <li><strong>POP:</strong> This feature allows you to read your messages from a third-party account in Gmail. Unlike IMAP connections, POP only works with a single device and doesn’t sync your email in real time. Instead, emails are downloaded, and you decide how often you want to download new emails. As an alternative, you can still link your third-party accounts in the Gmail app.</li>
</ul>

<p>These changes help provide the most secure and current options to access your messages in Gmail.</p>

<h2>Learn about changes to Gmailify</h2>

<p>You won’t be able to get specific features in Gmail applied to your third-party account, like:</p>

<ul>
  <li><a href="https://safety.google/intl/en_us/gmail/" rel="noopener" target="_blank">Spam protection</a></li>
  <li>Better email notifications on mobile</li>
  <li><a href="https://support.google.com/mail/answer/3094499" rel="noopener">Inbox categories</a></li>
  <li>Faster search with <a href="https://support.google.com/mail/answer/7190" rel="noopener">advanced search operators</a></li>
</ul>

<h3>What you need to do</h3>

<ul>
  <li>You can still read and send emails from your other account within the Gmail app. This uses a standard IMAP connection, which is supported in the Gmail mobile app.</li>
  <li><a href="https://support.google.com/mail/answer/6078445" rel="noopener">Learn how to add another email account to the Gmail app</a>.</li>
</ul>

<h2>Learn about changes to POP connections</h2>

<ul>
  <li>Gmail will no longer support checking emails from third-party accounts through POP.</li>
  <li>The option to "Check mail from other accounts" will no longer be available in Gmail on your computer.</li>
</ul>

<h3>What you need to do</h3>

<p><strong>Important:</strong> If you have a work or school account, your administrator can help migrate your email data into Google Workspace. <a href="https://support.google.com/a/topic/14012345" rel="noopener">Learn more about the data migration service</a>.</p>

<ul>
  <li>To continue to receive messages from your other account in Gmail, you need to set up IMAP access.
    <ul>
      <li>Check your email provider’s documentation for instructions on how to enable IMAP for your account.</li>
    </ul>
  </li>
  <li>To read your messages from your other account, use the Gmail app. <a href="https://support.google.com/mail/answer/6078445" rel="noopener">Learn how to add another email account to the Gmail app</a>.</li>
</ul>

<h2>Frequently asked questions</h2>
<p><a>Will I lose the emails I already imported?</a></p><p>No. All messages synced before the deprecation stay in Gmail.</p>
<p><a>Can I still use other email accounts in the Gmail app?</a></p><p>Yes. For third-party accounts like Yahoo! and Outlook, you can add them to the Gmail mobile app on Android and iPhone and iPad.</p>

<h2>Related resources</h2>

<ul>
  <li><a href="https://support.google.com/mail/answer/6304825" rel="noopener">Get Gmail features for your other email accounts</a></li>
  <li><a href="https://support.google.com/mail/answer/21289" rel="noopener">Add another email account on your computer</a></li>
</ul>
</div>      </article>            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Who is hiring? (October 2025) (164 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=45438503</link>
            <guid>45438503</guid>
            <pubDate>Wed, 01 Oct 2025 15:01:06 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=45438503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bigbox"><td><table><tbody><tr id="45438503"><td><span></span></td><td><center><a id="up_45438503" href="https://news.ycombinator.com/vote?id=45438503&amp;how=up&amp;goto=item%3Fid%3D45438503"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=45438503">Ask HN: Who is hiring? (October 2025)</a></span></td></tr><tr><td colspan="2"></td><td><span><span id="score_45438503">111 points</span> by <a href="https://news.ycombinator.com/user?id=whoishiring">whoishiring</a> <span title="2025-10-01T15:01:06 1759330866"><a href="https://news.ycombinator.com/item?id=45438503">4 hours ago</a></span> <span id="unv_45438503"></span> | <a href="https://news.ycombinator.com/hide?id=45438503&amp;goto=item%3Fid%3D45438503">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Who%20is%20hiring%3F%20%28October%202025%29&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=45438503&amp;auth=e62bfd643e03dbfc11da0f07461353f48e2b6638">favorite</a> | <a href="https://news.ycombinator.com/item?id=45438503">162&nbsp;comments</a></span></td></tr><tr><td colspan="2"></td><td><div><p>Please state the location and include REMOTE for remote work, REMOTE (US)
or similar if the country is restricted, and ONSITE when remote work is <i>not</i> an option.</p><p>Please only post if you personally are part of the hiring company—no
recruiting firms or job boards. One post per company. If it isn't a household name,
explain what your company does.</p><p>Please only post if you are actively filling a position and are committed
to responding to applicants.</p><p>Commenters: please don't reply to job posts to complain about
something. It's off topic here.</p><p>Readers: please only email if you are personally interested in the job.</p><p>Searchers: try <a href="https://dheerajck.github.io/hnwhoishiring/" rel="nofollow">https://dheerajck.github.io/hnwhoishiring/</a>,
<a href="https://amber-williams.github.io/hackernews-whos-hiring/" rel="nofollow">https://amber-williams.github.io/hackernews-whos-hiring/</a>,
<a href="http://nchelluri.github.io/hnjobs/" rel="nofollow">http://nchelluri.github.io/hnjobs/</a>, <a href="https://hnresumetojobs.com/" rel="nofollow">https://hnresumetojobs.com</a>,
<a href="https://hnhired.fly.dev/" rel="nofollow">https://hnhired.fly.dev</a>, <a href="https://kennytilton.github.io/whoishiring/" rel="nofollow">https://kennytilton.github.io/whoishiring/</a>,
<a href="https://hnjobs.emilburzo.com/" rel="nofollow">https://hnjobs.emilburzo.com</a>, or this (unofficial) Chrome extension:
<a href="https://chromewebstore.google.com/detail/hn-hiring-pro/mpfaljjblphnlloddaplgicpkinikjlp" rel="nofollow">https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal...</a>.</p><p>Don't miss these other fine threads:</p><p><i>Who wants to be hired?</i> <a href="https://news.ycombinator.com/item?id=45438501">https://news.ycombinator.com/item?id=45438501</a></p><p><i>Freelancer? Seeking freelancer?</i> <a href="https://news.ycombinator.com/item?id=45438502">https://news.ycombinator.com/item?id=45438502</a></p></div></td></tr><tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr></tbody></table><br>
</td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building the heap: racking 30 petabytes of hard drives for pretraining (291 pts)]]></title>
            <link>https://si.inc/posts/the-heap/</link>
            <guid>45438496</guid>
            <pubDate>Wed, 01 Oct 2025 15:00:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://si.inc/posts/the-heap/">https://si.inc/posts/the-heap/</a>, See on <a href="https://news.ycombinator.com/item?id=45438496">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>We built a storage cluster in downtown SF to store 90 million hours worth of video data. Why? We’re pretraining models to solve computer use. Compared to text LLMs like LLaMa-405B, which require ~60 TB of text data to train, videos are sufficiently large that we need 500 times more storage. Instead of paying the $12 million / yr it would cost to store all of this on AWS, we rented space from a colocation center in San Francisco to bring that cost down ~40x to $354k per year, including depreciation.</p>
<h2 id="why">Why</h2>
<p>Our use case for data is unique. Most cloud providers care highly about redundancy, availability, and data integrity, which tends to be unnecessary for ML training data. Since pretraining data is a commodity—we can lose any individual 5% with minimal impact—we can handle relatively large amounts of data corruption compared to enterprises who need guarantees that their user data isn’t going anywhere. In other words, we don’t need AWS’s 13 nines of reliability; 2 is more than enough.</p>
<p>Additionally, storage tends to be priced substantially above cost. Most companies use relatively small amounts of storage (even ones like Discord still use <a href="https://discord.com/blog/how-discord-stores-trillions-of-messages">under a petabyte</a> for messages), and the companies that use petabytes are so large that storage remains a tiny fraction of their total compute spend.</p>
<p>Data is one of our biggest contraints, and would be prohibitively expensive otherwise. As long as the cost predictions work out in favor of a local datacenter, and it would not consume too much of the core team’s time, it would make sense to stack hard drives ourselves. 

<span data-note-content="We talked to some engineers at the Internet Archive, which had basically the same problem as us; even after massive friends &amp; family discounts on AWS, it was still 10 times more cost-effective to buy racks and store the data themselves\! " data-number="1">
<sup data-sidenote="sidenote-0">[1]</sup>
<span id="sidenote-0">1. We talked to some engineers at the Internet Archive, which had basically the same problem as us; even after massive friends &amp; family discounts on AWS, it was still 10 times more cost-effective to buy racks and store the data themselves!</span>
</span></p>
<h2 id="the-cost-breakdown-cloud-alternatives-vs-in-house">The Cost Breakdown: Cloud Alternatives vs In-House</h2>
<p>Internet and electricity total $17.5k as our only recurring expenses (the price of colocation space, cooling, etc were bundled into electricity costs). One-time costs were dominated by hard drive capex. 

<span data-note-content=" When deciding the datacenter location we had multiple options across the Bay Area, including options in Fremont through Hurricane Electric for around $10k in setup fees and $12.8k per month, saving us $38.5k initially and $4.7k per month, but ended up opting for a datacenter that was only a couple blocks from our office in SF. Though this came at a premium, it was extremely helpful to get the initial nodes setup and for ongoing maintenance. Our team is just 5 people, so any friction in going to the datacenter would come at a noticeable cost to team productivity." data-number="2">
<sup data-sidenote="sidenote-1">[2]</sup>
<span id="sidenote-1">2. When deciding the datacenter location we had multiple options across the Bay Area, including options in Fremont through Hurricane Electric for around $10k in setup fees and $12.8k per month, saving us $38.5k initially and $4.7k per month, but ended up opting for a datacenter that was only a couple blocks from our office in SF. Though this came at a premium, it was extremely helpful to get the initial nodes setup and for ongoing maintenance. Our team is just 5 people, so any friction in going to the datacenter would come at a noticeable cost to team productivity.</span>
</span></p>
<p><img src="https://si.inc/the-heap/datacenter_cost_comparison.png" alt="cost comparison">
<em>Table 1: Cost comparison of cloud alternatives vs in-house. AWS is $1,130,000/month including estimated egress, Cloudflare is $270,000/month (with bulk-discounted pricing), and our datacenter is $29,500/month (including recurring costs and depreciation).</em></p>
<h3 id="monthly-recurring-costs">Monthly Recurring Costs</h3>
<table>
<thead>
<tr>
<th>Item</th>
<th>Cost</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Internet</td>
<td>$7,500/month</td>
<td>100Gbps DIA from Zayo, 1yr term.</td>
</tr>
<tr>
<td>Electricity</td>
<td>$10,000/month</td>
<td>1 kW/PB, $330/kW. Includes cabinet space &amp; cooling. 1yr term.</td>
</tr>
<tr>
<td><strong>Total Monthly</strong></td>
<td><strong>$17,500/month</strong></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="one-time-costs">One-Time Costs</h3>
<table>
<thead>
<tr>
<th>Category</th>
<th>Item</th>
<th>Cost</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td>Storage</td>
<td>Hard drives (HDDs)</td>
<td>$300,000</td>
<td>2,400 drives. Mostly 12TB used enterprise drives (3/4 SATA, 1/4 SAS). The JBOD DS4246s work for either.</td>
</tr>
<tr>
<td>Storage Infrastructure</td>
<td>NetApp DS4246 chassis</td>
<td>$35,000</td>
<td>100 dual SATA/SAS chassis, 4U each</td>
</tr>
<tr>
<td>Compute</td>
<td>CPU head nodes</td>
<td>$6,000</td>
<td>10 Intel RR2000s from eBay</td>
</tr>
<tr>
<td>Datacenter Setup</td>
<td>Install fee</td>
<td>$38,500</td>
<td>One-off datacenter install fee</td>
</tr>
<tr>
<td>Labor</td>
<td>Contractors</td>
<td>$27,000</td>
<td>Contractors to help physically screw in / install racks and wire cables</td>
</tr>
<tr>
<td>Networking &amp; Misc</td>
<td>Install expenses</td>
<td>$20,000</td>
<td>Power cables, 100GbE QSFP CX4 NICs, Arista router, copper jumpers, one-time internet install fee</td>
</tr>
<tr>
<td><strong>Total One-Time</strong></td>
<td></td>
<td><strong>$426,500</strong></td>
<td></td>
</tr>
</tbody>
</table>
<p>Our price assuming three-year depreciation (including for the one-off install fees) is $17.5k/month in fixed monthly costs (internet, power, etc.) and $12k/month in depreciation, for $29.5k/month overall.</p>
<p>We compare our costs to two main providers: AWS’s public pricing numbers as a baseline, and Cloudflare’s discounted pricing for 30PB of storage. It’s important to note that AWS egress would be substantially lower if we utilized AWS GPUs. This is not reflected on our graph because AWS GPUs are priced at substantially above market prices and large clusters are difficult to attain, untenable at our compute scales.</p>
<p>Here are the pricing breakdowns:</p>
<h3 id="aws-pricing-breakdown">AWS Pricing Breakdown</h3>
<table>
<thead>
<tr>
<th>Cost Component</th>
<th>Rate</th>
<th>Monthly Cost</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Storage</td>
<td>$0.021/GB/month</td>
<td>$630,000</td>
<td>For data over 500TB</td>
</tr>
<tr>
<td>Egress</td>
<td>$0.05/GB</td>
<td>$500,000</td>
<td>Entire dataset egressed quarterly (10 PB/month)</td>
</tr>
<tr>
<td><strong>Total AWS Monthly</strong></td>
<td></td>
<td><strong>$1,130,000</strong></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="cloudflare-r2-pricing">Cloudflare R2 Pricing</h3>
<table>
<thead>
<tr>
<th>Pricing Tier</th>
<th>Rate</th>
<th>Monthly Cost</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Published Rate</td>
<td>$0.015/GB/month</td>
<td>$450,000</td>
<td>No egress fees</td>
</tr>
<tr>
<td>Estimated Private Pricing 

<span data-note-content="
Cloudflare has a more reasonable estimate for the 30 PB, placing it at an overall monthly cost of $270k without egress fees. We also have bulk-discounted pricing estimates after getting pricing quotes—this was our main point of comparison for the datacenter.
" data-number="3">
<sup data-sidenote="sidenote-2">[3]</sup>
<span id="sidenote-2">3. Cloudflare has a more reasonable estimate for the 30 PB, placing it at an overall monthly cost of $270k without egress fees. We also have bulk-discounted pricing estimates after getting pricing quotes—this was our main point of comparison for the datacenter.</span>
</span></td>
<td>$0.009/GB/month</td>
<td>$270,000</td>
<td>Estimated rate for &gt;20 PB scale</td>
</tr>
</tbody>
</table>
<p>That brings monthly costs to $38/TB/month for AWS, $10/TB/month for Cloudflare, and $1/TB/month for our datacenter—about 38x lower and 10x lower respectively. (At the very cheapest end of the spectrum, Backblaze has a $6/TB product that is unsuitable for model training due to egress speed limitations; their $15/TB Overdrive AI-specific storage product is closer to Cloudflare’s in price &amp; performance)</p>
<p>While we use Cloudflare as a comparison point, we’ve sometimes done too much load for their R2 servers. In particular, in the past we’ve done enough load during large model training runs that they rate-limited us, later confirming we were saturating their metadata layer and the rate limit wasn’t synthetic. Because our metadata on the heap is so simple, and we have a 100Gbps DIA connection, we haven’t ran into any issues there. 

<span data-note-content=" We love Cloudflare and use many of their products often; we include this anecdote as a fact about our scale being difficult to handle, not as a dig! " data-number="4">
<sup data-sidenote="sidenote-3">[4]</sup>
<span id="sidenote-3">4. We love Cloudflare and use many of their products often; we include this anecdote as a fact about our scale being difficult to handle, not as a dig!</span>
</span></p>
<p>This setup was and is necessary for our video data pipelines, and we’re extremely happy that we made this investment. By gathering large scale data at low costs, we can be competitive with frontier labs with billions of dollars in capital.</p>
<h2 id="setupthe-process">Setup/The Process</h2>
<p>We cared a lot about getting this built <em>fast</em>, because this kind of project can easily stretch on for months if not careful. Hence Storage Stacking Saturday, or S3. We threw a hard drive stacking party in downtown SF and got our friends to come, offering food and custom-engraved hard drives to all who helped. The hard drive stacking started at 6am and continued for 36 hours (with a break to sleep), and by the end of that time we had 30 PB of functioning hardware racked and wired up. We brought in contractors for additional help and professional installation later on in the event.</p>
<p><img src="https://si.inc/the-heap/server-people.png" alt="hard drive stacking party">
<em>People at the hard drive stacking party!</em>
<img src="https://si.inc/the-heap/server.png" alt="hard drive stacking party">
<em>Cool shots of the servers</em></p>
<p>Our software is 200 lines of Rust code for writing (to determine the drive to write data onto) and a nginx webserver for reading data, with a simple SQLite db for tracking metadata like which heap node each file is on and what data split it belongs to. We kept this obsessively simple instead of using MinIO or Ceph because we didn’t need <em>any</em> of the features they provided; it’s much, much simpler to debug a 200-line program than to debug Ceph, and we weren’t worried about redundancy or sharding. All our drives were formatted with XFS.</p>
<p>The storage software landscape offers many options, but every option available comes with drawbacks. People experienced with Ceph strongly warned us to avoid it unless we were willing to hire dedicated Ceph specialists—our research confirmed this advice. Ceph appears far more complex than justified for most use cases, only worthwhile for companies that absolutely need maximum performance and customizability and are prepared to invest heavily in tuning. Minio presents an interesting option if S3 compatibility is essential, but otherwise remains a bit too fancy for us and similar use-cases. Weka and Vast are absurdly expensive at 2k / TB / year or so and are primarily designed for NVMEs, not spinning disks.</p>
<h2 id="post-mortem">Post-Mortem</h2>
<p>Building the datacenter was a large endeavor and we definitely learned lessons, both good and bad.</p>
<h3 id="things-that-we-got-correct">Things That We Got Correct</h3>
<ul>
<li>We think the redundancy &amp; capability tradeoffs we made are very reasonable at our disk speeds. We’re able to approximately saturate our 100G network for both read &amp; write.</li>
<li>Doing this locally a couple blocks away was well worth it because of the amount of debugging and manual work needed.</li>
<li>Ebay is good to find vendors but bad to actually buy things with. After finding vendors, they can often individually supply all the parts we need and provide warranties, which are extremely valuable.</li>
<li>100G dedicated internet is pretty important, and much much easier to debug issues with than using cloud products.</li>
<li>Having high-quality cable management during the racking process saved us a ton of time debugging in the long run; making it easy to switch up the networking saved us a lot of headache.</li>
<li>We had a very strong simplicity prior, and this saved an immense amount of effort. We are quite happy that we didn’t use ceph or minio. Unlike e.g. nginx, they do not work out of the box. We were willing to write a simple Rust script and roughly saturated our network read &amp; write at 100 Gbps without any fancy code.</li>
<li>We were basically right about the price and advantages this offered, and did not substantially overestimate the amount of time / effort it would take. While the improvements list is longer than this, <em>most of those are minor; fundamentally we built a cluster rivaling massive clouds for 40x cheaper.</em></li>
</ul>
<h3 id="difficult-bits">Difficult Bits</h3>
<p>A map of reality only gets you so far—while setting up the datacenter we ran into a couple problems and unexpected challenges. We’ll include a list:</p>
<ul>
<li>We used frontloaders instead of toploaders for our server rack. This meant we had to screw every single individual drive in—tedious for 2.4k HDDs</li>
<li>Our storage was not dense—we could have saved 5x the work on physical placement and screwing by having a denser array of hard drives</li>
<li>Shortcuts like daisy-chaining are usually a bad idea. We could have gotten substantially higher read/write speeds without daisy chaining networked nodes, giving each chassis its own HBA (Host Bus Adapter, not a significant cost).</li>
<li>Compatibility is key—specifically in networking functionally everything is locked to a specific brand. We had many pain points here. Fiber transceivers will ~never work unless used with the right brand, but copper cables are much more forgiving. <a href="http://fs.com/">FS.com</a> is pretty good and well priced (though their speed estimates were pretty inconsistent); Amazon will also often have the parts you need rapidly.</li>
<li>Networking was a substantial cost and required experimentation. We did not use DHCP as most enterprise switches don’t support it and we wanted public IPs for the nodes for convenient and performant access from our servers. While this is an area where we would have saved time with a cloud solution, we had our networking up within days and kinks ironed out within ~3 weeks.</li>
<li>We were often bottlenecked by easy access to servers via monitor/keyboard; idle crash carts during setup are helpful.</li>
</ul>
<h3 id="ideas-worth-trying">Ideas Worth Trying</h3>
<ul>
<li>Working KVMs are extremely useful, and you shouldn’t go without them or good IPMI. Physically going to a datacenter is really inconvenient, even if it’s a block away. IPMI is good, but only if you have pretty consistent machines.</li>
<li>Think through your management Ethernet network as much as your real network - it’s really nice to be able to SSH into servers while configuring the network, and IPMI is great!</li>
<li>Overprovision your network—e.g. if doable it’s worth having 400 Gigabit internally (you can use 100G cards etc for this!)</li>
<li>We could have substantially increased density at additional upfront cost by buying 90-drive SuperMicro SuperServers and putting 20TB drives into them. This would allow us to use 2 racks instead of 10, given us had about the equivalent of 20 AMD 9654s in total CPU capacity, and used less total power.</li>
</ul>
<h2 id="how-you-can-build-this-yourself">How You Can Build This Yourself</h2>
<p>Here’s what you need to replicate our setup.</p>
<h3 id="storage">Storage</h3>
<ul>
<li>
<p>10 CPU head nodes.</p>
<ul>
<li>We used Intel Rr2000 with Dual Intel Gold 6148 and 128GB of DDR4 ECC RAM per server (which are incredibly cheap and roughly worked for our use cases) but you have a lot of flexibility in what you use.</li>
<li>If you use the above configuration you likely won’t be able to do anything at all CPU-intensive on the servers (like on-device data processing or ZFS data compression / deduplication / etc, which is valuable if you’re storing non-video data).</li>
<li>Our CPU nodes cost $600 each—it seems quite reasonable to us to spend up to $3k each if you want ZFS / compression or the abiliy to do data processing on-CPU.</li>
</ul>
</li>
<li>
<p>100 DS4246 chassis—each can hold 24 hard drives.</p>
</li>
<li>
<p>2,400 3.5 inch HDDs—need to be all SATA or all SAS in each chassis.</p>
<ul>
<li><em>We would recommend SAS hard drives if possible</em>

<span data-note-content="if you use SAS drives you’ll need to deal with or disable mulipathing, which is reasonably simple" data-number="5">
<sup data-sidenote="sidenote-4">[5]</sup>
<span id="sidenote-4">5. if you use SAS drives you’ll need to deal with or disable mulipathing, which is reasonably simple</span>
</span> as they roughly double speed over similar SATA drives.</li>
<li>We used a mix of 12TB and 14TB drives—basically any size should work, roughly the larger the better holding price constant (density makes stacking easier + in general increases resale value).</li>
</ul>
</li>
<li>
<p>Physical parts to mount the chassis—you’ll need rails or l-brackets. We used l-brackets which worked well, as we haven’t needed to take the chassis out to slot hard drives. If you buy toploaders, you’ll need rails.</p>
</li>
<li>
<p>Multiple “crash carts” with monitors and keyboards that allow you to physically connect to your CPU head nodes and configure them—this is invaluable when you’re debugging network issues.</p>
</li>
</ul>
<h3 id="network">Network</h3>
<ul>
<li>
<p>A 100 GbE switch</p>
<ul>
<li>a used Arista is fine, should be QSFP28, should cost about $1-2k</li>
</ul>
</li>
<li>
<p>HBAs (Host Bus Adapters), which connect your head nodes to your DS4246 chassis.</p>
<ul>
<li>The best configuration we tried was with Broadcom 9305-16E HBAs, with 3x HBAs per server (make sure your server has physical space for them!) with SFF-8644 to QSFP mini SAS cables.</li>
<li>There are 4 slots per HBA, so you can cable each DS4246 chassis directly to the HBA.


<span data-note-content="The option we ended up going with for convenience was putting LSI SAS9207-8e HBAs, which have 2 ports each, into the CPU head nodes- then daisy-chaining the DS4246s together with QSFP+ to QSFP+ DACs.. We deployed this on Storage Stacking Saturday, then while debugging speeds tried the above method on one of the servers and got to \~4 Gbps per chassis-but didn’t find it worth it to swap everything out in pure labor because of the way we had set up some of our head nodes such that they were difficult to take out. Insofar as it is reasonably cheap to just do the above thing to start and we’ve tested it to work, you should probably do as we say, not as we did in this case\!" data-number="6">
<sup data-sidenote="sidenote-5">[6]</sup>
<span id="sidenote-5">6. The option we ended up going with for convenience was putting LSI SAS9207-8e HBAs, which have 2 ports each, into the CPU head nodes- then daisy-chaining the DS4246s together with QSFP+ to QSFP+ DACs.. We deployed this on Storage Stacking Saturday, then while debugging speeds tried the above method on one of the servers and got to ~4 Gbps per chassis-but didn’t find it worth it to swap everything out in pure labor because of the way we had set up some of our head nodes such that they were difficult to take out. Insofar as it is reasonably cheap to just do the above thing to start and we’ve tested it to work, you should probably do as we say, not as we did in this case!</span>
</span></li>
</ul>
</li>
<li>
<p>Network cards (NICs).</p>
<ul>
<li>We used Mellanox ConnectX-4 100GbE. Make sure they come in Ethernet mode and not Infiniband mode for ease of config.</li>
</ul>
</li>
<li>
<p>DAC (Direct Attach Copper) or AOC (Active Optical) cables, to connect the NICs in your head nodes to your switch and therefore the internet. You almost certainly want DACs if your racks are close together, as they are far more compatible with arbitrary networking equipment than AOCs.</p>
</li>
</ul>
<p>We would recommend that you find a supplier to sell you the CPU head nodes with the HBAs and NICs installed—there are a number of used datacenter / enterprise parts suppliers who are willing to do this. This is a substantial positive because it means that you don’t have to spend hours installing the HBAs/NICs yourself and can have a substantially higher degree of confidence in your operations.</p>
<ul>
<li>
<p>Serial cables—you’ll need these to connect to your switch!</p>
</li>
<li>
<p><strong>Optional but recommended:</strong> an Ethernet management network of some kind. If you can’t easily get ethernet, we’d recommend getting a wifi adapter <a href="https://www.amazon.com/BrosTrend-600Mbps-Adapter-Wireless-WNA016/dp/B0118SPFCK">like this</a> and then a ethernet switch <a href="https://www.amazon.com/Ethernet-Splitter-Optimization-Unmanaged-TL-SG105/dp/B00A128S24">like this</a> —it’s substantially easier to set up than the 100GbE, is a great backup for when that’s not working, and will allow you to do ~everything over SSH from the comfort of the office instead of in the datacenter.</p>
</li>
</ul>
<h3 id="datacenter-requirements">Datacenter Requirements</h3>
<ul>
<li>3.5 kW of usable power per cabinet, with 10 4U chassis + 1 2U (cabinets are 42U tall)</li>
<li>1 spare cabinet for the 1U or 2U 100GbE switch (you can obviously also just swap out one of the 4U chassis in another cabinet for the switch).</li>
<li>1 42U cabinet per 3 PB of storage</li>
<li>A dedicated 100G connection (will come in as a fiber pair probably via <a href="https://www.fs.com/products/104860.html">QSFP28 LR4</a>, but confirm with your datacenter provider before buying parts here!)</li>
<li>Ideally physically near your office—there is a lot of value in being able to walk over and debug issues instead of e.g. dealing with remote hands services to get internet to the nodes.</li>
</ul>
<p><strong>Some setup tips:</strong></p>
<ul>
<li>Make sure to first properly configure your switch. Depending on your switch model this should be relatively straightforward—you’ll need to physically connect to the switch and then configure the specific port that your 100GbE is connected to (you’ll get a fiber cross-connect from your datacenter that you should plug into a QSFP28 transceiver. <strong>Make sure that you get a transceiver that is compatible in form with the ISP, probably LR4, and specifically branded with your switch brand, otherwise it is very unlikely to work).</strong> Depending on your ISP you might have to talk to them to make sure that you can get “light” through the fiber cables from both ends, which might involve <a href="https://www.youtube.com/watch?v=OKtF97VT8ts">rolling the fiber</a> and otherwise making sure it’s working properly.
<ul>
<li>If your switch isn’t working / you haven’t configured one before, I’d suggest trying to directly plug the fiber cable from the ISP into one of your 10 heap servers, making sure to buy a transceiver that is compatible with your NIC brand (e.g. Mellanox). Once you get it working from there, move over to your switch and get it working.</li>
</ul>
</li>
<li>Once you can connect to the internet from your switch (simply ping 1.1.1.1 to check) you are ready to set up the netplans for the individual nodes. <strong>this is most easily done during the Ubuntu setup process, which will walk you through setting up internet for your CPU head nodes</strong>, but is also doable outside of that</li>
</ul>
<p>Once you have internet access to your nodes and have properly connected 1 cable to each DS4246, you should format &amp; mount the drives on each node, test that all of them are properly working, and then you are ready to deploy any software you want.</p>
<hr>
<p>If you end up building a similar storage cluster based on this writeup we’d love to hear from you—we’re very curious what can be improved, both in our guidance and in the object-level process. You can reach us at <a href="https://si.inc/cdn-cgi/l/email-protection#4d2e2223392c2e390d3e246324232e"><span data-cfemail="73101c1d0712100733001a5d1a1d10">[email&nbsp;protected]</span></a></p>
<p>If you came away from this post excited about our work, we’d love to chat. We’re a research lab currently focused on pretraining models to use computers, with the long-term goal of building general models that can learn in-context and do arbitrary tasks while aligned with human values; we’re hiring top researchers and engineers to help us train these. If you’re interested in chatting, shoot us an email at <a href="https://si.inc/cdn-cgi/l/email-protection#a2c8cdc0d1e2d1cb8ccbccc1"><span data-cfemail="63090c011023100a4d0a0d00">[email&nbsp;protected]</span></a>.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Autism Simulator (537 pts)]]></title>
            <link>https://autism-simulator.vercel.app/</link>
            <guid>45438346</guid>
            <pubDate>Wed, 01 Oct 2025 14:48:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://autism-simulator.vercel.app/">https://autism-simulator.vercel.app/</a>, See on <a href="https://news.ycombinator.com/item?id=45438346">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Unix philosophy and filesystem access makes Claude Code amazing (264 pts)]]></title>
            <link>https://www.alephic.com/writing/the-magic-of-claude-code</link>
            <guid>45437893</guid>
            <pubDate>Wed, 01 Oct 2025 14:05:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.alephic.com/writing/the-magic-of-claude-code">https://www.alephic.com/writing/the-magic-of-claude-code</a>, See on <a href="https://news.ycombinator.com/item?id=45437893">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>If you've talked to me lately about AI, you've almost certainly been subject to a long soliloquy about the wonders of Claude Code. What started as a tool I ran in parallel with other tools to aid coding has turned into my full-fledged agentic operating system, supporting all kinds of workflows.</p><div><p>Most notably, <a href="https://obsidian.md/"><span>Obsidian</span></a>, the tool I use for note-taking. The difference between Obsidian and Notion or Evernote is that all the files are just plain old Markdown files stored on your computer. You can sync, style, and save them, but ultimately, it's still a text file on your hard drive. A few months ago, I realized that this fact made my Obsidian notes and research a particularly interesting target for AI coding tools. What first started with trying to open my vault in <a href="https://cursor.com/"><span>Cursor</span></a> quickly moved to a sort of note-taking operating system that I grew so reliant on, I ended up standing up a server in my house so I could connect via SSH from my phone into my Claude Code + Obsidian setup and take notes, read notes, and think through things on the go.</p><p><img alt="CleanShot 2025-09-30 at 09.48.05@2x.png" height="2010" src="https://www.alephic.com/api/media/file/CleanShot%202025-09-30%20at%2009.48.05%402x.png" width="3248"></p><p>A few weeks ago, I went on <a href="https://every.to/podcast/how-to-use-claude-code-as-a-thinking-partner"><span>Dan Shipper's AI &amp; I Podcast</span></a> to wax poetic about my love for this setup. I did a pretty deep dive into the system I use, how it works, why it works, etc. I won't retread all those details—you can read the transcript or listen to the podcast—but I want to talk about a few other things related to Claude Code that I've come to realize since the conversation.</p><h2>Why is Claude Code special? What makes it better than Cursor?</h2><p>I've really struggled to answer this question. I'm also not sure it's better than Cursor for all things, but I do think there are a set of fairly exceptional pieces that work together in concert to make me turn to Claude Code whenever I need to build anything these days. Increasingly, that's not even about applying it to existing codebases as much as it's building entirely new things on top of its functionality (more on that in a bit).</p><p>So what's the secret? Part of it lies in how Claude Code approaches tools. As a terminal-based application, it trades accessibility for something powerful: native Unix command integration. While I typically avoid long blockquotes, the <a href="https://en.wikipedia.org/wiki/Unix_philosophy"><span>Unix Philosophy</span></a> deserves an exception—Doug McIlroy's original formulation captures it perfectly:</p><p>The Unix philosophy is documented by <a href="https://en.wikipedia.org/wiki/Doug_McIlroy"><span>Doug McIlroy</span></a> in the <a href="https://en.wikipedia.org/wiki/Bell_System_Technical_Journal"><span>Bell System Technical Journal</span></a> from 1978:</p><ol><li value="1"><ol><li value="1">Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new "features".</li><li value="2">Expect the output of every program to become the input to another, as yet unknown, program. Don't clutter output with extraneous information. Avoid stringently columnar or binary input formats. Don't insist on interactive input.</li><li value="3">Design and build software, even operating systems, to be tried early, ideally within weeks. Don't hesitate to throw away the clumsy parts and rebuild them.</li><li value="4">Use tools in preference to unskilled help to lighten a programming task, even if you have to detour to build the tools and expect to throw some of them out after you've finished using them.</li></ol></li></ol><p>It was later summarized by <a href="https://en.wikipedia.org/wiki/Peter_H._Salus"><span>Peter H. Salus</span></a> in A Quarter-Century of Unix (1994):</p><ul><li value="1"><ul><li value="1">Write programs that do one thing and do it well.</li><li value="2">Write programs to work together.</li><li value="3">Write programs to handle text streams, because that is a universal interface.</li></ul></li></ul><p>These fifty-year-old principles are exactly how LLMs want to use tools. If you look at how these models actually use the tools they're given, they are constantly "piping" output to input (albeit using their own fuzziness in between). (As an aside, the Unix | command allows you to string the output from one command into the input of another.) When models fail to weld their tools effectively, it is almost always because the tools are overly complex.</p><p><img alt="CleanShot 2025-09-30 at 09.49.30.gif" height="628" src="https://www.alephic.com/api/media/file/CleanShot%202025-09-30%20at%2009.49.30.gif" width="800"></p><p>So part one of why Claude Code can be so mind-blowing is that the commands that power Unix happen to be perfectly suited for use by LLMs. This is both because they're simple and also incredibly well-documented, meaning the models had ample source material to teach them the literal ins and outs.</p><p>But that still wasn't the whole thing. The other piece was obviously Claude Code's ability to write code initially and, more recently, prose (for me, at least). But while other applications like ChatGPT and Claude can write output, there was something different going on here. Last week, while reading <a href="https://newsletter.pragmaticengineer.com/p/how-claude-code-is-built"><span>The Pragmatic Engineer's deep dive into how Claude Code is built</span></a>. The answer was staring me in the face: filesystem access.&nbsp;</p><p>The filesystem changes everything. ChatGPT and Claude in the browser have two fatal flaws: no memory between conversations and a cramped context window. A filesystem solves both. Claude Code writes notes to itself, accumulates knowledge, and keeps running tallies. It has state and memory. It can think beyond a single conversation.</p><h2>AI Overhang</h2><p>Back in 2022, when I first played with the GPT-3 API, I said that even if models never got better than they were in that moment, we would still have a decade to discover the use cases. They did get better—reasoning models made tool calling reliable—but the filesystem discovery proves my point.</p><p>I bring this up because <a href="https://newsletter.pragmaticengineer.com/p/how-claude-code-is-built">in the Pragmatic Engineer interview</a>, Boris Cherney, who built the initial version of Claude Code, uses it to describe the aha:</p><p>In AI, we talk about “product overhang”, and this is what we discovered with the prototype.&nbsp;Product overhang means that a model is able to do a specific thing, but the product that the AI runs in isn’t built in a way that captures this capability. What I discovered about Claude exploring the filesystem was pure product overhang. The model could already do this, but there wasn’t a product built around this capability!</p><p>Again, I'd argue it's filesystem + Unix commands, but the point is that the capability was there in the model just waiting to be woken up, and once it was, we were off to the races. Claude Code works as a blueprint for building reliable agentic systems because it captures model capabilities instead of limiting them through over-engineered interfaces.</p><h2>Going Beyond Code</h2><p>I talked about my Claude Code + Obsidian setup, and I've actually taken it a step further by open-sourcing "<a href="https://github.com/heyitsnoah/claudesidian"><span>Claudesidian</span></a>," which pulls in a bunch of the tools and commands I use in my own Claude Code + Obsidian setup. It also goes beyond that and was a fun experimental ground for me. Most notably, I built an initial upgrade tool so that if changes are made centrally, you can pull them into your own Claudesidian, and the AI will help you check to see if you've made changes to the files being updated and, if so, attempt to smartly merge your changes with the new updates. Both projects follow the same Unix philosophy principles—simple, composable tools that do one thing well and work together. This is the kind of stuff that Claude Code makes possible, and why it's so exciting for me as a new way of building applications.</p><p>Speaking of which, one I'm not quite ready to release, but hopefully will be soon, is something I've been calling "Inbox Magic," though I'll surely come up with a better name. It's a Claude Code repo with access to a set of Gmail tools and a whole bunch of prompts and commands to effectively start operating like your own email EA. Right now, the functionality is fairly simple: it can obviously run searches or send emails on your behalf, but it can also do things like triage and actually run a whole training run on how you sound over email so it can more effectively draft emails for you. While Claude Code and ChatGPT both have access to my emails, they mostly grab one or two at a time. This system, because it can write things out to files and do lots of other fancy tricks, can perform a task like “find every single travel-related email in my inbox and use that to build a profile of my travel habits that I can use as a prompt to help ChatGPT/Claude do travel research that's actually aligned with my preferences.” Anyway, more on this soon, and if it's something you want to try out, ping me with your GitHub username, and as soon as I feel like I have something ready to test, I'll happily share it.</p><h2>A Few Takeaways</h2><p>While I generally shy away from conclusions, I think there are a few here worth reiterating.</p><ol><li value="1">The filesystem is a great tool to get around the lack of memory and state in LLMs and should be used more often.</li><li value="2">If you're trying to get tool calling working, focus on following the Unix philosophy.</li><li value="3">Claude Code represents a blueprint for future agentic systems—filesystem + Unix philosophy should be the template for building reliable, debuggable AI agents rather than complex multi-agent stuff that's floating around today. Tactically, this means when you’re building tool calling into your own projects, keeping them simple and letting the main model thread “pipe” them is the key. (As an aside, one big problem that needs to be solved in all these agents/chatbots is the ability to pipe things without it going through the context window.)</li><li value="4">Anyone who can't find use cases for LLMs isn't trying hard enough</li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cursor 1.7 (142 pts)]]></title>
            <link>https://cursor.com/changelog/1-7</link>
            <guid>45437735</guid>
            <pubDate>Wed, 01 Oct 2025 13:51:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cursor.com/changelog/1-7">https://cursor.com/changelog/1-7</a>, See on <a href="https://news.ycombinator.com/item?id=45437735">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3 id="autocomplete-for-agent"><a href="#autocomplete-for-agent">Autocomplete for Agent</a></h3><p>When writing prompts, autocomplete suggestions will appear based on recent changes. Tab to accept suggestions and attach files to context.</p><figure><video src="https://cdn.sanity.io/files/2hv88549/production/c59180ab6509a4c8cc6fe61d3cf7c36dd7ad4148.mp4" autoplay="" loop="" muted="" playsinline=""></video></figure><h3 id="hooks-beta"><a href="#hooks-beta">Hooks (beta)</a></h3><p>You can now observe, control, and extend the Agent loop using custom scripts. Hooks give you a way to customize and influence Agent behavior at runtime.</p><p>Use Hooks to audit Agent usage, block commands, or redact secrets from context. It's still in beta and we'd love to hear your feedback.</p><figure><video src="https://cdn.sanity.io/files/2hv88549/production/6bfb19e08fdc2162242aa94642df48cff1d7e680.mp4" autoplay="" loop="" muted="" playsinline=""></video></figure><h3 id="team-rules"><a href="#team-rules">Team rules</a></h3><p>Teams can now define and share global rules from the dashboard that will be applied to all projects. We’ve also shipped team rules for <a href="https://cursor.com/bugbot">Bugbot</a>, so behavior is consistent across repos.</p><figure></figure><p>Generate shareable deeplinks for reusable prompts. Useful for setup instructions in documentation, team resources, and sharing workflows. See our <a href="https://cursor.com/docs/integrations/deeplinks">documentation</a> for how to create them.</p><figure><video src="https://cdn.sanity.io/files/2hv88549/production/8df68e08d1ec72cb5c16b9e29580c1e8be1a10dd.mp4" autoplay="" loop="" muted="" playsinline=""></video></figure><h3 id="sandboxed-terminals"><a href="#sandboxed-terminals">Sandboxed terminals</a></h3><p>Commands now execute in a secure, sandboxed environment. If you’re on allowlist mode, non-allowlisted commands will automatically run in a sandbox with read/write access to your workspace and no internet access.</p><p>If a command fails and we detect the sandbox was the cause, you’ll be prompted to retry outside of the sandbox.</p><figure><video src="https://cdn.sanity.io/files/2hv88549/production/988f922156ea31ea5c4defb3f9d4a049e359b252.mp4" autoplay="" loop="" muted="" playsinline=""></video></figure><p>Quickly check the status of Cursor Agents right from your menubar.</p><figure><video src="https://cdn.sanity.io/files/2hv88549/production/eb50983a08b4878f422c8fb6166e04453dd5a58f.mp4" autoplay="" loop="" muted="" playsinline=""></video></figure><h3 id="image-file-support-for-agent"><a href="#image-file-support-for-agent">Image file support for Agent</a></h3><p>Agent can now read image files directly from your workspace and include them in context. Previously, only pasted images were supported.</p><figure><video src="https://cdn.sanity.io/files/2hv88549/production/0d33834548eb1bbe7aae74f2e749b7d2b9d6d45d.mp4" autoplay="" loop="" muted="" playsinline=""></video></figure></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: ChartDB Agent – Cursor for DB schema design (111 pts)]]></title>
            <link>https://app.chartdb.io/ai</link>
            <guid>45437594</guid>
            <pubDate>Wed, 01 Oct 2025 13:38:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://app.chartdb.io/ai">https://app.chartdb.io/ai</a>, See on <a href="https://news.ycombinator.com/item?id=45437594">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Minimal files and config for a PWA (134 pts)]]></title>
            <link>https://github.com/chr15m/minimal-pwa</link>
            <guid>45437326</guid>
            <pubDate>Wed, 01 Oct 2025 13:14:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/chr15m/minimal-pwa">https://github.com/chr15m/minimal-pwa</a>, See on <a href="https://news.ycombinator.com/item?id=45437326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      



    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>

      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p>

<react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  
  
</react-partial>





      

          

              




<header role="banner" data-is-top="true" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      

      <div>
        <div>
            <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>
          GitHub Copilot

        </p><p>

        Write better code with AI
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}" href="https://github.com/features/spark">
      
      <div>
        <p>
          GitHub Spark

            <span>
              New
            </span>
        </p><p>

        Build and deploy intelligent apps
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
        <p>
          GitHub Models

            <span>
              New
            </span>
        </p><p>

        Manage and compare prompts
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_platform_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
        <p>
          GitHub Advanced Security

        </p><p>

        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_platform_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>
          Actions

        </p><p>

        Automate any workflow
      </p></div>

    
</a></li>

                  </ul>
                </div>
            <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_platform_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>
          Codespaces

        </p><p>

        Instant dev environments
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_platform_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>
          Issues

        </p><p>

        Plan and track work
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_platform_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>
          Code Review

        </p><p>

        Manage code changes
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_platform_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>
          Discussions

        </p><p>

        Collaborate outside of code
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_platform_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>
          Code Search

        </p><p>

        Find more, search less
      </p></div>

    
</a></li>

                  </ul>
                </div>
            
        </div>

          <p>
            <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_features&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}" href="https://github.com/features">
              View all features
              
</a>          </p>
      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>

                      <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                      <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://github.com/resources/events">
      Events &amp; Webinars

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://github.com/partners">
      Partners

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                  </ul>
                </div>
</li>


                <li>
      

      <div>
                <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>
          GitHub Sponsors

        </p><p>

        Fund open source developers
      </p></div>

    
</a></li>

                  </ul>
                </div>
                <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>
          The ReadME Project

        </p><p>

        GitHub community articles
      </p></div>

    
</a></li>

                  </ul>
                </div>
                
            </div>
</li>


                <li>
      

      <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>
          Enterprise platform

        </p><p>

        AI-powered developer platform
      </p></div>

    
</a></li>

                  </ul>
                </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;platform&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;platform_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:chr15m/minimal-pwa" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="m1iT7lo9LlWcjZIU5mQgCKXl1-KrcgGuINf6XNpLMWXY0tKmv0w-I64Xn52PlAeH99xzGeJX3_8yqNKtDaAcpg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="chr15m/minimal-pwa" data-current-org="" data-current-owner="chr15m" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=chr15m%2Fminimal-pwa" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/chr15m/minimal-pwa&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="d26cbfff95843f3710249c937aee2c8e38ec6ac3f667c6f5be2db9c37918a046" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-f197728f-3b54-4d92-82b1-03a5c5de041b" for="icon-button-ed09aa16-7e48-41bb-9455-f3380047c5bd" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.6c63a6de228d6520804d.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div>
</header>

      
    </div>

  








    


    






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-project-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  





    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div data-view-component="true" id="repo-content-pjax-container">      

<react-partial partial-name="repos-overview" data-ssr="true" data-attempted-ssr="true" data-react-profiling="false">
  
  
  <div data-target="react-partial.reactRoot"><div itemscope="" itemtype="https://schema.org/abstract"><h2>Repository files navigation</h2><nav aria-label="Repository files" data-variant="inset"><ul role="list"><li><a href="#" aria-current="page"><span data-component="icon"></span><span data-component="text" data-content="README">README</span></a></li></ul></nav></div><div data-hpc="true"><article itemprop="text"><p dir="auto">This is the minimal set of files for a "progressive web app" to be installable on Android and iOS.</p>
<p dir="auto">It contains the smallest possible <code>manifest.json</code> and service worker to trigger the install flow on Chrome.</p>
<p dir="auto">An even smaller implementation that fits in a single HTML file is in <a href="https://github.com/chr15m/minimal-pwa/blob/main/single-file-pwa.html">single-file-pwa.html</a>. It has a manifest.json that is dynamically generated from JavaScript, and it is installable without a service worker.</p>
</article></div></div>
</react-partial>


      </div>

</turbo-frame>


    </main>
  </div>

          



    <ghcc-consent id="ghcc" data-locale="en" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>




  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Detect Electron apps on Mac that hasn't been updated to fix the system wide lag (153 pts)]]></title>
            <link>https://gist.github.com/tkafka/e3eb63a5ec448e9be6701bfd1f1b1e58</link>
            <guid>45437112</guid>
            <pubDate>Wed, 01 Oct 2025 12:54:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/tkafka/e3eb63a5ec448e9be6701bfd1f1b1e58">https://gist.github.com/tkafka/e3eb63a5ec448e9be6701bfd1f1b1e58</a>, See on <a href="https://news.ycombinator.com/item?id=45437112">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
    Detect Electron apps on mac where the Electron hasn't yet been updated to fix the system wide lag
  </p><div id="file-readme-md" tabindex="0" role="region" aria-label="README.md content, created by tkafka on 12:53PM today.">
    <article itemprop="text"><p dir="auto"><h2 dir="auto">Electron Apps Causing System-Wide Lag on Tahoe</h2><a id="user-content-electron-apps-causing-system-wide-lag-on-tahoe" aria-label="Permalink: Electron Apps Causing System-Wide Lag on Tahoe" href="#electron-apps-causing-system-wide-lag-on-tahoe"></a></p>
<p dir="auto">See:</p>
<ul dir="auto">
<li><a data-error-text="Failed to load title" data-id="3412185866" data-permission-text="Title is private" data-url="https://github.com/electron/electron/issues/48311" data-hovercard-type="issue" data-hovercard-url="/electron/electron/issues/48311/hovercard?comment_id=3332181420&amp;comment_type=issue_comment" href="https://github.com/electron/electron/issues/48311#issuecomment-3332181420">electron/electron#48311 (comment)</a></li>
<li><a href="https://mjtsai.com/blog/2025/09/30/electron-apps-causing-system-wide-lag-on-tahoe/" rel="nofollow">https://mjtsai.com/blog/2025/09/30/electron-apps-causing-system-wide-lag-on-tahoe/</a></li>
</ul>
<p dir="auto">Fixed versions:</p>
<ul dir="auto">
<li>36.9.2</li>
<li>37.6.0</li>
<li>38.2.0</li>
<li>39.0.0</li>
<li>and all above 39</li>
</ul>
<p dir="auto">This script detects apps with not yet updated versions of Electron.</p>
<p dir="auto"><h2 dir="auto">Temporary workaround:</h2><a id="user-content-temporary-workaround" aria-label="Permalink: Temporary workaround:" href="#temporary-workaround"></a></p>
<p dir="auto">Run</p>
<div dir="auto"><pre>launchctl setenv CHROME_HEADLESS 1</pre></div>
<p dir="auto">on every system start. The CHROME_HEADLESS flag has a side effect of disabling Electron app window shadows, which makes them ugly, but also stops triggering the issue.</p>
<p dir="auto"><h2 dir="auto">Example output</h2><a id="user-content-example-output" aria-label="Permalink: Example output" href="#example-output"></a></p>
<p dir="auto">(as of 1st oct 2025 - it lists all electron apps, but none shows the ✅ checkmark so far)</p>
<pre><code>❌ OpenMTP.app: Electron 18.3.15 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ DaVinci Resolve.app: Electron 36.3.2 (Contents/Applications/Electron.app/Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Electron.app: Electron 36.3.2 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Visual Studio Code.app: Electron 37.3.1 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Cursor.app: Electron 34.5.8 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Windsurf.app: Electron 34.4.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Claude.app: Electron 36.4.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Signal.app: Electron 38.1.2 (Contents/Frameworks/Electron Framework.framework/Electron Framework)
❌ Figma Beta.app: Electron 37.5.1 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Beeper Desktop.app: Electron 33.2.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Slack.app: Electron 38.1.2 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
</code></pre>
<p dir="auto"><h2 dir="auto">A bit of promo</h2><a id="user-content-a-bit-of-promo" aria-label="Permalink: A bit of promo" href="#a-bit-of-promo"></a></p>
<p dir="auto">If you'd appreciate a visual (Tufte-like) hour by hour forecast for iOS/Apple Watch/mac with nice widgets, I made one - check out 🌦️ <a href="https://apps.apple.com/app/apple-store/id1501958576" rel="nofollow">Weathergraph</a>.</p>
<p dir="auto">Thanks! Tomas</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TigerBeetle is a most interesting database (287 pts)]]></title>
            <link>https://www.amplifypartners.com/blog-posts/why-tigerbeetle-is-the-most-interesting-database-in-the-world</link>
            <guid>45436534</guid>
            <pubDate>Wed, 01 Oct 2025 11:33:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amplifypartners.com/blog-posts/why-tigerbeetle-is-the-most-interesting-database-in-the-world">https://www.amplifypartners.com/blog-posts/why-tigerbeetle-is-the-most-interesting-database-in-the-world</a>, See on <a href="https://news.ycombinator.com/item?id=45436534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>By many measures it’s safe to say that TigerBeetle is the most interesting database in the world. Like Costanza in Seinfeld, they seem to do the <em>opposite</em> of everyone else:</p><ul role="list"><li>Most teams write code fast. TigerBeetle tries to <strong>write code slow</strong>.</li><li>Most teams treat testing as a necessary evil. TigerBeetle is <strong>built entirely on Deterministic Simulation Testing (DST)</strong>.</li><li>Most teams build their software on top of loads of other software. TigerBeetle <strong>has zero dependencies</strong>.</li></ul><p>There’s even more. TigerBeetle enforces static memory allocation. They keep assertions enabled in production. They chose Viewstamped Replication over Raft, and even Zig instead of Rust!</p><p>This read is going to go behind the scenes of how TigerBeetle came to be, the incredibly novel software they’ve built, and all of the wacky, wonderful things that make them so special. Based on extensive interviews with the TigerBeetle team, we’re going to cover a few topics in technical detail:</p><ul role="list"><li>Why transactional databases should think in debits and credits, not SQL</li><li>An (actually) modern database: distributed by default, handling storage faults, and why TigerBeetle uses Zig</li><li>VOPR, TigerBeetle’s Deterministic Simulation Testing cluster</li><li>TigerStyle, and why you should use assertions</li></ul><p>Click on any section to jump straight there, if you’re curious.&nbsp;&nbsp;</p><h2><strong>Why we need a database that thinks in debits and credits</strong></h2><p>TigerBeetle’s website calls it “The Financial Transactions Database.” Its primitives are <strong>debits and credits</strong>, which are things you may be familiar with from your accounting requirement in college. And if you’re not a bank, you’re probably thinking this whole thing isn’t really for you. But Joran (TigerBeetle’s creator) would tell you otherwise: financial transactions, i.e. debits and credits, are actually <em>exactly</em> what transactional SQL was originally designed for.&nbsp;</p><p>Way back in 1985, Jim Gray (who would later win a Turing Award) wrote a seminal paper on transactions, titled <a href="https://jimgray.azurewebsites.net/papers/AMeasureOfTransactionProcessingPower.pdf">A Measure of Transaction Processing Power</a>. If you’ve heard of it before, it’s because in it, Gray defined a metric that 40 years later is <em>still</em> the most important measure for a database: <strong>TPS</strong>, or transactions per second. This would end up leading to such a fervent benchmark war among databases that an objective <em>council</em> – <a href="https://www.tpc.org/information/about/history5.asp">the TPC</a> – needed to be formed to moderate.</p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/68daca1c7780ffd190f23346_3d394e75.png" loading="lazy" alt=""></p><figcaption><em>The TPC in action, deciding on whether a young database had gone to the Dark Side (Oracle).</em>&nbsp;</figcaption></figure><p>But what does the “T” in TPS actually mean? What is a transaction?</p><p>Your first guess might be a SQL transaction, but that’s not it. Gray actually defined it as a <strong>business transaction</strong> derived from the real world. Which is the reason databases were invented in the first place: to power businesses. And indeed, <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2005/04/tr-2005-39.doc">20 years later</a>, Gray continued to see the standard measure of transaction processing as a “DebitCredit:”</p><blockquote><em>“A database system to debit a bank account, do the standard double-entry bookkeeping and then reply to the terminal.”</em></blockquote><p>Mind you, SQL had already been around since the 70s at this point. And yet the luminary Gray still chose the debit/credit model – because it was the <em>canonical example</em> of an everyday transaction. Debit/credit is the lingua franca of <a href="https://www.youtube.com/watch?v=lGyMiW6PnKI&amp;ab_channel=TuringAwardeeClips"><strong>what it means to transact</strong></a>. It is <em>not</em> just for accounting and banks. It’s the reason for a database to provide guarantees like ACID in the first place.</p><p>And yet, if you want to use a SQL database to implement debits and credits today, you are probably going to have a bad time. To handle one debit/credit, a typical system – like the <a href="https://mojaloop.io/">central bank switch</a> that Joran consulted on in 2020 – needs to query account balances, lock those rows, wait for decisions in code, then write back and record the debit/credit. All in all, you’re looking at <strong>10-20 SQL queries</strong> back and forth, while holding row locks across the network roundtrip time, <em>for each transaction</em>. This gets even worse when you consider the problem of hot rows, where many transactions often need to touch the same set of “house accounts”.&nbsp;</p><p>All the while (for better or worse), the world is moving faster and faster towards an “everything is a transaction” model. Countries like India and Brazil are doing billions of transactions per month in instant payments. With <a href="https://www.frbservices.org/financial-services/fednow">FedNow</a> in the U.S., we’re not far away from that reality either. Meanwhile, other sectors like energy, gaming, and cloud are all moving towards real-time billing. In less than a decade, the world has become at least three orders of magnitude more transactional. And yet the SQL databases we still use to power this are 20-30 years old. Can they hold up?</p><p><strong>This is where TigerBeetle comes in</strong>. They designed a state-of-the-art database, from the ground up, to power the next era of transactions. In TigerBeetle, a debit/credit is a first class primitive and 8,190 of them can pack into a single 1MiB query via a one solitary roundtrip to the database. They call it <a href="https://www.youtube.com/watch?v=yKgfk8lTQuE">“The 1000x Performance Idea,”</a> but in Joran’s words it’s “nothing special”.</p><p>They say databases take a decade to build. But TigerBeetle is complete and pretty much <a href="https://jepsen.io/analyses/tigerbeetle-0.16.11">Jepsen-proof</a> after just 3 and a half years. In June 2025, Kyle Kingsbury showed he was unable to break TigerBeetle’s foundations (he found 1 correctness bug in the read query engine, not affecting durability), even while corrupting the whole thing on every machine in various places.&nbsp;</p><p>The obvious question here – <strong>how</strong>? How did TigerBeetle ship a production-ready, Jepsen-passing consensus and storage engine in 3.5 years when it typically takes a decade or more?</p><h2><strong>An (actually) modern database: distributed by default, why TigerBeetle uses Zig, and handling storage faults</strong></h2><p>Imagine you wake up today and wisely decide to build a database from scratch. Instead of investing in the technology of 30 years ago – when the most popular relational databases today were built – you can pick <em>any</em> advancements in architecture, hardware, language, or research since then to implement. How would you build it? What would you utilize?</p><h3><strong>Distributed by default</strong></h3><p>One thing you’d probably start with is the deployment model.&nbsp;</p><p>When Postgres and MySQL were built, in a world of big iron (on-prem hardware), the dominant paradigm was <strong>single node</strong>. Now, in a world of shared cloud hardware, it’s <strong>distributed</strong>. It’s not safe enough to store your transactions only on a single disk or server. A modern database needs to replicate your transactions, with strict serializability, across machines, for redundancy, fault tolerance and high availability. And yet some of the most popular OLTP databases in the world today are still highly dependent on a single node architecture. Automated failover, at least with zero data loss in the cut over, is not always baked in by default.</p><p>So TigerBeetle built their database to be distributed by default. Doing that comes with some of the obvious things you need to do, like consensus. But the developer experience for running TigerBeetle distributed is very simple: you just install the binary on however many machines you want in the cluster. No async replication, no Zookeeper, etc. To make this possible, TigerBeetle invested heavily in their consensus protocol implementation, adopting the pioneering <a href="https://pmg.csail.mit.edu/papers/vr.pdf">Viewstamped Replication</a> from MIT. This is part of why TigerBeetle has zero dependencies, apart from the Zig toolchain — they literally invested in all their core dependencies.</p><h3><strong>Clock fault tolerance</strong></h3><p>Distributed by default also shows up in some unlikely places. For example: have you ever thought of a clock fault model?&nbsp;</p><p>Though it’s not technically required or advised for consensus – which uses logical clocks and not physical clocks – remember that TigerBeetle is a <em>transactions</em> database. The physical timestamps of transactions need to be accurate and comparable across different financial systems for auditing and compliance.</p><p>And here, readers will note that Linux has several clocks: <code>CLOCK_MONOTONIC_RAW</code>, <code>CLOCK_MONOTONIC</code> and <code>CLOCK_BOOTTIME</code>. All have slight but important differences. Which is the best monotonic clock to use? (clue: It doesn’t say <code>MONOTONIC</code> on the tin)</p><p>The challenge is that physical imperfections in hardware clocks cause clocks to tick at different speeds, so that time passes faster or slower than it should. These kinds of “drift” errors eventually add up to significant “skew” errors within a short space of time. Most of the time, Network Time Protocol (NTP) would correct for these errors. But if NTP silently stops working because of a partial network outage, then a highly available consensus cluster might otherwise be running blind, in the dark.</p><p>But even this is something TigerBeetle thought about. They combine <em>the majority of clocks</em> in the cluster to construct a fault-tolerant clock called “cluster time”. This cluster time then gets used to bring a server’s system time back into line if necessary, or shut down safely if TigerBeetle detects that there are too many faulty clocks (e.g. TigerBeetle can actually detect when something like Chrony, PTP, or NTP have stopped working and alert the operator).&nbsp;</p><p>They do this by tracking offset clock times between different TigerBeetle servers, sampling them, and passing them through <a href="https://en.wikipedia.org/wiki/Marzullo%27s_algorithm">Marzullo’s algorithm</a> to estimate the most accurate possible interval (again, just to get a sense of whether clocks are being synced by the underlying clock sync protocol correctly).</p><p>Small things like this are exactly why distributed by default is hard, and doesn’t work as an add-on for older database models. You can read more about this in TigerBeetle's <a href="https://tigerbeetle.com/blog/2021-08-30-three-clocks-are-better-than-one/">3 clocks are better than one</a> blog post.</p><p>‍</p><h3><strong>Handling storage faults</strong></h3><p>Another piece of “distributed by default” that deserves its own header is how TigerBeetle handles <strong>storage faults</strong> (or even the fact it handles them at all). Traditional databases assume that if disks fail, they do so predictably with a nice error message. For example, even <a href="https://www.sqlite.org/atomiccommit.html">SQLite’s docs</a> are clear that:</p><blockquote><em>SQLite does not add any redundancy to the database file for the purpose of detecting corruption or I/O errors. SQLite assumes that the data it reads is exactly the same data that it previously wrote.</em></blockquote><p>In reality, there are many more sinister possibilities: disks can silently return corrupt data, misdirect I/O (on the read or write path), or just suddenly get really slow (called <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/paper-1.pdf">gray failure</a> in the research), all without returning error codes.&nbsp;</p><p>TigerBeetle is built to be storage fault tolerant:</p><ul role="list"><li>TigerBeetle uses <a href="https://www.usenix.org/conference/fast18/presentation/alagappan">Protocol Aware Recovery</a> to remain available unless all copies of a piece of data get corrupted on every single replica.</li><li>All data in TigerBeetle is immutable, checksummed, and hash-chained, providing a strong guarantee that no corruption or tampering happened.</li><li>TigerBeetle puts as little software as possible between itself and the disk, including a custom page cache, writing data to disk with O_DIRECT, and even running on a raw block device directly (no filesystem necessary — to sidestep filesystem bugs <a href="https://news.ycombinator.com/item?id=36113828">which do tend to happen</a> from time to time).</li><li>They built their own implementation of LSM instead of using an off-the-shelf one – they call it an <a href="https://www.youtube.com/watch?v=yBBpUMR8dHw">LSM Forest</a>, which is something like 20 different LSM trees.</li></ul><p>As far as I’m aware TigerBeetle is the only distributed database that not only claims to be storage fault tolerant, but was also tested pretty hard and validated by Jepsen to be. If you have a local machine failure where even just a disk sector fails, then that storage engine is connected to the global consensus, and it can use the cluster to self heal. This is also a great example of why the modern database having access to modern research matters: <a href="https://www.usenix.org/conference/fast18/presentation/alagappan">Protocol-Aware Recovery</a>, which enables TigerBeetle to survive disk failures like this, is fairly recent (2018) research.</p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/68daca1c7780ffd190f23349_629bea61.png" loading="lazy" alt=""></p></figure><h3><strong>TigerBeetle in Zig</strong></h3><p>Another thing you’d think about when building a modern database from scratch is your choice of <strong>programming language</strong>. Postgres is written in C (c. 1970s), MySQL in C and C++ (1979), and MSSQL as well in C and C++. But programming languages have come a long way in the past 40 years. If you had your choice, what would you build a database in today?</p><p>The answer would probably be Rust or Zig. And indeed, TigerBeetle is built 100% in Zig:&nbsp;</p><ul role="list"><li>You get the whole C ecosystem available to you, extended with a phenomenal toolchain and compiler.</li><li>It’s easy to write, and especially easy to read, in some cases as easy as TypeScript (just a lot faster).</li><li>Zig lets you statically allocate memory, which is a core principle of TigerBeetle.</li><li>Zig has a great developer experience and you can learn it quickly (which ergo means you can get into the TigerBeetle src quickly).</li></ul><p>Of course, as new systems languages, Zig and Rust are related, and some of the early Rust team now work at TigerBeetle, including <a href="https://matklad.github.io/">Matklad</a> (creator of <a href="https://rust-analyzer.github.io/">Rust Analyzer</a>) and <a href="https://brson.github.io/">Brian Anderson</a> (co-creator of Rust with Graydon). They’ve <a href="https://matklad.github.io/2023/03/26/zig-and-rust.html">written extensively</a> about these languages and why Joran chose Zig in particular for TigerBeetle, given their design goals.</p><p>And here, of course, TigerBeetle is fanatical about static memory allocation, which I’ll talk more about in the next section. Not using dynamic memory allocation is “hard mode” in Rust (as matklad wrote about <a href="https://matklad.github.io/2022/10/06/hard-mode-rust.html">here</a>), but a breeze in Zig.</p><p>‍</p><h2><strong>Deterministic Simulation Testing and the VOPR</strong></h2><p>Sometimes, Deterministic Simulation Testing (DST) feels like the most transformational technology that the fewest developers know about. It’s a <a href="https://notes.eatonphil.com/2024-08-20-deterministic-simulation-testing.html">novel testing technique</a> made popular by the <a href="https://www.foundationdb.org/">FoundationDB</a> team (which now belongs to Apple); they used it to develop a more secure, bug-free distributed database in a shorter time span than arguably anyone had done before.&nbsp;</p><p>The <a href="https://www.youtube.com/watch?v=cHA8vyZvkCs">fundamentals of DST</a> go something like this. In distributed systems, there are essentially infinite combinations of concurrency issues: anything from lost messages to unpredictable thread execution order. You simply cannot use old-school unit and integration tests, or your system will suck. Formal verification, a more academic discipline that works on formulaic proofs that a program runs as intended, is too expensive and slow. So what are you to do?</p><p>The answer is a simulator that deterministically runs almost every possible scenario your system will face on a specific chronological timeline. The simulator accounts for external factors too, like issues with the OS, network, or disk, or simply different latencies. All in all, DST can give you the equivalent of years’ worth of testing in a very short time period (because time itself becomes deterministic—a while true loop); and DST is particularly well suited towards databases (I/O intensive, not compute intensive). If you’re familiar with Jepsen testing, think of it as <a href="https://antithesis.com/blog/is_something_bugging_you/">a subset of what DST can do</a>.&nbsp;</p><p>TigerBeetle is one of the most pioneering startups on the planet when it comes to DST. They’ve developed their own testing cluster – it’s nicknamed VOPR, short for Viewstamped Operation Replicator (<a href="https://www.youtube.com/watch?v=iRsycWRQrc8">after the WOPR simulator in the movie WarGames</a>). The VOPR constantly (and tirelessly) tests TigerBeetle under countless different conditions, covering everything from how nodes elect a leader to individual states and network faults. But it can simulate a whole distributed cluster virtually, all on a single thread.</p><p>As far as your author is aware, TigerBeetle’s VOPR is the single largest DST cluster on the planet. <a href="https://us13.campaign-archive.com/?u=32cd932058e988b44c838f7bc&amp;id=0c749f7b07">It runs on</a> 1,000 CPU cores, a number so unusually large that Hetzner sent them a special email asking if they were sure they wanted that many cores. The so-called VOPR-1000 is running 24x7x365, to catch rare conditions as far as possible before production. With time abstracted deterministically, and accelerated in the simulator by a factor of (roughly) 700x, this adds up to nearly 2 millennia of simulated runtime per day.</p><p>‍</p><h3><strong>But what if DST was fun?</strong></h3><p>Yea, distributed systems are cool. But you know what’s even cooler? Video games.</p><p>TigerBeetle turned DST into a game that lets you play through different failure scenarios in how the system reacts. You can play it <a href="https://sim.tigerbeetle.com/">here</a>.</p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/68daca1c7780ffd190f2334c_13e94615.png" loading="lazy" alt=""></p></figure><p>What’s perhaps even cooler is that this game is running an actual instance of the VOPR, simulating TigerBeetle…in your browser. It’s compiled to WebAssembly, and then TigerBeetle’s own engineers built a gaming frontend on top to visualize the real system</p><p>You can read more about how and why TigerBeetle built the simulator in <a href="https://tigerbeetle.com/blog/2023-07-11-we-put-a-distributed-database-in-the-browser/">this blog post</a>.</p><p>‍</p><h2><strong>TigerStyle and The Power of Ten</strong></h2><p>As you will continue to see with TigerBeetle, it is often not just the <em>what</em> they’ve built that catches the eye but also the <em>how</em>. There’s no better example than <strong>TigerStyle</strong>.</p><p><a href="https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/TIGER_STYLE.md">TigerStyle</a> is TigerBeetle’s engineering methodology, public on GitHub for all to see. Here’s how they describe it:</p><blockquote><em>“TigerBeetle's coding style is evolving. A collective give-and-take at the intersection of engineering and art. Numbers and human intuition. Reason and experience. First principles and knowledge. Precision and poetry. Just like music. A tight beat. A rare groove. Words that rhyme and rhymes that break. Biodigital jazz. This is what we've learned along the way. The best is yet to come.”</em></blockquote><p>Biodigital jazz is a term from <a href="https://en.wikipedia.org/wiki/Tron:_Legacy">Tron: Legacy</a>. In the context of the film, it represents the intertwining of human and digital elements, the chaotic yet structured nature of the “Grid” (the digital world), and the improvisational spirit of human potential within the confines of technology (I copied this from AI). For TigerBeetle, it’s an ethos of code; remembering to infuse everything they do with not just science, but art too.</p><p>More practically, TigerStyle lays out engineering and code principles for TigerBeetle, many derived from the original <a href="https://spinroot.com/gerard/pdf/P10.pdf">Power of Ten</a>, NASA’s tenets for writing foolproof code. TigerStyle spans from the thematic, like simplicity and elegance, to the applied, like how to name things. It’s even starting to impact other companies like Resonate and Turso; and <a href="https://youtu.be/tNZnLkRBYA8?t=11167">TigerStyle has even been discussed on Lex Fridman</a>. Here are a few highlights.</p><h3><strong>Using assertions, and the Power of Ten</strong></h3><p>Speaking of the Power of Ten…one of them (Rule 5) is about <strong>assertions</strong>. The idea is simple: explicitly encode your expectations of code behavior <em>while</em> you are writing it, not after the fact. You write them simply in a single line as booleans: assert(a &gt; b). TigerStyle calls for:</p><ul role="list"><li>Asserting all function arguments, return values, preconditions, and invariants. On average there should be at least 2 assertions per function.</li><li>Using assertions <em>instead</em> of comments when the assertion is both important and surprising.</li><li>Asserting the relationships between compile-time constants, so you can check a program’s design integrity before it even runs.</li><li>Not just assert what <em>should</em> happen, but also the negative space that you don’t expect – where interesting bugs can show up.</li></ul><p><a href="https://spinroot.com/gerard/pdf/P10.pdf">The Power of Ten</a> is an amazing artifact that covers so much more than just assertions…it’s a great resource for any modern programmer (and maybe we should train some LLMs on it too).</p><h3><strong>Thinking about performance</strong></h3><p>Much of TigerStyle centers around the idea that <em>writing</em> code is not the most important part of the cycle; instead, it’s <strong>reasoning about</strong> and <strong>designing</strong> the code. When it comes to performance, TigerStyle implores you to think about it from the start:&nbsp;</p><blockquote><em>“The best time to solve performance, to get the huge 1000x wins, is in the design phase, which is precisely when we can't measure or profile.”</em></blockquote><p>You should be doing basic napkin math on what TigerStyle calls “the four primary colors” – network, storage, memory, CPU – and how they’ll perform with respect to (“the two textures” — art!) bandwidth and latency. Then, there are a few more tactical tips, like distinguishing between the control plane and data plane, batching accesses, and extracting hot loops into stand-alone functions to reduce dependence on the compiler.&nbsp;</p><p>For more about TigerStyle, watch <a href="https://www.youtube.com/watch?v=w3WYdYyjek4&amp;t=5s&amp;ab_channel=TigerBeetle">Joran’s talk at Systems Distributed</a>.</p><h2>Try it out for yourself</h2><p>So is TigerBeetle a database? Yes. But it’s not much like any other database I’ve seen. They’ve taken modern research and applied it to an age-old form, giving their database unprecedented performance and stability guarantees. They’ve developed an art form around systems and storage engineering, and they haven’t forgotten to have fun along the way. And thanks to their clever use of DST, they were able to build this thing to Jepsen standards in only a few years.&nbsp;</p><p>You can get started with TigerBeetle <a href="https://tigerbeetle.com/#install">here</a> using a simple curl command.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Our efforts, in part, define us (256 pts)]]></title>
            <link>https://weakty.com/posts/efforts/</link>
            <guid>45435825</guid>
            <pubDate>Wed, 01 Oct 2025 09:22:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://weakty.com/posts/efforts/">https://weakty.com/posts/efforts/</a>, See on <a href="https://news.ycombinator.com/item?id=45435825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>What happens when something we enjoy doing that took effort becomes effortless? And what happens if that original effort was a foundation on which we saw value in ourselves?</p>
<p>If our efforts, in part, define us, then our efforts have intrinsic value. Our efforts may help us understand a position we want to occupy, an identity we carry, or an outlook we present. This value contributes to an internal economy of joy, self-respect, fulfillment, happiness. When effortful things become effortless, what becomes of our position in these economies?</p>
<p>As you can see, I have a few questions here.</p>
<hr>
<p>I know someone who spent a part of their adult life taking beautiful photographs, developing them by hand, framing them, cataloging them. Along came the ubiquity of digital cameras and smartphones, and "film" became infinitely available. Offhandedly, one day, this person mentioned that with the proliferation of smart phone cameras, and the ease with which one can take photos, they had found that some days their desire to continue was diminishing, and their work had lost meaning.</p>
<p>Technology has a history of making effortful things effortless, and there is sometimes a hidden loss in that advancement.</p>
<p>I figure people are continually being left behind in a similar manner day-to-day. Technology continues advancing (for the most part), and more things that remain effortful will become effortless. And "we" (ie, the populations who can afford to sit around and have crises of identity on these topics) will be further pushed to re-evaluate certain parts of our definitions of self.</p>
<p>For myself, in the last 10 years, my work of writing code has largely defined what I do with my working time. Now I experience large swaths of that work being created and done by AI (sometimes amazingly well, sometimes poorly), and I find myself thinking of the photographer above. It's not my wish that people can't have access to a more effortless way to write code, but I feel a strange sadness that there is less left to the act of the craft.</p>
<p>I have had this note in a draft state for several weeks now because I still can’t quite come to terms with how I’m feeling about things. There are so many nuances and unclear thoughts rolling around in my head about this shift. I think the only thing that is vaguely clear is that none of this would matter if making money wasn’t at play. If I was just writing code, (or taking film photographs) for fun in my free time because I enjoyed it, well, I don’t think I’d be feeling so conflicted.</p>
<p>Being paid to work and presenting my capacities through my craft is an exchange that I have been able to derive value from in its effortful-ness. Often times I've worked on utterly boring tasks that I would have loved to have a tool that could automate. But I didn't. And even in those menial moments I did derive some pleasure in my capacities. Of course, when it came to the real challenges, that was where I felt a pleasure and value in putting forth effort.</p>
<p>As a consultant, I work in a lot of different places, often for brief stints of time. And at many of these places, I see a large push, top-down, to encourage people to use AI. These employees, previously having entered an employment agreement where their capacities and experience would be exchanged for money, are now being asked that their abilities be augmented. In this way, the level continues to skew toward privileging production, often without understanding and people using their own perspectives.</p>
<p>When I see sentiments similar to mine, I often see reactions where people say that AI is simply a tool and that you must learn to use it and incorporate it into your toolbox. That's fine. That's well and good. But all I'm trying to say here is that I feel a lack and a loss for something. I don't understand it yet.</p>
<p>The title of this post, <em>our efforts, in part, define us</em>, is just a phrase that popped into my head. I'm not really sure if I even believe it or if I've fully fleshed out this single statement. But some part of it rings true to me. I wonder what will happen to us and our efforts. Will we be driven into further niches that are effortful, that we can derive value from? Will we become vague blobs that are formless, ill-defined, and despondent?</p>
<p>All of this presupposes a few things —that one can (and/or <em>should</em>) aim to derive value from work, that a meaningful identity is constructed by doing effortful things, that people generally are happier when they can use their skills and experiences to make something. And what’s more, there is a fine-line here between glorifying people with experience deriving value, and sounding like a shitty gatekeeper.</p>
<p>I will continue working for various clients. I suspect I will continue hearing leadership push AI on employees. And I will continue observing how people respond to this. Of course, for many people a job is <em>just a job</em>, as they say, and they'll do whatever they can to get it done more quickly (or work several jobs at once). Those very same people might find more value from their efforts now that AI is making their jobs easier. They can turn to better supporting their family, following other interests outside of work, finding other meaningful things, etc.</p>
<p>But at this time, I don't really see how this won’t further trample people’s spirits in the realm of work, unless we also reshape our expectations of work itself.</p>
<p>Is it worth the effort?</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I only use Google Sheets (287 pts)]]></title>
            <link>https://mayberay.bearblog.dev/why-i-only-use-google-sheets/</link>
            <guid>45435463</guid>
            <pubDate>Wed, 01 Oct 2025 08:06:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mayberay.bearblog.dev/why-i-only-use-google-sheets/">https://mayberay.bearblog.dev/why-i-only-use-google-sheets/</a>, See on <a href="https://news.ycombinator.com/item?id=45435463">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
  <header>
    <a href="https://mayberay.bearblog.dev/">
      <h2>
        Maybe-Ray
      </h2>
    </a>
    <nav>
      <p><a href="https://mayberay.bearblog.dev/">Home</a> <a href="https://mayberay.bearblog.dev/blog/">Blog</a></p>

    </nav>
  </header>
  <main>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-09-30T13:10Z">
                    30 Sep, 2025
                </time>
            </i>
        </p>
    

    <p>To cut things short, always use the easiest solution to solve a particular problem and once that solution does not work for the business anymore reassess what the new requirements are and either try enhance the current solution or find an alternative that better solve the problem. In my case the easiest solution is often creating a new Google sheet.</p>
<p>I entered the workforce about 9 months ago and my optimism for building new tools and services that help the small starting up business I work for has all vanished. I work in an environment that changes every 2 months or so, as my boss finds a new business venture she wants to enter. This has me starting and stopping quite a few projects that could have been solved in an afternoon with a quick Google Sheet.</p>
<p>I have listed a few examples below of some of the projects I have wasted time on instead of making a Google Sheet:</p>
<ol>
<li><p>I spent 2 months designing and making an admin panel to manage and track incoming cargo for the business. This panel was supposed to help the business categorise and better manage packages and customer data. This admin panel was used twice and never again. A Google Sheet could have been easily used for this and is currently being used for this task.</p>
</li>
<li><p>Three weeks were spent creating an MVP for a quote system that automatically calculated the duty and taxes for people ordering certain goods. Zimbabwean taxes and duties are often very complex and having our customers know exactly what to pay would create a better customer journey and make the process faster since we would not have to wait on our third party duty processing company to reply to us on every customer inquiry. In the end, we saw one of our competitors tax and duty breakdown table and we just copied it and put it in a Google Sheet.</p>
</li>
<li><p>Spent 2 months researching, having meetings (often &gt; 1hr long) and looking for a good CRM to use for the business. I would sit down compare and contrast different feature`s and prices for all the different CRMs we were looking into. We ended up using the free version of Oddo, that is not used that much anyway within the business. To my surprise a few weeks ago i noticed that Google Sheets has a CRM template built into it.</p>
</li>
</ol>
<p>I'm not saying that making a Google Sheet is the best solution to every problem but often times in my situation it is. I usually end up in situations were I never know the full scope of the problem until we start doing the actual work.</p>
<p>This is not to say that we do not need to plan out a project. The team should discuss workflows and information they might need but until we start doing the actual work we do not know full scope of the problem.</p>
<p>Once the full scope of the problem is known then we can start creating or enhancing the solutions we have. This helps because you do not end up being stuck with an extra workload that in the best case does not require all the features you are adding and in the worst case spending time on a project that will fail. So it is in your best interest to use the most basic solution to solve a problem.</p>
<p>Doing the smallest and easiest solution to a problem as a way to get to know the full scope and then iterating after that if needs be is by far the best solution (for me).</p>
<p>There are some caveats to this approach, I know a few organisation that have a thousand row spreadsheets that keep track of all their business transactions and employee information.</p>
<p>Creating a Google Sheet only works in situations were we do not know the full scope of the problem. Personally, I'm still new to this and learning when the best solution is making a Google Sheet or not. I just want to save people's time and effort and not have them build something that will never be used. But like all advice, think carefully about your own situation before committing a lot of time and effort especially in a business setting. It is perfectly fine for you to build useless programs and software in your spare time, that's the whole fun of it.</p>


    

    
        

        
            


        
    


  </main>
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Category Theory Illustrated – Natural Transformations (178 pts)]]></title>
            <link>https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/</link>
            <guid>45435422</guid>
            <pubDate>Wed, 01 Oct 2025 08:00:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/">https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/</a>, See on <a href="https://news.ycombinator.com/item?id=45435422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <!--<h1>Natural transformations</h1> -->
        
        

        

<blockquote>
  <p>I didn’t invent categories to study functors; I invented them to study natural transformations. — Saunders Mac Lane</p>
</blockquote>

<p>In this chapter, we will introduce the concept of a morphism between functors, or <em>natural transformation</em>. Understanding natural transformations will enable us to define category equality and some other advanced concepts.</p>

<p>Natural transformations really are at the heart of category theory, however, their importance is not obvious at first. So, before introducing them, I like to talk, once more, about the body of knowledge that this heart maintains (I am good with metaphors… in principle).</p>

<h2 id="equivalent-categories">Equivalent categories</h2>

<p>Our first section aims to introduce natural transformation as a motivating example for creating a way to say that two categories are equal. But for that, we need to understand what equal categories are and should be.</p>

<p>So, are you ready to hear about equivalent categories and natural transformations? Actually it is my opinion that you are not (no offence, they are just very hard!). So, we will take a longer route. I can put this next section anywhere in this book, and it would always be neither here nor there. But anyway, if you are studying math, you are probably interested in the <em>nature of the universe</em>. “What is the quintessential characteristic of all things in this world?” I hear you ask…</p>

<h2 id="objects-are-overrated-aka-heraclitus-was-right">Objects are overrated AKA Heraclitus was right!</h2>

<blockquote>
  <p>The world is the collection of facts, not of things. — Ludwig Wittgenstein</p>
</blockquote>

<p>What is the quintessential characteristic of all things in this world? Some 2500 years ago, the philosopher Parmenides gave an answer to this question, postulating that the nature of the universe is permanence, stasis. According to his view, what we perceive as processes/transformations/change is merely illusory appearances (“Whatever is is, and what is not cannot be”). He said that that things never really change, they only <em>appear</em> to change, or (another way to put it), only appearances change, but the <em>essence</em> does not (I think this is pretty much how the word “essence” came to exist).</p>

<p>Although far from obviously true, his view is easy for people to relate to — objects are all around us, everything we “see”, both literally (in real life), or metaphorically (in mathematics and other disciplines), can be viewed as <em>objects</em>, persisting through space and time. If we subscribe to this view, then we would think that the key to understanding the world is understanding <em>what objects are</em>. In my opinion, this is what set theory does, to some extent, as well as classical logic (Plato was influenced by Parmenides when he created his theory of forms).</p>

<p>However, there is another way to approach the question about the nature of the universe, which is equally compelling. Because, what is an object, when viewed by itself? Can we study an object in isolation? And will there anything left to study about it, once it is detached from its environment? If a given object undergoes a process to get all of it’s part replaced, is it still the same object?</p>

<p>Asking such questions might lead us to suspect that, although what we <em>see</em> when we look at the universe are the objects, it is the processes/relations/transitions or <em>morphisms</em> between the objects that are the real key to understanding it. For example, when we think hard about everyday objects we realize that each of them has a specific <em>functions</em> (note the term) without which, a thing would not be itself e.g. is a lamp that doesn’t glow, still a lamp? Is there food that is non-edible (or an edible item that isn’t food)? And this is even more valid for mathematical objects, which, without the functions that go between them, are not objects at all.</p>

<p>So, instead of thinking about objects that just happen to have some morphisms between them, we might take the opposite view and say <em>that objects are only interesting as sources and targets of morphisms.</em></p>

<p>Although old, dating back to Parmenides’ alleged rival Heraclitus, this view has been largely unexplored, until the 20th century, when a real mathematical revolution happened: Bertrand Russell created type theory, his student Ludwig Wittgenstein wrote a little book, from which the above quote comes, and this book inspired a group of mathematicians and logicians, known as the “Vienna circle”. Part of this group was Rudolph Carnap who coined the word “functor”…</p>

<h2 id="isomorphism-invariance">Isomorphism invariance</h2>

<p>An embodiment of Heraclitus’ view in the realm of category theory is the concept of <em>isomorphism invariance</em> that we implicitly touched several times.</p>

<p>All categorical constructions that we covered (products/coproducts, initial/terminal objects, functional objects in logic) are <em>isomorphism-invariant</em>. Or, equivalently, they define an objects <em>up to an isomorphism</em>. Or, in other words, if there are two or more objects that are isomorphic to one another, and one of them has a given property, then the rest of them would to also have this property as well.</p>

<p>In short, in category theory <strong>isomorphism = equality</strong>.</p>

<p>The key to understanding category theory lies in understanding isomorphism invariance. And the key to understanding isomorphism invariance are natural transformations.</p>

<h2 id="categorical-isomorphisms-are-not-isomorphism-invariant">Categorical isomorphisms are <em>not</em> isomorphism-invariant</h2>

<p>Let’s return to the question that we were pondering at the beginning of the previous chapter — what does it mean for two categories to be equal?</p>

<p>In the prev chapter, we talked a lot about how great isomorphisms are and how important they are for defining the concept of equality in category theory, but at the same time we said that <em>categorical isomorphisms</em> do not capture the concept of equality of categories.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/isomorphic_categories.svg" alt="Isomorphic categories"></p>

<p>This is because (though it may seem contradictory at first) <em>categorical isomorphisms are not isomorphism invariant</em>, i.e. categories that only differ by having some additional isomorphic objects aren’t isomorphic themselves.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equal_categories.svg" alt="Isomorphic categories"></p>

<p>For this reason, we need a new concept of equality of categories. A concept that would elucidate the <em>differences</em> between categories with different structure, but also the <em>sameness</em> of categories that have the same categorical structures, disregarding the differences that are irrelevant for category-theoretic standpoint. That concept is <em>equivalence</em>.</p>

<!--comic-->
<p><strong>Parmenides:</strong> This category surely cannot be equal to the other one — it has a different amount of objects!</p>

<p><strong>Heraclitus:</strong> Who cares bro, they are isomorphic.</p>

<h2 id="equivalences-are-isomorphism-invariant">Equivalences are isomorphism invariant</h2>

<p>To understand equivalent categories better, let’s go back to the functor between a given map and the area it represents (we will only consider the thin categories (AKA orders) for now). This functor would be invertible (and the categories — isomorphic) when the map should represent the area completely i.e. there should be arrow for each road and a point for each little place.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/isomorphic_map.svg" alt="Isomorphic categories"></p>

<p>Such a map is necessary if your goal is to know about all <em>places</em>, however, like we said, when working with category theory, we are not so interested in <em>places</em>, but in the <em>routes</em> that connect them i.e. we focus not on <em>objects</em> but on <em>morphisms</em>.</p>

<p>For example, if there are intersections that are positioned in such a way that there are routes from one and to the other and vice-versa a map may collapse them into one intersection and still show all routes that exist (the tree routes would be represented by the “identity route”).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_map.svg" alt="Equivalent categories"></p>

<p>These two categories are <em>not isomorphic</em> — going from one of them to the other and back again doesn’t lead you to the same object.</p>

<p>However, going from one of them to the other would lead you at least to an <em>isomorphic object</em>.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_map_equivalence.svg" alt="Equivalent categories"></p>

<p>In this case we say that the orders are <em>equivalent</em>.</p>

<h2 id="defining-equivalence-in-terms-of-objects">Defining equivalence in terms of objects</h2>

<p>We know that two orders are isomorphic if there are two functors, such that going from one to the other and back again leads you to the same object.</p>

<p>And two orders are equivalent if going from one of them to the other and back again leads you to the same object, <em>or to an object that is isomorphic to the one you started with.</em></p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders.svg" alt="Equivalent orders"></p>

<p>But when does this happen? To understand this, we plot the orders as a Hasse diagram.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_hasse.svg" alt="Equivalent orders"></p>

<p>You can see that, although not all objects are connected one-to-one, <em>all objects at a given level are connected to objects of the corresponding level</em>.</p>

<p>To formalize that notion, we remember the concept of <em>equivalence classes</em> that we covered in the chapter about orders. Let’s visualize the relationship of the equivalence classes of the two orders that we saw above.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_order_classes.svg" alt="Orders with isomorphic equivalence classes"></p>

<p>You can see that they are isomorphic. And that is no coincidence: two orders are equivalent precisely when the orders made of their equivalence classes are isomorphic.</p>

<p>This is a definition for equivalence of orders, but unfortunately, it does not hold for all categories — when we are working with orders, we can get away by just thinking about <em>objects</em>, but categories demands that we think about morphisms i.e. to prove two categories are equivalent, we should establish an isomorphism between their <em>morphisms</em>.</p>

<p>For example, the following two categories are <em>not</em> equivalent, although their equivalence classes are isomorphic — the category on the left has just one morphism, but the category on the right has two.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/unequal_categories.svg" alt="Non-equivalent categories"></p>

<p>One way of defining equivalence of categories is by generalizing the notion of equivalence classes of orders to what we call <em>skeletons</em> of categories, a skeleton of a category being a subcategory in which all objects that are isomorphic to one another are “merged” into one object (isomorphic objects are necessarily identical).</p>

<p>However, we will leave this (pardon my French) as an <em>exercise for the reader</em>. Why? We already did this when we generalized the notion of normal set-theoretic functions to <em>functors</em>, and so it makes more sense to build up on that notion. Also, we need a motivating example for introducing natural transformations, remember?</p>

<h2 id="defining-equivalence-in-terms-of-morphisms">Defining equivalence in terms of morphisms</h2>

<p>In the chapter about orders, we presented a definition of order <em>isomorphisms</em>, that is based on <em>objects</em>:</p>

<blockquote>
  <p>An order isomorphism is essentially an isomorphism  between the orders’ underlying sets (invertible function). However, besides their underlying sets, orders also have the arrows that connect them, so there is one more condition: in order for an invertible function to constitute an order isomorphism it has to <em>respect those arrows</em>, in other words it should be <em>order preserving</em>. More specifically, applying this function (let’s call it $F$) to any two elements in one set ($a$ and $b$) should result in two elements that have the same corresponding order in the other set (so $a ≤ b$ if and only if $F(a) ≤ F(b)$).</p>
</blockquote>

<p>That a way to define them, but it is not the best way. Now that we know about functors (which, as we said, serve as functions between the orders and other categories), we can devise a new, simpler definition, which would also be valid for all categories, not just orders, and for all forms of equality (isomorphism and equivalence).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/isomorphic_orders.svg" alt="isomorphic orders"></p>

<p>We begin with the definition of <strong>set isomorphism</strong>:</p>

<blockquote>
  <p>Two <strong>sets</strong> $A$ and $B$ are <strong>isomorphic</strong> (or $A ≅ B$) if there exist functions $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g = ID_{B}$ and $g \circ f = ID_{A}$.</p>
</blockquote>

<p>To amend it so it is valid for all categories  by just replacing the word “function” with “functor” and “set” with “category”:</p>

<blockquote>
  <p>Two <strong>categories</strong> $A$ and $B$ are <strong>isomorphic</strong> (or $A \cong B$) if there exist <em>functors</em> $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g = ID_{B}$ and $g \circ f = ID_{A}$.</p>
</blockquote>

<p><strong>Task 1:</strong> Check if that definition is valid.</p>

<p>Believe it or not, this definition, is just one find-and-replace operation away from the definition of <em>equivalence</em>. We get there only by replace equality with isomorphism (so, $=$ with $\cong$).</p>

<blockquote>
  <p>Two <strong>categories</strong> $A$ and $B$ are <strong>equivalent</strong> (or $A \simeq B$) if there exist <em>functors</em> $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g \cong ID_{B}$ and $g \circ f \cong ID_{A}$.</p>
</blockquote>

<p>Like we said at the beginning, with isomorphisms, going back and forth brings us to the same object, while with equivalence the object is just <em>isomorphic</em> to the original one. This is truly all there is to it.</p>

<p>There is only one problem, though — <em>we never said what it means for functors to be isomorphic</em>.</p>

<h2 id="natural-transformations-natural-isomorphisms-and-categorical-equivalence">Natural transformations, natural isomorphisms and categorical equivalence</h2>

<p>So, how can we make the above definition “come to life”? The title of this chapter outlines the things we need to define:</p>

<ol>
  <li><em>Morphisms between functors</em> (called <em>natural transformations</em>).</li>
  <li><em>Functor isomorphisms</em> (called <em>natural isomorphisms</em>).</li>
  <li>Finally <em>categorical equivalences</em>.</li>
</ol>

<p>If this sounds complicated, remember that we are doing the same thing we always did — talking about isomorphisms.</p>

<p>In the very first chapter of this book, we introduced <em>set isomorphisms</em>, which are quite easy, and now we reached the point to examine <em>functor isomorphisms</em>. So, we are doing the same thing. 
<!--comic-->
Although actually…</p>

<p>But actually, natural transformations are quite different from morphisms and functors, (the definition is not “recursive”, like the definitions of functor and morphism are). This is because functions and functors are both morphisms between objects (or <em>1-morphisms</em>), while natural transformations are <em>morphisms between morphisms</em> (known as <em>2-morphisms</em>).</p>

<p>But enough talking, let’s draw some diagrams. We know that natural transformations are morphisms between functors, so let’s draw two functors.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_functors_objects.svg" alt="Two functors"></p>

<p>The functors have the same signature. Naturally. How else can there be morphisms between them?</p>

<p>Now, a functor is comprised of two mappings (object mapping and morphism mapping) so a mapping between functors, would consist of “object mapping mapping” and “morphism mapping mapping” (yes, I often do get in trouble with my choice of terminology, why do you ask?).</p>

<h2 id="object-mapping-mapping">Object mapping mapping</h2>

<p>Let’s first connect the object mappings of the two functors, creating what we called “object mapping mapping”.</p>

<p>It is simpler than it sounds when we realize that we only need to connect the object in functors’ <em>target category</em> — the objects in the source category would just always be the same for both functors, as both functors would include <em>all</em> object from the source category (as that is what functors (and morphisms in general) do). In other words, mapping the two functors’ object components involves nothing more than specifying a bunch of morphisms in the target category: one morphism for each object in the source category i.e. each object from the image of the first functor, should have one arrow coming from it (and to an object of the second functor, so, for example, our current source category has two objects and we specify two morphisms.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_transformation.svg" alt="Two functors and a natural transformation"></p>

<p>Note that this mapping does not map every object from the target category,  i.e. not all objects have arrows coming from them (e.g. here the black and blue square do not have arrows), although, in some cases, it <em>might</em>.</p>

<p><strong>Task 2:</strong> When exactly would the mapping encompass all objects?</p>

<h2 id="morphism-mapping-mapping">Morphism mapping mapping</h2>

<p>The morphism part might seem hard… until we realize that, once the connections between the object mappings are already established, there is only one way to connect the morphisms — we take each morphism of the source category and connect the two morphisms given by the two functors, in the target category. And that’s all there is to it.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_functors.svg" alt="Two functors"></p>

<p>Oh, actually, there is also this condition that the above diagram should commute (the naturality condition), but that happens pretty much automatically.</p>

<h2 id="the-naturality-condition">The naturality condition</h2>

<p>Just like anything else in category theory, natural transformations have some laws that they are required to pass. In this case it’s one law, typically called the naturality law, or the naturality condition.</p>

<p>Before we state this law, let’s recap where are we now: We have two functors $F$ and $G$ that have the same type signature (so $F : C \to D$ and $G : C \to D$ for some categories $C$ and $D$), and a family of morphisms in the target category $D$ (denoted $\alpha : F \Rightarrow G$) one for each object in $C$, that map each object of the target of the functor $F$ (or the image of $F$ in $D$ as it is also called) to some objects of the image of $G$. This is a <em>transformation</em>, but not necessarily a <em>natural</em> one. A transformation is natural, when this diagram commutes for all morphisms in $C$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_transformation_square.svg" alt="The commuting square of a natural transformation"></p>

<p>i.e. a transformation is natural when every morphism $f$ in $C$ is mapped to morphisms $F(f)$ by $F$ and to $G(f)$ by $G$ (not very imaginative names, I know), in such a way, that we have $\alpha \circ F(f) = G(f) \circ \alpha$ i.e. when starting from the white square, when going right and then down (via the yellow square) is be equivalent to going down and then right (via the black one).</p>

<p>We may view a natural transformation is a mapping between morphisms and commutative squares: two functors and a natural transformation between two categories means that for each morphism in the source category of the functors, there exist one commutative square at the target category.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_transformation_squares.svg" alt="Commuting squares of a natural transformation"></p>

<p>When we fully understand this, we realize that commutative squares are made of morphisms too, so, like morphisms, they compose — for any two morphisms with appropriate type signatures that have we can compose to get a third one, we have two naturality squares which compose the same way.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_transformation_squares_composition.svg" alt="Composition of commuting squares of a natural transformation"></p>

<p>Which means natural transformation make up a…</p>

<p>(Oh wait, it’s too early for that, is it?)</p>

<h2 id="natural-isomorphisms">Natural isomorphisms</h2>

<p>After understanding natural transformations, natural isomorphisms, are a no-brainer: a natural transformation is just a family of morphisms in a given category that satisfy certain criteria, then what would a natural <em>isomorphism</em> be? That’s right — it is a family of <em>isomorphisms</em> that satisfy the same criteria. The diagram is the same as the one for ordinary natural transformation, except that $\alpha$ are not just ordinary morphisms, but isomorphisms.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_isomorphism.svg" alt="Two functors and a natural transformation"></p>

<p>And the turning those morphisms into isomorphisms makes the diagram commute in more than one way i.e. if we have the naturality condition</p>

<p>$\alpha \circ F(f) = G(f) \circ \alpha$ i.e. the two paths going from <strong>white</strong> to <strong>blue</strong> are equivalent.</p>

<p>We also have:</p>

<p>$F(f) \circ  \alpha  =   \alpha  \circ G(f)$ i.e. the two paths going from <strong>black</strong> to <strong>yellow</strong> are also equivalent.</p>

<h2 id="constructing-categorical-equivalences">Constructing categorical equivalences</h2>

<p>I am sorry, what were we talking about again? Oh yeah — categorical equivalence. Remember that categorical equivalence is the reason why we tackle natural transformations and isomorphisms? Or perhaps it was the other way around? Never mind, let’s recap what we discussed so far:</p>

<ol>
  <li>
    <p>At the beginning of the section we introduced the notion of equivalence as two functors, such that going from one of them to the other and back again leads you to the same object, or to an <em>object that is isomorphic</em> to the one you started with.</p>
  </li>
  <li>
    <p>And then, we discussed that for categories that are not thin (thick?) the situation is a bit more complex since they can have more than one morphism between two objects, and we should worry not only about isomorphic objects, but about <em>isomorphic morphisms</em>.</p>
  </li>
</ol>

<p>Now, we will show how these two notions are formalized by the definition that we presented.</p>

<blockquote>
  <p>Two <strong>categories</strong> $A$ and $B$ are <strong>equivalent</strong> (or $A \simeq B$) if there exist <em>functors</em> $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g \cong ID_{A}$ and $g \circ f \cong ID_{A}$.</p>
</blockquote>

<p>To understand, this how are the two related, let’s construct the identity functor of the category that we have been using as an example all this time. Note that we are drawing the one and the same category two times (as opposed to just drawing an arrow coming from each object to itself), to make the diagrams more readable.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_identity.svg" alt="The identity functor"></p>

<p>Then, we draw the composite of the two functors that establish an equivalence between the two categories, highlighting the 3 “interesting” objects, i.e. the ones due to which the categories aren’t isomorphic.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_composite.svg" alt="The composite functor between the two functors that make up the equivalence"></p>

<p>Now, we ask ourselves, in which cases does there exist an isomorphism between those two functors?</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_functors.svg" alt="An equivalence diagram"></p>

<p>The answer becomes trivial if we draw the isomorphism arrows connecting the three “interesting” objects in a different way (remember, this is the same category on the top and the bottom) — we can see that these are exactly the arrows that enable us to construct an isomorphism between the two functors (the others are just identity arrows).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_transformation.svg" alt="An equivalence diagram, showing a transformation"></p>

<p>And when would this isomorphism be such that preserves the structure of the category (so that each morphism from the output of the composite functor has an equivalent one in the output of the identity)? Exactly when the isomorphism is <em>natural</em> i.e. when every morphism is mapped to a commuting square, e.g. here is the commuting square of the morphism that is marked in red.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_natural_transformation.svg" alt=" An equivalence diagram, showing a natural transformation"></p>

<p>i.e. naturality condition assures us that the morphisms in the target of the functor behave in the same way as their counterparts in the source.</p>

<p>With this, we are finished with categorical equivalence, but not with natural transformations — natural transformations are a very general concept, and categorical equivalences are only a very narrow case of them.</p>

<h2 id="natural-transformations-in-programming-natural-transformations-on-the-list-functor">Natural transformations in programming. Natural transformations on the list functor</h2>

<p>In the course of this book, we learned that programming/computer science is the study of the category of types in programming languages. However (in order to avoid this being too obvious) in the computer science context, we use different terms for the standard category-theoretic concepts.</p>

<p>We learned that objects are known as <em>types</em>, products and coproducts are, respectively, <em>objects/tuple</em> types and <em>sum</em> types. And, in the last chapter, we learned that functors are known as <em>generic types</em>. Now it’s the time to learn what natural transformations are in this context. They are known as <em>(parametrically) polymorphic functions</em>.</p>

<h2 id="pointed-functors-again">Pointed functors again</h2>

<p>Now, suppose this sounds a bit vague. If only we had some example of a natural transformation in programming, that we can use… But wait, we did show a natural transformation in the previous chapter, when we talked about pointed functors.</p>

<p>That’s right, a functor is pointed when there is a natural transformation between it and the identity functor i.e. to have one green arrow for every object/type.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/pointed_functor.svg" alt="Pointed functor"></p>

<p>And this clearly is a natural transformation. As a matter of fact, if we get down to the nitty-gritty, we would see that it resembles a lot the equivalence diagram that we saw earlier — both transformations involve the identity functor, and both transformations have the same category as source and target, that’s why we can put everything in one circle (we don’t do that in the equivalence diagram, but that’s just a matter of presentation).</p>

<p>Actually, the only difference between the two transformations is that an equivalence is defined by a natural <em>natural isomorphism</em> of a given functors to the identity functor ( $ID \cong f \circ g $ and $ID \cong g \circ f$), while a pointed functor is defined by a one-way <em>natural transformation</em> from the identity functor ($ID \to f $)  i.e. the equivalence functor is pointed, but not the other way around).</p>

<h2 id="polymorphic-functions-as-natural-transformations">Polymorphic functions as natural transformations</h2>

<p>We said that a natural transformation is equivalent to a (parametrically) polymorphic function in programming. But wait, wasn’t natural transformation something else (and much more complicated):</p>

<blockquote>
  <p>Two functors $F$ and $G$ that have the same type signature (so $F : C \to D$ and $G : C \to D$ for some categories $C$ and $D$), and a family of morphisms in the target category $D$ (denoted $\alpha : F \Rightarrow G$) one for each object in $C$. Morphisms that map each object of the target of $F$ (or the image of $F$ in $D$ as it is also called) to some object in the target of $G$.</p>
</blockquote>

<p>Indeed it is (I wasn’t lying to you, in case you are wondering), however, in the case of programming, the source and target categories of both functors are the same category ($Set$), so the whole condition regarding the functors’ type signatures can be dropped.</p>

<blockquote>
  <p>Two <del>functors</del> generic types $F$ and $G$ <del>that have the same type signature</del> and a family of morphisms in $Set$ (denoted $\alpha : Set \Rightarrow Set$) one for each object in $Set$, that map each target object of the functor $F$ (or the image of $F$ in $D$ as it is also called) to some target objects of functor $G$.</p>
</blockquote>

<p>As we know from the last chapter, a functor in programming is a generic type (which, has to have the <code>map</code> function with the appropriate signature).</p>

<p>And what is a “family of morphisms in $Set$ one for each object in $Set$”? Well, the morphisms in the category $Set$ are functions, so that’s just a bunch of functions, one for each type.  In Haskell notation, if we denote a random type by the letter \(a\)), it is $alpha : \forall a. F a \to G a$.  But that’s exactly what polymorphic functions are.</p>

<p>Here is how would we write the above definition in a more traditional language  (we use capital <code>&lt;A&gt;</code> instead of $a$, as customary.</p>

<div><pre><code>
<span>function</span> <span>alpha</span><span>&lt;</span><span>A</span><span>&gt;</span><span>(</span><span>a</span><span>:</span> <span>F</span><span>&lt;</span><span>A</span><span>&gt;</span><span>)</span> <span>:</span> <span>G</span><span>&lt;</span><span>A</span><span>&gt;</span> <span>{</span>
<span>}</span>

</code></pre></div>

<p>Generic types work by replacing the <code>&lt;A&gt;</code> with some concrete type, like <code>string</code>, <code>int</code> etc. Specifically, the natural transformation from the identity functor to the list functor that puts each value in a singleton list looks like this $alpha :: \forall\ a. a \to List\ a$. Or in TypeScript:</p>

<div><pre><code>
<span>function</span> <span>array</span><span>&lt;</span><span>A</span><span>&gt;</span><span>(</span><span>a</span><span>:</span> <span>A</span><span>)</span> <span>:</span> <span>Array</span><span>&lt;</span><span>A</span><span>&gt;</span> <span>{</span>
    <span>return</span> <span>[</span><span>a</span><span>]</span>
<span>}</span>
</code></pre></div>

<h2 id="some-examples-of-natural-transformations">Some examples of natural transformations</h2>

<p>Once we rid ourselves of the feeling of confusion, that such an excessive amount of new terminology and concepts impose upon us (which can take years, by the way), we realize that there are, of course, many polymorphic functions/natural transformations that programmers use.</p>

<p>For example, in the previous chapter, we discussed one natural transformation/polymorphic function the function $\forall a.a \to [a]$ which puts every value in a singleton list. This function is a natural transformation between the identity functor and the list functor.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/pointed_functor_set_transformation.svg" alt="Natural transformation, defining a pointed functor in Set"></p>

<p>This is pretty much the only one that is useful with <em>this</em> signature (the others being $a \to [a, a]$, $a \to [a, a, a]$ etc.), but there are many examples with signature $list\ a \to list\ a$, such as the function to <em>reverse</em> a list.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/reverse_set_transformation.svg" alt="The natural transformation, for reversing a list in Set"></p>

<p>…or <em>take1</em> that retrieves the first element of a list</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/take_set_transformation.svg" alt="The natural transformation, for taking the first element of a list in Set"></p>

<p>or <em>flatten</em> a list of lists of things to a regular list of things (the signature of this one is a little different, it’s $list\ list\ a \to list\ a$).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/flatten_set_transformation.svg" alt="The natural transformation, for flattening a list in Set"></p>

<hr>

<p><strong>Task 3:</strong> Draw example naturality squares of the $reverse$ natural transformation.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/reverse_set_transformation_task.svg" alt="The natural transformation, for reversing a list in Set">
Do the same for the rest of the transformations.</p>

<hr>

<h2 id="the-naturality-condition-1">The naturality condition</h2>

<p>Before, we said that we shouldn’t worry too much about naturality, as it is satisfied every time. Statistically, however, this is not true — as far as I am concerned, about 99.999 percent of transformations aren’t really natural (I wonder if you can compute that percentage properly?). But at the same time, it just so happens (my favourite phrase when writing about maths) that all transformations that we care about <em>are</em> natural.</p>

<p>So, what does the naturality condition entail, in programming? To understand this, we construct some naturality squares of the transformations that we presented.</p>

<p>We choose two types that play the role of $a$, in our case $string$ and $num$ and one natural transformation, like the transformation between the identity functor and the list functor.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/pointed_functor_set.svg" alt="Pointed functor in Set"></p>

<p>The diagram commute when for all functions $f$, applying the $Ff$, the mapped/lifted version of $f$ with one functor (in our case this is just $F f : string \to num$ cause it is the identity functor), followed by ($alpha :: F b \to G\ b$), is equivalent to applying ($alpha:: F a \to G\ a$), and then the mapped version of $f$ with the other functor (in our case $G f :: List\ a \to List\ b$) i.e.</p><p>

\[\alpha \circ F\ f \cong G\ f \circ \alpha\]

</p><p>(in the programming world, you would also see it as something like  $\alpha (map\ f x) = map\ f (\alpha x)$, but note that here $map$ function means two different things on the two sides, Haskell is just smart enough to deduce which $fmap$ to use).</p>

<p>And in TypeScript, when we are talking specifically about the identity functor and the list functor, the equality is expressed as:</p>



<p>So, is this equation true in our case? To verify it, we take one last peak at the world of values.</p>

<p>We acquire an $f$, that is, we a function that acts on simple values (not lists), such as the function $length : string \to num$, which returns the number of characters a string has and convert it, (or <em>lift</em> it, as the terminology goes) to a function that acts on more complex values, using the list functor, (and the higher-order function $map$).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/lifted_function_f.svg" alt="A lifted function"></p>

<p>Then, we take the input and output types for this function (in this case $string$ and $num$), and the two morphisms of a natural transformation (e.g the abstract function $\forall a.a \to [a]$) that correspond to those two types.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/pointed_functor_set_transformations.svg" alt="Pointed functor in Set"></p>

<p>When we compose these two pairs of morphisms we observe that they indeed commute — we get two morphisms that are actually one and the same function.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/pointed_functor_set_internal.svg" alt="Pointed functor in Set"></p>

<p>The above square shows the transformation $\forall a.a \to [a]$ (which is between the identity functor and the list functor, here is another one, this time between the list functor and itself ($\forall a.[a] \to [a]$) — $reverse$</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/reverse_set_internal.svg" alt="Pointed functor in Set"></p>

<p>(and you can see that this would work not just for $length$, but for any other function).</p>

<p>So, why does this happen? Why do these particular transformations make up a commuting square for each and every morphism?</p>

<p>The answer is simple, at least in our specific case: the original, unlifted function $f :: a \to b$ (like our $length :: string \to num$) can only work on the individual values (not with structure), while the natural transformation functions, i.e. ones with signature  $list :: a \to list\ a$ only alter the structure, and not individual values. The naturality condition just says that these two types of functions can be applied in any order that we please, without changing the end result.</p>

<p>This means that if you have a sequence of natural transformations that you want to apply, (such as $reverse$ , $take$, $flatten$ etc) and some lifted functions ($F f$, $F g$), you can mix and match between the two sequences in any way you like and you will get the same result e.g.</p><p>

\[take1 \circ reverse \circ F\ f \circ F\ g\]

</p><p>is the same as</p><p>

\[take1 \circ F\ f \circ reverse \circ F\ g\]

</p><p>…or…</p><p>

\[F\ f \circ F\ g \circ take1 \circ reverse\]

</p><p>…or any other such sequence (the only thing that isn’t permitted is to flip the members of the two sequences — ($take1 \circ reverse$ is of course different from $reverse \circ take1$and if you have $F\ f \circ F\ g$, then $F\ g \circ F\ f$ won’t be permitted at all due to the different type signatures).</p>

<p><strong>Task 4:</strong> Prove the above results, using the formula of the naturality condition.</p>

<h2 id="non-natural-transformations">Non-natural transformations</h2>

<p>“Unnatural”, or “non-natural” transformations (let’s call them just <em>transformations</em>) are mentioned so rarely, that we might be inclined to ask if they exist. The answer is “yes and no”. Why yes? On one hand, transformations, consist of an innumerable amount of morphisms, forming an ever more innumerable amount of squares and obviously nothing stops some of these squares to be non-commuting.</p>

<p>For example, if we substitute one morphism from the family of morphisms that make up the natural transformation with some other random morphism that has the same signature, all squares that have this morphism as a component would stop commuting.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/unnatural_transformation_squares.svg" alt="Unnatural transformation"></p>

<p>This would result in something like an “almost-natural” transformation (e.g. an abstract function that reverses all lists, except lists of integers).</p>

<p>And in the category of sets, where morphisms are functions i.e. mappings between values, it is enough to move just one arrow of just one of those values in order to make the transformation “unnatural” (e.g. a function which reverses all lists, but one specific list).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/reverse_set_unnatural.svg" alt="Unnatural transformation in set --- like reverse, but one arrow is off"></p>

<p>Finally, if can just gather a bunch of random morphisms, one for each object, that fit the criteria, we get what I would call a “perfectly unnatural transformation” (but this is my terminology).</p>

<p>But, although they do exist, it is very hard to define non-natural transformations. For example, for categories that are <em>infinite</em>, there is no way to specify such “perfectly unnatural transformation” (ones where none of the squares commute) without resorting to randomness. And even transformations on finite categories, or the “semi-natural” transformations which we described above (the ones that include a single condition for a single value or type), are not possible to specify in some languages e.g. you can define such a transformation in Typescript, but not in Haskell.</p>

<p>To see why, let’s see what the type of a natural transformation is.</p><p>

\[\forall\ a.\ F a \to G a\]

</p><p>The key is that the definition should be valid <em>for all</em> types a. For this reason, there is no way for us to specify a different arrows for different types, without resorting to type downcasting, which is not permitted in languages like Haskell (as it breaks the principle of parametricity).</p>

<!--

-->

<h2 id="natural-transformations-again">Natural transformations again</h2>

<p>Now, after we saw the definition of natural transformations, it is time to see the definition of natural transformations (and if you feel that the quality of the humour in this book is deteriorating, that’s only because <em>things are getting serious</em>).</p>

<p>Let’s review again the commuting diagram that represents a natural transformation.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_functors.svg" alt="Two functors"></p>

<p>This diagram might prompt us into viewing natural transformations as some kind of “two-arrow functors” that have not one but two arrows coming from each of their morphisms — this notion, can be formalized, by using <em>product categories</em>.</p>

<p>Oh wait, I just realized we never covered product categories… but don’t worry, we will cover them now.</p>

<h2 id="product-groups-and-product-categories">Product groups and product categories</h2>

<p>We haven’t covered product categories, however some pages ago, when we covered monoids and groups, we talked about the concept of a <em>product group</em>. The good news is that product <em>categories</em> are a generalization of product <em>groups</em>…</p>

<p>The bad news is that you probably don’t remember much about product groups, as covered them briefly.</p>

<p>But don’t worry, we will do a more in-depth treatment now:</p>

<h2 id="product-groups">Product groups</h2>

<p>Given two groups $G$ and $H$, whose sets of elements can also be denoted $G$ and $H$…</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/groups_product.svg" alt="The Klein four as a product group"></p>

<p>(in this example we use two boolean groups, which we visualize as the groups of horizontal and vertical rotation of a square)</p>

<p>…the <em>product group</em> of these two groups is a group that has the cartesian product of these two sets $G \times H$ as its set of elements.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/klein_four_underlying_set.svg" alt="The Klein four as a product group"></p>

<p>And what can the group operation of such a group be? Well, I would say that out of the few possible groups operations for this set that <em>exist</em>, this is the <em>only</em> operation that is <em>natural</em> (I didn’t intend to involve natural transformation at this section, but they really do appear everywhere). So, let’s try to derive the operation of this group.</p>

<p>We know what a group operation is, in principle: A group operation combines two elements from the group into a third element i.e. it is a function with the following type signature:</p><p>

\[\circ :  (A, A) \to A\]

</p><p>or equivalently</p><p>

\[\circ :  A \to A \to A\]

</p><p>And for product groups, we said that the underlying set of the group (which we dubbed $A$ above) is a cartesian product of some other two sets which we dubbed $G$ and $H$. So, when we swap $A$ for $G \times H$ the definition becomes:</p><p>

\[\circ : G \times H \to G \times H \to G \times H\]

</p><p>i.e. the group operation takes one pair of elements from $G$ and $H$ and another pair of elements from $G$ and $H$, only to return — guess what — a pair of elements $G$ and $H$.</p>

<p>Let’s take an example. To avoid confusion, we take two totally different groups — the color-mixing group and the group of integers under addition. That would mean that a value of $G \times H$ would be a pair, containing a random color and a random number, and the operation would combine two combine two such pairs and produce another one.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_group_equations.svg" alt="Equations of the product of numbers and colors"></p>

<p>Now, the operation must produce a pair, containing a number and a color. Furthermore, it would be good if it produces a number <em>by using those two numbers</em>, not just picking one at random, and likewise for colors. And furthermore, we want it to work not just for monoids of numbers and colors, but all other monoids that can be given to us. It is obvious that there is only one solution, to get the elements of the new pair by combining the elements of the pairs given.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_group_solutions.svg" alt="Solutions of the product of numbers and colors"></p>

<p>And the operation of the product group of the two boolean groups which we presented earlier is the combination of the two operations</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/klein_four_as_product.svg" alt="The Klein four as a product group"></p>

<p>So, the general definition of the operation is the following ($g1$, $g2$ are elements of $G$ and $h1$ and $h2$ elements of $H$).</p><p>

\[(g1, h1) \circ (g2, h2) = ( (g1 \circ g2), (h1 \circ h2))\]

</p><p>And that are product groups.</p>

<h2 id="product-categories">Product categories</h2>

<p>We are back at tackling product <em>categories</em>.</p>

<p>Since we know what product <em>groups</em> are, and we know that groups are nothing but categories with just one object (and the group objects are the category’s morphisms, remember?), we are already almost there.</p>

<p>Here is a way to make a product category.</p>

<p>Take any two categories:</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_components.svg" alt="Product category - components"></p>

<p>Then take the set of all possible pairs of the objects of these categories.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_set.svg" alt="Product category - objects"></p>

<p>And, finally, we make a category out of that set by taking all morphisms coming from any of the two categories and replicate them to all pairs that feature some objects from their type signature, in the same way as we did for product groups (in this example, only one of the categories has morphisms).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_category.svg" alt="Product category"></p>

<p>This is the <em>product category</em> of the two categories.</p>

<h2 id="natural-transformations-as-functors-of-product-categories">Natural transformations as functors of product categories</h2>

<p>In this section we are interested with the products of one particular category, namely the category we called $2$, containing two objects and one morphism (stylishly represented in black and white).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/category_two.svg" alt="The category 2"></p>

<p>This category is the key to constructing a functor that is equivalent to a natural transformation:</p>

<ul>
  <li>Because it has two objects, it produces two copies of the source category.</li>
  <li>because the two objects are connected, the two copies are connected in the same way as the two “images” in the target category are connected.</li>
</ul>

<p>So, given a product category of $2$ and some other category $C$…</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_category_target_category.svg" alt="The category 2"></p>

<p>…there exist a natural transformation between $C$ and the product category $2\times C$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_category_natural_transformation.svg" alt="Product category"></p>

<p>Furthermore, this connection is two-way: any natural transformation from $C$ to some other category (call it $D$, as it is customary) can be represented as a functor $2 \times C \to D$.</p>

<p>That is, if we have a natural transformations $\alpha : F \Rightarrow G$ (where  $F: C \to D$ and  $G: C \to D$), then, we also have a functor  $2 \times C \to D$, such that if we take the subcategory of $2 \times C$ comprised of just those objects that have the $0$ object as part of the pair, and the morphisms between them, we get a functor that is equivalent to $F$, and if we consider the subcategory that contains $1$, then the functor is equivalent to $G$ (we write $\alpha(-,0)=F$ and $\alpha(-,1)=G$). Et voilà!</p>

<p><strong>Task 5:</strong> Show that the two definitions are equivalent.</p>

<p>This perspective helps us realize that a natural transformation can be viewed as a collection of commuting squares. The source functor defines the left-hand side of each square, the target functor — the right-hand side, and the transformation morphisms join these two sides.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_transformation_notation.svg" alt="Notation for natural transformation"></p>

<p>We can even retrieve the structure of the source category of these functors, which (as categories are by definition structure and nothing more) is equivalent to retrieving the category itself.</p>

<!--

-->

<h2 id="composing-natural-transformations">Composing natural transformations</h2>

<p>Natural transformations are surely a different beast than normal morphisms and functors and so they don’t compose in the same way. However, they do compose and here we will show how.</p>

<h2 id="the-identity-natural-transformation">The identity natural transformation</h2>

<p>Let’s first get one trivial definition out of the way: for each functor, we have the identity natural transformation (actually a natural isomorphism) between it and itself.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/identity_natural_transformation.svg" alt="The identity natural transformation"></p>

<h2 id="horizontal-composition">Horizontal composition</h2>

<p>The setup for composing natural transformations may look complicated the first time you see it: we need three categories $C$, $D$ and $E$ (just as composition of morphisms requires three objects). We need a total of four functors, distributed on two pairs, one pair of functors that goes from $C$ to $D$ and one that goes from $D$ to $E$ (so we can compose these two pairs of functors together, to get a new pair of functors that go $C \to E$). However, we will try to keep it simple and we will treat the natural transformation as a map from a morphism to a commuting square. As we showed above, this mapping already contains the two functors in itself.</p>

<p>So, let’s say that we have the natural transformation $\alpha$ involving the $C \to D$ functors (which we usually call $F$ and $G$).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition_notation.svg" alt="Notation for natural transformation"></p>

<p>So, what will happen if we have one more transformation $\bar\alpha$ involving the functors that go $D \to E$ (which are labelled $F’$ and $G’$)? Well, since a natural transformation maps each morphism to a square, and a square contains four morphisms (two projections by the two functors and two components of the transformation), a square would be mapped to four squares.</p>

<p>Let’s start by drawing two of them for each projection of the morphism in $C$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition_squares.svg" alt="Horizontal composition of natural transformation"></p>

<p>We have to have two more squares, corresponding to the two morphisms that are the components of the $\alpha$ natural transformation. However, these morphisms connect the objects that are the target of the two functors, objects that we already have on our diagram, so we just have to draw the connections between them.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition.svg" alt="Horizontal composition of natural transformation"></p>

<p>The result is an interesting structure which is sometimes visualized as a cube.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition_cube.svg" alt="Horizontal composition of natural transformation"></p>

<p>More interestingly, when we compose the commuting squares from the sides of the cube horizontally, we see that it contains not one, but two bigger commuting squares (they look like <em>rectangles</em> in this diagram), visualized in grey and red. Both of them connect morphisms $F’Ff$ and $G’Gf$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition_cube_commuting.svg" alt="Horizontal composition of natural transformation"></p>

<p>So, there is a natural transformation between the composite functor $F’ \circ F : C \to E$ and $G’ \circ G : C \to E$ — a natural transformation that is usually marked $\bar\alpha \bullet \alpha$ (with a black dot).</p>

<p><strong>Task 6:</strong> Show that natural transformations indeed compose i.e. that if you have natural transformations $F’Ff \Rightarrow F’Gf$  and  $F’Gf \Rightarrow G’Gf$ you have $F’Ff \Rightarrow G’Gf$.</p>

<h2 id="whiskering">Whiskering</h2>

<p>And an interesting special case of horizontal composition is horizontal composition involving the identity natural transformation: given a natural transformation $\bar\alpha$ involving functors with signature $D \to E$ and some functor with signature $F : C \to D$, we can take $\alpha$ to be the identity natural transformation between functor $F$ and itself and compose it with $\bar\alpha$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition_whiskering.svg" alt="Horizontal composition of natural transformation"></p>

<p>We get a new natural transformation $\bar\alpha \bullet \alpha$, that is practically the same as the one we started with (i.e. the same as $\bar\alpha$) so what’s the deal? We just found a way to <em>extend</em> natural transformations, using functors: i.e   we can use a functor with signature $C \to D$ to extend a $D \to E$ natural transformation and make it $C \to E$.</p>

<p><strong>Task 7</strong>: Try to extend the natural transformation in the other direction (by taking $\bar\alpha$ to be identity).</p>

<p>So, this is how you compose natural transformations. It’s too bad that this is form of composition is different from the standard categorical composition. So, I guess natural transformations do not form a category, like we hoped they would…</p>

<p>Well, OK, there is actually another way of composing categories, which might actually work.</p>

<h2 id="vertical-composition">Vertical composition</h2>

<p>Recall that categorical composition involves three objects and two successive arrows between them. For vertical composition of natural transformations, we will need three (or more) <em>functors</em> with the same type signature, say $F, G, H: C \to D$ i.e. (same source and target category) and two successive <em>natural transformations</em> between those functors i.e. $\alpha: F \to G$ and $\beta: G \to H$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/vertical_composition.svg" alt="Vertical composition of natural transformations"></p>

<p>We can combine each morphism of the natural transformation $\alpha$ (e.g. $a: F \to G$) and the corresponding morphism of the natural transformation $\beta$ (say $b:G \to H$) to get a new morphism, which we call $b \circ a : F \to H$ (the composition operator is the  usual white circle, as opposed to the black one, which denotes horizontal composition). And the set of all such morphisms are precisely the components of a new natural transformation: $\beta \circ \alpha : F \to H$.</p>

<h2 id="categories-of-functors">Categories of functors</h2>

<p>Now, we are approaching the end of the chapter, we will introduce our category and call it quits. To do that, we first introduce a more compressed notation for vertical composition of natural transformations (where they do indeed look vertical).</p>

<p>We started this chapter by looking at category of sets and using internal diagrams, displaying the set elements as points and the sets/objects as collections.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/vertical_composition_internal.svg" alt="Vertical composition of natural transformations - internal diagram"></p>

<p><strong>Task 8:</strong> identify the function, the three functors, and the two natural transformations used in this diagram.</p>

<!--
answer
(A little note, if you want to understand the diagram better: $F$ and $G$ are the $List$ functor, $H$ is the $ID$ functor, $\alpha$ is  $reverse: List \to List$ and $\beta$ is $head : List \to ID$ and $f$ is $length : string \to int$)
-->

<p>Then, we quickly passed to normal external diagrams, where objects are points and categories are collections.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/vertical_composition.svg" alt="Vertical composition of natural transformations"></p>

<p>And now we go one more level further, and show the category of categories, where categories are points and functors are morphisms.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/vertical_composition_cat.svg" alt="Vertical composition of natural transformations in Cat"></p>

<p>In this notation, we display natural transformations as (double) arrows between morphisms.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/vertical_composition_cat_2.svg" alt="Vertical composition of natural transformations in Cat"></p>

<p>And you can already see the new category that is formed: For each two categories (like $C$ and $D$ in this case), there exists a category which has functors for objects and natural transformations as morphisms.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/functor_category.svg" alt="Vertical composition of natural transformations in Cat"></p>

<p>Natural transformations compose with vertical compositions, and, of course, the identity natural transformation is the identity morphism.</p>

<h2 id="interchange-law">Interchange law</h2>

<p>Vertical and horizontal composition of natural transformations are related to each other in the following way:</p>

<p>If we have (as we had) two successive natural transformations, in the vertical sense, like $\alpha: F \to G$ and $\beta: G \to H$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/interchange_law_horizontal.svg" alt="The interchange law -- horizontal component"></p>

<p>And two successive ones, this time in horizontal sense e.g. $\bar\alpha: F’ \to G’$ and $\bar\beta: G’ \to H’$. (note that $\alpha$ has nothing to do with $\bar\alpha$ as $\beta$ has nothing to do with $\bar\beta$, we just call them that way to avoid using too many letters)</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/interchange_law_vertical.svg" alt="The interchange law -- vertical component"></p>

<p>And if the two pairs of natural transformations both start from the same category and the same functor, then the compositions of the two pairs of natural transformations obey the following law</p><p>

\[(β \circ α) \bullet (\bar β \circ \bar α) = (β \bullet \bar β) \circ (α \bullet \bar α)\]

</p><hr>

<p><strong>Task 9:</strong> Draw the paths of the two compositions of the transformations (on the two sides of the equation) and ensure that they indeed lead to the same place.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/interchange_law.svg" alt="The interchange law"></p>

<hr>

<h2 id="2-categories">2-Categories</h2>

<p>At this point you might be wondering the following (although statistically you are more likely to wonder what the heck is all this about): We know that all categories are objects of $Cat$, the category of small categories, in which functors play the role of morphisms.</p>

<p>But, functors between given categories also form a category, under vertical composition. Which means that $Cat$ not only has (as any other category) morphisms between objects, <em>but</em> also has <em>morphisms between morphisms</em>. And furthermore, those two types of morphisms compose in this very interesting way.</p>

<p>So, what does that make of $Cat$? I don’t know, perhaps we can call natural transformations “2-morphisms” and $Cat$ is some kind of “2-category”?</p>

<p>But wait, actually it’s way too early for you to find out. We haven’t even covered limits…</p>

<!--

-->


        
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Type Theory and Functional Programming (1999) [pdf] (172 pts)]]></title>
            <link>https://www.cs.cornell.edu/courses/cs6110/2015sp/textbook/Simon%20Thompson%20textbook.pdf</link>
            <guid>45435100</guid>
            <pubDate>Wed, 01 Oct 2025 07:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs.cornell.edu/courses/cs6110/2015sp/textbook/Simon%20Thompson%20textbook.pdf">https://www.cs.cornell.edu/courses/cs6110/2015sp/textbook/Simon%20Thompson%20textbook.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45435100">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[High-resolution efficient image generation from WiFi Mapping (123 pts)]]></title>
            <link>https://arxiv.org/abs/2506.10605</link>
            <guid>45434941</guid>
            <pubDate>Wed, 01 Oct 2025 06:33:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2506.10605">https://arxiv.org/abs/2506.10605</a>, See on <a href="https://news.ycombinator.com/item?id=45434941">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2506.10605">View PDF</a>
    <a href="https://arxiv.org/html/2506.10605v3">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDM's denoising diffusion model to the latent representation with text-based guidance before decoding using the LDM's pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Eshan Ramesh [<a href="https://arxiv.org/show-email/ba45c6b6/2506.10605" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2506.10605v1" rel="nofollow">[v1]</a></strong>
        Thu, 12 Jun 2025 11:47:23 UTC (6,672 KB)<br>
            <strong><a href="https://arxiv.org/abs/2506.10605v2" rel="nofollow">[v2]</a></strong>
        Fri, 4 Jul 2025 12:27:28 UTC (6,672 KB)<br>
    <strong>[v3]</strong>
        Fri, 5 Sep 2025 11:39:36 UTC (9,960 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>