<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 16 Aug 2023 22:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Luck be a Landlord is now banned in 13 countries on the Google Play Store (104 pts)]]></title>
            <link>https://blog.trampolinetales.com/luck-be-a-landlord-is-now-banned-in-13-countries-on-the-google-play-store/</link>
            <guid>37152133</guid>
            <pubDate>Wed, 16 Aug 2023 19:12:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.trampolinetales.com/luck-be-a-landlord-is-now-banned-in-13-countries-on-the-google-play-store/">https://blog.trampolinetales.com/luck-be-a-landlord-is-now-banned-in-13-countries-on-the-google-play-store/</a>, See on <a href="https://news.ycombinator.com/item?id=37152133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
<main>

        <article>

    

    <div>
        <p>I'm sorry if the following post comes off as a bit angry. I'm writing this off the cuff and am very aggravated by the situation.</p><p>Today I received an email from Google Play Support stating that Luck be a Landlord has been geo-blocked in the following countries: United Arab Emirates, Algeria, Iran, Jordan, South Korea, Libya, Oman, Palestine, Qatar, Saudi Arabia, Syria, Tunisia, and Yemen.</p><p>According to Google, the app "contains content that doesn't comply with the Gambling policy."</p><p>It should go without saying that I 100% disagree with this decision. Luck be a Landlord does not violate any gambling policy that Google has in their terms of service.</p><p>The e-mail states that "Apps that simulate gambling, or games of chance or skill that are conducive to gambling are prohibited in the above locales."</p><p>By that logic, you could argue any game with an element of chance or luck violates Google's Gambling policy.</p><p><a href="https://play.google.com/store/apps/details?id=com.halfbrick.jetpackjoyride&amp;ref=blog.trampolinetales.com">Jetpack Joyride</a>, a game with a literal slot-machine mechanic, is rated E10+ and isn't geo-blocked. <a href="https://play.google.com/store/apps/details?id=com.terrycavanaghgames.diceydungeons&amp;ref=blog.trampolinetales.com">Dicey Dungeons</a>, a game with themes of luck and rolling dice, isn't geo-blocked. A <a href="https://play.google.com/store/apps/details?id=com.sneakypanda.spincraftclassic&amp;ref=blog.trampolinetales.com">literal clone of my game</a>, which raised <a href="https://venturebeat.com/games/sneaky-panda-raises-6m-to-launch-new-kind-of-mobile-game/?ref=blog.trampolinetales.com">$6,000,000 in venture capital</a>, is somehow rated "E for Everyone" and isn't geo-blocked, despite <strong>literally having a battle pass and random-chance micro-transactions</strong>. Don't even get me started on how loot boxes are allowed (and encouraged) on these platforms!</p><p>It should go without saying that I don't think these other apps should be geo-blocked. I'm saying it's very easy to see that my game is getting unfair treatment.</p><p>I have brought this up to Google and they were dismissive of my reasoning, refusing to do anything.</p><p>I am exploring options of how to get this fixed but I am extremely upset that Google has made this ridiculous decision and that players in the aforementioned countries will be unable to download the game on the Google Play Store.</p><p>My apologies for those inconvenienced by this who were interested in playing Luck be a Landlord. The <a href="https://store.steampowered.com/app/1404850/Luck_be_a_Landlord/?ref=blog.trampolinetales.com">Steam version</a> and <a href="https://apps.apple.com/us/app/luck-be-a-landlord/id6450724928?ref=blog.trampolinetales.com">iOS version</a> of the game remain up without any issue at the time of me writing this.</p><p>-Dan</p>
    </div>

    

</article>
            

        
</main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New York City bans TikTok for government employees (112 pts)]]></title>
            <link>https://www.engadget.com/new-york-city-bans-tiktok-for-government-employees-174806575.html</link>
            <guid>37151308</guid>
            <pubDate>Wed, 16 Aug 2023 18:16:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/new-york-city-bans-tiktok-for-government-employees-174806575.html">https://www.engadget.com/new-york-city-bans-tiktok-for-government-employees-174806575.html</a>, See on <a href="https://news.ycombinator.com/item?id=37151308">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>New York City will ban TikTok from government devices, <a data-i13n="cpos:1;pos:1" href="https://www.theverge.com/2023/8/16/23834579/nyc-tiktok-ban-new-york-china-surveillance-spy" rel="nofollow noopener" target="_blank" data-ylk="elm:context_link;cpos:1;pos:1;itc:0"><em>The Verge reported</em></a> on Wednesday. City agencies have 30 days to remove the ByteDance-owned app from their devices. Employees will not be allowed to download or use TikTok on their city-sanctioned tech effective immediately. This comes three years after New York state banned TikTok from government devices in 2020, <a data-i13n="cpos:2;pos:1" href="https://www.timesunion.com/state/article/n-y-quietly-banned-tiktok-government-devices-17886372.php" rel="nofollow noopener" target="_blank" data-ylk="elm:context_link;cpos:2;pos:1;itc:0">according to <em>Times-Union</em></a>.</p>
<p>NYC Cyber Command, a subset of the Office of Technology and Innovation, spurred the decision after reporting to the city that TikTok posed a security threat. "NYC Cyber Command regularly explores and advances proactive measures to keep New Yorkers' data safe," a City Hall spokesperson said. "As part of these ongoing efforts, NYC Cyber Command determined that the TikTok application posed a security threat to the city’s technical networks and directed its removal from city-owned devices.”</p>
<p>Other states and localities, <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/montanas-governor-signs-bill-banning-tiktok-225326086.html" data-ylk="elm:context_link;cpos:3;pos:1;itc:0">notably Montana</a>, have made waves banning TikTok more generally across the jurisdiction. But on a wider scale, most legislators have taken an approach banning the app for government employees, <a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/tiktok-us-government-ban-devices-omnibus-bill-funding-nasa-234803320.html" data-ylk="elm:context_link;cpos:4;pos:1;itc:0">including the federal government</a>. Thirty-three states across parties lines <a data-i13n="cpos:5;pos:1" href="https://www.cnn.com/2023/01/16/tech/tiktok-state-restrictions/index.html" rel="nofollow noopener" target="_blank" data-ylk="elm:context_link;cpos:5;pos:1;itc:0">now have restrictions on the use of TikTok</a> on government-owned tech.</p>
<p>As legislation continues to resurface considering a total ban on TikTok and other apps affiliated with the Chinese government, ByteDance <a data-i13n="cpos:6;pos:1" href="https://www.engadget.com/can-tiktok-convince-the-us-its-not-a-national-security-threat-173030115.html" data-ylk="elm:context_link;cpos:6;pos:1;itc:0">fights to proven that its not a threat to national security</a>. TikTok CEO Shou Chew <a data-i13n="cpos:7;pos:1" href="https://www.engadget.com/heres-what-tiktoks-ceo-told-congress-about-the-apps-ties-to-china-and-teen-safety-201657076.html" data-ylk="elm:context_link;cpos:7;pos:1;itc:0">even testified in front of Congress</a> reiterating that "ByteDance is not an agent of China."</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Windows feature that resets system clocks based on random data is wreaking havoc (148 pts)]]></title>
            <link>https://arstechnica.com/security/2023/08/windows-feature-that-resets-system-clocks-based-on-random-data-is-wreaking-havoc/</link>
            <guid>37151220</guid>
            <pubDate>Wed, 16 Aug 2023 18:11:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/security/2023/08/windows-feature-that-resets-system-clocks-based-on-random-data-is-wreaking-havoc/">https://arstechnica.com/security/2023/08/windows-feature-that-resets-system-clocks-based-on-random-data-is-wreaking-havoc/</a>, See on <a href="https://news.ycombinator.com/item?id=37151220">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/broken-clock-800x680.jpg" alt="Windows feature that resets system clocks based on random data is wreaking havoc">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 12:single/related:ead7c219d0326d0abc616e5bfc87896a --><!-- empty -->
<p>A few months ago, an engineer in a data center in Norway encountered some perplexing errors that caused a Windows server to suddenly reset its system clock to 55 days in the future. The engineer relied on the server to maintain a routing table that tracked cell phone numbers in real time as they were being moved from one carrier to the other. A jump of eight weeks had dire consequences because it caused numbers that had yet to be transferred to be listed as having already been moved and numbers that had already been transferred to be reported as pending.</p>
<p>“With these updated routing tables, a lot of people were unable to make calls, as we didn't have a correct state!” the engineer, who asked to be identified only by his first name, Simen, wrote in an email. “We would route incoming and outgoing calls to the wrong operators! This meant, e.g., children could not reach their parents and vice versa.”</p>
<h2>A show-stopping issue</h2>
<p>Simen had experienced a similar error last August when a machine running Windows Server 2019 reset its clock to January 2023 and then changed it back a short time later. Troubleshooting the cause of that mysterious reset was hampered because the engineers didn’t discover it until after event logs had been purged. The newer jump of 55 days, on a machine running Windows Server 2016, prompted him to once again search for a cause, and this time, he found it.</p>
<p>The culprit was a little-known feature in Windows known as Secure Time Seeding. Microsoft <a href="https://learn.microsoft.com/en-us/archive/blogs/w32time/secure-time-seeding-improving-time-keeping-in-windows">introduced</a> the time-keeping feature in 2016 as a way to ensure that system clocks were accurate. Windows systems with clocks set to the wrong time can cause disastrous errors when they can’t properly parse time stamps in digital certificates or they execute jobs too early, too late, or out of the prescribed order. Secure Time Seeding, Microsoft said, was a hedge against failures in the battery-powered on-board devices designed to keep accurate time even when the machine is powered down.</p>                                            
                                                        
<p>“You may ask—why doesn’t the device ask the nearest time server for the current time over the network?” Microsoft engineers wrote. “Since the device is not in a state to communicate securely over the network, it cannot obtain time securely over the network as well, unless you choose to ignore network security or at least punch some holes into it by making exceptions.”</p>
<p>To avoid making security exceptions, Secure Time Seeding sets the time based on data inside an SSL handshake the machine makes with remote servers. These handshakes occur whenever two devices connect using the Secure Sockets Layer protocol, the mechanism that provides encrypted HTTPS sessions (it is also known as <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security">Transport Layer Security</a>). Because Secure Time Seeding (abbreviated as STS for the rest of this article) used SSL certificates Windows already stored locally, it could ensure that the machine was securely connected to the remote server. The mechanism, Microsoft engineers wrote, “helped us to break the cyclical dependency between client system time and security keys, including SSL certificates.”</p>
<p>Simen wasn’t the only person encountering wild and spontaneous fluctuations in Windows system clocks used in mission-critical environments. Sometime last year, a separate engineer named Ken began seeing similar time drifts. They were limited to two or three servers and occurred every few months. Sometimes, the clock times jumped by a matter of weeks. Other times, the times changed to as late as the year 2159.</p>
<p>“It has exponentially grown to be more and more servers that are affected by this,” Ken wrote in an email. “In total, we have around 20 servers (VMs) that have experienced this, out of 5,000. So it's not a huge amount, but it is considerable, especially considering the damage this does. It usually happens to database servers. When a database server jumps in time, it wreaks havoc, and the backup won’t run, either, as long as the server has such a huge offset in time. For our customers, this is crucial.”</p>
<p>Simen and Ken, who both asked to be identified only by their first names because they weren’t authorized by their employers to speak on the record, soon found that engineers and administrators had been reporting the same time resets since 2016.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Mathematics of Training LLMs (101 pts)]]></title>
            <link>https://www.latent.space/p/transformers-math#details</link>
            <guid>37150000</guid>
            <pubDate>Wed, 16 Aug 2023 16:59:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latent.space/p/transformers-math#details">https://www.latent.space/p/transformers-math#details</a>, See on <a href="https://news.ycombinator.com/item?id=37150000">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em><span>Invites are going out for </span><a href="https://ai.engineer/" rel="">AI Engineer Summit</a><span>! In the meantime, we have just announced our first </span><a href="https://partiful.com/e/jLALhobyikO5xq2JDDnm" rel="">Actually Open AI event</a><span> with Brev.dev and Langchain, Aug 26 in our SF HQ (we’ll record talks for those remote). See you soon (and join the Discord)!</span></em></p><p><em><span>Special thanks to </span><a href="https://twitter.com/nearcyan/status/1662937711156625408?s=20" rel="">@nearcyan</a><span> for helping us arrange this with the Eleuther team.</span></em></p><p><span>As startups and even VCs </span><a href="https://www.latent.space/p/ai-engineer" rel="">hoard GPUs</a><span> to attract talent, </span><strong>the one thing more valuable than GPUs is knowing how to use them</strong><span> (aka, </span><a href="https://horace.io/brrr_intro.html" rel="">make </a><em><a href="https://horace.io/brrr_intro.html" rel="">GPUs go brrrr</a><span>).</span></em></p><p><span>There is an incredible amount of </span><a href="https://commoncog.com/the-tacit-knowledge-series/" rel="">tacit knowledge</a><span> in the NLP community around training</span></p><p><span>, and until Eleuther.ai came along you pretty much had to work at Google or Meta to gain that knowledge. This makes it hard for non-insiders to even do simple estimations around costing out projects - it is well known how to trade $ for GPU hours</span></p><p><span>, but trading “$ for size of model” or “$ for quality of model” is less known and more valuable and full of opaque “it depends”. This is why </span><strong>rules of thumb for training</strong><span> are incredibly useful, because they cut through the noise and give you the simple 20% of knowledge that determines 80% of the outcome derived from hard earned experience.</span></p><p><span>Today’s guest, Quentin Anthony from EleutherAI, is one of the top researchers in </span><strong>high-performance deep learning</strong><span>. He’s one of the co-authors of </span><a href="https://blog.eleuther.ai/transformer-math/" rel="">Transformers Math 101</a><span>, which was one of the clearest articulations of training rules of thumb. We can think of no better way to dive into training math than to have Quentin run us through a masterclass on model weights, optimizer states, gradients, activations, and how they all impact memory requirements.</span></p><p>The core equation you will need to know is the following:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef9fba18-bb49-462c-b411-e0861730c88d_296x76.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef9fba18-bb49-462c-b411-e0861730c88d_296x76.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef9fba18-bb49-462c-b411-e0861730c88d_296x76.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef9fba18-bb49-462c-b411-e0861730c88d_296x76.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef9fba18-bb49-462c-b411-e0861730c88d_296x76.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef9fba18-bb49-462c-b411-e0861730c88d_296x76.png" width="248" height="63.67567567567568" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ef9fba18-bb49-462c-b411-e0861730c88d_296x76.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:76,&quot;width&quot;:296,&quot;resizeWidth&quot;:248,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef9fba18-bb49-462c-b411-e0861730c88d_296x76.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef9fba18-bb49-462c-b411-e0861730c88d_296x76.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef9fba18-bb49-462c-b411-e0861730c88d_296x76.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef9fba18-bb49-462c-b411-e0861730c88d_296x76.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Where </span><em><strong>C</strong></em><span> is the compute requirements to train a model, </span><em><strong>P</strong></em><span> is the number of parameters, and </span><em><strong>D</strong></em><span> is the size of the training dataset in tokens. This is also equal to </span><em><strong>τ</strong></em><span>, the throughput of your machine measured in FLOPs (Actual FLOPs/GPU * # of GPUs), multiplied by </span><em><strong>T</strong></em><span>, the amount of time spent training the model.</span></p><p>Taking Chinchilla scaling at face value, you can simplify this equation to be `C = 120(P^2)`.These laws are only true when 1000 GPUs for 1 hour costs the same as 1 GPU for 1000 hours, so it’s not always that easy to make these assumptions especially when it comes to communication overhead.  </p><p>There’s a lot more math to dive into here between training and inference, which you can listen to in the episode or read in the articles. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png" width="1456" height="726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;ZeRO illustration&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="ZeRO illustration" title="ZeRO illustration" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b03a3d-8e72-4020-b686-f145e1e6076f_2000x997.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The other interesting concept we covered is distributed training and strategies such as ZeRO and 3D parallelism. As these models have scaled, it’s become impossible to fit everything in a single GPU for training and inference. We leave these advanced concepts to the end, but there’s a lot of innovation happening around sharding of params, gradients, and optimizer states that you must know is happening in modern LLM training. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png" width="960" height="49" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:49,&quot;width&quot;:960,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;ZeRO legend&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="ZeRO legend" title="ZeRO legend" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7c82ff-b34f-45d9-8035-26316b6a9af9_960x49.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png" width="1209" height="699" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:699,&quot;width&quot;:1209,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;3D parallelism&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="3D parallelism" title="3D parallelism" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c858394-83c8-44ea-842c-38bd6f1a848a_1209x699.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>If you have questions, you can join the </span><a href="https://discord.gg/xyUJzrKV" rel="">Eleuther AI Discord</a><span> or follow </span><a href="https://twitter.com/QuentinAnthon15" rel="">Quentin on Twitter</a><span>. </span></p><ul><li><p><strong><a href="https://blog.eleuther.ai/transformer-math/" rel="">Transformers Math 101 Article</a></strong></p></li><li><p><a href="https://www.eleuther.ai/" rel="">Eleuther.ai</a></p></li><li><p><a href="https://huggingface.co/EleutherAI/gpt-neox-20b" rel="">GPT-NeoX 20B</a></p></li><li><p><a href="https://huggingface.co/docs/transformers/model_doc/bloom" rel="">BLOOM</a></p></li><li><p><a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" rel="">Turing NLG</a></p></li><li><p><a href="https://mosaicml.com/" rel="">Mosaic</a></p></li><li><p><a href="https://www.ornl.gov/news/frontier-supercomputer-debuts-worlds-fastest-breaking-exascale-barrier" rel="">Oak Ridge &amp; Frontier Supercomputer</a></p></li><li><p><a href="https://www.olcf.ornl.gov/summit/" rel="">Summit Supercomputer </a></p></li><li><p><a href="https://www.llnl.gov/" rel="">Lawrence Livermore Lab</a></p></li><li><p><a href="https://arxiv.org/abs/2305.13048" rel="">RWKV</a></p></li><li><p><a href="https://www.latent.space/p/flashattention#details" rel="">Flash Attention </a></p></li><li><p><a href="https://twitter.com/StasBekman" rel="">Stas Bekman</a></p></li></ul><ul><li><p>[00:00:00] Quentin's background and work at Eleuther.ai</p></li><li><p>[00:03:14] Motivation behind writing the Transformers Math 101 article</p></li><li><p>[00:05:58] Key equation for calculating compute requirements (tau x T = 6 x P x D)</p></li><li><p>[00:10:00] Difference between theoretical and actual FLOPs</p></li><li><p>[00:12:42] Applying the equation to estimate compute for GPT-3 training</p></li><li><p>[00:14:08] Expecting 115+ teraflops/sec per A100 GPU as a baseline</p></li><li><p>[00:15:10] Tradeoffs between Nvidia and AMD GPUs for training</p></li><li><p>[00:18:50] Model precision (FP32, FP16, BF16 etc.) and impact on memory</p></li><li><p>[00:22:00] Benefits of model quantization even with unlimited memory</p></li><li><p>[00:23:44] KV cache memory overhead during inference</p></li><li><p>[00:26:08] How optimizer memory usage is calculated</p></li><li><p>[00:32:03] Components of total training memory (model, optimizer, gradients, activations)</p></li><li><p>[00:33:47] Activation recomputation to reduce memory overhead</p></li><li><p>[00:38:25] Sharded optimizers like ZeRO to distribute across GPUs</p></li><li><p>[00:40:23] Communication operations like scatter and gather in ZeRO</p></li><li><p>[00:41:33] Advanced 3D parallelism techniques (data, tensor, pipeline)</p></li><li><p>[00:43:55] Combining 3D parallelism and sharded optimizers</p></li><li><p>[00:45:43] Challenges with heterogeneous clusters for distribution</p></li><li><p>[00:47:58] Lightning Round</p></li></ul><p><strong>Alessio</strong><span>: Hey everyone, welcome to the Latent Space podcast. This is Alessio, partner and CTO in Residence at </span><a href="https://decibel.vc/" rel="">Decibel Partners</a><span>, and I'm joined by my co-host Swyx, writer and editor of Latent Space. [00:00:20]</span></p><p><strong>Swyx</strong><span>: Hey, today we have a very special guest, Quentin Anthony from Eleuther.ai. The context for this episode is that we've been looking to cover Transformers math for a long time. And then one day in April, there's this blog post that comes out that literally is called Transformers Math 101 from Eleuther. And this is one of the most authoritative posts that I've ever seen. And I think basically on this podcast, we're trying to give people an intuition around what are the rules of thumb that are important in thinking about AI and reasoning by AI. And I don't think there's anyone more credible than the people at Eleuther or the people training actual large language models, especially on limited resources. So welcome, Quentin. [00:00:59]</span></p><p><strong>Quentin</strong><span>: Thank you. A little bit about myself is that I'm a PhD student at Ohio State University, starting my fifth year now, almost done. I started with Eleuther during the GPT-NeoX20B model. So they were getting started training that, they were having some problems scaling it. As we'll talk about, I'm sure today a lot, is that communication costs and synchronization and how do you scale up a model to hundreds of GPUs and make sure that things progress quickly is really difficult. That was really similar to my PhD work. So I jumped in and helped them on the 20B, getting that running smoothly. And then ever since then, just as new systems challenges arise, and as they move to high performance computing systems and distributed systems, I just sort of kept finding myself falling into projects and helping out there. So I've been at Eleuther for a little bit now, head engineer there now, and then finishing up my PhD and then, well, who knows where I'll go next.  [00:01:48]</span></p><p><strong>Alessio</strong><span>: Awesome. What was the inspiration behind writing the article? Was it taking some of those learnings? Obviously Eleuther is one of the most open research places out there. Is it just part of the DNA there or any fun stories there? [00:02:00]</span></p><p><strong>Quentin</strong><span>: For the motivation for writing, you very frequently see in like the DL training space, like these Twitter posts by like, for example, like Stas Bekman at Hugging Face, you'll see like a Twitter post that's like, oh, we just found this magic number and everything is like 20% faster. He’s super excited, but doesn't really understand what's going on. And the same thing for us, we very frequently find that a lot of people understand the theory or maybe the fundamentals of why like AI training or inference works, but no one knows like the nitty gritty details of like, how do you get inference to actually run correctly on your machine split across two GPUs or something like that. So we sort of had all of these notes that we had accumulated and we're sort of sharing among engineers within Eleuther and we thought, well, this would really help a lot of other people. It's not really maybe appropriate for like a paper, but for something like a blog post or technical report, this would actually maybe squeeze a lot of performance out of people's hardware they're already running on. So I guess there are a lot of projects in Eleuther that we're sort of trying to share notes with people in a way that typical institutions don't. They sort of live within that institution and then you go to a different institution and they do something very similar, but without the lessons of the previous. And it's because everyone's trying to do their own special sauce with their own stack. Whereas Eleuther, we don't really have that constraint and we can just share everything to everybody. [00:03:14]</span></p><p><strong>Swyx</strong><span>: Yeah, this is a level of openness that basically very few people actually embrace. One, it's an extra effort to write things down, of course, but two, it is secret sauce and so that not many people do it. And therefore, oftentimes the only way to learn this stuff is to actually work in one of the large model labs. And so you guys are doing a lot. The only other instance where I can think of where people actually open sourced their process was Facebook's OPT. What else is similar, like sort of trade knowledge, but not formal research knowledge? [00:03:45]</span></p><p><strong>Quentin</strong><span>: I would say Bloom. So the Hugging Face Bloom project in big science and all of that, that was very open. I'd say it's the same caliber, if not more detailed than OPT. Other than that, I think there was like a doc from Microsoft on like their Turing NLG. Their paper is pretty relaxed in that it did talk about some of those challenges. Other than like OPT and Bloom and us, I can't think of any. It's a new thing. [00:04:10]</span></p><p><strong>Swyx</strong><span>: It matters that you are going for the sort of good enough rules of thumb, because I think a lot of people try to go for precision and being overly precise actually is not helpful. Right. Yes. [00:04:20]</span></p><p><strong>Quentin</strong><span>: You'll see some like statements in the blog posts that are just like, we think this is about 1.2 in our experience. And, you know, we don't go any further into detail and it would take maybe an extra month for us to chase down every single little piece of memory. But instead, like getting good enough is still helpful to people. [00:04:36]</span></p><p><strong>Alessio</strong><span>: Let's jump into it. The first part of the article, and we'll put this in the show notes so people will be following along with the post. So we don't need to read every single equation and every footnote for it. [00:04:46]</span></p><p><strong>Swyx</strong><span>: Okay. [00:04:46]</span></p><p><strong>Alessio</strong><span>: But the core equation here is that not the cost of compute, but the compute required to turn a transformer model is roughly equal to tau times T, where like T is the, where tau is the hardware setup throughput that you have. So number of GPUs times the actual flops per GPU. And then T is the time spent. I think people can visualize that pretty easily. It's basically like how many GPUs do you have and how much do you let them run for? And the things that come to it that people have read before in the Chinchilla paper in a way, and the OpenAI scaling law is that you can then equal this to 6PD, where P is the number of parameters in the model and D is the size of the, of the dataset in tokens. So talk a little bit about how people should think about the two. I think a lot of times the focus is on tokens parameter ratio in the training dataset and people don't think as much about the actual flops per GPU, which you're going to mention later in the blog post too, in terms of how much you can get out. So how should people think about this when they're building a model and where should they go to this equation as they're starting to think about training their own transformer-based [00:05:58]</span></p><p><strong>Swyx</strong><span>: model? [00:05:58]</span></p><p><strong>Quentin</strong><span>: You touched a little bit on the fact that people usually start with the dataset. So you have some dataset that you want to train a model on. And then from there, from the 6PD, you should see, okay, I should have about six tokens per parameter. So that determines my model size thereabouts for Chinchilla Optimal. So since then we've seen that need more something like 20 or more than that to get a good quality model. But the next question that should be on your mind in terms of a systems perspective is how long is it going to take for this model to train and what kind of budget should I expect? So let's say I want some cloud instance for some amount of time and each of them will have some price attached to it. So that's where the throughput comes in. So now that you have this model, this number of parameters, you should map that to a transformer architecture and you should benchmark what throughput you get on your software stack for that type of model. So now you have your flops per second on a single GPU. And then given whatever parallelism scheme, which I'm sure we'll get into, like data parallelism or tensor parallelism or whatever else, how is that flops number going to scale to whatever number of GPUs? And then from there, you're going to get a time. And if you have a time, you have a cost. Those are like the business answers that you'll be able to get using this formula. That's why we sort of split it into the T and the throughput terms so that you can solve for one of them, which is usually get throughput, need time, and from time you get cost. In a nutshell, that's the answer. [00:07:19]</span></p><p><strong>Alessio</strong><span>: One thing that I noticed, you mentioned some of these laws are only true when a thousand GPUs for one hour cost the same as one GPU for a thousand hours, given that we have a shortage of the biggest GPUs out there. Any thoughts there on how people should prioritize this? [00:07:36]</span></p><p><strong>Quentin</strong><span>: Yeah, so I would say you should find what the minimum number of GPUs is to just fit your model first. The memory bottleneck is your biggest problem if you have a sizable model. If it's a small model, nobody cares. But most models that people care about will need to be split across multiple GPUs. So find the minimum number of GPUs to just fit your one instance of your model and then calculate how long that's going to take. If it's a reasonable amount of time, then you're done. If it takes too long, then you need to start worrying about having multiple instances of that model. I always feel like you should go with the minimum number of GPUs because the more number of GPUs that you have, the more likely it is for things to break. So I would say just find out what time is reasonable for you and then fit the number of GPUs to that and no more. Because people get greedy and they say, if I have twice the GPUs, I can get this done in half the time. And then you end up taking three times the time because everything is breaking every day. And that's when I am up at midnight trying to fix your model that's broken. [00:08:34]</span></p><p><strong>Swyx</strong><span>: We had a previous guest which has invested a lot in their framework for training these things. Would there not be an equivalent open source framework you guys would have made that would help with scaling up GPUs linearly like that? Or is this an oversimplification?  [00:08:50]</span></p><p><strong>Quentin</strong><span>: Okay, yeah. So maybe I should step back. Both Mosaic and us have our own sort of software stack recipe that scales well, theoretically. But I'll get to that in a minute. Mosaic is all based off optimizer sharding. So it's based off ZeRO. So you basically perfectly split your model optimizer and your parameters and your gradients across all of the different GPUs. So your aggregate memory is number of parameters divided by number of GPUs. Same thing for optimizer and so on. Whereas we at Eleuther use a Megatron deep speed based library. And for that, it's a bit more complex. So the efficiency can be a little higher, but it's more prone to failure at the same [00:09:30]</span></p><p><strong>Swyx</strong><span>: time. [00:09:30]</span></p><p><strong>Quentin</strong><span>: So you kind of have to tune it. In both cases, getting back to like the practical case, you should be able to get linear speed up by adding more GPUs. The problem is that there are hardware failures. You tend to have problems with like maybe loss will overflow if you have too many GPUs or maybe one GPU will hang. You might have software issues. You might have synchronization issues. And that's why I'm saying practically that you should take the minimum number of GPUs that you have because those are the easier cases to debug. That make sense? [00:10:00]</span></p><p><strong>Swyx</strong><span>: Yeah. [00:10:00]</span></p><p><strong>Quentin</strong><span>: Any more detail on any specific point? [00:10:02]</span></p><p><strong>Swyx</strong><span>: Not particularly, just because we haven't actually had to debug those things. But I imagine basically there's a lot of return towards encoding these knowledge into software and not repeating it again. So it makes a ton of sense. I think Alessio had more questions before we move too far into high level, more questions on just the equation itself. I think we want to spend time on essentially, this is the central equation of figuring out compute requirements. Yeah. [00:10:25]</span></p><p><strong>Alessio</strong><span>: Another thing in it is that the computer is like the forward pass and like the backwards pass and forward is 2PD, backward is 4PD. Why it's to the ratio between the two? Can you explain that? Why is it two and four? [00:10:39]</span></p><p><strong>Quentin</strong><span>: Yeah. [00:10:40]</span></p><p><strong>Alessio</strong><span>: Why is it twice the amount? [00:10:42]</span></p><p><strong>Quentin</strong><span>: Oh, okay. Intuitively for forward pass, you're just moving, you're propagating forward the inputs through the layer. And then in the backward pass, you're doing something a little more complex than that. You're doing back propagation. And I don't think I can explain it intuitively enough to go into more detail on the exact [00:10:58]</span></p><p><strong>Swyx</strong><span>: numbers. Yeah. [00:10:58]</span></p><p><strong>Quentin</strong><span>: That's okay. [00:10:59]</span></p><p><strong>Swyx</strong><span>: I feel like you want to get out a whiteboard and start drawing like, you know. [00:11:02]</span></p><p><strong>Quentin</strong><span>: That's what I would normally do. [00:11:03]</span></p><p><strong>Swyx</strong><span>: Tangents and gradients. It's actually surprisingly low to do the back propagation. Honestly, that's one of the fundamental things I love about the math of deep learning so far that as I've explored it, which is, it's surprisingly efficient as compared to other, I guess, numerical methods you might be exposed to and, you know, college calculus. Yeah. [00:11:22]</span></p><p><strong>Alessio</strong><span>: And I think the other thing is that things sound simple, you know, when people go on Twitter and say, Oh, 20 is like the optimal ratio. And it's like, then it's like, well, why is that the number? And the answer is usually much, much harder, like what we're seeing right now. So I think it's a, it's a good reminder that the numbers are simple, like all the best and most popular, like math equations are like, so elegant. Obviously the proof behind that is, it's not that easy. That's always a good reminder. [00:11:52]</span></p><p><strong>Swyx</strong><span>: I want to put this equation to the test a little bit. We can do this from either GPT-3's perspective or GPT-NeoX, whatever you're more comfortable with. You have this distinction of actual flops versus theoretical flops. And a lot of times when people report the flops it took to train a model, like we just saw one in Lama 2 where the estimate is something that the amount of flops and that's, that's what we go with. So GPT-3 took a 3.14 times 10 to the power 23 flops. That is the theoretical flops. I want to get to a point where I can sort of work out if a number passes the smell test. And I wonder how to do that because I should be able to plug in this equation, right? I know that GPT-3 was trained on 300 billion tokens. I know the parameter size of 175. Is it, is it just like a 6 times 175 times 300? Like I haven't done the math, but what are the nuances here that you might want to call out? [00:12:42]</span></p><p><strong>Quentin</strong><span>: Theoretical flops is usually given from, you have a given set of hardware and this is what you expect your hardware to get. The problem is that in practice, full utilization, that's the key word, right? Because in practice, there are a lot of cases where like you're spending time waiting on data movement from like the GPU to CPU. Or for example, you might be waiting to synchronize across the different GPUs. So there's a lot of idle time basically that you're going to be spending during training. [00:13:05]</span></p><p><strong>Swyx</strong><span>: Smell tests. [00:13:06]</span></p><p><strong>Quentin</strong><span>: I don't know if I have a smell test myself, to be honest, like maybe I'll look at like what sort of flops, what you would expect on like an A100. There's sort of just an expected flops for a given GPU that everyone sort of knows what you should expect. So like for an A100, that number is somewhere between 100 and 180. T flops is what you would expect to see on an A100. For a V100, like an older GPU, it's something more like 40 to 30. So people sort of know, given the kernels that we're running for a deep learning, what sort of flops you expect. And then you sort of compare that to the theory, to the theoretical flops that people are reporting and see if that matches your expectations. [00:13:47]</span></p><p><strong>Swyx</strong><span>: Yeah. [00:13:47]</span></p><p><strong>Alessio</strong><span>: And in the article you mentioned for the A100, like if you're seeing below 115 teraflops a second, there's something wrong with your model or hardware. How did you get to 115? Is it just, you know, production observability and like you've seen over months and months and months that like that's the baseline or how do you come up with the numbers like that? Yeah. [00:14:08]</span></p><p><strong>Quentin</strong><span>: For a number like that, we basically, we compared a lot of different frameworks. So like I mentioned before, Mosaic has their own framework and we have our own framework. They all have their own flop counters too, right? And we saw across a bunch of different hardware configurations that if you tune things correctly, you should be getting above 115 in pretty much all cases. So like there are some cases where things are tuned poorly or your system is a little weird, but we've never been able to get a new system and not been able to get above [00:14:35]</span></p><p><strong>Swyx</strong><span>: 115. [00:14:35]</span></p><p><strong>Quentin</strong><span>: If something is below 115, you have something really wrong in your software. But that's really all it is, is just comparing across software stacks and hardware systems. [00:14:44]</span></p><p><strong>Alessio</strong><span>: What about different GPUs? We had George Hotz on the podcast and he talked about AMD cards and how in theory their flops should be much better than some Nvidia cards, but the reality is like the CUDA runtime makes up for it. How should people think about improving that? You know, like do you see, okay, the A100 is like 115 teraflops. I'd rather just stick with this than try and figure out all the kinks of like a better AMD card or any thoughts there? [00:15:10]</span></p><p><strong>Swyx</strong><span>: Right. [00:15:10]</span></p><p><strong>Quentin</strong><span>: Well, that's sort of touching on developer time, right? And which ends up being more expensive because at the end of the day, the AMD and Rockham software stack has a long way to go. I would say most things run there, not particularly efficiently, but you're going to have weird bugs that no one has encountered before. One of the big pluses of going with the Nvidia and PyTorch stack is that there are thousands of GitHub issues with everyone facing the same problem as you and resolving them quickly and in an open source way is probably the biggest benefit of going with the Nvidia software stack right now. AMD has about the same hardware, software, not so much. And they haven't quite got the momentum in the open source realm, for example, to get close. Like something, for example, like Flash Attention, it's spread to more Nvidia GPU types than it has like to AMD at all. And waiting on those latest and greatest features to reach AMD is something that's prohibitive to a lot of people, but it's getting there. I'm running a lot of experiments on AMD right now because it's sort of reached the government lab supercomputers now. And so a lot of experiments are going there and it will catch up, I'd say within a few [00:16:14]</span></p><p><strong>Swyx</strong><span>: years. [00:16:14]</span></p><p><strong>Quentin</strong><span>: Awesome. [00:16:15]</span></p><p><strong>Swyx</strong><span>: Maybe just talk about what's available from the government labs and I heard the original, the origin of Eluther started with a grant for TPUs. Is that right? [00:16:24]</span></p><p><strong>Quentin</strong><span>: Yes, that was a little before me, but there was a lot of just like getting a grabbing a Google Cloud or TPU pod or something like that is a lot of the original TPU work on Mesh TensorFlow, which is like now like an ancient distributed deep learning library. [00:16:36]</span></p><p><strong>Quentin</strong><span>: Eluther got a grant, an insight grant with Oak Ridge last year, and we got quite a bit of Summit Compute. So Summit is a V100 based supercomputer. It's got some weirdness to it. So there's six V100 GPUs per node. And we did a lot of experiments there. It's a challenging system to scale to because your interconnect across nodes is kind of slow in comparison to within a node, which I think we'll get to later. But now Oak Ridge has moved to AMD. So the next grant that we're trying to work towards is on Frontier, which has four AMD GPUs per node and again has a slower interconnect across nodes. So we get all of those new challenges again to try and overlap things. But that's just like you have Oak Ridge, you have Lawrence Livermore. There's a lot of government supercomputers that you can apply for compute towards like open researchers too. It's sort of a new thing. I think we're one of the first like us and like Lion, for example, is another organization that's getting compute from government providers and such. They're all moving to AMD as well. And we look forward to exploring that with them. [00:17:42]</span></p><p><strong>Swyx</strong><span>: Yeah. [00:17:43]</span></p><p><strong>Alessio</strong><span>: The computing is definitely, it used to be easy to find the GPU. Now, not as much. So you got to find them anywhere. [00:17:49]</span></p><p><strong>Swyx</strong><span>: Yes. [00:17:49]</span></p><p><strong>Alessio</strong><span>: Let's talk about memory requirements a little bit. So you touched on this a little bit before and just before this, we had a trade out on the pockets from FlashAttention and memory speed was one of our main focuses, but this time we're being bound by actually memory size, like the VRAM itself, when it comes to model weights and parameters and optimizer states and all that fun stuff. Let's go through this and Sean, we can, we can take turns. There's a lot to cover here, but maybe we can start from model weights. So one topic we covered a lot in the past is precision and quantization. That's one of the obviously main driver of memory. You mentioned most of, in the article, most transformers are mixed precision, like FP16 plus FP32 or BF16 FP32, and they can be cast down. And you mentioned up to like INT8 without a lot of performance hit. So let's start there and maybe run people through some of the maths and like the byte per parameter ratio and different precision. [00:18:50]</span></p><p><strong>Swyx</strong><span>: Sure. [00:18:51]</span></p><p><strong>Quentin</strong><span>: So when I started deep learning, it was all FP32. You have 32 bits, four bytes per parameter. Things were pretty simple. You didn't have to do any loss scaling at all. But the problem was that you didn't get a whole lot of flops once NVIDIA moved to V100s and introduced Tensor cores. So Tensor cores do all of their computation at FP16 precision. So you're kind of throwing all of those away if you're doing things in FP32. So once the hardware moved to V100, the software moved to like mixed precision and APEX and AMP and such. And one counterintuitive part of mixed precision is that you actually require more memory when you're trained because you need an FP16 copy of the weights and an FP32 copy of the weights. The FP16 copy is where you're doing like your actual computation on the Tensor cores. So you get maybe it's not uncommon to get double the throughput that you would see before in FP32. And then you at each step update that FP32 copy with the FP16 update. So both need to be stored in memory. The problem with that is that FP16 is very precise but doesn't have a whole lot of range, [00:19:55]</span></p><p><strong>Swyx</strong><span>: dynamic range. [00:19:55]</span></p><p><strong>Quentin</strong><span>: So you have a really big mantissa if you're thinking in terms of like floating point representations, not a whole lot of exponent. So BF16 puts more of the bits from the mantissa back to the exponent. So you have a much higher range and a lower precision. And that gets rid of all of this instability problem and loss scaling and such that anyone familiar with debugging knows how unstable it can be, especially for large scale training. And BF16 does away with a lot of that, but it's only supported on A100s. So you see the back and forth between hardware and software. So every time NVIDIA introduces some new Tensor cores or BF16 support or something like that, the software adapts to support it and then training adapts. And then now you mentioned like Ind8 and such. Now we're seeing that you have some model that's been trained in FP16, FP32, whatever else. And then now you want to, with minimal loss and accuracy, quantize that model into a smaller representation like Ind8 and now like Ind4 and things like that and see what you can get away with. And then since deep learning is such like a stochastic problem that a lot of those last bits of precision don't really matter is what we're finding. And I expect that to continue. [00:21:06]</span></p><p><strong>Alessio</strong><span>: And so just to put some numbers to it, when you have a FP32, you need four bytes per parameter at inference time to load it in memory. If you have a eight bits model quantized down, you need one byte per parameter. So for example, in an H100, which is 80 gigabyte of memory, you could fit a 70 billion parameters in eight, you cannot fit a FP32 because you will need like 280 gigabytes of memory. So how much does that play into it? Like you mentioned it was all FP32 when you first started. Is it just like a development complexity thing, like going down to FP16 and then Ind8? Or if they could get a GPU with like a terabyte of VRAM, will people just load this memory as like FP32 weights or would they still want to quantize them to make them more efficient? Right. [00:22:00]</span></p><p><strong>Quentin</strong><span>: I would say even if you had infinite VRAM, you would still want a quantized model, just a bigger model that's quantized is what I would say. And that's because like I was mentioning there at the end, how like deep learning is very stochastic and a lot, you could have all the precision in the world, but ultimately it's meaningless when you still depend so much like on what the input is. And you depend so much on little variations and maybe a few more samples of training data would matter more. A lot of that precision in a nutshell doesn't really matter in deep learning. All that matters is the big picture. What is that neuron actually saying? And not the tiny details of what it might be thinking. Oh, I also wanted to mention that even if you have an A100, the actual model size is quite a bit smaller that you could load than what you mentioned. That's because of the KV cache. So the KV cache intuitively during inference, it only matters during inference and think intuitively if you're writing a paragraph, you want to remember every single previous word that you've written before you write the next word. So like what is autoregressive language modeling? It's filling in the next word, the next token. So if I say like the dog went to the, and I need to write the next word, I would say park or something. Before I write the next word, my memory is wiped and I have to read the whole thing again. That is life without a KV cache. And a KV cache says, remember everything that I've generated before, as well as all the context before what I've generated. But the memory overhead for a KV cache commonly is either comparable or larger than the model in some cases, if you have a really long context. And I think the exact equation is something like, oh, it's like two times the number of layers, times the number of heads, times the dimension of each head. And then there's two of those. You have one for K, one for V. But that was just a quick aside. Yeah. [00:23:44]</span></p><p><strong>Alessio</strong><span>: I know this is Transformers math, but do you think one of the interesting things about RNNs too, it's like moving away from this, like KV cache, the scales with the sequence length and having like a fixed sequence pass. I know those are some of the things that people are working on. [00:24:00]</span></p><p><strong>Swyx</strong><span>: Yeah. [00:24:00]</span></p><p><strong>Quentin</strong><span>: So there's a paper that I was involved with called RWKV that I would recommend people read. It is answering this exact question. So how do you get Transformers quality without this quadratic attention overhead that Transformers requires? So it is interesting. I don't know if I can really dive too deep into the technical details there. I'd recommend people read the paper. But yeah. [00:24:23]</span></p><p><strong>Swyx</strong><span>: Yeah. [00:24:23]</span></p><p><strong>Alessio</strong><span>: It's interesting to see if attention is all you need, or maybe attention is all we need, but we need better ways to make it infer in a good way. [00:24:33]</span></p><p><strong>Swyx</strong><span>: We've actually done an unreleased episode with one of the RWKV core members and they call it soft attention or light attention. I forget what they call it, but yeah, just ways to approximate it such that it's linear and not quadratic. That's great. Yeah. [00:24:47]</span></p><p><strong>Quentin</strong><span>: I didn't know that you were involved. [00:24:48]</span></p><p><strong>Swyx</strong><span>: That's great. How did you get involved? Is it just because like everyone just hangs out in Discord and talks about the future of Transformers? Oh yeah. [00:24:55]</span></p><p><strong>Quentin</strong><span>: I mean, the RWKV people specifically are in Eleuther all the time. Like they're very close collaboration with us. And my contribution was we have all of these experiments done by all of these people on RNNs and how they relate to Transformers and how do we turn that into a paper and disseminate that digestibly so that people don't have to read through like a Discord log from a year ago to understand what's going on. [00:25:16]</span></p><p><strong>Swyx</strong><span>: Oh my God. [00:25:16]</span></p><p><strong>Quentin</strong><span>: Just read this paper. So that took some work, but I wasn't a core contributor. So that's why I don't want to go into like the technical details. But yeah, that's how I did. [00:25:24]</span></p><p><strong>Swyx</strong><span>: We'll try to get that RWKV episode out. It seems like there's increasing mentions of it and they are doing pretty important work as far as scaling these models are concerned. Okay. So we discussed inference type quantization and memory requirements. And then you also had a section on training with a lot of stuff I think mentioned. I think we probably want to spend the most of our time on optimizer states and the Atom optimizer. Yeah. What are your takes on it and what should people keep in mind when they deal with these optimizers? Okay. [00:25:57]</span></p><p><strong>Quentin</strong><span>: I would say the Atom optimizer is good at what it does. It's sort of a broad question. So let me think. You have the copy of the weights and then you have your momentum and your variance that [00:26:08]</span></p><p><strong>Swyx</strong><span>: you store. [00:26:08]</span></p><p><strong>Quentin</strong><span>: And like, okay, maybe an intuitive explanation for momentum is that like, let's say you have a canyon and you're trying to get to the bottom. And if you're just doing basic SGD, then every step is going to be an equal size. Whereas if you're using something like Atom with the momentum term, then your steps should be progressively larger because you can see, oh, the general trend is we're heading downwards very quickly. But stepping back from that, since you have all of these extra terms in Atom, you require a lot more memory to store it. Like three times as much memory as SGD. And if you have all of this memory being spent on your optimizer states, then how do you distribute it across GPUs? Because you'll find that what ends up being your bottleneck more than just raw compute, raw flops on a given GPU is your parallelism. And that falls back onto how much model you can fit on a single GPU before you need to split it up across a bunch of GPUs. And then you end up spending time, more time with them talking to each other than actually making progress. So that's why all of this time in the blog post is spent on how do you distribute your model? What are all those different distributed strategies look like? Which ones are more efficient? And given that a lot of your memory is being spent optimizers, how do you distribute that optimizer specifically? Because a lot of people, when they talk about parallelism, they talk about model parallelism, the parameters themselves. In actuality, when you're training, a good portion of your memory is actually spent on optimizer states. So what specific part of that would you like to go into? Would you like to go into like zero or sharded optimizers? [00:27:36]</span></p><p><strong>Swyx</strong><span>: I think the sharded optimizer stuff is really interesting, but I think we're kind of leaving that towards the end, right? Because that's the maybe more advanced distributed sections. Here, I think we're just going for rough intuition for people who've maybe are familiar with the ideas of these optimizers, but haven't actually had to implement them yet. They read your code, but they don't really understand the intuition behind the code. I see. [00:28:00]</span></p><p><strong>Alessio</strong><span>: And Quentin, when you say in the blog post, it says, Adam is magic. How much of it is like actual magic, even to like people like you that are pretty close to the metal, so to speak? Are some of these things just come as gospel? It's like, I know this works, like I'm not touching it. I'm just leveraging it. How much of it are you actually thinking about improving on in your day-to-day work? I see. [00:28:22]</span></p><p><strong>Quentin</strong><span>: So I'm a systems guy. I'm an engineer. And a lot of these things come to me as magic. Adam comes to me as magic. I see it from the gods. I say, this is how a deep learning model is trained. And this is how the next step is calculated. And then I say, okay, how do I make that fast? I would say I do look at ways to improve upon it using things like second order optimizers. So there's a lot of research on there because they're hard to distribute. But the core contribution for me always comes down to someone else has done like some deep learning optimization and I need to make it run fast. So I can't really speak to the motivation of why Adam came about other than like simple, intuitive things like I mentioned with like the momentum. But what matters to me is that Adam takes more memory than SGD, specifically three times. And all of that memory needs to go somewhere and it needs to be split efficiently. [00:29:14]</span></p><p><strong>Swyx</strong><span>: Yeah. [00:29:14]</span></p><p><strong>Alessio</strong><span>: So when you add them all up, you got 12 bytes per parameter with vanilla Adam. [00:29:20]</span></p><p><strong>Swyx</strong><span>: Yeah. [00:29:20]</span></p><p><strong>Alessio</strong><span>: And then you still get the model parameters and memory too. So as you mentioned, you need to keep a copy of both for like a FB32, FB16 mixed, a copy of both quantization levels. So there's precision levels. So it's six bytes per parameter. Right. [00:29:36]</span></p><p><strong>Quentin</strong><span>: Taking a step back again, is that like, okay, most people think of your model getting big. So you need to split with model parallelism purely, something like tensor parallelism. But we can see that the model only takes like two bytes per parameter if we're doing FB16. Whereas the optimizer itself requires four bytes per parameter for the model states, four bytes for momentum, four bytes for variance. So what matters more is how do you split your optimizer efficiently and how do you store it efficiently? And something like bits and bytes, where the optimizer, you got like eight bit Adam, where those optimizer states is only one byte per parameter instead of four or something like that. That is going to give you a much better return on your model training and on your memory overhead required than if you were to, for example, quantize your pure like FB16 model weights down to int8 or something. So for training specifically, your optimizer memory matters a lot. The most in most cases. [00:30:31]</span></p><p><strong>Swyx</strong><span>: Well, yeah. [00:30:31]</span></p><p><strong>Alessio</strong><span>: And before we dive into zero, just to wrap up the items that you're going to shard later. So you have the parameters, you have the optimizer states, and then you have the gradients. Just maybe touch a little bit on that. And then we can talk about how to efficiently load them in GPUs. [00:30:48]</span></p><p><strong>Quentin</strong><span>: So the parameters are the FP32 copies of the parameters. We include them in the optimizer discussion. Some people don't, but just for clarity, it's 12 bytes per param for the optimizer states and four of them are for that FP32 copy of the weights. Four of them are for the momentum. I already went into why it's important to store momentum, but that's also per parameter. You need to store where that parameter is going and where it's been going in the past. You also need to know, okay, we know where it's going, but there's going to be bumps on this canyon that we're going down. So we need to store its variance. How often are those bumps? Should we be focusing more on the momentum? Or is this parameter just kind of jumping around everywhere? Those are all important answers that we need the optimizer to store, and it's per parameter. So that's where all three of those terms come from. And we also include some competing bits and bytes, for example, an SGD to show that depending on your optimizer, you may store all or none of these and in different representations. [00:31:50]</span></p><p><strong>Alessio</strong><span>: I'm looking at the total training memory. You essentially have model memory, optimizer memory, gradient memory, and activation memory. I think that's one of the last discussed things. So maybe just give people a little bit of a view. [00:32:03]</span></p><p><strong>Swyx</strong><span>: Yeah, this is completely new to me. [00:32:05]</span></p><p><strong>Alessio</strong><span>: Active, you know, recomputation, checkpointing, and all of that. [00:32:08]</span></p><p><strong>Swyx</strong><span>: Right. [00:32:09]</span></p><p><strong>Quentin</strong><span>: So, okay. So to summarize before activation checkpointing, which will be complicated, you have your model params, like I mentioned before, they used to be FP32. Now they're probably BF16, maybe FP16 if it's an older GPU. Then you have your optimizer. That's where a lot of the memory is going. And it's your high precision, usually FP32, copy of the weights. So that's four bytes per param. And then you have, optionally, a couple more terms like we just discussed, like momentum or variance or whatever else, depending on what your optimizer is. Then you have your gradients. So your gradients is what is the gradient update that we get after running the forward pass on the model. And that's going to be whatever your low precision copy of the weights is. So like two bytes per param, if you're using FP16 or BF16. And all of those are sort of set in stone. And that overhead is not going to go away for the duration of training. Your gradients might get cleared after you back propagate them, but your optimizer states and your model states aren't going away. That memory overhead will be there. Activation recomputation and activation memory is dynamic. So some people will come and have this problem where the model loads fine for training. But then when you actually run your first iteration, or you run some future iteration or something like that, you run out of memory, seemingly at random. And it's because of these activations that you're computing on the fly. Good summary, or do you want to get into activation recomputation now, or do you want me to touch on anything else? [00:33:35]</span></p><p><strong>Alessio</strong><span>: Yeah, I was going to say, when is the recomputation happening? How does it decide between recomputing versus storing? And talk a bit more about that, maybe. [00:33:47]</span></p><p><strong>Quentin</strong><span>: Yeah, okay. So there's a lot of different ways to do this, but I would say there are a few main ones. First is a very simple scheme. You recompute everything. Every single activation that you calculate is just going to be either used or thrown away until the end. So in that case, you care very much about memory. You care very little about compute. Maybe this would be a case where you have to distribute across a lot of different GPUs, for example. And your communication speed is really low. Then that might be a good case for you to just recompute everything. It happens rarely, but it happens. Next up would be something like selective recomputation. So in selective recomputation, which Megatron has a good paper on, and I believe the figure that we have in our blog post is from, in that case, you sort of do a weighted decision for each activation. So for really big activation tensors, you decide, is this going to be more expensive to save in terms of memory or to recompute in terms of compute? So that's sort of the smart scheme that Megatron implements. And there's a lot of different heuristics they use. It's probably not worth mentioning off this super long equation on a pod, but you should go and read that paper if you're interested on selective recomputation. And then a really stupid scheme that most people go with, including NeoX, would be something like, instead of doing all of these heuristics, you just say, if my tensor is bigger than X, I throw it away. And you set X to some static number, and that's it. And that is good enough for a lot of cases. [00:35:18]</span></p><p><strong>Swyx</strong><span>: Why is it good enough? [00:35:20]</span></p><p><strong>Quentin</strong><span>: You don't want to store more than, you know, X-sized tensor. And some fall above that, some fall below it. And you're not trying to squeeze. You care more about getting something close enough to what the actual heuristic should be without actually computing the heuristic because you don't want to spend the time writing that heuristic code. [00:35:37]</span></p><p><strong>Swyx</strong><span>: Cool. I think that does take us on a grand tour of the memory math. Is there any sort of high-level takeaway before we go into the distributed stuff? Zero and all that. Perhaps more detail than most people have ever encountered. And so I'll repeat the equation that Alessio mentioned again, which is total training memory now has all these components that you've mapped out for the first time as far as we're concerned. Model memory, optimizer memory, activation memory, gradient memory. We covered quite a few algorithms as to the choices you can make there. Anything else that you want to mention about just memory math? I don't think so. [00:36:11]</span></p><p><strong>Quentin</strong><span>: I think that about covers it. I will say that it's a very different scheme for training and inference. It's common for people to say, oh, BF16 is the best. Done. Whereas a more correct take is that during training, precision matters a bit more. So BF16 will be around longer for training than it will for inference, in which case your model is sort of already baked. And it definitely doesn't need some of those last bits of precision so you can get away much easier with going to int8 for inference rather than training. So everything that you learn for training has to be relearned for inference and vice versa. [00:36:44]</span></p><p><strong>Swyx</strong><span>: There's a third category. You're talking about training versus inference. This third category is emerging with regards to fine-tuning and perhaps parameter-efficient methods of fine-tuning. The naive way to implement fine-tuning is just to do more training. But I don't know if you've developed any intuitions over fine-tuning that's worth inserting here. Any intuitions? If you were to write fine-tuning math, what would go in there? That might be an interesting diff to training math. [00:37:10]</span></p><p><strong>Quentin</strong><span>: I think there's a lot of questions that are unanswered for fine-tuning. For example, we know scaling laws for training. And some people have done scaling laws for fine-tuning. But how does a model that's already been trained on one domain transfer to another in terms of fine-tuning size? How many tokens per parameter should you have for your fine-tuning dataset? Maybe I'm ignorant, but I feel like a lot of those sort of practical questions on how a model can transfer and how a model can learn or grok some new ability that wasn't in its original training dataset is something that I would definitely put inside a fine-tuning blog post. [00:37:45]</span></p><p><strong>Swyx</strong><span>: Something related to perplexity and, I guess, diversity of the tokens that you get. [00:37:49]</span></p><p><strong>Quentin</strong><span>: Yeah, sort of dataset transfer is something that I would be curious in. Learning rate transfer is another one. So your model has some decayed learning rate over the course of training. How does that change for fine-tuning? Things like that. [00:38:00]</span></p><p><strong>Swyx</strong><span>: All right, cool. Thanks for indulging that stuff. Sure. Yeah. [00:38:03]</span></p><p><strong>Alessio</strong><span>: I think after all of this, you can quickly do the math and see that training needs to be distributed to actually work because we just don't have hardware that can easily run this. So let's talk a bit about that. So zero is one of the first things that you mentioned here, which is focused on sharded optimizers. Maybe run people through that and how to think about it. [00:38:25]</span></p><p><strong>Swyx</strong><span>: Sure. [00:38:25]</span></p><p><strong>Quentin</strong><span>: So zero is centered around two communication operations. And the first is scatter. And people should be looking at the zero figure that I think we have. [00:38:35]</span></p><p><strong>Swyx</strong><span>: Yeah. [00:38:36]</span></p><p><strong>Quentin</strong><span>: So there's a figure in the paper with parameters, gradients, and optimizer states that people should be looking at when I'm talking about this. Every GPU is going to get its own equal portion of the slice. And if we're doing... There are different stages of zero, but let's just start off with assuming that it's an equal slice of the optimizer states, gradients, and parameters. That would be zero three, stage three in that case. And we do that with a scatter. And the scatter takes, say, one over end GPUs, plus this offset of that slice goes to that GPU. Now all of the GPUs have an equal slice that's in its rank order. And then during each training step, that GPU is going to wait for all of the other slices to communicate so that we now have a whole pie on that GPU, that single GPU. Once we have that whole pie, we do the forward pass on it. And then we distribute that forward pass to all of the others using a gather. So it's a scatter, reduced scatter specifically, and then a gather back to all the others. And you do that each step. So the point of it is that you're sharding these states across GPUs. And with the different stages, you'll see in that figure that the optimizer state is taking the most proportion, which is because of what I mentioned before. We're including the FP32 copy and we're doing atom. So we need those four bytes per param for momentum and for variance. And then zero stage one, which is the most common one, is just optimizer. Zero stage two is optimizer plus gradients. And zero stage three is optimizer gradients and model parameters. But it all comes back to this splitting up and then gathering together back and forth over and over. So you get a lot of communication overhead from zero. But the plus part of that is that you can overlap a lot of that movement with computation. [00:40:23]</span></p><p><strong>Alessio</strong><span>: How do you get the optimal number of GPUs to do this on? Is there a way to shard too much as well and put too much overhead? [00:40:31]</span></p><p><strong>Quentin</strong><span>: It depends more on what your interconnect is. Taking a step back, there is synchronization that's required, a lot of it, across all of these GPUs. And those tend to be cumulative. So if you go to too many GPUs on an interconnect that's too slow, then you're going to end up spending more time synchronizing. And that magic number where you spend more time synchronizing is going to be different depending on what your fabric is and what your GPU memory is specifically. Just how small of a slice is each GPU getting? I can't, for example, for Summit, that number comes out to be about 20 billion parameters. Now you have 20 billion parameters, and then your magic number of GPUs for that is going to be something like 100 to 200 scale. Beyond that, you're just going to end up spending more time communicating. And the actual flops dipping below some predetermined number by you is going to be whatever your sweet spot ends up being. [00:41:24]</span></p><p><strong>Alessio</strong><span>: And then, so this one was like hard for me to go through, so I'm excited to have you run through it, which is a 3D parallelism. [00:41:33]</span></p><p><strong>Swyx</strong><span>: It's fancy, it's cutting edge. [00:41:35]</span></p><p><strong>Alessio</strong><span>: Yeah, let's talk a bit more about that and some of the work. [00:41:38]</span></p><p><strong>Quentin</strong><span>: Okay, 3D parallelism. So what is each dimension? First is the really basic one. That's data parallelism. And data parallelism is you have a copy of the model. Let's say for simplicity, one copy fits on one GPU perfectly. Data parallelism is that now you have two GPUs, so you have one copy on GPU one, one copy on GPU two. Both of them do the forward and backward pass and then synchronize and average the gradients. And then that's a step. Data parallelism for 3D parallelism is actually zero. So it's, you're sharding the optimizer states across all of your different GPUs. Next up is tensor parallelism. Tensor parallelism is you split your model. Like say, if you have two GPUs, you split your model down the middle and each GPU on its tensor specifically is going to do its forward or backward operation on its tensor. And then only when necessary, it'll synchronize that tensor operation with the other GPU. It's a bit more complex than something like pipeline parallelism, which is the third dimension. In pipeline parallelism, let's say you have four layers in your model. And you have four GPUs. You put one layer on each GPU and then GPU one does the forward pass and then sends the output of its activations to GPU two. It does the forward pass, sends activations to three, and you're just moving down a line. That is a naive scheme in that all of the other GPUs are doing nothing while a single GPU is doing its forward or backward pass. So the reason it's called pipeline parallelism is because you're splitting your mini batch into micro batches. So GPU one will do the forward pass on micro batch one and then send to GPU two. And then while GPU two is running on that first micro batch, GPU one is working on the next micro batch. And so you're sort of pipelining the movement and computation of each micro batch. The problem with that is that you need a really big batch size in order to split it up into both mini batches and micro batches. So combining all three of those together, you get a 3D mesh of where each parameter and optimizer state and so on maps to each GPU. And that's 3D parallelism. So let's start diving into details on what have that made sense, what should I jump into more on? [00:43:55]</span></p><p><strong>Alessio</strong><span>: I think the main question is, do you need all of the GPUs to be the same to do this? Or can you have mismatching GPUs as well? [00:44:03]</span></p><p><strong>Quentin</strong><span>: Okay, two things matter. If there's a difference in VRAM for the two different kinds of GPUs, then you're going to be bottlenecked by whichever GPU has the lower amount of VRAM because it's going to run out of memory. And then you can't like whatever's left on the larger GPUs is going to be empty. As far as I'm aware, there's no like GPU single GPU aware memory overhead scheme that would account for that. The second problem is that let's say all of your GPUs have the same amount of VRAM, but half of them are really slow. And the problem with that is that those synchronizations that I mentioned earlier are going to kill you. So you're going to move as quickly as your slowest GPU in that case. So in both cases, you end up regressing to your slowest or smallest GPU. So you might as well have the same GPUs for all of them. Otherwise, you're wasting the nicer ones. And that also goes to your CPUs and your interconnect. So going back to the 20 billion parameter model that Eleuther was training, that was on a cluster that was sort of Frankenstein made during COVID when there was all of that shortage of network switches and such like that. So every node had a different network switch. And so you ended up moving at the speed of the slowest switch and getting everything tuned properly so that it's not worse than the slowest switch was challenging and is like a real world problem that sometimes comes up. [00:45:28]</span></p><p><strong>Alessio</strong><span>: Is this work widely accepted? Like I hadn't learned about this before studying for this episode. Is this something that people are still trying and researching? Or is everybody just aware of this and running this in production? [00:45:43]</span></p><p><strong>Quentin</strong><span>: What is this specifically? [00:45:44]</span></p><p><strong>Alessio</strong><span>: Like the sharded optimizers plus the 3D parallelism, bringing the two things together and having this kind of mesh strategy. [00:45:51]</span></p><p><strong>Quentin</strong><span>: I would say that a lot of major GPT-based models use this scheme. A lot of them now are sort of going with just a pure zero scheme. So just a pure sharded. You just shard everything. And then since that's so easy, everyone gets an equal slice. There's no such thing as a pipeline stage. There's no such thing as what tensor should go on which GPU. Instead, we shard everything equally and treat everything equally. It's a much easier problem to debug, to checkpoint, to run training on than it is with this 3D parallel scheme. I say 3D parallel gives you the most control and also the most ways to go wrong. And depending on whether you have more engineers or whether you have more GPUs, that should decide which of these you go with. [00:46:35]</span></p><p><strong>Swyx</strong><span>: It's also not too hard, right? You've basically outlined the five or six different numbers that you need to keep in your head. And it doesn't feel impossible that if you need to achieve that level of control, you've given everybody the main levers to do it with. And that's wonderful. Definitely. [00:46:51]</span></p><p><strong>Quentin</strong><span>: The problem that comes up is like, say, like, okay, GPT-4 came out. Now we have VLLMs. [00:46:57]</span></p><p><strong>Swyx</strong><span>: Whoa, what are VLLMs? Oh, okay. Virtual LLMs, like the Metro of Expert things? No, like visual. [00:47:03]</span></p><p><strong>Quentin</strong><span>: So now you have like multimodal models and such. How do you distribute that? Do you distribute it in a pipeline stage? And do you just shard it? Do you split the tensor and make a tensor parallel? It's sort of hard to change your model and add new features and such when you have this 3D parallel scheme. That's when I say hard. I mean, it's hard to sort of adapt and modify it to new features. [00:47:26]</span></p><p><strong>Alessio</strong><span>: I know we're at the hour mark, and I think we put our listeners through a very intense class today. So this was great, Quentin. And we're going to definitely link the article so that people can read it and follow along. Any other research that you're working on in this space that you want to shout out? I know one of our usual, I mean, wrong question is, what's the most interesting unsolved question in AI? So curious to hear if you think it's still on the training inference, math optimization, or are there more areas that people should pay attention to? [00:47:58]</span></p><p><strong>Quentin</strong><span>: I think in my area of research, there are two things that I think people should really care about. And the first is multimodal parallelism and RLHF. You were seeing more and more reinforcement learning and coming into the training loop. And so how do you split that some model or some GPUs are working on inference and some GPUs are working on training? And like I mentioned before, you have to relearn everything and they have very unique challenges. How do you split up a KV cache during training, for example? Those are challenges that are not well studied, I don't think. And then multimodal, you have like maybe a vision transformer and a text transformer. How do you split those up? Do you split them up equally? Do you put them on separate GPUs or do you just shard everything? And just maybe one GPU will have some vision, some text parameters. And then the second case I would say is that communication is very often a bottleneck. So we talk about 3D parallelism, but a lot of those like, for example, tensor parallelism, you can't go across nodes with. You'll just get killed in communication. So what I'm getting to is how should you compress your communication before it happens? So on the fly compression, you have some buffer that needs to be communicated. You compress it with a GPU kernel, then you send it across the network and then you decompress it, something like that. Making people spend less money on communication fabrics and more on GPUs as intended is sort of a thing that people need to explore. I think those are my two. [00:49:26]</span></p><p><strong>Alessio</strong><span>: Sean, you went over the other half of the lightning round before we wrap it up. [00:49:30]</span></p><p><strong>Swyx</strong><span>: That's a good brain dump. Cool. Yeah, I have so many more questions on the multimodal stuff, but that should be for another time. Acceleration, what has already happened in AI that you thought would take much longer? [00:49:42]</span></p><p><strong>Quentin</strong><span>: I would say flash attention. Guys, just talk to Tree. And flash attention is just sort of a really great set of kernels that I thought would take a while to get to us. [00:49:51]</span></p><p><strong>Alessio</strong><span>: Well, Quentin, thank you very much, man. This was super informative and I think hopefully helps demystify a little bit the blog post. I think people open it and it's like a lot of math on it. And I think you walking them through it was super helpful. So thank you so much for coming on. [00:50:07]</span></p><p><strong>Swyx</strong><span>: Of course. [00:50:08]</span></p><p><strong>Quentin</strong><span>: And I'm happy to answer any questions that people have offline if they have them. I do read my email. [00:50:13]</span></p><p><strong>Swyx</strong><span>: Email and Discord. Of course, yeah. [00:50:15]</span></p><p><strong>Quentin</strong><span>: Discord I'm even faster on. [00:50:16]</span></p><p><strong>Alessio</strong><span>: Thank you, everyone. [00:50:18]</span></p><p><strong>Swyx</strong><span>: Thanks, Quentin. [00:50:19]</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Theory of interstellar trade (1978) [pdf] (101 pts)]]></title>
            <link>https://www.princeton.edu/~pkrugman/interstellar.pdf</link>
            <guid>37149782</guid>
            <pubDate>Wed, 16 Aug 2023 16:45:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.princeton.edu/~pkrugman/interstellar.pdf">https://www.princeton.edu/~pkrugman/interstellar.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=37149782">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[July 2023 was the hottest month on record (115 pts)]]></title>
            <link>https://twitter.com/nasa/status/1691106509319806977?s=46&amp;t=H_jBB1XRvGbGkpJRBZAq5Q</link>
            <guid>37149736</guid>
            <pubDate>Wed, 16 Aug 2023 16:42:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/nasa/status/1691106509319806977?s=46&#x26;t=H_jBB1XRvGbGkpJRBZAq5Q">https://twitter.com/nasa/status/1691106509319806977?s=46&#x26;t=H_jBB1XRvGbGkpJRBZAq5Q</a>, See on <a href="https://news.ycombinator.com/item?id=37149736">Hacker News</a></p>
Couldn't get https://twitter.com/nasa/status/1691106509319806977?s=46&t=H_jBB1XRvGbGkpJRBZAq5Q: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[LK-99 isn’t a superconductor – how science sleuths solved the mystery (992 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-02585-7</link>
            <guid>37149349</guid>
            <pubDate>Wed, 16 Aug 2023 16:17:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-02585-7">https://www.nature.com/articles/d41586-023-02585-7</a>, See on <a href="https://news.ycombinator.com/item?id=37149349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02585-7/d41586-023-02585-7_25924888.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02585-7/d41586-023-02585-7_25924888.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Shards of a LK-99 crystal." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02585-7/d41586-023-02585-7_25924888.jpg">
  <figcaption>
   <p><span>Pure crystals of LK-99, synthesized by a team at the Max Planck Institute for Solid State Research in Stuttgart, Germany.</span><span>Credit: Pascal Puphal</span></p>
  </figcaption>
 </picture>
</figure><p>Researchers seem to have solved the puzzle of LK-99. Scientific detective work has unearthed evidence that the material is not a superconductor, and clarified its actual properties.</p><p>The conclusion dashes hopes that LK-99 — a compound of copper, lead, phosphorus and oxygen — marked the discovery of the first superconductor that works at room temperature and ambient pressure. Instead, studies have shown that impurities in the material — in particular, copper sulfide — were responsible for the sharp drops in electrical resistivity and partial levitation over a magnet, which looked similar to properties exhibited by superconductors.</p><p>“I think things are pretty decisively settled at this point,” says Inna Vishik, a condensed-matter experimentalist at the University of California, Davis.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02481-0" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02585-7/d41586-023-02585-7_25886824.jpg"><p>Claimed superconductor LK-99 is an online sensation — but replication efforts fall short</p></a>
 </article><p>The LK-99 saga began in late July, when a team led by Sukbae Lee and Ji-Hoon Kim at the Quantum Energy Research Centre, a start-up firm in Seoul, published preprints<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup><sup>,</sup><sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> claiming that LK-99 is a superconductor at normal pressure and temperatures up to at least 127 ºC (400 kelvin). All previously confirmed superconductors function only at extreme temperatures and pressures.</p><p>The extraordinary claim quickly grabbed the attention of the science-interested public and researchers, some of whom tried to replicate LK-99. <a href="https://www.nature.com/articles/d41586-023-02481-0" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02481-0" data-track-category="body text link">Initial attempts did not see signs of room-temperature superconductivity</a>, but were not conclusive. Now, after dozens of replication efforts, many experts are confidently saying that the evidence shows LK-99 is not a room-temperature superconductor. (Lee and Kim’s team did not respond to <i>Nature</i>’s request for comment.)</p><h2>Accumulating evidence</h2><p>The South Korean team based its claim on two of LK-99’s properties: levitation above a magnet and abrupt drops in resistivity. But separate teams in Beijing, at Peking University<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup> and the Chinese Academy of Sciences<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup> (CAS), found mundane explanations for these phenomena.</p><p>Another study<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup>, by US and European researchers, combined experimental and theoretical evidence to demonstrate how LK-99’s structure made superconductivity infeasible. And other experimenters synthesized and studied pure samples<sup><a href="#ref-CR6" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">6</a></sup> of LK-99, erasing doubts about the material’s structure and confirming that it is not a superconductor, but an insulator.</p><p>The only further confirmation would come from the Korean team sharing their samples, says Michael Fuhrer, a physicist at Monash University in Melbourne, Australia. “The burden’s on them to convince everybody else,” he says.</p><p>Perhaps the most striking evidence for LK-99’s superconductivity was a <a href="https://sciencecast.org/casts/suc384jly50n" data-track="click" data-label="https://sciencecast.org/casts/suc384jly50n" data-track-category="body text link">video</a> taken by the Korean team that showed a coin-shaped sample of silvery material wobbling over a magnet. The team said the sample was levitating because of the Meissner effect — a hallmark of superconductivity in which a material expels magnetic fields. Multiple unverified videos of LK-99 levitating subsequently circulated on social media, but none of the researchers who initially tried to replicate the findings observed any levitation.</p><h2>Half-baked levitation</h2><p>Several red flags popped out to Derrick van Gennep, a former condensed-matter researcher at Harvard University in Cambridge, Massachusetts, who now works in finance but was intrigued by LK-99. In the video, the same edge of the sample seemed to stick to the magnet, and it seemed delicately balanced. By contrast, superconductors that levitate over magnets can be spun and even held upside-down. “None of those behaviors look like what we see in the LK-99 videos,” van Gennep says.</p><p>He thought LK-99’s properties were more likely the result of ferromagnetism. So he constructed a pellet of compressed graphite shavings with iron filings glued to it. A <a href="https://twitter.com/VanGennepD/status/1688052003216261120" data-track="click" data-label="https://twitter.com/VanGennepD/status/1688052003216261120" data-track-category="body text link">video</a> made by Van Gennep shows that his disc — made of non-superconducting, ferromagnetic materials — mimicked LK-99’s behaviour.</p><p>On 7 August, the Peking University team reported that this “half-levitation” appeared in their LK-99 samples because of ferromagnetism. “It’s exactly like an iron-filing experiment,” says Yuan Li, a condensed-matter physicist and study co-author. The pellet experiences a lifting force but it’s not enough to levitate — only enough to balance on one end.</p><p>Li and his colleagues measured their sample’s resistivity, and found no sign of superconductivity. But they couldn’t explain the sharp resistivity drop seen by the Korean team.</p><h2>Impure samples</h2><p>In their preprint, the Korean authors note one particular temperature at which LK-99’s showed a tenfold drop in resistivity, from about 0.02 ohms per centimetre to 0.002 ohms per cm. “They were very precise about it. 104.8ºC,” says Prashant Jain, a chemist at the University of Illinois Urbana–Champaign. “I was like, wait a minute, I know this temperature.”</p><p>The reaction that synthesizes LK-99 uses an unbalanced recipe: for every 1 part copper-doped lead phosphate crystal — pure LK-99 — it makes, it produces 17 parts copper and 5 parts sulfur. These leftovers lead to numerous impurities — especially copper sulfide, which the Korean team reported in its sample.</p><p>Jain, a copper-sulfide expert, remembered 104ºC as the temperature at which Cu<sub>2</sub>S undergoes a phase transition if exposed to air. Below that temperature, Cu<sub>2</sub>S’s resistivity drops dramatically — a signal almost identical to LK-99’s purported superconducting phase transition. “I was almost in disbelief that they missed it.” Jain published a preprint<sup><a href="#ref-CR7" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">7</a></sup> on the important confounding effect on 7 August.</p><p>The next day, the CAS team reported on the effects of Cu<sub>2</sub>S impurities in LK-99. “Different contents of Cu<sub>2</sub>S can be synthesized using different processes,” says Jianlin Luo, a CAS physicist. The researchers tested two samples — the first heated in a vacuum, which resulted in 5% Cu<sub>2</sub>S content, and the second in air, which gave 70% Cu<sub>2</sub>S content.</p><p>The first sample’s resistivity increased relatively smoothly as it cooled, and appeared similar to samples from other replication attempts. But the second sample’s resistivity plunged near 112 ºC (385K) — closely matching the Korean team’s observations.</p><p>“That was the moment where I said, ‘Well, obviously, that’s what made them think this was a superconductor,’” says Fuhrer. “The nail in the coffin was this copper sulfide thing.”</p><p>Making conclusive statements about LK-99’s properties is difficult, because the material is finicky and samples contain varying impurities. “Even from our own growth, different batches will be slightly different,” says Li. But Li argues that samples that are close enough to the original are sufficient for checking whether LK-99 is a superconductor in ambient coniditions.</p><h2>Crystal clear</h2><p>With strong explanations for the resistivity drop and the half-levitation, many in the community were convinced that LK-99 was not a room-temperature superconductor. But mysteries lingered — namely, what were the material’s actual properties?</p><p>Initial theoretical attempts using an approach called density functional theory (DFT) to predict LK-99’s structure had hinted at interesting electronic signatures called ‘flat bands’. These are areas where the electrons move slowly and can be strongly correlated. In some cases, this behavior leads to superconductivity. But these calculations were based on unverified assumptions about LK-99’s structure.</p><p>To better understand the material, the US–European group<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup> performed precision X-ray imaging of their samples to calculate LK-99’s structure. Crucially, the imaging allowed them to make rigorous calculations that clarified the situation of the flat bands: they were not conducive to superconductivity. Instead, the flat bands in LK-99 came from strongly localized electrons, which cannot ‘hop’ in the way a superconductor requires.</p><p>On 14 August, a separate team, at the Max Planck Institute for Solid State Research in Stuttgart, Germany, reported<sup><a href="#ref-CR6" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">6</a></sup> synthesizing pure, single crystals of LK-99. Unlike previous synthesis attempts that relied on crucibles, the researchers used a technique called floating zone crystal growth that allowed them to avoid introducing sulfur into the reaction, eliminating the Cu<sub>2</sub>S impurities.</p><p>The result was a transparent purple crystal — pure LK-99, or Pb<sub>8.8</sub>Cu<sub>1.2</sub>P<sub>6</sub>O<sub>25</sub>. Separated from impurities, LK-99 is not a superconductor, but an insulator with a resistance in the millions of ohms — too high to run a standard conductivity test. It shows minor ferromagnetism and diamagnetism, but not enough for even partial levitation. “We therefore rule out the presence of superconductivity,” the team concluded.</p><p>The team suggests that the hints of superconductivity seen in LK-99 were attributable to Cu<sub>2</sub>S impurities, which are absent from their crystal. “This story is exactly showing why we need single crystals,” says Pascal Puphal, a specialist in crystal growth and the Max Planck physicist who led the study. “When we have single crystals, we can clearly study the intrinsic properties of a system.”</p><h2>Lessons learned</h2><p>Many researchers are reflecting on what they’ve learned from the summer’s superconductivity sensation.</p><p>For Leslie Schoop, a solid-state chemist at Princeton University in New Jersey, who co-authored the flat-bands study, the lesson about premature calculations is clear. “Even before LK-99, I have been giving talks about how you need to be careful with DFT, and now I have the best story ever for my next summer school,” she says.</p><p>Jain points to the importance of old, often overlooked data — the crucial measurements that he relied on for the resistivity of Cu<sub>2</sub>S were published in 1951.</p><p>While some commentators have pointed to the LK-99 saga as a model for reproducibility in science, others say that it’s an unusually swift resolution of a high-profile puzzle. “Often these things die this very slow death, where it’s just the rumors and nobody can reproduce it,” says Fuhrer.</p><p>When copper oxide superconductors were discovered in 1986, researchers leapt to probe their properties. But nearly four decades later, there is still debate over the material’s superconducting mechanism, says Vishik. Efforts to explain LK-99 came readily. “The detective work that wraps up all of the pieces of the original observation — I think that’s really fantastic,” she says. “And it’s relatively rare.”</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What happens to all the stuff we return? (145 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2023/08/21/the-hidden-cost-of-free-returns</link>
            <guid>37149327</guid>
            <pubDate>Wed, 16 Aug 2023 16:15:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2023/08/21/the-hidden-cost-of-free-returns">https://www.newyorker.com/magazine/2023/08/21/the-hidden-cost-of-free-returns</a>, See on <a href="https://news.ycombinator.com/item?id=37149327">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><figure data-testid="IframeEmbed"><div data-testid="IframeEmbedContainer"><p><span><inline-embed type="callout" meta="%7B%22type%22%3A%22callout%22%2C%22name%22%3A%22dropcap%22%2C%22body%22%3A%22%3Cp%3EListen%20to%20this%20article.%3C%2Fp%3E%5Cn%22%2C%22attrs%22%3A%7B%7D%7D" ref=""><p>Listen to this article.</p>
</inline-embed></span></p></div></figure><p>The twentysomething daughter of a friend of mine recently ordered half a dozen new dresses. She wasn’t planning to keep the lot; she’d been invited to the wedding of a college classmate and knew in advance that she was going to send back all but the one she liked best. “Swimsuits and dresses for weddings—you never buy just one,” Joanie Demer, a co-founder of the Krazy Coupon Lady, a shopping-strategy Web site, told me. For some online apparel retailers, returns now average forty per cent of sales.</p><p>Steady growth in Internet shopping has been accompanied by steady growth in returns of all kinds. A forest’s worth of artificial Christmas trees goes back every January. Bags of green plastic Easter grass go back every spring. Returns of large-screen TVs surge immediately following the Super Bowl. People who buy portable generators during weather emergencies use them until the emergencies have ended, and then those go back, too. A friend of mine returned so many digital books to Audible that the company now makes her call or e-mail if she wants to return another. People who’ve been invited to fancy parties sometimes buy expensive outfits or accessories, then return them the next day, caviar stains and all—a practice known as “wardrobing.” Brick-and-mortar shoppers also return purchases. “Petco takes back dead fish,” Demer said. “Home Depot and Lowe’s let you return dead plants, for a year. You just have to be shameless enough to stand in line with the thing you killed.” It almost goes without saying that Americans are the world’s leading refund seekers; consumers in Japan seldom return anything.</p><p>Earlier this year, I attended a three-day conference, in Las Vegas, conducted by the Reverse Logistics Association, a trade group whose members deal in various ways with product returns, unsold inventories, and other capitalist jetsam. The field is large and growing. Dale Rogers, a business professor at Arizona State, gave a joint presentation with his son Zachary, a business professor at Colorado State, during which they said that winter-holiday returns in the United States are now worth more than three hundred billion dollars a year. Zachary said, “So one and a half per cent of U.S. G.D.P.—which would be bigger than the G.D.P. of many countries around the world—is just the stuff that people got for Christmas and said, ‘Nah, do they have blue?’&nbsp;” The annual retail value of returned goods in the U.S. is said to be approaching a trillion dollars.</p><p>Most online shoppers assume that items they return go back into regular inventory, to be sold again at full price. That rarely happens. On the last day of the R.L.A. conference, I joined a “champagne roundtable” led by Nikos Papaioannou, who manages returns of Amazon’s house-brand electronic devices, including Kindles, Echos, and Blink home-security systems. He said that every item that’s returned to Amazon is subjected to what’s referred to in the reverse-logistics world as triage, beginning with an analysis of its condition. I asked what proportion of triaged products are resold as new.</p><p>“It’s minimal,” he said. “I’m not going to give you a specific number, because it’s so dependent on the product category. But our approach with this question is that, if the seal has been broken, if the wrap is not intact, then it’s not going back to the shelf.” Even though Papaioannou understands this fact as well as anyone, he said, he often shops the way the rest of us do. When he buys shoes, for example, he typically orders two pairs, a half size apart. In brick-and-mortar stores, a pair of tried-on shoes will be re-boxed and reshelved. “From an Amazon viewpoint, the moment the box opens, you’ve lost the opportunity,” he said.</p><p>For a long time, a shocking percentage of online returns were simply junked. The industry term is D.I.F., for “destroy in field.” (The Web site of Patriot Shredding, based in Maryland, says, “Product destruction allows you to protect your organization’s reputation and focus on the future.”) This still happens with cheap clothes, defective gadgets, and luxury items whose brand owners don’t want a presence at Ocean State Job Lot, but, in most product categories, it’s less common than it used to be. Almost all the attendees at the R.L.A. conference, of whom there were more than eight hundred, are involved, in one way or another, in seeking profitable, efficient, and (to the extent possible) environmentally conscionable ways of managing the detritus of unfettered consumerism. “Returns are inherently entrepreneurial,” Fara Alexander, the director of brand marketing at goTRG, a returns-management company based in Miami, told me. She and many thousands of people like her are active participants in the rapidly evolving but still only semi-visible economic universe known as the reverse supply chain.</p><p>People who weren’t born yesterday, but almost, often assume that easy refunds and exchanges began with the online shoe store Zappos, which was founded in 1999. Tony Hsieh, the company’s legendary late C.E.O., offered free returns for up to a year after purchase and encouraged people to order items in multiple styles and sizes. That policy, which was backed by intensely personal customer service, was so popular that the company’s revenues grew more than sixfold in four years. Amazon started a similar shoes-and-accessories site, called Endless, but it eventually gave up trying to compete, having bought Zappos for $1.2 billion.</p><p>America’s true refund pioneer was born a century before Hsieh, on a farm in northwestern Missouri. He moved to Kemmerer, Wyoming, in 1902, in order to become a one-third owner of a general store that was part of a small chain, called Golden Rule. Within a few years, he had bought out his partners and opened more stores, and in 1913 he consolidated his holdings under his own name: J.&nbsp;C. Penney. (The initials stand for James Cash.) Among his innovations was allowing customers to return anything, no questions asked. That approach made a permanent impression on Sam Walton, who went to work at a Penney’s store, in Des Moines, in 1940, immediately after graduating from the University of Missouri. Twenty-two years later, Walton founded his own chain, Walmart, and adopted a similarly generous return policy, which is still in effect. “Sam Walton was very, very customer-centric,” Chuck Johnston, who served as Walmart’s senior director of returns between 2005 and 2012 and is now the chief strategy officer at goTRG, told me. “People would bring in stuff that was clearly from Sears, and we would take it back, because we wanted a happy customer.” (Homer&nbsp;Simpson: “The customer’s always right; that’s why everyone likes us.”)</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>A century ago, the average return rate at Penney’s was probably something like two per cent; before Internet shopping truly took hold, retail returns had risen to more like eight or ten per cent. Returns to online retailers now average close to twenty per cent, and returns of apparel are often double that. Among the many reasons: products often look nothing like their online images—such as a crocheted bikini top that was barely big enough for the purchaser’s cat—and colors and fabrics appear different on different screens.</p><p>The pandemic accelerated growth in online shopping, and therefore in returns, by several years. Quarantined lawyers bought fewer neckties but more sweatpants and bedroom slippers. People who were suddenly forced to work from home ordered desks, chairs, and computers. In 2021, UPS delivered a huge unassembled storage unit to my house. It was actually meant for a neighbor, but I opened the box because I, too, had ordered a huge unassembled storage unit. (Like many people, my neighbor and I had decided that <em>COVID</em> had given us an opportunity to organize our swelling hoard of household crap, including household crap we’d bought because of <em>COVID</em>. I texted my neighbor, and he drove over and picked up his box—no return necessary.) Pre-pandemic, a common shopping strategy was to study possible purchases in a regular store, then save a few dollars by ordering from Amazon. When in-person shopping became difficult, the best way to compare products was to order multiples and send back the rejects.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a26959&quot;}" href="https://www.newyorker.com/cartoon/a26959" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Roz Chast</span></p></div></span></p></figure><p>Returns are expensive for sellers, since shipping alone often costs more than the items can be resold for. Many retailers have responded by shrinking their refund windows or by imposing fees for postage or so-called restocking. Some sellers offer store credit only. Amazon now adds a “frequently returned item” label to listings of problematic offerings and encourages potential purchasers to double-check descriptions and customer reviews of those items before ordering. The online business model of the eyeglasses seller Warby Parker is based on easy returns: customers can order as many as five frames, at no risk, to try on at home. The company still offers that option but has reduced return costs by employing an increasingly sophisticated online tool that allows customers to try on glasses virtually. (It also has physical stores, which have mirrors.) Back in the mail-order era, L.&nbsp;L. Bean suggested that shoe customers include a tracing of their foot in the envelope with their order form—an effective way to reduce returns, but more troublesome than ordering multiple pairs.</p><p>Despite the cost, retailers worry that discouraging returns discourages buying in the first place, driving revenues down. Easy returns are like free shipping: they can be a dealmaker or a deal-breaker when a consumer is deciding where to shop, even though in both cases the cost is ultimately borne by the consumer. Most online mattress sellers offer free returns, in some cases for up to a year; used mattresses can’t be resold, so the loss, usually some eight or nine per cent of sales, is folded into prices. Johnston said, “You’ve got to tread carefully, if you try to ratchet back ease of returns, so that you don’t drive your customer to your competitor.”</p><p>As a consequence, even as sellers are subtly and not so subtly discouraging returns, they’re also exploring ways to make them easier. Some Target stores now have drive-up refund windows. Many online returns no longer have to be repackaged: just get a QR code on the seller’s site and take the unboxed item to a location that consolidates shipments. Amazon offers Prime customers a seven-day “try before you buy” option on selected apparel and accessories. (You pay only for what you keep.) You might think that retailers would be pleased when customers fail to send back items they don’t want, but that isn’t true if those customers remain unhappy. One of the most popular presenters at R.L.A. was Spencer Kieboom, a former major-league baseball player, whose company, Pollen Returns, uses underemployed rideshare and delivery drivers to pick up unwanted items, for free, at buyers’ homes, thereby sparing them the nuisance of schlepping things to UPS on their own.</p><p>Some retailers simply refund certain purchases, no need to send anything back. (“When you ship a hundred-pound bag of dog food, you’re probably losing money on it already,” Johnston told me.) My wife ordered a funny poster for a high-school reunion, then decided it wasn’t funny enough. When she tried to return it, Amazon told her to keep it, and refunded her $32.72. Perhaps surprisingly, companies that sell sofa beds, dining tables, and other bulky, heavy items often do the same, because return freight is so expensive.</p><p>“There are people who think that open returns are an idea whose time has come and gone, but it’s a hallmark of successful American retail,” Dale Rogers told me. “If you make it easy to shop, and you reduce the risk to the consumer, what you get is a lifetime consumer.” It’s probably not a coincidence that the world’s two biggest retailers—Walmart, with revenues of five hundred and seventy-three billion dollars in 2022, and Amazon, with four hundred and sixty-nine billion—also offer some of the easiest returns.</p><p>Three years ago, the producers of a Canadian television show called “Marketplace” ordered boots, diapers, a toy train, a coffee maker, a printer, and several other items from Amazon Canada. They concealed a G.P.S. tracking device inside each one, then returned everything and monitored what happened next. Some of the items travelled hundreds of miles in trucks, with intermediate stops at warehouses and liquidation centers, ultimate disposition unknown. A brand-new women’s backpack ended up in a waste-processing center, en route to a landfill. The show included a surreptitiously recorded conversation with an employee of a “product-destruction” facility, who described receiving truckload after truckload of Amazon returns and shredding everything—ostensibly for recycling, although the recoverable content of a chewed-up random selection of consumer goods is not high.</p><p>If you leave money lying around, someone will pick it up. One morning at the R.L.A. conference, I spent half an hour with two executives of Liquidity Services, a company that, according to its Web site, offers “circular commerce solutions” to businesses of all kinds, in part by selling “any item in any condition, anywhere in the world.” John Daunt, the chief commercial officer, said, “It sounds like selling used stuff, but there’s a lot more to it than you would think.” Liquidity Services operates eight regional warehouse-size facilities in North America. The one closest to New York is in Pittston, Pennsylvania, at the outer edge of a business park that also includes distribution or return facilities owned by Amazon, Home Depot, Lennox, Neiman Marcus, PepsiCo, and a number of smaller companies. The rise of online shopping has been very good for people who build immense, low, flat-roofed metal structures. The Pittston complex includes two enormous buildings that belong to Lowe’s; between them, they have more than fifty acres under roof, plus loading docks and parking spaces for hundreds of semitrailers. Similar complexes now exist all over the United States, in locations that have easy access to highways and airports. More are always under construction.</p><p>For a liquidator, turning a profit depends on having the ability to quickly determine whether an item can be sold again at a reasonable price, and, if so, whether it requires human attention first. Liquidity Services and companies like it use automated and semiautomated routines to sort returned items, repair what can easily be repaired, wipe information from electronic devices, and funnel salable goods to likely customers. “A lot of what we do involves receiving a truckload and then finding another buyer for that truckload, who then will distribute it to mom-and-pop stores and other resellers downstream,” Daunt said. “Or, if they’re not quite big enough to handle that, we may sell it as pallets. We also have direct-to-consumer channels, and people will come to some of our facilities and pick up single items that they’ve bid for online.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>You can register as a buyer on Liquidity Services’ Web site right now, as I did recently, and place bids in any of hundreds of auctions. I didn’t do that, but I did spend a pleasant morning studying items that other people were bidding on, among them a two-pallet lot containing six hundred and fifty-four pounds of sports-related Amazon returns. The lot included seven pellet guns, six clear-plastic umbrellas, an assortment of punching bags and punching balls, a double-bladed lightsabre toy, a shatter-resistant over-the-door mini basketball hoop, eight yoga mats, a minnow trap, an indoor exercise trampoline, a pair of hiking poles, a kickboxing shield, a car refrigerator, two hoverboards (one with Bluetooth and one without), a jump-rope rack, a quiver’s worth of crossbow bolts, a fourteen-gallon red plastic gas can with a siphon pump, a set of four badminton racquets, and a mountain-bike handlebar. There were a hundred and fourteen items in all, and Liquidity Services had estimated their combined original retail value as six thousand five hundred and seventy-six dollars. The lot ended up attracting fifty bids. The winner paid nine hundred and twenty-five dollars, shipping not included. None of the fifty bidders were willing to offer more than fifteen cents on the dollar, and even at that price they were taking a chance, since there was no guarantee that any particular item would still function. Returned items are often damaged, dented, scratched, or inoperable, and even ones that don’t look too bad can be missing parts or accessories.</p><p>I also followed the auction for a truckload of women’s designer shoes: a little more than four tons of returns, all in their boxes, many in brand-new condition, with an original retail value that the company estimated as a hundred and eighty-one thousand seven hundred dollars. That auction expired with no bids, even though two hundred and fifty potential buyers, plus me, had looked at it. That outcome helps to explain why one R.L.A. attendee described apparel returns to me as “a nightmare.” Clothing is tough: fashions go out of fashion quickly, and the items are likely to be one-offs.</p><p>When I got home from Las Vegas, I discovered that I live not far from one of the few companies that deal successfully with high volumes of apparel returns, out-of-stock clothing, and excess inventories. It’s called N.E.J., and it’s been in business for more than thirty years. It’s based in Beacon Falls, Connecticut, an old industrial town that, a century ago, was famous for manufacturing rubber shoes. “Apparel is almost like vegetables,” Ed Mascolo, the owner, told me, as he showed me around. “Things can lose value quickly.”</p><p>The key to his business, Mascolo said, is “volume with velocity, supported by predictability.” N.E.J. doesn’t buy unwanted goods and resell them itself; it mainly contracts with large retailers to categorize and repackage truckloads of their returns and overstocks, then ships them to outlets and other secondary channels. On the day that I visited, some two hundred workers in the main building were opening pallet-size shipping containers, called Gaylords, and sorting their contents into wheeled bins. I watched other workers sorting, folding, bagging, hanging, boxing. Some were “delabelling” new arrivals—using an indelible marker to draw a black line across a tag or to add a conspicuous dot—in order to mark those items as goods that, among other things, can’t be returned.</p><p>Six years ago, Mascolo decided that he had learned enough about the apparel industry to enter it himself. N.E.J. bought and revived a bankrupt American clothing company called Bills Khakis. It sells pants, shorts, shirts, and other items, all made in the United States. “We custom-hem our pants to the half inch,” he said. “It’s a very old-school pant. Seventeen-inch pocket. Extra belt loops, longer rise. Our customer is fifty to seventy-five, and he tends to be a little more conservative in how he dresses.” When we met, Mascolo was wearing a pair of Bills five-pocket twill khakis (two hundred and twenty-five dollars) and a brown Bills leather belt (ninety-eight dollars). I asked him about his return policy.</p><p>“We take everything back,” he said.</p><p>Last year, in an official statement, Amazon told CNBC that none of its returns are sent to landfills. All that really means is that Amazon itself doesn’t send anything to a landfill, but many returns obviously get there anyway, and some avoid it only by being diverted to what the company described to CNBC as “energy recovery,” a euphemism for burning in a furnace.</p><p>Liquidators must quickly sort and resell goods, usually in bulk. Some companies do more. One of those is America’s Remanufacturing Company, based in Georgia, which contracts with brand owners to receive their returns and, when possible, to repair or refurbish them, so that they can be sold by others. (A.R.C. is also one of Amazon’s so-called external repair venders.) “We never want to just buy returns,” Paul Adamson, the company’s chief revenue officer, told me. “There’s a lack of value.”</p><p>An important moment in Adamson’s career occurred in 1991, when he was a sophomore at the University of New Hampshire and working part time in a RadioShack store. He got a call from someone at a company that provided rapid-turn-around computer-maintenance contracts to major corporations. The caller desperately needed a particular part. Adamson found the part, and then found so many others for the same maintenance company that it hired him. (He sat at a desk with a phone and a computer keyboard, but no computer. When he took a call, he would make typing sounds on the keyboard, then say, “Oh, I think I’ve got one left. Let me just call the warehouse and verify.”) He followed that job with several similar ones, “all on the reverse side.” He met A.R.C.’s previous owner through an electronics-recycling company in which he was a partner, and they hit it off.</p><p>When Adamson pitches A.R.C.’s services to potential clients, he told me, he argues that even with items that can be sold again the real value is in information. “We can tell you how many units are being returned and how many of those are defective, and we can help you understand both of those numbers,” he said. Recently, A.R.C.’s technicians determined that one reason customers were returning a particular high-end coffee maker was that it contained a cheap float valve, which was prone to malfunctioning when used with hard water. After identifying the flaw, they helped design a fix by working with the factory in China that was doing the manufacturing. A.R.C. handles so many returns that it can often spot defects before brand owners are aware of them—as it did, recently, after receiving just three returns of an appliance that turned out to have issues with condensation and heat. Some clients now send A.R.C. models for testing before they go to market. It also has clients for whom it does design work only.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>This spring, I met with Adamson at A.R.C.’s facility in Union Point, Georgia, a small town a little more than an hour east of Atlanta. The company’s building there is broad, low, and gray, and it’s on a short potholed road with an aspirational name: Industrial Boulevard. There’s a lumber warehouse on the left, a Dollar General on the right, and a cabinetmaking company across the street.</p><p>We walked through the receiving area, a large, open space that was filled with recent arrivals—tilting piles of household appliances, stacks of yellow bins containing miscellaneous Amazon returns—and stopped in front of a pallet on which half a dozen Husqvarna two-thousand-pounds-per square-inch electric pressure washers, made under a license by Briggs &amp; Stratton, had been stacked and bound with plastic stretch wrap. (A pressure washer is many homeowners’ second-favorite power tool, after their chainsaw. It shoots a stream of water at high velocity, and can be used to clean a roof, blast mold off a wooden deck, or scare away a bear, as a friend of mine did after being surprised by one while scrubbing down the inside of his swimming pool.) As Adamson and I watched, workers sorted units by model and year of manufacture. They checked electrical components and replaced damaged parts with parts they’d salvaged from returns they couldn’t repair. Much of the refurbishing was done on a manufacturing line that A.R.C. bought from a Briggs &amp; Stratton plant, in Wauwatosa, Wisconsin, and modified, in part by adding a car-wash-like cleaning system to one end.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a25207&quot;}" href="https://www.newyorker.com/cartoon/a25207" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>“The catch of the day looks a lot like that fish above the door, but a bit fresher.”</span></p><p><span>Cartoon by Frank Cotham</span></p></div></span></p></figure><p>For every item it processes, A.R.C. knows the potential resale price, what percentage of that price the brand owner is willing to spend on refurbishment, and the cost of each potential intervention. Some problems are too expensive to address; pressure washers with broken pumps are stripped of usable elements and thrown into a steel hopper, to be sent later to a local recycling company, which shreds them and recovers as much salable metal and plastic as it can. At the end of the line, a worker replaced each Husqvarna label with one from Murray, a brand that Briggs &amp; Stratton owns (and therefore a name it doesn’t have to license). Each unit also received a new serial number and a new box, which clearly identified it as a refurb. “These will all end up at the discount chain Ollie’s, where they’ll sell for maybe half of what a new one costs,” Adamson said. “Ollie’s picked up twelve truckloads here in the past week and a half, and they have another twenty or so to go—another ten thousand units over the next six weeks.” The pandemic was good&nbsp;for the refurb market, because in many product categories supply-chain problems made new items scarce.</p><p>A large number of the Amazon returns that A.R.C. receives, Adamson said, are “remorse returns”: you order something late at night after drinking too much wine, or maybe you and your spouse accidentally order the same thing. I saw bins of window curtains in another part of the building; all were from Amazon, many in packages that hadn’t been opened. Pressure washers, by contrast, are often returned because the people who bought them, usually men, don’t read instructions. “You’re always supposed to hook a pressure washer up to water before you turn it on, but a lot of people don’t do that, and they burn up the motor,” Adamson said. I asked whether Briggs &amp; Stratton couldn’t prevent that problem by adding a cutoff switch to the water tank. He said that such a fix was unlikely to be cost-effective, and that a more practical solution would be to add an extra warning tag or sticker.</p><p>Elsewhere, I saw technicians at long counters working on robotic vacuum cleaners. The units were plugged into outlets under the counter—they have to be charged before they can be evaluated—and hundreds, if not thousands, more were stacked nearby, on tall warehouse shelves. “The No. 1 issue with robot vacs is that people don’t know how to use them,” Adamson said. This is partly because the buyers tend to be older, but also because successfully making the necessary Wi-Fi connection can be frustrating even to people who do read instructions—an issue with other products as well. “A really good partner of ours does over fifty per cent of all the refurbishing of HP consumer printers in the U.S.,” Adamson said. “On all the newer printers, the only connection option is Wi-Fi, so when they refurb them they include a printer cable. Problem solved.”</p><p>Adamson told me that he used to be “an ardent hater” of companies that merely buy and sell returns. “I thought they just demonstrated the inefficiency of the reverse supply chain,” he said. “But my mind has changed over the years.” The fact that A.R.C. can’t profitably refurbish a particular item doesn’t mean that it won’t have value to someone else, even if it’s just a few cents’ worth of ground-up plastic. “There’s a guy in a small town in Alabama who buys trailer loads of returned air-conditioners from us,” he said. “When I Googled his property address, I saw that it’s a double-wide on four acres. He buys A.C.s that we can’t refurbish economically, then tinkers with them and sells them locally. It’s stuff I’m never going to touch, but he makes a living at it.”</p><p>The next day, I visited a different A.R.C. facility, this one in Augusta, an hour east of Union Point, and was shown around by David Hogan, the company’s C.E.O. At a workbench, two technicians were repairing upright vacuum cleaners, which were deluxe enough that A.R.C. could cost-effectively give them lots of individual attention. “We receive units that were very clearly just run until they stopped working,” Hogan said. “I mean, you’ve got to empty it, right? But some people don’t realize that.” Many American consumer goods are manufactured in Asia, for companies whose U.S. presence is limited to little more than marketing and sales departments. For companies like that, A.R.C. performs quality-control functions that used to be handled in-house. “You can’t beat the information you get from a product once a customer has touched it,” Hogan said.</p><p>The two technicians that Hogan and I watched are members of a rapidly vanishing species: people who know how to repair stuff. It used to be that when something went wrong with our dishwasher, washing machine, or oven, my wife or I would call a guy who owned a local appliance-repair company. Once, he got our dishwasher working again by taking apart the grinder and removing what he guessed were broken pieces of ceramic. (They were actually coyote teeth. Long story.) The last time I called him, seven or eight years ago, he said that he’d had to get a job as a greeter at Home Depot, because nowadays when appliances malfunction most people simply buy new ones.</p><p>That change is partly the result of consumer ignorance and laziness, but manufacturers are at fault, too. Almost all modern appliances contain electronics, which not only have a limited life span but are also usually impossible to repair and expensive to replace. Our former repairman once told my wife and me that we should always buy the “dumbest” appliances we could find. That was excellent advice, but it’s close to useless now, since even blenders and coffee makers contain microchips. He also told us that the deadliest enemy of electronic components is heat, and that, as a consequence, we should never self-clean an oven, never install two ovens side by side, and definitely never simultaneously self-clean two ovens that had been installed side by side—three valuable lessons that we learned the hard way.</p><p>Another challenge is that few products today are manufactured with repair in mind. “You see it when you get inside the product, as we do,” Hogan said. “A lot of it is materials selection, or the way the assembly was executed.” Two significant impediments to repair: components that are glued together rather than screwed, and pieces that were snapped together with plastic fasteners that break off when the pieces are pulled apart. A service that A.R.C. offers to some of its clients is what it calls same-unit repairs: something goes wrong, under warranty, with an expensive item like a shop vac, and the manufacturer sends you a UPS label addressed to A.R.C., whose technicians repair it and ship it back within a day or two. The company is currently building that side of the business, but it’s viable only with high-quality items, which don’t fall apart when you open them up.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>“At this company, we talk about how frustrated we are with some return practices—which is funny, because they’re what keep us in business,” Hogan said. He recently took part in a panel discussion at the Ray&nbsp;C. Anderson Center for Sustainable Business, at Georgia Tech, his alma mater. The topics included some of the same design issues we’d just been discussing—component quality, difficulty of repair, product life expectancy. He had asked the people in the room to imagine a world in which products were so well made and so easy to repair that a company like A.R.C. wouldn’t need to exist.</p><p>“I said, ‘Let me just theoretically offer you a deal,’&nbsp;” he told me. “&nbsp;‘I’ll sell you a computer for the same price as the one you have now—a nice, expensive computer. But it will be twice as durable, and it will weigh half as much, and its battery will last twice as long, and it will have twice the processing power and twice the memory.’&nbsp;” The only condition, he said, would be that returns would not be allowed, for any reason.</p><p>“This was Georgia Tech’s sustainability center, so these were super-smart engineering hippies,” he said. “There were probably forty or fifty people, all M.B.A.s.” Hogan assumed that they would all jump at the deal. But no hands went up—not one.</p><p>“I was blown away,” he said. “It’s just astounding how embedded returns are in our behavior. When I finished my talk, I said, ‘Thank you all. I definitely picked the right industry.’&nbsp;”&nbsp;♦</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spring 6.1 now compatible with virtual threads and JDK 21 (139 pts)]]></title>
            <link>https://github.com/spring-projects/spring-framework/wiki/What%27s-New-in-Spring-Framework-6.x</link>
            <guid>37147996</guid>
            <pubDate>Wed, 16 Aug 2023 14:52:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/spring-projects/spring-framework/wiki/What%27s-New-in-Spring-Framework-6.x">https://github.com/spring-projects/spring-framework/wiki/What%27s-New-in-Spring-Framework-6.x</a>, See on <a href="https://news.ycombinator.com/item?id=37147996">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wiki-body" data-view-component="true">
                <h2>What's New in Version 6.1</h2>
<h3>Core Container</h3>
<ul>
<li>
<a href="https://github.com/spring-projects/spring-framework/issues/23443">General compatibility with virtual threads</a> and JDK 21 overall.</li>
<li>Configuration options for virtual threads: a dedicated <a href="https://docs.spring.io/spring-framework/docs/6.1.0-SNAPSHOT/javadoc-api/org/springframework/core/task/VirtualThreadTaskExecutor.html" rel="nofollow">VirtualThreadTaskExecutor</a> and a <a href="https://docs.spring.io/spring-framework/docs/6.1.0-SNAPSHOT/javadoc-api/org/springframework/core/task/SimpleAsyncTaskExecutor.html#setVirtualThreads(boolean)" rel="nofollow">virtual threads mode on SimpleAsyncTaskExecutor</a>, plus an analogous <a href="https://docs.spring.io/spring-framework/docs/6.1.0-SNAPSHOT/javadoc-api/org/springframework/scheduling/concurrent/SimpleAsyncTaskScheduler.html" rel="nofollow">SimpleAsyncTaskScheduler</a> with a new-thread-per-task strategy and a virtual threads mode.</li>
<li>Lifecycle integration with Project CRaC for JVM checkpoint restore (see <a href="https://docs.spring.io/spring-framework/reference/6.1/integration/checkpoint-restore.html" rel="nofollow">related documentation</a>).</li>
<li>Lifecycle integrated <a href="https://github.com/spring-projects/spring-framework/issues/30831">pause/resume capability</a> and <a href="https://github.com/spring-projects/spring-framework/issues/27090">parallel graceful shutdown</a> for <code>ThreadPoolTaskExecutor</code> and <code>ThreadPoolTaskScheduler</code> as well as <code>SimpleAsyncTaskScheduler</code>.</li>
<li>Async/reactive destroy methods (e.g. on R2DBC <code>ConnectionFactory</code>); see <a href="https://github.com/spring-projects/spring-framework/issues/26991">26691</a>.</li>
<li>Async/reactive cacheable methods, including corresponding support in the <code>Cache</code> interface and in <code>CaffeineCacheManager</code>; see <a href="https://github.com/spring-projects/spring-framework/issues/17559">17559</a> and <a href="https://github.com/spring-projects/spring-framework/issues/17920">17920</a>.</li>
<li>Reactive <code>@Scheduled</code> methods (including Kotlin coroutines); see <a href="https://github.com/spring-projects/spring-framework/pull/29924">22924</a>.</li>
<li>Observation instrumentation of <code>@Scheduled</code> methods; see <a href="https://github.com/spring-projects/spring-framework/issues/29883">29883</a>.</li>
<li>Selecting a specific target scheduler for each <code>@Scheduled</code> method; see <a href="https://github.com/spring-projects/spring-framework/issues/20818">20818</a>.</li>
<li>
<code>Validator</code> factory methods for programmatic validator implementations; see <a href="https://github.com/spring-projects/spring-framework/pull/29890">29890</a>.</li>
<li>
<code>Validator.validateObject(Object)</code> with returned <code>Errors</code> and <code>Errors.failOnError</code> method for flexible programmatic usage; see <a href="https://github.com/spring-projects/spring-framework/issues/19877">19877</a>.</li>
<li>
<code>MethodValidationInterceptor</code> throws <code>MethodValidationException</code> subclass of <code>ConstraintViolationException</code> with violations adapted to <code>MessageSource</code> resolvable codes, and to <code>Errors</code> instances for <code>@Valid</code> arguments with cascaded violations. See <a href="https://github.com/spring-projects/spring-framework/issues/29825">29825</a>, and umbrella issue <a href="https://github.com/spring-projects/spring-framework/issues/30645">30645</a>.</li>
<li>Support for resource patterns in <code>@PropertySource</code>; see <a href="https://github.com/spring-projects/spring-framework/issues/21325">21325</a>.</li>
<li>Support for <code>Iterable</code> and <code>MultiValueMap</code> binding in <code>BeanWrapper</code> and <code>DirectFieldAccessor</code>; see <a href="https://github.com/spring-projects/spring-framework/pull/907">907</a> and <a href="https://github.com/spring-projects/spring-framework/issues/26297">26297</a>.</li>
<li>Revised <code>Instant</code> and <code>Duration</code> parsing (aligned with Spring Boot); see <a href="https://github.com/spring-projects/spring-framework/issues/22013">22013</a>.</li>
<li>Support for letters other than A-Z in property/field/variable names in SpEL expressions; see <a href="https://github.com/spring-projects/spring-framework/issues/30580">30580</a>.</li>
<li>Support for registering a <code>MethodHandle</code> as a SpEL function (see <a href="https://docs.spring.io/spring-framework/reference/6.1/core/expressions/language-ref/functions.html" rel="nofollow">related documentation</a>).</li>
</ul>
<h3>Data Access and Transactions</h3>
<ul>
<li>Common <code>TransactionExecutionListener</code> contract with before/afterBegin, before/afterCommit and before/afterRollback callbacks triggered by the transaction manager (for thread-bound as well as reactive transactions); see <a href="https://github.com/spring-projects/spring-framework/issues/27479">27479</a>.</li>
<li>
<code>@TransactionalEventListener</code> and <code>TransactionalApplicationListener</code> always run in the original thread, independent from an async multicaster setup; see <a href="https://github.com/spring-projects/spring-framework/issues/30244">30244</a>.</li>
<li>
<code>@TransactionalEventListener</code> and <code>TransactionalApplicationListener</code> can participate in reactive transactions when the <code>ApplicationEvent</code> gets published with the transaction context as its event source; see <a href="https://github.com/spring-projects/spring-framework/issues/27515">27515</a>.</li>
<li>A failed <code>CompletableFuture</code> triggers a rollback for an async transactional method; see <a href="https://github.com/spring-projects/spring-framework/issues/30018">30018</a>.</li>
<li>
<code>DataAccessUtils</code> provides various <code>optionalResult</code> methods with a <code>java.util.Optional</code> return type; see <a href="https://github.com/spring-projects/spring-framework/pull/27735">27735</a>.</li>
<li>The new <code>JdbcClient</code> provides a unified facade for query/update statements on top of <code>JdbcTemplate</code> and <code>NamedParameterJdbcTemplate</code>, with flexible parameter options as well as flexible result retrieval options; see <a href="https://github.com/spring-projects/spring-framework/issues/30931">30931</a>.</li>
<li>
<code>SimplePropertyRowMapper</code> and <code>SimplePropertySqlParameterSource</code> strategies for use with <code>JdbcTemplate</code>/<code>NamedParameterJdbcTemplate</code> as well as <code>JdbcClient</code>, providing flexible constructor/property/field mapping for result objects and named parameter holders; see <a href="https://github.com/spring-projects/spring-framework/issues/26594#issuecomment-1678725276">26594</a>.</li>
<li>
<code>SQLExceptionSubclassTranslator</code> can be configured with an overriding <code>customTranslator</code>; see <a href="https://github.com/spring-projects/spring-framework/issues/24634">24634</a>.</li>
<li>The R2DBC <code>DatabaseClient</code> provides <code>bindValues(Map)</code> for a pre-composed map of parameter values and <code>bindProperties(Object)</code> for parameter objects based on bean properties or record components, see <a href="https://github.com/spring-projects/spring-framework/issues/27282">27282</a>.</li>
<li>The R2DBC <code>DatabaseClient</code> provides <code>mapValue(Class)</code> for plain database column values and <code>mapProperties(Class)</code> for result objects based on bean properties or record components; see <a href="https://github.com/spring-projects/spring-framework/issues/26021">26021</a>.</li>
<li>
<code>BeanPropertyRowMapper</code> and <code>DataClassRowMapper</code> available for R2DBC as well; see <a href="https://github.com/spring-projects/spring-framework/pull/30530">30530</a>.</li>
</ul>
<h3>Web Applications</h3>
<ul>
<li>Spring MVC and WebFlux now have built-in method validation support for controller method parameters with <code>@Constraint</code> annotations. That means you no longer need <code>@Validated</code> at the controller class level to enable method validation via AOP proxy. Built-in method validation is layered on top of the existing argument validation for model attribute and request body arguments. The two are more tightly integrated and coordinated, e.g. avoiding cases with double validation. See <a href="https://github.com/spring-projects/spring-framework/wiki/Upgrading-to-Spring-Framework-6.x#web-applications">Upgrading to 6.1</a> for migration details, <a href="https://github.com/spring-projects/spring-framework/issues/29825">29825</a> for more on the built-in support in M1, and the umbrella issue <a href="https://github.com/spring-projects/spring-framework/issues/30645">30645</a> for related tasks and feedback.</li>
<li>
<a href="https://docs.spring.io/spring-framework/docs/6.1.0-SNAPSHOT/javadoc-api/org/springframework/web/ErrorResponse.html" rel="nofollow">ErrorResponse</a> allows <a href="https://docs.spring.io/spring-framework/reference/6.1/web/webmvc/mvc-ann-rest-exceptions.html#mvc-ann-rest-exceptions-i18n" rel="nofollow">customization</a> of <code>ProblemDetail</code> type via <code>MessageSource</code> and use of custom <code>ProblemDetail</code> through its builder.</li>
<li>Spring MVC throws <code>NoHandlerFoundException</code> or <code>NoResourceFoundException</code> (new in 6.1) to allow consistent handling of 404 errors, including with an RFC 7807 error response. See <a href="https://github.com/spring-projects/spring-framework/issues/29491">29491</a>.</li>
<li>The new <code>RestClient</code> is a synchronous HTTP client that offers an API similar to <code>WebClient</code>, using the same infrastructure as <code>RestTemplate</code>. See <a href="https://github.com/spring-projects/spring-framework/issues/29552">29552</a>.</li>
<li>Jetty-based <code>ClientHttpRequestFactory</code> for use with RestTemplate and RestClient; see <a href="https://github.com/spring-projects/spring-framework/issues/30564">30564</a>.</li>
<li>JDK HttpClient-based <code>ClientHttpRequestFactory</code> for use with RestTemplate and RestClient; see <a href="https://github.com/spring-projects/spring-framework/pull/30478">30478</a>.</li>
<li>Improved buffering in various <code>ClientHttpRequestFactory</code> implementations; see <a href="https://github.com/spring-projects/spring-framework/issues/30557">30557</a>.</li>
</ul>
<h3>Messaging Applications</h3>
<ul>
<li>Interface parameter annotations are detected for messaging handler methods as well (analogous to web handler methods).</li>
<li>The SpEL-based <code>selector</code> header support in WebSocket messaging is now disabled by default and must be explicitly enabled. See <a href="https://github.com/spring-projects/spring-framework/issues/30550">30550</a> and <a href="https://github.com/spring-projects/spring-framework/wiki/Upgrading-to-Spring-Framework-6.x#messaging-applications">Upgrading to 6.1</a> for migration details.</li>
</ul>
<h3>Testing</h3>
<ul>
<li>
<code>ApplicationContext</code> failure threshold support: avoids repeated attempts to load a failing <code>ApplicationContext</code> in the TestContext framework, based on a failure threshold which defaults to 1 but can be configured via a system property (see <a href="https://docs.spring.io/spring-framework/reference/6.1/testing/testcontext-framework/ctx-management/failure-threshold.html" rel="nofollow">related documentation</a>).</li>
<li>Support for recording asynchronous events with <code>@RecordApplicationEvents</code>. See <a href="https://github.com/spring-projects/spring-framework/pull/30020">30020</a>.
<ul>
<li>Record events from threads other than the main test thread.</li>
<li>Assert events from a separate thread – for example with Awaitility.</li>
</ul>
</li>
<li>Support for <code>null</code> in <code>MockHttpServletResponse.setCharacterEncoding()</code>. See <a href="https://github.com/spring-projects/spring-framework/issues/30341">30341</a>.</li>
</ul>
<h2>What's New in Version 6.0</h2>
<h3>JDK 17+ and Jakarta EE 9+ Baseline</h3>
<ul>
<li>Entire framework codebase based on Java 17 source code level now.</li>
<li>Migration from <code>javax</code> to <code>jakarta</code> namespace for Servlet, JPA, etc.</li>
<li>Runtime compatibility with Jakarta EE 9 as well as Jakarta EE 10 APIs.</li>
<li>Compatible with latest web servers: <a href="https://tomcat.apache.org/whichversion.html" rel="nofollow">Tomcat 10.1</a>, <a href="https://www.eclipse.org/jetty/download.php" rel="nofollow">Jetty 11</a>, <a href="https://github.com/undertow-io/undertow">Undertow 2.3</a>.</li>
<li>Early compatibility with <a href="https://spring.io/blog/2022/10/11/embracing-virtual-threads" rel="nofollow">virtual threads</a> (in preview as of JDK 19).</li>
</ul>
<h3>General Core Revision</h3>
<ul>
<li>Upgrade to ASM 9.4 and Kotlin 1.7.</li>
<li>Complete CGLIB fork with support for capturing CGLIB-generated classes.</li>
<li>Comprehensive foundation for <a href="https://spring.io/blog/2022/03/22/initial-aot-support-in-spring-framework-6-0-0-m3" rel="nofollow">Ahead-Of-Time transformations</a>.</li>
<li>First-class support for <a href="https://www.graalvm.org/" rel="nofollow">GraalVM</a> native images (see <a href="https://spring.io/blog/2022/09/26/native-support-in-spring-boot-3-0-0-m5" rel="nofollow">related Spring Boot 3 blog post</a>).</li>
</ul>
<h3>Core Container</h3>
<ul>
<li>Basic bean property determination without <code>java.beans.Introspector</code> by default.</li>
<li>AOT processing support in <code>GenericApplicationContext</code> (<code>refreshForAotProcessing</code>).</li>
<li>Bean definition transformation based on pre-resolved constructors and factory methods.</li>
<li>Support for early proxy class determination for AOP proxies and configuration classes.</li>
<li>
<code>PathMatchingResourcePatternResolver</code> uses NIO and module path APIs for scanning, enabling support for classpath scanning within a GraalVM native image and within the Java module path, respectively.</li>
<li>
<code>DefaultFormattingConversionService</code> supports ISO-based default <code>java.time</code> type parsing.</li>
</ul>
<h3>Data Access and Transactions</h3>
<ul>
<li>Support for predetermining JPA managed types (for inclusion in AOT processing).</li>
<li>JPA support for <a href="https://hibernate.org/orm/releases/6.1/" rel="nofollow">Hibernate ORM 6.1</a> (retaining compatibility with Hibernate ORM 5.6).</li>
<li>Upgrade to <a href="https://r2dbc.io/" rel="nofollow">R2DBC 1.0</a> (including R2DBC transaction definitions).</li>
<li>Aligned data access exception translation between JDBC, R2DBC, JPA and Hibernate.</li>
<li>Removal of JCA CCI support.</li>
</ul>
<h3>Spring Messaging</h3>
<ul>
<li>
<a href="https://docs.spring.io/spring-framework/docs/6.0.0-RC1/reference/html/web-reactive.html#rsocket-interface" rel="nofollow">RSocket interface client</a> based on <code>@RSocketExchange</code> service interfaces.</li>
<li>Early support for Reactor Netty 2 based on <a href="https://netty.io/wiki/new-and-noteworthy-in-5.0.html" rel="nofollow">Netty 5</a> alpha.</li>
<li>Support for Jakarta WebSocket 2.1 and its standard WebSocket protocol upgrade mechanism.</li>
</ul>
<h3>General Web Revision</h3>
<ul>
<li>
<a href="https://docs.spring.io/spring-framework/docs/6.0.0-RC1/reference/html/integration.html#rest-http-interface" rel="nofollow">HTTP interface client</a> based on <code>@HttpExchange</code> service interfaces.</li>
<li>Support for <a href="https://docs.spring.io/spring-framework/docs/6.0.0-RC1/reference/html/web.html#mvc-ann-rest-exceptions" rel="nofollow">RFC 7807 problem details</a>.</li>
<li>Unified HTTP status code handling.</li>
<li>Support for Jackson 2.14.</li>
<li>Alignment with Servlet 6.0 (while retaining runtime compatibility with Servlet 5.0).</li>
</ul>
<h3>Spring MVC</h3>
<ul>
<li>
<code>PathPatternParser</code> used by default (with the ability to opt into <code>PathMatcher</code>).</li>
<li>Removal of outdated Tiles and FreeMarker JSP support.</li>
</ul>
<h3>Spring WebFlux</h3>
<ul>
<li>New <code>PartEvent</code> API to stream multipart form uploads (both on <a href="https://docs.spring.io/spring-framework/docs/6.0.0-RC1/reference/html/web-reactive.html#partevent-2" rel="nofollow">client</a> and <a href="https://docs.spring.io/spring-framework/docs/6.0.0-RC1/reference/html/web-reactive.html#partevent" rel="nofollow">server</a>).</li>
<li>New <code>ResponseEntityExceptionHandler</code> to customize WebFlux exceptions and render RFC 7807 <a href="https://docs.spring.io/spring-framework/docs/6.0.0-RC1/reference/html/web-reactive.html#webflux-ann-rest-exceptions" rel="nofollow">error responses</a>.</li>
<li>
<code>Flux</code> return values for non-streaming media types (no longer collected to <code>List</code> before written).</li>
<li>Early support for Reactor Netty 2 based on <a href="https://netty.io/wiki/new-and-noteworthy-in-5.0.html" rel="nofollow">Netty 5</a> alpha.</li>
<li>JDK <code>HttpClient</code> integrated with <code>WebClient</code>.</li>
</ul>
<h3>Observability</h3>
<p>Direct Observability instrumentation with <a href="https://micrometer.io/docs/observation" rel="nofollow">Micrometer Observation</a> in several parts of the Spring Framework. The <code>spring-web</code> module now requires <code>io.micrometer:micrometer-observation:1.10+</code> as a compile dependency.</p>
<ul>
<li>
<code>RestTemplate</code> and <code>WebClient</code> are instrumented to produce HTTP client request observations.</li>
<li>Spring MVC can be instrumented for HTTP server observations using the new <code>org.springframework.web.filter.ServerHttpObservationFilter</code>.</li>
<li>Spring WebFlux can be instrumented for HTTP server observations using the new <code>org.springframework.web.filter.reactive.ServerHttpObservationFilter</code>.</li>
<li>Integration with Micrometer <a href="https://github.com/micrometer-metrics/context-propagation#context-propagation-library">Context Propagation</a> for <code>Flux</code> and <code>Mono</code> return values from controller methods.</li>
</ul>
<h3>Testing</h3>
<ul>
<li>Support for testing AOT-processed application contexts on the JVM or within a GraalVM native image.</li>
<li>Integration with HtmlUnit 2.64+ request parameter handling.</li>
<li>Servlet mocks (<code>MockHttpServletRequest</code>, <code>MockHttpSession</code>) are based on Servlet API 6.0 now.</li>
<li>New <code>MockHttpServletRequestBuilder.setRemoteAddress()</code> method.</li>
<li>The four abstract base test classes for JUnit 4 and TestNG no longer declare listeners via <code>@TestExecutionListeners</code> and
instead now rely on registration of default listeners.</li>
</ul>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brand-new Linux release, which I'm calling the Debian Linux Release (1993) (269 pts)]]></title>
            <link>https://wiki.debian.org/DebianHistory?action=AttachFile&amp;do=get&amp;target=Debian-announcement-1993.txt</link>
            <guid>37147617</guid>
            <pubDate>Wed, 16 Aug 2023 14:30:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.debian.org/DebianHistory?action=AttachFile&#x26;do=get&#x26;target=Debian-announcement-1993.txt">https://wiki.debian.org/DebianHistory?action=AttachFile&#x26;do=get&#x26;target=Debian-announcement-1993.txt</a>, See on <a href="https://news.ycombinator.com/item?id=37147617">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The uses and abuses of Cloud Data Warehouses (124 pts)]]></title>
            <link>https://materialize.com/blog/warehouse-abuse/</link>
            <guid>37146532</guid>
            <pubDate>Wed, 16 Aug 2023 13:10:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://materialize.com/blog/warehouse-abuse/">https://materialize.com/blog/warehouse-abuse/</a>, See on <a href="https://news.ycombinator.com/item?id=37146532">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>Cloud Data Warehouses (CDWs) are increasingly working their way into the dependency graph for important parts of the business: user-facing features, operational tools, customer comms, and even billing. Running this kind of operational work on a CDW might look promising initially but companies paint themselves into a corner as workloads expand:
Either the cost (in warehouse invoices) to deliver the work outpaces the value delivered, or hard performance limits inherent to the design of analytical data warehouses prevent teams from delivering the capabilities necessary to serve the work in production.</p>
<p><strong>Why?</strong>
Operational workloads have fundamental requirements that are diametrically opposite from the requirements for analytical systems, and we’re finding that a tool designed for the latter doesn’t always solve for the former. That said, teams running operational work on the warehouse aren’t completely irrational. There are many good reasons for building this way, especially initially.</p>
<h2 id="what-is-operational">What is operational?</h2>
<p>First, a working definition. An <strong>operational</strong> tool facilitates the day-to-day <strong>operation</strong> of your business.
Think of it in contrast to <strong>analytical</strong> tools that facilitate historical <strong>analysis</strong> of your business to inform longer term resource allocation or strategy.
If an operational system goes down for the day, there are people who will either be unable to do their job, or deliver a degraded service that day.</p>
<table><tbody><tr><td><strong>Analytical Work</strong></td>
   <td><strong>Operational Work</strong></td></tr>
  <tr><td><ul><li>Business Intelligence exploration
</li><li>Ad-Hoc Exploratory Analysis of metrics
</li><li>KPI Dashboards
</li><li>Prototyping and exploring hypothetical scenarios
</li></ul></td>
   <td><ul><li>Internal/External Alerts and Notifications
</li><li>Customer Segmentation
</li><li>Dynamic pricing &amp; recommendations
</li><li>Business Automation and Workflows
</li><li>Online feature serving for ML and AI
</li></ul></td></tr></tbody></table>
<p>To simplify things, most operational work can be generalized as <strong>automated interventions in the business.</strong></p>
<h2 id="how-is-it-different">How is it different?</h2>
<p>Going deeper into the technical requirements for analytical vs operational workloads, there are clear conflicts:</p>
<p><img src="https://res.cloudinary.com/mzimgcdn/image/upload/v1690396955/analytical-vs-operational-workloads-have-different-requirements.gif" alt="Analytical vs Operational Workloads"></p>
<h4 id="static-data-is-a-feature-for-analytical-work-but-a-bug-for-operational-work">Static data is a feature for analytical work, but a bug for operational work.</h4>
<p>When you’re doing iterative exploratory analysis or navigating between business reports, it’s convenient to be able to lock the input data down as a constant and assume only the SQL is changing.
But in operational workloads it’s reversed:
You want to lock down the SQL and always get as close as possible to the “current state” of data to operate on.
You don’t want to send notifications that no longer apply to customers.</p>
<h4 id="analytics-needs-historic-data-operations-needs-fresh-data">Analytics needs historic data, operations needs fresh data.</h4>
<p>Looking at how data changes over time is crucial to analytics, but less so for operations where you mainly just want the data to be as fresh as possible.</p>
<p><img src="https://res.cloudinary.com/mzimgcdn/image/upload/v1690461537/analytical-vs-operational-value-of-data-over-time.png" alt="Different time spans are important for analytical vs operational work"></p>
<h4 id="ad-hoc-sql-queries-are-a-vital-part-of-analytical-work-but-not-operational">Ad-Hoc SQL queries are a vital part of analytical work, but not operational.</h4>
<p>For analyst productivity, analytical tools need to be ready to answer a new SQL query fast, and most CDWs are really optimized for this (and make architectural tradeoffs to make this fast).
The operational workload, on the other hand, is more akin to traditional software development:
SQL might need to be iteratively written on a smaller scale of data in a dev environment, but in production the SQL is locked down by design.</p>
<h4 id="uptime-is-nice-to-have-for-analytics-but-its-mandatory-in-operations">Uptime is nice to have for analytics, but it’s mandatory in operations.</h4>
<p>This one is pretty self-explanatory. Yes, downtime is always annoying, but an operational system going down at 3am results in a pager going off and sleep being ruined. This is seldom the case for an analytical system.</p>
<p>It’s not all opposites, though.
Both types of work add value by combining different sources of data.
Both use SQL queries that are complex, join-heavy, multi-level.
Both need to handle many different team’s workflows without disruption.
A tool built from the ground up for operational purposes might share some design choices with analytical ones, but the differences add up to some pretty stark inefficiencies in both data freshness and total cost.</p>
<h2 id="we-come-to-praise-cloud-data-warehouses-not-to-bury-them">We come to praise Cloud Data Warehouses, not to bury them</h2>
<p>In spite of all this, data teams continue to expand into operational work on the warehouse.
Why?
We asked, here’s what comes up as motivating factors:</p>
<h4 id="the-warehouse-is-often-the-first-place-the-data-can-even-be-joined">The warehouse is often the first place the data can even be joined.</h4>
<p>Because operational source data is coming from multiple systems, the value is in joining that data together - when we see <em>this</em> signal <em>and</em> this <em>other</em> signal, take this action.
If the two signals are coming from a SaaS tool and your transactional database, joining the two sources in application logic can get complicated.
In contrast, a single data engineer can set up the loading and integration of data once, (sometimes it’s as simple as a few clicks in Fivetran) and other teams rarely have to come back with change requests to the pipelines. They just work autonomously in the warehouse, in SQL.
It’s appealing to stretch that model to cover operational work.</p>
<h4 id="the-sql-that-analysts-write-lives-after-them">The SQL that analysts write lives after them.</h4>
<p>The warehouse is where the SQL is first prototyped.
Many operational use cases start with a <em>hypothesis</em>, which needs to be validated with data.
The correct place to do that is on your historical data in your CDW.
So data teams find themselves with a fully prototyped use case, pondering, well, how do I get the data out of the warehouse and into my operational tools?</p>
<h4 id="its-a-way-to-centralize-complex-business-logic">It’s a way to centralize complex business logic.</h4>
<p>Keep in mind that this isn’t a “SQL vs Code” decision: it’s often a “SQL vs opaque point and click integrations” or “SQL vs microservices without clear owners” decision.
Operational workloads are often hidden in glue code, API configuration, and scripts whose creators have long since left the company.
SQL, especially the kind that’s tracked in git repos and organized in dbt projects, is the superior alternative.</p>
<h4 id="it-unlocks-sdlc-best-practices">It unlocks SDLC best practices.</h4>
<p>Dev/Stage/Prod workflows, automated tests, change review via pull requests, CI/CD, centralized logging…
All these things are becoming central to the way modern data teams manage a growing scope of responsibility.</p>
<h2 id="how-did-data-teams-get-here">How did data teams get here?</h2>
<p>Teams like <a href="https://materialize.com/customer-stories/superscript/">Superscript</a> find Materialize after hitting limits in warehouses, but reverse ETL tools like <a href="https://www.getcensus.com/" rel="nofollow">Census</a> and <a href="https://hightouch.com/" rel="nofollow">Hightouch</a> are evidence that others can succeed running some amount of operational work on the warehouse. Here’s why:</p>
<h4 id="the-data-size-frog-is-boiled-slowly">The data size frog is boiled slowly.</h4>
<p>Companies logically put in place “modern data stack” tooling to tackle the historical analytics workloads, and as warehouses have lowered the low-end cost to make themselves viable even for smaller businesses, companies are starting this journey earlier and earlier.
Operational workloads can particularly look viable early, purely because of the small scale of data involved. Data freshness becomes a problem over time as datasets grow, and the ETL pipeline goes from minutes to hours.</p>
<h4 id="its-possible-to-throw-money-at-the-problem">It’s possible to throw money at the problem.</h4>
<p>Initially, companies can pull (expensive) levers in the warehouse to keep up with operational requirements:
They can load data/run dbt more frequently, upgrade the resources dedicated to doing the work, and generally spend more to alleviate freshness.</p>
<p>We spoke to a company that prototyped fraud detection logic in their warehouse. Initially it was workable, data was loaded every 30 minutes and the query completed in 5 minutes. But as they grew, the data for the query grew, causing it to take more than 30 minutes to complete. Eventually they were running compute 24hrs a day just to deliver stale fraud-detection data at hourly intervals. This happened gradually.</p>
<h4 id="its-possible-to-throw-engineering-time-at-the-problem">It’s possible to throw (engineering) time at the problem.</h4>
<p>There are upfront pipeline optimizations that can be done on analytics warehouses, but they only buy performance with complexity. dbt has a useful solution for lowering the amount of data you work over: incremental models that let you specify logic to only take the changed rows, and merge it up. Unfortunately, this requires rewriting your SQL, handling new concepts like late arriving data, and essentially defining an entire lambda architecture in SQL, with all its <a href="https://discourse.getdbt.com/t/on-the-limits-of-incrementality/303" rel="nofollow">associated pitfalls</a>.</p>
<p>Ultimately, we believe serving operational workloads out of a data warehouse is a dead end: Either you run into a hard technical limit that forces you to walk back everything and initiate a major rebuild, or you run out of money as you approach those limits, because you’ve given it all to the warehouse so you can treat it like an OLTP database. That brings us to our next point:</p>
<h2 id="can-you-extend-an-analytical-data-warehouse-to-serve-operations">Can you extend an analytical data warehouse to serve operations?</h2>
<p>Warehouses themselves and an ecosystem of tools around them have recognized this trend and begun adding features to enable operational work, but that won’t solve the core problem. We argue that it comes down to the query model and architectural tradeoffs that were made to solve analytics users first.</p>
<h4 id="the-core-of-the-problem-a-batchorchestrated-query-model">The core of the problem: A batch/orchestrated query model.</h4>
<p>Somewhere deep in the bowels of a datacenter, servers are repeatedly pulling your entire universe of business data out of object storage, running a massive computation on it, and caching the result.
They do the same amount of work every time, even when only a few rows of input and output data change, unless you do the delicate work of writing incremental models.
Getting operational outputs to update when the inputs change is also a delicate exercise of chaining together a waterfall of loads, transforms, and reverse ETL syncs.</p>
<p>As a result of the query model, the rest of the architecture is misaligned with operational requirements too:</p>
<h4 id="a-fragile-serving-layer">A fragile serving layer.</h4>
<p>The first thing every tool querying a CDW does is cache the results (now you have to monitor and worry about cache invalidation, which usually adds a surprising amount of staleness).
This is because the query interface is just not designed for operational use-cases. There are hard, low limits on query concurrency, and point look-ups (<code>SELECT * FROM my_cached_table WHERE user_id=123;</code>) are costly and not performant when queried directly from the CDW, so Redis it is.</p>
<h4 id="loaders-optimized-for-infrequent-updates">Loaders optimized for infrequent updates.</h4>
<p>The problem also works its way into upstream tools, services, even APIs that are two degrees from the warehouse.
Every loading service is designed to build up a batch of updates and merge it in as infrequently as possible.</p>
<h2 id="how-will-operational-work-be-handled-in-the-future">How will operational work be handled in the future?</h2>
<p>Data practices are rapidly evolving, and always have. Consider how our use of CDWs evolved over time: Businesses organically found the ELT model, starting with Looker’s <a href="https://cloud.google.com/looker/docs/derived-tables" rel="nofollow">persistent derived tables (PDTs)</a>. Then the dbt community took a step back to look at the problem and emerged with a generalization of this to use SDLC practices to manage the full complexity.</p>
<p>We think that the next step in the eternal quest to deliver more shareholder value is that <a href="https://ian-macomber.medium.com/data-systems-tend-towards-production-be5a86f65561" rel="nofollow">data teams work will tend towards unlocking production, operational use cases</a>. Operational use cases will drive data teams to pick products that are designed from the ground up to service operational workloads. But that doesn’t mean that data teams will have to give up their tooling. The modern operational tools will have to meet data teams where they are - with dbt, SQL, and a cloud-native design.</p>
					
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How do we save water: Stop growing alfalfa in Imperial County (261 pts)]]></title>
            <link>https://www.desertsun.com/story/opinion/contributors/valley-voice/2023/02/05/growing-alfalfa-in-imperial-county-and-california-wastes-water/69860506007/</link>
            <guid>37146398</guid>
            <pubDate>Wed, 16 Aug 2023 13:00:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.desertsun.com/story/opinion/contributors/valley-voice/2023/02/05/growing-alfalfa-in-imperial-county-and-california-wastes-water/69860506007/">https://www.desertsun.com/story/opinion/contributors/valley-voice/2023/02/05/growing-alfalfa-in-imperial-county-and-california-wastes-water/69860506007/</a>, See on <a href="https://news.ycombinator.com/item?id=37146398">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><article><p>If California is&nbsp;<em>really</em>&nbsp;short of water, why are we shipping it to Asia?</p><partner-banner util-module-path="elements/partner" min-height="600" fluid="" outstream="" momentum=""></partner-banner><p>We can do one simple thing and our water supply crisis will be over. We can stop growing alfalfa.</p><p>The top water-using activity in California is growing alfalfa — a protein-rich type of hay. An alfalfa farmer can crop alfalfa 10 or even 12 times a year and sell it for $260 per ton. These are not the hardworking family farmers of yesteryear. They are giant agribusiness corporations pulling in $1.8 billion from selling&nbsp;<a href="https://protect-us.mimecast.com/s/tMG6CBBXwLtl3oQPECrtZoI?domain=apps1.cdfa.ca.gov">7 million tons of alfalfa&nbsp;</a>every year in California, according to a University of California, Davis study by Daniel Geisseler and William R. Horwath.</p><p>We make this business extremely profitable for them by selling them 3.4 million acre-feet (1 acre foot =325,851 gallons) of water every year for $35 to $60 per acre-foot. The water you get for your house costs you $800 per acre-foot today, and it could be double that in 10 years' time. You could supply the water needs of 40 million people with that 3.4 million acre-feet.</p><partner-inline util-module-path="elements/partner" placement="native-article_link" sizes="[[300, 250], [3, 3]]" min-height="250" fluid="" outstream=""></partner-inline><p>In Arizona, something even more insane is happening. Saudi Arabian companies are pumping up groundwater from beneath land they have purchased and are using it to grow alfalfa for export to the Middle East. They have no plan for replacing this scarce and finite groundwater that accumulated over thousands of years. Incredibly, Arizona does not regulate groundwater extraction in agricultural areas. California didn’t get serious about groundwater management until 2014 so we deserve some criticism too.</p><partner-inline util-module-path="elements/partner" placement="native-article_link" sizes="[[300, 250], [3, 3]]" min-height="250" fluid="" outstream=""></partner-inline><p>Alfalfa is used to feed farm animals like pigs and cows. But we export 70% of what we grow in California to Japan and China. The Japanese raise Kobe beef with it and send it back to us for $200 per pound. The Chinese use it to raise pigs to meet their rapidly expanding demand for pork as they become more affluent and eat more meat.</p><partner-banner util-module-path="elements/partner" fluid="" bottom="" lazy="" min-height="390" outstream=""></partner-banner><p>Why don’t they grow their own alfalfa? Because they&nbsp;<em>are&nbsp;</em>short of water and it is so much cheaper to buy ours. In fact, alfalfa has become an instrument for exporting subsidized water.</p><p>We need to ask ourselves: Is the California desert the right place to grow alfalfa? It needs around 6-foot depth of water applied to every field to sustain this crop for a year at the present cultivation intensity, according to a University of California study,&nbsp;<a href="https://protect-us.mimecast.com/s/y3TfCv2xPoUyoXmOMsXaQRE?domain=alfalfa.ucdavis.edu">“Irrigated Alfalfa Management.”</a></p><cta-atoms-container-inline util-module-path="elements/cta"></cta-atoms-container-inline><p>So, next time you see a truckload of hay going west on the freeway give it a wave!</p><p>Wave goodbye to $13,000 worth of scarce water that was sold to agribusiness for just $1,000 and is now headed overseas on that truck. And while this is happening, that slow-moving ecological train wreck, the Salton Sea, continues to dry up.</p><p>We need to fix this.</p><p><em>Gerald McKenna is a retired civil engineer who serves as secretary-treasurer on the Board of Directors of&nbsp; the Desert Water Agency. These are the writer’s personal opinions and do not reflect the official position of the Board or DWA. His email is&nbsp;<a href="mailto:gerald@geraldmckenna.com">gerald@geraldmckenna.com</a></em></p><partner-banner util-module-path="elements/partner" fluid="" bottom="" lazy="" min-height="600" outstream="" momentum=""></partner-banner><media-image image-set="https://www.gannett-cdn.com/presto/2023/02/02/PPAS/f98ff22b-8fdc-46b5-957e-b2de10220de6-Gerald_McKenna-8839-Edit4.jpg bestCrop, https://www.gannett-cdn.com/presto/2023/02/02/PPAS/f98ff22b-8fdc-46b5-957e-b2de10220de6-Gerald_McKenna-8839-Edit4.jpg?crop=2001,1501,x0,y300 4:3, https://www.gannett-cdn.com/presto/2023/02/02/PPAS/f98ff22b-8fdc-46b5-957e-b2de10220de6-Gerald_McKenna-8839-Edit4.jpg?crop=2001,2668,x0,y165 3:4, https://www.gannett-cdn.com/presto/2023/02/02/PPAS/f98ff22b-8fdc-46b5-957e-b2de10220de6-Gerald_McKenna-8839-Edit4.jpg?crop=2001,1126,x0,y450 16:9" image-alt="." credit="Gerald McKenna" caption="." orientation="vertical" util-module-path="elements/media"></media-image><lit-timestamp slot="timestamp" publishdate="2023-02-05 13:00:44 +0000 UTC" updatedate="2023-02-05 16:54:45 +0000 UTC"></lit-timestamp><p><a alt="Post the article to your Facebook Timeline" data-size="large" onclick="fireNavShareAnalytics('facebook');" rel="noopener" target="_blank"><svg view-box="0 0 24 24">
                <path d="M12.6143832,21 L3.99346182,21 C3.44462725,21 3,20.5550968 3,20.006476 L3,3.99345411 C3,3.44469364 3.44469709,3 3.99346182,3 L20.006608,3 C20.5552331,3 21,3.44469364 21,3.99345411 L21,20.006476 C21,20.5551667 20.5551632,21 20.006608,21 L15.4197395,21 L15.4197395,14.029408 L17.7594454,14.029408 L18.1097832,11.3128446 L15.4197395,11.3128446 L15.4197395,9.57849053 C15.4197395,8.79198274 15.6381418,8.25600363 16.7659836,8.25600363 L18.2044917,8.25537504 L18.2044917,5.82565895 C17.9557072,5.79255313 17.1017938,5.71858885 16.108332,5.71858885 C14.0343128,5.71858885 12.6143832,6.98457234 12.6143832,9.30945332 L12.6143832,11.3128446 L10.2686707,11.3128446 L10.2686707,14.029408 L12.6143832,14.029408 L12.6143832,21 L12.6143832,21 Z"></path>
            </svg><span>Facebook</span></a>
<a alt="Tweet about this article" data-size="large" onclick="fireNavShareAnalytics('twitter')" rel="noopener" target="_blank"><svg view-box="0 0 24 24">
                <path d="M21,6.77573131 C20.338616,7.07692308 19.6265188,7.28060672 18.8795563,7.3716143 C19.6423666,6.9035753 20.2276809,6.16143012 20.5034337,5.27735645 C19.7892235,5.71072589 19,6.02600217 18.1568938,6.19501625 C17.4849445,5.45937161 16.5245642,5 15.461701,5 C13.4236661,5 11.770206,6.69555796 11.770206,8.78656555 C11.770206,9.08342362 11.8019017,9.3716143 11.8652932,9.64897075 C8.79609086,9.4907909 6.07554147,7.98483207 4.25303751,5.69122427 C3.93502377,6.2524377 3.75330164,6.9035753 3.75330164,7.59696641 C3.75330164,8.91007584 4.40517697,10.0693391 5.39619651,10.7486457 C4.79186476,10.7302275 4.22134179,10.5579632 3.72266244,10.276273 L3.72266244,10.3228602 C3.72266244,12.1581798 4.9957739,13.6890574 6.68621236,14.035753 C6.37665082,14.1245937 6.05018489,14.1690141 5.71315372,14.1690141 C5.47543582,14.1690141 5.24300053,14.1462622 5.01796091,14.1018418 C5.4881141,15.6056338 6.85103011,16.7009751 8.46751189,16.7302275 C7.20390914,17.7464789 5.61067089,18.3521127 3.88114105,18.3521127 C3.58320127,18.3521127 3.28843106,18.3347779 3,18.3001083 C4.63444268,19.3726977 6.57633386,20 8.66085578,20 C15.4543053,20 19.1679873,14.2307692 19.1679873,9.22643554 C19.1679873,9.06175515 19.1648177,8.89707476 19.1584786,8.73564464 C19.8800845,8.20151679 20.5066033,7.53521127 21,6.77573131"></path>
            </svg><span>Twitter</span></a>
<a alt="Email this article" onclick="fireNavShareAnalytics('email')" rel="noopener" target="_blank"><svg view-box="0 0 24 24">
            <path d="M3,5.8757627 C3,5.39209232 3.39269552,5 3.8926228,5 L20.1073772,5 C20.6003592,5 21,5.40389442 21,5.8757627 L21,18.1242373 C21,18.6079077 20.6073045,19 20.1073772,19 L3.8926228,19 C3.39964084,19 3,18.5961056 3,18.1242373 L3,5.8757627 Z M12,11.09375 L3,6.74107143 L3,8.48214286 L12,12.8348214 L21,8.48214286 L21,6.74107143 L12,11.09375 Z"></path>
        </svg><span>Email</span></a></p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Today I learned you can pause the Windows Task Manager moving apps around (234 pts)]]></title>
            <link>https://www.theverge.com/2023/8/16/23834125/microsoft-windows-task-manager-pause-shortcut</link>
            <guid>37146268</guid>
            <pubDate>Wed, 16 Aug 2023 12:49:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/8/16/23834125/microsoft-windows-task-manager-pause-shortcut">https://www.theverge.com/2023/8/16/23834125/microsoft-windows-task-manager-pause-shortcut</a>, See on <a href="https://news.ycombinator.com/item?id=37146268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I can’t believe I’ve been struggling with apps in the Task Manager randomly moving around without realizing there’s a simple keyboard shortcut to pause the Task Manager and stop its contents in their tracks. Yup, all you have to do is hold down the CTRL key and it will pause the Task Manager on both Windows 10 and Windows 11, and perhaps even older versions of Windows, too.</p><p>This legitimately useful tip comes <a href="https://twitter.com/JenMsft/status/1691563112175165724">from Jen Gentleman</a>, a Microsoft employee on the Windows engineering team that regularly shares helpful shortcuts and tips for Windows. I’ve used Windows for more than 20 years, and I’m still learning the many ways you can do tasks in the operating system on a monthly basis.</p><p>If you’re used to turning to the Task Manager to end faulty tasks then you might not even need to open it at all soon. Microsoft is working on a <a href="https://www.theverge.com/2023/5/24/23736005/microsoft-windows-11-force-quit-taskbar-option-feature">force quit option to close apps</a> without the Task Manager in Windows 11 simply from a right-click option in the taskbar. But the hold CTRL option to pause the Task Manager will still be useful for everyone who hasn’t upgraded to Windows 11 just yet.</p><p>While I have your attention, you might also like to know that you can <a href="https://www.theverge.com/tldr/2022/3/24/22994423/windows-11-notepad-app-spin-gear-settings">spin the gear in Windows 11’s Notepad app</a>. It’s like a digital fidget spinner, and part of the many delightful little additions to Windows 11. Microsoft has hidden little Easter eggs like this in Windows for decades; there was even a hidden prompt in Windows 95 that was only <a href="https://twitter.com/thebookisclosed/status/1375531201071620100">discovered in 2021</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists are only beginning to understand how PFAS impacting our health (156 pts)]]></title>
            <link>https://www.nytimes.com/2023/08/16/magazine/pfas-toxic-chemicals.html</link>
            <guid>37145701</guid>
            <pubDate>Wed, 16 Aug 2023 11:57:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2023/08/16/magazine/pfas-toxic-chemicals.html">https://www.nytimes.com/2023/08/16/magazine/pfas-toxic-chemicals.html</a>, See on <a href="https://news.ycombinator.com/item?id=37145701">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2023/08/16/magazine/pfas-toxic-chemicals.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[They Tried to Kill Me (285 pts)]]></title>
            <link>https://www.nplusonemag.com/online-only/online-only/how-they-tried-to-kill-me/</link>
            <guid>37145491</guid>
            <pubDate>Wed, 16 Aug 2023 11:33:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nplusonemag.com/online-only/online-only/how-they-tried-to-kill-me/">https://www.nplusonemag.com/online-only/online-only/how-they-tried-to-kill-me/</a>, See on <a href="https://news.ycombinator.com/item?id=37145491">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>In 2022, <em>n+1</em> published four reports (<a href="https://www.nplusonemag.com/online-only/online-only/exodus-from-ukraine/" target="_blank" rel="noopener">1</a>, <a href="https://www.nplusonemag.com/online-only/online-only/sandbagging-in-odessa/" target="_blank" rel="noopener">2</a>, <a href="https://www.nplusonemag.com/online-only/online-only/leave-us-alone/" target="_blank" rel="noopener">3</a>, <a href="https://www.nplusonemag.com/issue-43/politics/in-kherson/" target="_blank" rel="noopener">4</a>) from the war in Ukraine by the Russian journalist Elena Kostyuchenko. In the following essay, Kostyuchenko describes—for the first time—why she fled Ukraine and reveals that she was poisoned last fall in Munich. (The essay originally appeared in Russian on <a href="https://meduza.io/feature/2023/08/15/ya-hochu-zhit-poetomu-ya-pishu-etot-tekst" target="_blank" rel="noopener">Meduza</a>’s website and was translated by Bela Shayevich.) Kostyuchenko’s book <a href="https://shop.nplusonemag.com/products/i-love-russia-by-elena-kostyuchenko" target="_blank" rel="noopener"><em>I Love Russia</em></a>, which includes her reporting from Ukraine and elsewhere, is forthcoming this fall from Penguin Press.</p><div><p><span>I didn’t want to write this</span> for a long time. I feel disgusted, afraid, ashamed.</p><p>I can’t write about everything I know because I have to protect the people who’ve saved my life.</p><p>On February 24, 2022, my country attacked Ukraine.</p><p>On February 24, I went to Ukraine on assignment from <em>Novaya Gazeta, </em>where I had been working for the previous seventeen years.</p><p>I crossed the Polish-Ukrainian border on the night of February 25.</p> <!-- Either there are no banners, they are disabled or none qualified for this location! --><p>Over the course of four weeks, thanks to the incredible support of countless Ukrainians, I was able to file four stories—from the border, Odesa, Mykolaiv, and Kherson. Kherson was under occupation. Getting in and out meant crossing the frontlines twice. In Kherson, Russian soldiers were kidnapping and torturing people. I was able to find people who had survived being tortured. By collating their stories, working in the field, I was able to find where the kidnapped people were being held—a former temporary detention center located at 3 Teploenergetiki Street. I learned the names of forty-four kidnapped people and the circumstances under which they were taken. I published my article and handed over what I had uncovered to the Ukrainian Prosecutor General’s office.</p><p>The next place that I was going to was Mariupol.</p><p>Mariupol was still resisting. There was active combat. On many days, there were no humanitarian corridors. The only occasionally passable road lay through Zaporizhzhia. It often came under fire, and, as you approached Mariupol, the Russian checkpoints began. Nevertheless, people traveled this road every day in order to try to rescue their loved ones from the city as it was being destroyed. Volunteers organized them into columns. I decided to travel with them.</p><p>On March 28, I entered Zaporizhzhia.&nbsp; Waiting at the checkpoint, (the Teroborona were examining my passport and press credentials), I started getting messages from friends. “Assholes.” “Hang in there.” “Let me know if I can help.” That’s how I found out that <em>Novaya Gazeta </em>had shut down. <em>Novaya </em>had received its second warning that year from the state censorship agency, Roskomnadzor, which meant it could now lose its license. I’d been anticipating this, I’d been waiting for it from the moment of the incursion, but I couldn’t know how painful it would be.</p><p>I decided I’d go to Mariupol anyway. Publish my piece wherever I could.</p><p>On March 29, I was meeting with volunteers and the people heading to Mariupol to rescue their relatives. I found someone willing to take me in their car despite my Russian passport.</p><p>We arranged to leave on the 31st.</p><p>I spent March 30, the eve of our trip, in a hotel. I was trying to gather my strength. A colleague from <em>Novaya </em>called me. She asked me if I was going to Mariupol. I was puzzled: only two people from the paper knew I was going to Mariupol—the editor-in-chief, Dmitry Muratov, and my editor Olga Bobrova. I said, Yes, I am going tomorrow. She said, “My sources have gotten in touch with me. They know that you’re going to Mariupol. They say that the Kadyrovites have orders to find you.”</p><p>The Kadyrovites, a Chechen subdivision of Rosgvardia, were actively engaged in the fighting around Mariupol—they manned the checkpoints. I knew that. My colleague said, “They’re not planning to hold you. They are going to kill you. That’s been approved.”</p><p>It was like running into a wall. I went deaf, everything went white. I said, “I don’t believe you.” She said, “That’s what I told them, too, that I didn’t believe them. Then they played me a recording of you talking to someone about Mariupol, planning your trip. I recognized your voice.”</p><p>She hung up, I sat down on the bed. I didn’t think anything, I just sat.</p><p>Forty minutes later, my source from Ukrainian military reconnaissance called me. He said, “We have information that an assassination of a female journalist from <em>Novaya Gazeta </em>is being organized in Ukraine. And all-points bulletin on you has been sent out to every Russian checkpoint.”</p><p>An hour later, Muratov called me. He said, “You can’t go to Mariupol anymore. You have to leave Ukraine this minute.”</p><p>But I couldn’t make myself go.</p><p>The following morning, I woke up to messages from an editor at <em>Novaya. </em>The Russian Prosecutor General’s Office and Roskomnadzor had sent them letters demanding they take my reporting from Ukraine down from their website, or else the site would be blocked. <em>Novaya </em>complied. Somehow, this was what crushed me. I started crying and couldn’t stop. Then rage came in place of the tears, and it filled my entire being.</p><p>I tried to find another way into Mariupol, looking to bypass the Russian checkpoints. This path did not exist—there was active combat everywhere. The only road was through Zaporizhzhia, and they were waiting for me on that road.</p><p>I was incapable of accepting my powerlessness. Rational arguments didn’t work on me. The only thing that stopped me from moving forward was thinking what would happen to the person who agreed to take me in their car. If I got killed, they wouldn’t be spared, either.</p><p>On the night of April 1, I left Ukraine.</p><p>I left in a very bad state. I had lice, mumps, and PTSD. My friends took me in, they passed me from hand to hand. My girlfriend Yana came from Russia, she took care of me, made sure I was eating and sleeping. I was planning on getting better, finishing the book I was writing, and going back to Russia. All of my work, my entire life, my mother and sister—they are all there. The worse the news from home got, the more I felt like I belonged there and nowhere else.</p><p>I thought about how they were going to kill me. But the more I thought about it, the calmer I got. I feel stupid and embarrassed remembering what was going through my head. I didn’t know who had issued the orders, I referred to the killers as “them.” I thought that they’d probably made an emotional decision. The war wasn’t working out the way they wanted at all, everything was going haywire. And I had just gotten in and out of Kherson, right under their noses—of course they’d gotten upset. They started looking into what I was doing, found out I was going to Mariupol, which was, in its entirety, one giant war crime, and this was their diabolical solution for keeping me out of there. The distance between the last Ukrainian checkpoint and the first Russian one was a few kilometers apart—a no-man’s-land, not under anyone’s control. The Russian soldiers could have claimed that I’d never even made it to them. People are always disappearing during a war. Who knows, maybe it’d been the Ukrainian soldiers who’d killed me? I am a Russian journalist after all, and the Ukrainians hate Russians, as everyone knows.</p><p>I thought, At least I’m alive, that is good.</p><p>On the evening of April 28, Muratov called me. He spoke in a very gentle voice. He said, “I know that you want to come home. But you cannot go back to Russia. They will kill you.”</p><p>I hung up the phone and started screaming. I stood in the street and screamed.</p><p>A month later, we were able to meet. Muratov said, They’ll make it look like a hate crime. The people on the right hate lesbians and you’re a lesbian.</p><p>Then I was working on my book. I wrote and only thought about what I was writing. There wasn’t room for anything else in my head and those were the best days.</p><p>At the end of September, I got in touch with Muratov again. I asked him to find out whether I could return to Russia. He called me back several days later. “No. No. No.”</p><p>I found an apartment in Berlin and moved there. On September 29, I began working for the website Meduza. We decided that my first reporting trip would be to Iran. I’d been there and I knew how to work there. I found people who would help me, got a visa, bought clothes. We decided that after Iran, I would go to Ukraine. Meduza asked me to submit the paperwork for a Ukrainian visa before I left.</p><hr><p><span>I couldn’t fill out an application</span> or make an appointment at the embassy on their website—it wouldn’t let me. The Ukrainian Ministry of Foreign Affairs hotline told me their website was being attacked by hackers, and until they were able to deal with it, it’d be impossible. I started looking for contacts within the embassy. I got someone to agree to see me in their consulate in Munich.</p><p>There is no justification for this, and there cannot be, but I have to say that I corresponded about my trip to Munich over Facebook Messenger. It’s not secure and I knew that. But I wasn’t in Russia, I was in Germany. I didn’t even think about the basic tenets of my security, the protocols I’d been following for years.</p><p>On the evening of October 17, I traveled to Munich. I took an overnight train, traveling in a seating car. I took off my shoes, lay down on the seat, and slept. People walked past me, they’d knock into my feet. I kept sleeping.</p><p>On the morning of October 18, I arrived. I went to meet my friend, tried to sleep, then went to the embassy. The staff there questioned me, asking what I was planning to do in Ukraine. They took my documents, but I still wasn’t able to apply for a visa—their internal system was glitching. We decided that I would come again another day.</p><p>My friend picked me up at the embassy and we went to get lunch. We sat outside at a restaurant. While we were sitting there, two different groups of her acquaintances happened to run into us. They came up to our table—there was a man, and then two women. I thought, What a small town Munich is, it really seems like everyone knows each other. I went to the bathroom and came back. All I could think about was the visa—I was unlikely to get it, but what if it worked out?</p><p>Then my friend took me to the train station. As we approached it, she said, “Listen, I have to tell you: you smell bad. Let me find you some deodorant.” She couldn’t find any. I remember I was shocked by what she said—she’s a very tactful person, and she would have never said anything if I hadn’t actually smelled terrible.</p><p>When I got on the train, I found my seat and immediately went to the bathroom. I wet some paper towels and started wiping myself off with them. I was covered in sweat. The sweat smelled strong and strange, like rotten fruit.</p><p>I sat down and started reading the manuscript of my book. After a while, I realized that I was just reading the same paragraph over and over and couldn’t move forward. My head ached.</p><p>I’d gotten Covid three weeks earlier. I thought, Do I really have it again? I called Yana. I said, I feel unwell. I said, I hope it’s not Covid, how will I go to Iran if it is?</p><p>I tried to get back to the book, but I kept feeling worse. My headache got so bad I couldn’t look at things anymore. I kept sweating, I went back to the bathroom and wiped myself off again.</p><hr><p><span>When I got out at the train station</span>, I realized that I couldn’t figure out how to get home. I knew that I needed to transfer to the subway, but I couldn’t figure out how. I considered going outside and calling a cab, but the very thought of having to find my location on the map in the app and figuring out how it corresponded to real streets terrified me. I thought to myself, this is too difficult a task, I won’t manage. I looked for the transfer &nbsp;for a long time. When I finally got down there, I burst into tears—I didn’t know what direction I was supposed to go in. Other passengers helped me.</p><p>The walk home from the subway is five minutes. It took much longer. Every few steps, I had to put my bag down—it seemed unbearably heavy—and rest.</p><p>On the stairs, I got short of breath. I thought to myself, This fucking Covid has really messed me up.</p><p>As soon as I got home, I went to sleep. I hoped that I would feel better when I got up.</p><p>But I only got worse.</p><p>I woke up from a pain in my stomach. It was strange—very strong, but not sharp, it was like it was being turned on and off. I tried to sit up and lay right back down. I felt so dizzy, it was like the room was spinning. With every rotation, I grew more nauseous. I got to the bathroom where I threw up.</p><p>I kept corresponding with the Iranians. I cried. It was supposed to be my first trip for my new job and now, this.</p><p>The pain in my stomach kept getting worse. It was painful to even touch the skin. I barely slept those first few nights—as soon as I would drift off, I would be woken up by the pain. My head kept spinning whenever I sat down or got up.</p><p>On the third day, it became clear that I was not going anywhere and that whatever I had wasn’t Covid.</p><p>It isn’t easy to see a doctor in Berlin. I was only able to get an appointment on October 28, ten days after I became ill.</p><p>It was a regular clinic in my neighborhood. The doctors—there were two of them—both immediately said that I had long Covid. “It can go on for up to six months. If you don’t feel better six months from now, come back.” But they did an ultrasound, too—all clear. They tapped on my stomach. I got them to do some blood tests. I came out of the clinic consoled—it was nothing, I’d get better soon.</p><p>The blood tests came back bad. The levels of ALT and AST in my liver were five times above normal. They tested my urine. There was blood in it.</p><p>The doctors stopped joking around. I was referred to another, more experienced specialist. She said that it was most likely viral hepatitis, which I had contracted during the war. We’ll figure out which one it is and then treat it, she said.</p><p>The hepatitis tests came back negative.</p><p>My symptoms kept changing. My stomach hurt less and I got less dizzy. But I was totally weak. My face started swelling. Then my fingers. I barely managed to take my rings off and could not get them back on again. My fingers looked like sausages. Then my feet started swelling. The swelling kept getting worse, I lost sight of my chin, my face was no longer my face. When I looked in the mirror, it took me a moment to recognize myself. Sometimes my heart would start racing as though I was running. Sometimes my palms and the bottoms of my feet would start to burn, turning red and shiny.</p><p>Everything was exhausting. It was hard to go down the stairs. Sometimes, we would go out for fifteen minutes, half an hour, and I would get so tired that I’d have to go home. I stopped being able to sleep, no longer from pain. It was as though my brain had forgotten how to fall asleep. I’d lay there for hours trying not to wake up Yana, looking up at the ceiling and wondering what was wrong with me.</p><p>My hepatic enzyme levels kept rising. There was still blood in my urine.</p><p>I kept going to doctors. The doctors would come up with theories, test them, come up with new ones. Autoimmune diseases, acute complicated complex pyelonephritis, systemic diseases.</p><p>Meduza put me in touch with a doctor they trusted. The doctor decided to retest me for hepatitis (the tests came back negative). While I was heading home from the hospital, he wrote to me, “Is it possible that you have been poisoned?” I replied, “No, I am not that dangerous.”</p><p>I told Yana, we laughed. She said, Oh yeah, the simplest explanation. She must have been poisoned—she is a Russian journalist.</p><hr><p><span>On December 12</span>, I went back to my neighborhood doctor. I got a new round of tests, the results had gotten worse, my ALT was seven times above normal. We sat in her office. She said nothing, going through her papers. Then she said, “Elena, there are two theories left. The first one is that the antidepressants you’re on may have suddenly started working aberrantly. But you recently changed medications and your symptoms and test results haven’t changed. That’s why we have a second theory. Please try to stay calm. You may have been poisoned.”</p><p>I laughed. The doctor stayed silent. I said, “That’s impossible.” She said, “We’ve ruled out all other options. I’m sorry. You need to go to the toxicology department at Charite.”</p><p>I spent the next three days lying there and thinking. I don’t remember what I was thinking about. Yana says that the first day I said it was stupid and that the doctors had made a mistake, it was just that they couldn’t diagnose me, and didn’t want to run any more tests. Then I stopped talking. Then I got in touch with Meduza and we started trying to figure out what to do next.</p><p>In order to get blood tests done for poisoning, you have to go to the police.</p><p>So I did. From the precinct, they sent me straight to the hospital. The police officers turned up too, to talk to me and the doctors.</p><p>My first session was with the Berlin Criminal Police and lasted nine hours. The police wanted to know everything: what I was working on, what I was planning to work on, whom I had been in contact with in Ukraine, which of my colleagues I was in contact now. I had to reconstruct October 17 and 18 minute by minute.</p><p>My clothes and apartment were checked for radiation. My body, too. They took the clothes I’d traveled to Munich with. Then the police did a “safety check” of my apartment. An officer asked me, “Why are your blinds open? You could easily be shot from the balcony across the way.” The police told me that I needed to follow new safety protocols. Like what? “Move. Take different routes home. Don’t get cabs directly to your destination, get out of cars a block away. Wear sunglasses.” “That’s it?” “Well, it will all help your chances.”</p><p>The police officers were mad at me. They didn’t show it, but after the third round of questioning, we started talking. The senior detective had run the investigation into the killing of Zelimkhan Khangoshvili, a former Chechen field commander, who had been shot in the Tiergarten in 2019. The killer was quickly caught thanks to eyewitnesses and security camera footage. His passport said he was Vadim Sokolov, but journalists and police established that his real name was Vadim Krasikov and that he had ties to the FSB. He received a life sentence in Germany for murder “on the orders of the Russian government, being a part of the Russian law enforcement apparatus.” The judge called what happened “state terrorism.” In 2022, Russia filed two separate requests to include Krasikov on the list of prisoners up for exchange, but Germany refused. A year earlier, the same detective investigated the poisoning of Petr Verzilov, the publisher of Mediazona and a member of Pussy Riot. He had been taken to Charite from Moscow on a private plane, convulsing and delirious. Verzilov’s friends found that the Berlin hospital was under surveillance. The police offered Verzilov protection and opened an investigation. “And we weren’t able to establish anything. Not even the substance used.” “How come?” “Because it’s impossible to ask a lab, ‘Was this person poisoned?’ You can only ask if there was a specific substance present in their body. And there are thousands of substances. That’s why it’s such a popular means of assassination.”</p><p>“We can’t understand why it took you this long to come to us. You should have called the police right away, as soon as you felt sick on the train. We would have met you at the station.”</p><p>“But I didn’t think I’d been poisoned. I’m still not sure.”</p><p>“Why didn’t you think so?”</p><p>“It seemed crazy to me. And I’m in Europe.”</p><p>“So what?”</p><p>“I felt like I was safe.”</p><p>“That is what drives us crazy,” the detective said. “You come here and act like you’re on vacation. Like this is some paradise. It doesn’t even occur to you to keep yourself safe. We have political killings here. The Russian special services are active in Germany. Your carelessness, yours and your colleagues, knows no bounds.”</p><p>I was not kept informed of the progress of the investigation.</p><p>On April 2, at a journalism conference, I was approached by Insider editor-in-chief Roman Dobrokhotov. He took me aside. “Lena, I have a personal question. But first I need to tell you something. Christo Grozev from Bellingcat and I have been investigating a series of poisonings in Europe. All the known targets are female Russian journalists. I want to ask you. You haven’t written anything for a long time—is it because you’ve been sick?”</p><p>And I told him what I am telling you now.</p><p>On May 2, I got a letter from the Berlin Prosecutor General’s office telling me that the investigation into my attempted assassination had been closed. The police had not found “any indication” that there had been an attempt to kill me. “Blood test results do not conclusively indicate poisoning.”</p><p>The doctors consulting Insider and Bellingcat said that the most likely explanation of what had happened to me was that I’d been poisoned with a chloroorganic compound. I passed this information on to the police. On July 21, the Prosecutor’s office reopened the case.</p><hr><p><span>How am I now?</span> The pain, nausea, and swelling have gone. I still have no energy. I left Meduza—I am a long way away from being able to return to the field. Right now, I can work three hours a day. This keeps increasing, but slowly. There are days when I can’t do anything. I lay there and try not to hate myself.</p><p>While I was writing this, I strove to establish the chronology of events, remember all the important details. But what details are really important? In November, a friend of mine came to Berlin. He is a publisher—not an activist, not a journalist, not a politician. He came over and he was horrified at the state I was in. He said, Do you understand that you may have been poisoned? Have you talked to your doctors about that? I said, I haven’t and I am not going to, that’s stupid. I said, Don’t try to infect me with your paranoia.</p><p>I lied to the police. It wasn’t that the idea of it “seemed crazy” to me. During my time at <em>Novaya Gazeta, </em>four of my colleagues were killed. I organized the funeral of Khimki journalist Mikhail Beketov, he’d been a friend. I knew that journalists got murdered. But I did not want to believe that they could kill me. I was protected from this thought by revulsion, shame, and exhaustion. It disgusted me to think that there were people who wanted me dead. I was ashamed to talk about it. Even with loved ones, let alone the police. And I felt how exhausted I was, how little strength I had left, that I wouldn’t be able to go on the run again.</p><p>My book is coming out in a few weeks. It is about how Russia descended into fascism. It is coming out in several languages simultaneously. The police believe that it might become a trigger. That the people who tried to kill me in Ukraine, and, possibly, in Germany, will try again.</p><p>I want to live.</p><p>That’s why I’m writing this.</p><p>I also want my colleagues and friends, activists, and political refugees currently living abroad to be careful. More careful than I have been. We are not safe and we will not be safe until there is regime change in Russia. The work we do helps to bring this regime down, and it is defending itself. If you are suddenly ill, please do not discount the possibility that you may have been poisoned. Tell your doctors. Fight for yourself. And if it’s already happened to you, please make contact with the investigative team at Insider or Bellingcat. They are looking for the people who are trying to kill us.</p><p><em>—Translated from the Russian by Bela Shayevich</em></p> <!-- START SUBSCRIBE LINK --><hr><p>If you like this article, please <a href="https://www.nplusonemag.com/subscribe/?affid=article">subscribe</a> or leave a tax-deductible tip below to support n+1.</p>  <!-- END SUBSCRIBE LINK --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nintendo Is Trying to Patent Some Broad Tears of the Kingdom Mechanics (221 pts)]]></title>
            <link>https://kotaku.com/nintendo-is-trying-to-patent-some-really-broad-tears-of-1850730637</link>
            <guid>37145427</guid>
            <pubDate>Wed, 16 Aug 2023 11:25:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kotaku.com/nintendo-is-trying-to-patent-some-really-broad-tears-of-1850730637">https://kotaku.com/nintendo-is-trying-to-patent-some-really-broad-tears-of-1850730637</a>, See on <a href="https://news.ycombinator.com/item?id=37145427">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure data-id="153131fc5015a8537210d0cb2c367d6f" data-recommend-id="image://153131fc5015a8537210d0cb2c367d6f" data-format="jpg" data-width="1280" data-height="720" data-lightbox="true" data-alt="Zelda is seen standing on a sky island." data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-alt="Zelda is seen standing on a sky island." data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="fair-use" data-hide="false" data-hidecredit="false"><p><span><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_645/153131fc5015a8537210d0cb2c367d6f.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_1315/153131fc5015a8537210d0cb2c367d6f.jpg"><img alt="Zelda is seen standing on a sky island." data-chomp-id="153131fc5015a8537210d0cb2c367d6f" data-format="jpg" data-alt="Zelda is seen standing on a sky island." data-anim-src="" src="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_645/153131fc5015a8537210d0cb2c367d6f.jpg"></picture></div></span></p><p><figcaption>Screenshot<!-- -->: <!-- -->Nintendo / Kotaku</figcaption></p></div><span data-id="153131fc5015a8537210d0cb2c367d6f" data-recommend-id="image://153131fc5015a8537210d0cb2c367d6f" data-format="jpg" data-width="1280" data-height="720" data-lightbox="true" data-alt="Zelda is seen standing on a sky island." data-recommended="false" data-hide="false"></span></figure><div><p>Nintendo is registering several new patents from <em>The Legend of Zelda: Tears of the Kingdom</em> that are extremely broad, to the point where they seem unreasonable for other developers to be beholden to.<br></p><div data-video-id="194644" data-monetizable="true" data-position="sidebar" data-video-title="The Week In Games: Return To Hyrule" data-video-blog-id="9" data-video-network="kotaku" data-video-duration="165" data-playlist="194644,194088,191491" data-current="194644"><div><p>The Week In Games: Return To Hyrule</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/194644/194644_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/194644/194644_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/194644/194644_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/194644/194644_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/20101.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://automaton-media.com/en/news/20230808-20590/&quot;,{&quot;metric25&quot;:1}]]" href="https://automaton-media.com/en/news/20230808-20590/" target="_blank" rel="noopener noreferrer"><em>Automaton</em></a></span>, a gaming website that focuses on Japanese games like <em>Zelda</em>, has a roundup of the 32 patents Nintendo put forth. Some of them are specific to Link’s latest adventure, including things like <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.j-platpat.inpit.go.jp/p0200&quot;,{&quot;metric25&quot;:1}]]" href="https://www.j-platpat.inpit.go.jp/p0200" target="_blank" rel="noopener noreferrer">Riju’s lightning ability</a></span>, which lets the player target enemies with a bow and bring down a lighting strike wherever the arrow lands. The weirder ones are related to baseline game design and coding that applies to plenty of other video games on the market. One of the hopeful patents relates to the physics of a character riding on top of a <!-- -->moving vehicle and reacting dynamically to it in a realistic manner.</p><figure data-id="134bce6323bf5735d06f9818838ebda9" data-recommend-id="image://134bce6323bf5735d06f9818838ebda9" data-format="jpg" data-width="1140" data-height="474" data-lightbox="true" data-alt="A character is shown standing on top of a moving vehicle." data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-alt="A character is shown standing on top of a moving vehicle." data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="fair-use" data-hide="false" data-hidecredit="false"><p><span><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_645/134bce6323bf5735d06f9818838ebda9.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_1315/134bce6323bf5735d06f9818838ebda9.jpg"><img alt="A character is shown standing on top of a moving vehicle." data-chomp-id="134bce6323bf5735d06f9818838ebda9" data-format="jpg" data-alt="A character is shown standing on top of a moving vehicle." data-anim-src="" data-src="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_645/134bce6323bf5735d06f9818838ebda9.jpg" src="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_645/134bce6323bf5735d06f9818838ebda9.jpg"></picture></div></span></p><p><figcaption>Image<!-- -->: <!-- -->J-Plat Pat via Automaton<!-- --> (<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.j-platpat.inpit.go.jp/p0200&quot;,{&quot;metric25&quot;:1}]]" href="https://www.j-platpat.inpit.go.jp/p0200" target="_blank" rel="noopener noreferrer">Fair Use</a></span>)</figcaption></p></div><span data-id="134bce6323bf5735d06f9818838ebda9" data-recommend-id="image://134bce6323bf5735d06f9818838ebda9" data-format="jpg" data-width="1140" data-height="474" data-lightbox="true" data-alt="A character is shown standing on top of a moving vehicle." data-recommended="false" data-hide="false"></span></figure><p>The distinction, according to Automaton’s translation of Japanese site Hatena Blog user <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://naoya2k.hatenablog.com/entry/2023/08/07/034044&quot;,{&quot;metric25&quot;:1}]]" href="https://naoya2k.hatenablog.com/entry/2023/08/07/034044" target="_blank" rel="noopener noreferrer">nayoa2k’s post</a></span> on the matter,<!-- --> is down to how <em>Tears of the Kingdom</em> codes these interactions. Link and the objects he rides on move together at the same <!-- -->speed, rather than Link being technically stationary on top of a moving object as is common in the physics of other games. The two are functionally the same, but given that plenty of video games displayed characters who can walk around on top of moving vehicles, it’s highly unlikely this kind of approach hasn’t been utilized before.<br></p><p>On top of trying to patent the tech, Nintendo seeks to patent the <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.j-platpat.inpit.go.jp/p0200&quot;,{&quot;metric25&quot;:1}]]" href="https://www.j-platpat.inpit.go.jp/p0200" target="_blank" rel="noopener noreferrer">loading screen that shows up</a></span> when the player is fast-<!-- -->traveling across Hyrule. This specifically refers to the screen that shows the map transition from the player’s starting point to their destination. Sure, that’s pretty specific and not something every game utilizes, but it’s still such a general concept that it feels almost petty to patent it when it’s hardly an iconic draw of <em>Tears of the Kingdom</em>.</p><p>It’s not uncommon for game developers to try to patent mechanics and features. One of the most famous examples is when Bandai Namco <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://kotaku.com/the-patent-on-loading-screen-mini-games-is-about-to-exp-1744705351&quot;,{&quot;metric25&quot;:1}]]" href="https://kotaku.com/the-patent-on-loading-screen-mini-games-is-about-to-exp-1744705351">had a patent on loading screen mini-games</a></span>, which finally ended in 2015.</p><p>Who knows if these patents actually go anywhere? But when game design concepts are gatekept like this, it only leads to a loss of innovation for other devs. Though these specific patents are small in the grand scheme of things, they can be a slippery slope for things like WB <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://kotaku.com/after-years-of-trying-wb-games-successfully-patented-s-1846213089&quot;,{&quot;metric25&quot;:1}]]" href="https://kotaku.com/after-years-of-trying-wb-games-successfully-patented-s-1846213089">patenting <em>Shadow of Mordor</em>’s Nemesis System</a></span>, which should be in more games.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Energy Jobs Have Increased in Nearly Every County in America (107 pts)]]></title>
            <link>https://www.energy.gov/articles/us-department-energy-finds-energy-jobs-have-increased-nearly-every-county-america</link>
            <guid>37145411</guid>
            <pubDate>Wed, 16 Aug 2023 11:24:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.energy.gov/articles/us-department-energy-finds-energy-jobs-have-increased-nearly-every-county-america">https://www.energy.gov/articles/us-department-energy-finds-energy-jobs-have-increased-nearly-every-county-america</a>, See on <a href="https://news.ycombinator.com/item?id=37145411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
      <a id="main-content" tabindex="-1"></a>      <div id="block-particle-content">
          
        
              <article role="article" about="/articles/doe-finds-energy-jobs-have-increased-nearly-every-county-america" typeof="schema:Article">

  
      <span property="schema:name" content="DOE Finds Energy Jobs Have Increased in Nearly Every County in America"></span>


  
  <div>
    <div>
  
      
                            <p><em><span>New DOE Analysis Finds Energy Jobs Increased in 95% of U.S. Counties from 2021 to 2022&nbsp;</span></em></p>
                  
      </div>
<div><p><span><span><strong><span>WASHINGTON, D.C. </span></strong><span>—&nbsp;<span>The U.S. Department of Energy (DOE) today released <a aria-label="/policy/2023-useer-county-level-data-faq" href="https://www.energy.gov/policy/2023-useer-county-level-data-faq" target="_blank">county-level data</a> on energy employment across the United States finding that energy jobs grew in nearly every county in 2022. This data builds on national and state-level data that was published in June in the <a aria-label="/policy/us-energy-employment-jobs-report-useer" href="https://www.energy.gov/policy/us-energy-employment-jobs-report-useer" target="_blank">2023 U.S. Energy and Employment Report (USEER)</a>—a comprehensive summary of national and state-level energy jobs, reporting by industry, technology, and region. Today’s release of the <a aria-label="/policy/2023-useer-county-level-data-faq" href="https://www.energy.gov/policy/2023-useer-county-level-data-faq" target="_blank">county-level data</a> shows that clean energy jobs are increasing in communities across the United States. &nbsp; </span></span></span></span></p>

<p><span><span><span><span>With the release of the county-level data, local and regional groups will be empowered to understand their local energy workforce and how to support the creation of good jobs in clean energy in their area. These findings illustrate how Bidenomics is working, and President Biden’s Investing in America agenda is ensuring all communities have access to affordable, reliable, clean electricity, helping deliver on the President's ambitious clean energy and climate goals. This release comes on the one-year anniversary of the Inflation Reduction Act, the largest investment in climate action in history that has already spurred over $110 billion of clean energy manufacturing announcements from the private sector and created over <a aria-label="https://climatepower.us/resources/new-climate-power-report-finds-that-since-the-passage-of-the-inflation-reduction-act-thousands-of-jobs-and-18-83-billion-in-investments-have-been-generated-in-georgia/" href="https://climatepower.us/resources/new-climate-power-report-finds-that-since-the-passage-of-the-inflation-reduction-act-thousands-of-jobs-and-18-83-billion-in-investments-have-been-generated-in-georgia/" target="_blank">170,000 jobs</a>. &nbsp; &nbsp;&nbsp;</span></span></span></span></p>

<p><span><span><span><span>“This new data confirms what we’ve been seeing and hearing on the ground in states across the country: Bidenomics is working, and the clean energy transformation is creating good jobs in every pocket of America,” said <strong>U.S. Secretary of Energy Jennifer M. Granholm</strong>. “As energy jobs continue to grow thanks to President Biden's Investing in America agenda, we’re transforming local economies and delivering healthier and more prosperous communities along the way.”&nbsp;</span></span></span></span></p>

<p><span><span><span><span>The county-level USEER data provides employment numbers for a range of energy technologies in every county of the United States. Data is reported for five energy sectors: Electric Power Generation; Transmission, Distribution, and Storage; Fuels; Energy Efficiency; and Motor Vehicles and Component Parts. Key county-level findings include:&nbsp;</span></span></span></span></p>

<ul><li><span><span><span>Energy employment is increasing in nearly every county across the United States. The number of energy jobs increased in 95% of American counties from 2021 to 2022.</span></span></span></li>
	<li><span><span><span>Energy efficiency jobs are the most widely geographically distributed of all energy sectors, with jobs in nearly all counties in the United States. From 2021 to 2022, energy efficiency jobs increased in 95% of counties in the United States. We expect energy efficiency jobs to continue to grow with DOE’s continued investment in <a aria-label="/articles/biden-harris-administration-opens-applications-states-and-territories-implement-85-billion" href="https://www.energy.gov/articles/biden-harris-administration-opens-applications-states-and-territories-implement-85-billion" target="_blank">home energy efficiency</a>.</span></span></span></li>
	<li><span><span><span>Motor vehicle jobs are also widely distributed, with jobs in 98% of U.S. counties. One hundred counties have more than 5,000 motor vehicle jobs, and 31 have more than 10,000 jobs.</span></span></span></li>
	<li><span><span><span>Solar jobs increased in the largest number of counties of any electric power generation technology, growing in 74% of U.S. counties. Solar is the largest electric power generation technology, and there are solar jobs in 79% of U.S. counties. Solar is the only technology within the electric power sector where any county has over 10,000 workers. All four of these counties are in California: Los Angeles, San Diego, San Francisco, and Santa Clara.&nbsp;</span></span></span></li>
</ul><p><span><span><span><span>With so many jobs across so many counties, energy jobs are making an impact in nearly every community in the nation, and DOE anticipates that impact will only grow as President Biden’s Investing in America agenda continues to spur billions in investments in clean energy, communities, and people. Through USEER and other <a aria-label="/investments-american-made-energy" href="https://www.energy.gov/investments-american-made-energy" target="_blank">mapping initiatives</a>, DOE is tracking energy jobs in every county of the United States, as well as tens of thousands of announced jobs in solar power, offshore wind, EV batteries, and more since the passage of the Inflation Reduction Act.&nbsp;</span></span></span></span></p>

<p><span><span>Achieving a clean energy transition requires harnessing the talent, grit, and innovative spirit of the diverse American workforce in every county of the country, and DOE is committed to engaging and investing in a skilled workforce to scale up the development and build-out of clean energy technologies. You can learn more about DOE’s commitment to supporting good-paying energy jobs on <a aria-label="/creating-clean-energy-union-jobs" href="https://www.energy.gov/creating-clean-energy-union-jobs" target="_blank">our website</a>.</span></span></p></div>


  </div>

</article>

      </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Htmx is part of the GitHub Accelerator (802 pts)]]></title>
            <link>https://htmx.org/posts/2023-06-06-htmx-github-accelerator/</link>
            <guid>37144985</guid>
            <pubDate>Wed, 16 Aug 2023 10:19:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmx.org/posts/2023-06-06-htmx-github-accelerator/">https://htmx.org/posts/2023-06-06-htmx-github-accelerator/</a>, See on <a href="https://news.ycombinator.com/item?id=37144985">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
  
  <p>We are excited to announce that htmx has been accepted into the first class of the 
<a rel="noopener" target="_blank" href="https://accelerator.github.com/">GitHub Open Source Accelerator</a>!  This is a tremendous opportunity to work with and
learn from some of the most successful open source developers and projects, and a great chance to get the message
out about hypermedia and htmx.</p>
<p>We plan on using this opportunity to begin work on htmx 2.0 and, we hope, possibly learn how to make working on htmx
a full time job!</p>
<p>Here are some of the other open source projects that we have met through the GitHub accelerator and that we recommend 
people check out:</p>
<ul>
<li><a href="https://boxyhq.com/">BoxyHQ</a> - BoxyHQ’s suite of APIs for security and privacy helps engineering teams build and ship compliant cloud applications faster.</li>
<li><a href="https://cal.com/">Cal.com</a> - Cal.com is a scheduling tool that helps you schedule meetings without the back-and-forth emails.</li>
<li><a href="https://www.crowd.dev/">Crowd.dev</a> - Centralize community, product, and customer data to understand which companies are engaging with your open source project.</li>
<li><a href="https://documenso.com/">Documenso</a> - The Open-Source DocuSign Alternative. We aim to earn your trust by enabling you to self-host the platform and examine its inner workings.</li>
<li><a href="https://erxes.io/">Erxes</a> - The Open-Source HubSpot Alternative. A single XOS enables to create unique and life-changing experiences ​​that work for all types of business.</li>
<li><a href="https://formbricks.com/">Formbricks</a> - Survey granular user segments at any point in the user journey. Gather up to 6x more insights with targeted micro-surveys. All open-source.</li>
<li><a href="https://forwardemail.net/">Forward Email</a> - Free email forwarding for custom domains. For 6 years and counting, we are the go-to email service for thousands of creators, developers, and businesses.</li>
<li><a href="https://gitwonk.com/">GitWonk</a> - GitWonk is an open-source technical documentation tool, designed and built focusing on the developer experience.</li>
<li><a href="https://www.hanko.io/">Hanko</a> - Open-source authentication and user management for the passkey era. Integrated in minutes, for web and mobile apps.</li>
<li><a href="https://infisical.com/">Infisical</a> - Open source, end-to-end encrypted platform that lets you securely manage secrets and configs across your team, devices, and infrastructure.</li>
<li><a href="https://novu.co/">Novu</a> - The open-source notification infrastructure for developers. Simple components and APIs for managing all communication channels in one place.</li>
<li><a href="https://openbb.co/">OpenBB</a> - Democratizing investment research through an open source financial ecosystem. The OpenBB Terminal allows everyone to perform investment research, from everywhere.</li>
<li><a href="https://www.sniffnet.net/">Sniffnet</a> - Sniffnet is a network monitoring tool to help you easily keep track of your Internet traffic.</li>
<li><a href="https://typebot.io/">Typebot</a> - Typebot gives you powerful blocks to create unique chat experiences. Embed them anywhere on your apps and start collecting results like magic.</li>
<li><a href="https://www.webiny.com/">Webiny</a> - Open-source enterprise-grade serverless CMS. Own your data. Scale effortlessly. Customize everything.</li>
<li><a href="https://webstudio.is/">Webstudio</a> - Webstudio is an open source alternative to Webflow</li>
</ul>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is this a good book for me, now? (197 pts)]]></title>
            <link>https://maryrosecook.com/blog/post/is-this-a-good-book-for-me-now</link>
            <guid>37144601</guid>
            <pubDate>Wed, 16 Aug 2023 09:23:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maryrosecook.com/blog/post/is-this-a-good-book-for-me-now">https://maryrosecook.com/blog/post/is-this-a-good-book-for-me-now</a>, See on <a href="https://news.ycombinator.com/item?id=37144601">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>I used to believe that every book has an objective value. And I used to believe that this value is fixed and universal.</p>

<p>Now, I believe it’s much more useful to say something in this form: this book has this value to this person in this context.</p>

<p>For example, Mindset by Carol Dweck was life changing to me when I read it in 2016.</p>

<p>The “me” part is important because I grew up thinking that intelligence is fixed and my skill in each activity I tried was based on talent and was fixed. So I thought I should to do the things I had a knack for, and I thought that the things I found difficult would stay difficult. Learning about a growth mindset was extremely valuable to me.</p>

<p>The 2016 part - the context - was also important. I’d just spent the last three years working at the Recurse Center, a place and community suffused with the idea that people can grow. I was primed for these ideas.</p>

<p>A second example. Around ten years ago I read You and your research by Richard Hamming. This is an essay by a mathematician who did ground-breaking research into telecommunications. He relates this anecdote:</p>

<blockquote>
  <p>I had been eating for some years with the Physics table at the Bell Telephone Laboratories restaurant…Fame, promotion and hiring by other companies ruined the average quality of the people so I shifted to the Chemistry table in another corner of the restaurant. I began by asking what the important problems were in chemistry, then later what important problems they were working on, and finally one day said, “If what you are working on is not important and not likely to lead to important things, then why are you working on it?” After that, I was not welcome and had to shift to eating with the Engineers.</p>
</blockquote>

<p>I read that ten years ago without effect. I read it again a couple of years ago and it helped me figure out what I want to work on. The same text and the same reader. A completely different outcome.</p>

<p>I think what changed is my context. Ten years ago, if I’d even tried to work on foundational problems in my field - programming - I’d just have kind of paddled around and had no idea how to make progress. I didn’t have the knowledge of the history of computing or programming to be able to make any kind of headway. In 2021, I did, because I’d accrued it.</p>

<p>The idea that a book’s value is best judged alongside the notional reader and their current context has some corollaries:</p>

<p>First, reading the books that your heroes cite as important will not necessarily be rewarding. If you admire Bret Victor for his work on computing interfaces, only some of his library will be high value to you because his library also includes lots of books that have nothing to do with UI.</p>

<p>Second, yes, it’s likely that “great books” may be high value in some more universal sense that is independent of reader and context. And, yes, this high value may come from something inherent in the quality of the books, rather than from the fact that they are about themes that are more relevant to more people. Yes, I probably wouldn’t dispute this. But I suspect that relevance to person and context is a better guide to what to read.</p>

<p>Third, book recommendation systems based on your reading history can be helpful, but only so much. You, now, are not represented by your reading history. You’ve changed. Making recommendations based on books you read twenty years ago might produce good books for you, now. But probably not.</p>

<p>What aspects of me and my context affect the value of a book?</p>

<p>First, what are my fantasies? Some of my friends have sci-fi fantasies. They love the idea of living on a space ship and landing on planets and fighting aliens and using advanced technology and all that bilge. That fantasy life appeals to them. Whereas I love the world of P.G. Wodehouse. The gentleman’s life, the flitting from manor to manor, the purloining of cow creamers to avoid the homicidal fellow guest. I don’t think either world is any more rich or meaningful or worthwhile than the other. It’s just personal taste.</p>

<p>Second, what is new to me? A while ago, I started reading The Little Kingdom. It’s a book about the early history of Apple. But I put it aside quickly. It wasn’t a bad book. I just already knew everything in it because I’ve read many other histories of Apple. This same thing can happen when coming much later to a book that was ahead of its time. It can seem like old hat because it’s already part of your cultural context.</p>

<p>Third, what am I ready for? I’m trying to get better at graphic design. I recently read a book about grid systems. It was pretty good. But I’m not really ready for that level of depth, so the book wasn’t very high value to me. This type of context is perhaps the most powerful. It was what was missing with Hamming and present with Dweck. I think it’s the main difference between learning slowly and learning quickly. Vygotsky called it the Zone of Proximal Development.</p>

<p>Fourth, what am I doing right now? I have a book on my shelves called Game Feel that is about making video games that feel physically good to play. I’m really excited to read it. But I’m holding off because I’m not currently making a game where the focus is on a good feel. If I read it at the moment, I’d retain and adopt a lot less of it than if I were to wait for when I can apply it.</p>

<p>These things helps me make better choices. There is another set of techniques that help me make a book as good as it can be for me, now:</p>

<p>First, skipping sections that aren’t good. This is tricky. Reading is a place. The more you skip, the more it becomes browsing. And browsing is not a place. You want to be in the place.</p>

<p>Second, dropping books that aren’t good. I find this hard because I feel good about finishing lots of books. But dropping bad books means I will be able to read so many more good books in my lifetime. When I drop a book, I try to say, “it’s not me. It’s not the book. It’s just not the book for me, now.” Even this is hard. Not getting very much out of Anna Karenina, supposedly one of the aesthetic and emotional heights of human expression and experience, doesn’t feel great. But, that’s the way it goes.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Tailwind CSS Won (144 pts)]]></title>
            <link>https://matt-rickard.com/why-tailwind-css-won</link>
            <guid>37143837</guid>
            <pubDate>Wed, 16 Aug 2023 07:27:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matt-rickard.com/why-tailwind-css-won">https://matt-rickard.com/why-tailwind-css-won</a>, See on <a href="https://news.ycombinator.com/item?id=37143837">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><h2></h2><p><span><div><p>Aug 14, 2023</p><article><div><p>Tailwind CSS is the new ubiquitous frontend framework. It replaces a generation of sites built with Twitter Bootstrap. However, Tailwind CSS is not a UI framework itself but has become synonymous to some degree with the UI components shipped through Tailwind UI (which is a UI framework). Why did Tailwind CSS become so popular? A few hypotheses:</p><ul><li><strong>No context switching from application logic. </strong>The tagline on the website reads, <em>“Rapidly build modern websites without ever leaving your HTML.” </em>That’s partly true, but few developers are writing HTML (instead, they are writing JSX or TSX). Switching to a CSS file to change styles is a costly context switch. Instead, developers write CSS as utility classes right in their application. This also vastly simplifies complex CSS build pipelines (which rarely worked).</li><li><strong>Copy-and-pastable. </strong>Bootstrap provided templates that were easy to get started with. It became the de facto landing page for any side project or new startup. But designs weren’t copy-pastable. Doing so would require you to copy the CSS and HTML. Instead, TailwindCSS is supremely easy to copy — everyone works with the same utility classes, so you can just copy and paste a list of classes or an HTML block into your application, and it should just work.</li><li><strong>Fewer dependencies, smaller surface. </strong>Tailwind is tree-shaken by default and doesn’t have its own ideas of grids or flexboxes (it just defaults to the underlying CSS concepts). Compare this to the last-generation kits like Bootstrap, which had a surface that forced users to adopt JS, HTML, CSS, and CSS build systems like Saas. Tailwind is easy to coexist with other frameworks.</li><li><strong>Reusability. </strong>For many years, developers thought that CSS reusability came through adding class hierarchies to CSS through preprocessors like Saas and Less. The best way to write the least amount of CSS is to just compose basic styles (without defining custom ones).</li></ul></div></article></div><div><p>Daily posts on startups, engineering, and AI</p></div></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Carl Sagan testifying before Congress on climate change (1985) [video] (139 pts)]]></title>
            <link>https://www.youtube.com/watch?v=Wp-WiNXH6hI</link>
            <guid>37143159</guid>
            <pubDate>Wed, 16 Aug 2023 05:28:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=Wp-WiNXH6hI">https://www.youtube.com/watch?v=Wp-WiNXH6hI</a>, See on <a href="https://news.ycombinator.com/item?id=37143159">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[NetMaker: Connect Everything with a WireGuard VPN (265 pts)]]></title>
            <link>https://www.netmaker.io/</link>
            <guid>37142388</guid>
            <pubDate>Wed, 16 Aug 2023 03:00:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.netmaker.io/">https://www.netmaker.io/</a>, See on <a href="https://news.ycombinator.com/item?id=37142388">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Privacy Preference Center</p></div><div><p>When you visit websites, they may store or retrieve data in your browser. This storage is often necessary for the basic functionality of the website. The storage may be used for marketing, analytics, and personalization of the site, such as storing your preferences. Privacy is important to us, so you have the option of disabling certain types of storage that may not be necessary for the basic functioning of the website. Blocking categories may impact your experience on the website.</p></div><div><p><strong>Manage Consent Preferences by Category</strong></p></div><div><p>These items are required to enable basic website functionality.</p></div><div><p>These items are used to deliver advertising that is more relevant to you and your interests. They may also be used to limit the number of times you see an advertisement and measure the effectiveness of advertising campaigns. Advertising networks usually place them with the website operator’s permission.</p></div><div><p>These items allow the website to remember choices you make (such as your user name, language, or the region you are in) and provide enhanced, more personal features. For example, a website may provide you with local weather reports or traffic news by storing data about your current location.</p></div><div><p>These items help the website operator understand how its website performs, how visitors interact with the site, and whether there may be technical issues. This storage type usually doesn’t collect information that identifies a visitor.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Putting Down the Pen: Reflecting on Oryx’s Journey (162 pts)]]></title>
            <link>https://www.oryxspioenkop.com/2023/08/putting-down-pen-reflecting-on-oryxs.html</link>
            <guid>37141463</guid>
            <pubDate>Wed, 16 Aug 2023 01:01:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oryxspioenkop.com/2023/08/putting-down-pen-reflecting-on-oryxs.html">https://www.oryxspioenkop.com/2023/08/putting-down-pen-reflecting-on-oryxs.html</a>, See on <a href="https://news.ycombinator.com/item?id=37141463">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<div id="post-body-8031291285305773410" itemprop="articleBody">
<meta content="By Stijn Mitzer En güzel deniz: henüz gidilmemiş olandır. En güzel çocuk: henüz büyümedi. En güzel günlerimiz: henüz yaşamadıklarımız. Ve sa..." name="twitter:description">
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvaZHx5v6tcHadcPJS32UMnzM7xOqZz6MjPWwGWXQuXdm6ho4sxbqZQwyZR0vjieO8JfmL_j-81TifqH-QitAqOlXsdUNMAqrxmI_t9nHCSpLN7eiFJKSgesTbs5gafwUAgcm4vX4094BlaMqCK2qPpldKTJA6cHddPoirb1Gvqa_MhqXOvaVUEVZh0gM/s2048/332.png"><img data-original-height="1514" data-original-width="2048" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvaZHx5v6tcHadcPJS32UMnzM7xOqZz6MjPWwGWXQuXdm6ho4sxbqZQwyZR0vjieO8JfmL_j-81TifqH-QitAqOlXsdUNMAqrxmI_t9nHCSpLN7eiFJKSgesTbs5gafwUAgcm4vX4094BlaMqCK2qPpldKTJA6cHddPoirb1Gvqa_MhqXOvaVUEVZh0gM/s16000/332.png"></a></p><p><i>By Stijn Mitzer</i><br></p><p><span><i>En güzel deniz: henüz gidilmemiş olandır. En güzel çocuk: henüz büyümedi. En güzel günlerimiz: henüz yaşamadıklarımız. Ve sana söylemek istediğim en güzel söz: henüz söylememiş olduğum sözdür</i> – The most beautiful sea, hasn't been crossed yet. The most beautiful child, hasn't grown up yet. Our most beautiful days, we haven't witnessed yet. And the most beautiful words I wanted to tell you, I haven't said yet.</span><span><span> <i>(<a href="https://youtu.be/ULW4g2jELFA?t=24">By</a> Nazım Hikmet)</i></span></span></p><p>Dear everyone,</p><p>I had always imagined 'penning' this farewell someday. You see, the journey of Oryx took a different path than its intended purpose. What Oryx was meant to be initially was a remedy for my teenage boredom at the age of 17. Back then, I was still in high school, and the manageable workload along with my recent departure from playing football left me with an abundance of spare time. An interest in the Arab Spring, in particular the Libyan and Syrian Revolutions, led me to spend more and more time scouring the internet for updates. As the Syrian Revolution evolved into protracted civil war, I decided to create a Twitter account to more closely monitor the unfolding events.</p><p>One of the accounts I followed was that of Eliot Higgins, who began reporting on the Syrian Civil War on his Brown Moses Blog. After asking him one day if he was going to report on the use of Italian-upgraded T-72 tanks in the war, I remember telling myself that if a ''high-school dropout who knew no more about weapons than the average Xbox owner'' was able to write these articles, so would I probably. That evening, I created a blog, picked a name (Oryx for the majestic animal, and Spioenkop, Afrikaans for 'spy hill', as a place from where one can watch events unfold around the world) and published my first article on Syria's T-72 MBTs. (For those interested, the article can be read <a href="https://web.archive.org/web/20141019093200/http://spioenkop.blogspot.com/2013/02/syrian-turms-t-equipped-t-72s.html">here</a>).</p><p>It was the 16th of February 2013, and little did I realise that the next decade would transform Oryx from a remedy for boredom into a project that would consume the majority of my time and energy. I can still recall the joy I felt when the T-72 article garnered 520 views in just several hours, contributing to a total view count of approximately 3500 for the entire blog that month. Fast forward ten years, and Oryx now achieves an average of 250,000 daily views. In the months following my inaugural article, I continued to write about Syria, a country that held my focus until 2017.&nbsp;However, a desire towards greater challenges was always present. My motivation thrives on challenges. Offer me the most difficult subject to analyse. Upon mastering the subject's intricacies, I seek out the next challenge.</p><p>I ultimately discovered my greatest challenge in the analysis of North Korea. Back in the early 2010s, the scarcity of photographs and videos emerging from the country, in stark contrast to the flood of visual content available now, intrigued me. The limited information available, coupled with the abundance of misinformation, arguably made it the most challenging country to analyse. Through a series of articles and our eventual book(s), Joost and I attempted to unravel the mysteries surrounding the Korean People's Army. Finishing the final pages of the book left me feeling satisfied with North Korea – we had done what we aimed for. We unearthed the answers to our questions. With this challenge resolved, I started looking for another subject that would keep me curious and motivated.</p><p>Finding a challenge this time around proved much harder than before. However, the Nagorno-Karabakh War, Türkiye('s defence industry) and the Tigray War eventually emerged as subjects that provided me with both analytical satisfaction and the desired level of complexity. Their status as topics that Western analysts scarcely delved into rendered them all the more interesting to me. In contrast to mainstream media, we weren't confined by the need to generate popular articles and headlines. Instead, we saw this as an opportunity to illuminate underreported conflicts like the Tigray War, the Libyan War and the War in Yemen. Continuously delving into various countries and conflicts kept Oryx fresh for me, but it has also brought me to a place where I feel that I've largely covered the subjects I intended to explore. The journey has been a source of pleasure, but it has now arrived at its final destination. </p><p>Since late 2021, the act of writing feels repetitive, almost as if I've written every sentence before. For me, this realisation serves as a clear sign that it's time to move on. In fact, I had already contemplated ending Oryx by the spring of 2022, but the Russian invasion of Ukraine infused me with renewed energy to keep going. But 1.5 years later, I have lost my spark. My interest in anything military is fading, and the 
constant pressure to keep up with everything is exhausting. I
usually fall asleep with my phone in hand, only to wake up finding 
I've been sleeping on it. I'm tired of all the death and 
destruction. It's been a whole decade of watching videos of people's bodies having been torn apart by bombs or parents holding their lifeless newborns who died as a result of armed conflict – it really gets to you.</p><p>Still, I take great joy in the opportunities that Oryx has brought
 me, as well as from the lifelong friendships with Joost and Kemal. While I'm aware of options such as securing a position at a 
think tank or even transforming Oryx into a lucrative private 
intelligence agency, these career paths hold no appeal for me. 
I think I possess a moral compass that doesn't align with such institutions' goals. Despite the potential for financial gains through Oryx, I consciously 
opt not to pursue them. To me, the act of <a href="https://www.oryxspioenkop.com/2023/07/patreon-with-purpose-how-your-donations.html">donating</a> our entire Patreon 
income to charities seemed like the only possible course of action. Amidst ongoing wars and natural disasters, it's difficult to justify to ourselves to hold onto money without considering the greater need. Money doesn't tempt me, especially when it's associated with conflict. True wealth, for me, is found within family, health, and finding happiness in the little things in life. A forest stroll or spending time with friends makes me feel genuinely rich. Learning this lesson at a young age is priceless.<br></p><p>Over the years I've come to realise that, to me, genuine success and happiness are scarcely influenced by popularity, recognition, or even publishing a book. While these achievements hold their own significance, they haven't truly brought me a sense of pride. My most significant accomplishments involve making those dear to me proud and understanding the essence of happiness at a young age. Oryx has shown me that that true happiness cannot be attained through fame, career accomplishments, or wealth. Despite Oryx gaining recognition – being featured on major TV channels, acknowledged by figures like John McCain and David Petraeus, and with our information used by intelligence agencies – my proudest moment remains being able to write a message in my book I gave to my then girlfriend. You've shown me what real happiness looks like T. Nothing could ever surpass that. Thank you for that.</p><p>Reflecting on the last decade, I hope that Oryx has and will continue to motivate others to set out on their own journey of analysis and writing. Starting at the age of 17 without ever taking any education in the field of defence or international relations, Oryx can be seen as evidence that great opportunities await those who choose a similar path.<i> </i>What added to the excitement was the interaction with readers on Twitter, which I've thoroughly enjoyed over the years.<i> </i>At a certain point, the number of messages became overwhelming, so I want to apologise if you never received a response. I also want to express my sincere appreciation to all those who have offered their assistance in various capacities to Oryx over the years, with a special acknowledgment to Jakub.<i> </i>What began as a childhood interest ignited by buying Buck Danny and Biggles comic strips when I was 8 years old blossomed into a hobby that has far exceeded any reasonable limits. Although I once contemplated a position with an intelligence agency, an offer never came to fruition (perhaps fortunate given their bureaucracy).<br></p><p>Lastly, I feel compelled to discuss the origin of the practice of list-making and its evolution over time. We began our venture into list-making in 2013 with the goal of aiding our internal analysis. The abundance and variations of North Korea's armored fighting vehicles (AFVs) posed a challenge, prompting us to catalog them before we could analyse them effectively. This initial list set the groundwork for subsequent lists, although it wasn't until the summer of 2014 that we embarked on compiling the 'losses lists,' intended to illustrate the staggering volume of armament and equipment captured by IS in the regions of Iraq and Syria. The rapid proliferation of these lists, owing to the relatively straightforward process of creating them, is probably what Oryx will primarily be remembered for. The lists gained such popularity that I found myself (somewhat jokingly) <a href="https://www.oryxspioenkop.com/2023/04/list-listing-oryx-list-of-lists.html">embracing</a> the entire act of list-making on Oryx with a list of lists. However, I must confess, I have an aversion to planning ahead and never create lists in my everyday life. Sorry!<br></p><p>As I bid farewell on October 1st, I'll leave you with these lines from my most beloved song&nbsp;<a href="https://youtu.be/rbTsG9jrJsU">Ue o Muite Arukō</a> by my favourite singer Sakamoto Kyu.</p><p>Shiawase wa kumo no ue ni - Happiness lies beyond the clouds<br>Shiawase wa sora no ue ni - Happiness lies up above the sky</p><div><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtnZJ-bwbP3CxrgnPayC7m3hemRUxyrBx0CGpxkGznAxI0peglIaclhnBvMY5TpTgy3_dnuKRmVGAnIBTONUspdY2paVxmrbxyfU2P6gidj0Kq9QOOuiKBXTy8xxOzRm0TtUlbsvu-CG--Uuoyw3MFUWztj8vtXnUMjVQx_ZWr4ex73BMRjT_pObvOMgM/s2048/25.png"><img data-original-height="1320" data-original-width="2048" height="412" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtnZJ-bwbP3CxrgnPayC7m3hemRUxyrBx0CGpxkGznAxI0peglIaclhnBvMY5TpTgy3_dnuKRmVGAnIBTONUspdY2paVxmrbxyfU2P6gidj0Kq9QOOuiKBXTy8xxOzRm0TtUlbsvu-CG--Uuoyw3MFUWztj8vtXnUMjVQx_ZWr4ex73BMRjT_pObvOMgM/w640-h412/25.png" width="640"></a></p><p>PS: Just to clarify, I'm not being sponsored by the Põhjala beer brand – there was a half-price deal for a second bottle at my nearby supermarket. My Dutch spirit remains intact, after all. <br></p></div>
</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modern CSV version 2 (311 pts)]]></title>
            <link>https://www.moderncsv.com/modern-csv-2-is-now-available/</link>
            <guid>37140159</guid>
            <pubDate>Tue, 15 Aug 2023 22:35:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.moderncsv.com/modern-csv-2-is-now-available/">https://www.moderncsv.com/modern-csv-2-is-now-available/</a>, See on <a href="https://news.ycombinator.com/item?id=37140159">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>And I think you’ll love it. I focused on several areas:</p>
<ol>
<li>Improved UI and user experience</li>
<li>Faster performance</li>
<li>Useful features</li>
<li>Updated documentation</li>
<li>For Mac users, Native Apple Silicon (ARM – M1, M2) compatibility</li>
</ol>
<p><a href="https://www.moderncsv.com/download" target="_blank"><span>Download Modern CSV 2</span></a></p><p>If that’s all you need to know, you can buy a license <a href="https://www.moderncsv.com/buy">here</a>.<br>
Or if you already have a version 1 license, you can upgrade <a href="https://www.moderncsv.com/upgrade">here</a>.<br>
For those that need more details, here you go.</p>
<h2>Improved UI and User Experience</h2>
<p>There are two areas of the user interface that I aimed to improve for usability: Preferences and File Metadata. I also added several new themes and a bunch of subtle improvements to make it look and feel better.</p>
<p><img decoding="async" fetchpriority="high" src="https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2.png" alt="Modern CSV v2" width="1068" height="592" srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2.png 1068w, https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2-300x166.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2-1024x568.png 1024w, https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2-768x426.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2-135x75.png 135w, https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2-480x266.png 480w" sizes="(max-width:767px) 480px, (max-width:1068px) 100vw, 1068px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201068%20592'%3E%3C/svg%3E" data-lazy-srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2.png 1068w, https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2-300x166.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2-1024x568.png 1024w, https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2-768x426.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2-135x75.png 135w, https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2-480x266.png 480w" data-lazy-src="https://www.moderncsv.com/wp-content/uploads/2023/04/Mac-v2.png"></p>
<h3>Preferences</h3>
<p>In version 1, preferences (i.e. program settings, keyboard shortcuts, and file extension options) could only be set by editing and saving a file. Some users really liked this and others preferred a UI. I decided to give everyone what they want. For those who like it, you can still edit the preference files just like in version 1. For everyone else, there is a Preferences window that is more intuitive and less prone to mistakes.</p>
<div id="attachment_1786"><p><img aria-describedby="caption-attachment-1786" decoding="async" src="https://www.moderncsv.com/wp-content/uploads/2023/04/Settings.png" alt="Settings Window" width="827" height="465" srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/Settings.png 827w, https://www.moderncsv.com/wp-content/uploads/2023/04/Settings-300x169.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/Settings-768x432.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/Settings-260x146.png 260w, https://www.moderncsv.com/wp-content/uploads/2023/04/Settings-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/Settings-133x75.png 133w" sizes="(max-width:767px) 480px, (max-width:827px) 100vw, 827px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20827%20465'%3E%3C/svg%3E" data-lazy-srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/Settings.png 827w, https://www.moderncsv.com/wp-content/uploads/2023/04/Settings-300x169.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/Settings-768x432.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/Settings-260x146.png 260w, https://www.moderncsv.com/wp-content/uploads/2023/04/Settings-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/Settings-133x75.png 133w" data-lazy-src="https://www.moderncsv.com/wp-content/uploads/2023/04/Settings.png"></p><p id="caption-attachment-1786">Settings Window</p></div>

<div id="attachment_1785"><p><img aria-describedby="caption-attachment-1785" decoding="async" src="https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts.png" alt="Keyboard Shortcuts Window" width="827" height="465" srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts.png 827w, https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts-300x169.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts-768x432.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts-260x146.png 260w, https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts-133x75.png 133w" sizes="(max-width:767px) 480px, (max-width:827px) 100vw, 827px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20827%20465'%3E%3C/svg%3E" data-lazy-srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts.png 827w, https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts-300x169.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts-768x432.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts-260x146.png 260w, https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts-133x75.png 133w" data-lazy-src="https://www.moderncsv.com/wp-content/uploads/2023/04/KeyboardShortcuts.png"></p><p id="caption-attachment-1785">Keyboard Shortcuts Window</p></div>

<div id="attachment_1790"><p><img aria-describedby="caption-attachment-1790" decoding="async" src="https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions.png" alt="File Extension Options" width="827" height="465" srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions.png 827w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions-300x169.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions-768x432.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions-260x146.png 260w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions-133x75.png 133w" sizes="(max-width:767px) 480px, (max-width:827px) 100vw, 827px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20827%20465'%3E%3C/svg%3E" data-lazy-srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions.png 827w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions-300x169.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions-768x432.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions-260x146.png 260w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions-133x75.png 133w" data-lazy-src="https://www.moderncsv.com/wp-content/uploads/2023/04/FileExtensionOptions.png"></p><p id="caption-attachment-1790">File Extension Options</p></div>
<h3>File Metadata</h3>
<p>In version 1, changing the file’s parameters (e.g. delimiter, character encoding, etc.) or header row/column settings was all done via command. In version 2, you can still do it via command, but there’s now a File Metadata pane to make it easier.</p>
<div id="attachment_1784"><p><img aria-describedby="caption-attachment-1784" decoding="async" src="https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata.png" alt="File Metadata Pane" width="388" height="703" srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata.png 388w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata-166x300.png 166w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata-81x146.png 81w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata-28x50.png 28w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata-41x75.png 41w" sizes="(max-width:767px) 388px, 388px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20388%20703'%3E%3C/svg%3E" data-lazy-srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata.png 388w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata-166x300.png 166w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata-81x146.png 81w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata-28x50.png 28w, https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata-41x75.png 41w" data-lazy-src="https://www.moderncsv.com/wp-content/uploads/2023/04/FileMetadata.png"></p><p id="caption-attachment-1784">File Metadata Pane</p></div>
<h3>Themes</h3>
<p>Version 1 had two themes – Light and Dark. Version 2 has five – Light, Dark, Dracula, Solarized Light, and Solarized Dark.</p>
<div id="attachment_1789"><p><img aria-describedby="caption-attachment-1789" decoding="async" src="https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula.png" alt="Dracula Theme" width="878" height="498" srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula.png 878w, https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula-300x170.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula-768x436.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula-257x146.png 257w, https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula-132x75.png 132w" sizes="(max-width:767px) 480px, (max-width:878px) 100vw, 878px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20878%20498'%3E%3C/svg%3E" data-lazy-srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula.png 878w, https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula-300x170.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula-768x436.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula-257x146.png 257w, https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula-132x75.png 132w" data-lazy-src="https://www.moderncsv.com/wp-content/uploads/2023/04/Dracula.png"></p><p id="caption-attachment-1789">Dracula Theme</p></div>

<div id="attachment_1788"><p><img aria-describedby="caption-attachment-1788" decoding="async" src="https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight.png" alt="Solarized Light Theme" width="878" height="498" srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight.png 878w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight-300x170.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight-768x436.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight-257x146.png 257w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight-132x75.png 132w" sizes="(max-width:767px) 480px, (max-width:878px) 100vw, 878px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20878%20498'%3E%3C/svg%3E" data-lazy-srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight.png 878w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight-300x170.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight-768x436.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight-257x146.png 257w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight-132x75.png 132w" data-lazy-src="https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedLight.png"></p><p id="caption-attachment-1788">Solarized Light Theme</p></div>

<div id="attachment_1787"><p><img aria-describedby="caption-attachment-1787" decoding="async" src="https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark.png" alt="Solarized Dark Theme" width="878" height="498" srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark.png 878w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark-300x170.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark-768x436.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark-257x146.png 257w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark-132x75.png 132w" sizes="(max-width:767px) 480px, (max-width:878px) 100vw, 878px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20878%20498'%3E%3C/svg%3E" data-lazy-srcset="https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark.png 878w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark-300x170.png 300w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark-768x436.png 768w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark-257x146.png 257w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark-50x28.png 50w, https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark-132x75.png 132w" data-lazy-src="https://www.moderncsv.com/wp-content/uploads/2023/04/SolarizedDark.png"></p><p id="caption-attachment-1787">Solarized Dark Theme</p></div>
<h2>Faster Performance</h2>
<p>The first thing you’ll notice is that it loads faster than version 1. The load time has been cut nearly in half.<br>
For loading large files in read-only mode, load time has been reduced by nearly 20%<br>
Lastly, the performance for loading files with many columns has been reduced by over 90%. That’s a full order of magnitude.</p>
<h2>Useful Features</h2>
<p>I make sure that any feature I add is going to be useful to a broad swatch of Modern CSV users. Most of them have been requested by users. The rest are features I find useful, and since I use my own product, I count as a user. Here are some of the most important new features:</p>
<p>* Select all results of a Find operation.<br>
* Open a new instance.<br>
* Deduplicate rows based on just a few columns (instead of only removing rows that are the same on every column). [Premium]<br>
* Reshape (change dimensions) of a range of cells. [Premium]<br>
* Statistics and Column Analysis. [Premium Business]<br>
* Column Lookup. [Premium Business]</p>
<p>For a more comprehensive list, see the <a href="https://www.moderncsv.com/download">Download page</a>.</p>
<h2>Updated Documentation</h2>
<p>The old documentation was single page and was starting to get pretty large. The new documentation has multiple pages with excellent navigation and search functionality. It’s more comprehensive but still concise. I endeavor to use graphics instead of words when possible.</p>
<h2>Native Apple Silicon (ARM – M1, M2) Compatibility</h2>
<p>Rosetta is no longer needed to run Modern CSV on Apple Silicon machines.</p>
<h2>Conclusion</h2>
<p>A lot of work went into making Modern CSV 2.0. I hope you find it makes editing and viewing CSV files even easier. Please feel free to try it out and let me know what you think.</p>
<p><a href="" target="_blank"><span>Download Modern CSV 2</span></a></p><p>If you’re ready to buy a license, you can do so here <a href="https://www.moderncsv.com/buy">here</a>.</p>
<p>If you already have a version 1 license, you can upgrade <a href="https://www.moderncsv.com/upgrade">here</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Is LLaMa.cpp Possible? (650 pts)]]></title>
            <link>https://finbarr.ca/how-is-llama-cpp-possible/</link>
            <guid>37140013</guid>
            <pubDate>Tue, 15 Aug 2023 22:18:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://finbarr.ca/how-is-llama-cpp-possible/">https://finbarr.ca/how-is-llama-cpp-possible/</a>, See on <a href="https://news.ycombinator.com/item?id=37140013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
    
    
    
    <p><em>If you want to read more of my writing, I have a <a href="https://finbarrtimbers.substack.com/">Substack</a>. Articles will be posted simultaneously to both places.</em></p>

<p>Recently, a <a href="https://github.com/ggerganov/llama.cpp">project</a> rewrote the <a href="https://github.com/facebookresearch/llama">LLaMa inference code</a> in raw C++. With some optimizations and quantizing the weights, this allows running a LLM locally on a wild variety of hardware:</p>

<ul>
  <li>On a <a href="https://twitter.com/rgerganov/status/1635604465603473408">Pixel5</a>, you can run the 7B parameter model at 1 tokens/s.</li>
  <li>On a <a href="https://simonwillison.net/2023/Mar/11/llama/">M2 Macbook Pro</a>, you can get ~16 tokens/s with the 7B parameter model</li>
  <li>You can <a href="https://twitter.com/miolini/status/1634982361757790209">even run the 7B model on a 4GB RAM Raspberry Pi</a>, albeit at 0.1 tokens/s.</li>
</ul>

<p>If you are like me, you saw this and thought: What? How is this possible? Don’t large models require expensive GPUs? I took my confusion and dove into the math surrounding inference requirements to understand the constraints we’re dealing with.</p>

<p>Let’s start with GPUs. GPUs have two main benefits for deep learning:</p>

<ol>
  <li>They have a large amount of memory bandwidth (<a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">A100</a>: 1935 GB/s, <a href="https://images.nvidia.com/aem-dam/Solutions/geforce/ada/ada-lovelace-architecture/nvidia-ada-gpu-architecture-whitepaper-1.03.pdf">4090</a>: 1008 GB/s)</li>
  <li>They have a large amount of compute (<a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">A100</a>: 312 TFLOPS of FP16, <a href="https://images.nvidia.com/aem-dam/Solutions/geforce/ada/ada-lovelace-architecture/nvidia-ada-gpu-architecture-whitepaper-1.03.pdf">4090</a>: 82.6 TFLOPS of FP16)</li>
</ol>

<p>When we talk about memory bandwidth, we’re talking about how long it takes to move things from the HBM memory (i.e. the RAM) into the on-chip memory. To actually do math with the GPU, we need to move the matrices in question into the on-chip memory, which is quite small (40MB on an A100, compared to 40-80GB of RAM). Note that the memory bandwidth is ~2 orders of magnitude smaller than the compute performance— this will matter later, as the memory bandwidth tends to be the bottleneck for inference.</p>

<p>What does this mean in the context of serving LLaMa? Let’s start with some <a href="https://kipp.ly/blog/transformer-inference-arithmetic/">inference arithmetic</a>. We can do some rough calculations on the inference performance of a LLM using <a href="https://kipp.ly/blog/transformer-param-count/">Kipply’s article</a><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>. First, some notation on the dimensions of the model:</p>

<ul>
  <li>The \(Q\), \(K\), and \(V\) weight matrices are all shape [ \(d_{\text{model}}\), \(d_{\text{head}}\)], and we have \(n_{\text{heads}}\) of them per layer; the attention output matrix has the same shape, for a total of  \(4 \times\) [ \(d_{\text{model}}\), \(n_{\text{heads}} \cdot d_{\text{head}}\)]. By convention, GPT-style networks have \(d_{\text{head}} \cdot n_{\text{heads}} = d_{\text{model}}\).</li>
  <li>The MLP has two weight matrices, of shape [ \(d_{\text{model}}\), \(4 \cdot d_{\text{model}}\)] and [ \(4\cdot d_{\text{model}}\), \(d_{\text{model}}\)]</li>
  <li>The embeddings matrix is of size [ \(d_{\text{vocab}}\), \(d_{\text{model}}\)].</li>
</ul>

<p>This gives us a handy equation for the number of parameters in a GPT-style model:<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup></p><p>

\[P = n_{\text{blocks}} \left( 4 \cdot d_{\text{model}}^2 + 2 \cdot 4 \cdot d_{\text{model}}^2\right) + n_{\text{vocab}} \cdot d_{\text{model}}\]

</p><p>For the duration of the post, I’m going to focus on the case where we’re running a ChatGPT style service locally, which is what LLaMa.cpp does, letting me assume a batch size of 1.</p>

<p>For efficient inference, the KV cache has to be stored in memory; the KV cache requires storing the KV values for every layer, which is equal to storing:</p><p>

\[n_{\text{bytes}} \cdot 2 \cdot d_{\text{model}}\]

</p><p>I use \(n_{\text{bytes}}\) here to indicate the number of bytes per param; for float32s, this is 4, for float16s, this is 2, etc. The 2 in the middle is because we have to store one set of weights for the K values, and one for the Vs.</p>

<p>Given a model with n layers, the total memory for the KV cache is:</p><p>

\[n_{\text{blocks}} \cdot n_{\text{bytes}} \cdot 2 \cdot d_{\text{model}}\]

</p><p>In addition to storing the KV cache in memory, we also need to store the weights themselves in memory; this requires \(n_{\text{bytes}} \cdot P\) bytes.</p>

<p><img src="https://finbarr.ca/static/images/llama-memory-weights.png" alt="Screenshot of table showing the memory required for LLaMa weights"></p>

<p>This is the advantage of quantization. By using less precision, we can radically decrease the amount of memory needed to store our models in memory. Note that, with int4 precision, <em>all of these models fit into memory on an A100</em> (which is the standard datacenter GPU right now), and all of them, except for the biggest model, fit into memory on high-end consumer GPUs (3090s/4090s, which have 24GB of RAM).</p>

<p>It takes approximately \(2P\) FLOPS to run inference on our model for a single token, because we are doing a bunch of matmuls with a total of \(P\) parameters, and multiplying a matrix of size \((m, n)\) with a vector of size \((n,)\) has a cost of \(2mn\).<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>With all that math out of the way, let’s calculate the requirements for running inference with LLaMa. The main requirements when it comes to sampling are:</p>

<ol>
  <li>Keep the KV cache in memory, in addition to all the parameters.</li>
  <li>Read all the weights from HBM into the on-chip memory. Because we sample auto-regressively, we have to repeat this for each token we sample.</li>
  <li>Do the actual matmuls to calculate the output of our network.</li>
</ol>

<p>The latency is the maximum of either the compute or the memory latency, as reading parameters into on-chip memory happens asynchronously in all modern tensor programming libraries. As a result, we write:</p><p>

\[\begin{align*}
\text{latency}_\text{model} &amp;= \text{max}(\text{latency}_\text{compute}, \text{latency}_\text{memory})\\
\text{latency}_\text{memory} &amp;= \dfrac{2 \cdot P \cdot n_{\text{bytes}}\cdot B}{n_{\text{memory bandwidth}}},\\
\text{latency}_\text{compute} &amp;= \dfrac{2 \cdot P}{n_{\text{flops}}},
\end{align*}\]

</p><p>where \(B\) is the batch size. As \(n_{\text{memory bandwidth}} = 1.935e12\), and  \(n_{\text{flops}} = 3.12e14,\) as long as the batch size is less than 161, the model is memory-bound.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup></p>

<p>With a batch size of 1, this is the same equation, as on most hardware (e.g. Nvidia GPUs), there is a linear speedup as you decrease the precision (you get twice the FLOPS when using fp16 vs fp32, which doubles again as you go to int8, and doubles once more as you go to int4s).</p>

<p>As LLaMa.cpp uses int4s, the RAM requirements are reduced to 1.33GB of memory for the KV cache, and 16.25GB of VRAM for the model parameters. That’s pretty good!</p>

<p>As the memory bandwidth is almost always<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">5</a></sup> much smaller than the number of FLOPS, memory bandwidth is the binding constraint.</p>

<p><img src="https://finbarr.ca/static/images/llama-inference-times.png" alt="Screenshot fo table showing the inference times to run the varying LLaMa models with varying precision levels on an A100"></p>

<h2 id="running-llama-on-an-a100">Running LLaMa on an A100</h2>

<p>On an A100 (80GB PCIe), the memory bandwidth is 1935GB/s. The int4 compute is 1248 TOPS. As such, the model is (heavily) memory-bound. We should expect to see inferences as given in the table; roughly 30 tokens/s with the 65B model, and 277 tokens/s with the 7B model.</p>

<h2 id="running-llama-on-a-m1-macbook-air">Running LLaMa on a M1 Macbook Air</h2>

<p>The M1 GPU has a bandwidth of <a href="https://www.macworld.com/article/783678/m2-vs-m1-chip-performance-graphics-ram.html">68.25 GB/s</a>, while the M1 GPU can do up to <a href="https://tlkh.dev/benchmarking-the-apple-m1-max#heading-gpu-matrix-multiplication-gemm-performance">5.5 TFLOPS</a> of fp16 compute. As such, we should expect a ceiling of ~1 tokens/s for sampling from the 65B model with int4s, and 10 tokens/s with the 7B model.</p>

<p>As the M2 Pro has 200 GB/s of bandwidth, and the M2 Max has 400 GB/s of bandwidth, we should expect massive improvements with them, going up to 6 tokens/s with the M2 Max with the 65B model. That’s pretty darn good for a laptop.</p>

<h2 id="running-llama-on-a-raspberry-pi-4">Running LLaMa on a Raspberry Pi 4</h2>

<p>A Raspberry Pi 4 has <a href="https://web.eece.maine.edu/~vweaver/group/green_machines.html">13.5 GFLOPS of compute</a>, and <a href="https://forums.raspberrypi.com/viewtopic.php?t=281183">~4GB/s of memory bandwidth</a>. Given this, we’d expect to see ~2 tokens/s with the 7B model if it was memory bound. Given that we’re currently seeing ~0.1 tokens/s, I suspect we’re actually compute-bound (although this is a stab in the dark— I can’t find enough information about the specs for a Raspberry Pi to determine this with any precision).</p>

<h2 id="summary">Summary</h2>

<p>Memory bandwidth is the limiting factor in almost everything to do with sampling from transformers. Anything that reduces the memory requirements for these models makes them <em>much</em> easier to serve— like quantization! This is yet another reason why distillation, or just <a href="https://finbarr.ca/llms-not-trained-enough/">training smaller models for longer</a>, is really important.</p>

<p><em>Note: I’m not an expert in CUDA, so I probably have errors in my math. If so, please shoot me an <a href="mailto:finbarrtimbers@gmail.com">email</a> and let me know- I’d love to hear from you so I can learn more about how this works and update this post.</em></p>

<p>Resources on transformer inference performance:</p>

<ul>
  <li><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">Large Transformer Model Inference Optimization</a></li>
  <li><a href="https://kipp.ly/blog/transformer-inference-arithmetic/">Transformer inference arithmetic</a></li>
  <li><a href="https://kipp.ly/blog/transformer-param-count/">LLM parameter counting</a></li>
  <li><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers</a></li>
</ul>

<p><em>Thank you to <a href="https://twitter.com/kaushikpatnaik?lang=en">Kaushik Patnaik</a>, <a href="https://twitter.com/arthurallshire">Arthur Allshire</a>, <a href="https://twitter.com/stanislavfort">Stanislav Fort</a>, and <a href="https://twitter.com/banburismus_">Tom McGrath</a> for reading early drafts of this.</em></p>



    
    <p>PS if you want to read more of my writing, subscribe to my <a href="https://finbarrtimbers.substack.com/">Substack</a>.</p>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You're a cyclist who was just struck by a car driver. Why it was your fault (109 pts)]]></title>
            <link>https://www.mcsweeneys.net/articles/youre-a-cyclist-who-was-just-struck-by-a-car-driver-heres-why-it-was-your-fault</link>
            <guid>37139980</guid>
            <pubDate>Tue, 15 Aug 2023 22:15:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mcsweeneys.net/articles/youre-a-cyclist-who-was-just-struck-by-a-car-driver-heres-why-it-was-your-fault">https://www.mcsweeneys.net/articles/youre-a-cyclist-who-was-just-struck-by-a-car-driver-heres-why-it-was-your-fault</a>, See on <a href="https://news.ycombinator.com/item?id=37139980">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="o-wrapper">
    <main>
        <header>
    <div>
      <div>
          <p><span><img role="none" src="https://edge-assets.mcsw.net/assets/search-614fbdcc4e71f0730ad039e484ec78a1085f24294fa0b4514da70b0a930b2dce.svg"></span></p>        </div>
      <div>
        <ul>
          <li><a href="https://www.mcsweeneys.net/">Internet Tendency</a></li>
          <li><a href="https://store.mcsweeneys.net/">The Store</a></li>
          <li><a href="https://store.mcsweeneys.net/t/categories/books">Books Division</a></li>
          <li><a href="https://store.mcsweeneys.net/t/categories/timothy-mcsweeneys-quarterly-concern">Quarterly Concern</a></li>
          <li><a href="https://thebeliever.net/">The Believer</a></li>
          <li><a href="https://www.mcsweeneys.net/donate">Donate</a></li>
        </ul>
      </div>
    </div>
    
  </header>


      
  <div>
    <h6>The Believer has returned</h6>
    
  </div>


      
<article>
    
   
    <div>
      <p><strong>You were riding during rush hour.</strong><br>
Why were you riding then? There are way too many cars on the road. If you were commuting, you should have contacted your boss and politely asked to work from 3:00 a.m. to 11:00 a.m. instead.</p>
<p><strong>You were riding at night or in the early morning.</strong><br>
There’s no way drivers can see you. Remember: if you’re one of those people who rides bikes because it keeps the mental darkness at bay, the best time to do so is in the middle of the workday.</p>
<p><strong>You were riding in the middle of the workday.</strong><br>
The only people who should ride their bikes during the workday are bike messengers, who I also dislike. They weave, they bob—it’s inappropriate. Bike messengers need to do what drivers do: go straight, get pissed off, and hate everyone.</p>
<p><strong>You were riding on a back road.</strong><br>
Those roads are narrow and have a lot of twists and turns. There are hardly any cyclists on them. Drivers weren’t expecting you!</p>
<p><strong>You were riding on a main road.</strong><br>
Again, too much traffic. We’ve been over this.</p>
<p><strong>You were riding in the morning, or at night, or on a quiet road, or a main road.</strong><br>
Do I honestly have to spell it out for you? The only appropriate time and place to ride a bike is a time beyond time and a place beyond place, where the space-time continuum is bent so strangely you are both everywhere and nowhere, eternal and nonexistent. You must become the smoke that comes from shadow, the sound of blue, the smell that emanates from the number twelve.</p>
<p><strong>You didn’t signal properly.</strong><br>
I mean, no, I don’t have any “evidence” for that, but you must have done something wrong for an upstanding citizen like the driver of a Ford Focus that looks like it got into a fight with a forklift to strike you. The stats are on my side. Sixty-six percent of drivers <a href="https://www.forbes.com/sites/carltonreid/2019/05/10/cyclists-break-far-fewer-road-rules-than-motorists-finds-new-video-study/?sh=2d9fafbd4bfa">routinely commit moving violations</a>, compared with 5 percent of cyclists when they have somewhere safe to ride. That’s why I believe drivers.</p>
<p><strong>Your bike isn’t an <span>SUV</span>.</strong><br>
If your bike were an <span>SUV</span>, we wouldn’t be having this conversation. You’d be fine. In fact, it would be the Ford Focus driver who’d be all messed up. And that’s why SUVs are considered safe.</p>
<p><strong>You forgot to go back in time and tell people that subsidizing the oil industry might be a bad idea.</strong><br>
When the oil and auto industries teamed up to bend public policy to their will, making a system of roads and parking lots that now function as a continuous subsidy and magnificent symbol of the normalization of injury and pollution, you had a lot of options. You could have objected. You could have shifted public opinion. Instead, you weren’t even born yet. And, rather than go back in time, all you’ve been doing is riding to get groceries and occasionally saying, “Please stop killing us.” On the effort scale? 1/10.</p>
<p><strong>Frankly, I’m not sure a driver even hit you.</strong><br>
Maybe you were just <a href="https://www.nytimes.com/2023/07/29/health/ebikes-safety-teens.html">clipped by a Nissan van</a>. Was there a driver in the van? Has the passive voice historically functioned to deflect responsibility and consolidate unjust power arrangements? These are all fascinating questions that, sadly, we will never know the answers to.</p>
<p><strong>Oops, looks like you died.</strong><br>
Miraculously, the driver has been arrested and will face involuntary vehicular homicide. For killing you, the driver will get <a href="https://chi.streetsblog.org/2015/11/17/driver-who-killed-cyclist-hector-avalos-sentenced-to-only-100-days-in-prison">a hundred days in prison</a>. Apparently, killing someone is <a href="https://www.outsideonline.com/culture/essays-culture/justice-drivers-hit-cyclists/">basically legal</a> if you do it with a car. Also, our liberal city council has decided to make positive change. As I write, they’re enforcing strict new rules to ensure no one can ride their bikes in this part of town ever again.</p>
    </div>
    

    <div>
    <p>
        Please help support our writers and keep our site ad-free by becoming a patron today!
    </p>
    
  </div>

</article>

    


      <div>
        <h5>Suggested Reads</h5>
        <ul>
            <li>
    <a href="https://www.mcsweeneys.net/articles/the-great-sag">
      <p>October  4, 2000</p>
      <p>The Great Sag</p>
</a>    
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/evidence-that-automakers-predicted-senate-hearings-but-not-the-outcome-of-the-2008-presidential-election">
      <p>December  5, 2008</p>
      <p>Evidence That Automakers Predicted Senate Hearings but Not the Outcome of the 2008 Presidential Election</p>
</a>    <p><span>by </span>Elizabeth Worthington</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/variations-on-the-spelling-of-vehicles-submitted-by-my-6th-graders-attempting-to-earn-extra-credit-on-a-weekly-spelling-test">
      <p>February 18, 2002</p>
      <p>Variations on the Spelling of ‘Vehicles,’ Submitted By My 6th Graders Attempting to Earn Extra Credit on a Weekly Spelling Test</p>
</a>    <p><span>by </span>Andre Theisen</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/our-rv-has-a-kitchen-bedroom-bathroom-and-plenty-of-space-for-our-seven-children">
      <p>June 12, 2023</p>
      <p>Our RV Has a Kitchen, Bedroom, Bathroom, and Plenty of Space for Our Seven Children</p>
</a>    <p><span>by </span>Bobbie Armstrong<span> and&nbsp;</span>Madeline Goetz</p>
  </li>

        </ul>
      </div>


  <section>
        <div>
      <h5>Trending 🔥</h5>
      <ol>
          <li>
    <a href="https://www.mcsweeneys.net/articles/youre-a-cyclist-who-was-just-struck-by-a-car-driver-heres-why-it-was-your-fault">
      <p>August 11, 2023</p>
      <p>You’re a Cyclist Who Was Just Struck by a Car Driver. Here’s Why It Was Your Fault</p>
</a>    <p><span>by </span>Chas Gillespie</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/how-to-ensure-your-annual-beach-vacation-destroys-your-relationship-with-your-extended-family">
      <p>July 26, 2023</p>
      <p>How to Ensure Your Annual Beach Vacation Destroys Your Relationship with Your Extended Family</p>
</a>    <p><span>by </span>Talia Argondezzi<span> and&nbsp;</span>Jeff Bender</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/i-regret-to-announce-that-i-will-not-be-canceling-my-plans-with-you-tonight">
      <p>August  4, 2023</p>
      <p>I Regret to Announce That I Will Not Be Canceling My Plans with You Tonight</p>
</a>    <p><span>by </span>Sam Shafaghi</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/what-your-favorite-80s-band-says-about-you">
      <p>July  5, 2011</p>
      <p>What Your Favorite ’80s Band Says About You</p>
</a>    <p><span>by </span>John K. Peck</p>
  </li>

      </ol>
    </div>

      <div>
    <h5>Recently</h5>
    <ul>
        <li>
    <a href="https://www.mcsweeneys.net/articles/im-racketeering-charges-and-im-here-to-rock-this-presidential-indictment-fest-like-you-wouldnt-believe">
      <p>August 15, 2023</p>
      <p>I’m Racketeering Charges, and I’m Here to Rock This Presidential Indictment-Fest Like You Wouldn’t Believe</p>
</a>    <p><span>by </span>Jess Keefe</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/welcome-to-your-new-city-in-the-northwest-where-recycling-is-so-simple">
      <p>August 15, 2023</p>
      <p>Welcome to Your New City in the Northwest, Where Recycling Is So Simple</p>
</a>    <p><span>by </span>Tori Multon</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/can-i-get-away-with-this-on-the-bus-an-faq-for-the-modern-commuter">
      <p>August 14, 2023</p>
      <p>Can I Get Away with This on the Bus? An <span>FAQ</span> for the Modern Commuter</p>
</a>    <p><span>by </span>Seif Drywater</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/predictive-texts-for-the-conflict-averse">
      <p>August 14, 2023</p>
      <p>Predictive Texts for the Conflict-Averse</p>
</a>    <p><span>by </span>Tom Ellison<span> and&nbsp;</span>Caitlin Kunkel</p>
  </li>

    </ul>
  </div>

  </section>


  

    
  
  
  




        

    </main>
  </div></div>]]></description>
        </item>
    </channel>
</rss>