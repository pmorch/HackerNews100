(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 22 Nov 2024 22:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Phased Array Microphone (2023) (269 pts)]]></title>
            <link>https://benwang.dev/2023/02/26/Phased-Array-Microphone.html</link>
            <guid>42215552</guid>
            <pubDate>Fri, 22 Nov 2024 17:10:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://benwang.dev/2023/02/26/Phased-Array-Microphone.html">https://benwang.dev/2023/02/26/Phased-Array-Microphone.html</a>, See on <a href="https://news.ycombinator.com/item?id=42215552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article>

  

  <div>
    <p>A 192-channel phased array microphone, with FPGA data acquisition and beamforming/visualization on the GPU.
Phased arrays allow for applications not possible with traditional directional microphones, as the directionality
can be changed instantly, after the recording is made, or even be focused at hundreds of thousands of points 
simultaneously in real time.</p>

<p>All designs are open source:</p>

<ul>
  <li><a href="https://github.com/kingoflolz/mic_host">Host software</a></li>
  <li><a href="https://github.com/kingoflolz/mic_gateware">FPGA gateware</a></li>
  <li><a href="https://github.com/kingoflolz/mic_hardware">PCB layout and schematics, mechanical components</a></li>
</ul>

<p><img src="https://benwang.dev/assets/mic%20block%20diagram.png" alt=""></p>
<p> Block diagram </p>

<p><img src="https://benwang.dev/assets/mic%20overall.jpg" alt=""></p>
<p> Glamor shot </p>

<h2 id="hardware">Hardware</h2>

<p>To create a phased array microphone, a large number of microphones needs to be placed in an arrangement with
 a wide distribution of spacing. For a linear array, exponential spacing between microphones is found to be optimal for
 broadband signals. To create a 2d array, these symmetrical linear arrays (“arms”) are be placed radially, which allows
the central (“hub”) board to be compact. The total cost for the array is approximately $700.</p>

<h3 id="arms">Arms</h3>

<p>The length of each arm is dictated by the limits of PCB manufacturing and assembly. These boards were made at JLCPCB,
where the maximum length for manufacturing and assembly of 4 layer PCBs was 570mm.</p>

<p>The microphones chosen were the <a href="https://www.lcsc.com/product-detail/MEMS-Microphones_MEMS-MSM261D4030H1CPM_C966942.html">cheapest digital output MEMS microphone</a>
(because there are a lot of them!), which were about $0.5. At this bottom of the barrel price
range, there is little differentiation in the performance characteristics between different microphones. Most have 
decent performance up to 10khz and unspecified matching of phase delay and volume.</p>

<p>These microphones output data using pulse density modulation (PDM), which provides a one bit output at a frequency
significantly higher than the audible range (up to 4 MHz), with the high sampling rate compensating for quantization noise. 
These microphones also support latching the data either on the rising or falling edge of the clock (DDR), which allows
two microphones to be multiplexed on a single wire, reducing the amount of connections required.</p>

<p>Each arm contains 8 microphones sharing 4 output lines, as well as an output buffer on the clock input line.
This ensures the rise times are reasonable, even with hundreds of microphones sharing the same clock signal.</p>

<p>For some reason (likely the low rigidity of the panel and some suboptimal solder paste stencil patterns combined with 
the LGA microphone footprints), the yields on the arm PCBs are not very good, with only 50% of them working out of the 
box. The most common fault was the clock line being shorted to either 3V3 or ground, which unfortunately requires trial
and error of removing microphones from the PCB until the short is resolved. Next time some series resistors on the
clock line would speed this process up a lot, and improving the panelization and paste stencil would likely 
improve yields so extensive rework isn’t required.</p>

<p>Even with rework, there are still some microphones which produce bogus data. These are just masked out in the code, as
there are enough working ones to make up for it (and it’s too much work to remove a bunch of arms to do more rework…)</p>

<p><img src="https://benwang.dev/assets/mic%20arm%20panel.png" alt=""></p>

<h3 id="hub">Hub</h3>

<p>An FPGA is used to collect all the data, due to the large number of low latency IOs available combined with the ability 
to communicate using high speed interfaces (e.g. Gigabit Ethernet). Specifically, the <a href="https://www.colorlight-led.com/product/colorlight-i5-led-display-receiver-card.html">Colorlight i5</a>
card is used, as it has enough IOs, is cheap and readily available, and has two integrated ethernet PHYs 
(only one is used for this project). The card is originally designed as an ethernet interface for LED panels, but has 
been <a href="https://github.com/wuxx/Colorlight-FPGA-Projects">fully reverse engineered</a>. About 100 GPIOs are broken out over 
the DDR2 connector, which is much easier to fan out than the BGA of the original FPGA.</p>

<p><img src="https://benwang.dev/assets/mic%20hub%20board.png" alt=""></p>

<p>Other than the FPGA, the hub contains some simple power management circuitry, and connectors for the arm boards as well
as an Ethernet connector with integrated magnetics.</p>

<h3 id="mechanical-design">Mechanical Design</h3>

<p>The arms are attached with M3 screws to the hub using <a href="https://www.lcsc.com/product-detail/Nuts_Sinhoo-SMTSO3080CTJ_C2916369.html">PCB mounted standoffs/nuts</a> 
, which conveniently can be assembled with SMD processes. The connections from each arm to the hub is made with 8 pin,
2mm pitch connectors.</p>

<p><img src="https://benwang.dev/assets/mic%20hub%20attachment.png" alt=""></p>

<p>The original mechanical design consists of slots on the arm PCBs which interlock with circumferential structural PCBs,
however the low torsional rigidity of the arms means the whole structure deformed too easily.</p>

<p><img src="https://benwang.dev/assets/mic%20structural%20pcb.png" alt=""></p>

<p>The final mechanical design consists of pieces of laser cut 1/4th inch MDF around the outer edge of the array, with each
arm attached to the MDF with some zip ties.</p>

<p><img src="https://benwang.dev/assets/mic%20arm%20attachment.jpg" alt=""></p>

<p>As the microphone array is mounted on the wall (which is very susceptible to reflections), a layer of acoustic foam is
used to attenuate the reflections to make calibration easier.</p>

<h2 id="gateware">Gateware</h2>

<p>The main objective for the gateware is to reliably transmit the raw acquired data losslessly to the computer for 
further processing, while keeping it as simple as possible. Performing decimation and filtering on the
FPGA would reduce the data rate, but sending the raw PDM data is achievable with Gigabit Ethernet. This
reduces the complexity of the FPGA code and allowing faster iteration. Compiling is much faster than place and route,
and it’s much easier to use a debugger in code than in gateware!</p>

<p>There are three major components to the gateware, a module for interfacing with the PDM interfaces, a module for 
creating fixed size packets from those readings, and a UDP streamer to write the packets to the Ethernet interface.</p>

<h3 id="pdm-interface">PDM Interface</h3>

<p>The PDM input module is a relatively simple piece of logic, which divides the 50 MHz system clock by a factor of 16 to 
output a 3.125MHz PDM clock, latches all 96 of the input pins after each clock edge, and then shifts out 32 bits of the 
data on each clock cycle. Each chunk of 192 bits is has a header added which is a 32 bit incrementing integer.</p>

<p>The PDM interface receives data at a rate of 3.125Mhz * 96 (input pins) * 2 (DDR), which is 600Mbps. With the header,
the data rate output from this module is 700Mbps, or approximately 40% utilization of the 32 bit output data path.</p>

<h3 id="packetizer">Packetizer</h3>

<p>The packetizer is essentially a FIFO buffer with a special interface on the input. A standard FIFO marks the output as available
whenever there is at least one item in the queue, but this would lead to smaller packets than requested as the ethernet
interface operates faster than the PDM output (leading to buffer underruns).
Thus, the packetizer waits until there is at least a full packet worth of 
data in the queue before starting a packet, which ensures constant sized packets.</p>

<p>48 PDM output blocks at 224 bits (192 bits of data with a 32 bit header) are placed into each packet, which totals
1344 bytes of data per packet, plus a 20 byte IPv4 header and an 8 byte UDP header, at a rate of approximately 65k pps.</p>

<p>This leads to a wire rate of 715 Mbps, or about 70% utilization of Gigabit Ethernet.</p>

<h3 id="udp-streamer">UDP Streamer</h3>

<p>The LiteEth project made this module very easy, as it abstracts out the lower level complexities of UDP and IP 
encapsulation, ARP tables and the like, and provides a convenient interface for simply hooking up a FIFO to a UDP 
stream. Occasionally there is some latency, but there is enough slack in the bus and buffer in the
packetizer FIFO to absorb any hiccups.</p>

<h3 id="utilization-and-timing">Utilization and Timing</h3>

<p>The FPGA on the Colorlight i5 is a <code>LFE5U-25F-6BG381C</code>, which has 25k LUTs. The design is 
placed and routed with the open source Project Trellis toolchain. By keeping the gateware very simple, 
the utilization on the device is quite low, and there is lots of room for additional functionality.</p>

<div><pre><code>Info: Device utilisation:
Info:                 DP16KD:    16/   56    28%
Info:                EHXPLLL:     1/    2    50%
Info:             TRELLIS_FF:  1950/24288     8%
Info:           TRELLIS_COMB:  3701/24288    15%
Info:           TRELLIS_RAMW:    49/ 3036     1%

Info: Max frequency for clock                   '$glbnet$crg_clkout': 73.17 MHz (PASS at 50.00 MHz)
Warning: Max frequency for clock '$glbnet$eth_clocks1_rx$TRELLIS_IO_IN': 124.07 MHz (FAIL at 125.00 MHz)
</code></pre></div>

<p>(Timing violations on eth rx clock is due to <a href="https://github.com/litex-hub/litex-boards/issues/40#issuecomment-1108817182">false positive from gray counter in liteeth</a>)</p>

<h2 id="software">Software</h2>

<h3 id="cic-filter">CIC Filter</h3>

<p>Each microphone produces a 1 bit signal at 3.125Mhz, and needs to be reduced to a more reasonable sample rate
and bit depth for further
processing. This is done very efficiently with a CIC filter, which only requires a few arithmetic operations to 
process each sample. For understanding more about CIC filters, <a href="https://tomverbeure.github.io/2020/09/30/Moving-Average-and-CIC-Filters.html">this series</a>
of blog posts from Tom Verbeure provides an excellent introduction. Following the nice graphs from there, 
I decided on a 4 stage, 16x decimation CIC filter which reduced the sample rate to a much reasonable 195kHz, at 32 bits.</p>

<p>To ingest the data at 3.125Mhz, the filter must be able to process each set of samples in 320ns. A naive implementation
in Rust wasn’t fast enough on a single core, but an implementation with some less abstraction (and a hence some more 
autovectorization) got there, and is what was used at the end. I also experimented with a version using SIMD 
intrinsics which was much faster, but ended up running into alignment issues when using it in together with other code.</p>

<p>Even with close to a billion bits per second of data to process, a single CPU core can do quite a few operations on 
each individual bit!</p>

<div><pre><code>test cic::bench_cic       ... bench: 574 ns/iter (+/- 79) = 41 MB/s
test cic::bench_fast_cic  ... bench: 181 ns/iter (+/- 24) = 132 MB/s
test cic::bench_simd_cic  ... bench:  36 ns/iter (+/- 0)  = 666 MB/s
</code></pre></div>

<h3 id="calibration">Calibration</h3>

<p>To perform array calibration, a speaker playing white noise is moved around the room in front of the array. An FFT based
cross correlation is performed between all pairs of microphones to compute relative delays.</p>

<p>A cross correlation can be
performed by computing the FFT of both signals (cached and computed once for each signal), and then computing the 
inverse FFT of the complex multiplication of the two. This is quite compute intensive, as there are over 18 thousand
pairs! For the window sizes used of 16-64k, the FFTs are memory bound, and thus the IFFT and peak finding is fused to
avoid writing the results to memory, which results in a 15x speedup. On a 7950X, this process runs in realtime.</p>

<p>Then the positions of the source at each timestep 
and the positions of each microphone is optimized using gradient descent (when you know PyTorch, all optimization 
problems look like gradient descent…). The loss function tries to minimize the difference between the measured
correlations and the ideal correlations, while trying to minimize the deviation of the microphone positions from
the initial positions as well as the jerk of the source trajectory.</p>

<p>As part of the calibration, the speed of sound is also a parameter which is optimized to obtain the best model of the
system, which allows this whole procedure to act as a ridiculously overengineered thermometer.</p>

<p>After a few hundred iterations, it converges to a reasonable solution for both
the source positions and the microphone positions, as well as constants such as the speed of sound. Fortunately this
problem vectorizes well for GPU, and converges in a few seconds.</p>

<p>The final mean position error is on the order of 1mm, and is able to correct for large scale systematic distortions
such as concavity from the lack of structural rigidity. The largest position error between the calibrated positions
and the designed positions is on the order of 5mm, which is enough to introduce significant phase errors to high 
frequency sound if uncorrected, although perhaps not strictly necessary (10khz sound has a wavelength of ~3.4cm).</p>

<p><img src="https://benwang.dev/assets/mic%20calibration.png" alt=""></p>

<h3 id="beamforming">Beamforming</h3>

<p>Beamforming is how the raw microphone inputs are processed to produce directional responses. The simplest method of
beamforming is delay-and-sum (DAS), where each signal is delayed according to its distance from the source. This is 
the type of beamforming implemented for this process, with the beamforming happening in the frequency domain.</p>

<p>In the frequency domain, a delay can be implemented by the complex multiplication of the signal with a linear phase term
proportional to the delay required, which also nicely handles delays which are not integer multiples of the sampling 
period.</p>

<p>Multiple nested subarrays of the original array are used for different frequency ranges. This reduces the processing
required for beamforming, as each frequency does not need to be beamformed with all the microphone. This also ensures
that the beamforming gains of all the frequencies are matched.</p>

<p><img src="https://benwang.dev/assets/mic%20subarray.png" alt=""></p>

<p>Two different types of beamforming visualizations are implemented, a 3d near field beamformer and a 2d far field
beamformer. When the audio source is far away, the wavefront is essentially a flat plane, and how far away the
source is does not meaningfully change the signals at the array. On the other hand, if the source is close to the 
array, the wavefront will have a significant curvature which allows the 3d location of the source to be determined.</p>

<p>The beamformer is implemented as a <a href="https://github.com/openai/triton/">Triton kernel</a>, a Python DSL which compiles to 
run on Nvidia GPUs. When beamforming to hundreds of thousands of points, the massive parallelism provided
by GPUs allows for results to be produced in real time. Some <a href="https://github.com/openai/triton/issues/974">current limitations</a>
with the Triton language around 
support to indexing with shared memory arrays lead to slightly suboptimal performance, but writing CUDA C++ doesn’t 
seem very fun…</p>

<h4 id="near-field-3d-beamforming">Near Field 3D Beamforming</h4>

<p>Near field 3D beamforming is performed a 5cm voxel grid with size 64x64x64. An update rate of 12hz is achieved on a
RTX 4090 with higher update rates limited by overhead of suboptimal CPU-GPU synchronization with the smaller work units. 
The voxel grid is then visualized using <a href="https://vispy.org/">VisPy</a>,
a high performance data visualization library which uses OpenGL. Modern games have millions of polygons, so rendering
a quarter million semi-transparent voxels at interactive framerates is no issue.</p>

<p>A quick demo of the voxel visualization below, note the reflection off the roof!</p>

<video controls="">
<source src="https://benwang.dev/assets/mic%203d%20demo.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>

<h4 id="far-field-2d-beamforming">Far Field 2D Beamforming</h4>

<p>Far field beamforming works similarly, but can be performed in higher resolution as there is no depth dimension
required. A 512x512 pixel grid is used, and the same 12hz update rate achieved. (The far field beamforming uses an 
approximation of just putting the points far away instead of actually assuming planar wavefront due to laziness…)</p>

<p>A demo of the 2d visualization here, but it’s not very exciting due to the poor acoustic environment of my room around 
the array, with lots of reflections and multipath.</p>

<video controls="">
<source src="https://benwang.dev/assets/mic%202d%20demo.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>

<h4 id="directional-audio">Directional Audio</h4>

<p>The previous two beamforming implementations compute the energy of sound from each location, but never materializes
the beamformed audio in memory. A time domain delay and sum beamformer is implemented to allow for directional audio 
recording. It takes a 3D coordinate relative from array center and outputs audio samples. 
An interesting aspect about this beamformer is that it is differentiable with regard to the location from the output. 
This means the location of the audio sources can be optimized
based on some differentiable loss function (like neural network), which might allow for some interesting applications 
such as using a forced alignment model of a multi-party transcript to determine the physical location of each speaker.</p>

<p>A speaker playing some audio is placed in front of the array, with another speaker placed approximately 45 degrees away
at the same distance from array center, playing white noise. The effectiveness of the beamforming can be demonstrated
by comparing the raw audio from a single microphone with the output from the beamforming.</p>

<p>Raw audio from a single microphone:</p>

<p><audio controls="">
<source src="https://benwang.dev/assets/mic%20raw.wav" type="audio/wav">
Your browser does not support the video tag.
</audio></p><p>Beamformed audio:</p>

<p><audio controls="">
<source src="https://benwang.dev/assets/mic%20beamformed.wav" type="audio/wav">
Your browser does not support the video tag.
</audio></p><h3 id="recording">Recording</h3>

<p>As the data from the microphone array is just UDP packets, it can be recorded with tools like <code>tcpdump</code>, and the 
packet capture file can be read to reinject the packets back into the listener. All the programs in the
previous sections are designed to work at real time, but can also work on recorded data using this process.</p>

<p>The tradeoff with this recording implementation is that the output data rate is quite high (due to faithfully recording
everything, even the quantization noise). At 87.5 MBps, a 1-hour recording would be 315 GB! A more optimized
implementation would do some compression, and do the recording after the CIC filter at a lower sample rate.</p>

<h2 id="next-steps">Next Steps</h2>

<p>I consider this project essentially complete, and don’t plan to work on it any further for the foreseeable future, but
there are still lots of possible cool extensions if you’d like to build one!</p>
<ul>
  <li>Using more advanced beamforming algorithms (<a href="https://ntrs.nasa.gov/api/citations/20080015889/downloads/20080015889.pdf">DAMAS</a> etc.)</li>
  <li>Better GUI to combine all existing functions (e.g. See where sound is coming from, and record audio from there)</li>
  <li>Combine differentiable beamforming and neural models (e.g. forced alignment example mentioned above)</li>
</ul>

  </div>

</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon to invest another $4B in Anthropic, OpenAI's biggest rival (386 pts)]]></title>
            <link>https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html</link>
            <guid>42215126</guid>
            <pubDate>Fri, 22 Nov 2024 16:25:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html">https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html</a>, See on <a href="https://news.ycombinator.com/item?id=42215126">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ArticleBody-InlineImage-107408832" data-test="InlineImage"><p>Anadolu | Anadolu | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Friday announced it would invest an additional $4 billion in Anthropic, the artificial intelligence startup founded by ex-OpenAI research executives.</p><p>The new funding brings the tech giant's total investment to $8 billion, though Amazon will retain its position as a minority investor, according to Anthropic, the San Francisco-based company behind the Claude chatbot and AI model.</p><p>Amazon Web Services will also become Anthropic's "primary cloud and training partner," according to a blog post. From now on, Anthropic will use AWS Trainium and Inferentia chips&nbsp;to train and deploy its largest AI models.</p><p>Anthropic is the company behind Claude — one of the chatbots that, like OpenAI's ChatGPT and&nbsp;Google's Gemini, has exploded in popularity. Startups like Anthropic and OpenAI, alongside tech giants such as&nbsp;<a href="https://www.cnbc.com/quotes/GOOG/">Google</a>,&nbsp;<a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a>,&nbsp;<a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a>&nbsp;and&nbsp;<a href="https://www.cnbc.com/quotes/META/">Meta</a>, are all part of a generative AI arms race to ensure they don't fall behind in a market&nbsp;<a href="https://www.bloomberg.com/professional/insights/data/generative-ai-races-toward-1-3-trillion-in-revenue-by-2032/#:~:text=Generative%20AI%20is%20poised%20to,our%20proprietary%20market%2Dsizing%20model." target="_blank">predicted to top $1 trillion</a>&nbsp;in revenue within a decade. </p><p>Some, like Microsoft and Amazon, are <a href="https://www.cnbc.com/2024/03/30/fomo-drives-tech-heavyweights-to-invest-billions-in-generative-ai-.html">backing generative AI startups with hefty investments</a> as well as working on in-house generative AI.</p><p>The partnership announced Friday will also allow AWS customers "early access" to an Anthropic feature: the ability for an AWS customer to do fine-tuning with their own data on Anthropic's Claude. It's a unique benefit for AWS customers, according to a company blog post.</p><p>In March, Amazon's $2.75 billion investment in Anthropic was the company's largest outside investment in its three-decade history. The companies announced an&nbsp;<a href="https://www.cnbc.com/2023/09/25/amazon-to-invest-up-to-4-billion-in-anthropic-a-rival-to-chatgpt-developer-openai.html">initial $1.25 billion investment</a>&nbsp;in September 2023.</p><p>Amazon does not have a seat on Anthropic's board.</p><p>News of Amazon's additional investment comes one month after <a href="https://www.cnbc.com/2024/10/22/anthropic-announces-ai-agents-for-complex-tasks-racing-openai.html">Anthropic announced</a> a significant milestone for the company: AI agents that can use a computer to complete complex tasks like a human would.</p><p>Anthropic's new Computer Use capability, part of its two newest AI models, allows its tech to interpret what's on a computer screen, select buttons, enter text, navigate websites, and execute tasks through any software and real-time internet browsing.</p><p>The tool can "use computers in basically the same way that we do," Jared Kaplan, Anthropic's chief science officer, told CNBC in an interview last month, adding it can do tasks with "tens or even hundreds of steps."</p><p>Amazon had early access to the tool, Anthropic told CNBC at the time, and early customers and beta testers included Asana, Canva and Notion. The company had been working on the tool since early this year, according to Kaplan.</p><p>In September, Anthropic&nbsp;<a href="https://www.cnbc.com/2024/09/04/amazon-backed-anthropic-rolls-out-claude-enterprise-ai-for-big-business.html">rolled out Claude Enterprise</a>, its biggest new product since its chatbot's debut, designed for businesses looking to integrate Anthropic's AI. In&nbsp;<a href="https://www.cnbc.com/2024/06/20/anthropic-claude-3point5-sonnet-ai-announced.html">June</a>, the company debuted its more powerful AI model, Claude 3.5 Sonnet, and in May, it rolled out its&nbsp;<a href="https://www.cnbc.com/2024/05/01/anthropic-iphone-ai-app-business-plan-to-compete-with-openai-announced.html">"Team" plan for smaller businesses</a>.</p><p>Last year, Google&nbsp;<a href="https://www.cnbc.com/2023/10/27/google-commits-to-invest-2-billion-in-openai-competitor-anthropic.html">committed</a>&nbsp;to invest $2 billion in Anthropic, after previously confirming it had taken a 10% stake in the startup alongside a large cloud contract between the two companies.</p></div><div id="SpecialReportArticle-RelatedContent-1"><h2>Don’t miss these insights from CNBC PRO</h2><div><ul><li><a href="https://www.cnbc.com/2024/11/14/warren-buffetts-berkshire-hathaway-takes-a-stake-in-dominos-pizza.html">Warren Buffett's Berkshire Hathaway takes a stake in Domino's Pizza</a></li><li><a href="https://www.cnbc.com/2024/11/17/wall-street-gears-up-for-ma-boom-these-names-could-be-attractive-targets.html">Wall Street is gearing up for an M&amp;A boom under Trump. These companies could be targets</a></li><li><a href="https://www.cnbc.com/2024/11/13/inflation-report-shows-market-could-have-a-recipe-for-disaster-heading-into-new-year-says-economist.html">Inflation report shows market could have a 'recipe for disaster' heading into new year, says economist</a></li><li><a href="https://www.cnbc.com/2024/11/18/morningstar-strategist-picks-2-stocks-from-a-sector-he-is-betting-on.html">Morningstar names cheap stocks in a sector that ‘deserves a place in everybody’s portfolio’</a></li><li><a href="https://www.cnbc.com/2024/11/18/these-2-active-etfs-have-outperformed-the-sp-500-this-year-last-year-and-over-5-years.html">These 2 active ETFs have outperformed the S&amp;P 500 this year, last year and over 5 years</a><br></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Salmon return to lay eggs in historic habitat after dam removal project (272 pts)]]></title>
            <link>https://www.opb.org/article/2024/11/17/salmon-return-to-lay-eggs-in-historic-habitat-after-largest-dam-removal-project-in-us-history/</link>
            <guid>42213663</guid>
            <pubDate>Fri, 22 Nov 2024 13:27:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.opb.org/article/2024/11/17/salmon-return-to-lay-eggs-in-historic-habitat-after-largest-dam-removal-project-in-us-history/">https://www.opb.org/article/2024/11/17/salmon-return-to-lay-eggs-in-historic-habitat-after-largest-dam-removal-project-in-us-history/</a>, See on <a href="https://news.ycombinator.com/item?id=42213663">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3><a href="https://www.opb.org/science_environment/">Science &amp; Environment</a></h3><h2>Less than a month after four towering dams on the Klamath River were demolished, hundreds of salmon made it into waters they have been cut off from for decades to spawn in cool creeks</h2></div><div><p>A giant female Chinook salmon flips on her side in the shallow water and wriggles wildly, using her tail to carve out a nest in the riverbed as her body glistens in the sunlight. In another moment, males butt into each other as they jockey for a good position to fertilize eggs.</p><p>These are scenes local tribes have dreamed of seeing for decades as they fought to bring down four hydroelectric dams blocking passage for struggling salmon along more than 400 miles (644 kilometers) of the Klamath River and its tributaries along the Oregon-California border.</p><p>Now, less than a month after those dams came down in the largest dam removal project in U.S. history, salmon are once more returning to spawn in cool creeks that have been cut off to them for generations. Video shot by the Yurok Tribe show that hundreds of salmon have made it to tributaries between the former Iron Gate and Copco dams, a hopeful sign for the <a href="https://apnews.com/article/klamath-dams-removal-california-oregon-river-salmon-44fefba145d74383aa70a68d50597299">newly freed waterway</a>.</p><p>“Seeing salmon spawning above the former dams fills my heart,” said Joseph L. James, chairman of the Yurok Tribe. “Our salmon are coming home. Klamath Basin tribes fought for decades to make this day a reality because our future generations deserve to inherit a healthier river from the headwaters to the sea.”</p><figure><picture><img src="https://opb-opb-prod.cdn.arcpublishing.com/resizer/v2/L4PZ6DW7WVFY7CAPQQYK4LFYBY.jpg?auth=dbf1036c45044667d8c89bc083e002747fbe8424784231f2f62fa4f20e53863d&amp;width=150" alt="FILE - Excess water spills over the top of a dam on the Lower Klamath River known as Copco 1 near Hornbrook, Calif."></picture><figcaption><p>FILE - Excess water spills over the top of a dam on the Lower Klamath River known as Copco 1 near Hornbrook, Calif.</p><p><em>Gillian Flaccus / AP</em></p></figcaption></figure><p>The Klamath River flows from its headwaters in southern Oregon and across the mountainous forests of northern California before it reaches the Pacific Ocean.</p><p>The completion of the hydroelectric dam removal project on Oct. 2 marked a <a href="https://apnews.com/article/klamath-dam-removal-completed-tribes-435b955f5bfdeaca82de66bfc6551ba1">major victory for local tribes</a>. Through protests, testimony and lawsuits, the tribes showcased the environmental devastation caused by the dams, especially to salmon, which were cut off from their historic habitat and dying in alarming numbers because of poor water-quality.</p><p>There have been lower concentrations of harmful algae blooms since the dam removal, Toz Soto, fisheries program manager with the Karuk Tribe, said during a press conference after the dams came down. In October, the water temperature during the day was an average of 8 degrees Celsius (14 degrees Fahrenheit) cooler compared to the same month over the last nine years, according to the Klamath River Renewal Corporation, the nonprofit entity created to oversee the project.</p><p>“All in all, the fish that came up this year were really healthy,” Soto said. “I didn’t see fish with bacterial infections and things like that, so water temperature’s already having an impact on the fishes’ health.”</p><p>The number of salmon that have quickly made it into previously inaccessible tributaries has also been encouraging. Experts have counted 42 redds, or salmon egg nests, and have tallied as many as 115 Chinook salmon in one day in Spencer Creek, which is above the former J.C. Boyle dam, the furthest upstream of the four removed dams, said Mark Hereford with the Oregon Department of Fish and Wildlife.</p><p>“They’re showing us where the good habitat is; they’re showing us where there’s a lack of habitat,” said Barry McCovey Jr, director of the Yurok Tribal Fisheries department. “So we can use these fish to inform us as river managers, as scientists, where restoration needs to take place.”</p><figure><picture><img src="https://opb-opb-prod.cdn.arcpublishing.com/resizer/v2/LGNPDXOGT5BVVFRWUYS2JOUJZM.jpg?auth=4044d38dcd2939dc685d1e8ed46985f104509dd24221fb95319420e09aed3ad1&amp;width=150" alt="FILE - A view shows the Copco 1 Dam in Hornbrook, Calif., Sunday, Sept. 17, 2023."></picture><figcaption><p>FILE - A view shows the Copco 1 Dam in Hornbrook, Calif., Sunday, Sept. 17, 2023.</p><p><em>Haven Daley / AP</em></p></figcaption></figure><p>Power company PacifiCorp built <a href="https://apnews.com/article/klamath-dam-california-removal-restoration-473a570024584c2e02837434e05693da">the dams</a> to generate electricity between 1918 and 1962. But the structures halted the natural flow of the waterway that was once known as the third-largest salmon-producing river on the West Coast. They disrupted the lifecycle of the region’s salmon, which spend most of their life in the Pacific Ocean but return to the chilly mountain streams to lay eggs.</p><p>At the same time, the dams only produced a fraction of PacifiCorp’s energy at full capacity, enough to power about 70,000 homes. They also didn’t provide irrigation, drinking water or flood control, according to Klamath River Renewal Corporation.</p><p>McCovey said the return of so many salmon happened faster than he had expected and makes him hopeful for the future of the river.</p><p>“Out of all the milestones that we’ve had, this one to me is the most significant,” he said. “It feels like catharsis. It feels like the right path.”</p><p>___</p><p><i>Associated Press reporter Sophie Austin contributed to this report.</i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A “meta-optics” camera that is the size of a grain of salt (189 pts)]]></title>
            <link>https://cacm.acm.org/news/a-camera-the-size-of-a-grain-of-salt-could-change-imaging-as-we-know-it/</link>
            <guid>42212992</guid>
            <pubDate>Fri, 22 Nov 2024 11:39:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/news/a-camera-the-size-of-a-grain-of-salt-could-change-imaging-as-we-know-it/">https://cacm.acm.org/news/a-camera-the-size-of-a-grain-of-salt-could-change-imaging-as-we-know-it/</a>, See on <a href="https://news.ycombinator.com/item?id=42212992">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><section id="sec1"><p id="p-1">When it comes to cameras, size matters, but not in the way you think.</p><p id="p-2">Any time a new smartphone is released, it is easy to drool over the latest, greatest, and biggest features that allow you to take even more stunning selfies composed of even more megapixels. However, in the world of cameras, smaller cameras could end up having a far greater impact on the world at large—and enable a ton of positive applications in society—than the next iPhone camera. Work from researchers at Princeton University and the University of Washington is pointing the way.</p><p id="p-3">A team of researchers from both institutions has published work that uses innovative methods and materials to create a “meta-optics” camera that is the size of a single grain of salt.</p><figure id="attachment_762770" aria-describedby="caption-attachment-762770"><a data-fslightbox="https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg" data-type="image" data-caption="" href="https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg">
				<img decoding="async" src="https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg?w=1024" alt="ultracompact camera system" width="1024" height="576" srcset="https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg 1200w, https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg?resize=300,169 300w, https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg?resize=768,432 768w, https://cacm.acm.org/wp-content/uploads/2024/11/111924.News_.A-Camera-the-Size.jpg?resize=1024,576 1024w" sizes="(max-width: 1024px) 100vw, 1024px">
			</a><figcaption id="caption-attachment-762770">The ultracompact camera system developed by researchers at Princeton University and the University of Washington relies on a technology called a metasurface, which is studded with 1.6 million cylindrical posts and can be produced much like a computer chip.</figcaption></figure><p id="p-4">The meta-optics camera is the first device of its kind to produce full-color images that are equal in quality to those produced by conventional cameras, which are an order of magnitude larger. In fact, the meta-optics camera is 500,000 times smaller than conventional cameras that capture the same level of image quality.</p><p id="p-5">The approach the researchers used to create this meta-optics camera’s small form factor is a huge deal.</p><p id="p-6">They used nano-structures called “metasurfaces” and novel approaches to hardware design to build a meta-optics camera far superior to past efforts, as well as implementing unique AI-powered image post-processing to create high-quality images from the camera.</p><p id="p-7">Their work is impressive on its own for breaking through past limitations of meta-optics imaging devices. Yet it is also notable because it opens the door to the creation of extremely small cameras that can create high-fidelity images for a range of industries and use-cases (for instance, by enabling the use of less-invasive medical imaging without compromising image quality).</p><p id="p-8">This work also unlocks the science-fiction-like possibilities of turning entire surfaces into cameras made up of thousands of such devices, and launching high-quality, ultra-light telescopes into space.</p><p id="p-9">Here’s how they did it—and why it could change the world of imaging as we know it.</p></section><section id="sec2"><h2>From conventional lenses to metasurfaces</h2><p id="p-10">All camera designers and engineers, no matter the type(s) of cameras they design, share the same challenge: they want to make their cameras as compact as possible while still allowing it to record as much light as possible.</p><p id="p-11">Smartphone cameras present a great example of the trade-offs inherent in solving this challenge. Each new smartphone packs more computational firepower into smaller and thinner frames, to the point where the newest generations of smartphones look positively futuristic. However, smartphone cameras are still obviously large and obtrusive on otherwise sleek smartphone frames because camera designers are packing more and more lenses into them so they can take higher-quality pictures.</p><p id="p-12">This means researchers are always on the hunt for ways to compress more optical power into smaller form factors, said Ethan Tseng, a researcher at Princeton who was part of the team that produced the salt-grain-sized meta-optics camera.</p><p id="p-13">“Metasurfaces have emerged as a promising candidate for performing this task,” Tseng said.</p><p id="p-14">A metasurface, Tseng explained, is an artificial, man-made material that allows us to affect light in unique ways. It is an ultrathin, flat surface just half a millimeter wide and studded with millions of cylindrical posts, which are called “nano-antennas.” These nano-antennas can be individually tuned by researchers to shape light in certain ways so that, together, they are capable of producing images just like standard refractive glass lenses—but in a device that is much, much smaller.</p><p id="p-15">“Using metasurfaces enables us to open a large design space of optics that we only hardly were able to access before with conventional refractive optics,” said Felix Heide, a Princeton professor who is the senior author of the study that produced the salt-grain-sized meta-optics camera.</p><p id="p-16">With a standard refractive lens, you can only really shape the surface of the lens and vary the material to get better results. However, with metasurfaces, researchers are able to modulate light at the sub-wavelength level, said Heide.</p><p id="p-17">In the salt-grain-sized camera, the research team was able to create a single metasurface that has more light-steering power than a traditional lens, dramatically reducing the overall size of the camera while still achieving similar results. The meta-optic lens itself is 0.5 millimeters in size, while the sensor is 1 millimeter in size, making the entire camera much, much smaller than traditional lenses.</p><p id="p-18">The researchers did not invent the concept of using metasurfaces for cameras, but they did determine how to make the approach work in a way that was actually useful in the real world. Meta-optics cameras have been designed before, but none of them can produce images that are of sufficient quality to deploy for imaging use cases.</p><p id="p-19">“Existing approaches have been unable to design a meta-optics camera that can capture crisp, wide-field-of-view full-color images,” said Tseng.</p><p id="p-20">The research team’s work changed that. Their meta-optics camera is the first high-quality, polarization-insensitive nano-optic imager for full-color, wide field-of-view imaging.</p><p id="p-21">“We addressed the shortcomings of previous meta-optics imaging systems through advances in both hardware design and software post-processing,” said Tseng. To do that, the researchers used artificial intelligence to address two challenges: lens design and image processing.</p><p id="p-22">First, the team used novel AI optimization algorithms to design the nano-antennas on the actual metasurface. Simulating the optical response of a metasurface and calculating the corresponding gradients can be quite computationally expensive, Tseng said, so the team created essentially fast “proxies” for metasurface physics that allowed them to compute how to design the metasurface very quickly.</p><p id="p-23">Then, a physics-based neural network was used to process the images captured by the meta-optics camera. Because the neural network was trained on metasurface physics, it can remove aberrations produced by the camera.</p><p id="p-24">“We were the first to treat the metasurface as an optimizable, differentiable layer that can perform computation with light,” said Heide. “This made it possible to effectively treat metasurfaces like layers in optical neural networks and piggyback on the large toolbox of AI to optimize these layers.”</p><p id="p-25">Finally, the metasurface physics simulator and the post-processing algorithm were combined into a single pipeline to fabricate the actual meta-optic camera, and then to reconstruct the images it captures into high-quality, full-color images.</p><p id="p-26">This innovative combination of hardware and software means that the researchers’ meta-optics camera produces images that could actually be used in real-world contexts, like medical imaging.</p><p id="p-27">“Only combined with computation were we able to explore this design space and make our lenses work for broadband applications,” said Heide.</p></section><section id="sec3"><h2>Better endoscopes, smartphone cameras, telescopes</h2><p id="p-28">The potential real-world applications of the research are vast.</p><p id="p-29">The most obvious one is medical imaging, which directly benefits from cameras that are as small as possible so as not to be invasive. “We are very excited about miniaturized optics in endoscopes, which could allow for novel non-invasive diagnosis and surgery,” said Heide.</p><p id="p-30">Ultra-compact endoscopes powered by a meta-optics camera could even image regions of the body that are difficult to reach with today’s technology.</p><p id="p-31">Another major area of interest for using meta-optics cameras—or cameras that incorporate meta-optics techniques—is consumer hardware. The ability to design cameras and lenses that are an order of magnitude smaller than those in devices today opens up exciting possibilities across smartphones, wearables, and augmented reality (AR) and virtual reality (VR) headsets.</p><p id="p-32">Your smartphone screen or the back of your phone itself could become a camera, says Heide. Wearables could bake high-quality cameras right into the surfaces of, say, eyeglasses. Or, VR headsets could become dramatically lighter and sleeker, leading to higher adoption and greater use of these devices on the go.</p><p id="p-33">Drones also could benefit from significantly smaller cameras. All drones require cameras of some type to perform their work, whether for military purposes like reconnaissance or civilian ones like order delivery. Much smaller cameras would result in far lighter drones that consume far less battery power, said Tseng.</p><p id="p-34">In fact, with a breakthrough like the meta-optics camera, the very nature of cameras can be rethought entirely.</p><p id="p-35">“Our tiny cameras have also recently allowed us to rethink large cameras as flat arrays of salt-grain cameras—effectively turning surfaces into cameras,” said Heide. Larger metasurfaces could even replace the lenses needed for telescopes, making it not only easier to build them but also to send more powerful lenses into space.</p><p id="p-36">While researchers are still in the early stages of brainstorming and engineering potential real-world applications for meta-optics cameras, the way in which metasurfaces are produced has them excited.</p><p id="p-37">“Metasurfaces are especially interesting because they can be made using the same mature technology used to produce computer chips,” said Tseng. Today’s computer chips are produced on wafers, and each wafer contains hundreds of identical copies of the chip. Metasurfaces are produced in an identical way, which holds the promise of greatly reducing the individual cost per metasurface produced, he said.</p><p id="p-38">Not to mention, while the exact materials used to make metasurfaces vary, the researchers used a silica wafer for their mounting surface and silicon nitride for their nano-antennas. Both materials are compatible with today’s semiconductor manufacturing techniques that pump out computer chips.</p><p id="p-39">This means going from sophisticated computer chips to meta-optics cameras might be easier than we think. If so, the picture for how to use these devices in many different industries could get much, much clearer.</p><h2 id="FurtherReading">Further Reading:</h2><ul id="reflist-1"><li></li><li></li><li></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A Marble Madness-inspired WebGL game we built for Netlify (374 pts)]]></title>
            <link>https://5-million-devs.netlify.com/</link>
            <guid>42212644</guid>
            <pubDate>Fri, 22 Nov 2024 10:31:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://5-million-devs.netlify.com/">https://5-million-devs.netlify.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42212644">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Rebels in the sky – Terminal game about space pirates (143 pts)]]></title>
            <link>https://github.com/ricott1/rebels-in-the-sky</link>
            <guid>42212071</guid>
            <pubDate>Fri, 22 Nov 2024 08:21:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ricott1/rebels-in-the-sky">https://github.com/ricott1/rebels-in-the-sky</a>, See on <a href="https://news.ycombinator.com/item?id=42212071">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Rebels in the Sky</h2><a id="user-content-rebels-in-the-sky" aria-label="Permalink: Rebels in the Sky" href="#rebels-in-the-sky"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description demo_v1.0.18.mp4">demo_v1.0.18.mp4</span>
    <span></span>
  </summary>

  <video src="https://github.com/user-attachments/assets/aaa02f04-06db-4da5-8fa4-732b60083e66" data-canonical-src="https://github.com/user-attachments/assets/aaa02f04-06db-4da5-8fa4-732b60083e66" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">It's the year 2101. Corporations have taken over the world.
The only way to be free is to join a pirate crew and start plundering the galaxy. The only mean of survival is to play basketball.</p>
<p dir="auto">Now it's your turn to go out there and make a name for yourself. Create your crew and start wandering the galaxy in search of worthy basketball opponents.</p>
<p dir="auto">The game is under heavy development and breaking changes are often introduced. If you can't continue an old game because the save file is invalid, you probably need to start a new one or open an issue to check if the save file can be migrated.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Just try it out!</h2><a id="user-content-just-try-it-out" aria-label="Permalink: Just try it out!" href="#just-try-it-out"></a></p>
<p dir="auto">Connect via SSH to try the game.</p>
<p dir="auto"><code>ssh rebels.frittura.org -p 3788</code></p>
<p dir="auto">Save files are deleted after 2 days of inactivity.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build</h3><a id="user-content-build" aria-label="Permalink: Build" href="#build"></a></p>
<p dir="auto">You need to have the rust toolchain installed --&gt; <a href="https://www.rust-lang.org/tools/install" rel="nofollow">https://www.rust-lang.org/tools/install</a>. Then you can clone the repo and build the game with</p>
<p dir="auto"><code>cargo build --release</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With cargo</h3><a id="user-content-with-cargo" aria-label="Permalink: With cargo" href="#with-cargo"></a></p>
<p dir="auto"><code>cargo install rebels</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">From the latest release page</h3><a id="user-content-from-the-latest-release-page" aria-label="Permalink: From the latest release page" href="#from-the-latest-release-page"></a></p>
<ul dir="auto">
<li>Download the latest release asset for your platform from <a href="https://rebels.frittura.org/" rel="nofollow">https://rebels.frittura.org</a>;</li>
<li>Give execution permissions to the executable with <code>chmod +x rebels</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Distro Packages</h3><a id="user-content-distro-packages" aria-label="Permalink: Distro Packages" href="#distro-packages"></a></p>
<details>
  <summary>Packaging status</summary>
<p dir="auto"><a href="https://repology.org/project/rebels-in-the-sky/versions" rel="nofollow"><img src="https://camo.githubusercontent.com/215eb0238498ff1dd9cb718b7e49c7e1ab1b8c2ea204728011c1ccb821c79ef2/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f766572746963616c2d616c6c7265706f732f726562656c732d696e2d7468652d736b792e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/vertical-allrepos/rebels-in-the-sky.svg"></a></p>
</details>
<p dir="auto"><h4 tabindex="-1" dir="auto">Arch Linux</h4><a id="user-content-arch-linux" aria-label="Permalink: Arch Linux" href="#arch-linux"></a></p>
<p dir="auto"><code>rebels-in-the-sky</code> can be installed from the <a href="https://archlinux.org/packages/extra/x86_64/rebels-in-the-sky/" rel="nofollow">official repositories</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pacman -S rebels-in-the-sky"><pre>pacman -S rebels-in-the-sky</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Run</h2><a id="user-content-run" aria-label="Permalink: Run" href="#run"></a></p>
<p dir="auto">This game runs as a terminal application, meaning that you just need to run the executable from your terminal with</p>
<p dir="auto"><code>./rebels</code></p>
<p dir="auto">Suggested minimal terminal size: 160x48. Not all terminals support the game colors nicely, so you might need to try different ones. Here is a list of tested terminals:</p>
<ul dir="auto">
<li>Linux: whatever the default terminal is, it should work</li>
<li>MacOs: <a href="https://iterm2.com/" rel="nofollow">iTerm2</a>, <a href="https://tabby.sh/" rel="nofollow">tabby</a>, <a href="https://wezfurlong.org/wezterm/index.html" rel="nofollow">WezTerm</a></li>
<li>Windows: <a href="https://tabby.sh/" rel="nofollow">tabby</a></li>
</ul>
<p dir="auto"><strong>Important</strong>: currently local bot teams are generated by default to make the game more enjoyable. This behaviour can be disabled by passing the <code>-f</code> flag to the executable. In the future, when more players will be available, the game will default to online teams only.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Music</h2><a id="user-content-music" aria-label="Permalink: Music" href="#music"></a></p>
<p dir="auto">Previous versions had the option to play music directly in the game, but this was removed to reduce the binary size and now music is streamed from internet radios. Nevertheless, you can still listen to the game soundtrack directly by connecting to <code>https://radio.frittura.org/rebels.ogg</code>!</p>
<p dir="auto">You can add more radio stations by including them in <code>assets/data/stream_data.json</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li>Planet gifs were generated using the <a href="https://deep-fold.itch.io/pixel-planet-generator" rel="nofollow">pixel planet generator</a> by <a href="https://deep-fold.itch.io/" rel="nofollow">Deep Fold</a>.</li>
<li>Special thanks to <a href="https://www.ildeposito.org/" rel="nofollow">Il Deposito</a> for inspiration and the great musical archive.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contribution</h2><a id="user-content-contribution" aria-label="Permalink: Contribution" href="#contribution"></a></p>
<p dir="auto">Join the <a href="https://discord.gg/ebjp33UrrV" rel="nofollow">discord</a>! There is no fixed roadmap for the game yet, anyone is welcome to participate with ideas.</p>
<p dir="auto">It is almost guaranteed that you will encounter bugs along your journey. If you do, please open an issue and describe what happened. If you are a developer and want to contribute, feel free to open a pull request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This software is released under the <a href="https://www.gnu.org/licenses/gpl-3.0.en.html" rel="nofollow">GPLv3</a> license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Story of the two thousand stolen Playdate handhelds (172 pts)]]></title>
            <link>https://podcast.play.date/episodes/s01e31/</link>
            <guid>42211689</guid>
            <pubDate>Fri, 22 Nov 2024 06:53:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://podcast.play.date/episodes/s01e31/">https://podcast.play.date/episodes/s01e31/</a>, See on <a href="https://news.ycombinator.com/item?id=42211689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<article id="s01e31">
							<header>
							
							<p><time datetime="2024-11-19T07:30:00.000Z">
									Tuesday, November 19, 2024
								</time>

								•

								54 minutes
							</p>
						</header>


	


	<p><audio controls="">
			<source src="https://download.panic.com/playdate_podcast/PlaydatePodcast-s01e31-True-Crime-Edition.mp3" type="audio/mpeg">

			Your browser does not support the <code>audio</code> element.
		</audio>
	</p>

	<p>Earlier this year, our Financial Controller, Jen, realized our Playdate inventory was 2,000 units short. How did that eventually lead us to a Circle K in North Las Vegas, and just how much should you tip for a roofing consultation, anyway?
Buckle up, because we are going for a ride—in Magnum P.I.'s cool car.</p>
<h2 id="show-notes" tabindex="-1">Show Notes</h2>
<ul>
<li><a href="https://gdcvault.com/play/1034707/The-Playdate-Story-What-Was">Cabel’s GDC Talk</a></li>
<li><a href="https://en.wikipedia.org/wiki/Magnum%2C_P.I.">Magnum, P.I.</a></li>
<li><a href="https://en.wikipedia.org/wiki/Froster">Circle K Froster</a></li>
<li><a href="https://podcast.play.date/episodes/s01e31/">Episode page with photos</a></li>
<li><a href="https://podcast.play.date/episodes/s01e31/transcript/">Episode transcript</a></li>
</ul>



	


	
	
	

<a href="https://podcast.play.date/episodes/s01e31/">Listen Now</a></article>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple will now be treated like a bank (103 pts)]]></title>
            <link>https://9to5mac.com/2024/11/21/apple-will-now-be-treated-like-a-bank-says-us-consumer-financial-protection-bureau/</link>
            <guid>42211525</guid>
            <pubDate>Fri, 22 Nov 2024 06:06:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5mac.com/2024/11/21/apple-will-now-be-treated-like-a-bank-says-us-consumer-financial-protection-bureau/">https://9to5mac.com/2024/11/21/apple-will-now-be-treated-like-a-bank-says-us-consumer-financial-protection-bureau/</a>, See on <a href="https://news.ycombinator.com/item?id=42211525">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1500" height="750" src="https://9to5mac.com/wp-content/uploads/sites/6/2024/11/Apple-will-now-be-treated-like-a-bank-says-US-Consumer-Financial-Protection-Bureau.webp?w=1500" alt="Apple will now be treated like a bank, says US&nbsp;Consumer Financial Protection Bureau | In-store Apple Pay transaction on Square terminal" srcset="https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2024/11/Apple-will-now-be-treated-like-a-bank-says-US-Consumer-Financial-Protection-Bureau.webp?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2024/11/Apple-will-now-be-treated-like-a-bank-says-US-Consumer-Financial-Protection-Bureau.webp?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2024/11/Apple-will-now-be-treated-like-a-bank-says-US-Consumer-Financial-Protection-Bureau.webp?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2024/11/Apple-will-now-be-treated-like-a-bank-says-US-Consumer-Financial-Protection-Bureau.webp?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>The popularity of <a href="https://9to5mac.com/guides/apple-pay/" target="_blank" rel="noreferrer noopener">Apple Pay</a> will now see the Cupertino company regulated by the US&nbsp;Consumer Financial Protection Bureau (CFPB), a watchdog whose role is normally limited to banks and financial services companies.</p>



<p>The decision means that the bureau will have the power to monitor and regulate <a href="https://9to5mac.com/guides/aapl/" target="_blank" rel="noreferrer noopener">Apple’s</a> policies and practices in regard to its mobile wallet services …</p>



<h2 id="h-the-consumer-financial-protection-bureau">The&nbsp;Consumer Financial Protection Bureau</h2>



<p>The CFPB is a US agency responsible for enforcing federal consumer financial law, but also has a broader role as a regulator to ensure that consumer financial products are “fair, transparent, and competitive.”</p>



<blockquote>
<p>We aim to make consumer financial markets work for consumers, responsible providers, and the economy as a whole. We protect consumers from unfair, deceptive, or abusive practices and take action against companies that break the law. We arm people with the information, steps, and tools that they need to make smart financial decisions.</p>
</blockquote>



<p>It was always able to ensure that mobile wallet services like Apple Pay and Google Pay complied with the law, but last year proposed that these services be treated much more like banks, giving the CFPB broader powers to enforce fairness and deal with consumer complaints.</p>



<h2 id="h-apple-pay-will-be-regulated-from-next-month">Apple Pay will be regulated from next month</h2>



<p><em><a href="https://www.bloomberg.com/news/articles/2024-11-21/apple-pay-other-tech-firms-come-under-cfpb-regulatory-oversight?embedded-checkout=true" target="_blank" rel="noreferrer noopener">Bloomberg</a></em> reports that the proposal has been finalized, and will take effect from next month.</p>



<blockquote>
<p>The top US consumer watchdog will supervise&nbsp;<a href="https://www.bloomberg.com/quote/AAPL:US" target="_blank" rel="noreferrer noopener">Apple Inc.</a>&nbsp;and other major technology firms that offer digital wallets and payment apps, finalizing a proposal from last year with several changes.</p>



<p>The US&nbsp;Consumer Financial Protection Bureau&nbsp;will now treat those companies more like banks as long as they handle more than 50 million transactions a year, conducted in US dollars, according to a statement Thursday.&nbsp;</p>
</blockquote>



<p>The agency’s director says the decision was made because mobile wallet services are now an integral part of people’s financial lives.</p>



<blockquote>
<p>“Digital payments have gone from novelty to necessity and our oversight must reflect this reality,” CFPB Director&nbsp;Rohit Choprasaid in the statement.</p>
</blockquote>



<p>More than 60% of the US population now uses a mobile wallet, with Apple Pay the most popular choice.</p>



<h2 id="h-9to5mac-s-take">9to5Mac’s Take</h2>



<p>Apple typically doesn’t change its policies to address legislative concerns until it is forced to do so in each of the countries and regions in which it operates, but on this occasion chose to act ahead of time.</p>



<p>The European Union required Apple to open up access to the NFC payment chip to banks and payment card companies, and it was likely that the CFPB would have imposed the same requirement on the company. Instead of limiting the change to the EU, the iPhone maker made the change globally, getting ahead of the game.</p>



<p>It’s more than a decade <a href="https://9to5mac.com/2013/09/17/why-touch-id-is-bigger-news-than-any-of-us-appreciated/" target="_blank" rel="noreferrer noopener">since I first speculated</a> that <a href="https://9to5mac.com/2015/06/22/opinion-apple-bank/" target="_blank" rel="noreferrer noopener">Apple may end up becoming a bank</a>. While that hasn’t happened yet, we have seen <a href="https://9to5mac.com/2023/05/02/apple-bank/" target="_blank" rel="noreferrer noopener">significant movement in this direction</a>. It already had to obtain banking licenses to <a href="https://9to5mac.com/guides/apple-pay-later/" target="_blank" rel="noreferrer noopener">launch Apple Pay Later</a>, though it later <a href="https://9to5mac.com/2024/06/17/apple-pay-later-united-states-ending/" target="_blank" rel="noreferrer noopener">withdrew the service</a> when it seemed likely to be subject to <a href="https://9to5mac.com/2024/06/19/apple-pay-later-withdrawal-reason/" target="_blank" rel="noreferrer noopener">even more regulation</a>. Today’s CFPB announcement means that whatever labels Apple chooses to use, Apple Pay will now be subject to bank-like regulatory oversight.</p>



<p><em>Photo by&nbsp;<a href="https://unsplash.com/@christiannkoepke?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" target="_blank" rel="noreferrer noopener">Christiann Koepke</a>&nbsp;on&nbsp;<a href="https://unsplash.com/photos/person-browsing-on-white-monitor-WiE01mC9AtY?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" target="_blank" rel="noreferrer noopener">Unsplash</a></em></p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBggKMLOFATDAGg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Mac to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon S3 now supports the ability to append data to an object (187 pts)]]></title>
            <link>https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-express-one-zone-append-data-object/</link>
            <guid>42211280</guid>
            <pubDate>Fri, 22 Nov 2024 04:46:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-express-one-zone-append-data-object/">https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-express-one-zone-append-data-object/</a>, See on <a href="https://news.ycombinator.com/item?id=42211280">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="aws-page-content" data-page-alert-target="true"> 
   <main id="aws-page-content-main" role="main" tabindex="-1"> 
    <div data-eb-tpl-root="" data-reactroot="" data-eb-tpl-n="awsm-whats-new/whats-new-post" data-eb-tpl-v="1.0.0" data-eb-ce="" data-eb-c-scope="d105f9bc-63d3-11ee-8c99-0242ac120002" data-eb-d-scope="DIRECTORIES" data-eb-locale="en-US" data-eb-1e70fe18="" data-eb-ssr-ce="" data-eb-tpl-ns="awsmWhatsNew" data-eb-slot="d105f9bc-63d3-11ee-8c99-0242ac120002" data-eb-slot-meta="{'version':'1.0','slotId':'d105f9bc-63d3-11ee-8c99-0242ac120002','experienceId':'d105f9bc-63d3-11ee-8c99-0242ac120002','allowBlank':false,'hasAltExp':false,'isRTR':false,'filters':{'limit':1,'query':'id \u003d \'v1578391091\''}}"> 
         <main> 
           
           
          <div><p>Amazon S3 Express One Zone now supports the ability to append data to an object. For the first time, applications can add data to an existing object in S3.</p><p>  Applications that continuously receive data over a period of time need the ability to add data to existing objects. For example, log-processing applications continuously add new log entries to the end of existing log files. Similarly, media-broadcasting applications add new video segments to video files as they are transcoded and then immediately stream the video to viewers. Previously, these applications needed to combine data in local storage before copying the final object to S3. Now, applications can directly append new data to existing objects and then immediately read the object, all within S3 Express One Zone.</p><p>  You can append data to objects in S3 Express One Zone in all AWS Regions where the storage class is <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-Endpoints.html" target="_blank">available</a>. You can get started using the AWS SDK, the AWS CLI, or Mountpoint for Amazon S3 (version 1.12.0 or higher). To learn more, visit the <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-buckets-objects-append.html" target="_blank">S3 User Guide</a>.</p></div> 
         </main> 
        </div> 
   </main> 
  </div><div data-lb-comp="modal" data-lb-modal-id="ie-deprecation-msg" data-ie10-deprecation-msg="You are using an outdated browser. Please upgrade to a modern browser to improve your experience."> 
      
     <p>
       AWS support for Internet Explorer ends on 07/31/2022. Supported browsers are Chrome, Firefox, Edge, and Safari. 
      <a href="https://aws.amazon.com/blogs/aws/heads-up-aws-support-for-internet-explorer-11-is-ending/" rel="noopener">Learn more »</a> 
     </p> 
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autoflow, a Graph RAG based and conversational knowledge base tool (206 pts)]]></title>
            <link>https://github.com/pingcap/autoflow</link>
            <guid>42210689</guid>
            <pubDate>Fri, 22 Nov 2024 02:42:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pingcap/autoflow">https://github.com/pingcap/autoflow</a>, See on <a href="https://news.ycombinator.com/item?id=42210689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">autoflow</h2><a id="user-content-autoflow" aria-label="Permalink: autoflow" href="#autoflow"></a></p>
  <p><a href="https://www.pingcap.com/tidb-cloud-serverless/?utm_source=tidb.ai&amp;utm_medium=community" rel="nofollow">
    <img src="https://raw.githubusercontent.com/pingcap/tidb.ai/main/frontend/app/public/nextra/icon-dark.svg" alt="TiDB.AI" width="100" height="100">
  </a>
</p></div>
<p dir="auto"><a href="https://hub.docker.com/r/tidbai/backend" rel="nofollow"><img src="https://camo.githubusercontent.com/d98265459cd0fa737dff9bcbe4f5790f3f775a570ed79391f941dfe644ad60bb/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f762f7469646261692f6261636b656e643f736f72743d73656d76657226617263683d616d643634266c6162656c3d7469646261692532466261636b656e6426636f6c6f723d626c7565266c6f676f3d66617374617069" alt="Backend Docker Image Version" data-canonical-src="https://img.shields.io/docker/v/tidbai/backend?sort=semver&amp;arch=amd64&amp;label=tidbai%2Fbackend&amp;color=blue&amp;logo=fastapi"></a>
<a href="https://hub.docker.com/r/tidbai/frontend" rel="nofollow"><img src="https://camo.githubusercontent.com/c12a65b263504840fe27d881a4f115e82c5dfbc2a0c6bf6823c68dfcb7e7602a/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f762f7469646261692f66726f6e74656e643f736f72743d73656d76657226617263683d616d643634266c6162656c3d74696462616925324666726f6e74656e642626636f6c6f723d626c7565266c6f676f3d6e6578742e6a73" alt="Frontend Docker Image Version" data-canonical-src="https://img.shields.io/docker/v/tidbai/frontend?sort=semver&amp;arch=amd64&amp;label=tidbai%2Ffrontend&amp;&amp;color=blue&amp;logo=next.js"></a>
<a href="https://tidb-ai-playwright.vercel.app/" rel="nofollow"><img src="https://camo.githubusercontent.com/93eef9bffb5e2de940b77726ad4647576bc184b08cace8b639643438108e1418/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636865636b2d72756e732f70696e676361702f746964622e61692f6d61696e3f6e616d6546696c7465723d45324525323054657374266c6162656c3d653265" alt="E2E Status" data-canonical-src="https://img.shields.io/github/check-runs/pingcap/tidb.ai/main?nameFilter=E2E%20Test&amp;label=e2e"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">An open source GraphRAG (Knowledge Graph) built on top of <a href="https://www.pingcap.com/ai?utm_source=tidb.ai&amp;utm_medium=community" rel="nofollow">TiDB Vector</a> and <a href="https://github.com/run-llama/llama_index">LlamaIndex</a> and <a href="https://github.com/stanfordnlp/dspy">DSPy</a>.</p>
<ul dir="auto">
<li><strong>Live Demo</strong>: <a href="https://tidb.ai/" rel="nofollow">TiDB.AI</a></li>
<li><strong>Documentation</strong>: <a href="https://tidb.ai/docs/?utm_source=github&amp;utm_medium=tidb.ai" rel="nofollow">Docs</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Perplexity-style Conversational Search page</strong>: Our platform features an advanced built-in website crawler, designed to elevate your browsing experience. This crawler effortlessly navigates official and documentation sites, ensuring comprehensive coverage and streamlined search processes through sitemap URL scraping.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1237528/341611735-9cc87d32-14ac-47c6-b664-efa7ec53e751.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzM0MTYxMTczNS05Y2M4N2QzMi0xNGFjLTQ3YzYtYjY2NC1lZmE3ZWM1M2U3NTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjMyMWVhMmUyOTExYTMyYzk5YjA5NGQ2YWIyZDNmMjNiMDBiNGJhZDZmYzRlYmIxYjM1MzU4M2RmMTBlNWEyYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.FenzZ_CnBmWquNEcDABbTau2CGViA66cBxkARP7wQIk"><img src="https://private-user-images.githubusercontent.com/1237528/341611735-9cc87d32-14ac-47c6-b664-efa7ec53e751.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzM0MTYxMTczNS05Y2M4N2QzMi0xNGFjLTQ3YzYtYjY2NC1lZmE3ZWM1M2U3NTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjMyMWVhMmUyOTExYTMyYzk5YjA5NGQ2YWIyZDNmMjNiMDBiNGJhZDZmYzRlYmIxYjM1MzU4M2RmMTBlNWEyYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.FenzZ_CnBmWquNEcDABbTau2CGViA66cBxkARP7wQIk" alt="out-of-box-conversational-search" title="Image Title"></a></p>
<p dir="auto">You can even edit the Knowledge Graph to add more information or correct any inaccuracies. This feature is particularly useful for enhancing the search experience and ensuring that the information provided is accurate and up-to-date.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1237528/341612004-7bc57b34-99b7-4c4b-a098-9ad33dd0dfdc.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzM0MTYxMjAwNC03YmM1N2IzNC05OWI3LTRjNGItYTA5OC05YWQzM2RkMGRmZGMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmRkYjQ0MzY4ZTU4YzBmODQ4ZTczNmYxZjUzMDI3MGM2OTZkMGY3YjA3NTJiZTVjZmNmOGZiZTcwM2M4NmM2OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.BSZKuaDcOlT0SAT00rvvKyikxrqG3qqC0YBIYwSdKJM"><img src="https://private-user-images.githubusercontent.com/1237528/341612004-7bc57b34-99b7-4c4b-a098-9ad33dd0dfdc.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzM0MTYxMjAwNC03YmM1N2IzNC05OWI3LTRjNGItYTA5OC05YWQzM2RkMGRmZGMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmRkYjQ0MzY4ZTU4YzBmODQ4ZTczNmYxZjUzMDI3MGM2OTZkMGY3YjA3NTJiZTVjZmNmOGZiZTcwM2M4NmM2OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.BSZKuaDcOlT0SAT00rvvKyikxrqG3qqC0YBIYwSdKJM" alt="out-of-box-conversational-search" title="Image Title"></a></p>
</li>
<li>
<p dir="auto"><strong>Embeddable JavaScript Snippet</strong>: Integrate our conversational search window effortlessly into your website by copying and embedding a simple JavaScript code snippet. This widget, typically placed at the bottom right corner of your site, facilitates instant responses to product-related queries.</p>
</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1237528/322688872-5a445231-a27a-4ae6-8287-a4f8cf7b64d0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzMyMjY4ODg3Mi01YTQ0NTIzMS1hMjdhLTRhZTYtODI4Ny1hNGY4Y2Y3YjY0ZDAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NjZhNDRkYThhMDI4M2E3YzljZTA4YTE5MGRmMTE5MjE2NzYxMGY5NDM4ODM4NGUyNWQxZGE4NWEyODM1MWIxYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.azwCKcbHBK2ZfPrOUUshHWXB7LTg8-T1EpjssOpl_qo"><img src="https://private-user-images.githubusercontent.com/1237528/322688872-5a445231-a27a-4ae6-8287-a4f8cf7b64d0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIyNzUzMDIsIm5iZiI6MTczMjI3NTAwMiwicGF0aCI6Ii8xMjM3NTI4LzMyMjY4ODg3Mi01YTQ0NTIzMS1hMjdhLTRhZTYtODI4Ny1hNGY4Y2Y3YjY0ZDAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NjZhNDRkYThhMDI4M2E3YzljZTA4YTE5MGRmMTE5MjE2NzYxMGY5NDM4ODM4NGUyNWQxZGE4NWEyODM1MWIxYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.azwCKcbHBK2ZfPrOUUshHWXB7LTg8-T1EpjssOpl_qo" alt="embeddable-javascript-snippet" title="Image Title"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deploy</h2><a id="user-content-deploy" aria-label="Permalink: Deploy" href="#deploy"></a></p>
<ul dir="auto">
<li><a href="https://tidb.ai/docs/deploy-with-docker" rel="nofollow">Deploy with Docker Compose</a> (with: 4 CPU cores and 8GB RAM)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tech Stack</h2><a id="user-content-tech-stack" aria-label="Permalink: Tech Stack" href="#tech-stack"></a></p>
<ul dir="auto">
<li><a href="https://www.pingcap.com/ai?utm_source=tidb.ai&amp;utm_medium=community" rel="nofollow">TiDB</a> – Database to store chat history, vector, json, and analytic</li>
<li><a href="https://www.llamaindex.ai/" rel="nofollow">LlamaIndex</a> - RAG framework</li>
<li><a href="https://github.com/stanfordnlp/dspy">DSPy</a> - The framework for programming—not prompting—foundation models</li>
<li><a href="https://nextjs.org/" rel="nofollow">Next.js</a> – Framework</li>
<li><a href="https://ui.shadcn.com/" rel="nofollow">shadcn/ui</a> - Design</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact Us</h2><a id="user-content-contact-us" aria-label="Permalink: Contact Us" href="#contact-us"></a></p>
<p dir="auto">You can reach out to us on <a href="https://twitter.com/TiDB_Developer" rel="nofollow">@TiDB_Developer</a> on Twitter.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome contributions from the community. If you are interested in contributing to the project, please read the <a href="https://github.com/pingcap/autoflow/blob/main/CONTRIBUTING.md">Contributing Guidelines</a>.</p>
<a href="https://next.ossinsight.io/widgets/official/compose-last-28-days-stats?repo_id=752946440" rel="nofollow">
  <themed-picture data-catalyst-inline="true"><picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/7688e1e5672b8d311d3d1627d88f4d75c78c16cd81bc2192dc4d5b8046ee909e/68747470733a2f2f6e6578742e6f7373696e73696768742e696f2f776964676574732f6f6666696369616c2f636f6d706f73652d6c6173742d32382d646179732d73746174732f7468756d626e61696c2e706e673f7265706f5f69643d37353239343634343026696d6167655f73697a653d6175746f26636f6c6f725f736368656d653d6461726b" width="655" height="auto" data-canonical-src="https://next.ossinsight.io/widgets/official/compose-last-28-days-stats/thumbnail.png?repo_id=752946440&amp;image_size=auto&amp;color_scheme=dark">
    <img alt="Performance Stats of pingcap/autoflow - Last 28 days" src="https://camo.githubusercontent.com/aaab798344e1bc436f9d385a67ad0cf4c10b4491ea69b2e1b632042848c559b4/68747470733a2f2f6e6578742e6f7373696e73696768742e696f2f776964676574732f6f6666696369616c2f636f6d706f73652d6c6173742d32382d646179732d73746174732f7468756d626e61696c2e706e673f7265706f5f69643d37353239343634343026696d6167655f73697a653d6175746f26636f6c6f725f736368656d653d6c69676874" width="655" height="auto" data-canonical-src="https://next.ossinsight.io/widgets/official/compose-last-28-days-stats/thumbnail.png?repo_id=752946440&amp;image_size=auto&amp;color_scheme=light">
  </picture></themed-picture>
</a>

<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">TiDB.AI is open-source under the Apache License, Version 2.0. You can <a href="https://github.com/pingcap/autoflow/blob/main/LICENSE.txt">find it here</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tailwind CSS v4.0 Beta 1 (159 pts)]]></title>
            <link>https://tailwindcss.com/blog/tailwindcss-v4-beta</link>
            <guid>42210553</guid>
            <pubDate>Fri, 22 Nov 2024 02:12:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tailwindcss.com/blog/tailwindcss-v4-beta">https://tailwindcss.com/blog/tailwindcss-v4-beta</a>, See on <a href="https://news.ycombinator.com/item?id=42210553">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>About eight months ago we <a href="https://tailwindcss.com/blog/tailwindcss-v4-alpha">open-sourced our progress</a> on Tailwind CSS v4.0. Hundreds of hours of fixing bugs, soul-crushing backward compatibility work, and troubleshooting Windows CI failures later, I’m excited to finally tag the first public beta release.</p>
<p>As I talked about when we published the first alpha, Tailwind CSS v4.0 is an all-new engine built for performance, and designed for the modern web.</p>
<ul role="list">
<li><strong>Built for performance</strong> — full builds in the new engine are up to 5x faster, and incremental builds are over 100x faster — and measured in microseconds.</li>
<li><strong>Unified toolchain</strong> — built-in import handling, vendor prefixing, and syntax transforms, with no additional tooling required.</li>
<li><strong>CSS-first configuration</strong> — a reimagined developer experience where you customize and extend the framework directly in CSS instead of a JavaScript configuration file.</li>
<li><strong>Designed for the modern web</strong> — built on native cascade layers, wide-gamut colors, and including first-class support for modern CSS features like container queries, <code>@starting-style</code>, popovers, and more.</li>
</ul>
<p>There’s so much more to say, but everything you need to get started is in the new beta documentation we published today:</p>
<p><a href="https://tailwindcss.com/docs/v4-beta">Get started with Tailwind CSS v4.0 Beta 1 →</a></p>
<p>Start building and help us bullet-proof this thing for the stable release early in the new year.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mechanically strong yet metabolizable plastic breaks down in seawater (119 pts)]]></title>
            <link>https://www.science.org/doi/abs/10.1126/science.ado1782?af=R</link>
            <guid>42210528</guid>
            <pubDate>Fri, 22 Nov 2024 02:07:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/abs/10.1126/science.ado1782?af=R">https://www.science.org/doi/abs/10.1126/science.ado1782?af=R</a>, See on <a href="https://news.ycombinator.com/item?id=42210528">Hacker News</a></p>
Couldn't get https://www.science.org/doi/abs/10.1126/science.ado1782?af=R: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. women are outpacing men in college completion in every major group (102 pts)]]></title>
            <link>https://www.pewresearch.org/short-reads/2024/11/18/us-women-are-outpacing-men-in-college-completion-including-in-every-major-racial-and-ethnic-group/</link>
            <guid>42210479</guid>
            <pubDate>Fri, 22 Nov 2024 01:52:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pewresearch.org/short-reads/2024/11/18/us-women-are-outpacing-men-in-college-completion-including-in-every-major-racial-and-ethnic-group/">https://www.pewresearch.org/short-reads/2024/11/18/us-women-are-outpacing-men-in-college-completion-including-in-every-major-racial-and-ethnic-group/</a>, See on <a href="https://news.ycombinator.com/item?id=42210479">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article><div><div><ul><li><span>Short Reads</span></li></ul></div>


<p>|</p>


<p><time datetime="2024-11-18T11:57:38-05:00">November 18, 2024</time></p></div>

<div>
<figure><img data-dominant-color="2d2f31" data-has-transparency="false" fetchpriority="high" decoding="async" width="1280" height="720" src="https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?w=640" alt="Students look for family and friends during UCLA's commencement ceremony on June 16, 2023. (Sarah Reingewirtz/MediaNews Group/Los Angeles Daily News via Getty Images)" srcset="https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg 1280w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=300,169 300w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=768,432 768w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=1024,576 1024w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=720,405 720w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=200,113 200w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=260,146 260w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=310,174 310w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=420,236 420w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=640,360 640w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=740,416 740w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=160,90 160w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=320,180 320w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=540,304 540w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=564,317 564w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=1128,634 1128w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=690,388 690w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=268,151 268w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=536,302 536w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=194,110 194w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=148,84 148w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_feature.jpg?resize=296,168 296w" sizes="(max-width: 640px) 100vw, 640px"><figcaption>Students look for family and friends during UCLA’s commencement ceremony on June 16, 2023. (Sarah Reingewirtz/MediaNews Group/Los Angeles Daily News via Getty Images)</figcaption></figure>



<p>Women between the ages of 25 and 34 continue to be more likely than men in the same age group to have a bachelor’s degree. The gender gap in bachelor’s degree completion appears in every major racial or ethnic group, though the size of the gap varies widely.</p>



<figure><a href="https://www.pewresearch.org/?attachment_id=192635"><img data-dominant-color="f6f6f5" data-has-transparency="false" decoding="async" width="840" height="908" src="https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?w=640" alt="A line chart showing that U.S. women have outpaced men in bachelor’s degree completion for decades." srcset="https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png 840w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=278,300 278w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=768,830 768w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=375,405 375w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=200,216 200w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=260,281 260w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=310,335 310w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=420,454 420w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=640,692 640w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=740,800 740w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=160,173 160w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_1.png?resize=320,346 320w" sizes="(max-width: 420px) 100vw, 420px"></a></figure>



<p>In 1995, young men and women were equally likely to hold a bachelor’s degree (25% each). Since then, there has been a growing gap between men and women in college completion. &nbsp;</p>



<p>Today, 47% of U.S. women ages 25 to 34 have a bachelor’s degree, compared with 37% of men.</p>



<p>The share of young women with a bachelor’s degree has increased by 22 percentage points since 1995, from 25% to 47%. Over the same period, men have seen a smaller increase (12 points, from 25% to 37%).</p>


<div id="how-we-did-this" data-wp-interactive="{&quot;namespace&quot;:&quot;prc-block\/collapsible&quot;}" data-wp-context="{&quot;collapsibleId&quot;:&quot;how-we-did-this&quot;,&quot;isOpen&quot;:false}" data-wp-class--is-open="context.isOpen" data-wp-init--scroll-into-view="callbacks.onInitScrollIntoView">

<p>Pew Research Center conducted this analysis to better understand the demographic characteristics driving the gender gap in bachelor’s degree completion in the United States.</p>



<p>This analysis uses data from the U.S. Census Bureau’s <a href="https://cps.ipums.org/cps/">Current Population Survey</a> (CPS) Annual Social and Economic Supplement for 1995 to 2024. The CPS microdata files analyzed were provided by the&nbsp;<a href="https://cps.ipums.org/cps/index.shtml">Integrated Public Use Microdata Series</a> (IPUMS) at the University of Minnesota. The&nbsp;<a href="https://www.census.gov/programs-surveys/saipe/guidance/model-input-data/cpsasec.html">Annual Social and Economic Supplement</a>&nbsp;(ASEC) of the CPS is conducted in March of each year and typically features an expanded sample of more than 75,000 households with about 70,000 interviews.</p>

</div>


<p>A <a href="https://www.pewresearch.org/short-reads/2021/11/08/whats-behind-the-growing-gap-between-men-and-women-in-college-completion/">2021 Pew Research Center survey</a> asked Americans without a bachelor’s degree why they chose not to seek one – and found some gender differences in the responses. For example, men without a bachelor’s degree were more likely than women to say they <em>just didn’t want to get one</em>. In turn, women without a bachelor’s degree were more likely than men to say they <em>couldn’t afford a four-year degree</em>.</p>



<h4 id="gender-gaps-in-college-completion-by-race-and-ethnicity">Gender gaps in college completion by race and ethnicity</h4>



<p>White, Black, Hispanic and Asian women between the ages of 25 and 34 are all more likely than their male counterparts to have a bachelor’s degree. But some groups have made more gains over time than others.</p>



<figure><a href="https://www.pewresearch.org/?attachment_id=192636"><img data-dominant-color="f8f8f6" data-has-transparency="false" decoding="async" width="1280" height="1464" src="https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?w=640" alt="Line charts showing that the gender gap in college completion varies by race and ethnicity in the U.S." srcset="https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png 1280w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=262,300 262w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=768,878 768w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=895,1024 895w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=354,405 354w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=200,229 200w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=260,297 260w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=310,355 310w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=420,480 420w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=640,732 640w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=740,846 740w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=160,183 160w, https://www.pewresearch.org/wp-content/uploads/sites/20/2024/11/SR_24.11.18_college-gender-gap_2.png?resize=320,366 320w" sizes="(max-width: 640px) 100vw, 640px"></a></figure>



<p>White, Black and Hispanic women have outpaced men in college completion in recent decades. For example, in 1995, young White women and men were equally likely to have a bachelor’s degree (29% each). The gap has increased to 10 points today (52% of White women vs. 42% of White men).</p>



<p>Similarly, in 1995, young Black women and men were about equally likely to have a bachelor’s degree (14% vs. 16%). Today, Black women are 12 points more likely than Black men to have one (38% of Black women vs. 26% of Black men).</p>



<p>There is also a growing gender gap between Hispanic men and women in college completion. In 1995, similar shares of Hispanic women and men had a bachelor’s degree (10% vs. 9%), but today the gap between groups is 9 points (31% vs. 22%).</p>



<p>The pattern is different for Asian adults, with women and men making comparable gains over the last few decades. In 1995, 42% of young Asian women and men had a bachelor’s degree. Today, 77% of young Asian women have one, as do 71% of young Asian men.</p>
</div>







<div>
<div>
<figure><img title="Photo of Kiley Hurst" src="https://www.pewresearch.org/wp-content/uploads/sites/20/2021/10/Kiley_Hurst.jpg?w=320" alt="Download Kiley Hurst's photo"></figure>
</div>



<p><a href="https://www.pewresearch.org/staff/kiley-hurst/">Kiley Hurst</a> <span>is a research analyst focusing on social and demographic research at Pew Research Center</span>.</p>
</div></article>

</div></div>]]></description>
        </item>
    </channel>
</rss>