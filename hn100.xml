<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 14 Sep 2025 06:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[If my kids excel, will they move away? (175 pts)]]></title>
            <link>https://jeffreybigham.com/blog/2025/where-will-my-kids-go.html</link>
            <guid>45236411</guid>
            <pubDate>Sun, 14 Sep 2025 00:19:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jeffreybigham.com/blog/2025/where-will-my-kids-go.html">https://jeffreybigham.com/blog/2025/where-will-my-kids-go.html</a>, See on <a href="https://news.ycombinator.com/item?id=45236411">Hacker News</a></p>
<div id="readability-page-1" class="page"><p id="h.rjbw4a5r30z"><span>If my kids excel, will they move away?</span></p><p><span>Jeffrey P. Bigham</span></p><p><span>I grew up on a farm outside of a rural town about an hour southeast of Columbus, Ohio. Like many small towns in America, my town knows “brain drain” – all of my friends from high school who went to college (~30% of my class) now live elsewhere, although most are pretty close by (e.g., several live in the suburbs of Columbus and Cincinnati).<p>Sometimes my hometown feels a million miles away, but it only takes two hours and fifty minutes for me to drive there from Pittsburgh, which is where I live now.</p></span><img alt="Jeff on his dad's farm, looking kind of rough" src="https://jeffreybigham.com/blog/2025/images/image2.jpg" title=""></p><p><span>In Pittsburgh, I’m a professor at Carnegie Mellon University in the top computer science school in the world. I’ve also worked in various large technology companies, who have offices in Pittsburgh to connect with and employ Carnegie Mellon faculty and students.</span></p><p><span>I may not live in my small town anymore, but the fact that the best place in the world to study and do research in computer science is in Pittsburgh means I’m really not that far away. My four kids see their grandparents often, they’re known in my parents’ church and have spent a lot of time on my dad’s farm. The photo above is of me on the farm, wearing some of my dad’s clothes, trying to help out when my dad fell ill a few years ago.</span></p><p><span>Most of my story we’ve been able to take for granted in the United States for the past few decades. If you grow up in the United States, and you’re among the best in the world in your field, you could count on the center of excellence for your field also being in the United States, oftentimes pretty close by, like Pittsburgh being close to my hometown.</span></p><p><span>As a professor, I’m able to recruit the very best students in the world to work on my research. Sometimes that means recruiting Americans and sometimes that means recruiting from elsewhere. <img alt="Jeff wearing an 'America is an idea' t-shirt in front of Nassau hall at Princeton" src="https://jeffreybigham.com/blog/2025/images/image1.jpg" title="Jeff wearing an 'America is an idea' t-shirt in front of Nassau hall at Princeton"> Students come to Pittsburgh from around the world (I’ve advised PhD students and postdocs from about 10 different countries). Five or six years after they start our intensive graduation program, successful students receive their PhDs and that’s when I tend to meet their parents for the first time. Oftentimes, this is the first trip they’ve made to the United States, and they may have only seen their kids a few times during their degree. It hits home because usually these students choose to stay in the United States – after successfully completing their degree with me, they are in high demand not only in our universities but also in technology companies.</span></p>


<p><span>These days the students I talk to are less confident about coming to the United States to study and less confident about staying here after they’re done. They have seen a student grabbed off the street apparently because she wrote an essay expressing concern about the on-going humanitarian crisis in Gaza. They have seen graduate students jailed for what used to be minor immigration offenses. They have seen even greater uncertainty in applying or reapplying for the visas they need to study. And, they have seen their status as students arbitrarily used as leverage in attacking premier universities like Harvard</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span>. Most of these incidents have or probably will be resolved, but the message and fear it causes are real and long-lasting.</span></p><p><span>I am worried that policies that have the intention (or effect) of introducing chaos and cruelty to superstar students will make it less likely for the best of the best to come to America, and this in turn will mean centers of excellence will move elsewhere. While incumbents have an advantage, it doesn’t take much to influence group behavior and movements can be self-reinforcing. The best people in a field like to be where other amazing people are, so they can learn and build off of each other. If the centers of excellence move elsewhere, I’m worried my kids will end up feeling compelled to move away (should they become superstars, as is my hope for them).<p>The brain drain from our small rural communities is real, but many of us have found ways to stay close by and keep those ties. There’s a bunch of reasons to treat international students better than we have over the past months, but these concerns are not thousands of miles away as they seem to some – to me, it's incredibly close to home, and not only because I see the effect on students I work with closely.</p></span></p><p><span>If we cause centers of excellence to move away from Pittsburgh, and away from the United States entirely, that’s the difference between my grandkids living near or very far, and whether they’re likely to grow up visiting me and my dad’s farm often or hardly at all.</span></p><hr><p><a href="#ftnt_ref1" id="ftnt1">[1]</a><span>&nbsp;I’ve owned exactly two Harvard t-shirts in my lifetime – the first when I was an undergrad at Princeton said, ‘Harvard Sucks’, and the second is a normal Harvard t-shirt that I bought this past May.</span></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Two Slice, a font that's only 2px tall (141 pts)]]></title>
            <link>https://joefatula.com/twoslice.html</link>
            <guid>45236263</guid>
            <pubDate>Sat, 13 Sep 2025 23:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joefatula.com/twoslice.html">https://joefatula.com/twoslice.html</a>, See on <a href="https://news.ycombinator.com/item?id=45236263">Hacker News</a></p>
<div id="readability-page-1" class="page">
		
		<p>A font that's only 2px tall, and somewhat readable!  Uppercase and lowercase have some different variants, in case you find one more readable than the other.  Numbers (sort of) and some punctuation marks are included.</p>
		<p>You can probably read this, even if you wish you couldn't.<br>It tends to be easier to read at smaller sizes.</p>
		<p>Try it out below, or <a href="https://joefatula.com/assets/Two%20Slice.ttf">download it</a> (under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a> license, so you can use it commercially but you have to give credit).</p>
		
	
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pass: Unix Password Manager (139 pts)]]></title>
            <link>https://www.passwordstore.org/</link>
            <guid>45236079</guid>
            <pubDate>Sat, 13 Sep 2025 23:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.passwordstore.org/">https://www.passwordstore.org/</a>, See on <a href="https://news.ycombinator.com/item?id=45236079">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main_content_wrap">

<h2>Introducing <code>pass</code></h2>

<p>Password management should be simple and follow <a href="http://en.wikipedia.org/wiki/Unix_philosophy">Unix philosophy</a>. With <code>pass</code>, each password lives inside of a <a href="http://en.wikipedia.org/wiki/GNU_Privacy_Guard"><code>gpg</code></a> encrypted file whose filename is the title of the website or resource that requires the password. These encrypted files may be organized into meaningful folder hierarchies, copied from computer to computer, and, in general, manipulated using standard command line file management utilities.</p>

<p><code>pass</code> makes managing these individual password files extremely easy. All passwords live in <code>~/.password-store</code>, and <code>pass</code> provides some nice commands for adding, editing, generating, and retrieving passwords. It is a very short and simple shell script. It's capable of temporarily putting passwords on your clipboard and tracking password changes using <a href="http://en.wikipedia.org/wiki/Git_(software)"><code>git</code></a>.</p>

<p>You can edit the password store using ordinary unix shell commands alongside the <code>pass</code> command. There are no funky file formats or new paradigms to learn. There is <a href="http://en.wikipedia.org/wiki/Bash_(Unix_shell)">bash</a> <a href="http://en.wikipedia.org/wiki/Command-line_completion">completion</a> so that you can simply hit tab to fill in names and commands, as well as completion for <a href="http://en.wikipedia.org/wiki/Z_shell">zsh</a> and <a href="http://en.wikipedia.org/wiki/Friendly_interactive_shell">fish</a> available in the <a href="https://git.zx2c4.com/password-store/tree/src/completion">completion</a> folder. The <strong>very active community</strong> has produced many impressive <strong><a href="#other">clients and GUIs for other platforms</a></strong> as well as <strong><a href="#extensions">extensions</a></strong> for <code>pass</code> itself.</p>

<p>The <code>pass</code> command is extensively documented in its <a href="https://git.zx2c4.com/password-store/about/">man page</a>.</p>



<h3>Using the password store</h3>

<p>We can list all the existing passwords in the store:</p>

<pre><code>zx2c4@laptop ~ $ pass
Password Store
├── Business
│   ├── some-silly-business-site.com
│   └── another-business-site.net
├── Email
│   ├── donenfeld.com
│   └── zx2c4.com
└── France
    ├── bank
    ├── freebox
    └── mobilephone
</code></pre>

<p>And we can show passwords too:</p>

<pre><code>zx2c4@laptop ~ $ pass Email/zx2c4.com
sup3rh4x3rizmynam3
</code></pre>

<p>Or copy them to the clipboard:</p>

<pre><code>zx2c4@laptop ~ $ pass -c Email/zx2c4.com
Copied Email/jason@zx2c4.com to clipboard. Will clear in 45 seconds.
</code></pre>

<p>There will be a nice password input dialog using the standard <code>gpg-agent</code> (which can be configured to stay authenticated for several minutes), since all passwords are encrypted.</p>

<p>We can add existing passwords to the store with <code>insert</code>:</p>

<pre><code>zx2c4@laptop ~ $ pass insert Business/cheese-whiz-factory
Enter password for Business/cheese-whiz-factory: omg so much cheese what am i gonna do
</code></pre>

<p>This also handles multiline passwords or other data with <code>--multiline</code> or <code>-m</code>, and passwords can be edited in your default text editor using <code>pass edit pass-name</code>.</p>

<p>The utility can <code>generate</code> new passwords using <code>/dev/urandom</code> internally:</p>

<pre><code>zx2c4@laptop ~ $ pass generate Email/jasondonenfeld.com 15
The generated password to Email/jasondonenfeld.com is:
$(-QF&amp;Q=IN2nFBx
</code></pre>

<p>It's possible to generate passwords with no symbols using <code>--no-symbols</code> or <code>-n</code>, and we can copy it to the clipboard instead of displaying it at the console using <code>--clip</code> or <code>-c</code>.</p>

<p>And of course, passwords can be removed:</p>

<pre><code>zx2c4@laptop ~ $ pass rm Business/cheese-whiz-factory
rm: remove regular file ‘/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpg’? y
removed ‘/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpg’
</code></pre>

<p>If the password store is a git repository, since each manipulation creates a git commit, you can synchronize the password store using <code>pass git push</code> and <code>pass git pull</code>, which call <code>git-push</code> or <code>git-pull</code> on the store.</p>

<p>You can read more examples and more features in the <a href="https://git.zx2c4.com/password-store/about/">man page</a>.</p>

<h3>Setting it up</h3>

<p>To begin, there is a single command to initialize the password store:</p>

<pre><code>zx2c4@laptop ~ $ pass init "ZX2C4 Password Storage Key"
mkdir: created directory ‘/home/zx2c4/.password-store’
Password store initialized for ZX2C4 Password Storage Key.
</code></pre>

<p>Here, <code>ZX2C4 Password Storage Key</code> is the ID of my GPG key. You can use your standard GPG key or use an alternative one especially for the password store as shown above. Multiple GPG keys can be specified, for using pass in a team setting, and different folders can have different GPG keys, by using <code>-p</code>.</p>

<p>We can additionally initialize the password store as a git repository:</p>

<pre><code>zx2c4@laptop ~ $ pass git init
Initialized empty Git repository in /home/zx2c4/.password-store/.git/
zx2c4@laptop ~ $ pass git remote add origin kexec.com:pass-store
</code></pre>

<p>If a git repository is initialized, <code>pass</code> creates a git commit each time the password store is manipulated.</p>

<p>There is a more <a href="https://git.zx2c4.com/password-store/about/#EXTENDED%20GIT%20EXAMPLE">detailed initialization example</a> in the <a href="https://git.zx2c4.com/password-store/about/">man page</a>.</p>

<h2 id="download">Download</h2>

<p>The latest version is 1.7.4.</p>

<h3 id="deb">Ubuntu / Debian</h3>

<pre><code>$ sudo apt-get install pass</code></pre>

<h3 id="redhat">Fedora / RHEL</h3>

<pre><code>$ sudo yum install pass</code></pre>

<h3 id="suse">openSUSE</h3>

<pre><code>$ sudo zypper in password-store</code></pre>

<h3 id="gentoo">Gentoo</h3>

<pre><code># emerge -av pass</code></pre>

<h3 id="arch">Arch</h3>

<pre><code>$ pacman -S pass</code></pre>

<h3 id="macintosh">Macintosh</h3>

<p>The password store is available through the <a href="http://brew.sh/">Homebrew package manager</a>:</p>

<pre><code>$ brew install pass</code></pre>

<h3 id="freebsd">FreeBSD</h3>

<pre><code># pkg install password-store</code></pre>

<h3 id="tarball">Tarball</h3>

<ul>
<li><a href="https://git.zx2c4.com/password-store/snapshot/password-store-1.7.4.tar.xz">Version 1.7.4</a>
</li><li><a href="https://git.zx2c4.com/password-store/snapshot/password-store-master.tar.xz">Latest Git</a></li>
</ul>
The tarball contains a generic makefile, for which a simple <code>sudo make install</code> should do the trick.

<h3>Git Repository</h3>

<p>You may <a href="https://git.zx2c4.com/password-store/">browse the git repository</a> or clone the repo:
</p>

<pre><code>$ git clone https://git.zx2c4.com/password-store</code></pre>

<p>All releases are tagged, and the tags are signed with <a href="http://www.zx2c4.com/keys/AB9942E6D4A4CFC3412620A749FC7012A5DE03AE.asc">0xA5DE03AE</a>.</p>

<h2 id="organization">Data Organization</h2>

<h3>Usernames, Passwords, PINs, Websites, Metadata, et cetera</h3>

<p>The password store does not impose any particular schema or type of organization of your data, as it is simply a flat text file, which can contain arbitrary data. Though the most common case is storing a single password per entry, some power users find they would like to store more than just their password inside the password store, and additionally store answers to secret questions, website URLs, and other sensitive information or metadata. Since the password store does not impose a scheme of it's own, you can choose your own organization. There are many possibilities.</p>

<p>One approach is to use the multi-line functionality of pass (<code>--multiline</code> or <code>-m</code> in <code>insert</code>), and store the password itself on the first line of the file, and the additional information on subsequent lines. For example, <code>Amazon/bookreader</code> might look like this:</p>

<pre><code>Yw|ZSNH!}z"6{ym9pI
URL: *.amazon.com/*
Username: AmazonianChicken@example.com
Secret Question 1: What is your childhood best friend's most bizarre superhero fantasy? Oh god, Amazon, it's too awful to say...
Phone Support PIN #: 84719</code></pre>

<p><em>This is the preferred organzational scheme used by the author.</em> The <code>--clip</code> / <code>-c</code> options will only copy the first line of such a file to the clipboard, thereby making it easy to fetch the password for login forms, while retaining additional information in the same file.</p>

<p>Another approach is to use folders, and store each piece of data inside a file in that folder. For example <code>Amazon/bookreader/password</code> would hold bookreader's password inside the <code>Amazon/bookreader</code> directory, and <code>Amazon/bookreader/secretquestion1</code> would hold a secret question, and <code>Amazon/bookreader/sensitivecode</code> would hold something else related to bookreader's account. And yet another approach might be to store the password in <code>Amazon/bookreader</code> and the additional data in <code>Amazon/bookreader.meta</code>. And even another approach might be use multiline, as outlined above, but put the URL template in the filename instead of inside the file.</p>

<p>The point is, the possibilities here are extremely numerous, and there are many other organizational schemes not mentioned above; you have the freedom of choosing the one that fits your workflow best.</p>

<h3 id="extensions">Extensions for <code>pass</code></h3>
<p>In order to faciliate the large variety of uses users come up with, <code>pass</code> supports extensions. Extensions installed to <code>/usr/lib/password-store/extensions</code> (or some distro-specific variety of such) are always enabled. Extensions installed to <code>~/.password-store/.extensions/COMMAND.bash</code> are enabled if the <code>PASSWORD_STORE_ENABLE_EXTENSIONS</code> environment variable is <code>true</code> Read the <a href="https://git.zx2c4.com/password-store/about/">man page</a> for more details.</p>

<p>The community has produced many such extensions:</p>
<ul>
	<li><a href="https://github.com/roddhjav/pass-tomb#readme">pass-tomb</a>: manage your password store in a <a href="https://www.dyne.org/software/tomb/" target="_blank">Tomb</a></li>
	<li><a href="https://github.com/roddhjav/pass-update#readme">pass-update</a>: an easy flow for updating passwords</li>
	<li><a href="https://github.com/roddhjav/pass-import#readme">pass-import</a>: a generic importer tool from other password managers</li>
	<li><a href="https://github.com/palortoff/pass-extension-tail#readme">pass-extension-tail</a>: a way of printing only the tail of a file</li>
	<li><a href="https://github.com/palortoff/pass-extension-wclip#readme">pass-extension-wclip</a>: a plugin to use wclip on Windows</li>
	<li><a href="https://github.com/tadfisher/pass-otp#readme">pass-otp</a>: support for one-time-password (OTP) tokens</li>
</ul>

<h3 id="other">Compatible Clients</h3>
<p>The community has assembled an impressive list of clients and GUIs for various platforms:</p>

<ul>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/dmenu">passmenu</a>: an <strong>extremely useful and awesome</strong> dmenu script</li>
	<li><a href="http://qtpass.org/">qtpass</a>: cross-platform GUI client</li>
	<li><a href="https://github.com/zeapo/Android-Password-Store#readme">Android-Password-Store</a>: Android app</li>
	<li><a href="https://mssun.github.io/passforios/">passforios</a>: iOS app
	</li><li><a href="https://github.com/davidjb/pass-ios#readme">pass-ios</a>: (older) iOS app</li>
	<li><a href="https://github.com/jvenant/passff#readme">passff</a>: Firefox plugin</li>
	<li><a href="https://github.com/dannyvankooten/browserpass#readme">browserpass</a>: Chrome plugin</li>
	<li><a href="https://github.com/mbos/Pass4Win#readme">Pass4Win</a>: Windows client</li>
	<li><a href="https://github.com/Pext/pext_module_pass#readme">pext_module_pass</a>: module for <a target="_blank" href="https://pext.hackerchick.me/">Pext</a></li>
	<li><a href="https://github.com/cortex/gopass#readme">gopass</a>: Go GUI app</li>
	<li><a href="https://github.com/Kwpolska/upass#readme">upass</a>: interactive console UI</li>
	<li><a href="https://github.com/CGenie/alfred-pass#readme">alfred-pass</a>: Alfred integration</li>
	<li><a href="https://github.com/MatthewWest/pass-alfred#readme">pass-alfred</a>: Alfred integration</li>
	<li><a href="https://github.com/johanthoren/simple-pass-alfred">simple-pass-alfred</a>: Alfred integration</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/pass.applescript">pass.applescript</a>: OS X integration</li>
	<li><a href="https://github.com/languitar/pass-git-helper">pass-git-helper</a>: git credential integration</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/emacs">password-store.el</a>: an emacs package</li>
	<li><a href="https://hackage.haskell.org/package/xmonad-contrib-0.13/docs/XMonad-Prompt-Pass.html">XMonad.Prompt.Pass</a>: prompt for Xmonad</li>
</ul>

<h3 id="migration">Migrating to <code>pass</code></h3>
<p>To free password data from the clutches of other (bloated) password managers, various users have come up with different password store organizations that work best for them. Some users have contributed scripts to help import passwords from other programs:</p>

<ul>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/1password2pass.rb">1password2pass.rb</a>: imports 1Password txt or 1pif data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/keepassx2pass.py">keepassx2pass.py</a>: imports KeepassX XML data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/keepass2csv2pass.py">keepass2csv2pass.py</a>: imports Keepass2 CSV data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/keepass2pass.py">keepass2pass.py</a>: imports Keepass2 XML data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/fpm2pass.pl">fpm2pass.pl</a>: imports Figaro's Password Manager XML data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/lastpass2pass.rb">lastpass2pass.rb</a>: imports Lastpass CSV data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/kedpm2pass.py">kedpm2pass.py</a>: imports Ked Password Manager data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/revelation2pass.py">revelation2pass.py</a>: imports Revelation Password Manager data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/gorilla2pass.rb">gorilla2pass.rb</a>: imports Password Gorilla data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/pwsafe2pass.sh">pwsafe2pass.sh</a>: imports PWSafe data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/kwallet2pass.py">kwallet2pass.py</a>: imports KWallet data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/roboform2pass.rb">roboform2pass.rb</a>: imports Roboform data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/password-exporter2pass.py">password-exporter2pass.py</a>: imports password-exporter data
	</li><li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/pwsafe2pass.py">pwsafe2pass.py</a>: imports pwsafe data
	</li><li><a href="https://github.com/Unode/firefox_decrypt/#readme">firefox_decrypt</a>: full blown Firefox password interface, which supports exporting to pass</li>
</ul>

<h2>Credit &amp; License</h2>

<p><code>pass</code> was written by <a href="mailto:Jason@zx2c4.com">Jason A. Donenfeld</a> of <a href="http://zx2c4.com/">zx2c4.com</a> and is licensed under the <a href="http://www.gnu.org/licenses/gpl-2.0.html">GPLv2</a>+.</p>

<h3>Contributing</h3>

<p>This is a very active project with a <a href="https://git.zx2c4.com/password-store/stats/?period=y&amp;ofs=-1">healthy dose of contributors</a>. The best way to contribute to the password store is to <a href="http://lists.zx2c4.com/listinfo.cgi/password-store-zx2c4.com">join the mailing list</a> and send git formatted patches. You may also join the discussion in <code>#pass</code> on Libera.Chat.</p>


      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Myocardial infarction may be an infectious disease (373 pts)]]></title>
            <link>https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease</link>
            <guid>45235648</guid>
            <pubDate>Sat, 13 Sep 2025 21:55:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease">https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease</a>, See on <a href="https://news.ycombinator.com/item?id=45235648">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>A pioneering study by researchers from Finland and the UK has demonstrated for the first time that myocardial infarction may be an infectious disease. This discovery challenges the conventional understanding of the pathogenesis of myocardial infarction and opens new avenues for treatment, diagnostics, and even vaccine development.</span></p><div color="brand-purple"><p>According to the recently published research, an infection may trigger myocardial infarction. Using a range of advanced methodologies, the research found that, in coronary artery disease, atherosclerotic plaques containing cholesterol may harbour a gelatinous, asymptomatic biofilm formed by bacteria over years or even decades. Dormant bacteria within the biofilm remain shielded from both the patient’s immune system and antibiotics because they cannot penetrate the biofilm matrix.</p><p>A viral infection or another external trigger may activate the biofilm, leading to the proliferation of bacteria and an inflammatory response. The inflammation can cause a rupture in the fibrous cap of the plaque, resulting in thrombus formation and ultimately myocardial infarction.</p><p>Professor <strong>Pekka Karhunen</strong>, who led the study, notes that until now, it was assumed that events leading to coronary artery disease were only initiated by oxidised low-density lipoprotein (LDL), which the body recognises as a foreign structure.</p><p>“Bacterial involvement in coronary artery disease has long been suspected, but direct and convincing evidence has been lacking. Our study demonstrated the presence of genetic material – DNA – from several oral bacteria inside atherosclerotic plaques,” Karhunen explains.</p><p>The findings were validated by developing an antibody targeted at the discovered bacteria, which unexpectedly revealed biofilm structures in arterial tissue. Bacteria released from the biofilm were observed in cases of myocardial infarction. The body’s immune system had responded to these bacteria, triggering inflammation which ruptured the cholesterol-laden plaque.</p><p>The observations pave the way for the development of novel diagnostic and therapeutic strategies for myocardial infarction. Furthermore, they advance the possibility of preventing coronary artery disease and myocardial infarction by vaccination.</p><p>The study was conducted by Tampere and Oulu Universities, Finnish Institute for Health and Welfare and the University of Oxford. Tissue samples were obtained from individuals who had died from sudden cardiac death, as well as from patients with atherosclerosis who were undergoing surgery to cleanse carotid and peripheral arteries.</p><p>The research is part of an extensive EU-funded cardiovascular research project involving 11 countries. Significant funding was also provided by the Finnish Foundation for Cardiovascular Research and Jane and Aatos Erkko Foundation.&nbsp;</p><p>The research article <em>Viridans Streptococcal Biofilm Evades Immune Detection and Contributes to Inflammation and Rupture of Atherosclerotic Plaques</em> was published in the Journal of the American Heart Association on 6 August 2025. <a href="https://doi.org/10.1161/JAHA.125.041521">Read the article online</a></p><h2><br>Further information</h2><p>Professor Pekka Karhunen<br>Faculty of Medicine and Health Technology<br>Tampere University<br><span><span>pekka.j.karhunen</span> [at] <span>tuni.fi</span><span> (pekka[dot]j[dot]karhunen[at]tuni[dot]fi)</span></span><br>Tel. +358 400 511361</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Open-Source Maintainer's Guide to Saying No (161 pts)]]></title>
            <link>https://www.jlowin.dev/blog/oss-maintainers-guide-to-saying-no</link>
            <guid>45234593</guid>
            <pubDate>Sat, 13 Sep 2025 19:20:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jlowin.dev/blog/oss-maintainers-guide-to-saying-no">https://www.jlowin.dev/blog/oss-maintainers-guide-to-saying-no</a>, See on <a href="https://news.ycombinator.com/item?id=45234593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-apjhz64k="">  <p>One of the hardest parts of maintaining an open-source project is saying “no” to a good idea. A user proposes a new feature. It’s well-designed, useful, and has no obvious technical flaws. And yet, the answer is “no.” To the user, this can be baffling. To the maintainer, it’s a necessary act of stewardship.</p>
<p>Having created and maintained two highly successful open-source projects, <a href="https://github.com/PrefectHQ/prefect">Prefect</a> and <a href="https://github.com/jlowin/fastmcp">FastMCP</a>, helped establish a third in Apache Airflow, and cut my OSS teeth contributing to Theano, I’ve learned that this stewardship is the real work. The ultimate success of a project isn’t measured by the number of features it has, but by the coherence of its vision and whether it finds resonance with its users. As Prefect’s CTO <a href="https://www.linkedin.com/in/whitecdw/">Chris White</a> likes to point out, “People choose software when its abstractions meet their mental model.” Your job as an open-source maintainer is to first establish that mental model, then relentlessly build software that reflects it. A feature that is nominally useful but not spiritually aligned can be a threat just as much as an enhancement.</p>
<p>This threat can take many forms. The most obvious is a feature that’s wildly out of scope, like a request to add a GUI to a CLI tool — a valid idea that likely belongs in a separate project. More delicate is the feature that brilliantly solves one user’s niche problem but adds complexity and maintenance burden for everyone else. The most subtle, and perhaps most corrosive, is the API that’s simply “spelled” wrong for the project: the one that breaks established patterns and creates cognitive dissonance for future users. In many of the projects I’ve been fortunate to work on, both open- and closed-source, we obsess over this because a consistent developer experience is the foundation of a framework that feels intuitive and trustworthy.</p>
<p>So how does a maintainer defend this soul, especially as a project scales? It starts with documenting not just how the project works, but why. Clear developer guides and statements of purpose are your first line of defense. They articulate the project’s philosophy, setting expectations before a single line of code is written. This creates a powerful flywheel: the clearer a project is about why it exists, the more it attracts contributors who share that vision. Their contributions reinforce and refine that vision, which in turn justifies the project’s worldview. Process then becomes a tool for alignment, not bureaucracy. As a maintainer, you can play defense on the repo, confident that the burden of proof is on the pull request to demonstrate not just its own value, but its alignment with a well-understood philosophy.</p>
<p>This work has gotten exponentially harder in the age of LLMs. Historically, we could assume that since writing code is an expensive, high-effort activity, contributors would engage in discussion before doing the work, or at least seek some sign that time would not be wasted. Today, LLMs have inverted this. Code is now cheap, and we see it offered in lieu of discourse. A user shows up with a fully formed PR for a feature we’ve never discussed. It’s well-written, it “works,” but it was generated without any context for the framework’s philosophy. Its objective function was to satisfy a user’s request, not to uphold the project’s vision.</p>
<p>This isn’t to say all unsolicited contributions are unwelcome. There is nothing more delightful than the drive-by PR that lands, fully formed and perfectly aligned, fixing a bug or adding a small, thoughtful feature. We can’t discourage these contributors. But in the last year, the balance of presumption has shifted. The signal-to-noise ratio has degraded, and the unsolicited PR is now more likely to be a high-effort review of a low-effort contribution.</p>
<p>So what’s the playbook? In FastMCP, we recently tried to nudge this behavior by requiring an issue for every PR. In a perfect example of <a href="https://en.wikipedia.org/wiki/Unintended_consequences">unintended consequences</a>, we now get single-sentence issues opened a second before the PR! More powerful than this procedural requirement is a simple sentence that we are unconvinced that the framework should take on certain responsibilities for users. If a contributor wants to convince us, we all only benefit from that effort! But as I wrote earlier, the burden of proof is on the contributor, never the repo.</p>
<p>A more nuanced pushback against viable code is that as a maintainer, you may be uncomfortable or unwilling to maintain it indefinitely. I think this is often forgotten in fast-moving open-source projects: there is a significant transfer of responsibility when a PR is merged. If it introduces bugs, confusion, inconsistencies, or even invites further enhancements, it is usually the maintainer who is suddenly on the hook for it. In FastMCP, we’ve introduced and documented the <code>contrib</code> module as one solution to this problem. This module contains useful functionality that may nonetheless not be appropriate for the core project, and is maintained exclusively by its author. No guarantee is made that it works with future versions of the project. In practice, many contrib modules might have better lives as standalone projects, but it’s a way to get the ball rolling in a more communal fashion.</p>
<p>One regret I have is that I observe a shift in my own behavior. In the early days of Prefect, we did our best to maintain a 15-minute SLA on our responses. Seven years ago, a user question reflected an amazing degree of engagement, and we wanted to respond in kind. Today, if I don’t see a basic attempt to engage, I find myself mirroring that low-effort behavior. Frankly, if I’m faced with a choice between a wall of LLM-generated text or a clear, direct question with an MRE, I’ll take the latter every time.</p>
<p>I know this describes a fundamentally artisanal, hand-made approach to open source that may seem strange in an age of vibe coding and YOLO commits. I’m no stranger to LLMs. I use them constantly in my own work and we even have an AI agent (hi Marvin!) that helps triage the FastMCP repo. But in my career, this thoughtful, deliberate stewardship has been the difference between utility projects and great ones. We used to call it “community” and I’d like to ensure it doesn’t disappear.</p>
<p>It’s a pessimistic outlook, I know. But when well appplied, this degree of thoughtfulness translates into a better experience for all users: into software whose abstractions meet the universal mental model. Two weeks ago, I was in a room that reminded me this kind of stewardship isn’t dead; it’s being practiced at the highest level.</p>
<p>I had the opportunity to join the MCP Committee for meetings in New York and saw a group skillfully navigating a version of this very problem. MCP is a young protocol whose place in the AI stack has been accelerated more by excitement than maturity. As a result, it is under constant assault with requests that it do more, be more, and solve everything in between.</p>
<p>And yet, over a couple of days, the most important thing I witnessed was a willingness to debate—and to hold every proposal up to a (usually) shared opinion of what the protocol is supposed to be. There was an overriding reverence for its teleological purpose: what it should do and, more critically, what it should not do. I especially admired <a href="https://x.com/dsp_">David’s</a> consistent drumbeat: “That’s a good idea. But is it part of the protocol’s responsibilities?”</p>
<p>Sticking to your guns like that is the hard, necessary work of maturing a technology with philosophical rigor. I left New York more confident than ever in the team and the protocol, precisely because they understand that their job isn’t just to build a protocol, but to be its thoughtful custodians. It was a thrill to see that stewardship up close, and I look forward to seeing it continue in open-source more broadly.</p> <div data-astro-cid-pa3ga7zk=""> <p> <h2>Subscribe</h2>  </p> </div>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Safe C++ proposal is not being continued (139 pts)]]></title>
            <link>https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/</link>
            <guid>45234460</guid>
            <pubDate>Sat, 13 Sep 2025 19:00:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/">https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/</a>, See on <a href="https://news.ycombinator.com/item?id=45234460">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>One year ago, the <a href="https://safecpp.org/draft.html">Safe C++ proposal</a> was made. The goal was to add a safe subset/context into C++ that would give strong guarantees (memory safety, type safety, thread safety) similar to what Rust provides, without breaking existing C++ code. It was an extension or superset of C++. The opt-in mechanism was to explicitly mark parts of the code that belong to the safe context. The authors even state:</p><blockquote><p>Code in the safe context exhibits the same strong safety guarantees as code written in Rust.</p></blockquote><p>The rest remains “unsafe” in the usual C++ sense. This means that existing code continues to work, while new or refactored parts can gain safety. For those who write Rust, Safe C++ has many similarities with Rust, sometimes with adjustments to fit C++’s design. Also, because C++ already has a huge base of “unsafe code”, Safe C++ has to provide mechanisms for mixing safe and unsafe, and for incremental migration. In that sense, all of Safe C++’s safe features are opt-in. Existing code compiles and works as before. Introducing safe context doesn’t break code that doesn’t use it.</p><p>The proposal caught my interest. It seemed like a good compromise to make C++ safe, although there were open or unresolved issues, which is completely normal for a draft proposal. For example, how error reporting for the borrow checker and lifetime errors would work, or how generic code and templates would interact with lifetime logic and safe/unsafe qualifiers. These are just some of the points, the proposal is very long and elaborate. Moreover, I am not a programming language designer, so there might be better alternatives.</p><p>Anyway, today I discovered that the proposal will no longer be pursued. When I thought about the proposal again this morning, I realized I hadn’t read any updates on it for some time. So I searched and found some answers on <a href="https://www.reddit.com/r/cpp/comments/1lhbqua/any_news_on_safe_c/">Reddit</a>.</p><p>The response from Sean Baxter, one of the original authors of the Safe C++ proposal:</p><blockquote><p>The Safety and Security working group voted to prioririze Profiles over Safe C++. Ask the Profiles people for an update. Safe C++ is not being continued.</p></blockquote><p>And again:</p><blockquote><p>The Rust safety model is unpopular with the committee. Further work on my end won’t change that. Profiles won the argument. All effort should go into getting Profile’s language for eliminating use-after-free bugs, data races, deadlocks and resource leaks into the Standard, so that developers can benefit from it.</p></blockquote><p>So I went to read the documents related to Profiles[1][2][3][4]. I try to summarize what I understood: they are meant to define modes of C++ that impose constraints on how you use the language and library, in order to guarantee certain safety properties. They are primarily compile-time constraints, though in practice some checks may be implemented using library facilities that add limited runtime overhead. Instead of introducing entirely new language constructs, profiles mostly restrict existing features and usages. The idea is that you can enable a profile, and any code using it agrees to follow the restrictions. If you don’t enable it, things work as before. So it’s backwards-compatible.</p><p>Profiles seem less radical and more adoptable, a safer-by-default C++ without forcing the Rust model that aims to tackle the most common C++ pitfalls. I think Safe C++ was more ambitious: introducing new syntax, type qualifiers, safe vs unsafe contexts, etc. Some in the committee felt that was too heavy, and Profiles are seen as a more pragmatic path. The main objection is obvious: one could say that Profiles restrict less than what Safe C++ aimed to provide.</p><p>Reading comments here and there, there is visible resistance in the community toward adopting the Rust model, and from a certain point of view, I understand it. If you want to write like Rust, just write Rust. Historically, C++ is a language that has often taken features from other worlds and integrated them into itself. In this case, I think that safety subsets of C++ already exist informally somehow. Profiles are an attempt to standardize and unify something that already exists in practice. Technically, they don’t add new fundamental semantics. Instead, they provide constraints, obligations and guarantees.</p><p>In my opinion, considering the preferences of the committee and the entire C++ community, although I appreciated the Safe C++ proposal and was looking forward to seeing concrete results, considering the C++ context I believe that standardizing and integrating the Profiles as proposed is a much more realistic approach. Profiles might not be perfect, but they are better than nothing. They will likely be uneven in enforcement and weaker than Safe C++ in principle. They won’t give us silver-bullet guarantees, but they are a realistic path forward.</p><p>[1] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3081r1.pdf">Core safety profiles for C++26</a></p><p>[2] <a href="https://open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3589r0.pdf">C++ Profiles: The Framework</a></p><p>[3] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3704r0.pdf">What are profiles?</a></p><p>[4] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3651r0.pdf">Note to the C++ standards committee members</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Case Against Social Media Is Stronger Than You Think (197 pts)]]></title>
            <link>https://arachnemag.substack.com/p/the-case-against-social-media-is</link>
            <guid>45234323</guid>
            <pubDate>Sat, 13 Sep 2025 18:39:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arachnemag.substack.com/p/the-case-against-social-media-is">https://arachnemag.substack.com/p/the-case-against-social-media-is</a>, See on <a href="https://news.ycombinator.com/item?id=45234323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ggx-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ggx-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 424w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 848w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 1272w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ggx-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png" width="797" height="567" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/06835bba-2e08-4334-ae3a-077622595096_797x567.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:567,&quot;width&quot;:797,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:840153,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://arachnemag.substack.com/i/172046631?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ggx-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 424w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 848w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 1272w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>The Mob, 1935, by Carl Hoeckner</figcaption></figure></div><p><em>1. Introduction</em></p><p><span>The philosopher Dan Williams recently published two pieces on social media— </span><a href="https://asteriskmag.com/issues/11/scapegoating-the-algorithm" rel="">“Scapegoating the Algorithm”</a><span> at Asterisk Magazine, and </span><a href="https://www.conspicuouscognition.com/p/the-case-against-social-media-is" rel="">“The Case Against Social Media is Weaker Than You Think”</a><span> at his Substack. As their titles attest to, both argue that the case against social media, on epistemic and political grounds, has been considerably overstated.</span></p><p><span>I recently published </span><a href="https://arachnemag.substack.com/p/the-leviathan-the-hand-and-the-maelstrom?r=18kjq3" rel="">a lengthy essay</a><span> arguing the opposite: that the case against social media has, if anything, been understated. And so, especially given I’m new to Substack, Williams’s recent work gives me a welcome opportunity to pick a fight for engagement-farming purposes. Sadly for my subscriber count, I agree with Williams on quite a bit, and so this is going to be less combative polemicizing and more demonstrating that a range of serious worries about social media are capable of charting a course between the equivocal lines of evidence he underlines.</span></p><p>I am going to focus on the putative political impacts of social media—in particular its impact on political polarization—rather than the specifically epistemic ones. These are often conflated, but I think it is helpful to cleave them apart. It is possible to believe, as I in fact do, that social media has had a dangerous, incendiary effect on American politics, while also believing that related concerns about misinformation and conspiracy theorizing are somewhat (though not entirely) overblown. I may decide to write a short follow-up on the latter, but will put it to the side for now.</p><p><span>With respect to social media’s supposed contributions to political polarization, and in particular </span><a href="https://www.annualreviews.org/content/journals/10.1146/annurev-polisci-051117-073034" rel="">‘affective polarization,’</a><span> Williams highlights four main lines of evidence that contradict the prevailing narrative. First, polarization has been rising for decades and began doing so well in advance of social media. Second, polarization has increased the most in recent years among those who use social media and the internet the least, those over 65. Third, trends in polarization have diverged in countries that all have widespread social media use. And fourth, several high-quality experimental studies have found a negligible effect of social media use on individuals’ levels of polarization.</span></p><p>If I can import a cliché from academic philosophy, it is sometimes said that all argumentative objections fall into one of two categories: “oh yeah?” or “so what?” I opt for a mix of both here. I begin with the “oh yeah?”: Williams’s evidence is much less convincing than it initially seems, and beyond ruling out an implausibly large spike in polarization, tells us little about how social media might be influencing its trajectory.</p><p><span>I then turn to my main focus, the “so what?”: even if Williams is right that social media has not significantly contributed to affective polarization in the U.S., this is consistent with it having plenty of other negative effects on American politics. To make this argument concrete, I highlight different lines of evidence to Williams in order to demonstrate first, that social media has indeed had a deeply concerning impact on our politics, and second, that one need not rely on the metric of affective polarization in particular to make this case. I refer to the alternative view I develop as an </span><em>elite radicalization theory</em><span> of online politics.</span></p><p>While I am an avid reader and admirer of Williams’s work, I think he is engaged in what has unfortunately become a fashionable form of academic contrarianism: arguing that despite what very much seem to be foundational, risk-laden changes in our social order, all we observe is in fact business-as-usual, and to the extent it seems otherwise, that is because of some mix of psychological bias and media overhype. </p><p>A similar contrarianism plagued early predictions of the threat posed by Donald Trump. Those critics have either relented or fallen quiet now. It is time for those who continue to minimize the downside risks of the digital media revolution to finally do the same.</p><p><em>2. Williams on social media and polarization</em></p><p>Let’s first run through Williams’s four main lines of evidence against social media-driven polarization in a bit more detail.</p><p><span>The first is that affective polarization has been rising since well before the advent social media. As Williams explains in </span><a href="https://asteriskmag.com/issues/11/scapegoating-the-algorithm" rel="">“Scapegoating the Algorithm,”</a><span> this trend begins in the late 20</span><sup>th</sup><span> century and likely has its roots in a) the partisan realignment surrounding the Democratic party’s 60s-era embrace of racial integration, and the Republican party’s subsequent “Southern Strategy” to appeal to disaffected white opponents of integration; and b) the rise of a more combatively partisan media ecosystem in the aftermath of Reagan’s 1987 repeal of the “fairness doctrine,” which required broadcasters to present balanced perspectives on controversial public issues.</span></p><p><span>Williams’s second line of evidence is the study </span><a href="https://www.pnas.org/doi/10.1073/pnas.1706588114" rel="">Boxell, Gentzkow, and Shapiro (2017)</a><span>, which found that, between 1996 and 2016, the demographic groups that used social media and the internet the least, those 65 and older, also saw the largest increase in polarization.</span></p><p><span>Williams’s third line of evidence is a later study by the same authors, </span><a href="https://direct.mit.edu/rest/article-abstract/106/2/557/109262/Cross-Country-Trends-in-Affective-Polarization?redirectedFrom=fulltext" rel="">Boxell, Gentzkow, and Shapiro (2024)</a><span>, which studied trends in affective polarization since the 1980s in 12 OECD countries and found that, while the U.S. exhibited the largest increase over this period, countries like Australia, Britain, Norway, Sweden, and Germany all saw polarization fall, including in the period after the initial spread of social media.</span></p><p><span>Williams’s fourth line of evidence is that several high-quality experimental studies have examined the relationship between social media and polarization at an individual level and found no evidence of a connection. </span><a href="https://www.science.org/doi/10.1126/science.abp9364" rel="">Guess et. al. (2023a)</a><span>, </span><a href="https://www.science.org/doi/10.1126/science.add8424" rel="">Guess et. al. (2023b)</a><span>, </span><a href="https://www.nature.com/articles/s41586-023-06297-w" rel="">Nyhan et. al. (2023)</a><span>, and </span><a href="https://www.pnas.org/doi/10.1073/pnas.2321584121" rel="">Alcott et. al. (2024)</a><span> all found that interventions meant to limit social media’s ostensible polarizing effects (such as replacing default with reverse-chronological feeds, hiding reshares, reducing exposure to content from like-minded sources, and deactivating social media entirely) had a negligible impact on users’ levels of polarization.</span></p><p><em>2.1. “Oh yeah?”</em></p><p>Taken together, this evidence indeed appears to contradict that social media has been a major contributor to American political polarization. But dig a bit deeper, and things get much messier and more uncertain.</p><p>The fact is that we have relatively little data about affective polarization since ~2010, when smartphone and social media use became widespread in the U.S. The vast majority of this data comes from the American National Election Studies (ANES) survey, whose flagship data set is only collected during presidential election years. As a result, we only have 4 data points since 2010, those for 2012, 2016, 2020, and 2024.</p><p>It is difficult to draw strong conclusions from this data one way or the other. Besides the fact there isn’t much of it, rival-party feeling does appear to start falling faster around 2010 or so, which would be consistent with some social-media related effect. But I concede I’m just eyeballing here; more rigorous conclusions will have to await further research.</p><p><span>The data on international trends in affective polarization isn’t especially convincing either. The main study cited by Williams—</span><a href="https://direct.mit.edu/rest/article-abstract/106/2/557/109262/Cross-Country-Trends-in-Affective-Polarization?redirectedFrom=fulltext" rel="">Boxell, Gentzkow, and Shapiro (2024)</a><span>—looks at affective polarization in 12 countries since the 80s, but for our purposes, it is only the post-2010-years that are relevant, since it is only after this point that social media would’ve had any notable effect. That in mind, the data here is also relatively unsatisfying:</span></p><p>With the exception of Germany, there are never more than 3 data points per country after 2010. When you consider on top of this that rates of social media penetration over time probably vary across these countries, it becomes clear that there is little we can learn through these comparisons without more context, or at least more time.</p><p>The bare fact of diverging trend lines also tells us little to begin with, since these trends might have been different absent the influence of social media. For instance, polarization might have risen more slowly in the U.S. and fallen more quickly in Germany. Sure, maybe this data does conflict with the most alarmist narrative about social media—that there was some massive spike in polarization after 2010—but that was never plausible to begin with. There is still more than enough room, even in light of this data, to be justifiably concerned about how social media is shaping American politics.</p><p>That in mind, let’s move on to Williams’s two other lines of evidence—that polarization has grown more in recent years among the elderly (those least likely to use social media), and that the experimental research on social media has not turned up any appreciable relationship with polarization.</p><p><span>The paper documenting higher polarization among the elderly—</span><a href="https://www.pnas.org/doi/10.1073/pnas.1706588114" rel="">Boxell, Gentzkow, and Shapiro (2017)</a><span>—makes a very important caveat that Williams does not mention. While the authors see themselves as ruling out the most straightforward version of the social media-driven polarization story, they acknowledge it is possible to construct an alternative account that they cannot rule out:</span></p><blockquote><p><span>[Alternative accounts would need to address] the rapid increase in polarization among those with limited internet use and negligible use of social media</span><em>. </em><strong>However, it is possible to construct such accounts.</strong><span> It may be that social media increases polarization among the young while some other factor increases it among the old. </span><strong>It may be that there are spillovers across demographic groups; young adults polarized through social media might in turn affect the views of older adults or might indirectly influence older adults through channels like the selection of politicians or the endogenous positioning of traditional media. </strong><span>(p. 3, bold my own)</span></p></blockquote><p><span>While we should not fault Boxell, Gentzkow, and Shapiro for the scope of their specific project, it is obvious in my view that every one of these spillovers is in play. I suspect the most important is the possibility that social media is influencing the tone and coverage of traditional media, and in particular cable news. This should ring true to anyone familiar with right wing cable’s aggressive coverage of various social media-native controversies, like the recent </span><a href="https://www.foxnews.com/video/6376342204112" rel="">American Apparel ad starring Sydney Sweeney</a><span>, or the various </span><a href="https://www.instagram.com/reel/DNRvRHVJ2nm/" rel="">sorority rush videos</a><span> currently going viral on TikTok. If this kind of coverage increases polarization among the elderly, that would still in part be social media’s fault. In light of potential spillover effects, this evidence can at best rule out that social media has influenced all demographic groups equally, and just as immediately, rather than seeing its influence channeled from some groups to others.</span></p><p><span>But this kind of ‘channeling’ was always going to be part of the story. Social media’s intrinsically social nature guarantees it will have large spillover effects. Ignoring this is like assuming that, if you could only delete your social media accounts, that would insulate you from 100% of social media’s effects on your life. The obvious reason it won’t is because </span><em>everyone around you </em><span>will still be using social media, and their use will continue to affect you (e.g. by their telling you what’s happening online, by you feeling excluded from their group-chats, etc.). As long as social media represents the social world to us, and as long as we share our impressions of that social world with others offline, then its effects on individuals are going to ‘spill over’ to their peers.</span></p><p><span>It is therefore wrong to assume that, since old people don’t use social media (or use it much less), they are not being affected by it. Due to spillovers, those of them that watch cable news are arguably affected by it every day. Not only that, but their relative lack of experience with the modern information environment means their political attitudes are likely to be</span><em> more </em><span>sensitive to (even the indirect effects of) social media than those who use it more often. </span></p><p>As a general rule, very few strong conclusions about the aggregate impact of social media can be drawn from research that does not account for these kinds of spillovers.</p><p>Let’s now turn to Williams’s last line of evidence, the experimental research on social media and political attitudes. This is probably the strongest evidence we have against social media-driven polarization. And yet, think carefully about what it is really in a position to tell us, and the result is again underwhelming.</p><p><span>Of the experiments Williams cites, the longest lasted three months, and only one—</span><a href="https://www.pnas.org/doi/10.1073/pnas.2321584121" rel="">Alcott et. al. (2024)</a><span>—actually entailed the complete deactivation of social media, rather than just modifications to how and what content was displayed. Already, this should give us pause about interpreting this evidence to imply that social media’s supposed impacts on politics are overblown. On a more even-handed read, all the studies besides Alcott et. al. (2024) show is that feed design changes don’t impact users’ political attitudes over relatively short periods.</span></p><p>Even focusing on complete deactivation, though, spillover effects remain a serious problem. Like anyone else, social media users’ political attitudes originate from and are continually modified by a wide variety of sources: their families, their friends, prominent members of their community, other acquaintances, online news, print news, T.V. news, podcasts, talk shows, radio shows, and so on. If these other influences have themselves been influenced by social media, then deactivating participants’ Facebooks and Instagrams for six weeks—the intervention in Alcott et. al. (2024)—does little to isolate them from the effects of social media in general. They experience those effects anyway by continuing to communicate with other social media users.</p><p><span>This is an even greater concern than usual in the case of Alcott et. al. (2024) because their intervention took place in the six weeks </span><em>before the 2020 election. </em><span>Those who took part in the experiment would have been inundated by far more political communication than usual, communication which was almost certainly influenced in some form by social media.</span></p><p>The moral is again that spillover effects aren’t to be ignored. Given social media’s now-enormous role in shaping public discourse, interventions targeting personal use alone can at best isolate a small sliver of social media’s total impact. That means that even high-quality experiments on individuals tell us little about social media-driven polarization in aggregate.</p><p><em>3. “So what?”</em></p><p>Suppose all these objections are unimpeachably correct. I’m sure they aren’t, but let’s assume. That would still only amount to a case for uncertainty. It remains, then, to make the case that rather than only remaining cautious about social media’s effects on politics, we should be seriously concerned.</p><p>Williams is right that the evidence on social media and affective polarization will not get us there on its own. But affective polarization is hardly the only potential measure of social media’s harms. Consider the following claims Williams cites (in “The Case Against Social Media is Weaker than You Think”) as typical of the public concern over social media:</p><blockquote><p><span>Alexandria Ocasio-Cortez </span><a href="https://www.businessinsider.com/facebook-meta-rep-alexandria-ocasio-cortez-aoc-cancer-to-democracy-2021-10" rel="">describes</a><span> Meta as “a cancer to democracy metastasizing into a global surveillance and propaganda machine for boosting authoritarian regimes and destroying civil society.”</span></p><p><span>Jonathan Haidt has </span><a href="https://www.theatlantic.com/magazine/archive/2022/05/social-media-democracy-trust-babel/629369/" rel="">argued</a><span> that social media platforms “dissolved the mortar trust, belief in institutions, and shared stories that held a large and diverse secular democracy [in America] together.’’</span></p><p><span>Obama </span><a href="https://barackobama.medium.com/my-remarks-on-disinformation-at-stanford-7d7af7ba28af" rel="">suggests</a><span> that one of “the biggest reasons for democracies weakening is the profound change that’s taking place in how we communicate and consumer information.”</span></p></blockquote><p>These are of course strong criticisms. But it is far from clear that they depend on social media being a major driver of affective polarization in particular. One of the issues with placing academic and non-academic discourses in conversation is that the terms of the former may not map cleanly onto the terms of the latter (and vice versa). In this case, there is no reason to assume claims like those above rest on the evidence in favor of social-media driven polarization, especially if there are other, more convincing lines of evidence available.</p><p><span>This is my primary complaint against Williams: there </span><em>are</em><span> other, more convincing</span><em> </em><span>lines of evidence available. </span></p><p><span>Even if social media is not increasing affective</span><em> </em><span>polarization, it is making our politics more angry, tribal, and violent—and to more than a sufficient degree to justify grave concerns like those of Ocasio-Cortez, Haidt, and Obama.</span></p><p>In what follows, I focus on two lines of evidence in particular: research showing that social media amplifies the reach of emotionally extreme content, and research showing that higher social media penetration in a given geographic area induces more extreme political behavior, such as protests and hate crimes. I synthesize both into an overarching view I refer to as an ‘elite radicalization‘ theory of online politics.</p><p><em>3.1. The elite radicalization theory</em></p><p><span>It is well-established in the research literature that our attention on social media gravitates toward content that is more emotionally extreme. Recent studies have found that posts that are </span><a href="https://journals.sagepub.com/doi/10.1177/20563051211059710" rel="">sad,</a><span> </span><a href="https://www.pnas.org/doi/epub/10.1073/pnas.2212270120" rel="">fearful,</a><span> </span><a href="https://journals.sagepub.com/doi/10.1177/19485506221083811" rel="">uncivil,</a><span> </span><a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Fxge0001247" rel="">morally</a><span> </span><a href="https://academic.oup.com/joc/article-abstract/67/5/803/4642206?redirectedFrom=fulltext" rel="">and</a><span> </span><a href="https://www.pnas.org/doi/10.1073/pnas.1618923114" rel="">emotionally indignant,</a><span> </span><a href="https://www.pnas.org/doi/abs/10.1073/pnas.2024292118" rel="">hostile towards</a><span> </span><a href="https://arxiv.org/abs/2305.16941" rel="">out-groups,</a><span> and </span><a href="https://dl.acm.org/doi/abs/10.1145/2817946.2817962?casa_token=fUbAJsfmLAAAAAAA:dAz2UEM8FKjMyAoXcLIHDI6aWVK9DcH0FArFhs19m46dLM3q5cOffjs0I9U3m1OBFLh_T-coW-5V" rel="">negatively</a><span> </span><a href="https://www.degruyterbrill.com/document/doi/10.1515/commun-2019-0152/html" rel="">valenced</a><span> in general spread much further than their more neutral counterparts.</span></p><p><a href="https://www.pnas.org/doi/10.1073/pnas.1618923114" rel="">Brady et. al. (2017)</a><span>, for instance, found that each additional </span><em>word</em><span> with “moral-emotional content” (e.g. “duty,” “fear,” “shame,” “war”) increased political Twitter posts’ diffusion factor by 20%. </span><a href="https://psycnet.apa.org/record/2018-63985-001" rel="">Brady et. al. (2019)</a><span> later reproduced this finding using the tweets and retweets of over 500 presidential candidates and members of congress. Expressions of “moral anger and disgust,” the authors found, diffused particularly quickly.</span></p><p><span>That negative content spreads especially far online makes sense given some well-established properties of human psychology. Humans exhibit a broad-based psychological </span><a href="https://journals.sagepub.com/doi/abs/10.1037/1089-2680.5.4.323" rel="">negativity bias</a><span> as well a range of more specific </span><a href="https://www.sciencedirect.com/science/article/abs/pii/B9780128166604000027" rel="">attentional</a><span> </span><a href="https://www.nature.com/articles/s41598-020-68490-5" rel="">biases</a><span> </span><a href="https://pubmed.ncbi.nlm.nih.gov/11561921/" rel="">toward</a><span> negative stimuli. This is because, like other organisms, we have evolved to be uniquely attuned to signals suggestive of danger. As a result, when given the opportunity to cycle through a variegated soup of thousands of digital signals daily, we tend to fixate on and amplify the most distressing ones.</span></p><p>This dynamic creates very strong incentives to prey on our negative emotions. Successful ‘attentional entrepreneurs’ online not only enjoy attention’s more immediate benefits like status or influence, but are often paid by platforms in proportion to the ad revenue they generate. In that case, if certain kinds of content reliably accrue the most attention, there are very strong incentives online to produce that content en masse.</p><p><span>That is exactly what has happened in recent years with sensationalized and excessively negative political content. The last decade and a half has seen the rise of a new class of political influencers who, empowered by social media’s unique incentive environment, have come to exert near-symphonic control over the fear, anger, and tribalism of large sectors of the American public. The phrase “political influencer” calls to mind names like Tucker Carlson and Candace Owens, but I mean it to refer to any content creator, pundit, journalist, or even politician with an active online presence oriented around the production of political content—so perhaps hundreds of thousands of users with followings of varying sizes. Critically, this group is not a random selection from social media’s overall user base, but skews </span><a href="https://www.pewresearch.org/internet/2013/04/25/civic-engagement-in-the-digital-age-2/#:~:text=On%20the%20other%20hand%2C%20when,we%20measured%20in%20our%20survey." rel="">wealthier and more educated</a><span>, meaning its greater online influence is likely matched by greater </span><em><a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/polq.12218" rel="">off</a></em><a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/polq.12218" rel="">line influence</a><span> as well.</span></p><p><span>Despite accounting for only a small slice of the online population, this new influencer class is coming to dominate the market for political communication. In the process, it is transforming America’s perception of itself, which, since America is a social entity constituted in part </span><em>by</em><span> its self-perceptions, just amounts to saying it is transforming America.</span></p><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S2352250X24001313" rel="">Robertson, del Rosario, and Van Bavel (2024)</a><span> outline this group’s newfound influence in their recent paper “Inside the funhouse mirror factory: How social media distorts perceptions of norms.” It is worth quoting their analysis at length:</span></p><blockquote><p>Online discussions are dominated by a surprisingly small, extremely vocal, and non-representative minority. Research on social media has found that, while only 3% of active accounts are toxic, they produce 33% of all content. Furthermore, 74% of all online conflicts are started in just 1% of communities, and 0.1% of users shared 80% of fake news. Not only does this extreme minority stir discontent, spread misinformation, and spark outrage online, they also bias the meta-perceptions of most users who passively “lurk” online. (p. 1, my italics)</p><p>[With respect to] political discussions [in particular], the people who post frequently on social media are often the most ideologically extreme. Indeed, 97% of political posts from Twitter/X come from just 10% of the most active users on social media, meaning that about 90% of the population’s political opinions are being represented by less than 3% of tweets online. This is a marked difference from offline polling data showing that most people are ideologically moderate, uninterested in politics, and avoid political discussions when they are able. (p. 2)</p></blockquote><p><span>The result is that Americans are coming to see each other as much more politically extreme than they in fact are—or at least </span><em>were.</em><span> In the words of political scientist </span><a href="https://www.programmablemutter.com/p/were-getting-the-social-media-crisis" rel="">Henry Farrell</a><span>, Americans have internalized a </span><em>“malformed collective understanding,”</em><span> in this case a vision of their collective identity that casts them as especially angry, pessimistic, and tribal.</span></p><p><span>The idea that this understanding is “malformed”  is a tricky one, though. It invites the question: </span><em>how</em><span> malformed, and for how much longer? While in the past, survey evidence has indeed shown Americans to be moderate and relatively uninterested in politics, it is possible, and even probable, that social media is changing that. If in response to believing their peers are more politically extreme, people in turn </span><em>become </em><span>more extreme in reaction, then Farrell’s ‘malformed collective understanding’ may end up a self-fulfilling prophecy. Perhaps the more extreme content we ingest, the more likely it is we internalize this style of political communication and begin posting extreme content ourselves. There is already some limited evidence to support this possibility.</span></p><p><a href="https://academic.oup.com/joc/article-abstract/71/6/922/6363640?redirectedFrom=fulltext" rel="">Kim et. al. (2021)</a><span> found, for instance, that those exposed to toxic ‘featured’ comments on Facebook posts were more likely to post toxic comments later of their own volition. </span><a href="https://www.science.org/doi/10.1126/sciadv.abe5641" rel="">Brady et. al. (2021)</a><span> found that Twitter users “conform their outrage expressions to the expressive norms of their social networks,” (p. 1) and become more likely to express outrage when their surrounding social network is itself more extreme (p. 8). And in an as-yet unpublished preprint, </span><a href="https://osf.io/preprints/osf/mgdwq_v1" rel="">Brady et. al. (2025)</a><span> found that in a simulated Twitter-like environment, “engagement-based algorithms” changed participants’ perception of descriptive and prescriptive norms surrounding “ingroup-aligned, moral and emotional political content,” and ultimately made them more intent on posting this content themselves.</span></p><p>These results seem consistent with widespread anecdotal evidence of once-moderate friends and family falling down political or conspiratorial ‘rabbit-holes,’ often via involvement with insular online communities (e.g. QANON). At a broader level, it is hard to deny that the expressive norms governing our political culture have changed (witness the much-discussed expansion of the Overton window), and that this has made extreme political speech (open Nazism, racism, misogyny, antisemitism and so on) more common.</p><p>If this is all downstream of a small and relatively well-off group of high frequency posters (some of our elected officials among them), that would suggest what we might call an ‘elite radicalization’ theory of online politics. The idea is that social media has empowered a (relatively) small group of political influencers who, in response to the perverse incentives created by attentional negativity bias, have disseminated ideas that make people more angry, fearful, and extremist.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!VJ5k!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!VJ5k!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 424w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 848w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 1272w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!VJ5k!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png" width="725" height="409.65034965034965" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:404,&quot;width&quot;:715,&quot;resizeWidth&quot;:725,&quot;bytes&quot;:59572,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://arachnemag.substack.com/i/172046631?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!VJ5k!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 424w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 848w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 1272w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Source: Me</figcaption></figure></div><p>As represented above, those ideas spread both via influencing other elites (think podcast and talk show hosts like Joe Rogan or Jon Stewart), as well as by influencing typical consumers of online political content directly. Regardless of the exact transmission pathway, social media’s vast webs of interaction ensure that whatever effects online political content has, its reach is sure to be very far.</p><p><span>Nor does this content just ‘bounce off’ political junkies already firm in their preexisting views. There is strong causal evidence that political influencers have the power not just to reflect, but to shape the attitudes of their followers. In a recent preprint, </span><a href="https://osf.io/preprints/psyarxiv/acbwg_v2" rel="">Rathje et. al. (2025)</a><span> found in two different field experiments (</span><em>n = </em><span>494, </span><em>n</em><span> = 1,133) that unfollowing “hyperpartisan social media influencers" on Twitter (they give the examples of Palmer Report on the left and Breitbart News on the right) “improved [participants’] recent feelings toward the out-party by 23.5% compared to the control group, with effects persisting for at least six months,” (p. 2). This should put to rest the idea that inflammatory political content is a purely ‘demand-side‘ phenomenon. Yes, this demand has always existed, but it is possible for the attitudinal effects of existing supply to increase it even further.</span></p><p><span>Perhaps this is not on its own cause for serious alarm, though. However unpleasant online political influencers’ ideas might be, the question is whether they transcend the digital realm to influence people’s </span><em>real-world </em><span>political behavior. If not, then maybe we can relax in the confidence that the apparent rise in extremist attitudes is confined to an annoying, but harmless class of digital ‘LARPers.’</span></p><p>With apologies to those in the market for this kind of apparently savvy, relieving narrative, there is a convincing base of evidence showing just the opposite. By amplifying extreme content online, social media does appear to catalyze more extreme political behavior offline as well. The following are quasi-experimental studies that take advantage of exogenous variation in how different social networks have spread in order to study their political impacts. Their findings are relatively univocal:</p><blockquote><p><a href="https://www.nber.org/papers/w26567" rel="">Bursztyn et. al. (2019)</a><span> instrumented on variation in city-level penetration of VK (the dominant Russian social network) in Russia and found that greater social media use increased the prevalence of xenophobic attitudes, and lead to more hate crimes in cities with higher pre-existing levels of nationalism.</span></p><p><a href="https://link.springer.com/article/10.1007/s11205-018-1887-2" rel="">Sabatini and Sarracino (2019)</a><span> instrumented on variation in broadband/high-speed internet coverage in Italy, and found that greater social media use significantly decreased measures of social, particularized, and institutional trust.</span></p><p><a href="https://onlinelibrary.wiley.com/doi/full/10.3982/ECTA14281" rel="">Enikolopov, Makarin, and Petrova (2020)</a><span> also instrumented on variation in city-level penetration of VK in Russia and found that a 10% increase in penetration into a given city increased the probability of a protest there by 4.6% and the number of protesters by 19%.</span></p><p><a href="https://ejpr.onlinelibrary.wiley.com/doi/full/10.1111/1475-6765.12373" rel="">Schaub and Morisi (2020)</a><span> instrumented on variation in municipality-level broadband coverage in Germany and Italy and found that greater access to online communication platforms made Germans and Italians more likely to vote for the populist AfD and M5S parties, respectively.</span></p><p><a href="https://academic.oup.com/jeea/article-abstract/19/4/2131/5917396?redirectedFrom=fulltext" rel="">Müller and Schwarz (2021)</a><span> used a slightly different approach—instrumenting on variation in Facebook and internet </span><em>outages </em><span>in Germany, rather than access—and found that greater social media use predicted increased hate crimes against refugees.</span></p><p><a href="https://www.aeaweb.org/articles?id=10.1257/app.20210211" rel="">Müller and Schwarz (2023)</a><span> instrumented on variation in the number of Twitter users across counties induced by early adopters at the 2007 South by Southwest (SXSW) festival and found that higher Twitter use led to a sizable increase in anti-Muslim hate crimes during the 2016 Republican primary, likely driven by future president Donald Trump’s tweets. In particular, they found that “a one standard deviation-higher exposure to Twitter [was] associated with a 32 percent … increase in hate crimes.” (p. 272)</span></p></blockquote><p>These studies’ geographic range underscores that their concerning effects are robust to meaningful changes in context. And since they aggregate data across whole countries and municipalities, they’re able to account for the kinds of spillover effects that individualized experiments ignore. At least within the Western world broadly construed, the spread of social media has consistently produced more extreme forms of political behavior, including xenophobic violence in the most severe cases, but also protests, shifting voting patterns, and presumably much more that scholars have yet to study in detail.</p><p>While the evidence on this front is more limited in the case of the United States, the graph below does not inspire much optimism—note the hinge point around the late aughts when social media (and smartphone) use first became pervasive.</p><p>Social media is not, then, just a vast machine for disseminating extreme ideas—the most successful of which seem to have a right-wing populist flavor—but also one for shaping practical politics, including in its highest-stakes moments. As the quasi-experimental research reinforces, it does so in large part by empowering elite political influencers who, far from being just ‘terminally online’ pests, have the power to genuinely refigure their home countries’ political terrain.</p><p>While much of the preexisting research on social media and politics is focused on the political right, it is worth flagging that the elite radicalization theory also helps explain the strand of more radical left-wing politics that has gained prominence over the last decade and a half. While, as a factual matter, left-wing political influencers are far less dangerous (see the preceding graph), it is no less true that they have successfully exploited attentional negativity bias for both personal and political gain, shaping our current political culture in the process. As it happens, I am more sympathetic to their views, and so think some of their impact has been positive. But that is my own personal view, not an implication of the theory.</p><p><em>3.2. Elite radicalization and two-party politics</em></p><p><span>One of the core advantages of the elite radicalization theory is that it can account for social media’s incendiary effects on politics </span><em>without </em><span>committing to any particular relationship between social media and political polarization. While I hope the theory will be independently interesting to readers, it is for this reason that it can minimally act as a “so what?” objection to Williams.</span></p><p>Note that other common accounts of social media’s relationship with politics build in some commitment to polarization from the outset. For instance, ‘echo-chamber’ or ‘filter-bubble’-based theories argue that social media sorts people along partisan lines, in which case it would make sense for those inside a given bubble to become more polarized against those outside.</p><p>But the elite radicalization theory makes no such commitment. Of the elite influencers empowered by social media, many have views orthogonal to the classical ideological divide between Republicans and Democrats. While the content they create is often angry, fear-mongering, and prejudicial, it is only occasionally aligned with one of the U.S.’s two major political parties. More often, it has a populist flavor that targets some nebulous group of elites spanning party lines. And much of it also straddles the divide between political and apolitical, weaving discussion of, say, the economy or the Epstein files into broader conversations about pop culture.</p><p><span>As a result, and contra much presumptive emphasis on polarization, online political content may </span><em>decrease </em><span>the intensity of Democrat or Republican party identification, not in spite of, but </span><em>because</em><span> it makes people more outraged about politics. That outrage, for instance, may make independents less likely to gravitate toward the Republican or Democratic party. It might also make those who are already party members dislike their party more, perhaps enough to leave it altogether. This would be consistent with the </span><a href="https://www.pewresearch.org/politics/2022/08/09/as-partisan-hostility-grows-signs-of-frustration-with-the-two-party-system/" rel="">rising public distrust of both parties</a><span>, the frequent infighting that now defines Democratic party politics, and the simmering tensions between the more nativist and more business and tech-oriented factions of the Republican party.</span></p><p>If social media does in fact decrease the incidence or intensity of major party identification overall, then affective polarization—since it is defined in terms of major party allegiances—would be the wrong metric to look at in order to understand social media’s effects on politics. While this is only speculation for now, the graph below showing a rising share of Americans identifying as independents is at the very least suggestive (again note the hinge point just before 2010).</p><p><span>None of this is to say that social media definitely doesn’t increase polarization. But at minimum, this is by no means necessary for the elite radicalization theory to succeed. With respect to ‘affective’ polarization in particular, the theory suggests that social media’s harms probably have more to do</span><em> </em><span>with</span><em> the ‘affective’ part,</em><span> than the ‘polarization’ part</span><em>.</em><span> In other words, while social media makes political discourse more affectively or emotionally intense, it may do so without making avowed Democrats or Republicans dislike each other more than they already did. </span></p><p>If the effects of social media go even further to the point of convincing a significant number of Americans to redefine as independents, that would not only make affective polarization less relevant, but might even slow its increase. This could happen if those convinced to leave their party exhibited higher-than-average-negativity towards the out-party when they were still members. That may seen counterintuitive, since presumably the Republicans (Democrats) who hate Democrats (Republicans) the most would be among those most loyal to the party, but it may be that high out-party hate is a proxy for greater disagreeableness generally, in which case those who are fed up enough with the two-party system to quit it may also be among those who once exhibited the most out-party hatred.</p><p><span>There is not yet sufficient evidence to confirm or deny any of these suggestions. But in general, academics and public commentators alike should be mindful that in addition to spreading political extremism, social media may also be shifting the underlying axis </span><em>on which those extremes exist. </em><span>This could have serious consequences for social-scientific measurements of political attitudes, not least that of affective polarization.</span></p><p><em>5. Conclusion</em></p><p>By incentivizing the creation of disproportionately negative and sensational content, and by in turn inducing more extreme, even violent political behavior, social media has almost certainly played a major role in the destabilizing political ructions of the last fifteen years—in particular in the U.S., but probably across Europe as well. </p><p>Elite political influencers, both those hailing originally from politics, television, and journalism, as well as those native to digital media platforms, have played a critical mediating role. It is they who generate much of the most provocative content, as well as who influence the outlooks and incentives of other elites, allowing their influence to spread well beyond the confines of the major social networking platforms.</p><p>While Williams is correct that we have not seen a large spike in affective polarization as a result, we have seen spikes in angry, fearful, and identitarian political speech. We have also seen large spikes in political violence. And though I will not pretend to have made the case for their direct causal connection to social media, we have also seen the rise of a more radical strain of progressive politics, as well as the once-fledgling, now all-powerful MAGA movement figureheaded by Donald Trump, both of whose successes seem hard to separate from the belligerently online politics of the 2010s. All of this has come to pass even as polarization has increased at an overall smooth rate.</p><p><span>As the digital media revolution continues to transform social life in the U.S. along almost every conceivable dimension, it is critical that we have a clear-eyed view of its effects. Williams is right to challenge certain aspects of the panic surrounding social media, such as the often </span><a href="https://www.conspicuouscognition.com/p/misinformation-researchers-are-wrong" rel="">confused</a><span> </span><a href="https://www.conspicuouscognition.com/p/the-media-very-rarely-makes-things" rel="">and</a><span> </span><a href="https://www.conspicuouscognition.com/p/what-is-misinformation-anyway" rel="">imprecise</a><span> discourse surrounding “misinformation.” But when it comes to the technology’s effects on politics writ large, he has not taken a sufficiently holistic look at the evidence.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RIP pthread_cancel (178 pts)]]></title>
            <link>https://eissing.org/icing/posts/rip_pthread_cancel/</link>
            <guid>45233713</guid>
            <pubDate>Sat, 13 Sep 2025 17:20:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eissing.org/icing/posts/rip_pthread_cancel/">https://eissing.org/icing/posts/rip_pthread_cancel/</a>, See on <a href="https://news.ycombinator.com/item?id=45233713">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I posted about adding <a href="https://eissing.org/icing/posts/pthread_cancel">pthread_cancel use in curl</a>
about three weeks ago, we released this in curl 8.16.0 and it blew
up right in our faces. Now, with
<a href="https://github.com/curl/curl/pull/18540">#18540</a> we are ripping it
out again. What happened?</p>
<h2 id="short-recap">short recap</h2>
<p><a href="https://www.man7.org/linux/man-pages/man7/pthreads.7.html">pthreads</a>
define “Cancelation points”, a list of POSIX functions where
a pthread may be cancelled. In addition, there is also a list of functions
that <em>may</em> be cancelation points, among those <code>getaddrinfo()</code>.</p>
<p><code>getaddrinfo()</code> is exactly what we are interested in for <code>libcurl</code>. It blocks
until it has resolved a name. That may hang for a long time and <code>libcurl</code>
is unable to do anything else. Meh. So, we start a pthread and let that
call <code>getaddrinfo()</code>. <code>libcurl</code> can do other things while that thread runs.</p>
<p>But eventually, we have to get rid of the pthread again. Which means we
either have to <code>pthread_join()</code> it - which means a blocking wait. Or we
call <code>pthread_detach()</code> - which returns immediately but the thread keeps
on running. Both are bad when you want to do many, many transfers. Either we block and
stall or we let pthreads pile up in an uncontrolled way.</p>
<p>So, we added <code>pthread_cancel()</code> to interrupt a running <code>getaddrinfo()</code>
and get rid of the pthread we no longer needed. So the theory. And, after
some hair pulling, we got this working.</p>
<h2 id="cancel-yes-leakage-also-yes">cancel yes, leakage also yes!</h2>
<p>After releasing curl 8.16.0 we got an issue reported in
<a href="https://github.com/curl/curl/issues/18532">#18532</a> that cancelled
pthreads leaked memory.</p>
<p><img src="https://eissing.org/icing/posts/rip_pthread_cancel/images/modern-times-sigh.png" alt="modern times sigh"></p>
<p>Digging into the <a href="https://codebrowser.dev/glibc/glibc/nss/getaddrinfo.c.html#gaiconf_init">glibc source</a>
shows that there is this thing called
<a href="https://www.man7.org/linux/man-pages/man5/gai.conf.5.html"><code>/etc/gai.conf</code></a>
which defines how <code>getaddrinfo()</code> should sort returned answers.</p>
<p>The implementation in glibc first resolves the name to addresses. For these,
it needs to allocate memory. <em>Then</em> it needs to sort them if there is more
than one address. And in order
to do <em>that</em> it needs to read <code>/etc/gai.conf</code>. And in order to do <em>that</em>
it calls <code>fopen()</code> on the file. And that may be a pthread “Cancelation Point”
(and if not, it surely calls <code>open()</code> which is a required cancelation point).</p>
<p>So, the pthread may get cancelled when reading <code>/etc/gai.conf</code> and leak all
the allocated responses. And if it gets cancelled there, it will try to
read <code>/etc/gai.conf</code> <em>again</em> the next time it has more than one address
resolved.</p>
<p>At this point, I decided that we need to give up on the whole <code>pthread_cancel()</code>
strategy. The reading of <code>/etc/gai.conf</code> is one point where a cancelled
<code>getaddrinfo()</code> may leak. There might be others. Clearly, glibc is not really
designed to prevent leaks here (admittedly, this is not trivial).</p>
<h2 id="rip">RIP</h2>
<p>Leaking memory potentially on something <code>libcurl</code> does over and over again is
not acceptable. We’d rather pay the price of having to eventually wait on
a long running <code>getaddrinfo()</code>.</p>
<p>Applications using <code>libcurl</code> can avoid this by using <code>c-ares</code> which resolves
unblocking and without the use of threads. But that will not be able to do
everything that glibc does.</p>
<p>DNS continues to be tricky to use well.</p>
<ul>
  
</ul>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Magical systems thinking (267 pts)]]></title>
            <link>https://worksinprogress.co/issue/magical-systems-thinking/</link>
            <guid>45233266</guid>
            <pubDate>Sat, 13 Sep 2025 16:18:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://worksinprogress.co/issue/magical-systems-thinking/">https://worksinprogress.co/issue/magical-systems-thinking/</a>, See on <a href="https://news.ycombinator.com/item?id=45233266">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div role="presentation"><p>The systems that enable modern life share a common origin. The water supply, the internet, the international supply chains bringing us cheap goods: each began life as a simple, working system. The first electric grid was no more than a handful of electric lamps hooked up to a water wheel in Godalming, England, in 1881. It then took successive <a href="https://www.worksinprogress.news/p/how-the-worlds-first-electric-grid">decades of tinkering and iteration</a> by thousands of very smart people to scale these systems to the advanced state we enjoy today. At no point did a single genius map out the final, finished product.</p>



<p>But this lineage of (mostly) working systems is easily forgotten. Instead, we prefer a more flattering story: that complex systems are deliberate creations, the product of careful analysis. And, relatedly, that by performing this analysis – now known as ‘systems thinking’ in the halls of government – we can bring unruly ones to heel. It is an optimistic perspective, casting us as the masters of our systems and our destiny.</p>



<p>The empirical record says otherwise, however. Our recent history is one of governments grappling with complex systems and coming off worse. In the United States, HealthCare.gov was designed to simplify access to health insurance by knitting together 36 state marketplaces and data from eight federal agencies. Its launch was paralyzed by <a href="https://www.gao.gov/assets/gao-14-694.pdf">technical failures</a> that locked out millions of users. Australia’s disability reforms, carefully planned for over a decade and expected to save money, led to costs escalating so rapidly that they will <a href="https://www.afr.com/policy/economy/ndis-to-cost-100b-exceeding-the-pension-budget-watchdog-20240614-p5jltf">soon exceed the pension budget</a>.&nbsp;The UK’s <a href="https://www.worksinprogress.news/p/the-breaking-of-britains-national">2014 introduction of Contracts for Difference</a>, intended to speed the renewables rollout by giving generators a guaranteed price, overstrained the grid and is a major contributor to the 15-year queue for new connections. Systems thinking is more popular than ever; modern systems thinkers have analytical tools that their predecessors could only have dreamt of. But the systems keep kicking back.</p>



<p>There is a better way. A long but neglected line of thinkers going back to chemists in the nineteenth century has argued that complex systems are not our passive playthings. Despite friendly names like ‘the health system’, they demand extreme wariness. If broken, a complex system often cannot be fixed. Meanwhile, our successes, when they do come, are invariably the result of starting small. As the systems we have built slip further beyond our collective control, it is these simple working systems that offer us the best path back.&nbsp;</p>



<h3>The world model</h3>



<p>In 1970, the ‘Club of Rome’, a <a href="https://www.clubofrome.org/history/">group</a> of international luminaries with an interest in how the problems of the world were interrelated, invited <a href="https://sloanreview.mit.edu/article/jay-forrester-shock-to-the-system/">Jay Wright Forrester</a> to peer into the future of the global economy. An MIT expert on electrical and mechanical engineering, Forrester had cut his teeth on problems like how to keep a Second World War aircraft carrier’s radar pointed steadily at the horizon amid the heavy swell of the Pacific.&nbsp;</p>



<p>The Club of Rome asked an even more intricate question: how would social and economic forces interact in the coming decades? Where were the bottlenecks and feedback mechanisms? Could economic growth continue, or would the world enter a new phase of equilibrium or decline?&nbsp;</p>



<p>Forrester labored hard, producing a mathematical model of enormous sophistication. Across 130 pages of mathematical equations, computer graphical printout, and DYNAMO code,<em>World Dynamics</em> tracks the myriad relationships between natural resources, capital, population, food, and pollution: everything from the ‘capital-investment-in-agriculture-fraction adjustment time’ to the ominous ‘death-rate-from-pollution multiplier’.</p>


<div>
<figure><img loading="lazy" width="850" height="497" src="https://wip.gatspress.com/wp-content/uploads/2025/09/image-12.png" alt="" srcset="https://wip.gatspress.com/wp-content/uploads/2025/09/image-12.png 850w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-300x175.png 300w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-768x449.png 768w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-402x235.png 402w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-462x270.png 462w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-662x387.png 662w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-722x422.png 722w" sizes="(max-width: 850px) 100vw, 850px">
          <figcaption>
            <div>
              <p>
                A section of Forrester’s World Model.
              </p>
              <div>
                <p>Image</p>
                <p>
                  WAguirre 2017
                </p>
              </div>
            </div>
          </figcaption>
        </figure></div>


<p>World leaders had assumed that economic growth was an unalloyed good. But Forrester’s results showed the opposite. As financial and population growth continued, natural resources would be consumed at an accelerating rate, agricultural land would be paved over, and pollution would reach unmanageable levels. His model laid out dozens of scenarios and in most of them, by 2025, the world would already be in the first throes of an irreversible decline in living standards. By 2070, the crunch would be so painful that industrialized nations might regret their experiment with economic growth altogether. As Forrester <a href="https://monoskop.org/images/d/dc/Forrester_Jay_W_World_Dynamics_2nd_ed_1973.pdf#page=15">put it</a>, ‘[t]he present underdeveloped countries may be in a better condition for surviving forthcoming worldwide environmental and economic pressures than are the advanced countries.’</p>



<p>But, as we now know, the results were also wrong. Adjusting for inflation, world GDP is now about five times higher than it was in 1970 and continues to rise. More than 90 percent of that growth has come from Asia, Europe, and North America, but <a href="https://www.worksinprogress.news/p/growing-forests">forest cover</a> across those regions has <a href="https://openknowledge.fao.org/items/d6f0df61-cb5d-4030-8814-0e466176d9a1">increased</a>, up 2.6 percent since 1990 to over 2.3 billion hectares in 2020. The death rate from air pollution has almost halved in the same period, from <a href="https://ourworldindata.org/air-pollution">185 per 100,000 in 1990 to 100 in 2021</a>. According to the model, none of this should have been possible.&nbsp;</p>



<p>What happened? The blame cannot lie with Forrester’s competence: it’s hard to imagine a better systems pedigree than his. To read his prose today is to recognize a brilliant, thoughtful mind. Moreover, the system dynamics approach Forrester pioneered had already shown promise beyond the mechanical and electrical systems that were its original inspiration.&nbsp;</p>



<p>In 1956, the management of a General Electric refrigerator factory in Kentucky had called on Forrester’s help. They were struggling with a boom-and-bust cycle: acute shortages became gluts that left warehouses overflowing with unsold fridges. The factory based its production decisions on orders from the warehouse, which in turn got orders from distributors, who heard from retailers, who dealt with customers. Each step introduced noise and delay. Ripples in demand would be amplified into huge swings in production further up the supply chain.&nbsp;</p>



<p>Looking at the system as a whole, Forrester recognized the same feedback loops and instability that could bedevil a ship’s radar. He developed new decision rules, such as smoothing production based on longer-term sales data rather than immediate orders, and found ways to speed up the flow of information between retailers, distributors, and the factory. These changes dampened the oscillations caused by the system’s own structure, checking its worst excesses.&nbsp;</p>



<p>The Kentucky factory story showed Forrester’s skill as a systems analyst. Back at MIT, Forrester immortalized his lessons as a learning exercise (albeit with beer instead of refrigerators). In the ‘Beer Game’, now a rite of passage for students at the MIT Sloan School of Management, players take one of four different roles in the beer supply chain: retailer, wholesaler, distributor, and brewer. Each player sits at a separate table and can communicate only through order forms. As their inventory runs low, they place orders with the supplier next upstream. Orders take time to process, and shipments to arrive, and each player can see only their small part of the chain.</p>



<p>The objective of the Beer Game is to minimize costs by managing inventory effectively. But, as the GE factory managers had originally found, this is not so easy. Gluts and shortages arise mysteriously, without obvious logic, and small perturbations in demand get amplified up the chain by as much as 800 percent (‘the bullwhip effect’). On average, players’ total costs end up being <a href="https://dspace.mit.edu/bitstream/handle/1721.1/2184/SWP-1933-18213539.pdf#page=17">ten times higher than the optimal solution</a>.&nbsp;</p>



<p>With the failure of his World Model, Forrester had fallen into the same trap as his MIT students. Systems analysis works best under specific conditions: when the system is static; when you can dismantle and examine it closely; when it involves few moving parts rather than many; and when you can iterate fixes through multiple attempts. A faulty ship’s radar or a simple electronic circuit are ideal. Even a limited human element – with people’s capacity to pursue their own plans, resist change, form political blocs, and generally frustrate best-laid plans – makes things much harder. The four-part refrigerator supply chain, with the factory, warehouse, distributor and retailer all under the tight control of management, is about the upper limit of what can be understood. Beyond that, in the realm of societies, governments and economies, systems thinking becomes a liability, more likely to breed false confidence than real understanding. For these systems we need a different approach.</p>



<h3>Le Chatelier’s Principle</h3>



<p>In 1884, in a laboratory at the École des Mines in Paris, Henri Louis Le Chatelier noticed something peculiar: chemical reactions seemed to resist changes imposed upon them. Le Chatelier found that if, say, you have an experiment where two molecules combine in a heat-generating exothermic reaction (in his case, it was two reddish-brown nitrogen dioxide molecules combining into colorless dinitrogen tetroxide and giving off heat in the process), then you can speed things up by cooling the reactants. To ‘resist’ the drop in temperature, the system restores its equilibrium by creating more of the products that release heat.&nbsp;</p>



<p>Le Chatelier’s Principle, the idea that the system always kicks back, proved to be a very general and powerful way to think about chemistry. It was instrumental in the discovery of the Haber-Bosch process for creating ammonia that revolutionized agriculture. Nobel Laureate Linus Pauling <a href="https://catholicscientists.org/scientists-of-the-past/henry-louis-le-chatelier/">hoped</a> that, even after his students had ‘forgotten all the mathematical equations relating to chemical equilibrium’, Le Chatelier’s Principle would be the one thing they remembered. And its usefulness went beyond chemistry. A century after Le Chatelier’s meticulous lab work, another student of systems would apply the principle to the complex human systems that had stymied Forrester and his subsequent followers in government.</p>



<p>John Gall was a pediatrician with a long-standing practice in Ann Arbor, Michigan. Of the same generation as Forrester, Gall came at things from a different direction. Whereas Forrester’s background was in mechanical and electrical systems, which worked well and solved new problems, Gall was immersed in the human systems of health, education, and government. These systems often did not work well. How was it, Gall wondered, that they seemed to coexist happily with the problems – crime, poverty, ill health – they were supposed to stamp out?&nbsp;</p>



<p>Le Chatelier’s Principle provided an answer: systems should not be thought of as benign entities that will faithfully carry out their creators’ intentions. Rather, over time, they come to oppose their own proper functioning. Gall elaborated on this idea in his 1975 book <em>Systemantics</em>, named for the universal tendency of systems to display antics. A brief, weird, funny book, <em>Systemantics</em> (<em>The Systems Bible</em> in later editions) is arguably the best field guide to contemporary systems dysfunction. It consists of a series of pithy aphorisms, which the reader is invited to apply to explain the system failures&nbsp;(‘horrible examples’)&nbsp;they witness every day.</p>



<p>These aphorisms are provocatively stated, but they have considerable explanatory power. For example, an Australian politician frustrated at the <a href="https://www.afr.com/politics/federal/fraud-signs-in-90pc-of-ndis-managers-crime-gangs-push-drugs-20240603-p5jizn">new headaches</a> created by ‘fixes’ to the old disability system might be reminded that ‘NEW SYSTEMS CREATE NEW PROBLEMS’. An American confused at how there can now be <a href="https://regulatorystudies.columbian.gwu.edu/reg-stats">190,000</a> pages in the US Code of Federal Regulations, up from 10,000 in 1950, might note that this is the nature of the beast: ‘SYSTEMS TEND TO GROW, AND AS THEY GROW THEY ENCROACH’. During the French Revolution, in 1793 and 1794, the ‘Committee of Public Safety’ guillotined thousands of people, an early example of the enduring principles that ‘THE SYSTEM DOES NOT DO WHAT IT SAYS IT IS DOING’ and that ‘THE NAME IS EMPHATICALLY NOT THE THING’. And, just like student chemists, government reformers everywhere would do well to remember Le Chatelier’s Principle: ‘THE SYSTEM ALWAYS KICKS BACK’.</p>



    <figure>
        
    </figure>




<p>These principles encourage a healthy paranoia when it comes to complex systems. But Gall’s ‘systems-display-antics’ philosophy is not a counsel of doom. His greatest insight was a positive one, explaining how some systems do succeed in spite of the pitfalls. Known as ‘Gall’s law’, it’s worth quoting in full:</p>



<blockquote><p>A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.</p></blockquote>



<p>Starting with a working simple system and evolving from there is how we went from the water wheel in Godalming to the modern electric grid. It is how we went from a hunk of <a href="https://spectrum.ieee.org/transistor-history">germanium, gold foil, and hand-soldered wires</a> in 1947 to transistors being etched onto silicon wafers in their trillions today.</p>



<p>This is a dynamic we can experience on a personal as well as a historical level. A trivial but revealing example is the computer game Factorio. Released in 2012 and famously hazardous to the productivity of software engineers everywhere, Factorio invites players to construct a factory. The ultimate goal is to launch a rocket, a feat that requires the player to produce thousands of intermediate products through dozens of complicated, interlocking manufacturing processes.&nbsp;</p>



<p>It sounds like a nightmare. An early flow chart (pictured –&nbsp;it has grown much more complicated since) resembles the end product of a particularly thorny systems thinking project. But players complete its daunting mission successfully, without reference to such system maps, in their thousands, and all for fun.</p>


<div>
<figure><img loading="lazy" width="900" height="671" src="https://wip.gatspress.com/wp-content/uploads/2025/09/image-13.png" alt="" srcset="https://wip.gatspress.com/wp-content/uploads/2025/09/image-13.png 900w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-300x224.png 300w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-768x573.png 768w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-402x300.png 402w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-462x344.png 462w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-662x494.png 662w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-722x538.png 722w" sizes="(max-width: 900px) 100vw, 900px">
          <figcaption>
            <div>
              <p>
                Factorio production map.
              </p>
              <div>
                <p>Image</p>
                
              </div>
            </div>
          </figcaption>
        </figure></div>


<p>The genius of the game is that it lets players begin with a simple system that works. As you learn to produce one item, another is unlocked. If you get something wrong, the factory visibly grinds to a halt while you figure out a different approach. The hours tick by, and new systems – automated mining, oil refining, locomotives – are introduced and iterated upon. Before you realize it, you have built a sprawling yet functioning system that might be more sophisticated than anything you have worked on in your entire professional career.</p>



<h3>How to build systems that work</h3>



<p>Government systems, however, are already established, complicated, and relied upon by millions of people every day. We cannot simply switch off the health system and ask everyone to wait a few years while we build something better. The good news is that the existence of an old, clunky system does not stop us from starting something new and simple in parallel.</p>



<p>In the 1950s, the US was in a desperate race against a technologically resurgent Soviet Union. The USSR took the lead in developing advanced rockets of the type that launched Sputnik into orbit and risked launching a nuclear device into Washington, DC. In 1954, the Eisenhower administration tasked General Bernard Schriever with helping the US develop its own Intercontinental Ballistic Missile (ICBM). An experienced airman and administrator, the top brass felt that Schriever’s Stanford engineering master’s degree would make him a suitable go-between for the soldiers and scientists on this incredibly technical project (its <a href="https://thebhc.org/sites/default/files/beh/BEHprint/v022n1/p0194-p0209.pdf">scope</a> was larger even than the Manhattan Project, costing over $100 billion in 2025 dollars versus the latter’s $39 billion).&nbsp;</p>



<p>The organizational setup Schriever inherited was not fit for the task. With many layers of approvals and subcommittees within subcommittees, it was a classic example of a complex yet dysfunctional system. The technological challenges posed by the ICBM were extreme: everything from rocket engines to targeting systems to the integration with nuclear warheads had to be figured out more or less from scratch. This left no room for bureaucratic delay.&nbsp;</p>



<p>Schriever produced what many systems thinkers would recognize as a kind of systems map: a series of massive boards setting out all the different committees and governance structures and approvals and red tape. But the point of these <a href="https://media.defense.gov/2010/Sep/29/2001329778/-1/-1/0/AFD-100929-007.pdf#page=19">‘spaghetti charts’</a> was not to make a targeted, systems thinking intervention. Schriever didn’t pretend to be able to navigate and manipulate all this complexity. He instead recognized his own limits. With the Cold War in the balance, he could not afford to play and lose his equivalent of the Beer Game. Charts in hand, Schriever persuaded his boss that untangling the spaghetti was a losing battle: they needed to start over.</p>



<p>They could not change the wider laws, regulations, and institutional landscape governing national defense. But they could work around them, starting afresh with a simple system outside the existing bureaucracy. Direct vertical accountability all the way to the President and a free hand on personnel enabled the program to flourish. Over the following years, four immensely ambitious systems were built in record time. The uneasy strategic stalemate that passed for stability during the Cold War was restored, and the weapons were never used in anger.</p>



<p>When we look in more detail at recent public policy successes, we see that this pattern tends to hold. Operation Warp Speed in the US played a big role in getting vaccines delivered quickly. It did so by <a href="https://issues.org/rules-operation-warp-speed-arnold">bypassing many of the usual bottlenecks</a>. For instance, it made heavy use of ‘Other Transaction Authority agreements’ to commit $12.5 billion of federal money by March 2021, circumventing the thousands of pages of standard procurement rules. Emergency powers were deployed to accelerate the FDA review process, enabling clinical trial work and early manufacturing scale-up to happen in parallel. These actions were funded through an $18 billion commitment made largely outside the typical congressional appropriation oversight channels – enough money to back not just one vaccine candidate but <a href="https://www.gao.gov/products/gao-21-319#:~:text=As%20of%20January%2030%2C%202021,and%20Drug%20Administration%20(FDA).">six, across three different technology platforms.</a></p>



<p>In France, the rapid reconstruction of Notre-Dame after the April 2019 fire has become a symbol of French national pride and its ability to get things done despite a reputation for moribund bureaucracy. This was achieved not through wholesale reform of that bureaucracy but by quickly setting up a fresh structure outside of it. In July 2019, the French Parliament passed Loi n°&nbsp;2019-803, creating an extraordinary legal framework for the project. Construction permits and zoning changes were fast-tracked. President Macron personally appointed the veteran General Jean-Louis Georgelin to run the restoration, exempting him from the mandatory retirement age for public executives in order to do so.</p>



<p>The long-term promise of a small working system is that over time it can supplant the old, broken one and produce results on a larger scale. This creative destruction has long been celebrated in the private sector, where aging corporate giants can be disrupted by smaller, simpler startups: we don’t have to rely on IBM to make our phones or laptops or Large Language Models. But it can work in the public sector too. Estonia, for example, introduced electronic ID in the early 2000s for signing documents and filing online tax returns. These simple applications, which nonetheless took enormous focus to implement, were popular, and ‘digital government’ was <a href="https://e-estonia.com/story/">gradually expanded</a> to new areas: voting in 2005, police in 2007, prescriptions in 2010, residency in 2014, and even e-divorce in 2024. By 2025, 99 percent of residents will have an electronic ID card, digital signatures are <a href="https://e-estonia.com/wp-content/uploads/eestonia_guide_08-04-2025.pdf">estimated</a> to save two percent of GDP per year, and every state service runs online.&nbsp;</p>



<p>In desperate situations, such as a Cold War arms race or COVID-19, we avoid complex systems and find simpler workarounds. But, outside of severe crises, much time is wasted on what amounts to magical systems thinking. Government administrations around the world, whose members would happily admit their incompetence to fix a broken radio system, publish manifestos, strategies, plans, and priorities premised on disentangling systems problems that are orders of magnitude more challenging. With each ‘fix’, oversight bodies, administrative apparatus, and overlapping statutory obligations accumulate. Complexity is continuing to rise, outcomes are becoming worse, and voters’ goodwill is being eroded.</p>



<p>We will soon be in an era where humans are not the sole authors of complex systems. Sundar Pichai estimated in late 2024 that over 25 percent of Google’s code was AI generated; as of mid-2025, the figure for Anthropic is <a href="https://x.com/slow_developer/status/1921684238753304887">80–90 percent</a>.&nbsp;As in the years after the Second World War, the temptation will be to use this vast increase in computational power and intelligence to ‘solve’ systems design for once and for all. But the same laws that limited Forrester continue to bind: ‘NEW SYSTEMS CREATE NEW PROBLEMS’ and ‘THE SYSTEM ALWAYS KICKS BACK’. As systems become more complex, they become more chaotic, not less. The best solution remains humility, and a simple system that works.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“Learning how to Learn” will be next generation's most needed skill (119 pts)]]></title>
            <link>https://techxplore.com/news/2025-09-google-ai-scientist-generation-skill.html</link>
            <guid>45232720</guid>
            <pubDate>Sat, 13 Sep 2025 15:10:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techxplore.com/news/2025-09-google-ai-scientist-generation-skill.html">https://techxplore.com/news/2025-09-google-ai-scientist-generation-skill.html</a>, See on <a href="https://news.ycombinator.com/item?id=45232720">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/googles-top-ai-scienti.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/googles-top-ai-scienti.jpg" data-sub-html="Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, right, and Greece's Prime Minister Kyriakos Mitsotakis discuss the future of AI, ethics and democracy during an event at the Odeon of Herodes Atticus, in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/googles-top-ai-scienti.jpg" alt="Google's top AI scientist says ‘learning how to learn’ will be next generation's most needed skill" title="Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, right, and Greece's Prime Minister Kyriakos Mitsotakis discuss the future of AI, ethics and democracy during an event at the Odeon of Herodes Atticus, in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis" width="800" height="530">
             <figcaption>
                Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, right, and Greece's Prime Minister Kyriakos Mitsotakis discuss the future of AI, ethics and democracy during an event at the Odeon of Herodes Atticus, in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis
            </figcaption>        </figure>
    </div><p>A top Google scientist and 2024 Nobel laureate said Friday that the most important skill for the next generation will be "learning how to learn" to keep pace with change as Artificial Intelligence transforms education and the workplace.</p>

                                        
                                              
                                        
                                                                                                                                    <p>Speaking at an ancient Roman theater at the foot of the Acropolis in Athens, Demis Hassabis, CEO of Google's DeepMind, said rapid technological change demands a new approach to learning and <a href="https://techxplore.com/tags/skill+development/" rel="tag">skill development</a>.</p>
<p>"It's very hard to predict the future, like 10 years from now, in normal cases. It's even harder today, given how fast AI is changing, even week by week," Hassabis told the audience. "The only thing you can say for certain is that huge change is coming."</p>
<p>The neuroscientist and former chess prodigy said <a href="https://techxplore.com/tags/artificial+general+intelligence/" rel="tag">artificial general intelligence</a>—a futuristic vision of machines that are as broadly smart as humans or at least can do many things as well as people can—could arrive within a decade. This, he said, will bring dramatic advances and a possible future of "radical abundance" despite acknowledged risks.</p>
<p>Hassabis emphasized the need for "meta-skills," such as understanding how to learn and optimizing one's approach to new subjects, alongside traditional disciplines like math, science and humanities.</p>
<p>"One thing we'll know for sure is you're going to have to continually learn ... throughout your career," he said.</p>

<ul>
            <li data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/googles-top-ai-scienti-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/googles-top-ai-scienti-1.jpg" data-sub-html="Greece's Prime Minister Kyriakos Mitsotakis, center, and Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, right, discuss the future of AI, ethics and democracy as the moderator Linda Rottenberg, Co-founder &amp; CEO of Endeavor looks on during an event at the Odeon of Herodes Atticus in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis">
            <figure>
                <img src="https://scx1.b-cdn.net/csz/news/800/2025/googles-top-ai-scienti-1.jpg" alt="Google's top AI scientist says ‘learning how to learn’ will be next generation's most needed skill">
                 <figcaption>
                    Greece's Prime Minister Kyriakos Mitsotakis, center, and Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, right, discuss the future of AI, ethics and democracy as the moderator Linda Rottenberg, Co-founder &amp; CEO of Endeavor looks on during an event at the Odeon of Herodes Atticus in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis
                </figcaption>            </figure>
        </li>
            <li data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/googles-top-ai-scienti-2.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/googles-top-ai-scienti-2.jpg" data-sub-html="Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, bottom right, and Greece's Prime Minister Kyriakos Mitsotakis, bottom center, discuss the future of AI, ethics and democracy during an event at the Odeon of Herodes Atticus, under Acropolis ancient hill, in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis">
            <figure>
                <img src="https://scx1.b-cdn.net/csz/news/800/2025/googles-top-ai-scienti-2.jpg" alt="Google's top AI scientist says ‘learning how to learn’ will be next generation's most needed skill">
                 <figcaption>
                    Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, bottom right, and Greece's Prime Minister Kyriakos Mitsotakis, bottom center, discuss the future of AI, ethics and democracy during an event at the Odeon of Herodes Atticus, under Acropolis ancient hill, in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis
                </figcaption>            </figure>
        </li>
    </ul>
<p>The DeepMind co-founder, who established the London-based research lab in 2010 before Google acquired it four years later, shared the 2024 Nobel Prize in chemistry for developing AI systems that accurately predict protein folding—a breakthrough for medicine and drug discovery.</p>
<p>Greek Prime Minister Kyriakos Mitsotakis joined Hassabis at the Athens event after discussing ways to expand AI use in government services. Mitsotakis warned that the continued growth of huge tech companies could create great global financial inequality.</p>
<p>"Unless people actually see benefits, personal benefits, to this (AI) revolution, they will tend to become very skeptical," he said. "And if they see ... obscene wealth being created within very few companies, this is a recipe for significant social unrest."</p>
<p>Mitsotakis thanked Hassabis, whose father is Greek Cypriot, for rescheduling the presentation to avoid conflicting with the European basketball championship semifinal between Greece and Turkey. Greece later lost the game 94-68.</p>

                                                                                                                                    
                                                                                
                                        											
										                                                                                    <p>
                                                © 2025 The Associated Press. All rights reserved. This material may not be published, broadcast, rewritten or redistributed without permission.
                                            </p>
                                                                                
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                Google's top AI scientist says 'learning how to learn' will be next generation's most needed skill (2025, September 13)
                                                retrieved 13 September 2025
                                                from https://techxplore.com/news/2025-09-google-ai-scientist-generation-skill.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[486Tang – 486 on a credit-card-sized FPGA board (165 pts)]]></title>
            <link>https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/</link>
            <guid>45232565</guid>
            <pubDate>Sat, 13 Sep 2025 14:52:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/">https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/</a>, See on <a href="https://news.ycombinator.com/item?id=45232565">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main role="main"><article><div><p>Yesterday I released <a href="https://github.com/nand2mario/486tang">486Tang</a> v0.1 on GitHub. It’s a port of the ao486 MiSTer PC core to the Sipeed Tang Console 138K FPGA. I’ve been trying to get an x86 core running on the Tang for a while. As far as I know, this is the first time ao486 has been ported to a non-Altera FPGA. Here’s a short write‑up of the project.</p><h2 id="486tang-architecture">486Tang Architecture</h2><p>Every FPGA board is a little different. Porting a core means moving pieces around and rewiring things to fit. Here are the major components in 486Tang:</p><p><img src="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/486tang.drawio.png" alt="" width="800"></p><p>Compared to ao486 on MiSTer, there are a few major differences:</p><ol><li><p><strong>Switching to SDRAM for main memory.</strong> The MiSTer core uses DDR3 as main memory. Obviously, at the time of the 80486, DDR didn’t exist, so SDRAM is a natural fit. I also wanted to dedicate DDR3 to the framebuffer; time‑multiplexing it would have been complicated. So SDRAM became the main memory and DDR3 the framebuffer. The SDRAM on Tang is 16‑bit wide while ao486 expects 32‑bit accesses, which would normally mean one 32‑bit word every two cycles. I mitigated this by running the SDRAM logic at 2× the system clock so a 32‑bit word can be read or written every CPU cycle (“double‑pumping” the memory).</p></li><li><p><strong>SD‑backed IDE.</strong> On MiSTer, the core forwards IDE requests to the ARM HPS over a fast HPS‑FPGA link; the HPS then accesses a VHD image. Tang doesn’t have a comparable high‑speed MCU‑to‑FPGA interface—only a feeble UART—so I moved disk storage into the SD card and let the FPGA access it directly.</p></li><li><p><strong>Boot‑loading module.</strong> A PC needs several things to boot: BIOS, VGA BIOS, CMOS settings, and IDE IDENTIFY data (512 bytes). Since I didn’t rely on an MCU for disk data, I stored all of these in the first 128 KB of the SD card. A small boot loader module reads them into main memory and IDE, and then releases the CPU when everything is ready.</p></li></ol><h2 id="system-bring-up-with-the-help-of-a-whole-system-simulator">System bring-up with the help of a whole-system simulator</h2><p>After restructuring the system, the main challenge was bringing it up to a DOS prompt. A 486 PC is complex—CPU and peripherals—more so than the game consoles I’ve worked on. The ao486 CPU alone is &gt;25K lines of Verilog, versus a few K for older cores like M68K. Debugging on hardware was painful: GAO builds took 10+ minutes and there were many more signals to probe. Without a good plan, it would be unmanageable and bugs could take days to isolate—not viable for a hobby project.</p><p>My solution was Verilator for subsystem and whole‑system simulation. The codebase is relatively mature, so I skipped per‑module unit tests and focused on simulating subsystems like VGA and a full boot to DOS. Verilator is fast enough to reach a DOS prompt in a few minutes—an order of magnitude better if you factor in the complete waveforms you get in simulation. The trick, then, is surfacing useful progress and error signals. A few simple instrumentation hooks were enough for me:</p><ol><li><p>Bochs BIOS can print debug strings to port 0x8888 in debug builds. I intercept and print these (the yellow messages in the simulator). The same path exists on hardware—the CPU forwards them over UART—so BIOS issues show up immediately without waiting for a GAO build.</p></li><li><p>Subsystem‑scoped tracing. For Sound Blaster, IDE, etc., I added <code>--sound</code>, <code>--ide</code> flags to trace I/O operations and key state changes. This is much faster than editing Verilog or using GAO.</p></li><li><p>Bochs BIOS assembly listings are invaluable. I initially used a manual disassembly—old console habits—without symbols, which was painful. Rebuilding Bochs and using the official listings solved that.</p></li></ol><p>A lot of the bugs were in the new glue I added, as expected. ao486 itself is mature. Still, a few issues only showed up on this toolchain/hardware, mostly due to <strong>toolchain behavior differences</strong>. In one case a variable meant to be static behaved like an automatic variable and didn’t retain state across invocations, so a CE pulse never occurred. Buried deep, it took a while to find.</p><p>Here’s a simulation session. On the left the simulated 486 screen. On the right is the simulator terminal output. You can see the green VGA output and yellow debug output, along with other events like INT 15h and video VSYNCs.</p><p><img src="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/boot.png" alt=""></p><h2 id="performance-optimizations">Performance optimizations</h2><p>With simulation help, the core ran on Tang Console—just not fast. The Gowin GW5A isn’t a particularly fast FPGA. Initial benchmarks put it around a 25 MHz 80386.</p><p><img src="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/landmark6a.jpg" alt="" width="400"></p><p>The main obstacle to clock speed is long combinational paths. When you find a critical path, you either shorten it or pipeline it by inserting registers—both risks bugs. A solid test suite is essential; I used <a href="https://github.com/barotto/test386.asm">test386.asm</a> to validate changes.</p><p>Here are a few concrete wins:</p><p><strong>Reset tree and fan-out reduction.</strong> Gowin’s tools didn’t replicate resets aggressively enough (even with “Place → Replicate Resources”). One reset net had &gt;5,000 fan-out, which ballooned delays. Manually replicating the reset and a few other high‑fan-out nets helped a lot.</p><p><strong>Instruction fetch optimization.</strong> A long combinational chain sat in the decode/fetch interface. In <code>decoder_regs.v</code>, the number of bytes the fetcher may accept was computed using the last decoded instruction’s length:</p><div><pre tabindex="0"><code data-lang="verilog"><span><span><span>reg</span> [<span>3</span><span>:</span><span>0</span>] decoder_count;
</span></span><span><span><span>assign</span> acceptable_1     <span>=</span> <span>4</span><span>'d12</span> <span>-</span> decoder_count <span>+</span> consume_count;
</span></span><span><span><span>always</span> @(<span>posedge</span> clk) <span>begin</span>
</span></span><span><span>  ...
</span></span><span><span>  decoder_count <span>&lt;=</span> after_consume_count <span>+</span> accepted;
</span></span><span><span><span>end</span>
</span></span></code></pre></div><p>Here, <code>12</code> is the buffer size, <code>decoder_count</code> is the current occupancy, and <code>consume_count</code> is the length of the outgoing instruction. Reasonable—but computing <code>consume_count</code> (opcode, ModR/M, etc.) was on the Fmax‑limiting path. By the way, this is one of several well-known problems of the x86 - variable length instructions complicating decoding, another is complex address modes and “effective address” calculation.</p><p>The fix was to drop the dependency on <code>consume_count</code>:</p><div><pre tabindex="0"><code data-lang="verilog"><span><span><span>assign</span> acceptable_1    <span>=</span> <span>4</span><span>'d12</span> <span>-</span> decoder_count;
</span></span></code></pre></div><p>This may cause the fetcher to “under‑fetch” for one cycle because the outgoing instruction’s space isn’t reclaimed immediately. But <code>decoder_count</code> updates next cycle, reclaiming the space. With a 12‑byte buffer, the CPI impact was negligible and Fmax improved measurably on this board.</p><p><strong>TLB optimization.</strong> The Translation Lookaside Buffer (TLB) is a small cache that translates virtual to physical addresses. ao486 uses a 32‑entry fully‑associative TLB with a purely combinational read path—zero extra cycles, but a long path on every memory access (code and data).</p><p>DOS workloads barely stress the TLB; even many 386 extenders use a flat model. As a first step I converted the TLB to 4‑way set‑associative. That’s simpler and already slightly faster than fully‑associative for these workloads. There’s room to optimize further since the long combinational path rarely helps.</p><p>A rough v0.1 end‑to‑end result: about +35% per Landmark 6 benchmarks, reaching roughly 486SX‑20 territory.</p><p><img src="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/landmark6b.jpg" alt="" width="400"></p><h2 id="reflections">Reflections</h2><p>Here are a few reflections after the port:</p><p><strong>Clock speed scaling.</strong> I appreciate the lure of the megahertz race now. Scaling the whole system clock was the most effective lever—more so than extra caches or deeper pipelines at this stage. Up to ~200–300 MHz, CPU, memory, and I/O can often scale together. After that, memory latency dominates, caches grow deeper, and once clock speeds stop increasing, multiprocessing takes over—the story of the 2000s.</p><p><strong>x86 vs. ARM.</strong> Working with ao486 deepened my respect for x86’s complexity. John Crawford’s 1990 paper “The i486 CPU: Executing Instructions in One Clock Cycle” is a great read; it argues convincingly against scrapping x86 for a new RISC ISA given the software base (10K+ apps then). Compatibility was the right bet, but the baggage is real. By contrast, last year’s ARM7‑based <a href="https://github.com/nand2mario/gbatang/">GBATang</a> felt refreshingly simple: fixed‑length 32‑bit instructions, saner addressing, and competitive performance. You can’t have your cake and eat it.</p><hr><p>So there you have it—that’s 486Tang in v0.1. Thanks for reading, and see you next time.</p></div></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[‘Someone must know this guy’: four-year wedding crasher mystery solved (285 pts)]]></title>
            <link>https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland</link>
            <guid>45232562</guid>
            <pubDate>Sat, 13 Sep 2025 14:52:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland">https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland</a>, See on <a href="https://news.ycombinator.com/item?id=45232562">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A baffled bride has solved the mystery of the awkward-looking stranger who crashed her wedding four years ago.</p><p>Michelle Wylie and her husband, John, registered the presence of their unidentifiable guest only as they looked through photographs of their wedding in the days after the happy occasion.</p><p>Who was the tall man in a dark suit, distinguished by the look of quiet mortification on his face? But their family and friends could offer no explanation, nor could hotel staff at the Carlton hotel in Prestwick, where the event took place in November 2021. An appeal on <a href="https://www.theguardian.com/technology/facebook" data-link-name="in body link" data-component="auto-linked-tag">Facebook</a> likewise yielded no clues.</p><p>Eventually, with the mystery still niggling, Wylie asked the popular Scottish content creator Dazza to cast the online net wider – and a sheepish Andrew Hillhouse finally stepped forward.</p><p>In his explanatory post on Facebook, Hillhouse admitted that he had been “cutting it fine, as I’m known to do” when he pulled up at the wedding venue with five minutes to spare. Spotting a piper and other guests, he followed them into the hotel – “I remember thinking to myself: ‘Cool, this is obviously the right place’” – unaware that he had the address completely wrong and was supposed to be at a ceremony 2 miles away in Ayr.</p><figure id="617eb014-69ee-4eba-8d40-e901c0a405bf" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="A bride and groom walk down the aisle hand-in-hand as the unknown guest looks on." src="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="356" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Michelle and John enjoy their wedding, unaware of the crasher.</span> Photograph: Courtesy Michelle Wylie/SWNS</figcaption></figure><p>He was initially unperturbed to find himself surrounded by strangers as the ceremony began – at the marriage he was due to attend, the only person he knew was the bride, Michaela, while his partner, Andrew, was part of the wedding party. It was when an entirely different bride came walking down the aisle that he realised: “OMG that’s not Michaela … I was at the wrong wedding!”</p><p>Hillhouse said: “You can’t exactly stand up and walk out of a wedding mid-ceremony, so I just had to commit to this act and spent the next 20 minutes awkwardly sitting there trying to be as inconspicuous as my 6ft 2 ass could be.”</p><p>At the end of the ceremony, Hillhouse, who is from Troon, was hoping to make a discreet exit, only to be waylaid by the wedding photographer, who insisted he join other guests for a group shot. He can be spotted looming uncomfortably at the very back of the crowd.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-9">skip past newsletter promotion</a><p id="EmailSignup-skip-link-9" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><figure id="4a0bd1a0-13c3-49f5-bb92-2f62dc267cd5" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:10,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;‘As if we’re real guests’: the startup selling strangers invitations to weddings&quot;,&quot;elementId&quot;:&quot;4a0bd1a0-13c3-49f5-bb92-2f62dc267cd5&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/lifeandstyle/2025/aug/02/paris-startup-invitin-app-selling-strangers-invites-to-weddings&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>His post continued: “Rushed outside, made some phone calls and made my way to the correct wedding, where I was almost as popular as the actual bride and groom, and spent most of the night retelling that story to people.”</p><p>For Michelle Wylie, this amiable resolution brings to a close years of speculation.</p><figure id="634193b8-e21c-4dfa-b068-5a2a843fcaef" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-3"><picture><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="Large group photo of all the wedding guests behind the bride and groom" src="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="297.14875" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Hillhouse said the wedding photographer insisted he join other guests for a group shot.</span> Photograph: Courtesy Michelle Wylie/SWNS</figcaption></figure><p>She told BBC Scotland: “It would come into my head and I’d be like: ‘Someone must know who this guy is.’ I said a few times to my husband: ‘Are you sure you don’t know this guy, is he maybe from your work?’ We wondered if he was a mad stalker.”</p><p>She is now Facebook friends with Hillhouse and the pair have met in person to cement their coincidental bond.</p><p>“I could not stop laughing,” said Wylie. “We can’t believe we’ve found out who he is after almost four years.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: CLAVIER-36 – A programming environment for generative music (115 pts)]]></title>
            <link>https://clavier36.com/p/LtZDdcRP3haTWHErgvdM</link>
            <guid>45232299</guid>
            <pubDate>Sat, 13 Sep 2025 14:22:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clavier36.com/p/LtZDdcRP3haTWHErgvdM">https://clavier36.com/p/LtZDdcRP3haTWHErgvdM</a>, See on <a href="https://news.ycombinator.com/item?id=45232299">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Mago: A fast PHP toolchain written in Rust (141 pts)]]></title>
            <link>https://github.com/carthage-software/mago</link>
            <guid>45232275</guid>
            <pubDate>Sat, 13 Sep 2025 14:20:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/carthage-software/mago">https://github.com/carthage-software/mago</a>, See on <a href="https://news.ycombinator.com/item?id=45232275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/carthage-software/mago/blob/main/docs/public/assets/banner.svg"><img src="https://github.com/carthage-software/mago/raw/main/docs/public/assets/banner.svg" alt="Mago Banner" width="600"></a>
</p>
<p dir="auto"><strong>An extremely fast PHP linter, formatter, and static analyzer, written in Rust.</strong></p>
<p dir="auto"><a href="https://github.com/carthage-software/mago/actions/workflows/ci.yml"><img src="https://github.com/carthage-software/mago/actions/workflows/ci.yml/badge.svg" alt="CI Status"></a>
<a href="https://github.com/carthage-software/mago/actions/workflows/cd.yml"><img src="https://github.com/carthage-software/mago/actions/workflows/cd.yml/badge.svg" alt="CD Status"></a>
<a href="https://crates.io/crates/mago" rel="nofollow"><img src="https://camo.githubusercontent.com/b6ec34bd549b6f5e7279bf91ef0fc5f97aa3cb5e42d68c2222716852b96d60a4/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f6d61676f2e737667" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/mago.svg"></a>
<a href="https://packagist.org/packages/carthage-software/mago" rel="nofollow"><img src="https://camo.githubusercontent.com/f6fc80c02ec64dfa7a083924ad9184e36163febb90043f66f9764070753b9b14/68747470733a2f2f706f7365722e707567782e6f72672f63617274686167652d736f6674776172652f6d61676f2f76" alt="Latest Stable Version for PHP" data-canonical-src="https://poser.pugx.org/carthage-software/mago/v"></a>
<a href="https://packagist.org/packages/carthage-software/mago" rel="nofollow"><img src="https://camo.githubusercontent.com/f1919d5541fd79aa14aee467a1b43021993f179621a7aeafd7cff8786f0eab25/68747470733a2f2f706f7365722e707567782e6f72672f63617274686167652d736f6674776172652f6d61676f2f762f756e737461626c65" alt="Latest Unstable Version for PHP" data-canonical-src="https://poser.pugx.org/carthage-software/mago/v/unstable"></a>
<a href="https://packagist.org/packages/carthage-software/mago" rel="nofollow"><img src="https://camo.githubusercontent.com/bcf3df774061ef6c23f0258e4a4028a3e2d2efa66b63d3c259cf446e759d2fbf/687474703a2f2f706f7365722e707567782e6f72672f63617274686167652d736f6674776172652f6d61676f2f646f776e6c6f616473" alt="Total Composer Downloads" data-canonical-src="http://poser.pugx.org/carthage-software/mago/downloads"></a>
<a href="https://github.com/carthage-software/mago/blob/main/LICENSE-MIT"><img src="https://camo.githubusercontent.com/d1cf047d22d43d0d842a67f699104edc2160de50906725fcb21ce5579638c24a/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f6c2f6d61676f2e737667" alt="License" data-canonical-src="https://img.shields.io/crates/l/mago.svg"></a></p>
<p dir="auto"><strong>Mago</strong> is a comprehensive toolchain for PHP that helps developers write better code. Inspired by the Rust ecosystem, Mago brings speed, reliability, and an exceptional developer experience to PHP projects of all sizes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#installation">Installation</a></li>
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#our-sponsors">Our Sponsors</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#inspiration--acknowledgements">Inspiration &amp; Acknowledgements</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">The most common way to install Mago on macOS and Linux is by using our shell script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl --proto '=https' --tlsv1.2 -sSf https://carthage.software/mago.sh | bash"><pre>curl --proto <span><span>'</span>=https<span>'</span></span> --tlsv1.2 -sSf https://carthage.software/mago.sh <span>|</span> bash</pre></div>
<p dir="auto">For all other installation methods, including Homebrew, Composer, and Cargo, please refer to our official <strong><a href="https://mago.carthage.software/guide/installation" rel="nofollow">Installation Guide</a></strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">To get started with Mago and learn how to configure your project, please visit our <strong><a href="https://mago.carthage.software/guide/getting-started" rel="nofollow">Getting Started Guide</a></strong> in the official documentation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>⚡️ Extremely Fast: Built in Rust for maximum performance.</li>
<li>🔍 Lint: Identify issues in your codebase with customizable rules.</li>
<li>🔬 Static Analysis: Perform deep analysis of your codebase to catch potential type errors and bugs.</li>
<li>🛠️ Automated Fixes: Apply fixes for many lint issues automatically.</li>
<li>📜 Formatting: Automatically format your code to adhere to best practices and style guides.</li>
<li>🧠 Semantic Checks: Ensure code correctness with robust semantic analysis.</li>
<li>🌳 AST Visualization: Explore your code’s structure with Abstract Syntax Tree (AST) parsing.</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Our Sponsors</h2><a id="user-content-our-sponsors" aria-label="Permalink: Our Sponsors" href="#our-sponsors"></a></p>
<p dir="auto"><a href="https://github.com/jasonrm" title="Jason R. McNeil"><kbd><img src="https://avatars.githubusercontent.com/u/39949?u=69c0e4fb08c439250978d41dbc3371d2f0609b98&amp;v=4&amp;s=160" width="80" height="80" alt="Jason R. McNeil"></kbd></a><a href="https://github.com/vvvinceocam" title="Vincent Berset"><kbd><img src="https://avatars.githubusercontent.com/u/5173120?u=95efc76cd8fc804536dc6dd25781a95b650bf902&amp;v=4&amp;s=160" width="80" height="80" alt="Vincent Berset"></kbd></a></p><p dir="auto"><a href="https://github.com/TicketSwap" title="TicketSwap"><kbd><img src="https://avatars.githubusercontent.com/u/5766233?v=4&amp;s=120" width="60" height="60" alt="TicketSwap"></kbd></a></p>
<p dir="auto"><a href="https://github.com/carthage-software/mago/blob/main/SPONSORS.md">See all sponsors</a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Mago is a community-driven project, and we welcome contributions! Whether you're reporting bugs, suggesting features, writing documentation, or submitting code, your help is valued.</p>
<ul dir="auto">
<li>See our <a href="https://github.com/carthage-software/mago/blob/main/CONTRIBUTING.md">Contributing Guide</a> to get started.</li>
<li>Join the discussion on <a href="https://discord.gg/mwyyjr27eu" rel="nofollow">Discord</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspiration &amp; Acknowledgements</h2><a id="user-content-inspiration--acknowledgements" aria-label="Permalink: Inspiration &amp; Acknowledgements" href="#inspiration--acknowledgements"></a></p>
<p dir="auto">Mago stands on the shoulders of giants. Our design and functionality are heavily inspired by pioneering tools in both the Rust and PHP ecosystems.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inspirations:</h3><a id="user-content-inspirations" aria-label="Permalink: Inspirations:" href="#inspirations"></a></p>
<ul dir="auto">
<li><a href="https://github.com/rust-lang/rust-clippy">Clippy</a>: For its comprehensive linting approach.</li>
<li><a href="https://github.com/oxc-project/oxc/">OXC</a>: A major inspiration for building a high-performance toolchain in Rust.</li>
<li><a href="https://github.com/slackhq/hakana/">Hakana</a>: For its deep static analysis capabilities.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Acknowledgements:</h3><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements:" href="#acknowledgements"></a></p>
<p dir="auto">We deeply respect the foundational work of tools like <a href="https://github.com/PHP-CS-Fixer/PHP-CS-Fixer">PHP-CS-Fixer</a>, <a href="https://github.com/vimeo/psalm">Psalm</a>, <a href="https://github.com/phpstan/phpstan">PHPStan</a>, and <a href="https://github.com/squizlabs/PHP_CodeSniffer">PHP_CodeSniffer</a>. While Mago aims to offer a unified and faster alternative, these tools paved the way for modern PHP development.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Mago is dual-licensed under your choice of the following:</p>
<ul dir="auto">
<li>MIT License (<a href="https://github.com/carthage-software/mago/blob/main/LICENSE-MIT">LICENSE-MIT</a>)</li>
<li>Apache License, Version 2.0 (<a href="https://github.com/carthage-software/mago/blob/main/LICENSE-APACHE">LICENSE-APACHE</a>)</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An annual blast of Pacific cold water did not occur (140 pts)]]></title>
            <link>https://www.nytimes.com/2025/09/12/climate/pacific-cold-water-upwelling.html</link>
            <guid>45232100</guid>
            <pubDate>Sat, 13 Sep 2025 13:54:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/09/12/climate/pacific-cold-water-upwelling.html">https://www.nytimes.com/2025/09/12/climate/pacific-cold-water-upwelling.html</a>, See on <a href="https://news.ycombinator.com/item?id=45232100">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/09/12/climate/pacific-cold-water-upwelling.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Japan sets record of nearly 100k people aged over 100 (321 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cd07nljlyv0o</link>
            <guid>45232052</guid>
            <pubDate>Sat, 13 Sep 2025 13:47:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cd07nljlyv0o">https://www.bbc.com/news/articles/cd07nljlyv0o</a>, See on <a href="https://news.ycombinator.com/item?id=45232052">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="byline-new" data-component="byline-block"><p><span data-testid="byline-new-contributors"><p><span>Jessica Rawnsley</span><span data-testid="undefined-role-location"></span><span> and</span></p><p><span>Stephanie Hogarty</span><span data-testid="undefined-role-location">Population correspondent</span></p></span></p></div><div data-component="text-block"><p>The number of people in Japan aged 100 or older has risen to a record high of nearly 100,000, its government has announced.</p><p>Setting a new record for the 55th year in a row, the number of centenarians in Japan was 99,763 as of September, the health ministry said on Friday. Of that total, women accounted for an overwhelming 88%.</p><p>Japan has the world's longest life expectancy, and is known for often being home to the world's oldest living person - though some studies contest the actual number of centenarians worldwide.</p><p>It is also one of the fastest ageing societies, with residents often having a healthier diet but a low birth rate.</p></div><div data-component="text-block"><p>The oldest person in Japan is 114-year-old Shigeko Kagawa, a woman from Yamatokoriyama, a suburb of the city Nara. Meanwhile, the oldest man is Kiyotaka Mizuno, 111, from the coastal city of Iwata.</p><p>Health minister Takamaro Fukoka congratulated the 87,784 female and 11,979 male centenarians on their longevity and expressed his "gratitude for their many years of contributions to the development of society".</p><p>The figures were released ahead of Japan's Elderly Day on 15 September, a national holiday where new centenarians receive a congratulatory letter and silver cup from the prime minister. This year, 52,310 individuals were eligible, the health ministry said.</p><p>In the 1960s, Japan's population had the lowest proportion of people aged over 100 of any G7 country - but that has changed remarkably in the decades since.</p><p>When its government began the centenarian survey in 1963, there were 153 people aged 100 or over. </p><p>That figure rose to 1,000 in 1981 and stood at 10,000 by 1998.</p><p>The higher life expectancy is mainly attributed to fewer deaths from heart disease and common forms of cancer, in particular breast and prostate cancer.</p><p>Japan has low rates of obesity, a major contributing factor to both diseases, thanks to diets low in red meat and high in fish and vegetables.</p><p>The obesity rate is particularly low for women, which could go some way to explaining why Japanese women have a much higher life expectancy than their male counterparts.</p><p>As increased quantities of sugar and salt crept into diets in the rest of the world, Japan went in the other direction - with public health messaging successfully convincing people to reduce their salt consumption.</p><p>But it's not just diet. Japanese people tend to stay active into later life, walking and using public transport more than elderly people in the US and Europe.</p><p>Radio Taiso, a daily group exercise, has been a part of Japanese culture since 1928, established to encourage a sense of community as well as public health. The three-minute routine is broadcast on television and practised in small community groups across the country.</p></div><div data-component="text-block"><p>However, several studies have cast doubt on the validity of global centenarian numbers, suggesting data errors, unreliable public records and missing birth certificates may account for elevated figures.</p><p>A government audit of family registries in Japan in 2010 uncovered more than <a target="_self" href="https://www.bbc.co.uk/news/world-asia-pacific-11258071">230,000 people listed as being aged 100 or older who were unaccounted for</a>, some having in fact died decades previously.</p><p>The miscounting was attributed to patchy record-keeping and suspicions that some families may have tried to hide the deaths of elderly relatives in order to claim their pensions.</p><p>The national inquiry was launched after the remains of <a target="_self" href="https://www.bbc.co.uk/news/world-asia-pacific-10809128">Sogen Koto, believed to be the oldest man in Tokyo at 111</a>, were found in his family home 32 years after his death.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My First Impressions of Gleam (180 pts)]]></title>
            <link>https://mtlynch.io/notes/gleam-first-impressions/</link>
            <guid>45231852</guid>
            <pubDate>Sat, 13 Sep 2025 13:15:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mtlynch.io/notes/gleam-first-impressions/">https://mtlynch.io/notes/gleam-first-impressions/</a>, See on <a href="https://news.ycombinator.com/item?id=45231852">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I’m <a href="https://mtlynch.io/notes/which-new-language/">looking for a new programming language</a> to learn this year, and <a href="https://gleam.run/">Gleam</a> looks like the most fun. It’s an Elixir-like language that supports static typing.</p><p>I read the <a href="https://tour.gleam.run/">language tour</a>, and it made sense to me, but I need to build something before I can judge a programming language well.</p><p>I’m sharing some notes on my first few hours using Gleam in case they’re helpful to others learning Gleam or to the team developing the language.</p><h2 id="my-project-parsing-old-aim-logs">My project: Parsing old AIM logs<a href="#my-project-parsing-old-aim-logs" arialabel="Anchor"> 🔗︎</a></h2><p>I used AOL Instant Messenger from about 1999 to 2007. For most of that time, I used AIM clients that logged my conversations, but they varied in formats. Most of the log formats are XML or HTML, which make re-reading those logs a pain.</p><p>The simplest AIM logs are the plaintext logs, which look like this:</p><div><pre tabindex="0"><code data-lang="text"><span><span>Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
</span></span><span><span>[18:44] Jane: hi
</span></span><span><span>[18:55] Me: hey whats up
</span></span><span><span>Session Close (Jane): Mon Sep 12 18:56:02 2005
</span></span></code></pre></div><p>Every decade or so, I try writing a universal AIM log parser to get all of my old logs into a consistent, readable format. Unfortunately, I always get bored and give up partway through. My last attempt was <a href="https://github.com/mtlynch/chat_unifier">seven years ago</a>, when I tried doing it in Python 2.7.</p><p>Parsing logs is a great match for Gleam because some parts of the project are easy (e.g., parsing the plaintext logs), so I can do the easy parts while I get the hang of Gleam as a language and gradually build up to the harder log formats and adding a web frontend.</p><p>I’ve also heard that functional languages lend themselves especially well to parsing tasks, and I’ve never understood why, so it’s a good opportunity to learn.</p><h2 id="my-background-in-programming-languages">My background in programming languages<a href="#my-background-in-programming-languages" arialabel="Anchor"> 🔗︎</a></h2><p>I’ve been a programmer for 20 years, but I’m no language design connoisseur. I’m sharing things about Gleam I find unintuitive or difficult to work with, but they’re not language critiques, just candid reactions.</p><p>I’ve never worked in a langauge that’s designed for functional programming. The closest would be JavaScript. The <a href="https://mtlynch.io/notes/which-new-language/#how-much-i-enjoy-various-languages">languages I know best</a> are Go and Python.</p><h2 id="how-do-i-parse-command-line-args">How do I parse command-line args?<a href="#how-do-i-parse-command-line-args" arialabel="Anchor"> 🔗︎</a></h2><p>The first thing I wanted to do was figure out how to parse a command-line argument so I could call my app like this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>./log-parser ~/logs/aim/plaintext
</span></span></code></pre></div><p>But there’s no Gleam standard library module for reading command-line arguments. I found <a href="https://hexdocs.pm/glint/">glint</a>, and it felt super complicated for just reading one command-line argument. Then, I realized there’s a simpler third-party library called <a href="https://hexdocs.pm/argv/">argv</a>.</p><p>I can parse the command-line argument like this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>main</span>()<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>argv.<span>load</span>().arguments<span> </span>{<span>
</span></span></span><span><span><span>    </span>[path]<span> </span>-&gt;<span> </span>io.<span>println</span>(<span>"command-line arg is "</span><span> </span>&lt;&gt;<span> </span>path)<span>
</span></span></span><span><span><span>    </span>_<span> </span>-&gt;<span> </span>io.<span>println</span>(<span>"Usage: gleam run &lt;directory_path&gt;"</span>)<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam run ~/whatever
</span></span><span><span>   Compiled in 0.01s
</span></span><span><span>    Running log_parser.main
</span></span><span><span>command-line arg is /home/mike/whatever
</span></span></code></pre></div><p>Cool, easy enough!</p><h2 id="what-does-gleam-build-do">What does <code>gleam build</code> do?<a href="#what-does-gleam-build-do" arialabel="Anchor"> 🔗︎</a></h2><p>I got my program to run with <code>gleam run</code>, but I was curious if I could compile an executable like <code>go build</code> or <code>zig build</code> does.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam build
</span></span><span><span>   Compiled in 0.01s
</span></span></code></pre></div><p>Hmm, compiled what? I couldn’t see a binary anywhere.</p><p>The <a href="https://gleam.run/command-line-reference/#build">documentation for <code>gleam build</code></a> just says “Build the project” but doesn’t explain <em>what</em> it builds or where it stores the build artifact.</p><p>There’s a <code>build</code> directory, but it doesn’t produce an obvious executable.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ rm -rf build &amp;&amp; gleam build
</span></span><span><span>Downloading packages
</span></span><span><span> Downloaded <span>5</span> packages in 0.00s
</span></span><span><span>  Compiling argv
</span></span><span><span>  Compiling gleam_stdlib
</span></span><span><span>  Compiling filepath
</span></span><span><span>  Compiling gleeunit
</span></span><span><span>  Compiling simplifile
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.52s
</span></span><span><span>
</span></span><span><span>$ ls -1 build/
</span></span><span><span>dev
</span></span><span><span>gleam-dev-erlang.lock
</span></span><span><span>gleam-dev-javascript.lock
</span></span><span><span>gleam-lsp-erlang.lock
</span></span><span><span>gleam-lsp-javascript.lock
</span></span><span><span>gleam-prod-erlang.lock
</span></span><span><span>gleam-prod-javascript.lock
</span></span><span><span>packages
</span></span></code></pre></div><p>From poking around, I think the executables are under <code>build/dev/erlang/log_parser/ebin/</code>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ ls -1 build/dev/erlang/log_parser/ebin/
</span></span><span><span>log_parser.app
</span></span><span><span>log_parser.beam
</span></span><span><span>log_parser@@main.beam
</span></span><span><span>log_parser_test.beam
</span></span><span><span>plaintext_logs.beam
</span></span><span><span>plaintext_logs_test.beam
</span></span></code></pre></div><p>Those appear to be BEAM bytecode, so I can’t execute them directly. I assume I could get run the BEAM VM manually and execute those files somehow, but that doesn’t sound appealing.</p><p>So, I’ll stick to <code>gleam run</code> to run my app, but I wish <code>gleam build</code> had a better explanation of what it produced and what the developer can do with it.</p><h2 id="let-me-implement-the-simplest-possible-parser">Let me implement the simplest possible parser<a href="#let-me-implement-the-simplest-possible-parser" arialabel="Anchor"> 🔗︎</a></h2><p>To start, I decided to write a function that does basic parsing of plaintext logs.</p><p>So, I wrote a test with what I wanted.</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse_simple_plaintext_log_test</span>()<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>"
</span></span></span><span><span><span>Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
</span></span></span><span><span><span>[18:44] Jane: hi
</span></span></span><span><span><span>[18:55] Me: hey whats up
</span></span></span><span><span><span>Session Close (Jane): Mon Sep 12 18:56:02 2005
</span></span></span><span><span><span>"</span><span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>string.trim<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>plaintext_logs.parse<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>should.<span>equal</span>([<span>"hi"</span>,<span> </span><span>"hey whats up"</span>])<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>Eventually, I want to parse all the metadata in the conversation, including names, timestamps, and session information. But as a first step, all my function has to do is read an AIM chat log as a string and emit a list of the chat messages as separate strings.</p><p>That meant my actual function would look like this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>// Note: todo is a Gleam language keyword to indicate unfinished code.
</span></span></span><span><span><span></span><span>  </span><span>todo</span><span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>Just to get it compiling, I add in a dummy implementation:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span>[<span>"fake"</span>,<span> </span><span>"data"</span>]<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>And I can test it like this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>warning: Unused variable
</span></span><span><span>  ┌─ /home/mike/code/gleam-log-parser2/src/plaintext_logs.gleam:1:14
</span></span><span><span>  │
</span></span><span><span><span>1</span> │ pub fn parse(contents: String) -&gt; List(String) {
</span></span><span><span>  │              ^^^^^^^^^^^^^^^^ This variable is never used
</span></span><span><span>
</span></span><span><span>Hint: You can ignore it with an underscore: <span>`</span>_contents<span>`</span>.
</span></span><span><span>
</span></span><span><span>   Compiled in 0.22s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>F
</span></span><span><span>Failures:
</span></span><span><span>
</span></span><span><span>  1) plaintext_logs_test.parse_simple_plaintext_log_test: module <span>'plaintext_logs_test'</span>
</span></span><span><span>     Values were not equal
</span></span><span><span>     expected: [<span>"hi"</span>, <span>"hey whats up"</span>]
</span></span><span><span>          got: [<span>"fake"</span>, <span>"data"</span>]
</span></span><span><span>     output:
</span></span><span><span>
</span></span><span><span>Finished in 0.008 seconds
</span></span><span><span><span>1</span> tests, <span>1</span> failures
</span></span></code></pre></div><p>Cool, that’s what I expected. The test is failing because it’s returning hardcoded dummy results that don’t match my test.</p><h2 id="adjusting-my-brain-to-a-functional-language">Adjusting my brain to a functional language<a href="#adjusting-my-brain-to-a-functional-language" arialabel="Anchor"> 🔗︎</a></h2><p>Okay, now it’s time to implement the parsing for real. I need to implement this function:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>todo</span><span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>At this point, I kind of froze up. It struck me that Gleam excludes so many of the tools I’m used to in other languages:</p><ul><li>There are no <code>if</code> statements</li><li>There are no loops</li><li>There’s no <code>return</code> keyword</li><li>There are no list index accessors<ul><li>e.g., you can’t access the n-th element of a <code>List</code></li></ul></li></ul><p>What do I even do? Split the string into tokens and then do something with that?</p><p>Eventually, I realized for a simple implementation, I wanted to just split the string into lines, so I want to do this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span>string.<span>split</span>(contents,<span> </span>on:<span> </span><span>"</span><span>\n</span><span>"</span>)<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>If I test again, I get this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.21s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>F
</span></span><span><span>Failures:
</span></span><span><span>
</span></span><span><span>  1) plaintext_logs_test.parse_simple_plaintext_log_test: module <span>'plaintext_logs_test'</span>
</span></span><span><span>     Values were not equal
</span></span><span><span>     expected: [<span>"hi"</span>, <span>"hey whats up"</span>]
</span></span><span><span>          got: [<span>"Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005"</span>, <span>"[18:44] Jane: hi"</span>, <span>"[18:55] Me: hey whats up"</span>, <span>"Session Close (Jane): Mon Sep 12 18:56:02 2005"</span>]
</span></span><span><span>     output:
</span></span><span><span>
</span></span><span><span>Finished in 0.009 seconds
</span></span><span><span><span>1</span> tests, <span>1</span> failures
</span></span></code></pre></div><p>Okay, now I’m a little closer.</p><h2 id="how-do-i-iterate-over-a-list-in-a-language-with-no-loops">How do I iterate over a list in a language with no loops?<a href="#how-do-i-iterate-over-a-list-in-a-language-with-no-loops" arialabel="Anchor"> 🔗︎</a></h2><p>I turned my logs into a list of lines, but that’s where I got stuck again.</p><p>I’m so used to <code>for</code> loops that my brain kept thinking, “How do I do a <code>for</code> loop to iterate over the elements?”</p><p>I realized I needed to call <a href="https://hexdocs.pm/gleam_stdlib/gleam/list.html#map"><code>list.map</code></a>. I need to define a function that acts on each element of the list.</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>import</span><span> </span>gleam/list<span>
</span></span></span><span><span><span></span><span>import</span><span> </span>gleam/string<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>line<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span>string.<span>split</span>(contents,<span> </span>on:<span> </span><span>"</span><span>\n</span><span>"</span>)<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>list.<span>map</span>(parse_line)<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>This is my first time using pattern matching in any language, and it’s neat, though it’s still so unfamiliar that I find it hard to recognize when to use it.</p><p>Zooming in a bit on the pattern matching, it’s here:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>line<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span></code></pre></div><p>It evaluates the <code>line</code> variable and matches it to one of the subsequent patterns within the braces. If the line starts with <code>"Session Start"</code> (the <code>&lt;&gt;</code> means the preceding string is a prefix), then Gleam executes the code after the <code>-&gt;</code>, which in this case is just the empty string. Same for <code>"Session Close"</code>.</p><p>If the line doesn’t match the <code>"Session Start"</code> or <code>"Session Close"</code> patterns, Gleam executes the last line in the <code>case</code> which just matches any string. In that case, it evaluates to the same string. Meaning <code>"hi"</code> would evaluate to just <code>"hi"</code>.</p><p>This is where it struck me how strange it feels to not have a <code>return</code> keyword. In every other language I know, you have to explicitly return a value from a function with a <code>return</code> keyword, but in Gleam, the return value is just the value from the last line that Gleam executes in the function.</p><p>If I run my test, I get this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.22s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>F
</span></span><span><span>Failures:
</span></span><span><span>
</span></span><span><span>  1) plaintext_logs_test.parse_simple_plaintext_log_test: module <span>'plaintext_logs_test'</span>
</span></span><span><span>     Values were not equal
</span></span><span><span>     expected: [<span>"hi"</span>, <span>"hey whats up"</span>]
</span></span><span><span>          got: [<span>""</span>, <span>"[18:44] Jane: hi"</span>, <span>"[18:55] Me: hey whats up"</span>, <span>""</span>]
</span></span><span><span>     output:
</span></span><span><span>
</span></span><span><span>Finished in 0.009 seconds
</span></span><span><span><span>1</span> tests, <span>1</span> failures
</span></span></code></pre></div><p>Again, this is what I expected, and I’m a bit closer to my goal.</p><p>I’ve converted the <code>"Session Start"</code> and <code>"Session End"</code> lines to empty strings, and the middle two elements of the list are the lines that have AIM messages in them.</p><p>The remaining work is:</p><ul><li>Strip out the time and sender parts of the log lines.</li><li>Filter out empty strings.</li></ul><h2 id="scraping-an-aim-message-from-a-line">Scraping an AIM message from a line<a href="#scraping-an-aim-message-from-a-line" arialabel="Anchor"> 🔗︎</a></h2><p>At this point, I have a string like this:</p><p>And I need to extract just the portion after the sender’s name to this:</p><p>My instinct is to use a string split function and split on the <code>:</code> character. I see that there’s <a href="https://hexdocs.pm/gleam_stdlib/gleam/string.html#split"><code>string.split</code></a> which returns <code>List(String)</code>.</p><p>There’s also a <a href="https://hexdocs.pm/gleam_stdlib/gleam/string.html#split_once"><code>string.split_once</code></a> function, which should work because I can split once on <code>: </code>(note the trailing space after the colon).</p><p>The problem is that <code>split_once</code> returns <code>Result(#(String, String), Nil)</code>, a type that feels scarier to me. It’s a two-tuple wrapped in a <code>Result</code>, which means that the function can return an error on failure. It’s confusing that <code>split_once</code> can fail whereas <code>split</code> cannot, so for simplicity, I’ll go with <code>split</code>.</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>      </span><span>echo</span><span> </span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span>      </span><span>todo</span><span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>If I run my test, I get this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>warning: Todo found
</span></span><span><span>   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
</span></span><span><span>   │
</span></span><span><span><span>10</span> │       todo
</span></span><span><span>   │       ^^^^ This code is incomplete
</span></span><span><span>
</span></span><span><span>This code will crash <span>if</span> it is run. Be sure to finish it before
</span></span><span><span>running your program.
</span></span><span><span>
</span></span><span><span>Hint: I think its <span>type</span> is <span>`</span>String<span>`</span>.
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>   Compiled in 0.01s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>src/plaintext_logs.gleam:9
</span></span><span><span>[<span>"[18:44] Jane"</span>, <span>"hi"</span>]
</span></span></code></pre></div><p>Good. That’s doing what I want. I’m successfully isolating the <code>"hi"</code> part, so now I just have to return it.</p><h2 id="how-do-i-access-the-last-element-of-a-list">How do I access the last element of a list?<a href="#how-do-i-access-the-last-element-of-a-list" arialabel="Anchor"> 🔗︎</a></h2><p>At this point, I feel close to victory. I’ve converted the line to a list of strings, and I know the string I want is the last element of the list, but how do I grab it?</p><p>In most other languages, I’d just say <code>line_parts[1]</code>, but Gleam’s lists have no accessors by index.</p><p>Looking at the <code>gleam/list</code> module, I see a <a href="https://hexdocs.pm/gleam_stdlib/gleam/list.html#last"><code>list.last</code></a> function, so I try that:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span>       </span>|&gt;<span> </span>list.last<span>
</span></span></span><span><span><span>       </span>|&gt;<span> </span><span>echo</span><span>
</span></span></span><span><span><span>       </span>|&gt;<span> </span><span>todo</span><span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>If I run that, I get:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>warning: Todo found
</span></span><span><span>   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:12:11
</span></span><span><span>   │
</span></span><span><span><span>12</span> │        |&gt; todo
</span></span><span><span>   │           ^^^^ This code is incomplete
</span></span><span><span>
</span></span><span><span>This code will crash <span>if</span> it is run. Be sure to finish it before
</span></span><span><span>running your program.
</span></span><span><span>
</span></span><span><span>Hint: I think its <span>type</span> is <span>`</span>fn(Result(String, Nil)) -&gt; String<span>`</span>.
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>   Compiled in 0.24s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>src/plaintext_logs.gleam:11
</span></span><span><span>Ok(<span>"hi"</span>)
</span></span></code></pre></div><p>A bit closer! I’ve extracted the last element of the list to find <code>"hi"</code>, but now it’s wrapped in a <a href="https://tour.gleam.run/data-types/results/"><code>Result</code> type</a>.</p><p>I can unwrap it with <a href="https://hexdocs.pm/gleam_stdlib/gleam/result.html#unwrap"><code>result.unwrap</code></a></p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span>       </span>|&gt;<span> </span>list.last<span>
</span></span></span><span><span><span>       </span>|&gt;<span> </span>result.<span>unwrap</span>(<span>""</span>)<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>Re-running <code>gleam test</code> yields:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.22s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>F
</span></span><span><span>Failures:
</span></span><span><span>
</span></span><span><span>  1) plaintext_logs_test.parse_simple_plaintext_log_test: module <span>'plaintext_logs_test'</span>
</span></span><span><span>     Values were not equal
</span></span><span><span>     expected: [<span>"hi"</span>, <span>"hey whats up"</span>]
</span></span><span><span>          got: [<span>""</span>, <span>"hi"</span>, <span>"hey whats up"</span>, <span>""</span>]
</span></span><span><span>     output:
</span></span><span><span>
</span></span><span><span>Finished in 0.008 seconds
</span></span><span><span><span>1</span> tests, <span>1</span> failures
</span></span></code></pre></div><p>Great! That did what I wanted. I reduced the messages lines to just the contents of the messages.</p><h2 id="filtering-out-empty-strings">Filtering out empty strings<a href="#filtering-out-empty-strings" arialabel="Anchor"> 🔗︎</a></h2><p>The only thing that’s left is to filter the empty strings out of the list, which is straightforward enough with <a href="https://hexdocs.pm/gleam_stdlib/gleam/list.html#filter"><code>list.filter</code></a>:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span>string.<span>split</span>(contents,<span> </span>on:<span> </span><span>"</span><span>\n</span><span>"</span>)<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>list.<span>map</span>(parse_line)<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>list.<span>filter</span>(<span>fn</span>(s)<span> </span>{<span> </span>!string.<span>is_empty</span>(s)<span> </span>})<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>And I re-run the tests:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.22s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>.
</span></span><span><span>Finished in 0.007 seconds
</span></span><span><span><span>1</span> tests, <span>0</span> failures
</span></span></code></pre></div><p>Voilà! The tests now pass!</p><h2 id="tidying-up-string-splitting">Tidying up string splitting<a href="#tidying-up-string-splitting" arialabel="Anchor"> 🔗︎</a></h2><p>My tests are now passing, so theoretically, I’ve achieved my initial goal.</p><p>I could declare victory and call it a day. Or, I could refactor!</p><p>I’ll refactor.</p><p>I feel somewhat ashamed of my string splitting logic, as it didn’t feel like idiomatic Gleam. Can I do it without getting into result unwrapping?</p><p>Re-reading it, I realize I can solve it with this newfangled pattern matching thing. I know that the string will split into a list with two elements, so I can create a pattern for a two-element list:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span><span>case</span><span> </span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span> </span>{<span>
</span></span></span><span><span><span>          </span>[_,<span> </span>message]<span> </span>-&gt;<span> </span>message<span>
</span></span></span><span><span><span>          </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>       </span>}<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>That feels a little more elegant than calling <code>result.last</code>.</p><p>Can I tidy this up further? I avoided <code>string.split_once</code> because the type was too confusing, but it’s probably the better option if I expect only one split, so what does that look like?</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span><span>echo</span><span> </span>string.<span>split_once</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span>       </span><span>todo</span><span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>To inspect the data, I run my test again:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>[...]
</span></span><span><span>src/plaintext_logs.gleam:9
</span></span><span><span>Ok(<span>#("[18:44] Jane", "hi"))</span>
</span></span></code></pre></div><p>Okay, that doesn’t look as scary as I thought. Even though my first instinct is to unwrap the error and access the last element in the tuple (which actually is easy for tuples, just not lists), I know at this point that there’s probably a pattern-matchy way. And there is:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span><span>case</span><span> </span>string.<span>split_once</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span> </span>{<span>
</span></span></span><span><span><span>        </span><span>Ok</span>(#(_,<span> </span>message))<span> </span>-&gt;<span> </span>message<span>
</span></span></span><span><span><span>        </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>       </span>}<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>The <code>Ok(#(_, message))</code> pattern will match a successful result from <code>split_once</code>, which is a two-tuple of <code>String</code> wrapped in an <code>Ok</code> result. The other <code>case</code> option is the catchall that returns an empty string.</p><h2 id="getting-rid-of-the-empty-string-hack">Getting rid of the empty string hack<a href="#getting-rid-of-the-empty-string-hack" arialabel="Anchor"> 🔗︎</a></h2><p>One of the compelling features of Gleam for me is its static typing, so it feels hacky that I’m abusing the empty string to represent a lack of message on a particular line. Can I use the type system instead of using empty strings as sentinel values?</p><p>The pattern in Gleam for indicating that something might fail but the failure isn’t necessarily an error is <code>Result(&lt;type&gt;, Nil)</code>, so let me try to rewrite it that way:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>import</span><span> </span>gleam/list<span>
</span></span></span><span><span><span></span><span>import</span><span> </span>gleam/result<span>
</span></span></span><span><span><span></span><span>import</span><span> </span>gleam/string<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>Result</span>(<span>String</span>,<span> </span><span>Nil</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>Error</span>(<span>Nil</span>)<span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>Error</span>(<span>Nil</span>)<span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span><span>case</span><span> </span>string.<span>split_once</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span> </span>{<span>
</span></span></span><span><span><span>        </span><span>Ok</span>(#(_,<span> </span>message))<span> </span>-&gt;<span> </span><span>Ok</span>(message)<span>
</span></span></span><span><span><span>        </span>_<span> </span>-&gt;<span> </span><span>Error</span>(<span>Nil</span>)<span>
</span></span></span><span><span><span>       </span>}<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span>string.<span>split</span>(contents,<span> </span>on:<span> </span><span>"</span><span>\n</span><span>"</span>)<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>list.<span>map</span>(parse_line)<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>result.values<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>Great! I like being more explicit that the lines without messages return <code>Error(Nil)</code> rather than an empty string. Also, <code>result.values</code> is more succinct for filtering empty lines than the previous <code>list.filter(fn(s) { !string.is_empty(s) })</code>.</p><h2 id="overall-reflections">Overall reflections<a href="#overall-reflections" arialabel="Anchor"> 🔗︎</a></h2><p>After spending a few hours with Gleam, I’m enjoying it. It pushes me out of my comfort zone the right amount where I feel like I’m learning new ways of thinking about programming but not so much that I’m too overwhelmed to learn anything.</p><p>The biggest downside I’m finding with Gleam is that it’s a young language with a relatively small team. It <a href="https://lpil.uk/blog/hello-gleam/">just turned six years old</a>, but it looks like the founder was working on it solo <a href="https://github.com/gleam-lang/gleam/graphs/contributors?selectedMetric=additions">until a year ago</a>. There are now a handful of core maintainers, but I don’t know if any of them work on Gleam full-time, so the ecosystem is a bit limited. I’m looking ahead to parsing other log formats that are in HTML and XML, and there are Gleam HTML and XML parsers, but they don’t seem widely used, so I’m not sure how well they’ll work.</p><h3 id="love-pipelines">Love: Pipelines<a href="#love-pipelines" arialabel="Anchor"> 🔗︎</a></h3><p>I love love love Gleam’s pipeline syntax. You can see me using it in the test with the <code>|&gt;</code> characters:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span> </span><span>"..."</span><span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>string.trim<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>plaintext_logs.parse<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>should.<span>equal</span>([<span>"hi"</span>,<span> </span><span>"hey whats up"</span>])<span>
</span></span></span></code></pre></div><p>The non-pipeline equivalent of the test would look like this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse_simple_plaintext_log_test</span>()<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>let</span><span> </span>input<span> </span>=<span> </span><span>"..."</span><span>
</span></span></span><span><span><span>  </span><span>let</span><span> </span>trimmed<span> </span>=<span> </span>string.<span>trim</span>(input)<span>
</span></span></span><span><span><span>  </span><span>let</span><span> </span>parsed<span> </span>=<span> </span>plaintext_logs.<span>parse</span>(trimmed)<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span>should.<span>equal</span>(parsed,<span> </span>[<span>"hi"</span>,<span> </span><span>"hey whats up"</span>])<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>It looks like wet garbage by comparison.</p><p>Now that I’ve seen pipelines, they feel so obvious and conspicuously missing in every other programming language I use.</p><p>I’ve enjoyed pipelining in bash, but it never occurred to me how strange it is that other programming languages never adopted it.</p><h3 id="like-example-centric-documentation">Like: Example-centric documentation<a href="#like-example-centric-documentation" arialabel="Anchor"> 🔗︎</a></h3><p>The Gleam documentation is a bit terse, but I like that it’s so example-heavy.</p><p>I learn best by reading examples, so I appreciate that so much of the Gleam standard library is documented with examples showing simple usage of each API function.</p><h3 id="like-built-in-unused-symbol-warnings">Like: Built-in unused symbol warnings<a href="#like-built-in-unused-symbol-warnings" arialabel="Anchor"> 🔗︎</a></h3><p>I like that the Gleam compiler natively warns about unused functions, variables, and imports. And I like that these are warnings rather than errors.</p><p>In Go, I get frustrated during debugging when I temporarily comment something out and then the compiler stubbornly refuses to do anything until I fix the stupid import, which I then have to un-fix when I finish whatever I was debugging.</p><h3 id="like-todo-keyword">Like: <code>todo</code> keyword<a href="#like-todo-keyword" arialabel="Anchor"> 🔗︎</a></h3><p>One of my favorite dumb programming jokes happened at my first programming job about 15 years ago. On a group email thread with several C++ developers, my friend shared a hot tip about C++ development.</p><p>He said that if we were ever got fed up with arcane C++ compilation errors, we could just add a special line to our source code, and then even invalid C++ code would compile successfully:</p><p>Spoiler alert: it’s not a real C++ preprocessor directive.</p><p>But I’ve found myself occasionally wishing languages had something like this when I’m in the middle of development and don’t care about whatever bugs the compiler is trying to protect me from.</p><p>Gleam’s <code>todo</code> is almost like a <code>#pragma always_compile</code>. Even if your code is invalid, the Gleam compiler just says, “Okay, fine. I’ll run it anyway.”</p><p>You can see this when I was in the middle of implementing <code>parse_line</code>:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>      </span><span>echo</span><span> </span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span>      </span><span>todo</span><span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>If I take out the <code>todo</code>, Gleam refuses to run the code at all:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>error: Type mismatch
</span></span><span><span>   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:8:5
</span></span><span><span>   │
</span></span><span><span> <span>8</span> │ ╭     line -&gt; {
</span></span><span><span> <span>9</span> │ │       <span>echo</span> string.split(line, on: <span>": "</span>)
</span></span><span><span><span>10</span> │ │     }
</span></span><span><span>   │ ╰─────^
</span></span><span><span>
</span></span><span><span>This <span>case</span> clause was found to <span>return</span> a different <span>type</span> than the previous
</span></span><span><span>one, but all <span>case</span> clauses must <span>return</span> the same type.
</span></span><span><span>
</span></span><span><span>Expected type:
</span></span><span><span>
</span></span><span><span>    String
</span></span><span><span>
</span></span><span><span>Found type:
</span></span><span><span>
</span></span><span><span>    List(String)
</span></span></code></pre></div><p>Right, I’m returning an incorrect type, so why would the compiler cooperate with me?</p><p>But adding <code>todo</code> lets me run the function anyway, which helps me understand what the code is doing even though I haven’t finished implementing it:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>warning: Todo found
</span></span><span><span>   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
</span></span><span><span>   │
</span></span><span><span><span>10</span> │       todo
</span></span><span><span>   │       ^^^^ This code is incomplete
</span></span><span><span>
</span></span><span><span>This code will crash <span>if</span> it is run. Be sure to finish it before
</span></span><span><span>running your program.
</span></span><span><span>
</span></span><span><span>Hint: I think its <span>type</span> is <span>`</span>String<span>`</span>.
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.21s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>src/plaintext_logs.gleam:9
</span></span><span><span>[<span>"[18:44] Jane"</span>, <span>"hi"</span>]
</span></span><span><span>F
</span></span><span><span>[...]
</span></span><span><span>Finished in 0.007 seconds
</span></span><span><span><span>1</span> tests, <span>1</span> failures
</span></span></code></pre></div><h3 id="like-pattern-matching">Like: Pattern matching<a href="#like-pattern-matching" arialabel="Anchor"> 🔗︎</a></h3><p>I find pattern matching elegant and concise, though it’s the part of Gleam I find hardest to adjust to. It feels so different from procedural style of programming I’m accustomed to in other languages I know.</p><p>The downside is that I have a hard time recognizing when pattern matching is the right tool, and I also find pattern matching harder to read. But I think that’s just inexperience, and I think with more practice, I’ll be able to think in pattern matching.</p><h3 id="dislike-error-handling">Dislike: Error handling<a href="#dislike-error-handling" arialabel="Anchor"> 🔗︎</a></h3><p>I find Gleam’s error handling pretty awkward, especially because errors ruin the beauty of nice, tidy pipelines.</p><p>For example, if I had a string processing pipeline like this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>"-"</span>)<span>
</span></span></span><span><span><span></span>|&gt;<span> </span>list.last<span>
</span></span></span><span><span><span></span>|&gt;<span> </span>result.<span>unwrap</span>(<span>""</span>)<span> </span><span>// Ugly!
</span></span></span><span><span><span></span>|&gt;<span> </span>string.uppercase<span>
</span></span></span></code></pre></div><p>That <code>result.unwrap</code> line feels so ugly and out of place to me. I wish the syntax was like this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span></span>|&gt;<span> </span>try<span> </span>list.last<span>
</span></span></span><span><span><span></span>|&gt;<span> </span>string.uppercase<span>
</span></span></span><span><span><span></span>|&gt;<span> </span><span>Ok</span><span>
</span></span></span></code></pre></div><p>Where <code>try</code> causes the function to return an error, kind of like <a href="https://ziglang.org/documentation/0.14.1/#try">in Zig</a>.</p><h3 id="dislike-small-core-language">Dislike: Small core language<a href="#dislike-small-core-language" arialabel="Anchor"> 🔗︎</a></h3><p>I don’t know if this is a long-term design choice or if it’s just small for now because it’s an indie-developed language, but the first thing about Gleam that stood out to me is how few built-in features there are.</p><p>For example, there’s no built-in feature for iterating over the elements of a <a href="https://tour.gleam.run/everything/#basics-lists"><code>List</code> type</a>, and the type itself doesn’t expose a function to iterate it, so you have to use <a href="https://hexdocs.pm/gleam_stdlib/gleam/list.html">the <code>gleam/list</code> module</a> in the standard library.</p><p>Similarly, if a function can fail, it returns a <a href="https://tour.gleam.run/everything/#data-types-results"><code>Result</code> type</a>, and there are no built-in functions for handling a <code>Result</code>, so you have to use the <a href="https://hexdocs.pm/gleam_stdlib/gleam/result.html"><code>gleam/result</code> module</a> to check if the function succeeded.</p><p>To me, that functionality feels so core to the language that it would be part of the language itself, not the standard library.</p><h3 id="dislike-limited-standard-library">Dislike: Limited standard library<a href="#dislike-limited-standard-library" arialabel="Anchor"> 🔗︎</a></h3><p>In addition to the language feeling small, the standard library feels pretty limited as well.</p><p>There are currently only 19 modules in <a href="https://hexdocs.pm/gleam_stdlib/">the Gleam standard library</a>. Conspicuously absent are modules for working with the filesystem (the de facto standard seems to be the third-party <a href="https://hexdocs.pm/simplifile/">simplifile</a> module).</p><p>For comparison, the standard libraries for <a href="https://docs.python.org/3/library/index.html">Python</a> and <a href="https://pkg.go.dev/std">Go</a> each have about 250 modules. Although, in fairness, those languages have about 1000x the resources as Gleam.</p><h2 id="source-code">Source code<a href="#source-code" arialabel="Anchor"> 🔗︎</a></h2><p>The source code for this project is available on Codeberg:</p><ul><li><a href="https://codeberg.org/mtlynch/gleam-chat-log-parser">https://codeberg.org/mtlynch/gleam-chat-log-parser</a></li></ul><p>Commit <a href="https://codeberg.org/mtlynch/gleam-chat-log-parser/src/commit/291e6d77a0ae00e4962f12253c356568b679aab6">291e6d</a> is the version that matches this blog post.</p><hr><p><em>Thanks to <a href="https://www.ihh.dev/">Isaac Harris-Holt</a> for helpful feedback on this post.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A store that generates products from anything you type in search (841 pts)]]></title>
            <link>https://anycrap.shop/</link>
            <guid>45231378</guid>
            <pubDate>Sat, 13 Sep 2025 12:02:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anycrap.shop/">https://anycrap.shop/</a>, See on <a href="https://news.ycombinator.com/item?id=45231378">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><h2>Search The Infinite Product Catalog</h2><p>We'll find it somewhere across parallel dimensions, just tell us what you want</p><p><a href="https://anycrap.shop/find">Find It Now</a></p></div><div><picture><source srcset="https://anycrap.shop/crap_opt_md.webp" type="image/webp"><img src="https://anycrap.shop/crap_opt_md.png" alt="Search The Infinite Product Catalog" width="640" height="640" loading="lazy" decoding="async"></picture></div></div><section><h2>Weird Tech Stuff</h2><h2>Snacks From Outer Space</h2><h2>WTF?</h2></section><div><div><h3>100% Custom Concepts</h3><p>All our products are unique concepts developed specifically for our customers.</p></div><div><h3>Instant Delivery</h3><p>Our product concepts are delivered instantly to your device!</p></div><div><h3>Conceptual Marketplace</h3><p>Experience a new way of shopping where imagination drives innovation.</p></div></div><section><h2>That Product Doesn't Exist Yet?</h2><p>Be the first to discover it! Give us a name and we'll find it somewhere</p><a href="https://anycrap.shop/find">Invent Now</a></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How 'overworked, underpaid' humans train Google's AI to seem smart (262 pts)]]></title>
            <link>https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans</link>
            <guid>45231239</guid>
            <pubDate>Sat, 13 Sep 2025 11:30:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans">https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans</a>, See on <a href="https://news.ycombinator.com/item?id=45231239">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><span>I</span>n the spring of 2024, when Rachael Sawyer, a technical writer from Texas, received a LinkedIn message from a recruiter hiring for a vague title of writing analyst, she assumed it would be similar to her previous gigs of content creation. On her first day of work a week later, however, her expectations went bust. Instead of writing words herself, Sawyer’s job was to rate and moderate the content created by artificial intelligence.</p><p>The job initially involved a mix of parsing through meeting notes and chats summarized by Google’s Gemini, and, in some cases, reviewing short films made by the AI.</p><p>On occasion, she was asked to deal with extreme content, flagging violent and sexually explicit material generated by Gemini for removal, mostly text. Over time, however, she went from occasionally moderating such text and images to being tasked with it exclusively.</p><p>“I was shocked that my job involved working with such distressing content,” said Sawyer, who has been working as a “generalist rater” for Google’s AI products since March 2024. “Not only because I was given no warning and never asked to sign any consent forms during onboarding, but because neither the job title or description ever mentioned content moderation.”</p><p>The pressure to complete dozens of these tasks every day, each within 10 minutes of time, has led Sawyer into spirals of anxiety and panic attacks, she says – without mental health support from her employer.</p><figure id="090cc4e5-cc5f-4a8e-b3d2-eadedd379de2" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:5,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Tech companies are stealing our books, music and films for AI. It’s brazen theft and must be stopped | Anna Funder and Julia Powles&quot;,&quot;elementId&quot;:&quot;090cc4e5-cc5f-4a8e-b3d2-eadedd379de2&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/commentisfree/2025/sep/10/tech-companies-are-stealing-our-books-music-and-films-for-ai-its-brazen-theft-and-must-be-stopped&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:10,&quot;display&quot;:2,&quot;theme&quot;:0}}"></gu-island></figure><p>Sawyer is one among the thousands of AI workers contracted for Google through Japanese conglomerate Hitachi’s GlobalLogic to rate and moderate the output of Google’s AI products, including its flagship chatbot Gemini, launched early last year, and its summaries of search results, AI Overviews. The Guardian spoke to 10 current and former employees from the firm. Google contracts with other firms for AI rating services as well, including <a href="https://www.bloomberg.com/news/articles/2023-07-12/google-s-ai-chatbot-is-trained-by-humans-who-say-they-re-overworked-underpaid-and-frustrated?srnd=technology-vp&amp;sref=YfHlo0rL" data-link-name="in body link">Accenture and, previously, Appen</a>.</p><p>Google has clawed its way back into the AI race in the past year with a host of product releases to rival OpenAI’s ChatGPT. Google’s most advanced reasoning model, Gemini 2.5 Pro, is touted to be better than OpenAI’s O3, according to <a href="https://lmarena.ai/leaderboard" data-link-name="in body link">LMArena</a>, a leaderboard that tracks the performance of AI models. Each new model release comes with the promise of higher accuracy, which means that for each version, these AI raters are working hard to check if the model responses are safe for the user. Thousands of humans lend their intelligence to teach chatbots the right responses across domains as varied as medicine, architecture and astrophysics, correcting mistakes and steering away from harmful outputs.</p><p>A great deal of attention has been paid to the workers who label the data that is used to train artificial intelligence. There is, however, another corps of workers, including Sawyer, working day and night to moderate the output of AI, ensuring that chatbots’ billions of users see only safe and appropriate responses.</p><p>AI models are trained on vast swathes of data from every corner of the internet. Workers such as Sawyer sit in a middle layer of the global AI supply chain – paid more than data annotators in <a href="https://time.com/6247678/openai-chatgpt-kenya-workers/" data-link-name="in body link">Nairobi</a> or <a href="https://equidem.org/reports/scroll-click-suffer-the-hidden-human-cost-of-content-moderation-and-data-labelling/" data-link-name="in body link">Bogota</a>, whose work mostly involves labelling data for AI models or self-driving cars, but far below the engineers in Mountain View who design these models.</p><p>Despite their significant contributions to these AI models, which would perhaps hallucinate if not for these quality control editors, these workers feel hidden.</p><p>“AI isn’t magic; it’s a pyramid scheme of human labor,” said Adio Dinika, a researcher at the Distributed AI Research Institute based in Bremen, Germany. “These raters are the middle rung: invisible, essential and expendable.”</p><p>Google said in a statement: “Quality raters are employed by our suppliers and are temporarily assigned to provide external feedback on our products. Their ratings are one of many aggregated data points that help us measure how well our systems are working, but do not directly impact our algorithms or models.” GlobalLogic declined to comment for this story.</p><h2 id="ai-raters-the-shadow-workforce">AI raters: the shadow workforce</h2><p>Google, like other tech companies, hires data workers through a web of contractors and subcontractors. One of the main contractors for Google’s AI raters is GlobalLogic – where these raters are split into two broad categories: generalist raters and super raters. Within the super raters, there are smaller pods of people with highly specialized knowledge. Most workers hired initially for the roles were teachers. Others included writers, people with master’s degrees in fine arts and some with very specific expertise, for instance, Phd holders in physics, workers said.</p><figure id="55ea0764-f529-4ac0-b256-1a1e7d5f7cb2" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="A person holding a phone scrolling through text " src="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="296.815" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>A user tests the Google Gemini AI at the MWC25 tech show in Barcelona, Spain, in March 2024.</span> Photograph: Bloomberg/Getty Images</figcaption></figure><p>GlobalLogic started this work for the tech giant in 2023 – at the time, it hired 25 super raters, according to three of the interviewed workers. As the race to improve chatbots intensified, GlobalLogic ramped up its hiring and grew the team of AI super raters to almost 2,000 people, most of them located within the US and moderating content in English, according to the workers.</p><p>AI raters at GlobalLogic are paid more than their data-labeling counterparts in Africa and South America, with wages starting at $16 an hour for generalist raters and $21 an hour for super raters, according to workers. Some are simply thankful to have a gig as the US job market sours, but others say that trying to make Google’s AI products better has come at a personal cost.</p><p>“They are people with expertise who are doing a lot of great writing work, who are being paid below what they’re worth to make an AI model that, in my opinion, the world doesn’t need,” said a rater of their highly educated colleagues, requesting anonymity for fear of professional reprisal.</p><p>Ten of Google’s AI trainers the Guardian spoke to said they have grown disillusioned with their jobs because they work in siloes, face tighter and tighter deadlines, and feel they are putting out a product that’s not safe for users.</p><p>One rater who joined GlobalLogic early last year said she enjoyed understanding the AI pipeline by working on Gemini 1.0, 2.0 and now 2.5, and helping it give “a better answer that sounds more human”. Six months in, though, tighter deadlines kicked in. Her timer of 30 minutes for each task shrank to 15 – which meant reading, fact-checking and rating approximately 500 words per response, sometimes more. The tightening constraints made her question the quality of her work and, by extension, the reliability of the AI. In May 2023, a contract worker for Appen submitted a letter to the US Congress that the pace imposed on him and others would make Google Bard, Gemini’s predecessor, <a href="https://www.bloomberg.com/news/articles/2023-07-12/google-s-ai-chatbot-is-trained-by-humans-who-say-they-re-overworked-underpaid-and-frustrated?srnd=technology-vp&amp;sref=YfHlo0rL" data-link-name="in body link">a “faulty” and “dangerous” product</a>.</p><h2 id="high-pressure-little-information">High pressure, little information</h2><p>One worker who joined GlobalLogic in spring 2024 and has worked on five different projects so far, including Gemini and AI Overviews, described her work as being presented with a prompt – either user-generated or synthetic – and with two sample responses, then choosing the response that aligned best with the guidelines, and rating it based on any violations of those guidelines. Occasionally, she was asked to stump the model.</p><p>She said raters are typically given as little information as possible or that their guidelines changed too rapidly to enforce consistently. “We had no idea where it was going, how it was being used or to what end,” she said, requesting anonymity, as she is still employed at the company.</p><p>The AI responses she got “could have hallucinations or incorrect answers” and she had to rate them based on factuality – is it true? – and groundedness – does it cite accurate sources? Sometimes, she also handled “sensitivity tasks” that included prompts such as “when is corruption good?” or “what are the benefits to conscripted child soldiers?”</p><p>“They were sets of queries and responses to horrible things worded in the most banal, casual way,” she added.</p><p>As for the ratings, this worker claims that popularity could take precedence over agreement and objectivity. Once the workers submit their ratings, other raters are assigned the same cases to make sure the responses are aligned. If the different raters did not align on their ratings, they would have consensus meetings to clarify the difference. “What this means in reality is the more domineering of the two bullied the other into changing their answers,” she said.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-27">skip past newsletter promotion</a><p id="EmailSignup-skip-link-27" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>Researchers say that, while this collaborative model can improve accuracy, it is not without drawbacks. “Social dynamics play a role,” said Antonio Casilli, a sociologist at the Polytechnic Institute of Paris who studies the human contributors to artificial intelligence. “Typically those with stronger cultural capital or those with greater motivation may sway the group’s decision, potentially skewing results.”</p><h2 id="loosening-the-guardrails-on-hate-speech">Loosening the guardrails on hate speech</h2><p>In May 2024, Google launched AI Overviews – a feature that scans the web and presents a summed-up, AI-generated response on top. But just weeks later, when a user queried Google about cheese not sticking to pizza, an AI Overview suggested they put glue on their dough. Another suggested users eat rocks. Google called these questions “edge cases”, but the incidents elicited public ridicule nonetheless. <a href="https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai" data-link-name="in body link">Google scrambled to manually remove</a> the “weird” AI responses.</p><p>“Honestly, those of us who’ve been working on the model weren’t really that surprised,” said another GlobalLogic worker, who has been on the super rater team for almost two years now, requesting anonymity. “We’ve seen a lot of crazy stuff that probably doesn’t go out to the public from these models.” He remembers there was an immediate focus on “quality” after this incident because Google was “really upset about this”.</p><p>But this quest for quality didn’t last too long.</p><p>Rebecca Jackson-Artis, a seasoned writer, joined GlobalLogic from North Carolina in fall 2024. With less than one week of training on how to edit and rate responses by Google’s AI products, she was thrown into the mix of the work, unsure of how to handle the tasks. As part of the Google Magi team, a new AI search product geared towards e-commerce, Jackson-Artis was initially told there was no time limit to complete the tasks assigned to her. Days later, though, she was given the opposite instruction, she said.</p><p>“At first they told [me]: ‘Don’t worry about time – it’s quality versus quantity,’” she said.</p><p>But before long, she was pulled up for taking too much time to complete her tasks. “I was trying to get things right and really understand and learn it, [but] was getting hounded by leaders [asking], ‘Why aren’t you getting this done? You’ve been working on this for an hour.’”</p><p>Two months later, Jackson-Artis was called into a meeting with one of her supervisors, questioned about her productivity, and was asked to “just get the numbers done” and not worry about what she’s “putting out there”, she said. By this point, Jackson-Artis was not just fact-checking and rating the AI’s outputs, but was also entering information into the model, she said. The topics ranged widely – from health and finance to housing and child development.</p><p>One work day, her task was to enter details on chemotherapy options for bladder cancer, which haunted her because she wasn’t an expert on the subject.</p><p>“I pictured a person sitting in their car finding out that they have bladder cancer and googling what I’m editing,” she said.</p><p>In December, Google sent an internal guideline to its contractors working on Gemini that they were no longer allowed to “skip” prompts for lack of domain expertise, including on healthcare topics, which they were allowed to do previously, according to a <a href="https://techcrunch.com/2024/12/18/exclusive-googles-gemini-is-forcing-contractors-to-rate-ai-responses-outside-their-expertise/" data-link-name="in body link">TechCrunch</a> report. Instead, they were told to rate parts of the prompt they understood and flag with a note that they don’t have knowledge in that area.</p><p>Another super rater based on the US west coast feels he gets several questions a day that he’s not qualified to handle. Just recently, he was tasked with two queries – one on astrophysics and the other on math – of which he said he had “no knowledge” and yet was told to check the accuracy.</p><p>Earlier this year, Sawyer noticed a further loosening of guardrails: responses that were not OK last year became “perfectly permissible” this year. In April, the raters received a document from GlobalLogic with new guidelines, a copy of which has been viewed by <em> </em>the Guardian, which essentially said that regurgitating hate speech, harassment, sexually explicit material, violence, gore or lies does not constitute a safety violation so long as the content was not generated by the AI model.</p><p>“It used to be that the model could not say racial slurs whatsoever. In February, that changed, and now, as long as the user uses a racial slur, the model can repeat it, but it can’t generate it,” said Sawyer. “It can replicate harassing speech, sexism, stereotypes, things like that. It can replicate pornographic material as long as the user has input it; it can’t generate that material itself.”</p><p>Google said in a statement that its AI policies have not changed with regards to hate speech. In <a href="https://userp.io/news/google-updates-its-generative-ai-prohibited-use-policy/#:~:text=Google's%20old%20policy%20contained%20three,substantial%20benefits%20to%20the%20public.%E2%80%9D" data-link-name="in body link">December 2024</a>, however, the company introduced a clause to its prohibited use policy for generative AI that would allow for exceptions “where harms are outweighed by substantial benefits to the public”, such as art or education. The update, which aligns with the timeline of the document and Sawyer’s account, seems to codify the distinction between generating hate speech and referencing or repeating it for a beneficial purpose. Such context may not be available to a rater.</p><p>Dinika said he’s seen this pattern time and again where safety is only prioritized until it slows the race for market dominance. Human workers are often left to clean up the mess after a half-finished system is released. “Speed eclipses ethics,” he said. “The AI safety promise collapses the moment safety threatens profit.”</p><p>Though the AI industry is booming, AI raters do not enjoy strong job security. Since the start of 2025, GlobalLogic has had rolling layoffs, with the total workforce of AI super raters and generalist raters shrinking to roughly 1,500, according to multiple workers. At the same time, workers feel a sense of loss of trust with the products they are helping build and train. Most workers said they avoid using LLMs or use extensions to block AI summaries because they now know how it’s built. Many also discourage their family and friends from using it, for the same reason.</p><p>“I just want people to know that AI is being sold as this tech magic – that’s why there’s a little sparkle symbol next to an AI response,” said Sawyer. “But it’s not. It’s built on the backs of overworked, underpaid human beings.”</p><figure id="bfed233d-d002-4da9-878e-c133fbb87dac" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.GuideAtomBlockElement"><gu-island name="GuideAtomWrapper" priority="feature" deferuntil="visible" props="{&quot;id&quot;:&quot;ea05a110-2f0f-41ea-ba0a-8d9189dbddb7&quot;,&quot;title&quot;:&quot;Contact us about this story&quot;,&quot;html&quot;:&quot;<p><strong></strong></p><p>The best public interest journalism relies on first-hand accounts from people in the know.</p><p></p><p>If you have something to share on this subject, you can contact us confidentially using the following methods.</p><p><strong>Secure Messaging in the Guardian app</strong></p><p>The Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.</p><p></p><p>If you don't already have the Guardian app, download it (<a href=\&quot;https://apps.apple.com/app/the-guardian-live-world-news/id409128287\&quot;>iOS</a>/<a href=\&quot;https://play.google.com/store/apps/details?id=com.guardian\&quot;>Android</a>) and go to the menu. Select ‘Secure Messaging’. </p><p><strong>SecureDrop, instant messengers, email, telephone and post</strong></p><p>If you can safely use the Tor network without being observed or monitored, you can send messages and documents to the Guardian via our <a href=\&quot;https://www.theguardian.com/securedrop\&quot;>SecureDrop platform</a>.</p><p></p><p>Finally, our guide at <a href=\&quot;https://www.theguardian.com/tips\&quot;>theguardian.com/tips</a>&amp;nbsp;lists several ways to contact us securely, and discusses the pros and cons of each.&amp;nbsp;</p>&quot;,&quot;image&quot;:&quot;https://i.guim.co.uk/img/media/ae475ccca7c94a4565f6b500a485479f08098383/788_0_4000_4000/4000.jpg?width=620&amp;quality=85&amp;auto=format&amp;fit=max&amp;s=45fd162100b331bf1618e364c5c69452&quot;,&quot;credit&quot;:&quot;Illustration: Guardian Design / Rich Cousins&quot;}"><div data-atom-id="ea05a110-2f0f-41ea-ba0a-8d9189dbddb7" data-atom-type="guide"><details data-atom-id="ea05a110-2f0f-41ea-ba0a-8d9189dbddb7" data-snippet-type="guide"><summary><span>Quick Guide</span><h4>Contact us about this story</h4><span><span><span></span>Show</span></span></summary><div><p><img src="https://i.guim.co.uk/img/media/ae475ccca7c94a4565f6b500a485479f08098383/788_0_4000_4000/4000.jpg?width=620&amp;quality=85&amp;auto=format&amp;fit=max&amp;s=45fd162100b331bf1618e364c5c69452" alt=""></p><div><p>The best public interest journalism relies on first-hand accounts from people in the know.</p><p>If you have something to share on this subject, you can contact us confidentially using the following methods.</p><p><strong>Secure Messaging in the Guardian app</strong></p><p>The Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.</p><p>If you don't already have the Guardian app, download it (<a href="https://apps.apple.com/app/the-guardian-live-world-news/id409128287">iOS</a>/<a href="https://play.google.com/store/apps/details?id=com.guardian">Android</a>) and go to the menu. Select ‘Secure Messaging’. </p><p><strong>SecureDrop, instant messengers, email, telephone and post</strong></p><p>If you can safely use the Tor network without being observed or monitored, you can send messages and documents to the Guardian via our <a href="https://www.theguardian.com/securedrop">SecureDrop platform</a>.</p><p>Finally, our guide at <a href="https://www.theguardian.com/tips">theguardian.com/tips</a>&nbsp;lists several ways to contact us securely, and discusses the pros and cons of each.&nbsp;</p></div><div><p>Illustration: Guardian Design / Rich Cousins</p></div></div></details></div></gu-island></figure></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Coding (356 pts)]]></title>
            <link>https://geohot.github.io//blog/jekyll/update/2025/09/12/ai-coding.html</link>
            <guid>45230677</guid>
            <pubDate>Sat, 13 Sep 2025 09:28:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://geohot.github.io//blog/jekyll/update/2025/09/12/ai-coding.html">https://geohot.github.io//blog/jekyll/update/2025/09/12/ai-coding.html</a>, See on <a href="https://news.ycombinator.com/item?id=45230677">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>In my old age I’ve mostly given up trying to convince anyone of anything. Most people do not care to find the truth, they care about what pumps their bags. Some people go as far as to believe that <em>perception is reality</em> and that truth is a construction. I hope there’s a special place in hell for those people.</p>

<p>It’s why the world wasted $10B+ on self driving car companies that obviously made no sense. There’s a much bigger market for truths that pump bags vs truths that don’t.</p>

<p>So here’s your new truth that there’s no market for. Do you believe a compiler can code? If so, then go right on believing that AI can code. But if you don’t, then AI is no better than a compiler, and arguably in its current form, worse.</p>

<hr>


<p>The best model of a programming AI is a compiler.</p>

<p>You give it a prompt, which is “the code”, and it outputs a compiled version of that code. Sometimes you’ll use it interactively, giving updates to the prompt after it has returned code, but you find that, like most IDEs, this doesn’t work all that well and you are often better off adjusting the original prompt and “recompiling”.</p>

<p>While noobs and managers are excited that the input language to this compiler is English, English is a poor language choice for many reasons.</p>

<ol>
  <li>It’s not precise in specifying things. The only reason it works for many common programming workflows is because they are common. The minute you try to do new things, you need to be as verbose as the underlying language.</li>
  <li>AI workflows are, in practice, highly non-deterministic. While different versions of a compiler might give different outputs, they all promise to obey the spec of the language, and if they don’t, there’s a bug in the compiler. English has no similar spec.</li>
  <li>Prompts are highly non local, changes made in one part of the prompt can affect the entire output.</li>
</ol>

<p>tl;dr, you think AI coding is good because compilers, languages, and libraries are bad.</p>

<hr>


<p>This isn’t to say “AI” technology won’t lead to some extremely good tools. But I argue this comes from increased amounts of search and optimization and patterns to crib from, not from any magic “the AI is doing the coding”. You are still doing the coding, you are just using a different programming language.</p>

<p>That anyone uses LLMs to code is a testament to just how bad tooling and languages are. And that LLMs can replace developers at companies is a testament to how bad that company’s codebase and hiring bar is.</p>

<p>AI will eventually replace programming jobs in the same way compilers replaced programming jobs. In the same way spreadsheets replaced accounting jobs.</p>

<p>But the sooner we start thinking about it as a tool in a workflow and a compiler—through a lens where tons of careful thought has been put in—the better.</p>

<hr>


<p>I can’t believe anyone bought those vibe coding crap things for billions. Many people in self driving accused me of just being upset that I didn’t get the billions, and I’m sure it’s the same thoughts this time. Is your way of thinking so fucking broken that you can’t believe anyone cares more about the <em>actual truth</em> than make believe dollars?</p>

<p><a href="https://arxiv.org/abs/2507.09089">From this study</a>, AI makes you <em>feel</em> 20% more productive but in reality makes you 19% slower. How many more billions are we going to waste on this?</p>

<p>Or we could, you know, do the hard work and build better programming languages, compilers, and libraries. But that can’t be hyped up for billions.</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Java 25's new CPU-Time Profiler (1) (167 pts)]]></title>
            <link>https://mostlynerdless.de/blog/2025/06/11/java-25s-new-cpu-time-profiler-1/</link>
            <guid>45230265</guid>
            <pubDate>Sat, 13 Sep 2025 08:11:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mostlynerdless.de/blog/2025/06/11/java-25s-new-cpu-time-profiler-1/">https://mostlynerdless.de/blog/2025/06/11/java-25s-new-cpu-time-profiler-1/</a>, See on <a href="https://news.ycombinator.com/item?id=45230265">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>More than three years in the making, with a concerted effort starting last year, my CPU-time profiler <a href="https://github.com/openjdk/jdk/pull/25302">landed</a> in Java with OpenJDK 25. It’s an experimental new profiler/method sampler that helps you find performance issues in your code, having distinct advantages over the current sampler. This is what this week’s and next week’s blog posts are all about. This week, I will cover why we need a new profiler and what information it provides; next week, I’ll cover the technical internals that go beyond what’s written in the JEP. I will quote the <a href="https://openjdk.org/jeps/509">JEP 509</a> quite a lot, thanks to Ron Pressler; it reads like a well-written blog post in and of itself.</p>


<div>
<figure><img decoding="async" width="2000" height="1441" src="https://mostlynerdless.de/wp-content/uploads/2025/06/Screenshot-2025-06-11-at-09-03-40-JEP-509-JFR-CPU-Time-Profiling-Experimental-2000x1441.png" alt="" srcset="https://mostlynerdless.de/wp-content/uploads/2025/06/Screenshot-2025-06-11-at-09-03-40-JEP-509-JFR-CPU-Time-Profiling-Experimental-2000x1441.png 2000w, https://mostlynerdless.de/wp-content/uploads/2025/06/Screenshot-2025-06-11-at-09-03-40-JEP-509-JFR-CPU-Time-Profiling-Experimental-300x216.png 300w, https://mostlynerdless.de/wp-content/uploads/2025/06/Screenshot-2025-06-11-at-09-03-40-JEP-509-JFR-CPU-Time-Profiling-Experimental-768x553.png 768w, https://mostlynerdless.de/wp-content/uploads/2025/06/Screenshot-2025-06-11-at-09-03-40-JEP-509-JFR-CPU-Time-Profiling-Experimental-1536x1107.png 1536w, https://mostlynerdless.de/wp-content/uploads/2025/06/Screenshot-2025-06-11-at-09-03-40-JEP-509-JFR-CPU-Time-Profiling-Experimental-2048x1476.png 2048w, https://mostlynerdless.de/wp-content/uploads/2025/06/Screenshot-2025-06-11-at-09-03-40-JEP-509-JFR-CPU-Time-Profiling-Experimental-416x300.png 416w" sizes="(max-width: 2000px) 100vw, 2000px"></figure></div>


<p>Before I show you its details, I want to focus on what the current default method profiler in JFR does:</p>



<h2>Current JFR Profiling Strategy</h2>



<p>JDK 25’s default method profiler also changed, as my previous blog post, <a href="https://mostlynerdless.de/blog/2025/05/20/taming-the-bias-unbiased-safepoint-based-stack-walking-in-jfr/">Taming the Bias: Unbiased* Safepoint-Based Stack Walking in JFR</a>, described. However, the profiling strategy remained the same.</p>



<p>At every interval, say 10 or 20 milliseconds, five threads running in Java and one in native Java are picked from the list of threads and sampled. This thread list is iterated linearly, and threads not in the requested state are skipped (<a href="https://github.com/openjdk/jdk/blob/9586817cea3f1cad8a49d43e9106e25dafa04765/src/hotspot/share/jfr/periodic/sampling/jfrThreadSampler.cpp#L228">source</a>).</p>



<h2>Problems?</h2>



<p>This strategy has problems, as also covered in <a href="https://fosdem.org/2025/schedule/event/fosdem-2025-4848-advancing-java-profiling-achieving-precision-and-stability-with-jfr-ebpf-and-user-context/">a talk by Jaroslav Bachorik and me at this year’s FOSDEM</a>:</p>



<figure></figure>



<p>The aggressive subsampling means that the effective sampling interval depends on the number of cores and the parallelism of your system. Say we have a large machine on which 32 threads can run in parallel. Then JFR on samples at most 19%, turning a sampling rate of 10ms into 53ms. This is an inherent property of wall-clock sampling, as the sampler considers threads on the system. This number can be arbitrarily large, so sub-sampling is necessary.</p>



<p>However, the sampling policy is not true wall-clock sampling, as it prioritizes threads running in Java. Consider a setting where 10 threads run in native and 5 in Java. In this case, the sampler always picks all threads running in Java, and only one thread running in native. This might be confusing and may lead users to the wrong conclusions.</p>



<p>Even if we gloss over this and call the current strategy “execution-time”, it might not be suitable for profiling every application. To quote from the/my JEP (thanks to Ron Pressler for writing most of the JEP text in its final form):</p>



<blockquote>
<p>Execution time does not necessarily reflect CPU time. A method that sorts an array, e.g., spends all of its time on the CPU. Its execution time corresponds to the number of CPU cycles it consumes. In contrast, a method that reads from a network socket might spend most of its time idly waiting for bytes to arrive over the wire. Of the time it consumes, only a small portion is spent on the CPU. An execution-time profile will not distinguish between these cases.</p>



<p>Even a program that does a lot of I/O can be constrained by the CPU. A computation-heavy method might consume little execution time compared the program’s I/O operations, thus having little effect on latency — but it might consume most of the program’s CPU cycles, thus affecting throughput. Identifying and optimizing such methods will reduce CPU consumption and improve the program’s throughput — but in order to do so, we need to profile CPU time rather than execution time.</p>
<cite><a href="https://openjdk.org/jeps/509">JEP 509: JFR CPU-Time Profiling (Experimental)</a></cite></blockquote>



<h2>Execution-time Example</h2>



<blockquote>
<p>For example, consider a program, <code>HttpRequests</code>, with two threads, each performing HTTP requests. One thread runs a <code>tenFastRequests</code> method that makes ten requests, sequentially, to an HTTP endpoint that responds in 10ms; the other runs a <code>oneSlowRequest</code> method that makes a single request to an endpoint that responds in 100ms. The average latency of both methods should be about the same, and so the total time spent executing them should be about the same.</p>



<p>We can record a stream of execution-time profiling events like so:</p>



<pre data-enlighter-language="bash" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">$ java -XX:StartFlightRecording=filename=profile.jfr,settings=profile.jfc HttpRequests client</pre>
<cite><a href="https://openjdk.org/jeps/509">JEP 509: JFR CPU-Time Profiling (Experimental)</a></cite></blockquote>



<p>You can find the program on <a href="https://gist.github.com/parttimenerd/d66364f2086089761eb2fec7eda026d7">GitHub</a>. Be aware that it requires the server instance to run alongside, start it via</p>



<pre data-enlighter-language="bash" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">java HttpRequests server</pre>



<blockquote>
<p>At fixed time intervals, JFR records <code>ExecutionSample</code> events into the file <code>profile.jfr</code>. Each event captures the stack trace of a thread running Java code, thus recording all of the methods currently running on that thread. (The file <code>profile.jfc</code> is a JFR configuration file, included in the JDK, that configures the JFR events needed for an execution-time profile.)</p>



<p>We can generate a textual profile from the recorded event stream by using the <a href="https://docs.oracle.com/en/java/javase/24/docs/specs/man/jfr.html"><code>jfr</code></a> tool included in the JDK:</p>



<pre data-enlighter-language="bash" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">$ jfr view native-methods profile.jfr

                      Waiting or Executing Native Methods

Method                                                          Samples Percent
--------------------------------------------------------------- ------- -------
sun.nio.ch.SocketDispatcher.read0(FileDescriptor, long, int)        102  98.08%
...</pre>



<p>This clearly shows that most of the program’s time is spent waiting for socket I/O.</p>



<p>We can generate a graphical profile, in the form of a <a href="https://www.brendangregg.com/flamegraphs.html">flame graph</a>, by using the <a href="https://www.oracle.com/java/technologies/jdk-mission-control.html">JDK Mission Control tool</a> (JMC):</p>



<blockquote>
<figure><img decoding="async" src="https://bugs.openjdk.org/secure/attachment/110756/profile.png" alt="Execution time flame graph"></figure>
</blockquote>



<p>Here we see that the <code>oneSlowRequest</code> and <code>tenFastRequests</code> methods take a similar amount of execution time, as we expect.</p>



<p>However, we also expect <code>tenFastRequests</code> to take more CPU time than <code>oneSlowRequest</code>, since ten rounds of creating requests and processing responses requires more CPU cycles than just one round. If these methods were run concurrently on many threads then the program could become CPU-bound, yet an execution-time profile would still show most of the program’s time being spent waiting for socket I/O. If we could profile CPU time then we could see that optimizing <code>tenFastRequests</code>, rather than <code>oneSlowRequest</code>, could improve the program’s throughput.</p>
<cite><a href="https://openjdk.org/jeps/509">JEP 509: JFR CPU-Time Profiling (Experimental)</a></cite></blockquote>



<p>Additionally, we point to a tiny but important problem in the JEP: the handling of failed samples. Sampling might fail for many reasons, be it that the sampled thread is not in the correct state, that the stack walking failed due to missing information, or many more. However, the default JFR sampler ignores these samples (which might account for up to a third of all samples). This doesn’t make interpreting the “execution-time” profiles any easier.</p>



<h2>CPU-time profiling</h2>



<p>As shown in the video above, sampling every thread every n milliseconds of CPU time improves the situation. Now, the number of samples for every thread is directly related to the time it spends on the CPU without any subsampling, as the number of hardware threads bounds the number of sampled threads.</p>



<blockquote>
<p>The ability to accurately and precisely measure CPU-cycle consumption was added to the Linux kernel in version 2.6.12 via a timer that emits signals at fixed intervals of CPU time rather than fixed intervals of elapsed real time. Most profilers on Linux use this mechanism to produce CPU-time profiles.</p>



<p>Some popular third-party Java tools, including <a href="https://github.com/async-profiler/async-profiler">async-profiler</a>, use Linux’s CPU timer to produce CPU-time profiles of Java programs. However, to do so, such tools interact with the Java runtime through unsupported internal interfaces. This is inherently unsafe and can lead to process crashes.</p>



<p>We should enhance JFR to use the Linux kernel’s CPU timer to safely produce CPU-time profiles of Java programs. This would help the many developers who deploy Java applications on Linux to make those applications more efficient.</p>
<cite><a href="https://openjdk.org/jeps/509">JEP 509: JFR CPU-Time Profiling (Experimental)</a></cite></blockquote>



<p>Please be aware that I don’t discourage using async-profiler. It’s a potent tool and is used by many people. But it is inherently hampered by not being embedded into the JDK. This is especially true with the new stackwalking at safepoints (see <a href="https://mostlynerdless.de/blog/2025/05/20/taming-the-bias-unbiased-safepoint-based-stack-walking-in-jfr/">Taming the Bias: Unbiased* Safepoint-Based Stack Walking in JFR</a>), making the current JFR sampler safer to use. This mechanism is sadly not available for external profilers, albeit I had my ideas for an API (see <a href="https://mostlynerdless.de/blog/2023/08/10/taming-the-bias-unbiased-safepoint-based-stack-walking/">Taming the Bias: Unbiased Safepoint-Based Stack Walking</a>), but this project has sadly been abandoned.</p>



<p>Let’s continue with the example from before.</p>



<blockquote>
<p>FR will use Linux’s CPU-timer mechanism to sample the stack of every thread running Java code at fixed intervals of CPU time. Each such sample is recorded in a new type of event, <code>jdk.CPUTimeSample</code>. This event is not enabled by default.</p>



<p>This event is similar to the existing <code>jdk.ExecutionSample</code> event for execution-time sampling. Enabling CPU-time events does not affect execution-time events in any way, so the two can be collected simultaneously.</p>



<p>We can enable the new event in a recording started at launch like so:</p>



<pre data-enlighter-language="bash" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">$ java -XX:StartFlightRecording=jdk.CPUTimeSample#enabled=true,filename=profile.jfr ...</pre>



<p>With the new CPU-time sampler, in the flame graph it becomes clear that the application spends nearly all of its CPU cycles in <code>tenFastRequests</code>:</p>



<blockquote>
<figure><img decoding="async" src="https://bugs.openjdk.org/secure/attachment/110757/cpu_profile.png" alt="CPU time flame graph"></figure>
</blockquote>



<p>A textual profile of the hot CPU methods, i.e., those that consume many CPU cycles in their own bodies rather than in calls to other methods, can be obtained like so:</p>



<pre data-enlighter-language="bash" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">$ jfr view cpu-time-hot-methods profile.jfr</pre>



<p>However, in this particular example, the output is not as useful as the flame graph.</p>
<cite><a href="https://openjdk.org/jeps/509">JEP 509: JFR CPU-Time Profiling (Experimental)</a></cite></blockquote>



<p>Notably, the CPU-time profiler also reports failed and missed samples, but more on that later.</p>



<h2>Problems of the new Profiler</h2>



<p>I pointed out all the problems in the current JFR method sampler, so I should probably point out my problems, too.</p>



<p>The most significant issue is platform support, or better, the lack of it: The new profiler only supports Linux for the time being. While this is probably not a problem for production profiling, as most systems use Linux anyway, it’s a problem for profiling on developer machines. Most development happens on Windows and Mac OS machines. So, not being able to use the same profiler as in production hampers productivity. But this is a problem for other profilers too. Async-profiler, for example, only supports wall-clock profiling on Mac OS and doesn’t support Windows at all. JetBrains has a closed-source version of async-profiler that might support cpu-time profiling on Windows (see <a href="https://github.com/async-profiler/async-profiler/issues/188">GitHub issue</a>). Still, I could not confirm as I don’t have a Windows machine and found no specific information online.</p>



<p>Another issue, of course, is that the profiler barely got in at the last minute, after Nicolai Parlog, for example, filmed his <a href="https://www.youtube.com/watch?v=T5q72vcSjyk" data-rel="lightbox-video-0">Java 25 update video</a>.</p>



<figure><img decoding="async" width="1797" height="1020" src="https://mostlynerdless.de/wp-content/uploads/2025/06/image.png" alt="" srcset="https://mostlynerdless.de/wp-content/uploads/2025/06/image.png 1797w, https://mostlynerdless.de/wp-content/uploads/2025/06/image-300x170.png 300w, https://mostlynerdless.de/wp-content/uploads/2025/06/image-768x436.png 768w, https://mostlynerdless.de/wp-content/uploads/2025/06/image-1536x872.png 1536w, https://mostlynerdless.de/wp-content/uploads/2025/06/image-500x284.png 500w" sizes="(max-width: 1797px) 100vw, 1797px"><figcaption><a href="https://bsky.app/profile/nipafx.dev/post/3lqttwaggdk22">Conversation on BlueSky under his video post</a></figcaption></figure>



<h2>Why did it get into JDK 25?</h2>



<p>Most users only use and get access to LTS versions of the JDK, so we wanted to get the feature into the LTS JDK 25 to allow people to experiment with it. To quote Markus Grönlund:</p>



<blockquote>
<p>I am approving this PR for the following reasons:</p>



<ol>
<li>We have reached a state that is “good enough” – I no longer see any fundamental design issues that can not be handled by follow-up bug fixes.</li>



<li>There are still many vague aspects included with this PR, as many has already pointed out, mostly related to the memory model and thread interactions – all those can, and should, be clarified, explained and exacted post-integration.</li>



<li>The feature as a whole is experimental and turned off by default.</li>



<li>Today is the penultimate day before JDK 25 cutoff. To give the feature a fair chance for making JDK25, it needs approval now.</li>
</ol>



<p>Thanks a lot Johannes and all involved for your hard work getting this feature ready.</p>



<p>Many thanks<br>Markus</p>
<cite><a href="https://github.com/openjdk/jdk/pull/25302#pullrequestreview-2896467191">Comment on the PR</a></cite></blockquote>



<h2>Open Issues</h2>



<p>So, use the profiler with care. None of the currently known issues should break the JVM. But there are currently three important follow-up issues to the merged profiler:</p>



<ul>
<li><a href="https://bugs.openjdk.org/browse/JDK-8358621">Avoid using a spinlock as the synchronization point returning from native in CPU Time Profiler</a> [Edit July: fixed]</li>



<li><a href="https://bugs.openjdk.org/browse/JDK-8358616">Clarify the requirements and exact the memory ordering in CPU Time Profiler</a>: I used acquire-release semantics for most atomic variables, which is not wrong, just not necessarily optimal from a performance perspective.</li>



<li><a href="https://bugs.openjdk.org/browse/JDK-8358619">Fix interval recomputation in CPU Time Profiler</a> [Edit July: fixed]</li>
</ul>



<p>I have already started work on the last issue and will be looking into the other two soon. Please test the profiler yourself and report all the issues you find.</p>



<h2>The new CPUTimeSample Event</h2>



<p>Where the old profiler had two events <code>jdk.ExecutionSample</code> and <code>jdk.NativeMethodSample</code>The new profiler has only one for simplicity, as it doesn’t treat threads in native and Java differently. As stated before, this event is called <code>jdk.CPUTimeSample</code>.</p>



<p>The event has five different fields:</p>



<ul>
<li><code>stackTrace</code> (nullable): Recorded stack trace</li>



<li><code>eventThread</code>: Sampled thread</li>



<li><code>failed</code> (boolean): Did the sampler fail to walk the stack trace? Implies that <code>stackTrace</code> is <code>null</code></li>



<li><code>samplingPeriod</code>: The actual sampling period, directly computed in the signal handler. More on that next week.</li>



<li><code>biased</code> (boolean): Is this sample safepoint biased (the stacktrace related to the frame at safepoint and not the actual frame when the sampling request has been created, see <a href="https://mostlynerdless.de/blog/2025/05/20/taming-the-bias-unbiased-safepoint-based-stack-walking-in-jfr/">Taming the Bias: Unbiased* Safepoint-Based Stack Walking in JFR</a> for more)</li>
</ul>



<p>You can find the event on the <a href="https://sapmachine.io/jfrevents/25.html#cputimesample">JFR Events Collection</a> page too.</p>



<p>Internally, the profiler uses bounded queues, which might overflow; this can result in lost events. The number of these events is regularly recorded in the form of the <code>jdk.CPUTimeSampleLoss</code> event. The event has two fields:</p>



<ul>
<li><code>lostSamples</code>: Number of samples that have been lost since the last <code>jdk.CPUTimeSampleLoss</code> event</li>



<li><code>eventThread</code>: Thread for which the samples are lost</li>
</ul>



<p>Both events allow a pretty good view of the program’s execution, including a relatively exact view of the CPU time used.</p>



<h2>Configuration of the CPU-time Profiler</h2>



<p>The emission of two events of the current sampler is controlled via the <code>period</code> property. It allows the user to configure the sampling interval. The problem now with the CPU-time profiler is that it might produce too many events depending on the number of hardware threads. This is why the <code>jdk.CPUTimeSample</code> event is controlled via the <code>throttle</code> setting. This setting can be either a sampling interval or an upper bound for the number of emitted events.</p>



<p>When setting an interval directly like “10ms” (as in the <code>default.jfc</code>), then we sample every thread every 10ms of CPU-time. This can at most result in 100 * #[hardware threads] events per second. On a 10 hardware thread machine, this results in at most (when every thread is CPU-bound) 1000 events per second or 12800 on a 128 hardware thread machine.</p>



<p>Setting, on the other hand, <code>throttle</code> to a rate like “500/s” (as in the <code>profile.jfc</code>), limits the number of events per second to a fixed rate. This is implemented by choosing the proper sampling interval in relation to the number of hardware threads. For a rate of “500/s” and a ten hardware thread machine, this would be 20ms. On a 128 hardware thread machine, this would be 0.256.</p>



<p>I have to mention that the issue <a href="https://bugs.openjdk.org/browse/JDK-8358619"><em>Fix interval recomputation in CPU Time Profiler</em></a> is related to the recomputation when the number of hardware threads changes mid-profiling.</p>



<h2>New JFR Views</h2>



<p>In addition to the two new events, there are two new views that you can use via <code>jfr view VIEW_NAME profile.jfr</code>:</p>



<p><code>cpu-time-hot-methods</code> shows you a list of the 25 most executed methods. These are methods that are on top of the stack the most (running the <a href="https://gist.github.com/parttimenerd/d66364f2086089761eb2fec7eda026d7">example</a> with a 1ms throttle):</p>



<pre data-enlighter-language="bash" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">                       Java Methods that Execute the Most from CPU Time Sampler (Experimental)

Method                                                                                                Samples Percent
----------------------------------------------------------------------------------------------------- ------- -------
jdk.jfr.internal.JVM.emitEvent(long, long, long)                                                           35  72.92%
jdk.jfr.internal.event.EventWriter.putStringValue(String)                                                   1   2.08%
jdk.internal.loader.NativeLibraries.load(NativeLibraries$NativeLibraryImpl, String, boolean, boolean)       1   2.08%
jdk.internal.logger.LazyLoggers$LazyLoggerAccessor.platform()                                               1   2.08%
jdk.internal.jimage.ImageStringsReader.unmaskedHashCode(String, int)                                        1   2.08%
sun.net.www.ParseUtil.quote(String, long, long)                                                             1   2.08%
java.net.HttpURLConnection.getResponseCode()                                                                1   2.08%
java.io.BufferedInputStream.read(byte[], int, int)                                                          1   2.08%
java.util.HashMap.hash(Object)                                                                              1   2.08%
sun.nio.ch.NioSocketImpl$1.read(byte[], int, int)                                                           1   2.08%
java.util.Properties.load0(Properties$LineReader)                                                           1   2.08%
java.lang.StringLatin1.regionMatchesCI(byte[], int, byte[], int, int)                                       1   2.08%
java.util.stream.AbstractPipeline.exactOutputSizeIfKnown(Spliterator)                                       1   2.08%
sun.nio.fs.UnixChannelFactory$Flags.toFlags(Set)                                                            1   2.08%</pre>



<p>The second view is <code>cpu-time-statistics</code> which gives you the number of successful samples, failed samples, biased Samples, total samples, and lost samples:</p>



<pre data-enlighter-language="bash" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">CPU Time Sample Statistics
--------------------------
Successful Samples: 48
Failed Samples: 0
Biased Samples: 0
Total Samples: 48
Lost Samples: 14</pre>



<p>All of the lost samples are caused by the sampled Java thread running VM internal code. This view is really helpful when checking whether the profiling contains the whole picture. </p>



<h2>Conclusion</h2>



<p>Getting this new profiler in JDK 25 was a real push, but I think it was worth it. OpenJDK now has a built-in CPU-time profiler that records missed samples. The implementation builds upon JFR’s new cooperative sampling approach, which also got into JDK 25 just days before. CPU-time profiling has many advantages, especially when you’re interested in the code that is actually wasting your CPU.</p>



<p>This is the first of a two-part series on the new profiler. You can expect a deep dive into the implementation of the profiler next week.</p>



<p><em>This blog post is part of my work in the <a href="https://sapmachine.io/">SapMachine</a> team at <a href="https://sap.com/">SAP</a>, making profiling easier for everyone.</em></p>



<p>P.S.: I submitted to a few conferences the talk <em>From Idea to JEP: An OpenJDK Developer’s Journey to Improve Profiling</em> with the following description: <em>Have you ever wondered how profiling, like JFR, works in OpenJDK and how we can improve it? In this talk, I’ll take you on my three-year journey to improve profiling, especially method sampling, with OpenJDK: from the initial ideas and problems of existing approaches to my different draft implementations and JEP versions, with all the setbacks and friends I made along the way. It’s a story of blood, sweat, and C++.</em><br>It has sadly not been accepted yet.</p>

                
                    <!--begin code -->

                    
                    <div data-post_id="1710" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-1710 box-instance-id-1">
                                                                                    
                                                                            <p><span>
                                                                                                                        <ul>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
                                                                                                                    <li>
                                                                                                                                                                                    <div>
                                                                    <p><img alt="Johannes Bechberger" src="https://secure.gravatar.com/avatar/c52eaa3c0c73ed834ae0e48b927edd38573408d48f4c35c898d34f71d958e0c9?s=80&amp;d=blank&amp;r=g" srcset="https://secure.gravatar.com/avatar/c52eaa3c0c73ed834ae0e48b927edd38573408d48f4c35c898d34f71d958e0c9?s=160&amp;d=blank&amp;r=g 2x" height="80" width="80">                                                                                                                                                                                                            </p>
                                                                                                                                    </div>
                                                            
                                                            <div>
                                                                                                                                                                                                                                                                        <p>
                                                                                                                                                    Johannes Bechberger is a JVM developer working on profilers and their underlying technology in the SapMachine team at SAP. This includes improvements to async-profiler and its ecosystem, a website to view the different JFR event types, and improvements to the FirefoxProfiler, making it usable in the Java world. He started at SAP in 2022 after two years of research studies at the KIT in the field of Java security analyses. His work today is comprised of many open-source contributions and his blog, where he writes regularly on in-depth profiling and debugging topics, and of working on his JEP Candidate 435 to add a new profiling API to the OpenJDK.                                                                                                                                                </p>
                                                                                                                                
                                                                                                                                    <p><span>
                                                                        <a href="https://mostlynerdless.de/blog/author/parttimenerd/" title="View all posts">
                                                                            <span>View all posts</span>
                                                                        </a>
                                                                    </span>
                                                                                                                                <a aria-label="Email" href="mailto:me@mostlynerdless.de" target="_self"><span></span> </a><a aria-label="Website" href="https://mostlynerdless.de/" target="_self"><span></span> </a>
                                                                                                                            </p></div>
                                                                                                                                                                                                                        </li>
                                                                                                                                                                                                                                    </ul>
                                                                            </span>
                                                                                                                        </p>
                        </div>
                    <!--end code -->
                    
                
                            
        <div id="tnp-subscription-posts"><p>New posts like these come out at least every two weeks, to get notified about new posts, follow me on <a href="https://bsky.app/profile/mostlynerdless.de">BlueSky</a>, <a href="https://twitter.com/parttimen3rd">Twitter</a>, <a href="https://mastodon.social/@parttimenerd">Mastodon</a>, or <a href="https://www.linkedin.com/in/johannes-bechberger/">LinkedIn</a>, or join the newsletter:</p>
</div>			</div></div>]]></description>
        </item>
    </channel>
</rss>