<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 05 Oct 2024 14:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[It's Time to Stop Taking Sam Altman at His Word (174 pts)]]></title>
            <link>https://www.theatlantic.com/technology/archive/2024/10/sam-altman-mythmaking/680152/</link>
            <guid>41749371</guid>
            <pubDate>Sat, 05 Oct 2024 12:17:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/technology/archive/2024/10/sam-altman-mythmaking/680152/">https://www.theatlantic.com/technology/archive/2024/10/sam-altman-mythmaking/680152/</a>, See on <a href="https://news.ycombinator.com/item?id=41749371">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header data-event-module="hero"><div><div><p>Understand AI for what it is, not what it might become.</p></div><div><figure><div data-flatplan-lead_figure_media="true"><picture><img alt="Photograph of Sam Altman" sizes="(min-width: 976px) 976px, 100vw" srcset="https://cdn.theatlantic.com/thumbor/5jtLZfNIDl-KsJwr6YwXHLCzkXM=/0x0:4800x2700/750x422/media/img/mt/2024/10/HR_1258550648/original.jpg 750w, https://cdn.theatlantic.com/thumbor/2-NVcZYYD5N8WHQFdXM5loRmrfk=/0x0:4800x2700/828x466/media/img/mt/2024/10/HR_1258550648/original.jpg 828w, https://cdn.theatlantic.com/thumbor/7ixFRAfgrl84vzHdnpxCaHDVJpk=/0x0:4800x2700/960x540/media/img/mt/2024/10/HR_1258550648/original.jpg 960w, https://cdn.theatlantic.com/thumbor/ricZNVcgmocI2el_zD6Se8AiX3w=/0x0:4800x2700/976x549/media/img/mt/2024/10/HR_1258550648/original.jpg 976w, https://cdn.theatlantic.com/thumbor/NDekaLljTZvaed6nGxnEFm5oaIE=/0x0:4800x2700/1952x1098/media/img/mt/2024/10/HR_1258550648/original.jpg 1952w" src="https://cdn.theatlantic.com/thumbor/7ixFRAfgrl84vzHdnpxCaHDVJpk=/0x0:4800x2700/960x540/media/img/mt/2024/10/HR_1258550648/original.jpg" id="article-lead-image" width="960" height="540"></picture></div><figcaption data-flatplan-lead_figure_caption="true">SeongJoon Cho / Bloomberg / Getty</figcaption></figure></div></div><div><p><time datetime="2024-10-04T16:57:23Z" data-flatplan-timestamp="true">October 4, 2024, 12:57 PM ET</time> </p></div><gpt-ad format="injector" sizes-at-0="mobile-wide" targeting-pos="injector-article-start" sizes-at-976="desktop-wide"></gpt-ad></header><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true">OpenAI announced this week that it has raised $6.6 billion in new funding and that the company is now valued at $157 billion overall. This is quite a feat for an organization that <a data-event-element="inline link" href="https://www.nytimes.com/2024/09/25/technology/mira-murati-openai.html">reportedly</a> burns through $7 billion a year—far more cash than it brings in—but it makes sense when you realize that OpenAI’s primary product isn’t technology. It’s stories.</p><p data-flatplan-paragraph="true">Case in point: Last week, CEO Sam Altman published an online manifesto titled “<a data-event-element="inline link" href="https://ia.samaltman.com/">The Intelligence Age</a>.” In it, he declares that the AI revolution is on the verge of unleashing boundless prosperity and radically improving human life. “We’ll soon be able to work with AI that helps us accomplish much more than we ever could without AI,” he writes. Altman expects that his technology will fix the climate, help humankind establish space colonies, and discover all of physics. He predicts that we may have an all-powerful superintelligence “in a few thousand days.” All we have to do is feed his technology enough energy, enough data, and enough chips.</p><p data-flatplan-paragraph="true">Maybe someday Altman’s ideas about AI will prove out, but for now, his approach is textbook Silicon Valley mythmaking. In these narratives, humankind is forever on the cusp of a technological breakthrough that will transform society for the better. The hard technical problems have basically been solved—all that’s left now are the details, which will surely be worked out through market competition and old-fashioned entrepreneurship. <em>Spend billions now; make trillions later! </em>This was the story of the dot-com boom in the 1990s, and of nanotechnology in the 2000s. It was the story of cryptocurrency and robotics in the 2010s. The technologies never quite work out like the Altmans of the world promise, but the stories keep regulators and regular people sidelined while the entrepreneurs, engineers, and investors build empires. (<em>The Atlantic</em> recently entered a corporate partnership with OpenAI.)</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/technology/archive/2023/06/ai-regulation-sam-altman-bill-gates/674278/">Read: AI doomerism is a decoy</a></p><p data-flatplan-paragraph="true">Despite the rhetoric, Altman’s products currently feel less like a glimpse of the future and more like the mundane, buggy present. ChatGPT and DALL-E were cutting-edge technology in 2022. People tried the chatbot and image generator for the first time and were astonished. Altman and his ilk spent the following year speaking in stage whispers about the awesome technological force that had just been unleashed upon the world. Prominent AI figures were among the thousands of people who signed an <a data-event-element="inline link" href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">open letter in March 2023</a> to urge a six-month pause in the development of large language models ( LLMs) so that humanity would have time to address the social consequences of the impending revolution. Those six months came and went. OpenAI and its competitors have released other models since then, and although tech wonks have dug into their purported advancements, for most people, the technology appears to have plateaued. <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2023/03/gpt4-release-rumors-hype-future-iterations/673396/">GPT-4</a> now looks less like the precursor to an all-powerful superintelligence and more like … well, any other chatbot.</p><p data-flatplan-paragraph="true">The technology itself seems much smaller once the novelty wears off. You can use a large language model to compose an email or a story—but not a particularly original one. The tools still hallucinate (meaning they confidently assert false information). They still fail in <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2024/05/google-search-ai-overview-health-webmd/678508/">embarrassing and unexpected ways</a>. Meanwhile, the web is filling up with useless “<a data-event-element="inline link" href="https://nymag.com/intelligencer/article/ai-generated-content-internet-online-slop-spam.html">AI slop</a>,” LLM-generated trash that costs practically nothing to produce and generates pennies of advertising revenue for the creator. We’re in a race to the bottom that everyone saw coming and no one is happy with. Meanwhile, the search for product-market fit at a scale that would justify all the inflated tech-company valuations keeps coming up short. Even OpenAI’s latest release, o1, was accompanied by a <a data-event-element="inline link" href="https://x.com/sama/status/1834283100639297910">caveat</a> from Altman that “it still seems more impressive on first use than it does after you spend more time with it.”</p><p data-flatplan-paragraph="true">In Altman’s rendering, this moment in time is just a waypoint, “the doorstep of the next leap in prosperity.” He still argues that the deep-learning technique that powers ChatGPT will effectively be able to solve any problem, at any scale, so long as it has enough energy, enough computational power, and enough data. Many computer scientists are <a data-event-element="inline link" href="https://press.princeton.edu/books/hardcover/9780691249131/ai-snake-oil?srsltid=AfmBOoq0PglTl-ZvYI9vexaTNJouMOR1cqpznKeo_-DOv-KWBWCvQrZu">skeptical of this claim</a>, maintaining that multiple significant scientific breakthroughs stand between us and artificial general intelligence. But Altman projects confidence that his company has it all well in hand, that science fiction will soon become reality. He may need <a data-event-element="inline link" href="https://www.wsj.com/tech/ai/sam-altman-seeks-trillions-of-dollars-to-reshape-business-of-chips-and-ai-89ab3db0">$7 trillion</a> or so to realize his ultimate vision—not to mention <a data-event-element="inline link" href="https://www.bloomberg.com/news/articles/2024-07-18/sam-altman-s-helion-energy-promises-fusion-power-by-2028">unproven fusion-energy technology</a>—but that’s peanuts when compared with all the advances he is promising.</p><p data-flatplan-paragraph="true">There’s just one tiny problem, though: Altman is no physicist. He is a serial entrepreneur, and quite clearly a talented one. He is one of Silicon Valley’s most revered talent scouts. If you look at Altman’s breakthrough successes, they all pretty much revolve around connecting early start-ups with piles of investor cash, not any particular technical innovation.</p><p id="injected-recirculation-link-1" data-view-action="view link - injected link - item 2" data-event-element="injected link" data-event-position="2"><a href="https://www.theatlantic.com/technology/archive/2024/09/sam-altman-openai-for-profit/680031/">Read: OpenAI takes its mask off</a></p><p data-flatplan-paragraph="true">It’s remarkable how similar Altman’s rhetoric sounds to that of <a data-event-element="inline link" href="https://a16z.com/the-techno-optimist-manifesto/">his fellow billionaire techno-optimists</a>. The project of techno-optimism, for decades now, has been to insist that if we just have faith in technological progress and free the inventors and investors from pesky regulations such as <a data-event-element="inline link" href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html">copyright law</a> and <a data-event-element="inline link" href="https://www.ftc.gov/news-events/news/press-releases/2024/09/ftc-announces-crackdown-deceptive-ai-claims-schemes">deceptive marketing</a>, then the marketplace will work its magic and everyone will be better off. Altman has made nice with lawmakers, insisting that artificial intelligence requires responsible regulation. But the company’s response to proposed regulation seems to be “<a data-event-element="inline link" href="https://time.com/6288245/openai-eu-lobbying-ai-act/">no, not like that</a>.” Lord, grant us regulatory clarity—but <a data-event-element="inline link" href="https://www.theverge.com/2024/8/21/24225648/openai-letter-california-ai-safety-bill-sb-1047">not just yet.</a></p><p data-flatplan-paragraph="true">At a high enough level of abstraction, Altman’s entire job is to keep us all fixated on an imagined AI future so we don’t get too caught up in the underwhelming details of the present. Why focus on how AI is being used to <a data-event-element="inline link" href="https://www.theatlantic.com/newsletters/archive/2024/09/ai-is-triggering-a-child-sex-abuse-crisis/680053/">harass and exploit children</a> when you can imagine the ways it will make your life easier? It’s much more pleasant fantasizing about a benevolent future AI, one that fixes the problems wrought by climate change, than dwelling upon the phenomenal <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2024/09/microsoft-ai-oil-contracts/679804/">energy</a> and <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2024/03/ai-water-climate-microsoft/677602/">water consumption</a> of actually existing AI today.</p><p data-flatplan-paragraph="true">Remember, these technologies already have a track record. The world can and should evaluate them, and the people building them, based on their results and their effects, not solely on their supposed potential.</p></section><div data-event-module="footer"><p><h3>About the Author</h3></p><div><address id="article-writer-0" data-event-element="author" data-event-position="1" data-flatplan-bio="true"><div><p><a href="https://www.theatlantic.com/author/david-karpf/" data-label="https://www.theatlantic.com/author/david-karpf/" data-action="click author - name">David Karpf</a> is an associate professor in the School of Media and Public Affairs at the George Washington University.</p></div></address></div></div><gpt-ad format="injector" sizes-at-0="mobile-wide,native,house" targeting-pos="injector-most-popular" sizes-at-976="desktop-wide,native,house"></gpt-ad></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux from Scratch (151 pts)]]></title>
            <link>https://www.linuxfromscratch.org/index.html</link>
            <guid>41747966</guid>
            <pubDate>Sat, 05 Oct 2024 05:43:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.linuxfromscratch.org/index.html">https://www.linuxfromscratch.org/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41747966">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

     <p>Linux From Scratch (LFS) is a project that provides you with
        step-by-step instructions for building your own custom Linux system,
        entirely from source code.
     </p>

     <p>Currently, the Linux From Scratch organization consists of the following
        subprojects:
     </p>

     <ul id="subs">

       <li><a href="https://www.linuxfromscratch.org/lfs/">LFS</a> :: Linux From Scratch is the main book, the
       base from which all other projects are derived.</li>
       
       <li><a href="https://www.linuxfromscratch.org/blfs/">BLFS</a> :: Beyond Linux From Scratch helps you
       extend your finished LFS installation into a more customized and usable
       system.</li>
       
       <li><a href="https://www.linuxfromscratch.org/alfs/">ALFS</a> :: Automated Linux From Scratch provides
       tools for automating and managing LFS and BLFS builds.</li>

       <li><a href="https://www.linuxfromscratch.org/hints/">Hints</a> :: The Hints project is a collection of
       documents that explain how to enhance your LFS system in ways that are
       not included in the LFS or BLFS books.</li>

       <li><a href="https://www.linuxfromscratch.org/patches/">Patches</a> :: The Patches project serves as a
       central repository for all patches useful to an LFS user.</li> 

       <li><a href="https://www.linuxfromscratch.org/lfs/LFS-EDITORS-GUIDE.html">LFS Editor's Guide</a> :: A document that 
       describes the LFS development process.</li> 

       <li><a href="https://www.linuxfromscratch.org/museum/">Museum</a> :: Copies of ancient LFS and BLFS versions.</li> 
     </ul>

       <br>
      
     </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple releases Depth Pro, an AI model that rewrites the rules of 3D vision (107 pts)]]></title>
            <link>https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/</link>
            <guid>41747863</guid>
            <pubDate>Sat, 05 Oct 2024 05:10:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/">https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=41747863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<section>
			
			<p><time title="2024-10-04T18:52:31+00:00" datetime="2024-10-04T18:52:31+00:00">October 4, 2024 11:52 AM</time>
			</p>
			
		</section>
		<div>
					<p><img width="750" height="421" src="https://venturebeat.com/wp-content/uploads/2024/10/nuneybits_Flat_vector_design_of_the_Apple_logo_integrated_into__6d245237-c107-4fb8-b31c-e089bfd3f5f2-1.webp?w=750" alt="Credit: VentureBeat made with Midjourney"></p><p><span>Credit: VentureBeat made with Midjourney</span></p>		</div><!-- .article-media-header -->
	</div><div id="primary">

		<article id="content">
			<div>
				<div id="boilerplate_2682874"><!-- wp:paragraph -->
<p><em>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. <a href="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav" data-type="link" data-id="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav">Learn More</a></em></p>
<!-- /wp:paragraph -->

<!-- wp:separator {"opacity":"css","className":"is-style-wide"} -->
<hr>
<!-- /wp:separator --></div><p><a href="https://machinelearning.apple.com/">Apple’s AI research team</a> has developed a new model that could significantly advance how machines perceive depth, potentially transforming industries ranging from augmented reality to autonomous vehicles.</p>



<p>The system, called&nbsp;<a href="https://arxiv.org/pdf/2410.02073">Depth Pro</a>, is able to generate detailed 3D depth maps from single 2D images in a fraction of a second—without relying on the camera data traditionally needed to make such predictions.</p>



<p>The technology, detailed in a research paper titled&nbsp;<em>“<a href="https://arxiv.org/pdf/2410.02073">Depth Pro: Sharp Monocular Metric Depth in Less Than a Second</a>,”</em>&nbsp;is a major leap forward in the field of monocular depth estimation, a process that uses just one image to infer depth.</p>



<p>This could have far-reaching applications across sectors where real-time spatial awareness is key. The model’s creators, led by Aleksei Bochkovskii and Vladlen Koltun, describe&nbsp;Depth Pro&nbsp;as one of the fastest and most accurate systems of its kind.</p>



<figure><img fetchpriority="high" decoding="async" width="830" height="853" src="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12%E2%80%AFAM.png?w=584" alt="" srcset="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png 830w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=300,308 300w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=768,789 768w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=584,600 584w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=52,52 52w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=400,411 400w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=750,771 750w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=578,594 578w" sizes="(max-width: 830px) 100vw, 830px"><figcaption>A comparison of depth maps from Apple’s Depth Pro, Marigold, Depth Anything v2, and Metric3D v2. Depth Pro excels in capturing fine details like fur and birdcage wires, producing sharp, high-resolution depth maps in just 0.3 seconds, outperforming other models in accuracy and detail. (credit: arxiv.org)</figcaption></figure>







<p>Monocular depth estimation has long been a challenging task, requiring either multiple images or metadata like focal lengths to accurately gauge depth.</p>



<p>But&nbsp;Depth Pro&nbsp;bypasses these requirements, producing high-resolution depth maps in just 0.3 seconds on a standard GPU. The model can create 2.25-megapixel maps with exceptional sharpness, capturing even minute details like hair and vegetation that are often overlooked by other methods.</p>



<p>“These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction,” the researchers explain in their paper. This architecture allows the model to process both the overall context of an image and its finer details simultaneously—an enormous leap from slower, less precise models that came before it.</p>



<figure><img decoding="async" width="846" height="953" src="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18%E2%80%AFAM.png?w=533" alt="" srcset="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png 846w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=300,338 300w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=768,865 768w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=533,600 533w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=400,451 400w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=750,845 750w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=578,651 578w" sizes="(max-width: 846px) 100vw, 846px"><figcaption>A comparison of depth maps from Apple’s Depth Pro, Depth Anything v2, Marigold, and Metric3D v2. Depth Pro excels in capturing fine details like the deer’s fur, windmill blades, and zebra’s stripes, delivering sharp, high-resolution depth maps in 0.3 seconds. (credit: arxiv.org)</figcaption></figure>



<h2 id="h-metric-depth-zero-shot-learning">Metric depth, zero-shot learning</h2>



<p>What truly sets&nbsp;Depth Pro&nbsp;apart is its ability to estimate both relative and absolute depth, a capability called “metric depth.”</p>



<p>This means that the model can provide real-world measurements, which is essential for applications like augmented reality (AR), where virtual objects need to be placed in precise locations within physical spaces.</p>



<p>And&nbsp;Depth Pro&nbsp;doesn’t require extensive training on domain-specific datasets to make accurate predictions—a feature known as “zero-shot learning.” This makes the model highly versatile. It can be applied to a wide range of images, without the need for the camera-specific data usually required in depth estimation models.</p>



<p>“Depth Pro produces metric depth maps with absolute scale on arbitrary images ‘in the wild’ without requiring metadata such as camera intrinsics,” the authors explain. This flexibility opens up a world of possibilities, from enhancing AR experiences to improving autonomous vehicles’ ability to detect and navigate obstacles.</p>



<p>For those curious to experience Depth Pro firsthand, a <a href="https://huggingface.co/spaces/akhaliq/depth-pro">live demo</a> is available on the Hugging Face platform.</p>



<figure><img decoding="async" width="1387" height="375" src="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50%E2%80%AFAM.png?w=800" alt="" srcset="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png 1387w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=300,81 300w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=768,208 768w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=800,216 800w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=400,108 400w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=750,203 750w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=578,156 578w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=930,251 930w" sizes="(max-width: 1387px) 100vw, 1387px"><figcaption>A comparison of depth estimation models across multiple datasets. Apple’s Depth Pro ranks highest overall with an average rank of 2.5, outperforming models like Depth Anything v2 and Metric3D in accuracy across diverse scenarios. (credit: arxiv.org)</figcaption></figure>



<h2 id="h-real-world-applications-from-e-commerce-to-autonomous-vehicles">Real-world applications: From e-commerce to autonomous vehicles</h2>



<p>This versatility has significant implications for various industries. In e-commerce, for example,&nbsp;Depth Pro&nbsp;could allow consumers to see how furniture fits in their home by simply pointing their phone’s camera at the room. In the automotive industry, the ability to generate real-time, high-resolution depth maps from a single camera could improve how self-driving cars perceive their environment, boosting navigation and safety.</p>



<p>“The method should ideally produce metric depth maps in this zero-shot regime to accurately reproduce object shapes, scene layouts, and absolute scales,” the researchers write, emphasizing the model’s potential to reduce the time and cost associated with training more conventional AI models.</p>



<h2 id="h-tackling-the-challenges-of-depth-estimation">Tackling the challenges of depth estimation</h2>



<p>One of the toughest challenges in depth estimation is handling what are known as “flying pixels”—pixels that appear to float in mid-air due to errors in depth mapping.&nbsp;Depth Pro&nbsp;tackles this issue head-on, making it particularly effective for applications like 3D reconstruction and virtual environments, where accuracy is paramount.</p>



<p>Additionally,&nbsp;Depth Pro&nbsp;excels in boundary tracing, outperforming previous models in sharply delineating objects and their edges. The researchers claim it surpasses other systems “by a multiplicative factor in boundary accuracy,” which is key for applications that require precise object segmentation, such as image matting and medical imaging.</p>



<h2 id="h-open-source-and-ready-to-scale">Open-source and ready to scale</h2>



<p>In a move that could accelerate its adoption, Apple has made&nbsp;Depth Pro&nbsp;open-source. The code, along with pre-trained model weights, is <a href="https://github.com/apple/ml-depth-pro" target="_blank" rel="noreferrer noopener">available on GitHub</a>, allowing developers and researchers to experiment with and further refine the technology. The repository includes everything from the model’s architecture to pretrained checkpoints, making it easy for others to build on Apple’s work.</p>



<p>The research team is also encouraging further exploration of&nbsp;Depth Pro’s potential in fields like robotics, manufacturing, and healthcare. “We release code and weights at&nbsp;<a href="https://github.com/apple/ml-depth-pro" target="_blank" rel="noreferrer noopener">https://github.com/apple/ml-depth-pro</a>,”&nbsp;the authors write, signaling this as just the beginning for the model.</p>



<h2 id="h-what-s-next-for-ai-depth-perception">What’s next for AI depth perception</h2>



<p>As artificial intelligence continues to push the boundaries of what’s possible,&nbsp;<em>Depth Pro</em>&nbsp;sets a new standard in speed and accuracy for monocular depth estimation. Its ability to generate high-quality, real-time depth maps from a single image could have wide-ranging effects across industries that rely on spatial awareness.</p>



<p>In a world where AI is increasingly central to decision-making and product development,&nbsp;<em>Depth Pro</em>&nbsp;exemplifies how cutting-edge research can translate into practical, real-world solutions. Whether it’s improving how machines perceive their surroundings or enhancing consumer experiences, the potential uses for&nbsp;<em>Depth Pro</em>&nbsp;are broad and varied.</p>



<p>As the researchers conclude, “Depth Pro dramatically outperforms all prior work in sharp delineation of object boundaries, including fine structures such as hair, fur, and vegetation.” With its open-source release,&nbsp;<em>Depth Pro</em>&nbsp;could soon become integral to industries ranging from autonomous driving to augmented reality—transforming how machines and people interact with 3D environments.</p>
<div id="boilerplate_2660155">
				<p><strong>VB Daily</strong></p>
				<p>Stay in the know! Get the latest news in your inbox daily</p>
				
				<p>By subscribing, you agree to VentureBeat's <a href="https://venturebeat.com/terms-of-service/">Terms of Service.</a></p>
				<p id="boilerplateNewsletterConfirmation">
					Thanks for subscribing. Check out more <a href="https://venturebeat.com/newsletters/">VB newsletters here</a>.
				</p>
				<p>An error occured.</p>
			</div>			</div><!-- .article-content -->

							
			
		</article><!-- #content .article-wrapper -->

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't squander public trust on bullshit (237 pts)]]></title>
            <link>https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit</link>
            <guid>41746180</guid>
            <pubDate>Fri, 04 Oct 2024 22:42:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit">https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit</a>, See on <a href="https://news.ycombinator.com/item?id=41746180">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>At 4.50am local time today, this statewide emergency alert was sent out to every cellphone in Texas:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg" width="1201" height="410" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:410,&quot;width&quot;:1201,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123540,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>I don’t know who Seth Altman is, nor do I care. Why? Because Seth Altman’s offense took place in Lubbock, Texas. I live in Austin, Texas. Four hundred miles away. What I do care about however is the misuse of emergency alert systems and public trust.</p><p><span>Sending out a screeching alert to 30million+ people over 250 million square miles in the middle of the night should only be used in the absolute DIREST OF CIRCUMSTANCES… circumstances like “Texas is under threat from hurricane/chemical leak/nuclear weapons, seek shelter now!” It should </span><em>never</em><span> be used for something that’s utterly irrelevant to 99.99% of people. </span></p><p>Why? Because the public’s trust in government emergency protocols is already hanging by a thread, and in order for those protocols to work when we really need them, they will need to be received and listened to. Instead, Texans by the hundreds of thousands are now turning off their phone’s emergency alerts, possibly forever. Why would anyone with a life to lead leave them on and risk getting their sleep disrupted over personally inconsequential events hundreds of miles away? </p><p>Such an outcome could be truly dangerous for Texas in the long run. If and when a real major emergency strikes, we will no longer have this important tool of public awareness or coordination. And that’s just the second order effects! There are likely going to be some excess deaths today as a direct result—there are 30m+ people in Texas, many of whom are in weak cardiovascular health. I would not be surprised at all if hospitals report an spike in cardiovascular events today. Not to mention an increase in road accidents; Texas is a notoriously driving-heavy state, and few things are worse for safe driving than sleep impairment.</p><p>So I hope the local government takes a long hard look at its alert-pressing finger. We all know the lesson of the Boy Who Cried Wolf, exhausting his village with his over-zealous cries. Well this time the village is thirty million people. Heads need to roll.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs, Theory of Mind, and Cheryl's Birthday (182 pts)]]></title>
            <link>https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb</link>
            <guid>41745788</guid>
            <pubDate>Fri, 04 Oct 2024 21:34:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb">https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb</a>, See on <a href="https://news.ycombinator.com/item?id=41745788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_product_navbar&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>

                  <li>
      
      
</li>

                  <li>
      
      <div>
                <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;white_papers_ebooks_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;white_papers_ebooks_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      White papers, Ebooks, Webinars

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:norvig/pytudes" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="PE5AWI3p_iSziKdZC8ErZLcqeumyHJV5L9EaTYKLjdtd6HPcmwaEpev76mcRxZhzajMzDiv0gUxEzL3nTiTiKQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="norvig/pytudes" data-current-org="" data-current-owner="norvig" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=norvig%2Fpytudes" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="21d4ffb04cbc87fb443820ed00ec47c8a2db97dff8dd678de0053e74fc7c83f8" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>
          
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Max Schrems wins privacy case against Meta over data on sexual orientation (128 pts)]]></title>
            <link>https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86</link>
            <guid>41745181</guid>
            <pubDate>Fri, 04 Oct 2024 20:17:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86">https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86</a>, See on <a href="https://news.ycombinator.com/item?id=41745181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>LONDON (AP) — The European Union’s top court said Friday that social media company Meta can’t use public information about a user’s sexual orientation obtained outside its platforms for personalized advertising under the bloc’s strict data privacy rules. </p><p>The decision from the Court of Justice of the European Union in Luxembourg is a <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/meta-facebook-instagram-whatsapp-ai-artificial-intelligence-8d6cb3424ee410c641d0acdb154601f5">victory for Austrian privacy activist Max Schrems</a></span>, who has been a thorn in the side of Big Tech companies over their compliance with 27-nation bloc’s data privacy rules. </p><p>The EU court issued its ruling after Austria’s supreme court asked for guidance in Schrems’ case on how to apply the privacy rules, known as the General Data Protection Regulation, or GDPR. </p><p>Schrems had complained that Facebook had processed personal data including information about his sexual orientation to target him with online advertising, even though he had never disclosed on his account that he was gay. The only time he had publicly revealed this fact was during a panel discussion. </p>
    

<p>“An online social network such as Facebook cannot use all of the personal data obtained for the purposes of targeted advertising, without restriction as to time and without distinction as to type of data,” the court said in a press release summarizing its decision. </p>



<p>Even though Schrems revealed he was gay in the panel discussion, that “does not authorise the operator of an online social network platform to process other data relating to his sexual orientation, obtained, as the case may be, outside that platform, with a view to aggregating and analysing those data, in order to offer him personalised advertising.” </p>
    
<p>Meta said it was awaiting publication of the court’s full judgment and that it “takes privacy very seriously.”</p><p>“Everyone using Facebook has access to a wide range of settings and tools that allow people to manage how we use their information,” the company said in a statement. </p>
    

<p>Schrems’ lawyer, Katharina Raabe-Stuppnig, lawyer representing Mr Schrems, welcomed the court’s decision. </p><p>“Meta has basically been building a huge data pool on users for 20 years now, and it is growing every day. However, EU law requires ‘data minimisation’,” she said in a statement. “Following this ruling only a small part of Meta’s data pool will be allowed to be used for advertising — even when users consent to ads.” </p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How were 70s versions of games like Pong built without a programmable computer? (193 pts)]]></title>
            <link>https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra</link>
            <guid>41745032</guid>
            <pubDate>Fri, 04 Oct 2024 19:57:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra">https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra</a>, See on <a href="https://news.ycombinator.com/item?id=41745032">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>They were made by mostly avoiding 'computing' concepts altogether, and treating it more like a mechanical thing.</p>
<p>For example with Pong a major component is usually timers - every xth of a second the timer will emit a signal. You have timers calibrated to match the horizontal refresh of the screen, so they'll 'ring' at the same point on each scanline. Then you have timers calibrated to the vertical refresh, so they'll ring on the same scanline each frame.</p>
<p>The ball is then just two discrete timers for vertical and horizontal position, and their rings are sent through an AND gate that will raise the voltage going to the display when both are ringing causing a white dot to appear. The paddles build on this concept with a medium length timer that can be started and stopped to define the length.</p>
<p>To move the ball or paddles the timers can be advanced or delayed by a control signal. Or more accurately the timers are always paused at a certain point, like during half of the horizontal blanking period. This pausing is then shortened once to advance the timer (move left/up) or increased once to delay it (move right/down).</p>
<p>Since both the paddle and the ball timers are emitting a '1' when they are to be displayed you can impliment collision detection by performing another AND operation. So if both a paddle and the ball are being drawn at the same moment you know they've collided and can adjust a latch controlling the ball direction accordingly.</p>
<p>If you get into the Atari 2600 you'll find that it's really weird compared to other consoles (sprites with no clearly defined X coordinate, instead only the ability to place it at the actual current location of the CRT beam or nudge it a small amount either way), but that it starts to make a lot of sense when you realize they were implementing their Pong logic for a programmable chip.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mitmproxy 11: Full HTTP/3 Support (269 pts)]]></title>
            <link>https://mitmproxy.org/posts/releases/mitmproxy-11/</link>
            <guid>41744434</guid>
            <pubDate>Fri, 04 Oct 2024 18:52:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitmproxy.org/posts/releases/mitmproxy-11/">https://mitmproxy.org/posts/releases/mitmproxy-11/</a>, See on <a href="https://news.ycombinator.com/item?id=41744434">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>We are excited to announce the release of mitmproxy 11, which introduces full support for HTTP/3 in both transparent
and reverse proxy modes. We’re also bringing in a ton of DNS improvements that we’ll cover in this blog post.</p>
<h5 id="editorial-note"><em>Editorial Note:</em></h5>
<p><em>Hi! I’m <a href="https://mitmproxy.org/authors/gaurav-jain/">Gaurav Jain</a>, one of the students selected for this year’s Google Summer of Code program to work on mitmproxy.
During this summer, I’ve worked on improving various low-level networking parts of mitmproxy some of which include
HTTP/3 and DNS. You can find my project report <a href="https://gist.github.com/errorxyz/af6f26549e9122f3ff3b93fd9d257df1">here</a>.</em></p>
<h2 id="http3">HTTP/3</h2>
<p>HTTP/3 now “just works” for reverse proxies. Your mitmproxy instance will listen for
both TCP and UDP packets and handle all HTTP versions thrown at it:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmproxy --mode reverse:https://http3.is
</span></span></code></pre></div><p>Our transparent proxy modes now all support HTTP/3 as well:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmproxy --mode wireguard
</span></span><span><span>$ mitmproxy --mode local
</span></span><span><span>$ mitmproxy --mode transparent
</span></span></code></pre></div><p>We have successfully tested HTTP/3 support with Firefox, Chrome, various cURL builds, and other clients to iron out
compatibility issues.
The only major limitation we are aware of at this time is that Chrome <a href="https://issues.chromium.org/issues/40138351#comment15">does not trust user-added Certificate Authorities for QUIC</a>.
This means you will either need to provide a publicly trusted certificate (e.g. from Let’s Encrypt), start Chrome with
a <a href="https://www.chromium.org/quic/playing-with-quic/#generate-certificates">command line switch</a>, or accept that it falls back to HTTP/2. Alternatively, Firefox doesn’t do such shenanigans.
For more HTTP/3 troubleshooting tips, you can check out <a href="https://github.com/mitmproxy/mitmproxy/issues/7025#issuecomment-2351138170">#7025</a>.</p>
<p>Bringing HTTP/3 support to mitmproxy is a major effort that was started in 2022 by <a href="https://mitmproxy.org/authors/manuel-meitinger/">Manuel Meitinger</a> and <a href="https://mitmproxy.org/authors/maximilian-hils/">Maximilian Hils</a>.
QUIC and HTTP/3 make up an increasing share of network traffic in the wild, and we’re super excited to have this ready
and enabled by default now!</p>
<h2 id="improved-dns-support">Improved DNS Support</h2>
<p>With the advent of DNS <a href="https://blog.cloudflare.com/speeding-up-https-and-http-3-negotiation-with-dns/">HTTPS records</a> and new privacy enhancements such as <a href="https://en.wikipedia.org/wiki/Server_Name_Indication#Encrypted_Client_Hello">Encrypted Client Hello (ECH)</a>, mitmproxy’s DNS
functionality is becoming increasingly important. We’re happy to share multiple advancements on this front:</p>
<h4 id="support-for-query-types-beyond-aaaaa">Support for Query Types Beyond A/AAAA</h4>
<p>mitmproxy’s old DNS implementation used <code>getaddrinfo</code> to resolve queries. This is convenient because everything is taken
care of by libc, but the <code>getaddrinfo</code> API only supports A/AAAA queries for IPv4 and IPv6 addresses. It doesn’t allow us
to answer queries for e.g. <a href="https://blog.cloudflare.com/speeding-up-https-and-http-3-negotiation-with-dns/">HTTPS records</a>, which are used to signal HTTP/3 support.</p>
<p>To overcome this limitation, we’ve reimplemented our DNS support on top of <a href="https://github.com/hickory-dns/hickory-dns">Hickory&nbsp;DNS</a>, a Rust-based DNS library.
Using Hickory, we now obtain the operating system’s default nameservers on Windows, Linux, and macOS and forward
non-A/AAAA queries there. This behavior can also be customized with the new <a href="https://docs.mitmproxy.org/stable/concepts-options/#dns_name_servers"><code>dns_name_servers</code> option</a>:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmdump --mode dns --set dns_name_servers<span>=</span>8.8.8.8
</span></span></code></pre></div><p><img src="https://mitmproxy.org/posts/releases/mitmproxy-11/dns.png" alt="dns"></p>
<h4 id="skipping-etchosts">Skipping /etc/hosts</h4>
<p>By switching to Hickory, we now also have the option to ignore the system’s hosts
file (<code>/etc/hosts</code> on Linux) with the new <a href="https://docs.mitmproxy.org/stable/concepts-options/#dns_use_hosts_file"><code>dns_use_hosts_file</code> option</a>. We plan to move mitmproxy’s internal
DNS resolution to Hickory as well, at which point this feature will become incredibly useful in allowing transparent
redirection on the same machine for specific domains. At the moment, such a setup would cause mitmproxy to recursively
connect to itself because we always take the hosts file into account.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ echo <span>"192.0.2.1 mitmproxy.org"</span> &gt;&gt; /etc/hosts
</span></span><span><span>
</span></span><span><span>$ mitmdump --mode dns
</span></span><span><span>$ dig @127.0.0.1 +short mitmproxy.org
</span></span><span><span>192.0.2.1
</span></span><span><span>
</span></span><span><span>$ mitmdump --mode dns --set dns_use_hosts_file<span>=</span>false
</span></span><span><span>$ dig @127.0.0.1 +short mitmproxy.org
</span></span><span><span>3.161.82.13
</span></span></code></pre></div><h4 id="support-for-dns-over-tcp">Support for DNS-over-TCP</h4>
<p>DNS uses UDP by default, but may also use TCP to support records that do not fit into a single UDP packet. mitmproxy has
previously gotten away with only supporting UDP, but now that we support arbitrary query types, message size and thus
TCP support is more important. Long story short, DNS-over-TCP works with mitmproxy 11!</p>
<h4 id="stripping-encrypted-client-hello-ech-keys">Stripping Encrypted Client Hello (ECH) Keys</h4>
<p>Unless a custom certificate is configured, mitmproxy uses the Server Name Indication (SNI) transmitted in the TLS
ClientHello to construct a valid certificate. Conversely, if no SNI is present, we may not be able
to generate a certificate that is trusted by the client.</p>
<p><a href="https://en.wikipedia.org/wiki/Server_Name_Indication#Encrypted_Client_Hello">Encrypted Client Hello (ECH)</a> is an exciting new technology to increase privacy on the web. In short, the client uses
the new DNS HTTPS records to obtain an ECH key before establishing a connection, and then already encrypts the initial
ClientHello handshake message with that key. If both DNS queries and handshake are encrypted, passive intermediaries
cannot learn the target domain, only the target IP address (which is not conclusive for shared hosting and Content Delivery
Networks). This is a great advancement for privacy, but also breaks mitmproxy’s way of generating certificates.
To fix this, mitmproxy now strips ECH keys from HTTPS records. This way the client has no keys to encrypt the initial
handshake message with, and mitmproxy still learns the target domain and can construct a matching certificate.</p>
<p>Of course, ECH adds complexity for us and sometimes makes mitmproxy harder to use for our users. Nonetheless, we are
excited to see these privacy advancements being made for the rest of the web!</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>This work supported by <a href="https://summerofcode.withgoogle.com/">Google Summer of Code</a> under the umbrella of the <a href="https://www.honeynet.org/">Honeynet&nbsp;Project</a>, and the
<a href="https://nlnet.nl/entrust/">NGI0 Entrust fund</a> established by <a href="https://nlnet.nl/">NLnet</a>. Thank you to my mentor <a href="https://mitmproxy.org/authors/maximilian-hils/">Maximilian Hils</a> for the
invaluable guidance and support.</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: FFmpeg-over-IP – Connect to remote FFmpeg servers (129 pts)]]></title>
            <link>https://github.com/steelbrain/ffmpeg-over-ip</link>
            <guid>41743780</guid>
            <pubDate>Fri, 04 Oct 2024 17:49:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/steelbrain/ffmpeg-over-ip">https://github.com/steelbrain/ffmpeg-over-ip</a>, See on <a href="https://news.ycombinator.com/item?id=41743780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">ffmpeg over IP</h2><a id="user-content-ffmpeg-over-ip" aria-label="Permalink: ffmpeg over IP" href="#ffmpeg-over-ip"></a></p>
<p dir="auto">Connect to remote ffmpeg servers. Are you tired of unsuccessfully trying to pass your GPU through to a docker
container running in a VM? So was I! <code>ffmpeg-over-ip</code> allows you to run an ffmpeg server on a machine with access
to a GPU (Linux, Windows, or Mac) and connect to it from a remote machine. The only thing you need is Node.js
installed and a shared filesystem (could be NFS, SMB, etc.) between the two machines.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><code>ffmpeg-over-ip</code> consists of two main parts, the server and the client. Both are packed neatly into single JS
files. You can download these from the <a href="https://www.npmjs.com/package/ffmpeg-over-ip?activeTab=code" rel="nofollow">npm interface</a> or by <code>npm install ffmpeg-over-ip</code> and then copying
them to the relevant places. You don't need any <code>node_modules</code> to run the server or the client.</p>
<p dir="auto">The javascript files require Node.js runtime to work. If you want standalone files that you can mount in a docker
container, you can find these in the <a href="https://github.com/steelbrain/ffmpeg-over-ip/releases">Github Releases</a>. On the releases page, you may have to click <strong>"Show all
assets"</strong> to see the files.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">The server and the client are both configured using JSONC (JSON with comments) configuration files. The paths
of these files can be flexible. To identify which paths are being used, you can invoke either with <code>--debug-print-search-paths</code>.</p>
<p dir="auto">Template/example configuration files are provided in this repository for your convinience. Unless the server and the client
share the same filesystem, you may have to specify <code>rewrites</code> in the server configuration file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Both the server and the client files are executable, so long as there is a Node.js installation available. If you intend
to use this in a docker container, you can directly mount the client file to where the container would expect a regular
ffmpeg executable to be, ie <code>docker run -v ./path/to/client-bin:/usr/lib/jellyfin-ffmpeg/ffmpeg ...</code>.</p>
<p dir="auto">The server and the client communicate commands over HTTP, so make sure that whatever port you specify on the server is
allowed through the firewall.</p>
<p dir="auto">Assuming you <strong>download one of the release files</strong>, here's what the usage would look like</p>
<p dir="auto">On the client side</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./ffmpeg-over-ip-client --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.client.jsonc # Change the stuff you want
$ ./ffmpeg-over-ip-client <use like ffmpeg, add ffmpeg args here>"><pre>$ ./ffmpeg-over-ip-client --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.client.jsonc <span><span>#</span> Change the stuff you want</span>
$ ./ffmpeg-over-ip-client <span>&lt;</span>use like ffmpeg, add ffmpeg args here<span>&gt;</span></pre></div>
<p dir="auto">On the server side</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./ffmpeg-over-ip-server --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.server.jsonc # Change the stuff you want, especially the rewrites
$ ./ffmpeg-over-ip-server"><pre>$ ./ffmpeg-over-ip-server --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.server.jsonc <span><span>#</span> Change the stuff you want, especially the rewrites</span>
$ ./ffmpeg-over-ip-server</pre></div>
<p dir="auto">Assuming you want to <strong>download these from npm</strong>, here's how you would do it</p>
<p dir="auto">On the client side:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-client --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.client.jsonc # Change the stuff you want
$ ./node_modules/.bin/ffmpeg-over-ip-client <use like ffmpeg, add ffmpeg args here>"><pre>$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-client --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.client.jsonc <span><span>#</span> Change the stuff you want</span>
$ ./node_modules/.bin/ffmpeg-over-ip-client <span>&lt;</span>use like ffmpeg, add ffmpeg args here<span>&gt;</span></pre></div>
<p dir="auto">On the server side:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-server --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.server.jsonc # Change the stuff you want, especially the rewrites
$ ./node_modules/.bin/ffmpeg-over-ip-server"><pre>$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-server --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.server.jsonc <span><span>#</span> Change the stuff you want, especially the rewrites</span>
$ ./node_modules/.bin/ffmpeg-over-ip-server</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The contents of this project are licensed under the terms of the MIT License.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Open source framework OpenAI uses for Advanced Voice (189 pts)]]></title>
            <link>https://github.com/livekit/agents</link>
            <guid>41743327</guid>
            <pubDate>Fri, 04 Oct 2024 17:01:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/livekit/agents">https://github.com/livekit/agents</a>, See on <a href="https://news.ycombinator.com/item?id=41743327">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://github.com/livekit/agents/raw/main/.github/banner_dark.png">
  <source media="(prefers-color-scheme: light)" srcset="https://github.com/livekit/agents/raw/main/.github/banner_light.png">
  <img alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png">
</picture></themed-picture>


<p>
Looking for the JS/TS library? Check out <a href="https://github.com/livekit/agents-js">AgentsJS</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ [NEW] OpenAI Realtime API support</h2><a id="user-content--new-openai-realtime-api-support" aria-label="Permalink: ✨ [NEW] OpenAI Realtime API support" href="#-new-openai-realtime-api-support"></a></p>
<p dir="auto">We're partnering with OpenAI on a new <code>MultimodalAgent</code> API in the Agents framework. This class completely wraps OpenAI’s Realtime API, abstract away the raw wire protocol, and provide an ultra-low latency WebRTC transport between GPT-4o and your users’ devices. This same stack powers Advanced Voice in the ChatGPT app.</p>
<ul dir="auto">
<li>Try the Realtime API in our <a href="https://playground.livekit.io/" rel="nofollow">playground</a> [<a href="https://github.com/livekit-examples/realtime-playground">code</a>]</li>
<li>Check out our <a href="https://docs.livekit.io/agents/openai" rel="nofollow">guide</a> to building your first app with this new API</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is Agents?</h2><a id="user-content-what-is-agents" aria-label="Permalink: What is Agents?" href="#what-is-agents"></a></p>
<p dir="auto">The Agents framework allows you to build AI-driven server programs that can see, hear, and speak in realtime. Your agent connects with end user devices through a LiveKit session. During that session, your agent can process text, audio, images, or video streaming from a user's device, and have an AI model generate any combination of those same modalities as output, and stream them back to the user.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Plugins for popular LLMs, transcription and text-to-speech services, and RAG databases</li>
<li>High-level abstractions for building voice agents or assistants with automatic turn detection, interruption handling, function calling, and transcriptions</li>
<li>Compatible with LiveKit's <a href="https://github.com/livekit/sip">telephony stack</a>, allowing your agent to make calls to or receive calls from phones</li>
<li>Integrated load balancing system that manages pools of agents with edge-based dispatch, monitoring, and transparent failover</li>
<li>Running your agents is identical across localhost, <a href="https://github.com/livekit/livekit">self-hosted</a>, and <a href="https://cloud.livekit.io/" rel="nofollow">LiveKit Cloud</a> environments</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To install the core Agents library:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install livekit-agents"><pre>pip install livekit-agents</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Plugins</h2><a id="user-content-plugins" aria-label="Permalink: Plugins" href="#plugins"></a></p>
<p dir="auto">The framework includes a variety of plugins that make it easy to process streaming input or generate output. For example, there are plugins for converting text-to-speech or running inference with popular LLMs. Here's how you can install a plugin:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install livekit-plugins-openai"><pre>pip install livekit-plugins-openai</pre></div>
<p dir="auto">The following plugins are available today:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Plugin</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-anthropic/" rel="nofollow">livekit-plugins-anthropic</a></td>
<td>LLM</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-azure/" rel="nofollow">livekit-plugins-azure</a></td>
<td>STT, TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-deepgram/" rel="nofollow">livekit-plugins-deepgram</a></td>
<td>STT</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-cartesia/" rel="nofollow">livekit-plugins-cartesia</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-elevenlabs/" rel="nofollow">livekit-plugins-elevenlabs</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-playht/" rel="nofollow">livekit-plugins-playht</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-google/" rel="nofollow">livekit-plugins-google</a></td>
<td>STT, TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-nltk/" rel="nofollow">livekit-plugins-nltk</a></td>
<td>Utilities for working with text</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-rag/" rel="nofollow">livekit-plugins-rag</a></td>
<td>Utilities for performing RAG</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-openai/" rel="nofollow">livekit-plugins-openai</a></td>
<td>LLM, STT, TTS, Assistants API, Realtime API</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-silero/" rel="nofollow">livekit-plugins-silero</a></td>
<td>VAD</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation and guides</h2><a id="user-content-documentation-and-guides" aria-label="Permalink: Documentation and guides" href="#documentation-and-guides"></a></p>
<p dir="auto">Documentation on the framework and how to use it can be found <a href="https://docs.livekit.io/agents" rel="nofollow">here</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example agents</h2><a id="user-content-example-agents" aria-label="Permalink: Example agents" href="#example-agents"></a></p>
<ul dir="auto">
<li>A basic voice agent using a pipeline of STT, LLM, and TTS [<a href="https://kitt.livekit.io/" rel="nofollow">demo</a> | <a href="https://github.com/livekit/agents/blob/main/examples/voice-pipeline-agent/minimal_assistant.py">code</a>]</li>
<li>Voice agent using the new OpenAI Realtime API [<a href="https://playground.livekit.io/" rel="nofollow">demo</a> | <a href="https://github.com/livekit-examples/realtime-playground">code</a>]</li>
<li>Super fast voice agent using Cerebras hosted Llama 3.1 [<a href="https://cerebras.vercel.app/" rel="nofollow">demo</a> | <a href="https://github.com/dsa/fast-voice-assistant/">code</a>]</li>
<li>Voice agent using Cartesia's Sonic model [<a href="https://cartesia-assistant.vercel.app/" rel="nofollow">demo</a>]</li>
<li>Agent that looks up the current weather via function call [<a href="https://github.com/livekit/agents/blob/main/examples/voice-pipeline-agent/function_calling_weather.py">code</a>]</li>
<li>Voice agent that performs a RAG-based lookup [<a href="https://github.com/livekit/agents/tree/main/examples/voice-pipeline-agent/simple-rag">code</a>]</li>
<li>Video agent that publishes a stream of RGB frames [<a href="https://github.com/livekit/agents/tree/main/examples/simple-color">code</a>]</li>
<li>Transcription agent that generates text captions from a user's speech [<a href="https://github.com/livekit/agents/tree/main/examples/speech-to-text">code</a>]</li>
<li>A chat agent you can text who will respond back with genereated speech [<a href="https://github.com/livekit/agents/tree/main/examples/text-to-speech">code</a>]</li>
<li>Localhost multi-agent conference call [<a href="https://github.com/dsa/multi-agent-meeting">code</a>]</li>
<li>Moderation agent that uses Hive to detect spam/abusive video [<a href="https://github.com/dsa/livekit-agents/tree/main/hive-moderation-agent">code</a>]</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's <a href="https://livekit.io/join-slack" rel="nofollow">Slack community</a>.</p>

<markdown-accessiblity-table><table>
<thead><tr><th colspan="2">LiveKit Ecosystem</th></tr></thead>
<tbody>
<tr><td>Realtime SDKs</td><td><a href="https://github.com/livekit/client-sdk-js">Browser</a> · <a href="https://github.com/livekit/client-sdk-swift">iOS/macOS/visionOS</a> · <a href="https://github.com/livekit/client-sdk-android">Android</a> · <a href="https://github.com/livekit/client-sdk-flutter">Flutter</a> · <a href="https://github.com/livekit/client-sdk-react-native">React Native</a> · <a href="https://github.com/livekit/rust-sdks">Rust</a> · <a href="https://github.com/livekit/node-sdks">Node.js</a> · <a href="https://github.com/livekit/python-sdks">Python</a> · <a href="https://github.com/livekit/client-sdk-unity">Unity</a> · <a href="https://github.com/livekit/client-sdk-unity-web">Unity (WebGL)</a></td></tr><tr></tr>
<tr><td>Server APIs</td><td><a href="https://github.com/livekit/node-sdks">Node.js</a> · <a href="https://github.com/livekit/server-sdk-go">Golang</a> · <a href="https://github.com/livekit/server-sdk-ruby">Ruby</a> · <a href="https://github.com/livekit/server-sdk-kotlin">Java/Kotlin</a> · <a href="https://github.com/livekit/python-sdks">Python</a> · <a href="https://github.com/livekit/rust-sdks">Rust</a> · <a href="https://github.com/agence104/livekit-server-sdk-php">PHP (community)</a></td></tr><tr></tr>
<tr><td>UI Components</td><td><a href="https://github.com/livekit/components-js">React</a> · <a href="https://github.com/livekit/components-android">Android Compose</a> · <a href="https://github.com/livekit/components-swift">SwiftUI</a></td></tr><tr></tr>
<tr><td>Agents Frameworks</td><td><b>Python</b> · <a href="https://github.com/livekit/agents-js">Node.js</a> · <a href="https://github.com/livekit/agent-playground">Playground</a></td></tr><tr></tr>
<tr><td>Services</td><td><a href="https://github.com/livekit/livekit">LiveKit server</a> · <a href="https://github.com/livekit/egress">Egress</a> · <a href="https://github.com/livekit/ingress">Ingress</a> · <a href="https://github.com/livekit/sip">SIP</a></td></tr><tr></tr>
<tr><td>Resources</td><td><a href="https://docs.livekit.io/" rel="nofollow">Docs</a> · <a href="https://github.com/livekit-examples">Example apps</a> · <a href="https://livekit.io/cloud" rel="nofollow">Cloud</a> · <a href="https://docs.livekit.io/home/self-hosting/deployment" rel="nofollow">Self-hosting</a> · <a href="https://github.com/livekit/livekit-cli">CLI</a></td></tr>
</tbody>
</table></markdown-accessiblity-table>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[12 Months of Mandarin (421 pts)]]></title>
            <link>https://isaak.net/mandarin/</link>
            <guid>41742432</guid>
            <pubDate>Fri, 04 Oct 2024 15:28:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://isaak.net/mandarin/">https://isaak.net/mandarin/</a>, See on <a href="https://news.ycombinator.com/item?id=41742432">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <p>Estimates for achieving intermediate fluency in Mandarin Chinese range up to spending years and around 4000 total hours (2,200h classroom hours, 1,800 outside). I did it in 1500 hours total and less than a year.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>There is a lot of disagreement on language proficiency estimates. They are unreliable and inaccurate. My rough best estimates:<br>
Mean: <strong>2,200h classroom + ~1800h outside -&gt; 50% pass ILR 3 -&gt; true average level is ~ILR 2+</strong><br>
My journey: <strong>150h classroom + 1350h outside -&gt; 50/50 shot at passing ILR 3 exam -&gt; true average level ILR 2+</strong><br>
Detailed walkthrough: One of our best sources, the US Department of State, sadly has a lot of content under NDAs, including exams. That said, they <a href="https://www.state.gov/foreign-language-training/?ref=isaak.net">estimate</a> that reaching "General Professional Fluency" (<a href="https://www.govtilr.org/Skills/ILRscale2.htm?ref=isaak.net">ILR 3</a>) takes 2,200 <em>classroom</em> hours over 88 weeks. This is 2,200/88 = 25 hours a week. Diplomats describe studying a language at the DOS as "the hardest thing you will do in your life". <em>Classroom</em> hours do not include the heavy homework and content consumption. For them, this is not a 25h part-time breeze. It is life.<br>
What the program sounds like, it takes more like 45h a week for 88 weeks (3096h). Add some private language practice, travelling, and content consumption, and studying for the actual exam (which seems brutal). Now, getting to ILR3 fluency seems like taking <strong>well above 4000 hours</strong>, all included.<br>
This is being good on paper. These kinds of language proficiency levels tend to fall short in real-life. In fact, they fall short even on paper: Only some <a href="https://www.reddit.com/r/languagelearning/comments/rd19bj/success_rates_in_2011_and_2012_of_the_fsi_at/?ref=isaak.net">50% pass</a> the ILR3 test after the 88 weeks. In additio, tests are a mediocre proxy for actual communication ability. My sense from reading online is that the diplomats at ILR3 on paper have only truly mastered something like ILR2.<br>
Similarly, I spent <strong>some 1,500 hours total</strong>, including everything like classroom hours, tutoring hours, content consumption, and conversations. On paper, I would guess I am at ILR3, but that is probably an overestimate. So call it ILR2+.<br>
To be fair, also take generously take some time off that 4000-hour estimate: Call it usually takes <strong>3000 hours to get to a similarly strong</strong> ILR2+ level / on-paper ILR3. Hence the 3000h vs 1500h comparison.<br>
This is my rough outline of the core estimate. I do not really care enough to get into more detail or studying for passing tests. I just enjoy learning the language and using it. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>Over the last 365 days, I studied Mandarin for fun. With anki, tutors, and traveling accelerating my learning, I ended up getting to the level of comfortable conversational fluency. My Mandarin isn't perfect nor perfectly fluent, but I can now handle everything up to technical conversations in the area of my PhD.&nbsp;</p><p>For serious language learners, I also jotted down a longer list of methodds: <a href="https://isaak.net/mandarinmethods/">isaak.net/mandarinmethods</a></p><h2 id="humble-beginnings-%E2%80%94-%E7%AD%9A%E8%B7%AF%E8%93%9D%E7%BC%95">Humble Beginnings — 筚路蓝缕</h2><p><strong>Month 1:</strong> Last September, I was deep into my math undergrad. It was pretty dry. I was looking for some fun non-math side project.<sup><a href="#fn1" id="fnref1">[1]</a></sup> I flirted with French, Russian, archery, parkour, and Japanese. But those didn’t ignite my passion. I happened to watch a snippet of the anime Demon Slayer in an obscure <a href="https://www.youtube.com/watch?v=-VoT0TY0emM&amp;ref=isaak.net">Chinese fan dub</a>. Ironically, this caught my attention. I also had lots of Chinese friends, so why not learn a little Mandarin? Oh my, I had no idea how obsessed I'd end up with this "little" side project.</p>
<p>Berkeley had a breakneck-speed Mandarin beginner class. I loved it. Within a week, we learned pinyin. We learned the tones. We learned to read. We learned to write. Then started talking immediately, every single day. Talking in horribly horribly broken Chinese, but nevertheless having conversations.<sup><a href="#fn2" id="fnref2">[2]</a></sup> I learned the very most important survival vocabulary, like: <em>I am Isaak</em> and <em>Yes, I live in America</em> and <em>Sorry, no, I’m not a basketball player for the Golden State Warriors</em>.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>I had virtually no math background, so I spent a lot of my time studying math from absolute scratch. It was brutal. It was rewarding. It also was very dry. Many months of many proofs, many \[ and some forgotten \], many months of many ∴ and many more □. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>Between running from Chinese class to discrete math classes, I’d be staring into the orange California sunset sky, and mumbling random Demon Slayer anime phrases to myself in Chinese, like "Lightning Breath - First Slash and Lightning" (雷之呼吸，一之型，霹雳一闪!). Oops... <a href="#fnref2">↩︎</a></p>
</li>
</ol>
</section>
<figure><img src="https://isaak.net/content/images/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg" alt="" loading="lazy" width="2000" height="1394" srcset="https://isaak.net/content/images/size/w600/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 600w, https://isaak.net/content/images/size/w1000/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 1000w, https://isaak.net/content/images/size/w1600/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 1600w, https://isaak.net/content/images/size/w2400/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption><i><em>Sorry no, I’m not Steph Curry. But come chat anyway! (Qingyuan, China)</em></i></figcaption></figure><p>The beginning was by far the hardest time, and many tuned out or dropped out. But I had lots of fun. I played a lot. I wrote a horrible poem about humanity colonizing Mars. My Chinese was absolute crap, but I was improving <em>fast</em>. Chinese is my fifth language, and I had a few tricks up my sleeve:</p><h2 id="intense-self-study-%E2%80%94-%E8%87%AA%E5%BC%BA%E4%B8%8D%E6%81%AF">Intense Self-Study — 自强不息</h2><p><strong>Month 3: </strong>Spaced repetition is a superweapon. The spaced repetition app Anki is the core reason why I was able to study Chinese efficiently. Alongside Anki, I adopted other methods to learn <em>faster</em>. </p><p>Frequency-based learning. Comprehensible input. Reading lots as soon as I could, especially graded readers. Buying a calligraphy pen-brush and learned how to write the 600 Chinese characters. FSRS. Creating a 100,000-card Anki megadeck. For all the nitty-gritty language learning tips, check out <a href="https://isaak.net/mandarinmethods/" rel="noreferrer">my methods post</a>. </p><p>Early on, I started watching anime dubs like Boruto or Scissor Seven. I really enjoyed myself despite barely understanding anything. Every few minutes I collected a new word to study. The content I watched in those early days felt like colorful images with funny sounds which occasionally made sense.&nbsp;</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdkzR7VAlJdrPr9X0XuxXJFcdOB9gnxfeRHyrtUpagDZ2krk5NsaA9653_HhAGhzu1L4LmF-vgN9e_rkNWXRd6CRNZex7DUk9SFfjqcITATrrGw8Nx_BcWKjrvXo06bFKYRAtk6bLBYaLLsZ_bK-LIu2ezL?key=va-1OSyREuqkqYOZib3JNw" alt="" loading="lazy" width="379" height="213"><figcaption><i><em>Colorful images with funny sounds which occasionally make sense</em></i></figcaption></figure><p>On day 70 I reached a vocabulary of 1050, of which 460 characters.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>I don’t study for tests and never took one. Yet, note that in the major Chinese system to track language progress, the fourth tier out of six (old HSK4) requires a vocabulary of roughly 1200. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>The other superweapon I implemented was personalized tutoring. My first month studying Chinese was mostly in a 20-people class. But then, I took Bloom's Two-Sigma effect to heart and got myself lots of 1-1 tutoring. The more time I spent on tutoring, the more it accelerated my studies.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Bloom%27s_2_sigma_problem?ref=isaak.net">Bloom’s Two-Sigma</a> effect is the phenomenon of tutored students vastly outperforming students in normal classes. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>There’s legends like <a href="https://www.chinese-forums.com/forums/topic/43939-independent-chinese-study-review/?ref=isaak.net"><u>Tamu</u></a> spending dozens of hours with tutors, but I’d mostly spend up to six hours a week. More would start to detract from my main focus, which were still my math studies. My default for working with tutors was to lead a "normal" conversation. I had two strict rules for conversations with tutors: 1. Only Chinese, no English. 2. Correct every single mistake I make. </p><figure><img src="https://isaak.net/content/images/2024/10/IMG_3800.jpeg" alt="" loading="lazy" width="2000" height="2042" srcset="https://isaak.net/content/images/size/w600/2024/10/IMG_3800.jpeg 600w, https://isaak.net/content/images/size/w1000/2024/10/IMG_3800.jpeg 1000w, https://isaak.net/content/images/size/w1600/2024/10/IMG_3800.jpeg 1600w, https://isaak.net/content/images/size/w2400/2024/10/IMG_3800.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption><i><em>I definitely lost all my social anxiety after 3 days of walking around tourist-packed Hawaii beaches loudly talking broken Chinese</em></i></figcaption></figure><p>At the start, this tutoring was excruciatingly slow. But it was very worth it. After the chat, I’d ask them to send me a summary of my key mistakes and newly learned vocabulary. It’d add that to my Anki.&nbsp;</p><p>I made lots of mistakes. I still do. Tutoring gives me a tight and fast feedback loop on fixing my mistakes.</p><h2 id="traveling-%E2%80%94-%E5%AD%A6%E4%BB%A5%E8%87%B4%E7%94%A8">Traveling — 学以致用</h2><p><strong>Month 4:</strong> Winter break was approaching. I knew all the Chinese I needed to know to travel. (<em>No, sorry, I’m not LeBron James.</em>) I figured out tourist visas, and just went for it. After Christmas in chafing-lips-freezing-cold Austria, I found myself wandering around fogged-up-glasses-humid Taipei.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>Winter in Taiwan was actually comfortable. Summer in Shanghai was crazy. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<figure><div><p><img src="https://isaak.net/content/images/2024/10/IMG_3882.jpeg" width="2000" height="2053" loading="lazy" alt="" srcset="https://isaak.net/content/images/size/w600/2024/10/IMG_3882.jpeg 600w, https://isaak.net/content/images/size/w1000/2024/10/IMG_3882.jpeg 1000w, https://isaak.net/content/images/size/w1600/2024/10/IMG_3882.jpeg 1600w, https://isaak.net/content/images/2024/10/IMG_3882.jpeg 2316w" sizes="(min-width: 720px) 720px"></p><p><img src="https://isaak.net/content/images/2024/10/conan-glasses.png" width="883" height="768" loading="lazy" alt="" srcset="https://isaak.net/content/images/size/w600/2024/10/conan-glasses.png 600w, https://isaak.net/content/images/2024/10/conan-glasses.png 883w" sizes="(min-width: 720px) 720px"></p></div><figcaption><p><i><em>Too humid to see, too humid to sweat, too cool to break a sweat </em></i><span>😎</span></p></figcaption></figure><p>I usually like to travel alone, and then figure out things on the spot. I’d walk around with airpods in, sugar-shocked from eating too many sugar gourds, explaining to my tutor in great detail what tasty novel things I ate at the night market. When not on a tutoring call, I’d sit in cafes and study, wander around markets, or talk to locals. </p><p>Being a foreigner with passable Mandarin is... amusing. When meditating in a small Buddhist temple town in middle-of-nowhere rural Sichuan, I was a local celebrity. 300 primary schooler filed past me, who definitely hadn't seen an Austrian-African foreigner speaking Mandarin. They totally lost it. It was fun, but also tiring. Eventually I preferred to be in the cities, where being a foreigner wasn’t a miracle.&nbsp;</p><p>At least, I got a good taste of why being famous must be great for exactly three minutes, and then quite frankly horrible forever after. Not again. I’m okay, thanks.</p><p>In total, I was in Mainland China three times this year, for a total of two months.<sup><a href="#fn1" id="fnref1">[1]</a></sup> It goes without saying that every journey gave me an enormous boost in my learning pace. The first time travelling got me from <em>broken</em> to <em>comfortable in all day-to-day situations</em>. Every time I travelled I learned roughly 1000 new words/characters. Every time immensely boosted my fluent expression and listening ability.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>I ended up going to China again fairly soon, in Spring break. This time I went with an adventure-hungry Austrian friend who was also learning Chinese, and we pushed each other to get better and better. In total, I’m lucky to have spent almost 2 months of this year traveling and working remote, most of that time in Shanghai. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfU23htFShMc4vmV-YwC2T4CE9qS_IPMTfnOep7B9g2MA-pXbecj4NJ3BB7BfuBLmAmSQ7NXGGUTjgwjzY6ejE6umiSeMRmT-O08hyAhaBLeXgdAdBm_7TUWinNUPsVUtDOWNYN9AjYLKqsUMb9cj2C3JAh?key=va-1OSyREuqkqYOZib3JNw" alt="" loading="lazy" width="194" height="145"><figcaption><i><em>Hot single qi sources near you (Emeishan, Sichuan)</em></i></figcaption></figure><h2 id="the-marathon-%E2%80%94-%E6%8C%81%E4%B9%8B%E4%BB%A5%E6%81%92">The Marathon — 持之以恒</h2><p><strong>Month 6:</strong> My Chinese still had far to go. Apart from the study sprints while traveling, I tried to keep up a consistently high pace back at home. Chinese wasn’t my focus then — math and neuro were. Chinese was consistently the largest side project, clocking some 15 hours a week.</p><p>Consistency was the most important part to keep a high pace of progress. Here’s what a typical focused day might’ve looked like:</p><ul><li>Wake up, 1 hour of Anki</li><li>Do my main thing for 8-9 hours (math undergrad, neuro grad school, …)</li><li>1 hour tutoring call before dinner some days</li><li>1 hour of Chinese content before sleep, e.g. anime dubs or books</li></ul><p>It was quite literally the marathon. Here’s my habitually doing Anki on a treadmill:</p><figure data-kg-thumbnail="https://isaak.net/content/media/2024/10/running-short-squarish-compressed-2_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://isaak.net/content/media/2024/10/running-short-squarish-compressed-2.mp4" poster="https://img.spacergif.org/v1/1080x1350/0a/spacer.png" width="1080" height="1350" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:11</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://isaak.net/content/media/2024/10/running-short-squarish-compressed-2_thumb.jpg"></figure><p>Some 7 months from start, I reached 5,000 known words/characters. The old highest level (HSK6) also required a vocabulary of 5,000 (different) words. So this was an epic goal to hit.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>Again, I wasn't taking studying for tests — I was studying for myself. But to compare, the (old) HSK6 requiring a vocabulary of 5,000 words and characters. Most of HSK6 is business vocabulary that's not useful to me. My goal was to pass my “personal HSK6”: 5,000 words which showed up commonly in the content I watched and loved. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>Immersion is best done while traveling. Still, I started to immerse myself as well as I could at home. My devices would be in Chinese. I started taking some notes primarily in Mandarin. I had lots of social support throughout too: I was lucky to be able to build new relationships entirely in Chinese. For example, my grad school supervisor, a Tsinghua graduate now at MIT, was more than happy to teach me neuroscience in Chinese. I had tutors. I turned older relationships fully Chinese. This had me constantly speaking Mandarin day-to-day.</p><p><strong>Month 12: </strong>Exactly 365 days after I started, I reached a vocabulary of 8000 words and characters.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdSe8Wt-7sEziRiPjp_mO0JZ6rHEK_kdZkBDlxJZ0e-7HBiN__o1wAXc8Fsdb8_Nu20BQTQWKO841-ADHl7o7VO1RB9XEtcyuQiQXyCRD5N4jYXjDtM76t_rpyLBg7dro0U04raEzKooGKAiPfYzuND38_U?key=va-1OSyREuqkqYOZib3JNw" alt="" loading="lazy" width="624" height="63"></figure><p>8000 words and characters makes most content I encounter relatively understandable. My vocabulary is a weird personal mix: Basics including everything up to HSK5, anime vocabulary, biology, mathematics, and random everyday stuff from travelling.</p><p>Vocabulary is only one part of fluency. It's important to keep eyes on the goal: The goal of any language is to communicate effectively. Prompted for feedback on my progress, my usually reserved tutor recently commented: “This was the fastest learning pace from zero to advanced conversational fluency I have ever seen." </p><p>That's kind, but being<em> fluent </em>feels like it’ll always be an overstatement. Especially for Chinese. I’m definitely not <em>Fluent™</em>. I sometimes still get my tones wrong. Full-speed native speech is sometimes still tough. Local dialects remain a complete mystery to me.</p><p>I’d say I’m <em>comfortable </em>with Chinese. I can <em>comfortably</em> travel in any Mandarin-speaking place. I can <em>comfortably </em>hold long conversations. I can <em>comfortably </em>watch most content. I can <em>comfortably </em>build relationships entirely in Mandarin.&nbsp;</p><p>The goal? I want to reach a level where the legendary Three-Body Problem will be <em>comfortably </em>readable.</p><hr><p>Curious about more? Check out my methods post: <a href="https://isaak.net/mandarinmethods/">isaak.net/mandarinmethods</a></p>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: One – A new React framework unifying web, native and local-first (452 pts)]]></title>
            <link>https://onestack.dev</link>
            <guid>41742278</guid>
            <pubDate>Fri, 04 Oct 2024 15:15:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onestack.dev">https://onestack.dev</a>, See on <a href="https://news.ycombinator.com/item?id=41742278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Creating websites and&nbsp;apps is simply too complex.</p><p>One is a new React framework for web and<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="blue" aria-describedby="tooltip-content" role="button" tabindex="0"><span>native</span></span>, built on Vite. It&nbsp;simplifies things with<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="green" aria-describedby="tooltip-content" role="button" tabindex="0"><span>universal</span></span>, <a href="https://onestack.dev/docs/routing" role="link">typed routing</a> seamlessly across<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="purple" aria-describedby="tooltip-content" role="button" tabindex="0"><span>static</span></span>,<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="red" aria-describedby="tooltip-content" role="button" tabindex="0"><span>server</span></span>, and<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="pink" aria-describedby="tooltip-content" role="button" tabindex="0"><span>client</span></span> <!-- -->pages. Plus, an amazing new solution&nbsp;to&nbsp;data.</p><div id="zero"><p>Local, First</p><p>Simpler code, better results, cross-platform — that's&nbsp;the goal.</p><p>With One and <a href="https://tamagui.dev/">Tamagui</a>, we're close… but there's still <em>one</em> big pain point.<!-- --> <b>Let's&nbsp;talk&nbsp;about&nbsp;data</b>.</p><p>Native apps feel better and are easier to write thanks to client-side databases. Say&nbsp;bye&nbsp;to server boundaries, lose&nbsp;the glue code, mutate instantly, and have things Just&nbsp;Work™&nbsp;offline…</p><p>So, <b>why don't we use them on&nbsp;the&nbsp;web?</b></p><p>Well, web needs small bundles, and has limited storage. Add in sync, caching, composition… there's 0 great options.</p><p>It's why we're excited to partner with<!-- --> <b><a target="_blank" href="https://zerosync.dev/" role="link">Zero</a></b> <!-- -->to include it as the default, ejectable solution to data. Zero solves for all the above <a href="https://onestack.dev/docs/data" role="link">and&nbsp;more</a>. It even works with&nbsp;Postgres.</p><p>One<!-- --> <span><svg viewBox="0 0 590 590" width="20.65" height="20.65" style="border-radius:1000px;overflow:hidden;width:20.65px;height:20.65px"><svg width="590px" height="590px" viewBox="0 0 590 590"><defs><filter x="-93.3%" y="-81.2%" width="286.7%" height="262.4%" filterUnits="objectBoundingBox" id="filter-1"><feGaussianBlur stdDeviation="22" in="SourceGraphic"></feGaussianBlur></filter><filter x="-13.5%" y="-46.9%" width="126.9%" height="193.8%" filterUnits="objectBoundingBox" id="filter-2"><feGaussianBlur stdDeviation="20" in="SourceGraphic"></feGaussianBlur></filter><filter x="-23.9%" y="-22.5%" width="147.8%" height="145.1%" filterUnits="objectBoundingBox" id="filter-3"><feGaussianBlur stdDeviation="41" in="SourceGraphic"></feGaussianBlur></filter></defs><g id="favicon" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g transform="translate(-0, 0)" fill-rule="nonzero"><circle id="Oval" fill="#F5CA05" cx="295" cy="295" r="295"></circle><circle id="Oval" fill="#FFFFFF" cx="311" cy="230" r="110"></circle><path d="M299.32294,286 L339.7951,281.25 C342.598367,279.138889 344,276.324074 344,272.805556 C344,269.287037 342.247958,267 338.743875,265.944444 L329.282851,265.944444 L321.398664,178.333333 C320.347439,173.055556 318.244989,172 312.988864,172 C307.732739,172 305.63029,173.583333 304.053452,175.166667 C302.476615,176.75 302.476615,182.555556 301.951002,183.611111 C301.42539,184.666667 291.438753,188.361111 287.759465,189.944444 C284.080178,191.527778 284.080178,201.555556 287.759465,203.666667 C291.438753,205.777778 298.271715,202.083333 301.42539,204.722222 L307.207127,267.527778 C299.339925,268.871363 294.784617,270.102844 293.541203,271.222222 C291.676081,272.901289 290.91314,274.388889 291.438753,279.666667 C291.789161,283.185185 294.417223,285.296296 299.32294,286 Z" id="Path-7" fill="#000000"></path><ellipse id="Oval" fill="#FFFFFF" opacity="0.453031994" filter="url(#filter-1)" transform="translate(200.0089, 137.737) rotate(46) translate(-200.0089, -137.737)" cx="200.008945" cy="137.73703" rx="35.3577818" ry="40.6350626"></ellipse><path d="M521,138 C482.503431,98.2196247 448.723549,71.1799277 419.660355,56.880909 C376.065564,35.432381 347.543959,28.4841486 295.097041,26.8563104 C242.650124,25.2284722 225.598176,37.942459 183.728994,56.880909 C155.816206,69.5065424 119.573208,92.6834255 75,126.411558 C102.798028,89.5392443 133.411045,63.0262947 166.839053,46.8727095 C216.981065,22.6423316 259.733728,10 295.097041,10 C330.460355,10 373.740828,20.0085949 428.633136,46.8727095 C465.228008,64.7821192 496.016963,95.1578827 521,138 Z" id="Path-8" fill="#FFFFFF" opacity="0.773065476" filter="url(#filter-2)"></path><path d="M361.057245,44 C431.694309,123.939704 467.935264,174.984191 469.780109,197.133462 C472.547375,230.357369 482.654123,254.819372 459.752272,321.224371 C436.85042,387.629371 415.418677,407.823985 383.224042,440.562863 C361.760952,462.388781 259.019605,478.230174 75,488.087041 C207.883501,556.029014 286.171,590 309.862498,590 C333.553996,590 368.739389,581.727273 415.418677,565.181818 C481.196175,535.021945 525.881624,499.994866 549.475024,460.10058 C584.865123,400.259152 591.955643,340.867492 589.586372,292.181818 C587.2171,243.496144 582.366118,196.314838 555.280613,172.31528 C537.223611,156.315576 472.482488,113.543815 361.057245,44 Z" id="Path-9" fill="#000000" opacity="0.0963076637" filter="url(#filter-3)"></path></g></g></svg></svg></span> <!-- -->is working to make Zero great on server and client. Our proof of concept has no flickers, waterfalls, or config.</p><p>We love it, and think you will too.</p><p><span></span><a href="https://onestack.dev/docs/data" role="link"><span>Read More</span></a></p></div><a target="_blank" href="https://testflight.apple.com/join/aNcDUHZY" role="link"><div><p><img width="80" height="80" src="https://onestack.dev/testflight.webp" alt="Testflight Icon"></p><p>Try the demo app on Testflight</p></div></a><div><p>Team</p><p>Hello. We're the creators of <a href="https://tamagui.dev/" role="link">Tamagui</a>. We built One out of our experience at<!-- --> <a href="https://app.uniswap.org/" role="link">Uniswap</a> and creating<!-- --> <a href="https://tamagui.dev/takeout" role="link">Takeout</a>.</p></div><div><p>Copyright 2024 Tamagui, LLC</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting my daily news from a dot matrix printer (821 pts)]]></title>
            <link>https://aschmelyun.com/blog/getting-my-daily-news-from-a-dot-matrix-printer/</link>
            <guid>41742210</guid>
            <pubDate>Fri, 04 Oct 2024 15:08:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aschmelyun.com/blog/getting-my-daily-news-from-a-dot-matrix-printer/">https://aschmelyun.com/blog/getting-my-daily-news-from-a-dot-matrix-printer/</a>, See on <a href="https://news.ycombinator.com/item?id=41742210">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>For a while now I've started my day by unlocking my phone and scrolling through different news and social media sites to see what's going on. It's not exactly <em>great</em> for my mental health and I've been trying to cut down on screen time for a while. I still want to stay up-to-date though, especially after I get up in the morning.</p>
<p>I recently purchased a dot matrix printer from eBay, and thought it would be a great excuse to have a custom "front page" printed out and ready for me each day. So, that's what I built!</p>
<p>Printer ASMR noises in the video below 👇</p>
<video width="270" height="360" controls="" title="Dotmatrix Printing Example">
  <source src="https://aschmelyun.com/storage/images/blog/dotmatrix_example_2_bt709.mp4" type="video/mp4">
</video>
<p>I'll take this article to dive in and show you what I used, how I set it up, and the <strong>PHP script</strong> that powers it all.</p>
<blockquote>
<p>Interested in that full source code? Check it out on the <a href="https://github.com/aschmelyun/dotmatrix-daily-news">GitHub repo</a>!</p>
</blockquote>
<h2>Purchasing the hardware</h2>
<p>The supply list for this project was pretty small, and with the exception of the printer, most of this can be found on Amazon or other online retailers.</p>
<ul>
<li>Dot matrix printer</li>
<li>Raspberry Pi Zero W [<a href="https://vilros.com/products/raspberry-pi-zero-w-basic-starter-kit-1">link</a>]</li>
<li>Serial to USB adapter [<a href="https://www.amazon.com/dp/B00IDU0T1Y?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1">link</a>]</li>
<li>Power supply</li>
</ul>
<p>The printer I purchased was a <a href="https://www.computerhistory.org/collections/catalog/102666267">Star NP-10</a> from what looks like the mid-80's. I can't be 100% sure, but any dot matrix printer with a serial port should do the trick. The prices range from about $80-120 USD, but I was able to get this one for about half that price because it was marked as "unsure if working".</p>
<p>It did need a little cleaning up and some tuning of the ink ribbon cartridge (isn't that cool, it's like a typewriter!), but after that it fired right up and ran through the test page print.</p>
<p>After that, I hooked everything up. The Raspberry Pi is connected to my WiFi, and then via USB to the serial port of the printer. After turning on the printer and <code>ssh</code>ing into the Pi, I can verify that the printer is available at <code>/dev/usb/lp0</code>.</p>
<p>Now, <strong>how do I get this thing to print?</strong></p>
<h2>Figuring out the printer's code</h2>
<p>Because the printer is available at <code>lp0</code> I wanted to see if I could just echo raw text to it and have it come through the printer. So I ran the following:</p>
<pre><code data-theme="material-theme-palenight" data-lang="bash"><!-- Syntax highlighted by torchlight.dev --><p><span>echo</span><span> </span><span>"</span><span>Hello, world!</span><span>"</span><span> </span><span>&gt;</span><span> </span><span>/dev/usb/lp0</span></p></code></pre>
<p>Which resulted in an error that the file couldn't be accessed. Bummer, a permissions issue. Easily fixed though with some <code>chmod</code>'ing:</p>
<pre><code data-theme="material-theme-palenight" data-lang="bash"><!-- Syntax highlighted by torchlight.dev --><p><span>sudo</span><span> </span><span>chmod</span><span> </span><span>666</span><span> </span><span>/dev/usb/lp0</span></p></code></pre>
<p>There might be a better way to handle that, but it allowed my echo to go through, and I saw the text available on the printer! Alright, I can send raw data to the printer via this file, so let's find a way to scale this up.</p>
<p>I use PHP as my language of choice in a day-to-day basis, and this is no exception. I write a basic script that accesses the file through <code>fopen()</code> and starts writing text to it. I try a few sentences, some spacing, and then some unicode art, but quickly find out that there's not as much character support on the printer as I was sending.</p>
<p><img src="https://aschmelyun.com/storage/images/blog/dotmatrix-encoding-errors.jpg" alt="Picture of a printed sheet showing a bunch of wrongly-encoded characters"></p>
<p>So I thought it was about time that I start digging into how this thing <em>actually works</em>. Thanks to the hard work and dedication of internet hoarders, I found a <a href="https://www.minuszerodegrees.net/manuals/Star%20Micronics/dot_matrix/Star%20Micronics%20-%20NP-10%20-%20Users%20Manual.pdf">full manual for the printer</a> scanned and uploaded as a PDF.</p>
<p>Come to find out, either because of the age or just the manufacturing decision, this printer has a <strong>very specific character set</strong> that it accepts. Loosely based off of the IBM PC's <a href="https://en.wikipedia.org/wiki/Code_page_437">Code Page 437</a> it consists mostly of your standard alpha-numeric characters, but with a small set of special symbols, lines, and boxes. Neat!</p>
<p>Sending these characters to the printer is pretty straightforward, I can just echo out the hex values with PHP like so:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>$</span><span>horizontalDouble </span><span>=</span><span> </span><span>"</span><span>\xCD</span><span>"</span><span>;</span></p><p><span>$</span><span>deg </span><span>=</span><span> </span><span>"</span><span>\xF8</span><span>"</span><span>;</span></p><p><span>echo</span><span> </span><span>str_repeat</span><span>($</span><span>horizontalDouble</span><span>,</span><span> </span><span>24</span><span>);</span></p><p><span>echo</span><span> </span><span>'</span><span>78</span><span>'</span><span> </span><span>.</span><span> </span><span>$</span><span>deg </span><span>.</span><span> </span><span>'</span><span>F</span><span>'</span><span> </span><span>.</span><span> PHP_EOL</span><span>;</span></p></code></pre>
<p>Alright, so I'm able to write text to the printer just fine, and include some special characters and design symbols. Now I need to figure out <em>what</em> I want to see every morning.</p>
<h2>Gathering the data</h2>
<p>I knew I wanted four distinct sections for my personal front page: <strong>weather, stocks, major news headlines, and a few top reddit posts</strong>. After all, that's usually what I end up look at on my phone in the morning.</p>
<p>Additionally, since this is an experimental project, I wanted to remain super cheap for this data, free if at all possible. Thankfully there's an amazing <a href="https://github.com/public-apis/public-apis">GitHub repo</a> for free and public APIs, so I just went through there and found the ones I needed.</p>
<ul>
<li>The weather pulls from <a href="https://open-meteo.com/en/docs">Open-Meteo</a> and no API key is needed</li>
<li>Stocks data pulls from <a href="https://twelvedata.com/docs">twelvedata</a> that offers a generous free tier</li>
<li>News headlines pull from <a href="https://developer.nytimes.com/get-started">NYTimes</a> which has a decent free tier, good enough for this project</li>
<li>Reddit posts pull from <a href="https://www.reddit.com/r/redditdev/comments/rvqirc/how_to_get_reddit_api_data_using_json/">Reddit JSON</a> which is free (but I had to spoof my User-Agent)</li>
</ul>
<p>For each of the sections, I wrote some basic PHP code to pull in the payload from the API endpoint and compile the data I wanted into a larger overall array. I only wanted specific stocks, types of headlines, and subreddit posts, and if any of the sections couldn't have data to present I just simply crash the script early so I can start it again at a later time.</p>
<p>This can be seen in this snippet which I use for pulling news headlines:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>// Get news headlines data</span></p><p><span>echo</span><span> </span><span>"</span><span>Fetching news headlines data...</span><span>"</span><span> </span><span>.</span><span> PHP_EOL</span><span>;</span></p><p><span>$</span><span>newsUrl </span><span>=</span><span> NEWS </span><span>.</span><span> </span><span>"</span><span>?api-key=</span><span>"</span><span> </span><span>.</span><span> NEWSKEY</span><span>;</span></p><p><span>$</span><span>newsData </span><span>=</span><span> </span><span>[];</span></p><p><span>$</span><span>newsAmount </span><span>=</span><span> </span><span>0</span><span>;</span></p><p><span>$</span><span>data </span><span>=</span><span> </span><span>json_decode</span><span>(</span><span>file_get_contents</span><span>($</span><span>newsUrl</span><span>),</span><span> </span><span>true);</span></p><p><span>if</span><span> </span><span>(!</span><span>isset</span><span>($</span><span>data</span><span>[</span><span>'</span><span>results</span><span>'</span><span>]))</span><span> </span><span>{</span></p><p><span>    </span><span>die</span><span>(</span><span>"</span><span>Unable to retrieve news data</span><span>"</span><span>);</span></p><p><span>}</span></p><p><span>foreach</span><span> </span><span>($</span><span>data</span><span>[</span><span>'</span><span>results</span><span>'</span><span>]</span><span> </span><span>as</span><span> </span><span>$</span><span>article</span><span>)</span><span> </span><span>{</span></p><p><span>    </span><span>if</span><span> </span><span>(</span></p><p><span>        </span><span>($</span><span>article</span><span>[</span><span>'</span><span>type</span><span>'</span><span>]</span><span> </span><span>===</span><span> </span><span>'</span><span>Article</span><span>'</span><span>)</span><span> </span><span>&amp;&amp;</span></p><p><span>        </span><span>(</span><span>in_array</span><span>($</span><span>article</span><span>[</span><span>'</span><span>section</span><span>'</span><span>],</span><span> </span><span>[</span><span>'</span><span>U.S.</span><span>'</span><span>,</span><span> </span><span>'</span><span>World</span><span>'</span><span>,</span><span> </span><span>'</span><span>Weather</span><span>'</span><span>,</span><span> </span><span>'</span><span>Arts</span><span>'</span><span>]))</span><span> </span><span>&amp;&amp;</span></p><p><span>        </span><span>($</span><span>newsAmount </span><span>&lt;</span><span> MAXNEWS</span><span>)</span></p><p><span>    </span><span>)</span><span> </span><span>{</span></p><p><span>        </span><span>$</span><span>newsData</span><span>[]</span><span> </span><span>=</span><span> </span><span>$</span><span>article</span><span>;</span></p><p><span>        </span><span>$</span><span>newsAmount</span><span>++;</span></p><p><span>    </span><span>}</span></p><p><span>}</span></p></code></pre>
<p>The <code>NEWS</code>, <code>NEWSKEY</code>, and <code>MAXNEWS</code> variables are all constants instantiated at the top of the script for easy editing.</p>
<p>Running this compiles everything I want to see displayed on the paper, but now I need to take on the actual task of formatting everything for the printer, and sending it the raw data.</p>
<h2>Printing out my front page</h2>
<p>I could just print out a heading for each section, but that's a little boring. I wanted a bit of <strong><em>flair</em></strong> to the project, so I decided to have a box at the top displaying the current date, day of the week, and my front page name all nicely bordered.</p>
<p>It took a little math, but I got everything working by using a combination of the hex values I talked about above, <code>str_repeat</code> and the knowledge that the page width for this printer is <strong>80 characters</strong>.</p>
<p>Now, just simply go through each section, print a little heading:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>str_repeat</span><span>($</span><span>horizontalSingle</span><span>,</span><span> </span><span>3</span><span>)</span><span> </span><span>.</span><span> </span><span>"</span><span> WEATHER </span><span>"</span><span> </span><span>.</span><span> </span><span>str_repeat</span><span>($</span><span>horizontalSingle</span><span>,</span><span> </span><span>(</span><span>PAGEWIDTH </span><span>-</span><span> </span><span>9</span><span>))</span><span> </span><span>.</span><span> </span><span>"</span><span>\n</span><span>"</span><span>;</span></p></code></pre>
<p>And then print out the data that I want displayed for that section:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>"</span><span>   </span><span>"</span><span> </span><span>.</span><span> </span><span>round</span><span>(($</span><span>weatherData</span><span>[</span><span>'</span><span>daily</span><span>'</span><span>][</span><span>'</span><span>daylight_duration</span><span>'</span><span>][</span><span>0</span><span>]</span><span> </span><span>/</span><span> </span><span>3600</span><span>),</span><span> </span><span>2</span><span>)</span><span> </span><span>.</span><span> </span><span>"</span><span>h of Sunlight  -  Sunrise: </span><span>"</span><span> </span><span>.</span><span> </span><span>date</span><span>(</span><span>'</span><span>g:ia</span><span>'</span><span>,</span><span> </span><span>strtotime</span><span>($</span><span>weatherData</span><span>[</span><span>'</span><span>daily</span><span>'</span><span>][</span><span>'</span><span>sunrise</span><span>'</span><span>][</span><span>0</span><span>]))</span><span> </span><span>.</span><span> </span><span>"</span><span>  -  Sunset: </span><span>"</span><span> </span><span>.</span><span> </span><span>date</span><span>(</span><span>'</span><span>g:ia</span><span>'</span><span>,</span><span> </span><span>strtotime</span><span>($</span><span>weatherData</span><span>[</span><span>'</span><span>daily</span><span>'</span><span>][</span><span>'</span><span>sunset</span><span>'</span><span>][</span><span>0</span><span>]))</span><span> </span><span>.</span><span> </span><span>"</span><span>\n</span><span>"</span><span>;</span></p></code></pre>
<p>For the weather and stocks, I knew I wouldn't hit the edge of the paper so I just wrote everything in single long lines. But that's different for the news headlines and Reddit posts.</p>
<p>If I just feed the printer one long line of text, it's smart enough to cut it and start printing on another line. But, I didn't want a word getting cut off in the middle and starting on the next line. So I implemented a small function to handle line length and instead return back an array of lines with a max length corresponding to the page width.</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>function</span><span> </span><span>splitString</span><span>($</span><span>string</span><span>,</span><span> </span><span>$</span><span>maxLength </span><span>=</span><span> PAGEWIDTH</span><span>)</span><span> </span><span>{</span></p><p><span>    </span><span>$</span><span>result </span><span>=</span><span> </span><span>[];</span></p><p><span>    </span><span>$</span><span>words </span><span>=</span><span> </span><span>explode</span><span>(</span><span>'</span><span> </span><span>'</span><span>,</span><span> </span><span>$</span><span>string</span><span>);</span></p><p><span>    </span><span>$</span><span>currentLine </span><span>=</span><span> </span><span>''</span><span>;</span></p><p><span>    </span><span>foreach</span><span> </span><span>($</span><span>words </span><span>as</span><span> </span><span>$</span><span>word</span><span>)</span><span> </span><span>{</span></p><p><span>        </span><span>if</span><span> </span><span>(</span><span>strlen</span><span>($</span><span>currentLine </span><span>.</span><span> </span><span>$</span><span>word</span><span>)</span><span> </span><span>&lt;=</span><span> </span><span>$</span><span>maxLength</span><span>)</span><span> </span><span>{</span></p><p><span>            </span><span>$</span><span>currentLine </span><span>.=</span><span> </span><span>($</span><span>currentLine </span><span>?</span><span> </span><span>'</span><span> </span><span>'</span><span> </span><span>:</span><span> </span><span>''</span><span>)</span><span> </span><span>.</span><span> </span><span>$</span><span>word</span><span>;</span></p><p><span>        </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span></p><p><span>            </span><span>if</span><span> </span><span>($</span><span>currentLine</span><span>)</span><span> </span><span>{</span></p><p><span>                </span><span>$</span><span>result</span><span>[]</span><span> </span><span>=</span><span> </span><span>$</span><span>currentLine</span><span>;</span></p><p><span>                </span><span>$</span><span>currentLine </span><span>=</span><span> </span><span>$</span><span>word</span><span>;</span></p><p><span>            </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span></p><p><span>                </span><span>// If a single word is longer than maxLength, split it</span></p><p><span>                </span><span>$</span><span>result</span><span>[]</span><span> </span><span>=</span><span> </span><span>substr</span><span>($</span><span>word</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>$</span><span>maxLength</span><span>);</span></p><p><span>                </span><span>$</span><span>currentLine </span><span>=</span><span> </span><span>substr</span><span>($</span><span>word</span><span>,</span><span> </span><span>$</span><span>maxLength</span><span>);</span></p><p><span>            </span><span>}</span></p><p><span>        </span><span>}</span></p><p><span>    </span><span>}</span></p><p><span>    </span><span>if</span><span> </span><span>($</span><span>currentLine</span><span>)</span><span> </span><span>{</span></p><p><span>        </span><span>$</span><span>result</span><span>[]</span><span> </span><span>=</span><span> </span><span>$</span><span>currentLine</span><span>;</span></p><p><span>    </span><span>}</span></p><p><span>    </span><span>return</span><span> </span><span>$</span><span>result</span><span>;</span></p><p><span>}</span></p></code></pre>
<p>Then I can just use it like so:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>foreach</span><span> </span><span>(</span><span>splitString</span><span>($</span><span>redditPost</span><span>)</span><span> </span><span>as</span><span> </span><span>$</span><span>line</span><span>)</span><span> </span><span>{</span></p><p><span>    </span><span>fwrite</span><span>($</span><span>printer</span><span>,</span><span> </span><span>$</span><span>line</span><span>)</span><span> </span><span>.</span><span> </span><span>"</span><span>\n</span><span>"</span><span>;</span></p><p><span>}</span></p></code></pre>
<p>Now all that's left to do is run the script!</p>
<h2>Usage and wrapping up</h2>
<p>I can fire off the printer manually by just running <code>php print.php</code> but I've instead set up a cron job to handle it for me.</p>
<p>Every morning at around 8am it starts printing out my personalized front page. I rip it off and check it out in the morning while drinking my coffee.</p>
<p><img src="https://aschmelyun.com/storage/images/blog/dotmatrix-example-print.jpg" alt="Example page printed"></p>
<p>As silly as it might sound, it just feels better having that finite amount of news on a single sheet of paper. Of being able to stop there instead of endlessly scrolling through websites and social media apps.</p>
<p>Also, this was a super fun project and I'm hoping I can find more uses for this dot matrix printer. Working with physical hardware (especially older specimens like this) is always a blast for me, and being able to integrate them with new technology or use them in interesting ways just ignites pure passion and reinforces why I became a programmer in the first place.</p>
<p>So, what do you think? If you have any ideas for projects like this, or just have a question or comment, I'd love to hear it! Catch me on <a href="https://twitter.com/aschmelyun">Twitter</a> if you'd like to chat more.</p>


            <div>
                
                <p>Subscribe using the form below and about 1-2 times a month you'll receive an email containing helpful hints, new packages, and interesting articles I've found on PHP, JavaScript, Docker and more.</p>
                
            </div>
        </div></div>]]></description>
        </item>
    </channel>
</rss>