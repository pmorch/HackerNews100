<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 24 May 2025 17:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Rotary Phone Dial Linux Kernel Driver (175 pts)]]></title>
            <link>https://gitlab.com/sephalon/rotary_dial_kmod</link>
            <guid>44080803</guid>
            <pubDate>Sat, 24 May 2025 13:02:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gitlab.com/sephalon/rotary_dial_kmod">https://gitlab.com/sephalon/rotary_dial_kmod</a>, See on <a href="https://news.ycombinator.com/item?id=44080803">Hacker News</a></p>
<div id="readability-page-1" class="page">





<header data-testid="navbar">
<a href="#content-body">Skip to content</a>
<div>
<nav aria-label="Explore GitLab">
<div>
<span>GitLab</span>
<a title="Homepage" id="logo" aria-label="Homepage" data-track-label="main_navigation" data-track-action="click_gitlab_logo_link" data-track-property="navigation_top" href="https://gitlab.com/"><svg aria-hidden="true" role="img" width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="m24.507 9.5-.034-.09L21.082.562a.896.896 0 0 0-1.694.091l-2.29 7.01H7.825L5.535.653a.898.898 0 0 0-1.694-.09L.451 9.411.416 9.5a6.297 6.297 0 0 0 2.09 7.278l.012.01.03.022 5.16 3.867 2.56 1.935 1.554 1.176a1.051 1.051 0 0 0 1.268 0l1.555-1.176 2.56-1.935 5.197-3.89.014-.01A6.297 6.297 0 0 0 24.507 9.5Z" fill="#E24329"></path>
  <path d="m24.507 9.5-.034-.09a11.44 11.44 0 0 0-4.56 2.051l-7.447 5.632 4.742 3.584 5.197-3.89.014-.01A6.297 6.297 0 0 0 24.507 9.5Z" fill="#FC6D26"></path>
  <path d="m7.707 20.677 2.56 1.935 1.555 1.176a1.051 1.051 0 0 0 1.268 0l1.555-1.176 2.56-1.935-4.743-3.584-4.755 3.584Z" fill="#FCA326"></path>
  <path d="M5.01 11.461a11.43 11.43 0 0 0-4.56-2.05L.416 9.5a6.297 6.297 0 0 0 2.09 7.278l.012.01.03.022 5.16 3.867 4.745-3.584-7.444-5.632Z" fill="#FC6D26"></path>
</svg>

</a></div>
<ul>
<li>

<div>
<ul>
<li>
<a href="https://about.gitlab.com/why-gitlab">Why GitLab
</a></li>
<li>
<a href="https://about.gitlab.com/pricing">Pricing
</a></li>
<li>
<a href="https://about.gitlab.com/sales">Contact Sales
</a></li>
<li>
<a href="https://gitlab.com/explore">Explore</a>
</li>
</ul>
</div>
</li>
<li>
<a href="https://about.gitlab.com/why-gitlab">Why GitLab
</a></li>
<li>
<a href="https://about.gitlab.com/pricing">Pricing
</a></li>
<li>
<a href="https://about.gitlab.com/sales">Contact Sales
</a></li>
<li>
<a href="https://gitlab.com/explore">Explore</a>
</li>
</ul>
<ul>
<li>
<a href="https://gitlab.com/users/sign_in?redirect_to_referer=yes">Sign in</a>
</li>
<li>
<a href="https://gitlab.com/users/sign_up"><span>
Get free trial

</span>

</a></li>
</ul>
</nav>
</div>
</header>

<div>


<div data-testid="top-bar">
<div data-testid="breadcrumb-links" id="js-vue-page-breadcrumbs-wrapper">


</div>
<div>





</div>
</div>

<div>
<main id="content-body" itemscope="" itemtype="http://schema.org/SoftwareSourceCode">











<header>
<div>
<div>
<div alt="rotary_dial_kmod" itemprop="image">
R
</div>

<h2 data-testid="project-name-content" itemprop="name">
rotary_dial_kmod


</h2>
</div>

</div>

</header>


<div>

<div data-blame-per-page="1000" id="tree-holder">

<div role="status" data-history-link="/sephalon/rotary_dial_kmod/-/commits/master" data-ref-type="heads" id="js-last-commit"><span aria-hidden=""></span><span>Loading</span>
</div>

</div>
</div>

</main>
</div>


</div>








</div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Go deep into AI/LLMs or just use them as tools? (148 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44079303</link>
            <guid>44079303</guid>
            <pubDate>Sat, 24 May 2025 07:05:46 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44079303">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="44079303">
      <td><span></span></td>      <td><center><a id="up_44079303" href="https://news.ycombinator.com/vote?id=44079303&amp;how=up&amp;goto=item%3Fid%3D44079303"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44079303">Ask HN: Go deep into AI/LLMs or just use them as tools?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_44079303">105 points</span> by <a href="https://news.ycombinator.com/user?id=pella_may">pella_may</a> <span title="2025-05-24T07:05:46 1748070346"><a href="https://news.ycombinator.com/item?id=44079303">3 hours ago</a></span> <span id="unv_44079303"></span> | <a href="https://news.ycombinator.com/hide?id=44079303&amp;goto=item%3Fid%3D44079303">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Go%20deep%20into%20AI%2FLLMs%20or%20just%20use%20them%20as%20tools%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44079303&amp;auth=07b696e93f6f3e8afff09888d039e6c6971f13bb">favorite</a> | <a href="https://news.ycombinator.com/item?id=44079303">58&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>I'm a software engineer with a solid full-stack background and web development. With all the noise around LLMs and AI, I’m undecided between two paths:</p><p>1. Invest time in learning the internals of AI/LLMs, maybe even switching fields and working on them</p><p>2. Continue focusing on what I’m good at, like building polished web apps and treat AI as just another tool in my toolbox</p><p>I’m mostly trying to cut through the hype. Is this another bubble that might burst or consolidate into fewer jobs long-term? Or is it a shift that’s worth betting a pivot on?</p><p>Curious how others are approaching this—especially folks who’ve made a similar decision recently.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Valve takes another step toward making SteamOS a true Windows competitor (118 pts)]]></title>
            <link>https://arstechnica.com/gaming/2025/05/valve-adds-steamos-compatible-game-label-as-it-prepares-to-expand-beyond-steam-deck/</link>
            <guid>44078930</guid>
            <pubDate>Sat, 24 May 2025 05:20:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gaming/2025/05/valve-adds-steamos-compatible-game-label-as-it-prepares-to-expand-beyond-steam-deck/">https://arstechnica.com/gaming/2025/05/valve-adds-steamos-compatible-game-label-as-it-prepares-to-expand-beyond-steam-deck/</a>, See on <a href="https://news.ycombinator.com/item?id=44078930">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      
                      
          <p>We've known for months now that Valve is expanding its Linux-based SteamOS operating system beyond the Steam Deck to other handheld PCs, starting with <a href="https://arstechnica.com/gaming/2024/08/valves-bespoke-steam-deck-os-will-be-officially-available-on-asus-rog-ally/">some versions of the Asus ROG Ally</a>. This week, Valve began making some changes to its Steam storefront to prepare for a future when the Deck isn't the only hardware running SteamOS.</p>
<p>A new "<a href="https://steamcommunity.com/groups/steamworks/announcements/detail/532097310616717411">SteamOS Compatible</a>" label will begin rolling out "over the next few weeks" to denote "whether a game and all of its middleware is supported on SteamOS," including "game functionality, launcher functionality, and anti-cheat support." Games that don't meet this requirement will be marked as "SteamOS Unsupported." As with current games and the Steam Deck, this label doesn't mean these games won't run, but it does mean there may be some serious compatibility issues that keep the game from running as intended.</p>
<p>Valve says that "over 18,000 titles on Steam [will] be marked SteamOS compatible out of the gate," and that game developers won't need to do anything extra to earn the label if their titles already support the Steam Deck.</p>
<figure>
    <p><img width="1462" height="779" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/steamos-compatible.png" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/05/steamos-compatible.png 1462w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/steamos-compatible-640x341.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/steamos-compatible-1024x546.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/steamos-compatible-768x409.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/steamos-compatible-980x522.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/steamos-compatible-1440x767.png 1440w" sizes="auto, (max-width: 1462px) 100vw, 1462px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The "SteamOS Compatible" designation that will show up for non-Steam-Deck SteamOS users.

              <span>
          Credit:

          
          Valve

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>SteamOS uses a collection of app translation technologies called Proton to make unmodified Windows applications run on SteamOS. This technology has dramatically improved SteamOS's game compatibility, compared to older SteamOS versions that required games to support Linux natively, but it still can't support every single game that Windows does.</p>

          
                      
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Make a Living as a Writer (150 pts)]]></title>
            <link>https://thewalrus.ca/how-to-make-a-living-as-a-writer/</link>
            <guid>44078813</guid>
            <pubDate>Sat, 24 May 2025 04:36:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thewalrus.ca/how-to-make-a-living-as-a-writer/">https://thewalrus.ca/how-to-make-a-living-as-a-writer/</a>, See on <a href="https://news.ycombinator.com/item?id=44078813">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-180098">

	
	
	
	<div>
		<!-- Ad-Auris -->
		
		<p><span>W</span><span>hen people</span> ask what I do for a living, I’m faced with two choices: either I can lie or I can bore them with the truth, which is too complicated to explain succinctly. While those around me have normal, definable jobs—accountant, journalist, engineer—my work requires headings and subheadings to get it across properly: a map of overlapping gigs and contracts.</p>

<p>“What do you do?” It’s a simple question that often gets asked on first dates. No matter how much I pare down my reply, it’s always long winded.</p>
<p>“Well, I’m a freelancer,” I start, “so I have a million little jobs . . .”</p>
<p>The first of my million little jobs is what I call “Horse News.” It works like this: every weekday, I wake up at 6 a.m. and make my way to my desk, stumbling and still half asleep. I flick on an old lamp and wince as my eyes adjust to the light. I turn on my computer and use a piece of software that shows me all of the American horse racing–related news from the past twenty-four hours. It pulls up radio clips, Fox News segments, and articles from publications called <em>BloodHorse</em> or <em>Daily Racing Form</em>—anything that could be relevant to my interests.</p>
<p>I sift through countless story summaries, many of which sound fake. <em>Army Wife defeats Crazy Beautiful Woman in race! Another doping scandal emerges in Northern California! A disgraced-but-very-good trainer is no longer banned from the track! A famous YouTuber has invested millions into a betting app!</em> I compile the important stuff into a newsletter: stories about track renovations, big events, the series of horse laws that were passed, then repealed, then approved again in 2023. </p>

<p>This is a true, real thing. These laws (known as the Horseracing Integrity and Safety Act) are meant to keep racehorses and jockeys safer. Tracks are required to provide on-site vets and doctors and to follow standardized safety protocols. But it is much cheaper, it turns out, to ignore the laws and have the horses race in dangerous conditions. Vets and safety gear are expensive, which is upsetting to the billionaires who own the racetracks. And so certain states have fought these laws, calling them unconstitutional. I have followed along, every step of the way.</p>

<p>When the newsletter is finished, I send it to my client, a company that owns racetracks across the US. Though, to be clear, I don’t work for them directly. I work for a reputation management firm. This company’s entire purpose is to monitor the news for other companies, keeping tabs on what the public is saying about their clients and the major trends in those industries. I didn’t know this was a real job until I started doing it.</p>
<p>I got this job the way I’ve gotten most of my jobs: through an acquaintance who heard I was looking for work. This is key to success in freelancing. You just need to build a roster of industry connections who know how desperate you are.</p>
<p>“It’s just an hour per morning,” she told me. “Usually less.”</p>
<p>“Sure,” I said, still not understanding what I was agreeing to. “I’ll do it.”</p>

<p>The reputation management firm has a slew of different clients, each of whom wants a personalized newsletter about their industry. There’s a fast food chain, a brewery, an environmental organization. But I was assigned to the horse racing client. And so I keep up with Horse News and Horse Laws. By 7:30 a.m., the report is done, and I go back to bed.</p>
<p>Horse News makes me feel like a bad person sometimes. Racing is an odd, archaic, and often cruel sport. The more I read about it, the more convinced I become that it should not exist. I root for Horse Laws and grow sad when a state bucks them. The thing about Horse News, though, is that someone has to compile it. It might as well be me.</p>
<figure id="attachment_180108" aria-describedby="caption-attachment-180108"><a href="https://thewalrus.ca/how-to-make-a-living-as-a-writer/attachment/drolet_horsenews/" rel="attachment wp-att-180108"><img fetchpriority="high" decoding="async" src="https://walrus-assets.s3.amazonaws.com/img/Drolet_HorseNews-scaled.jpeg" data-src="https://walrus-assets.s3.amazonaws.com/img/Drolet_HorseNews-scaled.jpeg" alt="" width="1600" height="1459" data-srcset="https://walrus-assets.s3.amazonaws.com/img/Drolet_HorseNews-scaled.jpeg 1600w, https://walrus-assets.s3.amazonaws.com/img/Drolet_HorseNews-1536x1401.jpeg 1536w, https://walrus-assets.s3.amazonaws.com/img/Drolet_HorseNews-2048x1868.jpeg 2048w" data-sizes="(max-width: 1600px) 100vw, 1600px" srcset="https://walrus-assets.s3.amazonaws.com/img/Drolet_HorseNews-scaled.jpeg 1600w, https://walrus-assets.s3.amazonaws.com/img/Drolet_HorseNews-1536x1401.jpeg 1536w, https://walrus-assets.s3.amazonaws.com/img/Drolet_HorseNews-2048x1868.jpeg 2048w"></a><figcaption id="caption-attachment-180108">Cartoon by Garbrielle Drolet</figcaption></figure>
<p><span>I</span><span> got the</span> offer to do Horse News not long after I moved to Montreal, at a time when I needed work more than ever.  </p>
<p>I was twenty-four and a full-time adult now, tasked with the question of how I planned to fill my time and make a living. </p>
<p>A year and a half earlier, when I’d finished my undergraduate studies in English and creative writing, I had immediately enrolled in another creative writing program. I wish I could say this was entirely because I was devoted to my craft or that it was my life’s dream to write a book, but that’s only a small part of the truth. The main reason I joined a master’s program was that I didn’t want to face what life would look like once I was no longer a student.</p>
<p>As I’d gotten closer to finishing my undergrad, I kept getting asked what came next. For years, the question “What are you going to do when you grow up?” had been answered the same way: I’m going to be a writer. This was an answer that adults found cute when I was a child and concerning as I got older. A writer, they echoed, mulling the word over slowly. <em>Interesting</em>. By the time I got to university, it was an answer that felt downright unacceptable. Sharing dreams about writing for a living elicited looks of mingled confusion and pity. <em>A writer?</em></p>
<p>I understood that being a writer was fraught. I understood that it was a hard way to make a living. There were no jobs in the industry, and books didn’t sell for as much as they used to. And so the question of what I wanted to do after graduating was one that made me physically sick, because I didn’t know what being a writer meant either.</p>
<p>So I decided the solution was grad school. If anyone dared to ask me what I was doing after that, I could shrug and tell them I had a few more years to think about it.</p>
<p>My plan worked for a year, though not exactly as expected. </p>
<p>First, the pandemic hit, and I moved to Nova Scotia with my now ex-girlfriend. Then, I became disabled. </p>

<p>I developed a nerve condition that became chronic. Pain had spread through my neck, my arms, my hands. When it first started, I couldn’t type at all. I had to re-adjust every aspect of my life: how I cooked, how I brushed my teeth, and how I worked. </p>
<p>By my second year in the program, I had moved to Toronto, but I was still struggling with voice-to-text and barely able to keep up with basic assignments. The thought of writing a thesis—an entire book—felt impossible. I was also writing freelance articles on the side to help pay my rent, and I simply couldn’t do both, mentally or physically. Forced to choose between work and school, I chose work. So I took medical leave, saying I would return in a year but unsure if I actually would.</p>
<p>Leaving school meant I had to face the question of who I was if I wasn’t a student—much earlier than anticipated. Without a schedule filled with classes to attend and readings to do, I was just a person with an empty calendar and one and a half arts degrees.</p>
<p>“What’re you going to do now?” a friend asked over beers at a Mexican restaurant in downtown Toronto. </p>
<p>I dragged a chip through guacamole. “I don’t know, to be honest. I mean, I’ll work, obviously.”</p>
<p>“I’m sure you could get an office job somewhere,” she said. “Or go back to being a barista, maybe.”</p>
<p>People kept suggesting jobs to me like this: Why don’t you just become a barista? A cashier? A secretary? Every time, it was a sharp reminder of how little they understood my physical limitations. <em>I’m too disabled for that</em>, I wanted to say.</p>
<p>I held my tongue, but it was true. My pain was so crippling at this point that I struggled to perform basic tasks around the house. I knew I was no longer able to do most of the jobs I’d had in high school or when I was an undergrad: I couldn’t work as a barista, my forearms too weak to tamp down espresso grounds; nor in retail, nor as a waitress, as the weight of my own dinner plate at home was enough to make me wince with pain. </p>
<p>As I scrolled through job postings for office work, I knew a nine-to-five wasn’t feasible either. I needed the kind of flexibility a job like that wouldn’t allow: the ability to take long breaks when I was in too much pain, to shift deadlines, to use tedious and time-consuming adaptive technology. Back then, I was in so much pain that I could barely use a mouse, commanding my entire computer with my voice. <em>Open Google Chrome. New tab</em>. Copy that. Paste that. In addition to being annoying in an office setting, it just wasn’t fast enough.</p>
<p>“I think I’ll just write,” I told my friend. “Like I’ve been doing, but full time.”</p>

<p>She blinked at me. “Will that be enough?”</p>
<p>I understood the question. I’d enjoyed the freelance writing I’d done, mostly penning articles about health and pop culture for Canadian outlets and the odd American one. It paid poorly and inconsistently.</p>
<p>For a long time, I’d thought of this freelance work as a stepping stone to a real job as a writer or an editor, with a salary and benefits. Now it seemed like going all in on freelancing was my only real career option. It was the only way, I thought, that I could truly work on my own schedule and tend to my needs without falling short of employer expectations.</p>
<p>“I’ll manage. It’ll work out, I’m sure of it.” I’d never been less sure of anything.</p>
<p>In the following weeks, I launched myself into freelancing, pitching an endless stream of articles and essays to my editors. I was lucky to have a few people who championed my work and encouraged me to send them my ideas. I’d never met any of them in person, which was strange; they felt fake to me, just email addresses that provided me with opportunities and paycheques. There had been even more in the past—editors I’d worked with and felt comfortable reaching out to—but many had faded away, either leaving the industry or simply starting to ignore my emails. </p>
<p>As I started writing more freelance pieces, I was, in a way, living the life I’d always wanted. I was a writer. It was my actual job. I balanced deadlines, rotating between articles and editors. I sent out more and more pitches. I worked late into the night, fuelled by instant coffee and bad music.</p>
<p>It wasn’t enough. The number of pitches I was landing couldn’t comfortably sustain me. And it often took ages for me to get paid for my work. A fully written article might be put on hold—it would sit and collect virtual dust, and I wouldn’t be paid until it was published. I knew I needed more consistent work. I longed for some sort of paycheque I could rely on month to month. My savings dwindled as I paid for rent, pricey physiotherapy appointments, and adaptive tools. I moved to Montreal, where the cost of living was lower, but I still struggled to get by.</p>
<p>This was when Horse News entered my life. As I settled into my new city, I was shown the ropes of this strange job: how to use the monitoring software, how to identify stories worth including in the newsletter, who the big players in Horse World were. I was promised hourly pay, with a lump sum deposited into my account at the end of each month. And I suddenly became aware of the possibility of odd jobs that were writing adjacent—the kind of unglamorous work that would pay the bills while allowing me to keep writing on my own schedule. </p>
<p><span>I</span><span>n the coming</span> months, other odd jobs entered and exited my roster. I wrote Instagram captions for a hospital foundation. I wrote online content for a bank (which always paid me late and said it was because they couldn’t figure out how to transfer the money, which made me grateful they were not my bank). Importantly, I wrote a column where I recapped episodes of <em>The Bachelorette</em>. I was constantly writing some odd articles for different publications. Throughout all of this, Horse News was the only stable work I had. Every weekday, without fail, the horses raced on, and I compiled my newsletter.</p>
<p>As new opportunities presented themselves, I found myself unable to say no to work. No matter how busy I was or how strange the job was, I accepted every single offer that came my way, worried the gigs would eventually dry up.</p>

<p>In early summer, as Montreal’s unbearably cold season gave way to an unbearably hot one, I got a text from a friend. She worked at a major Canadian newspaper, which, she said, wasn’t paying her enough. She’d taken on a side gig to compensate for the poor salary. She’d heard I was looking for work and thought I might be interested.</p>
<p>“What is it?” I texted.</p>
<p>“Writing erotica,” she answered.</p>
<p>The next week, I had a Zoom meeting with someone who worked at the company. She was young, in her late twenties, with pink cheeks and glossy blonde hair. She explained that she needed writers for an app she was running that was like a choose-your-own-adventure story, only hornier. Users, mostly women, would select a story and start reading. They were all written in the second person, placing users in the protagonist’s shoes: “You walk into a restaurant . . . You see a hot guy sitting at the bar . . . What will you do next?” They were then presented with two choices.</p>
<p>One would be boring (ignore the guy!), and the other would be depraved (ask him to go back to your place and [redacted]!). Choosing depravity cost $0.99.</p>
<p>These stories were long, most of them basically novels.</p>
<p>New chapters came out every week, each instalment getting increasingly risqué. This was a business strategy: users became invested in a story and were then charged money to read the new material.</p>
<p>“Do you think you’d be able to keep up with it?”</p>
<p>“I think so.”</p>
<p>I agreed to write one or two chapters per week. Each would be around 4,000 words long, and the story would ultimately have at least twenty chapters. I would get paid $120 (US) for each chapter.</p>

<p>If I had done the math or thought about this critically, I’d have realized this was a very bad idea. It was a monumental amount of work and creative energy to expend for pretty poor pay, especially as someone who couldn’t type much. Unfortunately, I was distracted by how fun the work sounded. Like many young women who grew up with the internet, I had lived through the days of reading whatever perverted and poorly written erotica I could find about my favourite fictional characters online. The prospect of now becoming a professional erotica writer was too enticing to turn down. Plus, if my friend was balancing full-time newspaper work with this, how hard could it be?</p>
<p>The woman who would become my editor nodded.</p>
<p>“The categories that perform best right now are domination, stepbrother, and campus stuff. You know, student–teacher situations?” She looked through a printout of figures. </p>
<p>“Vampire and werewolf stories are making a resurgence too.”</p>
<p>I jotted this down in a notebook, my handwriting messy and quick. <em>Campus, werewolf, domination</em>. “Got it.”</p>
<p>“By the way, the app store won’t let us use the words penis, vagina, or cock,” she said flatly.</p>
<p>“Oh,” I said. “Why not?”</p>
<p>“Terms of service stuff.”</p>
<p>“Got it.”</p>
<p>“Read a few of the stories for inspiration on how to work around this. You’ll get the hang of it.”</p>

<p>“Right.”</p>
<p>“People get really creative. Fruit works, sometimes.”</p>
<p>“Fruit?”</p>
<p>“You’ll see what I mean,” she said. “And you’ll need a pen name. Unless you want to use your own?”</p>
<p>I shook my head. “I’ll find a pen name.”</p>
<p>That afternoon, I sat on my friends’ balcony. I told them about my new job, which would somehow slot in alongside all the other jobs I was doing. It was one of the first truly warm days of summer, and we were determined to spend the entire thing outside. Between sips of iced coffee, we plotted out my story chapter by chapter, my friends enthusiastic about its trajectory.</p>
<p>“Maybe she can hook up with her roommate?” I suggested.</p>
<p>“Yes, that’s great,” John said. “Make it a love triangle.”</p>
<p>He dragged a finger through the air, drawing a triangle.</p>
<p>“I can’t believe you’re writing porn,” Maria said, leaning back in a wooden folding chair. “How fun.”</p>

<p>“Not porn. Erotica.”</p>
<p>“Same difference,” John said. He pulled the notes I’d scrawled toward him and squinted. “Okay, what happens next?”</p>
<p>By the end of the day, John and I had plotted out an entire story arc: the student and the TA’s tumultuous affair, the way they were almost found out, the forces that almost pulled them apart. Ultimately, love and sex brought them back together.</p>
<p>“This is basically an entire romance novel,” John said.</p>
<p>“Smuttier, though.”</p>
<p>“Of course.”</p>
<p>“And worse.”</p>
<p>Maria spent the day brainstorming pen-name ideas, which she would occasionally pipe up to suggest. “Madame Scarlett?” “Delilah Rose?” “Candy Mae?” “Jolene Fox?” “What kind of vibe are you looking for, anyways?”</p>
<p>Now, my days looked like this: I woke up at 6 a.m. and did Horse News; I hammered out whatever freelance writing assignment I was working on; I wrote erotica; I ended my workday around 5 p.m., tired and achy.</p>
<p>In the coming months, I sat in my hot, not air-conditioned apartment, sweating and damp, and wrote between 3,500 and 8,000 words of smut per week. Since I was doing this with voice-to-text, I had to keep my windows closed, mortified at the thought of my neighbours hearing me speak vile things into my computer: words like member, length, girth, and sometimes the names of fruit.</p>

<p>I worked on one story throughout the whole summer.</p>
<p>On weeks when, for whatever reason, I couldn’t keep up—say, my hands were worse than usual or I got too busy with other work—my boss at the app was understanding.</p>
<p>“Your health is more important than this,” she would say. <em>Rest</em>. It was the most compassion I’d ever gotten from an employer, which was nice but also annoying. Part of me hoped to be fired, freed entirely from my contract. But no—these people were, unfortunately, sweet and thoughtful.</p>
<p>Within a few weeks, I had come to hate the work. Though it was fun in the beginning, it quickly lost its charm, the sex scenes becoming tedious and exhausting once they were no longer new to me.</p>
<p>“There are only so many ways to write ‘they had sex,’ you know?” I told Maria one day.</p>
<p>She shook her head. “I really don’t.”</p>
<p>The biggest problem was just that I was overworked. Writing that much sapped all of my creative and physical energy, leaving me unwilling or unable to write much else.</p>
<p>When I neared the final chapter, my friends and I sat around with a bottle of wine and celebrated the fact that my life as an erotica writer was almost done. They suggested words and phrases I should try to sneak into the final chapter as a little personal challenge: cornucopia, sledgehammer, pumpernickel, Seinfeld, Donna Tartt, the Watergate scandal.</p>
<p>Maria squinted at John. “That last one is too silly,” she said. “She won’t be able to manage it.”</p>
<p>“Have faith,” I said.</p>

<p>I managed them all, laughing along the way as I tweaked the story to include them.</p>
<p>By the time it was done, I’d written over 70,000 words of smut. My editor asked if I wanted to renew my contract, and I declined. She insisted, saying we could alter the work schedule, maybe even up my pay by another $5 per chapter.</p>
<p>My story, she revealed, was gaining a devoted following, quickly becoming one of the most popular on the app. This felt nice—my anonymous magnum opus. Still, I said no.</p>
<p>As time passed in Montreal and I did more odd jobs, my hands got marginally better. This meant that, as long as I was very careful and worked within a strict set of limitations, there was one more type of work that became available to me again: cartooning.</p>
<p>I’d loved drawing since I was a kid. Growing up, I drew countless pictures of animals (especially birds), carefully copying them from the books I begged my mom to buy me.</p>
<p>When my pain first started in 2021 and I realized I would have to take a months-long break from drawing, it was a particularly tough blow. Drawing wasn’t as big a part of my income or my identity as writing was, but it still mattered to me immensely. What felt worse was the fact that, a month before I lost the ability to draw, I’d sold my first cartoon to <em>The New Yorker</em>—an accomplishment I’d worked toward for years, and which I worried I might never be able to repeat.</p>
<p>Now, in my very ergonomic home office, I could draw again (though I needed to set a timer beforehand to make sure I didn’t work for more than twenty minutes at a time).</p>
<p>When the timer went off, I’d stand and stretch and take a break. I limited the number of projects I took on so I wouldn’t overdo it. However, every now and then, I pitched a cartoon to <em>The New Yorker</em> or accepted a commission request for a portrait of someone’s dog.</p>
<p>Cartooning became a very small part of the tapestry of odd jobs that came together to make up an income. But it was one I was happy to be able to include.</p>
<p><span>O</span><span>n dates</span>, I try to condense this all into a short spiel. <em>I’m a writer. I do Horse News. I’m a copywriter. I also draw cartoons, sometimes, but that’s neither here nor there. Even this has omissions, but it’s the best I can do</em>.</p>

<p>“Wouldn’t you rather just have a normal job?” one date—a lawyer—asked.</p>
<p>It’s something I’ve wondered myself. Sometimes, looking at overlapping assignments and deadlines on my Google calendar, I feel overwhelmed and exhausted. But when I’m in pain, I can take a break in the middle of the day or even go back to bed if I need to.</p>
<p>“This suits me best,” I said.</p>
<p>I ended that date early, as I do with all weekday dates. I have a great excuse: <em>Horse News is due at 7:30 a.m. tomorrow morning</em>.</p>
<p><em>Excerpted from</em> <a href="https://www.penguinrandomhouse.ca/books/760991/look-ma-no-hands-by-gabrielle-drolet/9780771019142">Look Ma, No Hands</a> <em>by Gabrielle Drolet. Copyright © 2025 Gabrielle Drolet. Published by McClelland &amp; Stewart, a division of Penguin Random House Canada Limited. Reproduced by arrangement with the publisher. All rights reserved.</em></p>
<!-- AI CONTENT END 1 -->
		<div id="sexy_author_bio_widget-2"><p><a href="https://thewalrus.ca/author/gabrielle-drolet/" target="_top"><img alt="Gabrielle Drolet" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2070%2070'%3E%3C/svg%3E" data-src="https://secure.gravatar.com/avatar/80380f26cd399c63cc449337051d117b?s=70&amp;d=mm&amp;r=pg" data-srcset="https://secure.gravatar.com/avatar/80380f26cd399c63cc449337051d117b?s=140&amp;d=mm&amp;r=pg 2x" height="70" width="70" decoding="async"></a></p><p>Gabrielle Drolet is a Montreal-based writer and cartoonist whose work has appeared in the <em>New York Times</em>, the <em>Globe and Mail</em>, <em>The New Yorker</em>, and other publications.</p></div>	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

		
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Algebraic Effects? (237 pts)]]></title>
            <link>https://antelang.org/blog/why_effects/</link>
            <guid>44078434</guid>
            <pubDate>Sat, 24 May 2025 03:00:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antelang.org/blog/why_effects/">https://antelang.org/blog/why_effects/</a>, See on <a href="https://news.ycombinator.com/item?id=44078434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content">
                          
<p>Algebraic effects<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> (a.k.a. effect handlers) are a very useful up-and-coming feature that I personally think will see a huge surge in popularity in the programming
languages of tomorrow. They’re one of the core features of Ante, as well as being the focus of many research
languages including <a href="https://koka-lang.github.io/koka/doc/index.html">Koka</a>, <a href="https://effekt-lang.org/">Effekt</a>, <a href="https://www.eff-lang.org/">Eff</a>,
and <a href="https://flix.dev/">Flix</a>. However, while many articles or documentation snippets try to explain <em>what</em> effect handlers are (including
<a href="https://antelang.org/docs/language#algebraic-effects">Ante’s own documentation</a>), few really go in-depth on <em>why</em> you would want to use them.
In this post I’ll explain exactly that and will include as complete a list as possible on all the use-cases of algebraic effects.</p>
<h2 id="a-note-on-syntax-and-semantics">A Note on Syntax and Semantics</h2>
<p>I’ll be using Ante pseudocode for much of this article. If you’re not familiar with effect handlers or Ante I encourage you to read the documentation link
above or read from any of the other effectful languages for a good head start! But I recognize it’s hard to get buy-in to learn something before showing
why it is useful first (hence this blog post!). So I’ll give a quick elevator pitch on a good mental model to think about effects.</p>
<p>You can think of algebraic effects essentially as exceptions that you can resume. You can declare an effect function:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>SayMessage</span> <span>with</span>
    <span>// This effect function takes a Unit type and returns a Unit type.</span>
    <span>// Note that `Unit` is roughly the same as `void` in imperative languages.</span>
    <span>// There are differences between them but none that are relevant here.</span>
    <span>say_message</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Unit</span>
</code></pre></div><p>You can “throw” an effect by calling the function, and the function you’re in must declare it can use that effect similar to checked exceptions:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>foo</span> <span>()</span> <span>can</span> <span>SayMessage</span> <span>=</span>
    <span>say_message</span> <span>()</span>
    <span>42</span>
</code></pre></div><p>And you can “catch” effects with a <code>handle</code> expression (think of these as <code>try/catch</code> expressions):</p>
<div><pre tabindex="0"><code data-lang="ante"><span>handle</span> <span>foo</span> <span>()</span>
<span>|</span> <span>say_message</span> <span>()</span> <span>-&gt;</span>
    <span>print</span> <span>"Hello World!"</span>  <span>// print out Hello World!</span>
    <span>resume</span> <span>()</span>             <span>// then resume the computation, returning 42</span>
</code></pre></div><p>If you have further questions I again encourage you to read <a href="https://antelang.org/docs/language#algebraic-effects">some documentation</a> on effects, but now
that we can recognize effects when they’re used I’ll get into why exactly the idea of exceptions-you-can-resume are so useful!</p>
<hr>
<h2 id="user-defineable-control-flow">User-defineable control-flow</h2>
<p>The most common reason you’ll hear for why to have effect handlers is that they are a single language feature which
allow for implementing what would normally be multiple separate language features (generators, exceptions, async, coroutines, etc)
as libraries. Moreover, they solve the <a href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/">what color is your function</a>
problem by making functions polymorphic over effects. For example, a <code>map</code> function for vectors (growable arrays) can be written once:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>map</span> <span>(</span><span>input</span><span>:</span> <span>Vec</span> <span>a</span><span>)</span> <span>(</span><span>f</span><span>:</span> <span>a</span> <span>-&gt;</span> <span>b</span> <span>can</span> <span>e</span><span>)</span><span>:</span> <span>Vec</span> <span>b</span> <span>can</span> <span>e</span> <span>=</span>
    <span>// Implementation omitted!</span>
</code></pre></div><p>This function’s signature says that the input function <code>f</code> can perform <em>any</em> effect(s) <code>e</code>, and that
<code>map</code> will perform those same effects <code>e</code>. So we can instantiate this with an <code>f</code> that prints to stdout,
an <code>f</code> that calls asynchronous functions, an <code>f</code> that yields elements into a stream, etc. Most languages
with effect handlers will let you omit the polymorphic effect variable <code>e</code> as well, giving us the
old, familiar signature for <code>map</code>:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>map</span> <span>(</span><span>input</span><span>:</span> <span>Vec</span> <span>a</span><span>)</span> <span>(</span><span>f</span><span>:</span> <span>a</span> <span>-&gt;</span> <span>b</span><span>)</span><span>:</span> <span>Vec</span> <span>b</span> <span>=</span>
    <span>// Implementation omitted!</span>
</code></pre></div><p>Ok, back to the topic though. Effect handlers are cool because we can implement generators, exceptions, coroutines,
<a href="https://effekt-lang.org/docs/casestudies/ad">automatic differentiation</a>, and much more with them. Surely such
constructs are difficult to implement, requiring low-level knowledge though, right? Nope. Most of these are pretty
straightforward actually.</p>
<p>Let’s consider exceptions. Remember when I described algebraic effects as resumeable exceptions? This actually works
pretty well as a hint on how to implement exceptions via effects. How do we do it? Just don’t <code>resume</code> the effect when it is thrown:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Throw</span> <span>a</span> <span>with</span>
    <span>throw</span><span>:</span> <span>a</span> <span>-&gt;</span> <span>never_returns</span>

<span>safe_div</span> <span>x</span> <span>y</span> <span>=</span>
    <span>if</span> <span>y</span> <span>==</span> <span>0</span> <span>then</span>
        <span>throw</span> <span>"error: Division by zero!"</span>

    <span>x</span> <span>/</span> <span>y</span>

<span>// Output: "error: Division by zero!"</span>
<span>handle</span> 
    <span>safe_div</span> <span>5</span> <span>0</span>
    <span>print</span> <span>"successfully divided by zero"</span> <span>// we never get to this point</span>
<span>|</span> <span>throw</span> <span>msg</span> <span>-&gt;</span>
    <span>print</span> <span>msg</span>
</code></pre></div><p>How about something more advanced? Surely generators must be more difficult? Well, a little but the code still fits
onto a sticky note:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Yield</span> <span>a</span> <span>with</span>
    <span>yield</span><span>:</span> <span>a</span> <span>-&gt;</span> <span>Unit</span>

<span>yield_all_elements_of_vec</span> <span>(</span><span>vec</span><span>:</span> <span>Vec</span> <span>a</span><span>)</span><span>:</span> <span>Unit</span> <span>can</span> <span>Yield</span> <span>a</span> <span>=</span>
    <span>vec</span><span>.</span><span>for_each</span> <span>fn</span> <span>elem</span> <span>-&gt;</span>
        <span>yield</span> <span>elem</span>

<span>// To filter a generator we're going to take in a generator function to filter</span>
<span>// as well as a predicate to tell us which elements to keep</span>
<span>filter</span> <span>(</span><span>generator</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Unit</span> <span>can</span> <span>Yield</span> <span>a</span><span>)</span> <span>(</span><span>predicate</span><span>:</span> <span>a</span> <span>-&gt;</span> <span>Bool</span><span>)</span><span>:</span> <span>Unit</span> <span>can</span> <span>Yield</span> <span>a</span> <span>=</span>
    <span>handle</span> <span>generator</span> <span>()</span>
    <span>|</span> <span>yield</span> <span>x</span> <span>-&gt;</span>
        <span>// when `generator` yields us an element, re-raise it if `predicate` returns true for it</span>
        <span>if</span> <span>predicate</span> <span>x</span> <span>then</span>
            <span>yield</span> <span>x</span>
        <span>resume</span> <span>()</span>  <span>// continue yielding elements</span>

<span>// Finally, lets add a helper function for applying a function to each yielded element</span>
<span>my_for_each</span> <span>(</span><span>generator</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Unit</span> <span>can</span> <span>Yield</span> <span>a</span><span>)</span> <span>(</span><span>f</span><span>:</span> <span>a</span> <span>-&gt;</span> <span>Unit</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span>
    <span>handle</span> <span>generator</span> <span>()</span>
    <span>|</span> <span>yield</span> <span>x</span> <span>-&gt;</span>
        <span>f</span> <span>x</span>
        <span>resume</span> <span>()</span>

<span>// Let's use it!</span>
<span>yield_all_elements_of_vec</span> <span>(</span><span>Vec</span><span>.</span><span>of</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>])</span>
    <span>// `with` is sugar to apply effect handler functions</span>
    <span>with</span> <span>filter</span> <span>(</span><span>fn</span> <span>x</span> <span>-&gt;</span> <span>x</span> <span>%</span> <span>2</span> <span>==</span> <span>0</span><span>)</span>
    <span>with</span> <span>my_for_each</span> <span>print</span>  <span>// prints 2 then 4</span>
</code></pre></div><p>You can similarly implement a cooperative scheduler with a <code>yield: Unit -&gt; Unit</code> effect which
yields control back to a handler which switches execution to another function. <a href="https://effekt-lang.org/docs/casestudies/scheduler">Here’s an example</a>
of that in Effekt.</p>
<p>Basically, algebraic effects get you a lot of expressivity in your language, and as a bonus these different
effects compose well with each other. We’ll get into this more later but algebraic effects composing well
is a huge usability win over other effect abstractions.</p>
<hr>
<h2 id="as-an-abstraction">As an Abstraction</h2>
<p>Okay, now that the really flashy stuff is out of the way I want to go over some less obvious benefits of algebraic effects.
Since discussion on effects can often seem like they’re <em>only</em> for implementing generators, exceptions, async, etc., I want to
highlight that even if you don’t personally care for these features, there are still good reasons to use algebraic effects
in your run of the mill business application.</p>
<p>One reason to use them in such an application is that effects can be used for <em>dependency injection</em>. Let’s assume
we have code that touches a database:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>business_logic</span> <span>(</span><span>db</span><span>:</span> <span>Database</span><span>)</span> <span>(</span><span>x</span><span>:</span> <span>I32</span><span>)</span> <span>=</span>
    <span>db</span><span>.</span><span>query</span> <span>"..."</span>
    <span>db</span><span>.</span><span>query</span> <span>"..."</span>
    <span>db</span><span>.</span><span>query</span> <span>"..."</span>
    <span>x</span> <span>*</span> <span>2</span>
</code></pre></div><p>This is all well and fine until we want to use a different database, restrict access to this database, or you know, actually
test these functions. What we can do is move the database to an effect:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Database</span> <span>with</span>
    <span>query</span><span>:</span> <span>String</span> <span>-&gt;</span> <span>DbResponse</span>

<span>business_logic</span> <span>(</span><span>x</span><span>:</span> <span>I32</span><span>)</span> <span>can</span> <span>Database</span> <span>=</span>
    <span>query</span> <span>"..."</span>
    <span>query</span> <span>"..."</span>
    <span>query</span> <span>"..."</span>
    <span>x</span> <span>*</span> <span>2</span>
</code></pre></div><p>Now we can swap out the specific database used further up the call stack (let’s say in <code>main</code>) with a different database,
or even with a mock database for testing:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>mock_database</span> <span>(</span><span>f</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>a</span> <span>can</span> <span>Database</span><span>)</span><span>:</span> <span>a</span> <span>=</span>
    <span>handle</span> <span>f</span> <span>()</span>
    <span>|</span> <span>query</span> <span>_msg</span> <span>-&gt;</span>
        <span>// Ignore the message and always return Ok</span>
        <span>resume</span> <span>DbResponse</span><span>.</span><span>Ok</span>

<span>test_business_logic</span> <span>()</span> <span>=</span>
    <span>// Apply the `mock_database` handler to the rest of the function</span>
    <span>with</span> <span>mock_database</span>

    <span>assert</span> <span>(</span><span>business_logic</span> <span>0</span> <span>==</span> <span>0</span><span>)</span>
    <span>assert</span> <span>(</span><span>business_logic</span> <span>1</span> <span>==</span> <span>2</span><span>)</span>
    <span>assert</span> <span>(</span><span>business_logic</span> <span>21</span> <span>==</span> <span>42</span><span>)</span>
    <span>// etc</span>
</code></pre></div><p>We can even redirect print outs into a string:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>output_messages</span> <span>()</span><span>:</span> <span>U32</span> <span>can</span> <span>Print</span> <span>=</span>
    <span>print</span> <span>"Hello!"</span>
    <span>print</span> <span>"Not sure what to write here, honestly"</span>
    <span>1234</span>

<span>// Collect `print` calls into a single string, separating each with newlines</span>
<span>print_to_string</span> <span>(</span><span>f</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>a</span> <span>can</span> <span>Print</span><span>)</span><span>:</span> <span>a</span><span>,</span> <span>String</span> <span>can</span> <span>Print</span> <span>=</span>
    <span>mut</span> <span>all_messages</span> <span>=</span> <span>""</span>

    <span>handle</span>
        <span>result</span> <span>=</span> <span>f</span> <span>()</span>
        <span>result</span><span>,</span> <span>all_messages</span>
    <span>|</span> <span>print</span> <span>msg</span> <span>-&gt;</span>
        <span>all_messages</span> <span>:=</span> <span>all_messages</span> <span>++</span> <span>"</span><span>\n</span><span>"</span> <span>++</span> <span>msg</span>
        <span>resume</span> <span>()</span>

<span>// Now we can test `output_messages` without it printing to stdout</span>
<span>test_output_messages</span> <span>()</span> <span>=</span>
    <span>int</span><span>,</span> <span>messages</span> <span>=</span> <span>output_messages</span> <span>()</span> <span>with</span> <span>print_to_string</span>
    <span>assert</span> <span>(</span><span>int</span> <span>==</span> <span>1234</span><span>)</span>
    <span>assert</span> <span>(</span><span>messages</span> <span>==</span> <span>"Hello!</span><span>\n</span><span>Not sure what to write here, honestly"</span><span>)</span>
</code></pre></div><p>Or conditionally disable logging output:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Log</span> <span>with</span>
    <span>log</span><span>:</span> <span>LogLevel</span> <span>-&gt;</span> <span>String</span> <span>-&gt;</span> <span>Unit</span>

<span>type</span> <span>LogLevel</span> <span>=</span> <span>|</span> <span>Error</span> <span>|</span> <span>Warn</span> <span>|</span> <span>Info</span>

<span>LogLevel</span><span>.</span><span>greater_than_or_equal</span> <span>self</span> <span>(</span><span>other</span><span>:</span> <span>LogLevel</span><span>)</span><span>:</span> <span>Bool</span> <span>=</span>
    <span>match</span> <span>self</span><span>,</span> <span>other</span>
    <span>|</span> <span>Error</span><span>,</span> <span>_</span> <span>-&gt;</span> <span>true</span>
    <span>|</span> <span>Warn</span><span>,</span> <span>(</span><span>Warn</span> <span>|</span> <span>Info</span><span>)</span> <span>-&gt;</span> <span>true</span>
    <span>|</span> <span>Info</span><span>,</span> <span>Info</span> <span>-&gt;</span> <span>true</span>
    <span>|</span> <span>_</span><span>,</span> <span>_</span> <span>-&gt;</span> <span>false</span>

<span>foo</span> <span>()</span> <span>=</span>
    <span>log</span> <span>Info</span> <span>"Entering foo..."</span>
    <span>log</span> <span>Warn</span> <span>"foo is a fairly lazy example function"</span>
    <span>log</span> <span>Error</span> <span>"an error occurred!"</span>

<span>log_handler</span> <span>(</span><span>f</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>a</span> <span>can</span> <span>Log</span><span>)</span> <span>(</span><span>level</span><span>:</span> <span>LogLevel</span><span>)</span><span>:</span> <span>a</span> <span>can</span> <span>Print</span> <span>=</span>
    <span>handle</span> <span>f</span> <span>()</span>
    <span>|</span> <span>log</span> <span>msg_level</span> <span>msg</span> <span>-&gt;</span>
        <span>if</span> <span>level</span><span>.</span><span>greater_than_or_equal</span> <span>msg_level</span> <span>then</span>
            <span>print</span> <span>msg</span>
        <span>resume</span> <span>()</span>

<span>foo</span> <span>()</span> <span>with</span> <span>log_handler</span> <span>Error</span>  <span>// outputs "an error occurred!"</span>
</code></pre></div><hr>
<h2 id="cleaner-apis">Cleaner APIs</h2>
<p>Algebraic effects can also make designing cleaner APIs easier. A common pattern in just about
any programming language is the use of a <code>Context</code> object which often needs to be passed to
most functions in the program or library. We can encode this pattern as an effect. All
we need are functions to <code>get</code> the context and to <code>set</code> it:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Use</span> <span>a</span> <span>with</span>
    <span>get</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>a</span>
    <span>set</span><span>:</span> <span>a</span> <span>-&gt;</span> <span>Unit</span>
</code></pre></div><p>Most languages call this a state effect and it is generic over the type of state to use.</p>
<p>We can define a handler to provide the initial state value like so<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>state</span> <span>(</span><span>f</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>a</span> <span>can</span> <span>Use</span> <span>s</span><span>)</span> <span>(</span><span>initial</span><span>:</span> <span>s</span><span>)</span><span>:</span> <span>a</span> <span>=</span>
    <span>mut</span> <span>context</span> <span>=</span> <span>initial</span>
    <span>handle</span> <span>f</span> <span>()</span>
    <span>|</span> <span>get</span> <span>()</span> <span>-&gt;</span> <span>resume</span> <span>context</span>  <span>// give the context to the caller of `get`</span>
    <span>|</span> <span>set</span> <span>new_context</span> <span>-&gt;</span>
        <span>context</span> <span>:=</span> <span>new_context</span>
        <span>resume</span> <span>()</span>
</code></pre></div><p>And we can use this to help clean up code that uses one or more context objects.
Let’s imagine we have code which uses a vector internally and hands out references
to elements as indices into this vector. This would normally require passing around
the vector everywhere:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>type</span> <span>Strings</span> <span>=</span> <span>vec</span><span>:</span> <span>Vec</span> <span>String</span>
<span>type</span> <span>StringKey</span> <span>=</span> <span>index</span><span>:</span> <span>Usz</span>

<span>// `!` is a mutable reference</span>
<span>push_string</span> <span>(</span><span>strings</span><span>:</span> <span>!</span><span>Strings</span><span>)</span> <span>(</span><span>string</span><span>:</span> <span>String</span><span>)</span><span>:</span> <span>StringKey</span> <span>=</span>
    <span>key</span> <span>=</span> <span>StringKey</span> <span>(</span><span>strings</span><span>.</span><span>len</span> <span>())</span>
    <span>strings</span><span>.</span><span>push</span> <span>string</span>
    <span>key</span>

<span>get_string</span> <span>(</span><span>strings</span><span>:</span> <span>&amp;</span><span>Strings</span><span>)</span> <span>(</span><span>key</span><span>:</span> <span>StringKey</span><span>)</span><span>:</span> <span>&amp;</span><span>String</span> <span>=</span>
    <span>strings</span><span>.</span><span>get</span> <span>key</span> <span>|&gt;</span> <span>unwrap</span>

<span>append_with_separator</span> <span>(</span><span>strings</span><span>:</span> <span>!</span><span>Strings</span><span>)</span> <span>(</span><span>string1_key</span> <span>separator</span> <span>string2_key</span><span>:</span> <span>String</span><span>)</span> <span>=</span>
    <span>string1</span> <span>=</span> <span>get_string</span> <span>strings</span> <span>string1_key</span>
    <span>string2</span> <span>=</span> <span>get_string</span> <span>strings</span> <span>string2_key</span>
    <span>push_string</span> <span>strings</span> <span>(</span><span>string1</span> <span>++</span> <span>separator</span> <span>++</span> <span>string2</span><span>)</span>

<span>example</span> <span>(</span><span>strings</span><span>:</span> <span>!</span><span>Strings</span><span>)</span> <span>=</span>
    <span>string1</span> <span>=</span> <span>push_string</span> <span>strings</span> <span>"Hello!"</span>
    <span>string2</span> <span>=</span> <span>push_string</span> <span>strings</span> <span>"Goodbye."</span>

    <span>// We have to pass `strings` to every function in our call stack which needs it</span>
    <span>append_with_separator</span> <span>strings</span> <span>string1</span> <span>" "</span> <span>string2</span>

<span>run_example</span> <span>()</span> <span>=</span>
    <span>mut</span> <span>context</span> <span>=</span> <span>Strings</span> <span>(</span><span>Vec</span><span>.</span><span>new</span> <span>())</span>
    <span>example</span> <span>!</span><span>context</span>
</code></pre></div><p>Using a state effect essentially threads through the context automatically:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>type</span> <span>Strings</span> <span>=</span> <span>vec</span><span>:</span> <span>Vec</span> <span>String</span>
<span>type</span> <span>StringKey</span> <span>=</span> <span>index</span><span>:</span> <span>Usz</span>

<span>push_string</span> <span>(</span><span>string</span><span>:</span> <span>String</span><span>)</span><span>:</span> <span>StringKey</span> <span>can</span> <span>Use</span> <span>Strings</span> <span>=</span>
    <span>mut</span> <span>strings</span> <span>=</span> <span>get</span> <span>()</span> <span>:</span> <span>Strings</span>
    <span>key</span> <span>=</span> <span>StringKey</span> <span>(</span><span>strings</span><span>.</span><span>len</span> <span>())</span>
    <span>strings</span><span>.</span><span>push</span> <span>string</span>
    <span>// We could modify `Use a` to give mutable references or</span>
    <span>// use it via `Use !Strings` but for the sake of example</span>
    <span>// we just make sure to `set` here when mutating `strings`.</span>
    <span>set</span> <span>strings</span>
    <span>key</span>

<span>get_string</span> <span>(</span><span>key</span><span>:</span> <span>StringKey</span><span>)</span><span>:</span> <span>String</span> <span>can</span> <span>Use</span> <span>Strings</span> <span>=</span>
    <span>strings</span> <span>=</span> <span>get</span> <span>()</span> <span>:</span> <span>Strings</span>
    <span>strings</span><span>.</span><span>get</span> <span>key</span> <span>|&gt;</span> <span>unwrap</span>

<span>append_with_separator</span> <span>(</span><span>string1_key</span> <span>separator</span> <span>string2_key</span><span>:</span> <span>String</span><span>)</span> <span>can</span> <span>Use</span> <span>Strings</span> <span>=</span>
    <span>string1</span> <span>=</span> <span>get_string</span> <span>string1_key</span>
    <span>string2</span> <span>=</span> <span>get_string</span> <span>string2_key</span>
    <span>push_string</span> <span>(</span><span>string1</span> <span>++</span> <span>separator</span> <span>++</span> <span>string2</span><span>)</span>

<span>example</span> <span>()</span> <span>can</span> <span>Use</span> <span>Strings</span> <span>=</span>
    <span>string1</span> <span>=</span> <span>push_string</span> <span>"Hello!"</span>
    <span>string2</span> <span>=</span> <span>push_string</span> <span>"Goodbye."</span>
    <span>// No need to pass `strings` manually</span>
    <span>append_with_separator</span> <span>string1</span> <span>" "</span> <span>string2</span>

<span>run_example</span> <span>()</span> <span>=</span>
    <span>context</span> <span>=</span> <span>Strings</span> <span>(</span><span>Vec</span><span>.</span><span>new</span> <span>())</span>
    <span>example</span> <span>()</span> <span>with</span> <span>state</span> <span>context</span>
</code></pre></div><p>From the above we can see we now have to call <code>get</code> or <code>set</code> to access <code>strings</code>
in the primitive operations <code>push_string</code> and <code>get_string</code>, but we no longer have
to explicitly pass around <code>strings</code> in code that just uses these primitive operations.
Generally speaking, this trade-off works well for libraries and abstractions which
will usually completely wrap these operations, eliminating the need for code using
these libraries to care about the internal details of how context objects are passed around.</p>
<p>This pattern pops up in quite a few places. Using a <code>Use a</code> effect locks us to passing
around a particular context type but we can also abstract the functions we need into
an interface. If the interface requires an internal context to implement it will be
automatically passed around with the effect handler. This leads us into the next point:</p>
<h3 id="as-a-substitute-for-globals">As a substitute for globals</h3>
<p>There are a few interfaces programmers may often think of as stateless but actually
require passing around state, often by global values for convenience. Some examples
of this are generating random numbers or simply allocating memory.</p>
<p>Let’s consider an API for random numbers:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>Prng</span><span>.</span><span>new</span> <span>()</span><span>:</span> <span>Prng</span> <span>=</span> <span>...</span>

<span>// Return a random byte</span>
<span>Prng</span><span>.</span><span>random</span> <span>!</span><span>self</span><span>:</span> <span>U8</span> <span>=</span> <span>...</span>
</code></pre></div><p>We would have to require users to explicitly thread through the Prng object through their
program just to use random numbers for this API. This is perhaps a mild inconvenience but
it scales with the size of the program and is notable in that random numbers are usually
a small implementation detail to the program logic. Why should such a small implementation
detail cost so much to the terseness of the program? If we want to avoid this, we may make
the Prng a global, which many languages and libraries do, but this comes with the usual
downsides of globals - most notably requiring the object to be thread safe. If we make
it an effect like the following:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Random</span> <span>with</span>
    <span>// Return a random byte</span>
    <span>random</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>U8</span>
</code></pre></div><p>We gain the ability to thread it through a program mostly for free
(users must still explicitly initialize it somewhere up the call stack with a handler).
Plus, if we later decide we want to use <code>/dev/urandom</code> or some other source of random
numbers instead of the Prng object, we only need to swap out the effect handler. Nothing
else in the call stack needs to be changed.</p>
<p>Similarly, let’s consider an <code>Allocate</code> effect:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Allocate</span> <span>with</span>
    <span>allocate</span><span>:</span> <span>(</span><span>size</span><span>:</span> <span>Usz</span><span>)</span> <span>-&gt;</span> <span>Alignment</span> <span>-&gt;</span> <span>Ptr</span> <span>a</span>
    <span>free</span><span>:</span> <span>Ptr</span> <span>a</span> <span>-&gt;</span> <span>Unit</span>

<span>// example usage</span>
<span>Box</span><span>.</span><span>new</span> <span>(</span><span>elem</span><span>:</span> <span>a</span><span>)</span><span>:</span> <span>Box</span> <span>a</span> <span>can</span> <span>Allocate</span> <span>=</span>
    <span>...</span>
</code></pre></div><p>Such an effect would let us swap how we perform allocations by adding a different effect
handler for it somewhere up the call stack. We could use the global allocator for most calls,
then in a tight loop swap out each allocation in that loop with an arena allocator by just
adding a handler over the loop body.</p>
<p>I could go on with more examples of this (parsers, build systems, …) but I think
you get the gist.</p>
<h3 id="writing-in-a-direct-style">Writing in a Direct Style</h3>
<p>As a small note, effects being things that are thrown/performed rather than dedicated
values does often enable us to write in a more direct style compared to the alternative.</p>
<p>Exceptions are the easy example here, but just know this also applies to asynchronous
functions with <code>Future&lt;T&gt;</code> values or other types that are usually some wrapped effect.</p>
<p>So without exceptions we may use an error union or optional value like <code>Maybe t</code> which can be
<code>Some t</code> or <code>None</code>. If we have several computations returning results, we’ll need
to <code>map</code> the <code>Some</code> value in-between steps:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>// Imagine we have:</span>
<span>try_get_line_from_stdin</span> <span>()</span><span>:</span> <span>Maybe</span> <span>String</span> <span>can</span> <span>IO</span> <span>=</span> <span>...</span>
<span>try_parse</span> <span>(</span><span>s</span><span>:</span> <span>String</span><span>)</span><span>:</span> <span>Maybe</span> <span>U32</span> <span>=</span> <span>...</span>

<span>// read an integer from stdin, returning that value doubled</span>
<span>call_failable_functions</span> <span>()</span><span>:</span> <span>Maybe</span> <span>U32</span> <span>can</span> <span>IO</span> <span>=</span>
    <span>try_get_line_from_stdin</span> <span>()</span> <span>|&gt;.</span><span>and_then</span> <span>fn</span> <span>line</span> <span>-&gt;</span>
        <span>try_parse</span> <span>line</span> <span>|&gt;.</span><span>map</span> <span>fn</span> <span>x</span> <span>-&gt;</span>
            <span>x</span> <span>*</span> <span>2</span>
</code></pre></div><p>This is cumbersome enough languages like Rust provide syntax-sugar like <code>?</code>
to automatically return error values and focus on the good path. That isn’t
needed with effects though. The direct approach just works:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>// Now imagine we have:</span>
<span>get_line_from_stdin</span> <span>()</span><span>:</span> <span>String</span> <span>can</span> <span>Fail</span><span>,</span> <span>IO</span> <span>=</span> <span>...</span>
<span>parse</span> <span>(</span><span>s</span><span>:</span> <span>String</span><span>)</span><span>:</span> <span>U32</span> <span>can</span> <span>Fail</span> <span>=</span> <span>...</span>

<span>// read an integer from stdin, returning that value doubled</span>
<span>call_failable_functions</span> <span>()</span><span>:</span> <span>U32</span> <span>can</span> <span>Fail</span> <span>=</span>
    <span>line</span> <span>=</span> <span>get_line_from_stdin</span> <span>()</span>
    <span>x</span> <span>=</span> <span>parse</span> <span>line</span>
    <span>x</span> <span>*</span> <span>2</span>
</code></pre></div><p>If we need to go off the good path we can just apply a handler:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>call_failable_functions</span> <span>()</span><span>:</span> <span>U32</span> <span>can</span> <span>Fail</span> <span>=</span>
    <span>// get_line_from_stdin's Fail effect is now handled by `default` which returns "42" instead of failing</span>
    <span>line</span> <span>=</span> <span>get_line_from_stdin</span> <span>()</span> <span>with</span> <span>default</span> <span>"42"</span>
    <span>x</span> <span>=</span> <span>parse</span> <span>line</span>
    <span>x</span> <span>*</span> <span>2</span>
</code></pre></div><p>Compared to error unions we never have to wrap our data in <code>Some</code>/<code>Ok</code> and we don’t
have to worry about error types not composing well either:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>// Now imagine we have:</span>
<span>LibraryA</span><span>.</span><span>foo</span> <span>()</span><span>:</span> <span>U32</span> <span>can</span> <span>Throw</span> <span>LibraryA</span><span>.</span><span>Error</span> <span>=</span> <span>...</span>
<span>LibraryB</span><span>.</span><span>bar</span> <span>()</span><span>:</span> <span>U32</span> <span>can</span> <span>Throw</span> <span>LibraryB</span><span>.</span><span>Error</span> <span>=</span> <span>...</span>

<span>type</span> <span>MyError</span> <span>=</span> <span>message</span><span>:</span> <span>String</span>

<span>// Composing the different error types just works</span>
<span>my_function</span> <span>()</span><span>:</span> <span>Unit</span> <span>can</span> <span>Throw</span> <span>LibraryA</span><span>.</span><span>Error</span><span>,</span> <span>Throw</span> <span>LibraryB</span><span>.</span><span>Error</span><span>,</span> <span>Throw</span> <span>MyError</span> <span>=</span>
    <span>x</span> <span>=</span> <span>LibraryA</span><span>.</span><span>foo</span> <span>()</span>
    <span>y</span> <span>=</span> <span>LibraryB</span><span>.</span><span>bar</span> <span>()</span>
    <span>if</span> <span>x</span> <span>+</span> <span>y</span> <span>&lt;</span> <span>10</span> <span>then</span>
        <span>throw</span> <span>(</span><span>MyError</span> <span>"The results of `foo` and `bar` are too small"</span><span>)</span>
</code></pre></div><p>And if it gets too cumbersome to type out all those <code>Throw</code> clauses we can make a type alias
for the effects we want to handle:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>AllErrors</span> <span>=</span> <span>can</span> <span>Throw</span> <span>LibraryA</span><span>.</span><span>Error</span><span>,</span> <span>Throw</span> <span>LibraryB</span><span>.</span><span>Error</span><span>,</span> <span>Throw</span> <span>MyError</span>

<span>my_function</span> <span>()</span><span>:</span> <span>Unit</span> <span>can</span> <span>AllErrors</span> <span>=</span>
    <span>x</span> <span>=</span> <span>LibraryA</span><span>.</span><span>foo</span> <span>()</span>
    <span>y</span> <span>=</span> <span>LibraryB</span><span>.</span><span>bar</span> <span>()</span>
    <span>if</span> <span>x</span> <span>+</span> <span>y</span> <span>&lt;</span> <span>10</span> <span>then</span>
        <span>throw</span> <span>(</span><span>MyError</span> <span>"The results of `foo` and `bar` are too small"</span><span>)</span>
</code></pre></div><p>You can think of this as being similar to using an anonymous union type for error returns.
We don’t need to define explicit wrappers to combine all the errors we use as with tagged
unions, and different error types compose naturally into the union. This also means if a
library <code>can Throw String</code>, and our code also <code>can Throw String</code>, these will combine into
just one <code>can Throw String</code> effect. If we want to keep them separate we need to use a wrapper
type like <code>MyError</code> above.</p>
<hr>
<h2 id="guaranteeing-purity">Guaranteeing Purity</h2>
<p>Most languages with effect handlers (barring perhaps only OCaml) also use effects wherever
side-effects may occur. You may have noticed the <code>can Print</code> or <code>can IO</code> on previous examples,
and it’s true - you can’t use side-effects in Ante without marking that the function may perform
them<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>. Setting aside the cases when <code>IO</code> or print outs are redirected or used for mocking,
these effects are usually handled in <code>main</code> automatically - so what benefit does it actually
provide by making programmers mark these functions?</p>
<p>For one, a number of functions actually require other non-side-effectful (ie. pure) functions
as input. When spawning threads for example, we can’t allow the spawning thread to call into
handlers owned by our thread:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>// Spawn all the given functions as threads and wait until they complete</span>
<span>spawn_all</span> <span>(</span><span>functions</span><span>:</span> <span>Vec</span> <span>(</span><span>Unit</span> <span>-&gt;</span> <span>a</span> <span>pure</span><span>))</span><span>:</span> <span>Vec</span> <span>a</span> <span>can</span> <span>IO</span> <span>=</span> <span>...</span>
</code></pre></div><p>There is also a technique for concurrency called Software Transactional Memory (STM) which
requires pure functions. It works by running many functions simultaneously and if a value is
ever mutated out from under one thread while it was performing a transaction, it just restarts
that transaction. For the curious, there’s a proof of concept implementation of it in Effekt
<a href="https://github.com/effekt-community/effekt-stm/blob/main/stm.effekt">here</a>.</p>
<h3 id="replayability">Replayability</h3>
<p>Another neat aspect of purity is that it can give you replayability similar to the <code>rr</code> debugging
utility. This is the tech needed for deterministic network replication and log structured backups
used in databases and videogame networking.</p>
<p>To implement this you would need two handlers: <code>record</code> and <code>replay</code> which handle the top-level
effect emitted by <code>main</code>. In most languages this is named <code>IO</code>. <code>record</code> would record that the
effect occurred, re-raise it to be handled by the built-in <code>IO</code> handler, and record its result.
Then, on another run <code>replay</code> would handle <code>IO</code> and use the results from the effect log instead
of actually performing them. A particularly smart language could even <code>record</code> by default in
debug builds to always get deterministic debugging!</p>
<h3 id="capability-based-security">Capability-based Security</h3>
<p>The requirement to include all unhandled effects as part of the type signature of a function
helps greatly when auditing the security of libraries. When you call a function
<code>get_pi: Unit -&gt; F64</code> you know that it isn’t doing any sneaky IO in the background. If that
library is later updated to <code>get_pi: Unit -&gt; F64 can IO</code> you know something suspicious is
probably happening, and you’ll get an error in your code as long as the function you’re calling
<code>get_pi</code> in doesn’t already require the <code>IO</code> effect<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup><sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>. This has parallels with <a href="https://joeduffyblog.com/2015/11/10/objects-as-secure-capabilities/">Capability
Based Security</a>
(bonus paper <a href="https://arxiv.org/abs/2005.11444">Designing with Static Capabilities and Effects</a>)
where we must pass around capabilities like <code>fs: FileSystem</code> as explicit objects and only
functions with these objects can access the file system. With algebraic effects it works similarly
except the functions declare effects instead of taking capability parameters. There is a downside
to the effect approach though, and its the same one mentioned above: since effects are automatically
threaded through your program you won’t get an error if a function like <code>get_pi</code> is updated to
require <code>IO</code> if your function also already requires that effect. This can crop up anywhere
effects are used. E.g. with a <code>Fail</code> effect if a library function can’t <code>Fail</code> but then was
updated to possibly <code>Fail</code>, it’ll propagate upward to your existing <code>Fail</code> handler if used in
one of your functions that also can <code>Fail</code>. This may be fine but it may also be unintended depending
on the program. For example, perhaps a user may have preferred to handle it by providing a default value.</p>
<hr>
<p>Whew! That was a lot, but we made it through. Obviously this post focused on the positives of effects
and why I think they’re going to be much more pervasive in the future, but there are negatives as well.
Aside from the accidental handling of effects issue mentioned above, the main downside with effects has
traditionally been efficiency concerns, although it should be said that compilation output of effects has improved
greatly in recent years. Most languages with algebraic effects will optimize “tail-resumptive” effects
(any effect where the last thing the handler does is call <code>resume</code>) into normal closure calls. This is great
because this is already most effects in practice (citation needed - although almost all the examples in this blog
post fit in this category! Exceptions being the only, <em>ahem</em>, exception here since they do not <code>resume</code> at all).
Different languages also have their own strategies for optimizing the remaining
effect handlers: <a href="https://koka-lang.github.io/koka/doc/index.html">Koka</a> uses
<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2021/08/genev-icfp21.pdf">evidence passing</a>
and bubbles up effects to handlers to compile to C without a runtime, <a href="https://antelang.org/">Ante</a> and
<a href="https://github.com/ocaml-multicore/ocaml-effects-tutorial">OCaml</a> limit <code>resume</code> to only being called
at most once which precludes some effects like non-determinism but simplifies resource handling and
allows the internal continuations to be implemented more efficiently (e.g. via segmented stacks), and
<a href="https://effekt-lang.org/">Effekt</a> specializes handlers out of the program completely<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>!</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>The “algebraic” in algebraic effects is mostly a vestigial term. Using “effect handlers” is probably more accurate but I’ll be referring to these mostly as algebraic effects since that is the term most users are familiar with. Also I think it is confusing to say “effect handlers” when talking about the effect itself and not just the handler.&nbsp;<a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>This definition of <code>state</code> completely ignores ownership rules. We’d need a <code>Copy a</code> restriction for a real implementation but I didn’t want to get side-tracked explaining ownership and traits in this post since it isn’t relevant for effects in general. Most languages with algebraic effects allow pervasive sharing of values. Ante with its ownership semantics derived from Rust’s is a bit of a black sheep here.&nbsp;<a href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>The compiler can’t check <code>extern</code> definitions for you, so the type definitions on those have to be trusted. There is also a (planned) way to perform an <code>IO</code> effect only when compiling in debug mode to allow debug printouts while still maintaining effect safety on release mode.&nbsp;<a href="#fnref:3" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>This is one reason why it’s usually preferable to declare the minimum amount of effects. Like saying your function <code>can Print</code> rather than bringing in all of <code>can IO</code>. If you don’t know what the minimal set is, type inference can figure it out for you.&nbsp;<a href="#fnref:4" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>The addition of a new effect to <code>get_pi</code> would also break semantic versioning so it can’t be snuck into a bugfix version.&nbsp;<a href="#fnref:5" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>This comes with the limitation that most functions are second-class but you can still get first-class functions by boxing them and switching to a pay-as-you-go approach. See <a href="https://effekt-lang.org/tour/captures">the docs</a> as well as <a href="https://dl.acm.org/doi/10.1145/3527320">this paper</a>.&nbsp;<a href="#fnref:6" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>

                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: HNRelevant – Add a "related" section to Hacker News (102 pts)]]></title>
            <link>https://github.com/imdj/HNRelevant</link>
            <guid>44078024</guid>
            <pubDate>Sat, 24 May 2025 01:07:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/imdj/HNRelevant">https://github.com/imdj/HNRelevant</a>, See on <a href="https://news.ycombinator.com/item?id=44078024">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:imdj/HNRelevant" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="6pJ6dPAUMKCeh9DdZGK6K_iWTCQFA4Lx00GPM30tg_BXBoqGinCjlQaYRKvabwvO_NyBORrnubw5cYmQTohY_A" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="imdj/HNRelevant" data-current-org="" data-current-owner="imdj" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=imdj%2FHNRelevant" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/imdj/HNRelevant&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="86772ce886b4242f19d9c172d3b3e6e36c692ccb942e06ae80ec2ca3f6caaeda" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-977dbc33-7be8-403d-bffd-9ce042b80ff5" for="icon-button-d697fbe4-d09b-44c1-b6b4-7d0eaca9b9b2" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.19291721a114332ad118.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.22dfbc22ef0a2bf02523.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>

      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modification of acetaminophen to reduce liver toxicity and enhance drug efficacy (210 pts)]]></title>
            <link>https://www.societyforscience.org/regeneron-sts/2025-student-finalists/chloe-lee/</link>
            <guid>44077850</guid>
            <pubDate>Sat, 24 May 2025 00:29:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.societyforscience.org/regeneron-sts/2025-student-finalists/chloe-lee/">https://www.societyforscience.org/regeneron-sts/2025-student-finalists/chloe-lee/</a>, See on <a href="https://news.ycombinator.com/item?id=44077850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
			<main>
				
<article>
	
	<div id="content">
		<main>
			<div>
				<!--  -->
				<!--  -->
				

<div>
                    <h2>
                Chemical Modification of Acetaminophen To Reduce Liver Toxicity and Enhance Drug Efficacy
            </h2>
        
        <p>Chloe studied ways to reduce the toxicity of acetaminophen while keeping its painkilling properties.</p>


                                    <p><a href="https://sspcdn.blob.core.windows.net/files/Documents/SEP/STS/2025/posters/2025_STS_Poster_Lee.Chloe.pdf" target="_blank">
                    View Poster
                </a>
                        </p></div>


<section>
    <p>Chloe Yehwon Lee, 17, of Murphy, explored a way to lower the toxic effects of acetaminophen (Tylenol) on the liver for her Regeneron Science Talent Search chemistry project. The painkiller is used by over 60 million Americans each week, but it is also the leading cause of acute liver failure in the United States and the second most common cause of liver transplant worldwide. Chloe studied chemical changes to the acetaminophen molecule’s benzene ring to see if they could reduce liver toxicity.</p>

            
    </section>


<section>
    <p>Chloe studied chemical changes to the acetaminophen molecule’s benzene ring to see if they could reduce liver toxicity. She developed computer models of the modified molecules to test their ability to relieve pain and toxic effects. She found and synthesized a modified acetaminophen molecule that may be less toxic and may even kill pain better than the original. Her new molecule could be a first step in creating safer and more effective forms of acetaminophen.</p>

            
    </section>


<section>
    <p>Chloe is the child of Jiyong Lee and Eul Hyun Suh. At Plano East Senior High School, she is president of the school’s orchestra program and first violinist in the Greater Dallas Youth Orchestra. She is also the founder and president of her school’s Girls in STEM club.</p>

            
    </section>


<div>
                    <h2>
                Beyond the Project
            </h2>
        
        <p>Chloe is an award-winning violinist who performs with multiple orchestras. She has taught violin to younger students and plays with her school’s Ensembles for Elderly, which performs at assisted living and memory care centers.</p>
<p>FUN FACTS: An eraser as a prized possession? Absolutely! Chloe’s jumbo “For Really Big Mistakes” eraser is the perfect reminder that, together, we learn, fail and improve, one step at a time!</p>


            </div>

				
				

			</div>
					</main>
	</div>

</article>

			</main>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Root for your friends (305 pts)]]></title>
            <link>https://josephthacker.com/personal/2025/05/13/root-for-your-friends.html</link>
            <guid>44077533</guid>
            <pubDate>Fri, 23 May 2025 23:28:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://josephthacker.com/personal/2025/05/13/root-for-your-friends.html">https://josephthacker.com/personal/2025/05/13/root-for-your-friends.html</a>, See on <a href="https://news.ycombinator.com/item?id=44077533">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><img src="https://josephthacker.com/assets/images/root-for-your-friends-banner.png" alt="" width="400">
<strong>Heads‑up:</strong> The concept of this post might seem trivial, but it can improve your career, happiness, and the people you care about. Proceed <strong>without</strong> caution. It only takes about 10 minutes to read.</p>

<hr>

<h2 id="what-is-rootforyourfriends">What <em>is</em> “Root&nbsp;For&nbsp;Your&nbsp;Friends”?</h2>

<p>It’s getting excited for your friends when something good happens, and rejecting jealousy.</p>

<p>It’s deeply believing that <strong>a rising tide lifts all boats</strong>.</p>

<p>It’s understanding that most games in life aren’t zero‑sum; they’re wildly <em>positive‑sum</em>.</p>

<p><strong>Outcomes</strong>: If you read this post, you will be more:</p>
<ul>
  <li>excited for your friends</li>
  <li>generous with your praise and support</li>
  <li>open to collaborating with others</li>
  <li>likely to introduce your friends to people who can help them</li>
</ul>

<p><strong>Note</strong>: I call a friend who roots for you a <strong>hypeman</strong> or a <strong>hype friend</strong>.</p>

<p><img src="https://josephthacker.com/assets/images/Pasted%20image%2020250514102039.png" alt="" width="500"></p>

<h2 id="the-hypeman-flywheel">The Hypeman Flywheel</h2>

<p>The most underrated part of rooting for you friends is that it benefits everyone. A <strong>flywheel</strong> is a concept where each input creates a positive feedback loop that improves the next loop.</p>

<p>A good example of a flywheel in business is where a company collects and utilizes user analytics such that their improvement of the product means more people use the product, which creates more data for what’s working, which creates a better product, which drives morepeople to use the product, etc.</p>

<p>The <strong>friend flywheel</strong> is similar. It’s a positive feedback loop where you root for your friends, building them up and sharing info with them which creates good will and levels them up, and now they’re slightly more successful and informed and they often share info and deals back with you due to feeling closer with you. Then you level up and get access to more info and better deals and you share with them, and the flywheel keeps going.</p>

<h3 id="one-caveat">One Caveat</h3>

<p>Obviously the flywheel only works if your friends reciprocate.</p>

<p>Alex Hormozi says “The best way to change your life is to change your friends.” You don’t have to do this, but if you’re reading this and the quote resonates with you, maybe you should consider it.</p>

<p>Look for friends who aren’t threatened by your success. I talk about spotting friend who will “root for you” in a second.</p>

<h3 id="a-caveat-of-the-caveat">A caveat of the caveat</h3>

<p>Even if you root for the “wrong” friends, it’s still the best way to live. Life is better not feeling jealous. You can sleep so much easier at night by genuinely being happy for your friends, even if they’re a bit jealous of you.</p>

<p><img src="https://josephthacker.com/assets/images/Pasted%20image%2020250514102634.png" alt="" width="400"></p>

<hr>

<h2 id="do-you-have-a-hypeman">Do you have a hypeman?</h2>
<p><br>
Visualize something for me.</p>

<p>You just shipped a side‑project that lands on the front page of Hacker News. 
<strong>Who’s the first person you want to tell?</strong></p>

<p>That person is your <strong>hypeman</strong>—the friend who celebrates your victories like it’s their own milestone. No one comes to mind? Maybe you haven’t really trusted anyone with your wins yet. Let’s identify who you could do that with.</p>

<hr>

<h2 id="how-to-spot-friends-who-root-for-you">How to Spot Friends Who Root For You</h2>

<p>Here’s a list of things to look for that indicate a person might be a great friend:</p>

<ul>
  <li>People who speak honest truth to your face and praise you behind your back.</li>
  <li>People who consistently congratulate you when good things happen.</li>
  <li>People who like and share your stuff.</li>
  <li>People who intro you to people who might be able to help you.</li>
  <li>People who give you different ways to improve your product/brand/life.</li>
  <li>People whose default is “Let’s work on this together!”</li>
  <li>People who give meaningful feedback on your projects.</li>
  <li>People who say “We did it!” even when they did the majority of the work.</li>
</ul>

<hr>
<p><img src="https://josephthacker.com/assets/images/Pasted%20image%2020250514102324.png" alt="" width="500"></p>

<h2 id="how-to-be-a-hypeman">How to Be a Hypeman</h2>
<p>This is a two way street. You can’t expect your friends to root for you if you don’t root for them. Here’s how you can do that:</p>

<ul>
  <li><strong>Be quick to praise</strong>: Train your first instinct to praise.</li>
  <li><strong>Be tactfully honest</strong>: Good people value constructive criticism deeply.</li>
  <li><strong>Expand their vision</strong> – “That’s awesome… and imagine if you… and have you seen this…?”</li>
  <li><strong>Signal‑Boost</strong> – Shares and like their stuff all the time and ask them to tell you when they post.</li>
</ul>

<hr>

<p><img src="https://josephthacker.com/assets/images/Pasted%20image%2020250523103607.png" alt="" width="400"></p>
<h2 id="closing">Closing</h2>

<p><em>Rooting for your friends is the best way to live.</em> I pray you now believe that.</p>

<p>So yeah, go forth, reject jealousy, and root for your friends! This is a message a lot of people need to hear, so I’d love if you shared it.</p>

<p>- Joseph “rez0” Thacker</p>

<p><a href="https://thacker.beehiiv.com/subscribe">Sign up for my email list</a> to know when I post more content like this.
I also <a href="https://x.com/rez0__">post my thoughts on Twitter/X</a>.</p>

<meta name="twitter:card" content="summary_large_image">

<meta name="twitter:site" content="@rez0__">

<meta name="twitter:creator" content="@rez0__">

<meta property="og:url" content="https://josephthacker.com/personal/2025/05/13/root-for-your-friends.html">

<meta property="og:title" content="Root for Your Friends">

<meta property="og:description" content="Discover the power of rooting for your friends and how it can amplify success for everyone involved.">

<meta property="og:image" content="https://josephthacker.com/assets/images/root-for-your-friends-banner.png">


    




  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The world of Japan's PC-98 computer (141 pts)]]></title>
            <link>https://strangecomforts.com/the-strange-world-of-japans-pc-98-computer/</link>
            <guid>44076501</guid>
            <pubDate>Fri, 23 May 2025 20:51:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://strangecomforts.com/the-strange-world-of-japans-pc-98-computer/">https://strangecomforts.com/the-strange-world-of-japans-pc-98-computer/</a>, See on <a href="https://news.ycombinator.com/item?id=44076501">Hacker News</a></p>
Couldn't get https://strangecomforts.com/the-strange-world-of-japans-pc-98-computer/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built a more productive way to manage AI chats (144 pts)]]></title>
            <link>https://contextch.at</link>
            <guid>44076449</guid>
            <pubDate>Fri, 23 May 2025 20:46:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://contextch.at">https://contextch.at</a>, See on <a href="https://news.ycombinator.com/item?id=44076449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-overlay-container="true"><main><div><section><article><h2>Chat with <span>Context</span> from Web, Files and GitHub</h2><p>Easily set up multiple projects with web, file, and GitHub context. Start a new chat, and leverage your saved context to get instant answers and insights</p></article></section><section id="features"><div><h2>Features</h2><p>Context centric. Ingest content from anywhere, sites, files, GitHub repos then chat with it using AI. Instant knowledge base.</p></div><div><p><span></span><span>Effortless Ingestion</span></p><p><span></span><span>Smart Extraction</span></p><p><span></span><span>Unified Context</span></p><p><span></span><span>AI Conversations</span></p><p><span></span><span>Persistent Knowledge</span></p><p><span></span><span>Project Power</span></p><p><span></span><span>Multi-Chat Magic</span></p><p><span></span><span>Simple Pricing</span></p><p><span></span><span>GitHub Import</span></p></div><div><div tabindex="-1"><p><h2>Context builder</h2></p><div><p>Easily build your knowledge base with the context builder</p><ul><li><span>✓</span>Web, File and GitHub content ingestion</li><li><span>✓</span>Fast import content</li><li><span>✓</span>Multiple chats per project</li></ul></div></div><div tabindex="-1"><p><h2>Stay organized</h2></p><div><p>Manage multiple projects and chats seamlessly</p><ul><li><span>✓</span>Persistent project context</li><li><span>✓</span>Multiple chats per project</li><li><span>✓</span>No more repetitive work</li></ul></div></div><div tabindex="-1"><p><h2>Fair pricing</h2></p><div><p>Pay only for what you use with our credit system</p><ul><li><span>✓</span>Credit based system</li><li><span>✓</span>No subscription traps</li><li><span>✓</span>Flexible AI model selection</li></ul></div></div></div></section><section id="comparisons"><p><h2>Comparisons</h2></p><div><div tabindex="-1"><p><h3>Other AI Chat Tools</h3></p><div><ul><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 6l-12 12"></path><path d="M6 6l12 12"></path></svg><span>Manual data ingestion and tedious copy-pasting</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 6l-12 12"></path><path d="M6 6l12 12"></path></svg><span>Fragmented conversations</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 6l-12 12"></path><path d="M6 6l12 12"></path></svg><span>Importing your data requires payment</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 6l-12 12"></path><path d="M6 6l12 12"></path></svg><span>Single model for all projects and chats</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 6l-12 12"></path><path d="M6 6l12 12"></path></svg><span>Subscriptions!</span></li></ul></div></div><div tabindex="-1"><p><h3>With <a href="https://contextch.at/"><span>ContextChat</span></a></h3></p><div><ul><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12l5 5l10 -10"></path></svg><span>Build the context through the context builder</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12l5 5l10 -10"></path></svg><span>Flexible, pay-as-you-go pricing</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12l5 5l10 -10"></path></svg><span>Multiple projects and chats</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12l5 5l10 -10"></path></svg><span>Change AI model easily in the middle of a chat</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12l5 5l10 -10"></path></svg><span>Free models available, no credit card required</span></li></ul></div></div></div><div><h2>See how <!-- -->ContextChat<!-- --> compares to other tools</h2><p>We've created a detailed comparison of <!-- -->ContextChat<!-- --> vs traditional AI chat tools.</p><a aria-label="See SEO AI Pal vs competitors comparison" href="https://contextch.at/alternatives" tabindex="0" role="button" shadow="none">View comparison</a></div></section><section id="pricing"><h2>Pricing</h2><div tabindex="-1"><div><div><p>Always </p><p> using no-cost AI models</p></div><p><span>No credit card required</span></p></div><hr role="separator"><div><div><svg xmlns="http://www.w3.org/2000/svg" width="20" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12l5 5l10 -10"></path></svg><p><span>Web, File and GitHub Ingestion</span></p></div><div><svg xmlns="http://www.w3.org/2000/svg" width="20" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12l5 5l10 -10"></path></svg><p><span>Multiple chats per project</span></p></div><div><svg xmlns="http://www.w3.org/2000/svg" width="20" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12l5 5l10 -10"></path></svg><p><span>Flexible AI model selection</span></p></div><div><svg xmlns="http://www.w3.org/2000/svg" width="20" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12l5 5l10 -10"></path></svg><p><span>Pay-as-you-go credit system</span></p></div></div></div></section><div id="faq"><h2>Frequently Asked Questions</h2><p>Find answers to common questions about ContextChat.</p></div></div><!--$--><!--/$--><!--$--><!--/$--></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Genetic Boids Web Simulation (145 pts)]]></title>
            <link>https://attentionmech.github.io/genetic-boids/</link>
            <guid>44075911</guid>
            <pubDate>Fri, 23 May 2025 19:40:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://attentionmech.github.io/genetic-boids/">https://attentionmech.github.io/genetic-boids/</a>, See on <a href="https://news.ycombinator.com/item?id=44075911">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="control-panel">

    
    <h4>by @attentionmech
    </h4>
    <div>
      <h3>Population</h3>
      <p><label>Boid Count</label>
        
        <span id="boidCountDisplay">500</span>
      </p>
      
    </div>

    <div>
      <h3>Movement</h3>
      <p><label>Max Speed</label>
        
        <span id="maxSpeedDisplay">2.5</span>
      </p>
      <p><label>Max Force</label>
        
        <span id="maxForceDisplay">0.1</span>
      </p>
      <p><label>Initial Speed Range</label>
        
        <span id="initSpeedMinDisplay">1</span>
        
        <span id="initSpeedMaxDisplay">3</span>
      </p>
    </div>

    <div>
      <h3>Flocking Behavior</h3>
      <p><label>Alignment</label>
        
        <span id="alignWeightDisplay">1.0</span>
      </p>
      <p><label>Separation</label>
        
        <span id="sepWeightDisplay">1.5</span>
      </p>
      <p><label>Cohesion</label>
        
        <span id="cohWeightDisplay">1.0</span>
      </p>
    </div>

    <div>
      <h3>Perception Ranges</h3>
      <p><label>Alignment Range</label>
        
        <span id="alignRangeDisplay">40</span>
      </p>
      <p><label>Separation Range</label>
        
        <span id="sepRangeDisplay">20</span>
      </p>
      <p><label>Cohesion Range</label>
        
        <span id="cohRangeDisplay">40</span>
      </p>
    </div>

    


    <div>
      <h3>Genetic Signaling</h3>
      <p><label>Signal Probability</label>
        
        <span id="signalProbDisplay">0.002</span>
      </p>
      <p><label>Signal Range</label>
        
        <span id="signalRangeDisplay">50</span>
      </p>
      <p><label>Signal Force</label>
        
        <span id="signalForceDisplay">0.1</span>
      </p>
      <p><label>Genome Length</label>
        
        <span id="genomeLengthDisplay">6</span>
      </p>
    </div>

    <div>
      <h3>Visual</h3>
      <p><label>Boid Size</label>
        
        <span id="boidSizeDisplay">1.0</span>
      </p>
      <p><label>Signal Line Alpha</label>
        
        <span id="signalAlphaDisplay">150</span>
      </p>
      <p><label>Background</label>
        
        <span id="bgBrightnessDisplay">0</span>
      </p>
    </div>

    <div>
      <h3>Performance</h3>
      <p><label>Grid Cell Size</label>
        
        <span id="cellSizeDisplay">50</span>
      </p>
      <p><label>Frame Rate Target</label>
        
        <span id="targetFPSDisplay">60</span>
      </p>
    </div>

    <div>
      <h3>Presets</h3>
      
    </div>

    <div>
      <h3>Danger Zone</h3>
      
    </div>


    <h3><a href="https://x.com/@attentionmech">@attentionmech</a>
      </h3><h3><a href="https://github.com/attentionmech/genetic-boids">github</a>
          </h3><h3>

            <p><strong>STATS:</strong><br>
              FPS: <span id="fps">--</span><br>
              Active Signals: <span id="signalCount">--</span><br>
              Avg Speed: <span id="avgSpeed">--</span><br>
              Clusters: <span id="clusterCount">--</span>
            </p>
  </h3></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: DoubleMemory – more efficient local-first read-it-later app (148 pts)]]></title>
            <link>https://doublememory.com</link>
            <guid>44075451</guid>
            <pubDate>Fri, 23 May 2025 18:55:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doublememory.com">https://doublememory.com</a>, See on <a href="https://news.ycombinator.com/item?id=44075451">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: hcker.news – an ergonomic, timeline-based Hacker News front page (163 pts)]]></title>
            <link>https://hcker.news</link>
            <guid>44075353</guid>
            <pubDate>Fri, 23 May 2025 18:44:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hcker.news">https://hcker.news</a>, See on <a href="https://news.ycombinator.com/item?id=44075353">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>Feed Settings</p>
            <!-- View Row -->
            <div>
              <p>View</p>
              <div id="mode-options">
                <p><a href="#" data-mode="timeline">Timeline</a>
                <a href="#" data-mode="frontpage">Frontpage</a>
                <a href="#" data-mode="aggregate">Aggregate</a>
              </p></div>
            </div>

            <!-- Timeline Specific Rows (conditionally visible) -->
            <div id="timeline-filter-controls">
              <div>
                <p>Top</p>
                <div id="sort-by-options">
                  <p><a href="#" data-value="score">Points</a>
                  <a href="#" data-value="descendants">Comments</a>
                </p></div>
              </div>
              <div>
                <p>Show</p>
                <div id="timeline-filter-options">
                  <p><a href="#" data-value="top10">Top 10</a>
                  <a href="#" data-value="top20">Top 20</a>
                  <a href="#" data-value="top50">Top 50</a>
                  <a href="#" data-value="top100">Top 100</a>
                </p></div>
              </div>
              <div>
                <p>Period</p>
                <div id="timeline-groupby-options">
                  <p><a href="#" data-value="day">Day</a>
                  <a href="#" data-value="week">Week</a>
                  <a href="#" data-value="month">Month</a>
                </p></div>
              </div>
            </div>

            <!-- Frontpage Specific Rows (conditionally visible) -->
            

            <!-- Aggregate Specific Rows (conditionally visible) -->
            <div id="aggregate-filter-controls">
              <div>
                <p>Period</p>
                <div id="aggregate-period-options">
                  <p><a href="#" data-value="day">Day</a>
                  <a href="#" data-value="week">Week</a>
                  <a href="#" data-value="month">Month</a>
                  <a href="#" data-value="year">Year</a>
                  <a href="#" data-value="range">Range</a>
                </p></div>
              </div>
              <div id="aggregate-slider-row">
                <p>Range</p>
                
              </div>
            </div>
            <!-- Filters Row -->
            <div>
              <p>Filters</p>
              
            </div>
            <!-- Advanced Feed Settings Toggle -->
            <div>
              <p><a href="#" id="advanced-feed-settings-toggle-link">show advanced feed filters...</a>
            </p></div>
            <!-- Advanced Feed Settings Items - initially hidden -->
            <div id="exclude-filter-row">
              <p>Exclude</p>
              <p>
                <span>i
                  <span>
                    Filter out stories containing these words in the title.
                    Separate multiple terms with commas (e.g., "crypto, NFT, politics").
                    Case-insensitive.
                  </span>
                </span>
              </p>
            </div>
            <div id="include-filter-row">
              <p>Include</p>
              <p>
                <span>i
                  <span>
                    Only show stories containing these words in the title.
                    Separate multiple terms with commas (e.g., "golang, webdev").
                    Case-insensitive.
                  </span>
                </span>
              </p>
            </div>
            <div id="min-votes-row">
              <p>Min Votes</p>
              <p>
                <span>i
                  <span>
                    Only show stories with at least this many votes (score).
                    Leave blank or 0 to disable.
                  </span>
                </span>
              </p>
            </div>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Attention Wasn't All We Needed (120 pts)]]></title>
            <link>https://www.stephendiehl.com/posts/post_transformers/</link>
            <guid>44075105</guid>
            <pubDate>Fri, 23 May 2025 18:14:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stephendiehl.com/posts/post_transformers/">https://www.stephendiehl.com/posts/post_transformers/</a>, See on <a href="https://news.ycombinator.com/item?id=44075105">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
<p>There's a lot of modern techniques that have been developed since the original <em>Attention Is All You Need</em> paper. Let's look at some of the most important ones that have been developed over the years and try to implement the basic ideas as succinctly as possible. We'll use the Pytorch framework for most of the examples. Note that most of these examples are highly simplified sketches of the core ideas, if you want the full implementation please read the original paper or the production code in frameworks like PyTorch or Jax.</p>
<ol>
<li><a href="#group-query-attention">Group Query Attention</a></li>
<li><a href="#multi-head-latent-attention">Multi-head Latent Attention</a></li>
<li><a href="#flash-attention">Flash Attention</a></li>
<li><a href="#ring-attention">Ring Attention</a></li>
<li><a href="#pre-normalization">Pre-normalization</a></li>
<li><a href="#rmsnorm">RMSNorm</a></li>
<li><a href="#swiglu">SwiGLU</a></li>
<li><a href="#rotary-positional-embedding">Rotary Positional Embedding</a></li>
<li><a href="#mixture-of-experts">Mixture of Experts</a></li>
<li><a href="#learning-rate-warmup">Learning Rate Warmup</a></li>
<li><a href="#cosine-schedule">Cosine Schedule</a></li>
<li><a href="#adamw-optimizer">AdamW Optimizer</a></li>
<li><a href="#multi-token-prediction">Multi-token Prediction</a></li>
<li><a href="#speculative-decoding">Speculative Decoding</a></li>
</ol>
<h2 id="group-query-attention" tabindex="-1">Group Query Attention</h2>
<p>Ok starting off in no particular order, <strong>Grouped Query Attention</strong> is a technique to reduce the memory usage of the KV cache during inference.  Group Query Attention is an architectural optimization for the standard multi-head attention mechanism. The core idea behind GQA is based on the observation that the computational bottleneck and memory footprint in MHA are heavily influenced by the size of the K and V projections and their corresponding caches. GQA proposes to reduce this cost by sharing a single set of K and V projections across multiple Q heads. Instead of having \(N_h\) distinct heads for Q, K, and V (as in MHA), GQA uses \(N_h\) query heads but only \(N_{kv}\) key/value heads, where \(N_{kv} &lt; N_h\) and \(N_h\) is typically a multiple of \(N_{kv}\). These \(N_h\) query heads are divided into \(N_{kv}\) groups, with each group of \(N_h / N_{kv}\) query heads attending to the <em>same</em> key and value head. This structure significantly reduces the parameter count for <code>K</code> and <code>V</code> projection matrices and, more importantly, shrinks the size of the K/V cache needed during autoregressive decoding.</p>
<p>Let the input sequence representation be \(X \in \mathbb{R}^{L \times d_{\text{model}}}\), where \(L\) is the sequence length and \(d_{\text{model}}\) is the embedding dimension. GQA first projects \(X\) into queries, keys, and values using different linear transformations: \(Q = XW_Q\), \(K = XW_K\), and \(V = XW_V\). Here, \(W_Q \in \mathbb{R}^{d_{\text{model}} \times (N_h d_k)}\), \(W_K \in \mathbb{R}^{d_{\text{model}} \times (N_{kv} d_k)}\), and \(W_V \in \mathbb{R}^{d_{\text{model}} \times (N_{kv} d_k)}\), where \(d_k\) is the dimension of each head (<code>head_dim</code>). These are reshaped into \(N_h\) query heads \(Q_i \in \mathbb{R}^{L \times d_k}\) (\(i=1...N_h\)) and \(N_{kv}\) key/value heads \(K_j, V_j \in \mathbb{R}^{L \times d_k}\) (\(j=1...N_{kv}\)). The key step in GQA is sharing: for the \(i\)-th query head, the corresponding key and value heads are \(K_{\lceil i / g \rceil}\) and \(V_{\lceil i / g \rceil}\), where \(g = N_h / N_{kv}\) is the group size (number of queries per KV head). The attention output for the \(i\)-th head is computed as:</p>
<p>$$<br>
\text{Attention}(Q_i, K_{\lceil i / g \rceil}, V_{\lceil i / g \rceil}) = \text{softmax}\left(\frac{Q_i K_{\lceil i / g \rceil}^T}{\sqrt{d_k}}\right)V_{\lceil i / g \rceil}<br>
$$</p>
<p>In implementation, we do this by computing the \(N_{kv}\) key/value heads and then repeating or interleaving them \(g\) times to match the \(N_h\) query heads before the batched matrix multiplication for attention scores, as shown in the <a href="https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html"><code>repeat_interleave</code></a> step in the example code. Finally, the outputs of all \(N_h\) heads are concatenated and passed through an output projection \(W_O\).</p>
<p>GQA is primarily used as a technique to accelerate inference speed and reduce memory requirements without significantly compromising model performance. During autoregressive generation, the previously computed keys and values for the context sequence are cached and reused for subsequent token predictions. The size of this K/V cache is directly proportional to the number of K/V heads (\(N_{kv}\) in GQA, \(N_h\) in MHA). By reducing \(N_{kv}\), GQA drastically cuts down the memory bandwidth needed to load the K/V cache at each decoding step, which is the main performance bottleneck.</p>
<p>While it might slightly reduce the model's representational capacity compared to MHA (as K/V projections are shared), empirical results show that GQA achieves a favorable trade-off, maintaining most of the quality of MHA while offering substantial speedups and memory savings, making it popular for deploying large models efficiently. Multi-query attention (MQA), where \(N_{kv}=1\), is an extreme form of GQA.</p>
<pre><code><span>class</span> <span>GroupQueryAttention</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> num_heads<span>,</span> num_kv_heads<span>=</span><span>None</span><span>,</span> head_dim<span>=</span><span>64</span><span>,</span> dropout<span>=</span><span>0.0</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>dim <span>=</span> dim
        self<span>.</span>num_heads <span>=</span> num_heads
        self<span>.</span>num_kv_heads <span>=</span> num_kv_heads <span>if</span> num_kv_heads <span>else</span> num_heads
        self<span>.</span>head_dim <span>=</span> head_dim
        
        <span># Ensure num_heads is divisible by num_kv_heads</span>
        <span>assert</span> self<span>.</span>num_heads <span>%</span> self<span>.</span>num_kv_heads <span>==</span> <span>0</span><span>,</span> <span>"num_heads must be divisible by num_kv_heads"</span>
        
        <span># Number of queries per key-value head</span>
        self<span>.</span>num_queries_per_kv <span>=</span> self<span>.</span>num_heads <span>//</span> self<span>.</span>num_kv_heads
        
        <span># Projections</span>
        self<span>.</span>q_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>k_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> self<span>.</span>num_kv_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>v_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> self<span>.</span>num_kv_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>o_proj <span>=</span> nn<span>.</span>Linear<span>(</span>num_heads <span>*</span> head_dim<span>,</span> dim<span>)</span>
        self<span>.</span>dropout <span>=</span> nn<span>.</span>Dropout<span>(</span>dropout<span>)</span>
        
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> x<span>.</span>shape
        
        <span># Project to queries, keys, values</span>
        q <span>=</span> self<span>.</span>q_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        k <span>=</span> self<span>.</span>k_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_kv_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        v <span>=</span> self<span>.</span>v_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_kv_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Transpose for attention computation</span>
        q <span>=</span> q<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        k <span>=</span> k<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_kv_heads, seq_len, head_dim]</span>
        v <span>=</span> v<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_kv_heads, seq_len, head_dim]</span>
        
        <span># Repeat k,v for each query head in the group</span>
        k <span>=</span> k<span>.</span>repeat_interleave<span>(</span>self<span>.</span>num_queries_per_kv<span>,</span> dim<span>=</span><span>1</span><span>)</span>
        v <span>=</span> v<span>.</span>repeat_interleave<span>(</span>self<span>.</span>num_queries_per_kv<span>,</span> dim<span>=</span><span>1</span><span>)</span>
        
        <span># Scaled dot-product attention</span>
        scale <span>=</span> <span>1.0</span> <span>/</span> math<span>.</span>sqrt<span>(</span>self<span>.</span>head_dim<span>)</span>
        attn <span>=</span> torch<span>.</span>matmul<span>(</span>q<span>,</span> k<span>.</span>transpose<span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>)</span> <span>*</span> scale
        
        <span># Apply mask if provided</span>
        <span>if</span> mask <span>is</span> <span>not</span> <span>None</span><span>:</span>
            attn <span>=</span> attn<span>.</span>masked_fill<span>(</span>mask <span>==</span> <span>0</span><span>,</span> <span>-</span><span>1e9</span><span>)</span>
        
        <span># Softmax and dropout</span>
        attn <span>=</span> torch<span>.</span>softmax<span>(</span>attn<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        attn <span>=</span> self<span>.</span>dropout<span>(</span>attn<span>)</span>
        
        <span># Apply attention to values</span>
        out <span>=</span> torch<span>.</span>matmul<span>(</span>attn<span>,</span> v<span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        out <span>=</span> out<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads <span>*</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Output projection</span>
        out <span>=</span> self<span>.</span>o_proj<span>(</span>out<span>)</span>
        
        <span>return</span> out
</code></pre>
<h2 id="multi-head-latent-attention" tabindex="-1">Multi-head Latent Attention</h2>
<p><strong>Multi-head Latent Attention</strong> introduces a set of learnable "latent" vectors that act as an intermediary bottleneck between the input sequence elements. The core idea is to alleviate the quadratic computational cost \(O(L^2)\), where \(L\) is the sequence length, inherent in standard self-attention mechanisms. Instead of allowing every input element to attend directly to every other element, inputs first attend to a fixed number of latent units (\(N_{\text{latents}}\)), and these latents then attend back to the inputs (or variations thereof). This effectively decouples the direct interaction within the long input sequence, replacing it with two cross-attention steps involving the much smaller set of latents. This approach assumes that the essential information from the input sequence can be effectively summarized or compressed into these latent representations, thus maintaining representational power while significantly reducing computation, especially when \(N_{\text{latents}} \ll L\).</p>
<p>The mechanism involves two main stages of attention computation, typically within a multi-head framework. Let the input sequence be \(X \in \mathbb{R}^{L \times d}\) and the learnable latent array be \(L \in \mathbb{R}^{N_{\text{latents}} \times d}\). Both \(X\) and \(L\) are projected into Q, K, and V using shared or separate projection matrices. Let's denote the input projections as \(Q_X, K_X, V_X\) and latent projections as \(Q_L, K_L, V_L\), split across multiple heads. The first cross-attention step computes how latents attend to the input: the latent queries \(Q_L\) attend to the input keys \(K_X\) and aggregate information from input values \(V_X\). The attention output for the latents is</p>
<p>$$<br>
H_L = \text{Attention}(Q_L, K_X, V_X) = \text{softmax}\left(\frac{Q_L K_X^T}{\sqrt{d_k}}\right) V_X<br>
$$</p>
<p>Where \(d_k\) is the head dimension. In the second cross-attention step, the input queries \(Q_X\) attend to the keys derived from the latents (e.g., \(K_L\)) and aggregate information from the values associated with the latents (which could be \(V_L\) or, as implemented in the example code, the updated latent representation \(H_L\)). The final output \(O\) is then</p>
<p>$$<br>
O = \text{Attention}(Q_X, K_L, H_L) = \text{softmax}\left(\frac{Q_X K_L^T}{\sqrt{d_k}}\right) H_L<br>
$$</p>
<p>These operations are performed independently for each head, and the results are concatenated and passed through a final linear projection.</p>
<p>Multi-head Latent Attention is primarily employed in architectures designed to handle very long sequences or high-dimensional inputs where standard self-attention is computationally infeasible. Examples include processing long documents, high-resolution images (treating patches as a sequence), audio signals, or video data. By using a fixed number of latents (\(N_{\text{latents}}\)), the computational complexity is reduced from \(O(L^2)\) to \(O(L \cdot N_{\text{latents}})\), making it scalable to much larger inputs. The learnable latent vectors \(L\) (initialized randomly and updated via backpropagation, as seen in <code>self.latents = nn.Parameter(...)</code> in the code) adapt during training to function as a compressed representation or memory bank relevant to the task. While this introduces an information bottleneck, potentially limiting fine-grained local interactions compared to full self-attention, it excels at capturing global context efficiently and has proven effective in various modalities, enabling Transformer-like architectures to be applied to previously challenging domains due to sequence length constraints.</p>
<pre><code><span>class</span> <span>MultiHeadLatentAttention</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> num_heads<span>,</span> num_latents<span>=</span><span>64</span><span>,</span> head_dim<span>=</span><span>64</span><span>,</span> dropout<span>=</span><span>0.0</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>dim <span>=</span> dim
        self<span>.</span>num_heads <span>=</span> num_heads
        self<span>.</span>num_latents <span>=</span> num_latents
        self<span>.</span>head_dim <span>=</span> head_dim
        
        <span># Projections</span>
        self<span>.</span>q_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>k_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>v_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>o_proj <span>=</span> nn<span>.</span>Linear<span>(</span>num_heads <span>*</span> head_dim<span>,</span> dim<span>)</span>
        
        <span># Latent vectors (learned)</span>
        self<span>.</span>latents <span>=</span> nn<span>.</span>Parameter<span>(</span>torch<span>.</span>randn<span>(</span><span>1</span><span>,</span> num_latents<span>,</span> dim<span>)</span><span>)</span>
        
        self<span>.</span>dropout <span>=</span> nn<span>.</span>Dropout<span>(</span>dropout<span>)</span>
        
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> x<span>.</span>shape
        
        <span># Get latents for this batch</span>
        latents <span>=</span> self<span>.</span>latents<span>.</span>expand<span>(</span>batch_size<span>,</span> <span>-</span><span>1</span><span>,</span> <span>-</span><span>1</span><span>)</span>
        
        <span># Project inputs to queries, keys, values</span>
        q_x <span>=</span> self<span>.</span>q_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        k_x <span>=</span> self<span>.</span>k_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        v_x <span>=</span> self<span>.</span>v_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Project latents to queries, keys, values</span>
        q_latents <span>=</span> self<span>.</span>q_proj<span>(</span>latents<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> self<span>.</span>num_latents<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        k_latents <span>=</span> self<span>.</span>k_proj<span>(</span>latents<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> self<span>.</span>num_latents<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        v_latents <span>=</span> self<span>.</span>v_proj<span>(</span>latents<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> self<span>.</span>num_latents<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Transpose for attention computation</span>
        q_x <span>=</span> q_x<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        k_x <span>=</span> k_x<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        v_x <span>=</span> v_x<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        
        q_latents <span>=</span> q_latents<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, num_latents, head_dim]</span>
        k_latents <span>=</span> k_latents<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, num_latents, head_dim]</span>
        v_latents <span>=</span> v_latents<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, num_latents, head_dim]</span>
        
        <span># Scale factor for attention</span>
        scale <span>=</span> <span>1.0</span> <span>/</span> math<span>.</span>sqrt<span>(</span>self<span>.</span>head_dim<span>)</span>
        
        <span># Compute latent-to-input attention</span>
        attn_latent_to_input <span>=</span> torch<span>.</span>matmul<span>(</span>q_latents<span>,</span> k_x<span>.</span>transpose<span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>)</span> <span>*</span> scale
        
        <span># Apply mask if provided</span>
        <span>if</span> mask <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span># Expand mask for the latent queries</span>
            latent_mask <span>=</span> mask<span>.</span>unsqueeze<span>(</span><span>1</span><span>)</span><span>.</span>expand<span>(</span><span>-</span><span>1</span><span>,</span> self<span>.</span>num_heads<span>,</span> <span>-</span><span>1</span><span>,</span> <span>-</span><span>1</span><span>)</span>
            attn_latent_to_input <span>=</span> attn_latent_to_input<span>.</span>masked_fill<span>(</span>latent_mask <span>==</span> <span>0</span><span>,</span> <span>-</span><span>1e9</span><span>)</span>
        
        <span># Softmax and dropout</span>
        attn_latent_to_input <span>=</span> torch<span>.</span>softmax<span>(</span>attn_latent_to_input<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        attn_latent_to_input <span>=</span> self<span>.</span>dropout<span>(</span>attn_latent_to_input<span>)</span>
        
        <span># Apply attention weights to input values</span>
        latent_output <span>=</span> torch<span>.</span>matmul<span>(</span>attn_latent_to_input<span>,</span> v_x<span>)</span>  <span># [batch_size, num_heads, num_latents, head_dim]</span>
        
        <span># Compute input-to-latent attention</span>
        attn_input_to_latent <span>=</span> torch<span>.</span>matmul<span>(</span>q_x<span>,</span> k_latents<span>.</span>transpose<span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>)</span> <span>*</span> scale
        
        <span># Softmax and dropout</span>
        attn_input_to_latent <span>=</span> torch<span>.</span>softmax<span>(</span>attn_input_to_latent<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        attn_input_to_latent <span>=</span> self<span>.</span>dropout<span>(</span>attn_input_to_latent<span>)</span>
        
        <span># Updated latent values are used as values for input-to-latent attention</span>
        output <span>=</span> torch<span>.</span>matmul<span>(</span>attn_input_to_latent<span>,</span> latent_output<span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        
        <span># Reshape and apply output projection</span>
        output <span>=</span> output<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads <span>*</span> self<span>.</span>head_dim<span>)</span>
        output <span>=</span> self<span>.</span>o_proj<span>(</span>output<span>)</span>
        
        <span>return</span> output
</code></pre>
<h2 id="flash-attention" tabindex="-1">Flash Attention</h2>
<p><strong>Flash Attention</strong> (particularly the latest implementation <a href="https://pytorch.org/blog/flashattention-3/">FlashAttention-3</a>) addresses the significant memory bottleneck inherent in standard self-attention mechanisms within Transformers, particularly for long sequences. The conventional approach computes the full attention score matrix \( S = QK^T \), where \(Q, K \in \mathbb{R}^{N \times d}\) are the query and key matrices for a sequence of length \(N\). This requires storing the \(N \times N\) matrix \(S\), leading to \(O(N^2)\) memory complexity with respect to sequence length. This becomes prohibitive for large \(N\). Flash Attention overcomes this by avoiding the materialization and storage of the full \(S\) matrix in the GPU's slow high bandwidth memory. Instead, it leverages tiling and recomputation techniques, processing the attention computation in smaller blocks that fit into the much faster on-chip SRAM.</p>
<p>The core mechanism involves breaking the Q, K, and V matrices into blocks. Flash Attention iteratively loads blocks of K and V into SRAM, and for each block of Q, it computes the attention scores against the current K block also residing in SRAM. Crucially, it employs an online softmax algorithm. Instead of computing the full softmax denominator across all keys at once, it maintains running statistics (the maximum score seen so far for numerical stability, and the cumulative sum of exponentiated scores for normalization) as it iterates through the K/V blocks. This allows it to compute the correctly scaled attention output block-by-block without ever needing the complete \(N \times N\) matrix. By keeping intermediate results primarily within the fast SRAM and minimizing data transfer to and from high bandwidth memory, Flash Attention significantly reduces the memory footprint related to sequence length from \(O(N^2)\) down to \(O(N)\) (dominated by storing Q, K, V themselves) and achieves substantial speedups due to improved memory access patterns.</p>
<p>In practice the FlashAttention-3 implementation is a family of highly-optimized CUDA kernels that are designed to be efficient for different hardware configurations. But a minimal toy implementation in PyTorch is shown below:</p>
<pre><code><span>class</span> <span>FlashAttention</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> num_heads<span>,</span> head_dim<span>=</span><span>64</span><span>,</span> dropout<span>=</span><span>0.0</span><span>,</span> block_size<span>=</span><span>1024</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>dim <span>=</span> dim
        self<span>.</span>num_heads <span>=</span> num_heads
        self<span>.</span>head_dim <span>=</span> head_dim
        self<span>.</span>block_size <span>=</span> block_size
        
        <span># Projections</span>
        self<span>.</span>q_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>k_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>v_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>o_proj <span>=</span> nn<span>.</span>Linear<span>(</span>num_heads <span>*</span> head_dim<span>,</span> dim<span>)</span>
        
        self<span>.</span>dropout <span>=</span> nn<span>.</span>Dropout<span>(</span>dropout<span>)</span>
        
    <span>def</span> <span>_flash_attention_forward</span><span>(</span>self<span>,</span> q<span>,</span> k<span>,</span> v<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        <span># This is a simplified approximation of Flash Attention</span>
        <span># In practice, FlashAttention uses custom CUDA kernels for tiled attention</span>
        
        batch_size<span>,</span> num_heads<span>,</span> seq_len<span>,</span> head_dim <span>=</span> q<span>.</span>shape
        scale <span>=</span> <span>1.0</span> <span>/</span> math<span>.</span>sqrt<span>(</span>head_dim<span>)</span>
        
        <span># Initialize output and attention statistics</span>
        output <span>=</span> torch<span>.</span>zeros_like<span>(</span>q<span>)</span>
        normalizer <span>=</span> torch<span>.</span>zeros<span>(</span><span>(</span>batch_size<span>,</span> num_heads<span>,</span> seq_len<span>,</span> <span>1</span><span>)</span><span>,</span> device<span>=</span>q<span>.</span>device<span>)</span>
        
        <span># Process blocks of keys and values</span>
        <span>for</span> block_start <span>in</span> <span>range</span><span>(</span><span>0</span><span>,</span> seq_len<span>,</span> self<span>.</span>block_size<span>)</span><span>:</span>
            block_end <span>=</span> <span>min</span><span>(</span>block_start <span>+</span> self<span>.</span>block_size<span>,</span> seq_len<span>)</span>
            
            <span># Extract key and value blocks</span>
            k_block <span>=</span> k<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> block_start<span>:</span>block_end<span>]</span>
            v_block <span>=</span> v<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> block_start<span>:</span>block_end<span>]</span>
            
            <span># Compute attention scores for this block</span>
            attn_scores <span>=</span> torch<span>.</span>matmul<span>(</span>q<span>,</span> k_block<span>.</span>transpose<span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>)</span> <span>*</span> scale
            
            <span># Apply mask if provided</span>
            <span>if</span> mask <span>is</span> <span>not</span> <span>None</span><span>:</span>
                block_mask <span>=</span> mask<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>:</span><span>,</span> block_start<span>:</span>block_end<span>]</span>
                attn_scores <span>=</span> attn_scores<span>.</span>masked_fill<span>(</span>block_mask <span>==</span> <span>0</span><span>,</span> <span>-</span><span>1e9</span><span>)</span>
            
            <span># Apply softmax and dropout</span>
            attn_probs <span>=</span> torch<span>.</span>softmax<span>(</span>attn_scores<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
            attn_probs <span>=</span> self<span>.</span>dropout<span>(</span>attn_probs<span>)</span>
            
            <span># Update output with the attention results for this block</span>
            output <span>+=</span> torch<span>.</span>matmul<span>(</span>attn_probs<span>,</span> v_block<span>)</span>
            normalizer <span>+=</span> attn_probs<span>.</span><span>sum</span><span>(</span>dim<span>=</span><span>-</span><span>1</span><span>,</span> keepdim<span>=</span><span>True</span><span>)</span>
        
        <span># Normalize the output</span>
        output <span>=</span> output <span>/</span> <span>(</span>normalizer <span>+</span> <span>1e-6</span><span>)</span>
        
        <span>return</span> output
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> x<span>.</span>shape
        
        <span># Project queries, keys, values</span>
        q <span>=</span> self<span>.</span>q_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        k <span>=</span> self<span>.</span>k_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        v <span>=</span> self<span>.</span>v_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Transpose for attention computation</span>
        q <span>=</span> q<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        k <span>=</span> k<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        v <span>=</span> v<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        
        <span># Compute flash attention</span>
        output <span>=</span> self<span>.</span>_flash_attention_forward<span>(</span>q<span>,</span> k<span>,</span> v<span>,</span> mask<span>)</span>
        
        <span># Reshape and apply output projection</span>
        output <span>=</span> output<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads <span>*</span> self<span>.</span>head_dim<span>)</span>
        output <span>=</span> self<span>.</span>o_proj<span>(</span>output<span>)</span>
        
        <span>return</span> output
</code></pre>
<p>In reality the repository <a href="https://github.com/Dao-AILab/flash-attention">Dao-AILab/flash-attention</a> has a number of different implementations for different hardware configurations. The <code>flash_attn_qkvpacked_func</code> function is a minimal example of how to use the FlashAttention-3 implementation. It takes a packed QKV tensor as input and returns the attention output.</p>
<pre><code><span>import</span> torch
<span>from</span> flash_attn <span>import</span> flash_attn_qkvpacked_func

<span># Minimal configuration</span>
BATCH_SIZE<span>,</span> SEQ_LEN<span>,</span> NUM_HEADS<span>,</span> HEAD_DIM <span>=</span> <span>2</span><span>,</span> <span>64</span><span>,</span> <span>4</span><span>,</span> <span>32</span>
CAUSAL <span>=</span> <span>False</span>
DTYPE <span>=</span> torch<span>.</span>float16
DEVICE <span>=</span> <span>"cuda"</span>

<span># Create dummy packed QKV tensor</span>
<span># Shape: (batch_size, seq_len, 3, num_heads, head_dim)</span>
qkv <span>=</span> torch<span>.</span>randn<span>(</span>
    BATCH_SIZE<span>,</span>
    SEQ_LEN<span>,</span>
    <span>3</span><span>,</span>
    NUM_HEADS<span>,</span>
    HEAD_DIM<span>,</span>
    dtype<span>=</span>DTYPE<span>,</span>
    device<span>=</span>DEVICE<span>,</span>
<span>)</span>

<span>print</span><span>(</span><span><span>f"Input qkv shape: </span><span><span>{</span>qkv<span>.</span>shape<span>}</span></span><span>"</span></span><span>)</span>

<span># Call FlashAttention packed QKV function</span>
output <span>=</span> flash_attn_qkvpacked_func<span>(</span>
    qkv<span>,</span>
    dropout_p<span>=</span><span>0.0</span><span>,</span> <span># Set dropout probability (0.0 for no dropout)</span>
    causal<span>=</span>CAUSAL<span>,</span>
    softmax_scale<span>=</span><span>None</span> <span># Use default scaling (1 / sqrt(head_dim))</span>
<span>)</span>

<span># Output shape: (batch_size, seq_len, num_heads, head_dim)</span>
<span>print</span><span>(</span><span><span>f"Output shape: </span><span><span>{</span>output<span>.</span>shape<span>}</span></span><span>"</span></span><span>)</span>
<span>print</span><span>(</span><span>"FlashAttention call successful."</span><span>)</span>
</code></pre>
<h2 id="ring-attention" tabindex="-1">Ring Attention</h2>
<p><a href="https://arxiv.org/abs/2310.01889"><strong>Ring Attention</strong></a> uses blockwise computation of self-attention on multiple GPUs and enables training and inference of sequences that would be too long for a single devices. It addresses the significant memory bottleneck inherent in standard self-attention mechanisms, particularly when processing very long sequences where the quadratic memory complexity \(O(N^2)\) of the full attention score matrix becomes prohibitive.</p>
<p>The core idea is to distribute the computation across multiple processing units, like GPUs, arranged conceptually in a ring topology. This approach avoids the need for any single device to hold the entire <code>K</code> and <code>V</code> tensors. Instead, these tensors are sharded or chunked along the sequence length dimension, drastically reducing the peak memory requirement per device and enabling attention calculations over sequences that would otherwise exceed the memory capacity of individual accelerators.</p>
<p>In a practical distributed implementation, each device initially holds the <code>Q</code>, <code>K</code>, and <code>V</code> shards corresponding to its segment of the input sequence. The attention calculation unfolds in synchronized steps across this ring. During each step, a device calculates partial attention scores using its local <code>Q</code> shard and the <code>K</code> shard it currently possesses. The crucial element is the subsequent communication: the <code>K</code> and <code>V</code> shards are passed to the next device in the ring. This rotation repeats until every <code>Q</code> shard has interacted with every <code>K/V</code> shard. Throughout this process, each device accumulates partial outputs (weighted <code>V</code> vectors) and normalization factors (softmax denominators). Finalizing the attention output typically involves a collective operation across all devices to combine these partial results correctly for each segment of the sequence.</p>
<p>The Python example below offers a simulated Ring Attention logic on a single device, illustrating the underlying principles without necessitating actual multi-GPU hardware. The <code>_simulate_ring_attention</code> function mimics the distributed process by iterating through hypothetical shards. In each iteration, it selects slices of the <code>K</code> and <code>V</code> tensors (<code>k_shard</code>, <code>v_shard</code>) to represent the data one device would handle at a given step. It then computes attention scores between the full <code>Q</code> tensor (a simplification from a truly distributed setup) and the current <code>k_shard</code>. The simulation effectively captures the essence of the ring approach by accumulating the weighted values and the softmax normalizers across these iterations, mirroring how partial results would be combined in a distributed setting before a final normalization step yields the output. While demonstrating the computational flow, this simulation naturally doesn't provide the parallelism or memory savings of a true multi-device Ring Attention implementation.</p>
<pre><code><span>class</span> <span>RingAttention</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> num_heads<span>,</span> head_dim<span>=</span><span>64</span><span>,</span> dropout<span>=</span><span>0.0</span><span>,</span> num_shards<span>=</span><span>4</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>dim <span>=</span> dim
        self<span>.</span>num_heads <span>=</span> num_heads
        self<span>.</span>head_dim <span>=</span> head_dim
        self<span>.</span>num_shards <span>=</span> num_shards
        
        <span># Projections</span>
        self<span>.</span>q_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>k_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>v_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>o_proj <span>=</span> nn<span>.</span>Linear<span>(</span>num_heads <span>*</span> head_dim<span>,</span> dim<span>)</span>
        
        self<span>.</span>dropout <span>=</span> nn<span>.</span>Dropout<span>(</span>dropout<span>)</span>
        
    <span>def</span> <span>_simulate_ring_attention</span><span>(</span>self<span>,</span> q<span>,</span> k<span>,</span> v<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        <span># This simulates ring attention without actual multi-GPU support</span>
        batch_size<span>,</span> num_heads<span>,</span> seq_len<span>,</span> head_dim <span>=</span> q<span>.</span>shape
        scale <span>=</span> <span>1.0</span> <span>/</span> math<span>.</span>sqrt<span>(</span>head_dim<span>)</span>
        
        <span># Compute shard sizes</span>
        shard_size <span>=</span> <span>(</span>seq_len <span>+</span> self<span>.</span>num_shards <span>-</span> <span>1</span><span>)</span> <span>//</span> self<span>.</span>num_shards
        
        <span># Initialize outputs</span>
        output <span>=</span> torch<span>.</span>zeros_like<span>(</span>q<span>)</span>
        normalizer <span>=</span> torch<span>.</span>zeros<span>(</span><span>(</span>batch_size<span>,</span> num_heads<span>,</span> seq_len<span>,</span> <span>1</span><span>)</span><span>,</span> device<span>=</span>q<span>.</span>device<span>)</span>
        
        <span># Simulate sharded processing</span>
        <span>for</span> shard_idx <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_shards<span>)</span><span>:</span>
            start_idx <span>=</span> shard_idx <span>*</span> shard_size
            end_idx <span>=</span> <span>min</span><span>(</span>start_idx <span>+</span> shard_size<span>,</span> seq_len<span>)</span>
            
            <span># Process this shard's keys and values</span>
            <span>if</span> start_idx <span>&lt;</span> seq_len<span>:</span>
                k_shard <span>=</span> k<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> start_idx<span>:</span>end_idx<span>]</span>
                v_shard <span>=</span> v<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> start_idx<span>:</span>end_idx<span>]</span>
                
                <span># Compute attention scores</span>
                attn_scores <span>=</span> torch<span>.</span>matmul<span>(</span>q<span>,</span> k_shard<span>.</span>transpose<span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>)</span> <span>*</span> scale
                
                <span># Apply mask if provided</span>
                <span>if</span> mask <span>is</span> <span>not</span> <span>None</span><span>:</span>
                    shard_mask <span>=</span> mask<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>:</span><span>,</span> start_idx<span>:</span>end_idx<span>]</span>
                    attn_scores <span>=</span> attn_scores<span>.</span>masked_fill<span>(</span>shard_mask <span>==</span> <span>0</span><span>,</span> <span>-</span><span>1e9</span><span>)</span>
                
                <span># Apply softmax and dropout (accumulated over shards)</span>
                attn_probs <span>=</span> torch<span>.</span>softmax<span>(</span>attn_scores<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
                attn_probs <span>=</span> self<span>.</span>dropout<span>(</span>attn_probs<span>)</span>
                
                <span># Update output and normalizer</span>
                output <span>+=</span> torch<span>.</span>matmul<span>(</span>attn_probs<span>,</span> v_shard<span>)</span>
                normalizer <span>+=</span> attn_probs<span>.</span><span>sum</span><span>(</span>dim<span>=</span><span>-</span><span>1</span><span>,</span> keepdim<span>=</span><span>True</span><span>)</span>
        
        <span># Normalize the output</span>
        output <span>=</span> output <span>/</span> <span>(</span>normalizer <span>+</span> <span>1e-6</span><span>)</span>
        
        <span>return</span> output
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> x<span>.</span>shape
        
        <span># Project queries, keys, values</span>
        q <span>=</span> self<span>.</span>q_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        k <span>=</span> self<span>.</span>k_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        v <span>=</span> self<span>.</span>v_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Transpose for attention computation</span>
        q <span>=</span> q<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        k <span>=</span> k<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        v <span>=</span> v<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        
        <span># Compute ring attention</span>
        output <span>=</span> self<span>.</span>_simulate_ring_attention<span>(</span>q<span>,</span> k<span>,</span> v<span>,</span> mask<span>)</span>
        
        <span># Reshape and apply output projection</span>
        output <span>=</span> output<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads <span>*</span> self<span>.</span>head_dim<span>)</span>
        output <span>=</span> self<span>.</span>o_proj<span>(</span>output<span>)</span>
        
        <span>return</span> output
</code></pre>
<h2 id="pre-normalization" tabindex="-1">Pre-normalization</h2>
<p><strong>Pre-normalization</strong> (often referred to as Pre-LN) was a shift in the architectural design of residual blocks. Instead of applying the normalization layer <em>after</em> the main operation (like self-attention or a feed-forward network) as done in traditional post-normalization schemes, pre-normalization applies it <em>before</em>. This seemingly small change has significant implications for training dynamics. By normalizing the input <em>before</em> it enters the computationally intensive sub-layer, pre-normalization helps to stabilize the activations and gradients flowing through the network. This stabilization effect is particularly pronounced in very deep networks, mitigating issues like vanishing or exploding gradients and often allowing for higher learning rates and faster, more reliable convergence.</p>
<p>The typical implementation within a residual block follows the structure \( x + f(\text{norm}(x)) \), as demonstrated in the <code>PreNorm</code> class. Here, \( x \) is the input to the block, \( \text{norm}(\cdot) \) represents a normalization function like Layer Normalization (LN) or Root Mean Square Normalization (RMSNorm), and \( f(\cdot) \) denotes the main transformation function (e.g., multi-head attention or a position-wise feed-forward network). The input \( x \) first passes through the normalization layer (e.g., <code>self.norm(x)</code>). The normalized output is then processed by the function <code>fn</code>. Crucially, the output of this function is then added back to the <em>original</em>, unnormalized input \( x \) via the residual connection. This structure ensures a clean gradient path through the identity connection (<code>+ x</code>), further enhancing training stability compared to post-normalization where the normalization layer resides on the residual path itself.</p>
<pre><code><span>class</span> <span>PreNorm</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> fn<span>,</span> norm_type<span>=</span><span>'layer'</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>fn <span>=</span> fn
        
        <span>if</span> norm_type <span>==</span> <span>'layer'</span><span>:</span>
            self<span>.</span>norm <span>=</span> nn<span>.</span>LayerNorm<span>(</span>dim<span>)</span>
        <span>elif</span> norm_type <span>==</span> <span>'rms'</span><span>:</span>
            self<span>.</span>norm <span>=</span> RMSNorm<span>(</span>dim<span>)</span>
        <span>else</span><span>:</span>
            <span>raise</span> ValueError<span>(</span><span><span>f"Unknown normalization type: </span><span><span>{</span>norm_type<span>}</span></span><span>"</span></span><span>)</span>
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>,</span> <span>*</span>args<span>,</span> <span>**</span>kwargs<span>)</span><span>:</span>
        <span># Apply normalization first, then the function</span>
        <span>return</span> self<span>.</span>fn<span>(</span>self<span>.</span>norm<span>(</span>x<span>)</span><span>,</span> <span>*</span>args<span>,</span> <span>**</span>kwargs<span>)</span> <span>+</span> x
</code></pre>
<h2 id="rmsnorm" tabindex="-1">RMSNorm</h2>
<p><strong>RMSNorm</strong> (or Root Mean Square Normalization) is a simplification of the widely used LayerNorm, designed to reduce computational overhead while retaining comparable performance and often improving training stability. Unlike LayerNorm, which centers the activations by subtracting the mean and then scales by the standard deviation, RMSNorm omits the mean centering step entirely. The motivation behind this simplification stems from the empirical observation that the re-centering operation in LayerNorm accounts for a noticeable portion of its computational cost and that removing it often does not significantly harm, and can sometimes even benefit, model performance. It operates solely on the basis of re-scaling the inputs according to their root mean square magnitude.</p>
<p>The core mechanism of RMSNorm involves normalizing the activations within a layer by dividing them by their root mean square value, computed across the features (or a specified dimension). For an input vector \( x = (x_1, \dots, x_n) \), the RMS value is calculated as \( \text{RMS}(x) = \sqrt{\frac{1}{n} \sum_{i=1}^n x_i^2} \). The normalized output \( \bar{x}_i \) is then \( \bar{x}_i = \frac{x_i}{\text{RMS}(x) + \epsilon} \), where \( \epsilon \) is a small constant for numerical stability. Similar to LayerNorm, RMSNorm typically includes a learnable scaling parameter \( g \) (and sometimes a bias \( b \), although the original formulation often omits it to stick closer to the simplification principle), resulting in the final output \( y_i = g_i \bar{x}_i \). By foregoing the mean calculation, RMSNorm offers a reduction in computation (estimated to be 7-64% faster than LayerNorm on GPUs depending on the setup) and memory usage, making it an attractive alternative, especially for large models where efficiency is paramount.</p>
<pre><code><span>class</span> <span>RMSNorm</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> eps<span>=</span><span>1e-8</span><span>,</span> elementwise_affine<span>=</span><span>True</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>eps <span>=</span> eps
        self<span>.</span>elementwise_affine <span>=</span> elementwise_affine
        
        <span>if</span> elementwise_affine<span>:</span>
            self<span>.</span>weight <span>=</span> nn<span>.</span>Parameter<span>(</span>torch<span>.</span>ones<span>(</span>dim<span>)</span><span>)</span>
        <span>else</span><span>:</span>
            self<span>.</span>register_parameter<span>(</span><span>'weight'</span><span>,</span> <span>None</span><span>)</span>
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>)</span><span>:</span>
        <span># Calculate root mean square along the last dimension</span>
        rms <span>=</span> torch<span>.</span>sqrt<span>(</span>torch<span>.</span>mean<span>(</span>x <span>**</span> <span>2</span><span>,</span> dim<span>=</span><span>-</span><span>1</span><span>,</span> keepdim<span>=</span><span>True</span><span>)</span> <span>+</span> self<span>.</span>eps<span>)</span>
        
        <span># Normalize by RMS</span>
        x_normalized <span>=</span> x <span>/</span> rms
        
        <span># Apply scaling if using learnable parameters</span>
        <span>if</span> self<span>.</span>elementwise_affine<span>:</span>
            x_normalized <span>=</span> x_normalized <span>*</span> self<span>.</span>weight
        
        <span>return</span> x_normalized
</code></pre>
<h2 id="swiglu" tabindex="-1">SwiGLU</h2>
<p>SwiGLU is an activation function derived from the Gated Linear Unit (GLU) family, specifically tailored for enhancing the performance of neural networks. The core concept behind GLU variants is to introduce a gating mechanism that adaptively controls the flow of information through the network. Standard feed-forward layers typically apply a single non-linearity to a linear transformation of the input. In contrast, GLU-based activations split the output of a linear layer into two parts; one part acts as a "gate" after passing through a non-linearity, modulating the other part via element-wise multiplication. SwiGLU distinguishes itself by employing the Sigmoid-weighted Linear Unit (SiLU), also known as Swish (\( \text{SiLU}(x) = x \cdot \sigma(x) \), where \( \sigma \) is the sigmoid function), as the specific non-linearity applied to the gating part. This choice has been empirically shown to yield significant performance improvements in various Transformer-based models compared to other activations like ReLU or standard GLU variants using sigmoid or other functions for the gate.</p>
<p>The operational mechanism of SwiGLU within a feed-forward block typically involves projecting the input \( x \) using two separate linear transformations, yielding \( Wx + b \) and \( Vx + c \). The SwiGLU activation is then computed as \( \text{SwiGLU}(x, W, V, b, c) = \text{SiLU}(Wx + b) \odot (Vx + c) \), where \( \odot \) denotes element-wise multiplication. Effectively, the input is processed through two parallel linear paths. One path undergoes the SiLU activation to form the gate values, which then scale the output of the second linear path. This gating allows the network to dynamically control which features are passed forward based on the input context, leading to increased expressive power and better gradient flow compared to simpler activation functions. Its success in models like PaLM and LLaMA highlights its effectiveness in capturing complex patterns within language data.</p>
<pre><code><span>class</span> <span>SwiGLU</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim_in<span>,</span> dim_hidden<span>=</span><span>None</span><span>,</span> dim_out<span>=</span><span>None</span><span>,</span> bias<span>=</span><span>True</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        dim_hidden <span>=</span> dim_hidden <span>or</span> <span>4</span> <span>*</span> dim_in
        dim_out <span>=</span> dim_out <span>or</span> dim_in
        
        <span># Linear transformations for gating</span>
        self<span>.</span>w1 <span>=</span> nn<span>.</span>Linear<span>(</span>dim_in<span>,</span> dim_hidden<span>,</span> bias<span>=</span>bias<span>)</span>
        self<span>.</span>w2 <span>=</span> nn<span>.</span>Linear<span>(</span>dim_in<span>,</span> dim_hidden<span>,</span> bias<span>=</span>bias<span>)</span>
        
        <span># Output projection</span>
        self<span>.</span>w3 <span>=</span> nn<span>.</span>Linear<span>(</span>dim_hidden<span>,</span> dim_out<span>,</span> bias<span>=</span>bias<span>)</span>
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>)</span><span>:</span>
        <span># SwiGLU applies SiLU activation to one branch and gates it with the other</span>
        hidden1 <span>=</span> self<span>.</span>w1<span>(</span>x<span>)</span>
        hidden2 <span>=</span> self<span>.</span>w2<span>(</span>x<span>)</span>
        
        <span># SiLU (Swish) activation: x * sigmoid(x)</span>
        hidden1_act <span>=</span> hidden1 <span>*</span> torch<span>.</span>sigmoid<span>(</span>hidden1<span>)</span>
        
        <span># Element-wise product for gating</span>
        hidden <span>=</span> hidden1_act <span>*</span> hidden2
        
        <span># Output projection</span>
        <span>return</span> self<span>.</span>w3<span>(</span>hidden<span>)</span>
</code></pre>
<h2 id="rotary-positional-embedding" tabindex="-1">Rotary Positional Embedding</h2>
<p><a href="https://arxiv.org/abs/2104.09864"><strong>Rotary Positional Embedding</strong></a> (RoPE) introduces an elegant method for incorporating positional information directly into the self-attention mechanism of Transformer models, specifically designed to capture relative positional dependencies effectively. Traditional approaches often rely on adding absolute positional encodings to the input embeddings or using complex relative positional bias terms within the attention score calculation. RoPE takes a different route by viewing positional encoding as a rotation operation applied to the query and key vectors <em>before</em> their dot product is computed. The key insight is that by rotating the Q vector corresponding to position \( m \) and the K vector corresponding to position \( n \) by angles proportional to \( m \) and \( n \) respectively, the resulting dot product inherently depends only on the <em>relative</em> position \( m - n \) and the content of the vectors themselves, gracefully encoding relative distance without altering the vectors' magnitudes.</p>
<p>The core mathematical idea leverages the properties of complex number multiplication or, equivalently, 2D rotation matrices. Imagine representing pairs of embedding dimensions as complex numbers. RoPE applies a rotation to the query vector \( q_m \) at position \( m \) and the key vector \( k_n \) at position \( n \) using position-dependent rotation matrices \( R_m \) and \( R_n \), respectively. These matrices effectively rotate the vectors in 2D subspaces spanned by pairs of dimensions. The angle of rotation for each 2D subspace is determined by the absolute position (\( m \) or \( n \)) multiplied by a frequency term \( \theta_i \), which decreases for higher dimensions, analogous to sinusoidal embeddings. The crucial property is that the dot product between the rotated vectors, \( (R_m q_m)^T (R_n k_n) \), simplifies such that it only depends on the original vectors \( q_m, k_n \) and a rotation matrix \( R_{m-n} \) corresponding to the relative distance, effectively embedding relative positional information directly into the attention score calculation.</p>
<p>In practice, this rotation is implemented efficiently without explicit matrix multiplication. As shown in the <code>apply_rotary_pos_emb</code> function, the embedding dimensions are typically split into pairs. For each pair \( (x_i, x_{i+1}) \), the rotation corresponding to position \( m \) and frequency \( \theta_j \) (derived from <code>inv_freq</code> in the <code>RotaryEmbedding</code> class) is applied using trigonometric functions: the new pair \( (x'_i, x'_{i+1}) \) becomes \( (x_i \cos(m\theta_j) - x_{i+1} \sin(m\theta_j), x_{i+1} \cos(m\theta_j) + x_i \sin(m\theta_j)) \). The <code>RotaryEmbedding</code> class pre-computes the necessary cosine and sine values (<code>cos</code>, <code>sin</code>) based on the sequence length and the inverse frequency bands (<code>inv_freq</code>), which are derived from a base value (<code>base</code>) and the embedding dimension (<code>dim</code>). These pre-computed values represent \( \cos(m\theta_j) \) and \( \sin(m\theta_j) \) for all positions \( m \) and relevant frequencies \( \theta_j \).</p>
<p>Applying RoPE involves generating these <code>cos</code> and <code>sin</code> embeddings for the given sequence length and then using them to transform the Q and K vectors pair-wise across their head dimension <em>after</em> the initial linear projections but <em>before</em> the attention score calculation. This method offers several advantages: it naturally encodes relative positions, avoids adding positional information directly to word embeddings (potentially preserving more semantic information), and has shown strong empirical performance, including good generalization to sequence lengths longer than those seen during training. By integrating position information via rotation, RoPE provides a computationally efficient and effective mechanism for context-aware sequence modeling.</p>
<pre><code><span>class</span> <span>RotaryEmbedding</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> base<span>=</span><span>10000</span><span>,</span> interleaved<span>=</span><span>False</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>dim <span>=</span> dim
        self<span>.</span>base <span>=</span> base
        self<span>.</span>interleaved <span>=</span> interleaved
        
        <span># Generate inverse frequency bands</span>
        inv_freq <span>=</span> <span>1.0</span> <span>/</span> <span>(</span>base <span>**</span> <span>(</span>torch<span>.</span>arange<span>(</span><span>0</span><span>,</span> dim<span>,</span> <span>2</span><span>)</span><span>.</span><span>float</span><span>(</span><span>)</span> <span>/</span> dim<span>)</span><span>)</span>
        self<span>.</span>register_buffer<span>(</span><span>'inv_freq'</span><span>,</span> inv_freq<span>)</span>
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> seq_len<span>,</span> device<span>=</span><span>None</span><span>)</span><span>:</span>
        <span># Get device from buffer if not specified</span>
        <span>if</span> device <span>is</span> <span>None</span><span>:</span>
            device <span>=</span> self<span>.</span>inv_freq<span>.</span>device
            
        <span># Generate position indices</span>
        positions <span>=</span> torch<span>.</span>arange<span>(</span>seq_len<span>,</span> device<span>=</span>device<span>)</span><span>.</span><span>float</span><span>(</span><span>)</span>
        
        <span># Compute sinusoidal patterns</span>
        freqs <span>=</span> torch<span>.</span>outer<span>(</span>positions<span>,</span> self<span>.</span>inv_freq<span>)</span>
        
        <span># Get sine and cosine embeddings</span>
        emb <span>=</span> torch<span>.</span>cat<span>(</span><span>(</span>freqs<span>,</span> freqs<span>)</span><span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        cos <span>=</span> torch<span>.</span>cos<span>(</span>emb<span>)</span><span>[</span><span>:</span><span>,</span> <span>:</span>self<span>.</span>dim<span>]</span>
        sin <span>=</span> torch<span>.</span>sin<span>(</span>emb<span>)</span><span>[</span><span>:</span><span>,</span> <span>:</span>self<span>.</span>dim<span>]</span>
        
        <span>return</span> cos<span>,</span> sin

<span>def</span> <span>apply_rotary_pos_emb</span><span>(</span>q<span>,</span> k<span>,</span> cos<span>,</span> sin<span>,</span> interleaved<span>=</span><span>False</span><span>)</span><span>:</span>
    <span># Apply rotary embeddings to queries and keys</span>
    batch_size<span>,</span> num_heads<span>,</span> seq_len<span>,</span> head_dim <span>=</span> q<span>.</span>shape
    cos <span>=</span> cos<span>.</span>reshape<span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> seq_len<span>,</span> cos<span>.</span>shape<span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>  <span># [1, 1, seq_len, dim/2]</span>
    sin <span>=</span> sin<span>.</span>reshape<span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> seq_len<span>,</span> sin<span>.</span>shape<span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>  <span># [1, 1, seq_len, dim/2]</span>
    
    <span># Split queries and keys for rotation</span>
    half_dim <span>=</span> head_dim <span>//</span> <span>2</span>
    q1<span>,</span> q2 <span>=</span> q<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> <span>:</span>half_dim<span>]</span><span>,</span> q<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> half_dim<span>:</span><span>]</span>
    k1<span>,</span> k2 <span>=</span> k<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> <span>:</span>half_dim<span>]</span><span>,</span> k<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> half_dim<span>:</span><span>]</span>
    
    <span># Apply rotation using half-dim rotary embeddings</span>
    q_rotated <span>=</span> torch<span>.</span>cat<span>(</span><span>[</span>
        q1 <span>*</span> cos <span>-</span> q2 <span>*</span> sin<span>,</span>
        q2 <span>*</span> cos <span>+</span> q1 <span>*</span> sin
    <span>]</span><span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
    
    k_rotated <span>=</span> torch<span>.</span>cat<span>(</span><span>[</span>
        k1 <span>*</span> cos <span>-</span> k2 <span>*</span> sin<span>,</span>
        k2 <span>*</span> cos <span>+</span> k1 <span>*</span> sin
    <span>]</span><span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
    
    <span>return</span> q_rotated<span>,</span> k_rotated
</code></pre>
<h2 id="mixture-of-experts" tabindex="-1">Mixture of Experts</h2>
<p>Mixture of Experts (shortened as MoE) is a model architecture designed to significantly increase the parameter count, and thus the potential capacity, of a neural network without incurring a proportionally massive increase in computational cost during inference or training. The core idea is to replace computationally intensive components, like the feed-forward network block in a Transformer, with multiple, smaller "expert" networks. Crucially, not all experts process every input token. Instead, a lightweight "router" or "gating" network dynamically selects a small subset of experts (typically just one or two, known as top-k routing) deemed most suitable for processing each specific input token based on its features. This conditional computation allows MoE models to possess potentially trillions of parameters while only activating a small fraction of them for any given input, maintaining manageable FLOPs compared to a similarly sized dense model.</p>
<p>The router network is the core idea, it acts as a learned decision-maker. It takes the representation of an input token and produces scores or logits indicating the suitability of each available expert for that token. In the example code's <code>_compute_routing_weights</code> function, these logits are often processed using a top-k function to identify the <code>k</code> experts with the highest scores. The scores for these selected experts are then typically normalized, often using a softmax function, to produce routing weights. These weights determine the contribution of each selected expert to the final output for that token. During training, noise can be added to the router logits (as seen with <code>noise_std</code>) to encourage exploration and prevent the router from collapsing to always favor only a few experts early on.</p>
<p>Once the router selects the top-k experts and calculates their respective weights for a given input token, that token is dispatched only to those chosen experts. Each selected expert network (often a standard FFN, as shown in the <code>experts</code> ModuleList) processes the token independently. The outputs produced by these active experts are then combined to form the final output for that token. This combination is typically a weighted sum, where the output of each selected expert is multiplied by its corresponding routing weight calculated by the router. For instance, if experts <code>i</code> and <code>j</code> are selected with weights <code>w_i</code> and <code>w_j</code>, the final output for token <code>x</code> would be <code>w_i * expert_i(x) + w_j * expert_j(x)</code>. This ensures that the final representation incorporates specialized knowledge from the most relevant experts.</p>
<p>A challenge in training MoE models is ensuring that all experts are utilized effectively; otherwise, the router might learn to consistently overload a few "popular" experts while others remain underdeveloped. To counteract this, an auxiliary load balancing loss is typically incorporated into the training objective, as demonstrated by the <code>_compute_balance_loss</code> method. This loss encourages the router to distribute the computational load (i.e., the input tokens) more evenly across all available experts, often by penalizing imbalances in either the number of tokens assigned to each expert or the sum of routing weights directed towards each expert. By successfully implementing sparse activation via routing and incorporating load balancing, MoE enables the construction of extremely large yet computationally efficient models.</p>
<pre><code><span>class</span> <span>MixtureOfExperts</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> input_dim<span>,</span> hidden_dim<span>,</span> output_dim<span>,</span> num_experts<span>=</span><span>4</span><span>,</span> top_k<span>=</span><span>2</span><span>,</span> noise_std<span>=</span><span>1.0</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>input_dim <span>=</span> input_dim
        self<span>.</span>hidden_dim <span>=</span> hidden_dim
        self<span>.</span>output_dim <span>=</span> output_dim
        self<span>.</span>num_experts <span>=</span> num_experts
        self<span>.</span>top_k <span>=</span> <span>min</span><span>(</span>top_k<span>,</span> num_experts<span>)</span>
        self<span>.</span>noise_std <span>=</span> noise_std
        
        <span># Create experts</span>
        self<span>.</span>experts <span>=</span> nn<span>.</span>ModuleList<span>(</span><span>[</span>
            nn<span>.</span>Sequential<span>(</span>
                nn<span>.</span>Linear<span>(</span>input_dim<span>,</span> hidden_dim<span>)</span><span>,</span>
                nn<span>.</span>ReLU<span>(</span><span>)</span><span>,</span>
                nn<span>.</span>Linear<span>(</span>hidden_dim<span>,</span> output_dim<span>)</span>
            <span>)</span> <span>for</span> _ <span>in</span> <span>range</span><span>(</span>num_experts<span>)</span>
        <span>]</span><span>)</span>
        
        <span># Router network</span>
        self<span>.</span>router <span>=</span> nn<span>.</span>Linear<span>(</span>input_dim<span>,</span> num_experts<span>)</span>
        
        <span># Save expert counts and loads for balancing loss</span>
        self<span>.</span>register_buffer<span>(</span><span>'expert_counts'</span><span>,</span> torch<span>.</span>zeros<span>(</span>num_experts<span>)</span><span>)</span>
        
    <span>def</span> <span>_compute_routing_weights</span><span>(</span>self<span>,</span> x<span>)</span><span>:</span>
        <span># Calculate routing weights</span>
        routing_logits <span>=</span> self<span>.</span>router<span>(</span>x<span>)</span>  <span># [batch_size, seq_len, num_experts]</span>
        
        <span># Add noise during training to encourage exploration</span>
        <span>if</span> self<span>.</span>training <span>and</span> self<span>.</span>noise_std <span>&gt;</span> <span>0</span><span>:</span>
            noise <span>=</span> torch<span>.</span>randn_like<span>(</span>routing_logits<span>)</span> <span>*</span> self<span>.</span>noise_std
            routing_logits <span>=</span> routing_logits <span>+</span> noise
        
        <span># Get top-k experts for each token</span>
        routing_weights<span>,</span> selected_experts <span>=</span> torch<span>.</span>topk<span>(</span>routing_logits<span>,</span> self<span>.</span>top_k<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        
        <span># Normalize the routing weights with softmax</span>
        routing_weights <span>=</span> F<span>.</span>softmax<span>(</span>routing_weights<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        
        <span>return</span> routing_weights<span>,</span> selected_experts
    
    <span>def</span> <span>_compute_balance_loss</span><span>(</span>self<span>,</span> selected_experts<span>,</span> routing_weights<span>)</span><span>:</span>
        <span># Compute auxiliary load balancing loss</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> selected_experts<span>.</span>shape
        
        <span># Compute probability of each expert being selected across batch</span>
        expert_mask <span>=</span> torch<span>.</span>zeros<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_experts<span>,</span> device<span>=</span>selected_experts<span>.</span>device<span>)</span>
        
        <span># For each position in selected_experts, increment the corresponding expert index</span>
        <span>for</span> k <span>in</span> <span>range</span><span>(</span>self<span>.</span>top_k<span>)</span><span>:</span>
            expert_mask<span>.</span>scatter_<span>(</span><span>-</span><span>1</span><span>,</span> selected_experts<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> k<span>:</span>k<span>+</span><span>1</span><span>]</span><span>,</span> routing_weights<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> k<span>:</span>k<span>+</span><span>1</span><span>]</span><span>)</span>
        
        <span># Compute mean routing probability per expert</span>
        expert_routing_probs <span>=</span> expert_mask<span>.</span>mean<span>(</span>dim<span>=</span><span>[</span><span>0</span><span>,</span> <span>1</span><span>]</span><span>)</span>
        
        <span># Compute load balancing loss (all experts should receive equal probability)</span>
        target_probs <span>=</span> torch<span>.</span>ones_like<span>(</span>expert_routing_probs<span>)</span> <span>/</span> self<span>.</span>num_experts
        balance_loss <span>=</span> F<span>.</span>mse_loss<span>(</span>expert_routing_probs<span>,</span> target_probs<span>)</span> <span>*</span> self<span>.</span>num_experts
        
        <span>return</span> balance_loss
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> x<span>.</span>shape
        
        <span># Compute routing weights and selected experts</span>
        routing_weights<span>,</span> selected_experts <span>=</span> self<span>.</span>_compute_routing_weights<span>(</span>x<span>)</span>
        
        <span># Prepare output</span>
        output <span>=</span> torch<span>.</span>zeros<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>output_dim<span>,</span> device<span>=</span>x<span>.</span>device<span>)</span>
        
        <span># Dispatch to selected experts</span>
        <span>for</span> k <span>in</span> <span>range</span><span>(</span>self<span>.</span>top_k<span>)</span><span>:</span>
            <span># Extract the current expert indices and weights</span>
            expert_indices <span>=</span> selected_experts<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> k<span>]</span>  <span># [batch_size, seq_len]</span>
            expert_weights <span>=</span> routing_weights<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> k<span>]</span><span>.</span>unsqueeze<span>(</span><span>-</span><span>1</span><span>)</span>  <span># [batch_size, seq_len, 1]</span>
            
            <span># Dispatch to each expert</span>
            <span>for</span> expert_idx <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_experts<span>)</span><span>:</span>
                <span># Find tokens assigned to this expert</span>
                mask <span>=</span> <span>(</span>expert_indices <span>==</span> expert_idx<span>)</span>
                
                <span>if</span> mask<span>.</span><span>any</span><span>(</span><span>)</span><span>:</span>
                    <span># Gather input tokens assigned to this expert</span>
                    expert_inputs <span>=</span> x<span>[</span>mask<span>]</span>
                    
                    <span># Process with the expert</span>
                    expert_outputs <span>=</span> self<span>.</span>experts<span>[</span>expert_idx<span>]</span><span>(</span>expert_inputs<span>)</span>
                    
                    <span># Scatter outputs back with appropriate weights</span>
                    output<span>[</span>mask<span>]</span> <span>+=</span> expert_outputs <span>*</span> expert_weights<span>[</span>mask<span>]</span>
                    
                    <span># Update expert counts (for monitoring)</span>
                    self<span>.</span>expert_counts<span>[</span>expert_idx<span>]</span> <span>+=</span> mask<span>.</span><span>sum</span><span>(</span><span>)</span><span>.</span>item<span>(</span><span>)</span>
        
        <span># Compute auxiliary load balancing loss</span>
        balance_loss <span>=</span> self<span>.</span>_compute_balance_loss<span>(</span>selected_experts<span>,</span> routing_weights<span>)</span>
        
        <span># Return output and auxiliary loss</span>
        <span>return</span> output<span>,</span> balance_loss
</code></pre>
<h2 id="learning-rate-warmup" tabindex="-1">Learning Rate Warmup</h2>
<p>Learning rate warmup is a widely adopted heuristic employed during the initial phase of training neural networks to enhance stability and prevent divergence. At the beginning of training, model parameters are typically initialized randomly, often far from an optimal configuration. If a relatively large learning rate is used immediately, the initial gradients, which can also be large and erratic, may cause drastic parameter updates, potentially pushing the model into a poor region of the loss landscape or even causing numerical instability (e.g., loss explosion). Learning rate warmup mitigates this risk by starting the training process with a very small learning rate, which is then gradually increased over a predefined number of initial training steps (the "warmup steps") until it reaches its target base value, from which point a standard learning rate schedule (like decay) might commence.</p>
<p>The mechanism involves progressively scaling the base learning rate during the warmup phase. A common strategy, illustrated by the <code>LinearWarmupScheduler</code> class, is linear warmup. In this approach, the learning rate \( \eta_t \) at step \( t \) is calculated as \( \eta_t = \eta_{\text{base}} \times \frac{t}{T_{\text{warmup}}} \) for \( t &lt; T_{\text{warmup}} \), where \( \eta_{\text{base}} \) is the target base learning rate and \( T_{\text{warmup}} \) is the total number of warmup steps. As seen in the <code>get_lr</code> method, the scaling factor <code>scale</code> increases linearly from near zero to 1 over the <code>warmup_steps</code>. Once the step count <code>last_epoch</code> reaches <code>warmup_steps</code>, the learning rate becomes equal to the <code>base_lrs</code>, and the warmup phase concludes. This gentle ramp-up allows the model to adapt gradually during the critical early stages when activations and gradients might otherwise be volatile, leading to smoother convergence and often enabling the use of higher base learning rates later in training.</p>
<pre><code><span>class</span> <span>LinearWarmupScheduler</span><span>(</span>_LRScheduler<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> optimizer<span>,</span> warmup_steps<span>,</span> last_epoch<span>=</span><span>-</span><span>1</span><span>)</span><span>:</span>
        self<span>.</span>warmup_steps <span>=</span> warmup_steps
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span>optimizer<span>,</span> last_epoch<span>)</span>
    
    <span>def</span> <span>get_lr</span><span>(</span>self<span>)</span><span>:</span>
        <span>if</span> self<span>.</span>last_epoch <span>&lt;</span> self<span>.</span>warmup_steps<span>:</span>
            <span># During warmup: linearly increase from 0 to base LR</span>
            scale <span>=</span> <span>float</span><span>(</span>self<span>.</span>last_epoch <span>+</span> <span>1</span><span>)</span> <span>/</span> <span>float</span><span>(</span><span>max</span><span>(</span><span>1</span><span>,</span> self<span>.</span>warmup_steps<span>)</span><span>)</span>
            <span>return</span> <span>[</span>base_lr <span>*</span> scale <span>for</span> base_lr <span>in</span> self<span>.</span>base_lrs<span>]</span>
        <span>else</span><span>:</span>
            <span># After warmup: use base learning rate</span>
            <span>return</span> self<span>.</span>base_lrs
</code></pre>
<h2 id="cosine-schedule" tabindex="-1">Cosine Schedule</h2>
<p><strong>Cosine scheduling</strong> (sometimes called <strong>cosine annealing</strong>) is a learning rate schedule technique. Its core principle is to gradually decrease the learning rate over the course of training, following the shape of a cosine curve. Unlike step decay schedules, which reduce the learning rate abruptly at specific epochs, cosine annealing provides a smooth, continuous reduction. Typically, the learning rate starts at an initial high value and decreases following the first half-cycle of a cosine function, reaching a predefined minimum value (often close to zero) by the final training step. This smooth decay has been shown empirically to help the optimization process by allowing larger steps early in training for broad exploration of the loss landscape, and progressively smaller steps later on for fine-tuning and convergence towards a good minimum.</p>
<p>Early in training, a higher learning rate encourages faster exploration and helps escape poor local minima. As training progresses and the model parameters approach a more optimal region, reducing the learning rate becomes crucial to avoid overshooting the minimum and to allow for more precise convergence. The cosine function provides a schedule that starts with a relatively slow decay rate, allowing the optimizer to maintain momentum initially. The decay rate then accelerates towards the middle of the schedule before slowing down again as it approaches the minimum learning rate. This final slow-down phase near the end of training is particularly important, as it allows the optimizer to carefully settle into a potentially flat minimum, which empirical evidence suggests often correlates with better generalization performance compared to sharper minima.</p>
<p>Furthermore, cosine scheduling is often combined with a "warmup" phase, as seen in the code example. During this initial phase (e.g., <code>warmup_steps</code>), the learning rate is typically increased linearly from a very small value (or zero) up to the main initial learning rate. This warmup period helps stabilize training in the very beginning, especially for large models or datasets where large initial learning rates applied to randomly initialized weights could lead to instability or divergence. After the warmup, the cosine decay phase begins, smoothly decreasing the learning rate from its peak value down to the target minimum (<code>base_lr * min_lr_ratio</code>) over the remaining <code>total_steps - warmup_steps</code>. This combination of a gentle start (warmup) followed by a smooth, theoretically motivated decay (cosine annealing) provides a robust and effective learning rate strategy that often requires less hyperparameter tuning than step-based schedules and frequently leads to improved model accuracy.</p>
<p>There is a <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html">CosineScheduler</a> in PyTorch that implements this technique. Let's look at a simplified version of it:</p>
<pre><code><span>class</span> <span>CosineAnnealingWarmupScheduler</span><span>(</span>_LRScheduler<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> optimizer<span>,</span> warmup_steps<span>,</span> total_steps<span>,</span> min_lr_ratio<span>=</span><span>1e-4</span><span>,</span> last_epoch<span>=</span><span>-</span><span>1</span><span>)</span><span>:</span>
        self<span>.</span>warmup_steps <span>=</span> warmup_steps
        self<span>.</span>total_steps <span>=</span> total_steps
        self<span>.</span>min_lr_ratio <span>=</span> min_lr_ratio
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span>optimizer<span>,</span> last_epoch<span>)</span>
    
    <span>def</span> <span>get_lr</span><span>(</span>self<span>)</span><span>:</span>
        <span>if</span> self<span>.</span>last_epoch <span>&lt;</span> self<span>.</span>warmup_steps<span>:</span>
            <span># During warmup: linearly increase from 0 to base LR</span>
            scale <span>=</span> <span>float</span><span>(</span>self<span>.</span>last_epoch <span>+</span> <span>1</span><span>)</span> <span>/</span> <span>float</span><span>(</span><span>max</span><span>(</span><span>1</span><span>,</span> self<span>.</span>warmup_steps<span>)</span><span>)</span>
            <span>return</span> <span>[</span>base_lr <span>*</span> scale <span>for</span> base_lr <span>in</span> self<span>.</span>base_lrs<span>]</span>
        <span>else</span><span>:</span>
            <span># After warmup: cosine decay from base LR to min_lr</span>
            progress <span>=</span> <span>float</span><span>(</span>self<span>.</span>last_epoch <span>-</span> self<span>.</span>warmup_steps<span>)</span> <span>/</span> <span>float</span><span>(</span>
                <span>max</span><span>(</span><span>1</span><span>,</span> self<span>.</span>total_steps <span>-</span> self<span>.</span>warmup_steps<span>)</span>
            <span>)</span>
            <span># Cosine decay formula: min_lr + 0.5 * (base_lr - min_lr) * (1 + cos(pi * progress))</span>
            scale <span>=</span> self<span>.</span>min_lr_ratio <span>+</span> <span>0.5</span> <span>*</span> <span>(</span><span>1.0</span> <span>-</span> self<span>.</span>min_lr_ratio<span>)</span> <span>*</span> <span>(</span>
                <span>1.0</span> <span>+</span> math<span>.</span>cos<span>(</span>math<span>.</span>pi <span>*</span> progress<span>)</span>
            <span>)</span>
            <span>return</span> <span>[</span>base_lr <span>*</span> scale <span>for</span> base_lr <span>in</span> self<span>.</span>base_lrs<span>]</span>
</code></pre>
<h2 id="adamw-optimizer" tabindex="-1">AdamW Optimizer</h2>
<p><strong>AdamW</strong> (Adam with Decoupled Weight Decay) addresses a subtle issue in the standard implementation of weight decay (L2 regularization) within adaptive optimizers like Adam. This improved optimizer gives us a method where weight decay does not accumulate in the momentum nor variance. In traditional Adam, L2 regularization is often implemented by adding the decay term (\(\lambda \cdot \text{weight}\)) directly to the gradient before computing the moving averages (\(m_t\) and \(v_t\)). However, this couples the weight decay effect with the adaptive learning rate derived from the gradient moments. Consequently, parameters with historically large gradients (and thus larger \(v_t\) values) experience smaller effective weight decay, contrary to the goal of applying uniform regularization pressure. AdamW decouples these processes: it performs the standard Adam updates based purely on the gradients and separately applies the weight decay step directly to the weights, effectively restoring the original behavior of L2 regularization where weights decay proportionally to their magnitude, independent of their gradient history.</p>
<p>The core update mechanism of AdamW largely follows the standard Adam procedure but modifies how weight decay is applied. At each step \(t\) for a parameter \(\theta\), it first calculates the biased first moment estimate \(m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t\) and the biased second raw moment estimate \(v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2\), where \(g_t\) is the gradient at step \(t\), and \(\beta_1\), \(\beta_2\) are exponential decay rates. These are then bias-corrected: \(\hat{m}_t = m_t / (1 - \beta_1^t)\) and \(\hat{v}_t = v_t / (1 - \beta_2^t)\). The crucial difference lies in the update rule. Instead of incorporating weight decay into \(g_t\), AdamW first applies weight decay directly to the parameters: \(\theta_{t-1}' = \theta_{t-1} \cdot (1 - \text{lr} \cdot \lambda)\), where \(\text{lr}\) is the learning rate and \(\lambda\) is the weight_decay factor (as seen in the line <code>p.data.mul_(1 - group['lr'] * group['weight_decay'])</code>). Then, the standard Adam update using the bias-corrected moments is applied to these decayed weights: \(\theta_t = \theta_{t-1}' - \text{lr} \cdot \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)\). This corresponds to the code line <code>p.data.addcdiv_(exp_avg, denom, value=-step_size)</code>, operating on the already decayed <code>p.data</code>.</p>
<p>In deep learning practice, AdamW is employed similarly to Adam. It is instantiated by providing the model's parameters and key hyperparameters like the learning rate (\(\text{lr}\)), beta values (\(\beta_1\), \(\beta_2\)), epsilon (\(\epsilon\)), and the weight decay factor (\(\lambda\)). The <code>step()</code> method, typically called within the training loop after computing gradients (<code>loss.backward()</code>), executes the update logic described above for each parameter. Its primary advantage is improved generalization, particularly observed in training large models like Transformers where regularization is critical. By ensuring that the weight decay strength is independent of the adaptive learning rate scaling, AdamW often allows for better hyperparameter tuning (especially \(\text{lr}\) and \(\lambda\)) and can lead to models that perform better on unseen data compared to standard Adam with L2 regularization.</p>
<p>There is a highly optimized implementation of <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html">AdamW</a> in PyTorch. Let's look at a simplified version of it, the example below demonstrates a minimal PyTorch implementation, initializing state variables (like <code>exp_avg</code>, <code>exp_avg_sq</code>) and performing the decoupled weight decay and moment-based updates within the parameter loop.</p>
<pre><code><span>class</span> <span>AdamW</span><span>(</span>Optimizer<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> params<span>,</span> lr<span>=</span><span>1e-3</span><span>,</span> betas<span>=</span><span>(</span><span>0.9</span><span>,</span> <span>0.999</span><span>)</span><span>,</span> eps<span>=</span><span>1e-8</span><span>,</span>
                 weight_decay<span>=</span><span>1e-2</span><span>,</span> amsgrad<span>=</span><span>False</span><span>)</span><span>:</span>
        defaults <span>=</span> <span>dict</span><span>(</span>lr<span>=</span>lr<span>,</span> betas<span>=</span>betas<span>,</span> eps<span>=</span>eps<span>,</span>
                        weight_decay<span>=</span>weight_decay<span>,</span> amsgrad<span>=</span>amsgrad<span>)</span>
        <span>super</span><span>(</span>AdamW<span>,</span> self<span>)</span><span>.</span>__init__<span>(</span>params<span>,</span> defaults<span>)</span>
    
    <span>def</span> <span>step</span><span>(</span>self<span>,</span> closure<span>=</span><span>None</span><span>)</span><span>:</span>
        loss <span>=</span> <span>None</span>
        <span>if</span> closure <span>is</span> <span>not</span> <span>None</span><span>:</span>
            loss <span>=</span> closure<span>(</span><span>)</span>
        
        <span>for</span> group <span>in</span> self<span>.</span>param_groups<span>:</span>
            <span>for</span> p <span>in</span> group<span>[</span><span>'params'</span><span>]</span><span>:</span>
                <span>if</span> p<span>.</span>grad <span>is</span> <span>None</span><span>:</span>
                    <span>continue</span>
                
                <span># Get gradient</span>
                grad <span>=</span> p<span>.</span>grad<span>.</span>data
                
                <span>if</span> grad<span>.</span>is_sparse<span>:</span>
                    <span>raise</span> RuntimeError<span>(</span><span>'AdamW does not support sparse gradients'</span><span>)</span>
                
                amsgrad <span>=</span> group<span>[</span><span>'amsgrad'</span><span>]</span>
                state <span>=</span> self<span>.</span>state<span>[</span>p<span>]</span>
                
                <span># State initialization</span>
                <span>if</span> <span>len</span><span>(</span>state<span>)</span> <span>==</span> <span>0</span><span>:</span>
                    state<span>[</span><span>'step'</span><span>]</span> <span>=</span> <span>0</span>
                    <span># Exponential moving average of gradient values</span>
                    state<span>[</span><span>'exp_avg'</span><span>]</span> <span>=</span> torch<span>.</span>zeros_like<span>(</span>p<span>.</span>data<span>)</span>
                    <span># Exponential moving average of squared gradient values</span>
                    state<span>[</span><span>'exp_avg_sq'</span><span>]</span> <span>=</span> torch<span>.</span>zeros_like<span>(</span>p<span>.</span>data<span>)</span>
                    <span>if</span> amsgrad<span>:</span>
                        <span># Maintains max of all exp. moving avg. of sq. grad. values</span>
                        state<span>[</span><span>'max_exp_avg_sq'</span><span>]</span> <span>=</span> torch<span>.</span>zeros_like<span>(</span>p<span>.</span>data<span>)</span>
                
                exp_avg<span>,</span> exp_avg_sq <span>=</span> state<span>[</span><span>'exp_avg'</span><span>]</span><span>,</span> state<span>[</span><span>'exp_avg_sq'</span><span>]</span>
                <span>if</span> amsgrad<span>:</span>
                    max_exp_avg_sq <span>=</span> state<span>[</span><span>'max_exp_avg_sq'</span><span>]</span>
                
                beta1<span>,</span> beta2 <span>=</span> group<span>[</span><span>'betas'</span><span>]</span>
                
                state<span>[</span><span>'step'</span><span>]</span> <span>+=</span> <span>1</span>
                
                <span># Decay the first and second moment running average coefficient</span>
                exp_avg<span>.</span>mul_<span>(</span>beta1<span>)</span><span>.</span>add_<span>(</span>grad<span>,</span> alpha<span>=</span><span>1</span> <span>-</span> beta1<span>)</span>
                exp_avg_sq<span>.</span>mul_<span>(</span>beta2<span>)</span><span>.</span>addcmul_<span>(</span>grad<span>,</span> grad<span>,</span> value<span>=</span><span>1</span> <span>-</span> beta2<span>)</span>
                
                <span>if</span> amsgrad<span>:</span>
                    <span># Maintains the maximum of all 2nd moment running avg. till now</span>
                    torch<span>.</span>maximum<span>(</span>max_exp_avg_sq<span>,</span> exp_avg_sq<span>,</span> out<span>=</span>max_exp_avg_sq<span>)</span>
                    <span># Use the max. for normalizing running avg. of gradient</span>
                    denom <span>=</span> max_exp_avg_sq<span>.</span>sqrt<span>(</span><span>)</span><span>.</span>add_<span>(</span>group<span>[</span><span>'eps'</span><span>]</span><span>)</span>
                <span>else</span><span>:</span>
                    denom <span>=</span> exp_avg_sq<span>.</span>sqrt<span>(</span><span>)</span><span>.</span>add_<span>(</span>group<span>[</span><span>'eps'</span><span>]</span><span>)</span>
                
                bias_correction1 <span>=</span> <span>1</span> <span>-</span> beta1 <span>**</span> state<span>[</span><span>'step'</span><span>]</span>
                bias_correction2 <span>=</span> <span>1</span> <span>-</span> beta2 <span>**</span> state<span>[</span><span>'step'</span><span>]</span>
                step_size <span>=</span> group<span>[</span><span>'lr'</span><span>]</span> <span>*</span> math<span>.</span>sqrt<span>(</span>bias_correction2<span>)</span> <span>/</span> bias_correction1
                
                <span># Apply weight decay BEFORE the optimization step</span>
                <span>if</span> group<span>[</span><span>'weight_decay'</span><span>]</span> <span>!=</span> <span>0</span><span>:</span>
                    p<span>.</span>data<span>.</span>mul_<span>(</span><span>1</span> <span>-</span> group<span>[</span><span>'lr'</span><span>]</span> <span>*</span> group<span>[</span><span>'weight_decay'</span><span>]</span><span>)</span>
                
                <span># Update parameters</span>
                p<span>.</span>data<span>.</span>addcdiv_<span>(</span>exp_avg<span>,</span> denom<span>,</span> value<span>=</span><span>-</span>step_size<span>)</span>
        
        <span>return</span> loss
</code></pre>
<h2 id="multi-token-prediction" tabindex="-1">Multi-token Prediction</h2>
<p>Multi-token prediction is a technique developed to accelerate the inference speed of autoregressive language models. Normally, autoregressive generation predicts tokens one by one: the model takes a sequence, predicts the single most likely next token, appends it to the sequence, and repeats the process. This sequential nature, requiring one full forward pass of the model for each generated token, becomes a significant bottleneck for latency-sensitive applications. Multi-token prediction attempts to overcome this by modifying the model's prediction head to output probabilities for multiple future tokens simultaneously based on the current hidden state, thereby reducing the number of forward passes needed to generate a sequence of a given length.</p>
<p>The implementation typically involves adapting the final layer(s) of the language model. Instead of a single output layer (or "language model head") mapping the final hidden state to logits over the vocabulary for the next token, a multi-token predictor might employ several strategies. One approach, as shown in the example class using separate heads, is to have multiple distinct prediction heads, each trained to predict a token at a different future offset (e.g., one head for token \(t+1\), another for \(t+2\), up to \(t+N\)). These heads usually take the same final hidden state (e.g., corresponding to token \(t\)) as input but learn different projections specialized for their respective time steps. Another approach involves using a single shared prediction head, which might require more complex mechanisms, potentially involving learned transformations of the hidden state or incorporating positional information, to generate distinct probability distributions for each of the \(N\) future tokens from essentially the same starting representation.</p>
<p>Training a multi-token predictor involves teaching the model to correctly anticipate the sequence of \(N\) subsequent tokens given the preceding context. During the training phase, as illustrated in the <code>compute_loss</code> method, the model receives input sequences and its predictions for the next \(N\) tokens are compared against the actual \(N\) target tokens in the training data. A loss function, usually cross-entropy, is calculated for each predicted position (\(t+1\) to \(t+N\)) and then aggregated (e.g., averaged) to form the final loss signal used for backpropagation.</p>
<p>While this can show speed improvements, multi-token prediction has several drawbacks: the accuracy of predicting tokens further into the future tends to decrease, and the chosen sequence of \(N\) tokens might diverge from the optimal path that would have been taken by single-token generation. Therefore, it often represents a trade-off between generation speed and potential quality degradation that is very use-case dependent.</p>
<pre><code><span>class</span> <span>MultiTokenPredictor</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> hidden_dim<span>,</span> vocab_size<span>,</span> num_predicted_tokens<span>=</span><span>2</span><span>,</span> shared_prediction_head<span>=</span><span>False</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>hidden_dim <span>=</span> hidden_dim
        self<span>.</span>vocab_size <span>=</span> vocab_size
        self<span>.</span>num_predicted_tokens <span>=</span> num_predicted_tokens
        self<span>.</span>shared_prediction_head <span>=</span> shared_prediction_head
        
        <span>if</span> shared_prediction_head<span>:</span>
            <span># Share the same prediction head for all positions</span>
            self<span>.</span>lm_head <span>=</span> nn<span>.</span>Linear<span>(</span>hidden_dim<span>,</span> vocab_size<span>)</span>
        <span>else</span><span>:</span>
            <span># Use separate prediction heads for each position</span>
            self<span>.</span>lm_heads <span>=</span> nn<span>.</span>ModuleList<span>(</span><span>[</span>
                nn<span>.</span>Linear<span>(</span>hidden_dim<span>,</span> vocab_size<span>)</span> 
                <span>for</span> _ <span>in</span> <span>range</span><span>(</span>num_predicted_tokens<span>)</span>
            <span>]</span><span>)</span>
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> hidden_states<span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> hidden_states<span>.</span>shape
        
        <span># Get the hidden states for the last token</span>
        last_hidden <span>=</span> hidden_states<span>[</span><span>:</span><span>,</span> <span>-</span><span>1</span><span>]</span>
        
        <span>if</span> self<span>.</span>shared_prediction_head<span>:</span>
            <span># Use the shared head for all predictions</span>
            logits <span>=</span> self<span>.</span>lm_head<span>(</span>last_hidden<span>)</span>
            next_token_logits <span>=</span> logits<span>.</span>unsqueeze<span>(</span><span>1</span><span>)</span>
            
            <span># For positions beyond the first, we need to make a projection</span>
            multi_token_logits <span>=</span> <span>[</span><span>]</span>
            <span>for</span> i <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_predicted_tokens<span>)</span><span>:</span>
                <span>if</span> i <span>==</span> <span>0</span><span>:</span>
                    multi_token_logits<span>.</span>append<span>(</span>logits<span>)</span>
                <span>else</span><span>:</span>
                    <span># A simple projection for demonstration; in practice, this would be more complex</span>
                    projected_hidden <span>=</span> last_hidden <span>+</span> i <span>*</span> <span>0.1</span>  <span># Just a dummy transformation</span>
                    multi_token_logits<span>.</span>append<span>(</span>self<span>.</span>lm_head<span>(</span>projected_hidden<span>)</span><span>)</span>
            
            multi_token_logits <span>=</span> torch<span>.</span>stack<span>(</span>multi_token_logits<span>,</span> dim<span>=</span><span>1</span><span>)</span>
        <span>else</span><span>:</span>
            <span># Use separate heads for each position</span>
            multi_token_logits <span>=</span> torch<span>.</span>stack<span>(</span><span>[</span>
                head<span>(</span>last_hidden<span>)</span> <span>for</span> head <span>in</span> self<span>.</span>lm_heads
            <span>]</span><span>,</span> dim<span>=</span><span>1</span><span>)</span>
            
            <span># The first position is used for next-token prediction</span>
            next_token_logits <span>=</span> multi_token_logits<span>[</span><span>:</span><span>,</span> <span>0</span><span>:</span><span>1</span><span>]</span>
        
        <span>return</span> next_token_logits<span>,</span> multi_token_logits
    
    <span>def</span> <span>compute_loss</span><span>(</span>self<span>,</span> hidden_states<span>,</span> labels<span>,</span> ignore_index<span>=</span><span>-</span><span>100</span><span>)</span><span>:</span>
        <span># Get predictions</span>
        _<span>,</span> multi_token_logits <span>=</span> self<span>.</span>forward<span>(</span>hidden_states<span>)</span>
        
        <span># Prepare targets: shift labels to align with predictions</span>
        <span># We need the next num_predicted_tokens tokens after the input</span>
        shifted_labels <span>=</span> labels<span>[</span><span>:</span><span>,</span> <span>:</span><span>-</span>self<span>.</span>num_predicted_tokens<span>]</span>
        targets <span>=</span> <span>[</span><span>]</span>
        
        <span>for</span> i <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_predicted_tokens<span>)</span><span>:</span>
            targets<span>.</span>append<span>(</span>labels<span>[</span><span>:</span><span>,</span> <span>1</span><span>+</span>i<span>:</span><span>1</span><span>+</span>i<span>-</span>self<span>.</span>num_predicted_tokens<span>+</span><span>1</span><span>]</span><span>)</span>
        
        <span># Stack targets</span>
        stacked_targets <span>=</span> torch<span>.</span>stack<span>(</span>targets<span>,</span> dim<span>=</span><span>1</span><span>)</span>
        
        <span># Compute loss across all predicted positions</span>
        loss <span>=</span> <span>0</span>
        <span>for</span> i <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_predicted_tokens<span>)</span><span>:</span>
            loss <span>+=</span> F<span>.</span>cross_entropy<span>(</span>
                multi_token_logits<span>[</span><span>:</span><span>,</span> i<span>]</span><span>.</span>view<span>(</span><span>-</span><span>1</span><span>,</span> self<span>.</span>vocab_size<span>)</span><span>,</span>
                stacked_targets<span>[</span><span>:</span><span>,</span> i<span>]</span><span>.</span>reshape<span>(</span><span>-</span><span>1</span><span>)</span><span>,</span>
                ignore_index<span>=</span>ignore_index
            <span>)</span>
        
        <span>return</span> loss <span>/</span> self<span>.</span>num_predicted_tokens
</code></pre>
<h2 id="speculative-decoding" tabindex="-1">Speculative Decoding</h2>
<p>Speculative decoding is an clever technique designed to accelerate the inference process of large autoregressive language models, significantly reducing the wall-clock time required to generate text. Standard generation is bottlenecked by its sequential nature, where the computationally expensive large model (the "target" model) must perform a full forward pass to predict just one token at a time. Speculative decoding introduces a secondary, much smaller and faster "draft" model. This draft model rapidly generates a short sequence of candidate future tokens (a "draft"). The core idea is then to use the large target model to verify this entire draft sequence in a single, parallel forward pass, potentially accepting multiple tokens at once and thus amortizing the cost of the target model's computation over several generated tokens.</p>
<p>The mechanism hinges on comparing the predictions of the draft model with those of the target model. After the draft model proposes a sequence of \( k \) tokens, \( d_1, d_2, \dots, d_k \), the target model is run once on the original input sequence concatenated with the draft. This single pass yields the target model's probability distributions for the <em>next</em> token at <em>each</em> position within the draft sequence. For each draft token \( d_i \), its validity is checked against the target model's prediction at that same position. If the target model strongly agrees with the draft token (e.g., it would have also predicted \( d_i \) with high probability, or based on a specific acceptance rule comparing \( P_{\text{target}}(d_i) \) and \( P_{\text{draft}}(d_i) \)), the token is accepted. This verification proceeds sequentially through the draft.</p>
<p>The process continues until either all \( k \) draft tokens are accepted, or a draft token \( d_j \) is rejected. If rejection occurs at position \( j \), all preceding accepted draft tokens (\( d_1, \dots, d_{j-1} \)) are kept. The remaining part of the draft (\( d_j, \dots, d_k \)) is discarded. Crucially, the target model's probability distribution calculated at position \( j \) can then be used to sample a <em>corrected</em> token to append after the accepted sequence, ensuring the overall generation statistically follows the distribution of the more powerful target model. This corrected token, along with the previously accepted ones, forms the input for the next round of draft generation. By accepting multiple tokens per target model inference step on average, speculative decoding can achieve substantial speedups (e.g., 2-3x) with minimal impact on the quality of the generated text.</p>
<pre><code><span>class</span> <span>SimpleSpeculativeDecoding</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> target_model<span>,</span> draft_model<span>,</span> tokenizer<span>,</span> max_draft_tokens<span>=</span><span>5</span><span>)</span><span>:</span>
        self<span>.</span>target_model <span>=</span> target_model
        self<span>.</span>draft_model <span>=</span> draft_model
        self<span>.</span>tokenizer <span>=</span> tokenizer
        self<span>.</span>max_draft_tokens <span>=</span> max_draft_tokens
    
    <span>def</span> <span>generate</span><span>(</span>self<span>,</span> prompt<span>,</span> max_length<span>=</span><span>100</span><span>)</span><span>:</span>
        <span># Start with the prompt's token IDs</span>
        input_ids <span>=</span> self<span>.</span>tokenizer<span>.</span>encode<span>(</span>prompt<span>,</span> return_tensors<span>=</span><span>"pt"</span><span>)</span>
        
        <span># Keep generating until we reach max_length or an end token</span>
        <span>while</span> input_ids<span>.</span>shape<span>[</span><span>1</span><span>]</span> <span>&lt;</span> max_length<span>:</span>
            <span># Step 1: Generate multiple draft tokens</span>
            draft_input_ids <span>=</span> input_ids<span>.</span>clone<span>(</span><span>)</span>
            draft_tokens <span>=</span> <span>[</span><span>]</span>
            
            <span>with</span> torch<span>.</span>no_grad<span>(</span><span>)</span><span>:</span>
                <span>for</span> _ <span>in</span> <span>range</span><span>(</span>self<span>.</span>max_draft_tokens<span>)</span><span>:</span>
                    <span># Generate next token with draft model</span>
                    outputs <span>=</span> self<span>.</span>draft_model<span>(</span>draft_input_ids<span>)</span>
                    next_token_logits <span>=</span> outputs<span>.</span>logits<span>[</span><span>:</span><span>,</span> <span>-</span><span>1</span><span>,</span> <span>:</span><span>]</span>
                    next_token <span>=</span> torch<span>.</span>argmax<span>(</span>next_token_logits<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
                    
                    <span># Save the draft token</span>
                    draft_tokens<span>.</span>append<span>(</span>next_token<span>.</span>item<span>(</span><span>)</span><span>)</span>
                    
                    <span># Add to draft input</span>
                    draft_input_ids <span>=</span> torch<span>.</span>cat<span>(</span><span>[</span>draft_input_ids<span>,</span> next_token<span>.</span>unsqueeze<span>(</span><span>0</span><span>)</span><span>]</span><span>,</span> dim<span>=</span><span>1</span><span>)</span>
                    
                    <span># Check for end token</span>
                    <span>if</span> next_token<span>.</span>item<span>(</span><span>)</span> <span>==</span> self<span>.</span>tokenizer<span>.</span>eos_token_id<span>:</span>
                        <span>break</span>
            
            <span># Step 2: Verify with target model</span>
            <span>with</span> torch<span>.</span>no_grad<span>(</span><span>)</span><span>:</span>
                <span># Get target model probabilities for all tokens including drafts</span>
                verification_ids <span>=</span> torch<span>.</span>cat<span>(</span><span>[</span>input_ids<span>,</span> torch<span>.</span>tensor<span>(</span><span>[</span>draft_tokens<span>]</span><span>)</span><span>.</span>to<span>(</span>input_ids<span>.</span>device<span>)</span><span>]</span><span>,</span> dim<span>=</span><span>1</span><span>)</span>
                target_outputs <span>=</span> self<span>.</span>target_model<span>(</span>verification_ids<span>)</span>
                target_logits <span>=</span> target_outputs<span>.</span>logits<span>[</span><span>:</span><span>,</span> input_ids<span>.</span>shape<span>[</span><span>1</span><span>]</span><span>-</span><span>1</span><span>:</span><span>]</span>
                
                <span># Convert to probabilities</span>
                target_probs <span>=</span> F<span>.</span>softmax<span>(</span>target_logits<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
                
                <span># Accept tokens until mismatch or all drafts accepted</span>
                accepted_tokens <span>=</span> <span>[</span><span>]</span>
                <span>for</span> i<span>,</span> token_id <span>in</span> <span>enumerate</span><span>(</span>draft_tokens<span>)</span><span>:</span>
                    <span># Check if target model agrees with draft</span>
                    target_prob <span>=</span> target_probs<span>[</span><span>0</span><span>,</span> i<span>,</span> token_id<span>]</span><span>.</span>item<span>(</span><span>)</span>
                    draft_prob <span>=</span> <span>1.0</span>  <span># Draft chose this with highest probability</span>
                    
                    <span># Accept based on probability ratio (simplified)</span>
                    <span>if</span> random<span>.</span>random<span>(</span><span>)</span> <span>&lt;</span> <span>min</span><span>(</span><span>1.0</span><span>,</span> target_prob <span>/</span> draft_prob<span>)</span><span>:</span>
                        accepted_tokens<span>.</span>append<span>(</span>token_id<span>)</span>
                    <span>else</span><span>:</span>
                        <span># Rejection: get new token from target model</span>
                        new_token <span>=</span> torch<span>.</span>argmax<span>(</span>target_logits<span>[</span><span>0</span><span>,</span> i<span>]</span><span>)</span><span>.</span>item<span>(</span><span>)</span>
                        accepted_tokens<span>.</span>append<span>(</span>new_token<span>)</span>
                        <span>break</span>
            
            <span># Add accepted tokens to input_ids</span>
            input_ids <span>=</span> torch<span>.</span>cat<span>(</span><span>[</span>input_ids<span>,</span> torch<span>.</span>tensor<span>(</span><span>[</span>accepted_tokens<span>]</span><span>)</span><span>.</span>to<span>(</span>input_ids<span>.</span>device<span>)</span><span>]</span><span>,</span> dim<span>=</span><span>1</span><span>)</span>
            
            <span># Check for end token</span>
            <span>if</span> input_ids<span>[</span><span>0</span><span>,</span> <span>-</span><span>1</span><span>]</span><span>.</span>item<span>(</span><span>)</span> <span>==</span> self<span>.</span>tokenizer<span>.</span>eos_token_id<span>:</span>
                <span>break</span>
        
        <span># Decode the generated tokens</span>
        <span>return</span> self<span>.</span>tokenizer<span>.</span>decode<span>(</span>input_ids<span>[</span><span>0</span><span>]</span><span>)</span>
</code></pre>

          </div></div>]]></description>
        </item>
    </channel>
</rss>