<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 11 Nov 2024 09:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Apple threatened workers over their talk about pay and remote work, feds charge (165 pts)]]></title>
            <link>https://www.mercurynews.com/2024/11/06/apple-threatened-workers-over-talk-about-pay-remote-work-feds-charge/</link>
            <guid>42104762</guid>
            <pubDate>Mon, 11 Nov 2024 05:29:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mercurynews.com/2024/11/06/apple-threatened-workers-over-talk-about-pay-remote-work-feds-charge/">https://www.mercurynews.com/2024/11/06/apple-threatened-workers-over-talk-about-pay-remote-work-feds-charge/</a>, See on <a href="https://news.ycombinator.com/item?id=42104762">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary"><main id="main" role="main">		<article id="post-11253673">
			<header>
<div><h3>SUBSCRIBER ONLY</h3>
	<h2>

	
				<span>
			Apple illegally threatened workers over their talk about pay and remote work, feds charge		</span>



	
	
	</h2>
<h2>Software engineer Cher Scarlett allegedly railroaded out of Apple</h2></div></header><div><p>A manager in a phone call told a worker Apple did not want employees talking about wages or pay equity, the complaint alleged.</p>


			<p>Originally Published: <time datetime="2024-11-06 06:15:49">November 6, 2024 at 6:15 AM PST</time></p></div></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Virtual Windows 3.11 Computer (109 pts)]]></title>
            <link>https://pieter.com/</link>
            <guid>42104531</guid>
            <pubDate>Mon, 11 Nov 2024 04:08:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pieter.com/">https://pieter.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42104531">Hacker News</a></p>
Couldn't get https://pieter.com/: Error: connect ECONNREFUSED 2a01:4ff:1f0:9255::1:443]]></description>
        </item>
        <item>
            <title><![CDATA[Standing desk might be as bad as sitting all day (111 pts)]]></title>
            <link>https://www.sciencealert.com/your-standing-desk-might-actually-be-as-bad-as-sitting-all-day</link>
            <guid>42103761</guid>
            <pubDate>Mon, 11 Nov 2024 01:06:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencealert.com/your-standing-desk-might-actually-be-as-bad-as-sitting-all-day">https://www.sciencealert.com/your-standing-desk-might-actually-be-as-bad-as-sitting-all-day</a>, See on <a href="https://news.ycombinator.com/item?id=42103761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>In recent years, standing has been touted as a remedy to a sedentary lifestyle, especially for desk workers who spend long hours seated at their screens.</p><p>But a new study from researchers in Australia and the Netherlands has found standing for long periods of time might not be much better than sitting after all – and actually comes with its own life-threatening risks.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>Just under seven years of data from 83,013 adults were collected as part of the UK <a href="https://en.wikipedia.org/wiki/UK_Biobank">Biobank</a>, using wrist-worn devices to track their activity, sleep, and sedentary time. The amount of time individuals spent standing and sitting was matched with incidences of cardiovascular diseases – <a href="https://en.wikipedia.org/wiki/Coronary_artery_disease">coronary heart disease</a>, <a href="https://en.wikipedia.org/wiki/Heart_failure">heart failure</a> and <a href="https://en.wikipedia.org/wiki/Stroke">stroke</a> – as well as circulatory diseases – <a href="https://en.wikipedia.org/wiki/Hypotension#Orthostatic_hypotension">low blood pressure on standing</a>, <a href="https://en.wikipedia.org/wiki/Varicose_veins">varicose veins</a>, <a href="https://en.wikipedia.org/wiki/Chronic_venous_insufficiency">chronic venous insufficiency</a>, and <a href="https://en.wikipedia.org/wiki/Venous_ulcer">venous ulcers</a>.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>The researchers found no association between time spent standing and the risk of cardiovascular disease, suggesting standing desks and similar work postures might not be enough to stave off the health problems <a href="https://www.sciencealert.com/around-the-world-people-are-dying-too-soon-because-they-sit-around-too-much">associated with sitting around</a>.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>University of Sydney population health scientist Matthew Ahmadi suggests this might be because many studies in support of standing were based on 'soft endpoints' like improved blood pressure, insulin sensitivity, and triglyceride levels.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>His team's investigation, on the other hand, focused on 'hard clinical endpoints' – hospitalizations or death from these diseases – and found that whether people were seated or standing for the long periods of time they spent stationary didn't really make a difference to cardiovascular outcomes.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>"More time spent sitting didn't necessarily lower a person's risk of cardiovascular disease, nor did it increase the risk," Ahmadi <a href="https://www.youtube.com/watch?v=ZGuIk5KpCCQ">says</a>. "It was a null finding. But what it did do was actually increase their risk of circulatory diseases."</p><!-- START single/mrec -->
<!-- END single/mrec --><p>Standing for more than two hours a day increased that risk by 11 percent for every extra half hour, which is bad news for retail workers and the standing desk industry.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>Not that sitting is much better either: beyond 10 hours of daily sitting time, every extra hour spent on your butt increased circulatory disease risk by 26 percent.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>This suggests that a lack of walking or other movement while either sitting <em>or</em> standing could be driving the risk of orthostatic circulatory disease, the authors <a href="https://academic.oup.com/ije/article/53/6/dyae136/7822310">write</a>.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>Though the study's sample size is unrivaled, it's worth noting that as an observational study the research can't prove that standing or sitting cause any of these diseases. It does, however, add to the <a href="https://www.sciencealert.com/sitting-down-is-so-bad-for-you-even-sleeping-is-better">avalanche of research</a> on the <a href="https://www.sciencealert.com/just-12-minutes-of-intense-exercise-is-enough-to-change-biomarkers-in-your-blood">importance</a> of <a href="https://www.sciencealert.com/short-bursts-of-activity-lower-your-risk-of-early-death-study-finds">moving your body</a>.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>Ahmadi says the real take-home from this research is that standing, by itself, should not be considered a cure-all for the ailments of sedentary, seat-bound lifestyles.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>"Standing needs to be mixed in with other forms of activity that gets the body moving," he <a href="https://www.youtube.com/watch?v=ZGuIk5KpCCQ">says</a>.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>"We're not seeing the risk of orthostatic diseases… when someone's walking around. We're seeing it mainly when they're standing stationary, standing still, because you get that blood pooling in the lower extremities."</p><p>This research was published in the <em><a href="https://doi.org/10.1093/ije/dyae136">International Journal of Epidemiology</a>.</em></p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chegg is on its last legs after ChatGPT sent its stock down (101 pts)]]></title>
            <link>https://gizmodo.com/chegg-is-on-its-last-legs-after-chatgpt-sent-its-stock-down-99-2000522585</link>
            <guid>42103576</guid>
            <pubDate>Mon, 11 Nov 2024 00:19:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/chegg-is-on-its-last-legs-after-chatgpt-sent-its-stock-down-99-2000522585">https://gizmodo.com/chegg-is-on-its-last-legs-after-chatgpt-sent-its-stock-down-99-2000522585</a>, See on <a href="https://news.ycombinator.com/item?id=42103576">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              
              
              <p>If thinking about Chegg gives you PTSD to the days when you were in school, I might have some good news for you: The company known for textbook rentals and homework help is running on fumes. Chegg’s stock is down a whopping 99% since its highs in 2021, erasing $14.5 billion in value, and the company has lost half a million paid subscribers. After revenue keeps dropping quarter after quarter, there are doubts it will be able to continue paying its debts.</p> <p>Chegg should be familiar to most people who have been to college in recent years. It started out in the 2000s renting out textbooks and later expanded into online study guides, and eventually into a platform with pre-written answers to common homework questions.</p> <p>Unfortunately, the launch of ChatGPT all but annihilated Chegg’s business model. The company for years paid thousands of contractors to write answers to questions across every major subject, which is quite a labor intensive process—and there’s no guarantee they will even have the answer to your question. ChatGPT, on the other hand, has ingested pretty much the entire internet and has likely seen any history question you might throw at it.</p>

 <p>Granted, some of Chegg’s fall can be attributed to a subsiding of the pandemic lockdowns, when learning went virtual and the company needed to produce more answers. But as the <em>Wall Street Journal</em> <a href="https://www.wsj.com/tech/ai/how-chatgpt-brought-down-an-online-education-giant-200b4ff2?st=6tw8JN&amp;reflink=desktopwebshare_permalink">reports</a>, there does seem to be a correlation between the launch of ChatGPT and students dropping their Chegg subscriptions:</p> <blockquote> <p data-type="paragraph">Though Chegg has built its own AI products, the company is struggling to convince customers and investors it still has value in a market upended by ChatGPT.</p> <p data-type="paragraph">“It’s free, it’s instant, and you don’t really have to worry if the problem is there or not,” Jonah Tang, an M.B.A. candidate at Point Loma Nazarene University in San Diego, said of the advantages of using ChatGPT for homework help over Chegg.</p> <p data-type="paragraph">A survey of college students by investment bank Needham found 30% intended to use Chegg this semester, down from 38% in the spring, and 62% planned to use ChatGPT, up from 43%.</p> </blockquote> <p data-type="paragraph">It’s unclear what Chegg can do to stem the bleeding at this point. The company laid off 441 employees over the summer, a quarter of its workforce. It’s trying to target what the new CEO describes as “curious learners” by offering more comprehensive AI-assisted answers as well as live counseling.</p>

 <p data-type="paragraph">What’s perhaps most sad is that, according to the WSJ, employees actually asked for resources in 2022 to develop AI tools for automating answers in order to address the huge influx in new demand. Chegg’s leaders denied the request to start building AI tools until ChatGPT’s release, but even then some internally weren’t worried because of the chatbot’s propensity to make up incorrect answers.</p> <p data-type="paragraph">But, as with a tool like Wikipedia, students are willing to accept some risk because of the convenience. Students are told not to trust Wikipedia, but most use it anyway and head to the references section to grab citations. Of course, chatbots like ChatGPT have no concept of a subject like math; they’re just guessing the words necessary to make a sentence that sounds right. They will return answers that look <em>deceptively</em> correct but aren’t. It’s like having a calculator that’s right 50% of the time. For subjects like history, chatbots are somewhat better, but answers should be double-checked.</p>

 <p data-type="paragraph">Maybe Chegg could work harder to help people understand this? It seems like most students don’t care, though, or find that ChatGPT is good enough at sending them in the right direction, and the clock is running out for Chegg.</p>
                          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Australia's 3G Shutdown – Why your 4G/5G Phone is now Blocked (198 pts)]]></title>
            <link>https://medium.com/@jamesdwho/australias-3g-shutdown-why-your-4g-5g-phone-is-now-blocked-5900cd5361e2</link>
            <guid>42103257</guid>
            <pubDate>Sun, 10 Nov 2024 23:10:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@jamesdwho/australias-3g-shutdown-why-your-4g-5g-phone-is-now-blocked-5900cd5361e2">https://medium.com/@jamesdwho/australias-3g-shutdown-why-your-4g-5g-phone-is-now-blocked-5900cd5361e2</a>, See on <a href="https://news.ycombinator.com/item?id=42103257">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><div><div><h2 id="929f">Corporate Self Interest and the Failures of Government &amp; Regulators</h2><div aria-hidden="false"><a rel="noopener follow" href="https://medium.com/@jamesdwho?source=post_page---byline--5900cd5361e2--------------------------------"><div><p><img alt="James Parker" src="https://miro.medium.com/v2/resize:fill:88:88/1*7OHkwPRkcFhJPZCQCDUvdw.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div></div><figure></figure></div><div><h2 id="e6f1">Table of Contents</h2><ul><li id="196f"><a href="#61e4" rel="noopener ugc nofollow"><strong>Introduction</strong></a><br>- <a href="#9d86" rel="noopener ugc nofollow">Background</a><br>- <a href="#17d5" rel="noopener ugc nofollow">The Problem</a></li><li id="3327"><a href="#bf9d" rel="noopener ugc nofollow"><strong>The 28 October 2024 Shutdown</strong></a><br>- <a href="#f0ff" rel="noopener ugc nofollow">Scale of the issue</a><br>- <a href="#d66d" rel="noopener ugc nofollow">Last Minute Messaging to Telstra Customers</a><br>- <a href="#fd93" rel="noopener ugc nofollow">Optus Messaging</a></li><li id="9045"><a href="#b402" rel="noopener ugc nofollow"><strong>Why this happened</strong></a><br>- <a href="#926e" rel="noopener ugc nofollow">The Amendment to the Emergency Call Service Determination</a><br>- <a href="#9632" rel="noopener ugc nofollow">The ‘Public Consultation’</a></li><li id="9db9"><a href="#83ea" rel="noopener ugc nofollow"><strong>The 8 November Optus Outage</strong></a><br>- <a href="#380b" rel="noopener ugc nofollow">Optus Outage Review &amp; Findings</a><br>- <a href="#d55c" rel="noopener ugc nofollow">Optus Blocking Officially Supported Devices</a><br>- <a href="#3e46" rel="noopener ugc nofollow">Optus Blocking New Phones Purchased Retail</a></li><li id="7952"><a href="#064f" rel="noopener ugc nofollow"><strong>Impacts on Consumers</strong></a><br>- <a href="#0247" rel="noopener ugc nofollow">Costs to Consumers</a></li><li id="f26a"><a href="#243a" rel="noopener ugc nofollow"><strong>Impacts on Tourists &amp; Roamers</strong></a><br>- <a href="#2c1c" rel="noopener ugc nofollow">Issues with VoLTE &amp; Roaming</a></li><li id="b3b6"><a href="#3d79" rel="noopener ugc nofollow"><strong>Industry Awareness of the Problem</strong></a></li><li id="4ad0"><a href="#2fef" rel="noopener ugc nofollow"><strong>Lack of Government Oversight with Vodafone</strong></a><br>- <a href="#4f4e" rel="noopener ugc nofollow">Vodafone’s awareness of the Emergency Calling Issue</a></li><li id="62aa"><a href="#285d" rel="noopener ugc nofollow"><strong>Device Survey &amp; Results</strong></a><strong><br>- </strong><a href="#b1d8" rel="noopener ugc nofollow">List of Blocked Devices</a></li><li id="158d"><a href="#5268" rel="noopener ugc nofollow"><strong>What you can do!</strong></a></li></ul></div><div><h2 id="61e4">Introduction</h2><p id="4eaf">Australia is currently in the midst of the most significant change to the telecommunications landscape it has ever experienced in modern history.</p><p id="59d8">The shutdown of the 3G Mobile Network.</p><p id="e83d">Originally announced by Telstra in late 2019, the shutdown is supposed to usher in a new age of modern high speed communications to power the modern 4G &amp; 5G data dependent world we find ourselves in, and are increasingly reliant on.</p><figure><figcaption>SMH — The end is nigh for Telstra’s 3G network; termination set for 2024 — October 2019<br><a href="https://www.smh.com.au/technology/the-end-is-nigh-for-telstra-s-3g-network-termination-set-for-2024-20191009-p52z0n.html" rel="noopener ugc nofollow" target="_blank">https://www.smh.com.au/technology/the-end-is-nigh-for-telstra-s-3g-network-termination-set-for-2024-20191009-p52z0n.html</a></figcaption></figure><p id="84a7">As they did with the shutdown of 2G in 2018, the telcos &amp; industry have framed this as a necessary step forward in advancing their 4G and 5G coverage.</p><p id="5500">However, the handling of the shutdown has been met with disruption &amp; delays, mismanagement, industry self interest and an overall lack of preparedness by Government, Regulators and the sector at large.</p><p id="02e9">Whilst shutting down 3G sounds like a perfectly reasonable proposition, the realities with achieving that in the modern landscape of fractured technological standards and cost cutting means this process has been fraught with problems.</p><p id="dc80">All of which have been entirely foreseeable.</p><h2 id="9d86">Background</h2><p id="dc0b">The Shutdown of Telstra’s 3G Network was due to originally occur on <strong>30 June 2024, with Optus to follow from September 2024.</strong></p><p id="7638">Vodafone (TPG Telecom) was the first out of the gate with a planned switch-off from 15 December 2023.</p><p id="0931">TPG would later realise they got out scot-free, largely free from the controversies to come.</p><p id="8283">Though everyone has a had part to play in this, including TPG/Vodafone.</p><p id="4e40">On Sunday 17 March 2024 the Communications Minister stated she became aware that approximately 740,000 4G phones (that support VoLTE/4G Calling) would not be able to call 000/Emergency Services after the 3G switch off.</p><p id="82cc">Following on from that March announcement, on 6 May Telstra announced that they would delay their shutdown to 31 August.</p><p id="4c78">Then subject to a Senate Inquiry in July, Telstra and Optus eventually jointly delayed their shutdown until 28 October 2024.</p><figure><figcaption>Telstra, Optus to delay 3G network closure amid public safety concerns — 2024–08–14 <a href="https://www.abc.net.au/news/2024-08-14/telstra-optus-delay-3g-shutdown/104222598" rel="noopener ugc nofollow" target="_blank">https://www.abc.net.au/news/2024-08-14/telstra-optus-delay-3g-shutdown/104222598</a></figcaption></figure><p id="14cb"><em>You can learn more below.</em></p><p id="bef9">The entire time the industry and telcos have touted the shutdown as a critically necessary step in ensuring we can interact with the modern digital world.</p><p id="32fb">Meanwhile telcos like Telstra continue to post massive profits whilst simultaneously increasing the cost of mobile services for customers, with ever diminishing value in return.</p><figure><figcaption>SMH — ‘Telstra profits pass $2b on mobile growth’ — August 2023<br><a href="https://www.smh.com.au/business/companies/telstra-profits-pass-2b-on-mobile-growth-20230816-p5dx29.html" rel="noopener ugc nofollow" target="_blank">https://www.smh.com.au/business/companies/telstra-profits-pass-2b-on-mobile-growth-20230816-p5dx29.html</a></figcaption></figure><h2 id="17d5">The Problem</h2><p id="ba25"><strong>A key factor that the industry has been keen to not draw too much attention to for over a year is the number of 4G &amp; 5G devices that would no longer work, or would only work on some networks, including for Emergency Calling.</strong></p><p id="e358">The idea that a 4G or 5G phone could somehow be affected by the shutting down of older technologies like 2G &amp; 3G is a completely foreign concept to people, and rightfully so.</p><p id="4b81">The last 20-30 years of 2G &amp; 3G has enabled seamless global connectivity and greatly enhanced competition &amp; innovation in the mobile sector.</p><blockquote><p id="b2ce"><strong><em>Surely 4G &amp; 5G would be an extension of the same, because of course it would!</em></strong></p><p id="9600"><strong><em>How could it not be?….. right..?</em></strong></p></blockquote><p id="0ab8">Well this is where it all starts to fall apart.</p><p id="e0f3">Unlike with 2G &amp; 3G, <strong>4G &amp; 5G are Data only</strong> standards and have no built-in calling functionality, let alone one as well standardised as the traditional Circuited Switched calling from 2G &amp; 3G (GSM/UMTS).</p><p id="54f2">This becomes a problem as to enable calling over 4G &amp; 5G, devices need to have explicit software/firmware support, especially so for Emergency Calling.</p><p id="6c79">Calling on 4G Networks is enabled through the use of VoLTE (Voice over LTE aka ‘4G Calling’), which is a software/firmware VoIP (Voice over Internet Protocol) calling solution for mobile phones.</p><p id="e995">The world has used VoIP Calling for decades but introducing VoIP data based calling into the mobile sector has proven very difficult.</p><p id="da98">Voice Over LTE (4G Calling) devices have been around since as early as 2013, however over years the industry has failed to ensure interoperability.</p><figure><figcaption>‘Should we stop the shutdown of 2G/3G to save lives??’ | Rudolf van der Berg — Stratix | EENA 2022<br><a href="https://drive.google.com/file/d/1WC16k8C1gpeFRJif23yDIuLSRg1OJOnZ/view" rel="noopener ugc nofollow" target="_blank">https://drive.google.com/file/d/1WC16k8C1gpeFRJif23yDIuLSRg1OJOnZ/view</a></figcaption></figure><p id="fed6">The global interoperability with devices that people expect and have become accustomed to for the last 20+ years has entirely disappeared and the vast majority have no knowledge that it’s even happened.</p></div><div><p id="daac"><em>You can learn more about the technical specifics regarding calling on 4G in the articles below.</em></p></div><div><h2 id="bf9d">The 28 October 2024 Shutdown</h2><p id="3cd6">Starting on Monday the 28th of October Telstra and Optus began their shutdown process.</p><p id="de96">What many customers were not aware of is later that same day <strong>they would be entirely disconnected and blocked from all mobile services on their 4G or 5G Device, even on brand new devices!</strong></p><p id="4914">Parents that dropped kids off at school or day care early in the morning later found themselves without any mobile service, <strong>unable to call or text anyone.</strong></p><p id="6927">Whether unable to contact employers or family members, many Australians were caught off guard, surprisingly still the numbers to be affected.</p></div><div><h2 id="f0ff">Scale of the issue</h2><p id="5892">One would assume that the Shutdown of the 3G network would only impact those with phones from last decade, or those unwilling to upgrade.</p><p id="47ba">However the Government and ACMA was fully aware that people with newer 4G devices would be impacted and in sizeable numbers.</p><p id="c3ea">The ACMA and Government had been monitoring the shutdown, and based on numbers from industry expected approximately 258,000 4G mobile phones would be impacted once the shutdown began.</p><figure><figcaption>Emergency Call Service Amendment Determination 2024 Explanatory statement | F2024L01353ES Pg9 <a href="https://www.legislation.gov.au/F2024L01353/asmade/text/explanatory-statement" rel="noopener ugc nofollow" target="_blank">https://www.legislation.gov.au/F2024L01353/asmade/text/explanatory-statement</a></figcaption></figure><p id="9133">However that 258,000 Number is actually an estimate based on an earlier October figure.</p><p id="d0b6"><strong>On 1 October 2024 the total number of ‘affected phones’ was noted as 516,875 including 3G, 4G &amp; 5G.</strong></p><figure></figure><p id="22f8"><strong>More troubling is those numbers include thousands of 4G &amp; 5G Devices that actually work perfectly on 4G, including for Emergency Calling.</strong></p><p id="a96a">Lumped in with ‘incompatible’ devices because the telcos have next to no visibility of what individual devices can make emergency calls over 4G.</p></div><div><h2 id="d66d">Last Minute Messaging to Telstra Customers</h2><p id="dd46">For months in advance of the switch-off the messaging from the providers indicated that older and incompatible devices may lose access to calling as the network was shutdown, but at minimum Data &amp; SMS would remain on 4G &amp; 5G devices.</p><figure><figcaption>Telstra SMS 3498 System Message</figcaption></figure><p id="9a44">This was true with Telstra up until the 25th of October.</p><p id="ae24">However from Midday Friday the 25th of October, Telstra customers were instead told via SMS <strong>their 4G/5G device would be blocked from all services, including Data &amp; SMS.</strong></p><figure><figcaption><strong>Text Message Sent to Telstra Customers Midday (AEST) Friday 25 October 2024</strong></figcaption></figure><blockquote><p id="7e92"><strong><em>Telstra customers were given half a business day’s notice that their 4G or 5G device would be entirely blocked from all services from Monday!</em></strong></p></blockquote></div><div><h2 id="fd93">Optus Messaging</h2><p id="2932">From late August Optus had been telling customers that their 4G or 5G device “would stop working” or “would not work”.</p><p id="ca9a"><strong>However they consistently neglected to use the word “Block”.</strong></p><p id="a0a2">Sometimes hiding it within the fine print of emails.</p><figure></figure></div><div><figure><figcaption>Amaysim is an MVNO Owned by Optus</figcaption></figure><p id="1e0c">Someone with a new 4G or 5G device would rightfully assume those messages are wrong and Optus was being overcautious, especially if they recently just upgraded.</p><p id="1bb1"><em>People were even told to disregard the message if they’d recently upgraded.</em></p><p id="a02a">Little did they know their device would be blocked starting from Monday.</p><figure></figure><p id="24a7"><em>You can read more about Optus and their conduct with this below.</em></p><p id="ca22"><a rel="noopener" href="https://medium.com/@jamesdwho/australias-3g-shutdown-telcos-to-block-working-4g-5g-phones-2bf41e95de8a#d468"><em>Optus — ‘Keeping our customers safe’</em></a></p></div><div><h2 id="b402">Why this happened</h2><h2 id="926e">The Amendment to the Emergency Call Service Determination</h2><p id="4cc1"><strong>On the 21st of August </strong>the Minister of Communications, Michelle Rowland issued the ACMA with a direction to amend the ‘Emergency Calling Rules’ and require the carriers <strong>to ‘cease providing service’ to any phones the carriers have determined cannot call Emergency Services on 4G.</strong></p><figure><figcaption><a href="https://www.legislation.gov.au/F2024L01103/asmade/text" rel="noopener ugc nofollow" target="_blank">https://www.legislation.gov.au/F2024L01103/asmade/text</a></figcaption></figure></div><div><figure><figcaption><strong>ACMA (Emergency Call Service Determination) Direction 2024 — Explanatory Statement<br></strong>⁣<a href="https://www.legislation.gov.au/F2024L01103/asmade/text/explanatory-statement" rel="noopener ugc nofollow" target="_blank">https://www.legislation.gov.au/F2024L01103/asmade/text/explanatory-statement</a></figcaption></figure><p id="8243">Now on paper this may sound like a reasonable idea, but as with most things the devil is in the detail. <em>Especially anything involving technology.</em></p><p id="8538">Such an arguably appropriate and moral decision was made completely devoid of any understanding about how technology actually works, combined with the conflicts of interest in letting the carriers be the sole arbiters of what is allowed and what isn’t.</p><blockquote><p id="e7ae">It was obvious from the outset that this would go wrong.</p></blockquote><p id="6cd0">After learning about the direction from the Minister, in September I tried to warn her about the consequences &amp; harms that would result.</p></div><div><h2 id="9632">The ‘Public Consultation’</h2><p id="7094"><strong>On the 24th of September</strong> the ACMA released a draft of the changes to the ‘Emergency Call Service Determination’, along with an Open Consultation Feedback. <em>(The Consultation opened 4 Days after contacting the Minister through the office of a 3G Committee Senator)</em></p><figure><a href="https://www.acma.gov.au/consultations/2024-09/proposal-amend-ecs-determination"></a><figcaption><strong>ACMA — Proposal to amend the ECS Determination 2024<br></strong> <a href="https://www.acma.gov.au/consultations/2024-09/proposal-amend-ecs-determination" rel="noopener ugc nofollow" target="_blank">https://www.acma.gov.au/consultations/2024-09/proposal-amend-ecs-determination</a></figcaption></figure><p id="b4bd">However this consultation <strong>was only due to run for 2 weeks until the 8th of October 2024.</strong></p><p id="500a"><strong>Most Consultations by the ACMA run from anywhere from 4–5 weeks depending on the topic and stakeholders involved.</strong></p><p id="16ba">In total there were 40 submissions to the ACMA, including 11 directly from Industry. <em>(Including Industry Groups, MNOs, MVNOs etc)</em></p><p id="1c4d">You would have thought, given the seriousness of totally blocking hundreds of thousands of phones in a matter of weeks, that the ACMA would have ensured a sufficiently long public consultation period.</p><p id="4f66"><em>Or at minimum the Minister would have actually intervened and delayed the shutdown.</em></p><p id="ae9d">Even more questionable is why it was only opened on the 24th of September, when the ACMA had the direction from the Minister weeks earlier in late August/early September.</p><p id="2a88">I, along with other members of the public made submissions to the ACMA with very serious concerns regarding the Minister’s Direction and the Draft put forward by the ACMA.</p><p id="08f4"><a href="https://drive.google.com/file/d/1MteFp18gHtGWKRyFBvaEcNO5quyIksgB/view?usp=drive_link" rel="noopener ugc nofollow" target="_blank"><em>Submission to the ACMA - RE Proposed changes to the ECS Determination 2024 - James Parker | 2024–10–08</em></a></p><p id="3962">Even the Telcos raised some serious concerns about the technical feasibility of blocking devices.</p><p id="2b01"><strong>On Thursday 24 October,</strong> the finalised Amendment to the Emergency Call Service Determination was published on the Legilsation.gov.au website.</p><p id="87eb"><strong>Emergency Call Service Amendment Determination 2024</strong><br><a href="https://www.legislation.gov.au/F2024L01353/asmade/text" rel="noopener ugc nofollow" target="_blank">https://www.legislation.gov.au/F2024L01353/asmade/text</a></p><p id="27ba">From that point it became official, the Government <em>(much to some resistance from the Telcos)</em> were going to be<strong> forced to block over a quarter of a million people and their devices in the span of mere days and weeks.</strong></p><blockquote><p id="79f5"><strong>The concerns of consumers and industry were entirely disregarded.</strong></p></blockquote><p id="bbbd">The somewhat more reasonable suggestion by Telstra to just disable call service and leave Data &amp; SMS was rejected by the ACMA.</p><p id="e239"><strong>If the telco believes a device to be ‘incompatible’ it would be blocked, from all services. No exemptions, no dispute processes, no proof required, simply denied all services!</strong></p><p id="8bf8">I own devices that work perfectly but the Telcos don’t recognise they work, this includes devices that have placed multiple calls to 000 over 4G.</p><h2 id="f58c"><strong>Normal 4G Emergency Calling Behaviour (With Active Sim &amp; VoLTE)</strong></h2><figure></figure><p id="e333"><strong>Description: </strong>In the above example the device successfully places an Emergency Call over 4G with an active <strong>‘VoLTE’ </strong>service. This is the intended Calling behaviour for a device that supports 4G Emergency Calls.</p><p id="5adf">The device <em>(Android 9 — Xperia XZ1C — SD 835)</em> was running a GSMA (‘Open Market’) IR.92 + IR.51 VoLTE Modem Configuration.</p><blockquote><p id="7e28"><em>However, as Vodafone </em><strong><em>did not </em></strong><em>sell this model of device and it </em><strong><em>was not</em></strong><em> sold in Australia </em><strong><em>they do not recognise that it supports both VoLTE Calling and Emergency Calling!</em></strong></p></blockquote><figure></figure><figure></figure><h2 id="8817">Incompatible Devices</h2><p id="3228">Contrast the above with what happens with a device <strong>that actually is unable to make emergency calls over 4G.</strong></p><figure></figure><p id="01c6">What people also don’t understand is that these compatibility issues with VoLTE Calling and Emergency Calling <strong>are Software Problems, not hardware issues.</strong></p><p id="42db">Two identical phones running different software can have different VoLTE Calling and Emergency Calling functionality on various networks.</p><p id="eadb"><strong>This Industry has tried to pretend that 4G devices that rely on 3G for 000 are ‘hardwired’ this way and the only solution is to buy new devices, when that could not be further from the truth.</strong></p><figure><figcaption>Sony Xperia XZ1 — Qualcomm Snapdragon 835 Chipset — Tested Q2 2024 — <a href="https://docs.google.com/spreadsheets/d/1VoFf3HuHHwwyz3eWAgsdxP2Qif30_mwbCbtmzCt1Azw/" rel="noopener ugc nofollow" target="_blank">Extract from Testing</a></figcaption></figure><p id="e266"><strong><em>You can learn more below:</em></strong></p></div><div><h2 id="380b">Optus Outage Review &amp; Findings</h2><p id="8ffd">In the Government’s subsequent review into the <a href="https://en.wikipedia.org/wiki/2023_Optus_outage" rel="noopener ugc nofollow" target="_blank">8 November Optus Outage</a> <strong>it was noted that there is minimal if any testing between devices across networks.</strong></p><p id="b6ff"><em>An extract from the Report is below.</em></p><blockquote><p id="6aa4"><em>“Regardless of the technical reasons for the outage, it is clear that </em><strong><em>more testing of network interoperability is needed</em></strong><em> to ensure problems are identified and anticipated. The complexity of the carriers’ mobile networks and the necessity of interoperability to deliver Triple Zero calls demands a more thorough approach to testing than is currently in place. </em><strong><em>The carriers do not test with and/or across each other’s networks.</em></strong><em> While testing the camp on function on their own networks, even if covering all scenarios, </em><strong><em>the testing does not guarantee </em></strong><em>(as far as practicable)</em><strong><em> that calls will be picked up when a competitor’s network is unavailable.</em></strong></p><p id="bb1f"><em>Current testing regimes </em><strong><em>do not cover all devices sold by all carriers</em></strong><em>, and </em><strong><em>there is no system for addressing the capabilities of the devices of customers who bring their own</em></strong><em>. The countless variations in handsets, handset and SIM settings, and the alternative configurations between nodes within each of the networks, </em><strong><em>present a significant risk to the certain operation of the camp on functionality </em></strong><em>in all (or as many as might reasonably be anticipated) circumstances. </em><strong><em>This needs to be addressed.</em></strong></p><p id="6bc2"><strong><em>The establishment of cross-carrier end-to-end network and device testing including under the wide range of known scenarios would provide information and insight into potential issues before major outages occur.</em></strong><em> Such testing may have gone someway to anticipating the failure of some calls to connect to Triple Zero on the day of the outage.”</em></p></blockquote><p id="f653"><a href="https://www.infrastructure.gov.au/sites/default/files/documents/review_into_the_optus_outage_of_8_november.pdf" rel="noopener ugc nofollow" target="_blank"><em>Department of Infrastructure — ‘Review into the Optus outage of 8 November 2023 — Final Report’ — March 2024 — Pg22</em></a></p><p id="badf"><em>The report goes on further to say:</em></p><blockquote><p id="32da"><em>“Australian Standard AS/CA S042.1: Requirements for connection to an air interface of Telecommunications Network. This standard, [..] specifies the requirements for customer equipment (handsets), […]. It includes technical guidance for testing these handsets for various scenarios including accessing the emergency call service. </em><strong><em>Carriers use this guidance in conducting the testing that they do. There is currently no requirement for universal or cross-industry testing.”</em></strong></p></blockquote><p id="331e">As 2G and 3G calling are well understood and standardised, it hasn’t been necessary to carry out extensive device &amp; network testing.</p><p id="6015">As the standardisation of 4G VoLTE calling is extremely fragmented, testing for devices is necessary to confirm even basic compatibility, this is due to the industry failing to properly standardise VoLTE calling.</p><blockquote><p id="b678"><em>The ACMA, Department and Industry have decided </em><strong><em>not to</em></strong><em> properly address the BYOD device issue and </em><strong><em>will instead now require people to only use ‘supported’ handsets purchased directly from their telcos or associated handset partners.</em></strong></p></blockquote><blockquote><p id="52da">With this policy change you can now no longer use any device you want from any provider in the world, even if it may work perfectly!</p></blockquote><p id="0403"><strong>The telcos get to be judge, jury and execution, solely deciding what phones you are allowed to use and where you must have purchased them from.</strong></p></div><div><h2 id="d55c">Optus Blocking Officially Supported Devices</h2><p id="1ae3">Due to the failures by Optus during the 8 November 2023 outage, in early September began instigating a very regressive device ‘Whitelist’ blocking policy.</p><figure><figcaption><a href="https://www.optus.com.au/content/dam/optus/documents/for-you/support/mobiles-tablets-wearables/important-changes-3g/0830_an_important_update_on_mobile_handset_safety_legalv1_03.09.24.pdf" rel="noopener ugc nofollow" target="_blank">Optus — An Important Update on Mobile Handset Safety — 3 September 2024</a></figcaption></figure><blockquote><p id="1966">In essence if they (or their partners) didn’t sell a phone or test it, it’s blocked, even if it’s a supported hardware model!</p></blockquote><p id="4f4f"><strong>Optus is even blocking Officially Supported phones that are on their device support list!</strong></p><p id="2cad">I own an 4G Sony Xperia XZ Premium (2018) which is officially supported by Optus &amp; Telstra.</p><figure><figcaption>Optus VoLTE Support List <a href="https://www.optus.com.au/support/answer?id=20032" rel="noopener ugc nofollow" target="_blank">https://www.optus.com.au/support/answer?id=20032</a></figcaption></figure><figure><figcaption>Telstra VoLTE Support List <a href="https://www.telstra.com.au/support/mobiles-devices/enable-volte-mobile-phone#compatible-device" rel="noopener ugc nofollow" target="_blank">https://www.telstra.com.au/support/mobiles-devices/enable-volte-mobile-phone#compatible-device</a></figcaption></figure><p id="58f5"><strong>I have a Network Unlocked version originally sold through Telstra that is running the Australian Retail Software.</strong></p><p id="574e"><em>The phone works (worked) perfectly for VoLTE Calling on Telstra and Optus.</em></p><figure></figure><p id="1338"><strong>However as my device was sold by Telstra, Optus has now blocked it from their network!</strong></p><figure></figure><p id="086d"><strong>It’s exactly the same hardware model, but because it was sold by their competitor and they don’t think it works, it’s entirely blocked!</strong></p><p id="8c56">Even if loaded with the Optus specific software the device would still be blocked as <strong>the phone is being denied all service based on the TAC ‘Type Allocation Code’ section of the device IMEI. </strong>Not the actual functionality.<br><em>(The IMEI is essentially the Serial Number, similar to the VIN of a vehicle)</em></p><p id="91c1"><em>My Telstra Version of the XZ Premium has a TAC of </em><strong><em>35923708</em></strong><em><br>Where as an AU Retail versions may have a TAC of </em><strong><em>35783808</em></strong></p><p id="0af3">Both are identical ‘G8141’ Model Devices with the exact same hardware.</p><figure><figcaption>IMEI TAC Identifier Example</figcaption></figure><blockquote><p id="427f"><em>The conduct by Optus and the Telcos is extraordinarily anti-competitive and doesn’t seem even remotely legal.</em></p></blockquote></div><div><h2 id="064f">Impacts on Consumers</h2><p id="e0ec">A key element that has been neglected by Government, Regulators and Industry from the beginning is the impact on consumers.</p><p id="8a32">They may like to proudly display and talk about how there are processes and systems in place to deal with the ‘vulnerable &amp; elderly’, patting themselves on the back whilst simultaneously failing to actually ensure that consumers are correctly informed and protected during this process.</p><p id="5636">On the 14th of May Amaysim sent out a marketing email saying <strong>“IT’LL BE MAYHEM”</strong> when the switch-off occurs.</p><p id="0a23">This sort of marketing material was extremely unnecessary and only served to drive people to panic buy new phones.</p><figure></figure></div><div><h2 id="2903">Media Coverage</h2><p id="389d">Over the past year the bulk of the reporting that has been in the media <em>(primarily that of the commercial TV networks) </em>has been (and continues to be) very poor, <strong>for months there has been essentially little to no mention as to the possible impacts to 4G and 5G devices.</strong></p><p id="b4d5"><strong>On 21 August 2023</strong> both 7 News and 9 News aired stories about the 3G switch-off with their 6PM Bulletins.</p><blockquote><p id="7dd3"><strong><em>Both TV Networks included interviews from the providers however the fact that 4G and 5G devices could be affected wasn’t mentioned once!</em></strong></p></blockquote><figure><figcaption>YouTube — Australians warned of upcoming 3G switch-off from Telstra, Optus &amp; Vodafone — 7 News | 2023–08–21<br><a href="https://web.archive.org/web/20240124101628/https://www.youtube.com/watch?v=yJZotwC3hjg" rel="noopener ugc nofollow" target="_blank">https://web.archive.org/web/20240124101628/https://www.youtube.com/watch?v=yJZotwC3hjg</a></figcaption></figure><figure><figcaption>YouTube — 3G network to shutdown in a number of months — 9 News Australia | 2023–08–21<br><a href="https://www.youtube.com/watch?v=JiRdec-E8ps" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=JiRdec-E8ps</a></figcaption></figure><p id="13e7"><strong>On 22 August 2023</strong> I once again contacted the office of my Local Member Anika Wells regarding the TV news coverage. I asked for any updates from the Minister’s Office regarding the concerns I raised on 6 June 2023.</p><blockquote><p id="404b">I said<em> “Last night (21/08) the Brisbane 6PM nightly news bulletins on Channel 7 and Channel 9 covered the switch-off, but </em><strong><em>both failed to mention the VoLTE support issue and the impact the switch off will have on 4G Devices that lack VoLTE support on Australian networks. It seems like the telco’s aren’t being entirely upfront about this issue to the media and its critical people are properly informed about this.”</em></strong></p></blockquote><p id="f3cc"><strong>On 24 August 2023 </strong>I received a response that said:</p><p id="1b80"><strong><em>“We are yet to receive an update, we are liaising with Minister Rowland’s office on the status of the update”</em></strong></p><p id="f318"><em>It would take another month to receive a response on 25 September 2023.</em></p><p id="3b8f">I also raised these concerns with the aforementioned TV networks via email on 22 August. One of the networks did respond to my email, and <strong>I had an hour long on-camera interview with them in early September 2023, however the network never aired the story.</strong></p><blockquote><p id="e74f">No explanation was provided. After a number of follow-ups on 15 November they finally advised “At this stage we aren’t running the story sorry.”</p></blockquote><p id="c7e7">Blindly listening to industry and their Government relation lobbyists has resulted in the outcome we find ourselves in today.</p><p id="fdfc">By stark contrast France <strong>will have 3G until approx 2028/2029 </strong>and the UK will be <strong>keeping their 2G network around until approx 2030/2033</strong>.</p><blockquote><p id="af16">Both countries have more than 2.5x our population.</p></blockquote><p id="82b5"><em>Kore Wireless — ‘Global 2G &amp; 3G Network Closures’<br></em><a href="https://www.korewireless.com/2g-3g-network-sunset-dates" rel="noopener ugc nofollow" target="_blank"><em>https://www.korewireless.com/2g-3g-network-sunset-dates</em></a></p><p id="ebe4"><em>Mobile UK — ‘2G/3G Switch Off’<br></em><a href="https://www.mobileuk.org/2g-3g-switch-off" rel="noopener ugc nofollow" target="_blank"><em>https://www.mobileuk.org/2g-3g-switch-off</em></a></p></div><div><h2 id="0247">Costs to Consumers</h2><p id="41ba">Within the ACMA ‘ECS Explanatory Statement’ they set out a number of regulatory options in regards to the shutdown, including the blocking of working devices in Option 2.</p><p id="c2da">Within the paper they set out the expected costs to consumers.</p><figure><figcaption><strong>ACMA Assumes all ‘affected phones’ are Low End Phones, equivalent to $250 Samsung!</strong></figcaption></figure><p id="a3c0"><strong>It assumes that the average replacement cost for a consumer is only $250</strong>, and multiply over the total number of affected devices in use that consumers should be expected to bear in excess of $83 Million in costs.</p><figure><figcaption>ACMA Explanatory Statement registered 24/10/2024 to F2024L01353 | Page 37 <a href="https://www.legislation.gov.au/F2024L01353/asmade/downloads" rel="noopener ugc nofollow" target="_blank">https://www.legislation.gov.au/F2024L01353/asmade/downloads</a></figcaption></figure><p id="ea83">The overall proportion of costs for this approach will see 55% of the financial burden borne by consumers!</p><figure><figcaption>ACMA Explanatory Statement registered 24/10/2024 to F2024L01353 | Page 27 <a href="https://www.legislation.gov.au/F2024L01353/asmade/downloads" rel="noopener ugc nofollow" target="_blank">https://www.legislation.gov.au/F2024L01353/asmade/downloads</a></figcaption></figure><p id="296f">In my submissions to both the Optus Network Outage and the 3G Network Shutdown Senate Inquiries I warned that:</p><blockquote><p id="7631"><strong><em>Requiring Optus, Telstra or Vodafone VoLTE firmware on mobile devices provided by those companies will reduce choice for consumers in addition to allowing the major telcos to exert more control over the smartphone and telecommunications space.</em></strong></p><p id="1b17"><strong><em>Australian consumers should be free to purchase devices from any retailer, online store or international brand and have it work here in Australia, just as they have been able to do for decades with 2G &amp; 3G.</em></strong><em> <br>The consistent standardisation of 2G &amp; 3G has enabled seamless global connectivity and greatly enhanced competition in the mobile sector.</em></p><p id="e4b1"><strong><em>Currently providers are able to use this Telco Specific VoLTE firmware requirement as a stealth way of network locking their devices and preventing customers from switching to competitors or from using ‘BYO’ devices.</em></strong></p></blockquote><blockquote><p id="8e48">All of that has now occurred!</p></blockquote></div><div><div><h2 id="243a">Impacts on Tourists &amp; Roamers</h2><p id="033f">What people may not also been aware of is that the devices used by many International Tourists and Roamers <strong>were also entirely kicked off the networks of Telstra &amp; Optus on Monday Morning (28 Oct).</strong></p><p id="d93b">Even though the Direction from the Minister and Legislation by the ACMA states that Travellers<strong> should be exempt for up to 90 days</strong>.</p><figure><figcaption><strong>ACMA (Emergency Call Service Determination) Direction 2024 — Explanatory Statement | Page 2<br></strong>⁣<a href="https://www.legislation.gov.au/F2024L01103/asmade/text/explanatory-statement" rel="noopener ugc nofollow" target="_blank">https://www.legislation.gov.au/F2024L01103/asmade/text/explanatory-statement</a></figcaption></figure><p id="1fc3">The telcos in their Submission to the ACMA’s Open Consultation <strong>did warn</strong> they would struggle to exempt Roamers from the device blocking rules.</p><figure><figcaption><strong>Telstra — ECS Determination Public Submission — 9 October 2024 — Page 10 </strong><a href="https://www.acma.gov.au/consultations/2024-09/proposal-amend-ecs-determination" rel="noopener ugc nofollow" target="_blank">https://www.acma.gov.au/consultations/2024-09/proposal-amend-ecs-determination</a></figcaption></figure><p id="2958">However the ACMA and the Minister both seem to think that the rule of law can somehow overcome technological complexities and limitations.</p><p id="ba78"><em>A story we’ve heard before from many different Governments.</em></p><figure><figcaption><strong>Telstra | 3G network closure — Frequently asked questions - November 2024<br></strong><a href="https://www.telstra.com.au/support/mobiles-devices/3g-closure" rel="noopener ugc nofollow" target="_blank">https://www.telstra.com.au/support/mobiles-devices/3g-closure</a></figcaption></figure><p id="dac5">The failure to fully contextualise and understand the implications of this issue highlights the very clear lack of oversight there has been with this entire shutdown process.</p><p id="b068">For some context, in April 2019 the ACMA said there was<em> <br>‘no clear indication of migration paths towards the use of VoLTE handsets’</em>.</p><p id="77f0"><strong>Yet Telstra announced a 3G switch-off date later that same year for mid 2024.</strong></p><p id="a609"><a href="https://acma.gov.au/sites/default/files/2019-08/IFC-11-2019-Consultation-paper-Reconfiguring%20the%20900%20MHz%20band.docx" rel="noopener ugc nofollow" target="_blank"><em>ACMA — ‘Reconfiguring the 900 MHz band — Options paper’ — April 2019</em></a></p><p id="7e9d"><em>“One of the key issues for licensees is the </em><strong><em>uncertain timeline for the proliferation of Voice Over LTE</em></strong><em> (VoLTE)-enabled devices among consumers. The </em><strong><em>ACMA sought information from incumbent licensees</em></strong><em> on the expected timing and </em><strong><em>speed of consumer migration towards the use of VOLTE</em></strong><em> handsets, but </em><strong><em>still has no clear indication of intended migration paths</em></strong><em>. </em><strong><em>In the absence of receiving any further information</em></strong><em> to support a more detailed assessment, the </em><strong><em>ACMA considers that the proposed timeline outlined in this option </em></strong><em>(i.e. </em><strong><em>a mid-2024 clearance date</em></strong><em> for existing apparatus licences) </em><strong><em>provides enough opportunity for carriers to mitigate risks to the continuity of consumer services</em></strong><em>.”</em></p><p id="c1ea">Based on the above information it appears that little (if any) consideration was ever given to ensure handsets were compatible before allowing the providers to restructure any of the 3G Bands, let alone a complete switch-off of 3G.</p><p id="577c"><strong>When the ACMA made that assessment Android and iOS didn’t even have VoLTE Roaming Support.</strong><em> (Android 12–2021, iOS 15–2021)</em></p><p id="44a6">It’s clear the switch-off dates have been chosen not on specific compatibility or standardisation criteria, but rather based on commercial interests.</p><p id="6c3a"><em>You can learn more below.</em></p><p id="71a0"><strong>Now Tourists with blocked 4G &amp; 5G phones find themselves only able to connect to the Vodafone network. </strong><em>(Though some devices used by Roamers were blocked on Vodafone as well.)</em></p></div><div><figure></figure><figure><figcaption>Roaming Customers unable to Connect to Optus or Vodafone from Monday 28 October 2024</figcaption></figure></div><div><h2 id="2c1c">Issues with VoLTE &amp; Roaming</h2><p id="0b15"><strong>Worse still many lost the ability to make or receive calls via their Roaming Sim Card as VoLTE Roaming was only available on Optus and Telstra’s Networks.</strong></p><figure></figure><p id="dd6d">Seems there were some complaints after that happened, so now VoLTE Roaming is working on Vodafone AU.</p><p id="adb2">Furthermore to have a device with VoLTE Roaming Calling support, Optus advises you need a Samsung/Android Phone with Android 12 or above.</p><figure><figcaption><strong>‘Optus Device Compatibility &amp; VoLTE Roaming Activation’ — May 2024<br></strong><a href="https://www.optus.com.au/mobile/plans/international-roaming/volte" rel="noopener ugc nofollow" target="_blank">https://www.optus.com.au/mobile/plans/international-roaming/volte</a></figcaption></figure><figure><figcaption><strong>Android 13 distribution April 2023 — Source: Android Authority &amp; 9to5Google</strong> <a href="https://www.androidauthority.com/android-13-distribution-2023-3312803" rel="noopener ugc nofollow" target="_blank">https://www.androidauthority.com/android-13-distribution-2023-3312803</a> <a href="https://9to5google.com/2023/04/13/android-13-market-share-stats/2803" rel="noopener ugc nofollow" target="_blank">https://9to5google.com/2023/04/13/android-13-market-share-stats/2803</a></figcaption></figure><p id="08e1"><strong>However Android 12 was only released in late 2021</strong> so people who have Android phones older than 2020/2021 are likely to encounter issues with either switching providers, when travelling overseas or when making calls over 4G.</p><p id="7db5"><strong>Devices from Android version 4 to 11 (2020) make up 69% of the Global Android Device Market</strong> as of April 2023.</p><p id="56f9">4G/5G Devices without VoLTE calling support currently rely on the 2G/3G Networks to make and receive calls, this is via a technology called Circuit Switched Fallback (CSFB).</p><p id="361d">Even without the blocking, once 3G the shutdown is complete many 4G &amp; 5G phones used by Tourists will <strong>no longer be able to make calls or emergency calls in Australia </strong>either via Roaming or with a local sim card installed.</p><p id="fa35">Telstra &amp; the Telcos were asked at the 3G Senate Inquiry on 24 July:</p><p id="1e9d"><em>“How many roaming devices are reliant on the 3G network to make or receive roaming calls or Emergency Calls?”</em></p><p id="287f"><strong>They didn’t know that Answer and had to take it on Notice.</strong></p><p id="45c3">In the answers to Telstra’s Questions on Notice from the 3G Inquiry they said that in July there were <strong>2.3 million international roaming devices</strong> connected to their network and that<strong> Telstra ‘cannot confirm the 4G Voice calling capability of the devices’!</strong></p><figure><figcaption><strong>Telstra — Answers to questions taken on notice at a public hearing on 24 July 2024 (received 1 Aug 2024)</strong> <a href="https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Rural_and_Regional_Affairs_and_Transport/3GNetworkShutdown/Additional_Documents" rel="noopener ugc nofollow" target="_blank">https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Rural_and_Regional_Affairs_and_Transport/3GNetworkShutdown/Additional_Documents</a></figcaption></figure></div></div><div><h2 id="3d79">Industry Awareness of the Problem</h2><p id="e619">At the April 2022 European Emergency Number Association (EENA) Conference, Dutch Telecoms Consultant Rudolf van der Berg made a presentation to the conference outlining serious issues with the current 4G calling standards and protocols.</p><p id="9c88">One of the major issues he raised is that many 4G Phones (both European &amp; International) are completely unable to call (911/112) Emergency services without the presence of 2G/3G (Circuit Switched Calling) Networks.</p><p id="f110">These issues were noticed following on from the 3G network shutdowns that occurred in the US around that time and he warned this will become a widespread issue as 2G/3G networks are gradually shut down across the globe.</p><p id="77c5">The link to his presentation on the EENA YouTube page can be found below.</p><p id="fa3f"><em>Mr van der Berg is from the Dutch Telecommunications Consultancy firm Stratix, he was also an Economist/Policy Analyst at the OECD and has worked for many years in the Telecom Industry</em>.</p><figure><figcaption>YouTube -‘<a href="https://www.youtube.com/watch?v=sHjyLmFt-eg" rel="noopener ugc nofollow" target="_blank">EENA 2022: Access to emergency services is being impacted by the lack of VoLTE interoperability</a>’</figcaption></figure><p id="8958">These issues aren’t well known to the general public however in his EENA presentation he says these issues are <strong><em>“Common Knowledge”</em></strong> in the industry and that <strong><em>“..there is nobody who feels responsible to fix this”</em></strong>. <em>(16:45)</em></p><p id="2d26">On a slide in his presentation it says<strong><em> ‘telecom sector will deny </em></strong><em>[there’s a problem]</em><strong><em>, be angry over 5G investment and bargain for half baked measures..’</em></strong></p><figure><figcaption>‘Should we stop the shutdown of 2G/3G to save lives??’ - Slide 14 | Rudolf van der Berg — Stratix | EENA 2022<br><a href="https://drive.google.com/file/d/1WC16k8C1gpeFRJif23yDIuLSRg1OJOnZ/view" rel="noopener ugc nofollow" target="_blank">https://drive.google.com/file/d/1WC16k8C1gpeFRJif23yDIuLSRg1OJOnZ/view</a></figcaption></figure></div><div><h2 id="c7c4">Response from Industry Groups</h2><p id="422b">In response to his EENA 2022 presentation, the GSMA <em>(one of the world's largest industry groups for the Telecom Sector) </em>established a task force to try and address these issues.</p><p id="1786">The GSMA directly referenced the EENA video in a presentation about this problem to industry last year in 2023, a link to the slide deck from that presentation is below.</p><figure><figcaption>GSMA — ‘Additional Test Cases for IMS Emergency Calling’ — ITU-T Webinar — Wayne Cutler — 2023–06–21<br><a href="https://www.itu.int/cities/wp-content/uploads/2023/06/3_Wayne-Cuttler.pdf" rel="noopener ugc nofollow" target="_blank">https://www.itu.int/cities/wp-content/uploads/2023/06/3_Wayne-Cuttler.pdf</a></figcaption></figure><p id="fb2b">The aim of the taskforce is to improve the standards and advocate for device manufacturers, carriers and GSMA member organisations to align their devices and networks with a unified standard.</p><p id="6af9"><strong>It’s worth mentioning that Telstra, Optus and TPG/Vodafone are all members of the GSMA</strong>, so they cannot pretend they had no awareness of the of these issues prior to last year and the minimal progress made to fix the problem.</p><p id="daa0"><em>GSMA — ‘How we’re addressing VoLTE emergency call issues’ — 2023–05–12:<br></em><a href="https://www.gsma.com/services/blog/how-were-addressing-volte-emergency-call-issues/" rel="noopener ugc nofollow" target="_blank"><em>https://www.gsma.com/services/blog/how-were-addressing-volte-emergency-call-issues/</em></a></p><p id="db63"><strong>This work is still ongoing and it’s still very much NOT solved globally.</strong> <br><em>Major compatibility issues remain with many networks, device chipsets, handset models and different calling systems.</em></p></div><div><div><h2 id="2fef">Lack of Government Oversight with Vodafone</h2><p id="d2a6">Late January/Early February Vodafone customers started writing to the Minister complaining they lost call service on new VoLTE enabled 4G/5G Phones.</p><figure><figcaption>FOI 24–354 — Document 25</figcaption></figure><p id="7199"><strong>Initially these people were told their device ‘may be malfunctioning in some way’ and to contact the manufacturer or retailer.</strong></p><figure><figcaption>FOI 24–354 — Document 26</figcaption></figure><blockquote><p id="1e36"><a href="https://www.infrastructure.gov.au/sites/default/files/documents/foi-24-354--documents-for-release--pdf.pdf" rel="noopener ugc nofollow" target="_blank">FOI 24–354</a> | Pages 54 of 147 [PDF Page 42] &amp; 66 of 147 [PDF Page 54]</p></blockquote><p id="25f0">Furthermore, in emails released under FOI (in advance of the 3G inquiry), it was revealed that I was <strong>the only person in the entire country</strong> prior to February this year to warn the Minister and Government about the unintended consequences of switching off 3G and 4G devices being unable to make calls and Emergency Calls.</p><blockquote><p id="3c52">(My original 6 June email to the Minster released under FOI, starts page 4)<br><a href="https://www.infrastructure.gov.au/sites/default/files/documents/foi-24-354--documents-for-release--pdf.pdf" rel="noopener ugc nofollow" target="_blank">FOI 24–354 | Correspondence to, and responses from, the Minister for Communications relating to the shutdown of 3G networks between 27 March 2023 and 27 March 2024.</a></p></blockquote><p id="ba2b">On the 9th of February 2024 the Department of Infrastructure was talking to Optus regarding my 16 November ‘Optus Inquiry Submission’ which is the same day the Optus hearing was set to sit again. <br><em>(The hearing was cancelled on 8 Feb due to a reported Illness with the Chair)</em></p></div><div><blockquote><p id="e779"><em>Seems the Department wasn’t happy with the answers from Optus and the new complaints from Vodafone customers.</em></p></blockquote><p id="708c"><strong>Then on 26 February </strong>the providers briefed the Government about the issue of 4G VoLTE enabled phones being unable to call 000.</p><figure><figcaption><a href="https://www.infrastructure.gov.au/sites/default/files/documents/foi-24-353--documents-for-release--pdf.pdf" rel="noopener ugc nofollow" target="_blank">FOI 24–353</a> — Document 6 [FOI 24–353 — Senator Rennick 20240724] — Page 55 of 75 [PDF Page 50]<br><a href="https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Rural_and_Regional_Affairs_and_Transport/3GNetworkShutdown/Additional_Document" rel="noopener ugc nofollow" target="_blank">https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Rural_and_Regional_Affairs_and_Transport/3GNetworkShutdown/Additional_Documents</a></figcaption></figure><p id="df55">A few weeks later in March, the Minister held a press conference and announced 740,000 4G VoLTE capable phones wouldn’t be able to call 000 post shutdown. <strong>(Again that didn’t include all of the 4G/5G phones that wouldn’t be able to make any calls)</strong></p><figure><figcaption>YouTube — ‘3G Switch-Off Could Stop Triple-Zero Calls — 10 News First’ — 17 March 2024 (Quote at 1:45)<br><a href="https://www.youtube.com/watch?v=iK98wI6d-W0" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=iK98wI6d-W0</a></figcaption></figure><p id="9e12">After that press conference the messaging from the Department would change and included ‘categories’ of affected devices.</p><figure><figcaption>MC24–001972</figcaption></figure></div></div><div><h2 id="4f4e">TPG/Vodafone’s awareness of the 4G Emergency Calling Issue</h2><p id="6208"><strong>Sometime between late July and August 2023</strong> Vodafone <strong>removed </strong>a number devices from the ‘Supported VoLTE Devices’ page on the Vodafone Website. Many of these devices are able to make calls on 4G (with VoLTE) on Vodafone<strong> but not to Emergency numbers.</strong></p><p id="b152">I first noticed a number of devices were missing from the website on <strong>31 August 2023</strong> when registering the sim card I purchased for device testing. <em>(The removed devices were still listed on 10 July 2023).</em></p><p id="15cf">I had suspicions a number of those devices likely had compatibility issues given when they were released.</p><blockquote><p id="0c73"><em>Little did I realise at the time that I just witnessed what appeared to be Vodafone hiding the problem from the public.</em></p></blockquote><p id="3e94">Vodafone<strong> was not</strong> required to provide any formal quarterly reporting to the Department and Government, so it does bring into question their awareness of the scale of the compatibility issues last year, and the reasons why these devices were removed at the time without any alarms being raised for customers.</p><p id="8878">Additionally a number of the devices they removed from their list are still in some cases on Telstra’s and Optus’s Support List, which means many of these potentially affected devices may still in use and circulation.</p><p id="1e74"><em>In some instances the Telstra and Optus Firmware versions of those devices support Emergency Calling over 4G but not necessarily VoLTE Calling on all networks.</em></p><p id="0d54">A summary of the removed devices are below.<br><em>Archived copies of the Vodafone website can be found on The Internet Archive ‘Wayback Machine’.</em></p><p id="7971"><strong>Devices Removed from Vodafone’s Website between July-August 2023</strong></p><figure></figure><p id="e71b"><em>Further note, some of the above devices will have working calling and emergency calling when used with either Telstra or Optus Network Sim Cards but when a Vodafone Sim Card is inserted the phone may continue to make calls </em><strong><em>but will lose the ability to make 4G Emergency Calls.</em></strong></p><p id="f256"><a href="https://web.archive.org/web/20230606093226/http://www.vodafone.com.au/support/network/volte" rel="noopener ugc nofollow" target="_blank"><em>Internet Archive — Wayback Machine — ‘Vodafone AU VoLTE Device Support List’ — Before Removal — 2023–06–06</em></a></p><p id="1a90"><a href="https://web.archive.org/web/20231025002229/https://www.vodafone.com.au/support/network/volte" rel="noopener ugc nofollow" target="_blank"><em>Internet Archive — Wayback Machine — ‘Vodafone AU VoLTE Device Support List’ — After Removal — 2023–10–25</em></a></p><p id="7c92"><em>Learn more below</em></p><p id="b6da"><a rel="noopener" href="https://medium.com/@jamesdwho/australias-3g-switch-off-failures-of-government-industry-to-prepare-b621f90f7950#a6ff"><em>Vodafone’s awareness of the 4G Emergency Calling Issue</em></a></p></div><div><h2 id="285d">Device Survey &amp; Results</h2><p id="1ef5">Post shutdown I launched a Google Forms Survey in order to gather information in regards to the devices impacted, the messaging received by customers and the overall impacts shutdown and blocking of devices was having on customers.</p><figure><figcaption><strong>Australia’s 3G Shutdown — 4G/5G Device Blocking &amp; Capabilities Submission Form</strong> <a href="https://docs.google.com/forms/d/1TnX_McW4uMMrb8iu1GQCzthhq1gIs0x34cddMzhluKY/viewform" rel="noopener ugc nofollow" target="_blank">https://docs.google.com/forms/d/1TnX_McW4uMMrb8iu1GQCzthhq1gIs0x34cddMzhluKY/viewform</a></figcaption></figure><p id="34ee">The numbers paint a very stark picture and reveals how badly managed the communication for this process has been. Including that by the traditional major TV &amp; News Media Networks.</p><p id="d674">Within the survey results it’s very clear that the the vast majority cannot afford to buy a new phone.</p><blockquote><p id="2ae9">Of the over 300 Surveyed, 83% of people said purchasing another suitable device would have either a Moderate or Major Financial impact!</p></blockquote><p id="c3c6"><strong>Over 75% of respondents also say they’ve not been offered a replacement device by their provider!</strong> <em>(Let alone one fit-for-purpose or like-for-like!)</em></p></div></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IMG_0416 (1394 pts)]]></title>
            <link>https://ben-mini.github.io/2024/img-0416</link>
            <guid>42102506</guid>
            <pubDate>Sun, 10 Nov 2024 20:45:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ben-mini.github.io/2024/img-0416">https://ben-mini.github.io/2024/img-0416</a>, See on <a href="https://news.ycombinator.com/item?id=42102506">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
        <header>
          
            
            
            

  <p><time datetime="2024-11-03T00:00:00+00:00">November 3, 2024</time></p>

 <!-- Moved the date here -->
          
          


        </header>
      

      <hr>

      <section itemprop="text">
        
        <p>Between 2009 and 2012, Apple iPhones and iPod Touches included a feature called “Send to YouTube” that allowed users to upload videos directly to YouTube from the Photos app.</p>

<p><img src="https://ben-mini.github.io/assets/images/iphone-youtube.jpg" alt="iphone-youtube"></p>

<p>The feature worked… really well. In fact, <a href="https://www.macrumors.com/2009/06/25/youtube-daily-mobile-uploads-have-increased-400-since-launch-of-iphone-3gs/">YouTube reported a 1700% increase in total video uploads</a> during the first half of 2009- crediting that growth to its strong integrative ties to Apple and social networks. However, this two-click upload feature was short-lived when Apple severed ties Apple severed ties with YouTube by <a href="https://archive.nytimes.com/bits.blogs.nytimes.com/2012/08/06/apple-to-remove-youtube-app-from-iphone-and-ipad/">removing its homegrown app in 2012</a>.</p>

<p>While Send to YouTube can be thoroughly analyzed as a milestone on the “frenemy” timeline between Apple and Google, I want to explore a pleasant consequence of this moment. Apple uses the ‘IMG_XXXX’ naming convention for all images and videos captured on iOS devices, where XXXX is a unique sequence number. The first image you take is named “IMG_0001”, the second is “IMG_0002” and so on. During the Send to YouTube era of 2009 and 2012, the title of one’s YouTube video was defaulted to this naming convention. Unwitting content creators would then upload their videos on a public site with a barely-searchable name. To this day, <strong><em>there are millions of these videos.</em></strong></p>

<p><img src="https://ben-mini.github.io/assets/images/img-yt-search.png" alt="Screenshot 2024-11-03 at 10.37.58 AM"></p>

<p>Try searching for “IMG_XXXX” on YouTube, replacing “XXXX” with your favorite numbers (I used my birthday, 0416). See what you get!</p>

<p>There’s something surreal about these videos that engages you in a way you’ve never felt. None were edited, produced or paraded for mass viewing. In fact, many were likely uploaded by accident or with a misunderstanding that complete strangers could see it. YouTube automatically removes harmful or violent content, so what remains exists in a unique, almost paradoxical state: <em>forbidden, yet harmless.</em> Putting all this together, searching IMG_XXXX presents the viewer with the most authentic social feed ever seen on the Internet- in video, no less!</p>

<p>While many videos are redundant snippets of a concerts, basketball games, or kids’ recitals, you also get one-of-a-kind videos that provides a glimpse into a complete stranger’s life. You’ll see a tumultuous event that made them, their partner, or their friend say, “hey, let’s record this”. I’d like to show you three of these videos that I found in my search.</p>

<h3 id="img_0416-mar-17-2015---23-views-"><a href="https://www.youtube.com/watch?v=MR3mv5SbAi4">IMG_0416 (Mar 17, 2015) - 23 views </a></h3>

<!-- Courtesy of embedresponsively.com //-->

<p>
    <iframe src="https://www.youtube-nocookie.com/embed/MR3mv5SbAi4" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </p>

<p>The video shows a woman excitedly unboxing a book she received in the mail. From context clues, she seems to be a wife and mother from Memphis who’s unboxing the first published copy of her book. She thanks the friends, family, and publishers who made this happen.</p>

<p>After a quick Google Search, I was able to find the book: <u>*A Profit / Prophet to Her Husband: Are you ready to be a wife?*</u> The book is meant “to help wives understand who they are and who they were designed to be.” It clocks in at 94 pages and has 30 ratings on Amazon!  Go IMG_0416! I don’t care what you’re creating- <a href="https://ben-mini.github.io/2023/the-meaning-of-life">I’m just a fan of creators</a>. It looks like she kept at it- <a href="https://www.amazon.com/Secret-Loving-Yourself-Unconditionally-Self-Worth-ebook/dp/B086PVGPHZ?ref_=ast_author_dp">making a second book in 2020</a>!</p>

<h3 id="img_0416mov-june-24-2015---26-views"><a href="https://www.youtube.com/watch?v=JDuGXBteSno">IMG_0416.MOV (June 24, 2015) - 26 views</a></h3>

<!-- Courtesy of embedresponsively.com //-->

<p>
    <iframe src="https://www.youtube-nocookie.com/embed/JDuGXBteSno" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </p>

<p>The video appears to show a woman playing a matching card game that teaches you “the basic of the potash stuff” according to the cameraman. As the woman (who I assume is the cameraman’s supportive mother) flips two matching cards, she reads off the countries who produce the most <a href="https://en.wikipedia.org/wiki/Potash">potash</a>.</p>

<p>I honestly didn’t know anything about potash! Turns out that it is a mineral with large amount of potassium, which is helpful as a plant fertilizer. With Canada producing the largest reserves in the world, <a href="https://arc.net/l/quote/fdbongnj">the vast majority of Canadian potash is found in Saskatchewan</a>. I wonder if the family in the video  lives in Canada. Or, this is just another school project that teaches you random facts… I miss those!</p>

<h3 id="img_0416-feb-8-2011---114-views"><a href="https://www.youtube.com/shorts/HXBKniNtsHk">IMG_0416 (Feb 8, 2011) - 114 views</a></h3>

<!-- Courtesy of embedresponsively.com //-->

<p>
    <iframe src="https://www.youtube-nocookie.com/embed/HXBKniNtsHk" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </p>

<p>Let’s end of a fun one. The video shows a young man snorting powdered sugar and dealing with the consequences of it. Given his BU hoodie, Dunkin’ Donuts location, and ironic depiction of drug use, I gotta say this is a <strong>VERY</strong> Boston video.</p>

<p>What’s genuinely heartwarming is the shared laughter between the man, the camerawoman, and a motherly figure leaving Dunkin’. The camerawoman calls her “Myra” at the end, suggesting they all know each other. This mix of community at franchised restaurant, and the remark about having “nothing better to do” perfectly captures the heartbeat of American suburbia.</p>

        
      </section>

      

      

      
  

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MdBook – a command line tool to create books with Markdown (161 pts)]]></title>
            <link>https://rust-lang.github.io/mdBook/</link>
            <guid>42102262</guid>
            <pubDate>Sun, 10 Nov 2024 20:00:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rust-lang.github.io/mdBook/">https://rust-lang.github.io/mdBook/</a>, See on <a href="https://news.ycombinator.com/item?id=42102262">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox></mdbook-sidebar-scrollbox>
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                <div id="menu-bar">
                    

                    <h2>mdBook Documentation</h2>

                    
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        <h2 id="introduction"><a href="#introduction">Introduction</a></h2>
<p><strong>mdBook</strong> is a command line tool to create books with Markdown.
It is ideal for creating product or API documentation, tutorials, course materials or anything that requires a clean,
easily navigable and customizable presentation.</p>
<ul>
<li>Lightweight <a href="https://rust-lang.github.io/mdBook/format/markdown.html">Markdown</a> syntax helps you focus more on your content</li>
<li>Integrated <a href="https://rust-lang.github.io/mdBook/guide/reading.html#search">search</a> support</li>
<li>Color <a href="https://rust-lang.github.io/mdBook/format/theme/syntax-highlighting.html">syntax highlighting</a> for code blocks for many different languages</li>
<li><a href="https://rust-lang.github.io/mdBook/format/theme/index.html">Theme</a> files allow customizing the formatting of the output</li>
<li><a href="https://rust-lang.github.io/mdBook/format/configuration/preprocessors.html">Preprocessors</a> can provide extensions for custom syntax and modifying content</li>
<li><a href="https://rust-lang.github.io/mdBook/format/configuration/renderers.html">Backends</a> can render the output to multiple formats</li>
<li>Written in <a href="https://www.rust-lang.org/">Rust</a> for speed, safety, and simplicity</li>
<li>Automated testing of <a href="https://rust-lang.github.io/mdBook/cli/test.html">Rust code samples</a></li>
</ul>
<p>This guide is an example of what mdBook produces.
mdBook is used by the Rust programming language project, and <a href="https://doc.rust-lang.org/book/">The Rust Programming Language</a> book is another fine example of mdBook in action.</p>
<h2 id="contributing"><a href="#contributing">Contributing</a></h2>
<p>mdBook is free and open source. You can find the source code on
<a href="https://github.com/rust-lang/mdBook">GitHub</a> and issues and feature requests can be posted on
the <a href="https://github.com/rust-lang/mdBook/issues">GitHub issue tracker</a>. mdBook relies on the community to fix bugs and
add features: if you’d like to contribute, please read
the <a href="https://github.com/rust-lang/mdBook/blob/master/CONTRIBUTING.md">CONTRIBUTING</a> guide and consider opening
a <a href="https://github.com/rust-lang/mdBook/pulls">pull request</a>.</p>
<h2 id="license"><a href="#license">License</a></h2>
<p>The mdBook source and documentation are released under
the <a href="https://www.mozilla.org/MPL/2.0/">Mozilla Public License v2.0</a>.</p>

                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->

                            <a rel="next prefetch" href="https://rust-lang.github.io/mdBook/guide/installation.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">

                    <a rel="next prefetch" href="https://rust-lang.github.io/mdBook/guide/installation.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div>



        

        

        
        
        
        
        

        
        
        

        
        
        

        <!-- Custom JS scripts -->


    </div>
    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Is 3D printing being held back by an invalid patent? (156 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42102235</link>
            <guid>42102235</guid>
            <pubDate>Sun, 10 Nov 2024 19:56:00 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42102235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="42102235">
      <td><span></span></td>      <td><center><a id="up_42102235" href="https://news.ycombinator.com/vote?id=42102235&amp;how=up&amp;goto=item%3Fid%3D42102235"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=42102235">Is 3D printing being held back by an invalid patent?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_42102235">111 points</span> by <a href="https://news.ycombinator.com/user?id=K0balt">K0balt</a> <span title="2024-11-10T19:56:00 1731268560"><a href="https://news.ycombinator.com/item?id=42102235">5 hours ago</a></span> <span id="unv_42102235"></span> | <a href="https://news.ycombinator.com/hide?id=42102235&amp;goto=item%3Fid%3D42102235">hide</a> | <a href="https://hn.algolia.com/?query=Is%203D%20printing%20being%20held%20back%20by%20an%20invalid%20patent%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=42102235&amp;auth=0ea42a9b3ab20b58863a0176a86ab02cf1a396a1">favorite</a> | <a href="https://news.ycombinator.com/item?id=42102235">18&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Parts made using the FFM (FDM) process are stronger in some directions than in others. This is due to imperfect adhesion between printed layers. This directional or anisotropic structural strength is a significant limiting factor for the structural integrity of 3d printed parts made with the common FFM process.</p><p>A method to drastically reduce this effect is described in an expired Stratasys patent,  US1997/5653925A. As of this writing, I have not seen this feature implemented in any of the common open-source slicers. I would have expected this now-public-domain knowledge to have made its way into the slicers we use since the patent expired in 2017.</p><p>With some investigation, I believe this may be because of the 2023 patent US2023/11813789B2, to my knowledge only implemented as a proprietary in-house product of a 3d printing service house which appears to make the same claims illustrated in the 1997 Stratasys patent.</p><p>I am not expertly versed in patent law, but this patent would seem to have been granted in error. The fact that the 2023 patent actually references the Stratasys patent from 1997* makes the issuance of this patent even more baffling. At any rate, this seems like a low-hanging opportunity for a significant improvement in print strength that could benefit millions of users.</p><p>*The 2023 patent references the 1997 Stratasys patent as “teaching a process for adjusting the deposition rate  .. to provide a predetermined porosity rate” but neglects to mention that the core claim of the patent was the staggered bead heights claimed.</p><p>In referencing the 1997 patent the 2023 work also gives a single-digit error in the patent number, as well as other “minor” errors in other patent numbers referenced in the patent. These misleading references could have impeded the examiner’s role in determining the validity of the patent.</p><p>https://patentimages.storage.googleapis.com/08/f6/2b/d3d996474f4de2/US11813789.pdf</p><p>https://patentimages.storage.googleapis.com/ea/e7/3c/2b836c9a51e18b/US5653925.pdf</p><p>Looking up the 2023 patent filer leads to this:</p><p>https://www.addmangroup.com/</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pi Chess Board (214 pts)]]></title>
            <link>https://readymag.website/u2481798807/5057562/</link>
            <guid>42101742</guid>
            <pubDate>Sun, 10 Nov 2024 18:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://readymag.website/u2481798807/5057562/">https://readymag.website/u2481798807/5057562/</a>, See on <a href="https://news.ycombinator.com/item?id=42101742">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Algorithms We Develop Software By (188 pts)]]></title>
            <link>https://grantslatton.com/software-pathfinding#algorithms-we-develop-software-by</link>
            <guid>42101729</guid>
            <pubDate>Sun, 10 Nov 2024 18:37:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grantslatton.com/software-pathfinding#algorithms-we-develop-software-by">https://grantslatton.com/software-pathfinding#algorithms-we-develop-software-by</a>, See on <a href="https://news.ycombinator.com/item?id=42101729">Hacker News</a></p>
<div id="readability-page-1" class="page"><header>
        <nav>
            <a href="https://grantslatton.com/">Home</a>
        </nav>
    </header>
    
        
        
        
<p>I recently had a conversation with a distinguished tech CEO and engineer. I loved hearing his description of a software development methodology he's occasionally used, and it got me thinking about other heuristics and generalizations.</p>
<h2 id="his-method"><a href="#his-method">His method</a></h2>
<p>Start working on the feature at the beginning of the day. If you don't finish by the end of the day, delete it all and start over the next day. You're allowed to keep unit tests you wrote.</p>
<p>If, after a few days, you can't actually implement the feature, think of what groundwork, infrastructure, or refactoring would need to be done to enable it. Use this method to implement <em>that</em>, then come back to the feature.</p>
<p>He said he didn't invent this, but it was something adjacent to the <a href="https://en.wikipedia.org/wiki/Extreme_programming">Extreme Programming</a> movement of the late 90s and early 00s.</p>
<h2 id="some-thoughts-on-the-method"><a href="#some-thoughts-on-the-method">Some thoughts on the method</a></h2>
<h3 id="write-everything-twice"><a href="#write-everything-twice">"Write everything twice"</a></h3>
<p>A piece of advice I've given junior engineers is to write everything twice. Solve the problem. Stash your code onto a branch. Then write all the code again.</p>
<p>I discovered this method by accident after the laptop containing a few days of work died. Rewriting the solution only took 25% the time as the initial implementation, and the result was <em>much better</em>.</p>
<p>So you get maybe 2x higher quality code for 1.25x the time — this trade is usually a good one to make on projects you'll have to maintain for a long time.</p>
<p>N.B. Obviously, don't write <em>literally everything</em> twice. It's a heuristic. Apply intelligently.</p>
<p>The "start over each day" method is an even more extreme version of this. Every time you rewrite, you carve a smoother path to the solution. The final solution can be really, really clean.</p>
<h3 id="quantity-has-a-quality-all-of-its-own"><a href="#quantity-has-a-quality-all-of-its-own">"Quantity has a quality all of its own"</a></h3>
<p>Almost certainly apocryphal Stalin quote is applicable to becoming a good software engineer. As a junior engineer, there's simply no substitute for getting the first 100K lines of code under your belt. The "start over each day" method will help get you to those 100K lines faster.</p>
<p>You might think covering the same ground multiple times isn't as valuable as getting 100K diverse lines of code. I disagree. Solving the same problem repeatedly is actually really beneficial for <em>retaining</em> knowledge of patterns you figure out.</p>
<p>You only need 5K perfect lines to see all the major patterns once. The other 95K lines are repetition to rewire your neurons.</p>
<h3 id="comparison-to-the-gun-to-the-head-heuristic"><a href="#comparison-to-the-gun-to-the-head-heuristic">Comparison to the "gun to the head" heuristic</a></h3>
<p>Another heuristic I've used is to ask someone to come up with a solution to a problem. Maybe they say it'll take 4 weeks to implement. Then I say "gun to your head, you have to finish in 24 hours, what do you do?"</p>
<p>The purpose here is to break their frame and their anchoring bias. If you've just said something will take a month, doing it in a day must require a radically different solution.</p>
<p>The surprising thing about this technique is <em>how often it works</em>. How often someone — minutes after presenting their month-long plan — can be induced to figure out a plan that could potentially be done in a day.</p>
<p>In practice, none of the day-long plans are actually a day. The gun isn't actually to your head. You can go home and sleep. But the new solution can often actually be done in just a few days. A ten-minute thought experiment becomes a 10x time saving.</p>
<p>The purpose of the thought experiment isn't to generate the <em>real</em> solution. It's meant to put a lower bound on the solution. Then you think of a <em>real</em> solution with that lower bound in eyesight, and you'll find it's often better than your original solution.</p>
<h2 id="pathfinding"><a href="#pathfinding">Pathfinding</a></h2>
<p>The core of the matter here is pathfinding in problem space. Each path is a solution, and it's the engineer's job to find the best one.</p>
<p>There are a lot of kind of sketchy analogies to be drawn between these heuristics and different pathfinding algorithms. There's some relation to <a href="https://en.wikipedia.org/wiki/Iterative_deepening_A%2A">iterative deepening</a>, <a href="https://en.wikipedia.org/wiki/A*_search_algorithm#Bounded_relaxation">bounded relaxation A*</a>, <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a>, <a href="https://en.wikipedia.org/wiki/Simulated_annealing">simulated annealing</a>, and others.</p>
<p>I don't think there's <em>too</em> much to be learned by trying to concretize that analogy, but it's valuable to think about it conceptually. All the different search algorithms have different pros and cons, depending on your constraints and knowledge of the domain.</p>
<p>So too with the engineering heuristics. Becoming a better engineer is becoming a better pathfinder in problem space.</p>
<p>There's probably a compelling general theory to be concocted in this space, but that's beyond the scope of this post. Spin up a background thread in your brain and think about it. Maybe you'll find a good path to an answer.</p>

    
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Web Locks API (190 pts)]]></title>
            <link>https://developer.mozilla.org/en-US/docs/Web/API/Web_Locks_API</link>
            <guid>42101434</guid>
            <pubDate>Sun, 10 Nov 2024 17:46:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Locks_API">https://developer.mozilla.org/en-US/docs/Web/API/Web_Locks_API</a>, See on <a href="https://news.ycombinator.com/item?id=42101434">Hacker News</a></p>
<div id="readability-page-1" class="page"><article lang="en-US"><header></header><div><p><strong>Secure context:</strong> This feature is available only in <a href="https://developer.mozilla.org/en-US/docs/Web/Security/Secure_Contexts">secure contexts</a> (HTTPS), in some or all <a href="#browser_compatibility">supporting browsers</a>.</p> <p><strong>Note:</strong> This feature is available in <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API">Web Workers</a>.</p>
<p>The <strong>Web Locks API</strong> allows scripts running in one tab or worker to asynchronously acquire a lock, hold it while work is performed, then release it. While held, no other script executing in the same origin can acquire the same lock, which allows a web app running in multiple tabs or workers to coordinate work and the use of resources.</p></div><section aria-labelledby="concepts_and_usage"><h2 id="concepts_and_usage"><a href="#concepts_and_usage">Concepts and Usage</a></h2><div><p>A lock is an abstract concept representing some potentially shared resource, identified by a name chosen by the web app. For example, if a web app running in multiple tabs wants to ensure that only one tab is syncing data between the network and Indexed DB, each tab could try to acquire a "my_net_db_sync" lock, but only one tab will succeed (the <a href="https://en.wikipedia.org/wiki/Leader_election" target="_blank">leader election pattern</a>.)</p>
<p>The API is used as follows:</p>
<ol>
  <li>The lock is requested.</li>
  <li>Work is done while holding the lock in an asynchronous task.</li>
  <li>The lock is automatically released when the task completes.</li>
</ol>
<div><pre><code>navigator.locks.request("my_resource", async (lock) =&gt; {
  // The lock has been acquired.
  await do_something();
  await do_something_else();
  // Now the lock will be released.
});
</code></pre></div>
<p>While a lock is held, requests for the same lock from this execution context, or from other tabs/workers, will be queued. The first queued request will be granted only when the lock is released.</p>
<p>The API provides optional functionality that may be used as needed, including:</p>
<ul>
  <li>returning values from the asynchronous task</li>
  <li>shared and exclusive lock modes</li>
  <li>conditional acquisition</li>
  <li>diagnostics to query the state of locks in an origin</li>
  <li>an escape hatch to protect against deadlocks</li>
</ul>
<p>Locks are scoped to origins; the locks acquired by a tab from <code>https://example.com</code> have no effect on the locks acquired by a tab from <code>https://example.org:8080</code> as they are separate origins.</p>
<p>The main entry point is <a href="https://developer.mozilla.org/en-US/docs/Web/API/LockManager/request" title="navigator.locks.request()"><code>navigator.locks.request()</code></a> which requests a lock. It takes a lock name, an optional set of options, and a callback. The callback is invoked when the lock is granted. The lock is automatically released when the callback returns, so usually the callback is an <em>async function</em>, which causes the lock to be released only when the async function has completely finished.</p>
<p>
  The <code>request()</code> method itself returns a promise which resolves once the lock has been released;
  within an async function, a script can <code>await</code> the call to make the asynchronous code flow linearly.
  For example:
</p>
<div><pre><code>await do_something_without_lock();

// Request the lock.
await navigator.locks.request("my_resource", async (lock) =&gt; {
  // The lock has been acquired.
  await do_something_with_lock();
  await do_something_else_with_lock();
  // Now the lock will be released.
});
// The lock has been released.

await do_something_else_without_lock();
</code></pre></div></div></section><section aria-labelledby="options"><h3 id="options"><a href="#options">Options</a></h3><div><p>Several options can be passed when requesting a lock:</p>
<ul>
  <li><code>mode</code>: The default mode is "exclusive", but "shared" can be specified. There can be only one "exclusive" holder of a lock, but multiple "shared" requests can be granted at the same time. This can be used to implement the <a href="https://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock" target="_blank">readers-writer pattern</a>.</li>
  <li><code>ifAvailable</code>: If specified, the lock request will fail if the lock cannot be granted immediately without waiting. The callback is invoked with <code>null</code>.</li>
  <li><code>steal</code>: If specified, then any held locks with the same name will be released, and the request will be granted, preempting any queued requests for it.</li>
  <li><code>signal</code>: An <a href="https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal"><code>AbortSignal</code></a> can be passed in, allowing a lock request to be aborted. This can be used to implement a timeout on requests.</li>
</ul></div></section><section aria-labelledby="monitoring"><h3 id="monitoring"><a href="#monitoring">Monitoring</a></h3><p>The <a href="https://developer.mozilla.org/en-US/docs/Web/API/LockManager/query" title="navigator.locks.query()"><code>navigator.locks.query()</code></a> method can be used by scripts to introspect the state of the lock manager for the origin. This can be useful when debugging, for example, identifying why a lock could not be acquired. The results are a snapshot of the lock manager state, which identifies held and requested locks and some additional data (e.g. mode) about each, at the time the snapshot was taken.</p></section><section aria-labelledby="advanced_use"><h3 id="advanced_use"><a href="#advanced_use">Advanced use</a></h3><div><p>For more complicated cases, such as holding the lock for an arbitrary amount of time, the callback can return a promise explicitly resolved by the script:</p>
<div><pre><code>// Capture promise control functions:
let resolve, reject;
const p = new Promise((res, rej) =&gt; {
  resolve = res;
  reject = rej;
});

// Request the lock:
navigator.locks.request(
  "my_resource",
  // Lock is acquired.
  (lock) =&gt; p, // Now lock will be held until either resolve() or reject() is called.
);
</code></pre></div></div></section><section aria-labelledby="deadlocks"><h3 id="deadlocks"><a href="#deadlocks">Deadlocks</a></h3><p>A deadlock occurs when a process can no longer make progress because each part is waiting on a request that cannot be satisfied. This can occur with this API in complex use-cases, for example, if multiple locks are requested out-of-order. If tab 1 holds lock A and tab 2 holds lock B, then tab 1 attempts to also acquire lock B and tab 2 attempts to also acquire lock A, neither request can be granted. Web applications can avoid this through several strategies, such as ensuring lock requests are not nested, or are always well ordered, or have timeouts. Note that such deadlocks only affect the locks themselves and code depending on them; the browser, other tabs, and other script in the page is not affected.</p></section><section aria-labelledby="interfaces"><h2 id="interfaces"><a href="#interfaces">Interfaces</a></h2><div><dl>
  <dt id="lock"><a href="https://developer.mozilla.org/en-US/docs/Web/API/Lock"><code>Lock</code></a></dt>
  <dd>
    <p>Provides the name and mode of a previously requested lock, which is received in the callback to <a href="https://developer.mozilla.org/en-US/docs/Web/API/LockManager/request"><code>LockManager.request()</code></a>.</p>
  </dd>
  <dt id="lockmanager"><a href="https://developer.mozilla.org/en-US/docs/Web/API/LockManager"><code>LockManager</code></a></dt>
  <dd>
    <p>Provides methods for requesting a new <a href="https://developer.mozilla.org/en-US/docs/Web/API/Lock"><code>Lock</code></a> object and querying for an existing <a href="https://developer.mozilla.org/en-US/docs/Web/API/Lock"><code>Lock</code></a> object. To get an instance of <a href="https://developer.mozilla.org/en-US/docs/Web/API/LockManager"><code>LockManager</code></a>, call <a href="https://developer.mozilla.org/en-US/docs/Web/API/Navigator/locks"><code>navigator.locks</code></a>.</p>
  </dd>
</dl></div></section><section aria-labelledby="extensions_to_other_interfaces"><h3 id="extensions_to_other_interfaces"><a href="#extensions_to_other_interfaces">Extensions to other interfaces</a></h3><div><dl>
  <dt id="navigator.locks"><a href="https://developer.mozilla.org/en-US/docs/Web/API/Navigator/locks"><code>Navigator.locks</code></a> <span title="This value may not be changed.">Read only </span></dt>
  <dd>
    <p>Returns a <a href="https://developer.mozilla.org/en-US/docs/Web/API/LockManager"><code>LockManager</code></a> object that provides methods for requesting a new <a href="https://developer.mozilla.org/en-US/docs/Web/API/Lock"><code>Lock</code></a> object and querying for an existing <a href="https://developer.mozilla.org/en-US/docs/Web/API/Lock"><code>Lock</code></a> object.</p>
  </dd>
  <dt id="workernavigator.locks"><a href="https://developer.mozilla.org/en-US/docs/Web/API/WorkerNavigator/locks"><code>WorkerNavigator.locks</code></a> <span title="This value may not be changed.">Read only </span></dt>
  <dd>
    <p>Returns a <a href="https://developer.mozilla.org/en-US/docs/Web/API/LockManager"><code>LockManager</code></a> object which provides methods for requesting a new <a href="https://developer.mozilla.org/en-US/docs/Web/API/Lock"><code>Lock</code></a> object and querying for an existing <a href="https://developer.mozilla.org/en-US/docs/Web/API/Lock"><code>Lock</code></a> object.</p>
  </dd>
</dl></div></section><h2 id="specifications"><a href="#specifications">Specifications</a></h2><table><thead><tr><th scope="col">Specification</th></tr></thead><tbody><tr><td><a href="https://w3c.github.io/web-locks/">Web Locks API<!-- --> <br></a></td></tr></tbody></table><section aria-labelledby="browser_compatibility"><h2 id="browser_compatibility"><a href="#browser_compatibility">Browser compatibility</a></h2></section><h3 id="api.lockmanager"><a href="#api.lockmanager">api.LockManager</a></h3><p>BCD tables only load in the browser</p><h3 id="api.lock"><a href="#api.lock">api.Lock</a></h3><p>BCD tables only load in the browser</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Procrastination and the fear of not being good enough (480 pts)]]></title>
            <link>https://swapnilchauhan.com/blog/procrastination-and-the-fear-of-not-being-good-enough</link>
            <guid>42101327</guid>
            <pubDate>Sun, 10 Nov 2024 17:23:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swapnilchauhan.com/blog/procrastination-and-the-fear-of-not-being-good-enough">https://swapnilchauhan.com/blog/procrastination-and-the-fear-of-not-being-good-enough</a>, See on <a href="https://news.ycombinator.com/item?id=42101327">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[What's New in F# 9 (245 pts)]]></title>
            <link>https://learn.microsoft.com/en-us/dotnet/fsharp/whats-new/fsharp-9</link>
            <guid>42101312</guid>
            <pubDate>Sun, 10 Nov 2024 17:20:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/whats-new/fsharp-9">https://learn.microsoft.com/en-us/dotnet/fsharp/whats-new/fsharp-9</a>, See on <a href="https://news.ycombinator.com/item?id=42101312">Hacker News</a></p>
<div id="readability-page-1" class="page">
	<div>
		<a href="#main" tabindex="1">Skip to main content</a>

		<div id="unsupported-browser" hidden="">
				<p>This browser is no longer supported.</p>
				<p>Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.</p>
				
			</div>
		<!-- liquid-tag banners global -->

		<!-- site header -->
		<header id="ms--site-header" data-test-id="site-header-wrapper" role="banner" itemscope="itemscope" itemtype="http://schema.org/Organization">
			
			
			
		</header>
	</div>

	<div data-bi-name="body"><div id="main-column">

						<main id="main" role="main" data-bi-name="content" lang="en-us" dir="ltr"><!-- article-header -->
							
							<!-- end article-header --><!-- end mobile-contents button  -->

							<div><h2 id="whats-new-in-f-9">What's new in F# 9</h2><div>
											<ul data-bi-name="page info" lang="en-us" dir="ltr"><li>Article</li><li><time data-article-date="" aria-label="Article review date" datetime="2024-11-12T08:00:00Z" data-article-date-source="calculated">11/12/2024</time>
															</li><li>
															
														</li></ul>
										</div><nav id="center-doc-outline" data-bi-name="intopic toc" aria-label="In this article">
											<h2 id="ms--in-this-article">In this article</h2>
										</nav><!-- <content> --><p>F# 9 introduces a range of enhancements that make your programs safer, more resilient, and performant. This article highlights the major changes in F# 9, developed in the <a href="https://github.com/dotnet/fsharp" data-linktype="external">F# open source code repository</a>.</p>
<p>F# 9 is available in .NET 9. You can download the latest .NET SDK from the <a href="https://dotnet.microsoft.com/download" data-linktype="external">.NET downloads page</a>.</p>
<h2 id="nullable-reference-types">Nullable reference types</h2>
<p>Although F# is designed to avoid <code>null</code>, it can creep in when interfacing with .NET libraries written in C#. F# now provides a type-safe way to deal with reference types that can have <code>null</code> as a valid value.</p>
<p>For more details, watch out for an <a href="https://devblogs.microsoft.com/dotnet/tag/f/" data-linktype="external">upcoming blog post about this feature</a>.</p>
<p>Here are some examples:</p>
<pre><code>// Declared type at let-binding
let notAValue: string | null = null

let isAValue: string | null = "hello world"

let isNotAValue2: string = null // gives a nullability warning

let getLength (x: string | null) = x.Length // gives a nullability warning since x is a nullable string

// Parameter to a function
let len (str: string | null) =
    match str with
    | null -&gt; -1
    | NonNull s -&gt; s.Length  // binds a non-null result

// Parameter to a function
let len (str: string | null) =
    let s = nullArgCheck "str" str // Returns a non-null string
    s.Length  // binds a non-null result

// Declared type at let-binding
let maybeAValue: string | null = hopefullyGetAString()

// Array type signature
let f (arr: (string | null)[]) = ()

// Generic code, note 'T must be constrained to be a reference type
let findOrNull (index: int) (list: 'T list) : 'T | null when 'T : not struct =
    match List.tryItem index list with
    | Some item -&gt; item
    | None -&gt; null
</code></pre>
<h2 id="discriminated-union-is-properties">Discriminated union <code>.Is*</code> properties</h2>
<p>Discriminated unions now have auto-generated properties for each case, allowing you to check if a value is of a particular case. For example, for the following type:</p>
<pre><code>type Contact =
    | Email of address: string
    | Phone of countryCode: int * number: string

type Person = { name: string; contact: Contact }
</code></pre>
<p>Previously, you had to write something like:</p>
<pre><code>let canSendEmailTo person =
    match person.contact with
    | Email _ -&gt; true
    | _ -&gt; false
</code></pre>
<p>Now, you can instead write:</p>
<pre><code>let canSendEmailTo person =
    person.contact.IsEmail
</code></pre>
<h2 id="partial-active-patterns-can-return-bool-instead-of-unit-option">Partial active patterns can return <code>bool</code> instead of <code>unit option</code></h2>
<p>Previously, partial active patterns returned <code>Some ()</code> to indicate a match and <code>None</code> otherwise. Now, they can also return <code>bool</code>.</p>
<p>For example, the active pattern for the following:</p>
<pre><code>match key with
| CaseInsensitive "foo" -&gt; ...
| CaseInsensitive "bar" -&gt; ...
</code></pre>
<p>Was previously written as:</p>
<pre><code>let (|CaseInsensitive|_|) (pattern: string) (value: string) =
    if String.Equals(value, pattern, StringComparison.OrdinalIgnoreCase) then
        Some ()
    else
        None
</code></pre>
<p>Now, you can instead write:</p>
<pre><code>let (|CaseInsensitive|_|) (pattern: string) (value: string) =
    String.Equals(value, pattern, StringComparison.OrdinalIgnoreCase)
</code></pre>
<h2 id="prefer-extension-methods-to-intrinsic-properties-when-arguments-are-provided">Prefer extension methods to intrinsic properties when arguments are provided</h2>
<p>To align with a pattern seen in some .NET libraries, where extension methods are defined with the same names as intrinsic properties of a type, F# now resolves these extension methods instead of failing the type check.</p>
<p>Example:</p>
<pre><code>type Foo() =
    member val X : int = 0 with get, set

[&lt;Extension&gt;]
type FooExt =
    [&lt;Extension&gt;]
    static member X (f: Foo, i: int) = f.X &lt;- i; f

let f = Foo()

f.X(1) // We can now call the extension method to set the property and chain further calls
</code></pre>
<h2 id="empty-bodied-computation-expressions">Empty-bodied computation expressions</h2>
<p>F# now supports empty <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/language-reference/computation-expressions" data-linktype="relative-path">computation expressions</a>.</p>
<pre><code>let xs = seq { } // Empty sequence
</code></pre>
<pre><code>let html =
    div {
        p { "Some content." }
        p { } // Empty paragraph
    }
</code></pre>
<p>Writing an empty computation expression will result in a call to the computation expression builder's <code>Zero</code> method.</p>
<p>This is a more natural syntax compared to the previously available <code>builder { () }</code>.</p>
<h2 id="hash-directives-are-allowed-to-take-non-string-arguments">Hash directives are allowed to take non-string arguments</h2>
<p>Hash directives for the compiler previously only allowed string arguments passed in quotes. Now, they can take any type of argument.</p>
<p>Previously, you had:</p>
<pre><code>#nowarn "0070"
#time "on"
</code></pre>
<p>Now, you can write:</p>
<pre><code>#nowarn 0070
#time on
</code></pre>
<p>This also ties into the next two changes.</p>
<h2 id="extended-help-directive-in-fsi-to-show-documentation-in-the-repl">Extended #help directive in fsi to show documentation in the REPL</h2>
<p>The <code>#help</code> directive in F# Interactive now shows documentation for a given object or function, which you can now pass without quotes.</p>
<pre><code>&gt; #help List.map;;

Description:
Builds a new collection whose elements are the results of applying the given function
to each of the elements of the collection.

Parameters:
- mapping: The function to transform elements from the input list.
- list: The input list.
Returns:
The list of transformed elements.

Examples:
let inputs = [ "a"; "bbb"; "cc" ]

inputs |&gt; List.map (fun x -&gt; x.Length)
// Evaluates to [ 1; 3; 2 ]

Full name: Microsoft.FSharp.Collections.ListModule.map
Assembly: FSharp.Core.dll
</code></pre>
<p>See <a href="https://devblogs.microsoft.com/dotnet/enhancing-help-in-fsi/" data-linktype="external">Enhancing #help in F# Interactive blog post</a> for more details.</p>
<h2 id="allow-nowarn-to-support-the-fs-prefix-on-error-codes-to-disable-warnings">Allow #nowarn to support the FS prefix on error codes to disable warnings</h2>
<p>Previously, when you wanted to disable a warning and wrote <code>#nowarn "FS0057"</code>, you would get an <code>Invalid warning number 'FS0057'</code>. Even though the warning number is correct, it just wasn't supposed to have the <code>FS</code> prefix.</p>
<p>Now, you won't have to spend time figuring that out because the warning numbers are accepted even with the prefix.</p>
<p>All of these will now work:</p>
<pre><code>#nowarn 57
#nowarn 0057
#nowarn FS0057

#nowarn "57"
#nowarn "0057"
#nowarn "FS0057"
</code></pre>
<p>It's a good idea to use the same style throughout your project.</p>
<h2 id="warning-about-tailcall-attribute-on-non-recursive-functions-or-let-bound-values">Warning about TailCall attribute on non-recursive functions or let-bound values</h2>
<p>F# now emits a warning when you put the <code>[&lt;TailCall&gt;]</code> attribute somewhere it doesn't belong. While it has no effect on what the code does, it could confuse someone reading it.</p>
<p>For example, these usages will now emit a warning:</p>
<pre><code>[&lt;TailCall&gt;]
let someNonRecFun x = x + x

[&lt;TailCall&gt;]
let someX = 23

[&lt;TailCall&gt;]
let rec someRecLetBoundValue = nameof(someRecLetBoundValue)
</code></pre>
<h2 id="enforce-attribute-targets">Enforce attribute targets</h2>
<p>The compiler now correctly enforces the <code>AttributeTargets</code> on let values, functions, union case declarations, implicit constructors, structs, and classes. This can prevent some hard-to-notice bugs, such as forgetting to add the unit argument to an Xunit test.</p>
<p>Previously, you could write:</p>
<pre><code>[&lt;Fact&gt;]
let ``this test always fails`` =
  Assert.True(false)
</code></pre>
<p>When you ran the tests with <code>dotnet test</code>, they would pass. Since the test function is not actually a function, it was ignored by the test runner.</p>
<p>Now, with correct attribute enforcement, you will get an <code>error FS0842: This attribute is not valid for use on this language element</code>.</p>
<h2 id="updates-to-the-standard-library-fsharpcore">Updates to the <a href="https://fsharp.github.io/fsharp-core-docs/" data-linktype="external">standard library (FSharp.Core)</a></h2>
<h3 id="random-functions-for-collections">Random functions for collections</h3>
<p>The <code>List</code>, <code>Array</code>, and <code>Seq</code> modules have new functions for random sampling and shuffling. This makes F# easier to use for common data science, machine learning, game development, and other scenarios where randomness is needed.</p>
<p>All functions have the following variants:</p>
<ul>
<li>One that uses an implicit, thread-safe, shared <a href="https://learn.microsoft.com/en-us/dotnet/api/system.random" data-linktype="absolute-path">Random</a> instance</li>
<li>One that takes a <code>Random</code> instance as an argument</li>
<li>One that takes a custom <code>randomizer</code> function, which should return a float value greater than or equal to 0.0 and less than 1.0</li>
</ul>
<p>There are four functions (each with three variants) available: <code>Shuffle</code>, <code>Choice</code>, <code>Choices</code>, and <code>Sample</code>.</p>
<h4 id="shuffle">Shuffle</h4>
<p>The <code>Shuffle</code> functions return a new collection of the same type and size, with each item in a randomly mixed position. The chance to end up in any position is weighted evenly on the length of the collection.</p>
<pre><code>let allPlayers = [ "Alice"; "Bob"; "Charlie"; "Dave" ]
let round1Order = allPlayers |&gt; List.randomShuffle // [ "Charlie"; "Dave"; "Alice"; "Bob" ]
</code></pre>
<p>For arrays, there are also <code>InPlace</code> variants that shuffle the items in the existing array instead of creating a new one.</p>
<h4 id="choice">Choice</h4>
<p>The <code>Choice</code> functions return a single random element from the given collection. The random choice is weighted evenly on the size of the collection.</p>
<pre><code>let allPlayers = [ "Alice"; "Bob"; "Charlie"; "Dave" ]
let randomPlayer = allPlayers |&gt; List.randomChoice // "Charlie"
</code></pre>
<h4 id="choices">Choices</h4>
<p>The <code>Choices</code> functions select N elements from the input collection in random order, allowing elements to be selected more than once.</p>
<pre><code>let weather = [ "Raining"; "Sunny"; "Snowing"; "Windy" ]
let forecastForNext3Days = weather |&gt; List.randomChoices 3 // [ "Windy"; "Snowing"; "Windy" ]
</code></pre>
<h4 id="sample">Sample</h4>
<p>The <code>Sample</code> functions select N elements from the input collection in random order, without allowing elements to be selected more than once. N cannot be greater than the collection length.</p>
<pre><code>let foods = [ "Apple"; "Banana"; "Carrot"; "Donut"; "Egg" ]
let today'sMenu = foods |&gt; List.randomSample 3 // [ "Donut"; "Apple"; "Egg" ]
</code></pre>
<p>For a full list of functions and their variants, see (<a href="https://github.com/fsharp/fslang-design/blob/main/RFCs/FS-1135-random-functions-for-collections.md" data-linktype="external">RFC #1135</a>).</p>
<h3 id="parameterless-constructor-for-customoperationattribute">Parameterless constructor for <code>CustomOperationAttribute</code></h3>
<p>This constructor makes it easier to create a custom operation for a computation expression builder. It uses the name of the method instead of having to explicitly name it (when in most cases the name matches the method name already).</p>
<pre><code>type FooBuilder() =
    [&lt;CustomOperation&gt;]  // Previously had to be [&lt;CustomOperation("bar")&gt;]
    member _.bar(state) = state
</code></pre>
<h3 id="c-collection-expression-support-for-f-lists-and-sets">C# collection expression support for F# lists and sets</h3>
<p>When using F# lists and sets from C#, you can now use collection expressions to initialize them.</p>
<p>Instead of:</p>
<pre><code>FSharpSet&lt;int&gt; mySet = SetModule.FromArray([1, 2, 3]);
</code></pre>
<p>You can now write:</p>
<pre><code>FSharpSet&lt;int&gt; mySet = [ 1, 2, 3 ];
</code></pre>
<p>Collection expressions make it easier to use the F# immutable collections from C#. You might want to use the F# collections when you need their structural equality, which <a href="https://learn.microsoft.com/en-us/dotnet/api/system.collections.immutable" data-linktype="absolute-path">System.Collections.Immutable</a> collections don't have.</p>
<h2 id="developer-productivity-improvements">Developer productivity improvements</h2>
<h3 id="parser-recovery">Parser recovery</h3>
<p>There have been continuous improvements in parser recovery, meaning that tooling (for example, syntax highlighting) still works with code when you're in the middle of editing it and it might not be syntactically correct at all times.</p>
<p>For example, the parser will now recover on unfinished <code>as</code> patterns, object expressions, enum case declarations, record declarations, complex primary constructor patterns, unresolved long identifiers, empty match clauses, missing union case fields, and missing union case field types.</p>
<h3 id="diagnostics">Diagnostics</h3>
<p>Diagnostics, or understanding what the compiler doesn't like about your code, are an important part of the user experience with F#. There are a number of new or improved diagnostic messages or more precise diagnostic locations in F# 9.</p>
<p>These include:</p>
<ul>
<li>Ambiguous override method in object expression</li>
<li>Abstract members when used in non-abstract classes</li>
<li>Property that has the same name as a discriminated union case</li>
<li>Active pattern argument count mismatch</li>
<li>Unions with duplicated fields</li>
<li>Using <code>use!</code> with <code>and!</code> in computation expressions</li>
</ul>
<p>There is also a new compile-time error for classes with over 65,520 methods in generated <a href="https://learn.microsoft.com/en-us/dotnet/standard/managed-code#intermediate-language--execution" data-linktype="relative-path">IL</a>. Such classes aren't loadable by the CLR and result in a run-time error. (You won't author that many methods, but there have been cases with generated code.)</p>
<h3 id="real-visibility">Real visibility</h3>
<p>There is a quirk with how F# generates assemblies that results in private members being written to <a href="https://learn.microsoft.com/en-us/dotnet/standard/managed-code#intermediate-language--execution" data-linktype="relative-path">IL</a> as internal. This allows inappropriate access to private members from non-F# projects that have access to an F# project via <a href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.internalsvisibletoattribute" data-linktype="absolute-path"><code>InternalsVisibleTo</code></a>.</p>
<p>Now, there is an opt-in fix for this behavior available via the <code>--realsig+</code> compiler flag. Try it in your solution to see if any of your projects depend on this behavior. You can add it to your <code>.fsproj</code> files like this:</p>
<pre><code>&lt;PropertyGroup&gt;
    &lt;RealSig&gt;true&lt;/RealSig&gt;
&lt;/PropertyGroup&gt;
</code></pre>
<h2 id="performance-improvements">Performance improvements</h2>
<h3 id="optimized-equality-checks">Optimized equality checks</h3>
<p>Equality checks are now faster and allocate less memory.</p>
<p>For example:</p>
<pre><code>[&lt;Struct&gt;]
type MyId =
    val Id: int
    new id = { Id = id }

let ids = Array.init 1000 MyId
let missingId = MyId -1

// used to box 1000 times, doesn't box anymore
let _ = ids |&gt; Array.contains missingId
</code></pre>
<h4 id="benchmark-results-for-affected-array-functions-applied-to-a-2-member-struct">Benchmark results for affected array functions, applied to a 2-member struct</h4>
<p>Before:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Mean</th>
<th>Error</th>
<th>Gen0</th>
<th>Allocated</th>
</tr>
</thead>
<tbody>
<tr>
<td>ArrayContainsExisting</td>
<td>15.48 ns</td>
<td>0.398 ns</td>
<td>0.0008</td>
<td>48 B</td>
</tr>
<tr>
<td>ArrayContainsNonexisting</td>
<td>5,190.95 ns</td>
<td>103.533 ns</td>
<td>0.3891</td>
<td>24000 B</td>
</tr>
<tr>
<td>ArrayExistsExisting</td>
<td>17.97 ns</td>
<td>0.389 ns</td>
<td>0.0012</td>
<td>72 B</td>
</tr>
<tr>
<td>ArrayExistsNonexisting</td>
<td>5,316.64 ns</td>
<td>103.776 ns</td>
<td>0.3891</td>
<td>24024 B</td>
</tr>
<tr>
<td>ArrayTryFindExisting</td>
<td>24.80 ns</td>
<td>0.554 ns</td>
<td>0.0015</td>
<td>96 B</td>
</tr>
<tr>
<td>ArrayTryFindNonexisting</td>
<td>5,139.58 ns</td>
<td>260.949 ns</td>
<td>0.3891</td>
<td>24024 B</td>
</tr>
<tr>
<td>ArrayTryFindIndexExisting</td>
<td>15.92 ns</td>
<td>0.526 ns</td>
<td>0.0015</td>
<td>96 B</td>
</tr>
<tr>
<td>ArrayTryFindIndexNonexisting</td>
<td>4,349.13 ns</td>
<td>100.750 ns</td>
<td>0.3891</td>
<td>24024 B</td>
</tr>
</tbody>
</table>
<p>After:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Mean</th>
<th>Error</th>
<th>Gen0</th>
<th>Allocated</th>
</tr>
</thead>
<tbody>
<tr>
<td>ArrayContainsExisting</td>
<td>4.865 ns</td>
<td>0.3452 ns</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>ArrayContainsNonexisting</td>
<td>766.005 ns</td>
<td>15.2003 ns</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>ArrayExistsExisting</td>
<td>8.025 ns</td>
<td>0.1966 ns</td>
<td>0.0004</td>
<td>24 B</td>
</tr>
<tr>
<td>ArrayExistsNonexisting</td>
<td>834.811 ns</td>
<td>16.2784 ns</td>
<td>-</td>
<td>24 B</td>
</tr>
<tr>
<td>ArrayTryFindExisting</td>
<td>16.401 ns</td>
<td>0.3932 ns</td>
<td>0.0008</td>
<td>48 B</td>
</tr>
<tr>
<td>ArrayTryFindNonexisting</td>
<td>1,140.515 ns</td>
<td>22.7372 ns</td>
<td>-</td>
<td>24 B</td>
</tr>
<tr>
<td>ArrayTryFindIndexExisting</td>
<td>14.864 ns</td>
<td>0.3648 ns</td>
<td>0.0008</td>
<td>48 B</td>
</tr>
<tr>
<td>ArrayTryFindIndexNonexisting</td>
<td>990.028 ns</td>
<td>19.7157 ns</td>
<td>-</td>
<td>24 B</td>
</tr>
</tbody>
</table>
<p>You can read all the details here: <a href="https://devblogs.microsoft.com/dotnet/fsharp-developer-stories-how-weve-finally-fixed-a-9yearold-performance-issue/" data-linktype="external">F# Developer Stories: How we’ve finally fixed a 9-year-old performance issue</a>.</p>
<h3 id="field-sharing-for-struct-discriminated-unions">Field sharing for struct discriminated unions</h3>
<p>If fields in multiple cases of a struct discriminated union have the same name and type, they can share the same memory location, reducing the struct's memory footprint. (Previously, same field names weren't allowed, so there are no issues with binary compatibility.)</p>
<p>For example:</p>
<pre><code>[&lt;Struct&gt;]
type MyStructDU =
    | Length of int64&lt;meter&gt;
    | Time of int64&lt;second&gt;
    | Temperature of int64&lt;kelvin&gt;
    | Pressure of int64&lt;pascal&gt;
    | Abbrev of TypeAbbreviationForInt64
    | JustPlain of int64
    | MyUnit of int64&lt;MyUnit&gt;

sizeof&lt;MyStructDU&gt; // 16 bytes
</code></pre>
<p>Comparing to previous verion (where you had to use unique field names):</p>
<pre><code>[&lt;Struct&gt;]
type MyStructDU =
    | Length of length: int64&lt;meter&gt;
    | Time of time: int64&lt;second&gt;
    | Temperature of temperature: int64&lt;kelvin&gt;
    | Pressure of pressure: int64&lt;pascal&gt;
    | Abbrev of abbrev: TypeAbbreviationForInt64
    | JustPlain of plain: int64
    | MyUnit of myUnit: int64&lt;MyUnit&gt;

sizeof&lt;MyStructDU&gt; // 60 bytes
</code></pre>
<h3 id="integral-range-optimizations">Integral range optimizations</h3>
<p>The compiler now generates optimized code for more instances of <code>start..finish</code> and <code>start..step..finish</code> expressions. Previously, these were only optimized when the type was <code>int</code>/<code>int32</code> and the step was a constant <code>1</code> or <code>-1</code>. Other integral types and different step values used an inefficient <code>IEnumerable</code>-based implementation. Now, all of these are optimized.</p>
<p>This leads to anywhere from 1.25× up to 8× speed up in loops:</p>
<pre><code>for … in start..finish do …
</code></pre>
<p>List/array expressions:</p>
<pre><code>[start..step..finish]
</code></pre>
<p>and comprehensions:</p>
<pre><code>[for n in start..finish -&gt; f n]
</code></pre>
<h3 id="optimized-for-x-in-xs----in-list-and-array-comprehensions">Optimized <code>for x in xs -&gt; …</code> in list and array comprehensions</h3>
<p>On a related note, comprehensions with <code>for x in xs -&gt; …</code> have been optimized for lists and arrays, with notable improvements especially for arrays, with speedups up to 10× and ⅓ to ¼ allocation size.</p>

<h3 id="live-buffers-in-visual-studio">Live buffers in Visual Studio</h3>
<p>This previously opt-in feature has been thoroughly tested and is now enabled by default. The background compiler powering the IDE now works with live file buffers, meaning you don't have to save the files to disk to get the changes applied. Previously, this could cause some unexpected behavior. (Most notoriously when you tried to rename a symbol present in a file that had been edited but not saved.)</p>
<h3 id="analyzer-and-code-fix-for-removing-unnecessary-parentheses">Analyzer and code fix for removing unnecessary parentheses</h3>
<p>Sometimes extra parentheses are used for clarity, but sometimes they are just noise. For the latter case, you now get a code fix in Visual Studio to remove them.</p>
<p>For example:</p>
<pre><code>let f (x) = x // -&gt; let f x = x
let _ = (2 * 2) + 3 // -&gt; let _ = 2 * 2 + 3
</code></pre>
<h3 id="custom-visualizer-support-for-f-in-visual-studio">Custom visualizer support for F# in Visual Studio</h3>
<p>The debugger visualizer in Visual Studio now works with F# projects.</p>
<p><img src="https://learn.microsoft.com/en-us/dotnet/fsharp/media/whats-new/fsharp-9/vs-visualizer.gif" alt="debug visualizer" data-linktype="relative-path"></p>
<h3 id="signature-tooltips-shown-mid-pipeline">Signature tooltips shown mid-pipeline</h3>
<p>Previously, signature help wasn't offered in a situation like the following, where a function in the middle of a pipeline already had a complex curried parameter (for example, a lambda) applied to it. Now, the signature tooltip shows up for the next parameter (<code>state</code>):</p>
<p><img src="https://learn.microsoft.com/en-us/dotnet/fsharp/media/whats-new/fsharp-9/help.png" alt="tooltip" data-linktype="relative-path"></p>
</div>
							
							<!-- </content> -->

						</main><!-- recommendations section --><!-- end recommendations section -->

						<!-- feedback section --><div data-bi-name="open-source-feedback-section" data-open-source-feedback-section="" hidden="">
				<div>
					<span aria-hidden="true">
						<span></span>
					</span>
					<span>Collaborate with us on GitHub</span>
				</div>
				<span>
					The source for this content can be found on GitHub, where you can also create and review issues and pull requests. For more information, see <a href="https://learn.microsoft.com/contribute/content/dotnet/dotnet-contribute">our contributor guide</a>.
				</span>
			</div><!-- end feedback section -->

						<!-- feedback report section --><!-- end feedback report section --></div><div id="ms--additional-resources" data-bi-name="pageactions" role="complementary" aria-label="Additional resources">
								<h2 id="ms--additional-resources-heading" hidden="">Additional resources</h2>
								
								
								
								<nav id="side-doc-outline" data-bi-name="intopic toc" aria-label="In this article">
									<h3>In this article</h3>
								</nav>
								
							</div></div>
	<!--end of .mainContainer -->

	

	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenID Connect specifications published as ISO standards (278 pts)]]></title>
            <link>https://self-issued.info/?p=2573</link>
            <guid>42101181</guid>
            <pubDate>Sun, 10 Nov 2024 16:53:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://self-issued.info/?p=2573">https://self-issued.info/?p=2573</a>, See on <a href="https://news.ycombinator.com/item?id=42101181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

									<p><span><img decoding="async" src="https://self-issued.info/images/openid-logo.png" alt="OpenID logo"></span>I’m thrilled to report that the <a href="https://openid.net/connect">OpenID Connect</a> specifications have now been published as ISO/IEC standards.  They are:</p>
<ul>
<li><a href="https://www.iso.org/standard/89056.html">ISO/IEC 26131:2024 — Information technology — OpenID connect — OpenID connect core 1.0 incorporating errata set 2</a></li>
<li><a href="https://www.iso.org/standard/89057.html">ISO/IEC 26132:2024 — Information technology — OpenID connect — OpenID connect discovery 1.0 incorporating errata set 2</a></li>
<li><a href="https://www.iso.org/standard/89059.html">ISO/IEC 26133:2024 — Information technology — OpenID connect — OpenID connect dynamic client registration 1.0 incorporating errata set 2</a></li>
<li><a href="https://www.iso.org/standard/89060.html">ISO/IEC 26134:2024 — Information technology — OpenID connect — OpenID connect RP-initiated logout 1.0</a></li>
<li><a href="https://www.iso.org/standard/89061.html">ISO/IEC 26135:2024 — Information technology — OpenID connect — OpenID connect session management 1.0</a></li>
<li><a href="https://www.iso.org/standard/89062.html">ISO/IEC 26136:2024 — Information technology — OpenID connect — OpenID connect front-channel logout 1.0</a></li>
<li><a href="https://www.iso.org/standard/89063.html">ISO/IEC 26137:2024 — Information technology — OpenID connect — OpenID connect back-channel logout 1.0 incorporating errata set 1</a></li>
<li><a href="https://www.iso.org/standard/89064.html">ISO/IEC 26138:2024 — Information technology — OpenID connect — OAuth 2.0 multiple response type encoding practices</a></li>
<li><a href="https://www.iso.org/standard/89065.html">ISO/IEC 26139:2024 — Information technology — OpenID connect — OAuth 2.0 form post response mode</a></li>
</ul>
<p>I submitted the OpenID Connect specifications for publication by ISO as <a href="https://www.iso.org/deliverables-all.html#PAS">Publicly Available Specifications (PAS)</a> for the OpenID Foundation in December 2023.  Following the ISO approval vote, they are now published.  This should foster even broader adoption of OpenID Connect by enabling deployments in jurisdictions around the world that have legal requirements to use specifications from standards bodies recognized by international treaties, of which ISO is one.</p>
<p>Before submitting the specifications, the <a href="https://openid.net/wg/connect/">OpenID Connect working group</a> diligently worked through the process of applying <a href="https://self-issued.info/?p=2454">errata corrections</a> to the specifications, so that the ISO versions would have all known corrections incorporated.</p>
<p>Having successfully gone through the ISO PAS submission process once, the OpenID Foundation now plans to submit additional families of final specifications for publication by ISO.  These include the <a href="https://openid.net/specs/openid-financial-api-part-2-1_0.html">FAPI 1.0</a> specifications, and once they’re final, the <a href="https://openid.net/public-review-proposed-final-openid-connect-for-identity-assurance/">eKYC-IDA</a> specifications and <a href="https://openid.net/specs/fapi-2_0-security-profile.html">FAPI 2.0</a> specifications.</p>
<p>Thanks to all who helped us achieve this significant accomplishment!</p>
<p><a href="https://www.iso.org/standard/89056.html"><img decoding="async" src="https://self-issued.info/images/ISO-IEC_26131-2024_Cover.png" alt="ISO/IEC 26131:2024 Cover Page"></a></p>

								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Salary expectations questions – How should you answer them? (2020) (103 pts)]]></title>
            <link>https://fearlesssalarynegotiation.com/salary-expectations-interview-question/</link>
            <guid>42101107</guid>
            <pubDate>Sun, 10 Nov 2024 16:45:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fearlesssalarynegotiation.com/salary-expectations-interview-question/">https://fearlesssalarynegotiation.com/salary-expectations-interview-question/</a>, See on <a href="https://news.ycombinator.com/item?id=42101107">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      	
<p>by <a href="https://fearlesssalarynegotiation.com/about/">Josh Doody</a></p>

				
        <p>You’ve been job hunting for a while, and you finally hear back about a job you really want. The recruiter reaches out and asks if you’re free to chat for a few minutes so they can ask you a few questions. “Sure!”</p>

<p>Everything seems pretty straightforward—you talk about your background, how you found this job listing, stuff like that. Then the recruiter asks you a question that stops you in your tracks:</p>

<blockquote>
  <p>“So where are you right now in terms of salary, and what are your salary expectations if you make this move?”</p>
</blockquote>

<p><strong>Wait a minute.</strong> They want to know your current salary and salary expectations before you even start your job interviews?</p>

<p>Your salary expectations are one of the few things you know that the company doesn’t. That makes them extremely valuable and sharing them can make your salary negotiations very difficult and even cost you a lot of money.</p>


<!-- / convertkit-form-inline -->

<p><strong>Get the answer for your specific situation</strong></p>

<p>This is a detailed article, so you can use this table of contents to jump ahead if you already know what you’re looking for!</p>

<ul>
  <li><a href="#expected-salary-question">Salary expectations question - what to expect</a></li>
  <li><a href="#current-salary-question">Current salary question - what to expect</a></li>
  <li><a href="#expected-salary-answer">Salary expectations question - how to respond</a></li>
  <li><a href="#current-salary-answer">Current salary question - how to respond</a></li>
  <li><a href="#combined-answer">Combined answer to both questions</a></li>
  <li><a href="#when-they-ask-again">When they ask again</a></li>
  <li><a href="#when-they-insist">When they threaten to stop interviewing you</a></li>
  <li><a href="#special-circumstances">Answering salary questions on applications</a></li>
  <li><a href="#recruiter-pushback">Pushback from recruiters and how to respond</a></li>
  <li><a href="#already-shared-expected-salary">Already shared expected salary - how to recover</a></li>
  <li><a href="#already-shared-current-salary">Already shared current salary - how to recover</a></li>
  <li><a href="#summary">Summary</a></li>
</ul>

<p>This will usually come up in the “<a href="https://fearlesssalarynegotiation.com/book/interview/pre-interview-phase/">pre-interview</a>” or “pre-screen”, which is right at the beginning of the interview process. That’s why it’s such a sneaky question! It’s a salary negotiation tactic disguised as a gatekeeper-type interview question.</p>

<p>So when you hear the salary expectations question, you’ll be thinking “What do I need to say to get to my next interview? They asked for my current salary and salary expectations, so I’ll tell them that so we can move on.”</p>

<p><strong>You intuitively know that sharing your current salary or salary expectations probably isn’t in your best interest.</strong> But you’re also really excited about this opportunity and you don’t want to miss out. Plus, you’re not sure how to not answer this question.</p>

<p><strong>In a hurry?</strong></p>

<p>Here’s a short video with the basics of how to respond when asked for your salary expectations or current salary. Then I’ll tell you more about the nuances of this question and what to do when they don’t give up so easily:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Euhpm-O_D6I?rel=0" title="How to answer the salary expectations question" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>It’s easier to understand the salary expectations questions if we answer another question first: What is the recruiter or hiring manager <em>really</em> asking for here?</p>

<p>Let’s take each question—salary expectations and current salary–separately.</p>

<h2 id="what-are-your-salary-expectations"><a name="expected-salary-question"></a>“What are your salary expectations?”</h2>

<p>This question is tricky because it <em>sounds</em> like they’re giving you a chance to set the baseline for your new salary. It <em>sounds</em> like they’re asking you to contribute to the terms of the job offer you’ll hopefully get later.</p>

<p>But let’s re-frame this question to reflect what they’re <em>really</em> asking for:</p>

<blockquote>
  <p>“Can you take a wild guess what salary we might pay someone with your skillset and experience to do the job you applied for?”</p>
</blockquote>

<p>The truth is you probably have no idea what they pay people because that decision—what a company pays someone to do a job—is dependent on lots of factors that have nothing to do with <em>your</em> particular qualifications to do the job.</p>

<p>For example, what’s their hiring budget? How many positions do they need to fill for this job? How badly do they need to fill those positions? How’s the company doing in terms of revenue, profits, and growth?</p>

<p>When you answer the salary expectations question, you’re literally guessing a number that depends on tons factors you can’t assess.</p>

<p>But it gets worse.</p>

<p>What are the odds that you’ll actually guess the salary they’re willing to pay someone with your skillset and experience to do the job you’re interviewing for? Practically nil, right? You’re almost certainly going to over- or under-estimate what they’re willing to pay.</p>

<p>I know what you’re thinking, and it’s still a bad idea.</p>

<blockquote>
  <p>“What if I just state a really big expected salary and overshoot their range?”</p>
</blockquote>

<p>To which I say, “How confident are you that you know where the top of their pay range is?” I guess you could just say, “I would like to make one million dollars!” but what good would <em>that</em> do?</p>

<p><strong>You can’t win if you guess at their salaries.</strong></p>

<p>If you underestimate what they’re willing to pay, you’re leaving money on the table. If the real answer is that they would compensate someone like you up to $75,000 dollars, and you guess they would pay a salary of only $65,000, you very literally may have just cost yourself $10,000.</p>

<p>The one thing that might save you is when you guess so badly that you under-shoot the minimum salary they can pay you to do the job. For example, you might tell them your expected salary is $65,000, but the minimum they pay for that job is $70,000. Then they would pay you $70,000 even though you “only” asked for $65,000—a huge win! Except they’re paying you the absolute minimum salary they possibly can, and you could’ve gotten a lot more.</p>

<p>If you <em>over</em>estimate and tell them your salary expectation is $85,000, you may set off red flags that cause them to rethink the interview process altogether. This is pretty rare, but you could disqualify yourself by being “too expensive” for them. If your expected salary is well above their budgeted pay range, they may just move on to other similar candidates with lower salary expectations.</p>

<p>The bottom line is you probably aren’t going to guess what their <a href="https://fearlesssalarynegotiation.com/book/salary-structures/what-is-a-salary-structure/">salary structure</a> looks like, and if you try to guess you’ll cost yourself a lot of money.</p>

<p>This sounds pretty bad, right?</p>

<p>But wait! It gets <em>even worse</em>!</p>

<h3 id="the-real-salary-theyre-willing-to-pay-could-increase-throughout-your-interview">The “real” salary they’re willing to pay could increase throughout your interview</h3>

<p>So they’ve asked you to guess what they’re willing to pay someone with your skillset and experience to do the job you applied for. And you’ll almost certainly guess wrong and cost yourself money or even an opportunity to continue interviewing.</p>

<p><strong>What most people don’t know is that the company’s own answer to the salary expectations question could change as you move through the interview process.</strong></p>

<p>Sounds crazy, right? But it’s true. Here’s how.</p>

<p>Your job in the interview process is to tell a story about how their company or team will be better if they hire you. Each interview is another opportunity for you to tell this story in a more convincing way to another person at the company.</p>

<p>The better you are at telling this story, the more they’ll want to hire you. The more they want to hire you, the more they’ll be willing to pay, and they could literally change the budgeted salary specifically to give them a better shot at bringing you on board.</p>

<p>Let’s go back to our example earlier. Let’s say they’re hiring you for a role whose budget is $75,000. They ask you to guess what that budget is by sharing your expected salary, and you decline to guess because you read this article 😉</p>

<p>So you move on to the next stage of the interview process and do well. Then you talk to another person and you do well. Then you talk to the hiring manager and you do <em>really</em> well.</p>

<p>Now it’s time for them to decide whether to extend a job offer. They <em>definitely</em> want to make you an offer because you’ve done such a great job of convincing them that their company will be better if you’re a part of it. They <em>really</em> want to bring you on board.</p>

<p><strong>You’ve flipped their thinking.</strong></p>

<p>When they first started talking to you, they were thinking:</p>

<blockquote>
  <p>“What’s the minimum salary we would need to offer to bring this person on board?”</p>
</blockquote>

<p>Now they’re thinking:</p>

<blockquote>
  <p>“How much compensation do we need to offer to convince this person to join our team?”</p>
</blockquote>

<p>See the difference? Before, they were looking for a <em>minimum</em> compensation number. Now, they’re looking for a <em>compelling</em> compensation number.</p>

<p>Before they make you an offer, they’re going to huddle up internally, and the conversation might go something like this:</p>

<blockquote>
  <p><strong>Hiring Manager:</strong> “This is the perfect candidate for the job. We’ve been trying to fill this position for two months, and we haven’t seen any other candidates this strong. What did they say their salary expectations are? What do they want?”</p>
</blockquote>

<blockquote>
  <p><strong>Recruiter:</strong> “They didn’t share their salary expectations. Just said they would prefer to focus on the value they can add and that they want this to be a big step forward for them.”</p>
</blockquote>

<blockquote>
  <p><strong>Hiring Manager:</strong> “Huh. Ok. So what’s our budget for this role?”</p>
</blockquote>

<blockquote>
  <p><strong>Recruiter:</strong> “Well, we budgeted $75,000, but I’m not sure if that’s going to be enough. How badly do you want this person to join your team?”</p>
</blockquote>

<blockquote>
  <p><strong>Hiring Manager:</strong> “I mean, this is the perfect candidate. I don’t want anyone else. This is it. How much wiggle room do we have on that $75,000 budget?”</p>
</blockquote>

<blockquote>
  <p><strong>Recruiter:</strong> “The paygrade for this job caps out at $80,000.”</p>
</blockquote>

<blockquote>
  <p><strong>Hiring Manager:</strong> “Gosh. I’m not sure that will be enough. I don’t want to lose this candidate. What else can we do?”</p>
</blockquote>

<blockquote>
  <p><strong>Recruiter:</strong> “Well, we’re interviewing them for a Grade 2 position, so that’s why it caps out at $80,000. But the Grade 3 ‘Senior’ position for this role goes up to $95,000. The requisition is a Grade 2, but we could look at slotting them into the Grade 3 position and that would give us more room to work.”</p>
</blockquote>

<blockquote>
  <p><strong>Hiring Manager:</strong> “Ok, let’s do that. And we’ll offer… $80,000? And that gives us room to maneuver if they want to negotiate.”</p>
</blockquote>

<blockquote>
  <p><strong>Recruiter:</strong> “Sounds good. Let me get the other compensation numbers for the offer and then you can give them a call and offer them $80k and clarify that we’re considering them for the Grade 3 position.”</p>
</blockquote>

<p>To recap:</p>

<ul>
  <li>They had a salary range in mind.</li>
  <li>You convinced them that <em>you</em> are the candidate they need.</li>
  <li>They redefined the role to give them more room to pay you.</li>
  <li>They made a strong offer with room to <a href="https://fearlesssalarynegotiation.com/salary-negotiation-script/">counter offer</a> and negotiate.</li>
</ul>

<p>If you had told them your salary expectations, you may not have even gotten this far. True, maybe your guess would’ve been good. But what if you guessed lower? You would’ve cost yourself money. What if you guess higher? They may not have continued interviewing you if you were “too expensive”, and you never would have had the opportunity to show them that you’re the ideal candidate for the position they’re trying to fill.</p>

<p>And all of this is just to get a strong <em>initial</em> job offer. You can <em>continue</em> telling your story when you counter offer (see this <a href="https://fearlesssalarynegotiation.com/salary-negotiation-email-sample/">salary negotiation email sample</a> for an example of how you continue telling your story even as you counter offer), and that may drive their budget even higher.</p>


<!-- / convertkit-form-inline -->

<h2 id="whats-your-current-salary"><a name="current-salary-question"></a>“What’s your current salary?”</h2>

<p>This question is pretty straightforward, but not nearly as innocuous as it may sound. Here’s a different way of looking at this question:</p>

<blockquote>
  <p>“What’s the minimum salary we need to offer to convince you to change companies?”</p>
</blockquote>

<p>Let’s pause for a moment because this is important. I know you might be thinking about the last time this came up, and you told them your current salary, and they offered you <em>more</em> than that.</p>

<p>This is not a coincidence, and it’s not them paying you more than you’re worth. You told them your current salary was $48,000 and they offered you $50,000. Or you told them your current salary was $75,000 and they offered you $78,000. I realize this <em>feels</em> like “They paid me more than I asked for!” and it looks like a win.</p>

<p>But this is normal. They made you an offer that they hoped was just enough to entice you to leave your current job and go work for them. It was more, <strong>but it was not the best compensation they could offer you.</strong></p>

<p>When you disclosed your current salary, you gave them an easy out: they just took your answer and added a couple thousand dollars.</p>

<p>The good news is the new offer was higher than your current salary was. The bad news is you probably could’ve gotten more if you hadn’t shared your current salary with the recruiter.</p>

<p>Now that we’ve talked about why sharing your salary expectations or current salary is a bad idea, let’s talk about how to avoid sharing that information.</p>

<p>We’ll break this down into two parts: “What are your salary expectations?” and “What’s your current salary?”</p>

<h2 id="how-to-answer-the-whats-your-expected-salary-interview-question"><a name="expected-salary-answer"></a>How to answer the “What’s your expected salary?” interview question</h2>

<p>My pat answer to the “what are you looking for” part of the salary expectations question is this:</p>

<blockquote>
  <p>“I want this move to be a big step forward for me in terms of both responsibility and compensation.”</p>
</blockquote>

<p>This answer demonstrates that you want to contribute to the company by taking on additional responsibilities and that you want to be well compensated for those contributions.</p>

<h2 id="how-to-answer-the-whats-your-current-salary-interview-question"><a name="current-salary-answer"></a>How to answer the “What’s your current salary?” interview question</h2>

<p>For the “current salary” part of the question, I recommend answering something like this:</p>

<blockquote>
  <p>“I’m not really comfortable sharing that information. I would prefer to focus on the value I can add to this company and not what I’m paid at my current job.”</p>
</blockquote>

<p>It’s true that they may do some digging and put together a good educated guess as to what you’re making anyway, but maybe they won’t. If they don’t know what you’re currently making, that makes it more difficult for them to base an offer on your current salary, and that’s probably going to mean a higher initial offer for you.</p>

<p>It also means that their eventual offer will need to reflect both your <a href="https://fearlesssalarynegotiation.com/book/value/market-value-overview/">market value</a> and the value you’ll add to the company without being biased by your current salary.</p>

<h2 id="how-to-answer-the-whats-your-current-and-expected-salary-interview-question"><a name="combined-answer"></a>How to answer the “What’s your current and expected salary?” interview question</h2>

<p>Sometimes, they’ll ask both of these questions at once:</p>

<blockquote>
  <p>“So where are you right now in terms of salary, and what are your salary expectations if you make this move?”</p>
</blockquote>

<p>Here is my recommendation for a good answer to the full version of this question:</p>

<blockquote>
  <p>“I’m not comfortable sharing my current salary. I would prefer to focus on the value I can add to this company rather than what I’m paid at my current job. I don’t have a specific number in mind for a desired salary, and you know better than I do what value my skillset and experience could bring to your company. I want this move to be a big step forward for me in terms of both responsibility and compensation.”</p>
</blockquote>

<p><img src="https://fearlesssalarynegotiation.com/assets/imgs/post-images/DreadedSalaryQuestion.png" alt="The current and expected salary question" width="100%"></p>

<p>So let’s say that you’re playing a game, and your goal is to get past the salary expectations without sharing your salary requirements. This is Level 1, and the script I just gave you will usually win the game.</p>


<!-- / convertkit-form-inline -->

<h2 id="what-if-they-ask-again"><a name="when-they-ask-again"></a>What if they ask again?</h2>

<p>So you got to the end of Level 1, and you’re feeling pretty good that you didn’t disclose your salary expectations. Excellent!</p>

<p>Most of the time, that’s all it takes. Most recruiters won’t put up too much of a fight because asking those questions just slows them down. They want to fill the position just as badly as you want the job. But they often <em>have to</em> ask you about your salary requirements, so they do. Once your refuse to share, they can check that item off the list and move on.</p>

<p>But! Sometimes you complete Level 1 and the game doesn’t end. Instead, you move on to Level 2, where the pressure ramps up. Here’s what that sounds like:</p>

<blockquote>
  <p>“I really need something to share with HR.”</p>
</blockquote>

<p>…or…</p>

<blockquote>
  <p>“We can’t move forward without this information.”</p>
</blockquote>

<p>The first thing to try is just repeating that you’re not comfortable answering these questions:</p>

<blockquote>
  <p>“I’m just not comfortable discussing my current or expected salary. I prefer to focus on the value I can add in this position, and I look forward to hearing what you think is appropriate.”</p>
</blockquote>

<p>Sometimes that will work because they were willing to take one more pass at getting you to share your salary requirements. But what if <em>that</em> doesn’t work and they <em>insist</em> on getting this information?</p>

<h3 id="yes-this-is-uncomfortable-the-discomfort-is-worth-it">Yes, this is uncomfortable. The discomfort is worth it.</h3>

<p>You may have gotten this far just because you believe I know what I’m talking about. I made a good case that you shouldn’t share your salary expectations, you bought in, and you survived one or two volleys with the recruiter just based on the script above.</p>

<p><em>But they didn’t give up!</em> And now things feel awkward, and there’s a lot of pressure, and what if they just stop talking to you and go find an easier, more agreeable candidate for this job?</p>

<p><strong>I take off my “author” hat and put on my “coach” hat</strong></p>

<p>Let me emphasize that sharing your salary expectations or your current salary <em>is not in your best interest</em>. Those two pieces of information are two of three unique pieces of information you have:</p>

<ol>
  <li>Your salary expectations</li>
  <li>Your current salary</li>
  <li>How badly you need or want the job</li>
</ol>

<p>That’s basically all of the unique information you have.</p>

<p>Let’s compare that to the unique pieces of information the company has:</p>

<ol>
  <li>The salary range for the position</li>
  <li>The overall compensation budget available</li>
  <li>How many of these positions they’re trying to fill</li>
  <li>How long they’ve been trying to fill the position</li>
  <li>How badly they need to fill the position</li>
  <li>How many other candidates they have to consider for the job</li>
  <li>How much they like <em>you</em> for the position relative to those candidates</li>
</ol>

<p>See what I mean? If you give them two of the three unique pieces of information you have, you’re down to 1. And they’re up from <em>a lot of information that you don’t have</em> to <em>all but one piece of information</em>.</p>

<h2 id="what-if-they-threaten-to-stop-interviewing-you"><a name="when-they-insist"></a>What if they threaten to stop interviewing you?</h2>

<p>What if they tell you the interview process simply can’t continue unless you share your current salary or expected salary?</p>

<p>My coaching clients ask me this a lot. I have two answers:</p>

<p><strong>1. They won’t.</strong></p>

<p>It’s <em>extremely</em> unlikely that they will halt the interview process if you refuse to share this information. They’re interviewing you because you’re a qualified candidate, and they need a qualified candidate. That’s their primary objective. They would <em>also</em> like to get a good deal on which ever candidate they choose, but first they have to find the right candidate.</p>

<p>They’re not going to stop interviewing you just because you don’t make it easier for them to get a good deal on you.</p>

<p>But let’s say they <em>do</em> stop talking to you because you won’t share your current salary or salary expectations. What then?</p>

<p><strong>2. So what if they do?</strong></p>

<p>If they discontinue the interview process because you won’t share two of the three unique pieces of information you have, then they’re <em>extremely</em> motivated to get a bargain on your skillset and experience, and they’re not focused on finding the right candidate for the role itself.</p>

<p>Let’s take a step back and look at the situation: They need to fill a role with the right candidate. You’ve applied for the job, indicating that <em>you</em> could be the right candidate. But instead of exploring that further, they stop talking to you because you won’t share personal information about your salary history, or take a guess at what they’re willing to pay you to do the job.</p>

<p><strong>Do you really want to work for that company?</strong> If they’re that petty <em>before</em> they hire someone, how petty will they be once you’re their employee? Chances are, things will only get more difficult after they hire you.</p>

<p>You’re probably dodging a bullet. Do you really want to work somewhere that is so myopic that they ignore perfectly qualified candidates simply because the candidate won’t make the negotiation easier by sharing their compensation requirements?</p>

<p>I can’t answer that question for you, but I can tell you my answer: “Nope.”</p>

<h3 id="how-to-answer-when-they-keep-pushing">How to answer when they keep pushing</h3>

<p>Hopefully you’re convinced that they’re motivated to continue your interviews even if you don’t share this precious information, and you gave my pat answer and completed Level 1.</p>

<p>But the game kept going and now you’re at Level 2 and they’re asking <em>again</em>. What do you say to complete Level 2 and end this awkward conversation?</p>

<p>They’ve probably either implied or straight-out said something like this:</p>

<blockquote>
  <p>“I just need to be sure the salary range works for your requirements so we don’t waste each other’s time.”</p>
</blockquote>

<p>Nobody likes wasting time, and you especially don’t want to waste someone’s time when they might be the gatekeeper for a great job opportunity, right? So you’re inclined to just give them what they need and get on with the interviews.</p>

<p>Instead, I recommend a little conversational Judo—use the reason they gave for asking <em>again</em> as your own leverage. Here’s how:</p>

<blockquote>
  <p>“It sounds like you’re trying to qualify me for a salary range. If you want to tell me what that range is, I’m happy to tell you if it’s in the ballpark.”</p>
</blockquote>

<p>They claim they need to qualify you for a salary range, but they are also asking you to give up one of your precious pieces of information to do that. Instead of sharing your salary expectations, just ask them for the range.</p>

<p>The nice thing about the way I’ve worded this script is that you’re not actually committing to accepting a job offer in that salary range. You’re saying it’s “in the ballpark”. This matters because you still have full latitude to negotiate your compensation later on once they finally make you an offer.</p>

<p>(Yes, this is a semantic argument. This is about the the diciest semantic argument you’ll hear me make.)</p>

<p>So this is how you complete Level 2, and then it’s over right? Usually, but there are some recruiters who are very persistent and they will continue pushing. They may even claim the interview process cannot continue if you don’t share your salary expectations.</p>

<p>I already covered why I think this is silly, so let’s just cut to the chase: <strong>How do you end this conversation without sharing your current salary or expected salary?</strong></p>

<h3 id="heres-your-trump-card-for-the-salary-expectations-question">Here’s your trump card for the salary expectations question</h3>

<p>This is Level 3. It’s the last level in the game. You’ve told them you’re uncomfortable sharing your salary expectations, and they persisted. You’ve told them you’re happy to confirm if the range they’ve budgeted is in the ballpark of what compensation you’ll consider, and they persisted.</p>

<p>There aren’t many options left, but this one is very effective. Here we go:</p>

<blockquote>
  <p>“I’m not comfortable sharing my current employer’s proprietary compensation information, and I know they wouldn’t appreciate it if I did. I still work for them, and I’m just not comfortable sharing their proprietary information about how they pay people like me. I really don’t have a specific number in mind for an expected salary, and I look forward to hearing what you suggest.”</p>
</blockquote>

<p>This is a heavy-handed answer, but it’s necessary because of the situation. You’re basically saying, “I have ethical qualms with giving you the compensation information you’re demanding.” This has the advantage of putting the pressure back on the recruiter, who now has a clear Catch-22 if they continue to press: They’re asking you do something you see as unethical to move forward, but they probably don’t want to hire someone unethical.</p>

<p>If they <em>still</em> press after this answer, I’m all out of ideas because I’ve never seen anyone fail to complete the game with this script.</p>


<!-- / convertkit-form-inline -->

<h2 id="answering-salary-questions-on-applications"><a name="special-circumstances"></a>Answering salary questions on applications</h2>

<p>It’s more and more common to see salary questions on job applications—both paper and online. So let’s talk about how to use this strategy when confronted with the salary expectations questions on an application.</p>

<h3 id="answering-salary-questions-on-paper-job-applications">Answering salary questions on paper job applications</h3>

<p>If you’re filling out a paper job application with questions about your current, previous, or expected salary, just leave those questions blank. Either the recruiter will just let it go, or they’ll verbally ask you for those numbers, and you can fall back on the scripts we covered earlier.</p>

<h3 id="answering-salary-questions-on-online-job-applications">Answering salary questions on online job applications</h3>

<p>Online job applications can be trickier because they might <em>require</em> you to enter an answer in order to submit them. So there are two things to try:</p>

<p><strong>1. Submit the application with the salary fields left blank</strong></p>

<p>Sometimes, the online application won’t actually verify that you entered an answer to the salary questions, and you can submit it with blank answers. Give this a shot first.</p>

<p><strong>2. Submit the application with fake numbers</strong></p>

<p>This feels weird, but sometimes it’s the only way. Try entering “0” or “1” or “999999999” and submitting the form.</p>

<p>Again, the recruiter or hiring manager may follow up to ask for your current salary or expected salary, and you can just fall back on the scripts we covered earlier.</p>

<h3 id="what-if-they-ask-for-w-2s-or-pay-stubs-from-your-previous-jobs">What if they ask for W-2s or pay stubs from your previous jobs?</h3>

<p>Don’t share them. It’s none of their business.</p>

<p>It’s one thing to ask for proof of employment like a reference, but asking for your personal tax information is out of bounds.</p>

<p>Every time I’ve seen this, it’s usually a rogue recruiter who has created their own policy to ask for this sort of documentation to give them a better shot at getting your current salary or desired salary. The company itself may not make it a habit of asking for this information, but some recruiters take the idea of getting salary information further than others.</p>

<p>Regardless, you shouldn’t share this information because it has nothing to do with how qualified you are for the job they’re trying to fill, and your previous salary has no bearing on how valuable you will be in this role for their company.</p>

<h2 id="common-pushback-from-recruiters"><a name="recruiter-pushback"></a>Common pushback from recruiters</h2>

<p>I’ve been talking about the salary expectations question for years now. My coaching clients have used this advice to get much better job offers and negotiate better compensation than they ever had before. These tactics <em>work</em>.</p>

<p>But I still get some pushback, and it’s almost always from recruiters. My advice is <em>very</em> unpopular with recruiters.</p>

<p>So let’s take a minute to talk about their primary objection and why you should ignore it.</p>

<h3 id="we-dont-want-to-waste-each-others-time">“We don’t want to waste each other’s time”</h3>

<p>This is the most common objection I hear from recruiters who disagree with my advice on how to respond when asked for your current salary or salary expectations. Their concern is that they’ll invest lots of time—their time, hiring managers’ time, HR’s time—interviewing a candidate whose salary requirements exceed their hiring budget.</p>

<p>Here’s a typical comment on interview I did for <a href="https://www.glassdoor.com/">Glassdoor.com</a> about <a href="https://www.glassdoor.com/blog/9-things-to-never-say-in-a-salary-negotiation/">9 Things to Never Say in a Salary Negotiation</a>:</p>

<p><img src="https://fearlesssalarynegotiation.com/assets/imgs/post-images/DSQ_RecruiterWasteOfTime_4.png" alt="I don't deal with candidates who don't disclose salaries. It's a waste of my time." width="100%"></p>

<p>I understand how this could be a problem for them.</p>

<p>Recruiters spend most of their time finding candidates, and scheduling interviews. So if they continuously interview candidates, make them offers, then have their deals fall apart because they can’t agree on salary requirements, they could invest a lot of time interviewing candidates without filling jobs.</p>

<p>But your experience is different—you’re probably interviewing for just one or two jobs. It might be a little inconvenient if you sit through four or five interviews and get an offer that doesn’t meet your <a href="https://fearlesssalarynegotiation.com/minimum-acceptable-salary/">minimum acceptable salary</a>, but that’s a pretty small investment to find a good job. And since you only change jobs every couple of years or so, this is an investment you’ll make only occasionally.</p>

<p>So this is <em>their</em> problem, not <em>yours</em>, and they can solve it easily enough. Here’s my recommendation if a recruiter tells you they don’t want to waste your time on interviews if your expected salary doesn’t fit their hiring budget:</p>

<blockquote>
  <p>“It sounds like you’re trying to qualify me for a salary range. If you want to tell me what that range is, I’m happy to tell you if it’s in the ballpark.”</p>
</blockquote>

<p>As I mentioned earlier, this answer doesn’t box you in to accepting a job offer in that salary range. You’re just saying it’s “in the ballpark”, so you still have full latitude to negotiate your compensation when they finally make you an offer.</p>

<p>If they’re serious about respecting everyone’s time, then they’ll tell you the range so you can confirm it’s in the ballpark. And if they won’t share a range, then it’s clear they’re simply trying to gain information to benefit them during your salary negotiation later on.</p>

<blockquote>
  <p><span>Learn more</span> What should you do once you get past the salary expectations question?</p>
  <p><a href="https://fearlesssalarynegotiation.com/salary-negotiation-guide/">How to negotiate salary: 9 tips from a pro salary negotiator <i></i></a></p>
</blockquote>


<!-- / convertkit-form-inline -->



<p>What if you already declared your salary expectations and they made an offer that meets or exceeds them, but you want to negotiate for something even higher. How do you do that?</p>

<p><strong>Look for new information you can incorporate into your negotiation</strong>—ways in which your understanding of the position has changed since you declared your salary range. Here are two examples:</p>

<ul>
  <li>The job requirements are different than you thought they were—there’s more responsibility than you anticipated.</li>
  <li>The overall benefits package isn’t what you anticipated—there’s less paid vacation or sick time, the available health insurance coverage isn’t as robust as you thought, etc.</li>
</ul>

<p>Once you identify one or more things, wait for them to make an offer, which will probably be either below or at the low end of the expected salary range you disclosed. Then you can negotiate by saying something like this:</p>

<blockquote>
  <p>“I have learned a lot more about the company, the position, and the compensation package since I gave that initial salary range. Given what I’ve learned, I would be more comfortable at [your counteroffer].”</p>
</blockquote>

<p>It’s problematic to declare your salary expectations early in the interview process because you don’t know enough about the job, the company, and the available perks to pick a salary. But even if you already shared an expected salary, there are still ways to negotiate and improve your job offer.</p>

<h2 id="how-to-recover-if-youve-already-disclosed-your-current-salary"><a name="already-shared-current-salary"></a>How to recover if you’ve already disclosed your current salary</h2>

<p>So how do you recover if you’ve already disclosed your current salary depends how early you are in the interview process? Let’s take the interview process in two phases:</p>

<ol>
  <li>Early in the process—You haven’t started your formal interviews yet</li>
  <li>Late in the process—You already have a job offer</li>
</ol>

<h3 id="1-you-havent-started-your-formal-interviews-yet">1. You haven’t started your formal interviews yet</h3>

<p>There’s still time!</p>

<p>The problem you need to overcome is that most companies start by thinking, “What’s the minimum we need to offer this person to get them to take the job?”</p>

<p>Unfortunately, they have a simple formula to find that “minimum” number they need to offer: your current salary plus a little more to entice you to make the leap.</p>

<p>You want to switch their thinking to, “What do we have to offer this person to convince them to take the job?”</p>

<p>See the difference?</p>

<p>In the first one, they’re looking for the smallest number they can get away with. In the second one, they’re looking for a number big enough to convince you to make the leap. These are probably two <em>very</em> different numbers.</p>

<p><strong>How you change their thinking</strong></p>

<p>At every stage of the interview process, <strong>your primary goal is to tell a story about how their company will be better with you on their team</strong>. Just keep telling that story over and over again.</p>

<p>Every time they ask you a question, think, “How can I answer this question in a way that adds another chapter to the story about how their company will be better if they bring me on board?”</p>

<p>If you do change their thinking from “minimum number” to “biggest number necessary”, they may forget that you told them your current salary and focus on making a compelling offer instead.</p>

<p>That may be enough to break away from your current salary, but what if you try this and still get an offer that’s based on your current salary?</p>

<blockquote>
  <p><span>Learn more</span> How to answer interview questions so you stand out</p>
  <p><a href="https://fearlesssalarynegotiation.com/interview-preparation-guide/">Interview Preparation Guide <i></i></a></p>
</blockquote>

<h3 id="2-you-already-have-a-job-offer">2. You already have a job offer</h3>

<p>You may have experienced something like this:</p>

<p>You got a job offer, and felt great because it was more than you were currently making. Then, a few minutes later you thought, “Wait a minute. This offer is just my current salary plus 5%! Is that a coincidence?”</p>

<p>Nope, it’s not a coincidence 😕</p>

<p>Once you get this far, it’s much harder to recover and drastically improve your offer. But you have a couple options based on your own assessment of your <a href="https://fearlesssalarynegotiation.com/minimum-acceptable-salary/">minimum acceptable salary</a>:</p>

<ol>
  <li>If a counter of 10-20% above their offer is above your minimum acceptable salary, then that’s your best move. You could negotiate more than your minimum or you can fall back to your minimum as your line in the sand (your walk away number).</li>
  <li>If that won’t work, you can use this <a href="https://fearlesssalarynegotiation.com/how-to-negotiate-a-lowball-job-offer/">technique to negotiate a lowball job offer</a>—ask them if they can make improvements to the offer so you can consider them. This is your way of letting them know that they need to switch their thinking from “minimum” to “most convincing”.</li>
</ol>

<p>Your options are slim, but you do have options even if you’ve already disclosed your current salary!</p>

<h2 id="summarizing-your-options-for-answering--salary-expectations-questions"><a name="summary"></a>Summarizing your options for answering  salary expectations questions</h2>

<p>We’ve covered a lot of ground here, so let me give you a short-and-sweet summary of the scripts you can use to avoid sharing your salary expectations and current salary when asked for your salary expectations in a job interview.</p>

<p>Here’s the question again:</p>

<blockquote>
  <p><em>“So where are you right now in terms of salary, and what are you looking for if you make this move?”</em></p>
</blockquote>

<h3 id="level-1-answer">Level 1 answer</h3>

<blockquote>
  <p>“I’m not comfortable sharing my current salary. I would prefer to focus on the value I can add to this company rather than what I’m paid at my current job. I don’t have a specific number in mind for a desired salary, and you know better than I do what value my skillset and experience could bring to your company. I want this move to be a big step forward for me in terms of both responsibility and compensation.”</p>
</blockquote>

<p>If they keep pushing after that longer answer, you can shorten it up and try again:</p>

<blockquote>
  <p>“I’m just not comfortable discussing my current or expected salary. I prefer to focus on the value I can add in this position, and I look forward to hearing what you think is appropriate.”</p>
</blockquote>

<h3 id="level-2-answer">Level 2 answer</h3>

<p>If the Level 1 answer doesn’t quite work, it’s time to move on to Level 2, which requires a slightly more sophisticated answer:</p>

<blockquote>
  <p>“It sounds like you’re trying to qualify me for a salary range. If you want to tell me what that range is, I’m happy to tell you if it’s in the ballpark.”</p>
</blockquote>

<p>And if <em>that</em> doesn’t work, you can reach for the trump card—the Level 3 answer…</p>

<h3 id="level-3-answer">Level 3 answer</h3>

<blockquote>
  <p>“I’m not comfortable sharing my current employer’s proprietary compensation information, and I know they wouldn’t appreciate it if I did. I still work for them, and I’m just not comfortable sharing their proprietary information about how they pay people like me. I really don’t have a specific number in mind for an expected salary, and I look forward to hearing what you suggest.”</p>
</blockquote>

<p>That should enable you to move on to the actual job interviews.</p>

<h3 id="how-to-handle-paper-and-online-job-applications">How to handle paper and online job applications</h3>

<p>Leave salary questions blank if possible. This will work for paper applications, and even for some online applications that don’t check your entries before saving the form.</p>

<p>But if you <em>must</em> fill in the fields that ask for current salary or expected salary, use fake numbers like “0” or “1” or “999999999”. This will either move you along in the process, or prompt the recruiter to ask you for your salary expectations verbally (which you’re prepared for).</p>

<h3 id="what-to-do-when-they-ask-for-w-2s-or-pay-stubs-from-previous-jobs">What to do when they ask for W-2s or pay stubs from previous jobs</h3>

<p>Don’t share them. This isn’t any of their business.</p>

<h3 id="what-to-say-when-recruiters-ask-for-your-salary-expectations-to-save-time">What to say when recruiters ask for your salary expectations to save time</h3>

<p>They’re either trying to save time, or they’re trying to gather information to give them an advantage later on when you negotiate. If they’re genuinely trying to save time, then this answer should get you through:</p>

<blockquote>
  <p>“It sounds like you’re trying to qualify me for a salary range. If you want to tell me what that range is, I’m happy to tell you if it’s in the ballpark.”</p>
</blockquote>

<h3 id="how-to-recover-when-you-already-shared-your-expected-salary">How to recover when you already shared your expected salary</h3>

<p>Look for new information you can incorporate into your negotiation. For example, the job requirements or benefits package may be different than you anticipated. Then you can counter offer while saying something like:</p>

<blockquote>
  <p>“I have learned a lot more about the company, the position, and the compensation package since I gave that initial salary range. Given what I’ve learned, I would be more comfortable at [your counteroffer].”</p>
</blockquote>

<h3 id="how-to-recover-when-you-already-shared-your-current-salary">How to recover when you already shared your current salary</h3>

<p><strong>1. You haven’t started your formal interviews yet</strong></p>

<p>There’s still time to switch their thinking to “What do we need to offer to convince them to take this role?” Focus on interviewing well and telling a story about how their company will be better if you’re a part of it.</p>

<p><strong>2. You already have a job offer</strong></p>

<p>Your best bet is to negotiate the offer as well as possible. The offer itself will be based on your current salary, but with a good negotiation strategy, you can see how much room for improvement there is in their offer.</p>

<p>Next time you’re asked for your current salary or desired in a job interview, use these word-for-word scripts to avoid disclosing that information and get back to the interview process. This strategy will leave the salary expectations question open longer, allowing you to impress them more so you have more leverage during your salary negotiation.</p>

<h2 id="articles-you-may-also-like">Articles you may also like</h2>

<ul>
  <li><a href="https://fearlesssalarynegotiation.com/salary-increase-letter-sample/">A <strong>salary increase letter</strong> to ask your boss for a raise</a></li>
  <li><a href="https://fearlesssalarynegotiation.com/amazon-salary-negotiation/">Got a job offer from Amazon? Learn how to negotiate your salary</a></li>
</ul>

<blockquote>
  <p><span>Learn more</span> Make sure you have everything you need for your next interview</p>
  <p><a href="https://fearlesssalarynegotiation.com/interview-preparation-guide/">Interview Preparation Guide <i></i></a></p>
</blockquote>

				
				   
<!-- / convertkit-form-inline -->


      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JVM Anatomy Quarks (212 pts)]]></title>
            <link>https://shipilev.net/jvm/anatomy-quarks/</link>
            <guid>42100876</guid>
            <pubDate>Sun, 10 Nov 2024 16:09:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shipilev.net/jvm/anatomy-quarks/">https://shipilev.net/jvm/anatomy-quarks/</a>, See on <a href="https://news.ycombinator.com/item?id=42100876">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<div>
<p><a href="https://shipilev.net/jvm/anatomy-quarks/">"JVM Anatomy Quarks"</a> is the on-going mini-post series, where every post is describing some elementary piece of knowledge about JVM. The name underlines the fact that the single post cannot be taken in isolation, and most pieces described here are going to readily interact with each other.</p>
<p>The post should take about 5-10 minutes to read. As such, it goes deep for only a single topic, a single test, a single benchmark, a single observation. The evidence and discussion here might be anecdotal, not actually reviewed for errors, consistency, writing 'tyle, syntaxtic and semantically errors, duplicates, or also consistency. Use and/or trust this at your own risk.</p>
<div>
<p><img src="https://shipilev.net/jvm/anatomy-quarks/images/redhat-logo.svg" alt="350" width="85">
</p>
</div>
<p><strong>Aleksey Shipilëv, JVM/Performance Geek</strong><br>
Shout out at Twitter: <a href="http://twitter.com/shipilev">@shipilev</a>; Questions, comments, suggestions: <a href="mailto:aleksey@shipilev.net">aleksey@shipilev.net</a><br></p>
</div>
<div>
<h2 id="_complete_snapshots"><a href="#_complete_snapshots"></a>Complete Snapshots</h2>
<div>
<p>The series is on-going, the auto-generated complete bundles are here:<br>
  <a href="https://shipilev.net/jvm/anatomy-quarks/jvm-anatomy-quarks-complete.epub">ePUB</a> (smallest, under MB, Pandoc HTML-to-ePUB)<br>
  <a href="https://shipilev.net/jvm/anatomy-quarks/jvm-anatomy-quarks-complete.mobi">MOBI</a> (small, around MB, KindleGen ePUB-to-MOBI)<br>
  <a href="https://shipilev.net/jvm/anatomy-quarks/jvm-anatomy-quarks-complete.pdf">PDF</a> (very large — tens of MBs, high-quality wkhtmltopdf HTML-to-PDF)<br></p>
</div>
</div>
<div>
<h2 id="_individual_index"><a href="#_individual_index"></a>Individual Index</h2>

</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Chonkie – A Fast, Lightweight Text Chunking Library for RAG (129 pts)]]></title>
            <link>https://github.com/bhavnicksm/chonkie</link>
            <guid>42100819</guid>
            <pubDate>Sun, 10 Nov 2024 15:58:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bhavnicksm/chonkie">https://github.com/bhavnicksm/chonkie</a>, See on <a href="https://news.ycombinator.com/item?id=42100819">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/bhavnicksm/chonkie/blob/main/assets/chonkie_logo_br_transparent_bg.png"><img src="https://github.com/bhavnicksm/chonkie/raw/main/assets/chonkie_logo_br_transparent_bg.png" alt="Chonkie Logo"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🦛 Chonkie ✨</h2><a id="user-content--chonkie-" aria-label="Permalink: 🦛 Chonkie ✨" href="#-chonkie-"></a></p>
<p dir="auto"><a href="https://pypi.org/project/chonkie/" rel="nofollow"><img src="https://camo.githubusercontent.com/452e0a51be3c27c61123caec3c95eff4121003d3414c4823e2f9d708df5cf1ad/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f63686f6e6b69652e737667" alt="PyPI version" data-canonical-src="https://img.shields.io/pypi/v/chonkie.svg"></a>
<a href="https://github.com/bhavnicksm/chonkie/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/bbb0131728d9e5b16df0ea54ddafd9b7acd292fe1a7dbdd6b5ca10ff308f449a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f626861766e69636b736d2f63686f6e6b69652e737667" alt="License" data-canonical-src="https://img.shields.io/github/license/bhavnicksm/chonkie.svg"></a>
<a href="https://github.com/bhavnicksm/chonkie/blob/main/DOCS.md"><img src="https://camo.githubusercontent.com/c33432f0820d849043641e7030ef16256e854b9057e32f9e925e89c049b730e0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d444f43532e6d642d626c75652e737667" alt="Documentation" data-canonical-src="https://img.shields.io/badge/docs-DOCS.md-blue.svg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9bdd8924dd495fd588f87b4b2e197068582801d703c7e86dbc1da5288a94bde0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73697a652d32314d422d626c7565"><img src="https://camo.githubusercontent.com/9bdd8924dd495fd588f87b4b2e197068582801d703c7e86dbc1da5288a94bde0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73697a652d32314d422d626c7565" alt="Package size" data-canonical-src="https://img.shields.io/badge/size-21MB-blue"></a>
<a href="https://pepy.tech/project/chonkie" rel="nofollow"><img src="https://camo.githubusercontent.com/acbdeb1ad870b7f63e2d4028a7048b963dfe24cffee50c7d5111ef4a9036f5e1/68747470733a2f2f7374617469632e706570792e746563682f62616467652f63686f6e6b6965" alt="Downloads" data-canonical-src="https://static.pepy.tech/badge/chonkie"></a>
<a href="https://github.com/bhavnicksm/chonkie/stargazers"><img src="https://camo.githubusercontent.com/2988b697e689f00638e3713a8ac19d33564c4cfb9c60e7701eec9f4b61b4dc93/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f626861766e69636b736d2f63686f6e6b69652e737667" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/bhavnicksm/chonkie.svg"></a></p>
<p dir="auto"><em>The no-nonsense RAG chunking library that's lightweight, lightning-fast, and ready to CHONK your texts</em></p>
<p dir="auto"><a href="#installation">Installation</a> •
<a href="#usage">Usage</a> •
<a href="#supported-methods">Supported Methods</a> •
<a href="#benchmarks-%EF%B8%8F">Benchmarks</a> •
<a href="#acknowledgements">Acknowledgements</a> •
<a href="#citation">Citation</a></p>
</div>
<p dir="auto">so i found myself making another RAG bot (for the 2342148th time) and meanwhile, explaining to my juniors about why we should use chunking in our RAG bots, only to realise that i would have to write chunking all over again unless i use the bloated software library X or the extremely feature-less library Y. <em>WHY CAN I NOT HAVE SOMETHING JUST RIGHT, UGH?</em></p>
<p dir="auto">Can't i just install, import and run chunking and not have to worry about dependencies, bloat, speed or other factors?</p>
<p dir="auto">Well, with chonkie you can! (chonkie boi is a gud boi)</p>
<p dir="auto"><strong>🚀 Feature-rich</strong>: All the CHONKs you'd ever need <br>
<strong>✨ Easy to use</strong>: Install, Import, CHONK <br>
<strong>⚡ Fast</strong>: CHONK at the speed of light! zooooom <br>
<strong>🌐 Wide support</strong>: Supports all your favorite tokenizer CHONKS <br>
<strong>🪶 Light-weight</strong>: No bloat, just CHONK <br>
<strong>🦛 Cute CHONK mascot</strong>: psst it's a pygmy hippo btw <br>
<strong>❤️ <a href="#acknowledgements">Moto Moto</a>'s favorite python library</strong> <br></p>
<p dir="auto">What're you waiting for, <strong>just CHONK it</strong>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To install chonkie, simply run:</p>

<p dir="auto">Chonkie follows the rule to have minimal defualt installs, read the <a href="https://github.com/bhavnicksm/chonkie/blob/main/DOCS.md">DOCS</a> to know the installation for your required chunker, or simply install <code>all</code> if you don't want to think about it (not recommended).</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Here's a basic example to get you started:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# First import the chunker you want from Chonkie 
from chonkie import TokenChunker

# Import your favorite tokenizer library
# Also supports AutoTokenizers, TikToken and AutoTikTokenizer
from tokenizers import Tokenizer 
tokenizer = Tokenizer.from_pretrained(&quot;gpt2&quot;)

# Initialize the chunker
chunker = TokenChunker(tokenizer)

# Chunk some text
chunks = chunker(&quot;Woah! Chonkie, the chunking library is so cool! I love the tiny hippo hehe.&quot;)

# Access chunks
for chunk in chunks:
    print(f&quot;Chunk: {chunk.text}&quot;)
    print(f&quot;Tokens: {chunk.token_count}&quot;)"><pre><span># First import the chunker you want from Chonkie </span>
<span>from</span> <span>chonkie</span> <span>import</span> <span>TokenChunker</span>

<span># Import your favorite tokenizer library</span>
<span># Also supports AutoTokenizers, TikToken and AutoTikTokenizer</span>
<span>from</span> <span>tokenizers</span> <span>import</span> <span>Tokenizer</span> 
<span>tokenizer</span> <span>=</span> <span>Tokenizer</span>.<span>from_pretrained</span>(<span>"gpt2"</span>)

<span># Initialize the chunker</span>
<span>chunker</span> <span>=</span> <span>TokenChunker</span>(<span>tokenizer</span>)

<span># Chunk some text</span>
<span>chunks</span> <span>=</span> <span>chunker</span>(<span>"Woah! Chonkie, the chunking library is so cool! I love the tiny hippo hehe."</span>)

<span># Access chunks</span>
<span>for</span> <span>chunk</span> <span>in</span> <span>chunks</span>:
    <span>print</span>(<span>f"Chunk: <span><span>{</span><span>chunk</span>.<span>text</span><span>}</span></span>"</span>)
    <span>print</span>(<span>f"Tokens: <span><span>{</span><span>chunk</span>.<span>token_count</span><span>}</span></span>"</span>)</pre></div>
<p dir="auto">More example usages given inside the <a href="https://github.com/bhavnicksm/chonkie/blob/main/DOCS.md">DOCS</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Methods</h2><a id="user-content-supported-methods" aria-label="Permalink: Supported Methods" href="#supported-methods"></a></p>
<p dir="auto">Chonkie provides several chunkers to help you split your text efficiently for RAG applications. Here's a quick overview of the available chunkers:</p>
<ul dir="auto">
<li><strong>TokenChunker</strong>: Splits text into fixed-size token chunks.</li>
<li><strong>WordChunker</strong>: Splits text into chunks based on words.</li>
<li><strong>SentenceChunker</strong>: Splits text into chunks based on sentences.</li>
<li><strong>SemanticChunker</strong>: Splits text into chunks based on semantic similarity.</li>
<li><strong>SDPMChunker</strong>: Splits text using a Semantic Double-Pass Merge approach.</li>
</ul>
<p dir="auto">More on these methods and the approaches taken inside the <a href="https://github.com/bhavnicksm/chonkie/blob/main/DOCS.md">DOCS</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmarks 🏃‍♂️</h2><a id="user-content-benchmarks-️" aria-label="Permalink: Benchmarks 🏃‍♂️" href="#benchmarks-️"></a></p>
<blockquote>
<p dir="auto">"I may be smol hippo, but I pack a punch!" 🦛</p>
</blockquote>
<p dir="auto">Here's a quick peek at how Chonkie performs:</p>
<p dir="auto"><strong>Size</strong>📦</p>
<ul dir="auto">
<li><strong>Default Install:</strong> 21MB (vs 80-171MB for alternatives)</li>
<li><strong>With Semantic:</strong> Still lighter than the competition!</li>
</ul>
<p dir="auto"><strong>Speed</strong>⚡</p>
<ul dir="auto">
<li><strong>Token Chunking:</strong> 33x faster than the slowest alternative</li>
<li><strong>Sentence Chunking:</strong> Almost 2x faster than competitors</li>
<li><strong>Semantic Chunking:</strong> Up to 2.5x faster than others</li>
</ul>
<p dir="auto">Check out our detailed <a href="https://github.com/bhavnicksm/chonkie/blob/main/benchmarks/README.md">benchmarks</a> to see how Chonkie races past the competition! 🏃‍♂️💨</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">Chonkie would like to CHONK its way through a special thanks to all the users and contributors who have helped make this library what it is today! Your feedback, issue reports, and improvements have helped make Chonkie the CHONKIEST it can be.</p>
<p dir="auto">And of course, special thanks to <a href="https://www.youtube.com/watch?v=I0zZC4wtqDQ&amp;t=5s" rel="nofollow">Moto Moto</a> for endorsing Chonkie with his famous quote:</p>
<blockquote>
<p dir="auto">"I like them big, I like them chonkie."
~ Moto Moto</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use Chonkie in your research, please cite it as follows:</p>
<div data-snippet-clipboard-copy-content="@misc{chonkie2024,
  author = {Minhas, Bhavnick},
  title = {Chonkie: A Fast Feature-full Chunking Library for RAG Bots},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/bhavnick/chonkie}},
}"><pre><code>@misc{chonkie2024,
  author = {Minhas, Bhavnick},
  title = {Chonkie: A Fast Feature-full Chunking Library for RAG Bots},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/bhavnick/chonkie}},
}
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Typesetting Engines: A Programmer's Perspective (110 pts)]]></title>
            <link>https://blog.ppresume.com/posts/on-typesetting-engines</link>
            <guid>42100660</guid>
            <pubDate>Sun, 10 Nov 2024 15:21:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ppresume.com/posts/on-typesetting-engines">https://blog.ppresume.com/posts/on-typesetting-engines</a>, See on <a href="https://news.ycombinator.com/item?id=42100660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><article dir="ltr"><h2 id="table-of-contents">Table of Contents<a href="#table-of-contents" aria-label="Permalink for this section"></a></h2>
<ul>
<li><a href="#translations">Translations</a></li>
<li><a href="#prologue">Prologue</a></li>
<li><a href="#the-accessment-criteria">The Accessment Criteria</a>
<ul>
<li><a href="#the-sacred-line-breaking-algorithm">The Sacred Line Breaking Algorithm</a></li>
<li><a href="#cjk-typesetting-is-complicated">CJK Typesetting is Complicated</a>
<ul>
<li><a href="#cjk-character-set-is-huge">CJK Character Set is Huge</a></li>
<li><a href="#cultural-nuances">Cultural Nuances</a></li>
<li><a href="#font-pairing">Font Pairing</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#html--css">HTML &amp; CSS</a>
<ul>
<li><a href="#line-breaking">Line Breaking</a></li>
<li><a href="#cjk">CJK</a></li>
<li><a href="#pagination">Pagination</a></li>
<li><a href="#instant-preview">Instant Preview</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li><a href="#latex">LaTeX</a>
<ul>
<li><a href="#line-breaking-1">Line Breaking</a></li>
<li><a href="#cjk-1">CJK</a></li>
<li><a href="#pagination-1">Pagination</a></li>
<li><a href="#instant-preview-1">Instant Preview</a></li>
<li><a href="#conclusion-1">Conclusion</a></li>
</ul>
</li>
<li><a href="#latexjs">LaTeX.js</a>
<ul>
<li><a href="#line-breaking-2">Line Breaking</a></li>
<li><a href="#cjk-2">CJK</a></li>
<li><a href="#pagination-2">Pagination</a></li>
<li><a href="#instant-preview-2">Instant Preview</a></li>
<li><a href="#conclusion-2">Conclusion</a></li>
</ul>
</li>
<li><a href="#typst">Typst</a>
<ul>
<li><a href="#line-breaking-3">Line Breaking</a></li>
<li><a href="#cjk-3">CJK</a></li>
<li><a href="#pagination-3">Pagination</a></li>
<li><a href="#instant-preview-3">Instant Preview</a></li>
<li><a href="#conclusion-3">Conclusion</a></li>
</ul>
</li>
<li><a href="#react-pdf">React-pdf</a>
<ul>
<li><a href="#line-breaking-4">Line Breaking</a></li>
<li><a href="#cjk-4">CJK</a></li>
<li><a href="#pagination-4">Pagination</a></li>
<li><a href="#instant-preview-4">Instant Preview</a></li>
<li><a href="#conclusion-4">Conclusion</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
<li><a href="#revision">Revision</a>
<ul>
<li><a href="#nov-8-2024">Nov 8, 2024</a></li>
<li><a href="#nov-2-2024">Nov 2, 2024</a></li>
<li><a href="#nov-1-2024">Nov 1, 2024</a></li>
</ul>
</li>
</ul>
<h2 id="translations">Translations<a href="#translations" aria-label="Permalink for this section"></a></h2>
<p>This post is available in the following translations:</p>
<ul>
<li>Simplified Chinese: <a href="https://blog.ppresume.com/posts/zh-cn/on-typesetting-engines">排版引擎纵谈：程序员的视角</a></li>
<li>Traditional Chinese TW: <a href="https://blog.ppresume.com/posts/zh-tw/on-typesetting-engines">排版引擎縱談：程式設計師的視角</a></li>
</ul>
<h2 id="prologue">Prologue<a href="#prologue" aria-label="Permalink for this section"></a></h2>
<p>Typesetting is “architecture in two dimensions.”</p>
<p>If text and its fonts are the materials of the building, then typesetting is the
drawings of the building.</p>
<p>Typesetting is a big topic, it is both an art and an engineering technique that
has evolved significantly with the advent of digital technology. Obviously I
cannot cover this topic in one post, even a book cannot do.</p>
<p>Among many typesetting concepts, the typesetting engine is one of the core
concepts. Basically, <strong>a typesetting engine is a piece of software that decides
how the glyphs, graphics, tables, etc. are laid out for printing or digital
display</strong>.</p>
<p>When <a href="https://ppresume.com/" target="_blank" rel="noreferrer">PPResume</a> was
<a href="https://blog.ppresume.com/posts/introducing-ppresume" target="_blank" rel="noreferrer">launched</a>, some people
<a href="https://discord.com/channels/1139929039314894878/1139929039751090230/1160701786630389892" target="_blank" rel="noreferrer">asked</a>
me why chose LaTeX as the default typesetting engine for PPReseume. Hmmm, this
is a big topic.</p>
<p>In this post, I would like to explore the pros and cons of some popular
typesetting engines: HTML/CSS, <a href="https://latex.js.org/" target="_blank" rel="noreferrer">LaTeX.js</a>,
<a href="https://www.latex-project.org/" target="_blank" rel="noreferrer">LaTeX</a>, <a href="https://typst.app/" target="_blank" rel="noreferrer">Typst</a>,
<a href="https://react-pdf.org/" target="_blank" rel="noreferrer">react-pdf</a> and conclude why PPResume chose LaTeX as the
default typesetting engine.</p>
<p>But before we start, let us agree on some glossaries that will be used thoughout
whole post. Yes this is a long post and it takes time and energy to read.
<em>Don’t complain to me later. I warned you here!</em></p>
<p>Glossaries:</p>
<ul>
<li><strong><a href="https://en.wikipedia.org/wiki/Indo-European_languages" target="_blank" rel="noreferrer">Indo-European
languages</a></strong>:
a language family native to the overwhelming
majority of Europe, the Iranian plateau, and the northern Indian subcontinent.
Widely spoken indo-european languages includes English, French, Portuguese,
Russian, Dutch, and Spanish, etc.</li>
<li><strong><a href="https://en.wikipedia.org/wiki/CJK_characters" target="_blank" rel="noreferrer">CJK</a></strong>: Chinese, Japanese and
Korean languages.</li>
<li><strong><a href="https://www.creatopy.com/blog/what-is-a-character-set/" target="_blank" rel="noreferrer">Character Set</a></strong>:
the complete collection of characters, symbols, glyphs, and punctuation marks
available within a specific typeface or font.</li>
<li><strong><a href="https://en.wikipedia.org/wiki/Glyph" target="_blank" rel="noreferrer">Glyph</a></strong>: the specific shape, design,
or representation of a character in typography.</li>
<li><strong><a href="https://en.wikipedia.org/wiki/Syllabification" target="_blank" rel="noreferrer">Hyphenation</a></strong>, the practice
of breaking words at the end of lines to improve the overall appearance and
readability of text.</li>
<li><strong><a href="https://en.wikipedia.org/wiki/Typographic_alignment#Justified" target="_blank" rel="noreferrer">Justification</a></strong>:
the alignment of text within a block so that it is flush with both the left and
right margins, generally achieved by adjusting the spacing between words and
letters, creating a uniform appearance across each line of text.</li>
<li><strong><a href="https://www.myfonts.com/pages/fontscom-learning-fontology-level-2-text-typography-rags-widows-orphans" target="_blank" rel="noreferrer">Rag</a></strong>:
the uneven or irregular alignment of text along one margin of a block of text.
This typically occurs when text is aligned to one side (either left or right),
resulting in the opposite side appearing “ragged” or uneven.</li>
</ul>
<h2 id="the-accessment-criteria">The Accessment Criteria<a href="#the-accessment-criteria" aria-label="Permalink for this section"></a></h2>
<p>Each typesetting engine has its strengths and weaknesses, catering to different
needs and preferences. Web-based typesetting with HTML/CSS is extremely flexible
and <a href="https://alistapart.com/article/responsive-web-design/" target="_blank" rel="noreferrer">responsive</a>, ideal
for <a href="https://en.wikipedia.org/wiki/Search_engine_optimization" target="_blank" rel="noreferrer">SEO</a> and
interactive content. LaTeX.js provides a bridge between the web and LaTeX, while
LaTeX itself is the gold standard for academic and high-precision typesetting.
Typst is considered as a modern, improved LaTeX alternative. React-pdf allows
dynamic PDF generation with <a href="https://react.dev/" target="_blank" rel="noreferrer">react</a>. The choice of
typesetting engine depends very much on the specific requirements of the
project.</p>
<p>I am not a designer so I cannot talk too much about typesetting from the
perspective of art. Instead, I want to discuss some technical things about
typesetting engines from a programmer’s perspective. Meanwhile, this post is not
an academic benchmarking report, so I won’t evaluate every aspect of typesetting
engines. Instead, <strong>I will give some assessment criteria based on PPResume’s
requirements.</strong></p>
<p>When I wrote the first line code for PPResume, I’ve set 2 goals:</p>
<ul>
<li>it must produce top notch, high quality PDF</li>
<li>it must provide native support for multi languages</li>
</ul>
<p>To produce top notch, high quality PDF, the typesetting engine must have a top
tier <a href="https://en.wikipedia.org/wiki/Line_wrap_and_word_wrap" target="_blank" rel="noreferrer">line breaking
algorithm</a>, and to
provide native support for multi languages, the typesetting engine must support
languages with a huge character set (such as Chinese, Japanese and Korean, aka
CJK). Let us evaluate these two criteria before we dive into specific
typesetting engines.</p>
<p>Wait a minute, I almost forgot, to produce a PDF <strong>the typesetting engine must
support pagination</strong>. You may ask: is there any typesetting engine that does
not support pagination? The answer is neither a yes nor a no, depending on
whether you consider HTML &amp; CSS to be a typesetting engine. We will talk more
about this later when we talk about HTML &amp; CSS.</p>
<p>Finally, it would be better if PPResume could have an excellent user experience,
of all possible features I believe <strong>instant preview is the most wanted one</strong>.</p>
<p>In a nutshell, I will judge a typesetting engine by checking whether it meets
the following accessment criteria:</p>
<ol>
<li>Knuth Plass line breaking algorithm</li>
<li>CJK typesetting</li>
<li>Pagination</li>
<li>Instant Preview</li>
</ol>
<h3 id="the-sacred-line-breaking-algorithm">The Sacred Line Breaking Algorithm<a href="#the-sacred-line-breaking-algorithm" aria-label="Permalink for this section"></a></h3>
<p>Line breaking algorithms are one of the core techniques used in typesetting
engines. They play a crucial role in determining how text is arranged on a page
or screen.</p>
<p>The primary purpose of a line breaking algorithm is to determine the optimal
points at which to break lines of text in a paragraph. Line breaking algorithms
are essential to digital typesetting and form a core component of any system
that needs to present text in a visually appealing and readable format.</p>
<p>There are 3 key metrics that are used to assess the quality of a line breaking
algorithm:</p>
<ol>
<li><strong>Justification</strong>: line breaking algorithms work in conjunction with
<a href="https://en.wikipedia.org/wiki/Typographic_alignment#Justified" target="_blank" rel="noreferrer">justification</a>
techniques to create evenly spaced lines of text.</li>
<li><strong>Hyphenation</strong>: many advanced algorithms incorporate hyphenation to improve
line breaks, especially for languages with long words.</li>
<li><strong>Optimization</strong>: the algorithm typically tries to minimize unsightly gaps or
overly tight spacing between words across an entire paragraph.</li>
</ol>
<p>There are <a href="https://en.wikipedia.org/wiki/Line_wrap_and_word_wrap#Algorithm" target="_blank" rel="noreferrer">two
categories</a> of
line breaking algorithms:</p>
<ol>
<li><strong>Minimum number of lines</strong>: a gready algorithm that puts as many words on a
line as possible, then moving on to the next line to do the same until there
are no more words left to place. This method is used by many modern word
processors, such as <a href="https://www.libreoffice.org/discover/writer/" target="_blank" rel="noreferrer">LibreOffice
Writer</a> and Microsoft Word.</li>
<li><strong>Minimum raggedness</strong>: a dynamic programming algorithm, firstly used in TeX,
minimizes the sum of the squares of the lengths of the spaces at the end of
lines to <strong>produce a more aesthetically pleasing result than the greedy
algorithm</strong>, which does not always minimize squared space.</li>
</ol>
<p>Technically speaking, the minimum number of lines algorithm has faster speed, while
the minimum raggedness algorithm produces more visually pleasing result. Let me
show you an example here, in the following image, the top half is a
<a href="https://blog.ppresume.com/static/resources/on-typesetting-engines/knuth-plass-line-breaking-algorithm-demo.odt">LibreOffice
document</a>,
using the “minimum number of lines” approach , while the bottom half is a <a href="https://blog.ppresume.com/static/resources/on-typesetting-engines/knuth-plass-line-breaking-algorithm-demo.pdf">PDF
document</a>
generated by TeX using the “minimum raggedness” approach. You can very easily
see that the bottom half PDF looks less ragged on the right margin and more
visually appealing simply because the line breaking is more balanced and
justified.</p>
<p><img alt="Knuth Plass Line Breaking Algorithm" loading="lazy" width="2560" height="2830" decoding="async" data-nimg="1" srcset="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fknuth-plass-line-breaking-algorithm-demo-with-libreoffice-and-tex.1f37a0c4.webp&amp;w=3840&amp;q=75 1x" src="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fknuth-plass-line-breaking-algorithm-demo-with-libreoffice-and-tex.1f37a0c4.webp&amp;w=3840&amp;q=75"></p>
<p>Among all line breaking algorithms, the <a href="https://en.wikipedia.org/wiki/Knuth%E2%80%93Plass_line-breaking_algorithm" target="_blank" rel="noreferrer">Knuth Plass line breaking
algorithm</a>
is the gold standard for minimum raggedness approach. It is widely adopted by
various typesetting engines like <a href="https://en.wikipedia.org/wiki/TeX" target="_blank" rel="noreferrer">TeX</a>,
<a href="https://sile-typesetter.org/" target="_blank" rel="noreferrer">SILE</a> and
<a href="https://github.com/typst/typst/discussions/626" target="_blank" rel="noreferrer">Typst</a>, etc.</p>
<p>Back to PPResume’s case, one of the design goals for PPResume is to produce top
notch, high quality PDF, so the chosen typesetting engine must have a more
visually appealing line breaking algorithm, that being said, the typesetting
engine must adopt Knuth Plass line breaking algorithm.</p>
<h3 id="cjk-typesetting-is-complicated">CJK Typesetting is Complicated<a href="#cjk-typesetting-is-complicated" aria-label="Permalink for this section"></a></h3>
<p>Typesetting for <a href="https://en.wikipedia.org/wiki/CJK_characters" target="_blank" rel="noreferrer">CJK</a> (Chinese,
Japanese, and Korean) languages is generally considered to be more complicated
than Indo-European languages. Here is a classic
<a href="https://github.com/koreader/koreader/issues/6162" target="_blank" rel="noreferrer">discussion</a> from the
<a href="https://github.com/koreader/koreader" target="_blank" rel="noreferrer">koreader</a> project. There are several
reasons for this.</p>
<p>TL;DR: if you don’t want to delve into the details, you can check out the
following <a href="https://www.w3.org/" target="_blank" rel="noreferrer">W3C</a> draft notes to get an intuitive sense of
the complexity of typesetting requirements for CJK:</p>
<ul>
<li><a href="https://www.w3.org/TR/clreq/" target="_blank" rel="noreferrer">Requirements for Chinese Text Layout 中文排版需求</a></li>
<li><a href="https://www.w3.org/TR/jlreq/" target="_blank" rel="noreferrer">Requirements for Japanese Text Layout 日本語組版処理の要件（日本語版）
</a></li>
<li><a href="https://www.w3.org/TR/klreq/" target="_blank" rel="noreferrer">Requirements for Hangul Text Layout and Typography : 한국어 텍스트 레이아웃 및 타이포그래피를
위한 요구사항</a> to</li>
</ul>
<h4 id="cjk-character-set-is-huge">CJK Character Set is Huge<a href="#cjk-character-set-is-huge" aria-label="Permalink for this section"></a></h4>
<p>The root cause for this complexity is that the size of the character set for CJK
languages is much more larger than Indo-European languages. According to the
<a href="https://en.wikipedia.org/wiki/CJK_Unified_Ideographs" target="_blank" rel="noreferrer">CJK Unified Ideographs</a>,
as of Unicode 16.0, Unicode defines a total of 97,680 characters. <strong>This is
insanely huge</strong>. In contrast, Indo-European languages typically use the Latin
alphabet, which has a few hundred characters, much smaller than CJK. Hmmmm, 100k
characters, even creating a font that covers all of them is a huge amount of
work, labor-intensive and very expensive.</p>
<p><img alt="CJK Characters" loading="lazy" width="2500" height="2850" decoding="async" data-nimg="1" srcset="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FThe_old_man_is_72_years_old_final.9a5a2d12.webp&amp;w=3840&amp;q=75 1x" src="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FThe_old_man_is_72_years_old_final.9a5a2d12.webp&amp;w=3840&amp;q=75"></p>
<p>Taking PPResume as an example, we met two issues
(<a href="https://github.com/ppresume/community/issues/33" target="_blank" rel="noreferrer">1</a>,
<a href="https://github.com/ppresume/community/issues/63" target="_blank" rel="noreferrer">2</a>) where the fonts
recommended by <a href="https://ctan.org/pkg/ctex?lang=en" target="_blank" rel="noreferrer">CTeX</a> are missing some
characters. Unlike Indo-European languages, there are very few fonts that have
full coverage of the entire CJK character set, and most of them are commercial—
<a href="https://fonts.google.com/noto" target="_blank" rel="noreferrer">Noto</a> is one of the few exceptions that both has
good coverage of <a href="https://github.com/notofonts/noto-cjk" target="_blank" rel="noreferrer">CJK</a> characters and is
free to use.</p>
<h4 id="cultural-nuances">Cultural Nuances<a href="#cultural-nuances" aria-label="Permalink for this section"></a></h4>
<p>Each CJK language has its own set of typographic conventions that must be
followed, and these can vary greatly from culture to culture and context to
context. For example, punctuation placement and spacing rules differ between
Chinese, Japanese, and Korean texts. It is hard to imagine that the <a href="https://en.wikipedia.org/wiki/Quotation_mark" target="_blank" rel="noreferrer">quotation
mark</a> is used with completely
<a href="https://en.wikipedia.org/wiki/Quotation_mark#Chinese,_Japanese_and_Korean" target="_blank" rel="noreferrer">different
conventions</a>
in CJK:</p>
<blockquote>
<p>In Japan, corner brackets are used.</p>
<p>In South Korea, corner brackets and English-style quotes are used.</p>
<p>In North Korea, angle quotes are used.</p>
<p>In mainland China, English-style quotes (full width “ ”) are official and
prevalent; corner brackets are rare today. The Unicode code points used are the
English quotes (rendered as fullwidth by the font), not the fullwidth forms.</p>
<p>In Taiwan, Hong Kong and Macau, where traditional characters are used, corner
brackets are prevalent, although English-style quotes are also used.</p>
<p>In the Chinese language, double angle brackets are placed around titles of
books, documents, movies, pieces of art or music, magazines, newspapers, laws,
etc. When nested, single angle brackets are used inside double angle brackets.
With some exceptions, this usage parallels the usage of italics in English:</p>
<p>「你看過《三國演義》嗎？」他問我。</p>
<p>“Have you read Romance of the Three Kingdoms?”, he asked me.</p>
</blockquote>
<h4 id="font-pairing">Font Pairing<a href="#font-pairing" aria-label="Permalink for this section"></a></h4>
<p>When mixing CJK with other Indo-European languages, things become more
complicated.</p>
<p>Firstly, punctuations are different. For example, the
<a href="https://en.wikipedia.org/wiki/Comma" target="_blank" rel="noreferrer">comma</a> has different forms in Chinese and
English:</p>
<blockquote>
<p>English uses the comma  as a separator to separate parts of a sentence and
items in a list, while Chinese uses a Chinese comma  to separate
sensences, and a dedicated enumeration comma (顿号, ) to separate items in
a list (e.g. keyword &gt; list).</p>
<p>— <a href="https://ppresume.com/posts/multi-languagues-support#punctuations" target="_blank" rel="noreferrer">Multi Languages Support</a></p>
</blockquote>
<p>Meanwhile, a Latin font for Indo-European languages may cover only one thousand
glyphs, whereas a CJK font must cover at least thousands of glyphs, as mentioned
above.</p>
<p>Effective typesetting often requires CJK fonts to be paired with Latin fonts to
maintain visual consistency. This can be challenging as it requires combined
fonts that intelligently switch between character sets.</p>
<blockquote>
<p>So Chinese, Japanese and Korean fonts tend to be developed by Asian designers,
with an understandable emphasis on the elegance of the Asian characters.
Unfortunately this can be at the expense of the design of the Latin letters,
which may in some cases be really quite ugly.</p>
<p>The solution? Use an attractive Latin-script font for any Latin letters and
numbers, and an Asian font for the Chinese, Japanese or Korean characters.
Rather than making the poor typesetter manually change the font each time a
Latin letter or number appears, applications such as InDesign allow Combined
Fonts to be set within a document which intelligently switch the font according
to the nature of each letter or character.</p>
<p>— <a href="https://asianabsolute.co.uk/blog/typesetting-conventions-best-practices-chinese-japanese-korean/" target="_blank" rel="noreferrer">Typesetting conventions and best practices for CJK (Chinese, Japanese, Korean)</a></p>
</blockquote>
<p>Not all typesetting engines have built-in support for font pairing but this is
essential for PPResume to provide native support for multi languages.</p>
<p>In summary, the insanely huge size of CJK character sets, cultural nuances and
technical challenges contribute to the greater complexity of typesetting CJK
languages compared to Indo-European languages.</p>
<h2 id="html--css">HTML &amp; CSS<a href="#html--css" aria-label="Permalink for this section"></a></h2>
<p>Technically speaking, <a href="https://en.wikipedia.org/wiki/HTML" target="_blank" rel="noreferrer">HTML</a> (Hypertext
Markup Language) is not a typesetting engine, but a markup language used to
create the structure and content of web pages. It’s designed to define the
structure of a document, such as headings, paragraphs, lists, and links, and so
on.</p>
<p>While HTML can indirectly influence how text appears on a page (e.g. by using
the obsolete
<a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/font" target="_blank" rel="noreferrer">font</a> tags), it
cannot handle the complex tasks of typesetting, such as:</p>
<ul>
<li><strong>Font selection</strong>: HTML doesn’t have built-in mechanisms for selecting
specific fonts.</li>
<li><strong>Text formatting</strong>: HTML can control some aspects of text formatting (e.g.,
bold, italic, etc.), however, it cannot provide the granular control offered
by typesetting engines.</li>
<li><strong>Hyphenation</strong>: HTML doesn’t handle hyphenation.</li>
<li><strong>Pagination</strong>: HTML is not designed for pagination.</li>
</ul>
<p>HTML itself is cannot function as a typesetting engine, however, HTML &amp;
<a href="https://en.wikipedia.org/wiki/CSS" target="_blank" rel="noreferrer">CSS</a> (Cascading Style Sheets) together can
be considered as a rudimentary typesetting engine.</p>
<p>Although not as sophisticated as dedicated typesetting engines such as LaTeX or
<a href="https://en.wikipedia.org/wiki/Adobe_InDesign" target="_blank" rel="noreferrer">InDesign</a>, HTML &amp; CSS provide a
flexible way to control the layout and appearance of text on web pages.</p>
<ul>
<li>HTML is used to define the structure of the content, such as headings,
paragraphs, and lists.</li>
<li>CSS is used to style the HTML elements, controlling
aspects like:<!-- -->
<ul>
<li>Font selection: specify fonts, font sizes, and font styles.</li>
<li>Text formatting: control line spacing, letter spacing, text alignment, and
more.</li>
<li>Layout: create complex layouts using techniques like
<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/float" target="_blank" rel="noreferrer">float</a>,
<a href="https://css-tricks.com/snippets/css/a-guide-to-flexbox/" target="_blank" rel="noreferrer">flexbox</a>, and
<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/grid" target="_blank" rel="noreferrer">grid</a>.</li>
</ul>
</li>
</ul>
<p>By combining HTML &amp; CSS, you can achieve a wide range of text formatting and
layout effects. However, for more advanced typesetting tasks, such as complex
mathematical equations or precise control over typography, dedicated typesetting
engines may be more appropriate.</p>
<p>There are many resume builders on the market which use the HTML &amp; CSS as their
typesetting engine. Most are commercial, with only a few being free or open
source:</p>
<table><thead><tr><th>Website</th><th>Technique</th><th>Type</th></tr></thead><tbody><tr><td><a href="https://resume.io/" target="_blank" rel="noreferrer">https://resume.io</a></td><td>HTML Canvas</td><td>Commercial</td></tr><tr><td><a href="https://flowcv.com/" target="_blank" rel="noreferrer">https://flowcv.com/</a></td><td>HTML &amp; CSS</td><td>Commercial</td></tr><tr><td><a href="https://www.visualcv.com/" target="_blank" rel="noreferrer">https://www.visualcv.com/</a></td><td>HTML &amp; CSS</td><td>Commercial</td></tr><tr><td><a href="https://standardresume.co/" target="_blank" rel="noreferrer">https://standardresume.co/</a></td><td>HTML &amp; CSS</td><td>Commercial</td></tr><tr><td><a href="https://zety.com/" target="_blank" rel="noreferrer">https://zety.com/</a></td><td>HTML &amp; CSS</td><td>Commercial</td></tr><tr><td><a href="https://rxresu.me/" target="_blank" rel="noreferrer">https://rxresu.me</a></td><td>HTML &amp; CSS</td><td>Free &amp; open source</td></tr></tbody></table>
<p>On the one hand, from a business perspective, given the market is so crowded,
it is not wise for me to create another resume builder that uses HTML &amp; CSS as
the typesetting engine.</p>
<p>On the other hand, from a engineering perspective, HTML &amp; CSS does not implement
Knuth Plass line breaking algorithm, so it cannot meet PPResume’s needs.</p>
<h3 id="line-breaking">Line Breaking<a href="#line-breaking" aria-label="Permalink for this section"></a></h3>
<p>In fact, standard CSS do provide some options for adjusting text justification:</p>
<ul>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/text-align" target="_blank" rel="noreferrer"></a>:
sets the horizontal alignment of the inline-level content inside a block
element or table-cell box.</li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/text-wrap" target="_blank" rel="noreferrer"></a>:
controls how text inside an element is wrapped</li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/word-break" target="_blank" rel="noreferrer"></a>:
sets whether line breaks appear wherever the text would otherwise overflow its
content box.</li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/hyphens" target="_blank" rel="noreferrer"></a>:
specifies how words should be hyphenated when text wraps across multiple
lines.</li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/hanging-punctuation" target="_blank" rel="noreferrer"></a>:
specifies whether a punctuation mark should hang at the start or end of a line
of text</li>
</ul>
<p>Firefox even provides a
<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/text-justify" target="_blank" rel="noreferrer"></a>
option to set what type of justification should be applied to text when
text-align: justify; is set on an element, however, this option is only
available on Firefox.</p>
<p>However none of them apply proper hyphenation, so they
<a href="https://chriscummins.cc/2013/typesetting/" target="_blank" rel="noreferrer">cannot</a> produce the same visually
appealing result as a real Knuth Plass line breaking algorithm—Hacker News has a
valuable <a href="https://news.ycombinator.com/item?id=1134342" target="_blank" rel="noreferrer">discussion</a> about why
modern browsers are too lazy to implement the Knuth Plass line breaking
algorithm.</p>
<p>There are also a few JavaScript implementations for the Knuth-Plass linebreaking
algorithm, but none of them seems to be production ready:</p>
<ul>
<li><a href="https://github.com/bramstein/typeset" target="_blank" rel="noreferrer">https://github.com/bramstein/typeset</a></li>
<li><a href="https://github.com/robertknight/tex-linebreak" target="_blank" rel="noreferrer">https://github.com/robertknight/tex-linebreak</a></li>
</ul>
<h3 id="cjk">CJK<a href="#cjk" aria-label="Permalink for this section"></a></h3>
<p>HTML &amp; CSS—or the browser, provides support for CJK, that’s for sure, otherwise
the browser couldn’t be the world’s most widely adopted information platform on
the world. However, this doesn’t mean that every page containing CJK follows
typesetting best practices.</p>
<p>For example, it is highly recommended to put some space between CJK and Western
characters, plain HTML &amp; CSS cannot do this automatically—this needs the help of
JavaScript.</p>
<p>In general, it takes extra effort in order to follow best practices for CJK
typesetting in the browser. As mentioned above, <a href="https://www.w3.org/TR/clreq/" target="_blank" rel="noreferrer">Requirements for Chinese Text
Layout 中文排版需求</a> is a pretty good and authoritative
reference, and one of the authors, <a href="https://yijun.me/" target="_blank" rel="noreferrer">Chen Yijun</a>, has
published an open source project called <a href="https://hanzi.pro/" target="_blank" rel="noreferrer">Han</a> which provides
a pretty nice implementation if you want to typeset CJK with best practices.</p>
<p><img alt="Han.css" loading="lazy" width="500" height="500" decoding="async" data-nimg="1" srcset="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftypeset.44d8b8f7.webp&amp;w=640&amp;q=75 1x, https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftypeset.44d8b8f7.webp&amp;w=1080&amp;q=75 2x" src="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftypeset.44d8b8f7.webp&amp;w=1080&amp;q=75"></p>

<p>HTML &amp; CSS is not designed for paginated documents, though with the help of
JavaScript, it can simulate paginated documents
(<a href="https://github.com/Renovamen/oh-my-cv" target="_blank" rel="noreferrer">oh-my-cv</a> provides a good reference
<a href="https://github.com/Renovamen/oh-my-cv/blob/main/packages/vue-smart-pages/src/useSmartPages.ts" target="_blank" rel="noreferrer">implementation</a>).
HTML’s documents are essentially
<a href="https://alistapart.com/article/responsive-web-design/" target="_blank" rel="noreferrer">responsive</a>, flow like
water, can adapt
<a href="https://developer.mozilla.org/en-US/docs/Glossary/Viewport" target="_blank" rel="noreferrer">viewports</a> of any
size.</p>
<h3 id="instant-preview">Instant Preview<a href="#instant-preview" aria-label="Permalink for this section"></a></h3>
<p>HTML &amp; CSS can have instant preview if the resume generation process only
happens only on the client side, otherwise, if it happens on the server side,
there would be a round trip time from request to response and hence no instant
preview.</p>
<h3 id="conclusion">Conclusion<a href="#conclusion" aria-label="Permalink for this section"></a></h3>
<p>Before we conclude, I couldn’t resist showing you an excellent
<a href="https://latex.vercel.app/" target="_blank" rel="noreferrer">example</a> of how HTML &amp; CSS typesetting can be pushed
to its limit. It uses  and  to get an
optimal, aligned layout for paragraphs. This is almost the best that HTML &amp; CSS
can do. If you ever want to do some typesetting with HTML &amp; CSS, this would be
a very good reference.</p>
<p>In summary, while it is theoretically possible to get a top typesetting for HTML
&amp; CSS, just as dedicated typesetting engines, the effort would be enormous and
they may also be browser compatibility issues. So, for the time being at least,
if top notch typesetting is required, it is still recommended to use a dedicated
typesetting engine instead of tuning HTML &amp; CSS hand by hand.</p>
<ul>
<li>Pros<!-- -->
<ul>
<li><strong>Universal accessibility</strong>: HTML &amp; CSS is the backbone of the web, making it
accessible on any device with a browser.</li>
<li><strong>Responsive</strong>: HTMl &amp; CSS is responsive and can adapt to viewport of any
size</li>
<li><strong>Flexible</strong>: HTML &amp; CSS is extremely flexible, it is programmable with rich
set of standard APIs</li>
<li><strong>Instant preview</strong>: HTML &amp; CSS supports instant preview</li>
</ul>
</li>
<li>Cons<!-- -->
<ul>
<li><strong>Limited control over typesetting</strong>: compared to dedicated typesetting
engines, HTML/CSS offers less control over fine typographic details.</li>
<li><strong>Browser compatibility</strong>: different browsers may render same HTML &amp; CSS
different, making it challenging to keep consistency across devices.</li>
<li><strong>No native pagination</strong>: HTML &amp; CSS is not designed for paginated
documents, hence it does not provide first class utility to export to PDF</li>
<li><strong>Poor line breaking</strong>: as mentioned, HTML &amp; CSS do not implement Knuth
Plass line breaking algorithm</li>
<li><strong>Extra effort needed for CJK typesetting</strong>: HTML &amp; CSS needs extra
libraries and effort in order to follow CJK best typesetting practices</li>
</ul>
</li>
</ul>
<h2 id="latex">LaTeX<a href="#latex" aria-label="Permalink for this section"></a></h2>
<p><a href="https://en.wikipedia.org/wiki/TeX" target="_blank" rel="noreferrer">TeX</a> is a typesetting system created by
<a href="https://www-cs-faculty.stanford.edu/~knuth/" target="_blank" rel="noreferrer">Donald Knuth</a> in the late 1970s.
It is designed for the creation of high quality typeset documents, particularly
those containing complex mathematical and scientific notation. TeX is a
low-level system that requires the user to write commands in a specific language
to format documents. It has its own set of rules and macros for formatting text,
and it is highly customizable and extensible.</p>
<p><a href="https://www.latex-project.org/" target="_blank" rel="noreferrer">LaTeX</a>, on the other hand, is a document
preparation system that is built on top of TeX. It was created by <a href="https://www.lamport.org/" target="_blank" rel="noreferrer">Leslie
Lamport</a> in the early 1980s to simplify the document
preparation process. LaTeX provides a set of higher-level macros on top of TeX’s
lower-level programming language, making it more easier and intuitive to use.</p>
<p>One of the most frequently asked questions is, why use LaTeX instead of a word
processors like Microsoft Word? The TL;DR answer is: “for beauty”.
<a href="https://nitens.org/w/" target="_blank" rel="noreferrer">Dario</a> wrote an excellent post <a href="https://nitens.org/w/latex/" target="_blank" rel="noreferrer">The Beauty of
LaTeX</a> with dozens of examples showing the
nitty-gritty typesetting details between Microsoft Word and LaTeX. No need for
me to repeat here.</p>
<p>In summary, for professional typesetting, LaTeX excels in the following
features:</p>
<ul>
<li>line breaking with justification and hyphenation</li>
<li>advanced font features like kerning, ligature, small caps, etc.</li>
<li>mathematical formulas</li>
<li>programmable and extensable</li>
<li>consistency and stability</li>
<li>cross platform compatibility</li>
</ul>
<h3 id="line-breaking-1">Line Breaking<a href="#line-breaking-1" aria-label="Permalink for this section"></a></h3>
<p>TeX has the golden line breaking algorithm—the Knuth Plass line breaking
algorithm. After all Knuth is the author of TeX, right?</p>
<p>As mentioned above, the Knuth Plass line breaking algorithm does its best to
produce a more aesthetically pleasing result by reducing the raggedness to
minimum.</p>
<p>Under the hood, the Knuth Plass line breaking algorithm uses a “total-fit”
line breaking algorithm, in contrast to the “first-fit” approach used by many
other systems. This means:</p>
<ul>
<li>it considers all possible breakpoints in a paragraph simultaneously</li>
<li>it optimizes the layout globally across the entire paragraph</li>
<li>it can adjust earlier line breaks based on their effects on later lines</li>
</ul>
<p>This allows TeX to produce more visually appealing and balanced paragraphs
overall.</p>
<p>Meanwhile, unlike many systems that treat hyphenation separately, TeX’s line
breaking algorithm integrates hyphenation decisions directly. This allows for
more optimal placement of hyphens in the context of the entire paragraph.</p>
<p>Overall, TeX’s line breaking algorithm is considered one of the most
sophisticated and effective approaches to typesetting, and its core principles
continue to influence modern typesetting systems and remain at the forefront of
high-quality digital typography.</p>
<h3 id="cjk-1">CJK<a href="#cjk-1" aria-label="Permalink for this section"></a></h3>
<p>Regarding to CJK typesetting, LaTeX has pretty good support for CJK with the
help of some new engines and some packages:</p>
<ul>
<li>new engines like <a href="https://www.luatex.org/" target="_blank" rel="noreferrer">LuaTeX</a> and
<a href="https://tug.org/xetex/" target="_blank" rel="noreferrer">XeTeX</a></li>
<li>packages like <a href="https://ctan.org/pkg/xecjk?lang=en" target="_blank" rel="noreferrer">xeCJK</a>,
<a href="https://ctan.org/pkg/ctex?lang=en" target="_blank" rel="noreferrer">CTeX</a> and
<a href="https://ctan.org/pkg/luatexja?lang=en" target="_blank" rel="noreferrer">LuaTeX-ja</a></li>
</ul>
<p>For example, xeCJK package provide following commands to set fonts for CJK:</p>
<ul>
<li>: setting CJK fonts for the serif family of body text</li>
<li>: setting CJK fonts for the sans family of body text</li>
<li>: setting CJK fonts for the monospace family</li>
</ul>
<p>xeCJK also provides options for specifying punctuation styles for CJK, spacing
between CJK and non-CJK characters, etc.</p>
<p>Overall LaTeX’s CJK support is now quite mature, although it may take some time
to set up in different environments. Here’s a manual page from <a href="https://xml.web.cern.ch/XML/lgc2/xetexmain.pdf" target="_blank" rel="noreferrer">The XeTeX
Companion TEX meets OpenType and
Unicode</a>, you can get a glance
of XeTeX’s ability for CJK typesetting.</p>
<p><img alt="XeTeX for CJK" loading="lazy" width="2480" height="3188" decoding="async" data-nimg="1" srcset="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxetexmain.06587f4c.webp&amp;w=3840&amp;q=75 1x" src="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxetexmain.06587f4c.webp&amp;w=3840&amp;q=75"></p>

<p>LaTeX is designed from ground up for typesetting paginated documents, so yes it
has excellent support for pagination, you can easily adjust paper size,
orientation, margins, etc.</p>
<p>Check the <a href="https://ctan.org/pkg/geometry?lang=en" target="_blank" rel="noreferrer">geometry</a> package for details.</p>
<h3 id="instant-preview-1">Instant Preview<a href="#instant-preview-1" aria-label="Permalink for this section"></a></h3>
<p>LaTeX by default runs on the server side so there would be a round trip time
from the request to generate the PDF to the response for the generated PDF.</p>
<p>Using LaTeX as the typesetting engine means that we’re losing the ability for
instant preview. However there do have ways to mitigate this. The magic is
<a href="https://webassembly.org/" target="_blank" rel="noreferrer">WebAssembly</a>.</p>
<p>There’s some effort that goes into compiling LaTeX to WebAssembly (aka wasm) so
that it can run purely in a browser:</p>
<ul>
<li><a href="https://manuels.github.io/texlive.js/" target="_blank" rel="noreferrer">texlive.js</a>: the initial effort to
compile LaTeX to wasm, only support
<a href="https://www.tug.org/applications/pdftex/" target="_blank" rel="noreferrer">pdfTeX</a> engine</li>
<li><a href="https://www.swiftlatex.com/" target="_blank" rel="noreferrer">SwiftLaTeX</a>, a recent, modern trial to make
LaTeX Engines run in Browsers, support XeTeX with CJK.</li>
<li><a href="https://github.com/let-def/texpresso" target="_blank" rel="noreferrer">TeXpresso</a>: live rendering and error
reporting for LaTeX, check its
<a href="https://github.com/let-def/texpresso?tab=readme-ov-file#Screencasts" target="_blank" rel="noreferrer">screencasts</a>
for demo</li>
</ul>
<p>Although none of the above are actively maintained though, it is theoretically
possible to run LaTeX purely in a browser. This would drastically reduce the
round-trip time from browser to server, and we could get instant previews then.</p>
<h3 id="conclusion-1">Conclusion<a href="#conclusion-1" aria-label="Permalink for this section"></a></h3>
<p>Before concluding, I would like to share a bit of off-topic information here.
There are a very few choices for LaTeX based resume builders on the market:</p>
<ul>
<li><a href="https://resumepuppy.com/" target="_blank" rel="noreferrer">https://resumepuppy.com/</a>: the only commercial resume builders that use LaTeX
as far as I know, they declare that they have been trusted by 100,000+
professionals &amp; students.</li>
<li><a href="https://resumake.io/" target="_blank" rel="noreferrer">https://resumake.io/</a>: the open source one, with more than 3k stars on github.</li>
</ul>
<p>From a business perspective, this is a niche market and not too crowded, so it
might be worthwhile for me to create another LaTeX based resume builder.</p>
<p>OK time to conclude LaTeX.</p>
<ul>
<li>Pros<!-- -->
<ul>
<li><strong>Precision and control</strong>: LaTeX offers unparalleled control over document
layout and typography.</li>
<li><strong>Golden line breaking</strong>: Knuth Plass line breaking algorithm is the golden
standard for optimized line breaking, and it is invented by TeX authors</li>
<li><strong>Extensive support for CJK</strong>: there’re A vast collection of packages
that extends LaTeX’s capabilities for CJK support.</li>
</ul>
</li>
<li>Cons<!-- -->
<ul>
<li><strong>Steeper Learning Curve</strong>: LaTeX has a higher barrier to entry for new
users compared to WYSIWYG editors.</li>
<li><strong>No instant preview</strong>: by default LaTeX need a compilation process on
server and hence no instant preview.</li>
<li><strong>Old and arcane developer experience</strong>: LaTeX’s compilation log is
sometimes unreadable that can only be debugged with binary search approach</li>
</ul>
</li>
</ul>
<h2 id="latexjs">LaTeX.js<a href="#latexjs" aria-label="Permalink for this section"></a></h2>
<p><a href="https://latex.js.org/" target="_blank" rel="noreferrer">LaTeX.js</a> is a LaTeX to HTML5 translator that aims to
render LaTeX documents directly in the browser without the need for server-side
processing.</p>
<p>It provides a very impressive
<a href="https://latex.js.org/playground.html" target="_blank" rel="noreferrer">playground</a>, where on the left you can
enter some LaTeX code, on the right it will render the LaTeX code into a pretty
nice HTML document.</p>
<p><img alt="LaTeX.js Playground" loading="lazy" width="3216" height="2090" decoding="async" data-nimg="1" srcset="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flatex-js-playground.060bd789.webp&amp;w=3840&amp;q=75 1x" src="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flatex-js-playground.060bd789.webp&amp;w=3840&amp;q=75"></p>
<h3 id="line-breaking-2">Line Breaking<a href="#line-breaking-2" aria-label="Permalink for this section"></a></h3>
<p>LaTeX.js does not use Knuth Plass line breaking but instead uses  to minimize the raggedness for paragraphs.</p>
<p>Meanwhile, it also uses <a href="https://en.wikipedia.org/wiki/Soft_hyphen" target="_blank" rel="noreferrer">soft
hyphen</a>  to facilitate with
 for better line breaking.</p>
<p>Although these techniques produce much better visual result than normal HTML, it
is still not true Knuth Plass line breaking.</p>
<h3 id="cjk-2">CJK<a href="#cjk-2" aria-label="Permalink for this section"></a></h3>
<p>LaTeX.js supports CJK because it is just a transpiler on top of HTML &amp; CSS.
However, just like HTML &amp; CSS, it doesn’t follow CJK best practices and it’s
even harder and requires more work to tune itself according to CJK typesetting
best practices.</p>

<p>Looks like we can have a LaTeX in a browser? No, no, no, if things were really
that easy, the world would be a better place. LaTeX.js comes with lots of
<a href="https://latex.js.org/limitations.html" target="_blank" rel="noreferrer">limitations</a>, some of which are fatal
for a production-ready LaTeX replacement in a browser:</p>
<ul>
<li>horizontal
<a href="https://www.overleaf.com/learn/latex/Articles/Boxes_and_Glue%3A_A_Brief%2C_but_Visual%2C_Introduction_Using_LuaTeX" target="_blank" rel="noreferrer">glue</a>,
like <a href="https://latexref.xyz/_005chfill.html" target="_blank" rel="noreferrer"></a> in a paragraph of text, is
not possible</li>
<li>vertical glue makes no sense in HTML, and is impossible to emulate, except in
boxes with fixed height</li>
<li>the concept of pages does not really apply to HTML, so any macro related to
pagebreaks will be ignored, that being said, you cannot get a paged document
with LaTeX.js, which is a fatal deal breaker for a resume builder app</li>
</ul>
<h3 id="instant-preview-2">Instant Preview<a href="#instant-preview-2" aria-label="Permalink for this section"></a></h3>
<p>LaTeX.js provides instant preview because it is a client side library and runs
in a browser.</p>
<h3 id="conclusion-2">Conclusion<a href="#conclusion-2" aria-label="Permalink for this section"></a></h3>
<p>LaTeX.js provides only
<a href="https://latex.js.org/limitations.html#when-parsing-tex-as-a-context-free-grammar" target="_blank" rel="noreferrer">limited</a>
parsing capabilities for TeX/LaTeX, in other words, many LaTeX packages cannot
be used in LaTeX.js.</p>
<blockquote>
<p>This is a PEG parser, which means it interprets LaTeX as a context-free
language. However, TeX (and therefore LaTeX) is Turing complete, so TeX can only
really be parsed by a complete Turing machine. It is not possible to parse the
full TeX language with a static parser. See here (opens new window)for some
interesting examples.</p>
</blockquote>
<p>When I started PPResume at Dec, 2022, I also tried LaTeX.js for a while, but
after discovering its fatal limitations, I quickly dropped it in favour of
server-side LaTeX. As far as what I can tell, <strong>LaTeX.js is a good demo idea but
far from being a production-ready LaTeX replacement</strong>.</p>
<ul>
<li>Pros<!-- -->
<ul>
<li><strong>Instant preview</strong>: LaTeX.js processes LaTeX documents entirely on the
client side, which means it can render documents in real-time in the browser.
This eliminates the need for server-side LaTeX installations and compilations.</li>
<li><strong>Extensible</strong>: The project is implemented in JavaScript, making it easy to
integrate into web applications. New macros can also be added easily in
JavaScript.</li>
</ul>
</li>
<li>Cons<!-- -->
<ul>
<li><strong>Missing capabilites</strong>: LaTeX.js only covers a limited set of LaTeX
capabilities, it is far from being a production ready LaTeX replacement.
Lots of LaTeX packages cannot be used with LaTeX.js.</li>
<li><strong>No pagination</strong>: Some LaTeX features, like glue, paging, cannot be
translated to HTML, which is a deal breaker for producing paged documents
like PDF.</li>
<li><strong>Poor line breaking</strong>: LaTeX.js is based on HTML &amp; CSS and do not implement
Knuth Plass line breaking algorithm</li>
<li><strong>Extra effort needed for CJK typesetting</strong>: same as above, LaTeX.js is
based on HTML &amp; CSS hence it needs extra effort in order to follow CJK best
typesetting practices, and is harder to do this than plain HTML &amp; CSS</li>
</ul>
</li>
</ul>
<h2 id="typst">Typst<a href="#typst" aria-label="Permalink for this section"></a></h2>
<p><a href="https://typst.app/" target="_blank" rel="noreferrer">Typst</a> is a modern typesetting system designed to be
an intuitive and efficient alternative to LaTeX. It uses a syntax that is
heavily inspired by Markdown, making it more accessible to users who may find
LaTeX’s syntax complex. Typst allows users to compose documents in a text file,
similar to LaTeX, but with a focus on speed, simplicity, and error handling.</p>
<p><img alt="Typst App" loading="lazy" width="3216" height="2090" decoding="async" data-nimg="1" srcset="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftypst-app.e0da6f92.webp&amp;w=3840&amp;q=75 1x" src="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftypst-app.e0da6f92.webp&amp;w=3840&amp;q=75"></p>
<h3 id="line-breaking-3">Line Breaking<a href="#line-breaking-3" aria-label="Permalink for this section"></a></h3>
<p>Typst provide two options for line breaks:</p>
<ul>
<li>: determine the line breaks in a simple
first-fit style.</li>
<li>: optimize the line breaks for the whole
paragraph. This option
<a href="https://github.com/typst/typst/discussions/626" target="_blank" rel="noreferrer">implemented</a> the Knuth Plass
line breaking algorithm internally.</li>
</ul>
<p>The line breakingn in Typst would be better if  option and
<a href="https://typst.app/docs/reference/text/text/#parameters-hyphenate" target="_blank" rel="noreferrer"></a>
option are used together.</p>
<h3 id="cjk-3">CJK<a href="#cjk-3" aria-label="Permalink for this section"></a></h3>
<p>Because Typst is very young, its CJK support is not as mature as LaTeX. As a
result, there’re lots of <a href="https://github.com/typst/typst/issues?q=is%3Aissue+is%3Aopen+CJK" target="_blank" rel="noreferrer">open
issues</a> in
the Typst community. Here are some typical ones:</p>
<ul>
<li><a href="https://github.com/typst/typst/issues/276" target="_blank" rel="noreferrer">Better CJK support</a></li>
<li><a href="https://github.com/typst/typst/issues/792" target="_blank" rel="noreferrer">Ignore linebreaks between CJK characters in source
code</a></li>
<li><a href="https://github.com/typst/typst/issues/794" target="_blank" rel="noreferrer">Language-dependant font
configuration</a></li>
<li><a href="https://github.com/typst/typst/issues/1489" target="_blank" rel="noreferrer">Add support for ruby (CJK, e.g., furigana for
Japanese)</a></li>
<li><a href="https://github.com/typst/typst/issues/2348" target="_blank" rel="noreferrer">CJK punctuation at the start of paragraphs are not adjusted
sometimes</a></li>
<li><a href="https://github.com/typst/typst/issues/5040" target="_blank" rel="noreferrer">Writing Chinese text results in some characters falling back to a different
font in web app</a></li>
<li><a href="https://github.com/typst/typst/issues/5277" target="_blank" rel="noreferrer">0.12 handles CJK fonts
incorrectly</a></li>
</ul>
<p>Basically these issues can be categorised as follows:</p>
<ul>
<li>CJK font settings</li>
<li>punctuation rules</li>
<li>spacing styles between CJK and non-CJK characters</li>
<li>language aware line breaking</li>
</ul>
<p>I am 100% sure that Typst will be able to improve and solve these issues, but it
will take time. It is very likely that there will be some breaking changes in
the future.</p>

<p>Typst supports <a href="https://typst.app/docs/guides/page-setup-guide/" target="_blank" rel="noreferrer">pagination</a> out
of the box, fair enough as a dedicated typesetting engine.</p>
<h3 id="instant-preview-3">Instant Preview<a href="#instant-preview-3" aria-label="Permalink for this section"></a></h3>
<p>This part is a bit complicated.</p>
<p>Basically, Typst is an <a href="https://github.com/typst/typst/" target="_blank" rel="noreferrer">open source</a> project, it
can run as a CLI tool where you can just type in a command  and get a PDF in your local folder.</p>
<p>Typst provides a  command, combined with incremental compilation,
the PDF can be updated in milliseconds. There are also some extensions such as
<a href="https://github.com/Myriad-Dreamin/tinymist" target="_blank" rel="noreferrer">tinymist</a> which allows instant
preview on editors.</p>
<p>It can also run purely in a browser, as the project is written in rust and
designed to be able to be compiled to WebAssembly. In fact, the official Typst
<a href="https://typst.app/" target="_blank" rel="noreferrer">web app</a> run in a browsers via WebAssembly. However,
this part is <a href="https://github.com/typst/typst/issues/909" target="_blank" rel="noreferrer">not</a> open sourced:</p>
<blockquote>
<p>Typst can be compiled to WASM, but no JS glue is available, you’d have to
write that yourself. It’s not as simple as compile(string) because you also
need to provide fonts, and if you want a multi-file setup of course also
files.</p>
</blockquote>
<p>That being said, if you want instant preview for Typst in a browser, you are
mostly on your own to write a WebAssembly binding to typst.</p>
<h3 id="conclusion-3">Conclusion<a href="#conclusion-3" aria-label="Permalink for this section"></a></h3>
<p>In my opinion, Typst is a very promising alternative to LaTeX, but still very
young and lacks some key capabilites to handle complicated typesetting
scenarios.</p>
<ul>
<li>Pros<!-- -->
<ul>
<li><strong>User-friendly Syntax</strong>: Typst’s syntax is more straightforward and
consistent compared to LaTeX, making it easier for beginners to learn and
use.</li>
<li><strong>Fast compilation</strong>: Typst has incremental compilation which lead to a
faster compilation in milliseconds rather than seconds.</li>
<li><strong>Customizable line breaking</strong>: Typst provide options for users to opt in
Knuth Plass line breaking algorithm</li>
</ul>
</li>
<li>Cons<!-- -->
<ul>
<li><strong>Limited ecosystem</strong>: as a newer tool, Typst lacks the extensive package
ecosystem that LaTeX offers, which can limit functionality for advanced
typesetting needs.</li>
<li><strong>Unstable CJK typesetting</strong>: Typst still has lots of issues for CJK
typesetting and is constantly evolving.</li>
<li><strong>Instant preview is private</strong>: Typst do not open source their WebAssembly
bindings so there is no official instant preview feature on browser</li>
</ul>
</li>
</ul>
<h2 id="react-pdf">React-pdf<a href="#react-pdf" aria-label="Permalink for this section"></a></h2>
<p><a href="https://react-pdf.org/" target="_blank" rel="noreferrer">React-pdf</a> is react renderer for creating PDF files on
the browser and server.</p>
<h3 id="line-breaking-4">Line Breaking<a href="#line-breaking-4" aria-label="Permalink for this section"></a></h3>
<p>React-pdf internally <a href="https://react-pdf.org/advanced#hyphenation" target="_blank" rel="noreferrer">implements</a>
the Knuth and Plass line breaking algorithm. By default it’s set to hyphenate
english words.</p>
<p>This is one page from the example document in <a href="https://react-pdf.org/repl" target="_blank" rel="noreferrer">react-pdf
playground</a>, note the layout of the paragraph, the
text overall looks balanced and justified, much better than normal paragraphs in
normal HTML &amp; CSS.</p>
<p><img alt="React-pdf document" loading="lazy" width="2480" height="3507" decoding="async" data-nimg="1" srcset="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Freact-pdf-document.05cfde87.webp&amp;w=3840&amp;q=75 1x" src="https://blog.ppresume.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Freact-pdf-document.05cfde87.webp&amp;w=3840&amp;q=75"></p>
<h3 id="cjk-4">CJK<a href="#cjk-4" aria-label="Permalink for this section"></a></h3>
<p>React-pdf with default settings does not render CJK characters, you need to
<a href="https://github.com/diegomura/react-pdf/issues/867#issuecomment-713483012" target="_blank" rel="noreferrer">register a
font</a>
and quote it in styles.</p>

<p>Needless to say, react-pdf supports pagination because it is a library to
generate PDF. It also provides <a href="https://react-pdf.org/components#page" target="_blank" rel="noreferrer">options</a>
to specify page sizes, DPI, styles, etc.</p>
<h3 id="instant-preview-4">Instant Preview<a href="#instant-preview-4" aria-label="Permalink for this section"></a></h3>
<p>React-pdf can be used on both client side and server side.</p>
<p>If used on client side, then yes we have instant preview, again, you can check
the <a href="https://react-pdf.org/repl" target="_blank" rel="noreferrer">playground</a> for a live demo.
Otherwise, if used on server side with
<a href="https://react-pdf.org/node" target="_blank" rel="noreferrer">Node.js</a>, then no instant preview due to the round
trip time from request to response.</p>
<h3 id="conclusion-4">Conclusion<a href="#conclusion-4" aria-label="Permalink for this section"></a></h3>
<p>It seems that react-pdf would be a perfect choice as the typesetting engine for
a resume builder.</p>
<p>However, react-pdf is not a dedicated typesetting engine. It lacks many features
that are only available or work well with a dedicated typesetting engine. For
example, it has no built-in list items. Most importantly, even though it already
implements the Knuth-Plass line-breaking algorithm, typesetting is not just
about breaking paragraphs into lines, is it? You still need to tune the spacing
between paragraphs, adjust font size/styles, respect CJK best typesetting
practices, etc. All this tuning requires a huge amount of work that LaTeX
already provides out of the box.</p>
<p>In fact, there is an open source resume builder called
<a href="https://www.open-resume.com/resume-builder" target="_blank" rel="noreferrer">open-resume</a> which uses this
library to generate and update resume PDF in real time, you can check the output
PDF by yourself and compare it to the <a href="https://ppresume.com/gallery" target="_blank" rel="noreferrer">PDF generated by
LaTeX</a>.</p>
<p>OK conclusion:</p>
<ul>
<li>Pros<!-- -->
<ul>
<li><strong>React integration</strong>: react-pdf allows developers to create PDF documents
using react</li>
<li><strong>Instant preview</strong>: react-pdf provides instant preview when running on
client side</li>
<li><strong>Good line breaking</strong>: react-pdf implemented Knuth Plass line breaking
algorithm internally, better than plain HTML &amp; CSS</li>
<li><strong>Pagination</strong>: react-pdf support pagination out of the box, with
customizable page size, margins, etc.</li>
</ul>
</li>
<li>Cons<!-- -->
<ul>
<li><strong>Limited typesetting capabilites</strong>: after all react-pdf is a react library,
neither a professional nor a dedicated typesetting engine.</li>
<li><strong>Limited support for CJK</strong>: react-pdf can render CJK with manually
registered font, however, it doesn’t respect CJK best typesetting practices</li>
</ul>
</li>
</ul>
<h2 id="summary">Summary<a href="#summary" aria-label="Permalink for this section"></a></h2>
<p>The goal of PPResume is to be a professional resume builder that offers top
notch typesetting quality, with native support for multi languages.</p>
<p>As mentioned above, in order to meet PPResume’s requirements, the typesetting
engine must:</p>
<ul>
<li>adopt Knuth Plass line breaking algorithm</li>
<li>support CJK with respect to best typesetting practices</li>
<li>support pagination</li>
<li>(optional) support instant preview</li>
</ul>
<table><thead><tr><th>Typesetting Engine</th><th>Knuth Plass line breaking</th><th>CJK</th><th>Pagination</th><th>Instant Preview</th></tr></thead><tbody><tr><td>HTML &amp; CSS</td><td>No</td><td>Yes</td><td>Partial</td><td>Yes</td></tr><tr><td>LaTeX</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td></tr><tr><td>LaTeX.js</td><td>No</td><td>Yes</td><td>No</td><td>Yes</td></tr><tr><td>Typst</td><td>Yes</td><td>Partial</td><td>Yes</td><td>Partial</td></tr><tr><td>React-pdf</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td></tr></tbody></table>
<p>Both HTML &amp; CSS and LaTeX.js do not support Knuth Plass line breaking, react-pdf
and Typst’s CJK support is not production ready, hence LaTeX is our only option.</p>
<p>In the long run if there’re better choice, it is possible for PPResume to add
support for other typesetting engines.</p>
<p>Last but not least, having fun with <a href="https://polytype.dev/" target="_blank" rel="noreferrer">polytype</a>, a
<a href="https://en.wikipedia.org/wiki/Rosetta_Stone_(software)" target="_blank" rel="noreferrer">Rosetta Stone</a> for
typesetting engines.</p>
<p>Thanks for reading!</p>
<h2 id="revision">Revision<a href="#revision" aria-label="Permalink for this section"></a></h2>
<h3 id="nov-8-2024">Nov 8, 2024<a href="#nov-8-2024" aria-label="Permalink for this section"></a></h3>
<ul>
<li>typo fix: hypens -&gt; hyphens</li>
<li>add Chinese translations:<!-- -->
<ul>
<li>Simplified Chinese: <a href="https://blog.ppresume.com/posts/zh-cn/on-typesetting-engines" target="_blank" rel="noreferrer">排版引擎纵谈：程序员的视角</a></li>
<li>Traditional Chinese TW: <a href="https://blog.ppresume.com/posts/zh-tw/on-typesetting-engines" target="_blank" rel="noreferrer">排版引擎縱談：程式設計師的視角</a></li>
</ul>
</li>
</ul>
<h3 id="nov-2-2024">Nov 2, 2024<a href="#nov-2-2024" aria-label="Permalink for this section"></a></h3>
<ul>
<li>refer  in HTML &amp; CSS section, suggested by
<a href="https://www.reddit.com/r/programming/comments/1ggg67q/comment/luxjfsy/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button" target="_blank" rel="noreferrer">u/Jona-Anders</a></li>
<li>refer <a href="https://github.com/Myriad-Dreamin/tinymist" target="_blank" rel="noreferrer">tinymist</a> in Typst section,
suggested by
<a href="https://www.reddit.com/r/programming/comments/1ggg67q/comment/luxiasn/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button" target="_blank" rel="noreferrer">u/Afkadrian</a></li>
</ul>
<h3 id="nov-1-2024">Nov 1, 2024<a href="#nov-1-2024" aria-label="Permalink for this section"></a></h3>
<ul>
<li>typo fix: ctex -&gt; CTeX, suggested by <a href="https://liam.page/en/about/" target="_blank" rel="noreferrer">Liam Huang</a></li>
</ul></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything I've learned so far about running local LLMs (122 pts)]]></title>
            <link>https://nullprogram.com/blog/2024/11/10/</link>
            <guid>42100560</guid>
            <pubDate>Sun, 10 Nov 2024 14:53:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nullprogram.com/blog/2024/11/10/">https://nullprogram.com/blog/2024/11/10/</a>, See on <a href="https://news.ycombinator.com/item?id=42100560">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en">
<article>
  
  <time datetime="2024-11-10">
    November 10, 2024
  </time>
  <p>
    nullprogram.com/blog/2024/11/10/
  </p>

  <p>Over the past month I’ve been exploring the rapidly evolving world of
Large Language Models (LLM). It’s now accessible enough to run a LLM on a
Raspberry Pi smarter than the original ChatGPT (November 2022). A modest
desktop or laptop supports even smarter AI. It’s also private, offline,
unlimited, and registration-free. The technology is improving at breakneck
speed, and information is outdated in a matter of months. This article
snapshots my practical, hands-on knowledge and experiences — information I
wish I had when starting. Keep in mind that I’m a LLM layman, I have no
novel insights to share, and it’s likely I’ve misunderstood certain
aspects. In a year this article will mostly be a historical footnote,
which is simultaneously exciting and scary.</p>

<p>In case you’ve been living under a rock — as an under-the-rock inhabitant
myself, welcome! — LLMs are neural networks that underwent a breakthrough
in 2022 when trained for conversational “chat.” Through it, users converse
with a wickedly creative artificial intelligence indistinguishable from a
human, which smashes the Turing test and can be . Interacting with one for
the first time is unsettling, a feeling which will last for days. When you
bought your most recent home computer, you probably did not expect to have
a meaningful conversation with it.</p>

<p>I’ve found this experience reminiscent of the desktop computing revolution
of the 1990s, where your newly purchased computer seemed obsolete by the
time you got it home from the store. There are new developments each week,
and as a rule I ignore almost any information more than a year old. The
best way to keep up has been <a href="https://old.reddit.com/r/LocalLLaMA">r/LocalLLaMa</a>. Everything is hyped to the
stratosphere, so take claims with a grain of salt.</p>

<p>I’m wary of vendor lock-in, having experienced the rug pulled out from
under me by services shutting down, changing, or otherwise dropping my use
case. I want the option to continue, even if it means changing providers.
So for a couple of years I’d ignored LLMs. The “closed” models, accessibly
only as a service, have the classic lock-in problem, including <a href="https://arxiv.org/pdf/2307.09009">silent
degradation</a>. That changed when I learned I can run models close
to the state-of-the-art on my own hardware — the exact opposite of vendor
lock-in.</p>

<p>This article is about running LLMs, not fine-tuning, and definitely not
training. It’s also only about <em>text</em>, and not vision, voice, or other
“multimodal” capabilities, which aren’t nearly so useful to me personally.</p>

<p>To run a LLM on your own hardware you need <strong>software</strong> and a <strong>model</strong>.</p>

<h3 id="the-software">The software</h3>

<p>I’ve exclusively used the <em>astounding</em> <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>. Other options exist,
but for basic CPU inference — that is, generating tokens using a CPU
rather than a GPU — llama.cpp requires nothing beyond a C++ toolchain. In
particular, no Python fiddling that plagues much of the ecosystem. On
Windows it will be a 5MB <code>llama-server.exe</code> with no runtime dependencies.
From just two files, EXE and GGUF (model), both designed to <a href="https://justine.lol/mmap/">load via
memory map</a>, you could likely still run the same LLM 25 years from
now, in exactly the same way, out-of-the-box on some future Windows OS.</p>

<p>Full disclosure: I’m biased because <a href="https://github.com/ggerganov/llama.cpp/blob/ec450d3b/docs/build.md">the official Windows build process is
w64devkit</a>. What can I say? These folks have good taste! That being
said, you should only do CPU inference if GPU inference is impractical. It
works reasonably up to ~10B parameter models on a desktop or laptop, but
it’s slower. My primary use case is not built with w64devkit because I’m
using CUDA for inference, which requires a MSVC toolchain. Just for fun, I
ported llama.cpp to Windows XP and ran <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct">a 360M model</a> on a 2008-era
laptop. It was magical to load that old laptop with technology that, at
the time it was new, would have been worth billions of dollars.</p>

<p>The bottleneck for GPU inference is video RAM, or VRAM. These models are,
well, <em>large</em>. The more RAM you have, the larger the model and the longer
the context window. Larger models are smarter, and longer contexts let you
process more information at once. <strong>GPU inference is not worth it below
8GB of VRAM</strong>. If <a href="https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena">“GPU poor”</a>, stick with CPU inference. On the
plus side, it’s simpler and easier to get started with CPU inference.</p>

<p>There are many utilities in llama.cpp, but this article is concerned with
just one: <strong><code>llama-server</code> is the program you want to run.</strong> It’s an HTTP
server (default port 8080) with a chat UI at its root, and <a href="https://github.com/ggerganov/llama.cpp/blob/ec450d3b/examples/server/README.md#api-endpoints">APIs for use
by programs</a>, including other user interfaces. A typical invocation:</p>

<div><pre><code>$ llama-server --flash-attn --ctx-size 0 --model MODEL.gguf
</code></pre></div>

<p>The context size is the largest number of tokens the LLM can handle at
once, input plus output. Contexts typically range from 8K to 128K tokens,
and depending on the model’s tokenizer, normal English text is ~1.6 tokens
per word as counted by <code>wc -w</code>. If the model supports a large context you
may run out of memory. If so, set a smaller context size, like <code>--ctx-size
$((1&lt;&lt;13))</code> (i.e. 8K tokens).</p>

<p>I do not yet understand what flash attention is about, and I don’t know
why <code>--flash-attn</code>/<code>-fa</code> is not the default (lower accuracy?), but you
should always request it because it reduces memory requirements when
active and is well worth the cost.</p>

<p>If the server started successfully, visit it (<a href="http://localhost:8080/">http://localhost:8080/</a>) to
try it out. Though of course you’ll need a model first.</p>

<h3 id="the-models">The models</h3>

<p><a href="https://huggingface.co/">Hugging Face</a> (HF) is “the GitHub of LLMs.” It’s an incredible
service that has earned that title. “Small” models are around a few GBs,
large models are hundreds of GBs, and HF <em>hosts it all for free</em>. With a
few exceptions that do not matter in practice, you don’t even need to sign
up to download models! (I’ve been so impressed that after a few days they
got a penny-pincher like me to pay for pro account.) That means you can
immediately download and try any of the stuff I’m about to discuss.</p>

<p>If you look now, you’ll wonder, “There’s a lot of stuff here, so what the
heck am I supposed to download?” That was me one month ago. For llama.cpp,
the answer is <a href="https://github.com/ggerganov/ggml/blob/8a3d7994/docs/gguf.md">GGUF</a>. None of the models are natively in GGUF.
Instead GGUFs are in a repository with “GGUF” in the name, usually by a
third party: one of the heroic, prolific GGUF quantizers.</p>

<p>(Note how nowhere does the official documentation define what “GGUF”
stands for. Get used that. This is a technological frontier, and if the
information exists at all, it’s not in the obvious place. If you’re
considering asking your LLM about this once it’s running: Sweet summer
child, we’ll soon talk about why that doesn’t work. As far as I can tell,
“GGUF” has no authoritative definition.)</p>

<p>Since llama.cpp is named after the Meta’s flagship model, their model is a
reasonable start, though it’s not my personal favorite. The latest is
Llama 3.2, but at the moment only the 1B and 3B models — that is, ~1
billion and ~3 billion parameters — work in Llama.cpp. Those are a little
<em>too</em> small to be of much use, and your computer can likely to better if
it’s not a Raspberry Pi, even with CPU inference. Llama 3.1 8B is a better
option. (If you’ve got at least 24GB of VRAM then maybe you can even do
Llama 3.1 70B.)</p>

<p>If you search for Llama 3.1 8B you’ll find two options, one qualified
“instruct” and one with no qualifier. Instruct means it was trained to
follow instructions, i.e. to chat, and that’s nearly always what you want.
The other is the “base” model which can only continue a text. (Technically
the instruct model is still just completion, but we’ll get to that later.)
It would be great if base models were qualified “Base” but, for dumb path
dependency reasons, they’re usually not.</p>

<p>You will not find GGUF in the “Files” for the instruct model, nor can you
download the model without signing up in order to agree to the community
license. Go back to the search, add GGUF, and look for the matching GGUF
model: <a href="https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF">bartowski/Meta-Llama-3.1-8B-Instruct-GGUF</a>. bartowski is
one of the prolific and well-regarded GGUF quantizers. Not only will this
be in the right format for llama.cpp, you won’t need to sign up.</p>

<p>In “Files” you will now see many GGUFs. These are different quantizations
of the same model. The original model has <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a> tensors, but for
merely running the model we can throw away most of that precision with
minimal damage. It will be a tiny bit dumber and less knowledgeable, but
will require substantially fewer resources. <strong>The general recommendation,
which fits my experience, is to use <code>Q4_K_M</code></strong>, a 4-bit quantization. In
general, better to run a 4-bit quant of a larger model than an 8-bit quant
of a smaller model. Once you’ve got the basics understood, experiment with
different quants and see what you like!</p>

<h3 id="my-favorite-models">My favorite models</h3>

<p>Models are trained for different trade-offs and differ in strengths and
weaknesses, so no model is best at everything — especially on “GPU-poor”
configurations. My desktop system has an RTX 3050 Ti with 8GB VRAM, and
its limitations have shaped my choices. I can comfortably run ~10B models,
and ~30B models just barely enough to test their capabilities. For ~70B I
rely on third-party hosts. My “t/s” numbers are all on this system running
4-bit quants.</p>

<p>This list omits “instruct” from the model name, but assume the instruct
model unless I say otherwise. A few are <em>bona fide</em> open source, at least
as far as LLMs practically can be, and I’ve noted the license when that’s
the case. The rest place restrictions on both use and distribution.</p>

<ul>
  <li>
    <p>Mistral-Nemo-2407 (12B) [Apache 2.0]</p>

    <p>A collaboration between <a href="https://mistral.ai/">Mistral AI</a> and Nvidia (“Nemo”), the
most well-rounded ~10B model I’ve used, and my default. Inference starts
at a comfortable 30 t/s. It’s strengths are writing and proofreading,
and it can review code nearly as well as ~70B models. It was trained for
a context length of 128K, but its <a href="https://github.com/NVIDIA/RULER">effective context length is closer to
16K</a> — a limitation I’ve personally observed.</p>

    <p>The “2407” is a date (July 2024) as version number, a versioning scheme
I wholeheartedly support. A date tells you about its knowledge cut-off
and tech level. It sorts well. Otherwise LLM versioning is a mess. Just
as open source is bad with naming, AI companies do not comprehend
versioning.</p>
  </li>
  <li>
    <p>Qwen2.5-14B and Qwen2.5-72B</p>

    <p>Qwen models, by Alibaba Cloud, impressively punch above their weight at
all sizes. 14B inference starts at 11 t/s, with capabilities on par with
Mistral Nemo. If I could run 72B on my own hardware, it would probably
be my default. I’ve been trying it through Hugging Face’s inference API.
There’s a 32B model, but it’s impractical for my hardware, so I haven’t
spent much time with it.</p>
  </li>
  <li>
    <p>Gemma-2-2B</p>

    <p>Google’s model is popular, perhaps due to its playful demeanor. For me,
the 2B model <a href="https://github.com/skeeto/scratch/blob/master/userscript/reddit-llm-translate.user.js">is great for fast translation</a>. It’s amazing that LLMs
have nearly obsoleted Google Translate, and you can run it on your home
computer. Though it’s more resource-intensive, and refuses to translate
texts it finds offensive, which sounds like a plot element from a sci-fi
story. In my translation script, I send it text marked up with HTML.
Simply <em>asking</em> Gemma to preserve the markup Just Works! The 9B model is
even better, but slower, and I’d use it instead of 2B for translating my
own messages into another language.</p>
  </li>
  <li>
    <p>Phi3.5-Mini (4B) [MIT]</p>

    <p>Microsoft’s niche is training on synthetic data. The result is a model
that does well in tests, but doesn’t work so well in practice. For me,
its strength is document evaluation. I’ve loaded the context with up to
40K-token documents — it helps that it’s a 4B model — and successfully
queried accurate summaries and data listings.</p>
  </li>
  <li>
    <p>SmolLM2-360M [Apache 2.0]</p>

    <p>Hugging Face doesn’t just host models; their 360M model is unusually
good for its size. It fits on my 2008-era, 1G RAM, Celeron, and 32-bit
operating system laptop. It also runs well on older Raspberry Pis. It’s
creative, fast, converses competently, can write poetry, and a fun toy
in cramped spaces.</p>
  </li>
  <li>
    <p>Mixtral-8x7B (48B) [Apache 2.0]</p>

    <p>Another Mistral AI model, and more of a runner up. 48B seems too large,
but this is a <a href="https://mistral.ai/news/mixtral-of-experts/">Mixture of Experts</a> (MoE) model. Inference uses only
13B parameters at a time. It’s reasonably-suited to CPU inference on a
machine with at least 32G of RAM. The model retains more of its training
inputs, more like a database, but for reasons we’ll see soon, it isn’t
as useful as it might seem.</p>
  </li>
  <li>
    <p>Llama-3.1-70B and Llama-3.1-Nemotron-70B</p>

    <p>More models I cannot run myself, but which I access remotely. The latter
bears “Nemo” because it’s an Nvidia fine-tune. If I could run 70B models
myself, Nemotron might just be my default. I’d need to spent more time
evaluating it against Qwen2.5-72B.</p>
  </li>
</ul>

<p>Most of these models have <a href="https://huggingface.co/blog/mlabonne/abliteration">abliterated</a> or “uncensored” versions, in
which refusal is partially fine-tuned out at a cost of model degradation.
Refusals are annoying — such as Gemma refusing to translate texts it
dislikes — but doesn’t happen enough for me to make that trade-off. Maybe
I’m just boring. Also refusals seem to decrease with larger contexts, as
though “in for a penny, in for a pound.”</p>

<p>The next group are “coder” models trained for programming. In particular,
they have <em>fill-in-the-middle</em> (FIM) training for generating code inside
an existing program. I’ll discuss what that entails in a moment. As far as
I can tell, they’re no better at code review nor other instruct-oriented
tasks. It’s the opposite: FIM training is done in the base model, with
instruct training applied later on top, so instruct works <em>against</em> FIM!
In other words, you get better FIM results from base models, though you
lose the ability to converse with them.</p>

<p>There will be a section on evaluation later, but I want to note now that
<em>LLMs still produce poor code</em>, even at the state-of-the-art. The rankings
here are relative to other models, not about overall capability.</p>

<ul>
  <li>
    <p>DeepSeek-Coder-V2-Lite (16B)</p>

    <p>A self-titled MoE model from <a href="https://www.deepseek.com/">DeepSeek</a>. It uses 2B parameters
during inference, making it as fast as Gemma 2 2B but as smart as
Mistral Nemo, striking a great balance, especially because it
out-competes ~30B models at code generation. If I’m playing around with
FIM, this is my default choice.</p>
  </li>
  <li>
    <p>Qwen2.5-Coder-7B</p>

    <p>Qwen Coder is a close second. It works about as well, but slightly
slower since it’s not MoE. It’s a better choice than DeepSeek if you’re
memory-constrained, or, similarly, if your choices are GPU inference
Qwen versus CPU inference DeepSeek. While writing this article, Alibaba
Cloud released a new Qwen2.5-Coder-7B that’s reported to be better. They
failed to increment the version number, which is horribly confusing. The
community has taken to calling it Qwen2.5.1. (Remember what I said about
AI companies and versions?)</p>
  </li>
  <li>
    <p>Granite-8B-Code [Apache 2.0]</p>

    <p>IBM’s line of models is named Granite. In general Granite models are
disappointing, <em>except</em> that they’re unusually good at FIM. It’s tied
in second place with Qwen2.5 7B in my experience.</p>
  </li>
</ul>

<p>I also evaluated CodeLlama, CodeGemma, Codestral, and StarCoder. Their FIM
outputs were so poor as to be effectively worthless at that task, and I
found no reason to use these models. The negative effects of instruct
training were most pronounced for CodeLlama.</p>

<h3 id="the-user-interfaces">The user interfaces</h3>

<p>I pointed out Llama.cpp’s built-in UI, and I’d used similar UIs with other
LLM software. As is typical, no UI is to my liking, especially in matters
of productivity, so I built my own, <strong><a href="https://github.com/skeeto/illume">Illume</a></strong>. This command
line program converts standard input into an API query, makes the query,
and streams the response to standard output. Should be simple enough to
integrate into any extensible text editor, but I only needed it for Vim.
Vimscript is miserable, probably the second worst programming language
I’ve ever touched, so my goal was to write as little as possible.</p>

<p>I created Illume to scratch my own itch, to support my exploration of the
LLM ecosystem. I actively break things and add features as needed, and I
make no promises about interface stability. <em>You probably don’t want to
use it.</em></p>

<p>Lines that begin with <code>!</code> are directives interpreted by Illume, chosen
because it’s unlikely to appear in normal text. A conversation alternates
between <code>!user</code> and <code>!assistant</code> in a buffer.</p>

<div><pre><code>!user
Write a Haiku about time travelers disguised as frogs.

!assistant
Green, leaping through time,
Frog tongues lick the future's rim,
Disguised in pond's guise.
</code></pre></div>

<p>It’s still a text editor buffer, so I can edit the assistant response,
reword my original request, etc. before continuing the conversation. For
composing fiction, I can request it to continue some text (which does not
require instruct training):</p>

<div><pre><code>!completion
Din the Wizard stalked the dim castle
</code></pre></div>

<p>I can stop it, make changes, add my own writing, and keep going. I ought
to spend more time practicing with it. If you introduce out-of-story note
syntax, the LLM will pick up on it, and then you can use notes to guide
the LLM’s writing.</p>

<p>While the main target is llama.cpp, I query different APIs, implemented by
different LLM software, with incompatibilities across APIs (a parameter
required by one API is forbidden by another), so directives must be
flexible and powerful. So directives can set arbitrary HTTP and JSON
parameters. Illume doesn’t try to abstract the API, but exposes it at a
low level, so effective use requires knowing the remote API. For example,
the “profile” for talking to llama.cpp looks like this:</p>

<div><pre><code>!api http://localhost:8080/v1
!:cache_prompt true
</code></pre></div>

<p>Where <code>cache_prompt</code> is a llama.cpp-specific JSON parameter (<code>!:</code>). Prompt
cache nearly always better enabled, yet for some reason it’s disabled by
default. Other APIs refuse requests with this parameter, so then I must
omit or otherwise disable it. The Hugging Face “profile” looks like this:</p>

<div><pre><code>!api https://api-inference.huggingface.co/models/{model}/v1
!:model Qwen/Qwen2.5-72B-Instruct
!&gt;x-use-cache false
</code></pre></div>

<p>For the sake of HF, Illume can interpolate JSON parameters into the URL.
The HF API caches also aggressively caches. I never want this, so I supply
an HTTP parameter (<code>!&gt;</code>) to turn it off.</p>

<p>Unique to llama.cpp is an <code>/infill</code> endpoint for FIM. It requires a model
with extra metadata, trained a certain way, but this is usually not the
case. So while Illume can use <code>/infill</code>, I also added FIM configuration
so, after reading the model’s documentation and configuring Illume for
that model’s FIM behavior, I can do FIM completion through the normal
completion API on any FIM-trained model, even on non-llama.cpp APIs.</p>

<h3 id="fill-in-the-middle-fim-tokens">Fill-in-the-Middle (FIM) tokens</h3>

<p>It’s time to discuss FIM. To get to the bottom of FIM I needed to go to
the source of truth, the original FIM paper: <a href="https://arxiv.org/abs/2207.14255">Efficient Training of
Language Models to Fill in the Middle</a>. This allowed me to understand
how these models are FIM-trained, at least enough to put that training to
use. Even so, model documentation tends to be thin on FIM because they
expect you to run their code.</p>

<p>Ultimately an LLM can only predict the next token. So pick some special
tokens that don’t appear in inputs, use them to delimit a prefix and
suffix, and middle (PSM) — or sometimes ordered suffix-prefix-middle (SPM)
— in a large training corpus. Later in inference we can use those tokens
to provide a prefix, suffix, and let it “predict” the middle. Crazy, but
<em>this actually works!</em></p>

<div><pre><code>&lt;PRE&gt;{prefix}&lt;SUF&gt;{suffix}&lt;MID&gt;
</code></pre></div>

<p>For example when filling the parentheses of <code>dist = sqrt(x*x + y*y)</code>:</p>

<div><pre><code>&lt;PRE&gt;dist = sqrt(&lt;SUF&gt;)&lt;MID&gt;x*x + y*y
</code></pre></div>

<p>To have the LLM fill in the parentheses, we’d stop at <code>&lt;MID&gt;</code> and let the
LLM predict from there. Note how <code>&lt;SUF&gt;</code> is essentially the cursor. By the
way, this is basically how instruct training works, but instead of prefix
and suffix, special tokens delimit instructions and conversation.</p>

<p>Some LLM folks interpret the paper quite literally and use <code>&lt;PRE&gt;</code>, etc.
for their FIM tokens, although these look nothing like their other special
tokens. More thoughtful trainers picked <code>&lt;|fim_prefix|&gt;</code>, etc. Illume
accepts FIM templates, and I wrote templates for the popular models. For
example, here’s Qwen (PSM):</p>

<div><pre><code>&lt;|fim_prefix|&gt;{prefix}&lt;|fim_suffix|&gt;{suffix}&lt;|fim_middle|&gt;
</code></pre></div>

<p>Mistral AI prefers square brackets, SPM, and no “middle” token:</p>

<div><pre><code>[SUFFIX]{suffix}[PREFIX]{prefix}
</code></pre></div>

<p>With these templates I could access the FIM training in models unsupported
by llama.cpp’s <code>/infill</code> API.</p>

<p>Besides just failing the prompt, the biggest problem I’ve had with FIM is
LLMs not know when to stop. For example, if I ask it to fill out this
function (i.e. assign something <code>r</code>):</p>

<div><pre><code><span>def</span> <span>norm</span><span>(</span><span>x</span><span>:</span> <span>float</span><span>,</span> <span>y</span><span>:</span> <span>float</span><span>)</span> <span>-&gt;</span> <span>float</span><span>):</span>
    <span>return</span> <span>r</span>
</code></pre></div>

<p>(Side note: Static types, including the hints here, produce better results
from LLMs, acting as guardrails.) It’s not unusual to get something like:</p>

<div><pre><code><span>def</span> <span>norm</span><span>(</span><span>x</span><span>:</span> <span>float</span><span>,</span> <span>y</span><span>:</span> <span>float</span><span>)</span> <span>-&gt;</span> <span>float</span><span>):</span>
    <span>r</span> <span>=</span> <span>sqrt</span><span>(</span><span>x</span><span>*</span><span>x</span> <span>+</span> <span>y</span><span>*</span><span>y</span><span>)</span>
    <span>return</span> <span>r</span>

<span>def</span> <span>norm3</span><span>(</span><span>x</span><span>:</span> <span>float</span><span>,</span> <span>y</span><span>:</span> <span>float</span><span>,</span> <span>z</span><span>:</span> <span>float</span><span>)</span> <span>-&gt;</span> <span>float</span><span>):</span>
    <span>r</span> <span>=</span> <span>sqrt</span><span>(</span><span>x</span><span>*</span><span>x</span> <span>+</span> <span>y</span><span>*</span><span>y</span> <span>+</span> <span>z</span><span>*</span><span>z</span><span>)</span>
    <span>return</span> <span>r</span>

<span>def</span> <span>norm4</span><span>(</span><span>x</span><span>:</span> <span>float</span><span>,</span> <span>y</span><span>:</span> <span>float</span><span>,</span> <span>z</span><span>:</span> <span>float</span><span>,</span> <span>w</span><span>:</span> <span>float</span><span>)</span> <span>-&gt;</span> <span>float</span><span>):</span>
    <span>r</span> <span>=</span> <span>sqrt</span><span>(</span><span>x</span><span>*</span><span>x</span> <span>+</span> <span>y</span><span>*</span><span>y</span> <span>+</span> <span>z</span><span>*</span><span>z</span> <span>+</span> <span>w</span><span>*</span><span>w</span><span>)</span>
    <span>return</span> <span>r</span>
</code></pre></div>

<p>Where the original <code>return r</code> became the return for <code>norm4</code>. Technically
it fits the prompt, but it’s obviously not what I want. So be ready to
mash the “stop” button when it gets out of control. The three coder models
I recommended exhibit this behavior less often. It might be more robust to
combine it with a non-LLM system that understands the code semantically
and automatically stops generation when the LLM begins generating tokens
in a higher scope. That would make more coder models viable, but this goes
beyond my own fiddling.</p>

<p>Figuring out FIM and putting it into action revealed to me that FIM is
still in its early stages, and hardly anyone is generating code via FIM. I
guess everyone’s just using plain old completion?</p>

<h3 id="so-what-are-llms-good-for">So what are LLMs good for?</h3>

<p>LLMs are fun, but what the productive uses do they have? That’s a question
I’ve been trying to answer this past month, and it’s come up shorter than
I hoped. It might be useful to establish boundaries — tasks that LLMs
definitely cannot do.</p>

<p>First, <strong>LLMs are no good if correctness cannot be readily verified</strong>.
They are untrustworthy hallucinators. Often if you’re in position to
verify LLM output, you didn’t need it in the first place. This is why
Mixtral, with its large “database” of knowledge, isn’t so useful. It also
means it’s <em>reckless and irresponsible to inject LLM output into search
results</em> — just shameful.</p>

<p>LLM enthusiasts, who ought to know better, fall into this trap anyway and
propagate hallucinations. It makes discourse around LLMs less trustworthy
than normal, and I need to approach LLM information with extra skepticism.
Case in point: Recall how “GGUF” doesn’t have an authoritative definition.
Search for one and you’ll find an obvious hallucination that made it all
the way into official IBM documentation. I won’t repeat it hear as to not
make things worse.</p>

<p>Second, <strong>LLMs have goldfish-sized working memory</strong>. That is, they’re held
back by small context lengths. Some models are trained on larger contexts,
but their <a href="https://github.com/NVIDIA/RULER">effective context length</a> is usually much smaller. In
practice, an LLM can hold several book chapters worth of comprehension “in
its head” at a time. For code it’s 2k or 3k lines (code is token-dense).
That’s the most you can work with at once. Compared to a human, it’s tiny.
There are tools like <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">retrieval-augmented generation</a> and fine-tuning
to mitigate it… <em>slightly</em>.</p>

<p>Third, <strong>LLMs are poor programmers</strong>. At best they write code at maybe an
undergraduate student level who’s read a lot of documentation. That sounds
better than it is. The typical fresh graduate enters the workforce knowing
practically nothing about software engineering. Day one on the job is the
first day of their real education. In that sense, LLMs today haven’t even
begun their real education.</p>

<p><em>Writing new code is the easy part</em>. The hard part is maintaining code,
and writing new code with that maintenance in mind. Even when an LLM
produces code that works, there’s no thought to maintenance, nor could
there be. In general the reliability of generate code follows the inverse
square law by length, and generating more than a dozen lines at a time is
fraught. I really tried, but never saw LLM output beyond 2–3 lines of code
which I would consider acceptable.</p>

<p>Quality varies substantially by language. LLMs are better at Python than
C, and better at C than assembly. I suspect it’s related to the difficulty
of the language and the quality of the input. It’s trained on lots of
terrible C — the internet is loaded with it after all — and probably the
only labeled x86 assembly it’s seen is crummy beginner tutorials. Ask it
to use SDL2 and it <a href="https://nullprogram.com/blog/2023/01/08/">reliably produces the common mistakes</a> because
it’s been trained to do so.</p>

<p>What about boilerplate? That’s something an LLM could probably do with a
low error rate, and perhaps there’s merit to it. Though the fastest way to
deal with boilerplate is to not write it at all. Change your problem to
not require boilerplate.</p>

<p>Without taking my word for it, consider how it show up in the economics:
If AI companies could deliver the productivity gains they claim, they
wouldn’t sell AI. They’d keep it to themselves and gobble up the software
industry. Or consider the software products produced by companies on the
bleeding edge of AI. It’s still the same old, bloated web garbage everyone
else is building. (My LLM research has involved navigating their awful web
sites, and it’s made be bitter.)</p>

<p>In code generation, hallucinations are less concerning. You already knew
what you wanted when you asked, so you can review it, and your compiler
will help catch problems you miss (e.g. calling a hallucinated method).
However, small context and poor code generation remain roadblocks, and I
haven’t yet made this work effectively.</p>

<p>So then, what can I do with LLMs? A list is apt because LLMs love lists:</p>

<ul>
  <li>
    <p>Proofreading has been most useful for me. I give it a document such as
an email or this article (~8,000 tokens), tell it to look over grammar,
call out passive voice, and so on, and suggest changes. I accept or
reject its suggestions and move on. Most suggestions will be poor, and
this very article was long enough that even ~70B models suggested
changes to hallucinated sentences. Regardless, there’s signal in the
noise, and it fits within the limitations outlined above. I’m still
trying to apply this technique (“find bugs, please”) to code review, but
so far success is elusive.</p>
  </li>
  <li>
    <p>Writing short fiction. Hallucinations are not a problem; they’re a
feature! Context lengths are the limiting factor, though perhaps you can
stretch it by supplying chapter summaries, also written by LLM. I’m
still exploring this. If you’re feeling lazy, tell it to offer you three
possible story branches at each turn, and you pick the most interesting.
Or even tell it to combine two of them! LLMs are clever and will figure
it out. Some genres work better than others, and concrete works better
than abstract. (I wonder if professional writers judge its writing as
poor as I judge its programming.)</p>
  </li>
  <li>
    <p>Generative fun. Have an argument with Benjamin Franklin (note: this
probably violates the <a href="https://ai.meta.com/llama/use-policy/">Acceptable Use Policy</a> of some models), hang
out with a character from your favorite book, or generate a new scene of
<a href="https://nullprogram.com/blog/2023/06/22/#76-henry-iv">Falstaff’s blustering antics</a>. Talking to historical figures
has been educational: The character says something unexpected, I look it
up the old-fashioned way to see what it’s about, then learn something
new.</p>
  </li>
  <li>
    <p>Language translation. I’ve been browsing foreign language subreddits
through Gemma-2-2B translation, and it’s been insightful. (I had no idea
German speakers were so distrustful of artificial sweeteners.)</p>
  </li>
</ul>

<p>Despite the short list of useful applications, this is the most excited
I’ve been about a new technology in years!</p>



  
  <ol></ol>

  

  <nav>
  
    
  
  
  </nav>
</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ASCII Delimited Text – Not CSV or Tab Delimited Text (109 pts)]]></title>
            <link>https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/</link>
            <guid>42100499</guid>
            <pubDate>Sun, 10 Nov 2024 14:42:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/">https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/</a>, See on <a href="https://news.ycombinator.com/item?id=42100499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-21">
						
						
						<p><small>In <a href="https://ronaldduncan.wordpress.com/category/software/ascii/" rel="category tag">ASCII</a>, <a href="https://ronaldduncan.wordpress.com/category/software/development/" rel="category tag">development</a>, <a href="https://ronaldduncan.wordpress.com/category/software/file-formats/" rel="category tag">File Formats</a>, <a href="https://ronaldduncan.wordpress.com/category/software/" rel="category tag">software</a>, <a href="https://ronaldduncan.wordpress.com/category/technology/" rel="category tag">technology</a> on <strong>October 31, 2009</strong> at <strong>3:09 pm</strong></small></p><div>
							<p>Unfortunately a quick google search on “ASCII Delimited Text” shows that IBM and Oracle failed to read the ASCII specification and both define ASCII Delimited Text as a CSV format. &nbsp;ASCII Delimited Text should use the record separators defined as ASCII 28-31.</p>
<p>The most common formats are CSV (Comma Separated Values) and tab delimited text. &nbsp;Tab delimited text breaks when ever you have either a field with a tab or a new line in it, and CSV breaks depending on the implementation on Quotes, Commas and lines. Sadly Quotes, Commas and Tab characters are very common in text, and this makes the formats extremely bad for exporting and importing data. &nbsp;There are some other formats such as pipe (|) delimited text, and whilst better in that | is less frequently used they still suffer from being printable characters that are entered into text, and worst of all people, when they look at a file format and see the delimiter, think that it is a good idea to break things up with in fields using the same delimiter as the file format.</p>
<p>The <strong>most anoying thing</strong> about the <strong>whole problem</strong> is that it <strong>was solved by design</strong> in the <strong>ASCII character set</strong>.</p>
<p>If you use ASCII &nbsp;31 as your field separator instead of comma or tab, and ASCII 30 as your record separator instead of new line. &nbsp; Then you have a text file format that is trivial to write out and read in, with no restrictions on the text in fields or the need to try and escape characters.</p>
<p>It is even part of the design of the file encoding system. &nbsp;The ASCII standard calls these fields</p>
<ul>
<li>31 Unit Separator</li>
<li>30 Record Separator</li>
</ul>
<p>And ASCII has two more levels with Group and File Separators</p>
<ul>
<li>29 Group Separator</li>
<li>28 File Separator</li>
</ul>
<p>See <a href="http://en.wikipedia.org/wiki/Unit_separator">http://en.wikipedia.org/wiki/Unit_separator</a> and<br>
<a href="http://en.wikipedia.org/wiki/Delimiter#ASCII_Delimited_Text">http://en.wikipedia.org/wiki/Delimiter#ASCII_Delimited_Text</a></p>
<p>In summary ASCII Delimited Text is using the last 4 control characters (28-31) for their purpose as field and record delimiters and not using CSV (Comma Separated Values)</p>

			
																							</div>
					</div></div>]]></description>
        </item>
    </channel>
</rss>