<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 19 Jun 2024 07:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Google Gemini tried to kill me (178 pts)]]></title>
            <link>https://old.reddit.com/r/ChatGPT/comments/1diljf2/google_gemini_tried_to_kill_me/</link>
            <guid>40724283</guid>
            <pubDate>Wed, 19 Jun 2024 02:50:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/ChatGPT/comments/1diljf2/google_gemini_tried_to_kill_me/">https://old.reddit.com/r/ChatGPT/comments/1diljf2/google_gemini_tried_to_kill_me/</a>, See on <a href="https://news.ycombinator.com/item?id=40724283">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><hr>

<p><strong><a href="https://old.reddit.com/r/ChatGPT">r/ChatGPT</a> Rules</strong><br>
All users are encouraged to report posts to the moderators for review.</p>

<hr>

<p><strong>1: Malicious Communication</strong><br>
Posters and commenters are expected to act in good faith. Treat other users the way you want to be treated. Avoid straw-manning and bad-faith interpretations. Avoid presenting misinformation as factual.</p>

<p><strong>2: No Trashposts</strong><br>
Posts deemed to be entirely without value or effort may be removed if they have not generated interesting discussions before their discovery. Users are encouraged to report posts they feel are of significantly low effort. Specifically mentioning that ‚ÄúIs chat GPT down posts?‚Äù will be removed. The stickied FAQ deals with that.</p>

<p><strong>3: Self Advertising</strong><br>
Posts must be directly related to ChatGPT or the topic of LLMs. They may not be solely focused on advertising a single other LLM service. Find or establish a relative subreddit for that service.</p>

<p><strong>4: Political Discussion</strong><br>
Having ChatGPT create political content is completely fine. Discussion of the politics around AI and LLMs is allowed. This is not the place to discuss the merits of Trump's foreign policy or Hunter Biden‚Äôs laptop.</p>

<hr>

<p><strong><a href="https://discord.gg/r-chatgpt-1050422060352024636">Join us on Discord</a></strong></p>

<p><strong><a href="https://t.me/r_ChatGPT">Join us on Telegram</a></strong></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Senate passes bill to support advanced nuclear energy deployment (162 pts)]]></title>
            <link>https://www.reuters.com/business/energy/us-senate-passes-bill-support-advanced-nuclear-energy-deployment-2024-06-19/</link>
            <guid>40724201</guid>
            <pubDate>Wed, 19 Jun 2024 02:35:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/energy/us-senate-passes-bill-support-advanced-nuclear-energy-deployment-2024-06-19/">https://www.reuters.com/business/energy/us-senate-passes-bill-support-advanced-nuclear-energy-deployment-2024-06-19/</a>, See on <a href="https://news.ycombinator.com/item?id=40724201">Hacker News</a></p>
Couldn't get https://www.reuters.com/business/energy/us-senate-passes-bill-support-advanced-nuclear-energy-deployment-2024-06-19/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Willie Mays, SF Giants baseball player, has died (106 pts)]]></title>
            <link>https://www.sfchronicle.com/sports/giants/article/willie-mays-dead-obituary-17815215.php</link>
            <guid>40723758</guid>
            <pubDate>Wed, 19 Jun 2024 01:11:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sfchronicle.com/sports/giants/article/willie-mays-dead-obituary-17815215.php">https://www.sfchronicle.com/sports/giants/article/willie-mays-dead-obituary-17815215.php</a>, See on <a href="https://news.ycombinator.com/item?id=40723758">Hacker News</a></p>
Couldn't get https://www.sfchronicle.com/sports/giants/article/willie-mays-dead-obituary-17815215.php: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Open Source Python ETL (114 pts)]]></title>
            <link>https://amphi.ai/</link>
            <guid>40723356</guid>
            <pubDate>Wed, 19 Jun 2024 00:02:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://amphi.ai/">https://amphi.ai/</a>, See on <a href="https://news.ycombinator.com/item?id=40723356">Hacker News</a></p>
<div id="readability-page-1" class="page">

    <header>

        <nav>

            <div>
                <p><a href="https://amphi.ai/">
                    <img src="https://amphi.ai/icons/amphi_logo_paths.svg" alt="Amphi Logo">
                </a></p>
                
            </div>
        </nav>

    </header>

    <div>
            <div>
                
                <p>
                    Extract, transform and load data with low-code.
                    <br> Generate native Python code you can deploy anywhere.
                </p>

                <!--
                    <a href="https://www.linkedin.com/pulse/introducing-amphi-new-python-based-etl-thibaut-gourdel-xp16e/?trackingId=bYwCahXdTmiRMrPop6ASoA%3D%3D" target='_blank' class="inline-flex justify-between items-center py-1 px-1 pr-4 mb-5 text-sm text-gray-700 bg-gray-100 rounded-full dark:bg-gray-800 dark:text-white hover:bg-gray-200 dark:hover:bg-gray-700" role="alert">
                    <span class="text-xs bg-primary-600 rounded-full text-white px-4 py-1.5 mr-3">New</span> <span class="text-sm font-medium">Amphi's beta is out! Read announcement blog</span> 
                    <svg class="ml-2 w-5 h-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M7.293 14.707a1 1 0 010-1.414L10.586 10 7.293 6.707a1 1 0 011.414-1.414l4 4a1 1 0 010 1.414l-4 4a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg>
                     </a>
                    -->
                <p><code>
                        <span>
                            <span>
                                $
                            </span>
                    
                            <span>
                                <span>
                                    pip install
                                </span>
                    
                                <span>
                                    amphi-etl
                                </span>
                            </span>
                        </span>
                    
                    </code>

                </p>
            </div>
            <p><img src="https://amphi.ai/images/screenshot.png" alt="mockup">
            </p>
        </div>

    <div id="features">
            <div>
                
                <p>Amphi is a Python-based ETL designed
                    for
                    extracting, preparing and cleaning data from various sources and formats.
                    <br>Amphi excels at data integration from file to databases, data extraction and preparation for
                    data science and LLM-based systems, as well as API retrieval and enrichment.
                </p>

                <!--
                <p class="text-gray-500 sm:text-xl dark:text-gray-400 mb-2 lg:mb-2">üöÄ Use Amphi within the Jupyterlab
                    environment to design your data pipelines with a graphical user-interface and generate native Python
                    code you can deploy anywhere.</p>
                -->
                <p>üöÄ Use Amphi to design your data
                    pipelines with a graphical user-interface and generate native Python
                    code you can deploy anywhere.</p>
            </div>
            <div>
                <p><img src="https://amphi.ai/images/screenshot2.png" alt="Amphi screenshot"></p><div>
                    <h2>Learn how Amphi simplifies data
                        wrangling.</h2>

                    <p>
                        Use Amphi for efficient file integration, data extraction and preparation, handling formats such
                        as CSV, JSON and more.</p>

                    <a href="https://docs.amphi.ai/">
                        Documentation
                        <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
                            <path fill-rule="evenodd" d="M10.293 3.293a1 1 0 011.414 0l6 6a1 1 0 010 1.414l-6 6a1 1 0 01-1.414-1.414L14.586 11H3a1 1 0 110-2h11.586l-4.293-4.293a1 1 0 010-1.414z" clip-rule="evenodd"></path>
                        </svg>
                    </a>

                    <h3>Designed for data engineers and
                        scientists:</h3>
                    <!-- List -->
                    <ul role="list">
                        <li>
                            <!-- Icon -->
                            <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
                                <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path>
                            </svg>
                            <span>Structured and unstructured file ingestion (CSV, PDF, HTML ...)</span>
                        </li>
                        <li>
                            <!-- Icon -->
                            <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
                                <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path>
                            </svg>
                            <span>Data extraction, cleansing and preparation </span>
                        </li>
                        <li>
                            <!-- Icon -->
                            <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
                                <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path>
                            </svg>
                            <span>API retrieval and enrichement</span>
                        </li>
                    </ul>
                </div>
            </div>
            <div>
                <div>
                    <p><img src="https://amphi.ai/icons/mouse-pointer-24.svg" alt="bulb icon">
                    </p>
                    <h3>Low-Code Development</h3>
                    <p>Accelerate your data and ETL pipeline development with a
                        low-code tool, significantly reducing development and maintenance time compared to traditional
                        coding.</p>
                </div>
                <div>
                    <p><img src="https://amphi.ai/icons/circle-half-24.svg" alt="bulb icon">
                    </p>
                    <h3>Hybrid by Nature</h3>
                    <p>Hybrid by nature, the platform generates python code you
                        can deploy natively across various environments, from on-premises to cloud. Complete flexibility
                        and no lock-in.</p>
                </div>
                <div>
                    <p><img src="https://amphi.ai/icons/users-24.svg" alt="bulb icon">
                    </p>
                    <h3>Community-Driven</h3>
                    <p>Amphi is designed for flexibility and openness. Pipeline
                        definitions are stored as files for easy sharing, fostering collaboration and community
                        engagement.</p>
                </div>
                <div>
                    <p><img src="https://amphi.ai/icons/python.svg" alt="bulb icon">
                    </p>
                    <h3>Python Code Generation</h3>
                    <p>Develop data pipelines and generate native Python code
                        you own. Run the pipelines anywhere you'd like.</p>
                </div>
                <div>
                    <p><img src="https://amphi.ai/icons/lock-24.svg" alt="bulb icon">
                    </p>
                    <h3>Private &amp; Secure</h3>
                    <p>All data is stored and processed locally, and isn't
                        transferred on Amphi's servers. This ensures complete privacy and control.</p>
                </div>
                <div>
                    <p><img src="https://amphi.ai/icons/bulb.svg" alt="bulb icon">
                    </p>
                    <h3>AI-Native</h3>
                    <p>Embrace the future with our AI-native tool, designed to
                        integrate generative AI capabilities and address AI-oriented use cases such as RAG.</p>
                </div>
            </div>
        </div>

    


    <div>
                
                <p>Amphi for Jupyterlab is available
                    in public beta.</p>
                <p><a href="#">Get
                    started</a>
            </p></div>

    
    



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Why do message queue-based architectures seem less popular now? (169 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40723302</link>
            <guid>40723302</guid>
            <pubDate>Tue, 18 Jun 2024 23:50:39 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40723302">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="40723302">
      <td><span></span></td>      <td><center><a id="up_40723302" href="https://news.ycombinator.com/vote?id=40723302&amp;how=up&amp;goto=item%3Fid%3D40723302"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=40723302">Ask HN: Why do message queue-based architectures seem less popular now?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_40723302">112 points</span> by <a href="https://news.ycombinator.com/user?id=alexhutcheson">alexhutcheson</a> <span title="2024-06-18T23:50:39"><a href="https://news.ycombinator.com/item?id=40723302">4 hours ago</a></span> <span id="unv_40723302"></span> | <a href="https://news.ycombinator.com/hide?id=40723302&amp;goto=item%3Fid%3D40723302">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Why%20do%20message%20queue-based%20architectures%20seem%20less%20popular%20now%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=40723302&amp;auth=d89fd96bfaca9ed547dc18849946b15c16329347">favorite</a> | <a href="https://news.ycombinator.com/item?id=40723302">84&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>In the late 2000s and early 2010s, I remember seeing lots of hype around building distributed systems using message queues (e.g. Amazon SQS, RabbitMQ, ZeroMQ, etc.) A lot of companies had blog posts highlighting their use of message queues for asynchronous communication between nodes, and IIRC the official AWS design recommendations at the time pushed SQS pretty heavily.</p><p>Now, I almost never see engineering blog posts or HN posts highlighting use of message queues. I see occasional content related to Kafka, but nothing like the hype that message queues used to have.</p><p>What changed? Possible theories I'm aware of:</p><p>* Redis tackled most of the use-case, plus caching, so it no longer made sense to pay the operational cost of running a separate message broker. Kafka picked up the really high-scale applications.</p><p>* Databases (broadly defined) got a lot better at handling high scale, so system designers moved more of the "transient" application state into the main data stores.</p><p>* We collectively realize that message queues-based architectures don't work as well as we hoped, so we build most things in other ways now.</p><p>* The technology just got mature enough that it's not exciting to write about, but it's still really widely used.</p><p>If people have experience designing or implementing greenfield systems based on message queues, I'd be curious to hear about it. I'd also be interested in understanding any war stories or pain points people have had from using message queues in production systems.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[KidPix (453 pts)]]></title>
            <link>https://kidpix.app</link>
            <guid>40723220</guid>
            <pubDate>Tue, 18 Jun 2024 23:38:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kidpix.app">https://kidpix.app</a>, See on <a href="https://news.ycombinator.com/item?id=40723220">Hacker News</a></p>
<div id="readability-page-1" class="page">
<div id="container">
  
  <div id="hcontainer">
    
    <p id="paint">
      <canvas id="kiddopaint" width="1920" height="1200">:(</canvas>
    </p>
  </div>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Off-path TCP hijacking in NAT-enabled Wi-Fi networks (108 pts)]]></title>
            <link>https://blog.apnic.net/2024/06/18/off-path-tcp-hijacking-in-nat-enabled-wi-fi-networks/</link>
            <guid>40723150</guid>
            <pubDate>Tue, 18 Jun 2024 23:27:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.apnic.net/2024/06/18/off-path-tcp-hijacking-in-nat-enabled-wi-fi-networks/">https://blog.apnic.net/2024/06/18/off-path-tcp-hijacking-in-nat-enabled-wi-fi-networks/</a>, See on <a href="https://news.ycombinator.com/item?id=40723150">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-content">
                            <p><img width="555" height="202" src="https://blog.apnic.net/wp-content/uploads/2024/05/Off-path-hijack_ft-555x202.png?v=af34e3d1704d21e03c3ea0f5431f6fa27698ccf6df939e992668460ba6fa472d" alt="" decoding="async" fetchpriority="high" srcset="https://blog.apnic.net/wp-content/uploads/2024/05/Off-path-hijack_ft-555x202.png?v=af34e3d1704d21e03c3ea0f5431f6fa27698ccf6df939e992668460ba6fa472d 555w, https://blog.apnic.net/wp-content/uploads/2024/05/Off-path-hijack_ft-300x109.png?v=af34e3d1704d21e03c3ea0f5431f6fa27698ccf6df939e992668460ba6fa472d 300w, https://blog.apnic.net/wp-content/uploads/2024/05/Off-path-hijack_ft-1024x373.png?v=af34e3d1704d21e03c3ea0f5431f6fa27698ccf6df939e992668460ba6fa472d 1024w, https://blog.apnic.net/wp-content/uploads/2024/05/Off-path-hijack_ft-768x280.png?v=af34e3d1704d21e03c3ea0f5431f6fa27698ccf6df939e992668460ba6fa472d 768w, https://blog.apnic.net/wp-content/uploads/2024/05/Off-path-hijack_ft-624x227.png?v=af34e3d1704d21e03c3ea0f5431f6fa27698ccf6df939e992668460ba6fa472d 624w, https://blog.apnic.net/wp-content/uploads/2024/05/Off-path-hijack_ft-206x75.png?v=af34e3d1704d21e03c3ea0f5431f6fa27698ccf6df939e992668460ba6fa472d 206w, https://blog.apnic.net/wp-content/uploads/2024/05/Off-path-hijack_ft-256x93.png?v=af34e3d1704d21e03c3ea0f5431f6fa27698ccf6df939e992668460ba6fa472d 256w, https://blog.apnic.net/wp-content/uploads/2024/05/Off-path-hijack_ft.png?v=af34e3d1704d21e03c3ea0f5431f6fa27698ccf6df939e992668460ba6fa472d 1110w" sizes="(max-width: 555px) 100vw, 555px"></p><p><em>This post and paper were co-authored by Xuewei Feng, Qi Li, Kun Sun, Ziqiang Wang, and Ke Xu</em>.</p>



<p>Wi-Fi has emerged as one of the most popular technologies for providing Internet access, but it is also frequently exploited by malicious actors to launch various attacks. With the deployment of wireless security mechanisms like WPA2/WPA3 and the adoption of other protective strategies such as Access Point (AP) isolation, Address Resolution Protocol (ARP) protection, and rogue AP detection, off-path attackers (those unable to control the router) are finding it increasingly difficult to obtain confidential information of Wi-Fi users.</p>



<p>Our recent discovery in router firmware exposes a security flaw in routers‚Äô Network Address Translation (NAT) mapping handling, which can be exploited by attackers to bypass TCP‚Äôs built-in randomization. This facilitates off-path TCP hijacking attacks, intercepting Wi-Fi TCP traffic. Our <a href="https://www.ndss-symposium.org/ndss-paper/exploiting-sequence-number-leakage-tcp-hijacking-in-nat-enabled-wi-fi-networks/" target="_blank" rel="noreferrer noopener">research paper detailing the attack</a> has been accepted by NDSS 2024,</p>



<p>Figure 1 illustrates the scenario where an attacker and a victim client are connected to the same Wi-Fi network to access Internet services (for example, consider strangers connecting to the same Wi-Fi network of a coffee shop). For the attacker to hijack the TCP connection between the victim and the server offering common services such as social media or online finance, the attacker must detect the existence of the TCP connection first. Upon confirmation, the attacker then proceeds to infer the sequence and acknowledgment numbers of the ongoing bidirectional communication.</p>



<figure><a href="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-1.png"><img decoding="async" width="1024" height="476" src="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-1-1024x476.png" alt="Figure 1 ‚Äî Threat model of TCP hijacking attacks in Wi-Fi networks." srcset="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-1-1024x476.png 1024w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-1-300x140.png 300w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-1-768x357.png 768w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-1-1536x715.png 1536w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-1-624x290.png 624w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-1-1320x614.png 1320w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-1.png 1541w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Figure 1 ‚Äî Threat model of TCP hijacking attacks in Wi-Fi networks.</figcaption></figure>



<h2>Attack steps</h2>



<p>We‚Äôve observed that routers often employ port preservation strategies during NAT and lack reverse path validation as required by <a href="https://datatracker.ietf.org/doc/html/rfc3704" target="_blank" rel="noreferrer noopener">RFC 3704</a>, enabling attackers to deduce the source ports of other client connections. In the first step, following the method illustrated in Figure 2, the attacker can deduce the source port of other clients by altering the source port numbers specified in the forged SYN and SYN/ACK packets, then observing if it can receive the SYN/ACK sent by itself until identifying the correct port used by the victim client for subsequent attacks.</p>



<figure><a href="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-2.png"><img decoding="async" width="1006" height="1024" src="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-2-1006x1024.png" alt="Figure 2 ‚Äî Inferring the source port of the victim TCP connection." srcset="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-2-1006x1024.png 1006w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-2-295x300.png 295w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-2-768x782.png 768w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-2-624x635.png 624w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-2-1320x1343.png 1320w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-2.png 1419w" sizes="(max-width: 1006px) 100vw, 1006px"></a><figcaption>Figure 2 ‚Äî Inferring the source port of the victim TCP connection.</figcaption></figure>



<p>We‚Äôve found that most routers, for performance reasons, do not rigorously inspect the sequence numbers of TCP packets. Consequently, this introduces serious security vulnerabilities that attackers can exploit by crafting forged reset (RST) packets to maliciously clear NAT mappings in the router. In the second attack step, the attacker proceeds to steal the sequence number (SEQ) and acknowledgment number (ACK) of the normal TCP connection between the victim client and the server, as depicted in Figure 3.</p>



<figure><a href="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-3.png"><img loading="lazy" decoding="async" width="1024" height="899" src="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-3-1024x899.png" alt="Figure 3 ‚Äî Hijacking active connections." srcset="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-3-1024x899.png 1024w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-3-300x263.png 300w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-3-768x674.png 768w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-3-1536x1348.png 1536w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-3-624x548.png 624w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-3-1320x1159.png 1320w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-3.png 1652w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Figure 3 ‚Äî Hijacking active connections.</figcaption></figure>



<p>Once the attacker has obtained the source port, sequence number, and acknowledgment number used by the client connection, it can initiate TCP connection manipulation attacks. The TCP protocol is a critical foundational protocol of the Internet, carrying important network application protocols such as SSH, HTTP, and FTP. Therefore, hijacking attacks targeting TCP can be applied across various scenarios. For instance, SSH denial of service attacks, FTP private file downloads, and HTTP cache pollution, among others. Figure 4 illustrates the effect of an attacker poisoning a victim‚Äôs HTTP webpage.</p>



<figure><a href="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-4.png"><img loading="lazy" decoding="async" width="1024" height="539" src="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-4-1024x539.png" alt="Figure 4 ‚Äî Snapshots of web poisoning." srcset="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-4-1024x539.png 1024w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-4-300x158.png 300w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-4-768x404.png 768w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-4-1536x808.png 1536w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-4-624x328.png 624w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-4-1320x694.png 1320w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-4.png 1926w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Figure 4 ‚Äî Snapshots of web poisoning.</figcaption></figure>



<h2>Empirical study</h2>



<p>We conducted tests on 67 mainstream routers from 30 different manufacturers, including 360, Aruba, ASUS, Amazon, Cisco Meraki, China Mobile, Comfast, D-Link, GL.iNet, Google, H3C, Huawei, IP-COM, iKuai, JdCloud, Linksys, Mercury, Netgear, Netcore, Ruijie, Skyworth, Tenda, TP-Link, Ubiquiti, Volans, Wavlink, WiMaster, Xiaomi, ZTE, pfSense, and others. Among these, we found that 52 routers from 24 manufacturers were susceptible to this attack. The test results are illustrated in Table 1.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="727" src="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-5-1024x727.png" alt="Table 1 ‚Äî Partially tested routers from 30 vendors." srcset="https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-5-1024x727.png 1024w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-5-300x213.png 300w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-5-768x545.png 768w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-5-1536x1090.png 1536w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-5-2048x1453.png 2048w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-5-624x443.png 624w, https://blog.apnic.net/wp-content/uploads/2024/05/TCP-Hijacking-NAT-5-1320x937.png 1320w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Table 1 ‚Äî Partially tested routers from 30 vendors.</figcaption></figure>



<p>Additionally, we conducted measurement studies on 93 real-world Wi-Fi networks and found that 75 (81%) of them were susceptible to this attack. Our case studies indicate that terminating SSH connections, downloading private files from FTP servers, and injecting false HTTP response packets on average took 17.5, 19.4, and 54.5 seconds, respectively, with success rates of 87.4%, 82.6%, and 76.1%. The detailed experimental results are shown in the paper.</p>



<h2>Mitigation</h2>



<p>We have registered the issue to the affected manufacturers by submitting vulnerability reports and contacting them via email. As of now, we have received a positive response from the OpenWrt community, confirming our findings and releasing patches to fix the vulnerability. Additionally, seven router vendors (namely TP-Link, Huawei, Xiaomi, 360, Mercury, Ubiquiti, and Linksys) have acknowledged our report and are actively working to fix their products. Furthermore, we have been assigned 10 CVE identifiers for different vendors. Other vendors are still investigating the vulnerability.</p>



<p>To mitigate this attack, we suggest three countermeasures:</p>



<ol>
<li><strong>Random port allocation:</strong> Routers should employ a random selection strategy when creating new NAT mappings.</li>



<li><strong>Reverse path validation:</strong> Following the recommendation of RFC 3704, strict mode is utilized to filter out spoofed packets. In our testing, routers from ASUS, Netgear, ZTE, Aruba, Cisco Meraki, TP-LINK, and Mercury default to this recommendation, thus enhancing defence against our attacks.</li>



<li><strong>TCP window checking:</strong> We advocate for routers to rigorously inspect the sequence and acknowledgment numbers of received packets. OpenWrt has already implemented this mitigation after our disclosure.</li>
</ol>



<p><em>Yuxiang Yang is a PhD student. His research interests are network security and program analysis.</em></p>



<p><em>Xuewei Feng works on network security from a large-scale Internet measurements point of view.</em></p>



<p><em>Qi Li is an associate professor at Tsinghua University. His research interests are in network and system security.</em></p>



<p><em>Kun Sun is a professor at George Mason University. His research focuses on systems and network security. </em></p>



<p><em>Ziqiang Wang is a PhD student. His research focuses on network security.</em></p>



<p><em>Ke Xu is a professor at Tsinghua University. His research focuses on Internet architecture and network security.</em></p>

                            <!-- DISCUSS ON HN BUTTON: START -->
                                                            <p><a href="https://news.ycombinator.com/item?id=40723150" target="_blank">
                                    <i></i>
                                    <span>Discuss on Hacker News</span>
                                </a></p><hr>

                            <p id="views-disclaimer">The views expressed by the authors of this blog are their own
                                and do not necessarily reflect the views of APNIC. Please note a <a href="https://blog.apnic.net/?p=395">Code of Conduct</a> applies to this blog.
                            </p>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon fined $5.9M for breaking labor law in California (210 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2024/06/18/amazon-fine-labor-law-california/</link>
            <guid>40722155</guid>
            <pubDate>Tue, 18 Jun 2024 20:59:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2024/06/18/amazon-fine-labor-law-california/">https://www.washingtonpost.com/technology/2024/06/18/amazon-fine-labor-law-california/</a>, See on <a href="https://news.ycombinator.com/item?id=40722155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="ZXYJEWUEY5G67A27J5343ZQQTE" data-el="text" dir="null">SAN FRANCISCO ‚Äî California labor officials fined Amazon $5.9 million for violating a state law aimed at preventing warehouse workers from being pushed to work so quickly that their health and safety are at risk, according to citations issued in May.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="FIA3VBW3P5D7RNGMI6AIDHPPCQ" data-el="text" dir="null">It‚Äôs the largest fine the California Labor Commissioner‚Äôs Office has levied under the Warehouse Quota Law, which went into effect in 2022 and limits quotas for ‚Äúwork that must be performed at a specified speed or the worker suffers discipline,‚Äù the commission‚Äôs officer said in a news release Tuesday.</p></div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="JYM5VQJCFFH45PZ2DVXJ5Y3554" data-el="text" dir="null">California investigated two Amazon facilities near Los Angeles and in May found that the company failed to ‚Äúprovide written notice of quotas to which each employee is subject,‚Äù according to a copy of the citation shared with The Washington Post by the Warehouse Worker Resource Center, a nonprofit that advocates for improving working conditions at warehouses. The labor agency levied fines of $1.2 million at one Redland, Calif., Amazon facility, and $4.7 million at another in Moreno Valley.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="6VD24L6MXFHEHBKV5PXV3XJ2LY" data-el="text" dir="null">California Labor Commissioner Lilia Garc√≠a-Brower said in a statement that the ‚Äúundisclosed‚Äù quota system Amazon ‚Äúwas using in these two warehouses is exactly the kind of system that the Warehouse Quotas law was put in place to prevent.‚Äù</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="QQR5WJFXKREQRIKQHGLNRA423U" data-el="text" dir="null">‚ÄúUndisclosed quotas expose workers to increased pressure to work faster and can lead to higher injury rates and other violations by forcing workers to skip breaks,‚Äù the statement said.</p><div role="group" aria-roledescription="carousel" data-qa="article-body"><h2>GET CAUGHT UP<p>Stories to keep you informed</p></h2></div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="HTHAXL7LDJB7BCERQD42LMDQK4" data-el="text" dir="null">‚ÄúThe fact that workers are not informed of what quotas they are supposed to meet is highly dehumanizing,‚Äù said Deogracia Cornelio, a labor activist with the Warehouse Worker Resource Center, at a news conference Tuesday. ‚ÄúIt‚Äôs stressful. It leads people to have accidents.<b>‚Äù</b></p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="RZ4SHNWUUFAPJGQFIJQ7RYHFKI" data-el="text" dir="null">Amazon, the second-largest private employer in the United States, has long been criticized for the pace of work in its fulfillment centers and delivery stations. It is under investigation by <a href="https://www.washingtonpost.com/technology/2023/09/18/amazon-working-conditions-safety-osha-doj/?itid=lk_inline_manual_12" target="_blank">federal labor regulators</a>, a <a href="https://www.washingtonpost.com/business/2023/06/20/sanders-investigation-sentate-amazon/?itid=lk_inline_manual_12" target="_blank">congressional committee</a> and the U.S. attorney for the Southern District of New York regarding its <a href="https://www.washingtonpost.com/technology/2021/06/01/amazon-osha-injury-rate/?itid=lk_inline_manual_12" target="_blank">workplace injury rate</a>.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="762E3ODW6ZE3LHNQFK33IAKGZU" data-el="text" dir="null">‚ÄúWe disagree with the allegations made in the citations and have appealed,‚Äù  Amazon spokesperson Maureen Lynch Vogel stated in an email.  ‚ÄúThe truth is, we don‚Äôt have fixed quotas.‚Äù</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="SBNTJ4NL7NCMFJDJS4ATJNJVUU" data-el="text" dir="null">Amazon founder Jeff Bezos owns The Washington Post.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="LKCME2HSCNEGLIKACKJQP7IAXI" data-el="text" dir="null">Amazon  joins Sysco and Dollar General, which were fined $318,000 and $1.3 million in October and November, respectively, according to copies of the citations shared with The Post.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="ZUA7Q4ILZRBRPGETTZW6R6J7AI" data-el="text" dir="null">The fines against Amazon are small compared with the company‚Äôs size ‚Äî it brought in $574 billion in revenue last year ‚Äî but significant for a state labor agency. The Occupational Safety and Health Administration, the federal agency charged with preventing workplace safety issues, frequently investigates Amazon workplaces and has issued dozens of citations, but is severely limited in the size of fines it can bring. For example, Amazon was fined $7,000 after an <a href="https://www.washingtonpost.com/technology/2023/11/26/amazon-warehouse-death-7000-fine/?itid=lk_inline_manual_18" target="_blank">Indiana employee died in a workplace accident</a> last year.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="XRB3VRHZXBCAND6II3P27VDU2Y" data-el="text" dir="null">In Washington state, workplace safety regulators have repeatedly cited Amazon with ‚Äú<a href="https://www.washingtonpost.com/technology/2023/09/18/amazon-working-conditions-safety-osha-doj/?itid=lk_inline_manual_20" target="_blank">willful‚Äù violations</a> over ergonomic injuries that can lead to musculoskeletal disorders. The designation means the company knowingly and repeatedly failed to improve conditions for workers. The fines involved, which Amazon is contesting, totaled $60,000 in 2022 and $85,000 in 2023.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="M33PKHTL3FDI5OUIYWGOV6DBYI" data-el="text" dir="null">OSHA can pursue a corporate-wide settlement with employers that are repeat offenders, as it did with Dollar Tree and Family Dollar in 2023, a deal that cost the company $1.35 million and followed six years of investigation and more than $15 million in fines. But it‚Äôs unclear whether the agency is pursuing that strategy with Amazon.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="TOUR4SLEU5B2ZAMDRUMMJYMSHM" data-el="text" dir="null">The California law, known as AB 701, was sponsored by Lorena Gonzalez, who was a union official before becoming a State Assembly member and has since returned to labor advocacy. The bill was backed by unions including the Los Angeles County Federation of Labor and the Teamsters, which recently <a href="https://www.washingtonpost.com/technology/2024/06/04/amazon-union-teamsters/?itid=lk_inline_manual_23" target="_blank">signed an affiliation deal with the Amazon Labor Union</a> in New York.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="OUFN4CBUVJET3KOBO5OPYIELBI" data-el="text" dir="null">The Teamsters, which helped UPS workers <a href="https://www.washingtonpost.com/business/2023/07/25/ups-strike-deal-teamsters/?itid=lk_inline_manual_25" target="_blank">win a new contract last summer</a> after threats of a strike, offers much-needed financial, legal and organizational resources to the previously independent Amazon Labor Union, which Amazon still hasn‚Äôt recognized or agreed to bargain with two years after the group secured the <a href="https://www.washingtonpost.com/technology/2022/04/01/amazon-union-staten-island/?itid=lk_inline_manual_25" target="_blank">first union victory at an Amazon warehouse</a> in Staten Island in April 2022.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="ZNAIRP6T6BHVXFJEEWS4HSO3XA" data-el="text" dir="null">Warehouse quota regulations similar to the California law are now on the books in Washington state, New York, Oregon and Minnesota, according to the Warehouse Worker Resource Center statement.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="AEG66KAYEVGHRLV73R2ZNHXRXM" data-el="text" dir="null">Meanwhile, Sen. Edward J. Markey (D-Mass.)<a href="https://twitter.com/SenMarkey/status/1591827463583453190" target="_blank"> </a>has proposed a federal warehouse worker protection bill.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="GCG7YRIG7ZBOTO4AFWLQKQP45U" data-el="text" dir="null">‚ÄúToday, California took an essential step forward in fighting for warehouse worker protection and dignity, holding Amazon to account for a punishing work speed quota system that pushes workers to their physical limits ‚Äù he stated in an email.  ‚ÄúBut we need more than a patchwork of state laws.‚Äù</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Noam Chomsky has passed away (129 pts)]]></title>
            <link>https://twitter.com/PeoplesMomentum/status/1803142677174911102</link>
            <guid>40721271</guid>
            <pubDate>Tue, 18 Jun 2024 19:30:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/PeoplesMomentum/status/1803142677174911102">https://twitter.com/PeoplesMomentum/status/1803142677174911102</a>, See on <a href="https://news.ycombinator.com/item?id=40721271">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Nature retracts paper that claimed adult stem cell could become any type of cell (250 pts)]]></title>
            <link>https://retractionwatch.com/2024/06/18/nature-retracts-highly-cited-2002-paper-that-claimed-adult-stem-cells-could-become-any-type-of-cell/</link>
            <guid>40720629</guid>
            <pubDate>Tue, 18 Jun 2024 18:23:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://retractionwatch.com/2024/06/18/nature-retracts-highly-cited-2002-paper-that-claimed-adult-stem-cells-could-become-any-type-of-cell/">https://retractionwatch.com/2024/06/18/nature-retracts-highly-cited-2002-paper-that-claimed-adult-stem-cells-could-become-any-type-of-cell/</a>, See on <a href="https://news.ycombinator.com/item?id=40720629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main">

		
<article id="post-129437">
	
	<!-- .entry-header -->

	<div>
		<div>
<figure><a href="https://i0.wp.com/retractionwatch.com/wp-content/uploads/2024/06/catherine_2.jpg?ssl=1"><img fetchpriority="high" decoding="async" width="240" height="345" src="https://i0.wp.com/retractionwatch.com/wp-content/uploads/2024/06/catherine_2.jpg?resize=240%2C345&amp;ssl=1" alt="" srcset="https://i0.wp.com/retractionwatch.com/wp-content/uploads/2024/06/catherine_2.jpg?w=240&amp;ssl=1 240w, https://i0.wp.com/retractionwatch.com/wp-content/uploads/2024/06/catherine_2.jpg?resize=209%2C300&amp;ssl=1 209w" sizes="(max-width: 240px) 100vw, 240px" data-recalc-dims="1"></a></figure></div>


<p><em>Nature</em> has retracted a 2002 paper from the lab of Catherine Verfaillie purporting to show a type of adult stem cell could, under certain circumstances, ‚Äúcontribute to most, if not all, somatic cell types.‚Äù&nbsp;</p>



<p>The retracted article, ‚Äú<a href="https://www.nature.com/articles/nature00870">Pluripotency of mesenchymal stem cells derived from adult marrow</a>,‚Äù has been <a href="https://www.wsj.com/articles/SB1024602878849692040">controversial since its publication</a>. Still, it has been cited nearly 4,500 times, according to Clarivate‚Äôs Web of Science ‚Äì making it by far <a href="https://retractionwatch.com/the-retraction-watch-leaderboard/top-10-most-highly-cited-retracted-papers/">the most-cited retracted paper ever</a>.</p>



<p>In 2007, <em>New Scientist</em> reported on questions about <a href="https://www.newscientist.com/article/mg19325915-200-flawed-stem-cell-data-withdrawn/">data in the <em>Nature</em> paper</a> and <a href="https://www.newscientist.com/article/mg19325964-600-fresh-questions-on-stem-cell-findings/">another</a> of Verfaille‚Äôs <a href="https://ashpublications.org/blood/article/98/9/2615/53471/Purification-and-ex-vivo-expansion-of-postnatal">articles in <em>Blood</em></a>. <em>Nature </em><a href="https://www.nature.com/articles/nature05812">published a correction</a> that year.&nbsp;</p>



<p>The errors the authors corrected ‚Äúdo not alter the conclusions of the Article,‚Äù they wrote in the notice.&nbsp;</p>



<p>The University of Minnesota-Twin Cities in Minneapolis, where Verfaillie worked when the <em>Nature</em> paper was published, in 2008 found the <em>Blood</em> paper <a href="https://archive.ph/20120712090732/http://jobs.chronicle.com/article/U-of-Minnesota-Panel-Says/41756/">contained falsified images</a>, but Verfaillie was not responsible for the manipulations. <em>Blood</em> <a href="https://ashpublications.org/blood/article/113/10/2370/24360/Reyes-M-Lund-T-Lenvik-T-Aguiar-D-Koodie-L">retracted the article</a> in 2009 at the request of the authors.&nbsp;</p>



<p>Verfaillie moved to KU Leuven, where she is now an <a href="https://www.kuleuven.be/wieiswie/en/person/00048658">emeritus professor</a>. She has not responded to our request for comment.&nbsp;</p>



<p>KU Leuven conducted an investigation of Verfaillie‚Äôs work in 2019-2020, after Elisabeth Bik posted questions about the data in her papers, including <a href="https://pubpeer.com/publications/DF95522E3585E37663CAD1972E70BD#">the one from 2002 in <em>Nature</em></a>, on PubPeer. The <a href="https://nieuws.kuleuven.be/en/content/2020/ku-leuven-completes-research-on-possible-breach-of-research-integrity">university found</a> ‚Äúno breach of research integrity in the publications investigated.‚Äù&nbsp;</p>



<p>Bik tweeted about the retraction:&nbsp;</p>



<figure></figure>



<p>The notice mentions two image duplications Bik wrote about on PubPeer. Because the authors could not retrieve the original images, it states:&nbsp;</p>



<blockquote>
<p>the Editors no longer have confidence that the conclusion that multipotent adult progenitor cells (MAPCs) engraft in the bone marrow is supported.</p>



<p>Given the concerns above the Editors no longer have confidence in the reliability of the data reported in this article.</p>
</blockquote>



<p>According to the notice, most of the authors, including Verfaillie, agreed with the retraction. She now has four retractions, by <a href="http://retractiondatabase.org/RetractionSearch.aspx#?auth%3dVerfaillie%252c%2bCatherine%2bM">our count</a>.</p>



<p><em>Like Retraction Watch? You can make a&nbsp;</em><a href="http://paypal.com/us/fundraiser/charity/1909130"><em>tax-deductible contribution to support our work</em></a><em>,&nbsp;subscribe to our free&nbsp;<a href="http://eepurl.com/bNRlUn"><em>daily digest</em></a></em>&nbsp;<em>or&nbsp;<a href="https://retractionwatch.substack.com/">paid weekly update</a></em>,&nbsp;<em>follow us&nbsp;</em><a href="http://twitter.com/RetractionWatch"><em>on Twitter</em></a><em>, like us&nbsp;</em><a href="https://www.facebook.com/pages/Retraction-Watch/119209094864356"><em>on Facebook</em></a><em>, or add us to your&nbsp;</em><a href="https://retractionwatch.com/feed/" target="_blank" rel="noreferrer noopener"><em>RSS reader</em></a><em>. If you find a retraction that‚Äôs&nbsp;</em><a href="http://retractiondatabase.org/RetractionSearch.aspx"><em>not in The Retraction Watch Database</em></a><em>, you can&nbsp;</em><a href="https://docs.google.com/forms/d/e/1FAIpQLSeAsw4i5J8M7sOQ9GiG0_dglkim9gdPPba92yZRLfCq4u-o7w/viewform?c=0&amp;w=1"><em>let us know here</em></a><em>. For comments or feedback, email us at team@retractionwatch.com</em>.</p>




	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article><!-- #post-129437 -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electricity prices in France turn negative as renewable energy floods the grid (126 pts)]]></title>
            <link>https://fortune.com/2024/06/16/electricity-prices-france-negative-renewable-energy-supply-solar-power-wind-turbines/</link>
            <guid>40720183</guid>
            <pubDate>Tue, 18 Jun 2024 17:31:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/2024/06/16/electricity-prices-france-negative-renewable-energy-supply-solar-power-wind-turbines/">https://fortune.com/2024/06/16/electricity-prices-france-negative-renewable-energy-supply-solar-power-wind-turbines/</a>, See on <a href="https://news.ycombinator.com/item?id=40720183">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="article-content" id="article-content"><p>French electricity prices turned negative as a drop in demand and surging renewables output prompted some nuclear reactors to power down.</p><div>



<p>Daily consumption from Thursday through Sunday is seen falling by an average 6 gigawatts, a Bloomberg model shows. Sunny and blustery weather has driven up solar and wind generation, prompting the grid operator to request that <a href="https://fortune.com/company/electricite-de-france/" target="_blank" aria-label="Go to https://fortune.com/company/electricite-de-france/">Electricite de France</a> SA take several nuclear plants offline.</p>



<p>While more clean power is needed across Europe to reach climate goals, soaring renewables output and a lack of battery storage mean reactors sometimes have to be turned off during periods of low demand. It‚Äôs becoming increasingly common around weekends in France ‚Äî which gets about two-thirds of its electricity from its atomic fleet ‚Äî and also occurs in the Nordic region and Spain.</p>



<p>EDF halted its Golfech 2, Cruas 2 and Tricastin 1 nuclear plants, and plans to halt three others during the weekend. Some renewables producers will also have to curb generation to avoid paying a fee amid negative prices.</p>



<p>French day-ahead power fell to -‚Ç¨5.76 a megawatt-hour, the lowest in four years, in an auction on Epex Spot. Germany‚Äôs equivalent contract dropped to ‚Ç¨7.64.</p></div><p>Subscribe to the Fortune Next to Lead newsletter to get weekly strategies on how to make it to the corner office. <a href="https://fortune.com/newsletters/next-to-lead?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=next_to_lead" target="_self" aria-label="Go to https://fortune.com/newsletters/next-to-lead?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=next_to_lead">Sign up for free</a> before it launches on June 24, 2024.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Refusal in language models is mediated by a single direction (106 pts)]]></title>
            <link>https://arxiv.org/abs/2406.11717</link>
            <guid>40719981</guid>
            <pubDate>Tue, 18 Jun 2024 17:09:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2406.11717">https://arxiv.org/abs/2406.11717</a>, See on <a href="https://news.ycombinator.com/item?id=40719981">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2406.11717">View PDF</a></p><blockquote>
            <span>Abstract:</span>Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Andy Arditi [<a href="https://arxiv.org/show-email/1b890f91/2406.11717">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 17 Jun 2024 16:36:12 UTC (237 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[3D Gaussian Splatting as Markov Chain Monte Carlo (199 pts)]]></title>
            <link>https://ubc-vision.github.io/3dgs-mcmc/</link>
            <guid>40719975</guid>
            <pubDate>Tue, 18 Jun 2024 17:08:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ubc-vision.github.io/3dgs-mcmc/">https://ubc-vision.github.io/3dgs-mcmc/</a>, See on <a href="https://news.ycombinator.com/item?id=40719975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>
        3D Gaussian Splatting as Markov Chain Monte Carlo
      </p>

      <div>
            

            <p><span><sup>1</sup>University of British Columbia</span>
              <span><sup>2</sup>Google Research</span>
              <span><sup>3</sup>Google DeepMind</span>
              <br>
              <span><sup>4</sup>Simon Fraser University</span>
              <span><sup>5</sup>University of Toronto</span>
            </p>
          </div>

      

      

      <div>
        
        
        <p>
          Novel view reconstructions for <strong>(right) our method</strong>
          and <strong>(left) conventional</strong> 3D Gaussian Splatting with
          random initializations. Our method, even with random initialization,
          faithfully reconstructs the scene (e.g.. buildings at the back and
          the ground texture) providing much higher quality renderings.
        </p>
      </div>

      

      <h2>Abstract</h2>
      <p>
          While 3D Gaussian Splatting has recently become popular for neural
          rendering, current methods rely on carefully engineered cloning and
          splitting strategies for placing Gaussians, which can lead to
          poor-quality renderings, and reliance on a good initialization. In
          this work, we rethink the set of 3D Gaussians as a random sample
          drawn from an underlying probability distribution describing the
          physical representation of the scene---in other words, Markov Chain
          Monte Carlo (MCMC) samples. Under this view, we show that the 3D
          Gaussian updates can be converted as Stochastic Gradient Langevin
          Dynamics (SGLD) update by simply introducing noise. We then rewrite
          the densification and pruning strategies in 3D Gaussian Splatting as
          simply a deterministic state transition of MCMC samples, removing
          these heuristics from the framework. To do so, we revise the
          `cloning' of Gaussians into a relocalization scheme that
          approximately preserves sample probability. To encourage efficient
          use of Gaussians, we introduce a regularizer that promotes the
          removal of unused Gaussians. On various standard evaluation scenes,
          we show that our method provides improved rendering quality, easy
          control over the number of Gaussians, and robustness to
          initialization.
        </p>

      

      <h2>More Results</h2>

      <div>
        <table "="">
        <tbody><tr>
          <td colspan="4">
            '10' sequence from OMMO dataset
          </td>
        </tr>
        <tr>
          <td>3DGS-Random</td>
          <td>3DGS</td>
        </tr>
        <tr>
          <td colspan="2">
            <video id="00" width="95%" preload="auto" playsinline="" webkit-playsinline="" loop="" autoplay="" muted="">
              <source src="https://ubc-vision.github.io/3dgs-mcmc/resources/10/10.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>Ours-Random</td>
          <td>Ours</td>
        </tr>

        </tbody></table><table "="">
        <tbody><tr>
          <td colspan="4">
            'Stump' sequence from the MipNeRF360 dataset (pay attention to the
            details between the leaves)
          </td>
        </tr>
        <tr>
          <td>3DGS-Random</td>
          <td>3DGS</td>
        </tr>
        <tr>
          <td colspan="2">
            <video id="00" width="95%" preload="auto" playsinline="" webkit-playsinline="" loop="" autoplay="" muted="">
              <source src="https://ubc-vision.github.io/3dgs-mcmc/resources/stump/stump.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>Ours-Random</td>
          <td>Ours</td>
        </tr>
      
    
  

</tbody></table></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sharing new research, models, and datasets from Meta FAIR (214 pts)]]></title>
            <link>https://ai.meta.com/blog/meta-fair-research-new-releases/</link>
            <guid>40719921</guid>
            <pubDate>Tue, 18 Jun 2024 17:01:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/blog/meta-fair-research-new-releases/">https://ai.meta.com/blog/meta-fair-research-new-releases/</a>, See on <a href="https://news.ycombinator.com/item?id=40719921">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><b>Takeaways:</b></p><br><ul><li>Today, Meta FAIR is publicly releasing several new research artifacts. Our hope is that the research community can use them to innovate, explore, and discover new ways to apply AI at scale.</li><li>These lines of work build on our key principles of openness, collaboration, excellence, and scale.</li><li>We believe that access to state-of-the-art AI creates opportunities for everyone. That‚Äôs why we‚Äôre committed to the continued growth and development of an open AI ecosystem.</li></ul><br></div><div><ul><p>RECOMMENDED READS</p><li><a href="https://ai.meta.com/blog/meta-llama-3/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_self" data-lnfb-mode="ie"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path></svg><p>Introducing Meta Llama 3: The most capable openly available LLM to date</p></a></li><li><a href="https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_self" data-lnfb-mode="ie"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path></svg><p>OpenEQA: From word models to world models</p></a></li><li><a href="https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_self" data-lnfb-mode="ie"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path></svg><p>V-JEPA: The next step toward Yann LeCun‚Äôs vision of advanced machine intelligence</p></a></li></ul><div><p>For <a href="https://ai.meta.com/blog/fair-10-year-anniversary-open-science-meta/" target="_blank" data-lnfb-mode="ie"><u>more than a decade</u></a>, Meta‚Äôs Fundamental AI Research (FAIR) team has focused on advancing the state of the art in AI through open research. As innovation in the field continues to move at a rapid pace, we believe that collaboration with the global AI community is more important than ever. Maintaining an open science approach and sharing our work with the community help us stay true to our goal of building AI systems that work well for everyone and bring the world closer together.</p><p>Today, we‚Äôre excited to share some of the most recent FAIR research models with the global community. We‚Äôre publicly releasing six research artifacts that focus on themes at the core of our work: innovation, creativity, efficiency, and responsibility. These releases include image-to-text and text-to-music generation models, a multi-token prediction model, and a technique for detecting AI-generated speech. By publicly sharing our early research work, we hope to inspire iterations and ultimately help advance AI in a responsible way. We can‚Äôt wait to see what the community builds with these latest releases and continue the important conversations we‚Äôre having with the open source community.</p><br></div></div><p>Meta Chameleon</p><div><p>As we shared in our <a href="https://arxiv.org/abs/2405.09818" target="_blank" data-lnfb-mode="ie"><u>research paper</u></a> last month, Meta Chameleon is a family of models that can combine text and images as input and output any combination of text and images with a single unified architecture for both encoding and decoding. While most current late-fusion models use diffusion-based learning, Meta Chameleon uses tokenization for text and images. This enables a more unified approach and makes the model easier to design, maintain, and scale. The possibilities are endless‚Äîimagine generating creative captions for images or using a mix of text prompts and images to create an entirely new scene.</p><br></div></div><div><div><p>Today, we‚Äôre publicly releasing key components of our Chameleon 7B and 34B models under a research-only license. The models we‚Äôre releasing today were safety tuned and support mixed-modal inputs and text-only output to be used for research purposes. While we‚Äôve taken steps to develop these models responsibly, we recognize that risks remain. At this time, we are not releasing the Chameleon image generation model. With the existing models we‚Äôre sharing today, we hope to encourage the research community to design new detection and mitigation strategies that will help scale generative modeling research in a responsible way.</p></div><a href="https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/?gk_enable=chameleon_web_flow_is_live" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_7_BH"></a></div><div><div><p>Most modern LLMs have a simple training objective: predicting the next word. While this approach is simple and scalable, it‚Äôs also inefficient. It requires several orders of magnitude more text than what children need to learn the same degree of language fluency.</p><p>In April, we proposed a new approach to build better and faster LLMs by using <a href="https://arxiv.org/abs/2404.19737" target="_blank" data-lnfb-mode="ie"><u>multi-token prediction</u></a>. Using this approach, we train language models to predict multiple future words at once‚Äîinstead of the old one-at-a-time approach. This improves model capabilities and training efficiency while allowing for faster speeds. In the spirit of responsible open science, we‚Äôre releasing the pre-trained models for code completion under a non-commercial/research-only license. We hope this enables the research community to investigate our method and the trained models‚Äô behaviors independently.</p><br></div><a href="https://huggingface.co/facebook/multi-token-prediction" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_8_1k"></a><div><p>Meta Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation</p></div><div><p>Generative AI has enabled people to explore their creativity in new ways, such as by turning a text prompt into a clip of music. While existing text-to-music models like <a href="https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/" target="_blank" data-lnfb-mode="ie"><u>MusicGen</u></a> rely mainly on text inputs for music generation, our new model, Meta Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation (JASCO), is capable of accepting various conditioning inputs, such as specific chords or beats, to improve control over generated music outputs. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This allows the incorporation of both symbolic and audio-based conditions in the same text-to-music generation model.</p><p>Results suggest that JASCO is comparable to the evaluated baselines considering generation quality, while allowing significantly better and more versatile controls over the generated music. Today, we‚Äôre releasing the <a href="https://arxiv.org/pdf/2406.10970" target="_blank" data-lnfb-mode="ie"><u>research paper</u></a> together with a sample page. Later this month, we‚Äôll release inference code as part of the <a href="https://github.com/facebookresearch/audiocraft" target="_blank" data-lnfb-mode="ie"><u>AudioCraft</u></a> repository under an MIT license and the pre-trained model under CC-BY-NC. We look forward to releasing the code and models in the future.</p><br></div><p>AudioSeal</p><div><p>Generative AI tools are inspiring people to share their creations with their friends, family, and followers on social media. As with all AI innovations, it‚Äôs important that we do our part to help ensure responsible use of these tools. Today, we‚Äôre releasing AudioSeal, which we believe is the first audio watermarking technique designed specifically for the localized detection of AI-generated speech, making it possible to pinpoint AI-generated segments within a longer audio snippet. AudioSeal revamps classical audio watermarking by focusing on the detection of AI-generated content rather than steganography. Unlike traditional methods that rely on complex decoding algorithms, AudioSeal‚Äôs localized detection approach allows for faster and more efficient detection. This design enhances the detection speed by up to 485 times compared to previous methods, making it highly suitable for large-scale and real-time applications. Our approach achieves state-of-the-art performance in audio watermarking in terms of robustness and imperceptibility.</p><p>AudioSeal is being released under a commercial license. It‚Äôs just one of several lines of responsible research we‚Äôve shared to help prevent the misuse of generative AI tools. We include similar watermarks in speech samples generated by <a href="https://ai.meta.com/blog/seamless-communication/" target="_blank" data-lnfb-mode="ie"><u>SeamlessM4T v2</u></a>, our foundational translation model for text and speech, and <a href="https://audiobox.metademolab.com/" target="_blank" data-lnfb-mode="ie"><u>Audiobox</u></a>. We further detail our watermarking approach for <a href="https://arxiv.org/abs/2303.15435" target="_blank" data-lnfb-mode="ie"><u>images</u></a>, <a href="https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/" target="_blank" data-lnfb-mode="ie"><u>speech</u></a>, and <a href="https://arxiv.org/abs/2308.00113" target="_blank" data-lnfb-mode="ie"><u>text</u></a> models in recent releases.</p><br></div><a href="https://github.com/facebookresearch/audioseal" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_a_0B"><div><p>Get the model and training code</p><svg viewBox="0 0 36 36" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.746 10H26V21.254L24.0301 21.2735L24.0297 13.377L11.4067 26L10 24.5933L22.6039 11.9894L14.746 11.9894V10Z" fill="CurrentColor"></path></svg></div></a><div><p>Partnership supporting the release of the PRISM dataset</p></div><div><p>Getting feedback from a diverse group of people is important to improving LLMs, however there have been open questions in the research community about methods, domains, and objectives around the feedback process. We worked with our <a href="https://hannahkirk.github.io/prism-alignment/" target="_blank" data-lnfb-mode="ie"><u>external partners</u></a> to navigate these questions supporting the release of the PRISM dataset, which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries. The dataset maps each person‚Äôs preferences and fine-grained feedback to 8,011 live conversations with 21 different LLMs.</p><p>Meta advised on the compilation of the PRISM dataset by our external partners, by focusing conversations that center subjective and multicultural perspectives on topics where there is likely to be interpersonal and cross-cultural disagreement. Our paper demonstrates the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. While we hope this will serve as a community resource, we also want it to inspire broader participation in AI development and foster a more inclusive approach to technology design.</p></div><a href="https://huggingface.co/datasets/HannahRoseKirk/prism-alignment" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_b_pj"><div><p>Get the dataset from our external partners</p><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 36 36" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M21.9502 15.4645L13.7579 23.6569C13.3674 24.0475 12.7342 24.0475 12.3437 23.6569C11.9532 23.2664 11.9532 22.6333 12.3437 22.2427L20.536 14.0503H15.8792C15.3269 14.0503 14.8792 13.6026 14.8792 13.0503C14.8792 12.498 15.3269 12.0503 15.8792 12.0503H22.9502C23.5025 12.0503 23.9502 12.498 23.9502 13.0503V20.1214C23.9502 20.6737 23.5025 21.1214 22.9502 21.1214C22.398 21.1214 21.9502 20.6737 21.9502 20.1214V15.4645Z" fill="CurrentColor"></path></svg></div></a><a href="https://arxiv.org/abs/2404.16019" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_c_HN"><div><p>Read the technical report</p><svg viewBox="0 0 36 36" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.746 10H26V21.254L24.0301 21.2735L24.0297 13.377L11.4067 26L10 24.5933L22.6039 11.9894L14.746 11.9894V10Z" fill="CurrentColor"></path></svg></div></a></div><div><div><p>Measuring and improving geographical disparities in text-to-image generation systems</p></div><div><p>It‚Äôs important that text-to-image models work well for everyone and reflect the geographical and cultural diversity of the world. Improving these models requires new tools that enable researchers to gain a better understanding of where existing models may fall short. In order to address this goal, we‚Äôre detailing our recent research efforts and progress:</p><br><ul><li>We developed <a href="https://arxiv.org/pdf/2308.06198" target="_blank" data-lnfb-mode="ie"><u>automatic indicators</u></a> called ‚ÄúDIG In‚Äù to evaluate potential geographical disparities in text-to-image models. In addition, to understand how people in different regions vary in their perceptions of geographic representation, we conducted a large-scale annotation study. We collected more than 65,000 annotations and more than 20 survey responses per example covering appeal, similarity, consistency, and shared <a href="https://arxiv.org/abs/2405.04457" target="_blank" data-lnfb-mode="ie"><u>recommendations</u></a> for improved automatic and human evaluations of text-to-image models.</li></ul><br><ul><li>Through this work, we learned that people utilize specific components within an image when perceiving geographic representation, rather than viewing the entire image holistically. As part of our collaborative approach at Meta FAIR, we mentored a team of graduate students at UMass Amherst on a <a href="https://ai.meta.com/research/publications/decomposed-evaluations-of-geographic-disparities-in-text-to-image-models/" target="_blank" data-lnfb-mode="ie"><u>follow-up evaluation</u></a> that decomposes the previously introduced automatic indicators into foregrounded concepts and background representations.</li></ul><br><ul><li>Informed by the DIG In measurement work, we also explored methods of improving the diversity of outputs from text-to-image models. In this direction, we introduced the <a href="https://arxiv.org/abs/2406.04551" target="_blank" data-lnfb-mode="ie"><u>contextualized Vendi Score guidance</u></a>, which extends our previous <a href="https://arxiv.org/abs/2310.00158" target="_blank" data-lnfb-mode="ie"><u>feedback guidance work</u></a> and uses an inference-time intervention that guides state-of-the-art text-to-image latent diffusion models to increase the representation diversity of the generated samples while maintaining or improving the image quality and prompt-generation consistency.</li></ul></div><a href="https://github.com/facebookresearch/DIG-In" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_d_ij"></a></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Billard ‚Äì Generate music from ball collisions in 2D space (224 pts)]]></title>
            <link>https://billard.medusis.com/</link>
            <guid>40719782</guid>
            <pubDate>Tue, 18 Jun 2024 16:45:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://billard.medusis.com/">https://billard.medusis.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40719782">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="commands_container">
					<p><label>Gravity</label>
						

						<label>Speed</label>
						

						<label>Count</label>
						

						<label>Root</label>
						

						<label>Mode</label>
						

						<label>MIDI </label>
						

						<label>Duration</label>
						

						</p>
					<p>&nbsp;Ball</p>
					<p><label>Ball state</label>
						
						<label>Ball size</label>
						
						<label>Delete ball</label>
						

						</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Is ChatGPT Doing and Why Does It Work? (2023) (134 pts)]]></title>
            <link>https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/</link>
            <guid>40718566</guid>
            <pubDate>Tue, 18 Jun 2024 14:58:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/</a>, See on <a href="https://news.ycombinator.com/item?id=40718566">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            

<p><img title="What Is ChatGPT Doing ... and Why Does It Work?" src="https://content.wolfram.com/sites/43/2023/02/hero3-chat-exposition.png" width="620" height="205"></p>
<h2 id="its-just-adding-one-word-at-a-time">It‚Äôs Just Adding One Word at a Time</h2>
<p>That <a href="https://chat.openai.com/" target="_blank" rel="noopener">ChatGPT</a> can automatically generate something that reads even superficially like human-written text is remarkable, and unexpected. But how does it do it? And why does it work? My purpose here is to give a rough outline of what‚Äôs going on inside ChatGPT‚Äîand then to explore why it is that it can do so well in producing what we might consider to be meaningful text. I should say at the outset that I‚Äôm going to focus on the big picture of what‚Äôs going on‚Äîand while I‚Äôll mention some engineering details, I won‚Äôt get deeply into them. (And the essence of what I‚Äôll say applies just as well to other current ‚Äúlarge language models‚Äù [LLMs] as to ChatGPT.)</p>
<p>The first thing to explain is that what ChatGPT is always fundamentally trying to do is to produce a ‚Äúreasonable continuation‚Äù of whatever text it‚Äôs got so far, where by ‚Äúreasonable‚Äù we mean ‚Äúwhat one might expect someone to write after seeing what people have written on billions of webpages, etc.‚Äù<span id="more-45835"></span></p>
<p>So let‚Äôs say we‚Äôve got the text ‚Äú<em>The best thing about AI is its ability to</em>‚Äù. Imagine scanning billions of pages of human-written text (say on the web and in digitized books) and finding all instances of this text‚Äîthen seeing what word comes next what fraction of the time. ChatGPT effectively does something like this, except that (as I‚Äôll explain) it doesn‚Äôt look at literal text; it looks for things that in a certain sense ‚Äúmatch in meaning‚Äù. But the end result is that it produces a ranked list of words that might follow, together with ‚Äúprobabilities‚Äù:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img1.png" alt="" title="" width="453" height="137"> </p>
</div>
<p>And the remarkable thing is that when ChatGPT does something like write an essay what it‚Äôs essentially doing is just asking over and over again ‚Äúgiven the text so far, what should the next word be?‚Äù‚Äîand each time adding a word. (More precisely, as I‚Äôll explain, it‚Äôs adding a ‚Äútoken‚Äù, which could be just a part of a word, which is why it can sometimes ‚Äúmake up new words‚Äù.)</p>
<p>But, OK, at each step it gets a list of words with probabilities. But which one should it actually pick to add to the essay (or whatever) that it‚Äôs writing? One might think it should be the ‚Äúhighest-ranked‚Äù word (i.e. the one to which the highest ‚Äúprobability‚Äù was assigned). But this is where a bit of voodoo begins to creep in. Because for some reason‚Äîthat maybe one day we‚Äôll have a scientific-style understanding of‚Äîif we always pick the highest-ranked word, we‚Äôll typically get a very ‚Äúflat‚Äù essay, that never seems to ‚Äúshow any creativity‚Äù (and even sometimes repeats word for word). But if sometimes (at random) we pick lower-ranked words, we get a ‚Äúmore interesting‚Äù essay.</p>
<p>The fact that there‚Äôs randomness here means that if we use the same prompt multiple times, we‚Äôre likely to get different essays each time. And, in keeping with the idea of voodoo, there‚Äôs a particular so-called ‚Äútemperature‚Äù parameter that determines how often lower-ranked words will be used, and for essay generation, it turns out that a ‚Äútemperature‚Äù of 0.8 seems best. (It‚Äôs worth emphasizing that there‚Äôs no ‚Äútheory‚Äù being used here; it‚Äôs just a matter of what‚Äôs been found to work in practice. And for example the concept of ‚Äútemperature‚Äù is there because exponential distributions <a href="https://writings.stephenwolfram.com/2023/02/computational-foundations-for-the-second-law-of-thermodynamics/#textbook-thermodynamics">familiar from statistical physics</a> happen to be being used, but there‚Äôs no ‚Äúphysical‚Äù connection‚Äîat least so far as we know.)</p>
<p>Before we go on I should explain that for purposes of exposition I‚Äôm mostly not going to use the <a href="https://openai.com/blog/chatgpt/" target="_blank">full system that‚Äôs in ChatGPT</a>; instead I‚Äôll usually work with a simpler <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/">GPT-2 system</a>, which has the nice feature that it‚Äôs small enough to be able to run on a standard desktop computer. And so for essentially everything I show I‚Äôll be able to include explicit <a href="https://www.wolfram.com/language/">Wolfram Language</a> code that you can immediately run on your computer. (Click any picture here to copy the code behind it.)</p>
<p>For example, here‚Äôs how to get the table of probabilities above. First, we have to <a href="https://resources.wolframcloud.com/NeuralNetRepository">retrieve the underlying ‚Äúlanguage model‚Äù neural net</a>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img2.png" alt="" title="" width="466" height="123"> </p>
</div>
<p>Later on, we‚Äôll look inside this neural net, and talk about how it works. But for now we can just apply this ‚Äúnet model‚Äù as a black box to our text so far, and ask for the top 5 words by probability that the model says should follow: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img3.png" alt="" title="" width="557" height="70"> </p>
</div>
<p>This takes that result and makes it into an explicit formatted ‚Äú<a href="https://www.wolfram.com/language/elementary-introduction/2nd-ed/45-datasets.html">dataset</a>‚Äù:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img4.png" alt="" title="" width="390" height="195"> </p>
</div>
<p>Here‚Äôs what happens if one repeatedly ‚Äúapplies the model‚Äù‚Äîat each step adding the word that has the top probability (specified in this code as the ‚Äúdecision‚Äù from the model):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img5.png" alt="" title="" width="487" height="238"> </p>
</div>
<p>What happens if one goes on longer? In this (‚Äúzero temperature‚Äù) case what comes out soon gets rather confused and repetitive:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img6.png" alt="" title="" width="558" height="110"> </p>
</div>
<p>But what if instead of always picking the ‚Äútop‚Äù word one sometimes randomly picks ‚Äúnon-top‚Äù words (with the ‚Äúrandomness‚Äù corresponding to ‚Äútemperature‚Äù 0.8)? Again one can build up text: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img7.png" alt="" title="" width="481" height="182"> </p>
</div>
<p>And every time one does this, different random choices will be made, and the text will be different‚Äîas in these 5 examples:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img8.png" alt="" title="" width="489" height="134"> </p>
</div>
<p>It‚Äôs worth pointing out that even at the first step there are a lot of possible ‚Äúnext words‚Äù to choose from (at temperature 0.8), though their probabilities fall off quite quickly (and, yes, the straight line on this log-log plot corresponds to an <em>n</em><sup>‚Äì1</sup> <a href="https://www.wolframscience.com/nks/notes-8-8--zipfs-law/">‚Äúpower-law‚Äù decay that‚Äôs very characteristic of the general statistics of language</a>): </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img10.png" alt="" title="" width="461" height="279"> </p>
</div>
<p>So what happens if one goes on longer? Here‚Äôs a random example. It‚Äôs better than the top-word (zero temperature) case, but still at best a bit weird:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img11A.png" alt="" title="" width="617" height="115"> </p>
</div>
<p>This was done with the <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/">simplest GPT-2 model</a> (from 2019). With the newer and <a href="https://platform.openai.com/docs/model-index-for-researchers" target="_blank" rel="noopener">bigger GPT-3 models</a> the results are better. Here‚Äôs the top-word (zero temperature) text produced with the same ‚Äúprompt‚Äù, but with the biggest GPT-3 model:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img12.png" alt="" title="" width="542" height="134"> </p>
</div>
<p>And here‚Äôs a random example at ‚Äútemperature 0.8‚Äù:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img13.png" alt="" title="" width="602" height="86"> </p>
</div>
<h2 id="where-do-the-probabilities-come-from">Where Do the Probabilities Come From?</h2>
<p>OK, so ChatGPT always picks its next word based on probabilities. But where do those probabilities come from? Let‚Äôs start with a simpler problem. Let‚Äôs consider generating English text one letter (rather than word) at a time. How can we work out what the probability for each letter should be?</p>
<p>A very minimal thing we could do is just take a sample of English text, and calculate how often different letters occur in it. So, for example, <a href="https://www.wolfram.com/language/elementary-introduction/2nd-ed/34-associations.html#i-8">this counts letters in the Wikipedia article</a> on ‚Äúcats‚Äù: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img14-edit.png" alt="" title="" width="493" height="117"> </p>
</div>
<p>And this does the same thing for ‚Äúdogs‚Äù:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img15.png" alt="" title="" width="479" height="121"> </p>
</div>
<p>The results are similar, but not the same (‚Äúo‚Äù is no doubt more common in the ‚Äúdogs‚Äù article because, after all, it occurs in the word ‚Äúdog‚Äù itself). Still, if we take a large enough sample of English text we can expect to eventually get at least fairly consistent results: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img16.png" alt="" title="" width="544" height="147"> </p>
</div>
<p>Here‚Äôs a sample of what we get if we just generate a sequence of letters with these probabilities:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img17.png" alt="" title="" width="620" height="38"> </p>
</div>
<p>We can break this into ‚Äúwords‚Äù by adding in spaces as if they were letters with a certain probability:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img18.png" alt="" title="" width="522" height="38"> </p>
</div>
<p>We can do a slightly better job of making ‚Äúwords‚Äù by forcing the distribution of ‚Äúword lengths‚Äù to agree with what it is in English:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img19.png" alt="" title="" width="464" height="35"> </p>
</div>
<p>We didn‚Äôt happen to get any ‚Äúactual words‚Äù here, but the results are looking slightly better. To go further, though, we need to do more than just pick each letter separately at random. And, for example, we know that if we have a ‚Äúq‚Äù, the next letter basically has to be ‚Äúu‚Äù. </p>
<p>Here‚Äôs a plot of the probabilities for letters on their own:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img20.png" alt="" title="" width="338" height="210"> </p>
</div>
<p>And here‚Äôs a plot that shows the probabilities of pairs of letters (‚Äú2-grams‚Äù) in typical English text. The possible first letters are shown across the page, the second letters down the page: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img21.png" alt="" title="" width="260" height="263"> </p>
</div>
<p>And we see here, for example, that the ‚Äúq‚Äù column is blank (zero probability) except on the ‚Äúu‚Äù row. OK, so now instead of generating our ‚Äúwords‚Äù a single letter at a time, let‚Äôs generate them looking at two letters at a time, using these ‚Äú2-gram‚Äù probabilities. Here‚Äôs a sample of the result‚Äîwhich happens to include a few ‚Äúactual words‚Äù:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img22.png" alt="" title="" width="500" height="38"> </p>
</div>
<p>With sufficiently much English text we can get pretty good estimates not just for probabilities of single letters or pairs of letters (2-grams), but also for longer runs of letters. And if we generate ‚Äúrandom words‚Äù with progressively longer <em>n</em>-gram probabilities, we see that they get progressively ‚Äúmore realistic‚Äù:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img23.png" alt="" title="" width="586" height="158"> </p>
</div>
<p>But let‚Äôs now assume‚Äîmore or less as ChatGPT does‚Äîthat we‚Äôre dealing with whole words, not letters. There are about 40,000 <a href="https://reference.wolfram.com/language/ref/WordList.html">reasonably commonly used words in English</a>. And by looking at a large corpus of English text (say a few million books, with altogether a few hundred billion words), we can get an <a href="https://reference.wolfram.com/language/ref/WordFrequencyData.html">estimate of how common each word is</a>. And using this we can start generating ‚Äúsentences‚Äù, in which each word is independently picked at random, with the same probability that it appears in the corpus. Here‚Äôs a sample of what we get:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img24.png" alt="" title="" width="494" height="38"> </p>
</div>
<p>Not surprisingly, this is nonsense. So how can we do better? Just like with letters, we can start taking into account not just probabilities for single words but probabilities for pairs or longer <em>n</em>-grams of words. Doing this for pairs, here are 5 examples of what we get, in all cases starting from the word ‚Äúcat‚Äù:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img25.png" alt="" title="" width="453" height="134"> </p>
</div>
<p>It‚Äôs getting slightly more ‚Äúsensible looking‚Äù. And we might imagine that if we were able to use sufficiently long <em>n</em>-grams we‚Äôd basically ‚Äúget a ChatGPT‚Äù‚Äîin the sense that we‚Äôd get something that would generate essay-length sequences of words with the ‚Äúcorrect overall essay probabilities‚Äù. But here‚Äôs the problem: there just isn‚Äôt even close to enough English text that‚Äôs ever been written to be able to deduce those probabilities. </p>
<p>In a <a href="https://commoncrawl.org/" target="_blank" rel="noopener">crawl of the web</a> there might be a few hundred billion words; in books that have been digitized there might be another hundred billion words. But with 40,000 common words, even the number of possible 2-grams is already 1.6 billion‚Äîand the number of possible 3-grams is 60 trillion. So there‚Äôs no way we can estimate the probabilities even for all of these from text that‚Äôs out there. And by the time we get to ‚Äúessay fragments‚Äù of 20 words, the number of possibilities is larger than the number of particles in the universe, so in a sense they could never all be written down.</p>
<p>So what can we do? The big idea is to make a model that lets us estimate the probabilities with which sequences should occur‚Äîeven though we‚Äôve never explicitly seen those sequences in the corpus of text we‚Äôve looked at. And at the core of ChatGPT is precisely a so-called ‚Äúlarge language model‚Äù (LLM) that‚Äôs been built to do a good job of estimating those probabilities. </p>
<h2 id="what-is-a-model">What Is a Model?</h2>
<p>Say you want to know (as <a href="https://archive.org/details/bub_gb_49d42xp-USMC/page/404/mode/2up" target="_blank" rel="noopener">Galileo did back in the late 1500s</a>) how long it‚Äôs going to take a cannon ball dropped from each floor of the Tower of Pisa to hit the ground. Well, you could just measure it in each case and make a table of the results. Or you could do what is the essence of theoretical science: make a model that gives some kind of procedure for computing the answer rather than just measuring and remembering each case.</p>
<p>Let‚Äôs imagine we have (somewhat idealized) data for how long the cannon ball takes to fall from various floors:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img26.png" alt="" title="" width="274" height="149"> </p>
</div>
<p>How do we figure out how long it‚Äôs going to take to fall from a floor we don‚Äôt explicitly have data about? In this particular case, we can use known laws of physics to work it out. But say all we‚Äôve got is the data, and we don‚Äôt know what underlying laws govern it. Then we might make a mathematical guess, like that perhaps we should use a straight line as a model:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img27.png" alt="" title="" width="275" height="149"> </p>
</div>
<p>We could pick different straight lines. But this is the one that‚Äôs on average closest to the data we‚Äôre given. And from this straight line we can estimate the time to fall for any floor.</p>
<p>How did we know to try using a straight line here? At some level we didn‚Äôt. It‚Äôs just something that‚Äôs mathematically simple, and we‚Äôre used to the fact that lots of data we measure turns out to be well fit by mathematically simple things. We could try something mathematically more complicated‚Äîsay <em>a</em> + <em>b</em> <em>x</em> + <em>c</em> <em>x</em><sup>2</sup>‚Äîand then in this case we do better:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img29.png" alt="" title="" width="286" height="154"> </p>
</div>
<p>Things can go quite wrong, though. Like here‚Äôs <a href="https://reference.wolfram.com/language/ref/FindFit.html">the best we can do</a> with <em>a</em> + <em>b</em>/<em>x</em> + <em>c</em> sin(<em>x</em>):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img33.png" alt="" title="" width="285" height="154"> </p>
</div>
<p>It is worth understanding that there‚Äôs never a ‚Äúmodel-less model‚Äù. Any model you use has some particular underlying structure‚Äîthen a certain set of ‚Äúknobs you can turn‚Äù (i.e. parameters you can set) to fit your data. And in the case of ChatGPT, lots of such ‚Äúknobs‚Äù are used‚Äîactually, 175 billion of them.</p>
<p>But the remarkable thing is that the underlying structure of ChatGPT‚Äîwith ‚Äújust‚Äù that many parameters‚Äîis sufficient to make a model that computes next-word probabilities ‚Äúwell enough‚Äù to give us reasonable essay-length pieces of text.</p>
<h2 id="models-for-human-like-tasks">Models for Human-Like Tasks</h2>
<p>The example we gave above involves making a model for numerical data that essentially comes from simple physics‚Äîwhere we‚Äôve known for several centuries that ‚Äúsimple mathematics applies‚Äù. But for ChatGPT we have to make a model of human-language text of the kind produced by a human brain. And for something like that we don‚Äôt (at least yet) have anything like ‚Äúsimple mathematics‚Äù. So what might a model of it be like?</p>
<p>Before we talk about language, let‚Äôs talk about another human-like task: recognizing images. And as a simple example of this, let‚Äôs consider images of digits (and, yes, this is a <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/050b1a0a-f43a-4c28-b7e0-72607a918467/">classic machine learning example</a>):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img34.png" alt="" title="" width="397" height="56"> </p>
</div>
<p>One thing we could do is get a bunch of sample images for each digit:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img35.png" alt="" title="" width="312" height="56"> </p>
</div>
<p>Then to find out if an image we‚Äôre given as input corresponds to a particular digit we could just do an explicit pixel-by-pixel comparison with the samples we have. But as humans we certainly seem to do something better‚Äîbecause we can still recognize digits, even when they‚Äôre for example handwritten, and have all sorts of modifications and distortions:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/03/sw021423img36-4.png" alt="" title="" width="541" height="74"> </p>
</div>
<p>When we made a model for our numerical data above, we were able to take a numerical value <em>x</em> that we were given, and just compute <em>a + b x</em> for particular <em>a</em> and <em>b</em>. So if we treat the gray-level value of each pixel here as some variable <em>x<sub>i</sub></em> is there some function of all those variables that‚Äîwhen evaluated‚Äîtells us what digit the image is of? It turns out that it‚Äôs possible to construct such a function. Not surprisingly, it‚Äôs not particularly simple, though. And a typical example might involve perhaps half a million mathematical operations. </p>
<p>But the end result is that if we feed the collection of pixel values for an image into this function, out will come the number specifying which digit we have an image of. Later, we‚Äôll talk about how such a function can be constructed, and the idea of neural nets. But for now let‚Äôs treat the function as black box, where we feed in images of, say, handwritten digits (as arrays of pixel values) and we get out the numbers these correspond to:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img38.png" alt="" title="" width="565" height="58"> </p>
</div>
<p>But what‚Äôs really going on here? Let‚Äôs say we progressively blur a digit. For a little while our function still ‚Äúrecognizes‚Äù it, here as a ‚Äú2‚Äù. But soon it ‚Äúloses it‚Äù, and starts giving the ‚Äúwrong‚Äù result:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img39.png" alt="" title="" width="528" height="58"> </p>
</div>
<p>But why do we say it‚Äôs the ‚Äúwrong‚Äù result? In this case, we know we got all the images by blurring a ‚Äú2‚Äù. But if our goal is to produce a model of what humans can do in recognizing images, the real question to ask is what a human would have done if presented with one of those blurred images, without knowing where it came from.</p>
<p>And we have a ‚Äúgood model‚Äù if the results we get from our function typically agree with what a human would say. And the nontrivial scientific fact is that for an image-recognition task like this we now basically know how to construct functions that do this.</p>
<p>Can we ‚Äúmathematically prove‚Äù that they work? Well, no. Because to do that we‚Äôd have to have a mathematical theory of what we humans are doing. Take the ‚Äú2‚Äù image and change a few pixels. We might imagine that with only a few pixels ‚Äúout of place‚Äù we should still consider the image a ‚Äú2‚Äù. But how far should that go? It‚Äôs a question of <a href="https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis#sect-10-7--visual-perception">human visual perception</a>. And, yes, the answer would no doubt be different for bees or octopuses‚Äîand potentially <a href="https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/#alien-views-of-the-ruliad">utterly different for putative aliens</a>.</p>
<h2 id="neural-nets">Neural Nets</h2>
<p>OK, so how do our typical models for tasks like <a href="https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/">image recognition</a> actually work? The most popular‚Äîand successful‚Äîcurrent approach uses <a href="https://reference.wolfram.com/language/guide/NeuralNetworks.html">neural nets</a>. Invented‚Äîin a form remarkably close to their use today‚Äî<a href="https://www.wolframscience.com/nks/notes-10-12--history-of-ideas-about-thinking/">in the 1940s</a>, neural nets can be thought of as simple idealizations of how <a href="https://www.wolframscience.com/nks/notes-10-12--the-brain/">brains seem to work</a>. </p>
<p>In human brains there are about 100 billion neurons (nerve cells), each capable of producing an electrical pulse up to perhaps a thousand times a second. The neurons are connected in a complicated net, with each neuron having tree-like branches allowing it to pass electrical signals to perhaps thousands of other neurons. And in a rough approximation, whether any given neuron produces an electrical pulse at a given moment depends on what pulses it‚Äôs received from other neurons‚Äîwith different connections contributing with different ‚Äúweights‚Äù.</p>
<p>When we ‚Äúsee an image‚Äù what‚Äôs happening is that when photons of light from the image fall on (‚Äúphotoreceptor‚Äù) cells at the back of our eyes they produce electrical signals in nerve cells. These nerve cells are connected to other nerve cells, and eventually the signals go through a whole sequence of layers of neurons. And it‚Äôs in this process that we ‚Äúrecognize‚Äù the image, eventually ‚Äúforming the thought‚Äù that we‚Äôre ‚Äúseeing a 2‚Äù (and maybe in the end doing something like saying the word ‚Äútwo‚Äù out loud). </p>
<p>The ‚Äúblack-box‚Äù function from the previous section is a ‚Äúmathematicized‚Äù version of such a neural net. It happens to have 11 layers (though only 4 ‚Äúcore layers‚Äù):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img40A.png" alt="" title="" width="381" height="212"> </p>
</div>
<p>There‚Äôs nothing particularly ‚Äútheoretically derived‚Äù about this neural net; it‚Äôs just something that‚Äî<a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/LeNet-Trained-on-MNIST-Data/">back in 1998‚Äîwas constructed as a piece of engineering</a>, and found to work. (Of course, that‚Äôs not much different from how we might describe our brains as having been produced through the process of biological evolution.) </p>
<p>OK, but how does a neural net like this ‚Äúrecognize things‚Äù? The key is the <a href="https://www.wolframscience.com/nks/chap-6--starting-from-randomness#sect-6-7--the-notion-of-attractors">notion of attractors</a>. Imagine we‚Äôve got handwritten images of 1‚Äôs and 2‚Äôs:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img41.png" alt="" title="" width="244" height="244"> </p>
</div>
<p>We somehow want all the 1‚Äôs to ‚Äúbe attracted to one place‚Äù, and all the 2‚Äôs to ‚Äúbe attracted to another place‚Äù. Or, put a different way, if an image is somehow ‚Äú<a href="https://www.wolframscience.com/nks/notes-10-12--memory-analogs-with-numerical-data/">closer to being a 1</a>‚Äù than to being a 2, we want it to end up in the ‚Äú1 place‚Äù and vice versa. </p>
<p>As a straightforward analogy, let‚Äôs say we have certain positions in the plane, indicated by dots (in a real-life setting they might be positions of coffee shops). Then we might imagine that starting from any point on the plane we‚Äôd always want to end up at the closest dot (i.e. we‚Äôd always go to the closest coffee shop). We can represent this by dividing the plane into regions (‚Äúattractor basins‚Äù) separated by idealized ‚Äúwatersheds‚Äù:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img42.png" alt="" title="" width="204" height="205"> </p>
</div>
<p>We can think of this as implementing a kind of ‚Äúrecognition task‚Äù in which we‚Äôre not doing something like identifying what digit a given image ‚Äúlooks most like‚Äù‚Äîbut rather we‚Äôre just, quite directly, seeing what dot a given point is closest to. (The ‚ÄúVoronoi diagram‚Äù setup we‚Äôre showing here separates points in 2D Euclidean space; the digit recognition task can be thought of as doing something very similar‚Äîbut in a 784-dimensional space formed from the gray levels of all the pixels in each image.)</p>
<p>So how do we make a neural net ‚Äúdo a recognition task‚Äù? Let‚Äôs consider this very simple case:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img43.png" alt="" title="" width="128" height="128"> </p>
</div>
<p>Our goal is to take an ‚Äúinput‚Äù corresponding to a position {<em>x</em>,<em>y</em>}‚Äîand then to ‚Äúrecognize‚Äù it as whichever of the three points it‚Äôs closest to. Or, in other words, we want the neural net to compute a function of {<em>x</em>,<em>y</em>} like:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img44.png" alt="" title="" width="570" height="217"> </p>
</div>
<p>So how do we do this with a neural net? Ultimately a neural net is a connected collection of idealized ‚Äúneurons‚Äù‚Äîusually arranged in layers‚Äîwith a simple example being:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img45.png" alt="" title="" width="199" height="241"> </p>
</div>
<p>Each ‚Äúneuron‚Äù is effectively set up to evaluate a simple numerical function. And to ‚Äúuse‚Äù the network, we simply feed numbers (like our coordinates <em>x</em> and <em>y</em>) in at the top, then have neurons on each layer ‚Äúevaluate their functions‚Äù and feed the results forward through the network‚Äîeventually producing the final result at the bottom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img46.png" alt="" title="" width="253" height="244"> </p>
</div>
<p>In the traditional (biologically inspired) setup each neuron effectively has a certain set of ‚Äúincoming connections‚Äù from the neurons on the previous layer, with each connection being assigned a certain ‚Äúweight‚Äù (which can be a positive or negative number). The value of a given neuron is determined by multiplying the values of ‚Äúprevious neurons‚Äù by their corresponding weights, then adding these up and adding a constant‚Äîand finally applying a ‚Äúthresholding‚Äù (or ‚Äúactivation‚Äù) function. In mathematical terms, if a neuron has inputs <nobr><em>x</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub> ‚Ä¶}</nobr> then we compute <em>f</em>[<em>w</em> . <em>x</em> + <em>b</em>], where the weights <em>w</em> and constant <em>b</em> are generally chosen differently for each neuron in the network; the function <em>f </em>is usually the same.</p>
<p>Computing <em>w</em> . <em>x</em> + <em>b</em> is just a matter of matrix multiplication and addition. The ‚Äúactivation function‚Äù <em>f</em> introduces nonlinearity (and ultimately is what leads to nontrivial behavior). Various activation functions commonly get used; here we‚Äôll just use <tt><a href="http://reference.wolfram.com/language/ref/Ramp.html">Ramp</a></tt> (or ReLU):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img48.png" alt="" title="" width="567" height="77"> </p>
</div>
<p>For each task we want the neural net to perform (or, equivalently, for each overall function we want it to evaluate) we‚Äôll have different choices of weights. (And‚Äîas we‚Äôll discuss later‚Äîthese weights are normally determined by ‚Äútraining‚Äù the neural net using machine learning from examples of the outputs we want.)</p>
<p>Ultimately, every neural net just corresponds to some overall mathematical function‚Äîthough it may be messy to write out. For the example above, it would be:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img49.png" alt="" title="" width="542" height="115"> </p>
</div>
<p>The neural net of ChatGPT also just corresponds to a mathematical function like this‚Äîbut effectively with billions of terms. </p>
<p>But let‚Äôs go back to individual neurons. Here are some examples of the functions a neuron with two inputs (representing coordinates <em>x</em> and <em>y</em>) can compute with various choices of weights and constants (and <tt><a href="https://reference.wolfram.com/language/ref/Ramp.html">Ramp</a></tt> as activation function):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img50.png" alt="" title="" width="499" height="115"> </p>
</div>
<p>But what about the larger network from above? Well, here‚Äôs what it computes:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img51.png" alt="" title="" width="472" height="206"> </p>
</div>
<p>It‚Äôs not quite ‚Äúright‚Äù, but it‚Äôs close to the ‚Äúnearest point‚Äù function we showed above.</p>
<p>Let‚Äôs see what happens with some other neural nets. In each case, as we‚Äôll explain later, we‚Äôre using machine learning to find the best choice of weights. Then we‚Äôre showing here what the neural net with those weights computes:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img52.png" alt="" title="" width="599" height="415"> </p>
</div>
<p>Bigger networks generally do better at approximating the function we‚Äôre aiming for. And in the ‚Äúmiddle of each attractor basin‚Äù we typically get exactly the answer we want. But <a href="https://www.wolframscience.com/nks/notes-10-12--memory-analogs-with-numerical-data/">at the boundaries</a>‚Äîwhere the neural net ‚Äúhas a hard time making up its mind‚Äù‚Äîthings can be messier. </p>
<p>With this simple mathematical-style ‚Äúrecognition task‚Äù it‚Äôs clear what the ‚Äúright answer‚Äù is. But in the problem of recognizing handwritten digits, it‚Äôs not so clear. What if someone wrote a ‚Äú2‚Äù so badly it looked like a ‚Äú7‚Äù, etc.? Still, we can ask how a neural net distinguishes digits‚Äîand this gives an indication:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img53.png" alt="" title="" width="311" height="311"> </p>
</div>
<p>Can we say ‚Äúmathematically‚Äù how the network makes its distinctions? Not really. It‚Äôs just ‚Äúdoing what the neural net does‚Äù. But it turns out that that normally seems to agree fairly well with the distinctions we humans make. </p>
<p>Let‚Äôs take a more elaborate example. Let‚Äôs say we have images of cats and dogs. And we have a <a href="https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/">neural net that‚Äôs been trained to distinguish them</a>. Here‚Äôs what it might do on some examples: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img54.png" alt="" title="" width="354" height="356"> </p>
</div>
<p>Now it‚Äôs even less clear what the ‚Äúright answer‚Äù is. What about a dog dressed in a cat suit? Etc. Whatever input it‚Äôs given the neural net will generate an answer, and in a way reasonably consistent with how humans might. As I‚Äôve said above, that‚Äôs not a fact we can ‚Äúderive from first principles‚Äù. It‚Äôs just something that‚Äôs empirically been found to be true, at least in certain domains. But it‚Äôs a key reason why neural nets are useful: that they somehow capture a ‚Äúhuman-like‚Äù way of doing things.</p>
<p>Show yourself a picture of a cat, and ask ‚ÄúWhy is that a cat?‚Äù. Maybe you‚Äôd start saying ‚ÄúWell, I see its pointy ears, etc.‚Äù But it‚Äôs not very easy to explain how you recognized the image as a cat. It‚Äôs just that somehow your brain figured that out. But for a brain there‚Äôs no way (at least yet) to ‚Äúgo inside‚Äù and see how it figured it out. What about for an (artificial) neural net? Well, it‚Äôs straightforward to see what each ‚Äúneuron‚Äù does when you show a picture of a cat. But even to get a basic visualization is usually very difficult. </p>
<p>In the final net that we used for the ‚Äúnearest point‚Äù problem above there are 17 neurons. In the net for recognizing handwritten digits there are 2190. And in the net we‚Äôre using to recognize cats and dogs there are 60,650. Normally it would be pretty difficult to visualize what amounts to 60,650-dimensional space. But because this is a network set up to deal with images, many of its layers of neurons are organized into arrays, like the arrays of pixels it‚Äôs looking at.</p>
<p>And if we take a typical cat image</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img55.png" alt="Cat" title="Cat" width="123/2" height="123/2"></p>
<p>then we can represent the states of neurons at the first layer by a collection of derived images‚Äîmany of which we can readily interpret as being things like ‚Äúthe cat without its background‚Äù, or ‚Äúthe outline of the cat‚Äù:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img56.png" alt="" title="" width="599" height="355"> </p>
</div>
<p>By the 10th layer it‚Äôs harder to interpret what‚Äôs going on:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img57.png" alt="" title="" width="598" height="295"> </p>
</div>
<p>But in general we might say that the neural net is ‚Äúpicking out certain features‚Äù (maybe pointy ears are among them), and using these to determine what the image is of. But are those features ones for which we have names‚Äîlike ‚Äúpointy ears‚Äù? Mostly not. </p>
<p>Are our brains using similar features? Mostly we don‚Äôt know. But it‚Äôs notable that the first few layers of a neural net like the one we‚Äôre showing here seem to pick out aspects of images (like edges of objects) that seem to be similar to ones we know are picked out by the first level of visual processing in brains.</p>
<p>But let‚Äôs say we want a ‚Äútheory of cat recognition‚Äù in neural nets. We can say: ‚ÄúLook, this particular net does it‚Äù‚Äîand immediately that gives us some sense of ‚Äúhow hard a problem‚Äù it is (and, for example, how many neurons or layers might be needed). But at least as of now we don‚Äôt have a way to ‚Äúgive a narrative description‚Äù of what the network is doing. And maybe that‚Äôs because it truly is computationally irreducible, and there‚Äôs no general way to find what it does except by explicitly tracing each step. Or maybe it‚Äôs just that we haven‚Äôt ‚Äúfigured out the science‚Äù, and identified the ‚Äúnatural laws‚Äù that allow us to summarize what‚Äôs going on.</p>
<p>We‚Äôll encounter the same kinds of issues when we talk about generating language with ChatGPT. And again it‚Äôs not clear whether there are ways to ‚Äúsummarize what it‚Äôs doing‚Äù. But the richness and detail of language (and our experience with it) may allow us to get further than with images.</p>
<h2 id="machine-learning-and-the-training-of-neural-nets">Machine Learning, and the Training of Neural Nets</h2>
<p>We‚Äôve been talking so far about neural nets that ‚Äúalready know‚Äù how to do particular tasks. But what makes neural nets so useful (presumably also in brains) is that not only can they in principle do all sorts of tasks, but they can be incrementally ‚Äútrained from examples‚Äù to do those tasks.</p>
<p>When we make a neural net to distinguish cats from dogs we don‚Äôt effectively have to write a program that (say) explicitly finds whiskers; instead we just show lots of examples of what‚Äôs a cat and what‚Äôs a dog, and then have the network ‚Äúmachine learn‚Äù from these how to distinguish them.</p>
<p>And the point is that the trained network ‚Äúgeneralizes‚Äù from the particular examples it‚Äôs shown. Just as we‚Äôve seen above, it isn‚Äôt simply that the network recognizes the particular pixel pattern of an example cat image it was shown; rather it‚Äôs that the neural net somehow manages to distinguish images on the basis of what we consider to be some kind of ‚Äúgeneral catness‚Äù. </p>
<p>So how does neural net training actually work? Essentially what we‚Äôre always trying to do is to find weights that make the neural net successfully reproduce the examples we‚Äôve given. And then we‚Äôre relying on the neural net to ‚Äúinterpolate‚Äù (or ‚Äúgeneralize‚Äù) ‚Äúbetween‚Äù these examples in a ‚Äúreasonable‚Äù way.</p>
<p>Let‚Äôs look at a problem even simpler than the nearest-point one above. Let‚Äôs just try to get a neural net to learn the function: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img58.png" alt="" title="" width="203" height="124"> </p>
</div>
<p>For this task, we‚Äôll need a network that has just one input and one output, like: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img59.png" alt="" title="" width="141" height="170"> </p>
</div>
<p>But what weights, etc. should we be using? With every possible set of weights the neural net will compute some function. And, for example, here‚Äôs what it does with a few randomly chosen sets of weights: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img60.png" alt="" title="" width="597" height="209"> </p>
</div>
<p>And, yes, we can plainly see that in none of these cases does it get even close to reproducing the function we want. So how do we find weights that will reproduce the function?</p>
<p>The basic idea is to supply lots of ‚Äúinput ‚Üí output‚Äù examples to ‚Äúlearn from‚Äù‚Äîand then to try to find weights that will reproduce these examples. Here‚Äôs the result of doing that with progressively more examples:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img61.png" alt="" title="" width="597" height="259"> </p>
</div>
<p>At each stage in this ‚Äútraining‚Äù the weights in the network are progressively adjusted‚Äîand we see that eventually we get a network that successfully reproduces the function we want. So how do we adjust the weights? The basic idea is at each stage to see ‚Äúhow far away we are‚Äù from getting the function we want‚Äîand then to update the weights in such a way as to get closer. </p>
<p>To find out ‚Äúhow far away we are‚Äù we compute what‚Äôs usually called a ‚Äúloss function‚Äù (or sometimes ‚Äúcost function‚Äù). Here we‚Äôre using a simple (L2) loss function that‚Äôs just the sum of the squares of the differences between the values we get, and the true values. And what we see is that as our training process progresses, the loss function progressively decreases (following a certain ‚Äúlearning curve‚Äù that‚Äôs different for different tasks)‚Äîuntil we reach a point where the network (at least to a good approximation) successfully reproduces the function we want:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img62.png" alt="" title="" width="581" height="200"> </p>
</div>
<p>Alright, so the last essential piece to explain is how the weights are adjusted to reduce the loss function. As we‚Äôve said, the loss function gives us a ‚Äúdistance‚Äù between the values we‚Äôve got, and the true values. But the ‚Äúvalues we‚Äôve got‚Äù are determined at each stage by the current version of neural net‚Äîand by the weights in it. But now imagine that the weights are variables‚Äîsay <em>w<sub>i</sub></em>. We want to find out how to adjust the values of these variables to minimize the loss that depends on them.</p>
<p>For example, imagine (in an incredible simplification of typical neural nets used in practice) that we have just two weights <em>w</em><sub>1</sub>  and <em>w</em><sub>2</sub>. Then we might have a loss that as a function of <em>w</em><sub>1</sub> and <em>w</em><sub>2</sub> looks like this:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img68.png" alt="" title="" width="236" height="112"> </p>
</div>
<p>Numerical analysis provides a variety of techniques for finding the minimum in cases like this. But a typical approach is just to progressively follow the path of steepest descent from whatever previous <em>w</em><sub>1</sub>, <em>w</em><sub>2</sub> we had:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img71.png" alt="" title="" width="403" height="197"> </p>
</div>
<p>Like water flowing down a mountain, all that‚Äôs guaranteed is that this procedure will end up at some local minimum of the surface (‚Äúa mountain lake‚Äù); it might well not reach the ultimate global minimum.</p>
<p>It‚Äôs not obvious that it would be feasible to find the path of the steepest descent on the ‚Äúweight landscape‚Äù. But calculus comes to the rescue. As we mentioned above, one can always think of a neural net as computing a mathematical function‚Äîthat depends on its inputs, and its weights. But now consider differentiating with respect to these weights. It turns out that the chain rule of calculus in effect lets us ‚Äúunravel‚Äù the operations done by successive layers in the neural net. And the result is that we can‚Äîat least in some local approximation‚Äî‚Äúinvert‚Äù the operation of the neural net, and progressively find weights that minimize the loss associated with the output.</p>
<p>The picture above shows the kind of minimization we might need to do in the unrealistically simple case of just 2 weights. But it turns out that even with many more weights (ChatGPT uses 175 billion) it‚Äôs still possible to do the minimization, at least to some level of approximation. And in fact the big breakthrough in ‚Äúdeep learning‚Äù that occurred around 2011 was associated with the discovery that in some sense it can be easier to do (at least approximate) minimization when there are lots of weights involved than when there are fairly few.</p>
<p>In other words‚Äîsomewhat counterintuitively‚Äîit can be easier to solve more complicated problems with neural nets than simpler ones. And the rough reason for this seems to be that when one has a lot of ‚Äúweight variables‚Äù one has a high-dimensional space with ‚Äúlots of different directions‚Äù that can lead one to the minimum‚Äîwhereas with fewer variables it‚Äôs easier to end up getting stuck in a local minimum (‚Äúmountain lake‚Äù) from which there‚Äôs no ‚Äúdirection to get out‚Äù.</p>
<p>It‚Äôs worth pointing out that in typical cases there are many different collections of weights that will all give neural nets that have pretty much the same performance. And usually in practical neural net training there are lots of random choices made‚Äîthat lead to ‚Äúdifferent-but-equivalent solutions‚Äù, like these:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img72.png" alt="" title="" width="571" height="82"> </p>
</div>
<p>But each such ‚Äúdifferent solution‚Äù will have at least slightly different behavior. And if we ask, say, for an ‚Äúextrapolation‚Äù outside the region where we gave training examples, we can get dramatically different results:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img73.png" alt="" title="" width="571" height="83"> </p>
</div>
<p>But which of these is ‚Äúright‚Äù? There‚Äôs really no way to say. They‚Äôre all ‚Äúconsistent with the observed data‚Äù. But they all correspond to different ‚Äúinnate‚Äù ways to ‚Äúthink about‚Äù what to do ‚Äúoutside the box‚Äù. And some may seem ‚Äúmore reasonable‚Äù to us humans than others. </p>
<h2 id="the-practice-and-lore-of-neural-net-training">The Practice and Lore of Neural Net Training</h2>
<p>Particularly over the past decade, there‚Äôve been many advances in the art of training neural nets. And, yes, it is basically an art. Sometimes‚Äîespecially in retrospect‚Äîone can see at least a glimmer of a ‚Äúscientific explanation‚Äù for something that‚Äôs being done. But mostly things have been discovered by trial and error, adding ideas and tricks that have progressively built a significant lore about how to work with neural nets.</p>
<p>There are several key parts. First, there‚Äôs the matter of what architecture of neural net one should use for a particular task. Then there‚Äôs the critical issue of how one‚Äôs going to get the data on which to train the neural net. And increasingly one isn‚Äôt dealing with training a net from scratch: instead a new net can either directly incorporate another already-trained net, or at least can use that net to generate more training examples for itself.</p>
<p>One might have thought that for every particular kind of task one would need a different architecture of neural net. But what‚Äôs been found is that the same architecture often seems to work even for apparently quite different tasks. At some level this reminds one of the <a href="https://www.wolframscience.com/nks/chap-11--the-notion-of-computation#sect-11-3--the-phenomenon-of-universality">idea of universal computation</a> (and my <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/">Principle of Computational Equivalence</a>), but, as I‚Äôll discuss later, I think it‚Äôs more a reflection of the fact that the tasks we‚Äôre typically trying to get neural nets to do are ‚Äúhuman-like‚Äù ones‚Äîand neural nets can capture quite general ‚Äúhuman-like processes‚Äù.</p>
<p>In earlier days of neural nets, there tended to be the idea that one should ‚Äúmake the neural net do as little as possible‚Äù. For example, in <a href="https://reference.wolfram.com/language/ref/SpeechRecognize.html">converting speech to text</a> it was thought that one should first analyze the audio of the speech, break it into phonemes, etc. But what was found is that‚Äîat least for ‚Äúhuman-like tasks‚Äù‚Äîit‚Äôs usually better just to try to train the neural net on the ‚Äúend-to-end problem‚Äù, letting it ‚Äúdiscover‚Äù the necessary intermediate features, encodings, etc. for itself.</p>
<p>There was also the idea that one should introduce complicated individual components into the neural net, to let it in effect ‚Äúexplicitly implement particular algorithmic ideas‚Äù. But once again, this has mostly turned out not to be worthwhile; instead, it‚Äôs better just to deal with very simple components and let them ‚Äúorganize themselves‚Äù (albeit usually in ways we can‚Äôt understand) to achieve (presumably) the equivalent of those algorithmic ideas. </p>
<p>That‚Äôs not to say that there are no ‚Äústructuring ideas‚Äù that are relevant for neural nets. Thus, for example, having <a href="https://reference.wolfram.com/language/ref/ConvolutionLayer.html">2D arrays of neurons with local connections</a> seems at least very useful in the early stages of processing images. And having patterns of connectivity that concentrate on ‚Äúlooking back in sequences‚Äù seems useful‚Äîas we‚Äôll see later‚Äîin dealing with things like human language, for example in ChatGPT. </p>
<p>But an important feature of neural nets is that‚Äîlike computers in general‚Äîthey‚Äôre ultimately just dealing with data. And current neural nets‚Äîwith current approaches to neural net training‚Äî<a href="https://reference.wolfram.com/language/guide/NetEncoderDecoder.html">specifically deal with arrays of numbers</a>. But in the course of processing, those arrays can be completely rearranged and reshaped. And as an example, <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/LeNet-Trained-on-MNIST-Data/">the network we used for identifying digits above</a> starts with a 2D ‚Äúimage-like‚Äù array, quickly ‚Äúthickening‚Äù to many channels, but then <a href="https://reference.wolfram.com/language/ref/AggregationLayer.html">‚Äúconcentrating down‚Äù into a 1D array</a> that will ultimately contain elements representing the different possible output digits:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img74.png" alt="" title="" width="287" height="182"> </p>
</div>
<p>But, OK, how can one tell how big a neural net one will need for a particular task? It‚Äôs something of an art. At some level the key thing is to know ‚Äúhow hard the task is‚Äù. But for human-like tasks that‚Äôs typically very hard to estimate. Yes, there may be a systematic way to do the task very ‚Äúmechanically‚Äù by computer. But it‚Äôs hard to know if there are what one might think of as tricks or shortcuts that allow one to do the task at least at a ‚Äúhuman-like level‚Äù vastly more easily. It might take <a href="https://writings.stephenwolfram.com/2022/06/games-and-puzzles-as-multicomputational-systems/">enumerating a giant game tree</a> to ‚Äúmechanically‚Äù play a certain game; but there might be a much easier (‚Äúheuristic‚Äù) way to achieve ‚Äúhuman-level play‚Äù.</p>
<p>When one‚Äôs dealing with tiny neural nets and simple tasks one can sometimes explicitly see that one ‚Äúcan‚Äôt get there from here‚Äù. For example, here‚Äôs the best one seems to be able to do on the task from the previous section with a few small neural nets: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img75.png" alt="" title="" width="498" height="395"> </p>
</div>
<p>And what we see is that if the net is too small, it just can‚Äôt reproduce the function we want. But above some size, it has no problem‚Äîat least if one trains it for long enough, with enough examples. And, by the way, these pictures illustrate a piece of neural net lore: that one can often get away with a smaller network if there‚Äôs a ‚Äúsqueeze‚Äù in the middle that forces everything to go through a smaller intermediate number of neurons. (It‚Äôs also worth mentioning that ‚Äúno-intermediate-layer‚Äù‚Äîor so-called ‚Äú<a href="https://en.wikipedia.org/wiki/Perceptron" target="_blank" rel="noopener">perceptron</a>‚Äù‚Äînetworks can only learn essentially linear functions‚Äîbut as soon as there‚Äôs even one intermediate layer it‚Äôs <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" target="_blank" rel="noopener">always in principle possible</a> to approximate any function arbitrarily well, at least if one has enough neurons, though to make it feasibly trainable one typically has some kind of <a href="https://reference.wolfram.com/language/ref/BatchNormalizationLayer.html">regularization or normalization</a>.)</p>
<p>OK, so let‚Äôs say one‚Äôs settled on a certain neural net architecture. Now there‚Äôs the issue of getting data to train the network with. And many of the practical challenges around neural nets‚Äîand machine learning in general‚Äîcenter on acquiring or preparing the necessary training data. In many cases (‚Äúsupervised learning‚Äù) one wants to get explicit examples of inputs and the outputs one is expecting from them. Thus, for example, one might want images tagged by what‚Äôs in them, or some other attribute. And maybe one will have to explicitly go through‚Äîusually with great effort‚Äîand do the tagging. But very often it turns out to be possible to piggyback on something that‚Äôs already been done, or use it as some kind of proxy. And so, for example, one might use alt tags that have been provided for images on the web. Or, in a different domain, one might use closed captions that have been created for videos. Or‚Äîfor language translation training‚Äîone might use parallel versions of webpages or other documents that exist in different languages.</p>
<p>How much data do you need to show a neural net to train it for a particular task? Again, it‚Äôs hard to estimate from first principles. Certainly the requirements can be dramatically reduced by using ‚Äútransfer learning‚Äù to ‚Äútransfer in‚Äù things like lists of important features that have already been learned in another network. But generally neural nets need to ‚Äúsee a lot of examples‚Äù to train well. And at least for some tasks it‚Äôs an important piece of neural net lore that the examples can be incredibly repetitive. And indeed it‚Äôs a standard strategy to just show a neural net all the examples one has, over and over again. In each of these ‚Äútraining rounds‚Äù (or ‚Äúepochs‚Äù) the neural net will be in at least a slightly different state, and somehow ‚Äúreminding it‚Äù of a particular example is useful in getting it to ‚Äúremember that example‚Äù. (And, yes, perhaps this is analogous to the usefulness of repetition in human memorization.)</p>
<p>But often just repeating the same example over and over again isn‚Äôt enough. It‚Äôs also necessary to show the neural net variations of the example. And it‚Äôs a feature of neural net lore that those ‚Äúdata augmentation‚Äù variations don‚Äôt have to be sophisticated to be useful. Just slightly modifying images with basic image processing can make them essentially ‚Äúas good as new‚Äù for neural net training. And, similarly, when one‚Äôs run out of actual video, etc. for training self-driving cars, one can go on and just get data from running simulations in a model videogame-like environment without all the detail of actual real-world scenes.</p>
<p>How about something like ChatGPT? Well, it has the nice feature that it can do ‚Äúunsupervised learning‚Äù, making it much easier to get it examples to train from. Recall that the basic task for ChatGPT is to figure out how to continue a piece of text that it‚Äôs been given. So to get it ‚Äútraining examples‚Äù all one has to do is get a piece of text, and mask out the end of it, and then use this as the ‚Äúinput to train from‚Äù‚Äîwith the ‚Äúoutput‚Äù being the complete, unmasked piece of text. We‚Äôll discuss this more later, but the main point is that‚Äîunlike, say, for learning what‚Äôs in images‚Äîthere‚Äôs no ‚Äúexplicit tagging‚Äù needed; ChatGPT can in effect just learn directly from whatever examples of text it‚Äôs given.</p>
<p>OK, so what about the actual learning process in a neural net? In the end it‚Äôs all about determining what weights will best capture the training examples that have been given. And there are all sorts of detailed choices and ‚Äúhyperparameter settings‚Äù (so called because the weights can be thought of as ‚Äúparameters‚Äù) that can be used to tweak how this is done. There are different <a href="https://reference.wolfram.com/language/ref/CrossEntropyLossLayer.html">choices of loss function</a> (sum of squares, sum of absolute values, etc.). There are different ways to do loss minimization (how far in weight space to move at each step, etc.). And then there are questions like how big a ‚Äúbatch‚Äù of examples to show to get each successive estimate of the loss one‚Äôs trying to minimize. And, yes, one can apply machine learning (as we do, for example, in Wolfram Language) to automate machine learning‚Äîand to automatically set things like hyperparameters.</p>
<p>But in the end the whole process of training can be characterized by seeing how the loss progressively decreases (as in this <a href="https://reference.wolfram.com/language/ref/NetTrain.html">Wolfram Language progress monitor for a small training</a>):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img76.png" alt="" title="" width="261" height="327"> </p>
</div>
<p>And what one typically sees is that the loss decreases for a while, but eventually flattens out at some constant value. If that value is sufficiently small, then the training can be considered successful; otherwise it‚Äôs probably a sign one should try changing the network architecture. </p>
<p>Can one tell how long it should take for the ‚Äúlearning curve‚Äù to flatten out? Like for so many other things, there seem to be approximate <a href="https://arxiv.org/pdf/2001.08361.pdf" target="_blank" rel="noopener">power-law scaling relationships</a> that depend on the size of neural net and amount of data one‚Äôs using. But the general conclusion is that training a neural net is hard‚Äîand takes a lot of computational effort. And as a practical matter, the vast majority of that effort is spent doing operations on arrays of numbers, which is what GPUs are good at‚Äîwhich is why neural net training is typically limited by the availability of GPUs. </p>
<p>In the future, will there be fundamentally better ways to train neural nets‚Äîor generally do what neural nets do? Almost certainly, I think. The fundamental idea of neural nets is to create a flexible ‚Äúcomputing fabric‚Äù out of a large number of simple (essentially identical) components‚Äîand to have this ‚Äúfabric‚Äù be one that can be incrementally modified to learn from examples. In current neural nets, one‚Äôs essentially using the ideas of calculus‚Äîapplied to real numbers‚Äîto do that incremental modification. But it‚Äôs increasingly clear that having high-precision numbers doesn‚Äôt matter; 8 bits or less might be enough even with current methods. </p>
<p>With <a href="https://www.wolframscience.com/nks/chap-2--the-crucial-experiment#sect-2-1--how-do-simple-programs-behave">computational systems like cellular automata</a> that basically operate in parallel on many individual bits it‚Äôs never been clear <a href="https://content.wolfram.com/sw-publications/2020/07/approaches-complexity-engineering.pdf" target="_blank" rel="noopener">how to do this kind of incremental modification</a>, but there‚Äôs no reason to think it isn‚Äôt possible. And in fact, much like with the ‚Äú<a href="https://en.wikipedia.org/wiki/AlexNet" target="_blank" rel="noopener">deep-learning breakthrough of 2012</a>‚Äù it may be that such incremental modification will effectively be easier in more complicated cases than in simple ones.</p>
<p>Neural nets‚Äîperhaps a bit like brains‚Äîare set up to have an essentially fixed network of neurons, with what‚Äôs modified being the strength (‚Äúweight‚Äù) of connections between them. (Perhaps in at least young brains significant numbers of wholly new connections can also grow.) But while this might be a convenient setup for biology, it‚Äôs not at all clear that it‚Äôs even close to the best way to achieve the functionality we need. And something that involves the equivalent of progressive network rewriting (perhaps reminiscent of our <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">Physics Project</a>) might well ultimately be better.</p>
<p>But even within the framework of existing neural nets there‚Äôs currently a crucial limitation: neural net training as it‚Äôs now done is fundamentally sequential, with the effects of each batch of examples being propagated back to update the weights. And indeed with current computer hardware‚Äîeven taking into account GPUs‚Äîmost of a neural net is ‚Äúidle‚Äù most of the time during training, with just one part at a time being updated. And in a sense this is because our current computers tend to have memory that is separate from their CPUs (or GPUs). But in brains it‚Äôs presumably different‚Äîwith every ‚Äúmemory element‚Äù (i.e. neuron) also being a potentially active computational element. And if we could set up our future computer hardware this way it might become possible to do training much more efficiently.</p>
<h2 id="surely-a-network-thats-big-enough-can-do-anything">‚ÄúSurely a Network That‚Äôs Big Enough Can Do Anything!‚Äù </h2>
<p>The capabilities of something like ChatGPT seem so impressive that one might imagine that if one could just ‚Äúkeep going‚Äù and train larger and larger neural networks, then they‚Äôd eventually be able to ‚Äúdo everything‚Äù. And if one‚Äôs concerned with things that are readily accessible to immediate human thinking, it‚Äôs quite possible that this is the case. But the lesson of the past several hundred years of science is that there are things that can be figured out by formal processes, but aren‚Äôt readily accessible to immediate human thinking.</p>
<p>Nontrivial mathematics is one big example. But the general case is really computation. And ultimately the issue is the phenomenon of <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility">computational irreducibility</a>. There are some computations which one might think would take many steps to do, but which can in fact be ‚Äúreduced‚Äù to something quite immediate. But the discovery of computational irreducibility implies that this doesn‚Äôt always work. And instead there are processes‚Äîprobably like the one below‚Äîwhere to work out what happens inevitably requires essentially tracing each computational step:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img77.png" alt="" title="" width="553" height="279"> </p>
</div>
<p>The kinds of things that we normally do with our brains are presumably specifically chosen to avoid computational irreducibility. It takes special effort to do math in one‚Äôs brain. And it‚Äôs in practice largely impossible to ‚Äúthink through‚Äù the steps in the operation of any nontrivial program just in one‚Äôs brain. </p>
<p>But of course for that we have computers. And with computers we can readily do long, computationally irreducible things. And the key point is that there‚Äôs in general no shortcut for these. </p>
<p>Yes, we could memorize lots of specific examples of what happens in some particular computational system. And maybe we could even see some (‚Äúcomputationally reducible‚Äù) patterns that would allow us to do a little generalization. But the point is that computational irreducibility means that we can never guarantee that the unexpected won‚Äôt happen‚Äîand it‚Äôs only by explicitly doing the computation that you can tell what actually happens in any particular case.</p>
<p>And in the end there‚Äôs just a fundamental tension between learnability and computational irreducibility. Learning involves in effect <a href="https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis/">compressing data by leveraging regularities</a>. But computational irreducibility implies that ultimately there‚Äôs a limit to what regularities there may be.</p>
<p>As a practical matter, one can imagine building little computational devices‚Äîlike cellular automata or Turing machines‚Äîinto trainable systems like neural nets. And indeed such devices can serve as good ‚Äútools‚Äù for the neural net‚Äîlike <a href="https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/">Wolfram|Alpha can be a good tool for ChatGPT</a>. But computational irreducibility implies that one can‚Äôt expect to ‚Äúget inside‚Äù those devices and have them learn.</p>
<p>Or put another way, there‚Äôs an ultimate tradeoff between capability and trainability: the more you want a system to make ‚Äútrue use‚Äù of its computational capabilities, the more it‚Äôs going to show computational irreducibility, and the less it‚Äôs going to be trainable. And the more it‚Äôs fundamentally trainable, the less it‚Äôs going to be able to do sophisticated computation.</p>
<p>(For ChatGPT as it currently is, the situation is actually much more extreme, because the neural net used to generate each token of output is a pure ‚Äúfeed-forward‚Äù network, without loops, and therefore has no ability to do any kind of computation with nontrivial ‚Äúcontrol flow‚Äù.)</p>
<p>Of course, one might wonder whether it‚Äôs actually important to be able to do irreducible computations. And indeed for much of human history it wasn‚Äôt particularly important. But our modern technological world has been built on engineering that makes use of at least mathematical computations‚Äîand increasingly also more general computations. And if we look at the natural world, it‚Äôs <a href="https://www.wolframscience.com/nks/chap-8--implications-for-everyday-systems/">full of irreducible computation</a>‚Äîthat we‚Äôre slowly understanding how to emulate and use for our technological purposes. </p>
<p>Yes, a neural net can certainly notice the kinds of regularities in the natural world that we might also readily notice with ‚Äúunaided human thinking‚Äù. But if we want to work out things that are in the purview of mathematical or computational science the neural net isn‚Äôt going to be able to do it‚Äîunless it effectively ‚Äúuses as a tool‚Äù an ‚Äúordinary‚Äù computational system.</p>
<p>But there‚Äôs something potentially confusing about all of this. In the past there were plenty of tasks‚Äîincluding writing essays‚Äîthat we‚Äôve assumed were somehow ‚Äúfundamentally too hard‚Äù for computers. And now that we see them done by the likes of ChatGPT we tend to suddenly think that computers must have become vastly more powerful‚Äîin particular surpassing things they were already basically able to do (like progressively computing the behavior of computational systems like cellular automata). </p>
<p>But this isn‚Äôt the right conclusion to draw. Computationally irreducible processes are still computationally irreducible, and are still fundamentally hard for computers‚Äîeven if computers can readily compute their individual steps. And instead what we should conclude is that tasks‚Äîlike writing essays‚Äîthat we humans could do, but we didn‚Äôt think computers could do, are actually in some sense computationally easier than we thought. </p>
<p>In other words, the reason a neural net can be successful in writing an essay is because writing an essay turns out to be a ‚Äúcomputationally shallower‚Äù problem than we thought. And in a sense this takes us closer to ‚Äúhaving a theory‚Äù of how we humans manage to do things like writing essays, or in general deal with language. </p>
<p>If you had a big enough neural net then, yes, you might be able to do whatever humans can readily do. But you wouldn‚Äôt capture what the natural world in general can do‚Äîor that the tools that we‚Äôve fashioned from the natural world can do. And it‚Äôs the use of those tools‚Äîboth practical and conceptual‚Äîthat have allowed us in recent centuries to transcend the boundaries of what‚Äôs accessible to ‚Äúpure unaided human thought‚Äù, and capture for human purposes more of what‚Äôs out there in the physical and computational universe.</p>
<h2 id="the-concept-of-embeddings">The Concept of Embeddings</h2>
<p>Neural nets‚Äîat least as they‚Äôre currently set up‚Äîare fundamentally based on numbers. So if we‚Äôre going to to use them to work on something like text we‚Äôll need a way to <a href="https://reference.wolfram.com/language/guide/NetEncoderDecoder.html">represent our text with numbers</a>. And certainly we could start (essentially as ChatGPT does) by just assigning a number to every word in the dictionary. But there‚Äôs an important idea‚Äîthat‚Äôs for example central to ChatGPT‚Äîthat goes beyond that. And it‚Äôs the idea of ‚Äúembeddings‚Äù. One can think of an embedding as a way to try to represent the ‚Äúessence‚Äù of something by an array of numbers‚Äîwith the property that ‚Äúnearby things‚Äù are represented by nearby numbers.</p>
<p>And so, for example, we can think of a word embedding as trying to <a href="https://reference.wolfram.com/language/ref/FeatureSpacePlot.html">lay out words in a kind of ‚Äúmeaning space‚Äù</a> in which words that are somehow ‚Äúnearby in meaning‚Äù appear nearby in the embedding. The actual embeddings that are used‚Äîsay in ChatGPT‚Äîtend to involve large lists of numbers. But if we project down to 2D, we can show examples of how words are laid out by the embedding: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img78.png" alt="" title="" width="432" height="430"> </p>
</div>
<p>And, yes, what we see does remarkably well in capturing typical everyday impressions. But how can we construct such an embedding? Roughly the idea is to look at large amounts of text (here 5 billion words from the web) and then see ‚Äúhow similar‚Äù the ‚Äúenvironments‚Äù are in which different words appear. So, for example, ‚Äúalligator‚Äù and ‚Äúcrocodile‚Äù will often appear almost interchangeably in otherwise similar sentences, and that means they‚Äôll be placed nearby in the embedding. But ‚Äúturnip‚Äù and ‚Äúeagle‚Äù won‚Äôt tend to appear in otherwise similar sentences, so they‚Äôll be placed far apart in the embedding.</p>
<p>But how does one actually implement something like this using neural nets? Let‚Äôs start by talking about embeddings not for words, but for images. We want to find some way to characterize images by lists of numbers in such a way that ‚Äúimages we consider similar‚Äù are assigned similar lists of numbers. </p>
<p>How do we tell if we should ‚Äúconsider images similar‚Äù? Well, if our images are, say, of handwritten digits we might ‚Äúconsider two images similar‚Äù if they are of the same digit. Earlier we discussed a neural net that was trained to recognize handwritten digits. And we can think of this neural net as being set up so that in its final output it puts images into 10 different bins, one for each digit.</p>
<p>But what if we ‚Äúintercept‚Äù what‚Äôs going on inside the neural net before the final ‚Äúit‚Äôs a ‚Äò4‚Äô‚Äù decision is made? We might expect that inside the neural net there are numbers that characterize images as being ‚Äúmostly 4-like but a bit 2-like‚Äù or some such. And the idea is to pick up such numbers to use as elements in an embedding.</p>
<p>So here‚Äôs the concept. Rather than directly trying to characterize ‚Äúwhat image is near what other image‚Äù, we instead consider a well-defined task (in this case digit recognition) for which we can get explicit training data‚Äîthen use the fact that in doing this task the neural net implicitly has to make what amount to ‚Äúnearness decisions‚Äù. So instead of us ever explicitly having to talk about ‚Äúnearness of images‚Äù we‚Äôre just talking about the concrete question of what digit an image represents, and then we‚Äôre ‚Äúleaving it to the neural net‚Äù to implicitly determine what that implies about ‚Äúnearness of images‚Äù.</p>
<p>So how in more detail does this work for the digit recognition network? We can think of the network as consisting of 11 successive layers, that we might summarize iconically like this (with activation functions shown as separate layers):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img79.png" alt="" title="" width="600" height="35"> </p>
</div>
<p>At the beginning we‚Äôre feeding into the first layer actual images, represented by 2D arrays of pixel values. And at the end‚Äîfrom the last layer‚Äîwe‚Äôre getting out an array of 10 values, which we can think of saying ‚Äúhow certain‚Äù the network is that the image corresponds to each of the digits 0 through 9. </p>
<p>Feed in the image <img src="https://content.wolfram.com/sites/43/2023/02/sw021423number4.png" alt="" title="" width="24" height="14">and the values of the neurons in that last layer are:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img81.png" alt="" title="" width="519" height="52"> </p>
</div>
<p>In other words, the neural net is by this point ‚Äúincredibly certain‚Äù that this image is a 4‚Äîand to actually get the output ‚Äú4‚Äù we just have to pick out the position of the neuron with the largest value.</p>
<p>But what if we look one step earlier? The very last operation in the network is a so-called <a href="https://reference.wolfram.com/language/ref/SoftmaxLayer.html">softmax</a> which tries to ‚Äúforce certainty‚Äù. But before that‚Äôs been applied the values of the neurons are:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img82.png" alt="" title="" width="395" height="37"> </p>
</div>
<p>The neuron representing ‚Äú4‚Äù still has the highest numerical value. But there‚Äôs also information in the values of the other neurons. And we can expect that this list of numbers can in a sense be used to characterize the ‚Äúessence‚Äù of the image‚Äîand thus to provide something we can use as an embedding. And so, for example, each of the 4‚Äôs here has a slightly different ‚Äúsignature‚Äù (or ‚Äúfeature embedding‚Äù)‚Äîall very different from the 8‚Äôs:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img83.png" alt="" title="" width="529" height="72"> </p>
</div>
<p>Here we‚Äôre essentially using 10 numbers to characterize our images. But it‚Äôs often better to use much more than that. And for example in our digit recognition network we can get an array of 500 numbers by tapping into the preceding layer. And this is probably a reasonable array to use as an ‚Äúimage embedding‚Äù. </p>
<p>If we want to make an explicit visualization of ‚Äúimage space‚Äù for handwritten digits we need to ‚Äúreduce the dimension‚Äù, effectively by projecting the 500-dimensional vector we‚Äôve got into, say, 3D space:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img84.png" alt="" title="" width="326" height="354"> </p>
</div>
<p>We‚Äôve just talked about creating a characterization (and thus embedding) for images based effectively on identifying the similarity of images by determining whether (according to our training set) they correspond to the same handwritten digit. And we can do the same thing much more generally for images if we have a training set that identifies, say, which of 5000 common types of object (cat, dog, chair, ‚Ä¶) each image is of. And in this way we can make an image embedding that‚Äôs ‚Äúanchored‚Äù by our identification of common objects, but then ‚Äúgeneralizes around that‚Äù according to the behavior of the neural net. And the point is that insofar as that behavior aligns with how we humans perceive and interpret images, this will end up being an embedding that ‚Äúseems right to us‚Äù, and is useful in practice in doing ‚Äúhuman-judgement-like‚Äù tasks.</p>
<p>OK, so how do we follow the same kind of approach to find embeddings for words? The key is to start from a task about words for which we can readily do training. And the standard such task is ‚Äúword prediction‚Äù. Imagine we‚Äôre given ‚Äúthe ___ cat‚Äù. Based on a large corpus of text (say, the text content of the web), what are the probabilities for different words that might ‚Äúfill in the blank‚Äù? Or, alternatively, given ‚Äú___ black ___‚Äù what are the probabilities for different ‚Äúflanking words‚Äù?</p>
<p>How do we set this problem up for a neural net? Ultimately we have to formulate everything in terms of numbers. And one way to do this is just to assign a unique number to each of the 50,000 or so common words in English. So, for example, ‚Äúthe‚Äù might be 914, and ‚Äú cat‚Äù (with a space before it) might be 3542. (And these are the actual numbers used by GPT-2.) So for the ‚Äúthe ___ cat‚Äù problem, our input might be {914, 3542}. What should the output be like? Well, it should be a list of 50,000 or so numbers that effectively give the probabilities for each of the possible ‚Äúfill-in‚Äù words. And once again, to find an embedding, we want to ‚Äúintercept‚Äù the ‚Äúinsides‚Äù of the neural net just before it ‚Äúreaches its conclusion‚Äù‚Äîand then pick up the list of numbers that occur there, and that we can think of as ‚Äúcharacterizing each word‚Äù.</p>
<p>OK, so what do those characterizations look like? Over the past 10 years there‚Äôve been a sequence of different systems developed (<a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/ConceptNet-Numberbatch-Word-Vectors-V17.06/">word2vec</a>, <a href="https://resources.wolframcloud.com/NeuralNetRepository/search/?i=GloVe">GloVe</a>, <a href="https://resources.wolframcloud.com/NeuralNetRepository/search/?i=BERT">BERT</a>, <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/">GPT</a>, ‚Ä¶), each based on a different neural net approach. But ultimately all of them take words and characterize them by lists of hundreds to thousands of numbers. </p>
<p>In their raw form, these ‚Äúembedding vectors‚Äù are quite uninformative. For example, here‚Äôs what GPT-2 produces as the raw embedding vectors for three specific words: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img85.png" alt="" title="" width="467" height="131"> </p>
</div>
<p>If we do things like measure distances between these vectors, then we can find things like ‚Äúnearnesses‚Äù of words. Later we‚Äôll discuss in more detail what we might consider the ‚Äúcognitive‚Äù significance of such embeddings. But for now the main point is that we have a way to usefully turn words into ‚Äúneural-net-friendly‚Äù collections of numbers.</p>
<p>But actually we can go further than just characterizing words by collections of numbers; we can also do this for sequences of words, or indeed whole blocks of text. And inside ChatGPT that‚Äôs how it‚Äôs dealing with things. It takes the text it‚Äôs got so far, and generates an embedding vector to represent it. Then its goal is to find the probabilities for different words that might occur next. And it represents its answer for this as a list of numbers that essentially give the probabilities for each of the 50,000 or so possible words.</p>
<p>(Strictly, ChatGPT does not deal with words, but <a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener">rather with ‚Äútokens‚Äù</a>‚Äîconvenient linguistic units that might be whole words, or might just be pieces like ‚Äúpre‚Äù or ‚Äúing‚Äù or ‚Äúized‚Äù. Working with tokens makes it easier for ChatGPT to handle rare, compound and non-English words, and, sometimes, for better or worse, to invent new words.)</p>
<h2 id="inside-chatgpt">Inside ChatGPT</h2>
<p>OK, so we‚Äôre finally ready to discuss what‚Äôs inside ChatGPT. And, yes, ultimately, it‚Äôs a giant neural net‚Äîcurrently a version of the so-called GPT-3 network with 175 billion weights. In many ways this is a neural net very much like the other ones we‚Äôve discussed. But it‚Äôs a neural net that‚Äôs particularly set up for dealing with language. And its most notable feature is a piece of neural net architecture called a ‚Äútransformer‚Äù.</p>
<p>In the first neural nets we discussed above, every neuron at any given layer was basically connected (at least with some weight) to every neuron on the layer before. But this kind of fully connected network is (presumably) overkill if one‚Äôs working with data that has particular, known structure. And thus, for example, in the early stages of dealing with images, it‚Äôs typical to use so-called <a href="https://reference.wolfram.com/language/ref/ConvolutionLayer.html">convolutional neural nets</a> (‚Äúconvnets‚Äù) in which neurons are effectively laid out on a grid analogous to the pixels in the image‚Äîand connected only to neurons nearby on the grid. </p>
<p>The idea of transformers is to do something at least somewhat similar for sequences of tokens that make up a piece of text. But instead of just defining a fixed region in the sequence over which there can be connections, transformers instead introduce the notion of ‚Äú<a href="https://reference.wolfram.com/language/ref/AttentionLayer.html">attention</a>‚Äù‚Äîand the idea of ‚Äúpaying attention‚Äù more to some parts of the sequence than others. Maybe one day it‚Äôll make sense to just start a generic neural net and do all customization through training. But at least as of now it seems to be critical in practice to ‚Äúmodularize‚Äù things‚Äîas transformers do, and probably as our brains also do. </p>
<p>OK, so what does ChatGPT (or, rather, the GPT-3 network on which it‚Äôs based) actually do? Recall that its overall goal is to continue text in a ‚Äúreasonable‚Äù way, based on what it‚Äôs seen from the training it‚Äôs had (which consists in looking at billions of pages of text from the web, etc.) So at any given point, it‚Äôs got a certain amount of text‚Äîand its goal is to come up with an appropriate choice for the next token to add.</p>
<p>It operates in three basic stages. First, it takes the sequence of tokens that corresponds to the text so far, and finds an embedding (i.e. an array of numbers) that represents these. Then it operates on this embedding‚Äîin a ‚Äústandard neural net way‚Äù, with values ‚Äúrippling through‚Äù successive layers in a network‚Äîto produce a new embedding (i.e. a new array of numbers). It then takes the last part of this array and generates from it an array of about 50,000 values that turn into probabilities for different possible next tokens. (And, yes, it so happens that there are about the same number of tokens used as there are common words in English, though only about 3000 of the tokens are whole words, and the rest are fragments.)</p>
<p>A critical point is that every part of this pipeline is implemented by a neural network, whose weights are determined by end-to-end training of the network. In other words, in effect nothing except the overall architecture is ‚Äúexplicitly engineered‚Äù; everything is just ‚Äúlearned‚Äù from training data.</p>
<p>There are, however, plenty of details in the way the architecture is set up‚Äîreflecting all sorts of experience and neural net lore. And‚Äîeven though this is definitely going into the weeds‚ÄîI think it‚Äôs useful to talk about some of those details, not least to get a sense of just what goes into building something like ChatGPT.</p>
<p>First comes the embedding module. Here‚Äôs a schematic Wolfram Language representation for it for GPT-2:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img86.png" alt="" title="" width="424" height="217"> </p>
</div>
<p>The input is a <a href="https://reference.wolfram.com/language/ref/netencoder/SubwordTokens.html">vector of <em>n</em> tokens</a> (represented as in the previous section by integers from 1 to about 50,000). Each of these tokens is converted (by a <a href="https://reference.wolfram.com/language/ref/EmbeddingLayer.html">single-layer neural net</a>) into an embedding vector (of length 768 for GPT-2 and 12,288 for ChatGPT‚Äôs GPT-3). Meanwhile, there‚Äôs a ‚Äúsecondary pathway‚Äù that takes the <a href="https://reference.wolfram.com/language/ref/SequenceIndicesLayer.html">sequence of (integer) positions</a> for the tokens, and from these integers creates another embedding vector. And finally the embedding vectors from the token value and the token position are <a href="https://reference.wolfram.com/language/ref/ThreadingLayer.html">added together</a>‚Äîto produce the final sequence of embedding vectors from the embedding module.</p>
<p>Why does one just add the token-value and token-position embedding vectors together? I don‚Äôt think there‚Äôs any particular science to this. It‚Äôs just that various different things have been tried, and this is one that seems to work. And it‚Äôs part of the lore of neural nets that‚Äîin some sense‚Äîso long as the setup one has is ‚Äúroughly right‚Äù it‚Äôs usually possible to home in on details just by doing sufficient training, without ever really needing to ‚Äúunderstand at an engineering level‚Äù quite how the neural net has ended up configuring itself.</p>
<p>Here‚Äôs what the embedding module does, operating on the string <em>hello hello hello hello hello hello hello hello hello hello bye bye bye bye bye bye bye bye bye bye</em>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img87.png" alt="" title="" width="373" height="158"> </p>
</div>
<p>The elements of the embedding vector for each token are shown down the page, and across the page we see first a run of ‚Äú<em>hello</em>‚Äù embeddings, followed by a run of ‚Äú<em>bye</em>‚Äù ones. The second array above is the positional embedding‚Äîwith its somewhat-random-looking structure being just what ‚Äúhappened to be learned‚Äù (in this case in GPT-2).</p>
<p>OK, so after the embedding module comes the ‚Äúmain event‚Äù of the transformer: a sequence of so-called ‚Äúattention blocks‚Äù (12 for GPT-2, 96 for ChatGPT‚Äôs GPT-3). It‚Äôs all pretty complicated‚Äîand reminiscent of typical large hard-to-understand engineering systems, or, for that matter, biological systems. But anyway, here‚Äôs a schematic representation of a single ‚Äúattention block‚Äù (for GPT-2):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img88.png" alt="" title="" width="654" height="193"> </p>
</div>
<p>Within each such attention block there are a collection of ‚Äúattention heads‚Äù (12 for GPT-2, 96 for ChatGPT‚Äôs GPT-3)‚Äîeach of which operates independently on different chunks of values in the embedding vector. (And, yes, we don‚Äôt know any particular reason why it‚Äôs a good idea to split up the embedding vector, or what the different parts of it ‚Äúmean‚Äù; this is just one of those things that‚Äôs been ‚Äúfound to work‚Äù.) </p>
<p>OK, so what do the attention heads do? Basically they‚Äôre a way of ‚Äúlooking back‚Äù in the sequence of tokens (i.e. in the text produced so far), and ‚Äúpackaging up the past‚Äù in a form that‚Äôs useful for finding the next token. <a target="_self" href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#its-just-adding-one-word-at-a-time">In the first section above</a> we talked about using 2-gram probabilities to pick words based on their immediate predecessors. What the ‚Äúattention‚Äù mechanism in transformers does is to allow ‚Äúattention to‚Äù even much earlier words‚Äîthus potentially capturing the way, say, verbs can refer to nouns that appear many words before them in a sentence.</p>
<p>At a more detailed level, what an attention head does is to recombine chunks in the embedding vectors associated with different tokens, with certain weights. And so, for example, the 12 attention heads in the first attention block (in GPT-2) have the following (‚Äúlook-back-all-the-way-to-the-beginning-of-the-sequence-of-tokens‚Äù) patterns of ‚Äúrecombination weights‚Äù for the ‚Äú<em>hello</em>, <em>bye</em>‚Äù string above:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img89A.png" alt="" title="" width="550" height="179"> </p>
</div>
<p>After being processed by the attention heads, the resulting ‚Äúre-weighted embedding vector‚Äù (of length 768 for GPT-2 and length 12,288 for ChatGPT‚Äôs GPT-3) is passed through a standard <a href="https://reference.wolfram.com/language/ref/LinearLayer.html">‚Äúfully connected‚Äù neural net layer</a>. It‚Äôs hard to get a handle on what this layer is doing. But here‚Äôs a plot of the 768√ó768 matrix of weights it‚Äôs using (here for GPT-2):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img90A.png" alt="" title="" width="358" height="358"> </p>
</div>
<p>Taking 64√ó64 moving averages, some (random-walk-ish) structure begins to emerge:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img91.png" alt="" title="" width="360" height="359"> </p>
</div>
<p>What determines this structure? Ultimately it‚Äôs presumably some ‚Äúneural net encoding‚Äù of features of human language. But as of now, what those features might be is quite unknown. In effect, we‚Äôre ‚Äúopening up the brain of ChatGPT‚Äù (or at least GPT-2) and discovering, yes, it‚Äôs complicated in there, and we don‚Äôt understand it‚Äîeven though in the end it‚Äôs producing recognizable human language.</p>
<p>OK, so after going through one attention block, we‚Äôve got a new embedding vector‚Äîwhich is then successively passed through additional attention blocks (a total of 12 for GPT-2; 96 for GPT-3). Each attention block has its own particular pattern of ‚Äúattention‚Äù and ‚Äúfully connected‚Äù weights. Here for GPT-2 are the sequence of attention weights for the ‚Äúhello, bye‚Äù input, for the first attention head:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img92A.png" alt="" title="" width="512" height="167"> </p>
</div>
<p>And here are the (moving-averaged) ‚Äúmatrices‚Äù for the fully connected layers:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img93A.png" alt="" title="" width="569" height="422"> </p>
</div>
<p>Curiously, even though these ‚Äúmatrices of weights‚Äù in different attention blocks look quite similar, the distributions of the sizes of weights can be somewhat different (and are not always Gaussian):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img94A.png" alt="" title="" width="351" height="164"> </p>
</div>
<p>So after going through all these attention blocks what is the net effect of the transformer? Essentially it‚Äôs to transform the original collection of embeddings for the sequence of tokens to a final collection. And the particular way ChatGPT works is then to pick up the last embedding in this collection, and ‚Äúdecode‚Äù it to produce a list of probabilities for what token should come next.</p>
<p>So that‚Äôs in outline what‚Äôs inside ChatGPT. It may seem complicated (not least because of its many inevitably somewhat arbitrary ‚Äúengineering choices‚Äù), but actually the ultimate elements involved are remarkably simple. Because in the end what we‚Äôre dealing with is just a neural net made of ‚Äúartificial neurons‚Äù, each doing the simple operation of taking a collection of numerical inputs, and then combining them with certain weights. </p>
<p>The original input to ChatGPT is an array of numbers (the embedding vectors for the tokens so far), and what happens when ChatGPT ‚Äúruns‚Äù to produce a new token is just that these numbers ‚Äúripple through‚Äù the layers of the neural net, with each neuron ‚Äúdoing its thing‚Äù and passing the result to neurons on the next layer. There‚Äôs no looping or ‚Äúgoing back‚Äù. Everything just ‚Äúfeeds forward‚Äù through the network. </p>
<p>It‚Äôs a very different setup from a typical computational system‚Äîlike a <a href="https://www.wolframscience.com/nks/p78--turing-machines/">Turing machine</a>‚Äîin which results are repeatedly ‚Äúreprocessed‚Äù by the same computational elements. Here‚Äîat least in generating a given token of output‚Äîeach computational element (i.e. neuron) is used only once. </p>
<p>But there is in a sense still an ‚Äúouter loop‚Äù that reuses computational elements even in ChatGPT. Because when ChatGPT is going to generate a new token, it always ‚Äúreads‚Äù (i.e. takes as input) the whole sequence of tokens that come before it, including tokens that ChatGPT itself has ‚Äúwritten‚Äù previously. And we can think of this setup as meaning that ChatGPT does‚Äîat least at its outermost level‚Äîinvolve a ‚Äúfeedback loop‚Äù, albeit one in which every iteration is explicitly visible as a token that appears in the text that it generates.</p>
<p>But let‚Äôs come back to the core of ChatGPT: the neural net that‚Äôs being repeatedly used to generate each token. At some level it‚Äôs very simple: a whole collection of identical artificial neurons. And some parts of the network just consist of (‚Äú<a href="https://reference.wolfram.com/language/ref/LinearLayer.html">fully connected</a>‚Äù) layers of neurons in which every neuron on a given layer is connected (with some weight) to every neuron on the layer before. But particularly with its transformer architecture, ChatGPT has parts with more structure, in which only specific neurons on different layers are connected. (Of course, one could still say that ‚Äúall neurons are connected‚Äù‚Äîbut some just have zero weight.)</p>
<p>In addition, there are aspects of the neural net in ChatGPT that aren‚Äôt most naturally thought of as just consisting of ‚Äúhomogeneous‚Äù layers. And for example‚Äîas the iconic summary above indicates‚Äîinside an attention block there are places where ‚Äúmultiple copies are made‚Äù of incoming data, each then going through a different ‚Äúprocessing path‚Äù, potentially involving a different number of layers, and only later recombining. But while this may be a convenient representation of what‚Äôs going on, it‚Äôs always at least in principle possible to think of ‚Äúdensely filling in‚Äù layers, but just having some weights be zero.</p>
<p>If one looks at the longest path through ChatGPT, there are about 400 (core) layers involved‚Äîin some ways not a huge number. But there are millions of neurons‚Äîwith a total of 175 billion connections and therefore 175 billion weights. And one thing to realize is that every time ChatGPT generates a new token, it has to do a calculation involving every single one of these weights. Implementationally these calculations can be somewhat organized ‚Äúby layer‚Äù into highly parallel array operations that can conveniently be done on GPUs. But for each token that‚Äôs produced, there still have to be 175 billion calculations done (and in the end a bit more)‚Äîso that, yes, it‚Äôs not surprising that it can take a while to generate a long piece of text with ChatGPT. </p>
<p>But in the end, the remarkable thing is that all these operations‚Äîindividually as simple as they are‚Äîcan somehow together manage to do such a good ‚Äúhuman-like‚Äù job of generating text. It has to be emphasized again that (at least so far as we know) there‚Äôs no ‚Äúultimate theoretical reason‚Äù why anything like this should work. And in fact, as we‚Äôll discuss, I think we have to view this as a‚Äîpotentially surprising‚Äîscientific discovery: that somehow in a neural net like ChatGPT‚Äôs it‚Äôs possible to capture the essence of what human brains manage to do in generating language. </p>
<h2 id="the-training-of-chatgpt">The Training of ChatGPT</h2>
<p>OK, so we‚Äôve now given an outline of how ChatGPT works once it‚Äôs set up. But how did it get set up? How were all those 175 billion weights in its neural net determined? Basically they‚Äôre the result of very large-scale training, based on a huge corpus of text‚Äîon the web, in books, etc.‚Äîwritten by humans. As we‚Äôve said, even given all that training data, it‚Äôs certainly not obvious that a neural net would be able to successfully produce ‚Äúhuman-like‚Äù text. And, once again, there seem to be detailed pieces of engineering needed to make that happen. But the big surprise‚Äîand discovery‚Äîof ChatGPT is that it‚Äôs possible at all. And that‚Äîin effect‚Äîa neural net with ‚Äújust‚Äù 175 billion weights can make a ‚Äúreasonable model‚Äù of text humans write.</p>
<p>In modern times, there‚Äôs lots of text written by humans that‚Äôs out there in digital form. The public web has at least several billion human-written pages, with altogether perhaps a trillion words of text. And if one includes non-public webpages, the numbers might be at least 100 times larger. So far, more than 5 million digitized books have been made available (out of 100 million or so that have ever been published), giving another 100 billion or so words of text. And that‚Äôs not even mentioning text derived from speech in videos, etc. (As a personal comparison, <a href="https://www.stephenwolfram.com/publications/">my total lifetime output of published material</a> has been a bit under 3 million words, and over the <a href="https://writings.stephenwolfram.com/2012/03/the-personal-analytics-of-my-life/">past 30 years I‚Äôve written</a> about 15 million words of email, and altogether typed perhaps 50 million words‚Äîand in just the past couple of years I‚Äôve spoken more than 10 million words on <a href="https://www.stephenwolfram.com/livestreams">livestreams</a>. And, yes, I‚Äôll train a bot from all of that.)</p>
<p>But, OK, given all this data, how does one train a neural net from it? The basic process is very much as we discussed it in the simple examples above. You present a batch of examples, and then you adjust the weights in the network to minimize the error (‚Äúloss‚Äù) that the network makes on those examples. The main thing that‚Äôs expensive about ‚Äúback propagating‚Äù from the error is that each time you do this, every weight in the network will typically change at least a tiny bit, and there are just a lot of weights to deal with. (The actual ‚Äúback computation‚Äù is typically only a small constant factor harder than the forward one.)</p>
<p>With modern GPU hardware, it‚Äôs straightforward to compute the results from batches of thousands of examples in parallel. But when it comes to actually updating the weights in the neural net, current methods require one to do this basically batch by batch. (And, yes, this is probably where actual brains‚Äîwith their combined computation and memory elements‚Äîhave, for now, at least an architectural advantage.)</p>
<p>Even in the seemingly simple cases of learning numerical functions that we discussed earlier, we found we often had to use millions of examples to successfully train a network, at least from scratch. So how many examples does this mean we‚Äôll need in order to train a ‚Äúhuman-like language‚Äù model? There doesn‚Äôt seem to be any fundamental ‚Äútheoretical‚Äù way to know. But in practice ChatGPT was successfully trained on a few hundred billion words of text.</p>
<p>Some of the text it was fed several times, some of it only once. But somehow it ‚Äúgot what it needed‚Äù from the text it saw. But given this volume of text to learn from, how large a network should it require to ‚Äúlearn it well‚Äù? Again, we don‚Äôt yet have a fundamental theoretical way to say. Ultimately‚Äîas we‚Äôll discuss further below‚Äîthere‚Äôs presumably a certain ‚Äútotal algorithmic content‚Äù to human language and what humans typically say with it. But the next question is how efficient a neural net will be at implementing a model based on that algorithmic content. And again we don‚Äôt know‚Äîalthough the success of ChatGPT suggests it‚Äôs reasonably efficient.</p>
<p>And in the end we can just note that ChatGPT does what it does using a couple hundred billion weights‚Äîcomparable in number to the total number of words (or tokens) of training data it‚Äôs been given. In some ways it‚Äôs perhaps surprising (though empirically observed also in smaller analogs of ChatGPT) that the ‚Äúsize of the network‚Äù that seems to work well is so comparable to the ‚Äúsize of the training data‚Äù. After all, it‚Äôs certainly not that somehow ‚Äúinside ChatGPT‚Äù all that text from the web and books and so on is ‚Äúdirectly stored‚Äù. Because what‚Äôs actually inside ChatGPT are a bunch of numbers‚Äîwith a bit less than 10 digits of precision‚Äîthat are some kind of distributed encoding of the aggregate structure of all that text. </p>
<p>Put another way, we might ask what the ‚Äúeffective information content‚Äù is of human language and what‚Äôs typically said with it. There‚Äôs the raw corpus of examples of language. And then there‚Äôs the representation in the neural net of ChatGPT. That representation is very likely far from the ‚Äúalgorithmically minimal‚Äù representation (as we‚Äôll discuss below). But it‚Äôs a representation that‚Äôs readily usable by the neural net. And in this representation it seems there‚Äôs in the end rather little ‚Äúcompression‚Äù of the training data; it seems on average to basically take only a bit less than one neural net weight to carry the ‚Äúinformation content‚Äù of a word of training data.</p>
<p>When we run ChatGPT to generate text, we‚Äôre basically having to use each weight once. So if there are <em>n</em> weights, we‚Äôve got of order <em>n</em> computational steps to do‚Äîthough in practice many of them can typically be done in parallel in GPUs. But if we need about <em>n</em> words of training data to set up those weights, then from what we‚Äôve said above we can conclude that we‚Äôll need about <em>n</em><sup>2</sup> computational steps to do the training of the network‚Äîwhich is why, with current methods, one ends up needing to talk about billion-dollar training efforts.</p>
<h2 id="beyond-basic-training">Beyond Basic Training</h2>
<p>The majority of the effort in training ChatGPT is spent ‚Äúshowing it‚Äù large amounts of existing text from the web, books, etc. But it turns out there‚Äôs another‚Äîapparently rather important‚Äîpart too. </p>
<p>As soon as it‚Äôs finished its ‚Äúraw training‚Äù from the original corpus of text it‚Äôs been shown, the neural net inside ChatGPT is ready to start generating its own text, continuing from prompts, etc. But while the results from this may often seem reasonable, they tend‚Äîparticularly for longer pieces of text‚Äîto ‚Äúwander off‚Äù in often rather non-human-like ways. It‚Äôs not something one can readily detect, say, by doing traditional statistics on the text. But it‚Äôs something that actual humans reading the text easily notice.</p>
<p>And a <a href="https://openai.com/blog/instruction-following/" target="_blank" rel="noopener">key idea in the construction of ChatGPT</a> was to have another step after ‚Äúpassively reading‚Äù things like the web: to have actual humans actively interact with ChatGPT, see what it produces, and in effect give it feedback on ‚Äúhow to be a good chatbot‚Äù. But how can the neural net use that feedback? The first step is just to have humans rate results from the neural net. But then another neural net model is built that attempts to predict those ratings. But now this prediction model can be run‚Äîessentially like a loss function‚Äîon the original network, in effect allowing that network to be ‚Äútuned up‚Äù by the human feedback that‚Äôs been given. And the results in practice seem to have a big effect on the success of the system in producing ‚Äúhuman-like‚Äù output. </p>
<p>In general, it‚Äôs interesting how little ‚Äúpoking‚Äù the ‚Äúoriginally trained‚Äù network seems to need to get it to usefully go in particular directions. One might have thought that to have the network behave as if it‚Äôs ‚Äúlearned something new‚Äù one would have to go in and run a training algorithm, adjusting weights, and so on.</p>
<p>But that‚Äôs not the case. Instead, it seems to be sufficient to basically tell ChatGPT something one time‚Äîas part of the prompt you give‚Äîand then it can successfully make use of what you told it when it generates text. And once again, the fact that this works is, I think, an important clue in understanding what ChatGPT is ‚Äúreally doing‚Äù and how it relates to the structure of human language and thinking. </p>
<p>There‚Äôs certainly something rather human-like about it: that at least once it‚Äôs had all that pre-training you can tell it something just once and it can ‚Äúremember it‚Äù‚Äîat least ‚Äúlong enough‚Äù to generate a piece of text using it. So what‚Äôs going on in a case like this? It could be that ‚Äúeverything you might tell it is already in there somewhere‚Äù‚Äîand you‚Äôre just leading it to the right spot. But that doesn‚Äôt seem plausible. Instead, what seems more likely is that, yes, the elements are already in there, but the specifics are defined by something like a ‚Äútrajectory between those elements‚Äù and that‚Äôs what you‚Äôre introducing when you tell it something.</p>
<p>And indeed, much like for humans, if you tell it something bizarre and unexpected that completely doesn‚Äôt fit into the framework it knows, it doesn‚Äôt seem like it‚Äôll successfully be able to ‚Äúintegrate‚Äù this. It can ‚Äúintegrate‚Äù it only if it‚Äôs basically riding in a fairly simple way on top of the framework it already has.</p>
<p>It‚Äôs also worth pointing out again that there are inevitably ‚Äúalgorithmic limits‚Äù to what the neural net can ‚Äúpick up‚Äù. Tell it ‚Äúshallow‚Äù rules of the form ‚Äúthis goes to that‚Äù, etc., and the neural net will most likely be able to represent and reproduce these just fine‚Äîand indeed what it ‚Äúalready knows‚Äù from language will give it an immediate pattern to follow. But try to give it rules for an actual ‚Äúdeep‚Äù computation that involves many potentially computationally irreducible steps and it just won‚Äôt work. (Remember that at each step it‚Äôs always just ‚Äúfeeding data forward‚Äù in its network, never looping except by virtue of generating new tokens.)</p>
<p>Of course, the network can learn the answer to specific ‚Äúirreducible‚Äù computations. But as soon as there are combinatorial numbers of possibilities, no such ‚Äútable-lookup-style‚Äù approach will work. And so, yes, just like humans, it‚Äôs time then for neural nets to ‚Äúreach out‚Äù and use actual computational tools. (And, yes, <a href="https://www.wolframalpha.com/">Wolfram|Alpha</a> and <a href="https://www.wolfram.com/language/">Wolfram Language</a> are <a href="https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/">uniquely suitable</a>, because they‚Äôve been built to ‚Äútalk about things in the world‚Äù, just like the language-model neural nets.)</p>
<h2 id="what-really-lets-chatgpt-work">What Really Lets ChatGPT Work?</h2>
<p>Human language‚Äîand the processes of thinking involved in generating it‚Äîhave always seemed to represent a kind of pinnacle of complexity. And indeed it‚Äôs seemed somewhat remarkable that human brains‚Äîwith their network of a ‚Äúmere‚Äù 100 billion or so neurons (and maybe 100 trillion connections) could be responsible for it. Perhaps, one might have imagined, there‚Äôs something more to brains than their networks of neurons‚Äîlike some new layer of undiscovered physics. But now with ChatGPT we‚Äôve got an important new piece of information: we know that a pure, artificial neural network with about as many connections as brains have neurons is capable of doing a surprisingly good job of generating human language.</p>
<p>And, yes, that‚Äôs still a big and complicated system‚Äîwith about as many neural net weights as there are words of text currently available out there in the world. But at some level it still seems difficult to believe that all the richness of language and the things it can talk about can be encapsulated in such a finite system. Part of what‚Äôs going on is no doubt a reflection of the ubiquitous phenomenon (that first became evident in the <a href="https://www.wolframscience.com/nks/chap-2--the-crucial-experiment#sect-2-1--how-do-simple-programs-behave">example of rule 30</a>) that computational processes can in effect greatly amplify the apparent complexity of systems even when their underlying rules are simple. But, actually, as we discussed above, neural nets of the kind used in ChatGPT tend to be specifically constructed to restrict the effect of this phenomenon‚Äîand the computational irreducibility associated with it‚Äîin the interest of making their training more accessible.</p>
<p>So how is it, then, that something like ChatGPT can get as far as it does with language? The basic answer, I think, is that language is at a fundamental level somehow simpler than it seems. And this means that ChatGPT‚Äîeven with its ultimately straightforward neural net structure‚Äîis successfully able to ‚Äúcapture the essence‚Äù of human language and the thinking behind it. And moreover, in its training, ChatGPT has somehow ‚Äúimplicitly discovered‚Äù whatever regularities in language (and thinking) make this possible.</p>
<p>The success of ChatGPT is, I think, giving us evidence of a fundamental and important piece of science: it‚Äôs suggesting that we can expect there to be major new ‚Äúlaws of language‚Äù‚Äîand effectively ‚Äúlaws of thought‚Äù‚Äîout there to discover. In ChatGPT‚Äîbuilt as it is as a neural net‚Äîthose laws are at best implicit. But if we could somehow make the laws explicit, there‚Äôs the potential to do the kinds of things ChatGPT does in vastly more direct, efficient‚Äîand transparent‚Äîways.</p>
<p>But, OK, so what might these laws be like? Ultimately they must give us some kind of prescription for how language‚Äîand the things we say with it‚Äîare put together. Later we‚Äôll discuss how ‚Äúlooking inside ChatGPT‚Äù may be able to give us some hints about this, and how what we know from building computational language suggests a path forward. But first let‚Äôs discuss two long-known examples of what amount to ‚Äúlaws of language‚Äù‚Äîand how they relate to the operation of ChatGPT.</p>
<p>The first is the syntax of language. Language is not just a random jumble of words. Instead, there are (fairly) definite <a href="https://www.wolframscience.com/nks/notes-10-12--computer-and-human-languages/">grammatical rules</a> for how words of different kinds can be put together: in English, for example, nouns can be preceded by adjectives and followed by verbs, but typically two nouns can‚Äôt be right next to each other. Such grammatical structure can (at least approximately) be captured by a set of rules that define how what amount to <a href="https://reference.wolfram.com/language/ref/TextStructure.html">‚Äúparse trees‚Äù can be put together</a>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img96.png" alt="" title="" width="613" height="262"> </p>
</div>
<p>ChatGPT doesn‚Äôt have any explicit ‚Äúknowledge‚Äù of such rules. But somehow in its training it implicitly ‚Äúdiscovers‚Äù them‚Äîand then seems to be good at following them. So how does this work? At a ‚Äúbig picture‚Äù level it‚Äôs not clear. But to get some insight it‚Äôs perhaps instructive to look at a much simpler example.</p>
<p>Consider a ‚Äúlanguage‚Äù formed from sequences of (‚Äôs and )‚Äôs, with a <a href="https://www.wolframscience.com/nks/notes-7-9--nested-lists/">grammar that specifies</a> that parentheses should always be balanced, as represented by a parse tree like:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img97.png" alt="" title="" width="531" height="123"> </p>
</div>
<p>Can we train a neural net to produce ‚Äúgrammatically correct‚Äù parenthesis sequences? There are various ways to handle sequences in neural nets, but let‚Äôs use transformer nets, as ChatGPT does. And given a simple transformer net, we can start feeding it grammatically correct parenthesis sequences as training examples. A subtlety (which actually also appears in ChatGPT‚Äôs generation of human language) is that in addition to our ‚Äúcontent tokens‚Äù (here ‚Äú(‚Äù and ‚Äú)‚Äù) we have to include an ‚ÄúEnd‚Äù token, that‚Äôs generated to indicate that the output shouldn‚Äôt continue any further (i.e. for ChatGPT, that one‚Äôs reached the ‚Äúend of the story‚Äù).</p>
<p>If we set up a transformer net with just one attention block with 8 heads and feature vectors of length 128 (ChatGPT also uses feature vectors of length 128, but has 96 attention blocks, each with 96 heads) then it doesn‚Äôt seem possible to get it to learn much about parenthesis language. But with 2 attention blocks, the learning process seems to converge‚Äîat least after 10 million or so examples have been given (and, as is common with transformer nets, showing yet more examples just seems to degrade its performance).</p>
<p>So with this network, we can do the analog of what ChatGPT does, and ask for probabilities for what the next token should be‚Äîin a parenthesis sequence:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img98.png" alt="" title="" width="503" height="83"> </p>
</div>
<p>And in the first case, the network is ‚Äúpretty sure‚Äù that the sequence can‚Äôt end here‚Äîwhich is good, because if it did, the parentheses would be left unbalanced. In the second case, however, it ‚Äúcorrectly recognizes‚Äù that the sequence can end here, though it also ‚Äúpoints out‚Äù that it‚Äôs possible to ‚Äústart again‚Äù, putting down a ‚Äú(‚Äù, presumably with a ‚Äú)‚Äù to follow. But, oops, even with its 400,000 or so laboriously trained weights, it says there‚Äôs a 15% probability to have ‚Äú)‚Äù as the next token‚Äîwhich isn‚Äôt right, because that would necessarily lead to an unbalanced parenthesis.</p>
<p>Here‚Äôs what we get if we ask the network for the highest-probability completions for progressively longer sequences of (‚Äôs:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img99A.png" alt="" title="" width="341" height="331"> </p>
</div>
<p>And, yes, up to a certain length the network does just fine. But then it starts failing. It‚Äôs a pretty typical kind of thing to see in a ‚Äúprecise‚Äù situation like this with a neural net (or with machine learning in general). Cases that a human ‚Äúcan solve in a glance‚Äù the neural net can solve too. But cases that require doing something ‚Äúmore algorithmic‚Äù (e.g. explicitly counting parentheses to see if they‚Äôre closed) the neural net tends to somehow be ‚Äútoo computationally shallow‚Äù to reliably do. (By the way, even the full current ChatGPT has a hard time correctly matching parentheses in long sequences.)</p>
<p>So what does this mean for things like ChatGPT and the syntax of a language like English? The parenthesis language is ‚Äúaustere‚Äù‚Äîand much more of an ‚Äúalgorithmic story‚Äù. But in English it‚Äôs much more realistic to be able to ‚Äúguess‚Äù what‚Äôs grammatically going to fit on the basis of local choices of words and other hints. And, yes, the neural net is much better at this‚Äîeven though perhaps it might miss some ‚Äúformally correct‚Äù case that, well, humans might miss as well. But the main point is that the fact that there‚Äôs an overall syntactic structure to the language‚Äîwith all the regularity that implies‚Äîin a sense limits ‚Äúhow much‚Äù the neural net has to learn. And a key ‚Äúnatural-science-like‚Äù observation is that the transformer architecture of neural nets like the one in ChatGPT seems to successfully be able to learn the kind of nested-tree-like syntactic structure that seems to exist (at least in some approximation) in all human languages.</p>
<p>Syntax provides one kind of constraint on language. But there are clearly more. A sentence like ‚ÄúInquisitive electrons eat blue theories for fish‚Äù is grammatically correct but isn‚Äôt something one would normally expect to say, and wouldn‚Äôt be considered a success if ChatGPT generated it‚Äîbecause, well, with the normal meanings for the words in it, it‚Äôs basically meaningless. </p>
<p>But is there a general way to tell if a sentence is meaningful? There‚Äôs no traditional overall theory for that. But it‚Äôs something that one can think of ChatGPT as having implicitly ‚Äúdeveloped a theory for‚Äù after being trained with billions of (presumably meaningful) sentences from the web, etc. </p>
<p>What might this theory be like? Well, there‚Äôs one tiny corner that‚Äôs basically been known for two millennia, and that‚Äôs <a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/">logic</a>. And certainly in the syllogistic form in which Aristotle discovered it, logic is basically a way of saying that sentences that follow certain patterns are reasonable, while others are not. Thus, for example, it‚Äôs reasonable to say ‚ÄúAll X are Y. This is not Y, so it‚Äôs not an X‚Äù (as in ‚ÄúAll fishes are blue. This is not blue, so it‚Äôs not a fish.‚Äù). And just as one can somewhat whimsically imagine that Aristotle discovered syllogistic logic by going (‚Äúmachine-learning-style‚Äù) through lots of examples of rhetoric, so too one can imagine that in the training of ChatGPT it will have been able to ‚Äúdiscover syllogistic logic‚Äù by looking at lots of text on the web, etc. (And, yes, while one can therefore expect ChatGPT to produce text that contains ‚Äúcorrect inferences‚Äù based on things like syllogistic logic, it‚Äôs a quite different story when it comes to more sophisticated formal logic‚Äîand I think one can expect it to fail here for the same kind of reasons it fails in parenthesis matching.)</p>
<p>But beyond the narrow example of logic, what can be said about how to systematically construct (or recognize) even plausibly meaningful text? Yes, there are things like <a href="https://en.wikipedia.org/wiki/Mad_Libs" target="_blank" rel="noopener">Mad Libs</a> that use very specific ‚Äúphrasal templates‚Äù. But somehow ChatGPT implicitly has a much more general way to do it. And perhaps there‚Äôs nothing to be said about how it can be done beyond ‚Äúsomehow it happens when you have 175 billion neural net weights‚Äù. But I strongly suspect that there‚Äôs a much simpler and stronger story.</p>
<h2 id="meaning-space-and-semantic-laws-of-motion">Meaning Space and Semantic Laws of Motion</h2>
<p>We discussed above that inside ChatGPT any piece of text is effectively represented by an array of numbers that we can think of as coordinates of a point in some kind of ‚Äúlinguistic feature space‚Äù. So when ChatGPT continues a piece of text this corresponds to tracing out a trajectory in linguistic feature space. But now we can ask what makes this trajectory correspond to text we consider meaningful. And might there perhaps be some kind of ‚Äúsemantic laws of motion‚Äù that define‚Äîor at least constrain‚Äîhow points in linguistic feature space can move around while preserving ‚Äúmeaningfulness‚Äù?</p>
<p>So what is this linguistic feature space like? Here‚Äôs an example of how single words (here, common nouns) might get laid out if we project such a feature space down to 2D:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img100.png" alt="" title="" width="558" height="524"> </p>
</div>
<p>We saw another example above based on words representing plants and animals. But the point in both cases is that ‚Äúsemantically similar words‚Äù are placed nearby.</p>
<p>As another example, here‚Äôs how words corresponding to different parts of speech get laid out:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img101-edit.png" alt="" title="" width="575" height="385"> </p>
</div>
<p>Of course, a given word doesn‚Äôt in general just have ‚Äúone meaning‚Äù (or necessarily correspond to just one part of speech). And by looking at how sentences containing a word lay out in feature space, one can often ‚Äútease apart‚Äù different meanings‚Äîas in the example here for the word ‚Äúcrane‚Äù (bird or machine?):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img102.png" alt="" title="" width="593" height="364"> </p>
</div>
<p>OK, so it‚Äôs at least plausible that we can think of this feature space as placing ‚Äúwords nearby in meaning‚Äù close in this space. But what kind of additional structure can we identify in this space? Is there for example some kind of notion of ‚Äúparallel transport‚Äù that would reflect ‚Äúflatness‚Äù in the space? One way to get a handle on that is to look at analogies:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img103.png" alt="" title="" width="552" height="279"> </p>
</div>
<p>And, yes, even when we project down to 2D, there‚Äôs often at least a ‚Äúhint of flatness‚Äù, though it‚Äôs certainly not universally seen.</p>
<p>So what about trajectories? We can look at the trajectory that a prompt for ChatGPT follows in feature space‚Äîand then we can see how ChatGPT continues that:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img104.png" alt="" title="" width="549" height="339"> </p>
</div>
<p>There‚Äôs certainly no ‚Äúgeometrically obvious‚Äù law of motion here. And that‚Äôs not at all surprising; we fully expect this to be a <a href="https://writings.stephenwolfram.com/2021/09/multicomputation-a-fourth-paradigm-for-theoretical-science/#linguistics">considerably more complicated story</a>. And, for example, it‚Äôs far from obvious that even if there is a ‚Äúsemantic law of motion‚Äù to be found, what kind of embedding (or, in effect, what ‚Äúvariables‚Äù) it‚Äôll most naturally be stated in. </p>
<p>In the picture above, we‚Äôre showing several steps in the ‚Äútrajectory‚Äù‚Äîwhere at each step we‚Äôre picking the word that ChatGPT considers the most probable (the ‚Äúzero temperature‚Äù case). But we can also ask what words can ‚Äúcome next‚Äù with what probabilities at a given point:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img105.png" alt="" title="" width="359" height="227"> </p>
</div>
<p>And what we see in this case is that there‚Äôs a ‚Äúfan‚Äù of high-probability words that seems to go in a more or less definite direction in feature space. What happens if we go further? Here are the successive ‚Äúfans‚Äù that appear as we ‚Äúmove along‚Äù the trajectory:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img106.png" alt="" title="" width="578" height="566"> </p>
</div>
<p>Here‚Äôs a 3D representation, going for a total of 40 steps:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2023/02/sw021423img107.png" alt="" title="" width="359" height="294"> </p>
</div>
<p>And, yes, this seems like a mess‚Äîand doesn‚Äôt do anything to particularly encourage the idea that one can expect to identify ‚Äúmathematical-physics-like‚Äù ‚Äúsemantic laws of motion‚Äù by empirically studying ‚Äúwhat ChatGPT is doing inside‚Äù. But perhaps we‚Äôre just looking at the ‚Äúwrong variables‚Äù (or wrong coordinate system) and if only we looked at the right one, we‚Äôd immediately see that ChatGPT is doing something ‚Äúmathematical-physics-simple‚Äù like following geodesics. But as of now, we‚Äôre not ready to ‚Äúempirically decode‚Äù from its ‚Äúinternal behavior‚Äù what ChatGPT has ‚Äúdiscovered‚Äù about how human language is ‚Äúput together‚Äù. </p>
<h2 id="semantic-grammar-and-the-power-of-computational-language">Semantic Grammar and the Power of Computational Language</h2>
<p>What does it take to produce ‚Äúmeaningful human language‚Äù? In the past, we might have assumed it could be nothing short of a human brain. But now we know it can be done quite respectably by the neural net of ChatGPT. Still, maybe that‚Äôs as far as we can go, and there‚Äôll be nothing simpler‚Äîor more human understandable‚Äîthat will work. But my strong suspicion is that the success of ChatGPT implicitly reveals an important ‚Äúscientific‚Äù fact: that there‚Äôs actually a lot more structure and simplicity to meaningful human language than we ever knew‚Äîand that in the end there may be even fairly simple rules that describe how such language can be put together.</p>
<p>As we mentioned above, syntactic grammar gives rules for how words corresponding to things like different parts of speech can be put together in human language. But to deal with meaning, we need to go further. And one version of how to do this is to think about not just a syntactic grammar for language, but also a semantic one. </p>
<p>For purposes of syntax, we identify things like nouns and verbs. But for purposes of semantics, we need ‚Äúfiner gradations‚Äù. So, for example, we might identify the concept of ‚Äúmoving‚Äù, and the concept of an ‚Äúobject‚Äù that ‚Äúmaintains its identity independent of location‚Äù. There are endless specific examples of each of these ‚Äúsemantic concepts‚Äù. But for the purposes of our semantic grammar, we‚Äôll just have some general kind of rule that basically says that ‚Äúobjects‚Äù can ‚Äúmove‚Äù. There‚Äôs a lot to say about how all this might work (<a href="https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/">some of which I‚Äôve said before</a>). But I‚Äôll content myself here with just a few remarks that indicate some of the potential path forward.</p>
<p>It‚Äôs worth mentioning that even if a sentence is perfectly OK according to the semantic grammar, that doesn‚Äôt mean it‚Äôs been realized (or even could be realized) in practice. ‚ÄúThe elephant traveled to the Moon‚Äù would doubtless ‚Äúpass‚Äù our semantic grammar, but it certainly hasn‚Äôt been realized (at least yet) in our actual world‚Äîthough it‚Äôs absolutely fair game for a fictional world.</p>
<p>When we start talking about ‚Äúsemantic grammar‚Äù we‚Äôre soon led to ask ‚ÄúWhat‚Äôs underneath it?‚Äù What ‚Äúmodel of the world‚Äù is it assuming? A syntactic grammar is really just about the construction of language from words. But a semantic grammar necessarily engages with some kind of ‚Äúmodel of the world‚Äù‚Äîsomething that serves as a ‚Äúskeleton‚Äù on top of which language made from actual words can be layered.</p>
<p>Until recent times, we might have imagined that (human) language would be the only general way to describe our ‚Äúmodel of the world‚Äù. Already a few centuries ago there started to be formalizations of specific kinds of things, based particularly on mathematics. But now there‚Äôs a much more general approach to formalization: <a href="https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/">computational language</a>.</p>
<p>And, yes, that‚Äôs been my big project over the course of more than four decades (as now embodied in the <a href="https://www.wolfram.com/language/">Wolfram Language</a>): to develop a precise symbolic representation that can talk as broadly as possible about things in the world, as well as abstract things that we care about. And so, for example, we have symbolic representations for <a href="https://reference.wolfram.com/language/ref/entity/City.html">cities</a> and <a href="https://reference.wolfram.com/language/guide/MolecularStructureAndComputation.html">molecules</a> and <a href="https://reference.wolfram.com/language/guide/ImageRepresentation.html">images</a> and <a href="https://reference.wolfram.com/language/guide/NeuralNetworkConstruction.html">neural networks</a>, and we have built-in knowledge about how to compute about those things. </p>
<p>And, after decades of work, we‚Äôve covered a lot of areas in this way. But in the past, we haven‚Äôt particularly dealt with ‚Äú<a href="https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/">everyday discourse</a>‚Äù. In ‚ÄúI bought two pounds of apples‚Äù we can <a href="https://reference.wolfram.com/language/ref/entity/Food.html">readily represent</a> (and do nutrition and other computations on) the ‚Äútwo pounds of apples‚Äù. But we don‚Äôt (quite yet) have a symbolic representation for ‚ÄúI bought‚Äù.</p>
<p>It‚Äôs all connected to the idea of semantic grammar‚Äîand the goal of having a generic symbolic ‚Äúconstruction kit‚Äù for concepts, that would give us rules for what could fit together with what, and thus for the ‚Äúflow‚Äù of what we might turn into human language. </p>
<p>But let‚Äôs say we had this ‚Äúsymbolic discourse language‚Äù. What would we do with it? We could start off doing things like generating ‚Äúlocally meaningful text‚Äù. But ultimately we‚Äôre likely to want more ‚Äúglobally meaningful‚Äù results‚Äîwhich means ‚Äúcomputing‚Äù more about what can actually exist or happen in the world (or perhaps in some consistent fictional world). </p>
<p>Right now in Wolfram Language we have a huge amount of built-in computational knowledge about lots of kinds of things. But for a complete symbolic discourse language we‚Äôd have to build in additional ‚Äúcalculi‚Äù about general things in the world: if an object moves from A to B and from B to C, then it‚Äôs moved from A to C, etc. </p>
<p>Given a symbolic discourse language we might use it to make ‚Äústandalone statements‚Äù. But we can also use it to ask questions about the world, ‚ÄúWolfram|Alpha style‚Äù. Or we can use it to state things that we ‚Äúwant to make so‚Äù, presumably with some external actuation mechanism. Or we can use it to make assertions‚Äîperhaps about the actual world, or perhaps about some specific world we‚Äôre considering, fictional or otherwise. </p>
<p>Human language is fundamentally imprecise, not least because it isn‚Äôt ‚Äútethered‚Äù to a specific computational implementation, and its meaning is basically defined just by a ‚Äúsocial contract‚Äù between its users. But computational language, by its nature, has a certain fundamental precision‚Äîbecause in the end what it specifies can always be ‚Äúunambiguously executed on a computer‚Äù. Human language can usually get away with a certain vagueness. (When we say ‚Äúplanet‚Äù does it include exoplanets or not, etc.?) But in computational language we have to be precise and clear about all the distinctions we‚Äôre making.</p>
<p>It‚Äôs often convenient to leverage ordinary human language in making up names in computational language. But the meanings they have in computational language are necessarily precise‚Äîand might or might not cover some particular connotation in typical human language usage.</p>
<p>How should one figure out the fundamental ‚Äúontology‚Äù suitable for a general symbolic discourse language? Well, it‚Äôs not easy. Which is perhaps why little has been done since the primitive beginnings Aristotle made more than two millennia ago. But it really helps that today we now know so much about how to think about the world computationally (and it doesn‚Äôt hurt to have a ‚Äúfundamental metaphysics‚Äù from our <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">Physics Project</a> and the <a href="https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/">idea of the ruliad</a>).</p>
<p>But what does all this mean in the context of ChatGPT? From its training ChatGPT has effectively ‚Äúpieced together‚Äù a certain (rather impressive) quantity of what amounts to semantic grammar. But its very success gives us a reason to think that it‚Äôs going to be feasible to construct something more complete in computational language form. And, unlike what we‚Äôve so far figured out about the innards of ChatGPT, we can expect to design the computational language so that it‚Äôs readily understandable to humans.</p>
<p>When we talk about semantic grammar, we can draw an analogy to syllogistic logic. At first, syllogistic logic was essentially a collection of rules about statements expressed in human language. But (yes, two millennia later) <a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#the-history">when formal logic was developed</a>, the original basic constructs of syllogistic logic could now be used to build huge ‚Äúformal towers‚Äù that include, for example, the operation of modern digital circuitry. And so, we can expect, it will be with more general semantic grammar. At first, it may just be able to deal with simple patterns, expressed, say, as text. But once its whole computational language framework is built, we can expect that it will be able to be used to erect tall towers of ‚Äúgeneralized semantic logic‚Äù, that allow us to work in a precise and formal way with all sorts of things that have never been accessible to us before, except just at a ‚Äúground-floor level‚Äù through human language, with all its vagueness.</p>
<p>We can think of the construction of computational language‚Äîand semantic grammar‚Äîas representing a kind of ultimate compression in representing things. Because it allows us to talk about the essence of what‚Äôs possible, without, for example, dealing with all the ‚Äúturns of phrase‚Äù that exist in ordinary human language. And we can view the great strength of ChatGPT as being something a bit similar: because it too has in a sense ‚Äúdrilled through‚Äù to the point where it can ‚Äúput language together in a semantically meaningful way‚Äù without concern for different possible turns of phrase.</p>
<p>So what would happen if we applied ChatGPT to underlying computational language? The computational language can describe what‚Äôs possible. But what can still be added is a sense of ‚Äúwhat‚Äôs popular‚Äù‚Äîbased for example on reading all that content on the web. But then‚Äîunderneath‚Äîoperating with computational language means that something like ChatGPT has immediate and fundamental access to what amount to ultimate tools for making use of potentially irreducible computations. And that makes it a system that can not only ‚Äúgenerate reasonable text‚Äù, but can expect to work out whatever can be worked out about whether that text actually makes ‚Äúcorrect‚Äù statements about the world‚Äîor whatever it‚Äôs supposed to be talking about. </p>
<h2 id="so-what-is-chatgpt-doing-and-why-does-it-work">So ‚Ä¶ What Is ChatGPT Doing, and Why Does It Work?</h2>
<p>The basic concept of ChatGPT is at some level rather simple. Start from a huge sample of human-created text from the web, books, etc. Then train a neural net to generate text that‚Äôs ‚Äúlike this‚Äù. And in particular, make it able to start from a ‚Äúprompt‚Äù and then continue with text that‚Äôs ‚Äúlike what it‚Äôs been trained with‚Äù.</p>
<p>As we‚Äôve seen, the actual neural net in ChatGPT is made up of very simple elements‚Äîthough billions of them. And the basic operation of the neural net is also very simple, consisting essentially of passing input derived from the text it‚Äôs generated so far ‚Äúonce through its elements‚Äù (without any loops, etc.) for every new word (or part of a word) that it generates.</p>
<p>But the remarkable‚Äîand unexpected‚Äîthing is that this process can produce text that‚Äôs successfully ‚Äúlike‚Äù what‚Äôs out there on the web, in books, etc. And not only is it coherent human language, it also ‚Äúsays things‚Äù that ‚Äúfollow its prompt‚Äù making use of content it‚Äôs ‚Äúread‚Äù. It doesn‚Äôt always say things that ‚Äúglobally make sense‚Äù (or correspond to correct <nobr>computations)‚Äî</nobr>because (without, for example, <a href="https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/">accessing the ‚Äúcomputational superpowers‚Äù of Wolfram|Alpha</a>) it‚Äôs just saying things that ‚Äúsound right‚Äù based on what things ‚Äúsounded like‚Äù in its training material. </p>
<p>The specific engineering of ChatGPT has made it quite compelling. But ultimately (at least until it can use outside tools) ChatGPT is ‚Äúmerely‚Äù pulling out some ‚Äúcoherent thread of text‚Äù from the ‚Äústatistics of conventional wisdom‚Äù that it‚Äôs accumulated. But it‚Äôs amazing how human-like the results are. And as I‚Äôve discussed, this suggests something that‚Äôs at least scientifically very important: that human language (and the patterns of thinking behind it) are somehow simpler and more ‚Äúlaw like‚Äù in their structure than we thought. ChatGPT has implicitly discovered it. But we can potentially explicitly expose it, with semantic grammar, computational language, etc.</p>
<p>What ChatGPT does in generating text is very impressive‚Äîand the results are usually very much like what we humans would produce. So does this mean ChatGPT is working like a brain? Its underlying artificial-neural-net structure was ultimately modeled on an idealization of the brain. And it seems quite likely that when we humans generate language many aspects of what‚Äôs going on are quite similar.</p>
<p>When it comes to training (AKA learning) the different ‚Äúhardware‚Äù of the brain and of current computers (as well as, perhaps, some undeveloped algorithmic ideas) forces ChatGPT to use a strategy that‚Äôs probably rather different (and in some ways much less efficient) than the brain. And there‚Äôs something else as well: unlike even in typical algorithmic computation, ChatGPT doesn‚Äôt internally ‚Äúhave loops‚Äù or ‚Äúrecompute on data‚Äù. And that inevitably limits its computational capability‚Äîeven with respect to current computers, but definitely with respect to the brain.</p>
<p>It‚Äôs not clear how to ‚Äúfix that‚Äù and still maintain the ability to train the system with reasonable efficiency. But to do so will presumably allow a future ChatGPT to do even more ‚Äúbrain-like things‚Äù. Of course, there are plenty of things that brains don‚Äôt do so well<span>‚Äî</span>particularly involving what amount to irreducible computations. And for these both brains and things like ChatGPT have to seek ‚Äúoutside tools‚Äù‚Äîlike <a href="https://www.wolfram.com/language/">Wolfram Language</a>.</p>
<p>But for now it‚Äôs exciting to see what ChatGPT has already been able to do. At some level it‚Äôs a great example of the fundamental scientific fact that large numbers of simple computational elements can do remarkable and unexpected things. But it also provides perhaps the best impetus we‚Äôve had in two thousand years to understand better just what the fundamental character and principles might be of that central feature of the human condition that is human language and the processes of thinking behind it.</p>
<h2 id="thanks">Thanks</h2>
<p>I‚Äôve been following the development of neural nets now for about 43 years, and during that time I‚Äôve interacted with many people about them. Among them‚Äîsome from long ago, some from recently, and some across many years‚Äîhave been: Giulio Alessandrini, Dario Amodei, Etienne Bernard, Taliesin Beynon, Sebastian Bodenstein, Greg Brockman, Jack Cowan, Pedro Domingos, Jesse Galef, Roger Germundsson, Robert Hecht-Nielsen, Geoff Hinton, John Hopfield, Yann LeCun, Jerry Lettvin, Jerome Louradour, Marvin Minsky, Eric Mjolsness, Cayden Pierce, Tomaso Poggio, Matteo Salvarezza, Terry Sejnowski, Oliver Selfridge, Gordon Shaw, Jonas Sj√∂berg, Ilya Sutskever, Gerry Tesauro and Timothee Verdier.  For help with this piece, I‚Äôd particularly like to thank Giulio Alessandrini and Brad Klee.</p>
<h2 id="additional-resources">Additional Resources</h2>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Framework: A new RISC-V Mainboard from DeepComputing (494 pts)]]></title>
            <link>https://frame.work/blog/introducing-a-new-risc-v-mainboard-from-deepcomputing</link>
            <guid>40718124</guid>
            <pubDate>Tue, 18 Jun 2024 14:15:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frame.work/blog/introducing-a-new-risc-v-mainboard-from-deepcomputing">https://frame.work/blog/introducing-a-new-risc-v-mainboard-from-deepcomputing</a>, See on <a href="https://news.ycombinator.com/item?id=40718124">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>We‚Äôre excited to share a preview of a <a href="https://frame.work/products/deep-computing-risc-v-mainboard">Framework Laptop 13 Mainboard with a new CPU architecture</a> today, and it‚Äôs probably not the one you think it is. The team at <a href="https://deepcomputing.io/" target="_blank" rel="noopener">DeepComputing</a> has built the first ever partner-developed Mainboard, and it uses a RISC-V processor! This is a huge milestone both for expanding the breadth of the Framework ecosystem and for making RISC-V more accessible than ever. We designed the Framework Laptop to enable deep flexibility and personalization, and now that extends all the way to processor architecture selection. DeepComputing is demoing an early prototype of this Mainboard in a Framework Laptop 13 at the RISC-V Summit Europe next week, and we‚Äôll be sharing more as this program progresses.</p>
<p>There is excellent philosophical alignment between RISC-V and Framework. Both are built on the idea that an open ecosystem is more powerful than the sum of its parts. To explain why, first we‚Äôll go into what RISC-V even is. <a href="https://riscv.org/" target="_blank" rel="noopener">RISC-V</a> is a fully open Instruction Set Architecture (ISA), which is the interface point between software and hardware. It‚Äôs a defined set of instructions that software is compiled and assembled into that the processor executes to run the actual program. x86 (or the latest version, x86-64) is the most common ISA for PCs today, and it‚Äôs what is used in the processors for each Framework Laptop we‚Äôve shipped to date. The x86 ISA was invented by Intel, extended on by AMD, and is proprietary, with Intel and AMD being effectively the only two companies able to use and create processors around it. ARM is another popular ISA, owned by Arm Holdings. Arm licenses the ARM architecture out, which enables companies to pay a license fee for cores to make their own processors that leverage it. What makes RISC-V unique is that it is an entirely open architecture, which means that anyone can extend on it and create their own processors that use it without paying a fee. RISC-V International is the collaborative organization that exists to help develop the standard and define common versions to ensure cross-compatibility of hardware and software. There are hundreds of companies now developing cores and chips around RISC-V, but most of these have been hidden away in embedded applications. The DeepComputing RISC-V Mainboard is one of the first instances of leveraging this ecosystem for the main processor in a consumer-facing product.</p>
<p>All of this is what makes RISC-V unique from an ecosystem enablement perspective. The actual technology is equally interesting. The base instruction set of RISC-V is simple and streamlined, while there are a number of extensions enabling high performance and specialized compute. This means that RISC-V cores can be developed for anything from tiny control CPUs embedded inside a sensor (the Fingerprint Reader we‚Äôve used in Framework Laptops since 2021 actually has a RISC-V core!) to monstrous multi-hundred-core server processors. The DeepComputing RISC-V Mainboard uses a <a href="https://doc-en.rvspace.org/JH7110/PDF/JH7110_Product_Brief.pdf" target="_blank" rel="noopener">JH7110</a> processor from StarFive which has four <a href="https://starfivetech.com/uploads/u74mc_core_complex_manual_21G1.pdf" target="_blank" rel="noopener">U74 RISC-V cores</a> from SiFive. SiFive is the company that developed CPU cores using the RISC-V ISA, StarFive is the processor designer that integrated those CPU cores with other peripherals, DeepComputing created a Mainboard leveraging that processor, and Framework makes laptops that can use the Mainboard. The power of an open ecosystem!</p>
<p>This Mainboard is extremely compelling, but we want to be clear that in this generation, it is focused primarily on enabling developers, tinkerers, and hobbyists to start testing and creating on RISC-V. The peripheral set and performance aren‚Äôt yet competitive with our Intel and AMD-powered Framework Laptop Mainboards. This board also has soldered memory and uses MicroSD cards and eMMC for storage, both of which are limitations of the processor. It is a great way to start playing with RISC-V though inside of a thin, light, refined laptop. The Mainboard will be able to drop into any Framework Laptop 13 chassis or into the Cooler Master Mainboard Case. DeepComputing is also working closely with the teams at Canonical and Red Hat to ensure Linux support is solid through Ubuntu and Fedora. We‚Äôll continue to keep you up to date as we work with the team at DeepComputing to complete development of this new Mainboard and enable access to it. You can <a href="https://frame.work/products/deep-computing-risc-v-mainboard">sign up in the Framework Marketplace</a> to get notified when we have updates.</p>
<p>We have a couple of other updates around scaling access to Framework Laptop 13. The first is that just like we did for Framework Laptop 16 last week, today we‚Äôre sharing <a href="https://github.com/FrameworkComputer/Framework-Laptop-13" target="_blank" rel="noopener">open source CAD for the Framework Laptop 13 shell</a>, enabling development of skins, cases, and accessories. The second is that we now have <a href="https://frame.work/products/factory-seconds-framework-laptop-13-diy-edition-11th-gen-intel-core">Framework Laptop 13 Factory Seconds</a> systems available with British English and German keyboards, making entering the ecosystem more affordable than ever. We‚Äôre eager to continue growing a new Consumer Electronics industry that is grounded in open access, repairability, and customization at every level.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tiny beauty: how I make scientific art from behind the microscope (102 pts)]]></title>
            <link>https://www.nature.com/immersive/d41586-024-02011-6/index.html</link>
            <guid>40717804</guid>
            <pubDate>Tue, 18 Jun 2024 13:45:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/immersive/d41586-024-02011-6/index.html">https://www.nature.com/immersive/d41586-024-02011-6/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=40717804">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article" role="main">
      <header id="section-UcOjt2desJ" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsInRleHRGYWRlIjoibm9uZSIsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
        <div data-scrollymoly-basis="true" data-testid="ENG-5457">
                  
                  <p>Steve Gschmeissner images tiny creatures and viruses to show the public an unseen world</p>
                </div>
        
      </header>
      <div data-scrollymoly-basis="true" data-testid="ENG-5457" id="section-miV6NMlR8W" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
                  
                  <p>Cheese fungus, head lice, human sperm, a bee eye, a microplastic bobble: scientific photographer Steve Gschmeissner has imaged them all under the probing lens of a scanning electron microscope (SEM). In his colourized electron micrographs, faecal bacteria resemble thin spaghetti, silica-walled diatoms look like cubes of breakfast cereal and a segmented tardigrade resembles a curled-up, tubby piglet.&nbsp;</p>
                  <p>Gschmeissner, who has been imaging microbes, cancer cells and invertebrates for about 50 years, has crafted an extraordinary array of more than 10,000 SEM images, some of which have been featured in <em>Nature</em>.</p>
                  <p>He spoke to <em>Nature</em> about the importance of scientific images, looking at imploding cancer cells and the miniature world he found on a rotten raspberry.</p>
                  
                </div>
      <div data-mediarenderer="true" id="section-qRfTKsZ9MV" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
          <sh-background-transition data-lazyload-container="true" data-lazyload-trigger="true" data-transitions="">
            
            <div slot="foreground">
                          <p>This image of a rind of Camembert cheese shows <em>Penicillium</em> <em>camemberti</em> fungus (green) and bacteria (blue). The mix of bacteria and fungi contributes to the cheese's final flavour.</p>
                          
                        </div>
            
          </sh-background-transition>
        </div>
      <div id="section-oHxzPzee8e" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
                    <p><strong>How did you begin creating your collection of electron-microscope shots?</strong></p>
                    <p>My undergraduate degree is in zoology, and I first started doing electron microscopy in the department of anatomy at the Royal College of Surgeons of England in London. I then moved to Cancer Research UK (CRUK), also in London, where I was head of electron-microscopy services until 2006. When I was 57, I met Rose Taylor, the creative director at the Science Photo Library in London, and she helped me to realize that there is commercial demand for photographic imagery. For a few years, I continued to work at CRUK until I felt confident enough to pursue commercial image production, and then started doing that as a part-time business.</p>
                  </div>
      
      <div id="section-k2niz5mQgZ" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
              <div data-card-canvas-item="true" data-card-canvas="true">
                    <div data-lazyload-item="true" data-lazyload-container="true">
                        <picture>
                          <source data-srcset="./assets/7ITCEAMiq4/005_covid-750-750x616.webp 750w" type="image/webp" media="(min-width: 900px)" sizes="100vw">
                          <source data-srcset="./assets/peKORQ1Jy7/005_covid-750-422-750x422.webp 750w" type="image/webp" media="(max-width: 900px)" sizes="100vw">
                          <source data-srcset="./assets/7ITCEAMiq4/005_covid-750-750x616.jpg 750w" type="image/jpeg" media="(min-width: 900px)" sizes="100vw">
                          <source data-srcset="./assets/peKORQ1Jy7/005_covid-750-422-750x422.jpg 750w" type="image/jpeg" media="(max-width: 900px)" sizes="100vw"><img data-src="./assets/7ITCEAMiq4/005_covid-750-750x616.jpg" alt="Coloured scanning electron micrograph of the SARS-CoV-2 Delta variant (red dots) budding from a Caco-2 human gut epithelial cell." src="https://www.nature.com/immersive/d41586-024-02011-6/assets/7ITCEAMiq4/005_covid-750-750x616.jpg">
                        </picture>
                      </div>
                    <div>
                      <p>Pictured: The Delta variant of SARS-CoV-2 (red) buds from a human gut epithelial cell (blue).</p>
                      <p>Pictured: The Delta variant of SARS-CoV-2 (red) buds from a human gut epithelial cell (blue).</p>
                    </div>
                  </div>
              <div>
                    
                    <p><strong>What are some of the projects you‚Äôve worked on recently?</strong></p>
                    <p>For the past six years, I‚Äôve been collaborating with Greg Towers, a molecular virologist at University College London, who supplies me with samples to photograph. We‚Äôve looked at a variety of viruses, including SARS-CoV-2, which causes COVID-19.</p>
                    <p>The latest work I‚Äôve done with Towers is a <a href="https://stories.sciencephoto.com/portfolio/cancer-cell-death-with-steve-gschmeissner/" target="_blank">project on cancer-cell death</a>. It‚Äôs the sort of work I love doing: science that tells a story with images. It‚Äôs been one of my most enjoyable and successful recent projects, because there‚Äôs very little else out there that shows what happens to cancer cells during chemotherapy.</p>
                  </div>
            </div>
      
      <div id="section-or6JMn35cZ" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
                    <p><strong>What kinds of story are you telling with these images of cancer-cell death?</strong></p>
                    <p>We show that chemotherapy isn‚Äôt a simple process, and it causes cell death in several ways. Basically, you want a chemotherapy drug ‚Äî in this case, doxorubicin ‚Äî to cause programmed cell death, called apoptosis, in which the cell implodes. This is ideal from a medical perspective, because the nucleus shrinks and fragments, minimizing damage to surrounding tissues. But chemotherapy can also cause necrosis, an uncontrolled destructive process, which is a less favourable outcome. Necrosis causes inflammation and other cell damage. Our SEM images show both apoptosis and necrosis.</p>
                  </div>
      <div data-scrollymoly-basis="true" data-testid="ENG-5457" id="section-qGJpZfs6zu" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsInRleHRGYWRlIjoibm9uZSIsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
                  <p><strong>How important is it in the fields of science and medicine to see human cells and bacteria in fine detail?</strong></p>
                  <p>It‚Äôs absolutely essential. One of my most important roles is to help make science more accessible to the general public. In general, scientists often don‚Äôt appreciate how powerful images can be. But we live in a visual world, and many people can relate to images. What surprises me is that there is massive demand for high-resolution images. Not only are they used during research, but they‚Äôre almost like an art form. A lot of my images are sold as stand-alone art. My work has inspired fashion designers and even been used by the artist Damien Hirst.</p>
                </div>
      
      <div id="section-lFSf5wuQxe" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
                    <p><strong>What sorts of reaction do you get from the public?</strong></p>
                    <p>Children love anything that‚Äôs a bit gross or a bit scary. People say to me, ‚ÄúYou‚Äôre looking at horrible things ‚Äî cancer cells, viruses, bacteria ‚Äî but you make them look attractive and beautiful. Is that a contradiction?‚Äù It sounds odd, but a cancer cell can produce a beautiful image. People who have cancer have said to me, ‚ÄúSeeing the image helps me relate to my cancer.‚Äù Or, ‚ÄúI know what I‚Äôm up against.‚Äù</p>
                  </div>
      <div data-mediarenderer="true" id="section-5lnY1mBzSH" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
          <sh-background-transition data-lazyload-container="true" data-lazyload-trigger="true" data-transitions="2 fade">
            
            
            <div slot="foreground">
              <div>
                            <p><strong>Can you tell me about the process of creating these images?</strong></p>
                            <p>When you use an electron microscope, you‚Äôre looking at the specimen in a vacuum. It has to be prepared so that it can survive in that vacuum, under bombardment by electrons. You preserve it and dehydrate and carefully dry it so that it doesn‚Äôt become distorted. Then, you coat it in a very fine layer of gold or another metal, to reflect the electrons from its surface. Only black-and-white images are formed in this way, so you have to colour them.</p>
                          </div>
              <div>
                            <p>All my colouring is done in Photoshop. The amount of time it takes to colour an image depends on its complexity; it can range from half an hour to several hours, and often takes several sessions.</p>
                            <p>Pictured: The SARS-CoV-2 Delta variant (blue dots) buds from a human gut epithelial cell that is displaying protrusions (pink) of its plasma membrane.</p>
                          </div>
            </div>
            
          </sh-background-transition>
        </div>
      <div data-scrollymoly-basis="true" data-testid="ENG-5457" id="section-1KanHK0kGr" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsInRleHRGYWRlIjoibm9uZSIsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
                  <p>For a few years, I‚Äôve worked at the School of Pharmacy at University College London. I pay to rent time on its SEM. Costs vary from institute to institute, but rental fees of ¬£100‚Äì200 (US$125‚Äì250) for an hour in front of the microscope are not unusual.</p>
                  <p>From start to finish, the process of creating an image that I‚Äôm proud of can take several days; cells might need to be cultured, preserved, dried, coated, photographed, coloured and captioned. It took me a few years to hone my colouring skill, and the colouring process is constantly improving as my technique and the software improve. &nbsp;</p>
                </div>
      <div data-scrollymoly-basis="true" data-testid="ENG-5457" id="section-e4dvpODHVF" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsInRleHRGYWRlIjoibm9uZSIsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
                  <p><strong>What‚Äôs your favourite thing you‚Äôve photographed using the SEM?</strong></p>
                  <p>I think viruses, because you can‚Äôt visualize them in any other way. But I will look at anything. I love looking at insects because of their complex anatomical details. I don‚Äôt know what I‚Äôm going to see under the microscope, so I often find things that are unexpected or visually interesting.</p>
                  <p>For example, some raspberries that I was growing in a garden plot last year were infected by fungus. I thought, ‚ÄúThat‚Äôd make a nice image.‚Äù When I got a sample into the microscope, I found a couple of little creatures grazing on the fungus on the leaf. Not only did I image the fungus, but I imaged a mite and a hoverfly larva that was eating the fungus.</p>
                  <p>In the past ten years, people have realized how crucial our biomes are. We have millions and millions of symbiotic bacteria. We would die without them ‚Äî they play an essential part in food digestion, and help to train the immune system. The microscopic world, the bacterial world, the fungal world: the world wouldn‚Äôt exist as we know it without them.</p>
                </div>
      
      <div id="section-KRDkqdZqIC" data-effects="eyJ0ZXh0UGVyTGluZSI6ZmFsc2UsImhhc0VmZmVjdHMiOmZhbHNlLCJuZWVkc0JhY2tncm91bmRDbG9uZSI6ZmFsc2UsImVuY29kZWQiOiIifQ==">
                    <p>This interview has been edited for length and clarity.</p>
                    
                    <div>
                      <p><span>Springer Nature</span></p>

                      <p>

                      <img src="https://www.nature.com/e25cxdya/article/d41586-024-02011-6" width="1" height="1" alt="">
                    </p></div>
                  </div>
      
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sculpting the moon in R: Subdivision surfaces and displacement mapping (129 pts)]]></title>
            <link>https://www.tylermw.com/posts/rayverse/displacement-mapping.html</link>
            <guid>40717519</guid>
            <pubDate>Tue, 18 Jun 2024 13:19:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tylermw.com/posts/rayverse/displacement-mapping.html">https://www.tylermw.com/posts/rayverse/displacement-mapping.html</a>, See on <a href="https://news.ycombinator.com/item?id=40717519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">






<main id="quarto-document-content">

<p><img src="https://www.tylermw.com/posts/images/2024/moon_preview.jpg"></p>


<blockquote>
<p>We like the moon<br>
The moon is very useful, everyone<br>
Everybody like the moon<br>
Because it light up the sky at night<br>
and it lovely and it makes the tides go<br>
and we like it<br>
But not as much as cheese<br>
We really like cheese</p>
</blockquote>
<p>‚Äî The Spongmonkies, 2002</p>
<section id="introduction">
<h2 data-anchor-id="introduction">Introduction</h2>
<div id="callout-1">
<p>In this post, we explore subdivision surfaces and displacement mapping within the rayverse, using the <code>rayvertex</code>, <code>rayimage</code>, and <code>rayrender</code> packages. We demonstrate how to create detailed and smooth 3D models by subdividing meshes and applying displacement textures. These techniques enhance both artistic and data visualization projects by providing realistic and intricate surface details.</p>
</div>
<div><p>Bumpy objects and smooth objects. These are the two demons you must slay if you want to be successful in 3D rendering. </p><p><span>Just kidding! There are thousands of demons you must slay. The princess is always in another castle.</span></p></div>
<div><p>Triangles, the primary building blocks of computer graphics, don‚Äôt directly lend themselves to either extremely smooth or realistically bumpy objects. Since triangles are flat primitives, borders between non-coplanar adjacent triangles will always have sharp edges. You can introduce approximations to work around this (such as per-vertex normals), but approximations always have failure modes, particularly with certain rendering algorithms. Bumpy objects suffer from the same issue: approximating a rough surface using a bump map or a normal map (which change the underlying surface normal without actually changing the geometry) can lead to non-physically accurate results.</p><p><span>There‚Äôs also NURBS, quadratic surfaces, curves, and more exotic objects</span></p></div>
<div><p>Historically, renderers worked around this by subdividing meshes into ‚Äúmicropolygons‚Äù: polygons smaller than a single pixel. This means triangle edges were not visible, as they only existed a on sub-pixel scale. This subdivision enabled the rendering of smooth objects and the rendered image no longer had visible discontinuities in the surface normal due to geometry. This subdivision process allowed for bumpy surfaces as well: by displacing the vertices of these micropolygons, you could easily generate highly detailed surfaces given a basic low-resolution mesh and a displacement texture. It can be significantly easier to work with displacement information in 2D form and use it to displace a low-resolution 3D mesh, rather than try to construct a fully-detailed 3D mesh directly. </p><p><span>The Reyes rendering algorithm‚Äìwhich brought us Toy Story and Star Trek II: Wrath of Khan‚Äìactually works off of quadrilaterals, not triangles, but the idea is the same</span><span>Rayshader has always performed a basic version of displacement mapping when generating meshes, but it‚Äôs been limited to displacing a flat surface.</span></p></div>
<div><p>This brings us to why I implemented subdivision surfaces and displacement mapping in the rayverse: there‚Äôs plenty of 2D data out there and not all of it lives on a flat, Cartesian plane. If you‚Äôre plotting data that spans the entire planet (or moon!), there‚Äôs always a struggle to pick the right projection, knowing that there‚Äôs no ideal way to transform a sphere to 2D without warping the data in some way. Expressing the data in its native curved space avoids those issues entirely.</p><p><span>While 3D plotting has its own set of issues, it also has many advantages‚Äìsee <a href="https://www.tylermw.com/posts/data_visualization/3d-ggplots-with-rayshader.html">this blog post</a> for more information</span></p></div>
<p>So let‚Äôs dive into subdivision surfaces and displacement mapping! What is it, how it was implemented, things to keep in mind when using it, and what you can do with it in the rayverse.</p>

</section>
<section id="subdivision">

<p>We‚Äôll start by loading three useful rayverse packages: <code>rayvertex</code> to construct <code>raymesh</code> objects and manipulate them directly, <code>rayimage</code> to manipulate displacement textures and load a variety of image files, and <code>rayrender</code> to visualize these meshes in a high-quality pathtracer.</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>library</span>(rayvertex)</span>
<span id="cb1-2"><span>library</span>(rayimage)</span>
<span id="cb1-3"><span>library</span>(rayrender)</span></code></pre></div>
<div><p>Now, let‚Äôs render an image of some shapes with Loop subdivision. We‚Äôll be using <code>rayvertex</code> because it renders faster and has more fine-grained control over the meshing process. Note here the new (as of <code>rayvertex</code> v0.11.0) print output showing a command-line preview of the materials, as well as a preview of the scene information.</p><p><span>Fun fact: Loop subdivision is not named after some cyclical mesh property or iterative process that the name might suggest: it‚Äôs actually named after the graphics researcher Charles Loop.</span></p></div>
<div>
<div id="annotated-cell-2"><pre><code><span id="annotated-cell-2-1">base_material <span>=</span> <span>material_list</span>(<span>diffuse=</span><span>"red"</span>, <span>ambient =</span> <span>"red"</span>,</span>
<span id="annotated-cell-2-2">                              <span>type =</span> <span>"phong"</span>, <span>shininess =</span> <span>5</span>,</span>
<span id="annotated-cell-2-3">                              <span>diffuse_intensity =</span> <span>0.8</span>, <span>ambient_intensity =</span> <span>0.2</span>)</span>
<span id="annotated-cell-2-4">base_material</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-2" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="1,2,3,4" data-code-annotation="1">Create and print the red cube material</span>
</dd>
</dl>
</div>
<pre><code><span>‚Ä¢ rayvertex_material</span>
<span>‚Ä¢</span> <span>type:</span> phong
<span>‚Ä¢</span> <span>diffuse:</span> #ff0000 <span> </span> <span>| intensity:</span> 0.8
<span>‚Ä¢</span> <span>ambient:</span> #ff0000 <span> </span> <span>| intensity:</span> 0.2
<span>‚Ä¢</span> <span>shininess:</span> 5
</code></pre>
</div>
<div>
<div id="annotated-cell-3"><pre><code><span id="annotated-cell-3-1">base_material2 <span>=</span> <span>material_list</span>(<span>diffuse=</span><span>"purple"</span>, <span>ambient =</span> <span>"purple"</span>,</span>
<span id="annotated-cell-3-2">                               <span>type =</span> <span>"phong"</span>, <span>shininess =</span> <span>5</span>,</span>
<span id="annotated-cell-3-3">                               <span>diffuse_intensity =</span> <span>0.8</span>, <span>ambient_intensity =</span> <span>0.2</span>)</span>
<span id="annotated-cell-3-4">base_material2</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-3" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="1,2,3,4" data-code-annotation="1">Create and print the purple sphere material</span>
</dd>
</dl>
</div>
<pre><code><span>‚Ä¢ rayvertex_material</span>
<span>‚Ä¢</span> <span>type:</span> phong
<span>‚Ä¢</span> <span>diffuse:</span> #a020f0 <span> </span> <span>| intensity:</span> 0.8
<span>‚Ä¢</span> <span>ambient:</span> #a020f0 <span> </span> <span>| intensity:</span> 0.2
<span>‚Ä¢</span> <span>shininess:</span> 5
</code></pre>
</div>
<p>Note how the materials print a preview of the actual colors and relevant (i.e.&nbsp;non-default) material settings. This is due to <code>rayvertex</code>‚Äôs new integration with the <code>cli</code> package and a whole collection of new pretty-print functions.</p>
<div>
<div id="annotated-cell-4"><pre><code><span id="annotated-cell-4-1">scene <span>=</span> <span>cube_mesh</span>(<span>material =</span> base_material, <span>scale =</span> <span>0.8</span>) <span>|&gt;</span></span>
<span id="annotated-cell-4-2">  <span>add_shape</span>(<span>sphere_mesh</span>(<span>radius=</span><span>0.5</span>,<span>position =</span> <span>c</span>(<span>-</span><span>1.2</span>,<span>0</span>,<span>0</span>), <span>low_poly =</span> <span>TRUE</span>,</span>
<span id="annotated-cell-4-3">                        <span>normals =</span> <span>TRUE</span>,</span>
<span id="annotated-cell-4-4">                        <span>material =</span> base_material2)) <span>|&gt;</span></span>
<span id="annotated-cell-4-5">  <span>add_shape</span>(<span>obj_mesh</span>(<span>r_obj</span>(), <span>position=</span><span>c</span>(<span>1.2</span>,<span>0</span>,<span>0</span>)))</span>
<span id="annotated-cell-4-6">scene</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-4" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="1,2,3,4,5" data-code-annotation="1">Create the 3D scene</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="6" data-code-annotation="2">Print the scene information</span>
</dd>
</dl>
</div>
<div>
<pre><code>‚îÄ‚îÄ Scene Description ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></pre>
</div>
<pre><code><span>‚Ä¢</span> Summary - <span>Meshes</span>: <span>3</span> | <span>Unique Materials</span>: <span>4</span>
<span>‚Ñπ</span> XYZ Bounds - <span>Min</span>: <span>c(-1.69, -0.51, -0.49)</span> | <span>Max</span>: <span>c(1.70, 0.49, 0.51)</span>
             shapes  vertices texcoords   normals materials
          <span>&lt;ray_shp&gt;</span> <span>&lt;ray_dat&gt;</span> <span>&lt;ray_dat&gt;</span> <span>&lt;ray_dat&gt;</span> <span>&lt;ray_mat&gt;</span>
<span>1</span>   <span>&lt;</span>T:<span>12</span><span>|</span><span>UV</span><span>|</span><span>N</span><span>|</span>M:<span>1</span><span>&gt;</span>     <span>&lt;</span><span>8</span><span>x</span><span>3</span><span>&gt;</span>     <span>&lt;</span><span>4</span><span>x</span><span>2</span><span>&gt;</span>     <span>&lt;</span><span>6</span><span>x</span><span>3</span><span>&gt;</span>   <span>&lt;</span><span>phong</span><span>&gt;</span>
<span>2</span>   <span>&lt;</span>T:<span>48</span><span>|</span><span>UV</span><span>|</span><span>N</span><span>|</span>M:<span>1</span><span>&gt;</span>    <span>&lt;</span><span>26</span><span>x</span><span>3</span><span>&gt;</span>    <span>&lt;</span><span>34</span><span>x</span><span>2</span><span>&gt;</span>    <span>&lt;</span><span>26</span><span>x</span><span>3</span><span>&gt;</span>   <span>&lt;</span><span>phong</span><span>&gt;</span>
<span>3</span> <span>&lt;</span>T:<span>2280</span><span>|</span><span>UV</span><span>|N|</span>M:<span>6</span><span>&gt;</span>  <span>&lt;</span><span>1520</span><span>x</span><span>3</span><span>&gt;</span>  <span>&lt;</span><span>1520</span><span>x</span><span>2</span><span>&gt;</span>    <span>&lt;none&gt;</span>      <span>&lt;</span><span>6x</span><span>&gt;</span>
</code></pre>
</div>
<p>The <code>rayvertex</code> scene information now also includes a dense, readable tabular summary of each individual mesh that makes up the scene. Rather than looking at a verbose print-out of lists of vertex data and material information, now you can get a sense of the scene at a glance.</p>
<div>
<div id="annotated-cell-5"><pre><code><span id="annotated-cell-5-1"><span>rasterize_scene</span>(scene, <span>lookfrom =</span> <span>c</span>(<span>-</span><span>4</span>,<span>3</span>,<span>10</span>), <span>lookat=</span><span>c</span>(<span>-</span><span>0.05</span>,<span>0</span>,<span>0</span>),</span>
<span id="annotated-cell-5-2">                <span>fov=</span><span>9</span>, <span>height =</span> <span>550</span>, <span>width=</span><span>1100</span>,</span>
<span id="annotated-cell-5-3">                <span>light_info =</span> <span>directional_light</span>(<span>c</span>(<span>1</span>,<span>1</span>,<span>2</span>)))</span></code></pre></div>

<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/render_basic_scene-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>We‚Äôve now rendered some basic low-polygon shapes. Low-polygon here means that you can clearly see lighting artifacts from the chunky triangles that make up the mesh: there are discontinuities on the sphere from the vertex normal interpolation, resulting in unphysical ‚Äúlines‚Äù of light that run along the edges of the triangles.</p>
<p>Let‚Äôs subdivide the meshes with the new <code>subdivide_mesh()</code> function and determine how many subdivisions renders any individual triangle too small to see. Each subdivision level increases the number of triangles by a factor of four. We won‚Äôt initially add vertex normals so we can see exactly what‚Äôs going on with the geometry.</p>
<div>
<div id="annotated-cell-6"><pre><code><span id="annotated-cell-6-1">scene <span>|&gt;</span> </span>
<span id="annotated-cell-6-2">  <span>subdivide_mesh</span>(<span>subdivision_levels =</span> <span>2</span>, <span>normals =</span> <span>FALSE</span>) <span>|&gt;</span></span>
<span id="annotated-cell-6-3">  <span>rasterize_scene</span>(<span>lookfrom =</span> <span>c</span>(<span>-</span><span>4</span>,<span>3</span>,<span>10</span>), <span>lookat=</span><span>c</span>(<span>-</span><span>0.05</span>,<span>0</span>,<span>0</span>),</span>
<span id="annotated-cell-6-4">                  <span>fov=</span><span>9</span>, <span>height =</span> <span>550</span>, <span>width=</span><span>1100</span>,</span>
<span id="annotated-cell-6-5">                  <span>light_info =</span> <span>directional_light</span>(<span>c</span>(<span>0.2</span>,<span>1</span>,<span>2</span>)))</span></code></pre></div>

<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide2-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Note how the cube has shrunk considerably and the sharp edges of the letter R have collapsed in on themselves. This phenomenon occurs because subdivision algorithms, like Loop subdivision, work by averaging the positions of vertices to create a smoother surface. Sharp edges consisting of large triangles will be much more affected by this process. To fix this, we can turn off vertex interpolation and first apply a non-interpolated simple subdivision step and then follow it up with regular Loop subdivision step to more accurately maintain the initial shape of the object.</p>
<div>
<div id="annotated-cell-7"><pre><code><span id="annotated-cell-7-1">scene <span>|&gt;</span> </span>
<span id="annotated-cell-7-2">  <span>subdivide_mesh</span>(<span>normals =</span> <span>FALSE</span>, <span>simple =</span> <span>TRUE</span>) <span>|&gt;</span></span>
<span id="annotated-cell-7-3">  <span>subdivide_mesh</span>(<span>subdivision_levels =</span> <span>2</span>, <span>normals =</span> <span>FALSE</span>) <span>|&gt;</span></span>
<span id="annotated-cell-7-4">  <span>rasterize_scene</span>(<span>lookfrom =</span> <span>c</span>(<span>-</span><span>4</span>,<span>3</span>,<span>10</span>), <span>lookat=</span><span>c</span>(<span>-</span><span>0.05</span>,<span>0</span>,<span>0</span>),</span>
<span id="annotated-cell-7-5">                  <span>fov=</span><span>9</span>, <span>height =</span> <span>550</span>, <span>width=</span><span>1100</span>,</span>
<span id="annotated-cell-7-6">                  <span>light_info =</span> <span>directional_light</span>(<span>c</span>(<span>0.2</span>,<span>1</span>,<span>2</span>)))</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-7" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="2" data-code-annotation="1">Subdivide the scene but don‚Äôt smooth the mesh</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="3" data-code-annotation="2">Subdivide the scene normally, but don‚Äôt calculate normals to better show the triangle faces</span>
</dd>
</dl>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide_basic-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Note how the cube is still cube-ish, but with nicely curved edges! The R logo looks nicer here too, as the large corner that makes up the R didn‚Äôt collapse. The only problem is the low-poly sphere is now more like a 20-sided D&amp;D die, which isn‚Äôt great if you‚Äôd like the limit of the subdivision process to result in an identical (but smoothed) version of the original mesh. But it‚Äôs nice to have a workflow for adding rounded bevels to models, which adding a simple subdivision step provides. Let‚Äôs get back to trying to trying to subdivide until we can‚Äôt tell the mesh is made of individual triangles anymore. How about three subdivisions?</p>
<div>
<div id="annotated-cell-8"><pre><code><span id="annotated-cell-8-1">scene <span>|&gt;</span> </span>
<span id="annotated-cell-8-2">  <span>subdivide_mesh</span>(<span>subdivision_levels =</span> <span>3</span>, <span>normals =</span> <span>FALSE</span>) <span>|&gt;</span></span>
<span id="annotated-cell-8-3">  <span>rasterize_scene</span>(<span>lookfrom =</span> <span>c</span>(<span>-</span><span>4</span>,<span>3</span>,<span>10</span>), <span>lookat=</span><span>c</span>(<span>-</span><span>0.05</span>,<span>0</span>,<span>0</span>),</span>
<span id="annotated-cell-8-4">                  <span>fov=</span><span>9</span>, <span>height =</span> <span>550</span>, <span>width=</span><span>1100</span>,</span>
<span id="annotated-cell-8-5">                  <span>light_info =</span> <span>directional_light</span>(<span>c</span>(<span>0.2</span>,<span>1</span>,<span>2</span>)))</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-8" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="2" data-code-annotation="1">Subdivide the objects three times</span>
</dd>
</dl>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide3-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Not yet. Still visible. Let‚Äôs subdivide again.</p>
<div>
<div id="annotated-cell-9"><pre><code><span id="annotated-cell-9-1">scene <span>|&gt;</span> </span>
<span id="annotated-cell-9-2">  <span>subdivide_mesh</span>(<span>subdivision_levels =</span> <span>4</span>, <span>normals =</span> <span>FALSE</span>) <span>|&gt;</span></span>
<span id="annotated-cell-9-3">  <span>rasterize_scene</span>(<span>lookfrom =</span> <span>c</span>(<span>-</span><span>4</span>,<span>3</span>,<span>10</span>), <span>lookat=</span><span>c</span>(<span>-</span><span>0.05</span>,<span>0</span>,<span>0</span>),</span>
<span id="annotated-cell-9-4">                  <span>fov=</span><span>9</span>, <span>height =</span> <span>550</span>, <span>width=</span><span>1100</span>,</span>
<span id="annotated-cell-9-5">                  <span>light_info =</span> <span>directional_light</span>(<span>c</span>(<span>0.2</span>,<span>1</span>,<span>2</span>)))</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-9" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="2" data-code-annotation="1">Subdivide the objects four times</span>
</dd>
</dl>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide4-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Almost! But no cigar.</p>
<div>
<div id="annotated-cell-10"><pre><code><span id="annotated-cell-10-1">scene <span>|&gt;</span></span>
<span id="annotated-cell-10-2">  <span>subdivide_mesh</span>(<span>subdivision_levels =</span> <span>5</span>, <span>normals =</span> <span>FALSE</span>) <span>-&gt;</span></span>
<span id="annotated-cell-10-3">scene_5x</span>
<span id="annotated-cell-10-4"></span>
<span id="annotated-cell-10-5"><span>rasterize_scene</span>(scene_5x, <span>lookfrom =</span> <span>c</span>(<span>-</span><span>4</span>,<span>3</span>,<span>10</span>), <span>lookat=</span><span>c</span>(<span>-</span><span>0.05</span>,<span>0</span>,<span>0</span>),</span>
<span id="annotated-cell-10-6">                <span>fov=</span><span>9</span>, <span>height =</span> <span>550</span>, <span>width=</span><span>1100</span>,</span>
<span id="annotated-cell-10-7">                <span>light_info =</span> <span>directional_light</span>(<span>c</span>(<span>0.2</span>,<span>1</span>,<span>2</span>)))</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-10" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="1,2,3" data-code-annotation="1">Render the scene</span>
</dd>
<dt data-target-cell="annotated-cell-10" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="5,6,7" data-code-annotation="2">Subdivide the mesh five times and save it to an object</span>
</dd>
</dl>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide5-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>There we go. We can compare the before and after mesh sizes to see how mary more triangles this required to reach continuity. Note the number of vertices and the <code>T:</code> field in the <code>shapes</code> column indicating the number of triangles.</p>
<div>

<div>
<dl>
<dt data-target-cell="annotated-cell-11" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-11" data-code-lines="1" data-code-annotation="1">Printing the scene information before subdivision</span>
</dd>
</dl>
</div>
<div>
<pre><code>‚îÄ‚îÄ Scene Description ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></pre>
</div>
<pre><code><span>‚Ä¢</span> Summary - <span>Meshes</span>: <span>3</span> | <span>Unique Materials</span>: <span>4</span>
<span>‚Ñπ</span> XYZ Bounds - <span>Min</span>: <span>c(-1.69, -0.51, -0.49)</span> | <span>Max</span>: <span>c(1.70, 0.49, 0.51)</span>
             shapes  vertices texcoords   normals materials
          <span>&lt;ray_shp&gt;</span> <span>&lt;ray_dat&gt;</span> <span>&lt;ray_dat&gt;</span> <span>&lt;ray_dat&gt;</span> <span>&lt;ray_mat&gt;</span>
<span>1</span>   <span>&lt;</span>T:<span>12</span><span>|</span><span>UV</span><span>|</span><span>N</span><span>|</span>M:<span>1</span><span>&gt;</span>     <span>&lt;</span><span>8</span><span>x</span><span>3</span><span>&gt;</span>     <span>&lt;</span><span>4</span><span>x</span><span>2</span><span>&gt;</span>     <span>&lt;</span><span>6</span><span>x</span><span>3</span><span>&gt;</span>   <span>&lt;</span><span>phong</span><span>&gt;</span>
<span>2</span>   <span>&lt;</span>T:<span>48</span><span>|</span><span>UV</span><span>|</span><span>N</span><span>|</span>M:<span>1</span><span>&gt;</span>    <span>&lt;</span><span>26</span><span>x</span><span>3</span><span>&gt;</span>    <span>&lt;</span><span>34</span><span>x</span><span>2</span><span>&gt;</span>    <span>&lt;</span><span>26</span><span>x</span><span>3</span><span>&gt;</span>   <span>&lt;</span><span>phong</span><span>&gt;</span>
<span>3</span> <span>&lt;</span>T:<span>2280</span><span>|</span><span>UV</span><span>|N|</span>M:<span>6</span><span>&gt;</span>  <span>&lt;</span><span>1520</span><span>x</span><span>3</span><span>&gt;</span>  <span>&lt;</span><span>1520</span><span>x</span><span>2</span><span>&gt;</span>    <span>&lt;none&gt;</span>      <span>&lt;</span><span>6x</span><span>&gt;</span>
</code></pre>
</div>
<div>

<div>
<dl>
<dt data-target-cell="annotated-cell-12" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-12" data-code-lines="1" data-code-annotation="1">Printing the scene information after subdivision</span>
</dd>
</dl>
</div>
<div>
<pre><code>‚îÄ‚îÄ Scene Description ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></pre>
</div>
<pre><code><span>‚Ä¢</span> Summary - <span>Meshes</span>: <span>3</span> | <span>Unique Materials</span>: <span>4</span>
<span>‚Ñπ</span> XYZ Bounds - <span>Min</span>: <span>c(-1.63, -0.45, -0.45)</span> | <span>Max</span>: <span>c(1.70, 0.46, 0.45)</span>
                shapes    vertices   texcoords   normals materials
             <span>&lt;ray_shp&gt;</span>   <span>&lt;ray_dat&gt;</span>   <span>&lt;ray_dat&gt;</span> <span>&lt;ray_dat&gt;</span> <span>&lt;ray_mat&gt;</span>
<span>1</span>   <span>&lt;</span>T:<span>12288</span><span>|</span><span>UV</span><span>|N|</span>M:<span>1</span><span>&gt;</span>    <span>&lt;</span><span>6146</span><span>x</span><span>3</span><span>&gt;</span>    <span>&lt;</span><span>6146</span><span>x</span><span>2</span><span>&gt;</span>    <span>&lt;none&gt;</span>   <span>&lt;</span><span>phong</span><span>&gt;</span>
<span>2</span>   <span>&lt;</span>T:<span>49152</span><span>|</span><span>UV</span><span>|N|</span>M:<span>1</span><span>&gt;</span>   <span>&lt;</span><span>24578</span><span>x</span><span>3</span><span>&gt;</span>   <span>&lt;</span><span>24578</span><span>x</span><span>2</span><span>&gt;</span>    <span>&lt;none&gt;</span>   <span>&lt;</span><span>phong</span><span>&gt;</span>
<span>3</span> <span>&lt;</span>T:<span>2334720</span><span>|</span><span>UV</span><span>|N|</span>M:<span>6</span><span>&gt;</span> <span>&lt;</span><span>1167360</span><span>x</span><span>3</span><span>&gt;</span> <span>&lt;</span><span>1167360</span><span>x</span><span>2</span><span>&gt;</span>    <span>&lt;none&gt;</span>      <span>&lt;</span><span>6x</span><span>&gt;</span>
</code></pre>
</div>
<p>Yikes! We can see it grew by a lot: Five subdivision levels resulted in a <span>\(4^5 = 1024\)</span>x increase in the mesh size. Of course, we can turn the <code>normals</code> option back on, which then calculates smoothed vertex normals. With vertex normals, we only need three subdivision levels to achieve the same visual fidelity as the 5x subdivided mesh. Since the triangles are relatively small compared to the low-poly original, we won‚Äôt see the same sort of lighting discontinuities noted in the first render.</p>
<div>
<div id="cb5"><pre><code><span id="cb5-1"><span>translate_mesh</span>(<span>subdivide_mesh</span>(scene, <span>subdivision_levels =</span> <span>3</span>) ,<span>position=</span><span>c</span>(<span>0</span>,<span>1.1</span>,<span>0</span>)) <span>|&gt;</span> </span>
<span id="cb5-2">  <span>add_shape</span>(scene_5x) <span>|&gt;</span> </span>
<span id="cb5-3">  <span>add_shape</span>(<span>translate_mesh</span>(scene,<span>position=</span><span>c</span>(<span>0</span>,<span>2.2</span>,<span>0</span>))) <span>|&gt;</span> </span>
<span id="cb5-4">  <span>rasterize_scene</span>(<span>lookfrom =</span> <span>c</span>(<span>-</span><span>4</span>,<span>3</span>,<span>10</span>), <span>lookat=</span><span>c</span>(<span>-</span><span>0.05</span>,<span>1.1</span>,<span>0</span>),</span>
<span id="cb5-5">                  <span>fov=</span><span>18</span>, <span>height =</span> <span>1100</span>, <span>width=</span><span>1100</span>,</span>
<span id="cb5-6">                  <span>light_info =</span> <span>directional_light</span>(<span>c</span>(<span>0.2</span>,<span>1</span>,<span>2</span>)))</span></code></pre></div>
<div id="fig-subdivide_both">
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/fig-subdivide_both-1.png" width="1100">
</p>
<figcaption id="fig-subdivide_both-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Top Row: Original low-poly meshes. Middle Row: Highly subdivided (1024x triangles) scene without vertex normals. Bottom Row: 3x subdivided scene (64x triangles) with vertex normals.
</figcaption>
</figure>
</div>
</div>
<p>And that‚Äôs it for subdivision surfaces, at least for now!</p>
</section>
<section id="displacement-mapping">
<h2 data-anchor-id="displacement-mapping">Displacement Mapping</h2>
<div><p>We just subdivided a low-red mesh to make it smooth‚Äìhow do we make it bumpy? We can do that by applying a displacement texture, which offsets each vertex via its vertex normal. Here‚Äôs an example of a displacement texture of the moon:</p><p><span>If there aren‚Äôt any vertex normals in the mesh, it first calculates them by averaging the directions of all the faces connected to a single vertex. This interface also supports vector displacement, but I‚Äôm not going to get into that in this post</span></p></div>
<div>
<div id="cb6"><pre><code><span id="cb6-1">disp_moon <span>=</span> <span>ray_read_image</span>(<span>"../images/2024/ldem_3_8bit.jpg"</span>) </span>
<span id="cb6-2"><span>plot_image</span>(disp_moon)</span></code></pre></div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/moon_disp_image-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Higher regions are lighter, and depressions are darker. If you‚Äôve used worked in GIS software or with <code>rayshader</code> before, you might ask: isn‚Äôt this just a digital elevation model (DEM)? Sure is! Let‚Äôs check out the Monterey Bay data included in <code>rayshader</code> to see:</p>
<div>
<div id="annotated-cell-15"><pre><code><span id="annotated-cell-15-1"><span>library</span>(rayshader) </span>
<span id="annotated-cell-15-2"></span>
<span id="annotated-cell-15-3">montereybay <span>|&gt;</span></span>
<span id="annotated-cell-15-4">  <span>height_shade</span>(<span>texture =</span> <span>colorRampPalette</span>(<span>c</span>(<span>"black"</span>,<span>"white"</span>))(<span>100</span>)) <span>|&gt;</span></span>
<span id="annotated-cell-15-5">  <span>plot_map</span>()</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-15" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-15" data-code-lines="3,4,5" data-code-annotation="1">Use rayshader to plot the black to white color mapping of the Monterey Bay DEM</span>
</dd>
</dl>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/mont_bay-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>What displacement mapping does is allow this type of transformation to be applied to any 3D surface, rather than just a 2D plane (as in <code>rayshader</code>). So displacement mapping allows us to apply these displacements to a sphere. In a universe full of spheres and ellipsoids, this capability can be quite useful. Our image data ranges from 0-1 in this case, and the difference (from the mean elevation) between the highest point on the moon (6.7 miles) and the lowest (-5.4 miles) is 12.1 miles, which when compared to the moon‚Äôs radius (1,079.6 miles) is about a 1.1 percent variation. So with a unit sphere representing the moon, our displacement scale is 0.011.</p>
<div><p>Let‚Äôs visualize the displacement on a basic small sphere mesh. :</p><p><span>I have to call <code>smooth_normals_mesh()</code> and <code>add_sphere_uv_mesh()</code> because the displacement algorithm here requires one UV coordinate/normal per vertex</span></p></div>
<div>
<div id="annotated-cell-16"><pre><code><span id="annotated-cell-16-1">lights <span>=</span> <span>directional_light</span>(<span>c</span>(<span>1</span>,<span>0</span>,<span>0</span>)) <span>|&gt;</span></span>
<span id="annotated-cell-16-2">  <span>add_light</span>(<span>directional_light</span>(<span>c</span>(<span>-</span><span>1</span>,<span>0</span>,<span>0</span>),</span>
<span id="annotated-cell-16-3">                              <span>color=</span><span>"dodgerblue"</span>,</span>
<span id="annotated-cell-16-4">                              <span>intensity=</span><span>1</span>))</span>
<span id="annotated-cell-16-5"></span>
<span id="annotated-cell-16-6">white_material <span>=</span> <span>material_list</span>(<span>diffuse =</span> <span>"white"</span>,</span>
<span id="annotated-cell-16-7">                               <span>ambient =</span> <span>"white"</span>,</span>
<span id="annotated-cell-16-8">                               <span>diffuse_intensity =</span> <span>0.9</span>,</span>
<span id="annotated-cell-16-9">                               <span>ambient_intensity =</span> <span>0.1</span>)</span>
<span id="annotated-cell-16-10"></span>
<span id="annotated-cell-16-11"><span>sphere_mesh</span>(<span>material =</span> white_material) <span>|&gt;</span></span>
<span id="annotated-cell-16-12">  <span>smooth_normals_mesh</span>() <span>|&gt;</span></span>
<span id="annotated-cell-16-13">  <span>add_sphere_uv_mesh</span>(<span>override_existing =</span> <span>TRUE</span>) <span>-&gt;</span></span>
<span id="annotated-cell-16-14">basic_sphere_uv</span>
<span id="annotated-cell-16-15"></span>
<span id="annotated-cell-16-16">basic_sphere_uv <span>|&gt;</span></span>
<span id="annotated-cell-16-17">  <span>displace_mesh</span>(disp_moon, <span>displacement_scale =</span> <span>0.011</span>) <span>|&gt;</span></span>
<span id="annotated-cell-16-18">  <span>rasterize_scene</span>(<span>light_info =</span> lights, <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>13</span>)</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-16" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-16" data-code-lines="1,2,3,4" data-code-annotation="1">Generate the lighting for the scene</span>
</dd>
<dt data-target-cell="annotated-cell-16" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-16" data-code-lines="6,7,8,9" data-code-annotation="2">Generate the basic white material for the sphere</span>
</dd>
<dt data-target-cell="annotated-cell-16" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-16" data-code-lines="11,12,13,14" data-code-annotation="3">Create a smooth sphere and add unique normals and UV coordinates for each vertex</span>
</dd>
<dt data-target-cell="annotated-cell-16" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-16" data-code-lines="16,17,18" data-code-annotation="4">Displace the sphere with the moon data, scaled by 0.011, and render it</span>
</dd>
</dl>
</div>
<div>
<pre><code>Displacing mesh with 512x1024 texture</code></pre>
</div>
<div>
<pre><code>Setting `lookat` to: c(-0.00, -0.00, 0.00)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/moon_disp-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Wait‚Äìnothing happened? What went wrong? Well, let‚Äôs look at the mesh info and compare the number of vertices in the mesh to the resolution of the image:</p>
<div>

<div>
<pre><code>‚îÄ‚îÄ Scene Description ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></pre>
</div>
<pre><code><span>‚Ä¢</span> Summary - <span>Meshes</span>: <span>1</span> | <span>Unique Materials</span>: <span>1</span>
<span>‚Ñπ</span> XYZ Bounds - <span>Min</span>: <span>c(-1.00, -1.00, -1.00)</span> | <span>Max</span>: <span>c(1.00, 1.00, 1.00)</span>
            shapes  vertices texcoords   normals materials
         <span>&lt;ray_shp&gt;</span> <span>&lt;ray_dat&gt;</span> <span>&lt;ray_dat&gt;</span> <span>&lt;ray_dat&gt;</span> <span>&lt;ray_mat&gt;</span>
<span>1</span> <span>&lt;</span>T:<span>960</span><span>|</span><span>UV</span><span>|</span><span>N</span><span>|</span>M:<span>1</span><span>&gt;</span>   <span>&lt;</span><span>482</span><span>x</span><span>3</span><span>&gt;</span>   <span>&lt;</span><span>482</span><span>x</span><span>2</span><span>&gt;</span>   <span>&lt;</span><span>482</span><span>x</span><span>3</span><span>&gt;</span> <span>&lt;</span><span>diffuse</span><span>&gt;</span>
</code></pre>
</div>
<div>
<div id="annotated-cell-18"><pre><code><span id="annotated-cell-18-1"><span>dim</span>(disp_moon)</span>
<span id="annotated-cell-18-2"><span>prod</span>(<span>dim</span>(disp_moon)[<span>1</span><span>:</span><span>2</span>])</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-18" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-18" data-code-lines="2" data-code-annotation="1">Multiply the dimensions of the texture together to get the number of pixels</span>
</dd>
</dl>
</div>
<div>
<pre><code>[1]  512 1024    3
[1] 524288</code></pre>
</div>
</div>
<p>So there‚Äôs half a million points, but only about 500 total vertices. Let‚Äôs write a function to show ourselves exactly where and how much of the displacement map we‚Äôre sampling. The green pixels are the places in the elevation model we are actually using to displace out mesh.</p>
<div>
<div id="annotated-cell-19"><pre><code><span id="annotated-cell-19-1">get_displacement_access_info <span>=</span> <span>function</span>(mesh, image) {</span>
<span id="annotated-cell-19-2">  image_coords <span>=</span> <span>round</span>(<span>matrix</span>(<span>dim</span>(image)[<span>2</span><span>:</span><span>1</span>]<span>-</span><span>1</span>,<span>ncol=</span><span>2</span>,</span>
<span id="annotated-cell-19-3">                         <span>nrow=</span><span>nrow</span>(mesh<span>$</span>texcoords[[<span>1</span>]]),</span>
<span id="annotated-cell-19-4">                         <span>byrow=</span><span>TRUE</span>) <span>*</span> mesh<span>$</span>texcoords[[<span>1</span>]])</span>
<span id="annotated-cell-19-5"></span>
<span id="annotated-cell-19-6">  <span>for</span>(i <span>in</span> <span>seq_len</span>(<span>nrow</span>(image_coords))) {</span>
<span id="annotated-cell-19-7">    image[<span>1</span><span>+</span>image_coords[i,<span>2</span>],<span>1</span><span>+</span>image_coords[i,<span>1</span>],<span>1</span><span>:</span><span>3</span>] <span>=</span> <span>c</span>(<span>0</span>,<span>1</span>,<span>0</span>)</span>
<span id="annotated-cell-19-8">  }</span>
<span id="annotated-cell-19-9">  total_pixels <span>=</span> <span>as.integer</span>(<span>prod</span>(<span>dim</span>(image)[<span>1</span><span>:</span><span>2</span>]))</span>
<span id="annotated-cell-19-10">  total_pixels_sampled <span>=</span> <span>as.integer</span>(<span>sum</span>(image[,,<span>1</span>] <span>==</span> <span>0</span> <span>&amp;</span> image[,,<span>2</span>] <span>==</span> <span>1</span> <span>&amp;</span> image[,,<span>3</span>] <span>==</span> <span>0</span>))</span>
<span id="annotated-cell-19-11">  <span>message</span>(<span>sprintf</span>(<span>"Total pixels sampled: %i/%i (%0.5f%%)"</span>,</span>
<span id="annotated-cell-19-12">                  total_pixels_sampled, total_pixels, total_pixels_sampled<span>/</span>total_pixels<span>*</span><span>100</span>))</span>
<span id="annotated-cell-19-13">  <span>plot_image</span>(image)</span>
<span id="annotated-cell-19-14">}</span>
<span id="annotated-cell-19-15"><span>get_displacement_access_info</span>(basic_sphere_uv, disp_moon )</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-19" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-19" data-code-lines="2,3,4" data-code-annotation="1">Take the UV coordinates (which range from 0-1) and map them to the pixel coordinates by multiplying the number of rows and columns by each UV pair.</span>
</dd>
<dt data-target-cell="annotated-cell-19" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-19" data-code-lines="6,7,8" data-code-annotation="2">Loop over the image and set each accessed pixel to green.</span>
</dd>
<dt data-target-cell="annotated-cell-19" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-19" data-code-lines="9" data-code-annotation="3">Calculate the total number of pixels.</span>
</dd>
<dt data-target-cell="annotated-cell-19" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-19" data-code-lines="10" data-code-annotation="4">Calculate the total number of accessed (green) pixels.</span>
</dd>
<dt data-target-cell="annotated-cell-19" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-19" data-code-lines="11,12" data-code-annotation="5">Print the access information.</span>
</dd>
<dt data-target-cell="annotated-cell-19" data-target-annotation="6">6</dt>
<dd>
<span data-code-cell="annotated-cell-19" data-code-lines="13" data-code-annotation="6">Plot the image with green pixels marking data used to displace the mesh.</span>
</dd>
</dl>
</div>
<div>
<pre><code>Total pixels sampled: 482/524288 (0.09193%)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/mesh_show_uv-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>So, we‚Äôre sampling 0.092% of the pixels in the image‚Äìno wonder it‚Äôs such a poor approximation! Let‚Äôs use subdivision to increase the size of our mesh, which should sample more of the underlying displacement texture and thus give us a better approximation.</p>
<div>
<div id="annotated-cell-20"><pre><code><span id="annotated-cell-20-1">generate_moon_mesh <span>=</span> <span>function</span>(subdivision_levels, displacement_texture, displacement_scale) {</span>
<span id="annotated-cell-20-2">  <span>sphere_mesh</span>(<span>material =</span> white_material) <span>|&gt;</span> </span>
<span id="annotated-cell-20-3">    <span>smooth_normals_mesh</span>() <span>|&gt;</span> </span>
<span id="annotated-cell-20-4">    <span>subdivide_mesh</span>(<span>subdivision_levels =</span> subdivision_levels) <span>|&gt;</span></span>
<span id="annotated-cell-20-5">    <span>add_sphere_uv_mesh</span>(<span>override_existing =</span> <span>TRUE</span>) <span>|&gt;</span></span>
<span id="annotated-cell-20-6">    <span>displace_mesh</span>(displacement_texture, displacement_scale) <span>|&gt;</span></span>
<span id="annotated-cell-20-7">    <span>rotate_mesh</span>(<span>c</span>(<span>0</span>,<span>90</span>,<span>0</span>))</span>
<span id="annotated-cell-20-8">}</span>
<span id="annotated-cell-20-9">moon_subdivided_2 <span>=</span> <span>generate_moon_mesh</span>(<span>2</span>, disp_moon, <span>0.011</span>)</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-20" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-20" data-code-lines="4" data-code-annotation="1">Subdivide the moon</span>
</dd>
<dt data-target-cell="annotated-cell-20" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-20" data-code-lines="5" data-code-annotation="2">Add new UV coords with a spherical mapping</span>
</dd>
<dt data-target-cell="annotated-cell-20" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-20" data-code-lines="6" data-code-annotation="3">Displace with the displacement texture</span>
</dd>
<dt data-target-cell="annotated-cell-20" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-20" data-code-lines="7" data-code-annotation="4">Rotate the mesh to orient it at the camera</span>
</dd>
</dl>
</div>
<div>
<pre><code>Displacing mesh with 512x1024 texture</code></pre>
</div>
<div id="cb14"><pre><code><span id="cb14-1"><span>rasterize_scene</span>(moon_subdivided_2, <span>light_info =</span> lights,</span>
<span id="cb14-2">                <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>13</span>)</span></code></pre></div>
<div>
<pre><code>Setting `lookat` to: c(-0.00, -0.00, -0.00)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide_displace2-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Still looks nothing like the moon. Let‚Äôs check out the displacement texture access pattern.</p>
<div>
<div id="cb16"><pre><code><span id="cb16-1"><span>get_displacement_access_info</span>(moon_subdivided_2, disp_moon )</span></code></pre></div>
<div>
<pre><code>Total pixels sampled: 7682/524288 (1.46523%)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/mesh_show_uv2-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Better, but still extremely sparse. What about three subdivision levels?</p>
<div>
<div id="cb18"><pre><code><span id="cb18-1">moon_subdivided_3 <span>=</span> <span>generate_moon_mesh</span>(<span>3</span>, disp_moon, <span>0.011</span>)</span></code></pre></div>
<div>
<pre><code>Displacing mesh with 512x1024 texture</code></pre>
</div>
<div id="cb20"><pre><code><span id="cb20-1"><span>rasterize_scene</span>(moon_subdivided_3, <span>light_info =</span> lights,</span>
<span id="cb20-2">                <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>13</span>)</span></code></pre></div>
<div>
<pre><code>Setting `lookat` to: c(-0.00, -0.00, -0.00)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide_displace3-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Maybe some hints of craters? If you squint. Time to check the UV coords.</p>
<div>
<div id="cb22"><pre><code><span id="cb22-1"><span>get_displacement_access_info</span>(moon_subdivided_3 , disp_moon )</span></code></pre></div>
<div>
<pre><code>Total pixels sampled: 30722/524288 (5.85976%)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/mesh_show_uv3-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Alright, so we‚Äôre sampling the big craters with a decent density of points. I‚Äôll note here that at a resolution of 512x1024 and about 500 vertices in our original mesh, we‚Äôll need about a factor of 1000x more vertices to densely sample from our UV texture. Let‚Äôs march on and see if that‚Äôs the case.</p>
<div>
<div id="cb24"><pre><code><span id="cb24-1">moon_subdivided_4 <span>=</span> <span>generate_moon_mesh</span>(<span>4</span>, disp_moon, <span>0.011</span>)</span></code></pre></div>
<div>
<pre><code>Displacing mesh with 512x1024 texture</code></pre>
</div>
<div id="cb26"><pre><code><span id="cb26-1"><span>rasterize_scene</span>(moon_subdivided_4, <span>light_info =</span> lights,</span>
<span id="cb26-2">                <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>13</span>)</span></code></pre></div>
<div>
<pre><code>Setting `lookat` to: c(-0.00, -0.00, -0.00)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide_displace4-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Okay, so we‚Äôre definitely starting to see distinct craters.</p>
<div>
<div id="cb28"><pre><code><span id="cb28-1"><span>get_displacement_access_info</span>(moon_subdivided_4 , disp_moon )</span></code></pre></div>
<div>
<pre><code>Total pixels sampled: 122840/524288 (23.42987%)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/mesh_show_uv4-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Much denser, but not connected. So indeed, we shall proceed to five subdivision levels.</p>
<div>
<div id="cb30"><pre><code><span id="cb30-1">moon_subdivided_5 <span>=</span> <span>generate_moon_mesh</span>(<span>5</span>, disp_moon, <span>0.011</span>)</span></code></pre></div>
<div>
<pre><code>Displacing mesh with 512x1024 texture</code></pre>
</div>
<div id="cb32"><pre><code><span id="cb32-1"><span>rasterize_scene</span>(moon_subdivided_5, <span>light_info =</span> lights,</span>
<span id="cb32-2">                <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>13</span>)</span></code></pre></div>
<div>
<pre><code>Setting `lookat` to: c(-0.00, -0.00, -0.00)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide_displace5-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Look at that! Definite improvement over four. And the UV texcoord access?</p>
<div>
<div id="cb34"><pre><code><span id="cb34-1"><span>get_displacement_access_info</span>(moon_subdivided_5, disp_moon )</span></code></pre></div>
<div>
<pre><code>Total pixels sampled: 467649/524288 (89.19697%)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/mesh_show_uv5-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Definitely lots of densely interconnected green, along with some interesting patterns from what I can only assume are interpolation and sampling artifacts related to the original mesh structure. We‚Äôll do six levels and see if there‚Äôs any difference.</p>
<div>
<div id="cb36"><pre><code><span id="cb36-1">moon_subdivided_6 <span>=</span> <span>generate_moon_mesh</span>(<span>6</span>, disp_moon, <span>0.011</span>)</span></code></pre></div>
<div>
<pre><code>Displacing mesh with 512x1024 texture</code></pre>
</div>
<div id="cb38"><pre><code><span id="cb38-1"><span>rasterize_scene</span>(moon_subdivided_6, <span>light_info =</span> lights,</span>
<span id="cb38-2">                <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>13</span>)</span></code></pre></div>
<div>
<pre><code>Setting `lookat` to: c(-0.00, -0.00, -0.00)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide_displace6-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Everything‚Äôs a little sharper! But up close you can start to make out the individual pixels from the displacement mesh. Five is probably good enough.</p>
<div>

<div>
<pre><code>‚îÄ‚îÄ Scene Description ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></pre>
</div>
<pre><code><span>‚Ä¢</span> Summary - <span>Meshes</span>: <span>1</span> | <span>Unique Materials</span>: <span>1</span>
<span>‚Ñπ</span> XYZ Bounds - <span>Min</span>: <span>c(-0.99, -1.00, -0.99)</span> | <span>Max</span>: <span>c(0.99, 1.00, 0.99)</span>
                shapes    vertices   texcoords     normals materials
             <span>&lt;ray_shp&gt;</span>   <span>&lt;ray_dat&gt;</span>   <span>&lt;ray_dat&gt;</span>   <span>&lt;ray_dat&gt;</span> <span>&lt;ray_mat&gt;</span>
<span>1</span> <span>&lt;</span>T:<span>3932160</span><span>|</span><span>UV</span><span>|</span><span>N</span><span>|</span>M:<span>1</span><span>&gt;</span> <span>&lt;</span><span>1966082</span><span>x</span><span>3</span><span>&gt;</span> <span>&lt;</span><span>1966082</span><span>x</span><span>2</span><span>&gt;</span> <span>&lt;</span><span>1966082</span><span>x</span><span>3</span><span>&gt;</span> <span>&lt;</span><span>diffuse</span><span>&gt;</span>
</code></pre>
<div id="cb42"><pre><code><span id="cb42-1"><span>get_displacement_access_info</span>(moon_subdivided_6, disp_moon )</span></code></pre></div>
<div>
<pre><code>Total pixels sampled: 504426/524288 (96.21162%)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/mesh_show_uv6-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>However, 2 million vertices is a lot of wasted memory to sample our half million pixel image. Let‚Äôs use the new <code>displacement_sphere()</code> function to generate a sphere that has exactly one vertex per pixel in our texture. That way we aren‚Äôt under or oversampling our image.</p>
<div>
<div id="cb44"><pre><code><span id="cb44-1">moon_displacement_sphere <span>=</span> <span>displacement_sphere</span>(disp_moon, <span>displacement_scale =</span> <span>0.011</span>) <span>|&gt;</span> </span>
<span id="cb44-2">  <span>set_material</span>(<span>material =</span> white_material) <span>|&gt;</span> </span>
<span id="cb44-3">  <span>rotate_mesh</span>(<span>c</span>(<span>0</span>,<span>90</span>,<span>0</span>))</span></code></pre></div>
<div>
<pre><code>Displacing mesh with 512x1024 texture</code></pre>
</div>
<div id="cb46"><pre><code><span id="cb46-1"><span>rasterize_scene</span>(moon_displacement_sphere, <span>light_info =</span> lights,</span>
<span id="cb46-2">                <span>lookat=</span><span>c</span>(<span>0</span>,<span>0</span>,<span>0</span>), <span>lookfrom=</span><span>c</span>(<span>0</span>,<span>0</span>,<span>10</span>),</span>
<span id="cb46-3">                <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>13</span>)</span></code></pre></div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/subdivide_displace_sphere-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Great! Let‚Äôs check out how much of the displacement texture was used to generate this mesh.</p>
<div>

<div>
<pre><code>‚îÄ‚îÄ Scene Description ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></pre>
</div>
<pre><code><span>‚Ä¢</span> Summary - <span>Meshes</span>: <span>1</span> | <span>Unique Materials</span>: <span>1</span>
<span>‚Ñπ</span> XYZ Bounds - <span>Min</span>: <span>c(-1.01, -1.01, -1.01)</span> | <span>Max</span>: <span>c(1.00, 1.01, 1.00)</span>
                shapes   vertices  texcoords    normals materials
             <span>&lt;ray_shp&gt;</span>  <span>&lt;ray_dat&gt;</span>  <span>&lt;ray_dat&gt;</span>  <span>&lt;ray_dat&gt;</span> <span>&lt;ray_mat&gt;</span>
<span>1</span> <span>&lt;</span>T:<span>1045506</span><span>|</span><span>UV</span><span>|</span><span>N</span><span>|</span>M:<span>1</span><span>&gt;</span> <span>&lt;</span><span>524288</span><span>x</span><span>3</span><span>&gt;</span> <span>&lt;</span><span>524288</span><span>x</span><span>2</span><span>&gt;</span> <span>&lt;</span><span>524288</span><span>x</span><span>3</span><span>&gt;</span> <span>&lt;</span><span>diffuse</span><span>&gt;</span>
</code></pre>
<div id="cb49"><pre><code><span id="cb49-1"><span>get_displacement_access_info</span>(moon_displacement_sphere, disp_moon)</span></code></pre></div>
<div>
<pre><code>Total pixels sampled: 524288/524288 (100.00000%)</code></pre>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/disp_ideal-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>All green! Exactly as many vertices as pixels in the image, by construction. While this might sound good, this type of mesh can actually lead to visual artifacts. Let‚Äôs say we really cared about accurately visualizing the north and south poles. We‚Äôll zoom in and see what they look like with this perfectly mapped mesh.</p>
<div>
<div id="cb51"><pre><code><span id="cb51-1"><span>rasterize_scene</span>(moon_displacement_sphere, </span>
<span id="cb51-2">                <span>light_info =</span> <span>directional_light</span>(<span>c</span>(<span>0</span>,<span>1</span>,<span>-</span><span>1</span>)),</span>
<span id="cb51-3">                <span>lookat=</span><span>c</span>(<span>0</span>,<span>1</span>,<span>0</span>), <span>lookfrom=</span><span>c</span>(<span>5</span>,<span>5</span>,<span>5</span>),</span>
<span id="cb51-4">                <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>2</span>) </span></code></pre></div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/poles-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Pucker up! We see here what‚Äôs referred to as ‚Äútexture pinching‚Äù at the poles, which happens due to the convergence of the longitudinal lines and the corresponding increasing density of vertices. Not great for texturing if you have any interesting phenomena at the poles you want to accurately display. But there‚Äôs a better way to represent displaced data on a sphere: let‚Äôs take a cube object and subdivide it.</p>
<div>
<div id="annotated-cell-41"><pre><code><span id="annotated-cell-41-1"><span>cube_mesh</span>(<span>material =</span> white_material) <span>|&gt;</span></span>
<span id="annotated-cell-41-2">  <span>subdivide_mesh</span>(<span>subdivision_levels =</span> <span>4</span>) <span>-&gt;</span></span>
<span id="annotated-cell-41-3">cube_low_res</span>
<span id="annotated-cell-41-4"></span>
<span id="annotated-cell-41-5"><span>rasterize_scene</span>(cube_low_res, <span>light_info =</span> lights,</span>
<span id="annotated-cell-41-6">                <span>lookat=</span><span>c</span>(<span>0</span>,<span>0</span>,<span>0</span>), <span>lookfrom=</span><span>c</span>(<span>0</span>,<span>0</span>,<span>10</span>),</span>
<span id="annotated-cell-41-7">                <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>6</span>)</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-41" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-41" data-code-lines="1,2,3" data-code-annotation="1">Subdivide a basic 8 vertex sphere</span>
</dd>
<dt data-target-cell="annotated-cell-41" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-41" data-code-lines="5,6,7" data-code-annotation="2">Render the subdivided sphere.</span>
</dd>
</dl>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/cube_example-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>This obviously isn‚Äôt a sphere, but we can fix that. Let‚Äôs project the vertices to a sphere centered at the origin. We‚Äôll also remap the UV coordinates using the <code>add_sphere_uv_mesh()</code> function, and recalculate the normals post-projection using <code>smooth_normals_mesh()</code>.</p>
<div>
<div id="annotated-cell-42"><pre><code><span id="annotated-cell-42-1">map_cube_to_sphere <span>=</span> <span>function</span>(mesh) {</span>
<span id="annotated-cell-42-2">  project_vertex_to_sphere <span>=</span> <span>function</span>(x) {</span>
<span id="annotated-cell-42-3">    x<span>/</span><span>sqrt</span>(<span>sum</span>(x<span>*</span>x))</span>
<span id="annotated-cell-42-4">  }</span>
<span id="annotated-cell-42-5"></span>
<span id="annotated-cell-42-6">  mesh<span>$</span>vertices[[<span>1</span>]] <span>=</span> <span>t</span>(<span>apply</span>(mesh<span>$</span>vertices[[<span>1</span>]],<span>1</span>,project_vertex_to_sphere))</span>
<span id="annotated-cell-42-7">  <span>add_sphere_uv_mesh</span>(mesh, <span>override_existing =</span> <span>TRUE</span>) <span>|&gt;</span></span>
<span id="annotated-cell-42-8">    <span>smooth_normals_mesh</span>()</span>
<span id="annotated-cell-42-9">}</span>
<span id="annotated-cell-42-10"></span>
<span id="annotated-cell-42-11"><span>cube_mesh</span>(<span>material =</span> white_material) <span>|&gt;</span></span>
<span id="annotated-cell-42-12">  <span>subdivide_mesh</span>(<span>subdivision_levels =</span> <span>9</span>) <span>|&gt;</span></span>
<span id="annotated-cell-42-13">  <span>map_cube_to_sphere</span>()  <span>-&gt;</span></span>
<span id="annotated-cell-42-14">spherized_cube</span>
<span id="annotated-cell-42-15">spherized_cube</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-42" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-42" data-code-lines="1" data-code-annotation="1">Define a function to transform a mesh to a sphere</span>
</dd>
<dt data-target-cell="annotated-cell-42" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-42" data-code-lines="2,3,4" data-code-annotation="2">Define helper function to map vertices (centered at zero) to a sphere by dividing by their length, measured from the origin.</span>
</dd>
<dt data-target-cell="annotated-cell-42" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-42" data-code-lines="6" data-code-annotation="3">Apply the helper function to all the vertices in the mesh</span>
</dd>
<dt data-target-cell="annotated-cell-42" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-42" data-code-lines="7,8" data-code-annotation="4">Add UV coords and normals</span>
</dd>
<dt data-target-cell="annotated-cell-42" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-42" data-code-lines="11,12,13,14" data-code-annotation="5">Subdivide the cube nine times to get approximately half a million vertices (to match the resolution of the displacement texture)</span>
</dd>
<dt data-target-cell="annotated-cell-42" data-target-annotation="6">6</dt>
<dd>
<span data-code-cell="annotated-cell-42" data-code-lines="15" data-code-annotation="6">Print the new subdivided cube-to-sphere mesh info</span>
</dd>
</dl>
</div>
<div>
<pre><code>‚îÄ‚îÄ Scene Description ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></pre>
</div>
<pre><code><span>‚Ä¢</span> Summary - <span>Meshes</span>: <span>1</span> | <span>Unique Materials</span>: <span>1</span>
<span>‚Ñπ</span> XYZ Bounds - <span>Min</span>: <span>c(-1.00, -1.00, -1.00)</span> | <span>Max</span>: <span>c(1.00, 1.00, 1.00)</span>
                shapes    vertices   texcoords     normals materials
             <span>&lt;ray_shp&gt;</span>   <span>&lt;ray_dat&gt;</span>   <span>&lt;ray_dat&gt;</span>   <span>&lt;ray_dat&gt;</span> <span>&lt;ray_mat&gt;</span>
<span>1</span> <span>&lt;</span>T:<span>3145728</span><span>|</span><span>UV</span><span>|</span><span>N</span><span>|</span>M:<span>1</span><span>&gt;</span> <span>&lt;</span><span>1572866</span><span>x</span><span>3</span><span>&gt;</span> <span>&lt;</span><span>1572866</span><span>x</span><span>2</span><span>&gt;</span> <span>&lt;</span><span>1572866</span><span>x</span><span>3</span><span>&gt;</span> <span>&lt;</span><span>diffuse</span><span>&gt;</span>
</code></pre>
</div>
<div>
<div id="annotated-cell-43"><pre><code><span id="annotated-cell-43-1"><span>rasterize_scene</span>(spherized_cube, <span>light_info =</span> lights,</span>
<span id="annotated-cell-43-2">                <span>lookat=</span><span>c</span>(<span>0</span>,<span>0</span>,<span>0</span>), <span>lookfrom=</span><span>c</span>(<span>0</span>,<span>0</span>,<span>10</span>),</span>
<span id="annotated-cell-43-3">                <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>13</span>)</span></code></pre></div>

<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/unnamed-chunk-4-1.png" width="1100"></p>
</figure>
</div>
</div>
<div><p>Looks like‚Ä¶ a sphere. There‚Äôs nothing indicating this used to be a cube! And the nice thing about this mesh is it has no extreme convergence at the poles. Let‚Äôs displace the subdivided mesh and see how the moon looks.</p><p><span>‚ÄúOn the internet, no one knows you‚Äôre a cube.‚Äù</span></p></div>
<div>
<div id="annotated-cell-44"><pre><code><span id="annotated-cell-44-1">spherized_cube <span>|&gt;</span></span>
<span id="annotated-cell-44-2">  <span>displace_mesh</span>(disp_moon, <span>0.011</span>) <span>|&gt;</span></span>
<span id="annotated-cell-44-3">  <span>rotate_mesh</span>(<span>c</span>(<span>0</span>,<span>90</span>,<span>0</span>)) <span>-&gt;</span></span>
<span id="annotated-cell-44-4">cube_moon</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-44" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-44" data-code-lines="1,2,3,4" data-code-annotation="1">Displace the spherized cube</span>
</dd>
</dl>
</div>
<div>
<pre><code>Displacing mesh with 512x1024 texture</code></pre>
</div>
</div>
<div>
<div id="annotated-cell-45"><pre><code><span id="annotated-cell-45-1"><span>rasterize_scene</span>(cube_moon, <span>light_info =</span> lights,</span>
<span id="annotated-cell-45-2">                <span>lookat=</span><span>c</span>(<span>0</span>,<span>0</span>,<span>0</span>), <span>lookfrom=</span><span>c</span>(<span>0</span>,<span>0</span>,<span>10</span>),</span>
<span id="annotated-cell-45-3">                <span>width =</span> <span>1100</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>13</span>)</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-45" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-45" data-code-lines="1,2,3" data-code-annotation="1">Render the displaced mesh</span>
</dd>
</dl>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/cube_moon_render-1.png" width="1100"></p>
</figure>
</div>
</div>
<p>Looks good to me. Let‚Äôs compare the two polar meshes. Note that the initial poor rectangle-to-sphere mapping we did above is referred to as a ‚ÄúUV sphere‚Äù in 3D graphics.</p>
<div>
<div id="annotated-cell-46"><pre><code><span id="annotated-cell-46-1">moon_displacement_sphere <span>|&gt;</span></span>
<span id="annotated-cell-46-2">  <span>rasterize_scene</span>(<span>light_info =</span> <span>directional_light</span>(<span>c</span>(<span>0</span>,<span>1</span>,<span>-</span><span>1</span>)),</span>
<span id="annotated-cell-46-3">                  <span>lookat=</span><span>c</span>(<span>0</span>,<span>1</span>,<span>0</span>), <span>lookfrom=</span><span>c</span>(<span>5</span>,<span>5</span>,<span>5</span>),</span>
<span id="annotated-cell-46-4">                  <span>width =</span> <span>550</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>2</span>, <span>plot =</span> <span>FALSE</span>) <span>-&gt;</span></span>
<span id="annotated-cell-46-5">polar_image</span>
<span id="annotated-cell-46-6"></span>
<span id="annotated-cell-46-7">cube_moon <span>|&gt;</span></span>
<span id="annotated-cell-46-8">  <span>rasterize_scene</span>(<span>light_info =</span>  <span>directional_light</span>(<span>c</span>(<span>0</span>,<span>1</span>,<span>-</span><span>1</span>)),</span>
<span id="annotated-cell-46-9">                <span>lookat=</span><span>c</span>(<span>0</span>,<span>1</span>,<span>0</span>), <span>lookfrom=</span><span>c</span>(<span>5</span>,<span>5</span>,<span>5</span>),</span>
<span id="annotated-cell-46-10">                <span>width =</span> <span>550</span>, <span>height =</span> <span>550</span>, <span>fov=</span><span>2</span>, <span>plot =</span> <span>FALSE</span>) <span>-&gt;</span></span>
<span id="annotated-cell-46-11">polar_image_cube</span>
<span id="annotated-cell-46-12"></span>
<span id="annotated-cell-46-13"><span>plot_image_grid</span>(<span>list</span>(polar_image,polar_image_cube),<span>dim =</span> <span>c</span>(<span>1</span>,<span>2</span>))</span></code></pre></div>
<div>
<dl>
<dt data-target-cell="annotated-cell-46" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-46" data-code-lines="1,2,3,4,5" data-code-annotation="1">Render and save the UV sphere displacement map to an image array</span>
</dd>
<dt data-target-cell="annotated-cell-46" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-46" data-code-lines="7,8,9,10,11" data-code-annotation="2">Render and save the spherized cube displacement map to an image array</span>
</dd>
<dt data-target-cell="annotated-cell-46" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-46" data-code-lines="13" data-code-annotation="3">Plot the images side by side using rayimage.</span>
</dd>
</dl>
</div>
<div>
<figure>
<p><img src="https://www.tylermw.com/posts/rayverse/displacement-mapping_files/figure-html/compare_polar_images-1.png" width="1100"></p>
<figcaption>Left: UV sphere. Right: Spherized cube.</figcaption>
</figure>
</div>
</div>
<p>No celestial b-hole! This option is included in the <code>displacement_sphere()</code> function by setting <code>use_cube = TRUE</code>.</p>
<p>And that‚Äôs it for the new features in <code>rayvertex</code> and <code>rayrender</code>! <code>rayrender</code> has both of these features, but they aren‚Äôt standalone functions: they are built-in to the mesh functions that support textures (e.g.&nbsp;<code>mesh3d_model()</code>, <code>obj_model()</code>, and <code>raymesh_model()</code>. You can install the latest development versions of both packages from r-universe or github via the following:</p>
<div id="cb54"><pre><code><span id="cb54-1"><span>install.packages</span>(<span>c</span>(<span>"rayvertex"</span>,<span>"rayrender"</span>), <span>repos =</span> <span>"https://tylermorganwall.r-universe.dev"</span>)</span>
<span id="cb54-2"><span>#or</span></span>
<span id="cb54-3">remotes<span>::</span><span>install_github</span>(<span>"tylermorganwall/rayvertex"</span>)</span>
<span id="cb54-4">remotes<span>::</span><span>install_github</span>(<span>"tylermorganwall/rayrender"</span>)</span></code></pre></div>
</section>
<section id="summary">
<h2 data-anchor-id="summary">Summary</h2>
<p>In this post, we‚Äôve explored the concepts of subdivision surfaces and displacement mapping. We began by discussing the fundamental challenges in 3D rendering, such as the difficulty of representing smooth and bumpy objects using flat, planar triangles. We then delved into historical solutions, like the use of micropolygons, and how modern techniques, including Loop subdivision, allow for the creation of detailed and smooth 3D models.</p>
<p>I demonstrated how to implement these techniques using the rayvertex, rayimage, and rayrender packages. I showed you how subdividing a mesh can significantly increase its resolution, allowing for smoother and more detailed surfaces. Additionally, we examined how displacement mapping can be used to add realistic texture to 3D models by manipulating vertex positions based on a 2D texture map.</p>
</section>
</main> 

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YaFSDP: a sharded data parallelism framework, faster for pre-training LLMs (128 pts)]]></title>
            <link>https://github.com/yandex/YaFSDP</link>
            <guid>40716701</guid>
            <pubDate>Tue, 18 Jun 2024 11:54:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yandex/YaFSDP">https://github.com/yandex/YaFSDP</a>, See on <a href="https://news.ycombinator.com/item?id=40716701">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">YaFSDP</h2><a id="user-content-yafsdp" aria-label="Permalink: YaFSDP" href="#yafsdp"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/yandex/YaFSDP/blob/main/assets/yafsdp_logo.png#gh-light-mode-only"><img src="https://github.com/yandex/YaFSDP/raw/main/assets/yafsdp_logo.png#gh-light-mode-only" width="400px"></a>
 <a target="_blank" rel="noopener noreferrer" href="https://github.com/yandex/YaFSDP/blob/main/assets/yafsdp_logo_white.png#gh-dark-mode-only"><img src="https://github.com/yandex/YaFSDP/raw/main/assets/yafsdp_logo_white.png#gh-dark-mode-only" width="400px"></a>
</p>
&nbsp;
<ul dir="auto">
<li><a href="#overview">Overview</a></li>
<li><a href="#advantages-over-fsdp">Advantages over FSDP</a></li>
<li><a href="#examples">Examples</a></li>
<li><a href="#issues-and-questions">Issues and questions</a></li>
<li><a href="#citation">Citation</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">YaFSDP is a Sharded Data Parallelism framework, designed to work well with transformer-like
neural network architectures.</p>
<p dir="auto">You can find more info on YaFSDP internals in our blog posts on
<a href="https://medium.com/yandex/yafsdp-a-tool-for-faster-llm-training-and-optimized-gpu-utilization-is-no-632b7539f5b3" rel="nofollow">Medium</a>
and <a href="https://habr.com/ru/companies/yandex/articles/817509/" rel="nofollow">Habr</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Advantages over FSDP</h2><a id="user-content-advantages-over-fsdp" aria-label="Permalink: Advantages over FSDP" href="#advantages-over-fsdp"></a></p>
<p dir="auto">YaFSDP is up to 20% faster for pre-training LLMs and performs better in high
memory pressure conditions. It is designed to reduce communications and memory
operations overhead.</p>
<p dir="auto">YaFSDP:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/yandex/YaFSDP/blob/main/assets/ya_fsdp.png"><img src="https://github.com/yandex/YaFSDP/raw/main/assets/ya_fsdp.png" alt="ya_fsdp"></a></p>
<p dir="auto">FSDP:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/yandex/YaFSDP/blob/main/assets/fsdp.png"><img src="https://github.com/yandex/YaFSDP/raw/main/assets/fsdp.png" alt="fsdp"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Benchmarks</h3><a id="user-content-benchmarks" aria-label="Permalink: Benchmarks" href="#benchmarks"></a></p>
<p dir="auto">We've compared YaFSDP with FSDP on a variety of pre-training setups ranging from:</p>
<ul dir="auto">
<li>7B to 70B parameters</li>
<li>64 to 256 devices</li>
<li>2048 to 8192 tokens per sequence</li>
</ul>
<table>
<thead>
<tr>
<th>model</th>
<th>gpu-count</th>
<th>seq-len</th>
<th>num-ckpt-layers</th>
<th>speedup</th>
<th>YaFSDP iteration time (s)</th>
<th>FSDP iteration time (s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama 2 7B</td>
<td>64</td>
<td>2048</td>
<td>0</td>
<td>9.92%</td>
<td>0.81</td>
<td>0.90</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>64</td>
<td>4096</td>
<td>0</td>
<td>3.43%</td>
<td>1.16</td>
<td>1.21</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>64</td>
<td>8192</td>
<td>0</td>
<td>2.68%</td>
<td>2.23</td>
<td>2.29</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>128</td>
<td>2048</td>
<td>0</td>
<td>9.57%</td>
<td>0.87</td>
<td>0.97</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>128</td>
<td>4096</td>
<td>0</td>
<td>2.42%</td>
<td>1.19</td>
<td>1.22</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>128</td>
<td>8192</td>
<td>0</td>
<td>2.32%</td>
<td>2.25</td>
<td>2.31</td>
</tr>
<tr>
<td>Llama 2 13B</td>
<td>128</td>
<td>2048</td>
<td>0</td>
<td>12.10%</td>
<td>1.55</td>
<td>1.76</td>
</tr>
<tr>
<td>Llama 2 13B</td>
<td>128</td>
<td>4096</td>
<td>0</td>
<td>3.49%</td>
<td>2.06</td>
<td>2.14</td>
</tr>
<tr>
<td>Llama 2 34B</td>
<td>128</td>
<td>2048</td>
<td>0</td>
<td>20.70%</td>
<td>3.39</td>
<td>4.27</td>
</tr>
<tr>
<td>Llama 2 34B</td>
<td>256</td>
<td>2048</td>
<td>0</td>
<td>21.99%</td>
<td>3.51</td>
<td>4.50</td>
</tr>
<tr>
<td>Llama 2 34B</td>
<td>256</td>
<td>4096</td>
<td>5</td>
<td>8.35%</td>
<td>5.33</td>
<td>5.81</td>
</tr>
<tr>
<td>Llama 2 70B</td>
<td>256</td>
<td>2048</td>
<td>10</td>
<td>21.48%</td>
<td>6.97</td>
<td>8.87</td>
</tr>
<tr>
<td>Llama 2 70B</td>
<td>256</td>
<td>4096</td>
<td>50</td>
<td>7.17%</td>
<td>11.07</td>
<td>11.93</td>
</tr>
<tr>
<td>Llama 3 8B</td>
<td>64</td>
<td>2048</td>
<td>0</td>
<td>11.91%</td>
<td>0.97</td>
<td>1.10</td>
</tr>
<tr>
<td>Llama 3 8B</td>
<td>64</td>
<td>4096</td>
<td>0</td>
<td>7.86%</td>
<td>1.36</td>
<td>1.48</td>
</tr>
<tr>
<td>Llama 3 70B</td>
<td>256</td>
<td>2048</td>
<td>20</td>
<td>26.60%</td>
<td>7.17</td>
<td>9.76</td>
</tr>
</tbody>
</table>
<p dir="auto">Details:</p>
<ul dir="auto">
<li>In each run per-device batch size is set to 1.</li>
<li><code>speedup</code> represents relative iteration time decrease between YaFSDP and FSDP runs.</li>
<li><code>num-ckpt-layers</code> refers to the number of transformer layers to which
activation checkpointing was applied.</li>
<li>Performance was measured using a cluster of hosts with A100 80 GB GPUs.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">You can find examples of LLM training using ü§ó stack in the <code>examples</code> folder:</p>
<ol dir="auto">
<li><code>clm.md</code> for causal pre-training</li>
<li><code>sft.md</code> for supervised fine-tuning</li>
</ol>
<p dir="auto">Notice that both examples require a Docker image, which can be built using
<code>docker/build.sh</code> script. The image is based on the <a href="https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-24-02.html" rel="nofollow">NVIDIA PyTorch
image</a>
with some patched ü§ó libraries. Patches for the libraries can be found in the
<code>patches</code> folder.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Issues and questions</h2><a id="user-content-issues-and-questions" aria-label="Permalink: Issues and questions" href="#issues-and-questions"></a></p>
<p dir="auto">If you encounter any bugs of have any questions <a href="https://github.com/yandex/YaFSDP/issues/new">feel free to open a GitHub issue</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use this codebase, please cite it by using the following BibTeX entry:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{YaFSDP2024,
  author =       {Mikhail Khrushchev and Anton Frolov and Ruslan Vasilev},
  title =        {YaFSDP: Yet another Fully Sharded Data Parallel},
  howpublished = {\url{https://github.com/yandex/YaFSDP}},
  year =         {2024}
}"><pre><span>@misc</span>{<span>YaFSDP2024</span>,
  <span>author</span> =       <span><span>{</span>Mikhail Khrushchev and Anton Frolov and Ruslan Vasilev<span>}</span></span>,
  <span>title</span> =        <span><span>{</span>YaFSDP: Yet another Fully Sharded Data Parallel<span>}</span></span>,
  <span>howpublished</span> = <span><span>{</span>\url{https://github.com/yandex/YaFSDP}<span>}</span></span>,
  <span>year</span> =         <span><span>{</span>2024<span>}</span></span>
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The misunderstood Kessler Syndrome (109 pts)]]></title>
            <link>https://aerospaceamerica.aiaa.org/features/understanding-the-misunderstood-kessler-syndrome/</link>
            <guid>40716235</guid>
            <pubDate>Tue, 18 Jun 2024 10:58:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aerospaceamerica.aiaa.org/features/understanding-the-misunderstood-kessler-syndrome/">https://aerospaceamerica.aiaa.org/features/understanding-the-misunderstood-kessler-syndrome/</a>, See on <a href="https://news.ycombinator.com/item?id=40716235">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <section id="content-start">
      


<hr>
<h2>Nation-states are blowing up satellites. Companies are launching megaconstellations of thousands of satellites. Dead rocket stages whiz around the planet for years. And yet, the International Space Station hasn‚Äôt been destroyed, payloads reach deep space unharmed, and we‚Äôre not trapped on Earth ‚Äî at least not by debris. Either calamity is not upon us or we just don‚Äôt recognize it. Jon Kelvey takes the measure of Kessler Syndrome.</h2><p><span>I</span><span>rony isn‚Äôt just limited to life in 1-g. Last year, a discarded payload adapter from a European Space Agency Vega rocket was orbiting Earth as it had for the past 10 years, when radars showed it had company ‚Äî a small number of new objects traveling with it. ESA concluded that a ‚Äúhypervelocity impact‚Äù with a piece of debris had broken fragments from the adapter. </span></p>
<p><span>Here‚Äôs the irony: ESA was preparing to dispatch a spacecraft to the adapter to demonstrate a technique for removing such debris, the goal being to reduce the odds of collisions that would make the trash problem worse.</span></p>
<p><span>The collision ‚Äúactually shows how much we are running against the clock here,‚Äù says Portuguese engineer Tiago Soares. He‚Äôs the lead engineer at ESA‚Äôs Clean Space office, which helped conceive the planned cleanup demonstration, ClearSpace-1.</span></p>
<p><span>‚ÄúWe need to have reactive removal services available,‚Äù he says. Otherwise, debris could continue colliding with the growing number of spacecraft in orbit, producing more hazardous debris, and ‚Äúit‚Äôs going to be much harder in the future to operate in space. There‚Äôll be a lot more risks of losing satellites or losing a mission.‚Äù</span></p>
<p><span>Soares is referring to the Kessler Syndrome, a term inspired by the 1978 paper, ‚ÄúCollision frequency of artificial satellites: The creation of a debris belt,‚Äù in the Journal of Geophysical Research. It was written by Donald Kessler, a researcher in NASA‚Äôs Environmental Effects Office at Johnson Space Center in Houston, and a colleague at NASA Johnson, space scientist Burton Cour-Palais. They devised equations to model the distribution of known objects in orbit, predicting how likely they were to collide and create orbital debris over time. </span></p>
<p><span>My review of the literature and interviews with seven experts ranging from aerospace engineers to planetary scientists to astrodynamacists show that the scientific community hasn‚Äôt yet reached a consensus about whether the Kessler Syndrome has begun, or, if it has not begun, how bad it will be when it starts. There is consensus, however, that the basic concept is sound and that the space community needs to clean up its act. </span></p>
<p><span>For the moment, even the definition of ‚ÄúKessler Syndrome‚Äù is open to debate. As Kessler, who retired decades ago, pointed out in a 2010 paper, it is ‚Äúan orbital debris term that has become popular outside the professional orbital debris community without ever having a strict definition.‚Äù </span></p>
<p><span>In 2013, the term went mainstream in dramatic fashion in the film ‚ÄúGravity.‚Äù A Russian missile destroys a satellite, spewing debris that collides with a space shuttle orbiter and the International Space Station, causing a cascade of collisions that take out all communications satellites in about 90 minutes.</span></p>
<p><span>That depiction was ‚ÄúKessler Syndrome on steroids that defies physics,‚Äùsays Abhishek Tripathi, director of mission operations at the UC Berkeley Space Sciences Lab. </span></p>
<p><span>That was one thing that all experts I spoke with agreed on: A Kessler Syndrome cascade is something that, whether it has begun or not, would play out over the course of decades if not centuries, rather than fitting into the runtime of a Hollywood drama. </span></p>
<p><span>For Tripathi‚Äôs part, he doesn‚Äôt think the syndrome has begun, and he doubts peaceful space operations will change that. </span></p>
<p><span>‚ÄúA lot of things have to go wrong for us to end up in a Kessler Syndrome situation by slowly boiling the frog,‚Äù he says, referring to predictions of a slowly unfolding cascade. But: ‚ÄúWe have the launch capacity to intentionally cause a Kessler Syndrome if we wanted to.‚Äù</span></p>
<p><span>Specifically, he worries about a nation-state kicking off a space war by knocking out satellites with missiles.</span></p>
<p><span>As for accidental collisions, meet physicist Mark Matney, whose first boss at NASA was none other than Kessler. Matney works in the Orbital Debris Program Office at NASA Johnson, and perhaps not surprisingly, he can rattle off the worst collisions to date. The most recent serious incident came in 2009 when an Iridium communications satellite and a Russian Cosmos satellite collided, generating some 2,000 pieces of debris at least 10 centimeters in diameter. </span></p>
<p><span>‚ÄúI tell people that‚Äôs a harbinger of things to come,‚Äù Matney says.</span></p>
<p><span>In his view, Iridium-Cosmos was ‚Äúthe opening move‚Äù of the Kessler Syndrome: It is one of several unplanned collisions that have occurred, as Kessler predicted, and debris from Iridium-Cosmos could cause future collisions that grow debris, he warns. It‚Äôs difficult, Matney says, to look at orbit right now and say for certain that a cascade is in motion, because of the long time frame over which Kessler Syndrome would play out. </span></p>
<p><span>‚ÄúI don‚Äôt think it‚Äôs acute yet,‚Äù he says, but ‚Äúwe‚Äôre on a timescale of something like a one in 10 chance each year of another major collision.‚Äù </span></p>
<p><span>He does not take comfort that it‚Äôs been 15 years and counting since the last big smashup ‚Äî even if the gap might seem surprising, considering that today there are some 8,000 operational satellites in space compared to around 1,000 in 2009 and 300 in 1978. There‚Äôs also much more debris, in part because of the nation-state actions that Tripathi worries about. India destroyed a satellite with a missile in 2019 to demonstrate an antisatellite weapon, and Russia conducted a similar demonstration in 2021. Together, the two tests generated a little over 1,500 pieces of debris. Following the Russian shootdown, the seven crew members aboard ISS had to temporarily take shelter in their Crew Dragon and Soyuz capsules, in case the station was struck. Regarding satellite proliferation, while there are many more satellites, the company responsible for most of them, SpaceX, places its Starlink satellites in a low orbit so they can naturally deorbit relatively soon ‚Äî within five or six years, per SpaceX ‚Äî if they fail.</span></p>
<p><span>A long gap in time between collisions is not unique. Thirteen years before the Iridium-Cosmos collision, the French CERISE satellite broke up after colliding with a piece of debris from an Ariane 1 rocket. In 1991, the Russian Cosmos 1934 satellite collided with debris and broke up in orbit. </span></p>
<p><span>Somewhere in the middle in terms of views about Kessler Syndrome is Vishnu Reddy, an Earth and space scientist at the University of Arizona, where he also directs Space4, the university‚Äôs space safety, security and sustainability center. He questions the timing, not the plausibility of the Kessler Syndrome. </span></p>
<p><span>‚ÄúI think we‚Äôre not there yet, but we‚Äôre approaching the situation very quickly,‚Äù he says. ‚ÄúThe debate is about when it will happen, whether it is five years from now, 10 years from now or 20 years from now.‚Äù </span></p>
<p><span>Rather than relying on Kessler‚Äôs original mathematics and analysis, the experts are applying newer mathematics and models to simulate how debris and spacecraft interact in orbit ‚Äî colliding or passing by each other ‚Äî and how the debris created by collisions or events like the spontaneous explosion of an old rocket motor may increase the odds of further collisions and debris growth. In Massachusetts, Richard Linares, an astrodynamicist and professor at MIT, and his colleagues load the masses, volumes and velocities of known spacecraft and debris into the MIT Orbital Capacity Assessment Tool. MOCAT then calculates the motions of orbital objects forward in time. </span></p>
<p><span>‚ÄúWe can calculate how many objects are generated from a breakup event‚Äù ‚Äî when an object explodes or disintegrates due to a collision ‚Äî ‚Äúand each object will have a size and a mass,‚Äù Linares says. ‚ÄúWe can probably go up to 20 million objects in our simulation.‚Äù</span></p>
<p><span>MOCAT also runs Monte Carlo algorithms that take the positions and velocities of thousands of objects in orbit, among other information, and create myriad random simulations to show a range of potential future debris scenarios based on various parameters entered by the scientists. These include the number of rockets projected to reach orbit in a given year and a hypothetical number of major collisions between satellites occurring in the next 20 years. </span></p>
<p><span>‚ÄúIn those models, we see that we could have exponential growth [of debris] if the space traffic is too large,‚Äù Linares says. </span></p>
<p><span>That exponential growth is what could make accessing space a more risky and expensive affair, where certain orbital lanes could become so clogged they are no longer worth trying to operate within.</span></p>
<p><span>ESA and NASA use similar analytical approaches, ESA with its Debris Environment Long-Term Analysis software and NASA with its LEO-to-GEO Environment Debris software. Soares says ESA‚Äôs calculations suggest that orbital debris will continue to grow over the next two centuries even if all rocket launches stopped today. </span></p>
<p><span>‚ÄúIt would more than double the number of debris in orbit without us sending anything else up there,‚Äù he says.</span></p>
<p><span>In contrast, NASA‚Äôs modeling doesn‚Äôt predict exponential debris growth, according to Matney, but rather linear growth over the next 200 years ‚Äî even if launches continue. But things get more complicated when you factor in that LEO isn‚Äôt a monolithic expanse, he says, and ‚Äúin some altitude regions [debris growth] is exponential, some linear.‚Äù </span></p>
<p><span>At around 400 kilometers and into the 500-km realm ‚Äî home to ISS and the SpaceX Starlink satellites among others ‚Äî atmospheric drag plays a major role. Dead satellites and debris usually slow and burn up in the atmosphere in just a few years. This natural cleansing process accelerates when the sun becomes more active and solar coronal mass ejections strike Earth and cause the atmosphere to swell. </span></p>
<p><span>‚ÄúIn those altitudes, we can probably do a lot and we will be forgiven,‚Äù Linares says. </span></p>
<p><span>But this atmospheric drag drops off quickly as one goes higher. By the time you get around 600 km, the altitude of the Hubble Space Telescope, ‚Äúnow you‚Äôre talking about decades for things to drag down,‚Äù Matney says. </span></p>
<p><span>‚ÄúWhen you get up to 800 or 900 km, we‚Äôre now talking about centuries for things to drag down,‚Äù he adds. ‚ÄúWhen we get up to 1,000 km, you‚Äôre talking about millennia.‚Äù </span></p>
<p><span>A lot of satellites fly in those higher altitudes already, including some 70 Iridium communications satellites, at least 600 OneWeb broadband internet satellites and NASA‚Äôs Earth-observation spacecraft, including Landsat 8 and 9. </span></p>
<p><span>‚ÄúIf there are collision events,‚Äù Linares says, ‚Äúthose altitudes can very quickly turn into a Kessler type of scenario where they grow very rapidly into millions upon millions [of pieces] of debris.‚Äù</span></p>
<p><span>However, humanity would not be ‚Äúlocked in‚Äù on Earth in such an event, he says, given that crewed spacecraft headed for deep space would cross the problematic altitudes so quickly. But there are still plausible scenarios that are far from ideal. Linares sees a potential future where ‚Äúhumans probably don‚Äôt have any incentive to launch satellites, because we‚Äôre losing 50% of them‚Äù to collisions with debris, he says. </span></p>
<p><span>Matney puts it like this: Kessler Syndrome ‚Äúwon‚Äôt cause orbital altitudes to be unusable. It‚Äôs more like a gradual degradation that‚Äôs going to cost everybody more money.‚Äù </span></p>
<p><span>The prospect of losing money leads naturally to the question of just what should be done about orbital debris, and there are two generally agreed-upon major actions to take. </span></p>
<p><span>‚ÄúThe first step is to make sure that new junk doesn‚Äôt get added,‚Äù says Reddy, the University of Arizona professor. ‚ÄúThe second step is to remove large pieces of junk that have the potential to be the sources of cascading debris events in a Kessler Syndrome scenario.‚Äù </span></p>
<p><span>Not creating new debris is largely a matter of adhering to the rule in the United States and Europe that satellites must not stay in orbit longer than five years after their mission is completed. </span></p>
<p><span>‚ÄúIt‚Äôs the stuff you learn in kindergarten: You clean up your messes; don‚Äôt hurt your neighbor,‚Äù Matney says. </span></p>
<p><span>Removing existing space junk could have the biggest impact on the debris curve. </span><span>‚ÄúIt depends on how much remediation you do, but in principle, it actually flattens it out and starts letting it go down,‚Äù he says. </span></p>
<p><span>That‚Äôs why ESA isn‚Äôt giving up on ClearSpace-1, or the idea behind the mission. Soares, the lead engineer, notes that ESA had its Copernicus satellites built with interfaces so they can be grabbed and redirected to deorbit should they fail. Eventually, he says, the goal is to build spacecraft that can grapple and remove large objects that pose a major debris hazard, such as ESA‚Äôs multiton Envisat, which died in 2012. </span></p>
<p><span>‚ÄúIt‚Äôs not easy because the satellite was not at all designed to be removed,‚Äù Soares says. </span></p>
<p><span>No matter which view of the Kessler Syndrome one adheres to, the risk it describes might not be a fait accompli. ‚ÄúThis is a problem that we have the capability and, hopefully, the willpower to solve,‚Äù says Matney. </span></p>
    <hr>
    
    <section>
      <p><img data-del="avatar" src="https://aerospaceamerica.aiaa.org/wp-content/uploads/2022/05/Jon-Kelvey.jpg" width="400">      </p>
      <div>  
        <h2>About Jon Kelvey </h2>
        <p> Jon previously covered space for The Independent in the U.K. His work has appeared in Air and Space Smithsonian, Slate and the Washington Post. He is based in Maryland.</p>
      </div>
    </section>
  
<section><div id="additional-image-1"><p><img data-src="https://aerospaceamerica.aiaa.org/wp-content/uploads/2024/02/ClearSpace-1.jpg" alt="A spacecraft orbits above Earth with a view of the planet's curvature and atmospheric layers. Sunlight beams into the scene from the top." src="https://aerospaceamerica.aiaa.org/wp-content/uploads/2024/02/ClearSpace-1.jpg"></p><p>The European Space Agency and ClearSpace of Switzerland are targeting 2026 for a debris removal mission, in which a long-discarded payload adapter, at right in this illustration, will be grappled by a ClearSpace-made spacecraft equipped with an ESA-developed robotic arm.  Credit: ClearSpace</p></div><div id="additional-image-2"><p><img data-src="https://aerospaceamerica.aiaa.org/wp-content/uploads/2024/02/Hubble-array-closeup.jpg" alt="Close-up of a damaged guitar with a large, irregular hole on the top, surrounded by cracked wood and chipped paint, adjacent to several strings." src="https://aerospaceamerica.aiaa.org/wp-content/uploads/2024/02/Hubble-array-closeup.jpg"></p><p>Today, many spacecraft experience some form of collision with micrometeoroids or minute pieces of debris. These solar cells from the Hubble Space Telescope, installed in 1993 and brought back to Earth by NASA astronauts in 2002, experienced multiple impacts during their nearly nine years on orbit.  Credit: European Space Agency</p></div><div id="additional-image-3"><p><img data-src="https://aerospaceamerica.aiaa.org/wp-content/uploads/2024/02/Starlink-stack.jpeg" alt="Close-up view of a large industrial machine with rows of black components and red caps, captured from a low angle." src="https://aerospaceamerica.aiaa.org/wp-content/uploads/2024/02/Starlink-stack.jpeg"></p><p>SpaceX in February said it plans to deorbit 100 of the early satellites in its Starlink broadband constellation, due to an unspecified ‚Äúcommon issue‚Äù with these satellites ‚Äúthat could increase the probability of failure in the future.‚Äù Almost 6,000 Starlinks have been launched since 2019, including some 1,300 of the v2 Mini variant pictured here. Credit: SpaceX</p></div></section>    </section>

      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chat Control: Incompatible with Fundamental Rights (788 pts)]]></title>
            <link>https://freiheitsrechte.org/en/themen/digitale-grundrechte/chatkontrolle</link>
            <guid>40715695</guid>
            <pubDate>Tue, 18 Jun 2024 09:34:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://freiheitsrechte.org/en/themen/digitale-grundrechte/chatkontrolle">https://freiheitsrechte.org/en/themen/digitale-grundrechte/chatkontrolle</a>, See on <a href="https://news.ycombinator.com/item?id=40715695">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="intro">
    <p>
      
      The EU Commission has presented a draft regulation that is to lay down rules for preventing and combating sexual violence against children (Chat Control Regulation). The planned regulation raises such significant fundamental rights concerns that the GFF is joining the debate while the draft is still being deliberated at EU level. The most important points of criticism at a glance.
    </p>
  </div><div id="g185914">
      
                
  
                                                      <figure id="c186055" role="figure" aria-labelledby="c186055-caption">
  <template><iframe width="200" height="113" src="https://www.youtube.com/embed/XFwdeB_h_yw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Chatkontrolle: Felix Reda erkl√§rt die f√ºnf gr√∂√üten Gefahren f√ºr unsere Grundrechte"></iframe></template>

  <div>
              <div>
          <picture>
  <img src="https://freiheitsrechte.org/external-images/__800x452_crop_center-center_none/https-i-ytimg-com-vi-xfwdeb-h-yw-hqdefault-jpg-3f056f.pjpeg" width="800" height="452" loading="lazy" role="presentation">
</picture>
        </div>
      
      <div>
        <p><strong>External Content from YouTube</strong></p>
        <p>Please see the privacy policy of YouTube if you are loading external content.</p>

        
      </div>
    </div>

      <figcaption id="c186055-caption">Felix Reda explains the five biggest threats for our fundamental rights</figcaption>
  </figure>
                      <p>The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52022PC0209&amp;from=EN" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>EU Commission's draft regulation on chat control</span></a> is currently being negotiated in the European Parliament and the Council of Ministers. With the fight against sexual violence against children, the draft pursues an objective that is essential for the protection of children and their rights and can justify restrictions of fundamental rights. However, there are considerable doubts about the effectiveness of the proposed measures. We are convinced that the draft violates the EU Charter of Fundamental Rights in crucial points. We have summarised the five most important fundamental rights objections to the chat control proposal here. </p>

<h3 id="chat-control-violates-the-right-to-privacy">
  <span>Chat control violates the right to privacy</span></h3>
<p>The EU Commission's proposal provides for a whole range of obligations for certain online services such as internet access providers, app stores, hosting platforms and interpersonal communications services. Interpersonal communications services are, for example, email services such as GMail or instant messaging services such as WhatsApp. The term ‚Äúchat control" is often used colloquially to refer to the EU Commission‚Äôs draft regulation as a whole. Chat control in the narrower sense is the part of the draft according to which authorities can oblige providers of communications services such as WhatsApp to monitor private communications. This is a particularly serious restriction on the right to privacy and the protection of personal data (Art. 7 and 8 of the EU Charter of Fundamental Rights): The monitoring is not limited to persons specifically suspected of having committed a crime. Additionally, unlike data retention, which is also incompatible with the Charter but is limited to metadata ‚Äì i.e. information about who communicated with whom at what time ‚Äì chat control includes the surveillance of the contents of private messages.</p>
<p>Authorities can impose so-called "detection orders" against providers of interpersonal communications services. This means that authorities can, for example, oblige messenger services to monitor the communications of all their users. It is sufficient that the authority has identified a significant risk that the service in question is being used for the dissemination of depictions of sexual violence against children. Detection orders do not have to be limited to monitoring the communications of specific users who are under suspicion. Instead, authorities can order that the content of all communications of all users of the service be monitored preventively. This is therefore a form of <a href="https://verfassungsblog.de/my-spy-is-always-with-me/" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>mass surveillance without probable cause</span></a>.</p>
<p>Such a detection order can oblige service providers to filter content for known as well as unknown depictions of sexual violence against children. In addition, they can include an obligation to detect attempts by adults to solicit minors (grooming). Content detected in this way must be forwarded by the service providers to a newly created EU centre, which will pass the information on to the law enforcement authorities of the member states after a plausibility check. Although service providers are free to choose which technologies they use to comply with the detection order, these technologies must in any case be able to analyse the contents of communications. In order to detect known depictions of sexual violence against children, an automated comparison of sent media files with a reference database may be sufficient. To detect unknown depictions of sexual violence and grooming, machine learning must be used to analyse the semantic content of chats. These methods are particularly prone to error: they only make an assumption about the meaning of the content based on patterns in the analysed communication - without actually understanding the content or the context of the conversation. In its case law on data retention, the European Court of Justice has indicated that indiscriminate mass surveillance of the contents of communications would <a href="https://verfassungsblog.de/thank-you-very-much-your-mail-is-perfectly-fine/" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>violate the essence of the right to privacy</span></a>. </p>
<p>Indiscriminate mass surveillance is incompatible with the fundamental rights to privacy and data protection under the EU Charter, whether it involves encrypted or unencrypted communications. At the centre of public criticism of chat control, however, is the fact that the draft regulation does not exempt end-to-end encrypted communication services from detection orders. These services ensure that only the people involved in a private conversation can read the communication content ‚Äì neither the service provider nor third parties can decrypt it. More and more people are specifically choosing end-to-end encrypted messengers to protect themselves. If the provider of such a messenger receives a detection order, it cannot reject it on the grounds that the service provider cannot access the contents of its users‚Äô communications. The EU Commission's draft pays lip service to the importance of end-to-end encryption. However, service providers may only choose between technologies that allow them to detect illegal content in private communications, it states. In other words, service providers who offer end-to-end encryption without backdoors will not be able to implement any detection orders they may receive from authorities and thus come into conflict with the law. This attack on end-to-end encryption increases the intensity of the restriction of fundamental rights caused by indiscriminate mass surveillance.</p>
<h3 id="threat-of-chilling-effects-for-communication-freedoms">
  <span>Threat of chilling effects for communication freedoms</span></h3>
<p><a href="https://curia.europa.eu/juris/document/document.jsf?text=&amp;docid=232084&amp;pageIndex=0&amp;doclang=EN&amp;mode=lst&amp;dir=&amp;occ=first&amp;part=1&amp;cid=1248552" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>The European Court of Justice has already warned</span></a> on several occasions that indiscriminate mass surveillance has an indirect negative impact on freedom of expression (Article 11 of the EU Charter of Fundamental Rights): communication participants are prevented from freely expressing their opinions if they cannot be sure of the confidentiality of their communications. This particularly affects professional secrecy holders, such as journalists communicating with their sources, whistleblowers and opposition activists. This danger will be exacerbated if the Chat Control Regulation, as proposed by the EU Commission, attacks the end-to-end encryption of messenger services. The aforementioned groups of people use such messengers for good reason. If this possibility is taken away from them because service providers have to weaken end-to-end encryption, considerable "chilling effects", i.e. a deterrent effect for the exercise of the fundamental right to freedom of expression and information, can be expected.</p>
<p>This effect occurs regardless of whether service providers monitor the contents of private communications through a backdoor in the encryption technology or by scanning the content on the user's device before it is encrypted (client-side scanning). The communication participants expect their communication to remain confidential from the moment when they enter a message into the chat programme on their mobile phone ‚Äì not only at the moment when this message is delivered to its addressee. The decisive factor is that the expectation of confidentiality and integrity of the communication process is shaken to such an extent that those affected feel compelled to restrict the exercise of their freedom of communication themselves. </p>
<h3 id="de-facto-filtering-obligations-for-hosting-providers-without-safeguards">
  <span>De facto filtering obligations for hosting providers without safeguards</span></h3>
<p>Public criticism of the proposal has concentrated on the phrase ‚Äúchat control‚Äù, which highlights the planned obligations on messengers to scan private chats. But the planned obligations for hosting services that store third-party content on behalf of their users do not stand up to fundamental rights scrutiny either. Hosting services include those that make third-party content publicly available (platforms such as YouTube, hosting services of public websites) as well as those that offer their customers private cloud storage (Dropbox, iCloud Drive). They also include services where content is only accessible to a certain closed group of people (private accounts on Twitter, closed groups on Facebook, hosting providers of company websites with restricted access). Insofar as the planned obligations for hosting providers relate to non-public content, the threats to privacy and freedom of expression described under 1. and 2. are also relevant for hosting services. In addition, there are specific problems: many of the envisaged procedural fundamental rights safeguards for detection orders may end up being evaded entirely in the case of hosting services. This is due to the different privacy rules for communications on messengers on the one hand and hosting services on the other. </p>
<p>Hosting services (including private cloud storage providers such as Google Drive or Dropbox) can not only be required to scan private content under the Chat Control Regulation, they may also scan content voluntarily. The Chat Control Regulation stipulates that all service providers must first carry out their own risk analysis as to whether their services pose a risk of being abused for sexual violence against children. Only if, in the view of the authorities, a service provider responds to this risk analysis with insufficient voluntary measures will they impose a detection order. In the context of these self-selected measures, hosting service providers may resort to error-prone filters to monitor private user uploads. In this scenario, there is no public scrutiny of the impact of such measures on the fundamental rights of users.</p>
<p>In this respect, hosting services differ from messenger services: Messenger and email programmes such as Whatsapp, Signal or ProtonMail fall under the e-Privacy Directive, which in principle prohibits these service providers from monitoring the private communication content of their users. A temporary derogation from this prohibition, which itself raises <a href="https://www.patrick-breyer.de/wp-content/uploads/2021/03/Legal-Opinion-Screening-for-child-pornography-2021-03-04.pdf" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>serious fundamental rights concerns</span></a>, is to be replaced by the Chat Control Regulation. After the Chat Control Regulation comes into force, messengers and email service providers may only access the contets of private communications on the basis of a detection order. For hosting providers such as private cloud storage, on the other hand, the e-Privacy Directive with its ban on monitoring private communications does not apply. </p>
<p>For hosting providers, it will regularly be attractive to avoid a looming detection order through "voluntary" measures. In this way, the companies retain more control ‚Äì also over the costs. There is a strong incentive to avoid costly measures to protect users' fundamental rights. A likely scenario, then, is that hosting services will 'voluntarily' deploy error-prone filtering programmes without the procedural safeguards foreseen for authorities‚Äô detection orders.</p>
<p>Before imposing a detection order, an authority must weigh the risk posed by the service against the interference with the users' fundamental rights. In this regard, the European Court of Justice has set narrow limits for the mandatory <a href="https://curia.europa.eu/juris/liste.jsf?language=en&amp;num=C-401/19" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>use of filtering systems</span></a>. These are only compatible with the prohibition of general monitoring obligations if the filters function so faultlessly that the service providers do not have to perform an "independent assessment of the content" in order to rule out false positives. At least in the case of unknown depictions of sexual violence against children and grooming, the filter systems are incapable of meeting the Court‚Äôs standards. If a hosting service "voluntarily" filters content as part of its duty to minimise risk, there is no public assessment of whether the filtering systems are compatible with users‚Äô fundamental rights. As a result, innocent users may <a href="https://www.nytimes.com/2022/08/21/technology/google-surveillance-toddler-photo.html" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>inadvertently locked out of their accounts or even falsely reported to law enforcement authorities</span></a>.</p>
<h3 id="website-blocking-obligations-require-surveillance-of-internet-users">
  <span>Website blocking obligations require surveillance of Internet users</span></h3>
<p>The draft regulation provides for blocking obligations on internet access providers relating to individual websites (URLs). Before an authority issues a blocking order, it must require internet access providers to provide the authority with information about users' access to the URL in question. To be able to collect the necessary information about the access to individual URLs and pass it on to the authorities, internet access providers would have to monitor the surfing behaviour of all their customers preventively and comprehensively. However, such surveillance would be incompatible with the prohibition on general monitoring obligations and with the fundamental right to privacy. Additionally, this information is technically inaccessible to the internet access providers if the URL is encrypted using the https protocol. Almost all websites now use https to ensure that, for example, address or credit card data that users enter into web forms is transmitted in encrypted form. The widespread use of https is <a href="https://www.bsi.bund.de/DE/Themen/Verbraucherinnen-und-Verbraucher/Informationen-und-Empfehlungen/Cyber-Sicherheitsempfehlungen/cyber-sicherheitsempfehlungen_node.html" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>recommended by the Federal Office for Information Security</span></a>.</p>
<p>The targeted blocking of individual URLs is equally impossible for internet access providers <a href="https://edpb.europa.eu/news/news/2022/proposal-combat-child-sexual-abuse-online-presents-serious-risks-fundamental-rights_en" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>without abandoning https encryption</span></a> and monitoring the contents of their users‚Äô online activities. DNS-based website blocking is not suitable for the planned blocking of individual URLs, because DNS blocking always affects entire domains. A DNS block directed against an individual file on a share hosting platform would also affect all other content hosted by the same share hoster and would thus not meet the <a href="https://curia.europa.eu/juris/document/document.jsf?text=&amp;docid=149924&amp;pageIndex=0&amp;doclang=EN&amp;mode=lst&amp;dir=&amp;occ=first&amp;part=1&amp;cid=279088" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>requirement of the European Court of Justice that website blocking must be strictly targeted</span></a>. In practice, therefore, there is a considerable danger that internet access providers will either over-comply with the blocking orders to the detriment of users‚Äô freedom of expression and information by using DNS blocking to block access to an entire domain. Or they will attempt to implement more targeted blocking and monitor the surfing behaviour of their customers, while sacrificing the security of online communications via https encryption in the process.</p>
<h3 id="age-verification-endangers-freedom-of-communication">
  <span>Age verification endangers freedom of communication</span></h3>
<h3>
  <span> </span></h3>
<p>The draft regulation stipulates that all providers of messenger and email services that are at risk of being used for grooming must verify the age of their users. The risk identified does not have to be significant ‚Äì the obligation to implement age verification would therefore apply in principle to all email and messaging services that enable communication between minors and adults. In addition, the age verification obligation also applies to all app store providers. They must also prevent underage users from downloading apps that pose a significant risk of being used for grooming.</p>
<p>Service providers may choose between age assessment methods (for example, AI-based facial analysis, <a href="https://about.instagram.com/blog/announcements/new-ways-to-verify-age-on-instagram" target="_blank" rel="noopener noreferrer"><svg viewBox="0 0 200 200" role="presentation" focusable="false"><use xlink:href="https://freiheitsrechte.org/assets/icons/icons.ce777a03f1.svg#icon-external-link"></use></svg><span>as already used by Instagram</span></a>) and age verification methods (using an identity document or digital proof of identity). Both procedures are extremely intrusive for users. Age verification via identity documents comes close to banning anonymous internet use. AI-supported facial analysis, on the other hand, is often outsourced by service providers to external companies, leaving users with little control over the handling of this particularly sensitive personal data. If the technology makes a wrong assessment, young-looking adults can also be excluded from using certain apps. Those who do not possess identification documents or do not want to entrust their biometric data to a company are excluded from crucial communication technologies. Using a modern smartphone without an app store is hardly possible. Doing without messenger services is also unreasonable, especially for people who, for good reason, attach particular importance to anonymous internet use (whistleblowers, victims of stalking, politically persecuted people). In contrast to service providers, users cannot always choose between different age verification procedures. </p>
<p>For underage users (especially teenagers), their fundamental rights to freedom of expression and information are severely restricted if app stores categorically refuse to allow them to install certain apps without weighing these rights against the risk the app poses to underage users. Due to the <a href="https://freiheitsrechte.org/themen/freiheit-im-digitalen/grundrechte-im-digitalen"><span>strong market concentration in this area</span></a>, the possibilities to switch to an alternative app store are limited. </p>



            <figure id="c209972" role="figure">
  <div>
          <picture>
  <img src="https://freiheitsrechte.org/uploads/documents/_content_xl/Arcadia-Logo-Logotype-yellow-on-transparent_2023-03-09-110110_apny.jpg" srcset="https://freiheitsrechte.org/uploads/documents/_content_l/Arcadia-Logo-Logotype-yellow-on-transparent_2023-03-09-110110_apny.jpg 500w, https://freiheitsrechte.org/uploads/documents/_content_xl/Arcadia-Logo-Logotype-yellow-on-transparent_2023-03-09-110110_apny.jpg 1024w" width="1024" height="295" loading="lazy" role="presentation">
</picture>
      </div>

  </figure>
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chat Control Must Be Stopped ‚Äì Now (871 pts)]]></title>
            <link>https://threema.ch/en/blog/posts/stop-chat-control</link>
            <guid>40715449</guid>
            <pubDate>Tue, 18 Jun 2024 08:49:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://threema.ch/en/blog/posts/stop-chat-control">https://threema.ch/en/blog/posts/stop-chat-control</a>, See on <a href="https://news.ycombinator.com/item?id=40715449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ios-wrapper"><main role="main"><section id="article-604">
	

	
	
	<p><img src="https://threema.ch/blog/content/blog-chat-control.png" alt="Chat Control Must Be Stopped ‚Äì Now!"></p>	<article>
		<p><b>With its legislative proposal known as ‚ÄúChat Control,‚Äù the EU Commission is trying to establish an unprecedented mass-surveillance apparatus of Orwellian proportions in the European Union. If EU citizens don‚Äôt stand up for privacy now, it may be too late.</b></p>

<p>This Wednesday, June 19, 2024, the EU Council <a href="https://www.patrick-breyer.de/en/council-to-greenlight-chat-control-take-action-now/">could be</a> voting on the controversial Chat Control bill. Should it pass, the consequences would be devastating: Under the pretext of child protection, EU citizens would no longer be able to communicate in a safe and private manner on the Internet. The European market‚Äôs location advantage would suffer a massive hit due to a substantial decrease in data security. And EU professionals like lawyers, journalists, and physicians could no longer uphold their duty to confidentiality online. All while children wouldn‚Äôt be better protected in the least bit. On the contrary, Chat Control could have a negative impact on minors in particular.</p>

<div id="flap-content">
<p>The EU Commission‚Äôs proposed Child Sexual Abuse Regulation (commonly known as ‚ÄúChat Control‚Äù) aims to combat the circulation of CSAM (child sexual abuse material) on digital platforms by requiring service providers to implement some sort of detection mechanism that automatically scans users‚Äô (media) messages for both known and potential CSAM and reports detected cases to authorities.</p>
</div>

<p>It doesn‚Äôt matter how the EU Commission is trying to sell it ‚Äì as ‚Äúclient-side scanning,‚Äù ‚Äúupload moderation,‚Äù or ‚ÄúAI detection‚Äù ‚Äì, Chat Control is still mass surveillance. And regardless of its technical implementation, mass surveillance is always an incredibly bad idea, for a whole plethora of reasons. Here are just three:</p>

<div id="info-1">
            <p>One distinguishing factor between totalitarian states and democracies is that only in the former can the government invade citizens‚Äô privacy for no apparent reason. Modern democracies recognize privacy as a basic human right. The EU itself acknowledges it in the <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:12012P/TXT&amp;from=EN">Charter of Fundamental Rights</a> (article 7):</p>

            <blockquote>
                <p><i>Everyone has the right to respect for his or her private and family life, home and communications.</i></p>
            </blockquote>

            <p>Imagine, for example, the police could randomly enter your home without having any reason to believe you‚Äôre involved in illegal activities, simply to snoop around and see whether they can find something suspicious by sheer chance.
            </p>

            <p>In a healthy democracy, it should be the citizens who oversee the government ‚Äì mass surveillance is the inversion of this democratic principle. With a measure like Chat Control, the EU would violate one of its own basic rights and put its citizens under general suspicion, thereby profoundly disrupting the trust between citizens and the government.
            </p>
        </div>

<div id="info-2">
            <p>Mass surveillance of standard communication channels like instant messengers only affects law-abiding citizens. Given their involvement in illegal activities, criminals will go to any length to avoid surveillance. After all, why would any criminal continue to use a communication channel that‚Äôs known to be under government surveillance?  
            </p>
            <p>However, flying under Chat Control‚Äôs radar wouldn‚Äôt even require resorting to some other, obscure means of communication that can‚Äôt be (or isn‚Äôt yet) surveilled. Criminals could, for example, simply manually encrypt illegal content before sharing it.
            </p>
            <p>Ordinary, unsuspecting Internet users, for whom it wouldn‚Äôt be practical to handle day-to-day communication with friends and family members in this manner, would therefore be the only ones really affected by mass surveillance. And besides <a href="https://www.spiegel.de/netzwelt/netzpolitik/kinderpornografie-zahl-der-falschen-verdaechtigungen-bei-online-bildern-massiv-gestiegen-a-a746b118-82e7-4560-8ba4-45f02489768c">inevitable false positives</a> (think family photos of a beach vacation), minors (i.e., the ones who should be protected) engaging in consensual ‚Äúsexting‚Äù would probably amount for the vast majority of CSAM reports.
            </p>
        </div>

<div id="info-3">
            <p>It‚Äôs not just citizens‚Äô privacy that would suffer from the proposed mass surveillance. Because Chat Control essentially requires communication services to install a backdoor, it‚Äôs also citizens‚Äô security that would take a massive hit.</p>
            <p>The reason secure communication services like Threema employ end-to-end encryption is to make sure no one except the intended recipient is able to read a message, not even the service provider. Introducing a backdoor into such a system is like adding a weak link to a strong chain lock. Sure, the government can now open the lock without a key, but so can any burglar.
            </p>
            <p>The negative impact a backdoor has on security can hardly be overstated. It‚Äôs not just the first place anyone would try to penetrate a system that‚Äôs otherwise secure, it‚Äôs like an API for hackers. And EU interior ministers seem to be well aware of this: why else would <a href="https://www.eureporter.co/business/data/mass-surveillance-data/2024/04/15/leak-eu-interior-ministers-want-to-exempt-themselves-from-chat-control-bulk-scanning-of-private-messages/">they themselves want to be exempt from Chat Control‚Äôs surveillance?</a>
            </p>
        </div>

<p>Of course, sharing CSAM is an absolutely intolerable, horrific crime that must be punished. Before CSAM can be shared online, however, a child must have suffered abuse in real life, which is what effective child protection should be trying to prevent (and what Chat Control does not focus on). For this and many other reasons, child protection organizations such as Germany‚Äôs Federal Child Protection Association are against Chat Control, <a href="https://www.bundestag.de/resource/blob/935798/3c6f75f6c2056130bca757bf7b4d0445/Stellungnahme-Tuerk-data.pdf">arguing that it‚Äôs ‚Äúneither proportionate nor effective</a>.‚Äù</p>

<p>Besides, there‚Äôs no way of really knowing whether Chat Control would actually be (or remain) limited to CSAM. Once the mass-surveillance apparatus is installed, it could easily be extended to detect content other than CSAM without anyone noticing it. From a service provider‚Äôs point of view, the detection mechanism, which is created and maintained by third parties, essentially behaves like a black box.</p>

<h3>What can you do?</h3>

<p>Since the matter may be decided this Wednesday, June 19, 2024, time is a critical factor. If you‚Äôre a EU citizen, please consider <a href="https://www.patrick-breyer.de/en/council-to-greenlight-chat-control-take-action-now/">contacting your government‚Äôs representative <i>today</i></a>, asking them to vote against Chat Control.</p>

<p>It may also help to take to the digital streets, spread the word online, and raise awareness for the EU‚Äôs dubious plan to establish an unprecedented mass-surveillance apparatus that would essentially nullify the right to data privacy and set a highly dangerous precedent in doing so.</p>

<div>
    <h4>What would Chat Control mean for Threema users in the EU?</h4>
    <p>While Threema would be subject to Chat Control, the business solution Threema Work would be out of scope according to our current knowledge. However it‚Äôs still not entirely clear how Chat Control would have to be implemented by service providers, and it‚Äôs questionable whether such a blatant violation of the right to privacy would hold up in European courts.
    </p>
    <p>What is crystal clear, however, is that there will never be a Threema version that‚Äôs spying on its users in any way, shape, or form. The reason Threema was created is to provide a highly secure, completely private, and anonymous means of communication. Once it‚Äôs no longer possible to offer such a service in the European Union, we will be forced to take consequences.
    </p>
    <p>We will carefully consider all options (including legal actions, technical workarounds, etc.) first, and if we come to the conclusion that there‚Äôs no other way, we‚Äôll call on fellow communication services to join us in leaving the EU.
    </p>
</div>
	</article>
	
</section>



</main>



	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cyber Scarecrow, making your computer look 'scary' to malware (549 pts)]]></title>
            <link>https://www.cyberscarecrow.com/</link>
            <guid>40715250</guid>
            <pubDate>Tue, 18 Jun 2024 08:08:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cyberscarecrow.com/">https://www.cyberscarecrow.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40715250">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img alt="scarecrow logo" loading="lazy" width="80" height="80" decoding="async" data-nimg="1" srcset="https://www.cyberscarecrow.com/_next/image?url=%2Fscarecrow_128.ico&amp;w=96&amp;q=75 1x, https://www.cyberscarecrow.com/_next/image?url=%2Fscarecrow_128.ico&amp;w=256&amp;q=75 2x" src="https://www.cyberscarecrow.com/_next/image?url=%2Fscarecrow_128.ico&amp;w=256&amp;q=75"></p><p>Software that runs in the background of your computer, which makes it look 'scary' to viruses and malware.</p><div><p><a href="https://www.cyberscarecrow.com/download">Download Scarecrow</a><a href="https://www.cyberscarecrow.com/about">How does it work? </a></p></div><br></div></div>]]></description>
        </item>
    </channel>
</rss>