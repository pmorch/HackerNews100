<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 06 Jun 2025 02:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Tokasaurus: An LLM Inference Engine for High-Throughput Workloads (110 pts)]]></title>
            <link>https://scalingintelligence.stanford.edu/blogs/tokasaurus/</link>
            <guid>44195961</guid>
            <pubDate>Thu, 05 Jun 2025 21:27:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scalingintelligence.stanford.edu/blogs/tokasaurus/">https://scalingintelligence.stanford.edu/blogs/tokasaurus/</a>, See on <a href="https://news.ycombinator.com/item?id=44195961">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="center">
          
          

          

          <hr>

          
            
              <p><img src="https://scalingintelligence.stanford.edu/imgs/teasers/tokasaurus.png" alt="">
                

              </p>
            

            <h2 id="tldr">TL;DR</h2>

<p>We’re releasing Tokasaurus, a new LLM inference engine optimized for throughput-intensive workloads. With small models, Tokasaurus benefits from very low CPU overhead and dynamic <a href="https://arxiv.org/abs/2402.05099">Hydragen</a> grouping to exploit shared prefixes. For larger models, Tokasaurus supports async tensor parallelism for GPUs with NVLink and a fast implementation of pipeline parallelism for GPUs without. On throughput-focused benchmarks, Tokasaurus can outperform vLLM and SGLang by up to 3x+.</p>

<hr>

<h2 id="table-of-contents">Table of Contents</h2>

<ul>
  <li><a href="#intro">Intro</a></li>
  <li><a href="#optimizing-small-models">Optimizing Small Models</a></li>
  <li><a href="#optimizing-bigger-models">Optimizing Big Models</a></li>
  <li><a href="#try-it-out">Try it Out</a></li>
  <li><a href="#benchmarking-details">Benchmarking Details</a></li>
  <li><a href="#acknowledgements">Acknowledgements</a></li>
</ul>

<hr>

<h2 id="intro">Intro</h2>

<p>As LLMs get <a href="https://www.youtube.com/watch?v=gAjR4_CbPpQ">smarter, faster, and cheaper,</a> the community keeps finding new ways to use them. Our own recent work has explored using models to <a href="https://arxiv.org/abs/2501.14723">scan every file in a codebase</a>, <a href="https://arxiv.org/abs/2407.21787">sample 10,000 attempts for math and code problems</a>, and <a href="https://arxiv.org/abs/2502.15964">collaborate with other models to minimize cloud costs</a>. Inference is now also an important part of the training process, where we use models to <a href="https://arxiv.org/abs/2404.14219">generate synthetic data</a> or as part of <a href="https://arxiv.org/abs/2504.04736">RL pipelines</a> that generate and train on model completions.</p>

<p>Crucially, these new inference workloads look quite different than the original LLM use case of serving a chatbot. Here, we care primarily about the total time and cost required to complete a large batch of sequences, and we care much less (if at all) about the individual latency of a single generation. In other words, we want high throughput!</p>

<p>Open-source inference engines (i.e. dedicated systems for running efficient LLM inference) like <a href="https://arxiv.org/abs/2303.06865">FlexGen</a>, <a href="https://arxiv.org/abs/2309.06180">vLLM</a>, and <a href="https://arxiv.org/abs/2312.07104">SGLang</a> have been enormously valuable to the community. Inspired by and learning from these projects, we built a new engine, <a href="https://github.com/ScalingIntelligence/tokasaurus/tree/main">Tokasaurus</a>, designed from the ground up to handle throughput-focused workloads. We’ve optimized Tokasaurus for efficiently serving large and small models alike, allowing it to outperform existing engines on throughput benchmarks. In the rest of this blog, we’ll walk through some of these optimizations and show off a few settings where Tokasaurus really shines.</p>

<hr>

<h2 id="optimizing-small-models">Optimizing Small Models</h2>

<p>To benchmark Tokasaurus with small models, we’ll use two workloads:</p>

<ul>
  <li>Completing chatbot prompts from the ShareGPT dataset (this is a common benchmark for testing inference engines).</li>
  <li>Reproducing an experiment from <a href="https://arxiv.org/abs/2407.21787">Large Language Monkeys</a>, where we take 128 problems from the <a href="https://arxiv.org/abs/2110.14168">GSM8K</a> math dataset and sample 1024 answers to every problem. The distinguishing feature of this workload is that there’s a lot of prefix sharing across sequences.</li>
</ul>

<p><img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/small.png" alt="Tokasaurus small models">
  <img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/monkeys.png" alt="Tokasaurus large batch sampling">
</p>

<p>Tokasaurus outperforms vLLM and SGLang on both of these benchmarks, in particular achieving over 2x the throughput of other engines on the Large Language Monkeys workload. Two main features contribute to these wins with small models:</p>

<h3 id="minimizing-cpu-overhead">Minimizing CPU Overhead</h3>

<p>LLM engines perform many different tasks on the CPU, like handling web requests, tokenizing inputs/detokenizing outputs, managing KV cache allocation, and preparing inputs for the model. If these CPU-side tasks cause the GPU-side model to stall, throughput can take a <a href="https://blog.vllm.ai/2024/09/05/perf-update.html">big hit</a>. To avoid these stalls, inference engines commonly make many CPU-side <a href="https://blog.vllm.ai/2024/09/05/perf-update.html">tasks</a> <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/">asynchronous</a>: while the GPU runs a forward pass for batch N, the CPU-side of the engine post-processes the results from batch N-1 and prepares the inputs for batch N+1.</p>

<p>Tokasaurus goes one step further, making the CPU-side of the engine (what we call the manager) both asynchronous and adaptive. The manager’s goal is to maintain a deep queue of inputs for the model to run forward passes on. The manager monitors the size of this queue and can detect if the model is close to exhausting it (and therefore stalling the GPU). In these cases, the manager will automatically start skipping optional steps (like checking for stop strings and onboarding new sequences) until the model’s input queue is sufficiently deep again. This combination of asynchrony and adaptivity lets Tokasaurus serve small models with much less CPU overhead.</p>

<h3 id="dynamic-prefix-identification-and-exploration">Dynamic Prefix Identification and Exploration</h3>

<p>Prefix sharing comes up all the time in LLM inference — not just when repeatedly sampling like in the Large Language Monkeys benchmark, but also when asking many questions about a long document or reusing a system prompt across many chatbot conversations.</p>

<p>Shared prefixes allow attention to be computed more efficiently. We first explored this idea last year with <a href="https://arxiv.org/abs/2402.05099">Hydragen</a> (aka <a href="https://flashinfer.ai/2024/02/02/cascade-inference.html">cascade attention</a> and <a href="https://arxiv.org/abs/2403.08845">bifurcated attention</a>), but at the time we didn’t address the problem of detecting these shared prefixes in an engine where sequences are constantly starting and finishing. With Tokasaurus, we solve this detection problem by running a greedy depth-first search algorithm before every model forward pass that iteratively finds the longest shared prefixes possible. Hydragen is most impactful for small models, which spend a relatively larger fraction of total FLOPs on attention.</p>

<hr>

<h2 id="optimizing-bigger-models">Optimizing Bigger Models</h2>

<p>Tokasaurus can also efficiently serve bigger models across multiple GPUs! Here, the most important optimizations are our implementations of pipeline parallelism (PP) and tensor parallelism (TP), which allow us to maximize throughput on GPUs with or without NVLink.</p>

<h3 id="pipeline-parallelism-for-the-gpu-poor">Pipeline Parallelism for the GPU Poor</h3>

<p>One of our original goals with Tokasaurus was to efficiently run multi-GPU inference on our lab’s L40S GPUs, which don’t have fast inter-GPU NVLink connections. Without NVLink, the communication costs incurred running TP across a node of eight GPUs are substantial. Therefore, efficient support for PP (which requires much less inter-GPU communication) was a high priority. PP needs a large batch in order to run efficiently, since batches from the manager are subdivided into microbatches that are spread out across pipeline stages. When optimizing for throughput, we’re generally already using the largest batch size that fits in GPU memory, so PP is often a natural fit for throughput-focused workloads. When benchmarking against vLLM’s and SGLang’s pipeline parallel implementations using Llama-3.1-70B on eight L40S GPUs, Tokasaurus improves throughput by over 3x:</p>

<p><img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/pipeline.png" alt="Tokasaurus small models">
</p>

<h3 id="async-tensor-parallel-for-the-gpu-rich">Async Tensor Parallel for the GPU Rich</h3>

<p>If you do have GPUs with NVLink (e.g. B200s and certain models of H100s and A100s), Tokasaurus has something for you too! Models in Tokasaurus can be torch compiled end-to-end, allowing us to take advantage of <a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487">Async Tensor Parallelism (Async-TP)</a>. This is a relatively new feature in PyTorch that can overlap inter-GPU communication with other computations, partially hiding the cost of communication. In our benchmarks, we found that Async-TP adds a lot of CPU overhead to the model forward pass and only starts improving throughput with very large batch sizes (e.g. 6k+ tokens). Tokasaurus maintains torch-compiled versions of our models with and without Async-TP enabled, allowing us to automatically switch on Async-TP whenever the batch size is big enough:</p>

<p><img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/big.png" alt="Tokasaurus small models">
</p>

<hr>

<h2 id="try-it-out">Try it Out</h2>

<p>Tokasaurus started as an internal lab effort to run our inference experiments faster, and we’re excited to share it more broadly! You can check out the Tokasaurus code on <a href="https://github.com/ScalingIntelligence/tokasaurus/tree/main">GitHub</a> and install the package from PyPI with:</p>



<p>Currently, we support models from the Llama-3 and Qwen-2 families and support any combination of data, tensor, and pipeline parallel within a single node.</p>

<p>Tokasaurus is written in pure Python (although we do use attention and sampling ops from the excellent <a href="https://arxiv.org/abs/2501.01005">FlashInfer</a> package). We hope that this makes the engine easier to fork and hack on, à la <a href="https://github.com/pytorch-labs/gpt-fast">GPT-fast</a>.</p>

<h2 id="benchmarking-details">Benchmarking Details</h2>

<p>The commands for reproducing our benchmarks are available <a href="https://github.com/ScalingIntelligence/tokasaurus/blob/main/logs/blog_commands.md">here</a>. For each benchmark, we configure all engines with the same KV cache size and maximum number of running requests. We’ve made a best effort to tune each engine’s remaining parameters. We report the average throughput across runs after completing a warmup run. For each benchmark, all engines are run on the same machine.</p>

<p>We use <a href="https://github.com/sgl-project/sglang/blob/7e257cd666c0d639626487987ea8e590da1e9395/python/sglang/bench_serving.py">this script</a> from SGLang for our ShareGPT benchmarks and <a href="https://github.com/ScalingIntelligence/tokasaurus/blob/a0155181f09c0cf40783e01a625b041985667a92/tokasaurus/benchmarks/monkeys_gsm8k.py">this custom script</a> for the Large Language Monkeys benchmark. To standardize our benchmarking scripts and interface, all experiments send requests through the OpenAI API. We also experimented with vLLM’s Python API (i.e. <code>LLM.generate()</code>) on the Large Language Monkeys benchmark with Llama-1B and measured roughly a 5% throughput increase (thanks to the vLLM team for the tip!).</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>Huge thanks to <a href="https://www.primeintellect.ai/">Prime Intellect</a> and <a href="https://www.together.ai/">Together AI</a> for providing us with compute for this project.</p>

<p>Also, we’re grateful to Dan Biderman, Simon Guo, Manat Kaur, and Avanika Narayan for beta testing the engine!</p>

<hr>

<p>If you find Tokasaurus useful, please use the following citation:</p>

<div><pre><code><span>@misc</span><span>{</span><span>juravsky2025tokasaurus</span><span>,</span>
  <span>author</span>       <span>=</span> <span>{Jordan Juravsky and Ayush Chakravarthy and Ryan Ehrlich and Sabri Eyuboglu and Bradley Brown and Joseph Shetaye and Christopher R{\'e} and Azalia Mirhoseini}</span><span>,</span>
  <span>title</span>        <span>=</span> <span>{Tokasaurus: An LLM Inference Engine for High-Throughput Workloads}</span><span>,</span>
  <span>year</span>         <span>=</span> <span>{2025}</span><span>,</span>
  <span>howpublished</span> <span>=</span> <span>{\url{https://scalingintelligence.stanford.edu/blogs/tokasaurus/}}</span>
<span>}</span>
</code></pre></div>


          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Eleven v3 (167 pts)]]></title>
            <link>https://elevenlabs.io/v3</link>
            <guid>44194521</guid>
            <pubDate>Thu, 05 Jun 2025 18:41:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elevenlabs.io/v3">https://elevenlabs.io/v3</a>, See on <a href="https://news.ycombinator.com/item?id=44194521">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-state="closed" id="radix-«R7fphtndibnb»" role="region" aria-labelledby="radix-«R3fphtndibnb»" data-orientation="vertical"><p>Afrikaans (afr), Arabic (ara), Armenian (hye), Assamese (asm), Azerbaijani (aze), Belarusian (bel), Bengali (ben), Bosnian (bos), Bulgarian (bul), Catalan (cat), Cebuano (ceb), Chichewa (nya), Croatian (hrv), Czech (ces), Danish (dan), Dutch (nld), English (eng), Estonian (est), Filipino (fil), Finnish (fin), French (fra), Galician (glg), Georgian (kat), German (deu), Greek (ell), Gujarati (guj), Hausa (hau), Hebrew (heb), Hindi (hin), Hungarian (hun), Icelandic (isl), Indonesian (ind), Irish (gle), Italian (ita), Japanese (jpn), Javanese (jav), Kannada (kan), Kazakh (kaz), Kirghiz (kir), Korean (kor), Latvian (lav), Lingala (lin), Lithuanian (lit), Luxembourgish (ltz), Macedonian (mkd), Malay (msa), Malayalam (mal), Mandarin Chinese (cmn), Marathi (mar), Nepali (nep), Norwegian (nor), Pashto (pus), Persian (fas), Polish (pol), Portuguese (por), Punjabi (pan), Romanian (ron), Russian (rus), Serbian (srp), Sindhi (snd), Slovak (slk), Slovenian (slv), Somali (som), Spanish (spa), Swahili (swa), Swedish (swe), Tamil (tam), Telugu (tel), Thai (tha), Turkish (tur), Ukrainian (ukr), Urdu (urd), Vietnamese (vie), Welsh (cym)</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: ClickStack – Open-source Datadog alternative by ClickHouse and HyperDX (177 pts)]]></title>
            <link>https://github.com/hyperdxio/hyperdx</link>
            <guid>44194082</guid>
            <pubDate>Thu, 05 Jun 2025 18:01:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hyperdxio/hyperdx">https://github.com/hyperdxio/hyperdx</a>, See on <a href="https://news.ycombinator.com/item?id=44194082">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a href="https://hyperdx.io/" rel="nofollow">
    <themed-picture data-catalyst-inline="true"><picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/logo_dark.png#gh-dark-mode-only">
      <img alt="hyperdx logo" src="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/logo_light.png#gh-light-mode-only">
    </picture></themed-picture>
  </a>
</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">HyperDX</h2><a id="user-content-hyperdx" aria-label="Permalink: HyperDX" href="#hyperdx"></a></p>
<p dir="auto"><a href="https://hyperdx.io/" rel="nofollow">HyperDX</a>, a core component of
<a href="https://clickhouse.com/use-cases/observability" rel="nofollow">ClickStack</a>, helps engineers
quickly figure out why production is broken by making it easy to search &amp;
visualize logs and traces on top of any ClickHouse cluster (imagine Kibana, for
ClickHouse).</p>
<p dir="auto">
  <a href="https://clickhouse.com/docs/use-cases/observability/clickstack/overview" rel="nofollow">Documentation</a> • <a href="https://hyperdx.io/discord" rel="nofollow">Chat on Discord</a>  • <a href="https://play.hyperdx.io/search" rel="nofollow">Live Demo</a>  • <a href="https://github.com/hyperdxio/hyperdx/issues/new">Bug Reports</a> • <a href="https://github.com/hyperdxio/hyperdx/blob/main/CONTRIBUTING.md">Contributing</a> • <a href="https://clickhouse.com/use-cases/observability" rel="nofollow">Website</a>
</p>
<ul dir="auto">
<li>🕵️ Correlate/search logs, metrics, session replays and traces all in one place</li>
<li>📝 Schema agnostic, works on top of your existing ClickHouse schema</li>
<li>🔥 Blazing fast searches &amp; visualizations optimized for ClickHouse</li>
<li>🔍 Intuitive full-text search and property search syntax (ex. <code>level:err</code>),
SQL optional!</li>
<li>📊 Analyze trends in anomalies with event deltas</li>
<li>🔔 Set up alerts in just a few clicks</li>
<li>📈 Dashboard high cardinality events without a complex query language</li>
<li><code>{</code> Native JSON string querying</li>
<li>⚡ Live tail logs and traces to always get the freshest events</li>
<li>🔭 OpenTelemetry supported out of the box</li>
<li>⏱️ Monitor health and performance from HTTP requests to DB queries (APM)</li>
</ul>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hyperdxio/hyperdx/blob/main/.github/images/search_splash.png"><img alt="Search logs and traces all in one place" src="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/search_splash.png" title="Search logs and traces all in one place"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Spinning Up HyperDX</h2><a id="user-content-spinning-up-hyperdx" aria-label="Permalink: Spinning Up HyperDX" href="#spinning-up-hyperdx"></a></p>
<p dir="auto">HyperDX can be deployed as part of ClickStack, which includes ClickHouse,
HyperDX, OpenTelemetry Collector and MongoDB.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -p 8080:8080 -p 4317:4317 -p 4318:4318 docker.hyperdx.io/hyperdx/hyperdx-all-in-one"><pre>docker run -p 8080:8080 -p 4317:4317 -p 4318:4318 docker.hyperdx.io/hyperdx/hyperdx-all-in-one</pre></div>
<p dir="auto">Afterwards, you can visit <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a> to access the HyperDX UI.</p>
<p dir="auto">If you already have an existing ClickHouse instance, want to use a single
container locally, or are looking for production deployment instructions, you
can view the different deployment options in our
<a href="https://clickhouse.com/docs/use-cases/observability/clickstack/deployment" rel="nofollow">deployment docs</a>.</p>
<blockquote>
<p dir="auto">If your server is behind a firewall, you'll need to open/forward port 8080,
8000 and 4318 on your firewall for the UI, API and OTel collector
respectively.</p>
</blockquote>
<blockquote>
<p dir="auto">We recommend at least 4GB of RAM and 2 cores for testing.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hosted ClickHouse Cloud</h3><a id="user-content-hosted-clickhouse-cloud" aria-label="Permalink: Hosted ClickHouse Cloud" href="#hosted-clickhouse-cloud"></a></p>
<p dir="auto">You can also deploy HyperDX with ClickHouse Cloud, you can
<a href="https://console.clickhouse.cloud/signUp" rel="nofollow">sign up for free</a> and get started in
just minutes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Instrumenting Your App</h2><a id="user-content-instrumenting-your-app" aria-label="Permalink: Instrumenting Your App" href="#instrumenting-your-app"></a></p>
<p dir="auto">To get logs, metrics, traces, session replay, etc into HyperDX, you'll need to
instrument your app to collect and send telemetry data over to your HyperDX
instance.</p>
<p dir="auto">We provide a set of SDKs and integration options to make it easier to get
started with HyperDX, such as
<a href="https://clickhouse.com/docs/use-cases/observability/clickstack/sdks/browser" rel="nofollow">Browser</a>,
<a href="https://clickhouse.com/docs/use-cases/observability/clickstack/sdks/nodejs" rel="nofollow">Node.js</a>,
and
<a href="https://clickhouse.com/docs/use-cases/observability/clickstack/sdks/python" rel="nofollow">Python</a></p>
<p dir="auto">You can find the full list in
<a href="https://clickhouse.com/docs/use-cases/observability/clickstack" rel="nofollow">our docs</a>.</p>
<p dir="auto"><strong>OpenTelemetry</strong></p>
<p dir="auto">Additionally, HyperDX is compatible with
<a href="https://opentelemetry.io/" rel="nofollow">OpenTelemetry</a>, a vendor-neutral standard for
instrumenting your application backed by CNCF. Supported languages/platforms
include:</p>
<ul dir="auto">
<li>Kubernetes</li>
<li>Javascript</li>
<li>Python</li>
<li>Java</li>
<li>Go</li>
<li>Ruby</li>
<li>PHP</li>
<li>.NET</li>
<li>Elixir</li>
<li>Rust</li>
</ul>
<p dir="auto">(Full list <a href="https://opentelemetry.io/docs/instrumentation/" rel="nofollow">here</a>)</p>
<p dir="auto">Once HyperDX is running, you can point your OpenTelemetry SDK to the
OpenTelemetry collector spun up at <code>http://localhost:4318</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome all contributions! There's many ways to contribute to the project,
including but not limited to:</p>
<ul dir="auto">
<li>Opening a PR (<a href="https://github.com/hyperdxio/hyperdx/blob/main/CONTRIBUTING.md">Contribution Guide</a>)</li>
<li><a href="https://github.com/hyperdxio/hyperdx/issues/new">Submitting feature requests or bugs</a></li>
<li>Improving our product or contribution documentation</li>
<li>Voting on <a href="https://github.com/hyperdxio/hyperdx/issues">open issues</a> or
contributing use cases to a feature request</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Motivation</h2><a id="user-content-motivation" aria-label="Permalink: Motivation" href="#motivation"></a></p>
<p dir="auto">Our mission is to help engineers ship reliable software. To enable that, we
believe every engineer needs to be able to easily leverage production telemetry
to quickly solve burning production issues.</p>
<p dir="auto">However, in our experience, the existing tools we've used tend to fall short in
a few ways:</p>
<ol dir="auto">
<li>They're expensive, and the pricing has failed to scale with TBs of telemetry
becoming the norm, leading to teams aggressively cutting the amount of data
they can collect.</li>
<li>They're hard to use, requiring full-time SREs to set up, and domain experts
to use confidently.</li>
<li>They requiring hopping from tool to tool (logs, session replay, APM,
exceptions, etc.) to stitch together the clues yourself.</li>
</ol>
<p dir="auto">We hope you give HyperDX in ClickStack a try and let us know how we're doing!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<ul dir="auto">
<li><a href="https://github.com/hyperdxio/hyperdx/issues/new">Open an Issue</a></li>
<li><a href="https://discord.gg/FErRRKU78j" rel="nofollow">Discord</a></li>
<li><a href="mailto:support@hyperdx.io">Email</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">HyperDX Usage Data</h2><a id="user-content-hyperdx-usage-data" aria-label="Permalink: HyperDX Usage Data" href="#hyperdx-usage-data"></a></p>
<p dir="auto">HyperDX collects anonymized usage data for open source deployments. This data
supports our mission for observability to be available to any team and helps
support our open source product run in a variety of different environments.
While we hope you will continue to support our mission in this way, you may opt
out of usage data collection by setting the <code>USAGE_STATS_ENABLED</code> environment
variable to <code>false</code>. Thank you for supporting the development of HyperDX!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/hyperdxio/hyperdx/blob/main/LICENSE">MIT</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini-2.5-pro-preview-06-05 (309 pts)]]></title>
            <link>https://deepmind.google/models/gemini/pro/</link>
            <guid>44193328</guid>
            <pubDate>Thu, 05 Jun 2025 16:44:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/models/gemini/pro/">https://deepmind.google/models/gemini/pro/</a>, See on <a href="https://news.ycombinator.com/item?id=44193328">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="modal-content-pro" tabindex="-1">
        
  
  
  
    
      

      
      
        
          
            <div>
              
                
                
                  
                  <div><div>
    <p>Preview</p>

    
      <h2>Gemini 2.5 Pro</h2>
    

    <p data-block-key="6huf1">Best for coding and complex prompts</p>

    
    <a data-gtm-tag="cta-selection" href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro-preview-06-05&amp;utm_source=deepmind.google&amp;utm_medium=referral&amp;utm_campaign=gdm&amp;utm_content=" rel="noopener" target="_blank">
      <span>Try in Google AI Studio</span>
      
    </a>
  </div>
      
    
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="1920" height="1200" srcset="https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w1920-h1200-n-nu-rw 1x, https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w3840-h2400-n-nu-rw 2x"><source media="(min-width: 1024px)" type="image/webp" width="1440" height="1200" srcset="https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w1440-h1200-n-nu-rw 1x, https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w2880-h2400-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="1024" height="1200" srcset="https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w1024-h1200-n-nu-rw 1x, https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w2048-h2400-n-nu-rw 2x"><source type="image/webp" width="600" height="1200" srcset="https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w600-h1200-n-nu-rw 1x, https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w1200-h2400-n-nu-rw 2x">
      <img alt="" height="1200" role="presentation" src="https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w1440-h1200-n-nu" width="1440">
    </picture>
    
  
  </div>
                
              
                
                
                  
                  <gemini-large-text>
  <p data-block-key="6ej0f">Gemini 2.5 Pro is our most advanced model yet, excelling at coding and complex prompts.</p></gemini-large-text>
                
              
                
                
                  
                  <p>
    
    <h2 data-in-view="">Pro performance</h2>
    
    
  </p>
                
              
                
                
                  
                  <section>
  <ul>
    
      <li>
        


<div>
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="306" height="172" srcset="https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w306-rw 1x, https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w612-rw 2x"><source media="(min-width: 1024px)" type="image/webp" width="415" height="233" srcset="https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w415-rw 1x, https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w830-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="460" height="258" srcset="https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w460-rw 1x, https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w920-rw 2x"><source type="image/webp" width="500" height="281" srcset="https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w500-rw 1x, https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w1000-rw 2x">
      <img alt="" height="233" loading="lazy" role="presentation" src="https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w415" width="415">
    </picture>
    
  <div><p>Enhanced reasoning</p><p data-block-key="ym6bt">State-of-the-art in key math and science benchmarks.</p></div></div>
      </li>
    
      <li>
        


<div>
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="306" height="172" srcset="https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w306-rw 1x, https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w612-rw 2x"><source media="(min-width: 1024px)" type="image/webp" width="415" height="233" srcset="https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w415-rw 1x, https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w830-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="460" height="258" srcset="https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w460-rw 1x, https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w920-rw 2x"><source type="image/webp" width="500" height="281" srcset="https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w500-rw 1x, https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w1000-rw 2x">
      <img alt="" height="233" loading="lazy" role="presentation" src="https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w415" width="415">
    </picture>
    
  <div><p>Advanced coding</p><p data-block-key="ym6bt">Easily generate code for web development tasks.</p></div></div>
      </li>
    
      <li>
        


<div>
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="306" height="172" srcset="https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w306-rw 1x, https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w612-rw 2x"><source media="(min-width: 1024px)" type="image/webp" width="415" height="233" srcset="https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w415-rw 1x, https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w830-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="460" height="258" srcset="https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w460-rw 1x, https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w920-rw 2x"><source type="image/webp" width="500" height="281" srcset="https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w500-rw 1x, https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w1000-rw 2x">
      <img alt="" height="233" loading="lazy" role="presentation" src="https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w415" width="415">
    </picture>
    
  <div><p>Natively multimodal</p><p data-block-key="ym6bt">Understands input across text, audio, images and video.</p></div></div>
      </li>
    
      <li>
        


<div>
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="306" height="172" srcset="https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w306-rw 1x, https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w612-rw 2x"><source media="(min-width: 1024px)" type="image/webp" width="415" height="233" srcset="https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w415-rw 1x, https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w830-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="460" height="258" srcset="https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w460-rw 1x, https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w920-rw 2x"><source type="image/webp" width="500" height="281" srcset="https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w500-rw 1x, https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w1000-rw 2x">
      <img alt="" height="233" loading="lazy" role="presentation" src="https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w415" width="415">
    </picture>
    
  <div><p>Long context</p><p data-block-key="ym6bt">Explore vast datasets with a 1-million token context window.</p></div></div>
      </li>
    
  </ul>
</section>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
    
    <h2 data-in-view="">Deep Think</h2>
    <p data-block-key="qbz3s">We’re making Gemini 2.5 Pro even better by introducing an enhanced reasoning mode called Deep Think.</p>
    
  </div>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <gemini-large-text>
  <p data-block-key="1m4qq">It uses our latest cutting edge research in reasoning - including parallel thinking techniques - resulting in incredible performance.</p></gemini-large-text>
                
              
                
                
                  
                  
                
              
                
                
                  
                  <div>
    <p data-in-view="">Methodology</p>
    
    <p data-block-key="943wb">All Gemini results come from our runs. USAMO 2025: https://matharena.ai. LiveCodeBench V6: * o3 High: Internal runs since numbers are not available in official leaderboard, o4-mini High: https://livecodebench.github.io/leaderboard.html (2/1/2025-5/1/2025). MMMU: Self reported by OpenAI</p>
    
  </div>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
    <p data-in-view="">Preview</p>
    <h2 data-in-view="">Native audio</h2>
    <p data-block-key="qbz3s">Converse in more expressive ways with native audio outputs that capture the subtle nuances of how we speak. Seamlessly switch between 24 languages, all with the same voice.</p>
    <p data-in-view=""><a data-gtm-tag="cta-selection" href="https://aistudio.google.com/prompts/new_chat" rel="noopener" target="_blank">
      <span>Try in Google AI Studio</span>
      
    </a></p>
  </div>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
    
    <h2 data-in-view="">Vibe-coding nature with 2.5 Pro</h2>
    <p data-block-key="92f1c">Images transformed into code-based representations of its natural behavior.</p>
    
  </div>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <div>
    
    <h2 data-in-view="">Hands-on with 2.5 Pro</h2>
    <p data-block-key="92f1c">See how Gemini 2.5 Pro uses its reasoning capabilities to create interactive simulations and do advanced coding.</p>
    
  </div>
                
              
                
                
                  
                  


<gdm-carousel id="carousel-1b2e262f-afdc-4a18-b0ec-2d5c57938c97" data-in-view="">
  
  
  <div>




<div aria-label="Item 1" id="carousel-item-47e360bb-1052-4a98-b031-e53c2babca0b"><h3>Make an interactive animation</h3><p>See how Gemini 2.5 Pro uses its reasoning capabilities to create an interactive animation of “cosmic fish” with a simple prompt.</p></div>




<div aria-label="Item 2" id="carousel-item-8762ba04-d45f-460e-bd31-47513806e315"><h3>Create your own dinosaur game</h3><p>Watch Gemini 2.5 Pro create an endless runner game, using executable code from a single line prompt.</p></div>




<div aria-label="Item 3" id="carousel-item-64fc0ed6-9c1d-4ee1-897d-d100a0b5f686"><h3>Code a fractal visualization</h3><p>See how Gemini 2.5 Pro creates a simulation of intricate fractal patterns to explore a Mandelbrot set.</p></div>




<div aria-label="Item 4" id="carousel-item-49ab3970-cfaf-4be3-8a0f-8227d96efc2a"><h3>Plot interactive economic data</h3><p>Watch Gemini 2.5 Pro use its reasoning capabilities to create an interactive bubble chart to visualize economic and health indicators over time.</p></div>




<div aria-label="Item 5" id="carousel-item-fb3da96d-96a2-4ece-a55c-6fada6aa69bc"><h3>Animate complex behavior</h3><p>See how Gemini 2.5 Pro creates an interactive Javascript animation of colorful boids inside a spinning hexagon.</p></div>




<div aria-label="Item 6" id="carousel-item-88b1474a-0698-4f5d-ad60-f34fa00598bc"><h3>Code particle simulations</h3><p>Watch Gemini 2.5 Pro use its reasoning capabilities to create an interactive simulation of a reflection nebula.</p></div></div>
  
</gdm-carousel>
                
              
                
                
                  
                  <div>
    
    <h2 data-in-view="">Benchmarks</h2>
    <p data-block-key="92f1c">Gemini 2.5 Pro leads common benchmarks by meaningful margins.</p>
    
  </div>
                
              
                
                
                  
                  


                
              
                
                
                  
                  
                
              
                
                
                  
                  


                
              
            </div>
          
        
      

      
    
  
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Restricts Android Sideloading–What It Means for User Autonomy and Freedom (398 pts)]]></title>
            <link>https://puri.sm/posts/google-restricts-android-sideloading-what-it-means-for-user-autonomy-and-the-future-of-mobile-freedom/</link>
            <guid>44193198</guid>
            <pubDate>Thu, 05 Jun 2025 16:29:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://puri.sm/posts/google-restricts-android-sideloading-what-it-means-for-user-autonomy-and-the-future-of-mobile-freedom/">https://puri.sm/posts/google-restricts-android-sideloading-what-it-means-for-user-autonomy-and-the-future-of-mobile-freedom/</a>, See on <a href="https://news.ycombinator.com/item?id=44193198">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><ul><li><a href="#abh_about">About</a></li><li><a href="#abh_posts">Latest Posts</a></li></ul><div><section><p><a href="https://puri.sm/posts/author/purism/" title="Purism"><img src="https://puri.sm/wp-content/uploads/gravatar/head-logo.jpg" width="80" alt="Purism"></a></p><div><h3><a href="https://puri.sm/posts/author/purism/">Purism</a></h3><p>Beautiful, Secure, Privacy-Respecting Laptops, Tablets, PCs, and Phones</p></div></section><section><p><a href="https://puri.sm/posts/author/purism/" title="Purism"><img src="https://puri.sm/wp-content/uploads/gravatar/head-logo.jpg" width="80" alt="Purism"></a></p></section></div></div><p dir="auto" data-sourcepos="5:1-5:530">Google has recently implemented new restrictions on sideloading Android apps, citing growing security concerns. In a pilot program launched in Singapore, the tech giant now blocks the installation of certain sideloaded apps—particularly those requesting sensitive permissions such as SMS access or accessibility services—if they are downloaded via web browsers, messaging apps, or file managers. The move, developed in partnership with Singapore’s Cyber Security Agency, is designed to prevent fraud and malware-enabled scams.</p><p dir="auto" data-sourcepos="7:1-7:398">In parallel, Google has rolled out its Play Integrity API, which allows developers to limit app functionality when sideloaded, effectively pushing users to install apps only through the Google Play Store. These policies reinforce Google’s control over Android’s ecosystem under the guise of security but have sparked renewed concern over digital autonomy, innovation suppression, and user rights.</p><p dir="auto" data-sourcepos="9:1-9:377">Critics argue that while these measures may reduce malicious activity, they also further consolidate Google’s monopolistic grip over app distribution—restricting user freedom, innovation, and competition. Sideloading, a longstanding pillar of Android’s openness, is now being marginalized, placing the Android platform closer to the walled-garden approach of Apple’s iOS.</p><h2>A Secure, Private Alternative: Purism’s PureOS, Secure Apps, and Librem Phones</h2><p dir="auto" data-sourcepos="11:1-12:398">In response to the increasing surveillance, manipulation, and corporate overreach embedded in mainstream mobile ecosystems, Purism offers a privacy-respecting solution. Powered by PureOS, a Debian-based Linux operating system, the Librem 5 and Liberty Phones enable full user autonomy, privacy, and data sovereignty—free from the predatory surveillance capitalism that fuels targeted advertising.</p><p><img src="https://puri.sm/wp-content/uploads/2022/07/l5-front-apps-shadow-crop.png" alt="" width="700" height="438" srcset="https://puri.sm/wp-content/uploads/2022/07/l5-front-apps-shadow-crop.png 700w, https://puri.sm/wp-content/uploads/2022/07/l5-front-apps-shadow-crop-300x188.png 300w" sizes="(max-width: 700px) 100vw, 700px"></p><p dir="auto" data-sourcepos="14:1-14:318">PureOS supports secure, free and open source applications that do not rely on exploitative data mining, addictive algorithms, or behavioral manipulation. With no forced reliance on corporate app stores or intrusive APIs, Purism’s ecosystem restores control to the user while ensuring high levels of security and transparency.</p><p dir="auto" data-sourcepos="16:1-16:198">As Google further locks down Android and Big Tech doubles down on user exploitation, Purism stands as a beacon for those seeking ethical, secure, and open alternatives in the mobile computing space.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rare black iceberg spotted off Labrador coast could be 100k years old (123 pts)]]></title>
            <link>https://www.cbc.ca/news/canada/newfoundland-labrador/black-iceberg-labrador-coast-1.7551078</link>
            <guid>44193120</guid>
            <pubDate>Thu, 05 Jun 2025 16:21:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cbc.ca/news/canada/newfoundland-labrador/black-iceberg-labrador-coast-1.7551078">https://www.cbc.ca/news/canada/newfoundland-labrador/black-iceberg-labrador-coast-1.7551078</a>, See on <a href="https://news.ycombinator.com/item?id=44193120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="detailContent"><!--$--><p><span><a href="https://www.cbc.ca/news/canada/newfoundland-labrador"><span>NL</span></a></span></p><!--/$--><p>A fish harvester from Carbonear, N.L., snapped a photo of a black iceberg while fishing for shrimp of the coast of Labrador in mid May. It caused a sensation on social media, and impressed a Memorial University professor who says it's likely a very old piece of ice dating back thousands of years.</p><h2 lang="en">Berg may contain ice coloured by millennia of dirt — or even a meteorite strike</h2><!--$--><!--/$--><!--$--><div data-cy="storyWrapper"><!--$--><figure><p><img alt="A pitch-black diamond shaped iceberg floats in the distance surrounded by other icebergs and chunks of floating ice.  " src="https://i.cbc.ca/1.7551191.1748976256!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/black-iceberg.jpg?im=" data-cy="leadmedia-story-img" fetchpriority="high"></p><figcaption>This black iceberg was spotted more than 100 kilometres off the coast of Labrador in mid-May. Fisher Hallur Antoniussen took a photo of it to show crewmates, but it quickly took off after being posted on social media. <!-- --> <!-- -->(Submitted by Hallur Antoniussen)</figcaption></figure><!--/$--><div><p dir="ltr">A rare black iceberg spotted off the coast of Labrador is making a splash on social media after a fish harvester living in Carbonear, N.L., took a photo of it while fishing for shrimp last month.</p><p dir="ltr">Originally from the Faroe Islands, Hallur Antoniussen was working with a crew on board the Saputi factory freezer trawler off the coast of Labrador in mid-May.&nbsp;</p><p dir="ltr">He'd never seen an iceberg like this one before.&nbsp;</p><p dir="ltr">"I have seen icebergs that are rolled, what they say have rolled in the beach with some rocks in it. This one here is completely different. It's not only that he is all black. He is almost ... in a diamond shape," Antoniussen said in an interview with CBC Radio's <em>Labrador Morning</em>.</p><p dir="ltr">He spotted the berg after going up the ship's crane when they were more than 100 kilometres offshore&nbsp;in the Hopedale channel, located between Nain and Hopedale.&nbsp;</p><p dir="ltr">A crew member had counted 47 icebergs in the area just the day before.&nbsp;&nbsp;</p><p dir="ltr">Antoniussen doesn't think it's a berg that tipped over — or rolled on the beach — picking up dirt and rocks after getting grounded. He's seen a lot of icebergs over his 50 years of fishing off of Greenland, and more recently off&nbsp;the Labrador coast since 1989.&nbsp;</p><p dir="ltr">The 64-year-old said it was hard to estimate the size of the iceberg at sea but figured it was at least three times the size of a regular bungalow.&nbsp;</p><p dir="ltr">He took a picture from roughly six kilometres away.&nbsp;</p><p dir="ltr">"It's something you don't see very often, and a camera is not something I run around [with] when I'm working. So, I just ran to my room and took my phone and snapped this picture," he said.&nbsp;</p><div dir="ltr"><figure><p><img loading="lazy" alt="An aerial shot of icebergs floating in a fiord surrounded by mountains in Greenland. " srcset="https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=Resize%3D300 300w,https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=Resize%3D460 460w,https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=Resize%3D620 620w,https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=Resize%3D780 780w,https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=Resize%3D1180 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=" data-cy="image-img"></p><figcaption>Memorial professor Lev Tarasov says ice from all over Greenland is converging toward its coastline, and then breaks off to form icebergs after reaching the water. Pictured here is the Kangerlussuaq Fjord outlet in Greenland. <!-- --> <!-- -->(Submitted by Lev Tarasov )</figcaption></figure></div><p dir="ltr">Antoniussen said the berg looked like a rock with lots of really dark greys and black veins in it, and quickly ruled out that a shadow was being cast on it.&nbsp;</p><p dir="ltr">He took the photo to show other crew members on the fishing boat. Then Antoniussen posted it on Facebook, and it soon took off, garnering hundreds of comments after being shared around.&nbsp;</p><p dir="ltr">Commenters have mused about everything from aliens to precious metals, and even dinosaurs being hidden in the ice.</p><p dir="ltr">"It's an Oil Berg," said one poster.</p><p dir="ltr">"Looks like a giant [woolly] mammoth!" exclaimed another.</p><p dir="ltr">Antoniessen is clear: this is a real photo.&nbsp;</p><p dir="ltr">Other people wondered if the iceberg has volcanic ash in it, a result of some ancient eruption.&nbsp;</p><h2 dir="ltr"><strong>An impressive iceberg&nbsp;</strong></h2><p dir="ltr">Lev Tarasov, a Memorial University professor of physical oceanography, doesn't rule that last theory&nbsp;out completely.&nbsp;</p><p dir="ltr">Tarasov&nbsp;says there are volcanoes beneath the ice caps of Iceland, and while he's not exactly sure about volcanoes in Greenland, he added that scientists have measured hotspots in the landmass's central region.</p><p dir="ltr">Like Antoniussen, he hasn't seen an iceberg quite like this one before.&nbsp;</p><p dir="ltr">Tarasov observed smaller versions of the black iceberg during his fieldwork on the Kangerlussuaq Fjord in Greenland last summer — just not as impressive, he said.&nbsp;&nbsp;</p><div dir="ltr"><figure><p><img loading="lazy" alt="Using a saw, a man cuts into rock in Greenland with tall mountains and icebergs dotting the water below.   " src="https://i.cbc.ca/1.7551199.1748976666!/fileImage/httpImage/lev-tarasov.jpg" data-cy="image-img"></p><figcaption>Memorial University's Lev Tarasov is shown here conducted fieldwork in Greenland last summer. The professor says the black iceberg might contain ice that's more than 100,000 years old.<!-- --> <!-- -->(Submitted by Lev Tarasov)</figcaption></figure></div><p dir="ltr">He guesses the ice in the berg is at least 1,000 years old, but could also be exponentially more ancient —&nbsp;even formed as many as&nbsp;100,000 years ago.</p><p dir="ltr">Tarasov said ice from all over Greenland is slowly converging toward its coastline, and when it gets there, it breaks off to form icebergs.</p><p dir="ltr">Those icebergs can take one to three years before reaching the Newfoundland and Labrador coastline.&nbsp;</p><h2 dir="ltr"><strong>A terrestrial journey&nbsp;</strong></h2><p dir="ltr">Tarasov says it's a reminder just how dynamic ice can be.&nbsp;</p><p dir="ltr">Ice streams, also&nbsp;known as outlet glaciers,&nbsp;move much faster than other parts of the ice sheet; they carry ice from the interior, traveling through deep valleys or channels out to the coast.&nbsp;</p><p dir="ltr">They pick up rocks and dirt along the way.&nbsp;&nbsp;</p><p dir="ltr">"There's parts of the ice that are actually flowing up to 20 kilometres per year, which would mean that ... the ice is moving maybe a few metres every hour," Tarasov said.&nbsp;</p><p dir="ltr">The bottom of the ice grinds against the earth's crust, he explained. There's a whole lot of churning, turning&nbsp;all that rock and sediment into a powder that then spreads up through columns of ice.&nbsp;</p><p dir="ltr">It would take a long time for that ground-up rock to spread so uniformly throughout the ice, Tarasov said.</p><h2 dir="ltr"><strong>Tip of the iceberg&nbsp;</strong></h2><p dir="ltr">Tarasov&nbsp;theorizes that the black berg was probably part of a much larger chunk of ice before it broke off into the water.</p><p dir="ltr">"Over time, as it travels around Baffin Bay and down the coast of Labrador, it's melting away. So I think a lot of that ice is melted away. Maybe the part that's clean is underneath, right? Again, 90 per cent&nbsp;of the ice is underneath the water. So we're only seeing the tip of the iceberg on top," he said.&nbsp;</p><div dir="ltr"><figure><p><img loading="lazy" alt="This is a scene of a rocky landscape with glacial ice in Greenland. Two human subjects are dwarfed by the immensity of it.  " srcset="https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=Resize%3D300 300w,https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=Resize%3D460 460w,https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=Resize%3D620 620w,https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=Resize%3D780 780w,https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=Resize%3D1180 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=" data-cy="image-img"></p><figcaption>Tarasov conducted fieldwork on the Kangerlussuaq Fjord in Greenland last summer, here showing the magnitude of the landscape and ice on a human scale. Note the size of human subjects in the middle of the photo. <!-- --> <!-- -->(Submitted by Lev Tarasov )</figcaption></figure></div><p dir="ltr">Tarasov thinks the iceberg rolled over at some point, and is now showing its underbelly.&nbsp;</p><p dir="ltr">He also offers another possible explanation for the iceberg's intriguing colour.</p><p dir="ltr">There is strong evidence showing that an asteroid struck the northwest corner of Greenland some 12,000 years ago, he said. The iceberg could have some dust from that meteorite strike if it came from the area.&nbsp;&nbsp;</p><p dir="ltr">No matter what, the ice likely isn't new:&nbsp;it's quite possible the dirt on the iceberg may not have seen the "light of day for hundreds of thousands of years," Tarasov said.</p><p dir="ltr"><em><strong>Download our&nbsp;</strong></em><a href="https://www.cbc.ca/newsapp/"><em><strong>free CBC News app</strong></em></a><em><strong>&nbsp;to sign up for push alerts for CBC Newfoundland and Labrador.&nbsp;</strong></em></p></div></div><!--/$--><!--$--><!--/$--><!--$--><!--/$--><!--$--><!--/$--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I think I'm done thinking about GenAI for now (143 pts)]]></title>
            <link>https://blog.glyph.im/2025/06/i-think-im-done-thinking-about-genai-for-now.html</link>
            <guid>44193018</guid>
            <pubDate>Thu, 05 Jun 2025 16:10:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.glyph.im/2025/06/i-think-im-done-thinking-about-genai-for-now.html">https://blog.glyph.im/2025/06/i-think-im-done-thinking-about-genai-for-now.html</a>, See on <a href="https://news.ycombinator.com/item?id=44193018">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <h2 id="the-problem">The Problem</h2>
<p>Like many other self-styled thinky programmer guys, I like to imagine myself as
a sort of <a href="https://en.wikipedia.org/wiki/Sherlock_Holmes">Holmesian</a> genius,
making trenchant observations, collecting them, and then synergizing them into
brilliant deductions with the keen application of my powerful mind.</p>
<p>However, several years ago, I had an epiphany in my self-concept.  I finally
understood that, to the extent that I <em>am</em> usefully clever, it is less in a
Holmesian idiom, and more, shall I we,
<a href="https://en.wikipedia.org/wiki/Adrian_Monk">Monkesque</a>.</p>
<p>For those unfamiliar with either of the respective franchises:</p>
<ul>
<li>Holmes is a towering intellect honed by years of training, who catalogues
  intentional, systematic observations and deduces logical, factual conclusions
  from those observations.</li>
<li>Monk, on the other hand, while also a reasonably intelligent guy, is highly
  neurotic, wracked by unresolved trauma and profound grief.  As both a
  consulting job and a coping mechanism, he makes a habit of erratically
  wandering into crime scenes, and, driven by a carefully managed jenga tower
  of mental illnesses, leverages his dual inabilities to solve crimes.  First,
  he is unable to filter out apparently inconsequential details, building up a
  mental rat’s nest of trivia about the problem; second, he is unable to let go
  of any minor incongruity, obsessively ruminating on the collection of facts
  until they all make sense in a consistent timeline.</li>
</ul>
<p>Perhaps surprisingly, this tendency serves both this fictional wretch of a
detective, and myself, reasonably well.  I find annoying incongruities in
abstractions and I fidget and fiddle with them until I end up building
something that <a href="https://twisted.org/">a lot of people like</a>, or perhaps
something that a smaller number of people get <a href="https://automat.readthedocs.io/en/latest/"><em>really</em> excited
about</a>.  At worst, at least <a href="https://fritter.readthedocs.io/en/latest/"><em>I</em>
eventually understand what’s going
on</a>.  This is a self-soothing
activity but it turns out that, managed properly, it can very effectively
soothe others as well.</p>
<p>All that brings us to today’s topic, which is an incongruity I cannot smooth
out or fit into a logical framework to make sense.  I am, somewhat reluctantly,
a <a href="https://blog.glyph.im/2024/05/grand-unified-ai-hype.html">genAI</a>
<a href="https://blog.glyph.im/2025/03/a-bigger-database.html">skeptic</a>.  However, I am, <em>even more</em>
reluctantly, exposed to genAI Discourse every damn minute of every damn day.
It is relentless, inescapable, and exhausting.</p>
<p>This preamble about personality should hopefully help you, dear reader, to
understand how I usually address problematical ideas by thinking and thinking
and fidgeting with them until I manage to write some words — or perhaps a new
open source package — that logically orders the ideas around it in a way which
allows my brain to calm down and let it go, and how that process is important
to me.</p>
<p>In this particular instance, however, genAI has defeated me.  I cannot make it
make sense, but I need to stop thinking about it anyway.  It is too much and I
need to give up.</p>
<p>My goal with this post is not to <em>convince</em> anyone of anything in particular —
and we’ll get to why that is a bit later — but rather:</p>
<ol>
<li>to set out my current understanding in one place, including all the various
   negative feelings which are still bothering me, so I can stop repeating it
   elsewhere,</li>
<li>to explain <em>why</em> I cannot build a case that I think <em>should</em> be particularly
   convincing to anyone else, particularly to someone who actively disagrees
   with me,</li>
<li>in so doing, to illustrate why I think the discourse is so fractious and
   unresolvable, and finally</li>
<li>to give myself, and hopefully by proxy to give others in the same situation,
   permission to just peace out of this nightmare quagmire corner of the
   noosphere.</li>
</ol>
<p>But first, just because I can’t <em>prove</em> that my interlocutors are <a href="https://xkcd.com/386/">Wrong On The
Internet</a>, doesn’t mean I won’t explain why I <em>feel</em>
like they are wrong.</p>
<h2 id="the-anti-antis">The Anti-Antis</h2>
<p>Most recently, at time of writing, there have been a spate of “the genAI
discourse is bad” articles, almost exclusively written from the perspective of,
not <em>boosters</em> exactly, but pragmatically minded (albeit concerned) genAI
users, wishing for the skeptics to be more pointed and accurate in our
critiques.  This is anti-anti-genAI content.</p>
<p>I am not going to link to any of these, because, as part of their
self-fulfilling prophecy about the “genAI discourse”, they’re <em>also</em> all bad.</p>
<p>Mostly, however, they had very little worthwhile to respond to because they
were straw-manning their erstwhile interlocutors.  They are all getting annoyed
at “bad genAI criticism” while failing to engage with — and often failing to
even <em>mention</em> — most of the actual <em>substance</em> of any serious genAI
criticism.  At least, any of the criticism that I’ve personally read.</p>
<p>I understand wanting to avoid a callout or Gish-gallop culture and just express
your own ideas.  So, I understand that they didn’t link directly to particular
sources or go point-by-point on anyone else’s writing.  Obviously I get it,
since that’s exactly what this post is doing too.</p>
<p>But if you’re going to talk about how bad the genAI conversation is, without
even <em>mentioning</em> huge categories of problem like “climate impact” or
“disinformation”<sup id="fnref:1:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:1:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:1">1</a></sup> even once, I honestly don’t know what conversation you’re
even talking about.  This is peak “make up a guy to get mad at” behavior, which
is especially confusing in this circumstance, because there’s an absolutely
<em>huge</em> crowd of actual people that you could already be mad at.</p>
<p>The people writing these pieces have historically seemed very thoughtful to me.
Some of them I know personally.  It is worrying to me that their critical
thinking skills appear to have substantially degraded <em>specifically</em> after
spending a bunch of time intensely using this technology which I believe has a
<em>scary</em> risk of <a href="https://skepchick.org/2025/05/chatgpt-is-creating-cult-leaders/">degrading one’s critical thinking
skills</a>.
Correlation is not causation or whatever, and sure, from a rhetorical
perspective this is “post hoc ergo propter hoc” and maybe a little “ad hominem”
for good measure, but correlation can still be <em>concerning</em>.</p>
<p>Yet, I cannot <em>effectively</em> respond to these folks, because they are making a
<em>practical</em> argument that I cannot, despite my best efforts, find compelling
evidence to refute categorically.  <em>My</em> experiences of genAI are all extremely
bad, but that is barely even anecdata.  <em>Their</em> experiences are
neutral-to-positive.  Little scientific data exists.  How to resolve this?<sup id="fnref:2:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:2:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:2">2</a></sup></p>
<h2 id="the-aesthetics">The Aesthetics</h2>
<p>As I begin to state my <em>own</em> position, let me lead with this: my factual
analysis of genAI is hopelessly negatively biased.  I find the vast majority of
the aesthetic properties of genAI to be <em>intensely</em> unpleasant.</p>
<p>I have been trying <em>very</em> hard to correct for this bias, to try to pay
attention to the facts and to have a clear-eyed view of these systems’
capabilities. But the feelings are visceral, and the effort to compensate is
tiring.  It is, in fact, the desire to stop making this <em>particular</em> kind of
effort that has me writing up this piece and trying to take an intentional
break from the subject, despite its intense relevance.</p>
<p>When I say its “aesthetic qualities” are unpleasant, I don’t just mean the
aesthetic elements of output of genAIs themselves. The aesthetic quality of
genAI writing, visual design, animation and so on, while <em>mostly</em> atrocious, is
also highly variable.  There are cherry-picked examples which look… fine.
Maybe even good.  For years now, there have been, famously, literally
award-winning aesthetic outputs of genAI<sup id="fnref:3:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:3:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:3">3</a></sup>.</p>
<p>While I am ideologically predisposed to see any “good” genAI art as accruing
the benefits of either a survivorship bias from thousands of terrible outputs
or simple plagiarism rather than its own inherent quality, I cannot deny that
in many cases it <em>is</em> “good”.</p>
<p>However, I am not just talking about the product, but the process; the
aesthetic experience of interfacing with the genAI system itself, rather than
the aesthetic experience of the outputs of that system.</p>
<p>I am not a visual artist and I am not really a writer<sup id="fnref:4:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:4:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:4">4</a></sup>, particularly not a
writer of fiction or anything else whose experience is primarily aesthetic.  So
I will speak directly to the experience of software development.</p>
<p>I have seen very few successful examples of using genAI to produce whole,
working systems.  There are no shortage of highly public <a href="https://neuromatch.social/@jonny/114622547164112473">miserable
failures</a>, particularly
from <a href="https://old.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/">the vendors of these systems
themselves</a>,
where the outputs are confused, self-contradictory, full of subtle errors and
generally unusable.  While few studies exist, it sure <em>looks</em> like this is an
automated way of producing a <a href="https://wiki.c2.com/?NetNegativeProducingProgrammer">Net Negative Productivity
Programmer</a>, throwing out
chaff to slow down the rest of the team.<sup id="fnref:5:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:5:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:5">5</a></sup></p>
<p>Juxtapose this with my aforementioned psychological motivations, to wit, I want
to have everything in the computer be <em>orderly</em> and <em>make sense</em>, I’m sure most
of you would have no trouble imagining that sitting through this sort of
practice would make me <em>extremely</em> unhappy.</p>
<p>Despite this plethora of negative experiences, executives are aggressively
mandating the use of AI<sup id="fnref:6:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:6:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:6">6</a></sup>.  It looks like <em>without</em> such mandates, most
people will not bother to use such tools, so the executives will need muscular
policies to enforce its use.<sup id="fnref:7:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:7:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:7">7</a></sup></p>
<p>Being forced to sit and argue with a robot while it struggles and fails to
produce a working output, while you have to rewrite the code at the end anyway,
is incredibly demoralizing.  This is the kind of activity that activates <a href="https://hbr.org/2019/07/6-causes-of-burnout-and-how-to-avoid-them">every
single major cause of
burnout</a> at
once.</p>
<p>But, at least in that scenario, the thing <em>ultimately doesn’t work</em>, so there’s
a hope that after a very stressful six month pilot program, you can go to
management with a pile of meticulously collected evidence, and shut the whole
thing down.</p>
<p>I am inclined to believe that, in fact, it doesn’t work well enough to be used
this way, and that we are going to see a big crash.  But that is not the most
aesthetically distressing thing.  The most distressing thing is that maybe it
<em>does</em> work; if not well enough to actually do the work, at least ambiguously
enough to fool the executives long-term.</p>
<p><a href="https://github.com/cloudflare/workers-oauth-provider?tab=readme-ov-file#written-using-claude">This
project</a>,
in particular, stood out to me as an example.  Its author, a self-professed “AI
skeptic” who “thought LLMs were glorified Markov chain generators that didn’t
actually understand code and couldn’t produce anything novel”, did a
green-field project to test this hypothesis.</p>
<p>Now, this particular project is not <em>totally</em> inconsistent with a world in
which LLMs cannot produce anything novel.  One could imagine that, out in the
world of open source, perhaps there is enough “OAuth provider written in
TypeScript” blended up into the slurry of “borrowed<sup id="fnref:8:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:8:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:8">8</a></sup>” training data that the
minor constraint of “make it work on Cloudflare Workers” is a small tweak<sup id="fnref:9:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:9:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:9">9</a></sup>. It
is not fully dispositive of the question of the viability of “genAI coding”.</p>
<p>But it is a data point related to that question, and thus it did make me
contend with what might happen if it <em>were</em> actually a fully demonstrative
example.  I reviewed the commit history, as the author suggested.  For the sake
of argument, I tried to ask myself if I would like working this way.  Just for
clarity on this question, I wanted to suspend judgement about everything else;
assuming:</p>
<ul>
<li>the model could be created with ethically, legally, voluntarily sourced
  training data</li>
<li>its usage involved consent from labor rather than authoritarian mandates</li>
<li>sensible levels of energy expenditure, with minimal CO2 impact</li>
<li>it is substantially more efficient to work this way than to just write the
  code yourself</li>
</ul>
<p>and so on, and so on… would I <em>like</em> to use this magic robot that could mostly
just emit working code for me?  Would I use it if it were <em>free</em>, in all senses
of the word?</p>
<p>No. I absolutely would not.</p>
<p>I found the experience of reading this commit history and imagining myself
using such a tool — without exaggeration — nauseating.</p>
<p>Unlike <a href="https://duckduckgo.com/?q=i+hate+code+review">many programmers</a>, I love
code review.  I find that it is one of the best parts of the process of
programming.  I can help people learn, and develop their skills, and learn
<em>from</em> them, and appreciate the decisions they made, develop an impression of a
fellow programmer’s style.  It’s a great way to build a mutual theory of mind.</p>
<p>Of course, it can still be really annoying; people make mistakes, often can’t
see things I find obvious, and in particular when you’re reviewing a lot of
code from a lot of different people, you often end up having to repeat
explanations of the <em>same</em> mistakes.  So I can see why many programmers,
particularly those more introverted than I am, hate it.</p>
<p>But, ultimately, when I review their code and work hard to provide clear and
actionable feedback, people learn and grow and it’s worth that investment in
inconvenience.</p>
<p>The process of coding with an “agentic” LLM appears to be the process of
carefully distilling all the worst parts of code review, and removing and
discarding all of its benefits.</p>
<p>The lazy, dumb, lying robot asshole keeps making the same mistakes over and
over again, never improving, never genuinely reacting, always obsequiously
<em>pretending</em> to take your feedback on board.</p>
<p>Even when it “does” actually “understand” and manages to load your instructions
into its context window, 200K tokens later it will slide cleanly out of its
memory and you will have to say it again.</p>
<p>All the while, it is attempting to trick you.  It gets most things right, but
it consistently makes mistakes in the places that you are least likely to
notice.  In places where a person <em>wouldn’t</em> make a mistake.  Your brain keeps
trying to develop a theory of mind to predict its behavior but there’s no mind
there, so it always behaves infuriatingly randomly.</p>
<p><a href="https://youtu.be/_2C2CNmK7dQ">I don’t think I am the only one who feels this way.</a></p>
<h2 id="the-affordances">The Affordances</h2>
<p>Whatever our environments <a href="https://en.wikipedia.org/wiki/Affordance">afford</a>,
we tend to do more of.  Whatever they resist, we tend to do less of.  So in a
world where we were all writing all of our code and emails and blog posts and
texts to each other with LLMs, what do they afford that existing tools do not?</p>
<p>As a weirdo who enjoys code review, I also enjoy process engineering.  The
central question of almost all process engineering is to continuously ask: how
shall we shape our tools, to better shape ourselves?</p>
<p>LLMs are an affordance for <em>producing more text, faster</em>.  How is that going to
shape us?</p>
<p>Again arguing in the alternative here, assuming the text is free from errors
and hallucinations and whatever, it’s all correct and fit for purpose, that
means it reduces the pain of circumstances where you have to repeat yourself.
Less pain!  Sounds great; I don’t like pain.</p>
<p>Every codebase has places where you need boilerplate.  Every organization has
defects in its information architecture that require repetition of certain
information rather than a link back to the authoritative source of truth.
Often, these problems persist for a very long time, because it is difficult to
overcome the institutional inertia required to make real progress rather than
going along with the status quo.  But this is often where the highest-value
projects can be found. <a href="https://www.phrases.org.uk/meanings/408900.html">Where there’s muck, there’s
brass</a>.</p>
<p>The process-engineering function of an LLM, therefore, is to prevent
fundamental problems from ever getting fixed, to reward the rapid-fire
overwhelm of infrastructure teams with an immediate, catastrophic cascade of
legacy code that is now much harder to delete than it is to write.</p>
<hr>
<p>There is a scene in Game of Thrones where Khal Drogo kills himself.  He does so
by replacing a stinging, burning, therapeutic antiseptic wound dressing with
some cool, soothing mud.  The mud felt nice, addressed the immediate pain,
removed the discomfort of the antiseptic, and immediately gave him a lethal
infection.</p>
<p>The pleasing feeling of immediate progress when one prompts an LLM to solve
some problem feels like cool mud on my brain.</p>
<h3 id="the-economics">The Economics</h3>
<p>We are in the middle of a <a href="https://blog.glyph.im/2024/05/grand-unified-ai-hype.html">mania</a> around
this technology.  As I have written about before, I believe the mania will end.
There will then be a crash, and a “winter”.  But, as I may not have stressed
sufficiently, this crash will be the biggest of its kind — so big, that it is
arguably not of a kind at all.  The level of investment in these technologies
is <em>bananas</em> and the possibility that the investors will recoup their
investment seems close to zero.  Meanwhile, that cost keeps going up, and up,
and up.</p>
<p>Others have reported on this in detail<sup id="fnref:10:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:10:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:10">10</a></sup>, and I will not reiterate that all
here, but in addition to being a looming and scary industry-wide (if we are
lucky; more likely it’s probably “world-wide”) economic threat, it is also
going to drive some panicked behavior from management.</p>
<p><a href="https://en.wikipedia.org/wiki/Elite_panic">Panicky behavior from management</a>
stressed that their idea is not panning out is, famously, the cause of much
human misery.  I expect that even in the “good” scenario, where <em>some</em> profit
is ultimately achieved, will still involve mass layoffs rocking the industry,
panicked re-hiring, destruction of large amounts of wealth.</p>
<p>It feels bad to think about this.</p>
<h3 id="the-energy-usage">The Energy Usage</h3>
<p>For a long time I believed that the energy impact was overstated.  I am even on
record, <a href="https://mastodon.social/@glyph/112242020641010867">about a year ago</a>,
saying I didn’t think the energy usage was a big deal.  I think I was wrong
about that.</p>
<p>It initially seemed like it was letting regular old data centers off the hook.
But recently I have learned that, while the numbers are incomplete because the
vendors aren’t sharing information, they’re also <em>extremely</em> bad.<sup id="fnref:11:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:11:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:11">11</a></sup></p>
<p>I think there’s probably a version of this technology that isn’t a climate
emergency nightmare, but that’s not the version that the general public has
access to today.</p>
<h2 id="the-educational-impact">The Educational Impact</h2>
<p>LLMs are making academic cheating <em>incredibly</em> rampant.<sup id="fnref:12:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:12:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:12">12</a></sup></p>
<p>Not only is it so common as to be nearly universal, it’s also extremely harmful
to learning.<sup id="fnref:13:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:13:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:13">13</a></sup></p>
<p>For learning, genAI is a <a href="https://bsky.app/profile/samhalpert.bsky.social/post/3lmt3coqvqk2w">forklift at the
gym</a>.</p>
<p>To some extent, LLMs are simply revealing a structural rot within education and
academia that has been building for decades if not centuries.  But it was
within those inefficiencies and the inconveniences of the academic experience
that real learning <em>was</em>, against all odds, still happening in schools.</p>
<p>LLMs produce a frictionless, streamlined process where students can
effortlessly glide through the entire credential, learning nothing.  Once
again, they dull the pain without regard to its cause.</p>
<p>This is not good.</p>
<h2 id="the-invasion-of-privacy">The Invasion of Privacy</h2>
<p>This is obviously only a problem with the big cloud models, but then, the big
cloud models are the only ones that people actually use.  If you are having
conversations about anything private with ChatGPT, you are sending all of that
private information directly to Sam Altman, to do with as he wishes.</p>
<p>Even if you don’t think he is a particularly bad guy, maybe he won’t even
create the privacy nightmare on purpose.  Maybe he will be forced to do so as a
result of some bizarre kafkaesque accident.<sup id="fnref:14:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:14:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:14">14</a></sup></p>
<p>Imagine the scenario, for example, where a woman is tracking her cycle and
uploading the logs to ChatGPT so she can chat with it about a health concern.
Except, surprise, you don’t have to imagine, you can just search for it, as <em>I</em>
have personally, organically, seen three separate women on YouTube, at least
one of whom <em>lives in Texas</em>, not only do this on camera but <em>recommend doing
this to their audiences</em>.</p>
<p>Citation links withheld on this particular claim for hopefully obvious reasons.</p>
<p>I assure you that I am neither particularly interested in menstrual products
nor genAI content, and if <em>I</em> am seeing this more than once, it is probably a
distressingly large trend.</p>
<h2 id="the-stealing">The Stealing</h2>
<p>The training data for LLMs is stolen.  I don’t mean like “pirated” in the sense
where someone illicitly shares a copy they obtained legitimately; I mean their
scrapers are ignoring both norms<sup id="fnref:15:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:15:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:15">15</a></sup> and laws<sup id="fnref:16:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:16:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:16">16</a></sup> to obtain copies under false
pretenses, destroying other people’s infrastructure<sup id="fnref:17:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:17:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:17">17</a></sup>.</p>
<h2 id="the-fatigue">The Fatigue</h2>
<p>I have provided references to numerous articles outlining rhetorical and
sometimes data-driven cases for the existence of certain properties and
consequences of genAI tools.  But I can’t <em>prove</em> any of these properties,
either at a point in time or as a durable ongoing problem.</p>
<p>The LLMs themselves are simply too large to model with the usual kind of
heuristics one would use to think about software.  I’d sooner be able to
predict the physics of dice in a casino than a 2 trillion parameter neural
network.  They resist scientific understanding, not just because of their size
and complexity, but because unlike a natural phenomenon (which could of course
be considerably larger and more complex) they <em>resist experimentation</em>.</p>
<p>The first form of genAI resistance to experiment is that every discussion is a
<a href="https://en.wikipedia.org/wiki/Motte-and-bailey_fallacy">motte-and-bailey</a>.  If
I use a free model and get a bad result I’m told it’s because I should have
used the paid model.  If I get a bad result with ChatGPT I should have used
Claude. If I get a bad result with a chatbot I need to start using an agentic
tool. If an agentic tool deletes my hard drive by putting <code>os.system(“rm -rf
~/”)</code> into <code>sitecustomize.py</code> then I guess I should have built my own MCP
integration with a completely novel heretofore never even considered security
sandbox or something?</p>
<p>What configuration, exactly, would let me make a categorical claim about these
things?  What specific methodological approach should I stick to, to get
reliably adequate prompts?</p>
<p>For the record though, if the idea of the free models is that they are going to
be provocative demonstrations of the impressive capabilities of the commercial
models, and the results are consistently dogshit, I am finding it increasingly
<a href="https://ioc.exchange/@kevinriggle/114617713278070348">hard to care</a> how much
better the paid ones are supposed to be, especially since the “better”-ness
cannot really be quantified in any meaningful way.</p>
<p>The motte-and-bailey doesn’t stop there though.  It’s a war on all fronts.
Concerned about energy usage?  That’s OK, you can use a local model.  Concerned
about infringement?  That’s okay, somewhere, somebody, maybe, has figured out
how to train models consensually<sup id="fnref:18:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:18:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:18">18</a></sup>.  Worried about the politics of enriching
the richest monsters in the world?  Don’t worry, you can always download an
“open source” model from Hugging Face.  It doesn’t matter that many of these
properties are mutually exclusive and attempting to fix one breaks two others;
there’s always an answer, the field is so abuzz with so many people trying to
pull in so many directions at once that it is legitimately difficult to
understand what’s going on.</p>
<p>Even here though, I can see that characterizing everything this way is unfair
to a hypothetical sort of person.  If there is someone working at one of these
thousands of AI companies that have been springing up like toadstools after a
rain, and they <em>really are</em> solving one of these extremely difficult problems,
how can I handwave that away?  We need people working on problems, that’s like,
the whole point of having an economy.  And I really don’t like shitting on
other people’s earnest efforts, so I try not to dismiss whole fields.  Given
how AI has gotten into <em>everything</em>, in a way that e.g. cryptocurrency never
did, painting with that broad a brush inevitably ends up tarring a bunch of
stuff that isn’t even really AI at all.</p>
<p>The second form of genAI resistance to experiment is the inherent obfuscation
of productization.  The models themselves are already complicated enough, but
the <em>products</em> that are built around the models are evolving extremely rapidly.
ChatGPT is not just a “model”, and with the rapid<sup id="fnref:19:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:19:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:19">19</a></sup> deployment of Model
Context Protocol tools, the edges of all these things will blur even further.
Every LLM is now just an enormous unbounded soup of arbitrary software doing
arbitrary whatever.  How could I possibly get my arms around that to understand
it?</p>
<h2 id="the-challenge">The Challenge</h2>
<p>I have woefully little experience with these tools.</p>
<p>I’ve tried them out a little bit, and almost every single time the result has
been a disaster that has not made me curious to push further.  Yet, I keep
hearing from all over the industry that I should.</p>
<p>To some extent, I feel like the motte-and-bailey characterization above is
fair; if the technology itself can really do real software development, it
ought to be able to do it in multiple modalities, and there’s nothing anyone
can <em>articulate</em> to me about GPT-4o which puts it in a fundamentally different
class than GPT-3.5.</p>
<p>But, also, I consistently hear that the <em>subjective experience</em> of using the
premium versions of the tools is actually good, and the free ones are actually
bad.</p>
<p>I keep struggling to find ways to try them “the right way”, the way that people
I know and otherwise respect claim to be using them, but I haven’t managed to
do so in any meaningful way yet.</p>
<p>I do not want to be using the cloud versions of these models with their
potentially hideous energy demands; I’d like to use a local model.  But there
is obviously not a nicely composed way to use local models like this.</p>
<p>Since there are apparently <em>zero</em> models with ethically-sourced training data,
and litigation is ongoing<sup id="fnref:20:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:20:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:20">20</a></sup> to determine the legal relationships of training
data and outputs, even if I can be comfortable with some level of plagiarism on
a project, I don’t feel that I can introduce the existential legal risk into
<a href="https://pypi.org/user/glyph/">other people’s infrastructure</a>, so I would need
to make a <em>new</em> project.</p>
<p>Others have differing opinions of course, including some within my dependency
chain, which does worry me, but I still don’t feel like I can freely contribute
further to the problem; it’s going to be bad enough to unwind any impact
upstream.  Even just for my own sake, I don’t want to make it worse.</p>
<p>This especially presents a problem because I have <a href="https://mastodon.social/@glyph/112498550495367755">way too much stuff going
on</a> already.  A new project
is not practical.</p>
<p>Finally, even if I <em>did</em> manage to satisfy all of my quirky<sup id="fnref:21:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:21:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:21">21</a></sup> constraints,
would this experiment really be worth anything?  The models and tools that
people are raving about are the big, expensive, harmful ones.  If I proved to
myself yet again that a small model with bad tools was unpleasant to use, I
wouldn’t really be addressing my opponents’ views.</p>
<p>I’m stuck.</p>
<h2 id="the-surrender">The Surrender</h2>
<p>I am writing this piece to make <em>my</em> peace with giving up on this topic, at
least for a while.  While I do idly hope that some folks might find bits of it
convincing, and perhaps find ways to be more mindful with their own usage of
genAI tools, and consider the harm they may be causing, that’s not actually the
goal.  And that is not the goal because it is just so much goddamn <em>work</em> to
prove.</p>
<p>Here, I must return to my philosophical hobbyhorse of
<a href="https://en.wikipedia.org/wiki/Language_game_(philosophy)">sprachspiel</a>.  In
this case, specifically to use it as an analytical tool, not just to understand
<em>what</em> I am trying to say, but what the <em>purpose</em> for my speech is.</p>
<p>The concept of sprachspiel is most frequently deployed to describe the <em>goal</em>
of the language game being played, but in game theory, that’s only half the
story.  Speech — particularly rigorously justified speech — has a <em>cost</em>, as
well as a benefit.  I can make shit up pretty easily, but if I want to do
anything remotely like scientific or academic rigor, that cost can be
astronomical.  In the case of developing an abstract understanding of LLMs, the
cost is just too high.</p>
<p>So what is my goal, then?  To be king Canute, standing astride the shore of
“tech”, whatever that is, commanding the LLM tide not to rise?  This is a
multi-trillion dollar juggernaut.</p>
<p>Even the rump, loser, also-ran fragment of it has the power to literally
suffocate us in our homes<sup id="fnref:22:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:22:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:22">22</a></sup> if they so choose, completely insulated from any
consequence.  If the power curve starts there, imagine what the <em>winners</em> in
this industry are going to be capable of, irrespective of the technology
they’re building - just with the resources they have to hand.  Am I going to
write a blog post that can rival their propaganda apparatus?  Doubtful.</p>
<p>Instead, I will just have to concede that maybe I’m wrong.  I don’t have the
skill, or the knowledge, or the energy, to demonstrate with any level of rigor
that LLMs are generally, in fact, hot garbage.  Intellectually, I will have to
acknowledge that maybe the boosters are right.  Maybe it’ll be OK.</p>
<p>Maybe the carbon emissions aren’t so bad.  Maybe everybody is keeping them
secret in ways that they don’t for other types of datacenter for perfectly
legitimate reasons.  Maybe the tools really can write novel and correct code,
and with a little more tweaking, it won’t be so difficult to get them to do it.
Maybe by the time they become a mandatory condition of access to developer
tools, they won’t be miserable.</p>
<p>Sure, I even sincerely agree, intellectual property really has been a pretty
bad idea from the beginning.  Maybe it’s OK that we’ve made an exception to
those rules.  The rules were stupid anyway, so what does it matter if we let a
few billionaires break them?  Really, everybody should be able to break them
(although of course, regular people can’t, because we can’t afford the lawyers
to fight off the MPAA and RIAA, but that’s a problem with the legal system, not
tech).</p>
<p>I come not to praise “AI skepticism”, but to bury it.</p>
<p>Maybe it really is all going to be fine.  Perhaps I am simply catastrophizing;
I have been known to do that from time to time.  I can even sort of believe it,
in my head.  Still, even after writing all this out, I can’t quite manage to
believe it in the pit of my stomach.</p>
<p>Unfortunately, that feeling is not something that you, or I, can argue with.</p>
<hr>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Thank you to <a href="https://blog.glyph.im/pages/patrons.html">my patrons</a>. Normally, I would say, “who are
supporting my writing on this blog”, but in the case of this piece, I feel more
like I should apologize to them for this than to thank them; these thoughts
have been preventing me from thinking more productive, useful things that I
actually have relevant skill and expertise in; this felt more like a creative
blockage that I just needed to expel than a deliberately written article.  If
you like what you’ve read here and you’d like to read more of it, well, too
bad; I am <em>sincerely</em> determined to stop writing about this topic.  But, if
you’d like to read more stuff like <em>other</em> things I have written, or you’d like
to support my <a href="https://github.com/glyph/">various open-source endeavors</a>, you
can <a href="https://blog.glyph.im/pages/patrons.html">support my work as a sponsor</a>!</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Seven Days at the Bin Store (143 pts)]]></title>
            <link>https://defector.com/seven-days-at-the-bin-store</link>
            <guid>44192995</guid>
            <pubDate>Thu, 05 Jun 2025 16:07:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://defector.com/seven-days-at-the-bin-store">https://defector.com/seven-days-at-the-bin-store</a>, See on <a href="https://news.ycombinator.com/item?id=44192995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>This spring, a new business opened on the main drag of my West Philadelphia neighborhood, provoking both excitement and trepidation.</p><p>“I saw it just the other day and feared it,” one friend texted. “Like what the actual fuck is that shit,” said another. “Why?!!!” said a third. “Who is that for?”</p><p>Until last summer, the corner storefront at Baltimore Avenue and S. Melville Street was a moderately overpriced hipster vintage store. After it closed, it sat empty for nearly a year. Then, in March, it sprang to life.&nbsp;</p><p>A “grand opening” banner went up over the door. A miniature wacky inflatable tube man flailed around outside. Posters with the logos for Walmart, Amazon, Costco, and Best Buy covered the windows, declaring “CRAZY DEALS, AMAZING BINZ.” I had to check it out.</p><p>AMAZING BINZ is on the first floor of a rowhome, narrow and long. There’s one central aisle, and on either side, big wooden tray tables—the proverbial binz—overflowing with undifferentiated piles of consumer <em>stuff</em>: unopened Halloween costumes, an ice mold shaped like a penis, a banner of many glittery penises wearing grass skirts, a staggering number of “reusable hot and cold gel compression sleeve[s] for elbow,” a single loose pregnancy test, something called Wokaar for “waxing the nose beard.”</p><p>At least half the products are still in boxes, and there are signs on the walls warning customers to NOT OPEN THE BOXES, so you have to scan the barcode with your phone or decipher clipped product descriptions: “module stool,” “gratitude journal,” “Xmas tea light green.” But the great innovation of Amazing Binz is its pricing structure, which is splashed across the facade in Spanish and English and makes good on the promise of CRAZY DEALS. On Fridays, when the bins are freshly stocked, everything costs $10. On Saturdays, $8. Sundays: $6. Mondays: $4. Tuesdays: $2. Wednesdays: $1. Thursday is bin store Sabbath, when the shop is closed and restocked.&nbsp;</p><p>The store is like nothing else on the block, which boasts, among other things, a yoga studio, two vape shops, a volunteer-run book store <em>and </em>a Marxist reading room, and no fewer than four Ethiopian joints.&nbsp;</p><p>Where did all this stuff come from? Who opened this store, and why? Is it profitable? What does it mean for the uneven gentrification of Baltimore Avenue and West Philadelphia? I decided to spend a week visiting Amazing Binz every day. Here is what I found.</p><h2><strong>Thursday: Restock Day</strong></h2><p>On Thursdays, Amazing Binz is closed. One of the owners, Ahmed, has graciously allowed me to observe the restock. Overall, Ahmed has been very gracious about my fixation on his store.</p><p>When I arrive at 10:30 a.m., he’s smoking a cigarette and nervously awaiting a delivery truck with 18 pallets of mixed merchandise from Target. Amazing Binz gets its inventory from major corporations’ overstock and returned products, taking advantage of the vast weird world of “reverse logistics.”&nbsp;</p><p>“ You've got the front end, where you order the product, it comes into the port, it goes to a warehouse, to a store, and then to our front door,” says Cathy Roberson, a researcher at the Reverse Logistics Association, an industry trade group. “What if once it hits our front door, we don't like it, or it's broken or something? That's the reverse part. ”&nbsp;</p><p>Returns, repairs, refurbished products, and even recalls fall into the purview of reverse logistics. They are joined there by products that never made it to a consumer because the season ended, or a box was a little dented, or the purchaser never picked up their order, or a retailer was just running out of room in their warehouse.</p><p>That pile of excess stuff is growing. About 17 percent of all merchandise gets returned, according to the National Retail Federation. That’s up from just eight percent in 2019. For online purchases, it’s almost 30 percent.&nbsp;</p><p>Liquidators are nothing new: T.J. Maxx, Ocean State Job Lot, Nordstrom Rack. But as the scale of the excess grows, the reverse logistics industry is expanding, and methods of disposal are diversifying. Some corporations, like Amazon, do their own reselling through a <a href="https://www.amazon.com/Amazon-Bulk-Liquidations/b?ie=UTF8&amp;node=23511005011" target="_blank" rel="noreferrer noopener">bulk liquidation page</a>. There are middlemen like <a href="https://bstock.com/amazoneu/" target="_blank" rel="noreferrer noopener">B-Stock</a>, an eBay-like site where anyone can shop for truckloads of unwanted stuff. Influencers are buying pallets to unbox on stream, capitalizing on both the goods and the views. It’s a whole universe of brokers, wholesalers, and secondhand retail all trying to claw back a little bit of money from the growing pile. That’s where Amazing Binz comes in.</p><p>“The goal for all the reverse logistics stuff,” says Roberson, “is to keep things out of the landfill.”&nbsp;</p><p>Today, Ahmed is expecting half a truckload with about 3,500 to 4,500 individual pieces, ranging from kitchen appliances to toys to clothing; he doesn’t know exactly what. The semi-truck pulls up right on time, and Ahmed unloads it with a forklift, cigarette still in mouth. Pallets fill the sidewalk, glistening shrink-wrapped towers of stuff: air fryers, microwaves, a vacuum, a sled, a machine that tosses a football, My Little Pony-branded diapers, and a go-kart.&nbsp;</p><figure><img alt="Shrinkwrapped pallets outside the bin store." src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><p>Inside the store, the bins are still half-full from the last truckload, which came from Amazon. There are many products I’ve only ever seen on the internet: a carrying case and monogrammed straw cover for a Stanley cup, a big plastic grinder for shredding chicken, silicone molds for making the viral Dubai chocolate bar at home.&nbsp;</p><p>For this restock, Ahmed was hoping for a different caliber of item. It’s a mixed success: some quality products, but more clothing and returned items than he was hoping for. Electronics are the holy grail; appliances and home goods do well, too. The best items are the ones that retain their value, because Ahmed sees his customers as agents in the reverse logistics economy, too.</p><p>“You can buy stuff from here and then you can resell it,” Ahmed tells me. “Through eBay or Amazon. You can sell on Facebook market. You will get your money back in less than one day if you wanna resell it. Which is a good thing for everybody right now, for the public. A lot of people need to work.”</p><p>It’s a slog of a day. I leave after a few hours, and come back at 6:00 p.m., nearly eight hours into this operation. Ahmed and two helpers are still unloading and stocking. They won’t be done until after 10:00 p.m.</p><h2><strong>Friday: $10</strong></h2><p>I get to Amazing Binz at 9:35 a.m. It opens at 10:00 a.m. Already, two people are standing at the door with their faces pressed to the glass.&nbsp;</p><p>Soon, six people are waiting, then a dozen. A woman and her kids show up in a taxi. At least one person has come from the county over. Amazing Binz <a href="https://www.instagram.com/amazingbinz/" target="_blank" rel="noreferrer noopener">posts their new hauls on Instagram</a>, so some people have already scouted what they want: a kid’s scooter, a shower caddy. Fridays offer the highest upside: a chance to score something really valuable for just $10. All the real shoppers are lined up this morning, looking for an edge.&nbsp;</p><p>People are arriving every minute; the line swells to 25. Most of the people waiting are middle-aged black men and women. Some are resellers, and many seem well-versed in Philadelphia’s liquidation economy. A few bring up <a href="https://turn7.com/" target="_blank" rel="noreferrer noopener">Turn 7</a>, a giant liquidation warehouse with a bin model that closed last month. I learn about another bin store in New Jersey, and one in Northeast Philly called <a href="https://www.instagram.com/blackfridayoutletphilly/?hl=en" target="_blank" rel="noreferrer noopener">Black Friday Outlet</a>, a nationwide bin store chain. Someone tells me there’s another bin store opening up <em>today</em>, just 10 blocks away.</p><p>“When I first got into this industry, there were no more than 10 bin stores in the entire country,” says Nebraska-based wholesaler and content creator Colton Carlson, who opened his first store in 2018. “Fast forward a few years, [and] there ended up being 3,000 bin stores.” Today, he thinks there could be as many as 10,000 in the U.S.&nbsp;</p><p>Bin stores are distinguished by their pricing model—a set price per item or weight, and falling prices to incentivize quick sales—and by their agnostic approach to inventory. They’ll take nearly anything and everything.</p><p>“Once people caught on that you could buy a truckload of inventory, you dump it in your bins and you make a ton of money, then they really just started opening that rapid-fire at that point,” says Carlson.</p><p>The explosion of online returns got the bin-store craze started, but the pandemic put it into overdrive. Factory shutdowns and slowdowns at ports meant for weird lags in the supply chain that led to seasonal products arriving past their season, or goods just being abandoned at port. To try and weather the uncertainty, and keep up with exploding online shopping, retailers overstocked on inventory.</p><p>“In the beginning of the pandemic, we bought everything in sight,” says Roberson. “Then all of a sudden we, the consumer, decided we've had enough. We've bought everything. We've gotta tighten our belts now.”</p><p>Consumer spending dropped, and retailers were left holding the bag in overstuffed warehouses. Some, like Target, “ kind of got caught with their pants down,” says Roberson. They were stuck with <a href="https://www.cnbc.com/2022/06/07/target-markdowns-plan-to-cut-inventory.html" target="_blank" rel="noreferrer noopener">so much excess inventory</a>, they sold it for pennies on the dollar. The secondhand market flooded with more cheap stuff than normal, creating opportunities for resellers.</p><p>“On opening day, we had about 300 people lined up down the street,” says Carlson, who thinks his was the biggest bin store in the country at the time. “Every weekend from that, it was just the same thing: a line down the street with 200, 300 people ready to find a deal.”</p><figure><img alt="Wokaar nose hair waxing kit on top of a pile of goods." src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><p>This Friday morning at Amazing Binz, there are now over 30 people waiting to be let inside.</p><p>Finally, the doors open and the crowd floods into the narrow shop, honing in on the premier items. Scooters and grills and Target-branded home goods fly out of the bins. People are hauling around air fryers so no one else can take them. It is, frankly, a bit of a mad house. One reseller, who didn’t find anything he wanted, says, “One of these days, someone’s going to get shot.” A woman declares, “I will never do this again.”</p><p>Generally, people seem happy with their finds. The guy who wanted the scooters gets the scooters; the woman who wanted the shower caddy gets the shower caddy. It all happens fast. By the end of the day, the first two bins are completely empty.&nbsp;</p><h2><strong>Saturday: $8</strong></h2><p>It’s farmer’s market day in West Philadelphia, Baltimore Avenue’s busiest day. The store is much calmer than Friday morning, but still doing a brisk business. Where yesterday there were piles of home goods and appliances, the bins have been restocked with less high-end fare: soccer balls, bike inner tubes, sand art kits, clothing, shoes.</p><p>I want to get a sense of how neighbors are feeling. Once, while digging in the bins, I heard a young guy muttering to himself. “This is so weird. What am I looking at?” We made eye contact. “Have you ever seen a store like this?” he asked me. “It doesn’t feel right. I don’t think this will be here long.”&nbsp;</p><p>I check West Willy, the easily parodied local Facebook group. A post about Amazing Binz has over 50 comments, ranging from “omg I am EXCITED,” to “Maybe I can find things I need and not weep over the price,” to “No offense to the owners but this store feels like where late stage capitalism goes for one final hurrah.”</p><p>I ask the book vendor who sets up outside the Marxist storefront how he feels about Amazing Binz. “It shows the corporation’s incursion into the neighborhood at every level, big to small,” he says. He adds that he likes that it’s a minority-owned business.</p><p>I ask a resident who happens to be walking by, a long-timer who moved to the neighborhood in the 1970s when it was, as he says, Philadelphia’s answer to Haight-Ashbury. “It means a potential downturn of the local market that had been popping up with restaurants and boutiques,” he says.</p><p>Baltimore Avenue runs from the University of Pennsylvania campus in the east, west through a neighborhood that’s a mixture of million-dollar houses, squats-turned-group-houses, and some of the city’s most persistent poverty. For the six-ish years I’ve lived here, I’ve watched a handful of bougie businesses open and quickly fold, leaving behind empty storefronts. Housing prices just go up and up, but the retail strip suggests an uneven gentrification. Those fluctuating fortunes are mirrored in starkly divergent views of whether Amazing Binz is an “amazing store with amazing stuff and amazing prices” (their first Google review) or “a place that sells crap” (a post on West Willy).&nbsp;</p><p>It’s an odd location for a store like this. Liquidators usually look for huge spaces, in industrial areas or big shopping centers at the edge of town. When I ask Ahmed how he came to open Amazing Binz here, he said it was actually not his first choice. He wanted to open a cafe or a sweet shop, but says he couldn’t get the permits or the zoning. The landlord wouldn’t give him a break on rent, which is nearly $4,000 a month.&nbsp;</p><p>So after about eight months of sitting on the location, Ahmed decided he needed to open something with low overhead and little startup capital. There’s a few bin stores in Northeast Philly, where he lives, so he’d seen this model before, and thought it would be good for the neighborhood. At the start of this week, he hung a Free Palestine flag behind the counter. He’s from the West Bank and felt comfortable hanging the flag because, he says, “Our neighbors support us.”</p><p>Nearly every time I’m here, there are happy customers, usually quite a few. There's a real mix of the neighborhood. Serious diggers, often in headphones, methodically overturn every pile. A PTA mom sends her husband home for the car after she fills three baskets with school supplies. Some gigglers revel in the slop and the strangely sentimental: the Dick Pics in Nature calendar, the crystal heart paperweight engraved with a 25th wedding anniversary message.</p><figure><figure><img alt="Dick Pics in Nature calendar" src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><figure><img alt="crystal heart paperweight engraved with a 25th wedding anniversary message" src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><span><figure></figure></span></figure><p>But there are haters. At parties and potlucks across West Philadelphia, my neighbors split into camps: for or against the bin store. Houses divide against themselves. Pros: It’s cheap, it’s fun, and at least the stuff isn’t getting thrown away. Cons: An uncanny feeling of glimpsing the collective consumer psyche, AI slop in consumer form, the horrors of production for production’s sake.&nbsp;</p><p>At the farmer’s market, a friend of a friend calls Amazing Binz “dark and sinister,” and compares it to a big sifting funnel. All the consumer drivel of the world goes in at the top, and all the unsaleable, undigested, unwanted stuff just falls out the bottom. “The bin store,” he says, “is like the last step before they just throw it in the ocean.”</p><h2><strong>Sunday: $6</strong></h2><p>When I get to Amazing Binz, Omran—Amazing Binz’s social media hype man—is <a href="https://www.instagram.com/p/DIZPZP2PCee/" target="_blank" rel="noreferrer noopener">filming a video</a> with a shopper who found the toner she usually buys at Sephora, for $65, here in the bins for $6.</p><p>This is one of Omran’s Instagram shticks. He asks customers what they bought and how much they paid, and then replies, “I know daht’s right,” a phrase also tagged on all the posts.&nbsp;</p><p>Omran does some of the ordering for Amazing Binz. He says he buys his inventory through direct relationships with people at warehouses, not through auction sites, where the prices are often higher. Ahmed tells me the average truckload costs about $16,000, and will contain thousands of individual pieces. The key is to keep the cost per item at around $2, and to get a mix of higher- and lower-value items so they balance each other out.&nbsp;</p><p>Most bin stores do a hybrid model, where they sell higher-quality stuff on the side at prices above $10. Amazing Binz calls it the VIP section. With this week’s Target haul, I assumed that the nicer items would end up there. But no, the owners priced most of it at $10, hoping the allure of a huge discount on a big-ticket item draws people in and gets them to spend more.&nbsp;</p><p>This is the bin-store promise: teasing the chance to find Oura rings and AirPods for just $10. Once, while I talked to Ahmed,<strong> </strong>a neighbor walked by and waved. “She got those Beats headphones here,” he says. “Ten dollars. She was so happy.”&nbsp;</p><p>But the price flattening works in both directions, I realize on a $6 Sunday. A friend is shopping for a mermaid-themed bachelorette party, and we find plenty: That previously mentioned garland of penises is still here (two, actually), as well as a sheet of mermaid stickers and a curly straw that spells "BRIDE."</p><p>The problem is that $6 is too expensive for a straw, especially a straw that I know has been here for three weeks, at least. This is one effect of the bin store: There’s plenty of legitimately good stuff, at good prices, but that&nbsp; usually gets snapped up in the first few days. Anything that remains through the whole pricing cycle and back again is revealed to be worth nothing at all.</p><p>My friend, digging for penis straws, says maybe it’s good to see. We have to confront this stuff. This is the cost of our collective Amazon addiction.&nbsp;</p><p>There are times when the bin store does feel like an art installation, a message, or a warning. Wander through it for a few days and you will start to feel like you are in a room in a haunted house, one where the corporeal forms of the world economy’s least-wanted products are trapped, unable to move on.</p><p>With or without Amazing Binz, all of this stuff is just out there, already made. A lot of it is useful but excessive, rendered obsolete by an overheated economy. Shopping here can feel a bit like fighting the last, losing battle of reverse logistics. What can still be made useful enough to be kept out of the landfill, just a little longer?</p><h2><strong>Monday: $4</strong></h2><p>It’s a slow day at Amazing Binz, again. This week has been big on gloomy weather, and low on foot traffic. I find a Trump 2024 Punisher logo drink koozie in the pile. These have been popping up in the bins since that first Amazon haul, along with MAGA and Let’s Go Brandon flags. Every time a customer points one out to the owners, they throw it away.</p><figure><img alt="" src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><p>A woman approaches the counter with a pair of leopard-print flats, but the two shoes are connected by a stretchy white loop of elastic that’s gotten tangled up with at least five other pairs’ elastic, creating an unholy rats nest of shoes that she needs help untangling.&nbsp;</p><p>On slower days, it’s hard not to wonder how long Amazing Binz will last. Philadelphia may be behind the times. In the rest of the country, the bin-store bubble has started to burst. On YouTube, I find a cadre of wholesale and reselling influencers, including a bin-store niche. I watch their videos touting the model, how easy it is to start one, and how to succeed. Then I watch them change their tunes.</p><p>“<a href="https://www.youtube.com/watch?v=8c9US81xrM8&amp;t=10s" target="_blank" rel="noreferrer noopener">Unpopular opinion: Bin stores really suck right now</a>,” declared creator Lindey Glenn in November 2023. She linked to Carlson’s most recent video: “<a href="https://www.youtube.com/watch?v=OfFRAfKmTfQ" target="_blank" rel="noreferrer noopener">Bin store burnout</a>.”</p><p>After opening his first store in late 2018, Carlson had a few good years riding high on the novelty of the business model and the glut of unwanted pandemic-era merchandise. But as more bin stores opened, the demand and prices for the same truckloads went up. Colton says that after a while, he was just breaking even on the bins, which at his store, started at a high of only $5.&nbsp;</p><p>He found other revenue streams, like selling entire pallets to resellers, and sorting out the niche and expensive products to flip on Poshmark or eBay. But the challenges persisted. The bin store is a finely tuned operation. A broker might promise a great haul with new electronics, and then deliver a bunch of dirty and broken returns. A truckload could arrive late, or not at all, leaving owners in the lurch on what should be their most profitable day. These problems are exacerbated in small stores, like Amazing Binz, where it’s hard to store extra merchandise to weather that uncertainty.</p><p>And the stuff itself creates logistical headaches. Boxes get opened, sometimes from theft, sometimes just from jostling around in the bins. Pieces get lost, trash and detritus build up alongside the unsold products, the difference between them increasingly hard to discern. In fact, when it was time to clear his bins, Carlton would dump all of the unsold inventory mixed with trash back onto a pallet and sell the whole thing for $50, with the caveat that the buyer had to go toss the garbage somewhere else.</p><p>Carlson saw the writing on the wall. He sold his bin store in 2023. Now he’s focusing on his wholesale business, and opening a side-by-side discount grocery and clothing store, which is where he thinks the secondhand retail trends are headed.</p><p>The Reverse Logistics Association has also seen a decline in bin-store fortunes. Roberson says they lost a few bin-store members because of bankruptcy. After the pandemic, “it was getting more difficult to buy up that excess inventory because there really wasn't excess inventory.”</p><p>But economic shocks are good for the secondhand economy. Roberson thinks tariffs could, eventually, lead to another bin-store bump. At the start of 2025, she says, many retailers overstocked on inventory again, filling their warehouses to get ahead of price hikes. That could mean they’ll soon be sitting on overstock, especially if consumers decide they’re not willing to pay higher prices. Supply chain disruptions will produce more excess. Already, <a href="https://www.wsj.com/economy/trade/importers-china-trade-chaos-tariffs-b3463832?st=XFNmLu&amp;reflink=article_copyURL_share" target="_blank" rel="noreferrer noopener">goods are stalled</a> at ports around the world.&nbsp;</p><p>Roberson predicts a temporary dip in the fortunes of bin-store owners, as secondhand goods get more expensive along with everything else, and then a resurgence. Carlson agrees. “ I think the secondhand market is going to keep growing because that's the whole reason people are shopping at these stores [to see] if they can save $10. I don't see consumer spending going down for this industry, or even like thrift stores at all. I think that's gonna continuously go up. However, the problem's gonna lie on whoever owns the bin store.”</p><h2><strong>Tuesday: $2</strong></h2><p>Ahmed seems extremely tired today. He’s barely taken a day off since they opened, not even for Eid. Everything is pretty picked over. The sand art is all gone, the bedding and curtains. There’s just two soccer balls left. I do my customary scan of the bins, from the back of the store to the front. There’s nothing new, just increasingly empty boxes of gel compression elbow sleeves, loose whippit canisters,<strong> </strong>and batteries rolling across the bottom.</p><figure><figure><img alt="A bin overflowing with items" src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><figure><img alt="A pregnancy test in a bin" src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><span><figure></figure></span></figure><p>According to Ahmed, the cost of pallets is going up. On a recent load of electronics, he paid $12 per item. For the time being, Amazing Binz is just breaking even, but it’s not sustainable. Ahmed says he may have to raise prices, and that he’ll even have to reevaluate the whole business in a few months. “This type of business, it’s not for this neighborhood,” he says.</p><p>Omran, ever the salesman, is darkly optimistic that they can keep prices low. "Anything to save the community some money,” he says.  "We're not looking to get rich. Being rich is pointless nowadays. It's about, how can you survive? How can I feed my kids? How can we pay our bills?”</p><p>I’m reminded of a conversation I had with the truck driver, Nick, who dropped off the Target load. He used to drive a taxi, until Uber ruined that. He prefers truck driving, but it was better at the height of the pandemic, when everyone was shopping online.&nbsp;</p><p>Around that time, Nick’s wife was a third-party seller on Amazon. She stocked her inventory based on social media trends, a task complicated by the algorithm. The more she looked at videos of, say, people showing off their Stanley cup accessories, the more TikTok fed her those videos, until it was hard to know if people were actually buying them in the real world, or if she was just in an online feedback loop. Plus, Amazon charges high storage fees, so unsold product just sitting in their warehouse cost her money. She quit, and abandoned her merchandise. That’s where a lot of the stuff in the Amazing Binz came from: failing third-party vendors, looking to offload the detritus of brief and bygone trends.</p><p>For a second, an image of the 21st century economy seems to come into focus here at the bin store. Nick, his wife, Amazing Binz’s owners, and even me, a freelance journalist, we’re all supposedly free agents, “entrepreneurs,” but in reality, we’re limited to buying whatever gets kicked down the funnel, and flipping it for all it’s worth. It’s like I can see our labor getting discounted by the day. Already, I've asked my editor if I can resell a version of this story elsewhere, to recoup a little more of my Amazing Binz investment.</p><h2><strong>Wednesday: $1</strong></h2><p>At 6:45 p.m. on Wednesday, a little over an hour until closing time, the shop is packed. Everything is now one dollar.</p><p>An elderly woman finds a shoe she likes, a black slide with gold studs all over. But it's just one shoe. Ahmed is insisting she buy it, and says he’ll keep the other shoe for her when he finds it. “For a dollar you can’t beat it,” he says. He’s in a good mood, joking with this woman and her daughter. She’s apparently been looking for three days.&nbsp;</p><p>“Those shoes are really cute,” I tell her.&nbsp;</p><p>“Do you work here?” she asks. Maybe I’ve been here too long.</p><p>After days of finding empty boxes, I finally find a loose elbow gel sleeve. It is heavy, slinky, comforting. After this, I find many of them, hiding in the bins like eels. Everything swims around the piles, and I revisit them as old friends, including the unopened boxes, known only by their clipped epithets: “Kiss Me I’m Irish wooden sign with lights,” “Lazy Susan for Refrigerator New.” The Jujube Enucleator box is full one day, and empty the next, without me ever seeing what was inside. The Dick Pics in Nature calendar remains, week after week. Who knows how many times these items have been bought and sold before they arrived here, how many more times they might be bought and sold. I imagine it all ending up in the same places eventually: the landfill or the waterways, where it will exist forever, first as choking hazard, then as nurdle.&nbsp;</p><p>In the dwindling bins, I find another Trump flag, folded in plastic but unmistakable. Omran says I can have it. I unfurl it, stiff and plasticky, creased into a grid. I don’t want it, but what am I supposed to do? It will end up in the garbage one way or another. I stuff it in my bag.&nbsp;</p><p>I ask, “How long have you guys been open?” It’s April 16, and Omran says they opened the doors on March 21.</p><p>“I thought it was more than a month,” says Ahmed.&nbsp;</p><p>“It’s been a long month,” says Omran.</p><p>The Trump flag joins a small museum of items I have purchased since the bin store opened: a cat bed ($10), a felted flower garland ($6), four post-Valentine heart-shaped chocolate boxes ($5 for all), two small spring-form pans ($4 each), a large penis-shaped ice cube mold ($4), four bike inner tubes ($2 each), a box of ant traps that don’t work ($2).&nbsp;</p><p>Did I need any of these things? Maybe the ant traps, if they’d worked, maybe the inner tubes. Mostly, I bought them because they were there, the same way I watch an Instagram reel because it pops up next. The irony is that if I were in the market for new pans or bedding, Amazing Binz would at this point be my best local option. It’s also simultaneously the most fun and most unsettling store on Baltimore Avenue, a combination that I find addictive, as many bin enjoyers do.</p><p>I take one last lap, back into the farthest corner of the store, where the oldest stuff has accumulated in deep drifts, where the Wokaar lives, now separated from its box. Looking for what? I don’t know. It’s concentrated back here, layers of unwanted product, filtered all the way down the supply chain to this last bin, where it’s piling up like sand. As I stare at the piles, I almost expect them to start breaking down in front of me, each individual item decomposing into a pool of microplastics, what remains of its value finally depleted. I leave the store, and the bins where they are. They’ll be full again tomorrow.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The impossible predicament of the death newts (400 pts)]]></title>
            <link>https://crookedtimber.org/2025/06/05/occasional-paper-the-impossible-predicament-of-the-death-newts/</link>
            <guid>44191620</guid>
            <pubDate>Thu, 05 Jun 2025 13:40:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://crookedtimber.org/2025/06/05/occasional-paper-the-impossible-predicament-of-the-death-newts/">https://crookedtimber.org/2025/06/05/occasional-paper-the-impossible-predicament-of-the-death-newts/</a>, See on <a href="https://news.ycombinator.com/item?id=44191620">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>The simple version: the newt is in an arms race with the common garter snake, <em>Thamnophis sirtalis</em>.&nbsp; The garter snake is a small-to-medium sized snake that is common all over North America.&nbsp; It’s a slim, elegant little creature that is usually found in or near water.&nbsp; And while it will eat pretty much anything it can catch, the garter snake particularly likes amphibians: small frogs, salamanders and newts.&nbsp; And in the Pacific Northwest, it <em>really</em> likes snacking on the Rough-Skinned Newt.</p><p>So the garter snakes of the Pacific Northwest have been evolving resistance to tetrodotoxin.&nbsp; (As you may recall, tetrodotoxin is the stuff that <a href="https://crookedtimber.org/2025/03/14/occasional-paper-the-interesting-home-life-of-the-blue-ringed-octopus/">makes the blue-ringed octopus so deadly</a>.&nbsp; It’s produced by symbiotic bacteria that live in the newt’s body, mostly on its skin.)&nbsp; And as the garter snakes evolve resistance, the newts have to evolve ever greater toxicity.&nbsp; And as the newts get more toxic… right.&nbsp; Feedback loop!&nbsp; That’s the simple version.</p><p>Except this is biology, so of course it’s not that simple.</p><div><p>One thing to keep in mind is that nothing in nature is free. The newt’s toxicity comes with a cost: the metabolic load of supporting all those bacteria.&nbsp; More toxicity means more bacteria means more load.&nbsp; A very toxic newt has to consume more calories than its less-toxic cousin.</p><p>Meanwhile, evolving resistance also comes at a cost.&nbsp; We don’t know that directly, but we can infer it pretty well.&nbsp; If resistance to tetrodotoxin were cheap and easy, everything would evolve it.&nbsp; </p><p>There have actually been attempts to measure the effect on the snakes!&nbsp; They haven’t found them, but keep in mind that tetrodotoxin is a neurotoxin.&nbsp; To resist it, you have to make changes to the biochemistry of your nervous system.&nbsp; Even a small snake has a very very complex nervous system, where those changes might show up in ways that are hard to measure.&nbsp; Like, if the resistant snakes were clumsier or had slower reflexes, sure, we could see that.&nbsp; But maybe they’re suffering from much more subtle neurological effects, like being prone to insomnia or hallucinations or sexual dysfunction.&nbsp; Or maybe they’re just a bit dim.&nbsp; </p><p>We don’t know, but we’re pretty sure there must be something.&nbsp; &nbsp;We know that <a href="https://link.springer.com/article/10.1007/s10886-024-01517-7?fromPaywallRec=true">garter snakes outside of the Pacific Northwest are much less resistant to tetrodotoxin</a>.&nbsp; They’ll drop dead from doses that their Oregon cousins simply ignore.&nbsp; So evolving the resistance must have some cost or drawback.</p><p>(Note that this means the newt is double-whammied:&nbsp; not only does it suffer the metabolic load of carrying around all those bacteria, but it also has to evolve resistance to tetrodotoxin, accepting whatever negative effects that brings.&nbsp; And things are only going to get worse.)</p><p>Now: when the snakes eat Rough-Skinned Newts, they may sometimes show signs of discomfort. The snake may visibly gag.&nbsp; It may writhe in obvious unease.&nbsp; In some cases, it may go into respiratory distress.&nbsp; Eating the newt looks pretty unpleasant.&nbsp; Yet the snakes persist.</p><p>Okay then — if evolving toxin resistance carries a cost, and if&nbsp;<em>even with resistance</em> eating the newts is unpleasant, then why then do garter snakes insist on eating newts?&nbsp; They cheerfully eat frogs, fish, and other non-toxic prey items.&nbsp; Why don’t they just eat those, and leave the poor newts alone?</p><p>Turns out there is an answer:&nbsp; <a href="https://link.springer.com/article/10.1023/B:JOEC.0000045585.77875.09">the garter snakes sequester the tetrodotoxin</a>, storing it in their livers.&nbsp; This makes them toxic to their own predators.&nbsp; (Of which there are plenty.&nbsp; They’re not large snakes, so they’re hunted by everything from raccoons to ravens.)&nbsp; But they don’t harbor the bacteria, so they don’t produce tetrodotoxin of their own. So eventually, the toxin that they’ve ingested breaks down.&nbsp; And then they need to eat another newt to refresh their defense.</p><p>(If you are That Guy:&nbsp; now you can name a snake that is poisonous without being venomous.)</p><p>So this explains why the snakes go after the newts particularly, preferring them to less toxic prey.&nbsp; They <em>want</em> to eat toxic newts.&nbsp; And it explains why the newts keep evolving to be more toxic: the snake may want to eat newts generally, but if an individual newt packs enough of a wallop, the snake <a href="https://link.springer.com/article/10.1007/s00049-010-0057-z">may just retch it up and go after a different one</a>.&nbsp; Newts with weaker poison?&nbsp; They get eaten.&nbsp; Snakes with less resistance?&nbsp; Have trouble finding newts they can choke down, and don’t get to steal their poison.&nbsp; So the arms race continues.</p><p>This also explains why the newts, despite being very toxic to anything that’s not a garter snake, haven’t evolved aposematic coloring or signals.&nbsp; “Aposematic” is just a fancy word for “warning”, meaning something — usually color — that tells the world Do Not Mess With Me:</p><p><img decoding="async" src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fres.cloudinary.com%2Frainforest-cruises%2Fimages%2Fc_fill%2Cg_auto%2Ff_auto%2Cq_auto%2Fw_2560%2Ch_1861%2Fv1627055916%2FPoison-Dart-Frogs-Facts-rana-de-dardo-envenenada-amarillenta%2FPoison-Dart-Frogs-Facts-rana-de-dardo-envenenada-amarillenta.jpg&amp;f=1&amp;nofb=1&amp;ipt=08b23ddaf5aa7edcd44506b627b0819a1169f932e06f09d41f146f6d24d69d9c" alt="13 Interesting Poison Dart Frogs Facts - Rainforest Cruises" width="237" height="172"><img loading="lazy" decoding="async" src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fguloinnature.com%2Fwp-content%2Fuploads%2F2022%2F08%2F8-1-e1664541955412.png&amp;f=1&amp;nofb=1&amp;ipt=7a5788815878ecb746369fb7609cbab7d9ef55e63314cba4ceef46de08284727" alt="What is aposematic coloration? | Gulo in Nature" width="273" height="174"></p><p><img loading="lazy" decoding="async" src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.color-meanings.com%2Fwp-content%2Fuploads%2F2023%2F06%2Fstriped-lionfish-close-up-blue-background-14878-768x491.jpeg&amp;f=1&amp;nofb=1&amp;ipt=d517fafecdfbd90d6e7acca301a9ab884dc3355231a4a71a2e0461e46de29459" alt="Aposematic Coloration: Nature's Brightly Colored Warning Signs | Color ..." width="361" height="231"><img loading="lazy" decoding="async" src="https://newbeautifulera.wordpress.com/wp-content/uploads/2013/03/gary-larsen.png?w=500" alt="Gary Larsen" width="181" height="232"><br>[The Far Side by Gary Larsen, copyright 1984]</p><p>The Rough-Skinned Newt, like many newts, has a brightly colored underside that it can flash when threatened.&nbsp; But the rest of it is a dull mottled grey, camouflage.&nbsp; Presumably that’s because if the newt ever tried to evolve full-body colors like an Amazonian poison dart frog, it would just be a big “come eat me” sign to garter snakes.&nbsp; Probably a certain number of newts get eaten by birds or fish or whatever anyway because they’re not aposematic enough.&nbsp; The bird or fish may not survive the experience, but that’s not much comfort to the newt.</p><p>In sum: the unfortunate newt is not once, not twice, but three times screwed over here.&nbsp; They have to be extra-toxic, carrying that metabolic load, just to <em>maybe</em> make the garter snakes think twice about eating them.&nbsp; Then they have to evolve defenses against their own toxin.&nbsp; But they can’t evolve aposematic coloring, because that’ll just lead to the snakes gobbling them all up.&nbsp; And finally, they can’t go back to being not-very-toxic, because the snakes will just eat more of them to gain the same amount of tetrodotoxin.&nbsp; They can’t win, they can’t break even, and they can’t leave the game.</p><p><img loading="lazy" decoding="async" src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcaliforniaherps.com%2Fsalamanders%2Fimages%2Ftgranulosaventmend.jpg&amp;f=1&amp;nofb=1&amp;ipt=3790a4834b4a97caa8b5adfb847fac5f6910139893a2e6b08515baa8fbceb8fd" alt="Rough-skinned Newt - Taricha granulosa" width="351" height="234"><br>[Copyright Gary Nafis, 2019]</p><p>There is even more going on here.&nbsp; There are literally dozens of papers about this snake-newt interaction.&nbsp; And we still haven’t reached the end of it!&nbsp; Just a few examples:</p><p>— The newts that live in the northern end of their range, up in Alaska, don’t have garter snakes around.&nbsp; As expected, they mostly aren’t very toxic.&nbsp; But only mostly!&nbsp; One paper found <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.2068">odd little pockets of unexpectedly toxic newts</a> up there.&nbsp; Why?&nbsp; We don’t know.</p><p>— Meanwhile, the newts of Vancouver Island, offshore, also aren’t very toxic.&nbsp; Because there are no snakes there?&nbsp; Ha ha, no.&nbsp; There are three different species of garter snake on Vancouver.&nbsp; But&nbsp; for some reason, the newts and snakes there seem to be living in harmony.&nbsp; Well, relative harmony.&nbsp; The snakes of Vancouver do eat newts.&nbsp; But so far, the snakes and newts don’t seem to have started an arms race like they have on the mainland.&nbsp; Why not?&nbsp; We don’t know.</p><p>— A question that occurred, digging through these papers:&nbsp; if the garter snakes are becoming toxic from eating newts, might the <em>snake</em>&nbsp; begin to evolve aposematic coloring?&nbsp; And sure enough, when I look at photos of Oregon garter snakes, a lot of them seem to have bright orange markings that look a bit aposematic:</p><p><img loading="lazy" decoding="async" src="https://2img.net/h/stevenbolgartersnakes.com/wp-content/uploads/2012/09/Thamnophis-sirtalis-concinnus-11.jpg" width="423" height="282"><br>[copyright Stephen Bol, 2023]</p><p>But on the other hand garter snake markings vary wildly.&nbsp; Lots of garter snakes outside the Pacific Northwest have orange spots or stripes, and lots of snakes within the region don’t.&nbsp; And as far as I can tell, nobody has even tried to research this yet.</p><p>— A thing to keep in mind: the Pacific Northwest, where all this is playing out, is a very young ecosystem in geological terms.&nbsp; Just 20,000 years ago, during the last Ice Age, the Pacific coast of Washington and Oregon looked a lot like Greenland today: a thin coastal strip of cold tundra,&nbsp; everything else covered by an ice cap.&nbsp; The newts and the snakes are relatively recent colonists.&nbsp; So we may be catching a snapshot of a relationship that is still evolving, and that may not be long-term stable.</p><p>— And finally, I have oversimplified this whole thing, because there are other species of newt in the genus <em>Taricha&nbsp;</em>that are pretty toxic — not as crazy deadly as their Rough-Skinned cousins, but still more toxic than normal newts — while the garter snake genus <em>Thamnophis </em>is just a taxonomic mess.&nbsp;&nbsp;</p><p>In other words, we’ve learned a lot, but mysteries still abound.&nbsp;</p><p>And that is probably enough about newts for now.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Twitter's new encrypted DMs aren't better than the old ones (194 pts)]]></title>
            <link>https://mjg59.dreamwidth.org/71646.html</link>
            <guid>44191591</guid>
            <pubDate>Thu, 05 Jun 2025 13:37:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mjg59.dreamwidth.org/71646.html">https://mjg59.dreamwidth.org/71646.html</a>, See on <a href="https://news.ycombinator.com/item?id=44191591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>(Edit: Twitter could improve this significantly with very few changes - I wrote about that <a href="https://mjg59.dreamwidth.org/71933.html">here</a>. It's unclear why they'd launch without doing that, since it entirely defeats the point of using HSMs)</p><p>When Twitter[1] launched encrypted DMs a couple <br>of years ago, it was the worst kind of end-to-end <br>encrypted - <em>technically</em> e2ee, but in a way that made it relatively easy for Twitter to inject new encryption keys and get everyone's messages anyway. It was also lacking a whole bunch of features such as "sending pictures", so the entire thing was largely a waste of time. But a couple of days ago, Elon <a href="https://x.com/elonmusk/status/1929238157872312773">announced</a> the arrival of "XChat", a new encrypted message platform <q>built on Rust with (Bitcoin style) encryption, whole new architecture</q>. Maybe this time they've got it right?</p><p>tl;dr - no. Use Signal. Twitter can probably obtain your private keys, and admit that they can MITM you and have full access to your metadata.</p><p>The new approach is pretty similar to the old one in that it's based on pretty straightforward and well tested cryptographic primitives, but merely using good cryptography doesn't mean you end up with a good solution. This time they've pivoted away from using the underlying cryptographic primitives directly and into higher level abstractions, which is probably a good thing. They're using <a href="https://libsodium.gitbook.io/doc/public-key_cryptography/sealed_boxes">Libsodium's boxes</a> for message encryption, which is, well, fine? It doesn't offer <a href="https://en.wikipedia.org/wiki/Forward_secrecy">forward secrecy</a> (if someone's private key is leaked then all existing messages can be decrypted) so it's a long way from the state of the art for a messaging client (Signal's had forward secrecy for over a decade!), but it's not inherently broken or anything. It is, however, written in C, not Rust[2].</p><p>That's about the extent of the good news. Twitter's old implementation involved clients generating keypairs and pushing the public key to Twitter. Each client (a physical device or a browser instance) had its own private key, and messages were simply encrypted to every public key associated with an account. This meant that new devices couldn't decrypt old messages, and also meant there was a maximum number of supported devices and terrible scaling issues and it was pretty bad. The new approach generates a keypair and then stores the private key using the <a href="https://juicebox.xyz/assets/whitepapers/juiceboxprotocol_revision7_20230807.pdf">Juicebox</a> protocol. Other devices can then retrieve the private key. </p><p>Doesn't this mean Twitter has the private key? Well, no. There's a PIN involved, and the PIN is used to generate an encryption key. The stored copy of the private key is encrypted with that key, so if you don't know the PIN you can't decrypt the key. So we brute force the PIN, right? Juicebox actually protects against that - before the backend will hand over the encrypted key, you have to prove knowledge of the PIN to it (this is done in a clever way that doesn't directly reveal the PIN to the backend). If you ask for the key too many times while providing the wrong PIN, access is locked down.</p><p>But this is true only if the Juicebox backend is trustworthy. If the backend is controlled by someone untrustworthy[3] then they're going to be able to obtain the encrypted key material (even if it's in an HSM, they can simply watch what comes out of the HSM when the user authenticates if there's no validation of the HSM's keys). And now all they need is the PIN. Turning the PIN into an encryption key is done using the <a href="https://en.wikipedia.org/wiki/Argon2">Argon2id</a> key derivation function, using 32 iterations and a memory cost of 16MB (the Juicebox white paper says 16KB, but (a) that's laughably small and (b) the <a href="https://github.com/juicebox-systems/juicebox-sdk/blob/main/rust/sdk/src/pin.rs#L54">code</a> says 16 * 1024 in an argument that takes kilobytes), which makes it computationally and moderately memory expensive to generate the encryption key used to decrypt the private key. How expensive? Well, on my (not very fast) laptop, that takes less than 0.2 seconds. How many attempts to I need to crack the PIN? Twitter's chosen to fix that to 4 digits, so a maximum of 10,000. You aren't going to need many machines running in parallel to bring this down to a very small amount of time, at which point private keys can, to a first approximation, be extracted at will.</p><p>Juicebox attempts to defend against this by supporting sharding your key over multiple backends, and only requiring a subset of those to recover the original. </p><s>I can't find any evidence that Twitter's does seem to be making use of this,</s><p>Twitter uses three backends and requires data from at least two, but all the backends used are under x.com so are presumably under Twitter's direct control. Trusting the keystore without needing to trust whoever's hosting it requires a trustworthy communications mechanism between the client and the keystore. If the device you're talking to can prove that it's an HSM that implements the attempt limiting protocol and has no other mechanism to export the data, this can be made to work. Signal makes use of something along these lines using <a href="https://en.wikipedia.org/wiki/Software_Guard_Extensions">Intel SGX</a> for contact list and settings storage and recovery, and <a href="https://security.googleblog.com/2022/10/SecurityofPasskeysintheGooglePasswordManager.html">Google</a> and <a href="https://support.apple.com/en-gb/102651">Apple</a> also have documentation about how they handle this in ways that make it difficult for them to obtain backed up key material. Twitter has no documentation of this, and as far as I can tell does nothing to prove that the backend is in any way trustworthy. (Edit to add: The Juicebox API does support authenticated communication between the client and the HSM, but that relies on you having some way to prove that the public key you're presented with corresponds to a private key that only exists in the HSM. Twitter gives you the public key whenever you communicate with them, so even if they've implemented this properly you can't prove they haven't made up a new key and MITMed you the next time you retrieve your key)</p><p>On the plus side, Juicebox <em>is</em> written in Rust, so Elon's not 100% wrong. Just mostly wrong.</p><p>But ok, at least you've got viable end-to-end encryption even if someone can put in some (not all that much, really) effort to obtain your private key and render it all pointless? Actually no, since you're still relying on the Twitter server to give you the public key of the other party and there's no out of band mechanism to do that or verify the authenticity of that public key at present. Twitter can simply give you a public key where they control the private key, decrypt the message, and then reencrypt it with the intended recipient's key and pass it on. The <a href="https://help.x.com/en/using-x/encrypted-direct-messages">support page</a> makes it clear that this is a known shortcoming and that it'll be fixed at some point, but they said that about the original encrypted DM support and it never was, so that's probably dependent on whether Elon gets distracted by something else again. And the server knows who and when you're messaging even if they haven't bothered to break your private key, so there's a lot of metadata leakage.</p><p>Signal doesn't have these shortcomings. Use Signal.</p><p>[1] I'll respect their name change once Elon respects his daughter</p><p>[2] There are implementations written in Rust, but Twitter's using the C one with <a href="https://github.com/ionspin/kotlin-multiplatform-libsodium">these</a> JNI bindings </p><p>[3] Or someone nominally trustworthy but who's been compelled to act against your interests - even if Elon were absolutely committed to protecting all his users, his overarching goals for Twitter require him to have legal presence in multiple jurisdictions that are not necessarily above placing employees in physical danger if there's a perception that they could obtain someone's encryption keys</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Notes Will Gain Markdown Export at WWDC, and, I Have Thoughts (273 pts)]]></title>
            <link>https://daringfireball.net/linked/2025/06/04/apple-notes-markdown</link>
            <guid>44191558</guid>
            <pubDate>Thu, 05 Jun 2025 13:32:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daringfireball.net/linked/2025/06/04/apple-notes-markdown">https://daringfireball.net/linked/2025/06/04/apple-notes-markdown</a>, See on <a href="https://news.ycombinator.com/item?id=44191558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Box">


<dl>
<dt><a href="https://9to5mac.com/2025/06/03/exclusive-ios-26-messages-carplay-more/">9to5Mac Reports Apple Notes Will Gain Markdown Export at WWDC, and, You’ll Be Unsurprised to Know, I Have Thoughts</a></dt>
<dd>
<p>Marcus Mendes, in a piece at 9to5Mac with multiple spoilers for next week’s keynote:</p>

<blockquote>
  <p>Apple is working on supporting the ability to export notes in
Markdown from Apple Notes, which is something third-party apps
have supported for years. Granted, this is a niche feature, but as
a fierce participant in the niche, I can confirm: this is huge.</p>
</blockquote>

<p>When this story first started spreading this morning, it was getting repeated as Notes “gaining Markdown support”, which implied something like <a href="https://bear.app/">Bear</a> or <a href="https://obsidian.md/">Obsidian</a>, where you can type Markdown syntax characters while editing, and perhaps optionally see the Markdown syntax in your notes. “Markdown notes app” is really like a class of notes apps unto itself.</p>

<p>Some people find this surprising, but I personally don’t want to use a Markdown notes app. <a href="https://daringfireball.net/2004/03/dive_into_markdown">I created Markdown two decades ago</a> and have used it ever since for one thing and one thing only: writing for the web at Daring Fireball. My <a href="https://daringfireball.net/projects/markdown/">original description</a> of what it is still stands: “Markdown is a text-to-HTML conversion tool for web writers.” Perhaps an even better description of Markdown is Matthew Butterick’s, <a href="https://docs.racket-lang.org/pollen/quick-tour.html#%28part._.Markdown_mode%29">from the documentation for Pollen</a>: “Markdown is a simplified notation system for HTML.”</p>

<p>The other great use case for Markdown is in a context where you either <em>need</em> or just <em>want</em> to be saving to a plain text file or database field. That’s not what Apple Notes is or should be. I can see why many technically-minded people want to use Markdown “everywhere”. It’s quite gratifying that Markdown has not only become so popular, but <a href="https://daringfireball.net/2004/03/introducing_markdown">after 21 years</a>, seemingly continues to grow in popularity, to the point now where there clearly are a lot of people who seemingly enjoy writing in Markdown more than even I do. But I think it would be a huge mistake for Apple to make Apple Notes a “Markdown editor”, even as an option. It’s trivial to create malformed Markdown syntax; it shouldn’t be possible to have a malformed note in Apple Notes. I <em>craft</em> posts for Daring Fireball; I dash off notes in Apple Notes.</p>

<p>Apple Notes offers a great WYSIWYG rich text editing interface that works great on an iPhone and even better on a Mac, which I think is exactly appropriate. Particularly clever are the limited formatting options, where you don’t pick a font per se, but rather only from a set of predefined styles, like headings, lists, and block quote. It’s not nerdy at all. That’s the Macintosh way. (But that’s why I think Apple Notes’s use of hashtags, rather than real tokenized tags <a href="https://support.apple.com/guide/mac-help/tag-files-and-folders-mchlp15236/mac">like in the Finder</a>, was an enormous mistake on Apple’s part. Real tokenized tags can contain spaces (so a multi-word tag can just be “Words Written Naturally” not “#WordsCrammedTogether”) and don’t need to be prefixed with an ugly, nerdy-looking <code>#</code> character. Notes using hashtags is like if the Finder disallowed spaces and uppercase letters in filenames.)</p>

<p>But Markdown <em>export</em> from Notes? That sounds awesome. Frankly, perhaps the biggest problem with Apple Notes is that its export functionality is rather crude — <a href="https://support.apple.com/guide/iphone/export-or-print-notes-iphdf551cfa2/ios">PDF and, of all formats, Pages</a>. Exporting and/or copying the selected text as Markdown would be pretty cool. Very curious to see how they handle images though, if this rumor is true.</p>

<p>★ <em>Wednesday, 4 June 2025</em></p>
</dd>
</dl>




<!-- Google Analytics -->

<!-- 
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-593949-1']);
  _gaq.push (['_gat._anonymizeIp']);
  _gaq.push(['_trackPageview']);
  (function() {
	var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
	ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
	var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
 -->

<!-- Asynchronously load Mint -->
<!-- No, screw mint
<script type="text/javascript">
(function () {
	var ma = document.createElement('script');
	ma.type = 'text/javascript';
	ma.src = '/mint/?js';
	ma.async = true;
	var s = document.getElementsByTagName('script')[0];
	s.parentNode.insertBefore(ma, s);
})();
</script>
-->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[10 Years of Betting on Rust (112 pts)]]></title>
            <link>https://tably.com/tably/10-years-of-betting-on-rust</link>
            <guid>44190190</guid>
            <pubDate>Thu, 05 Jun 2025 10:19:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tably.com/tably/10-years-of-betting-on-rust">https://tably.com/tably/10-years-of-betting-on-rust</a>, See on <a href="https://news.ycombinator.com/item?id=44190190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="textbox" translate="no" spellcheck="false"><h2>10 years of betting on Rust <span>and what I’m looking forward to next.</span></h2><p><span>By Alec Mocatta, Founder<br>1 Jun, 2025</span></p><hr data-tably-img="fwAoAAAAaHR0cHM6Ly90YWJseS5jb20vc3RhdGljL2pFVzNMWjNEWGNPLnN2ZwEIBoID"><p>I wrote my first line of Rust in June 2015, <a href="https://hn.algolia.com/?dateRange=custom&amp;dateStart=0&amp;dateEnd=1437004800&amp;query=rust&amp;sort=byPopularity&amp;type=story" rel="noopener noreferrer" target="_blank">a month after the buzz of Rust 1.0 landing</a>. Coming from C, Python, and JavaScript, I never looked back. Two Rust-based startups and 500k lines of Rust later, here are some reflections on the milestone.</p><h2>The early days were painful</h2><p>Version compatibility was poor—both between crates and with the compiler itself. A bug-fix update could force a compiler bump, which in turn dragged in unstable crates like <a href="https://github.com/serde-deprecated/syntex/tree/master" rel="noopener noreferrer" target="_blank"><code>syntex</code></a> (<a href="https://blog.rust-lang.org/2017/02/02/Rust-1.15/" rel="noopener noreferrer" target="_blank">once</a> a key <code>serde</code> dependency) and their dependents. In practice we “updated the world” and temporally pinned when a critical bug fix or feature demanded it—early on this was as often as every 6-week compiler release cycle. An awful lot of time was wasted binary-searching for compatible version combinations.</p><p>“Fighting the borrow checker” was real for me. Traits came naturally given background in C++, Java and Objective-C, but lifetimes and the idea of “proofs” that (<a href="https://aturon.github.io/blog/2017/07/08/lifetime-dispatch/#cant-we-just-rule-out-bad-specializations" rel="noopener noreferrer" target="_blank">*mostly</a>) don’t affect codegen took a while to grok. Patient friends and colleagues made this easier!</p><p>After a couple years of our codebase and team growing, compile times became painful. Large types were a <a href="https://github.com/rust-lang/rust/issues/38528" rel="noopener noreferrer" target="_blank">recurring</a> issue (and <a href="https://github.com/rust-lang/rust/issues/140944" rel="noopener noreferrer" target="_blank">still are</a>, occasionally), requiring diagnosis and mitigation. Investments brought it down for us, but iteration cycles still took a hit and rapid prototyping generally required effort to set up.</p><h2>The people were and are exceptional</h2><p>The Rust ecosystem has an impressive amount of programming “taste”, manifesting in dependencies with relatively simple builds, elegant implementations, and fast and robust performance. Reaching for TypeScript or Python is a relative exercise in frustration. There’s a reason Rust has taken the <a href="https://survey.stackoverflow.co/2024/technology#admired-and-desired" rel="noopener noreferrer" target="_blank">“most loved/admired language”</a> title for nine years now!</p><p>“What went right” is an essay in itself, but the evolving cadre of dedicated, opinionated and earnest volunteers, with strong mores around saying “no” and “not yet”, are I think the crux of it.</p><p>I reaped the benefits as an employer. We’ve been fortunate to be one of few Rust opportunities in London, with a pipeline of talented engineers keen to work in their favourite language. That your average Rust programmer is better than your average &lt;most other languages&gt; programmer has been a bonus.</p><h2>Rust has become a safe bet (in some domains)</h2><p>The early days necessitated a lot of <a href="https://softwareengineering.stackexchange.com/questions/388092/what-exactly-is-yak-shaving/388236#388236" rel="noopener noreferrer" target="_blank">yak shaving</a>. Omissions in <code>std</code>, for example, led to our evolving a library of workarounds, hacks and extensions: code that simplified or optimised our application code but wasn’t ready or wasn’t justifiable to merge upstream. Over time the rate of additions to it has slowed, and we regularly find ourselves now removing from it. <a href="https://doc.rust-lang.org/stable/std/vec/struct.Vec.html#method.extract_if" rel="noopener noreferrer" target="_blank">Our</a> <a href="https://doc.rust-lang.org/std/primitive.u64.html#method.midpoint" rel="noopener noreferrer" target="_blank">custom</a> <a href="https://doc.rust-lang.org/stable/std/thread/struct.Builder.html#method.spawn_unchecked" rel="noopener noreferrer" target="_blank">implementations</a> <a href="https://github.com/rust-lang/rust/issues/133125" rel="noopener noreferrer" target="_blank">begone</a>! Nowadays being able to rely upon <code>std</code>’s primitives having even relatively <a href="https://doc.rust-lang.org/std/collections/struct.VecDeque.html#method.rotate_left" rel="noopener noreferrer" target="_blank">uncommon</a> or <a href="https://github.com/rust-lang/rust/pull/128261" rel="noopener noreferrer" target="_blank">abstract</a> implementations, and them be well-optimised, is a joy.</p><p>This reliability now extends to much of the Rust experience. Building and upgrading is wildly more predictable, with fewer non-Rust dependencies, ~no surprise compiletime/codegen/inlining blowups, less ecosystem reliance on nightly features and greater respecting of semver. <a href="https://blog.rust-lang.org/2018/12/06/Rust-1.31-and-rust-2018/#non-lexical-lifetimes" rel="noopener noreferrer" target="_blank">Inference</a> <a href="https://blog.rust-lang.org/2021/10/21/Rust-1.56.0/#disjoint-capture-in-closures" rel="noopener noreferrer" target="_blank">improvements</a> have made the borrow checker friendlier to newcomers. And far fewer ICEs on nightly! We used to see new ICEs or link errors almost weekly, now it’s quarterly at most.</p><p>The crate ecosystem is more predictable too: newer crates like <a href="https://github.com/BurntSushi/jiff" rel="noopener noreferrer" target="_blank"><code>jiff</code></a>, <a href="https://github.com/pola-rs/polars" rel="noopener noreferrer" target="_blank"><code>polars</code></a> and <a href="https://github.com/tauri-apps/tauri" rel="noopener noreferrer" target="_blank"><code>tauri</code></a> build on the hard-won lessons of earlier projects, while stalwarts like <a href="https://github.com/tokio-rs/tokio" rel="noopener noreferrer" target="_blank"><code>tokio</code></a>, <a href="https://github.com/hyperium/hyper" rel="noopener noreferrer" target="_blank"><code>hyper</code></a> and <a href="https://github.com/rust-lang/regex" rel="noopener noreferrer" target="_blank"><code>regex</code></a> have earned robustness through years of heavy production use.</p><p>Ten years ago choosing Rust for production opted you into reinventing wheels and working around known and unknown issues. While still the case for some domains—namely Rust in the browser for us—for general systems and backend engineering that is a thing of the past. Rust empowers us to focus on business logic while producing remarkably fast and robust applications.</p><h2>Rust today feels like what programming should be</h2><p>More than a safe bet, Rust has a degree of programmer empathy that is unprecedented in large software projects: simple and robust builds, the best <a href="https://kobzol.github.io/rust/rustc/2025/05/16/evolution-of-rustc-errors.html" rel="noopener noreferrer" target="_blank">error messages</a> and linting around, great docs and IDE integration, and strong <a href="https://kobzol.github.io/rust/rustc/2023/07/30/optimizing-rust-ci-2023.html" rel="noopener noreferrer" target="_blank">CI</a> and <a href="https://rustc-dev-guide.rust-lang.org/tests/crater.html" rel="noopener noreferrer" target="_blank">regression testing</a>. Rust feels like a passion project, a labour of love by and for programmers.</p><p>10 years ago could we have predicted this? Well, I think some did. Pained by the status quo, some saw the potential of a language, an ecosystem, designed <i>well</i> by people like them. They volunteered their time and made Rust what it is. As Graydon Hoare, one of Rust’s original authors, <a href="https://rustfoundation.org/media/10-years-of-stable-rust-an-infrastructure-story/" rel="noopener noreferrer" target="_blank">puts it</a>:</p><blockquote><p>Rust today is the result of significant investments made by forward-looking institutions and the efforts of thousands of individuals who shared a belief in the long-term payoff of what they were building.</p></blockquote><p>I bet my time and (my investors’) money on Rust 10 years ago because I believed in this payoff. The enthusiasm was infections, the potential palpable, and smart minds were converging. I’m so glad that has only grown, and so grateful to <a href="https://github.com/topics/rust" rel="noopener noreferrer" target="_blank">everyone</a> who’s <a href="https://thanks.rust-lang.org/rust/all-time/" rel="noopener noreferrer" target="_blank">contributed</a> to it.</p><h2>What I’m looking forward to over the next 10 years</h2><h3>Simpler and faster builds</h3><p>With growing engineering and testing bandwidth we can continue to replace battle-tested but complex or slow dependencies with simpler and faster ones. I’ve directly or indirectly benefitted from this with <a href="https://blog.rust-lang.org/2024/05/17/enabling-rust-lld-on-linux/#benefits" rel="noopener noreferrer" target="_blank">linking</a>, <a href="https://github.com/rust-lang/rust/pull/95035#issuecomment-1073966631" rel="noopener noreferrer" target="_blank">locks</a>, <a href="https://github.com/rust-lang/rust/pull/73441" rel="noopener noreferrer" target="_blank">backtraces</a>, <a href="https://github.com/rust-lang/rust/issues/35437" rel="noopener noreferrer" target="_blank">platform-optimised routines</a>, <a href="https://www.memorysafety.org/blog/rustls-server-perf/" rel="noopener noreferrer" target="_blank">TLS</a>, <a href="https://github.com/rust-lang/rustup/issues/3790" rel="noopener noreferrer" target="_blank">HTTP</a>, <a href="https://github.com/rust-lang/cargo/issues/11813" rel="noopener noreferrer" target="_blank">git</a>, <a href="https://github.com/rust-lang/cargo/pull/15417" rel="noopener noreferrer" target="_blank">compression</a>, <a href="https://github.com/rust-lang/rust/pull/37817" rel="noopener noreferrer" target="_blank">building</a> and <a href="https://github.com/rust-lang/rustup" rel="noopener noreferrer" target="_blank">switching</a> toolchains, and <a href="https://github.com/rust-lang/cargo/pull/9992" rel="noopener noreferrer" target="_blank">build automation</a>. A few I’m particularly looking forward to gaining more attention are a <a href="https://github.com/sunfishcode/eyra" rel="noopener noreferrer" target="_blank">pure-Rust</a> and <a href="https://github.com/rust-lang/wg-cargo-std-aware" rel="noopener noreferrer" target="_blank">less special</a> <code>std</code>, reduced reliance on system <a href="https://github.com/rust-lang/rust/pull/140525" rel="noopener noreferrer" target="_blank">linkers</a> and <a href="https://doc.rust-lang.org/rustc/codegen-options/index.html#link-self-contained" rel="noopener noreferrer" target="_blank">libs</a>, <a href="https://github.com/ctz/graviola" rel="noopener noreferrer" target="_blank">pure-Rust crypto</a>, a <a href="https://github.com/cberner/redb" rel="noopener noreferrer" target="_blank">durable BTreeMap</a>, and a Rust <a href="https://github.com/bevyengine/bevy" rel="noopener noreferrer" target="_blank">game engine</a>. The latter not for me to use but because game devs bring some of the most <a href="https://www.reddit.com/r/rust/comments/lcsek8/any_thoughts_on_panic_unreachable_unchecked/" rel="noopener noreferrer" target="_blank">aggressive optimisation</a> I’ve seen and I’d love to benefit from it.</p><hr data-tably-img="fwAoAAAAaHR0cHM6Ly90YWJseS5jb20vc3RhdGljL2xwbkFraUxJRGl1LnBuZwEmCYQC"><p><a href="https://hn.algolia.com/?query=How%20to%20speed%20up%20the%20Rust%20compiler" rel="noopener noreferrer" target="_blank">Significant expertise and effort</a> is going into performance work. In the last few months Tably saw a 60% compile time improvement across frontend and backend. Many of the biggest wins have come from meta changes that aren’t visible on this chart, including <a href="https://github.com/rust-lang/rust/issues/45320#issuecomment-337957101" rel="noopener noreferrer" target="_blank">ThinLTO and backend parallelism</a>, <a href="https://blog.rust-lang.org/2016/09/08/incremental/" rel="noopener noreferrer" target="_blank">incremental</a>, <a href="https://internals.rust-lang.org/t/evaluating-pipelined-rustc-compilation/10199" rel="noopener noreferrer" target="_blank">metadata pipelining</a>, <a href="https://blog.rust-lang.org/2023/11/09/parallel-rustc/" rel="noopener noreferrer" target="_blank">front-end parallelism</a>. Meta changes that might bear further fruit are: <a href="https://github.com/rust-lang/rustc_codegen_cranelift" rel="noopener noreferrer" target="_blank">Rust-oriented codegen</a>, incremental <a href="https://github.com/davidlattimore/wild" rel="noopener noreferrer" target="_blank">linkers</a>, <a href="https://github.com/rust-lang/rust/issues/127634#issuecomment-2224274508" rel="noopener noreferrer" target="_blank">dead-code elimination</a>, <a href="https://blog.rust-lang.org/inside-rust/2024/12/04/trait-system-refactor-initiative/" rel="noopener noreferrer" target="_blank">a new solver</a>, and <a href="https://github.com/rust-lang/cargo/issues/10673" rel="noopener noreferrer" target="_blank"><code>cargo test</code> caching results</a>.</p><h3>Improved portability and less <code>#[cfg()]</code></h3><p>Testing the power set of valid <code>#[cfg()]</code> options, targets and features on CI is generally prohibitive. This creates untested code paths and potential compile failures, alongside annoyances like fragmented and incomplete documentation, impaired IDE functionality and <code>unused_imports</code> warnings.</p><p>A <a href="https://github.com/rust-lang/rfcs/blob/master/text/1868-portability-lint.md" rel="noopener noreferrer" target="_blank">lint</a> was proposed to mitigate this. A mooted better approach is <a href="https://github.com/rust-lang/rust/issues/41619#issuecomment-2056902943" rel="noopener noreferrer" target="_blank">moving <code>#[cfg()]</code> into the trait system</a> (<a href="https://www.reddit.com/r/rust/comments/lkgoyk/comment/gnjufff/" rel="noopener noreferrer" target="_blank">cc</a>) with bounds like <code>where Platform: Unix</code> and <code>where Platform: Avx2</code>. With this the compiler naturally guarantees compilation under all supported options, targets and features. Specializing items with OS-specific or architecture-specific implementations can be done via, appropriately, <a href="https://github.com/rust-lang/rust/issues/31844" rel="noopener noreferrer" target="_blank">specialization</a>. Metadata can be shared between dev/test/release, different feature combination and different platform builds, resulting in less recompilation, faster CI, and bringing a crates.io MIR <a href="https://www.reddit.com/r/rust/comments/5hav0q/towards_a_worldwide_precompiled_crate_cache_for/" rel="noopener noreferrer" target="_blank">cache</a> closer to reality.</p><h3>Everything being <code>const</code></h3><p>Constant evaluation reduces dependence on macros and build scripts, and brings forward run-time work and panics to compile-time. Currently we only scratch the surface of this as only a small subset of Rust code is supported (no traits, no allocation), and that which is supported runs <a href="https://docs.rs/hex-literal" rel="noopener noreferrer" target="_blank">too slowly</a> for nontrivial use cases.</p><p>Assuming portability all code can be evaluated during compilation (<a href="https://docs.rs/crabtime/latest/crabtime/#-one-shot-evaluation" rel="noopener noreferrer" target="_blank">cc</a>). Unportable code like FFI or assembly without a generic fallback is getting less common. <a href="https://github.com/rust-lang/const-eval/issues/1" rel="noopener noreferrer" target="_blank">Various</a> effects-inspired attempts to enable ubiquitous <code>const</code> have been <a href="https://github.com/rust-lang/rust/issues/67792" rel="noopener noreferrer" target="_blank">made</a> and <a href="https://github.com/rust-lang/rust/issues/110395" rel="noopener noreferrer" target="_blank">reverted</a> and <a href="https://github.com/rust-lang/rfcs/pull/3762" rel="noopener noreferrer" target="_blank">attempted again</a>. I hope we dodge this extra language complexity with the simple assumption that “everything can be executed in a <code>const</code> context”. There are <a href="https://github.com/rust-lang/const-eval/issues/20#issuecomment-464124376" rel="noopener noreferrer" target="_blank">tricky</a> <a href="https://github.com/rust-lang/unsafe-code-guidelines/issues/522" rel="noopener noreferrer" target="_blank">issues</a> to overcome but the language simplicity will be worth it.</p><h3>Simpler concurrency</h3><p><code>async</code> has a relatively high complexity cost for us due to the <a href="https://docs.rs/tokio-util/0.7.15/tokio_util/task/struct.LocalPoolHandle.html#method.spawn_pinned" rel="noopener noreferrer" target="_blank"><code>'static</code> bound</a> (<a href="https://maciej.codes/2022-06-09-local-async.html" rel="noopener noreferrer" target="_blank">cc</a>), <a href="https://google.github.io/comprehensive-rust/concurrency/async-pitfalls/cancellation.html" rel="noopener noreferrer" target="_blank">cancellation-safety</a>, and <a href="https://google.github.io/comprehensive-rust/concurrency/async-pitfalls/async-traits.html" rel="noopener noreferrer" target="_blank">restrictions</a> relating to <a href="https://blog.rust-lang.org/2023/12/21/async-fn-rpit-in-traits/#is-it-okay-to-use-async-fn-in-traits-what-are-the-limitations" rel="noopener noreferrer" target="_blank">traits</a> and <a href="https://blog.rust-lang.org/2023/12/21/async-fn-rpit-in-traits/#dynamic-dispatch" rel="noopener noreferrer" target="_blank"><code>dyn</code></a>. These currently <a href="https://github.com/tokio-rs/tokio/issues/2596#issuecomment-708441238" rel="noopener noreferrer" target="_blank">seem</a> <a href="https://www.reddit.com/r/rust/comments/1gvblzb/comment/ly0o3hi/" rel="noopener noreferrer" target="_blank">insoluble</a>. Bifurcation between <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html" rel="noopener noreferrer" target="_blank">sync</a> and <a href="https://docs.rs/futures/latest/futures/prelude/stream/trait.StreamExt.html" rel="noopener noreferrer" target="_blank">async</a> primitives and <a href="https://github.com/rust-lang/futures-rs/issues/2109#issuecomment-608413396" rel="noopener noreferrer" target="_blank">ecosystem</a> <a href="https://github.com/tokio-rs/tokio/issues/1529#issuecomment-2149262072" rel="noopener noreferrer" target="_blank">idiosyncrasies</a> further increase <a href="https://eta.st/2021/03/08/async-rust-2.html" rel="noopener noreferrer" target="_blank">the</a> <a href="https://trouble.mataroa.blog/blog/asyncawait-is-real-and-can-hurt-you/" rel="noopener noreferrer" target="_blank">async</a> <a href="https://bitbashing.io/async-rust.html" rel="noopener noreferrer" target="_blank">tax</a>. <a href="https://blog.yoshuawuyts.com/extending-rusts-effect-system/" rel="noopener noreferrer" target="_blank">Effects-inspired</a> solutions <a href="https://www.reddit.com/r/rust/comments/119y8ex/comment/j9pt1h3/" rel="noopener noreferrer" target="_blank">seem unpromising</a>.</p><p>Pre-1.0 Rust had a solution: <a href="https://stackoverflow.com/questions/29791031/what-happened-to-libgreen" rel="noopener noreferrer" target="_blank"><code>libgreen</code></a>. It achieved concurrency in user-space without this bifurcation, but at <a href="https://github.com/rust-lang/rfcs/blob/master/text/0230-remove-runtime.md#the-problems" rel="noopener noreferrer" target="_blank">significant performance, portability and maintenance cost</a> and so was <a href="https://github.com/rust-lang/rust/pull/18967" rel="noopener noreferrer" target="_blank">removed</a>. With increased engineering bandwidth this may be worth revisiting. Again the language simplicity would be worth it. (One day I’ll create a PoC of zero-cost wrapping of <code>std::{fs, net}</code> plus <code>fiber::{spawn, select}</code> using <a href="https://github.com/Xudong-Huang/generator-rs" rel="noopener noreferrer" target="_blank"><code>generator</code></a>!)</p><h3>Excelling in more domains</h3><p>Rust in the browser feels under-explored: we bump into issues that suggest there <a href="https://github.com/rustwasm/wasm-bindgen/issues/2388" rel="noopener noreferrer" target="_blank">aren’t</a> <a href="https://github.com/rustwasm/wasm-bindgen/issues/2392" rel="noopener noreferrer" target="_blank">many</a> <a href="https://github.com/rustwasm/wasm-bindgen/pull/2598" rel="noopener noreferrer" target="_blank">others</a> <a href="https://github.com/rustwasm/wasm-bindgen/pull/3899" rel="noopener noreferrer" target="_blank">using</a> <a href="https://github.com/rustwasm/wasm-bindgen/issues/3801" rel="noopener noreferrer" target="_blank">it</a> <a href="https://github.com/rustwasm/wasm-bindgen/issues/3687#issuecomment-1806486879" rel="noopener noreferrer" target="_blank">seriously</a>. Table stakes like <a href="https://github.com/rustwasm/wasm-bindgen/issues/3687" rel="noopener noreferrer" target="_blank">avoiding UB</a> and getting stack traces <a href="https://github.com/getsentry/sentry-javascript/issues/9968" rel="noopener noreferrer" target="_blank">cross-browser</a> took our time and effort. <a href="https://github.com/leptos-rs/leptos" rel="noopener noreferrer" target="_blank"><code>leptos</code></a>—the web framework behind this site—has come a long way (much like our previous choice <a href="https://github.com/sycamore-rs/sycamore" rel="noopener noreferrer" target="_blank"><code>sycamore</code></a>), yet rough edges remain.</p><p>Some domains like rapid prototyping and <a href="https://www.bartoszsypytkowski.com/is-rust-a-good-fit-for-business-apps/" rel="noopener noreferrer" target="_blank">business logic</a> may remain outside of Rust’s reach due to deliberate design decisions that trade off iteration speed. For most domains though I think it’s a matter of time. Adoption hurdles remain for GUIs, machine learning, and <a href="https://loglog.games/blog/leaving-rust-gamedev/" rel="noopener noreferrer" target="_blank">game development</a> due to entrenched ecosystems, high switching costs, cultural inertia, and established tooling. But those barriers aren’t permanent. Rust continues to mature and steadily break into these domains, thanks to relentless community efforts and innovation.</p><h2>Conclusion</h2><p>Looking forward the trajectory is clear and exciting. The positive feedback loop of adoption boosting engineering and testing bandwidth, which then fuels further adoption, is accelerating. The upcoming decade promises further refinement, faster compile times, broader domain adoption, and an increasingly seamless developer experience. At Tably, we remain committed to pushing the boundaries of what’s possible with Rust, excited to witness—and contribute to—the language’s next chapter. Here’s to another 10 years of betting on Rust!</p></div><div><div><h2>10 years of betting on Rust <span>and what I’m looking forward to next.</span><br></h2></div><p><span></span><span></span><span>By Alec Mocatta, Founder
1 Jun, 2025</span><br></p><p><img src="https://tably.com/static/jEW3LZ3DXcO.svg" width="1544" height="898"><span></span><span></span></p><p><span></span><span></span>I wrote my first line of Rust in June 2015, <a target="_blank" rel="noopener noreferrer" href="https://hn.algolia.com/?dateRange=custom&amp;dateStart=0&amp;dateEnd=1437004800&amp;query=rust&amp;sort=byPopularity&amp;type=story">a month after the buzz of Rust 1.0 landing</a>. Coming from C, Python, and JavaScript, I never looked back. Two Rust-based startups and 500k lines of Rust later, here are some reflections on the milestone.<br></p><h2><span></span><span></span>The early days were painful<br></h2><p><span></span><span></span>Version compatibility was poor—both between crates and with the compiler itself. A bug-fix update could force a compiler bump, which in turn dragged in unstable crates like <a target="_blank" rel="noopener noreferrer" href="https://github.com/serde-deprecated/syntex/tree/master"><code>syntex</code></a> (<a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2017/02/02/Rust-1.15/">once</a> a key <code>serde</code> dependency) and their dependents. In practice we “updated the world” and temporally pinned when a critical bug fix or feature demanded it—early on this was as often as every 6-week compiler release cycle. An awful lot of time was wasted binary-searching for compatible version combinations.<br></p><p><span></span><span></span>“Fighting the borrow checker” was real for me. Traits came naturally given background in C++, Java and Objective-C, but lifetimes and the idea of “proofs” that (<a target="_blank" rel="noopener noreferrer" href="https://aturon.github.io/blog/2017/07/08/lifetime-dispatch/#cant-we-just-rule-out-bad-specializations">*mostly</a>) don’t affect codegen took a while to grok. Patient friends and colleagues made this easier!<br></p><p><span></span><span></span>After a couple years of our codebase and team growing, compile times became painful. Large types were a <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/38528">recurring</a> issue (and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/140944">still are</a>, occasionally), requiring diagnosis and mitigation. Investments brought it down for us, but iteration cycles still took a hit and rapid prototyping generally required effort to set up.<br></p><h2><span></span><span></span>The people were and are exceptional<br></h2><p><span></span><span></span>The Rust ecosystem has an impressive amount of programming “taste”, manifesting in dependencies with relatively simple builds, elegant implementations, and fast and robust performance. Reaching for TypeScript or Python is a relative exercise in frustration. There’s a reason Rust has taken the <a target="_blank" rel="noopener noreferrer" href="https://survey.stackoverflow.co/2024/technology#admired-and-desired">“most loved/admired language”</a> title for nine years now!<br></p><p><span></span><span></span>“What went right” is an essay in itself, but the evolving cadre of dedicated, opinionated and earnest volunteers, with strong mores around saying “no” and “not yet”, are I think the crux of it.<br></p><p><span></span><span></span>I reaped the benefits as an employer. We’ve been fortunate to be one of few Rust opportunities in London, with a pipeline of talented engineers keen to work in their favourite language. That your average Rust programmer is better than your average &lt;most other languages&gt; programmer has been a bonus.<br></p><h2><span></span><span></span>Rust has become a safe bet (in some domains)<br></h2><p><span></span><span></span>The early days necessitated a lot of <a target="_blank" rel="noopener noreferrer" href="https://softwareengineering.stackexchange.com/questions/388092/what-exactly-is-yak-shaving/388236#388236">yak shaving</a>. Omissions in <code>std</code>, for example, led to our evolving a library of workarounds, hacks and extensions: code that simplified or optimised our application code but wasn’t ready or wasn’t justifiable to merge upstream. Over time the rate of additions to it has slowed, and we regularly find ourselves now removing from it. <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/stable/std/vec/struct.Vec.html#method.extract_if">Our</a> <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/std/primitive.u64.html#method.midpoint">custom</a> <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/stable/std/thread/struct.Builder.html#method.spawn_unchecked">implementations</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/133125">begone</a>! Nowadays being able to rely upon <code>std</code>’s primitives having even relatively <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/std/collections/struct.VecDeque.html#method.rotate_left">uncommon</a> or <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/128261">abstract</a> implementations, and them be well-optimised, is a joy.<br></p><p><span></span><span></span>This reliability now extends to much of the Rust experience. Building and upgrading is wildly more predictable, with fewer non-Rust dependencies, ~no surprise compiletime/codegen/inlining blowups, less ecosystem reliance on nightly features and greater respecting of semver. <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2018/12/06/Rust-1.31-and-rust-2018/#non-lexical-lifetimes">Inference</a> <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2021/10/21/Rust-1.56.0/#disjoint-capture-in-closures">improvements</a> have made the borrow checker friendlier to newcomers. And far fewer ICEs on nightly! We used to see new ICEs or link errors almost weekly, now it’s quarterly at most.<br></p><p><span></span><span></span>The crate ecosystem is more predictable too: newer crates like <a target="_blank" rel="noopener noreferrer" href="https://github.com/BurntSushi/jiff"><code>jiff</code></a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/pola-rs/polars"><code>polars</code></a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/tauri-apps/tauri"><code>tauri</code></a> build on the hard-won lessons of earlier projects, while stalwarts like <a target="_blank" rel="noopener noreferrer" href="https://github.com/tokio-rs/tokio"><code>tokio</code></a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/hyperium/hyper"><code>hyper</code></a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/regex"><code>regex</code></a> have earned robustness through years of heavy production use.<br></p><p><span></span><span></span>Ten years ago choosing Rust for production opted you into reinventing wheels and working around known and unknown issues. While still the case for some domains—namely Rust in the browser for us—for general systems and backend engineering that is a thing of the past. Rust empowers us to focus on business logic while producing remarkably fast and robust applications.<br></p><h2><span></span><span></span>Rust today feels like what programming should be<br></h2><p><span></span><span></span>More than a safe bet, Rust has a degree of programmer empathy that is unprecedented in large software projects: simple and robust builds, the best <a target="_blank" rel="noopener noreferrer" href="https://kobzol.github.io/rust/rustc/2025/05/16/evolution-of-rustc-errors.html">error messages</a> and linting around, great docs and IDE integration, and strong <a target="_blank" rel="noopener noreferrer" href="https://kobzol.github.io/rust/rustc/2023/07/30/optimizing-rust-ci-2023.html">CI</a> and <a target="_blank" rel="noopener noreferrer" href="https://rustc-dev-guide.rust-lang.org/tests/crater.html">regression testing</a>. Rust feels like a passion project, a labour of love by and for programmers.<br></p><p><span></span><span></span>10 years ago could we have predicted this? Well, I think some did. Pained by the status quo, some saw the potential of a language, an ecosystem, designed <i>well</i> by people like them. They volunteered their time and made Rust what it is. As Graydon Hoare, one of Rust’s original authors, <a target="_blank" rel="noopener noreferrer" href="https://rustfoundation.org/media/10-years-of-stable-rust-an-infrastructure-story/">puts it</a>:<br></p><blockquote><p>Rust today is the result of significant investments made by forward-looking institutions and the efforts of thousands of individuals who shared a belief in the long-term payoff of what they were building.<br></p><span></span><span></span></blockquote><p><span></span><span></span>I bet my time and (my investors’) money on Rust 10 years ago because I believed in this payoff. The enthusiasm was infections, the potential palpable, and smart minds were converging. I’m so glad that has only grown, and so grateful to <a target="_blank" rel="noopener noreferrer" href="https://github.com/topics/rust">everyone</a> who’s <a target="_blank" rel="noopener noreferrer" href="https://thanks.rust-lang.org/rust/all-time/">contributed</a> to it.<br></p><h2><span></span><span></span>What I’m looking forward to over the next 10 years<br></h2><h3><span></span><span></span>Simpler and faster builds<br></h3><p><span></span><span></span>With growing engineering and testing bandwidth we can continue to replace battle-tested but complex or slow dependencies with simpler and faster ones. I’ve directly or indirectly benefitted from this with <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2024/05/17/enabling-rust-lld-on-linux/#benefits">linking</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/95035#issuecomment-1073966631">locks</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/73441">backtraces</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/35437">platform-optimised routines</a>, <a target="_blank" rel="noopener noreferrer" href="https://www.memorysafety.org/blog/rustls-server-perf/">TLS</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rustup/issues/3790">HTTP</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/cargo/issues/11813">git</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/cargo/pull/15417">compression</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/37817">building</a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rustup">switching</a> toolchains, and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/cargo/pull/9992">build automation</a>. A few I’m particularly looking forward to gaining more attention are a <a target="_blank" rel="noopener noreferrer" href="https://github.com/sunfishcode/eyra">pure-Rust</a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/wg-cargo-std-aware">less special</a> <code>std</code>, reduced reliance on system <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/140525">linkers</a> and <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/rustc/codegen-options/index.html#link-self-contained">libs</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/ctz/graviola">pure-Rust crypto</a>, a <a target="_blank" rel="noopener noreferrer" href="https://github.com/cberner/redb">durable BTreeMap</a>, and a Rust <a target="_blank" rel="noopener noreferrer" href="https://github.com/bevyengine/bevy">game engine</a>. The latter not for me to use but because game devs bring some of the most <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/lcsek8/any_thoughts_on_panic_unreachable_unchecked/">aggressive optimisation</a> I’ve seen and I’d love to benefit from it.<br></p><p><img src="https://tably.com/static/lpnAkiLIDiu.png" width="2342" height="644"><span></span><span></span></p><p><span></span><span></span><a target="_blank" rel="noopener noreferrer" href="https://hn.algolia.com/?query=How%20to%20speed%20up%20the%20Rust%20compiler">Significant expertise and effort</a> is going into performance work. In the last few months Tably saw a 60% compile time improvement across frontend and backend. Many of the biggest wins have come from meta changes that aren’t visible on this chart, including <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/45320#issuecomment-337957101">ThinLTO and backend parallelism</a>, <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2016/09/08/incremental/">incremental</a>, <a target="_blank" rel="noopener noreferrer" href="https://internals.rust-lang.org/t/evaluating-pipelined-rustc-compilation/10199">metadata pipelining</a>, <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2023/11/09/parallel-rustc/">front-end parallelism</a>. Meta changes that might bear further fruit are: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rustc_codegen_cranelift">Rust-oriented codegen</a>, incremental <a target="_blank" rel="noopener noreferrer" href="https://github.com/davidlattimore/wild">linkers</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/127634#issuecomment-2224274508">dead-code elimination</a>, <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/inside-rust/2024/12/04/trait-system-refactor-initiative/">a new solver</a>, and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/cargo/issues/10673"><code>cargo test</code> caching results</a>.<br></p><h3><span></span><span></span>Improved portability and less <code>#[cfg()]</code><br></h3><p><span></span><span></span>Testing the power set of valid <code>#[cfg()]</code> options, targets and features on CI is generally prohibitive. This creates untested code paths and potential compile failures, alongside annoyances like fragmented and incomplete documentation, impaired IDE functionality and <code>unused_imports</code> warnings.<br></p><p><span></span><span></span>A <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rfcs/blob/master/text/1868-portability-lint.md">lint</a> was proposed to mitigate this. A mooted better approach is <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/41619#issuecomment-2056902943">moving <code>#[cfg()]</code> into the trait system</a> (<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/lkgoyk/comment/gnjufff/">cc</a>) with bounds like <code>where Platform: Unix</code> and <code>where Platform: Avx2</code>. With this the compiler naturally guarantees compilation under all supported options, targets and features. Specializing items with OS-specific or architecture-specific implementations can be done via, appropriately, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/31844">specialization</a>. Metadata can be shared between dev/test/release, different feature combination and different platform builds, resulting in less recompilation, faster CI, and bringing a crates.io MIR <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/5hav0q/towards_a_worldwide_precompiled_crate_cache_for/">cache</a> closer to reality.<br></p><h3><span></span><span></span>Everything being <code>const</code><br></h3><p><span></span><span></span>Constant evaluation reduces dependence on macros and build scripts, and brings forward run-time work and panics to compile-time. Currently we only scratch the surface of this as only a small subset of Rust code is supported (no traits, no allocation), and that which is supported runs <a target="_blank" rel="noopener noreferrer" href="https://docs.rs/hex-literal">too slowly</a> for nontrivial use cases.<br></p><p><span></span><span></span>Assuming portability all code can be evaluated during compilation (<a target="_blank" rel="noopener noreferrer" href="https://docs.rs/crabtime/latest/crabtime/#-one-shot-evaluation">cc</a>). Unportable code like FFI or assembly without a generic fallback is getting less common. <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/const-eval/issues/1">Various</a> effects-inspired attempts to enable ubiquitous <code>const</code> have been <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/67792">made</a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/110395">reverted</a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rfcs/pull/3762">attempted again</a>. I hope we dodge this extra language complexity with the simple assumption that “everything can be executed in a <code>const</code> context”. There are <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/const-eval/issues/20#issuecomment-464124376">tricky</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/unsafe-code-guidelines/issues/522">issues</a> to overcome but the language simplicity will be worth it.<br></p><h3><span></span><span></span>Simpler concurrency<br></h3><p><span></span><span></span><code>async</code> has a relatively high complexity cost for us due to the <a target="_blank" rel="noopener noreferrer" href="https://docs.rs/tokio-util/0.7.15/tokio_util/task/struct.LocalPoolHandle.html#method.spawn_pinned"><code>'static</code> bound</a> (<a target="_blank" rel="noopener noreferrer" href="https://maciej.codes/2022-06-09-local-async.html">cc</a>), <a target="_blank" rel="noopener noreferrer" href="https://google.github.io/comprehensive-rust/concurrency/async-pitfalls/cancellation.html">cancellation-safety</a>, and <a target="_blank" rel="noopener noreferrer" href="https://google.github.io/comprehensive-rust/concurrency/async-pitfalls/async-traits.html">restrictions</a> relating to <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2023/12/21/async-fn-rpit-in-traits/#is-it-okay-to-use-async-fn-in-traits-what-are-the-limitations">traits</a> and <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2023/12/21/async-fn-rpit-in-traits/#dynamic-dispatch"><code>dyn</code></a>. These currently <a target="_blank" rel="noopener noreferrer" href="https://github.com/tokio-rs/tokio/issues/2596#issuecomment-708441238">seem</a> <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/1gvblzb/comment/ly0o3hi/">insoluble</a>. Bifurcation between <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/std/iter/trait.Iterator.html">sync</a> and <a target="_blank" rel="noopener noreferrer" href="https://docs.rs/futures/latest/futures/prelude/stream/trait.StreamExt.html">async</a> primitives and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/futures-rs/issues/2109#issuecomment-608413396">ecosystem</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/tokio-rs/tokio/issues/1529#issuecomment-2149262072">idiosyncrasies</a> further increase <a target="_blank" rel="noopener noreferrer" href="https://eta.st/2021/03/08/async-rust-2.html">the</a> <a target="_blank" rel="noopener noreferrer" href="https://trouble.mataroa.blog/blog/asyncawait-is-real-and-can-hurt-you/">async</a> <a target="_blank" rel="noopener noreferrer" href="https://bitbashing.io/async-rust.html">tax</a>. <a target="_blank" rel="noopener noreferrer" href="https://blog.yoshuawuyts.com/extending-rusts-effect-system/">Effects-inspired</a> solutions <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/119y8ex/comment/j9pt1h3/">seem unpromising</a>.<br></p><p><span></span><span></span>Pre-1.0 Rust had a solution: <a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/29791031/what-happened-to-libgreen"><code>libgreen</code></a>. It achieved concurrency in user-space without this bifurcation, but at <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rfcs/blob/master/text/0230-remove-runtime.md#the-problems">significant performance, portability and maintenance cost</a> and so was <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/18967">removed</a>. With increased engineering bandwidth this may be worth revisiting. Again the language simplicity would be worth it. (One day I’ll create a PoC of zero-cost wrapping of <code>std::{fs, net}</code> plus <code>fiber::{spawn, select}</code> using <a target="_blank" rel="noopener noreferrer" href="https://github.com/Xudong-Huang/generator-rs"><code>generator</code></a>!)<br></p><h3><span></span><span></span>Excelling in more domains<br></h3><p><span></span><span></span>Rust in the browser feels under-explored: we bump into issues that suggest there <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/issues/2388">aren’t</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/issues/2392">many</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/pull/2598">others</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/pull/3899">using</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/issues/3801">it</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/issues/3687#issuecomment-1806486879">seriously</a>. Table stakes like <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/issues/3687">avoiding UB</a> and getting stack traces <a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry-javascript/issues/9968">cross-browser</a> took our time and effort. <a target="_blank" rel="noopener noreferrer" href="https://github.com/leptos-rs/leptos"><code>leptos</code></a>—the web framework behind this site—has come a long way (much like our previous choice <a target="_blank" rel="noopener noreferrer" href="https://github.com/sycamore-rs/sycamore"><code>sycamore</code></a>), yet rough edges remain.<br></p><p><span></span><span></span>Some domains like rapid prototyping and <a target="_blank" rel="noopener noreferrer" href="https://www.bartoszsypytkowski.com/is-rust-a-good-fit-for-business-apps/">business logic</a> may remain outside of Rust’s reach due to deliberate design decisions that trade off iteration speed. For most domains though I think it’s a matter of time. Adoption hurdles remain for GUIs, machine learning, and <a target="_blank" rel="noopener noreferrer" href="https://loglog.games/blog/leaving-rust-gamedev/">game development</a> due to entrenched ecosystems, high switching costs, cultural inertia, and established tooling. But those barriers aren’t permanent. Rust continues to mature and steadily break into these domains, thanks to relentless community efforts and innovation.<br></p><h2><span></span><span></span>Conclusion<br></h2><p><span></span><span></span>Looking forward the trajectory is clear and exciting. The positive feedback loop of adoption boosting engineering and testing bandwidth, which then fuels further adoption, is accelerating. The upcoming decade promises further refinement, faster compile times, broader domain adoption, and an increasingly seamless developer experience. At Tably, we remain committed to pushing the boundaries of what’s possible with Rust, excited to witness—and contribute to—the language’s next chapter. Here’s to another 10 years of betting on Rust!<br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From tokens to thoughts: How LLMs and humans trade compression for meaning (108 pts)]]></title>
            <link>https://arxiv.org/abs/2505.17117</link>
            <guid>44189426</guid>
            <pubDate>Thu, 05 Jun 2025 07:59:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2505.17117">https://arxiv.org/abs/2505.17117</a>, See on <a href="https://news.ycombinator.com/item?id=44189426">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2505.17117">View PDF</a>
    <a href="https://arxiv.org/html/2505.17117v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Chen Shani [<a href="https://arxiv.org/show-email/dc6ef439/2505.17117" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2505.17117v1" rel="nofollow">[v1]</a></strong>
        Wed, 21 May 2025 16:29:00 UTC (2,582 KB)<br>
    <strong>[v2]</strong>
        Mon, 26 May 2025 21:13:36 UTC (2,582 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Air Lab – A portable and open air quality measuring device (341 pts)]]></title>
            <link>https://networkedartifacts.com/airlab/simulator</link>
            <guid>44189329</guid>
            <pubDate>Thu, 05 Jun 2025 07:42:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://networkedartifacts.com/airlab/simulator">https://networkedartifacts.com/airlab/simulator</a>, See on <a href="https://news.ycombinator.com/item?id=44189329">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[End of an Era: Landsat 7 Decommissioned After 25 Years of Earth Observation (106 pts)]]></title>
            <link>https://www.usgs.gov/news/national-news-release/end-era-landsat-7-decommissioned-after-25-years-earth-observation</link>
            <guid>44188248</guid>
            <pubDate>Thu, 05 Jun 2025 04:09:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.usgs.gov/news/national-news-release/end-era-landsat-7-decommissioned-after-25-years-earth-observation">https://www.usgs.gov/news/national-news-release/end-era-landsat-7-decommissioned-after-25-years-earth-observation</a>, See on <a href="https://news.ycombinator.com/item?id=44188248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure role="group">
  <div>
    
  

    <figcaption>These Landsat 7 images showcase the first and last captures of the Las Vegas area, taken on July 4, 1999, and May 28, 2024, respectively. The images highlight the city, the surrounding desert landscape, and Lake Mead, using shortwave infrared (SWIR), near-infrared (NIR), and red bands to emphasize differences in vegetation, water, and urban growth. The final image, marking the satellite’s 25th anniversary, stands as a tribute to Landsat 7's quarter-century legacy of Earth observation.</figcaption>
  </div>
</figure>
<p>While Landsat 7’s long watch over Earth comes to an end, Landsat 8, launched in 2013, and Landsat 9, launched in 2020, continue to work together to create a complete snapshot of Earth every eight days. Their successor—Landsat Next—is currently planned to launch in the early 2030s and provide even greater coverage and detail.</p><p>Launched in 1999 as a joint mission of the USGS and NASA, Landsat 7 significantly enhanced Earth observations and provided a key part of the Landsat program’s five decade-plus record of imaging the planet’s surface. The satellite's imagery will remain&nbsp;<a href="https://earthexplorer.usgs.gov/">archived</a> at the USGS Earth Resources Observation and Science Center, continuing to support scientific discovery and decision-making for the future.</p><p>“The Landsat satellites have delivered over 50 years of extraordinary science data, economic value and national security benefits by informing decisions in every sector of the economy—from monitoring drought in the West to guiding disaster recovery,” said <strong>Sarah Ryker, USGS Acting Director</strong>. “For 25 of those years, Landsat 7’s data helped farmers, land managers, city planners, and scientists, as well as communities around the world better understand and manage land, water, and other natural resources.”</p><p>Landsat 7 achieved many milestones over its 25 years of operation and was the first Landsat to downlink data to the newly established USGS ground station in Sioux Falls, South Dakota. It was also the first Landsat satellite to be fully operated 24/7 by the USGS after being launched by NASA.&nbsp;</p><p>Its Enhanced Thematic Mapper Plus sensor delivered improved high-resolution imagery that expanded its capabilities, capturing critical historical events such as the aftermath of 9/11, Hurricane Katrina, and the Deepwater Horizon oil spill. The satellite also contributed to important projects, including the&nbsp;<a href="https://lima.usgs.gov/">Landsat Image Mosaic of Antarctica</a>, and inspired the "<a href="https://www.usgs.gov/centers/eros/earth-art">Earth As Art</a>" collection, showcasing stunning visuals of the planet.&nbsp;</p><p>After ending its official mission in 2024, the USGS prepared Landsat 7 for decommissioning to follow responsible space practices and U.S. policies on keeping space clear of debris. The final steps included carefully lowering the satellite's orbit to decrease the risk of collisions and ensuring that all energy sources, such as fuel and batteries, are depleted to prevent the satellite from accidentally turning back on or creating debris. As Landsat 7 begins this decommissioned phase, it will drift silently in orbit for about 55 years before reentering Earth’s atmosphere.&nbsp;</p><p>To learn more about Landsat 7’s distinguished mission, visit: <a href="https://www.usgs.gov/landsat-missions/news/a-final-farewell-landsat-7">A Final Farewell to Landsat 7 | U.S. Geological Survey</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Old payphones get new life, thanks to Vermont engineer (125 pts)]]></title>
            <link>https://www.core77.com/posts/137183/Engineer-Fixes-and-Re-Installs-Old-Payphones-Provides-Free-Calls-to-the-Public</link>
            <guid>44188204</guid>
            <pubDate>Thu, 05 Jun 2025 04:00:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.core77.com/posts/137183/Engineer-Fixes-and-Re-Installs-Old-Payphones-Provides-Free-Calls-to-the-Public">https://www.core77.com/posts/137183/Engineer-Fixes-and-Re-Installs-Old-Payphones-Provides-Free-Calls-to-the-Public</a>, See on <a href="https://news.ycombinator.com/item?id=44188204">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
    <div id="post_header">
        

            <ul>
                


                            </ul>
        

            <h2>Engineer Fixes and Re-Installs Old Payphones, Provides Free Calls to the Public </h2>
                    <h2>Patrick Schlott's RandTel</h2>
                    

        
    </div>

    




            <section id="post">




<p id="e6b1c2_1150" data-ic-marker="cba9c1_2864">Payphones "were the only things that were built to last for decades and be out in the elements," says electrical engineer Patrick Schlott. He should know; as a hobby, he buys secondhand payphones, rewires them, then asks local businesses in rural Vermont if they'd let him install them. His goal is to offer, for free, public telephone service. (Schlott foots the bill himself.)</p><p id="c5e647_2917" data-ic-marker="7cf2fe_2466"><img src="https://s3files.core77.com/blog/images/1723601_81_137183_kpeo47g3E.jpg"></p><p id="9a6bcd_3553" data-ic-marker="dba584_3342"><img src="https://s3files.core77.com/blog/images/1723605_81_137183_vA5MEq9TD.jpg"> </p><p id="4bf8c5_5474" data-ic-marker="ae6d05_1832"><img src="https://s3files.core77.com/blog/images/1723606_81_137183_qJB0s8tAa.jpg">  </p><p id="5e9bc_4785" data-ic-marker="4cc280_1160">"It's assumed most folks own cell phones," writes Schlott. "Well, not everyone does, sometimes they don't work out on dirt roads, sometimes you forget your charger, and sometimes you just really need to make a phone call. We aim to provide a valuable public service to the community while teaching people about the US telephone system that has over a century of history behind it."</p><p id="80e5b4_741" data-ic-marker="e1b546_1909"><img src="https://s3files.core77.com/blog/images/1723598_81_137183_3jwvCPXrg.jpg"> </p><p id="df568f_1321" data-ic-marker="fde2a4_1693"><img data-image-width="880" data-image-height="879" data-image-id="1723604" src="https://s3files.core77.com/blog/images/1723604_81_137183_7Ov2GElFL.jpg"></p><p id="d88795_6843" data-ic-marker="5fc854_3489">Schlott's company, <a href="https://randtel.co/" rel="">RandTel</a>, currently operates three phones in his neck of Vermont: One at the North Tunbridge General Store in Tunbridge, one at the Latham Library and a third—a rotary model from the 1950s--at the town of Randolph's information booth. He's particularly proud of that last one, as "This installation is 100% solar-powered, provided graciously by Catamount Solar," he writes. "Many thanks to the White River Valley Chamber of Commerce for hosting!"</p><p id="54cf2d_185" data-ic-marker="307d76_3913"><img src="https://s3files.core77.com/blog/images/1723595_81_137183_gT0mw7bIA.jpg"></p><p id="8e0ad6_568" data-ic-marker="ec336b_1835"><img src="https://s3files.core77.com/blog/images/1723596_81_137183_uRqLcCL_e.jpg"> </p><p id="6b31de_964" data-ic-marker="6c8ffe_1656"><img src="https://s3files.core77.com/blog/images/1723610_81_137183_Vg4976xOm.jpg"></p><p id="147407_1071" data-ic-marker="92d262_1337"><img src="https://s3files.core77.com/blog/images/1723603_81_137183_OOm_XuQld.jpg"> </p><p id="da9100_451" data-ic-marker="746ec4_3951"><img src="https://s3files.core77.com/blog/images/1723597_81_137183_8ao8X9lBJ.jpg"> </p><p id="32c173_36" data-ic-marker="74d4b0_2910"><img src="https://s3files.core77.com/blog/images/1723599_81_137183_gyZ4iO7wV.jpg"></p><p id="2ac73e_107" data-ic-marker="3b6999_3940"><img src="https://s3files.core77.com/blog/images/1723600_81_137183_P67Na8vKw.jpg"> </p><p id="1cc4a4_5093" data-ic-marker="1d346b_3959">Here's a look at what Schlott does:</p><p id="406a7e_2513"><iframe width="560" height="315" src="https://www.youtube.com/embed/DwXTUM2Y7L0?si=LQ_geDiPeryisVer" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe><em id="aa17fa_4828">Enter a caption (optional)</em></p>
        

            </section>

                


                


<div>
    <ul>
        <li data-this-post-id="137183" data-this-author-id="0">
            
            
            <p>Favorite This</p>
        </li>
       <li data-this-post-title="Engineer Fixes and Re-Installs Old Payphones, Provides Free Calls to the Public " data-this-post-id="137183" data-this-author-id="0">
            
            
            <p>Comment</p>
        </li>
        
    </ul> 
</div>






           


        </div></div>]]></description>
        </item>
    </channel>
</rss>