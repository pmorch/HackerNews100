<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 30 Oct 2024 23:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Chain-of-Thought Can Hurt Performance on Tasks Where Thinking Makes Humans Worse (134 pts)]]></title>
            <link>https://arxiv.org/abs/2410.21333</link>
            <guid>41999340</guid>
            <pubDate>Wed, 30 Oct 2024 19:42:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.21333">https://arxiv.org/abs/2410.21333</a>, See on <a href="https://news.ycombinator.com/item?id=41999340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.21333">View PDF</a>
    <a href="https://arxiv.org/html/2410.21333v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, we find that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. We also identify three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, our results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help us identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, we offer a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Ryan Liu [<a href="https://arxiv.org/show-email/2af21f3d/2410.21333">view email</a>]      <br>    <strong>[v1]</strong>
        Sun, 27 Oct 2024 18:30:41 UTC (2,612 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Steam games will need to disclose kernel-level anti-cheat on store pages (353 pts)]]></title>
            <link>https://www.gamingonlinux.com/2024/10/steam-games-will-now-need-to-fully-disclose-kernel-level-anti-cheat-on-store-pages/</link>
            <guid>41999314</guid>
            <pubDate>Wed, 30 Oct 2024 19:39:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamingonlinux.com/2024/10/steam-games-will-now-need-to-fully-disclose-kernel-level-anti-cheat-on-store-pages/">https://www.gamingonlinux.com/2024/10/steam-games-will-now-need-to-fully-disclose-kernel-level-anti-cheat-on-store-pages/</a>, See on <a href="https://news.ycombinator.com/item?id=41999314">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						<p>Valve announced a change for Steam today that will make things a lot clearer for everyone, as developers will now need to clearly list the kernel-level anti-cheat used on Steam store pages.</p>

<p>In the Steamworks Developer <a href="https://steamcommunity.com/groups/steamworks/announcements/detail/4547038620960934857?snr=2___" rel="noopener nofollow" target="_blank">post</a> Valve said: "We've heard from more and more developers recently that they're looking for the right way to share anti-cheat information about their game with players. At the same time, players have been requesting more transparency around the anti-cheat services used in games, as well as the existence of any additional software that will be installed within the game."</p>

<p>Developers with games already on Steam will also need to do this, as it's not just for new games coming up for release, and it is also part of the release process now too. So Valve will be doing checks on games to ensure the notices are there and correct.</p>

<p>However, it's only being <em>forced</em> for kernel-level anti-cheat. If it's only client-side or server-side, it's <em>optional</em>, but Valve say "we generally think that any game that makes use of anti-cheat technology would benefit from letting players know".</p>

<p>Valve's example pictured below:</p>

<p><img src="https://uploads.golmedia.net/uploads/articles/article_media/7927772011730316555gol1.png"></p>
						<p><span>Article taken from <a href="https://www.gamingonlinux.com/">GamingOnLinux.com.</a></span>
						
					</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's TOS doesn't eliminate a user's Fourth Amendment rights, judge rules [pdf] (242 pts)]]></title>
            <link>https://ww3.ca2.uscourts.gov/decisions/isysquery/0814a460-fe8f-42ef-9e82-cf94f952eb28/1/doc/23-6181_opn.pdf</link>
            <guid>41998891</guid>
            <pubDate>Wed, 30 Oct 2024 18:58:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ww3.ca2.uscourts.gov/decisions/isysquery/0814a460-fe8f-42ef-9e82-cf94f952eb28/1/doc/23-6181_opn.pdf">https://ww3.ca2.uscourts.gov/decisions/isysquery/0814a460-fe8f-42ef-9e82-cf94f952eb28/1/doc/23-6181_opn.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=41998891">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[CFPB finalizes personal financial data rights rule (117 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2024/10/no-matter-what-bank-says-its-your-money-your-data-and-your-choice</link>
            <guid>41998192</guid>
            <pubDate>Wed, 30 Oct 2024 17:55:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2024/10/no-matter-what-bank-says-its-your-money-your-data-and-your-choice">https://www.eff.org/deeplinks/2024/10/no-matter-what-bank-says-its-your-money-your-data-and-your-choice</a>, See on <a href="https://news.ycombinator.com/item?id=41998192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><span>The Consumer Finance Protection Bureau (CFPB) has just </span><a href="https://www.consumerfinance.gov/about-us/newsroom/cfpb-finalizes-personal-financial-data-rights-rule-to-boost-competition-protect-privacy-and-give-families-more-choice-in-financial-services/"><span>finalized a rule</span></a><span> that makes it easy and safe for you to figure out which bank will give you the best deal and switch to that bank, with just a couple of clicks.&nbsp;</span></p>
<p><span>We </span><i><span>love</span></i><span> this kind of thing: the coolest thing about a digital world is how easy it is to switch from product or service to another—in theory. Digital tools are so flexible, anyone who wants your business can </span><a href="https://www.eff.org/deeplinks/2019/10/adversarial-interoperability"><span>write a program</span></a><span> to import your data into a new service and forward any messages or interactions that show up at the old service.</span></p>
<p><span>That's the theory. But in practice, companies have figured out how to use law - IP law, cybersecurity law, contract law, trade secrecy law—to </span><a href="https://www.eff.org/deeplinks/2016/07/section-1201-dmca-cannot-pass-constitutional-scrutiny"><span>literally criminalize</span></a><span> this kind of marvelous digital flexibility, so that it can end up being even </span><i><span>harder</span></i><span> to switch away from a digital service than it is to hop around among traditional, analog ones.</span></p>
<p><span>Companies </span><i><span>love</span></i><span> lock-in. The harder it is to quit a product or service, the worse a company can treat you without risking your business. Economists call the difficulties you face in leaving one service for another the "</span><a href="https://www.eff.org/deeplinks/2021/08/facebooks-secret-war-switching-costs"><span>switching costs</span></a><span>" and businesses go to great lengths to raise the switching costs they can impose on you if you have the temerity to be a disloyal customer.&nbsp;</span></p>
<p><span>So long as it's easier to coerce your loyalty than it is to earn it, companies win and their customers lose. That's where the new CFPB rule comes in.</span></p>
<p><span>Under this rule, you can authorize a third party - another bank, a comparison shopping site, a broker, or just your bookkeeping software - to request your account data from your bank. The bank has to give the third party </span><i><span>all</span></i><span> the data you've authorized. This data can include your transaction history and all the data needed to set up your payees and recurring transactions somewhere else.</span></p>
<p><span>That means that—for example—you can authorize a comparison shopping site to access some of your bank details, like how much you pay in overdraft fees and service charges, how much you earn in interest, and what your loans and credit cards are costing you. The service can use this data to figure out which bank will cost you the least and pay you the most.&nbsp;</span></p>
<p><span>Then, once you've opened an account with your new best bank, you can direct it to request </span><i><span>all</span></i><span> your data from your old bank, and with a few clicks, get fully set up in your new financial home. All your payees transfer over, all your regular payments, all the transaction history you'll rely on at tax time. "Painless" is an admittedly weird adjective to apply to household finances, but this comes pretty darned close.</span></p>
<p><span>Americans lose a </span><i><span>lot</span></i><span> of money to banking fees and low interest rates. How much? Well, CFPB economists, using a </span><i><span>very</span></i><span> conservative methodology, estimate that this rule will make the American public </span><i><span>at least $677 million</span></i><span> better off, </span><i><span>every year.</span></i></p>
<p><span>Now, that $677 million has to come from somewhere, and it does: it comes from the banks that are currently charging sky-high fees and paying rock-bottom interest. The largest of these banks are </span><a href="https://www.americanbanker.com/news/cfpbs-open-banking-rule-faces-suit-from-bank-policy-institute"><span>suing the CFPB</span></a><span> in a bid to block the rule from taking effect.</span></p>
<p><span>These banks claim that they are doing this to protect us, their depositors, from a torrent of fraud that would be unleashed if we were allowed to give third parties access to our own financial data. Clearly, this is the only reason a giant bank would want to make it harder for us to change to a competitor (it can't possibly have anything to do with the $677 million we stand to save by switching).</span></p>
<p><span>We've heard arguments like these before. While EFF takes a back seat to no one when it comes to defending user security (</span><a href="https://www.eff.org/cases/bernstein-v-us-dept-justice"><span>we practically invented this</span></a><span>), we reject the idea that </span><a href="https://www.eff.org/deeplinks/2023/12/without-interoperability-apple-customers-will-never-be-secure"><span>user security is improved when corporations lock us in</span></a><span> (and </span><a href="https://www.eff.org/document/letter-bruce-schneier-senate-judiciary-regarding-app-store-security"><span>leading security experts agree with us</span></a><span>).</span></p>
<p><span>This is not to say that a </span><i><span>bad</span></i><span> data-sharing interoperability rule wouldn't be, you know, </span><i><span>bad</span></i><span>. A rule that lacked the proper safeguards could indeed enable a wave of fraud and identity theft the likes of which we've never seen.</span></p>
<p><span>Thankfully, this is a </span><i><span>good</span></i><span> interoperability rule! </span><a href="https://www.eff.org/deeplinks/2023/10/you-wanna-break-your-bank-cfpb-wants-help-you-do-it"><span>We liked it when it was first proposed</span></a><span>, and it got </span><i><span>even better</span></i><span> through the rulemaking process.</span></p>
<p><span>First, the CFPB had the wisdom to know that a federal finance agency probably wasn't the best—or only—group of people to design a data-interchange standard. Rather than telling the banks exactly how they should transmit data when requested by their customers, the CFPB instead said, "These are the data you need to share and these are the characteristics of a good standards body. So long as you use a standard from a good standards body that shares this data, you're in compliance with the rule." This is an approach we've advocated for years, and it's the first time we've seen it in the wild.</span></p>
<p><span>The CFPB also instructs the banks to fail safe: any time a bank gets a request to share your data that it thinks might be fraudulent, they have the right to block the process until they can get more information and confirm that everything is on the up-and-up.</span></p>
<p><span>The rule also regulates the third parties that can get your data, establishing stringent criteria for which kinds of entities can do this. It also limits </span><i><span>how</span></i><span> they can use your data (strictly for the purposes you authorize) and what they need to do with the data when that has been completed (delete it forever), and what else they are allowed to do with it (nothing). There's also a mini "</span><a href="https://www.ftc.gov/news-events/news/press-releases/2024/10/federal-trade-commission-announces-final-click-cancel-rule-making-it-easier-consumers-end-recurring"><span>click-to-cancel</span></a><span>" rule that guarantees that you can instantly revoke any third party's access to your data, for any reason.</span></p>
<p><span>The CFPB has had the authority to make a rule like this since its founding in 2010, with the passage of the Consumer Financial Protection Act (CFPA). Back when the CFPA was working its way through Congress, the banks howled that they were being forced to give up "their" data to their competitors.</span></p>
<p><span>But it's not their data. It's </span><i><span>your</span></i><span> data. The decision about who you share it with belongs to </span><i><span>you</span></i><span>, and you alone. </span></p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Karma connection in Chrome Web Store (138 pts)]]></title>
            <link>https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/</link>
            <guid>41997823</guid>
            <pubDate>Wed, 30 Oct 2024 17:23:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/">https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/</a>, See on <a href="https://news.ycombinator.com/item?id=41997823">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>Somebody <a href="https://gist.github.com/c0m4r/45e15fc1ec13c544393feafca30e74de">brought to my attention</a> that the Hide YouTube Shorts extension for Chrome changed hands and turned malicious. I looked into it and could confirm that it contained two undisclosed components: one performing <a href="https://www.investopedia.com/terms/a/affiliate-fraud.asp">affiliate fraud</a> and the other sending users’ every move to some Amazon cloud server. But that wasn’t all of it: I discovered eleven more extensions written by the same people. Some contained only the affiliate fraud component, some only the user tracking, some both. A few don’t appear to be malicious yet.</p>
<p>While most of these extensions were supposedly developed or bought by a person without any other traces online, one broke this pattern. Karma shopping assistant has been on Chrome Web Store since 2020, the company behind it founded in 2013. This company employs more than 50 people and secured tons of cash in venture capital. Maybe a mistake on my part?</p>
<p>After looking thoroughly this explanation seems unlikely. Not only does Karma share some backend infrastructure and considerable amounts of code with the malicious extensions. Not only does Karma Shopping Ltd. admit to selling users’ browsing profiles in their privacy policy. There is even more tying them together, including a mobile app developed by Karma Shopping Ltd. whereas the identical Chrome extension is supposedly developed by the mysterious evildoer.</p>
<figure><img src="https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/karma.png" alt="Screenshot of the karmanow.com website, with the Karma logo visible and a yellow button “Add to Chrome - It’s Free”" width="718" height="468"></figure>

<div id="tocBox">
  <h4>Contents</h4>
  <nav id="TableOfContents">
  <ul>
    <li><a href="#the-affected-extensions">The affected extensions</a></li>
    <li><a href="#hiding-in-plain-sight">Hiding in plain sight</a></li>
    <li><a href="#affiliate-fraud-functionality">Affiliate fraud functionality</a></li>
    <li><a href="#browsing-profile-collection">Browsing profile collection</a></li>
    <li><a href="#who-is-behind-this">Who is behind this?</a></li>
    <li><a href="#what-does-karma-shopping-want-with-the-data">What does Karma Shopping want with the data?</a></li>
  </ul>
</nav>
</div>

<h2 id="the-affected-extensions"><a href="#the-affected-extensions"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>The affected extensions</h2>
<p>Most of the extensions in question changed hands relatively recently, the first ones in the summer of 2023. The malicious code has been added immediately after the ownership transfer, with some extensions even requesting additional privileges citing bogus reasons. A few extensions have been developed this year by whoever is behind this.</p>
<p>Some extensions from the latter group don’t have any obvious malicious functionality at this point. If there is tracking, it only covers the usage of the extension’s user interface rather than the entire browsing behavior. This can change at any time of course.</p>
<div><table>
<thead>
<tr>
<th>Name</th>
<th>Weekly active users</th>
<th>Extension ID</th>
<th>Malicious functionality</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hide YouTube Shorts</td>
<td>100,000</td>
<td>aljlkinhomaaahfdojalfmimeidofpih</td>
<td>Affiliate fraud, browsing profile collection</td>
</tr>
<tr>
<td>DarkPDF</td>
<td>40,000</td>
<td>cfemcmeknmapecneeeaajnbhhgfgkfhp</td>
<td>Affiliate fraud, browsing profile collection</td>
</tr>
<tr>
<td>Sudoku On The Rocks</td>
<td>1,000</td>
<td>dncejofenelddljaidedboiegklahijo</td>
<td>Affiliate fraud</td>
</tr>
<tr>
<td>Dynamics 365 Power Pane</td>
<td>70,000</td>
<td>eadknamngiibbmjdfokmppfooolhdidc</td>
<td>Affiliate fraud, browsing profile collection</td>
</tr>
<tr>
<td>Israel everywhere</td>
<td>70</td>
<td>eiccbajfmdnmkfhhknldadnheilniafp</td>
<td>–</td>
</tr>
<tr>
<td>Karma | Online shopping, but better</td>
<td>500,000</td>
<td>emalgedpdlghbkikiaeocoblajamonoh</td>
<td>Browsing profile collection</td>
</tr>
<tr>
<td>Where is Cookie?</td>
<td>93</td>
<td>emedckhdnioeieppmeojgegjfkhdlaeo</td>
<td>–</td>
</tr>
<tr>
<td>Visual Effects for Google Meet</td>
<td>1,000,000</td>
<td>hodiladlefdpcbemnbbcpclbmknkiaem</td>
<td>Affiliate fraud</td>
</tr>
<tr>
<td>Quick Stickies</td>
<td>106</td>
<td>ihdjofjnmhebaiaanaeeoebjcgaildmk</td>
<td>–</td>
</tr>
<tr>
<td>Nucleus: A Pomodoro Timer and Website Blocker</td>
<td>20,000</td>
<td>koebbleaefghpjjmghelhjboilcmfpad</td>
<td>Affiliate fraud, browsing profile collection</td>
</tr>
<tr>
<td>Hidden Airline Baggage Fees</td>
<td>496</td>
<td>kolnaamcekefalgibbpffeccknaiblpi</td>
<td>Affiliate fraud</td>
</tr>
<tr>
<td>M3U8 Downloader</td>
<td>100,000</td>
<td>pibnhedpldjakfpnfkabbnifhmokakfb</td>
<td>Affiliate fraud</td>
</tr>
</tbody>
</table></div>
<h2 id="hiding-in-plain-sight"><a href="#hiding-in-plain-sight"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Hiding in plain sight</h2>
<p>Whoever wrote the malicious code chose not to obfuscate it but to make it blend in with the legitimate functionality of the extension. Clearly, the expectation was that nobody would look at the code too closely. So there is for example this:</p>
<div><pre tabindex="0"><code data-lang="js"><span><span><span>if</span> <span>(</span><span>window</span><span>.</span><span>location</span><span>.</span><span>href</span><span>.</span><span>startsWith</span><span>(</span><span>"http"</span><span>)</span> <span>||</span>
</span></span><span><span>    <span>window</span><span>.</span><span>location</span><span>.</span><span>href</span><span>.</span><span>includes</span><span>(</span><span>"m.youtube.com"</span><span>))</span> <span>{</span>
</span></span><span><span>  <span>…</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>It <em>looks</em> like the code inside the block would only run on YouTube. Only when you stop and consider the logic properly you realize that it runs on every website. In fact, that’s the block wrapping the calls to malicious functions.</p>
<p>The malicious functionality is split between content script and background worker for the same reason, even though it could have been kept in one place. This way each part looks innocuous enough: there is some data collection in the content script, and then it sends a <code>check_shorts</code> message to the background worker. And the background worker “checks shorts” by querying some web server. Together this just <em>happens</em> to send your entire browsing history into the Amazon cloud.</p>
<p>Similarly, there are some complicated checks in the content script which eventually result in a <code>loadPdfTab</code> message to the background worker. The background worker dutifully opens a new tab for that address and, strangely, closes it after 9 seconds. Only when you sort through the layers it becomes obvious that this is actually about adding an affiliate cookie.</p>
<p>And of course there is a bunch of usual complicated conditions, making sure that this functionality is not triggered too soon after installation and generally doesn’t pop up reliably enough that users could trace it back to this extension.</p>
<h2 id="affiliate-fraud-functionality"><a href="#affiliate-fraud-functionality"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Affiliate fraud functionality</h2>
<p>The affiliate fraud functionality is tied to the <code>kra18.com</code> domain. When this functionality is active, the extension will regularly download data from <code>https://www.kra18.com/v1/selectors_list?&amp;ex=90</code> (90 being the extension ID here, the server accepts eight different extension IDs). That’s a long list containing 6,553 host names:</p>
<figure><img src="https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/selectors.png" alt="Screenshot of JSON data displayed in the browser. The selectors key is expanded, twenty domain names like drinkag1.com are visible in the list." width="325" height="448"></figure>

<p>Whenever one of these domains is visited and the moons are aligned in the right order, another request to the server is made with the full address of the page you are on. For example, the extension could request <code>https://www.kra18.com/v1/extension_selectors?u=https://www.tink.de/&amp;ex=90</code>:</p>
<figure><img src="https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/affiliate_link.png" alt="Screenshot of JSON data displayed in the browser. There are keys shortsNavButtonSelector, url and others. The url key contains a lengthy URL from awin1.com domain." width="573" height="145"></figure>

<p>The <code>shortsNavButtonSelector</code> key is another red herring, the code only <em>appears</em> to be using it. The important key is <code>url</code>, the address to be opened in order to set the affiliate cookie. And that’s the address sent via <code>loadPdfTab</code> message mentioned before if the extension decides that right now is a good time to collect an affiliate commission.</p>
<p>There are also additional “selectors,” downloaded from <code>https://www.kra18.com/v1/selectors_list_lr?&amp;ex=90</code>. Currently this functionality is only used on the <code>amazon.com</code> domain and will replace some product links with links going through <code>jdoqocy.com</code> domain, again making sure an affiliate commission is collected. That domain is owned by Common Junction LLC, an affiliate marketing company that published a <a href="https://www.cj.com/case-study/shoptagr-cj-publisher-onboarding-team-case-study">case study</a> on how their partnership with Karma Shopping Ltd. (named Shoptagr Ltd. back then) helped drive profits.</p>
<h2 id="browsing-profile-collection"><a href="#browsing-profile-collection"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Browsing profile collection</h2>
<p>Some of the extensions will send each page visit to <code>https://7ng6v3lu3c.execute-api.us-east-1.amazonaws.com/EventTrackingStage/prod/rest</code>. According to the extension code, this is an Alooma backend. Alooma is a data integration platform which has been acquired by Google a while ago. Data transmitted could look like this:</p>
<figure><img src="https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/tracking.png" alt="Screenshot of query string parameters displayed in Developer Tools. The parameters are: token: sBGUbZm3hp, timestamp: 1730137880441, user_id: 90, distinct_id: 7796931211, navigator_language: en-US, referrer: https://www.google.com/, local_time: Mon Oct 28 2024 18:51:20 GMT+0100 (Central European Standard Time), event: page_visit, component: external_extension, external: true, current_url: https://example.com/" width="488" height="251"></figure>

<p>Yes, this is sent for each and every page loaded in the browser, at least after you’ve been using the extension for a while. And <code>distinct_id</code> is my immutable user ID here.</p>
<p>But wait, it’s a bit different for the Karma extension. Here you can opt out! Well, that’s only if you are using Firefox because Mozilla is rather strict about unexpected data collection. And if you manage to understand what “User interactions” means on this options page:</p>
<figure><img src="https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/karma_options.png" alt="Screenshot of an options page with two switches labeled User interactions and URL address. The former is described with the text: Karma is a community of people who are working together to help each other get a great deal. We collect anonymized data about coupon codes, product pricing, and information about Karma is used to contribute back to the community. This data does not contain any personably identifiable information such as names or email addresses, but may include data supplied by the browser such as url address." width="575" height="398"></figure>

<p>Well, I may disagree with the claim that <a href="https://palant.info/2020/02/18/insights-from-avast/jumpshot-data-pitfalls-of-data-anonymization/">url addresses do not contain personably identifiable information</a>. And: yes, this is the entire page. There really isn’t any more text.</p>
<p>The data transmitted is also somewhat different:</p>
<figure><img src="https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/tracking2.png" alt="Screenshot of query string parameters displayed in Developer Tools. The parameters are: referrer: https://www.google.com/, current_url: https://example.com/, browser_version: 130, tab_id: 5bd19785-e18e-48ca-b400-8a74bf1e2f32, event_number: 1, browser: chrome, event: page_visit, source: extension, token: sBGUbZm3hp, version: 10.70.0.21414, timestamp: 1730138671937, user_id: 6372998, distinct_id: 6b23f200-2161-4a1d-9400-98805c17b9e3, navigator_language: en-US, local_time: Mon Oct 28 2024 19:04:31 GMT+0100 (Central European Standard Time), ui_config: old_save, save_logic: rules, show_k_button: true, show_coupon_scanner: true, show_popups: true" width="495" height="430"></figure>

<p>The <code>user_id</code> field no longer contains the extension ID but my personal identifier, complementing the identifier in <code>distinct_id</code>. There is a <code>tab_id</code> field adding more context, so that it is not only possible to recognize which page I navigated to and from where but also to distinguish different tabs. And some more information about my system is always useful of course.</p>
<h2 id="who-is-behind-this"><a href="#who-is-behind-this"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Who is behind this?</h2>
<p>Eleven extensions on my list are supposedly developed by a person going by the name Rotem Shilop or Roni Shilop or Karen Shilop. This isn’t a very common last name, and if this person really exists it managed to leave no traces online. Yes, I also searched in Hebrew. Yet one extension is developed by Karma Shopping Ltd. (formerly Shoptagr Ltd.), a company based in Israel with at least 50 employees. An accidental association?</p>
<p>It doesn’t look like it. I’m not going into the details of shared code and tooling, let’s just say: it’s very obvious that all twelve extensions are being developed by the same people. Of course, there is still the possibility that the eleven malicious extensions are not associated directly with Karma Shopping but with some rogue employee or contractor or business partner.</p>
<p>However, it isn’t only the code. As <a href="#browsing-profile-collection">explained above</a>, five extensions including Karma share the same tracking backend which is found nowhere else. They are even sending the same access token. Maybe this backend isn’t actually run by Karma Shopping and they are only one of the customers of some third party? Yet if you look at the data being sent, clearly the Karma extension is considered first-party. It’s the other extensions which are sending <code>external: true</code> and <code>component: external_extension</code> flags.</p>
<p>Then maybe Karma Shopping is merely buying data from a third party, without actually being affiliated with their extensions? Again, this is possible but unlikely. One indicator is the <code>user_id</code> field in the data sent by these extensions. It’s the same extension ID that they use for internal communication with the <code>kra18.com</code> server. If Karma Shopping were granting a third party access to their server, wouldn’t they assign that third party some IDs of their own?</p>
<p>And those affiliate links produced by the <code>kra18.com</code> server? Some of them clearly mention <code>karmanow.com</code> as the affiliate partner.</p>
<figure><img src="https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/affiliate_link2.png" alt="Screenshot of JSON data displayed in the browser. url key is a long link pointing to go.skimresources.com. sref query parameter of the link is https://karmanow.com. url query parameter of the link is www.runinrabbit.com." width="855" height="95"></figure>

<p>Finally, if we look at Karma Shopping’s mobile apps, they develop two of them. In addition to the Karma app, the app stores also contain an app called “Sudoku on the Rocks,” developed by Karma Shopping Ltd. Which is a very strange coincidence because an identical “Sudoku on the Rocks” extension also exists in the Chrome Web Store. Here however the developer is Karen Shilop. And Karen Shilop chose to include hidden affiliate fraud functionality in their extension.</p>
<p>By the way, guess who likes the Karma extension a lot and left a five-star review?</p>
<figure><img src="https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/review.png" alt="Screenshot of a five-star review by Rona Shilop with a generic-looking avatar of woman with a cup of coffee. The review text says: Thanks for making this amazing free extension. There is a reply by Karma Support saying: We’re so happy to hear how much you enjoy shopping with Karma." width="631" height="241"></figure>

<p>I contacted Karma Shopping Ltd. via their public relations address about their relationship to these extensions and the Shilop person but didn’t hear back so far.</p>
<p><strong>Update</strong> (2024-10-30): An extension developer told me that they were contacted on multiple independent occasions about selling their Chrome extension to Karma Shopping, each time by C-level executives of the company, from official <code>karmanow.com</code> email addresses. The first outreach was in September 2023, where Karma was supposedly looking into adding extensions to their portfolio as part of their growth strategy. They offered to pay between $0.2 and $1 per weekly active user.</p>

<p>It is obvious why Karma Shopping Ltd. would want to add their affiliate functionality to more extensions. After all, affiliate commissions are their line of business. But why collect browsing histories? Only to publish <a href="https://jonathan-65927.medium.com/far-from-being-impulsive-buyers-millennials-agonize-over-online-purchases-bc0dbbf5f2ba">semi-insightful articles on people’s shopping behavior</a>?</p>
<p>Well, let’s have a look at <a href="https://www.karmanow.com/privacy">their privacy policy</a> which is actually meaningful for a change. Under 1.3.4 it says:</p>
<blockquote>
<p><strong>Browsing Data.</strong> In case you a user of our browser extensions we may collect data regarding web browsing data, which includes web pages visited, clicked stream data and information about the content you viewed.</p>
<p><strong>How we Use this Data.</strong> We use this Personal Data (1) in order to provide you with the Services and feature of the extension and (2) we will share this data in an aggregated, anonymized manner, for marketing research and commercial use with our business partners.</p>
<p><strong>Legal Basis.</strong> (1) We process this Personal Data for the purpose of providing the Services to you, which is considered performance of a contract with you. (2) When we process and share the aggregated and anonymized data we will ask for your consent.</p>
</blockquote>
<p>First of all, this tells us that Karma collecting browsing data is official. They also openly state that they are selling it. Good to know and probably good for their business as well.</p>
<p>As to the legal basis: I am no lawyer but I have a strong impression that they don’t deliver on the “we will ask for your consent” promise. No, not even that Firefox options page qualifies as informed consent. And this makes this whole data collection rather doubtful in the light of GDPR.</p>
<p>There is also a difference between anonymized and pseudonymized data. The data collection seen here is pseudonymized: while it doesn’t include my name, there is a persistent user identifier which is still linked to me. It is usually fairly easy to deanonymize pseudonymized browsing histories, e.g. because people tend to visit their social media profiles rather often.</p>
<p>Actually anonymized data would not allow associating it with any single person. This is very hard to achieve, and we’ve seen <a href="https://palant.info/2020/02/18/insights-from-avast/jumpshot-data-pitfalls-of-data-anonymization/">promises of aggregated and anonymized data go very wrong</a>. While it’s theoretically possible that Karma correctly anonymizes and aggregates data on the server side, this is a rather unlikely outcome for a company that, as <a href="#browsing-profile-collection">we’ve seen above</a>, confuses the lack of names and email addresses with anonymity.</p>
<p>But of course these considerations only apply to the Karma extension itself. Because related extensions like Hide YouTube Shorts just straight out lie:</p>
<figure><img src="https://palant.info/2024/10/30/the-karma-connection-in-chrome-web-store/privacy.png" alt="Screenshot of a Chrome Web Store listing. Text under the heading Privacy: The developer has disclosed that it will not collect or use your data." width="494" height="90"></figure>

<p>Some of these extensions actually used to have a privacy policy before they were bought. Now only three still have an identical and completely bogus privacy policy. Sudoku on the Rocks happens to be among these three, and the same privacy policy is linked by the Sudoku on the Rocks mobile apps which are officially developed by Karma Shopping Ltd.</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cheap solar panels are changing the world (107 pts)]]></title>
            <link>https://www.theatlantic.com/science/archive/2024/10/solar-power-energy-revolution-global-south/680351/</link>
            <guid>41996425</guid>
            <pubDate>Wed, 30 Oct 2024 15:46:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/science/archive/2024/10/solar-power-energy-revolution-global-south/680351/">https://www.theatlantic.com/science/archive/2024/10/solar-power-energy-revolution-global-south/680351/</a>, See on <a href="https://news.ycombinator.com/item?id=41996425">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header data-event-module="hero"><div><figure><div data-flatplan-lead_figure_media="true"><picture><img alt="Color picture of a fuzzy white orb against an orange, red, and black background" sizes="(min-width: 976px) 976px, 100vw" srcset="https://cdn.theatlantic.com/thumbor/1tVAqPhOKJ9-Ama_9rJvchKXqa0=/0x0:2700x1519/750x422/media/img/mt/2024/10/2024_10_23_7764/original.jpg 750w, https://cdn.theatlantic.com/thumbor/poGcucq5ljH2A8eXPv8DSzC6ihQ=/0x0:2700x1519/828x466/media/img/mt/2024/10/2024_10_23_7764/original.jpg 828w, https://cdn.theatlantic.com/thumbor/vzl34JyqeomOmxgWi5RcYdaQFhI=/0x0:2700x1519/960x540/media/img/mt/2024/10/2024_10_23_7764/original.jpg 960w, https://cdn.theatlantic.com/thumbor/-NIQgd0bqmqphOjZdN1F-p3_-jQ=/0x0:2700x1519/976x549/media/img/mt/2024/10/2024_10_23_7764/original.jpg 976w, https://cdn.theatlantic.com/thumbor/qsDScksnq-e6h-728nh23UbXE6I=/0x0:2700x1519/1952x1098/media/img/mt/2024/10/2024_10_23_7764/original.jpg 1952w" src="https://cdn.theatlantic.com/thumbor/vzl34JyqeomOmxgWi5RcYdaQFhI=/0x0:2700x1519/960x540/media/img/mt/2024/10/2024_10_23_7764/original.jpg" id="article-lead-image" width="960" height="540"></picture></div><figcaption data-flatplan-lead_figure_caption="true">Franz Gruenewald / Connected Archives</figcaption></figure></div><gpt-ad format="injector" sizes-at-0="mobile-wide" targeting-pos="injector-article-start" sizes-at-976="desktop-wide"></gpt-ad></header><div data-view-action="view - audio player - start" data-view-label="680351" data-event-module="audio player" data-event-content-type="narrated" data-event-module-state="start" data-event-view="true"><div><p><img alt="Color picture of a fuzzy white orb against an orange, red, and black background" sizes="80px" srcset="https://cdn.theatlantic.com/thumbor/NrENC2WTV6PoxwAzBNU1kHPt0qU=/591x0:2110x1519/80x80/media/img/mt/2024/10/2024_10_23_7764/original.jpg 80w, https://cdn.theatlantic.com/thumbor/KnebwVKvhRfzj3DksnCWgV3ylB4=/591x0:2110x1519/96x96/media/img/mt/2024/10/2024_10_23_7764/original.jpg 96w, https://cdn.theatlantic.com/thumbor/r5lm88Wb1WGE8NpCKHyimiJOHB8=/591x0:2110x1519/128x128/media/img/mt/2024/10/2024_10_23_7764/original.jpg 128w, https://cdn.theatlantic.com/thumbor/dz3XkvczCXE5zjTXKlTRdjQ0rLA=/591x0:2110x1519/160x160/media/img/mt/2024/10/2024_10_23_7764/original.jpg 160w, https://cdn.theatlantic.com/thumbor/_FBXT_ch4BJpeyXbD_oKGoZRNXM=/591x0:2110x1519/192x192/media/img/mt/2024/10/2024_10_23_7764/original.jpg 192w, https://cdn.theatlantic.com/thumbor/_GioWRDwGCI0on9bq086G6tlBI8=/591x0:2110x1519/256x256/media/img/mt/2024/10/2024_10_23_7764/original.jpg 256w, https://cdn.theatlantic.com/thumbor/yUkHVOX7E_V2sZEvyP-kabOZtjY=/591x0:2110x1519/384x384/media/img/mt/2024/10/2024_10_23_7764/original.jpg 384w, https://cdn.theatlantic.com/thumbor/6WNHdMgu43KzHaLNHOnBijKJnJs=/591x0:2110x1519/512x512/media/img/mt/2024/10/2024_10_23_7764/original.jpg 512w" src="https://cdn.theatlantic.com/thumbor/NrENC2WTV6PoxwAzBNU1kHPt0qU=/591x0:2110x1519/80x80/media/img/mt/2024/10/2024_10_23_7764/original.jpg" width="80" height="80"></p></div><p>Produced by ElevenLabs and News Over Audio (NOA) using AI narration.</p></div><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true"><em><small>Updated at 1:40 p.m. ET on October 25, 2024</small></em></p><p data-flatplan-paragraph="true">Last month, an energy think tank released some rare good news for the climate: The world is on track to install 29 percent more solar capacity this year than it did the year before, according to a report from <a data-event-element="inline link" href="https://ember-energy.org/latest-insights/solar-power-continues-to-surge-in-2024/">Ember</a>. “In a single year, in a single technology, we’re providing as much new electricity as the entirety of global growth the year before,” Kingsmill Bond, a senior energy strategist at RMI, a clean-energy nonprofit, told me. A decade or two ago, analysts “did not imagine in their wildest dreams that solar by the middle of the 2020s would already be supplying all of the growth of global electricity demand,” he said. Yet here we are.</p><p data-flatplan-paragraph="true">In the United States, solar accounted for <a data-event-element="inline link" href="https://grist.org/energy/solar-hits-a-renewable-energy-milestone-not-seen-since-wwii/">more than half</a> of all new power last year. But the most dramatic growth is happening overseas. The latest global report from the International Energy Agency (IEA) notes that solar is on track to <a data-event-element="inline link" href="https://www.iea.org/reports/world-energy-outlook-2024">overtake</a> all other forms of energy by 2033. The world’s use of fossil fuels is already plateauing (the U.S., for its part, hit its peak demand for fossil-fuel energy <a data-event-element="inline link" href="https://www.eia.gov/todayinenergy/detail.php?id=45096#:~:text=The%20share%20of%20U.S.%20total%20energy%20consumption%20that%20originated%20from,increase%20in%20natural%20gas%20consumption.">way back in 2007</a>). Energy demand is still rising, but renewables are stepping in to make up the difference. “The really interesting debate now,” Bond said, “is actually: When do we push fossil fuels off the plateau? And from our numbers, if solar keeps on growing this way, it’s going to be off the plateau by the end of this decade.”</p><p data-flatplan-paragraph="true">The advantages of solar speak for themselves. Solar can be built faster and with fewer permits than other forms of energy infrastructure, mostly because the panels are flat and modular (unlike, say, a towering wind turbine or a hulking gas-fired power plant). It’s also adaptable at any scale, from an individual erecting a single panel to a utility company assembling a solar farm. And now, thanks to remarkable drops in prices for solar panels, mainly from China, simple market forces seem to be driving an all-out solar boom. “This is unstoppable,” Heymi Bahar, a senior energy analyst at the IEA, told me.</p><p data-flatplan-paragraph="true">Globally, some 40 percent of solar’s growth is in the form of people powering their own homes and businesses, Bahar said. Perhaps nowhere is this better illustrated than in Africa, where Joel Nana, a project manager at Sustainable Energy Africa in Cape Town, has been leading an effort to help countries regulate and integrate the explosion of small-scale solar. When Nana and his team started quantifying just how much new solar was around, “we were actually shocked,” he told me. In South Africa, for example, the total amount of energy produced from solar systems in 2019 was thought to be about 500 megawatts, Nana said. But in the first quarter of 2023, when researchers <a data-event-element="inline link" href="https://www.sseg.org.za/wp-content/uploads/2024/02/Status-of-EG-in-South-African-Municipalities-2023-FINAL-2.pdf">used satellite imagery</a> to count all of the solar installations in the country, they estimated that solar was producing a combined 5,700 megawatts of energy—only 55 percent of which had been declared to the government. That story of rapid, invisible growth is being repeated across the continent. Kenya now has about 200 megawatts of rooftop solar installed, representing 9 percent of the country’s total energy use, Nana said. Namibia has about 96 megawatts of rooftop solar capacity in its system, he said—a whopping 15 percent of its energy mix. “It’s been happening for three or four years, maybe five years, completely off the radar,” Nana said.</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/magazine/archive/2020/03/climate-change-peer-pressure/605515/">From the March 2020 issue: Thy neighbor’s solar panels</a></p><p data-flatplan-paragraph="true">Solar seems to have passed a tipping point: In many countries, the low cost of the technology is propelling its own growth, despite little government help. In South Africa, businesses such as shopping malls and factories have historically run diesel generators to deal with frequent power outages. Many still do, but now others are saving money by installing solar panels. Electricity from a diesel generator costs about 10 rand per kilowatt-hour, Nana said; with solar panels, it plummets to about two rand. “It’s literally a no-brainer for a business owner,” he said. Businesses make up 80 percent of small-scale solar capacity in the country, according to his research. Soon, Nana hopes, arrays and batteries will become cheap enough that more homeowners across the continent will be able to afford switching to solar. And, as the journalist Bill McKibben has reported, some homeowners in African countries who have never been connected to the grid are getting electricity for the very first time via solar-panel kits, <a data-event-element="inline link" href="https://www.newyorker.com/magazine/2017/06/26/the-race-to-solar-power-africa">skipping over</a> a fossil-fuel phase entirely.</p><p data-flatplan-paragraph="true">Across the global South, solar is capturing unprecedented portions of the energy market. <a data-event-element="inline link" href="https://www.bloomberg.com/news/articles/2024-08-09/pakistan-sees-solar-boom-as-chinese-imports-surge-bnef-says?sref=OVk78rkt">Pakistan</a>, for example, imported the equivalent of a quarter of its total energy capacity in Chinese solar panels in just the first six months of this year. Many countries in the global South lack significant fossil-fuel resources, and importing them is expensive. “By far the easiest way to obtain economic growth in a country with a lot of sunshine and no fossil fuels is by exploiting your own domestic resources,” Bond said. Already, in countries including Brazil, Morocco, Mexico, and Uruguay, solar and wind make up a bigger share of electricity generation than it does in global-North countries. By 2030, RMI <a data-event-element="inline link" href="https://rmi.org/insight/powering-up-the-global-south/?utm_campaign=heatmap_am&amp;utm_medium=email&amp;_hsenc=p2ANqtz--BBjdCevYygk974VQXcaVPv4j6TzxSY16EAOWZzFTxiQVb1IaUzr5KB04lHP5hWB-Z4i04a3GWJstmBl3UfnMazBK6ig&amp;_hsmi=329173745&amp;utm_content=329173745&amp;utm_source=hs_email">predicts</a>, the global South will have quadrupled its solar and wind capacity.</p><p data-flatplan-paragraph="true">That estimate doesn’t account for China, which is experiencing an unparalleled solar boom. In addition to supplying the rest of the world with panels, China installed more than half of the planet’s new solar capacity within its own borders in 2023, and the Ember report says it’s on track to add a similar amount this year. In 2023, the country more than doubled its own solar capacity year over year. “Nobody was expecting that it would be so high,” Bahar said.</p><p id="injected-recirculation-link-1" data-view-action="view link - injected link - item 2" data-event-element="injected link" data-event-position="2"><a href="https://www.theatlantic.com/science/archive/2021/06/why-the-us-doesnt-really-make-solar-panels-anymore-industrial-policy/619213/">Read: Why America doesn’t really make solar panels anymore</a></p><p data-flatplan-paragraph="true">Last year, at the United Nations Climate Change Conference, or COP28, in Dubai, 132 countries and the European Union <a data-event-element="inline link" href="https://www.cop28.com/en/global-renewables-and-energy-efficiency-pledge#:~:text=Commit%20to%20work%20together%20to,starting%20points%20and%20national%20circumstances">pledged</a> to triple the world’s renewable-energy capacity by 2030. According to Bahar, it’s the only promise of the many made in Dubai that’s likely to even be close to fulfilled: The world is on track to add 2.7 times its renewable capacity by then, and 80 percent of that increase will come from solar. To make use of all this growth, the world will have to add much more storage and transmission capacity, neither of which are keeping up with solar’s pace. The IEA, where Bahar works, will advocate for new pledges on those two fronts at COP29 next month. A world that mostly runs on solar power will also need something else—such as hydropower, nuclear, or geothermal—to generate energy when the sun isn’t shining in the evenings and winters. Jessika Trancik, an MIT professor who models clean-energy development, told me that governments need to steer investments toward storage and alternate forms of energy to compensate for that inherent downtime. That way, the world can have a reliable energy mix when 50 or 60 percent of electricity generation comes from solar and wind. That may seem far off, she said—solar made up about <a data-event-element="inline link" href="https://ember-energy.org/latest-insights/the-global-solar-revolution/">5.5 percent</a> of global electricity in 2023—but with the exponential growth of cheap solar, “before you know it, it’s upon you.”</p><p data-flatplan-paragraph="true">For Africa’s quiet solar boom to meet its full potential, governments will need to regulate and subsidize the technology, Nana said. Federal departments in Namibia, Kenya, and Eswatini have largely ignored the ascendance of solar technology within their borders, Nana said. Yet in South Africa, he’s seeing bright spots. Last year, the government began providing subsidies for solar for the first time. This year, its updated energy plan acknowledged that small-scale solar will be the biggest player in the country in the next decade. If South Africa is any indication, a solar revolution will arrive in more countries in the coming years. It may even sneak up on them.</p><hr><p data-flatplan-paragraph="true"><small><em>This article originally misstated that solar power made up 5.5 percent of global energy in 2023. It made up 5.5 percent of global electricity.</em></small></p></section><div data-event-module="footer"><p><h3>About the Author</h3></p><div><address id="article-writer-0" data-event-element="author" data-event-position="1" data-flatplan-bio="true"><div><p><a href="https://www.theatlantic.com/author/zoe-schlanger/" data-event-element="image"><img alt="" loading="lazy" src="https://cdn.theatlantic.com/thumbor/u8S5JVOtiftKFSYJUPgPfsZK710=/468x9:2384x1925/120x120/media/img/authors/2023/10/Sten_SSENSE_ZoeSchlanger_18_hi_res_version/original.jpg" width="60" height="60"></a></p><div><p><a href="https://www.theatlantic.com/author/zoe-schlanger/" data-label="https://www.theatlantic.com/author/zoe-schlanger/" data-action="click author - name">Zoë Schlanger</a> is a staff writer at <em>The Atlantic</em>. She is the author of <em><a href="https://bookshop.org/p/books/the-light-eaters-how-the-unseen-world-of-plant-intelligence-offers-a-new-understanding-of-life-on-earth-zoe-schlanger/20890522?ean=9780063073852">The Light Eaters</a>, </em>about the world of plant-behavior-and-intelligence research.</p></div></div></address></div></div><gpt-ad format="injector" sizes-at-0="mobile-wide,native,house" targeting-pos="injector-most-popular" sizes-at-976="desktop-wide,native,house"></gpt-ad></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pushing the frontiers of audio generation (154 pts)]]></title>
            <link>https://deepmind.google/discover/blog/pushing-the-frontiers-of-audio-generation/</link>
            <guid>41995730</guid>
            <pubDate>Wed, 30 Oct 2024 15:02:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/pushing-the-frontiers-of-audio-generation/">https://deepmind.google/discover/blog/pushing-the-frontiers-of-audio-generation/</a>, See on <a href="https://news.ycombinator.com/item?id=41995730">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
    
  
  
  
    

    
    
      
        <div>
          
            
            
              
              
<div>
    <div>
      <p>Technologies</p>
      

      
    <dl>
      
        <dt>Published</dt>
        <dd><time datetime="2024-10-30">30 October 2024</time></dd>
      
      
        <dt>Authors</dt>
        
      
    </dl>
  

      
    </div>

    
      
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/_jjPl1Kv1mv1Qz5QdR_nJ-3rnaqSrwTabpyrdabzYl7hytlaIMj_i3elkt5o3pPVCjI-9M0Nuazevo3Jr81VubuIV4QY9IZsxVDPktGMGeODFIFn=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/_jjPl1Kv1mv1Qz5QdR_nJ-3rnaqSrwTabpyrdabzYl7hytlaIMj_i3elkt5o3pPVCjI-9M0Nuazevo3Jr81VubuIV4QY9IZsxVDPktGMGeODFIFn=w2144-h1206-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/_jjPl1Kv1mv1Qz5QdR_nJ-3rnaqSrwTabpyrdabzYl7hytlaIMj_i3elkt5o3pPVCjI-9M0Nuazevo3Jr81VubuIV4QY9IZsxVDPktGMGeODFIFn=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/_jjPl1Kv1mv1Qz5QdR_nJ-3rnaqSrwTabpyrdabzYl7hytlaIMj_i3elkt5o3pPVCjI-9M0Nuazevo3Jr81VubuIV4QY9IZsxVDPktGMGeODFIFn=w1856-h1044-n-nu-rw 2x"><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/_jjPl1Kv1mv1Qz5QdR_nJ-3rnaqSrwTabpyrdabzYl7hytlaIMj_i3elkt5o3pPVCjI-9M0Nuazevo3Jr81VubuIV4QY9IZsxVDPktGMGeODFIFn=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/_jjPl1Kv1mv1Qz5QdR_nJ-3rnaqSrwTabpyrdabzYl7hytlaIMj_i3elkt5o3pPVCjI-9M0Nuazevo3Jr81VubuIV4QY9IZsxVDPktGMGeODFIFn=w1056-h594-n-nu-rw 2x">
      <img alt="An illustration depicting speech patterns, iterative progress on dialogue generation,  and a relaxed conversation between two voices." height="603" src="https://lh3.googleusercontent.com/_jjPl1Kv1mv1Qz5QdR_nJ-3rnaqSrwTabpyrdabzYl7hytlaIMj_i3elkt5o3pPVCjI-9M0Nuazevo3Jr81VubuIV4QY9IZsxVDPktGMGeODFIFn=w1072-h603-n-nu" width="1072">
    </picture>
    
  
    
  </div>
            
          
            
            
              
              <div>
  <h4 data-block-key="yrcbl">Our pioneering speech generation technologies are helping people around the world interact with more natural, conversational and intuitive digital assistants and AI tools.</h4><p data-block-key="3cjpl">Speech is central to human connection. It helps people around the world exchange information and ideas, express emotions and create mutual understanding. As our technology built for generating natural, dynamic voices continues to improve, we’re unlocking richer, more engaging digital experiences.</p><p data-block-key="bh9mr">Over the past few years, we’ve been pushing the frontiers of audio generation, developing models that can create high quality, natural speech from a range of inputs, like text, tempo controls and particular voices. This technology powers single-speaker audio in many Google products and experiments — including <a href="https://blog.google/products/gemini/made-by-google-gemini-ai-updates/" rel="noopener" target="_blank">Gemini Live</a>, <a href="https://deepmind.google/technologies/gemini/project-astra/" rel="noopener" target="_blank">Project Astra</a>, <a href="https://cloud.google.com/text-to-speech/docs/voice-types" rel="noopener" target="_blank">Journey Voices</a> and <a href="https://blog.youtube/news-and-events/made-on-youtube-2024/" rel="noopener" target="_blank">YouTube’s auto dubbing</a> — and is helping people around the world interact with more natural, conversational and intuitive digital assistants and AI tools.</p><p data-block-key="7838d">Working together with partners across Google, we recently helped develop two new features that can generate long-form, multi-speaker dialogue for making complex content more accessible:</p><ul><li data-block-key="5j7f7"><a href="https://notebooklm.google/" rel="noopener" target="_blank">NotebookLM Audio Overviews</a> turns uploaded documents into engaging and lively dialogue. With one click, two AI hosts summarize user material, make connections between topics and banter back and forth.</li><li data-block-key="9q17j"><a href="https://illuminate.google.com/" rel="noopener" target="_blank">Illuminate</a> creates formal AI-generated discussions about research papers to help make knowledge more accessible and digestible.</li></ul><p data-block-key="c3kro">Here, we provide an overview of our latest speech generation research underpinning all of these products and experimental tools.</p>
</div>
            
          
            
            
              
              <div>
  <h2 data-block-key="auxvk">Pioneering techniques for audio generation</h2><p data-block-key="2rhdn">For years, we've been investing in audio generation research and exploring new ways for generating more natural dialogue in our products and experimental tools. In our previous research on <a href="https://research.google/blog/soundstorm-efficient-parallel-audio-generation/" rel="noopener" target="_blank">SoundStorm</a>, we first demonstrated the ability to generate 30-second segments of natural dialogue between multiple speakers.</p><p data-block-key="ecfnq">This extended our earlier work, <a href="https://research.google/blog/soundstream-an-end-to-end-neural-audio-codec/" rel="noopener" target="_blank">SoundStream</a> and <a href="https://google-research.github.io/seanet/audiolm/examples/" rel="noopener" target="_blank">AudioLM</a>, which allowed us to apply many text-based language modeling techniques to the problem of audio generation.</p><p data-block-key="7m7rr">SoundStream is a neural audio codec that efficiently compresses and decompresses an audio input, without compromising its quality. As part of the training process, SoundStream learns how to map audio to a range of acoustic tokens. These tokens capture all of the information needed to reconstruct the audio with high fidelity, including properties such as <a href="https://en.wikipedia.org/wiki/Prosody_(linguistics)" rel="noopener" target="_blank">prosody</a> and <a href="https://en.wikipedia.org/wiki/Timbre" rel="noopener" target="_blank">timbre</a>.</p><p data-block-key="3bngd">AudioLM treats audio generation as a language modeling task to produce the acoustic tokens of codecs like SoundStream. As a result, the AudioLM framework makes no assumptions about the type or makeup of the audio being generated, and can flexibly handle a variety of sounds without needing architectural adjustments — making it a good candidate for modeling multi-speaker dialogues.</p>
</div>
            
          
            
            
              
              


            
          
            
            
              
              




<figure aria-labelledby="single-media-ab666b1b-5b78-44a6-a805-9b31cf82c4e3-figcaption">
  
  
    <figcaption id="single-media-ab666b1b-5b78-44a6-a805-9b31cf82c4e3-figcaption">
      <p data-block-key="1snyy">Example of a multi-speaker dialogue generated by NotebookLM Audio Overview, based on a few potato-related documents.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <p data-block-key="6qplr">Building upon this research, our latest speech generation technology can produce 2 minutes of dialogue, with improved naturalness, speaker consistency and acoustic quality, when given a script of dialogue and speaker turn markers. The model also performs this task in under 3 seconds on a single <a href="https://cloud.google.com/tpu/docs/v5e" rel="noopener" target="_blank">Tensor Processing Unit (TPU) v5e chip</a>, in one inference pass. This means it generates audio over 40-times faster than real time.</p>
            
          
            
            
              
              <div>
  <h2 data-block-key="uq3en">Scaling our audio generation models</h2><p data-block-key="du2ro">Scaling our single-speaker generation models to multi-speaker models then became a matter of data and model capacity. To help our latest speech generation model produce longer speech segments, we created an even more efficient speech codec for compressing audio into a sequence of tokens, in as low as 600 bits per second, without compromising the quality of its output.</p><p data-block-key="g9ht">The tokens produced by our codec have a hierarchical structure and are grouped by time frames. The first tokens within a group capture phonetic and prosodic information, while the last tokens encode fine acoustic details.</p><p data-block-key="7vshj">Even with our new speech codec, producing a 2-minute dialogue requires generating over 5000 tokens. To model these long sequences, we developed a specialized <a href="https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/" rel="noopener" target="_blank">Transformer</a> architecture that can efficiently handle hierarchies of information, matching the structure of our acoustic tokens.</p><p data-block-key="8djch">With this technique, we can efficiently generate acoustic tokens that correspond to the dialogue, within a single autoregressive inference pass. Once generated, these tokens can be decoded back into an audio waveform using our speech codec.</p>
</div>
            
          
            
            
              
              




<figure aria-labelledby="single-media-a9d3eb76-9a4f-464c-a5fa-a7f1fbd5b760-figcaption">
  
  
    <figcaption id="single-media-a9d3eb76-9a4f-464c-a5fa-a7f1fbd5b760-figcaption">
      <p data-block-key="57qqn">Animation showing how our speech generation model produces a stream of audio tokens autoregressively, which are decoded back to a waveform consisting of a two-speaker dialogue.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <p data-block-key="269cj">To teach our model how to generate realistic exchanges between multiple speakers, we pretrained it on hundreds of thousands of hours of speech data. Then we finetuned it on a much smaller dataset of dialogue with high acoustic quality and precise speaker annotations, consisting of unscripted conversations from a number of voice actors and realistic <a href="https://en.wikipedia.org/wiki/Speech_disfluency" rel="noopener" target="_blank">disfluencies</a> — the “umm”s and “aah”s of real conversation. This step taught the model how to reliably switch between speakers during a generated dialogue and to output only studio quality audio with realistic pauses, tone and timing.</p><p data-block-key="8shit">In line with our <a href="https://ai.google/responsibility/principles/" rel="noopener" target="_blank">AI Principles</a> and our commitment to developing and deploying AI technologies responsibly, we’re incorporating our SynthID technology to watermark non-transient AI-generated audio content from these models, to help safeguard against the potential misuse of this technology.</p>
</div>
            
          
            
            
              
              <div>
  <h2 data-block-key="wao8a">New speech experiences ahead</h2><p data-block-key="dr1sf">We’re now focused on improving our model’s fluency, acoustic quality and adding more fine-grained controls for features, like prosody, while exploring how best to combine these advances with other modalities, such as video.</p><p data-block-key="82sf">The potential applications for advanced speech generation are vast, especially when combined with our Gemini family of models. From enhancing learning experiences to making content more universally accessible, we’re excited to continue pushing the boundaries of what’s possible with voice-based technologies.</p>
</div>
            
          
            
            
              
              


            
          
            
            
              
              
            
          
            
            
              
              



  
    
  

            
          
        </div>
      
    

    
  
  

  

  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[M4 MacBook Pro (637 pts)]]></title>
            <link>https://www.apple.com/newsroom/2024/10/new-macbook-pro-features-m4-family-of-chips-and-apple-intelligence/</link>
            <guid>41995701</guid>
            <pubDate>Wed, 30 Oct 2024 15:00:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2024/10/new-macbook-pro-features-m4-family-of-chips-and-apple-intelligence/">https://www.apple.com/newsroom/2024/10/new-macbook-pro-features-m4-family-of-chips-and-apple-intelligence/</a>, See on <a href="https://news.ycombinator.com/item?id=41995701">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    
    
	
	

</nav>





<main id="main" role="main"> 




<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">






    
    
    









    





    <div>
        
		
        <div>
                    
                    
                        <span>PRESS RELEASE</span>
                    
                    
                        <span>October 30, 2024</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        Apple’s new MacBook&nbsp;Pro features the incredibly powerful M4 family of chips and ushers in a new era with Apple&nbsp;Intelligence
    

                    </h2>
                
            </div>

        <div>
                
                
                    With an advanced 12MP Center Stage camera, Thunderbolt&nbsp;5 on M4&nbsp;Pro and M4&nbsp;Max models, and an all-new nano-texture display option, MacBook&nbsp;Pro gets even more capable and even more pro
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, An almost shut MacBook Pro emits a colorful glowing light.">
        <div>
             
              
              <div>
                Supercharged by Apple Intelligence, even more powerful Apple silicon with the M4 family of chips, and new capabilities, MacBook Pro accelerates pro workloads like never before.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-hero.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-hero_big" aria-label="Download media, An almost shut MacBook Pro emits a colorful glowing light."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong><span>CUPERTINO, CALIFORNIA</span> </strong>Apple today unveiled the new <a href="https://www.youtube.com/watch?v=G0cmfY7qdmY" target="_blank" rel="nofollow" data-analytics-exit-link="">MacBook Pro</a>, powered by the M4 family of chips — M4, M4 Pro, and M4 Max — delivering much faster performance and enhanced capabilities. The new MacBook Pro is built for Apple Intelligence, the personal intelligence system that transforms how users work, communicate, and express themselves, while protecting their privacy. Now available in space black and silver finishes, the 14-inch MacBook Pro includes the blazing-fast performance of M4 and three Thunderbolt 4 ports, starting with 16GB of memory, all at just $1,599. The 14- and 16-inch models with M4 Pro and M4 Max offer Thunderbolt 5 for faster transfer speeds and advanced connectivity. All models include a Liquid Retina XDR display that gets even better with an all-new nano-texture display option and up to 1000 nits of brightness for SDR content, an advanced 12MP Center Stage camera, along with up to 24 hours of battery life, the longest ever in a Mac.<sup>1</sup> The new MacBook Pro is available to pre-order today, with availability beginning November 8.
</div>
                 
             
                 <div>“MacBook Pro is an incredibly powerful tool that millions of people use to do their life’s best work, and today we’re making it even better,” said John Ternus, Apple’s senior vice president of Hardware Engineering. “With the powerful M4 family of chips, and packed with pro features like Thunderbolt 5, an advanced 12MP Center Stage camera, an all-new nano-texture display option, and Apple Intelligence, the new MacBook Pro continues to be, by far, the world’s best pro laptop.”
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="macbook-pro-details">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-6f87eb0dc4ce953520c7f9cc606f58cf" href="#gallery-6f87eb0dc4ce953520c7f9cc606f58cf" data-ac-gallery-trigger="gallery-6f87eb0dc4ce953520c7f9cc606f58cf"><span>A look at the front and the back of the new MacBook Pro.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-e77c52288c1b1d227891ed9d8eff5844" href="#gallery-e77c52288c1b1d227891ed9d8eff5844" data-ac-gallery-trigger="gallery-e77c52288c1b1d227891ed9d8eff5844"><span>A close-up of the keyboard on the new MacBook Pro.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-6f87eb0dc4ce953520c7f9cc606f58cf" aria-labelledby="gallery-dotnav-6f87eb0dc4ce953520c7f9cc606f58cf" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:front-and-back">
                                
                                <div>
                                    <div>The new MacBook Pro is transformed with Apple Intelligence, the blazing performance of the M4 family, an advanced 12MP Center Stage camera, an all-new nano-texture display option, and more.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-lineup.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-lineup_big" aria-label="Download media, A look at the front and the back of the new MacBook Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-e77c52288c1b1d227891ed9d8eff5844" aria-labelledby="gallery-dotnav-e77c52288c1b1d227891ed9d8eff5844" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:screen-and-keyboard">
                                
                                <div>
                                    <div>The new MacBook Pro is transformed with Apple Intelligence, the blazing performance of the M4 family, an advanced 12MP Center Stage camera, an all-new nano-texture display option, and more.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-Magic-Keyboard-close-up.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-Magic-Keyboard-close-up_big" aria-label="Download media, A close-up of the keyboard on the new MacBook Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Supercharged by the M4 Family of Chips</strong>
</h2>
                 
             
                 <div>Built using second-generation 3-nanometer technology, the <a href="https://www.apple.com/newsroom/2024/10/apple-introduces-m4-pro-and-m4-max" target="_blank">M4 family</a> is the most advanced lineup of chips for a personal computer. The M4 family features phenomenal single-threaded CPU performance with the world’s fastest CPU core,<sup>2</sup> along with outstanding multithreaded CPU performance for the most demanding workloads. Combined with machine learning accelerators in the CPU, an advanced GPU, and a faster and more efficient Neural Engine, Apple silicon is built from the ground up to deliver incredible performance for AI. Together with faster unified memory, each chip also includes increased memory bandwidth, so large language models (LLMs) and other large projects run smoothly and on device. Additionally, the industry-leading performance per watt of the M4 family means that users get up to 24 hours of battery life, raising the bar of what users can do on a single charge.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A graphic shows the M4 family of chips: M4, M4 Pro, and M4 Max.">
        <div>
             
              
              <div>
                The new MacBook&nbsp;Pro features the M4 family of chips, the most advanced lineup of chips ever built for a pro laptop.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-chip-series-3up.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-chip-series-3up_big" aria-label="Download media, A graphic shows the M4 family of chips: M4, M4 Pro, and M4 Max."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>New 14-inch MacBook Pro with M4</strong>
</h2>
                 
             
                 <div>The 14-inch MacBook Pro with M4 is the ideal choice for entrepreneurs, students, creators, or anyone doing what they love. Featuring a more powerful 10-core CPU, with four performance cores and six efficiency cores, and a faster 10-core GPU with Apple’s most advanced graphics architecture, the new MacBook Pro starts with 16GB of faster unified memory with support for up to 32GB, along with 120GB/s of memory bandwidth. With M4, MacBook Pro is up to 1.8x faster than the 13-inch MacBook Pro with M1 for tasks like editing gigapixel photos, and even more demanding workloads like rendering complex scenes in Blender are up to 3.4x faster.<sup>1</sup> With a Neural Engine that’s over 3x more powerful than in M1, it’s great for features in Apple Intelligence and other AI workloads. The M4 model also supports two high-resolution external displays in addition to the built-in display, and now features three Thunderbolt 4 ports so users can connect all their peripherals.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A person sits on the floor, working on a MacBook Pro propped up on their lap.">
        <div>
             
              
              <div>
                MacBook Pro empowers users to work and be creative wherever they are, with even more game-changing performance and extraordinary battery life.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-lifestyle-01.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-lifestyle-01_big" aria-label="Download media, A person sits on the floor, working on a MacBook Pro propped up on their lap."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong>MacBook Pro with M4 delivers:<sup>1</sup></strong>
</div>
                 
             
                 <div><ul>
<li>Up to 7x faster image processing in Affinity Photo when compared to the 13‑inch MacBook Pro with Core i7, and up to 1.8x faster when compared to the 13-inch MacBook Pro with M1.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 10.9x faster 3D rendering in Blender when compared to the 13‑inch MacBook Pro with Core i7, and up to 3.4x faster when compared to the 13‑inch MacBook Pro with M1.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 9.8x faster scene edit detection in Adobe Premiere Pro when compared to the 13‑inch MacBook Pro with Core i7, and up to 1.7x faster when compared to the 13‑inch MacBook Pro with M1.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>M4 brings phenomenal performance to the new 14-inch MacBook Pro, from creative tasks to even more demanding workloads such as rendering complex scenes in Blender.</div>
        
            <a aria-label="Download video: MacBook Pro Blender Workflow" data-analytics-title="Download video - MacBook Pro Blender Workflow" download="" href="https://www.apple.com/newsroom/videos/videos-2024/autoplay/2024/10/apple-macbook-pro-m4-blender/downloads/Apple-MacBook-Pro-M4-Blender.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>MacBook Pro with M4 Pro: A Pro Powerhouse</strong>
</h2>
                 
             
                 <div>For researchers, developers, engineers, creative pros, or anyone that needs even faster performance for more demanding workflows, MacBook Pro with M4 Pro offers a tremendous performance boost. M4 Pro features a powerful 14-core CPU with 10 performance cores and four efficiency cores for a jump in multicore performance, along with up to a 20-core GPU that is twice as powerful as M4. With M4 Pro, the new MacBook Pro gets a massive 75 percent increase in memory bandwidth over the prior generation — double that of any AI PC chip.<sup>3</sup> The new MacBook Pro with M4 Pro is up to 3x faster than models with M1 Pro, speeding up workflows like geo mapping, structural engineering, and data modeling.<sup>1</sup>
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, Two people work in a lab, with one using the new MacBook Pro.">
        <div>
             
              
              <div>
                The new 14- and 16-inch MacBook Pro with M4 Pro is ideal for researchers, developers, engineers, creative pros, or for anyone looking for even faster performance.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-lifestyle-02.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-lifestyle-02_big" aria-label="Download media, Two people work in a lab, with one using the new MacBook Pro."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong>MacBook Pro with M4 Pro offers:<sup>1</sup></strong>
</div>
                 
             
                 <div><ul>
<li>Up to 4x faster scene rendering performance with Maxon Redshift when compared to the 16-inch MacBook Pro with Core i9, and up to 3x faster when compared to the 16-inch MacBook Pro with M1 Pro.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 5x faster simulation of dynamical systems in MathWorks MATLAB when compared to the 16-inch MacBook Pro with Core i9, and up to 2.2x faster when compared to the 16-inch MacBook Pro with M1 Pro.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 23.8x faster basecalling for DNA sequencing in Oxford Nanopore MinKNOW when compared to the 16-inch MacBook Pro with Core i9, and up to 1.8x faster when compared to the 16-inch MacBook Pro with M1 Pro.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="macbook-pro-m4-pro-workflows">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-67b1179c669e36224b07ff92d8ff1291" href="#gallery-67b1179c669e36224b07ff92d8ff1291" data-ac-gallery-trigger="gallery-67b1179c669e36224b07ff92d8ff1291"><span>A user works in Fusion on the new MacBook Pro.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-5d95f85875db4499732c2b5318c2f27b" href="#gallery-5d95f85875db4499732c2b5318c2f27b" data-ac-gallery-trigger="gallery-5d95f85875db4499732c2b5318c2f27b"><span>A user works in Luna Modeler on the new MacBook Pro.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-1d2a1510f37c521f0c1ef7d42b17661e" href="#gallery-1d2a1510f37c521f0c1ef7d42b17661e" data-ac-gallery-trigger="gallery-1d2a1510f37c521f0c1ef7d42b17661e"><span>A user works in Xcode on the new MacBook Pro.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-67b1179c669e36224b07ff92d8ff1291" aria-labelledby="gallery-dotnav-67b1179c669e36224b07ff92d8ff1291" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:fusion">
                                
                                <div>
                                    <div>The new MacBook Pro with M4 Pro speeds up a variety of workflows such as structural engineering, data modeling, and more.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-Fusion.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-Fusion_big" aria-label="Download media, A user works in Fusion on the new MacBook Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-5d95f85875db4499732c2b5318c2f27b" aria-labelledby="gallery-dotnav-5d95f85875db4499732c2b5318c2f27b" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:luna-modeler">
                                
                                <div>
                                    <div>The new MacBook Pro with M4 Pro speeds up a variety of workflows such as structural engineering, data modeling, and more.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-Luna-Modeler.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-Luna-Modeler_big" aria-label="Download media, A user works in Luna Modeler on the new MacBook Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-1d2a1510f37c521f0c1ef7d42b17661e" aria-labelledby="gallery-dotnav-1d2a1510f37c521f0c1ef7d42b17661e" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:xcode">
                                
                                <div>
                                    <div>The new MacBook Pro with M4 Pro speeds up a variety of workflows such as structural engineering, data modeling, and more.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-Cinema-4D-Slack-Finder-Xcode.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-Cinema-4D-Slack-Finder-Xcode_big" aria-label="Download media, A user works in Xcode on the new MacBook Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>MacBook Pro with M4 Max: The Ultimate in Pro Performance</strong>
</h2>
                 
             
                 <div>Designed for pros like data scientists, 3D artists, and composers who constantly push workflows to the limit, MacBook Pro with M4 Max empowers users to work on projects that were previously only imaginable on a desktop. M4 Max brings up to a 16-core CPU, up to a 40-core GPU, over half a terabyte per second of unified memory bandwidth, and a Neural Engine that is over 3x faster than M1 Max, allowing on-device AI models to run faster than ever. With M4 Max, MacBook Pro delivers up to 3.5x the performance of M1 Max, ripping through heavy creative workloads like visual effects, 3D animation, and film scoring.<sup>1</sup> It also supports up to 128GB of unified memory, so developers can easily interact with LLMs that have nearly 200 billion parameters. And with the powerful Media Engine in M4 Max, which features two ProRes accelerators, MacBook Pro performance is amazing even when taking 4K120 fps ProRes video captured with the new iPhone 16 Pro and editing it in Final Cut Pro.
</div>
                 
             
                 <div><strong>MacBook Pro with M4 Max enables:<sup>1</sup></strong>
</div>
                 
             
                 <div><ul>
<li>Up to 7.8x faster scene rendering performance with Maxon Redshift when compared to the 16-inch MacBook Pro with Intel Core i9, and up to 3.5x faster when compared to the 16-inch MacBook Pro with M1 Max.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 4.6x faster build performance when compiling code in Xcode when compared to the 16‑inch MacBook Pro with Intel Core i9, and up to 2.2x faster when compared to the 16‑inch MacBook Pro with M1 Max.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 30.8x faster video processing performance in Topaz Video AI when compared to the 16‑inch MacBook Pro with Intel Core i9, and up to 1.6x faster when compared to the 16-inch MacBook Pro with M1 Max.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="macbook-pro-m4-max-workflows">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-e7f3015f7a7b3c6c39f9c8f972852143" href="#gallery-e7f3015f7a7b3c6c39f9c8f972852143" data-ac-gallery-trigger="gallery-e7f3015f7a7b3c6c39f9c8f972852143"><span>Flame is shown on MacBook Pro with M4 Max.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-51972600b23331ea62e5a7a7473b2cda" href="#gallery-51972600b23331ea62e5a7a7473b2cda" data-ac-gallery-trigger="gallery-51972600b23331ea62e5a7a7473b2cda"><span>LM Studio is shown on MacBook Pro with M4 Max.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-e7f3015f7a7b3c6c39f9c8f972852143" aria-labelledby="gallery-dotnav-e7f3015f7a7b3c6c39f9c8f972852143" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:flame">
                                
                                <div>
                                    <div>With M4 Max, MacBook Pro rips through the heaviest creative workloads like visual effects, 3D animation, and film scoring.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-Flame.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-Flame_big" aria-label="Download media, Flame is shown on MacBook Pro with M4 Max."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-51972600b23331ea62e5a7a7473b2cda" aria-labelledby="gallery-dotnav-51972600b23331ea62e5a7a7473b2cda" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:lm-studio">
                                
                                <div>
                                    <div>With support for up to 128GB of unified memory, developers can easily interact with LLMs that have nearly 200 billion parameters.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-LM-Studio.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-LM-Studio_big" aria-label="Download media, LM Studio is shown on MacBook Pro with M4 Max."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Industry-Leading Liquid XDR Display Gets Even Better</strong>
</h2>
                 
             
                 <div>The new MacBook Pro introduces an all-new nano-texture display option that dramatically reduces glare and distractions from reflections. In bright lighting conditions, the new MacBook Pro can now show SDR content at up to 1000 nits and still displays HDR content at up to 1600 nits of peak brightness. All together, it’s a game-changing experience for users working outdoors.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A user works on MacBook Pro outdoors in a field of vegetables.">
        <div>
             
              
              <div>
                A game changer when working outdoors, the Liquid Retina XDR display gets even better with an all-new nano-texture display option and up to 1000 nits of brightness for SDR content.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-lifestyle-03.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-lifestyle-03_big" aria-label="Download media, A user works on MacBook Pro outdoors in a field of vegetables."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>New 12MP Center Stage Camera</strong>
</h2>
                 
             
                 <div>MacBook Pro includes a new 12MP Center Stage camera that delivers enhanced video quality in challenging lighting conditions. Video calls are even more engaging with Center Stage, which automatically keeps users centered in the frame as they move around. The new camera also supports Desk View, which adds a whole new dimension to video calls. And with studio-quality mics and a phenomenal six-speaker sound system with support for Spatial Audio, MacBook Pro delivers an incredibly immersive audio experience whether users are listening to music or watching a movie in Dolby Atmos.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>MacBook Pro features a new 12MP Center Stage camera, which keeps users centered in the frame, and supports Desk View, which adds a whole new dimension to video calls.</div>
        
            <a aria-label="Download video: MacBook Pro Video Call with Center Stage" data-analytics-title="Download video - MacBook Pro Video Call with Center Stage" download="" href="https://www.apple.com/newsroom/videos/videos-2024/autoplay/2024/10/apple-macbook-pro-m4-center-stage-camera/downloads/Apple-MacBook-Pro-M4-Center-Stage-camera.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Thunderbolt 5 Comes to the Mac</strong>
</h2>
                 
             
                 <div>MacBook Pro with M4 Pro and M4 Max features Thunderbolt 5 ports that more than double transfer speeds up to 120 Gb/s, enabling faster external storage, expansion chassis, and powerful docking and hub solutions. For example, by connecting just a single cable, pros like music producers can now light up their entire studio. All MacBook Pro models feature an HDMI port that supports up to 8K resolution, a SDXC card slot, a MagSafe 3 port for charging, and a headphone jack, along with support for Wi-Fi 6E and Bluetooth 5.3.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A close-up of the keyboard on MacBook Pro and its Thunderbolt 5 ports with plugged-in peripherals.">
        <div>
             
              
              <div>
                MacBook Pro with M4 Pro and M4 Max now features Thunderbolt 5 with faster transfer speeds that enable powerful docking and hub solutions, allowing pros to connect to higher-bandwidth gear with a single cable.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/new-macbook-pro/article/Apple-MacBook-Pro-M4-connectivity.zip" download="" data-analytics-title="download image - Apple-MacBook-Pro-M4-connectivity_big" aria-label="Download media, A close-up of the keyboard on MacBook Pro and its Thunderbolt 5 ports with plugged-in peripherals."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>A New Era with Apple Intelligence on the Mac</strong>
</h2>
                 
             
                 <div><a href="https://www.apple.com/newsroom/2024/10/apple-intelligence-is-available-today-on-iphone-ipad-and-mac/" target="_blank">Apple Intelligence</a> ushers in a new era for the Mac, bringing personal intelligence to the personal computer. Combining powerful generative models with industry-first privacy protections, Apple Intelligence harnesses the power of Apple silicon and the Neural Engine to unlock new ways for users to work, communicate, and express themselves on Mac. It is available in U.S. English with macOS Sequoia 15.1. With systemwide Writing Tools, users can refine their words by rewriting, proofreading, and summarizing text nearly everywhere they write. With the newly redesigned Siri, users can move fluidly between spoken and typed requests to accelerate tasks throughout their day, and Siri can answer thousands of questions about Mac and other Apple products. New Apple Intelligence features will be available in December, with additional capabilities rolling out in the coming months. Image Playground gives users a new way to create fun original images, and Genmoji allows them to create custom emoji in seconds. Siri will become even more capable, with the ability to take actions across the system and draw on a user’s personal context to deliver intelligence that is tailored to them. In December, ChatGPT will be integrated into Siri and Writing Tools, allowing users to access its expertise without needing to jump between tools.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>Apple Intelligence transforms the things users do every day on their Mac. With brand-new Writing Tools, users can rewrite, proofread, or summarize everything from daily emails to important projects.</div>
        
            <a aria-label="Download video: Apple Intelligence Writing Tools" data-analytics-title="Download video - Apple Intelligence Writing Tools" download="" href="https://www.apple.com/newsroom/videos/videos-2024/autoplay/2024/10/apple-macbook-pro-m4-writing-tools/downloads/Apple-MacBook-Pro-M4-Writing-Tools.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <div>Apple Intelligence does all this while protecting users’ privacy at every step. At its core is on-device processing, and for more complex tasks, Private Cloud Compute gives users access to Apple’s even larger, server-based models and offers groundbreaking protections for personal information. In addition, users can access ChatGPT for free without creating an account, and privacy protections are built in — their IP addresses are obscured and OpenAI won’t store requests. For those who choose to connect their account, OpenAI’s data-use policies apply.
</div>
                 
             
                 <h2><strong>An Unrivaled Experience with macOS Sequoia</strong>
</h2>
                 
             
                 <div><a href="https://www.apple.com/macos/macos-sequoia-preview/" target="_blank">macOS Sequoia</a> completes the new MacBook Pro experience with a host of exciting features, including iPhone Mirroring, allowing users to wirelessly interact with their iPhone, its apps, and notifications directly from their Mac.<sup>4</sup> Safari, the world’s fastest browser,<sup>5</sup> now offers Highlights, which quickly pulls up relevant information from a site; a smarter, redesigned Reader with a table of contents and high-level summary; and a new Video Viewer to watch videos without distractions. With Distraction Control, users can hide items on a webpage that they may find disruptive to their browsing. Gaming gets even more immersive with features like Personalized Spatial Audio and improvements to Game Mode, along with a breadth of exciting titles, including the upcoming Assassin’s Creed Shadows. Easier window tiling means users can stay organized with a windows layout that works best for them. The all-new Passwords app gives convenient access to passwords, passkeys, and other credentials, all stored in one place. And users can apply new beautiful built-in backgrounds for video calls, which include a variety of color gradients and system wallpapers, or upload their own photos.
</div>
                 
             
                 <h2><strong>The Perfect Time to Upgrade or Switch to a Mac</strong>
</h2>
                 
             
                 <div>Upgraders will get monumental improvements over Intel-based MacBook Pro models, including the amazing features of Apple Intelligence. When compared to an Intel-based MacBook Pro, the new MacBook Pro provides nearly 10x faster performance for AI-based workloads,<sup>1</sup> and for graphics-intensive workloads, users get up to 20x faster performance.<sup>6</sup> With battery life on the new MacBook Pro now up to 24 hours, upgraders will also experience up to 14 additional hours. And with the Liquid Retina XDR display, a new 12MP Center Stage camera, an immersive six-speaker sound system, the unrivaled experience of macOS Sequoia, and more, there’s never been a better time to upgrade or switch to MacBook Pro.
</div>
                 
             
                 <h2><strong>MacBook Air: The World’s Most Popular Laptop Now Starts at 16GB</strong>
</h2>
                 
             
                 <div>MacBook Air is the world’s most popular laptop, and with Apple Intelligence, it’s even better. Now, models with M2 and M3 double the starting memory to 16GB, while keeping the starting price at just $999 — a terrific value for the world’s best-selling laptop.
</div>
                 
             
                 <h2><strong>Better for the Environment</strong>
</h2>
                 
             
                 <div>The new MacBook Pro is built to last and incredibly durable, created from a custom alloy that uses 100 percent recycled aluminum in the enclosure. It also uses 100 percent recycled rare earth elements in all magnets, and 100 percent recycled tin soldering, gold plating, and copper in multiple printed circuit boards. The packaging for the 14-inch MacBook Pro is now entirely fiber-based, joining the 16-inch MacBook Pro and bringing Apple closer to its goal to remove plastic from its packaging by 2025.
</div>
                 
             
                 <div>Today, Apple is carbon neutral for global corporate operations and, as part of its ambitious Apple 2030 goal, plans to be carbon neutral across its entire carbon footprint by the end of this decade.
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>
             
                 
                 
             
                 <div><ul>
<li>Customers can pre-order the new MacBook Pro starting today, October 30, on <a href="https://www.apple.com/store/" target="_blank">apple.com/store</a> and in the Apple Store app in 28 countries and regions, including the U.S. It will begin arriving to customers, and will be in Apple Store locations and Apple Authorized Resellers, beginning Friday, November 8.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The 14-inch MacBook Pro with M4 starts at <strong>$1,599</strong> (U.S.) and <strong>$1,499</strong> (U.S.) for education; the 14‑inch MacBook Pro with M4 Pro starts at <strong>$1,999</strong> (U.S.) and <strong>$1,849 </strong>(U.S.) for education; and the 16‑inch MacBook Pro starts at <strong>$2,499</strong> (U.S.) and <strong>$2,299 </strong>(U.S.) for education. All models are available in space black and silver.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Additional technical specifications, including the nano-texture display and configure-to-order options, are available at <a href="https://www.apple.com/mac/" target="_blank">apple.com/mac</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>MacBook Air with M2 and M3 comes standard with 16GB of unified memory, and is available in midnight, starlight, silver, and space gray, starting at <strong>$999</strong> (U.S.) and <strong>$899</strong> (U.S.) for education.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>New accessories with USB-C — including Magic Keyboard (<strong>$99 </strong>U.S.), Magic Keyboard with Touch ID (<strong>$149</strong> U.S.), Magic Keyboard with Touch ID and Numeric Keypad (<strong>$179</strong> U.S.), Magic Trackpad (<strong>$129 </strong>U.S.), Magic Mouse (<strong>$79</strong> U.S.), and Thunderbolt 5 Pro Cable (<strong>$69</strong>) — are available at <a href="https://www.apple.com/store/" target="_blank">apple.com/store</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple Intelligence is available now as a free software update for Mac with M1 and later, and can be accessed in most regions around the world when the device and Siri language are set to U.S. English. The first set of features is in beta and available with macOS Sequoia 15.1, with more features rolling out in the months to come.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple Intelligence is quickly adding support for more languages. In December, Apple Intelligence will add support for localized English in <em>Australia</em>, <em>Canada</em>, <em>Ireland</em>, <em>New Zealand</em>, <em>South Africa</em>, and the <em>U.K.,</em> and in April, a software update will deliver expanded language support, with more coming throughout the year. Chinese, English (India), English (Singapore), French, German, Italian, Japanese, Korean, Portuguese, Spanish, Vietnamese, and other languages will be supported.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit <a href="https://www.apple.com/shop/trade-in/" target="_blank">apple.com/shop/trade-in</a> to see what their device is worth.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>AppleCare+ for Mac provides unparalleled service and support. This includes unlimited incidents of accidental damage, battery service coverage, and 24/7 support from the people who know Mac best.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Every customer who buys directly from Apple Retail gets access to Personal Setup. In these guided online sessions, a Specialist can walk them through setup, or focus on features that help them make the most of their new device. Customers can also learn more about getting started with their new device with a Today at Apple session at their nearest Apple Store.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    




    




    
    
    





    
    
    <div>
            <ol>
<li>Testing was conducted by Apple from August through October 2024. Battery life varies by use and configuration. See <a href="https://www.apple.com/macbook-pro/" target="_blank">apple.com/macbook-pro</a> for more information.</li>
<li>Testing was conducted by Apple in October 2024 using shipping competitive systems and select industry-standard benchmarks.</li>
<li>Based on published technical specifications of shipping competitive chips as of October 2024.</li>
<li>Available on Mac computers with Apple&nbsp;silicon and Intel-based Mac computers with a T2 Security Chip. Requires that the user’s iPhone and Mac are signed in with the same Apple&nbsp;Account using two-factor authentication, their iPhone and Mac are near each other and have Bluetooth and Wi-Fi turned on, and their Mac is not using AirPlay or Sidecar. Some iPhone features (e.g., camera and microphone) are not compatible with iPhone&nbsp;Mirroring.</li>
<li>Testing was conducted by Apple in August 2024. See <a href="https://www.apple.com/safari/" target="_blank">apple.com/safari</a> for more information.</li>
<li>Results are compared to previous-generation 1.7GHz quad-core Intel Core i7-based 13-inch MacBook&nbsp;Pro systems with Intel Iris Plus Graphics 645, 16GB of RAM, and 2TB SSD.</li>
</ol>

        </div>



    
    
    






    

















		
		
			
























		
		

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Internal representations of LLMs encode information about truthfulness (126 pts)]]></title>
            <link>https://arxiv.org/abs/2410.02707</link>
            <guid>41995201</guid>
            <pubDate>Wed, 30 Oct 2024 14:22:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.02707">https://arxiv.org/abs/2410.02707</a>, See on <a href="https://news.ycombinator.com/item?id=41995201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.02707">View PDF</a>
    <a href="https://arxiv.org/html/2410.02707v3">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Hadas Orgad [<a href="https://arxiv.org/show-email/660cb3e9/2410.02707">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2410.02707v1">[v1]</a></strong>
        Thu, 3 Oct 2024 17:31:31 UTC (2,525 KB)<br>
            <strong><a href="https://arxiv.org/abs/2410.02707v2">[v2]</a></strong>
        Mon, 7 Oct 2024 14:46:11 UTC (2,530 KB)<br>
    <strong>[v3]</strong>
        Mon, 28 Oct 2024 12:33:44 UTC (2,360 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thunderbird for Android Now Available (181 pts)]]></title>
            <link>https://blog.thunderbird.net/2024/10/thunderbird-for-android-8-0-takes-flight/</link>
            <guid>41995041</guid>
            <pubDate>Wed, 30 Oct 2024 14:11:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.thunderbird.net/2024/10/thunderbird-for-android-8-0-takes-flight/">https://blog.thunderbird.net/2024/10/thunderbird-for-android-8-0-takes-flight/</a>, See on <a href="https://news.ycombinator.com/item?id=41995041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								<img src="https://blog.thunderbird.net/files/2024/10/blog-banner-TBfA.jpg" alt="featured post title image">
						<section>
												

				
<p>Just over two years ago, we <a href="https://blog.thunderbird.net/2022/06/revealed-thunderbird-on-android-plans-k9/">announced our plans</a> to bring Thunderbird to Android by taking K-9 Mail under our wing. The journey took a <a href="https://blog.thunderbird.net/2023/12/when-will-thunderbird-for-android-be-released/">little longer than we had originally anticipated</a> and there was a lot to learn along the way, but the wait is finally over! For all of you who have ever asked “when is Thunderbird for Android coming out?”, the answer is – today! We are excited to announce that the first stable release of Thunderbird for Android is out now, and we couldn’t be prouder of the newest, most mobile member of the Thunderbird family.</p>



<h2>Resources</h2>



<ul>
<li><strong>What’s New</strong>: <a href="https://support.mozilla.org/kb/new-thunderbird-android-version-8">https://support.mozilla.org/kb/new-thunderbird-android-version-8</a></li>



<li><strong>Detailed Release Notes:</strong> <a href="https://github.com/thunderbird/thunderbird-android/releases/tag/THUNDERBIRD_8_0">https://github.com/thunderbird/thunderbird-android/releases/tag/THUNDERBIRD_8_0</a></li>



<li><strong>Community Support Forum</strong>: Thunderbird for Android has its own home on the <a href="https://support.mozilla.org/en-US/products/thunderbird-android">official Mozilla Support (SUMO) forums</a>. Find the help you need to configure and use the newest Thunderbird from our community on a mobile friendly site.</li>



<li><strong>Import Settings:</strong> Whether you’re importing your information from K-9 Mail or Thunderbird on the desktop, transfer your information <a href="https://support.mozilla.org/en-US/kb/thunderbird-android-import">quickly and easily with our guide</a>.</li>



<li><strong>System Requirements</strong>: Thunderbird for Android runs on mobile devices running Android 5 and above.</li>



<li><strong>Platform Availability</strong>: Download Thunderbird for Android from the following places. Availability on F-Droid will be coming soon.


<ul>
<li><a href="https://play.google.com/store/apps/details?id=net.thunderbird.android&amp;referrer=utm_campaign%3Dandroid_release_appeal_2024%26utm_medium%3Dweb%26utm_source%3Dblog%26utm_content%3Dlink">Google Play Store</a></li>



<li><a href="https://github.com/thunderbird/thunderbird-android/releases/tag/THUNDERBIRD_8_0">GitHub Releases</a> (apk)</li>
</ul>



<ul>
<li>The <a href="https://thunderbird.net/mobile/">Thunderbird website</a> (on an Android device)</li>
</ul>
</li>



<li><strong>Get Involved:</strong> Thunderbird for Android thrives thanks to community support, and you can be part of the community! We are grateful to everyone who donates their skill and time to answer support questions, test releases, translate and more. <a href="https://blog.thunderbird.net/2024/09/contribute-to-thunderbird-for-android/">Find out all the ways to get in where you fit in.</a></li>



<li><strong>Support Us:</strong> We are 100% donor-supported. Your gift helps us develop new apps (like this one!), improve speed and stability, promote Thunderbird and software freedom, and provide downloads free-of-charge to millions. Donate on <a href="https://www.thunderbird.net/?form=tfa&amp;utm_campaign=android_release_appeal_2024&amp;utm_medium=web&amp;utm_source=blog.thunderbird.net&amp;utm_content=link">our webpage</a> or in the app.</li>



<li><strong>Suggest New Features:</strong> We know you have great ideas for future features. You can share them on Mozilla Connect, where community members can upvote and comment on them. Our team uses the feedback here to help shape our roadmap.</li>
</ul>



<h2>Thanks for Helping Thunderbird for Android Fly</h2>



<p>Thank you for being a part of the community and sharing this adventure on Android with us! We’re especially grateful to all of you who have helped us test the beta and release candidate images. Your feedback helped us find and fix bugs, test key features, and polish the stable release. We hope you enjoy using the newest Thunderbird, now and for a long time to come!</p>
				

			</section>
			

			
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dropbox announces 20% global workforce reduction (449 pts)]]></title>
            <link>https://blog.dropbox.com/topics/company/an-update-from-drew</link>
            <guid>41994640</guid>
            <pubDate>Wed, 30 Oct 2024 13:42:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.dropbox.com/topics/company/an-update-from-drew">https://blog.dropbox.com/topics/company/an-update-from-drew</a>, See on <a href="https://news.ycombinator.com/item?id=41994640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>










<header>
    <div>
        <div>
            
            <figure>
                <div>
                    
                    
    

        

        
        
        <!--<sly data-sly-use.assetPath1x="com.dropbox.aem.common.models.utils.RewriterHelperModel"/>-->

        
        
        <!--<sly data-sly-test.scaledImagePath=""/>-->

		 <!--optimized image webp-->
        
        
        

        
        <!--<img data-sly-test.highRes="false"
                srcset=", /cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg 2x"
                src="/cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg"
                aria-hidden=""
                alt="1200x628"
                class=""
                data-sly-attribute.width="auto"
                data-sly-attribute.height="auto"
                data-sly-attribute.data-id=""
                data-aem-asset-id="8448f2b6-1123-45bd-9e1f-cb32e1e3daae:Dropbox_Glyph_grey_1200x628.jpg"
                data-trackable="true" />
        <img data-sly-test="true"
                src="/cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg"
                aria-hidden="false"
                alt="1200x628"
                class=""
                data-sly-attribute.width="auto"
                data-sly-attribute.height="auto"
                data-sly-attribute.data-id=""
                data-aem-asset-id="8448f2b6-1123-45bd-9e1f-cb32e1e3daae:Dropbox_Glyph_grey_1200x628.jpg"
                data-trackable="true" />-->

	    
      <picture>
        <source media="(min-width: 800px)" srcset="https://aem.dropbox.com/cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg/_jcr_content/renditions/Dropbox_Glyph_grey_1200x628.webp" fallbackimage="https://aem.dropbox.com/cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg">
        <source media="(min-width: 480px)" srcset="https://aem.dropbox.com/cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg/_jcr_content/renditions/Dropbox_Glyph_grey_1200x628.tablet.webp" fallbackimage="https://aem.dropbox.com/cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg/_jcr_content/renditions/Dropbox_Glyph_grey_1200x628.tablet.webp">
        <source media="(min-width: 0px)" srcset="https://aem.dropbox.com/cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg/_jcr_content/renditions/Dropbox_Glyph_grey_1200x628.mobile.webp" fallbackimage="https://aem.dropbox.com/cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg/_jcr_content/renditions/Dropbox_Glyph_grey_1200x628.mobile.webp">
         <img loading="lazy" src="https://aem.dropbox.com/cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg/_jcr_content/renditions/Dropbox_Glyph_grey_1200x628.webp" fallbackimage="https://aem.dropbox.com/cms/content/dam/dropbox/blog/company/2023/glyphs/Dropbox_Glyph_grey_1200x628.jpg" onerror="window.failedAttempts=0;this.setAttribute('src',this.getAttribute('fallbackimage'));window.failedAttempts++;if(window.failedAttempts == 1)this.onerror=null" aria-hidden="false" alt="1200x628" data-aem-asset-id="8448f2b6-1123-45bd-9e1f-cb32e1e3daae:Dropbox_Glyph_grey_1200x628.jpg" data-trackable="true" height="auto" width="auto">
        
      </picture>

    

                    
                </div>
                
            </figure>
            
        </div>
        <div data-color="coconut">
            <p>
                
                
                    <a href="https://blog.dropbox.com/topics/company">Company</a>
                
            </p>
            
            
            <p>Published on
                October 30, 2024
            </p>
            <!-- <p data-sly-test="" class="b01-article-hero-plank__photographer"></p> -->
        </div>
    </div>
</header>
</div><div data-highlight="cloud">

<p><i>Today, our cofounder and CEO Drew Houston shared the difficult news that we’ll be making reductions to our global workforce. He sent the following email to all employees:<br>
&nbsp;</i></p>
<p>Hi everyone,</p>
<p>I’m writing to let you all know that after careful consideration, we've decided to reduce our global workforce by approximately 20% or 528 Dropboxers.</p>
<p>As CEO, I take full responsibility for this decision and the circumstances that led to it, and I’m truly sorry to those impacted by this change.</p>
<h3>Why we're making this decision</h3>
<p>As we've shared over the last year, we're in a transitional period as a company. Our FSS business has matured, and we've been working to build our next phase of growth with products like Dash. However, navigating this transition while maintaining our current structure and investment levels is no longer sustainable.</p>
<p>We continue to see softening demand and macro headwinds in our core business. But external factors are only part of the story. We’ve heard from many of you that our organizational structure has become overly complex, with excess layers of management slowing us down.</p>
<p>And while I'm proud of the progress we’ve made in the last couple years, in some parts of the business, we’re still not delivering at the level our customers deserve or performing in line with industry peers. So we're making more significant cuts in areas where we're over-invested or underperforming while designing a flatter, more efficient team structure overall.</p>
<h3>The opportunity ahead</h3>
<p>The changes we're making today, while difficult, come at a pivotal moment when the market is accelerating precisely where we've placed our biggest bets. It's been tremendously rewarding over the last few weeks to see customers and prospects light up when using Dash for Business for the first time, much like people did when we first launched Dropbox.&nbsp;</p>
<p>And this time we're starting from a position of strength. Millions of customers trust us as the home for their most important files, making the leap to organizing all their cloud content a natural evolution.</p>
<p>But we're not operating on our own schedule. This market is moving fast and investors are pouring hundreds of millions of dollars into this space. This both validates the opportunity we've been pursuing and underscores the need for even more urgency, even more aggressive investment, and decisive action.</p>
<p>The steps we’re taking today are necessary to both strengthen our core product and accelerate the growth of our new products. We’ll share more about our 2025 strategy in the days ahead.</p>
<h3>Taking care of impacted employees</h3>
<p>To those leaving Dropbox, we're committed to supporting you through this transition. You’ll be eligible to receive the following benefits and support:</p>
<p><b>Severance, equity, and transition payment&nbsp;</b></p>
<ul>
<li>All impacted employees will be eligible for sixteen weeks of pay, starting today, with one additional week of pay for each completed year of tenure at Dropbox. Internationally, severance packages will vary depending on regional practices and statutory requirements.</li>
<li>All impacted employees will receive their Q4 equity vest.</li>
<li>Those on the Corporate Bonus plan will be eligible to receive a pro-rated lump sum transition payment equivalent to their 2024 bonus target based on company performance forecasts and aligned with their level.</li>
<li>We will pay out eligible remaining current and approved upcoming paid leaves, including medical or family leaves.</li>
<li>We will support impacted visa holders by providing additional time to transition and access to 1:1 immigration consultation.</li>
</ul>
<p><b>Healthcare and benefits</b></p>
<ul>
<li>US employees will be eligible for up to six months of COBRA.</li>
<li>Canada-based employees will be eligible for a one-month healthcare extension.</li>
<li>All employees will continue to have access to Modern Health to support their mental well-being.</li>
</ul>
<p><b>Devices</b></p>
<ul>
<li>Impacted employees will be eligible to keep company devices (phones, tablets, laptops, and peripherals) for personal use.</li>
</ul>
<p><b>Job placement</b></p>
<ul>
<li>Job placement services and career coaching will be available at no cost.</li>
</ul>
<h3>Next steps</h3>
<p>We'll be sharing more details on high-level changes later today and will host company-wide Town Halls later this week to answer questions and discuss our plans in more detail.</p>
<p>I know this is incredibly difficult and unwelcome news. To everyone leaving Dropbox, I’m deeply grateful for everything you've done for our company and our customers.</p>
<p>Drew</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gross Apple Marketing (235 pts)]]></title>
            <link>https://jonathanbuys.com/Gross_Apple_Marketing/</link>
            <guid>41994567</guid>
            <pubDate>Wed, 30 Oct 2024 13:33:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jonathanbuys.com/Gross_Apple_Marketing/">https://jonathanbuys.com/Gross_Apple_Marketing/</a>, See on <a href="https://news.ycombinator.com/item?id=41994567">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
	    <h3>Gross Apple Marketing</h3>
	    <time datetime="2024-10-29T15:47:31+00:00" pubdate="pubdate">October 29, 2024</time>

		<p>I’m not sure what’s going on over in Cupertino for them to think that <em>any</em> of the recent Apple Intelligence ads they’ve been running are a good idea. They’re cringy at best, and honestly just flat out insulting.</p>

<p>In one a <a href="https://www.youtube.com/watch?v=3m0MoYKwVTM">schlub writes an email</a> to his boss and uses AI to make it sound ‘more professional’, in another a young woman uses it to <a href="https://www.youtube.com/watch?v=TPe8revsg3k">lie about remembering an acquaintance’s name</a>. In another the same young woman again uses it to <a href="https://www.youtube.com/watch?v=_eJy6QyHaFM">lie about reading an email</a> from a college, to her face, while she’s sitting with her. In yet another, <a href="https://blog.blankbaby.com/2024/10/apple-intelligence-smug-and-gross.html">linked to recently by Scott McNulty</a>, a woman uses AI to lie to her husband about getting him something for his birthday.</p>

<p>If this is what Apple thinks their AI is for, I honestly don’t know that I want any part of it.</p>

<p>Compare and contrast with the <a href="https://jonathanbuys.com/Scout/">video I posted yesterday</a>, and with this beautiful animation from Canonical.</p>

<p>
<iframe src="https://www.youtube.com/embed/q5yM4ZYwB_s?si=u4GcyOGtdR1QztNe" frameborder="0" allowfullscreen=""></iframe>
</p>

<p>I’ve watched that little animation several times, and they tell a better story in a minute twenty-five than all of Apple’s AI commercials combined.</p>

	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lessons learned from a successful Rust rewrite (110 pts)]]></title>
            <link>https://gaultier.github.io/blog/lessons_learned_from_a_successful_rust_rewrite.html</link>
            <guid>41994189</guid>
            <pubDate>Wed, 30 Oct 2024 12:39:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gaultier.github.io/blog/lessons_learned_from_a_successful_rust_rewrite.html">https://gaultier.github.io/blog/lessons_learned_from_a_successful_rust_rewrite.html</a>, See on <a href="https://news.ycombinator.com/item?id=41994189">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<div>
			<p><a href="https://gaultier.github.io/blog"> ⏴ Back to all articles</a></p>

			<p>Published on 2024-10-30</p>
		</div>
		
 <p><strong>Table of contents</strong></p><ul>

<li>
	<a href="#what-worked-well">What worked well</a>
		</li>

<li>
	<a href="#what-did-not-work-so-well">What did not work so well</a>
		<ul>

<li>
	<a href="#i-am-still-chasing-undefined-behavior">I am still chasing Undefined Behavior</a>
		</li>

<li>
	<a href="#miri-does-not-always-work-and-i-still-have-to-use-valgrind">Miri does not always work and I still have to use Valgrind</a>
		</li>

<li>
	<a href="#i-am-still-chasing-memory-leaks">I am still chasing memory leaks</a>
		</li>

<li>
	<a href="#cross-compilation-does-not-always-work">Cross-compilation does not always work</a>
		</li>

<li>
	<a href="#cbindgen-does-not-always-work">Cbindgen does not always work</a>
		</li>

<li>
	<a href="#unstable-abi">Unstable ABI</a>
		</li>

<li>
	<a href="#no-support-for-custom-memory-allocators">No support for custom memory allocators</a>
		</li>

<li>
	<a href="#complexity">Complexity</a>
		</li>
</ul>
</li>

<li>
	<a href="#conclusion">Conclusion</a>
		</li>
</ul>

<p><em>Discussions: <a href="https://old.reddit.com/r/rust/comments/1gflxxh/lessons_learned_from_a_successful_rust_rewrite/?">/r/rust</a>, <a href="https://old.reddit.com/r/programming/comments/1gfljj7/lessons_learned_from_a_successful_rust_rewrite/?">/r/programming</a>, <a href="https://news.ycombinator.com/item?id=41994189">HN</a></em></p>
<p>I have written about my on-going rewrite-it-to-Rust effort at work: <a href="https://gaultier.github.io/blog/you_inherited_a_legacy_cpp_codebase_now_what.md">1</a>, <a href="https://gaultier.github.io/blog/how_to_rewrite_a_cpp_codebase_successfully.md">2</a>, <a href="https://gaultier.github.io/blog/rust_c++_interop_trick.html">3</a>. And now it's finished, meaning it's 100% Rust and 0% C++ - the public C API has not changed, just the implementation, one function at time until the end. Let's have a look back at what worked, what didn't, and what can be done about it.</p>
<p>For context, I have written projects in pure Rust before, so I won't mention all of the usual Rust complaints, like "learning it is hard", they did not affect me during this project.</p>
<h2 id="what-worked-well">
	<a href="#what-worked-well">What worked well</a>
	
</h2>
<p>The rewrite was done incrementally, in a stop-and-go fashion. At some point, as I expected, we had to add brand new features while the rewrite was on-going and that was very smooth with this approach. Contrast this with the (wrong) approach of starting a new codebase from scratch in parallel, and then the feature has to be implemented twice.</p>
<p>The new code is much, much simpler and easier to reason about. It is roughly the same number of lines of code as the old C++ codebase, or slightly more. Some people think that equivalent Rust code will be much shorter (I have heard ratios of 1/2 or 2/3), but in my experience, it's not really the case. C++ can be incredibly verbose in some instances, but Rust as well. And the C++ code will often ignore some errors that the Rust compiler forces the developer to handle, which is a good thing, but also makes the codebase slightly bigger.</p>
<p>Undergoing a rewrite, even a bug-for-bug one like ours, opens many new doors in terms of performance. For example, some fields in C++ were assumed to be of a dynamic size, but we realized that they were always 16 bytes according to business rules, so we stored them in an array of a fixed size, thus simplifying lots of code and reducing heap allocations. That's not strictly due to Rust, it's just that having this holistic view of the codebase yields many benefits.</p>
<p>Related to this: we delete lots and lots of dead code. I estimate that we removed perhaps a third or half of the whole C++ codebase because it was simply never used. Some of it were half-assed features some long-gone customer asked for, and some were simply never run or even worse, never even built (they were C++ files not even present in the CMake build system). I feel that modern programming languages such as Rust or Go are much more aggressive at flagging dead code and pestering the developer about it, which again, is a good thing.</p>
<p>We don't have to worry about out-of-bounds accesses and overflow/underflows with arithmetic. These were the main issues in the C++ code. Even if C++ containers have this <code>.at()</code> method to do bounds check, in my experience, most people do not use them. It's nice that this happens by default. And overflows/underflows checks are typically never addressed in C and C++ codebases.</p>
<p>Cross-compilation is pretty smooth, although not always, see next section.</p>
<p>The builtin test framework in Rust is very serviceable. All the ones I used in C++ were terrible and took so much time to even compile.</p>
<p>Rust is much more concerned with correctness than C++, so it sparked a lot of useful discussions. For example: oh, the Rust compiler is forcing me to check if this byte array is valid UTF8 when I try to convert it to a string. The old C++ code did no such check. Let's add this check.</p>
<p>It felt so good to remove all the CMake files. On all the C or C++ projects I worked on, I never felt that CMake was worth it and I always lost a lot of hours to coerce it into doing what I needed.</p>
<h2 id="what-did-not-work-so-well">
	<a href="#what-did-not-work-so-well">What did not work so well</a>
	
</h2>
<p>This section is surprisingly long and is the most interesting in my opinion. Did Rust hold its promises?</p>
<h3 id="i-am-still-chasing-undefined-behavior">
	<a href="#i-am-still-chasing-undefined-behavior">I am still chasing Undefined Behavior</a>
	
</h3>
<p>Doing an incremental rewrite from C/C++ to Rust, we had to use a lot of raw pointers and <code>unsafe{}</code> blocks. And even when segregating these to the entry point of the library, they proved to be a big pain in the neck.</p>
<p>All the stringent rules of Rust still apply inside these blocks but the compiler just stops checking them for you, so you are on your own. As such, it's so easy to introduce Undefined Behavior. I honestly think from this experience that it is easier to inadvertently introduce Undefined Behavior in Rust than in C++, and it turn, it's easier in C++ than in C.</p>
<p>The main rule in Rust is: <s>multiple read-only pointers XOR one mutable pointer</s> <code>multiple read-only reference XOR one mutable reference</code>. That's what the borrow checker is always pestering you about.</p>
<p>But when using raw pointers, it's so easy to silently break, especially when porting C or C++ code as-is, which is mutation and pointer heavy:</p>
<p><em>Note: Astute readers have pointed out that the issue in the snippet below is having multiple mutable references, not pointers, and that using the syntax <code>let a = &amp;raw mut x;</code> in recent Rust versions, or <code>addr_of_mut</code> in older versions, avoids creating multiple mutable references.</em></p>
<pre><code>fn main() {
    let mut x = 1;
    unsafe {
        let a: *mut usize = &amp;mut x;
        let b: *mut usize = &amp;mut x;

        *a = 2;
        *b = 3;
    }
}
</code></pre>
<p>You might think that this code is dumb and obviously wrong, but in a big real codebase, this is not so easy to spot, especially when these operations are hidden inside helper functions or layers and layers of abstraction, as Rust loves to do.</p>
<p><code>cargo run</code> is perfectly content with the code above. The Rust compiler can and will silently assume that there is only one mutable pointer to <code>x</code>, and make optimizations, and generate machine code, based on that assumption, which this code breaks.</p>
<p>The only savior here is <a href="https://github.com/rust-lang/miri">Miri</a>:</p>
<pre><code>$ cargo +nightly-2024-09-01 miri r
error: Undefined Behavior: attempting a write access using &lt;2883&gt; at alloc1335[0x0], but that tag does not exist in the borrow stack for this location
 --&gt; src/main.rs:7:9
  |
7 |         *a = 2;
  |         ^^^^^^
  |         |
  |         attempting a write access using &lt;2883&gt; at alloc1335[0x0], but that tag does not exist in the borrow stack for this location
  |         this error occurs as part of an access at alloc1335[0x0..0x8]
  |
  [...]
 --&gt; src/main.rs:4:29
  |
4 |         let a: *mut usize = &amp;mut x;
  |                             ^^^^^^
help: &lt;2883&gt; was later invalidated at offsets [0x0..0x8] by a Unique retag
 --&gt; src/main.rs:5:29
  |
5 |         let b: *mut usize = &amp;mut x;
  |                             ^^^^^^
  [...]
</code></pre>
<p>So, what could have been a compile time error, is now a runtime error. Great. I hope you have 100% test coverage! Thank god there's Miri.</p>
<p>If you are writing <code>unsafe{}</code> code without Miri checking it, or if you do so without absolutely having to, I think this is foolish. It will blow up in your face.</p>
<p>Miri is awesome. But...</p>
<h3 id="miri-does-not-always-work-and-i-still-have-to-use-valgrind">
	<a href="#miri-does-not-always-work-and-i-still-have-to-use-valgrind">Miri does not always work and I still have to use Valgrind</a>
	
</h3>
<p>I am not talking about some parts of Miri that are experimental. Or the fact that running code under Miri is excruciatingly slow. Or the fact that Miri only works in <code>nightly</code>.</p>
<p>No, I am talking about code that Miri cannot run, period:</p>
<pre><code>    |
471 |     let pkey_ctx = LcPtr::new(unsafe { EVP_PKEY_CTX_new_id(EVP_PKEY_EC, null_mut()) })?;
    |                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ can't call foreign function `␁aws_lc_0_16_0_EVP_PKEY_CTX_new_id` on OS `linux`
    |
    = help: if this is a basic API commonly used on this target, please report an issue with Miri
    = help: however, note that Miri does not aim to support every FFI function out there; for instance, we will not support APIs for things such as GUIs, scripting languages, or databases
</code></pre>
<p>If you are using a library that has parts written in C or assembly, which is usual for cryptography libraries, or video compression, etc, you are out of luck.</p>
<p>So we resorted to add a feature flag to split the codebase between parts that use this problematic library and parts that don't. And Miri only runs tests with the feature disabled.</p>
<p>That means that there is a lot of <code>unsafe</code> code that is simply not being checked right now. Bummer.</p>
<p>Perhaps there could be a fallback implementation for these libraries that's entirely implemented in software (and in pure Rust). But that's not really feasible for most libraries to maintain two implementations just for Rust developers.</p>
<p>I resorted to run the problematic tests in <code>valgrind</code>, like I used to do with pure C/C++ code. It does not detect many things that Miri would, for example having more than one mutable pointer to the same value, which is perfectly fine in C/C++/Assembly, but not in Rust.</p>
<h3 id="i-am-still-chasing-memory-leaks">
	<a href="#i-am-still-chasing-memory-leaks">I am still chasing memory leaks</a>
	
</h3>
<p>Our library offers a C API, something like this:</p>
<pre><code>void* handle = MYLIB_init();

// Do some stuff with the handle...

MYLIB_release(handle);
</code></pre>
<p>Under the hood, <code>MYLIB_init</code> allocates some memory and <code>MYLIB_release()</code> frees it. This is a very usual pattern in C libraries, e.g. <code>curl_easy_init()/curl_easy_cleanup()</code>.</p>
<p>So immediately, you are thinking: well, it's easy to forget to call <code>MYLIB_release</code> in some code paths, and thus leak memory. And you'd be right. So let's implement them to illustrate. We are good principled developers so we write a Rust test:</p>
<pre><code>#[no_mangle]
pub extern "C" fn MYLIB_init() -&gt; *mut std::ffi::c_void {
    let alloc = Box::leak(Box::new(1usize));

    alloc as *mut usize as *mut std::ffi::c_void
}

#[no_mangle]
pub extern "C" fn MYLIB_do_stuff(_handle: *mut std::ffi::c_void) {
    // Do some stuff.
}

#[no_mangle]
pub extern "C" fn MYLIB_release(handle: *mut std::ffi::c_void) {
    let _ = unsafe { Box::from_raw(handle as *mut usize) };
}

fn main() {}

#[cfg(test)]
mod test {
    #[test]
    fn test_init_release() {
        let x = super::MYLIB_init();

        super::MYLIB_do_stuff(x);

        super::MYLIB_release(x);
    }
}
</code></pre>
<p>A Rust developer first instinct would be to use RAII by creating a wrapper object which implements <code>Drop</code> and automatically calls the cleanup function.
However, we wanted to write our tests using the public C API of the library like a normal C application would, and it would not have access to this Rust feature.
Also, it can become unwieldy when there are tens of types that have an allocation/deallocation function. It's a lot of boilerplate!</p>
<p>And often, there is complicated logic with lots of code paths, and we need to ensure that the cleanup is always called. In C, this is typically done with <code>goto</code> to an <code>end:</code> label that always cleans up the resources. But Rust does not support this form of <code>goto</code>.</p>
<p>So we solved it with the <a href="https://docs.rs/scopeguard/latest/scopeguard/">defer</a> crate in Rust and implementing a <a href="https://www.gingerbill.org/article/2015/08/19/defer-in-cpp/">defer</a> statement in C++.</p>
<p>However, the Rust borrow checker really does not like the <code>defer</code> pattern. Typically, a cleanup function will take as its argument as <code>&amp;mut</code> reference and that precludes the rest of the code to also store and use a second <code>&amp;mut</code> reference to the same value. So we could not always use <code>defer</code> on the Rust side.</p>
<h3 id="cross-compilation-does-not-always-work">
	<a href="#cross-compilation-does-not-always-work">Cross-compilation does not always work</a>
	
</h3>
<p>Same issue as with Miri, using libraries with a Rust API but with parts implemented in C or Assembly will make <code>cargo build --target=...</code> not work out of the box. It won't affect everyone out there, and perhaps it can be worked around by providing a sysroot like in C or C++. But that's a bummer still. For example, I think Zig manages this situation smoothly for most targets, since it ships with a C compiler and standard library, whereas <code>cargo</code> does not.</p>
<h3 id="cbindgen-does-not-always-work">
	<a href="#cbindgen-does-not-always-work">Cbindgen does not always work</a>
	
</h3>
<p><a href="https://github.com/mozilla/cbindgen">cbindgen</a> is a conventionally used tool to generate a C header from a Rust codebase. It mostly works, until it does not. I hit quite a number of limitations or bugs. I thought of contributing PRs, but I found for most of these issues, a stale open PR, so I didn't. Every time, I thought of dumping <code>cbindgen</code> and writing all of the C prototypes by hand. I think it would have been simpler in the end.</p>
<p>Again, as a comparison, I believe Zig has a builtin C header generation tool.</p>
<h3 id="unstable-abi">
	<a href="#unstable-abi">Unstable ABI</a>
	
</h3>
<p>I talked about this point in my previous articles so I won't be too long. Basically, all the useful standard library types such as <code>Option</code> have no stable ABI, so they have to be replicated manually with the <code>repr(C)</code> annotation, so that they can be used from C or C++. This again is a bummer and creates friction. Note that I am equally annoyed at C++ ABI issues for the same reason.</p>
<p>Many, many hours of hair pulling would be avoided if Rust and C++ adopted, like C, a <a href="https://daniel.haxx.se/blog/2024/10/30/eighteen-years-of-abi-stability/">stable ABI</a>.</p>
<h3 id="no-support-for-custom-memory-allocators">
	<a href="#no-support-for-custom-memory-allocators">No support for custom memory allocators</a>
	
</h3>
<p>With lots of C libraries, the user can provide its own allocator at runtime, which is often very useful. In Rust, the developer can only pick the global allocator at compile time. So we did not attempt to offer this feature in the library API.</p>
<p>Additionally, all of the aforementioned issues about cleaning up resources would have been instantly fixed by using an <a href="https://gaultier.github.io/blog/tip_of_the_day_2.html">arena allocator</a>, which is not at all idiomatic in Rust and does not integrate with the standard library (even though there are crates for it). Again, Zig and Odin all support arenas natively, and it's trivial to implement and use them in C. I really longed for an arena while chasing subtle memory leaks.</p>
<h3 id="complexity">
	<a href="#complexity">Complexity</a>
	
</h3>
<p>From the start, I decided I would not touch async Rust with a ten-foot pole, and I did not miss it at all, for this project.</p>
<p>Whilst reading the docs for <code>UnsafeCell</code> for the fourth time, and pondering whether I should use that or <code>RefCell</code>, while just having been burnt by the pitfalls of <code>MaybeUninit</code>, and asking myself if I need <code>Pin</code>, I really asked myself what life choices had led me to this.</p>
<p>Pure Rust is already very complex, but add to it the whole layer that is mainly there to deal with FFI, and it really becomes a beast. Especially for new Rust learners.</p>
<p>Some developers in our team straight declined to work on this codebase, mentioning the real or perceived Rust complexity.
Now, I think that Rust is still mostly easier to learn than C++, but admittedly not by much, especially in this FFI heavy context.</p>
<h2 id="conclusion">
	<a href="#conclusion">Conclusion</a>
	
</h2>
<p>I am mostly satisfied with this Rust rewrite, but I was disappointed in some areas, and it overall took much more effort than I anticipated. Using Rust with a lot of C interop feels like using a completely different language than using pure Rust. There is much friction, many pitfalls, and many issues in C++, that Rust claims to have solved, that are in fact not really solved at all.</p>
<p>I am deeply grateful to the developers of Rust, Miri, cbindgen, etc. They have done tremendous work. Still, the language and tooling, when doing lots of C FFI, feel immature, almost pre v1.0. If the ergonomics of <code>unsafe</code> (which are being worked and slightly improved in the recent versions), the standard library, the docs, the tooling, and the unstable ABI, all improve in the future, it could become a more pleasant experience.</p>
<p>I think that all of these points have been felt by Microsoft and Google, and that's why they are investing real money in this area to improve things.</p>
<p>If you do not yet know Rust, I recommend for your first project to use pure Rust, and stay far away from the whole FFI topic.</p>
<p>I initially considered using Zig or Odin for this rewrite, but I really did not want to use a pre v1.0 language for an enterprise production codebase (and I anticipated that it would be hard to convince other engineers and managers). Now, I am wondering if the experience would have really been worse than with Rust. Perhaps the Rust model is really at odds with the C model (or with the C++ model for that matter) and there is simply too much friction when using both together.</p>
<p>If I have to undertake a similar effort in the future, I think I would strongly consider going with Zig instead. We'll see. In any case, the next time someone say 'just rewrite it in Rust', point them to this article, and ask them if that changed their mind ;)</p>
<p><a href="https://gaultier.github.io/blog"> ⏴ Back to all articles</a></p>

<blockquote id="donate">
  <p>If you enjoy what you're reading, you want to support me, and can afford it: <a href="https://paypal.me/philigaultier?country.x=DE&amp;locale.x=en_US">Donate</a>. That allows me to write more cool articles!</p>
</blockquote>

<blockquote>
  <p>
    This blog is <a href="https://github.com/gaultier/blog">open-source</a>!
    If you find a problem, please open a Github issue.
    The content of this blog as well as the code snippets are under the <a href="https://en.wikipedia.org/wiki/BSD_licenses#3-clause_license_(%22BSD_License_2.0%22,_%22Revised_BSD_License%22,_%22New_BSD_License%22,_or_%22Modified_BSD_License%22)">BSD-3 License</a> which I also usually use for all my personal projects. It's basically free for every use but you have to mention me as the original author.
  </p>
</blockquote>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EPA cancels pesticide shown to be harmful to unborn babies (155 pts)]]></title>
            <link>https://www.thenewlede.org/2024/10/epa-cancels-pesticide-shown-to-be-harmful-to-unborn-babies/</link>
            <guid>41993832</guid>
            <pubDate>Wed, 30 Oct 2024 11:41:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thenewlede.org/2024/10/epa-cancels-pesticide-shown-to-be-harmful-to-unborn-babies/">https://www.thenewlede.org/2024/10/epa-cancels-pesticide-shown-to-be-harmful-to-unborn-babies/</a>, See on <a href="https://news.ycombinator.com/item?id=41993832">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                                        
<p>Citing a need to protect the unborn babies of pregnant women, the US Environmental Protection Agency (EPA) on Tuesday banned a pesticide used to kill weeds on farms, golf courses and athletic fields.</p>



<p>The action comes after years of mounting scientific evidence of the dangers posed by exposure to the chemical dimethyl tetrachloroterephthalate, also known as DCPA or Dacthal. &nbsp;</p>



<p>“With the final cancellation of DCPA, we’re taking a definitive step to protect pregnant women and their unborn babies,” Michal Freedhoff, assistant administrator for the EPA Office of Chemical Safety and Pollution Prevention, said in a press release. “The science showing the potential for irreversible harm to unborn babies’ developing brains, in addition to other lifelong consequences from exposure, demands decisive action to remove this dangerous chemical from the marketplace.”</p>
<p>The agency said “robust studies” demonstrated “thyroid toxicity,” and said that unborn babies whose pregnant mothers are exposed to DCPA could experience changes to fetal thyroid hormone levels. Such changes are “generally linked to low birth weight, impaired brain development, decreased IQ, and impaired motor skills later in life, some of which may be irreversible,” the EPA said.</p>
<p>DCPA was registered to control weeds in both agricultural and non-agricultural settings, but has largely been used to control weeds in fields growing crops such as broccoli, brussels sprouts, cabbage and onions.</p>



<p>The EPA action comes after&nbsp;<a href="https://www.epa.gov/pesticides/dcpa-dacthal-technical-herbicide-product-suspended-epa" target="_blank" rel="noreferrer noopener">years of research and smaller moves</a>&nbsp;by the EPA to limit the impact of DCPA on public health.&nbsp;In April, the <a href="https://www.thenewlede.org/2024/04/concerned-about-developing-babies-epa-warns-about-danger-of-weed-killer-used-on-farms-golf-courses/" target="_blank" rel="noreferrer noopener">agency issued a rare warning</a>&nbsp;that the pesticide posed “serious, permanent and irreversible health risks,” especially to farmworkers involved in tasks such as transplanting, weeding and harvesting after the pesticide has been applied.</p>



<p>In August, the EPA issued an&nbsp;“<a href="https://www.epa.gov/newsreleases/epa-issues-emergency-order-stop-use-pesticide-dacthal-address-serious-health-risk-4" target="_blank" rel="noreferrer noopener">emergency suspension</a>” of the chemical, marking the first time in almost 40 years the agency took such an emergency action. The agency said that, following the suspension, it received a letter from AMVAC Chemical Corporation, the sole manufacturer of DCPA, stating its intent to voluntarily cancel pesticide products containing DCPA sold in the US. AMVAC later said it would also cancel all international registrations.&nbsp;&nbsp;</p>
<p>The EPA cancellation prohibits anyone from distributing or selling DCPA pesticide products and bars anyone from using existing supplies.&nbsp;</p>
<p>(Featured photo by <a href="https://unsplash.com/@jorbrain">Jordan González</a> for&nbsp;<a href="https://unsplash.com/plus?referrer=%2Fphotos%2Fa-couple-of-people-that-are-sitting-in-the-grass-jVq0I80khBs" rel="nofollow">Unsplash+</a>)</p>

            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Classic 3D videogame shadow techniques (300 pts)]]></title>
            <link>https://30fps.net/pages/videogame-shadows/</link>
            <guid>41993012</guid>
            <pubDate>Wed, 30 Oct 2024 08:49:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://30fps.net/pages/videogame-shadows/">https://30fps.net/pages/videogame-shadows/</a>, See on <a href="https://news.ycombinator.com/item?id=41993012">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="text">

<p>Towards the end of Wim Wenders’s excellent <em>Perfect Days</em>, the protagonist Hirayama is drinking beer under a bridge after he has seen a Businessman courting his crush. Suddenly the Businessman joins him under the bridge. As it turns out, things aren’t actually that simple but the point is their conversation takes them to some fundamental questions:</p>
<blockquote>
<p><strong>Businessman:</strong> Shadows. Do they get darker when they overlap?<br> <strong>Hirayama:</strong> Not sure.<br> <strong>Businessman:</strong> So many things I still don’t know… That’s how life ends… I guess.<br> <strong>Hirayama:</strong> Let’s find out now.<br> <strong>Businessman:</strong> What?<br></p>
</blockquote>
<p>Then they step into to the light of a street lamp and investigate their shadows (<a href="https://www.youtube.com/watch?v=C6cdPxQjHb0">the full scene</a>):</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/Perfect_Days_shadows.jpg" alt=""><figcaption>An iconic scene in <em>Perfect Days</em> (2023). Still via <a href="https://film-grab.com/2024/04/12/perfect-days/">film-grabs.com</a>.</figcaption>
</figure>
<p>Even though the Businessman sees no difference, Hirayama is convinced the overlapping shadows <em>do</em> become darker. “It has to get darker to make sense.” What a moving scene.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/metalgear-crop.jpg">
<figcaption>
Shadows do become darker when<br>they overlap in Metal Gear Solid.
</figcaption>
</figure>
<p>Unfortunately Hirayama is mistaken. Shadows don’t get any darker there. There’s just one light source, and relatively far away, so the shadow is simply an absence of light. It doesn’t matter how many times the light is blocked.</p>
<p>When it comes to 3D videogames, shadows are something else. It’s easy to paint dark blob under some character’s feet and assume everything else is lit. Perhaps Hirayama was recalling the blob shadow in <em>Metal Gear Solid</em> that <em>does</em> become darker when it overlaps with others?</p>
<p>In the real world, shadows simply exist but in games they are both engineered and designed. They must run well but also look good. I find this relationship fascinating and I’m going to show you why. Let’s start simple.</p>

<h2 id="on-screen-2d-shadows">On-screen 2D shadows</h2>
<p>You can draw a shadow image to the screen before you draw a character. I’m not talking about shadow sprites <a href="https://www.mobygames.com/game/365/duke-nukem-3d/screenshots/dos/700154/">like in Duke Nukem 3D</a> but literally a 2D image without any scaling. This works if the character is in front of everything like in <em>Winter Gold</em> or <em>MDK</em>.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/16539773-winter-gold-snes-downhill-crop.png"> <img src="https://30fps.net/pages/videogame-shadows/10559518-mdk-dos-that-spaceship-is-deploying-a-lot-of-troops-crop.jpg">
<figcaption>
Winter Gold (1996, SNES) and MDK (1996, PC) draw animated 2D shadow images. Also check out Winter&nbsp;Gold’s <a href="https://www.youtube.com/watch?v=-Pr909aVsNo">sick Amiga-style intro video</a> :)
</figcaption>
</figure>
<p>I said it was simple.</p>
<h2 id="blob-shadow">Blob shadow</h2>
<p>OK now in 3D. Draw a dark disc under the character. Done.</p>
<p>Well, you should also align the shadow disc with the ground and also decide how to handle situations where the shadow would reach over a ledge. For example in <em>Super Mario 64</em>, the blobs are drawn using a special hardware feature that effectively clips the shadow to show up only on the ground plane.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/sm64_blob2.jpg"> <img src="https://30fps.net/pages/videogame-shadows/sm64_shadow_decal2.jpg"> <img src="https://30fps.net/pages/videogame-shadows/sm64_blobs.jpg">
<figcaption>
Super Mario 64 (1996, Nintendo 64) uses blob shadows for characters.<br> <strong>Left:</strong> A blob shadow overlapping with tree shadows. <strong>Center:</strong> A blob shadow getting clipped using a hardware decal feature. <strong>Right:</strong> All moving characters and objects have their own shadow. The clipped shadow inside that transparent bubble shows the limitation of the Nintendo 64 decal feature. Screenshots taken in the <a href="https://ares-emu.net/">ares</a> emulator.
</figcaption>
</figure>
<p>The blob shadow can also be animated. In Super Mario 64 it becomes smaller when jumping and <a href="https://youtu.be/F2sEmf_HzGI?t=3793">in <em>Metal Gear Solid</em> it changes shape</a>. If you’re feeling ambitious, you can also solve the shadow-over-a-ledge issue by <a href="http://blog.wolfire.com/2009/06/how-to-project-decals/">projecting the blob quad like a decal</a>.</p>
<h2 id="planar-shadows-with-a-render-texture">Planar shadows with a render texture</h2>
<p>The blob is just a texture and usually textures can be rendered to at runtime. So render the character from the top and use that instead of a dark circle. This works great in <em>Crash 3</em> (<a href="https://youtu.be/HG-NRnGp3RA?t=267">video</a>) but not so well in <em>Soldier of Fortune</em> because they kept the shadow resolution so low.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/16310901-crash-bandicoot-warped-playstation-c-boxes-allow-you-to-restart-crop2.png"> <img src="https://30fps.net/pages/videogame-shadows/soldieroffortune.jpg">
<figcaption>
<strong>Left:</strong> Crash Bandicoot: Warped (1998, PlayStation) renders shadow textures at runtime. <strong>Right:</strong> Soldier of Fortune (2000, PC) also does but at a lower resolution. A cropped still from <a href="https://www.youtube.com/watch?v=wq82etLV0a8">a video by FirstPlays HD</a>.
</figcaption>
</figure>
<p>Note that this is distinct from <a href="https://learnopengl.com/Advanced-Lighting/Shadows/Shadow-Mapping">shadow mapping</a> where a depth map is rendered from the light’s point of view. Here we render only a black &amp; white image that is used as a texture. So in a sense we’re talking about a 1-bit shadow map.</p>
<p>How could we make the shadows sharper?</p>
<h2 id="planar-shadows-with-geometry">Planar shadows with geometry</h2>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/f19-stealth-fighter-back.png">
<figcaption>
The F-117A plane casts a shadow in<br> <em>F-19 Stealth Fighter</em> (1988, DOS).
</figcaption>
</figure>
<p>One intuitive option is to flatten a shadow caster on a plane by projecting it away from the light. Then render it the second time but now in black. They are usually kept opaque to hide how object parts are drawn on top of another. Naturally the shadow will be correct only on a flat floor.</p>
<p>Some early flight simulators draw a top-down flat shadow when on a runway. During my research I expected to see examples where the shadow is also seen when in flight but couldn’t find any.</p>

<figure>
<img src="https://30fps.net/pages/videogame-shadows/kingpin.jpg"> <img src="https://30fps.net/pages/videogame-shadows/half-life-shadows1.jpg">
<figcaption>
<a href="https://quakewiki.org/wiki/GLQuake">GLQuake’s</a> planar shadows in two games. <strong>Left:</strong> <em>Kingpin: Life of Crime</em> (1999, PC) gets away with black planar shadows. <strong>Right:</strong> Shadows exhibit transparency issues in <a href="https://www.moddb.com/downloads/half-life-1-alpha-052"><em>Half-Life</em> alpha 0.52</a>. Screenshot taken via Wine on Linux.
</figcaption>
</figure>
<p>Visually these look the same as black stencil shadows cast on a flat plane.</p>
<h3 id="shadows-on-terrain">Shadows on terrain</h3>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/virus.png">
<figcaption>
Virus (1987) Atari ST port. The little ship casts<br> a neat drop shadow.
</figcaption>
</figure>
<p>David Braben’s 1987 <a href="https://www.youtube.com/watch?v=MNXypBxNGMo"><em>Virus</em></a> on Acorn Archimedes and other home computers draws spaceships that cast top-down shadows on terrain.</p>
<p>A more elaborate example is <em>Interstate ’76</em>. There they tilt and stretch a planar shadow to match the ground slope. The shadows <a href="https://30fps.net/pages/videogame-shadows/interstate76_shadow_inside_terrain.jpg">occasionally penetrate the ground</a> but are pretty convincing overall. Interestingly, the below software-rendered screenshot has slightly transparent shadows while the hardware accelerated ones are pitch black.</p>
<p>They also had the courage to try to <a href="https://youtu.be/9UyOuqsvs64?t=52">project shadows for large objects like bridges</a> which isn’t, well, entirely successful.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/interstate-76.png" alt=""><figcaption>Interstate ’76 (1997, PC) has sun casting tilted planar shadows on terrain.</figcaption>
</figure>
<p>But how do you cast shadows on any kind of scene?</p>
<h2 id="projected-texture-drop-shadow">Projected texture drop shadow</h2>
<figure><a id="projected-texture-drop-shadow">
<img src="https://30fps.net/pages/videogame-shadows/dropshadow_ownby_2010.jpg"> <img src="https://30fps.net/pages/videogame-shadows/buzzjump.jpg">
</a><figcaption><a id="projected-texture-drop-shadow">
<strong>Left:</strong> A slide from the deck </a><a href="https://advances.realtimerendering.com/s2010/Ownby,Hall%20and%20Hall%20-%20Toystory3%20(SIGGRAPH%202010%20Advanced%20RealTime%20Rendering%20Course).pdf"><em>Toy Story 3: The Video Game Rendering Techniques</em></a>. <br><strong>Right:</strong> The drop shadow gets darker when it overlaps with other shadows. A still from <a href="https://www.youtube.com/watch?v=_8fa81Spo3s">an Xbox 360 gameplay video</a> of <em>Toy Story 3: The Video Game</em> (2010).
</figcaption>
</figure>
<p>This approach bears a lot of similarity to <em>Planar shadows with a render texture</em> presented earlier but works on surfaces of any shape. The game renders a shadow texture from the top but instead of showing it on a flat plane, the <a href="https://paroj.github.io/gltut/Texturing/Tut17%20Projective%20Texture.html">texture is projected</a> to other objects. Think of it as the <a href="https://en.wikipedia.org/wiki/Bat-Signal">Bat-Signal</a> but pointed straight down from the sky.</p>
<p>Shadows like this can be made really sharp but they can look strange on vertical surfaces and occassionally even appear on the ceiling. See this <a href="https://youtu.be/EU4CbUoPBi4?t=752">gameplay video of <em>Sonic Adventure 2: Battle</em></a> (2001, GameCube).</p>
<p>This technique also works great for trees:</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/12333235-the-elder-scrolls-iv-oblivion-xbox-360-the-speedtree-engine-at-w.jpg" alt=""><figcaption>The Elder Scrolls IV: Oblivion (2006, Xbox 360) has beautiful projected tree shadows.</figcaption>
</figure>
<p>Projected shadows can show through objects which makes them suitable only for special cases. Shadow maps are something you can use anywhere.</p>
<h2 id="shadow-maps">Shadow maps</h2>
<p>The de facto approach to shadows. The game draws a depth image, the shadow map, from the point of view of the light and reads from that image when rendering the world. This is easy to do since you can reuse the game engine’s regular rendering code.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/hl2_2004_shadows.jpg" alt=""><figcaption><em>Half-Life 2 (2004, PC)</em> used shadow maps for characters. Dynamic flashlight shadows (not shown) were added in the 2007 <em>Episode Two</em> expansion.</figcaption>
</figure>
<p>The limited resolution of the shadow map gives rise to well-known artifacts with <a href="https://learn.microsoft.com/en-us/windows/win32/dxtecharts/common-techniques-to-improve-shadow-depth-maps#shadow-acne-and-erroneous-self-shadowing">inventive names such as “Peter Panning” and “shadow acne”</a>. Many tricks have been proposed to allocate more shadow map area to surfaces near the camera where the extra resolution is needed the most. Shadow maps usually need some tweaking to look right.</p>
<p>Before shadow maps became dominant, there was a popular competitor.</p>
<h2 id="stencil-shadows">Stencil shadows</h2>
<p>The has-been approach to shadows. Stencil shadows draw sharp shadows on any kind of surface. They create a unique film-noir look that’s hard to emulate with shadow maps. Most well-known example is of course <em>Doom 3</em> with its dark rooms:</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/Doom3snap4.jpg" alt=""><figcaption>Doom 3 (2004) had no static level lighting and all shadows were computed at runtime. Screenshot from <a href="http://warp.povusers.org/doom3snapshots/"><em>Doom 3 shadow engine snapshots</em></a>.</figcaption>
</figure>
<p>Stencil shadows are based on the idea of <em>shadow volumes</em>, invisible geometry that cuts the world into lit and shadowed spaces. The game applies lighting only on pixels that don’t lie inside a shadow volume.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/Shadow_volume_illustration.png" alt=""><figcaption>The game needs to construct “shadow volume” meshes shown in yellow. Surfaces inside the volumes stay unlit. <a href="https://commons.wikimedia.org/wiki/File:Shadow_volume_illustration.png">Shadow volume illustration</a> by Rainwarrior, CC BY-SA 3.0.</figcaption>
</figure>
<p>Stencil shadows need the world to be drawn many times to work. Simplified a bit, the game first draws the whole world with ambient lighting. Then for each light, all the shadow volumes, followed by the world again, affecting only unshadowed pixels. The volumes are drawn with different <a href="https://learnopengl.com/Advanced-OpenGL/Stencil-testing"><em>stencil operations</em></a> set <a href="https://registry.khronos.org/OpenGL-Refpages/gl4/html/glStencilOpSeparate.xhtml">for front and back faces</a>. It’s a lot of pixels to draw.</p>
<p>Possibly the earliest shipping game with stencil shadows is <em>Severance: Blade of Darkness</em> from 2001 whose shadows <a href="https://youtu.be/irwzEDLZ2gk?t=417">look great</a>.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/11323681-blade-of-darkness-windows-blade-was-also-well-known-for-its-wond-edit.jpg" alt=""><figcaption>Severance: Blade of Darkness (2001, PC) had stencil shadows.</figcaption>
</figure>
<p>Reading the <a href="https://30fps.net/pages/videogame-shadows/blade_of_darkness_edge_review.jpg">Edge UK edition March 2001 review</a> (<a href="https://retrocdn.net/images/2/2d/Edge_UK_095.pdf">pdf</a>) of the game makes it clear that despite the graphics advancements, the world wasn’t ready for a soulslike back then.</p>
<p>Stencil shadows are not used much nowadays. One reason is their unpredictable runtime cost. The cost is dependent on how large the volume is on screen and therefore varies a lot. Also an <a href="https://en.wikipedia.org/wiki/Shadow_volume#Depth_fail">optimized algorithm</a> was patented. For Doom 3, <a href="https://web.archive.org/web/20100131044756/http://techreport.com/discussions.x/7113">Id Software apparently reached</a> some sort of <a href="https://web.archive.org/web/20090818013827/http://www.theinquirer.net/inquirer/news/1019517/creative-background-doom-iii-shadow-story">a deal</a>.</p>
<h3 id="soft-stencil-shadows">Soft stencil shadows</h3>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/silent_hill_2_shadows1_crop2.jpg"> <img src="https://30fps.net/pages/videogame-shadows/silenthill2_color_buffer.png"> <img src="https://30fps.net/pages/videogame-shadows/silenthill2_shadow_buffer.png">
<figcaption>
<strong>Left:</strong> Silent Hill 2 (2001, PlayStation 2) has soft stencil shadows. <strong>Center:</strong> 2x zoomed crop. <strong>Right:</strong> A shadow debug view with unlit areas colored black. <a href="https://imgsli.com/MzExMjcy/">Screenshots</a> from the PCSX2 emulator lightened up for visualization.
</figcaption>
</figure>
<p>Stencil shadows don’t need to be sharp. Another game from 2001, <em>Silent Hill 2</em> on the PlayStation 2, blurred the stencil shadows afterwards as seen above. <a href="https://youtu.be/nna2yt1c9gI?t=354">It looks pretty much perfect</a> on the console.</p>
<h2 id="simplified-character-shadows">Simplified character shadows</h2>
<p>What if the shadows are cast by a simpler model than what’s shown on screen? For example in Zelda on the Nintendo 64, Link’s feet cast shadows even though nothing else does:</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/15709360-the-legend-of-zelda-ocarina-of-time-nintendo-64-in-zoras-domain.jpg" alt=""><figcaption>The Legend of Zelda: Ocarina of Time (1998, Nintendo 64) has Link’s feet cast shadows as if they were tall vertical cylinders. I presume the shadows are two stretched decals.</figcaption>
</figure>
<p>One unique approach is seen in <em>Hyperblade</em> where players on a futuristic hockey arena cast planar shadows as simple animated shapes.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/hyperblade.png" alt=""><figcaption>Hyperblade (1996, PC) projects a simplified planar shadow that moves. It’s not perfect as seen in the rightmost image. Stills from a <a href="https://youtu.be/EpScHKYEEpE?t=43">video by Bit Games Reviews</a>.</figcaption>
</figure>
<h2 id="shadows-in-static-level-lighting">Shadows in static level lighting</h2>
<p>Vertex colors and lightmaps are techniques to capture lighting of a game level. They have been used in many games as the only way to show large scale shadows, which is why I’ve included them here.</p>
<h3 id="vertex-colors">Vertex colors</h3>
<p><em>Ico</em> shows how sophisticated shadows can look with just old school per-vertex lighting.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/ico.jpg" alt=""><figcaption>Ico (2001, PlayStation 2) has level lighting baked to vertex colors. For characters <a href="https://youtu.be/QCOAqyOE_V4?t=438">it uses stencil shadows</a>.</figcaption>
</figure>
<p>For low-poly maps even sharp shadows can be represented with vertex colors. A prime example is <em>Tony Hawk Pro Skater 2</em> (2000, Playstation) which looks amazing considering the simplicity of the technique.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/thps-2-dreamcast-venice-beach-1.jpg" alt=""><figcaption>“Venice Beach” level in Tony Hawk Pro Skater 2 (2000, PlayStation) with its sharp vertex color shadows. This shot is from the Dreamcast port.</figcaption>
</figure>
<h3 id="lightmaps">Lightmaps</h3>
<p>Lightmaps are the classic way to store level lighting and shadows. Instead of storing a color for each vertex, there’s a second set of textures that represent only lighting. The resolution can vary per area, making the shadows more accurate where needed. On the other hand, lightmaps consume more memory than vertex colors.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/11192353-mirrors-edge-windows-training-grounds-crop.jpg" alt=""><figcaption>Mirror’s Edge (2008, PC) is basically Lightmaps: The Game.</figcaption>
</figure>
<p>Lightmaps were popularized by <em>Quake</em> (1996, PC) and <a href="https://jbush001.github.io/2015/06/11/quake-lightmaps.html">this is how they look</a>.</p>
<p>That concludes our look into traditional shadow techniques. Let’s talk a bit about lighting in general next.</p>
<h2 id="shadows-in-modern-games">Shadows in modern games</h2>
<p>Modern games use the traditional techniques when appropriate. Some examples:</p>
<ul>
<li>Variants of <strong>shadow mapping</strong> such as <a href="https://learnopengl.com/Guest-Articles/2021/CSM">Cascaded Shadow Maps</a> to cover large areas while still staying fast.</li>
<li><strong>Lightmaps</strong> in combination with other techniques such as <a href="https://computergraphics.stackexchange.com/a/244">light probes</a>. <em>Call of Duty</em> <a href="https://30fps.net/pages/videogame-shadows/CoD_Hemispheres_Presentation_Notes_p6.jpg">still has lightmaps</a>, see the <a href="https://advances.realtimerendering.com/s2024/content/Roughton/SIGGRAPH%20Advances%202024%20-%20Hemispheres%20Presentation%20Notes.pdf">Hemispherical Lighting Insights</a> slides.</li>
<li>The <strong>simplified character model idea</strong>. <em>The Last of Us</em> (2013, PlayStation 3) casts soft character shadows with stretched spheres. See <a href="https://30fps.net/pages/videogame-shadows/tlou1.jpg">this slide</a> of the deck <a href="http://miciwan.com/SIGGRAPH2013/Lighting%20Technology%20of%20The%20Last%20Of%20Us.pdf">Lighting Technology of The Last Of Us (2013)</a>. Also Unreal Engine supports simplified <a href="https://dev.epicgames.com/documentation/en-us/unreal-engine/capsule-shadows-overview-in-unreal-engine">“capsule shadows”</a> for characters.</li>
<li><strong>Projected shadows</strong>. In <em>Hot Wheels Track Attack</em>&nbsp;(2010, Wii) they render a shadow mesh to a texture and project that on the race track, as described in <a href="https://www.bryanmcphail.com/wp/?p=640">one of the developer’s blog</a>. The game looks great <a href="https://youtu.be/k85H5C7P0RY?t=349">in motion</a>!</li>
</ul>
<h3 id="ray-traced-shadows">Ray-traced shadows</h3>
<p>In the beginning we established that shadows are formed by a lack of light. If the game really tries to simulate physically correct lighting then shadows will naturally appear. Even small geometric details <a href="https://images.nvidia.com/geforce-com/international/comparisons/control/control-ray-traced-contact-shadows-interactive-comparison-001-on-vs-off.html">will cast accurate shadows</a>, unlike in shadow maps. Big lamps will naturally create soft shadows and indirect light will brighten dark corners. An incredible amount of time and money have been invested in ray tracing algorithms and hardware to make this dream reality.</p>
<p>In practice modern games have such complex scenes the above simulated solution has to be approximated. For example in the ray-traced shadows of <em>Alan Wake 2</em> (2023) each pixel receives lighting <a href="https://30fps.net/pages/videogame-shadows/aw2_restir.jpg">only from a single randomly chosen light</a>. The result is eventually <a href="https://30fps.net/pages/videogame-shadows/aw2_direct_lighting.jpg">fed to a denoiser</a> that intelligently smooths out the noisy picture. See <a href="https://www.nvidia.com/en-us/on-demand/session/gdc24-gdc1003/?playlistId=playList-821861a9-571a-4073-abab-d60ece4d1e49">the whole presentation</a> for details. Therefore even ray-traced shadows won’t be “perfect” and will have their own look, depending on the tradeoffs made.</p>
<p>Finally, the obvious option.</p>
<h2 id="no-shadows">No shadows</h2>
<p>Sometimes your priorities are elsewhere.</p>
<figure>
<img src="https://30fps.net/pages/videogame-shadows/16483987-alone-in-the-dark-dos-shall-we-dance-scaled.jpg" alt=""><figcaption><em>Alone in the Dark</em> (1992, DOS) had no character shadows.</figcaption>
</figure>

<hr>
<p>In the movie scene, when Hirayama is carefully studying the shadows, his new friend makes an observation:</p>
<blockquote>
<p><strong>Businessman:</strong> You’re really into this.<br></p>
</blockquote>
<p>As computer graphics enthusiasts, I think we can symphatize.</p>
<p><em>All screenshots provided by <a href="https://www.mobygames.com/">MobyGames</a> unless noted otherwise.</em> <em>Thanks to mankeli for detailed notes on an early draft of this article. Thanks to noby, msqrt, shaiggon and Warma for feedback.</em></p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Async Rust is not safe with io_uring (185 pts)]]></title>
            <link>https://tonbo.io/blog/async-rust-is-not-safe-with-io-uring</link>
            <guid>41992975</guid>
            <pubDate>Wed, 30 Oct 2024 08:42:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tonbo.io/blog/async-rust-is-not-safe-with-io-uring">https://tonbo.io/blog/async-rust-is-not-safe-with-io-uring</a>, See on <a href="https://news.ycombinator.com/item?id=41992975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div data-svelte-h="svelte-184ng9o"> <p>October 30, 2024 by Tzu Gwo</p></div> <h2 data-svelte-h="svelte-s902pt">TL;DR</h2> <pre data-svelte-h="svelte-8brff3">	1. Clone <a href="https://github.com/ethe/io-uring-is-not-cancellation-safe">this repository</a> on a Linux system that supports io_uring.</pre> <pre data-svelte-h="svelte-vfyltr">	2. Try switching <a href="https://github.com/ethe/io-uring-is-not-cancellation-safe/blob/master/src/main.rs#L9-L10">these two lines.</a></pre> <pre data-svelte-h="svelte-1om2w1r">	3. Execute cargo run for a while.</pre> <p data-svelte-h="svelte-a92z3x">The demo shows that even though the behavior appears similar, TCP connections leak when
				using the io_uring driver but not with the epoll driver. I've also <a href="https://github.com/ethe/io-uring-is-not-cancellation-safe/branches/all">tested this across various io_uring runtimes,</a> and it turns out to be a common issue across all of them.</p> <h2 data-svelte-h="svelte-1nsv6xs">Barbara's TCP connection mysteriously leaked</h2> <p data-svelte-h="svelte-10biiqt">Barbara had a lot of experience developing web services with async Rust. One day, she read a
				blog about io_uring, which described it as the next-generation async I/O interface for
				Linux. Interested, Barbara decided to try it out in her sidecar web service.</p> <p data-svelte-h="svelte-njq492">Rust's "async/await" model is separate from the async runtime and I/O interface
				implementations, making it easy to switch between different runtimes. Barbara was very
				familiar with Tokio, the most popular async runtime in Rust, which uses epoll for I/O
				interface. So, she looked for an async runtime that supported io_uring to transform her web
				service into an io_uring-based version.</p> <p data-svelte-h="svelte-1pil8jj">After some research, Barbara discovered several async runtimes like <a href="https://github.com/DataDog/glommio">glommio,</a> <a href="https://github.com/bytedance/monoio">monoio,</a>
				and
				<a href="https://github.com/compio-rs/compio">compio</a> that supported io_uring.
				She decided to give one of them a try—monoio, in particular, which provided both epoll and io_uring
				interfaces and allowed for easy switching. It seemed like the perfect fit for Barbara's io_uring
				exploration.</p> <p data-svelte-h="svelte-6gwc0">With her familiarity with Tokio, Barbara quickly wrote her first HTTP server demo:</p>    <p data-svelte-h="svelte-1lcrgpz">Barbara thought, "Great, this looks no different from a typical Tokio program—first bind to
				an address, then continuously accept new TCP connections in a loop and process them."</p> <p data-svelte-h="svelte-133qamx">Barbara then considered her next steps. She decided to learn how to implement asynchronous
				control, such as timeouts, so that if the TCP listener did not accept a connection for a
				while, it could switch to handling some sidecar tasks (like logging) before resuming
				acceptance:</p>    <p data-svelte-h="svelte-1ocoebl">Using the concurrency primitive "select" to add timeouts to futures worked well with
				io_uring. Barbara was pleased and quickly updated her web service to use io_uring,
				eventually deploying it. Everything ran smoothly until one day she noticed something odd in
				the client logs: some requests were never processed. To investigate, Barbara wrote a minimal
				example, only to find the issue was far more complex than expected.</p> <p data-svelte-h="svelte-1q7x5qs">Barbara found that while the client running in a child thread was connecting correctly, the
				server in the main thread wasn’t proceeding as it should. Instead, the timeout kept getting
				triggered, as if the client's connection had vanished. <b>A TCP connection leak had occurred.</b>
				And it wasn't just monoio—this issue affected all async runtimes that used io_uring.</p> <h2 data-svelte-h="svelte-1ujqvyi">What’s going on?</h2> <p data-svelte-h="svelte-1q0g62a">Before understanding why using "select" for timeout control in an io_uring-based async
				runtime leads to TCP connection leaks, we need to first understand why this issue doesn’t
				occur with epoll.</p> <p data-svelte-h="svelte-1d2d4e6">The entire async Rust ecosystem is built around a core asynchronous primitive from the
				standard library: Future. Its definition is as follows:</p>    <p data-svelte-h="svelte-1efgqfq">In Rust, all asynchronous operations—not just those manually written by async library
				developers but also those written by users using "async" blocks—are defined as recursive
				future structures, which get instantiated when ".await" is called. The entire structure
				contains all the state that must be saved across suspended futures during pending
				operations. The async executor is then responsible for repeatedly calling the "poll" method
				to advance this state until completion. Consider this example async block:</p>    <p data-svelte-h="svelte-1ntevmc">will transform to below by compiler:</p>    <p data-svelte-h="svelte-d9uza6">For a more detailed explanation of futures and how they are executed, I recommend reading
				<a href="https://en.ihcblog.com/rust-runtime-design-1/">ihciah's blog.</a>
				He is one of the core authors of monoio.</p> <p data-svelte-h="svelte-w78mu4">Async Rust makes a few core assumptions about futures:</p> <pre data-svelte-h="svelte-sjik6d">	1. The state of futures only change when they are polled.</pre> <pre data-svelte-h="svelte-thrp6b">	2. Futures are implicitly cancellable by simply never polling them again.
				</pre> <p data-svelte-h="svelte-1sa0hh5">Futures bound to epoll adhere to these assumptions, which relates to the mechanism of epoll:
				epoll is not an asynchronous syscall mechanism; it’s an event notification mechanism. In the
				above example, the actual behavior of the "listener.accept()" future, simplified, is as
				follows:</p>    <p data-svelte-h="svelte-6wikbk">"self.accept()" runs synchronously, either succeeding by obtaining a TCP stream or
				encountering a "would block" exception, leaving it in a pending state until the kernel is
				ready. To cancel this operation, you simply stop polling, as the syscall only happens during
				polling.</p> <p data-svelte-h="svelte-1jfsmfl">However, io_uring-bound futures break these two assumptions:</p> <pre data-svelte-h="svelte-1l5soen">	1. The syscall is executed asynchronously by the kernel, not during polling. The kernel commit the TCP stream into a kernel / user shared ring buffer, meaning the accept event is completed implicitly.</pre> <pre data-svelte-h="svelte-17rm9ec">	2. You cannot simply cancel an io_uring-bound future by stopping polling, as the kernel might complete the syscall at any time, <b>even during the cancellation progress</b>.
				</pre> <p data-svelte-h="svelte-11qp05s">A step-by-step explanation of the earlier example will make this process clearer:</p>    <h2 data-svelte-h="svelte-ipliht">How to solve this?</h2> <p data-svelte-h="svelte-1qr0z9y">Before discussing the solution, we need to break the problem down into two parts:</p> <pre data-svelte-h="svelte-19la56n">	1. <b>I/O Safety</b>: Ensuring that accepted TCP streams are properly closed without leaking connections.</pre> <pre data-svelte-h="svelte-17qcix0">	2. <b>Halt Safety</b> (proposed by Yoshua Wuyts): Handling connections that have already been opened when they are cancelled, allowing them to continue being processed.
				</pre> <h3 data-svelte-h="svelte-6xjdyo">I/O Safety</h3> <p data-svelte-h="svelte-1a2p9rh">First of all, we are fortunate that the I/O safety problem can be addressed now, which safe
				Rust aims to ensure this in the future. Rust provides the Drop trait to define custom
				behavior when a value is cleaned up. Thus, we can do something like this:</p>    <p data-svelte-h="svelte-1jocbtu">We just need to encourage async runtimes to implement this fix.</p> <h3 data-svelte-h="svelte-6hh2fm">Halt Safety</h3> <p data-svelte-h="svelte-7uppsm">Halt safety is more complicated. Monoio provides a component called "cancellable I/O" to
				properly handle the cancellation of io_uring-bound futures. A complete example can be found
				here: <a href="https://github.com/ethe/io-uring-is-not-cancellation-safe/blob/cancelable-io/src/main.rs">cancellable I/O example.</a> You can run this branch to see that the connection handling behavior now matches that of epoll.
				Here, I’ll show a simplified usage:</p>    <p data-svelte-h="svelte-1fi05wx">As you can see, besides performing the accept operation in the regular select branch, the
				timeout branch explicitly cancels the accept future. Afterwards, it proceeds to .await the
				accept future again to confirm if a TCP stream was ready during the timeout period.</p> <p data-svelte-h="svelte-usjqk4">Monoio's component partially solves the problem, but there's still an issue: since a future
				is a recursive structure, an io_uring-bound future may not be directly at the place where
				cancellation occurs:</p>    <p data-svelte-h="svelte-qxefi9">Canceling a future that contains an io_uring-bound future will also affect its inner
				io_uring-bound futures. This means that the cancellation safety of io_uring-bound futures is
				"contagious." Simply converting an io_uring-bound future to cancellable I/O does not solve
				all the issues.</p> <p data-svelte-h="svelte-1e286rt">Another key issue is that if you forget to handle the cancellation of an io_uring-bound
				future, there are no compile-time checks to catch it. For io_uring-bound futures, you need
				to ".await" them after cancellation to see if they have completed. This means they must be
				<b>used exactly once,</b>
				a concept called
				<a href="https://en.wikipedia.org/wiki/Substructural_type_system">linear types,</a> which ensures correct usage of resources at compile time.</p> <p data-svelte-h="svelte-1t3pja8">Unfortunately, Rust lacks the support for this kind of type system. For more details on why
				adding linear logic to Rust is challenging, you can refer to Without Boats' blog:
				<a href="https://without.boats/blog/changing-the-rules-of-rust/#:~:text=Let%E2%80%99s%20say%20you%20want%20Rust%20to%20support%20types%20which%20can%E2%80%99t%20go%20out%20of%20scope%20without%20running%20their%20destructor.%20This%20is%20one%20of%20the%20two%20different%20definitions%20of%20%E2%80%9Clinear%20types%2C%E2%80%9D">Changing the rules of Rust.</a></p> <h2 data-svelte-h="svelte-1tliofq">Why wrote this?</h2> <p data-svelte-h="svelte-1i0gxro">There has been a lot of discussion about memory safety in the context of io_uring. For more
				details, you can refer to these resources:</p> <pre data-svelte-h="svelte-1yhsrhe"> • <a href="https://blog.yoshuawuyts.com/async-cancellation-1/">Async Cancellation by yoshuawuyts</a></pre> <pre data-svelte-h="svelte-13a8yeb"> • <a href="https://without.boats/blog/io-uring/">Notes on io-uring by withoutboats</a></pre> <pre data-svelte-h="svelte-vye3w7"> • <a href="https://github.com/bytedance/monoio/blob/master/docs/en/why-async-rent.md">Async Rent by ihciah</a></pre> <p data-svelte-h="svelte-1wcx2zr">However, the community rarely addresses I/O safety and halt safety with io_uring in async
				Rust. I'm presenting a specific case to draw attention to this topic. The title of this blog
				might sound a bit dramatic, but everyone has different definitions and understandings of
				"safety." What do you think about this issue:</p> <pre data-svelte-h="svelte-15q3iv8"> • Keep things as they are; I/O safety and halt safety do not need guarantees from the language.</pre> <pre data-svelte-h="svelte-eoedff"> • Rust should ensure I/O safety (this is already a goal outlined in the RFC, but not yet implemented in Rust.)</pre> <pre data-svelte-h="svelte-ts4uky"> • Rust should ensure halt safety (rarely discussed!)</pre></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Eighteen Years of ABI Stability (153 pts)]]></title>
            <link>https://daniel.haxx.se/blog/2024/10/30/eighteen-years-of-abi-stability/</link>
            <guid>41992899</guid>
            <pubDate>Wed, 30 Oct 2024 08:25:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.haxx.se/blog/2024/10/30/eighteen-years-of-abi-stability/">https://daniel.haxx.se/blog/2024/10/30/eighteen-years-of-abi-stability/</a>, See on <a href="https://news.ycombinator.com/item?id=41992899">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>Exactly eighteen years ago today, on October 30 2006, we shipped <a href="https://curl.se/ch/7.16.0.html">curl 7.16.0</a> that among a whole slew of new features and set of bugfixes bumped the libcurl SONAME number from 3 to 4.</p>



<h2>ABI breakage</h2>



<p>This bump meant that libcurl 7.16.0 was not binary compatible with the previous releases. Users could not just easily and transparently bump up to this version from the previous, but they had to check their use of libcurl and in some cases adjust source code.</p>



<p>This was not the first ABI breakage in the curl project, but at this time our use base was larger than at any of the previous bumps and this time people complained about the pains and agonies such a break brought them.</p>



<h2>We took away FTP features</h2>



<p>In the 7.16.0 release we removed a few FTP related features and their associated options. Before this release, you could use curl to do “third party” transfers over FTP, and in this release you could no longer do that. That is a feature when the client (curl) connects to server A and instructs that server to communicate with server B and do file transfers among themselves, without sending data to and from the client.</p>



<p>This is an FTP feature that was not implemented well in curl and it was poorly tested. It was also a feature that barely no FTP server allowed and subsequently this was not used by many users. We ripped it out.</p>



<h2>A near pitchfork situation</h2>



<p>Because so few people used the removed features, barely anyone actually noticed the ABI breakage. It remained theoretical to most users and I believe that detail only made people more upset over the SONAME bump because they did not even see the necessity: we just made their lives more complicated for no benefit (to them).</p>



<p>The Debian project even decided to override our decision <em>“no, that is not an ABI breakage”</em> and added a local patch in their build that lowered the SONAME number back to 3 again in their builds. A patch they would stick to for many years to come.</p>



<p>The obvious friction this bump caused, even when in reality it actually did not affect many users and the loud feedback we received, made a huge impact on me. It had not previously dawned on me exactly how important this was.</p>



<p>I decided there and then to do the utmost to never go through this again. To put ABI compatibility at the top of the priority list. Make it one of the most fundamental key properties of libcurl.</p>



<p><strong>Do. Not. Break. The. ABI</strong></p>



<p>(we don’t break the API either)</p>



<h2>A never-breaking ABI</h2>



<p>The decision was initially made to avoid the negativity the bump brought, but I have since over time much more come to appreciate the upsides.</p>



<p><em>Application authors everywhere can always and without risk keep upgrading to the latest libcurl.</em></p>



<p>It sounds easy and simple, but the impact is huge. The examples, the documentation, the applications, everything can just always upgrade and continue. As libcurl over time has become even more popular and compared to 2006, used in many magnitudes more installations, it has grown into an even more important aspect of the curl life. Possibly <em>the</em> single most important properly of curl.</p>



<p>There is a small caveat here and that is that we occasionally of course have bugs and regressions, so when I say that users can always upgrade, that is true in the sense that we have not broken the ABI since. We have however had a few regressions that sometimes have triggered some users to downgrade again or wait a little longer for the next release that has the bug fixed.</p>



<p>When we took that decision in 2006 we had less than 50,000 lines of product code. Today we are approaching 180,000 lines.</p>



<h2>Effects of never breaking ABI</h2>



<p>We know that once we adopt a change, we are stuck with it for decades to come. It makes us double-check every knot before we accept new changes.</p>



<p>Once accepted and shipped, we keep supporting code and features that we otherwise could have reconsidered and perhaps removed. Sometimes we think of a better way to do something <em>after</em> the initial merge, but by then it is too late to change. We can then always introduce new and better ways to do things, but we have to keep supporting the old way as well.</p>



<p>A most fundamental effect is that we can never shrink the list of options we support. We can never actually rename something. Doing new things and features consistently over this long time is hard if not impossible, as we learn new things and paradigms vary through the decades.</p>



<h2>How</h2>



<p>The primary way we maintain this is by manual code view and code inspection of every change. Followed of course by a large range of tests that make sure that assumptions remain.</p>



<p>Occasionally we have (long) discussions around subtle details when someone proposes a change that potentially might be considered an ABI break. Or not.</p>



<p>What exactly is covered by <em>ABI compatibility</em> is not always straight forward or easy to have carved in stone. In particular since the project can be built and run on such a wide range of systems and architectures.</p>



<h2>Deprecating</h2>



<p>We <em>can</em> still remove functionality if the conditions are right.</p>



<p>Some features and options are documented and work in a way so that something is <em>requested</em> or <em>asked for</em> and libcurl then tries to satisfy that ask. Like for example libcurl once supported HTTP/1 pipelining like that.</p>



<p>libcurl still provides the option to enable pipelining and applications can still ask for it so it is still ABI and API compatible, but a modern libcurl simply will never do it because that functionality has been removed.</p>



<p>Example two: we dropped support for NPN a few years back. NPN being a TLS extension called Next Protocol Negotiation that was used briefly in the early days of HTTP/2 development before ALPN was introduced and replaced NPN.  Virtually nothing requires NPN anymore, and users can still set the option asking for it, but it will never actually happen over the wire.</p>



<p>Furthermore, a typical libcurl build involves multiple third party libraries that provide features it needs. For things like TLS, SSH, compression and binary HTTP protocol management. Over the years, we have removed support for several such libraries and introduced support for new, in ways that was never visible in the API or ABI. Some users just had to switch to building curl with different helper libraries.</p>



<p>In reality, libcurl is typically more stable than most existing servers and URLs. The libcurl examples you wrote in 2006 can still be built with the modern libcurl, but the servers and URLs you used back then most probably cannot be used anymore.</p>



<h2>If no one can spot it, it did not happen</h2>



<p>As blunt as it may sound, it has came down to this fundamental statement several times to judge if a change is an ABI breakage or not:</p>



<p><em>If no one can spot an ABI change, it is not an ABI change</em></p>



<p>Of course what makes it harder than it sounds is that it is extremely difficult to actually know if someone will notice something ahead of time. libcurl is used in so ridiculously many installations and different setups, second-guessing whatever everyone does and wants is darned close to impossible.</p>



<p>Adding to the challenge is the crazy long upgrade cycles some of our users seem to sport. It is not unusual to see questions appear on the mailing lists from users bumping from curl versions from eight or ten years ago. The fact that we have not  heard users comment on a particular change might just mean that they are still stuck on ancient versions.</p>



<p>Getting frustrated comments from users today about a change we landed five years ago is hard to handle.</p>



<h2>Forwards compatible</h2>



<p>I should emphasize that all this means that users can always upgrade to a <em>later</em> release. It does not necessarily mean that they can switch back to an older version without problems. We do add new features over time and if you start using a new feature, the application of course will not work, or even still compile, if you would switch to a libcurl version from before that feature was added.</p>



<h2>How long is never</h2>



<p>What I have laid out here is our plan and ambition. We have managed to stick to this for eighteen years now and there is no particular known blockers in the known future either.</p>



<p>I cannot rule out that we might at some point in the future run into an obstacle so huge or complicated that we will be forced to do the unthinkable. To break the ABI. But until we see absolutely no other way forward, it is not going to happen.</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Flame Graphs (173 pts)]]></title>
            <link>https://www.brendangregg.com/blog//2024-10-29/ai-flame-graphs.html</link>
            <guid>41992419</guid>
            <pubDate>Wed, 30 Oct 2024 06:54:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.brendangregg.com/blog//2024-10-29/ai-flame-graphs.html">https://www.brendangregg.com/blog//2024-10-29/ai-flame-graphs.html</a>, See on <a href="https://news.ycombinator.com/item?id=41992419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Imagine halving the resource costs of AI and what that could mean for the planet and the industry -- based on extreme estimates such savings could reduce the total US power usage by over 10% by 2030<span size="-2"><sup>1</sup></span>. At Intel we've been creating a new analyzer tool to help reduce AI costs called <em>AI Flame Graphs</em>: a visualization that shows an AI accelerator or GPU hardware profile along with the full software stack, based on my <strong><a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">CPU flame graphs</a></strong>. Our first version is available to customers in the <strong><a href="https://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html">Intel Tiber AI Cloud</a></strong> as a preview for the Intel Data Center GPU Max Series (previously called Ponte Vecchio). Here is an example:</p>

<center><a href="https://www.brendangregg.com/blog/images/2024/matrixAIflamegraph.svg"><img src="https://www.brendangregg.com/blog/images/2024/matrixAIflamegraph.png" width="700"></a><br><span size="-1"><i>Simple example: SYCL matrix multiply microbenchmark</i></span></center>

<p>(Click for interactive <a href="https://www.brendangregg.com/blog/images/2024/matrixAIflamegraph.svg">SVG</a>.) The <span color="#00bb00">green</span> frames are the actual instructions running on the AI or GPU accelerator, <span color="#008888">aqua</span> shows the source code for these functions, and <span color="#bb0000">red</span> (C), <span color="#888800">yellow</span> (C++), and <span color="#a04000">orange</span> (kernel) show the CPU code paths that initiated these AI/GPU programs. The <span color="#808080">gray</span> "-" frames just help highlight the boundary between CPU and AI/GPU code. The x-axis is proportional to cost, so you look for the widest things and find ways to reduce them.</p>

<div><center><img src="https://www.brendangregg.com/blog/images/2024/AIflamegraph-legend.png" width="150"><br><span size="-1"><i>Layers</i></span></center></div>

<p>This flame graph shows a simple program for SYCL (a high-level C++ language for accelerators) that tests three implementations of matrix multiply, running them with the same input workload. The flame graph is dominated by the slowest implementation, multiply_basic(), which doesn't use any optimizations and consumes at 72% of stall samples and is shown as the widest tower. On the right are two thin towers for multiply_local_access() at 21% which replaces the accessor with a local variable, and multiply_local_access_and_tiling() at 6% which also adds matrix tiling. The towers are getting smaller as optimizations are added.</p>

<p>This flame graph profiler is a prototype based on Intel EU stall profiling for hardware profiling and <a href="https://ebpf.io/">eBPF</a> for software instrumentation. It's designed to be <strong>easy and low-overhead</strong>, just like a CPU profiler. You should be able to generate a flame graph of an existing AI workload whenever you want, without having to restart anything or launch additional code via an interposer.</p>

<h2>Instruction-offset Profiling</h2>

<p>This is not the first project to build an AI profiler or even something called an AI Flame Graph, however, others I've seen focus on tracing CPU stacks and timing accelerator execution, but don't profile the instruction offsets running on the accelerator; or do profile them but via expensive binary instrumentation. I wanted to build AI flame graphs that work like CPU flame graphs: Easy to use, negligible cost, production safe, and shows everything. A daily tool for developers, with most of the visualization <em>in the language of the developer</em>: source code functions.</p>

<p>This has been an internal AI project at Intel for the past year. Intel was already investing in this space, building the EU stall profiler capability for the Intel Data Center GPU Max Series that provides an approximation of HW instruction sampling. I was lucky to have <strong>Dr. Matthew (Ben) Olson</strong>, an Intel AI engineer who has also worked on eBPF performance tooling (<a href="https://github.com/intel/processwatch">processwatch</a>) as well as memory management research, join my team and do most of the development work. His background has helped us power through difficulties that seemed insurmountable. We've also recently been joined by <strong>Dr. Brandon Kammerdiener</strong> (coincidentally another graduate of the University of Tennessee, like Ben), who also has eBPF and memory internals experience, and has been helping us take on harder and harder workloads. And <strong>Gabriel Muñoz</strong> just joined today to help with releases. Now that our small team has shown that this is possible, we'll be joined by other teams at Intel to develop this further.</p>

<p>We could have built a harder-to-use and higher-overhead version months ago using Intel <a href="https://www.brendangregg.com/blog//2024-10-29/binary%20instrumentation">GTPin</a> but for widespread adoption it needs minimal overhead and ease of use so that developers don't hesitate to use this daily and to add it to deployment pipelines.</p>

<h2>What's a Flame Graph?</h2>

<div><center><img src="https://www.brendangregg.com/blog/images/2024/flamegraph-cost.png" width="300"></center></div>

<p>A <a href="https://www.brendangregg.com/flamegraphs.html">flame graph</a> is a visualization I invented in 2011 for showing sampled code stack traces. It has become the standard for CPU profiling and analysis, helping developers quickly find performance improvements and eliminate regressions. A CPU flame graph shows the "big picture" of running software, with x-axis proportional to CPU cost. The example picture on the right summarizes how easy it can be to go from compute costs to responsible code paths. Prior to flame graphs, it could take hours to understand a complex profile by reading through <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html#Problem">hundreds of pages of output</a>. Now it takes seconds: all you have to do is look for the widest rectangles.</p>

<p>Flame graphs have had worldwide adoption. They have been the basis for five startups so far, have been adopted in over thirty performance analysis products, and have had <a href="https://www.brendangregg.com/Slides/YOW2022_flame_graphs/#8">over eighty implementations</a>.</p>

<p>My first implementation of flame graphs took a few hours on a Wednesday night after work. The real effort has been in the decade since, where I worked with different profilers, runtimes, libraries, kernels, compilers, and hypervisors to get flame graphs working properly in different environments, including fixing stack walking and symbolization. Earlier this year I posted about the final missing piece: Helping distros <a href="https://www.brendangregg.com/blog/2024-03-17/the-return-of-the-frame-pointers.html">enable frame pointers</a> so that profiling works across standard system libraries.</p>

<p>Similar work is necessary for AI workloads: fixing stacks and symbols and getting profiling to work for different hardware, kernel drivers, user-mode drivers, frameworks, runtimes, languages, and models. A lot more work, too, as AI analysis has less maturity than CPU analysis.</p>

<h2>Searching Samples</h2>

<p>If you are new to flame graphs, it's worth mentioning the built-in search capability. In the earlier example, most of the stall samples are caused by sbid: software scoreboard dependency. As that may be a unique search term, you can run search (Ctrl-F, or click "Search") on "sbid" and it will highlight it in magenta:</p>

<center><img src="https://www.brendangregg.com/blog/images/2024/AIflamegraph-search.png" width="530"></center>

<p>Search also shows the total number of stack samples that contained sbid in the bottom right: 78.4%. You can search for any term in the flame graph: accelerator instructions, source paths, function names, etc., to quickly calculate the percentage of stacks where it is present (excluding vertical overlap) helping you prioritise performance work.</p>

<p>Note that the samples are EU stall-based, which means theoretical performance wins can take the percentages down to zero. This is different to timer-based samples as are typically used in CPU profiling. Stalls mean you better focus on the pain, the parts of the code that aren't making forward progress, but you aren't seeing resource usage by unstalled instructions. I'd like to supuport timer-based samples in the future as well, so we can have both views.</p>

<h2>Who will use this?</h2>

<p>At a recent golang conference, I asked the audience of 200+ to raise their hands if they were using CPU flame graphs. Almost every hand went up. I know of companies where flame graphs are a daily tool that developers use to understand and tune their code, reducing compute costs. This will become a daily tool for AI developers.</p>

<p>My employer will use this as well for evaluation analysis, to find areas to tune to beat competitors, as well as to better understand workload performance to aid design.</p>

<h2>Why is AI profiling hard?</h2>

<p>Consider CPU instruction profiling: This is easy when the program and symbol table are both in the file system and in a standardized file format (such as ELF) as is the case with native compiled code (C). CPU profiling gets hard for JIT-complied code, like Java, as instructions and symbols are dynamically generated and placed in main memory (the process heap) without following a universal standard. For such JITted code we use runtime-specific methods and agents to retrieve snapshots of the heap information, which is different for each runtime.</p>

<p>AI workloads also have different runtimes (and frameworks, languages, user-mode drivers, compilers, etc.) any of which can require special tinkering to get their CPU stacks and symbols to work. These CPU stacks are shown as the red, orange, and yellow frames in the AI Flame Graph. Some AI workloads are easy to get these frames working, some (like PyTorch) are a lot more work. </p>

<div><center><img src="https://www.brendangregg.com/blog/images/2024/AIsourcezoom.png" width="450"></center></div>

<p>But the real challenge is instruction profiling of actual GPU and AI accelerator programs -- shown as the aqua and green frames -- and correctly associating them with the CPU stacks beneath them. Not only may these GPU and AI programs not exist in the file system, but they may not even exist in main memory! Even for running programs. Once execution begins, they may be deallocated from main memory and only exist in special accelerator memory, beyond the direct reach of OS profilers and debuggers. Or within reach, but only through a prohibitively high-overhead HW-specific debugger interface.</p>

<p>There's also no /proc representation for these programs either (I've been proposing building an equivalent) so there's no direct way to even tell what is running and what isn't, and all the other /proc details. Forget instruction profiling, even ps(1) and all the other process tools do not work.</p>

<p>It's been a mind-bending experience, revealing what gets taken for granted because it has existed in CPU land for decades: A process table. Process tools. Standard file formats. Programs that exist in the file system. Programs running from main memory. Debuggers. Profiliers. Core dumping. Disassembling. Single stepping. Static and dynamic instrumentation. Etc. For GPUs and AI, this is all far less mature. It can make the work exciting at times, when you think something is impossible and then find or devise a way.</p>

<p>Fortunately we have a head start as some things do exist. Depending on the runtime and kernel driver, there are debug interfaces where you can list running accelerator programs and other statistics, as used by tools like intel_gpu_top(1). You can kill -9 a GPU workload using intel_gpu_abrt(1). Some interfaces can even generate basic ELF files for the running accelerator programs that you can try to load in a debugger like gdb(1). And there is support for GPU/AI program disassembly, if you can get your hands on the binary. It feels to me like GPU/AI debugging, OS style, is about two years old. Better than zero, but still early on, and lots more ahead of us. A decade, at least.</p>

<h2>What do AI developers think of this?</h2>

<p>We've shown AI Flame Graphs to other AI developers at Intel and a common reaction is to be a bit puzzled, wondering what to do with it. AI developers think about their bit of code, but with AI Flame Graphs they can now see the entire stack for the first time, including the HW, and many layers they don't usually think about or don't know about. It basically looks like a pile of gibberish with their code only a small part of the flame graph.</p>

<div><center><a href="https://www.brendangregg.com/Slides/YOW2022_flame_graphs/#8"><img src="https://www.brendangregg.com/blog/images/2024/flamegraph-montage.png" width="190"></a><br><span size="-1"><i>CPU Flame Graph Implementations</i></span></center></div>

<p>This reaction is similar to people's first experiences with CPU flame graphs, which show parts of the system that developers and engineers typically don't work on, such as runtime internals, system libraries, and kernel internals. Flame graphs are great at highlighting the dozen or so functions that matter the most, so it becomes a problem of learning what those functions do across a few different code bases, which are typically open source. Understanding a dozen such functions can take a few hours or even a few days -- but if this leads to a 10% or 2x cost win, it is time well spent. And the next time the user looks at a flame graph, they start saying "I've seen that function before" and so on. You can get to the point where understanding the bulk of a CPU flame graph takes less than a minute: look for the widest tower, click to zoom, read the frames, done.</p>

<p>I'm encouraged by the success of CPU flame graphs, with over 80 implementations and countless real world case studies. Sometimes I'm browsing a performance issue I care about on github and hit page down and there's a CPU flame graph. They are everywhere.</p>

<p>I expect AI developers will also be able to understand AI Flame Graphs in less than a minute, but to start with people will be spending a day or more browsing code bases they didn't know were involved. Publishing case studies of found wins will also help people learn how to interpret them, and also help explain the value.</p>

<h2>What about PyTorch?</h2>

<p>Another common reaction we've had is that AI developers are using PyTorch, and initially we didn't support it as it meant walking Python stacks, which isn't trivial. But prior work has been done there (to support CPU profiling) and after a lot of tinkering we now have the first PyTorch AI Flame Graph:</p>

<center><a href="https://www.brendangregg.com/blog/images/2024/PyTorchFlamegraph.svg"><img src="https://www.brendangregg.com/blog/images/2024/PyTorchFlamegraph.png" width="700"></a><br><span size="-1"><i>PyTorch frames in pink </i></span></center>

<p>(Click for interactive <a href="https://www.brendangregg.com/blog/images/2024/PyTorchFlamegraph.svg">SVG</a>.) The PyTorch functions are at the bottom and are colored pink. This example runs oneDNN kernels that are JIT-generated, and don't have a source path so that layer just reads "jit". Getting all other the layers included was a real pain to get going, but an important milestone. We think if we can do PyTorch we can do anything.</p>

<p>In this flame graph, we show PyTorch running the Llama 2 7B model using the Intel Extensions for PyTorch (IPEX). This flame graph shows the origin of the GPU kernel execution all the way back to the Python source code shown in pink. Most samples are from a stack leading up to a gemm_kernel (matrix multiply) shown in aqua, which like the previous example has many stalls due to software scoreboarding.</p>

<p>There are two instructions (0xa30 and 0xa90) that combined are 27% of the entire profile. I expect someone will ask: Can't we just click on instructions and have it bring up a dissassembly view with full source? Yes, that should be possible, but I can't answer how we're going to provide this yet. Another expected question I can't yet answer: Since there are now multiple products providing AI auto-tuning of CPU workloads using CPU flame graphs (including <a href="https://granulate.io/">Intel Granulate</a>) can't we have AI auto-tuning of <em>AI</em> workloads using AI Flame Graphs?</p>

<h2>First Release: Sometimes hard and with moderate overhead</h2>

<p>Getting AI Flame Graphs to work with some workloads is easy, but others are currently hard and cost moderate overhead. It's similar to CPU profiling, where some workloads and languages are easy to profile, whereas others need various things fixed. Some AI workloads use many software dependencies that need various tweaks and recompilation (e.g., enabling frame pointers so that stack walking works) making setup time consuming. PyTorch is especially difficult and can take over a week of OS work to be ready for AI Flame Graphs. We will work on getting these tweaks changed upstream in their respective repositories, something involving teams inside and outside of Intel, and is a process I'd expect to take at least a year. During that time AI workloads will gradually become easier to flame graph, and with lower-overhead as well.</p>

<p>I'm reminded of eBPF in the early days: You had to patch and recompile the kernel and LLVM and Clang, which could take multiple days if you hit errors. Since then all the eBPF dependency patches have been merged, and default settings changed, so that eBPF "just works." We'll get there with AI Flame Graphs too, but right now it's still those early days.</p>

<p>The changes necessary for AI Flame Graphs are really about improving debugging in general, and are a requirement for <a href="https://www.brendangregg.com/Slides/eBPFSummit2023_FastByFriday/">Fast by Friday</a>: A vision where we can root-cause analyze anything in five days or less.</p>

<h2>Availability</h2>

<p>AI Flame Graphs will first become available on the <a href="https://www.brendangregg.com/blog//2024-10-29/yes,%20Intel%20has%20a%20public%20cloud">Intel Tiber AI Cloud</a> as a preview feature for the Intel Data Center GPU Max Series. If you are currently deployed there you can ask through the Intel service channel for early access. As for if or when it will support other hardware types, be in other Intel products, be officially launched, be open source, etc., these involve various other teams at Intel and they need to make their own announcements before I can discuss them here.</p>

<h2>Conclusions</h2>

<p>Finding performance improvements for AI data centers of just fractions of a percent can add up to planetary savings in electricity, water, and money. If AI flame graphs have the success that CPU flame graphs have had, I'd expect finding improvements of over 10% will be common, and 50% and higher will eventually be found*. But it won't be easy in these early days as there are still many software components to tweak and recompile, and software layers to learn about that are revealed in the AI flame graph.</p>

<p>In the years ahead I imagine others will build their own AI flame graphs that look the same as this one, and there may even be startups selling them, but if they use more difficult-to-use and higher-overhead technologies I fear they could turn companies off the idea of AI flame graphs altogether and prevent them from finding sorely needed wins. This is too important to do badly. AI flame graphs should be easy to use, cost negligible overhead, be production safe, and show everything. Intel has proven it's possible.</p>

<h2>Disclaimer</h2>

<p><span size="-1">
* This is a personal blog post that makes personal predictions but not guarantees of possible performance improvements. Feel free to take any claim with a grain of salt, and feel free to wait for an official publication and public launch by Intel on this technology.</span></p><p><span size="-1"><sup>1</sup> Based on halving the Arm CEO Rene Haas' estimate of 20-25% quoted in <a href="https://arstechnica.com/ai/2024/06/is-generative-ai-really-going-to-wreak-havoc-on-the-power-grid/">Taking a closer look at AI's supposed energy apocalypse</a> by Kyle Orland of ArsTechnica.
</span></p>

<h2>Thanks</h2>

<p><i>Thanks to everyone at Intel who have helped us make this happen. Markus Flierl has driven this project and made it a top priority, and Greg Lavender has expressed his support. Special thanks to Michael Cole, Matthew Roper, Luis Strano, Rodrigo Vivi, Joonas Lahtinen, Stanley Gambarin, Timothy Bauer, Brandon Yates, Maria Kraynyuk, Denis Samoylov, Krzysztof Raszknowski, Sanchit Jain, Po-Yu Chen, Felix Degrood, Piotr Rozenfeld, Andi Kleen, and all of the other coworkers that helped clear things up for us, and thanks in advance for everyone else who will be helping us in the months ahead.</i></p><p><i>My final thanks is to the companies and developers who do the actual hands-on work with flame graphs, collecting them, examining them, finding performance wins, and applying them.<br>You are helping save the planet.</i></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jaywalking legalized in New York City (181 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2024/oct/29/new-york-jaywalking-legal</link>
            <guid>41992399</guid>
            <pubDate>Wed, 30 Oct 2024 06:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2024/oct/29/new-york-jaywalking-legal">https://www.theguardian.com/us-news/2024/oct/29/new-york-jaywalking-legal</a>, See on <a href="https://news.ycombinator.com/item?id=41992399">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Jaywalking – that time-honored practice of crossing the street outside of the crosswalk or against the traffic light – is now legal in <a href="https://www.theguardian.com/us-news/new-york" data-link-name="in body link" data-component="auto-linked-tag">New York</a> City.</p><p>Legislation passed by the city council last month officially became law over the weekend after the city’s mayor, Eric Adams, declined to take action – either by signing or vetoing it – after 30 days.</p><p>Council member Mercedes Narcisse, a Brooklyn Democrat who sponsored the legislation, said on Tuesday that the new law ends racial disparities in enforcement, noting that more than 90% of the jaywalking tickets issued last year went to Black and Latino people.</p><p>“Let’s be real, every New Yorker jaywalks. People are simply trying to get where they need to go,” she said in an emailed statement. “Laws that penalize common behaviors for everyday movement shouldn’t exist, especially when they unfairly impact communities of color.”</p><p>The new law permits pedestrians to cross a roadway at any point, including outside of a crosswalk. It also allows for crossing against traffic signals and specifically states that doing so is no longer a violation of the city’s administrative code.</p><p>But the new law also warns that pedestrians crossing outside of a crosswalk do not have the right of way and that they should yield to other traffic that has the right of way.</p><p>Liz Garcia, an Adams spokesperson, declined to elaborate on the mayor’s decision to let the bill become law without his action.</p><p>But she noted the bill makes it clear that crossing against the light and mid-block is highly risky behavior. People may also still be liable in civil actions for accidents caused by jaywalking, Garcia added.</p><p>“All road users are safer when everyone follows traffic rules,” she said in a statement. “We continue to encourage pedestrians to take advantage of safety mechanisms in place – such as daylighting, pedestrian islands, and leading pedestrian intervals – by crossing in a crosswalk with the walk signal.”</p><p>Other cities and states, from Denver and Kansas City, Missouri, to California, Nevada and Virginia, have decriminalized jaywalking in recent years, according to America Walks, a Seattle-based group that’s been tracking the proposals.</p><p>“Cities that truly care about safety focus on street design, speeding and dangerously large vehicles,” Mike McGinn, the group’s executive director, said Tuesday. “Not jaywalking laws.”</p><p>The laws were pushed by the auto industry in the 1930s as a way to keep people off streets and make more room for vehicles, according to America Walks.</p><p>The term “jaywalking” dates to the early 20th century and has its roots in midwestern slang for a country bumpkin or rube, according to dictionary maker Merriam-Webster.</p><p>In New York City, where struggles between pedestrians and motorists are constant, the jaywalking law had been on the books since 1958 and carried a penalty of up to $250.</p><p>In the 1969 film Midnight Cowboy, Dustin Hoffman famously yells: “I’m walking here!” as his character is almost hit by a cab while crossing the street in Manhattan.</p><p>The Legal Aid Society called the legislation long overdue. The non-profit organization, which provides free legal representation to New Yorkers who cannot afford a lawyer, said police for decades have used the violation as a pretext to stop, question and frisk residents – especially those of color.</p><p>“With this legislation now codified, we hope that both the Adams Administration and the City Council will continue to abolish relic laws that serve no public safety purpose and only ensnare people in the criminal legal system,” the organization said in a statement.</p><p>Police department spokespeople didn’t immediately respond to emails seeking comment, and a spokesperson for its largest union declined to weigh in.</p><p>But Narcisse said officers she has spoken to say their time could be better spent on other police work, rather than issuing tickets for jaywalking.</p><p>“No one’s ever said: ‘I’m so glad they caught that jaywalker.’ By eliminating these penalties, we allow our police officers to focus on issues that truly matter,” she said.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[15 Billion Miles Away, NASA's Voyager 1 Breaks Its Silence (200 pts)]]></title>
            <link>https://scitechdaily.com/15-billion-miles-away-nasas-voyager-1-breaks-its-silence/</link>
            <guid>41992394</guid>
            <pubDate>Wed, 30 Oct 2024 06:48:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scitechdaily.com/15-billion-miles-away-nasas-voyager-1-breaks-its-silence/">https://scitechdaily.com/15-billion-miles-away-nasas-voyager-1-breaks-its-silence/</a>, See on <a href="https://news.ycombinator.com/item?id=41992394">Hacker News</a></p>
Couldn't get https://scitechdaily.com/15-billion-miles-away-nasas-voyager-1-breaks-its-silence/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Australia/Lord_Howe is the weirdest timezone (906 pts)]]></title>
            <link>https://ssoready.com/blog/engineering/truths-programmers-timezones/</link>
            <guid>41992314</guid>
            <pubDate>Wed, 30 Oct 2024 06:21:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ssoready.com/blog/engineering/truths-programmers-timezones/">https://ssoready.com/blog/engineering/truths-programmers-timezones/</a>, See on <a href="https://news.ycombinator.com/item?id=41992314">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
            Timezones are weird. But only finitely so. Here's the exact conceptual model you should have of them.
        </p><div>
            <p>The standard trope when talking about timezones is to rattle off falsehoods
programmers believe about them. These lists are only somewhat enlightening –
it’s really hard to figure out what truth is just from the contours of
falsehood.</p>
<p>So here’s an alternative approach. I’m gonna show you some weird timezones. In
fact, the <em>weirdest</em> timezones. They’re each about as weird as timezones are
allowed to get in some way.</p>
<ul>
<li><code>Asia/Kathmandu</code> has a weird offset from UTC</li>
<li><code>Africa/Casablanca</code> doesn’t fit into the timezone model cleanly, so it’s hard-coded</li>
<li><code>America/Nuuk</code> does daylight savings at -01:00 (yes, with a negative)
<ul>
<li>and <code>Africa/Cairo</code> and <code>America/Santiago</code> do it at 24 o’clock (not 0 o’clock)</li>
</ul>
</li>
<li><code>Australia/Lord_Howe</code>, population 382 and <a href="https://en.wikipedia.org/wiki/Dryococelus_australis">some notable stick
bugs</a>, has the weirdest
daylight savings rule</li>
</ul>
<p>To learn how their weirdness is represented in software, we’ll look at the raw
timezone files that all software ultimately relies on. From there, two things
will become clear:</p>
<ul>
<li>Yeah, this stuff is weird</li>
<li>But only finitely so, because ultimately a computer’s gotta implement them</li>
</ul>
<p>But first, an aside on the calendar.</p>
<h2 id="pgxiiream-pope-gregory-xiii-rules-everything-around-me">PGXIIREAM: Pope Gregory XIII rules everything around me</h2>
<p>Unless you’re doing some fairly exotic things where you’re finding yourself
saying things like</p>
<blockquote>
<p>Oh yeah the OCR on Japanese driving licenses pops out things like “平成 8”,
that’s just <a href="https://en.wikipedia.org/wiki/Japanese_era_name">how they sometimes say
1996</a> over there. That’s why
we have this in the parser:</p>
<pre tabindex="0"><code>eras = { "大正": 1912, "昭和": 1926, "平成": 1989 }
</code></pre><p>One of these days we’ll need to add <code>"令和": 2019</code>, but it hasn’t come up yet.</p>
</blockquote>
<p>or</p>
<blockquote>
<p>We’re gonna need to set up a per-country feature flag when deciding whether
banks are closed for Eid. <a href="https://www.economist.com/middle-east-and-africa/2019/06/06/when-is-eid-al-fitr">Saudi Arabia and Iran don’t agree on when the lunar
month
starts</a>.</p>
</blockquote>
<p>Then yeah, sure, you may need to write software that knows about the Japanese or
Islamic calendar systems.</p>
<p>Cases like this are a small minority. The reality of the world is that the
Western system of timekeeping is the dominant one, and even in e.g. Japan and
the Muslim world, almost everyone who uses computers is familiar with the
Gregorian system.</p>
<p>With computers, we project the Gregorian system into the future and past, which
is called the proleptic Gregorian calendar and isn’t historically accurate but
nobody really cares except <a href="https://en.wikipedia.org/wiki/Old_Style_and_New_Style_dates">Russian revolution
nerds</a>.</p>
<p>This calendar system is pretty much good enough, and barring any <a href="https://en.wikipedia.org/wiki/French_Republican_calendar">rationalist
coups d’etat</a>, is the
one we’ll be stuck with for a long time. It does one thing well: it’s very good
at keeping the sun at the same place in the sky across the years. It doesn’t let
the months drift around the seasons like the Roman calendar did.</p>
<p>Technically, this “keep the sun roughly in the same place whenever it’s the same
time-of-day” is called “mean solar time”. And that’s why GMT, Greenwich Mean
Time, is called that way. It’s about the mean solar time of the <a href="https://en.wikipedia.org/wiki/Royal_Observatory,_Greenwich">English
observatory in
Greenwich</a>.</p>
<p>By the way, we technically don’t call it GMT anymore. Unless you’re talking
about what time people in London say it is, you probably technically mean
<a href="https://en.wikipedia.org/wiki/Coordinated_Universal_Time">UTC</a>.</p>
<p><img src="https://ssoready.com/blog/engineering/truths-programmers-timezones/utc.png" alt=""></p>
<p><a href="https://en.wikipedia.org/wiki/Coordinated_Universal_Time">Coordinated Universal
Time</a> is basically
just a modern formalization of GMT. It’s useful because almost everyone on the
planet has agreed to base their clocks off of an <em>offset</em> from UTC. It’s still a
solar mean time, but the connection to Greenwich isn’t really there anymore.</p>
<p>I bring this up because you may have heard of a weird modern quirk on Pope
Gregory’s sun-following endeavors:</p>
<h2 id="leap-seconds-dont-matter">Leap seconds don’t matter</h2>
<p>The Earth’s rotation is slowing down. Days are getting longer. So you need to
correct for it if you want to keep IRL days in sync with computer days.</p>
<p>The nerd task force assigned to this problem is the <a href="https://en.wikipedia.org/wiki/International_Earth_Rotation_and_Reference_Systems_Service">International Earth
Rotation and Reference Systems
Service</a>,
which has two primary goals:</p>
<ol>
<li>Watch the Earth rotate, and report back on their findings</li>
<li>Break Wikipedia’s CSS with their long name</li>
</ol>
<figure>
<div>
<p><img src="https://ssoready.com/blog/engineering/truths-programmers-timezones/iers.png"></p><figcaption>timecops</figcaption>
</div>
</figure>
<p>If the days are getting longer, and they’re doing so at a fairly unpredictable
rate, the simplest solution is to have IERS occasionally just insert an extra
second in the day to make clocks go slower. It’s called a <a href="https://en.wikipedia.org/wiki/Leap_second">leap
second</a>.</p>
<p>You should completely ignore the fact that this is a thing. It’s a cool novelty,
but it’s effectively just a detail you can ignore, because:</p>
<ol>
<li><a href="https://go.dev/play/p/9RwZu2jmlPl">It’s not like programming languages support representing 61-second minutes
anyway</a></li>
<li>You (and by you I mean your cloud provider) can just run your clocks slower
around the time of the leap second, and pretend to everyone else over
<a href="https://en.wikipedia.org/wiki/Network_Time_Protocol">NTP</a> that their clocks
are running fast. This is called leap smearing.</li>
</ol>
<p>Btw it’s called UTC (Universal Time Coordinated? huh?) because the same folks
who publish UTC also publish UT1, which is UTC sans the leap seconds. There were
other UTs before the Coordinated variant came up.</p>
<h2 id="weird-time-zones">Weird time zones</h2>
<p>OK! Let’s start looking at some weird time zones, and find out how your computer
knows to represent them.</p>
<h2 id="asiakathmandu-is-on-a-weird-offset"><code>Asia/Kathmandu</code> is on a weird offset</h2>
<p>Most of the world is on a whole number of hours before or after UTC. About a
fifth the world by population is on a half-hour offset from UTC; in particular,
India is 5h30m ahead of UTC.</p>
<p>Nepal is 5h45m ahead of UTC:</p>
<div><pre tabindex="0"><code data-lang="text"><span><span>$ TZ=UTC date ; TZ=Asia/Kathmandu date
</span></span><span><span>Tue Jul 30 23:52:11 UTC 2024
</span></span><span><span>Wed Jul 31 05:37:11 +0545 2024
</span></span></code></pre></div><p>If you’re like me, you must be have at one point wondered how in the <em>world</em>
your computer knows this fact.</p>
<p>Here’s a hint:</p>
<div><pre tabindex="0"><code data-lang="text"><span><span>$ TZ=Asia/Kathmandu strace -e trace=openat date
</span></span><span><span>...
</span></span><span><span>openat(AT_FDCWD, "/usr/share/zoneinfo/Asia/Kathmandu", O_RDONLY|O_CLOEXEC) = 3
</span></span><span><span>Wed Jul 31 05:40:49 +0545 2024
</span></span></code></pre></div><p>On your filesystem is a database called the IANA Timezone Database, aka tzdb or
zoneinfo. It’s a bunch of binary files, encoded in <a href="https://www.rfc-editor.org/rfc/rfc8536.html">Timezone Information
Format</a>. The names of those files
act as timezone identifiers, which is where you see strings like
<code>America/Los_Angeles</code> or <code>Europe/London</code> come from:</p>
<pre tabindex="0"><code>$ tree /usr/share/zoneinfo
...
├── America
│&nbsp;&nbsp; ├── Los_Angeles
├── Europe
│&nbsp;&nbsp; ├── London
...
</code></pre><p>At the very end of <code>/usr/share/zoneinfo/Asia/Kathmandu</code> is this little string:</p>
<pre tabindex="0"><code>cat /usr/share/zoneinfo/Asia/Kathmandu
...
&lt;+0545&gt;-5:45
</code></pre><p>The syntax is here pretty obtuse, but what it means is:</p>
<blockquote>
<p>Unless otherwise specified, UTC is 5h45m behind this timezone. Call this time
<code>+0545</code>.</p>
</blockquote>
<p>That’s precisely how software can figure out the time in Nepal. That’s also why
the output from <code>date</code> above has <code>+0545</code> in it.</p>
<h2 id="why-strings-like-pdt-or-cet-are-pretty-meaningless">Why strings like <code>PDT</code> or <code>CET</code> are pretty meaningless</h2>
<p>In the example above, <code>+0545</code> is called a “designator”. It’s a pretty-ish string
describing which <em>part</em> of a timezone a timestamp is in. It’s meant to be used
for outputting timestamps, and is only unambiguous if you already know what
timezone the timestamp was taken in.</p>
<p>Just <em>how</em> ambiguous are these designators? I wrote a <code>tzdump</code> script that
converts TZIF files to JSON. Here’s the top hits:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>find -L /usr/share/zoneinfo -type f <span>\
</span></span></span><span><span><span></span>  <span>|</span> xargs -n1 ./tzdump <span>\
</span></span></span><span><span><span></span>  <span>|</span> jq -r <span>'"\(.ID)\t\(.Transitions[].LocalTimeType.Designation)"'</span> <span>\
</span></span></span><span><span><span></span>  <span>|</span> sort <span>|</span> uniq <span>|</span> sort -k <span>2</span> <span>|</span> uniq -f <span>1</span> -c <span>|</span> sort -n <span>|</span> awk <span>'{ print $1 "\t" $3 }'</span> <span>|</span> tail -r
</span></span></code></pre></div><p>The most popular designators are:</p>
<pre tabindex="0"><code>66	CST
58	CDT
56	CET
56	CEST
</code></pre><p>A total of <strong>66 timezones</strong> use <code>CST</code>, either in the past or future. Many
timezones are functionally exact clones of each other – there’s no difference
between <code>America/Phoenix</code> and
<a href="https://en.wikipedia.org/wiki/Creston,_British_Columbia"><code>America/Creston</code></a>,
but they each get their own file – but still. There’s a lot of ambiguity in
there.</p>
<p>In case you’re curious, only 33 designators are unique to a timezone. A lot more
are functionally unique, but I’m too lazy to dedupe logically-equivalent
timezones right now.</p>
<p>As an extra fun bit of trivia, designators are not strictly uppercase/numeric.
<code>ChST</code>, appearing in <code>Pacific/Saipan</code>, stands for Chamorro Standard Time. It’s
the only designator with a lowercase name. <code>CHST</code> is not taken, sadly for those
of us who love bugs.</p>
<h2 id="how-are-timezones-with-dst-represented">How are timezones with DST represented?</h2>
<p>When we looked at Kathmandu, we had this string telling us the Nepalese time
rules:</p>
<pre tabindex="0"><code>&lt;+0545&gt;-5:45
</code></pre><p>Ok, simple enough. But what about a timezone with DST transitions? The syntax
has lots of defaults (DST will be a one-hour jump, it happens at 2am by default,
etc) but <code>Europe/Athens</code> is a good example of one that uses most of the syntax:</p>
<pre tabindex="0"><code>$ cat /usr/share/zoneinfo/Europe/Athens
...
EET-2EEST,M3.5.0/3,M10.5.0/4
</code></pre><p>That syntax means:</p>
<blockquote>
<p>Standard time is called <code>EET</code>, it’s 2 hours ahead of UTC. DST is called <code>EEST</code>
(it’s 3 hours ahead, an implicit default relative to standard time). Start DST
in month <code>3</code> on the last instance of (<code>5</code>) day <code>0</code> (Sunday) in that month, at
3am local (<code>/3</code>). End DST on month <code>10</code> on the last Sunday at 4am local
(<code>5.0/4</code>).</p>
</blockquote>
<p>So yeah, your computer does a bunch of <a href="https://howardhinnant.github.io/date_algorithms.html">kind of
gnarly</a> logic to figure
out what date-and-time a timestamp corresponds to, then figures out whether it’s
inside or outside DST to figure out the current local time. Delightful.</p>
<p>In case you’re curious, the spec says “5” means “last instance of”, and “1”
means “first instance of”. But only weeks “1”, “2”, and “5” are used:</p>
<pre tabindex="0"><code>$ find -L /usr/share/zoneinfo -type f | xargs -n1 ./tzdump | jq -r 'if .Rules.DST == null then empty else "\(.ID)\t\(.Rules.DST.Week)" end' | sort -k2 | uniq -f 1 -c | awk '{ print $1 "\t" $3 }'
18	1
89	2
81	5
</code></pre><p>Here’s a fun twist: on my Mac 100% of timezones either don’t have DST at all or
use this nth-instance-of-day-of-month rules to do DST switching. But inside
<code>/var/db/timezone</code> there’s different versions of tzdb. In there is a version
with other kinds of timezones in it:</p>
<pre tabindex="0"><code>$ cat /var/db/timezone/tz/2024a.1.0/zoneinfo/Africa/Casablanca
...
XXX-2&lt;+01&gt;-1,0/0,J365/23
</code></pre><p>That timezone basically means “we are perpetually on daylight savings”, because
the <code>J###</code> syntax means “<code>###</code>-th day of the year, not counting Feb 29 if there
is one” (J stands for “Julian calendar”).</p>
<p>Technically, that timezone also exercises the prefixless (i.e. without <code>M</code> or
<code>J</code>) syntax for indicating days, where <code>###</code> means “<code>###</code>-th day of year,
counting any Feb 29”. But in this case it’s a distinction without a difference.</p>
<p>(Aside: All this stuff comes from POSIX. <a href="https://www.gnu.org/software/libc/manual/html_node/TZ-Variable.html">GNU’s docs about the POSIX <code>TZ</code> env
var</a>, which
TZIF builds on, are the best I know of online for this stuff.)</p>
<p>But this is just the start of the weirdness that is <code>Africa/Casablanca</code>.</p>
<h2 id="africacasablanca-and-asiagaza-follow-the-moon-but-timezones-follow-the-sun"><code>Africa/Casablanca</code> and <code>Asia/Gaza</code> follow the moon, but timezones follow the sun</h2>
<p>The TZIF format supports three possible rules for deciding on your daylight
savings transition day:</p>
<ul>
<li>Rules like “first Tuesday of March”</li>
<li>Rules like “45th day of the year”</li>
<li>Rules like “45th day of the year, Feb 29 doesn’t count”</li>
</ul>
<p>Morocco and Gaza do their daylight savings based on Ramadan. Ramadan is a month
in the Islamic calendar. The Islamic calendar is based on the moon. The lunar
calendar isn’t a clean multiple of the solar calendar; from the Gregorian
perspective, lunar months seem to slowly “rotate” around the year, because
they’re basically on a different modulo. There’s a problem there for our heroes
at the tzdb.</p>
<p>The solution? The dumbest possible one.</p>
<p>A TZIF file ends with the footer syntax we’ve been talking about to this point.
But it <em>starts</em> with a big long list of historical data about a timezone. If a
country ever changes timezone rules, TZIF represents that by encoding the new
rule in the footer, and hard-coding all the old transitions.</p>
<p>But you can also just take these hard-coded transitions and put them into the
future. The hard-coded transitions take precedence over the footer. So the TZIF
folks:</p>
<ol>
<li>Picked a year far enough into the future (2086, as it turns out)</li>
<li>Wrote a script in
<a href="https://github.com/eggert/tz/blob/339e81d1ade620e70ecc78c2b4ec1309a6b80a2f/asia#L3494-L3512">emacs</a>
<a href="https://github.com/eggert/tz/blob/339e81d1ade620e70ecc78c2b4ec1309a6b80a2f/africa#L861-L878">lisp</a>
to calculate Ramadan</li>
<li>Use the output of that script to generate transitions for Morocco and Gaza</li>
</ol>
<p>And that’s why in practice Morocco and Gaza are just hard-coded in the tzdb,
unlike every other timezone.</p>
<p>In case you’re hoping for more fun timezones like this, I’m afraid you’re out of
luck. The others at the bottom of this list, which filters for transitions
beyond 2025, are just synonyms of Casablanca and Gaza.</p>
<pre tabindex="0"><code>$ find -L /var/db/timezone/tz/2024a.1.0/zoneinfo/ -type f | xargs -n1 ./tzdump | jq 'select(.Transitions[].TransitionTime &gt; 1735689600) | .ID' -r | uniq -c | sort -n
  26 /var/db/timezone/tz/2024a.1.0/zoneinfo//Africa/Cairo
...
  26 /var/db/timezone/tz/2024a.1.0/zoneinfo//US/Pacific
  26 /var/db/timezone/tz/2024a.1.0/zoneinfo//WET
  26 /var/db/timezone/tz/2024a.1.0/zoneinfo//posixrules
 130 /var/db/timezone/tz/2024a.1.0/zoneinfo//Africa/Casablanca
 130 /var/db/timezone/tz/2024a.1.0/zoneinfo//Africa/El_Aaiun
 184 /var/db/timezone/tz/2024a.1.0/zoneinfo//Asia/Gaza
 184 /var/db/timezone/tz/2024a.1.0/zoneinfo//Asia/Hebron
</code></pre><p>It looks like every other timezone just has 26 transitions beyond 2025, which I
think are just there to make software that doesn’t know about the TZIF footer
transition rules be accurate a few years into the future anyway.</p>
<h2 id="americanuuk-transitions-to-dst-at--1-oclock"><code>America/Nuuk</code> transitions to DST at -1 o’clock</h2>
<p><a href="https://en.wikipedia.org/wiki/Nuuk">Nuuk</a> is in
<a href="https://en.wikipedia.org/wiki/Iceland">Greenland</a>, and is part of the <a href="https://en.wikipedia.org/wiki/Special_territories_of_members_of_the_European_Economic_Area">greater
EU cinematic
universe</a>.</p>
<p>All of Europe (idk whether this is an EU/EEZ/EFTA/CoE thing) syncs up their
daylight savings, except for <a href="https://en.wikipedia.org/wiki/Greenland">Iceland</a>,
which doesn’t do DST at all (<code>Atlantic/Reykjavik</code>, which is technically <a href="https://github.com/eggert/tz/blob/7748036bace8562b9c047f368c8eba5f35e8c4b4/backward#L226">an
alias for
<code>Africa/Abidjan</code></a>,
is basically just UTC; their rule string is just <code>GMT0</code>).</p>
<p>Most Europeans are familiar with three major timezones, which we can refer to as
<code>Europe/Lisbon</code> (western), <code>Europe/Brussels</code> (central), and <code>Europe/Athens</code>
(eastern). They’re each one hour ahead of the other, and so their timezone
transitions look like:</p>
<pre tabindex="0"><code># I'm gonna space these out to highlight the symmetry,
# and also spell out the implicit "/2"

Europe/Lisbon:   WET0WEST ,M3.5.0/1,M10.5.0/2
Europe/Brussels: CET-1CEST,M3.5.0/2,M10.5.0/3
Europe/Athens:   EET-2EEST,M3.5.0/3,M10.5.0/4
</code></pre><p>In other words, Lisbon springs forward at 1am, Brussels at 2am, and Athens at
3am. But those times are <em>local</em>. In reality, they’re all at the same instant.</p>
<p>This makes good sense. It’s good for business that the time difference between
any two spots in Europe is always the same.</p>
<p>Greenland would like to be part of the action. Thing is, Greenland is pretty far
west of continental Europe. Whereas Lisbon’s standard time is UTC, Greenland’s
is 3 hours behind UTC. Here’s their daylight transition rules:</p>
<pre tabindex="0"><code>$ cat /var/db/timezone/tz/2024a.1.0/zoneinfo/America/Nuuk
&lt;-02&gt;2&lt;-01&gt;,M3.5.0/-1,M10.5.0/0
</code></pre><p>Take note of <code>M3.5.0/-1</code>. The first part is the standard European DST start day.
The <code>/-1</code> part? That means that instead of doing DST at like 2am (<code>/2</code>),
Greenland does it at -1 o’clock (<code>/-1</code>). The way the rules file is encoded,
daylight savings for Greenland is meant to happen on Sunday, but in fact happens
at 11pm on the Saturday before. Super weird.</p>
<p>I’m guessing this breaks software, because America/Nuuk and its aliases are one
of those timezones whose transition rules are just entirely ommitted in
<code>/usr/share/zoneinfo</code> on my Mac. They’re only available in other copies of tzdb
in <code>/var/db/timezone</code>.</p>
<h2 id="oh-americasantiago-and-africacairo-transition-at-24-oclock">Oh, <code>America/Santiago</code> and <code>Africa/Cairo</code> transition at 24 o’clock</h2>
<p>Nuuk is the earliest anyone does a transition. Santiago and Cairo are the
latest. They both do transitions at 24 o’clock? Like, the next day?</p>
<pre tabindex="0"><code>America/Santiago: &lt;-04&gt;4&lt;-03&gt;,M9.1.6/24,M4.1.6/24
</code></pre><pre tabindex="0"><code>Africa/Cairo: EET-2EEST,M4.5.5/0,M10.5.4/24
</code></pre><p>I think they’re both encoded like that because of weirdness in how the
governments define the rules. Like <code>M10.5.4/24</code> means “last Thurday of October,
24 o’clock”, which really means “the day after the last Thursday of October”.
But that’s not the same thing as “last Friday of October” if the month ends on
Thursday?</p>
<p>Both of these files are also in Mac’s list of naughty timezones that don’t go in
<code>/usr/share/zoneinfo</code>.</p>
<h2 id="australialord_howe-has-the-weirdest-dst-transition"><code>Australia/Lord_Howe</code> has the weirdest DST transition</h2>
<p>When you do a DST transition, you “spring forward” and “fall back”. <em>Surely</em>
everyone agrees it’s a <em>one-hour</em> jump, right?</p>
<p>Here’s a script to check. What is the time difference between standard and
daylight time in every timezone?</p>
<pre tabindex="0"><code>$ find -L /usr/share/zoneinfo -type f | xargs -n1 ./tzdump | jq 'if .Rules.DST == null then "\(.ID)\t0" else "\(.ID)\t\(.Rules.DST.LocalTimeType.UTCOffsetSeconds - .Rules.Std.LocalTimeType.UTCOffsetSeconds)" end' -r | sort -n -k 2 | uniq -c -f 1 | awk '{ print $1 "\t" $3 }'

410	0
2	1800
185	3600
1	7200
</code></pre><p>Hmm. 410 timezones just don’t DST at all. 185 have a 3600-second, i.e. 1-hour,
difference. And then there are the malcontents.</p>
<p>The 7200-second, i.e. 2-hour, jump is <code>Antarctica/Troll</code>. Fitting.</p>
<pre tabindex="0"><code>&lt;+00&gt;0&lt;+02&gt;-2,M3.5.0/1,M10.5.0/3
</code></pre><p>So during the winter (i.e. the northern summer) they use Norway time? But there
are <a href="https://en.wikipedia.org/wiki/Troll_(research_station)">like 6 people over the winter at
Troll</a>? Do these 6 souls
appreciate their contribution to software esoterica? I hope they do. Apparently
they use <a href="https://github.com/eggert/tz/blob/7748036bace8562b9c047f368c8eba5f35e8c4b4/antarctica#L212-L236">like four different times during the
year</a>
down there in practice, but there’s no syntax to express that.</p>
<p>OK but the real question is what’s up with the two 1800 transitions. They’re
synonyms for each other. It’s <code>Australia/Lord_Howe</code>, which has a <strong>powerful</strong>
30-minute DST transition:</p>
<pre tabindex="0"><code>&lt;+1030&gt;-10:30&lt;+11&gt;-11,M10.1.0,M4.1.0
</code></pre><p>10h30m ahead of UTC standard, 11h DST. Love this for them. Running cron jobs on
an hourly basis doesn’t in practice have very weird interactions with DST.
Everywhere else on the planet, every 60 minutes you’re back to the same spot on
the clock.</p>
<p>Except Lord Howe Island. Heroes. On the first Sunday of October, a 60-minute
timegap only puts you halfway around the clock. All your cron jobs are now
staggered relative to the local wall clock.</p>
<p>In case you’re curious, <a href="https://en.wikipedia.org/wiki/Lord_Howe_Island">Lord Howe
Island</a> belongs to Australia. It
has 382 people at the latest census. It’s a bit of a natural paradise, and
apparently to preserve that there’s a cap of 400 tourists at a time.</p>
<p>Probably the most famous aspect of Lord Howe is <a href="https://en.wikipedia.org/wiki/Ball%27s_Pyramid">Ball’s
Pyramid</a>.</p>
<figure>
<div>
<p><img src="https://ssoready.com/blog/engineering/truths-programmers-timezones/balls.png"></p><figcaption>Ball's Pyramid Memorial for Stickbugs and Software Engineers who write Timezone-related Code.</figcaption>
</div>
</figure>
<p>It’s an old collapsed volcano. It looks cool. It has some rare <a href="https://en.wikipedia.org/wiki/Ball%27s_Pyramid#Dryococelus_australis">stick
bugs</a>.</p>
<h2 id="big-takeaways">Big takeaways</h2>
<p>Timezones are weird, but finitely so. All they consist of is:</p>
<ul>
<li>An ID, e.g. <code>America/Los_Angeles</code></li>
<li>A set of hard-coded transitions, which range from the past into the future</li>
<li>A set of rules for how future transitions may happen</li>
</ul>
<p>Any given time in a timezone is just:</p>
<ul>
<li>An offset from UTC</li>
<li>With a “designator” time that doesn’t mean much</li>
<li>(This usually isn’t outputted anywhere) Whether the time is considered DST</li>
</ul>
<p>You can always uniquely identify what UTC time someone is referring to whenever
they tell you their timezone + local time + current time designator. The
timezone + designator gives you an offset, and you can apply the offset to the
local time to get UTC.</p>
<p>Like, it’s weird, it’s quirky, but it’s not like all bets are off.</p>
<p>Also:</p>
<ul>
<li>Don’t let people bully you into thinking that just because something is
complicated, it’s impossible.</li>
<li>This is because almost every standard (except ISO8601, whatever) is
just a file, and you can read it. You are smart. You can do it. Embrace
the weirdness of Greenland’s daylight savings. Believe in yourself.</li>
<li>If I were UN secretary general, I would kick out any countries that I deem
insufficiently considerate of Paul Eggert’s time.</li>
</ul>
<h2 id="appendix-other-weird-stuff-in-zoneinfo">Appendix: Other weird stuff in zoneinfo</h2>
<p>Honestly, there’s some stuff in zoneinfo that I can’t figure out. Even I have
nerd-sniping limits. Exercises for the reader.</p>
<p>These time zones have <em>hundreds</em> of hard-coded transitions out into the future.
I don’t understand why, it’s not like they all have lunar calendar stuff going
on.</p>
<ul>
<li>Asia/Jerusalem has 780 transitions in the future, out of 901 total</li>
<li>Africa/Cairo has 800 transitions in the future, out of 929 total</li>
<li>America/Nuuk has 800 transitions in the future, out of 889 total</li>
<li>America/Santiago has 800 transitions in the future, out of 931 total</li>
<li>Pacific/Easter has 800 transitions in the future, out of 911 total</li>
<li>Asia/Gaza has 982 transitions in the future, out of 1106 total</li>
</ul>
<p>They all lack a rules footer, but our friend Africa/Casablanca has a mere 132
transitions hard-coded and lacks a rules footer too. What’s up with that?</p>
<hr>
<p>P.S. If you’re the type of weird to think this stuff is neat: email me.
<a href="mailto:ulysse.carion@ssoready.com">ulysse.carion@ssoready.com</a> ;)</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[1BRC Coding Challenge: Nerd Sniping the Java Community (112 pts)]]></title>
            <link>https://www.infoq.com/presentations/1brc/</link>
            <guid>41992081</guid>
            <pubDate>Wed, 30 Oct 2024 05:02:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.infoq.com/presentations/1brc/">https://www.infoq.com/presentations/1brc/</a>, See on <a href="https://news.ycombinator.com/item?id=41992081">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="presentationNotes">
                                    <h2>Transcript</h2>

<p>Morling: I would like to talk about a viral coding challenge which I did in January of this year, called, The One Billion Row Challenge. I would like to tell a little bit the story behind it, how all this went, what I experienced during January. Then, of course, also some of the things I learned, some of the things the community learned from doing that. This is how it started. On January 1st, I put out this tweet, I put out a blog post introducing this challenge. Was a little bit on my mind to do something like that for quite a while. Between Christmas and New Year's Eve, I finally found the time to do it. I thought, let me go and put out this challenge. I will explain exactly what it was, and how it worked. That's how it started. Then it got picked up quite quickly. People started to work on that. They implemented this in Java, which was the initial idea, but then they also did in other languages, other ecosystems, like .NET, Python, COBOL, with databases, Postgres, Snowflake, Apache Pinot, all those good things.</p>

<p>There was an article on InfoQ, which was the most read article for the first six months of the year, about this. There also was this guy, I didn't know him, Prime, a popular YouTuber. He also covered this on his stream. What did I do then? I learned how to print 162 PDF files and send them out to people with their individual certificate of achievement. I learned how to send coffee mugs and T-shirts to people all around the world, because I wanted to give out some prices. I sent those things to Taiwan, South America, the Republic of South Korea, and so on. That was what happened during January.</p>

<p>Who am I? I work as a software engineer at a company called Decodable. We built a managed platform for stream processing based on Apache Flink. This thing is completely a side effort for me. It was a private thing, but then Decodable helped to sponsor it and supported me with doing that.</p>

<h2>The Goals</h2>

<p>What was the idea behind this? Why did I do this? I thought I would like to learn something new. There's all those new Java versions every six months. They come with new APIs, new capabilities. It's really hard to keep track of all those developments. I would like to know what's new in those new Java versions, and what can I do with those things? I wanted to learn about it, but also, I wanted to give the community an avenue to do that so that everybody can learn something new. Then, of course, you always want to have some fun. It should be a cool thing to do. You don't want to just go and read some blogs. You would like to get some hands-on experience.</p>

<p>Finally, also, the idea was to inspire others to do the same. This was the thing, which I think was a bit specific about this challenge, you could actually go and take inspiration from other people's implementations. Nothing was secret. You could go and see what they did, and be inspired by them. Obviously, you shouldn't just take somebody else's code and submit it as your own implementation. That would make much sense. You could take inspiration, and people actually did that, and they teamed up in some cases. The other motivation for that was, I wanted to debunk this idea, which sometimes people still have, Java is slow, and nothing could be further from the truth, if you look at modern versions and their capabilities. Still, this is on people's minds, and I wanted to help and debunk this. As we will see, I think that definitely was the case.</p>

<h2>How Did it Work?</h2>

<p>Let's get a bit into it. How did it work? You can see it all here in this picture. The idea was, we have a file with temperature measurements, and it's essentially like a CSV file, only that it wasn't a comma as a separator, but a semicolon with two columns, and a station name, like Hamburg or Munich, and so on. Then, a temperature measurement value associated to that, randomized values. The task was, process that file, aggregate the values from that file, and for each of those stations, determine the minimum value, the maximum value, and the average temperature value. Easy. Then the caveat only was, this has 1 billion rows as the name of the challenge gives away.</p>

<p>This file, if you generate this on your machine, it has a size of around 13 gigabytes, so quite sizable. Then you had to print out the results, as you can see it here. This already goes to show a little bit that I didn't spend super much time to prepare this, because this is just the two-string output of the Java HashMap implementation. Random to have this as the expected output. Then as people were implementing this, for instance, with relational databases, they actually went to great lengths to emulate that output format. I should have chosen a relational output format, next time.</p>

<h2>The Rules</h2>

<p>A little bit more about the rules. First of all, this was focused on Java. Why? It's the platform I know best, I would like to support. This is also what I wanted to spread the knowledge on. Then you could choose any version. Any new versions, preview versions, all the kinds of distributions, like GraalVM, or all the different JDK providers which are out there. You managed using this tool called SDKMAN. Who has heard about SDKMAN? You should go and check out SDKMAN, and you should use it to manage Java versions. It's a very nice tool for doing that, and switching back and forth between different versions. That's it. Java only. No dependencies.</p>

<p>It wouldn't make much sense to pull in some library and do the task for you. You should program this by yourself. No caching between runs. I did five runs of each implementation. Then I discarded the fastest and the slowest one, and I took the average value from the remaining three runs. It would make sense to have any caching there. Otherwise, you could just do the task once, persist the result in the file, read it back, and it would be very fast. That doesn't make much sense. You were allowed to take inspiration by others. Of course, not again, just resubmit somebody else's implementation. You could take inspiration.</p>

<h2>Evaluation Environment</h2>

<p>In terms of how I ran this. My company spent €100 on this machine which I got on the Hetzner cloud. Very beefy server with 32 cores, out of which I mostly used only 8 cores, I will explain later on. Quite a bit of RAM. Really, the file was always coming from a RAM disk. I wanted to make sure that disk I/O is not part of the equation, just because it's much less predictable, would have made the life much harder for me. Only a purely CPU bound problem here. How would we go and do this? This is my baseline implementation. I'm an average Java developer, so that's what I came up with. I use this Java Streams API. I use this files.lines method, which gives me a stream with the lines of a file. I read that file from disk, then I map each of my lines there using the split method. I want to separate the station name from the value. Then I collect the results, the lines into this grouping collector. I group it by the station name.</p>

<p>Then for each of my stations, I need to aggregate those values, which happens here in my aggregator implementation. Whenever a new value gets added to an existing aggregator object, I keep track of the min, the max, and in order to calculate average I keep track of the sum and the count of the values. Pretty much straight forward. That's adding a line. Then, if I run this in parallel, I would need to merge two aggregators. That's what's happening here. Again, pretty much straight forward. Finally, if I'm done, I need to reduce my processed results, and I emit such a result through object with the min, the max value. Then, for the average, I just divide sum by count, and I print it out. On this machine, this ran in about five minutes. Not super-fast, but also not terribly bad. Writing this code, it took me half an hour or maybe less. It's decent. Maybe, if you were to solve this problem in your job, you might call it a day and just go home, have a coffee and be done with it. Of course, for the purpose of this challenge, we want to go quite a bit faster and see how much we can move the needle here.</p>

<h2>The First Submission</h2>

<p>With this challenge, the important thing is somebody has to come and participate. That was a bit my concern like, what happens if nobody does it? It would be a bit embarrassing. Roy Van Rijn, another Java champion from the Netherlands, he was instantly interested in this, and an hour later or so, after I had put out the post, he actually created his own first implementation, and it wasn't very fancy or very elaborate. His idea just was, I want to be part of it. I want to put out a submission so other people can see, this is something we also could do. This was really great to see, because as soon as the first person comes along and takes part, then also other people will come along and take part. Of course, he kept working on that. He was one of the most active people who iterated on his implementation, but he was the very first one to submit something.</p>

<h2>Parallelization</h2>

<p>Let's dive a little bit into the means of what you can do to make this fast. People spent the entire month of January working on this, and they went down to a really deep level, essentially counting CPU instructions. My idea is, I want to give you some ideas of what exists, what you can do. Then maybe later on, if you find yourself in that situation where you would like to optimize certain things, then you might remember, I've heard about it. Then you could go and learn really deep. That's the idea.</p>

<p>Let's talk about parallelization, first of all, because we have many CPU cores. On my server, which I use to evaluate it, I have 32 cores, 64 with hyperthreading. We would like to make use of that. Would be a bit wasteful to just use a single core. How can we go about this? Going back to my simple baseline implementation, the first thing I could do is I could just say, let me add this parallel call, so this part of the Java Streams API.</p>

<p>Now this will process this pipeline, or I should say, part of this streaming pipeline in parallel. Just doing this, just adding this single method call, gets us down to 71 seconds. From 4 minutes 50 seconds, to 71 seconds by just adding a few characters for one method call. I think that's a pretty good thing. Very easy win. Of course, that's not all we can do. In particular, if you think about it, yes, it gets us down by quite a bit, but it's not eight times faster than what we had initially, but we have 8 CPU cores which I'm using here. Why is it not eight times faster? This parallel operator, this applies to the processing logic. All this aggregating and grouping logic, this happens in parallel, but this reading of the file from memory, this still happens sequentially.</p>

<p>The entire file, the reading part, that's sequentially, and we have still all our other CPU cores sitting idle, so we would like to also parallelize that. This comes back then to this notion that I would like to go out and learn something new, because all those new Java versions, they come with new APIs, the JEPs, the Java Enhancement Proposals. One of them, which was added recently, is the foreign function and memory API. You can see it here, so that's taken straight from the JEP, but essentially, it's a Java API which allows you to make use of native methods.</p>

<p>It's a replacement, much easier to use than the old JNI API. It also allows you to make use of native memory. Instead of the heap, which is managed by the JVM, you get the ability to manage your own memory section, like an off-heap memory, and you will be in charge of maintaining that, and making sure you free it, and so on. That's what we would like to use here, because we could memory map this file and then process it there in parallel. Let's see how we can go about that.</p>

<p>I mentioned there's a bit of code, but I will run you through it. That's a bit of a recurring theme. The code you will see, it gets more dense as we progress. Again, you don't really have to understand everything. I would like to give you a high-level intuition. What do we do here? First of all, we determine the degree of parallelism. We just say, how many CPU cores do we have? Eight in our case, so that's our degree of parallelism. Next, we want to memory map this file. You could have memory map files also in earlier Java versions, but for instance, you had size limits. You couldn't memory map an entire 13-gig file all at once, whereas now here with the new foreign memory API, that's possible. We can do this. You map the file. We have this Arena object there. This is essentially our representation of this memory. There's different kinds of Arenas. In this case, I'm just using this global Arena which just exists and is accessible from everywhere within my application. That's where I have that file, and now I can access that entire section of memory in parallel using multiple threads.</p>

<p>In order to do so, we need to split up that file and the memory representation. That's what happens here. First of all, roughly speaking, we divide into eight equal chunks. We take our entire size divided by eight. That's our estimate of chunk sizes. Now, of course, what would happen is, in all likelihood, we will end up in the middle of a line. This is not really desirable, where, ideally, we would like to have our worker processes, they should work on entire lines. What's happening here is, we went to roughly one-eighth of the file, we just keep going to the next line ending character. Then we say, that's the end of this chunk, and the starting point of the next chunk. Then we process those chunks, essentially, just using threads. We will see later on how to do this. We start our threads, we join them. In the end, we've got to wait. Now this parallelizes the entire thing.</p>

<p>Now we really make use of all our 8 cores for the entire time, also while we do the I/O. There's one caveat. Just by the nature of things, one of those CPU cores will always be the slowest. At some point, all the other seven, they will just wait for the last one to finish, because it's a little bit unequally distributed. What people, in the end, did, instead of using 8 chunks, they split up this file in much smaller chunks. Essentially, they had a backlog of those chunks. Whenever one of those worker threads was done with the current chunk, it would go and pick up the next one.</p>

<p>By that, you make sure that all your 8 threads are utilized equally all the time. The ideal chunk size, as it turned out, was 2 megabytes. Why 2 megabytes? This specific CPU, which is in this machine which I used, it has a second level cache size of 16 megabytes, 8 threads processing 2 megabytes at a time. It's just the best in terms of predictive I/O and so on. This is what people found out. This already goes to show, we really get down to the level of a specific CPU and the specific architecture to really optimize for that problem by doing those kinds of things. That's parallel I/O.</p>

<h2>1BRC - Mythbusters, and Trivial Task?</h2>

<p>This challenge, it was going, people were participating. They had a good time. Of course, whenever something happens, there's also conspiracy theories. That's what I fear. People said, is this actually an engineering problem? At Decodable, you had this problem and you didn't know how to do it, so you farmed it out to the community. I can tell you, this would have been the most silly thing I could have done, because I created so much work by running this challenge for myself. I didn't do much besides it during the entire month of January. It was not that. It was just a genuine thing, which I felt would be interesting to me and the community. Was it an add for GraalVM? Because many people actually used GraalVM, and we will see later on more about it. Also, no. It was just like GraalVM lends itself really well towards that problem. Finally, is it an add for this AMD EPYC processor? Also, no.</p>

<p>Really, no conspiracies going on here. Who is on Hacker News? I read Hacker News way too often. Of course, you always have the Hacker News guy who says, that's a trivial problem. Everybody who knows how to program just a little bit, they will have solved this in two hours, and it's like a boring thing. Then, on the other hand, you have all the people from the Java community, and also, big names like Thomas Würthinger, who is the engineering lead at GraalVM, or Cliff Click, who was one of the original people behind the JVM, or Aleksey Shipilev, and all those people, they spend the entire month of January doing this. Of course, the Hacker News dude, he does it in two hours. Always interesting to see.</p>

<h2>Parsing</h2>

<p>Let's dive a little more into parsing that. We have seen how to make use of our CPU cores, but what actually happens there to process a line? Let's take a closer look at that. If we want to get away from what we had initially with just splitting up the file using regex and so on, that's not very efficient. Let's see what we can do here. That's, again, something I would be able to come up with just processing those input lines, character by character. What's happening here is, we have a little bit of a state machine. We read our characters. We keep reading the line until it has no more characters. Then we use the semicolon character which separates our station name from the temperature value to switch these states. Depending on which state we are, so do we either read the bytes which make up the station name, or do we read up the bytes which make up the measurement value? We need to add them into some builder or buffer which aggregates those particular values.</p>

<p>Then, if we are at the end of a line, so we have found the line ending character, then we need to consume those two buffers for the station and for the measurement, which we have built up. For the measurement, we will need to see how we convert that into an integer value, because that's also what people figured out. The problem was described as a double or a floating-point arithmetic, so with values like 21.7 degrees, but then again, randomly, I always only had a single fractional digit. People realized, this data actually, it always only has a single fractional digit. Let's take advantage of that and just consider that as an integer problem by just multiplying the number by 100, for the means of calculation. Then at the end, of course, divide it by 100, or by 10. That's something which people did a lot, and which I underestimated how much they would take advantage of the particular characteristics of that dataset.</p>

<p>For that conversion, we can see it here, and it makes sense, so we process or we consume those values. If we see the minus character, we negate the value. If we see the first one of our two digits, we multiply it by 100 or by 10. That's how we get our value there. Doing that, it gets us down to 20 seconds. This is already an order of magnitude faster than my initial baseline implementation. So far, nothing really magic has happened. One takeaway also for you should be, how much does it make sense to keep working on such a thing? Again, if this is a problem you're faced with in your everyday job, maybe stop here. It's well readable, well maintainable. It's an order of magnitude faster than the native baseline implementation, so that's pretty good.</p>

<p>Of course, for the purposes of this challenge, we probably need to go a bit further. What else can we do? We can, again, come back to the notion of parallelism and try to process multiple values at once, and now we have different means of parallelization. We already saw how to make the most out of all our CPU cores. That's one degree of parallelism. We could think about scaling out to multiple compute nodes, which is what we typically would do with our datastores. For that problem, it's not that relevant, we would have to split up that file and distribute it in a network. Maybe not that desirable, but that would be the other end of the spectrum. Whereas we also can go into the other direction and parallelize within specific CPU instructions. This is what happens here with SIMD, Single Instruction, Multiple Data.</p>

<p>Essentially all these CPUs, they have extensions which allow you to apply the same kind of operation onto multiple values at once. For instance, here, we would like to find the line ending character. Now, instead of comparing byte by byte, we can use such a SIMD instruction to apply this to 8 or maybe 16, or maybe even more bytes at once, and it will, of course, speed up things quite a bit. The problem is, in Java, you didn't really have a good means to make use of those SIMD instructions because it's a portable, abstract language, it just wouldn't allow you to get down to this level of CPU specificity. There's good news.</p>

<p>There's this vector API, which is still incubating, I think, in the eighth incubating version or so, but this API allows you now to make use of those vectorized instructions at extensions. You would have calls like this compare call with this equal operator, and then this will be translated to the right SIMD instruction of the underlying architecture. This would translate to the Intel or AMD64 extensions. Also, for Arm, it would do that. Or it would fall back to a scalar execution if your specific machine doesn't have any vector extensions. That's parallelization on the instruction level. I did another talk about it, https://speakerdeck.com/gunnarmorling/to-the-moon-and-beyond-with-java-17-apis, which shows you how to use SIMD for solving FizzBuzz.</p>

<p>Sticking to that pattern, applying the same operation to multiple values at once, we also can do what's called SWAR, SIMD Within A Register. Again, I realize, the code gets more dense. I probably wouldn't even be able right now to explain each and every single line, or it would take me a while. The idea here is, this idea of doing the same thing, like equals to multiple values all at once, we also can do this within a single variable. Because if you have 8 bytes, we also could see one long, that's 64 bits, that's also 8 bytes. We can apply the right level of bit level magic to a long value, and then actually apply this operation to all the 8 bytes. It's like bit level masking and shifting, and so on. That's what's happening here. There's a very nice blog post by Richard Startin, which shows you, step by step, how to do this, or how to use this to find the first zero byte in a string.</p>

<p>I have put the math up here on the right-hand side, so you actually can go and follow along, and you will see, this actually gives you the first zero byte in a long like that. That's SIMD Within A Register, SWAR. Now the interesting thing is, if you look at this code, something is missing here. Is somebody realizing what we don't have here? There's no ifs, there's no conditionals, no branching in that code. This is actually very relevant, because we need to remember how our CPUs actually work. If you look at how a CPU would take and go and execute our code, it always has this pipelined approach. Each instruction has this phase of, it's going to be fetched from memory, it's decoded, it's executed, and finally the result is written back. Now actually multiple of those things happen in parallel. While we decode our one instruction, the CPU will already go and fetch the next one. It's a pipelined parallelized approach.</p>

<p>Of course, in order for this to work, the CPU actually needs to know what is this next instruction, because otherwise we wouldn't know what to fetch. In order for it to know, we can't really have any ifs, because then we wouldn't know, which way will we go? Will we go left or right? If you have a way for expressing this problem in this branchless way, as we have seen it before, then this is very good, very beneficial for what's called the branch predictor in the CPU, so it always knows which are the next instructions. We never have this situation that we actually need to flush this pipeline because we took a wrong path in this predictive execution. Very relevant for that. I didn't really know much about those things, but people challenged it. One of the resources they employed a lot is this book, "Hacker's Delight". I recommend everybody to get this if this is interesting to you. Like this problem, like finding the first zero byte in a string, you can see it here. All those algorithms, routines are described in this book. If this is the thing which gets you excited, definitely check out, and get this book.</p>

<h2>Then, Disaster Struck</h2>

<p>Again, people were working on the challenge. It was going really well. They would send me pull requests every day, and I didn't expect that many people to participate. That's why I always went to the server and executed them manually. At some point, someday I woke up, I saw, that's the backlog of implementations which people had sent over the night, so let's say 10 PRs to review and to evaluate, and suddenly all those results were off. It was like twice as fast as before. I ran one of the implementations which I had run on the day before, and suddenly it was much faster. I was wondering, what's going on? What happened is, this workload, I had it initially on a virtual server. I thought, I'm smart. I try to be smart, so I get dedicated virtual CPU cores, so I won't have any noisy neighbors on that machine, this kind of thing.</p>

<p>What I didn't expect is that they would just go and move this workload to another machine. I don't know why. Maybe it was random. Maybe they saw there was lots of load in that machine. In any case, it just got moved to another host, which was faster than before. This, of course, was a big problem for me, because all the measurements which I had done so far, they were off and not comparable anymore. That was a problem. I was a bit desperate at that point in time. This is where the wonders of the community really were very apparent. Good things happened. I realized, I need to get a dedicated server so that this cannot happen again. I need to have a box, which I can use exclusively. As I was initially paying out of my own pocket for that, I thought, I don't want to go there. I don't want to spend €100. As I mentioned, Decodable, my employer, they stepped up to sponsor it.</p>

<p>Then, of course, I also needed help with maintaining that, because I'm not the big operations guy. I know a little bit about it. Then for that thing, you would, for instance, like to turn off the hyperthreading, or you would like to turn off turbo boost to have stable results. I wasn't really well-versed in doing those things, but the community came to help. In particular, René came to help. He offered his help to set up the thing. We had a call. I spoke to him. I had not known him. It was the first time I ever spoke to him, but we had a great phone conversation. In the end, he just sent me his SSH key. I uploaded his key to the machine, gave him the key to the kingdom, and then he was going and configuring everything the right way. There were multiple people, many people like that, who came and helped, because otherwise I just could not have done it.</p>

<h2>The 1BRC Community</h2>

<p>All this was a bit of a spontaneous thing. Of course, I put out some rules and how this should work. Then, I wasn't very prescriptive. Like, what is the value range? How long could station names be? What sort of UTF character planes and whatnot? I didn't really specify it. Of course, people asked, how long can a station name be? What kinds of characters can it contain, and so on? We had to nail down the rules and the boundaries of the challenge. Then people actually built a TCK, a test kit. It was actually a test suite which you then had to pass. Not only you want to be fast, you also want to be correct. People built this test suite, and it grew, actually, over time. Then whenever a new submission, a new entry came in, it, first of all, had to pass those tests. Then if it was valid, then I would go and evaluate it and take the runtime. This is how this looked like. You can see it here.</p>

<p>It had example files with measurements, and an expected file, what should be the result for that file? Then the test runner would go process the implementation against that set of files, and ensure that result is right. That's the test kit. The other thing, which also was very important as well, I had to run all those things on that machine. There's quite a few things which were related to that, like just making sure the machine is configured correctly. Then, I had five runs, and I want to discard fastest and slowest, all those things. Jason, here, he came to help and scripted all that. It was actually very interesting to see how he did it. I would really recommend to go to the repo, and just check out the shell scripts which exist, which are used for running those evaluations. It's a bit like a master class in terms of writing shell scripts, with very good error handling, colored output, all this good stuff to make it really easy and also safe to run those things. If you have to do shell scripting, definitely check out those scripts.</p>

<h2>Bookkeeping</h2>

<p>Then, let's talk about one more thing, which is also very relevant, and this is what I would call bookkeeping. If you remember the initial code I showed, I had this Java Streams implementation, and I used this collector for grouping the values into different buckets, per weather station name. People realized, that's another thing which we can optimize a lot ourselves. By intuition, you would use a HashMap for that. You would use the weather station name as the key in that HashMap. Java HashMap is a generic structure. It works well for a range of use cases. Then, if we want to get the most performance for one particular use case, then we may be better off implementing a bespoke, specific data structure ourselves. This is what we can see here. I think it might sound maybe scary, but actually it is not scary. It's relatively simple. What happens here? We say, we would like to keep track of the measurements per our station name. It's like a map, but it is backed by an array, so those buckets.</p>

<p>The idea now is, we take the hash key of our station name and we use this as the index within that array, and at that particular slot in the array, we will manage the aggregator object for a particular station name. We take the hash code, and we want to make sure we don't have an overflow. That's why we take it with logical end with the size of the array. We always have it well indexed in the array. Then we need to check, at that particular position in the array, is something there already? If nothing is there, that means, we have the first instance of a particular station in our hands, so the first value for Hamburg or the first value for Munich. We just go create this aggregator object there and store it at that particular offset in the array. That makes sense. The other situation, of course, is we go to the particular index in the array, and in all likelihood, something will be there already. If you have another value for the same station, something will be there already.</p>

<p>The problem is we don't know yet, is this actually the aggregator object for that particular key we have in our hands, or is it something else? Because multiple station names could have the same key. Which means, in that case, if something exists already at this particular array slot, you need to fall back and compare the actual name. Only if the incoming name is also the name of the aggregate object in that slot, then we can go and add the value to that. That's why it's called linear probing. Otherwise, we will just keep iterating in that array until we either have found a free slot, so then we can go install it there, or we have found the slot for the key which we have in our hands. I think it's relatively simple. Now for this particular case, this performs much better, actually, than what we could get with just using Java HashMap.</p>

<p>Of course, it depends a lot on the particular hash function here which we use to find that index. This is where it goes back to people really optimized a lot for the particular dataset, so they used hash functions which would be collision free for the particular dataset. This was a bit against what I had in mind, because the problem was this file, as I mentioned, it has a size of 13 gigabytes, and I just didn't have a good way for distributing 13 gigabytes to people out there. That's why, instead, they would have to generate it themselves. I had the data generator, and everybody could use this generator to create the file for themselves and then use it for their own testing purposes. The problem was, in this data generator, I had a specific key set. I had around 400 different station names with the idea being, that's just an example, but people took it very literally, and they optimized then a lot for those 400 station names. They used hash functions, which would not have any collisions, for those 400 names. Again, people will take advantage of everything they can.</p>

<p>The problem with all that is it also creates lots of work for me, because you cannot really prove the absence of hash collisions. Actually, whenever somebody sent in their implementation, I had to go and check out, do they actually handle this case, the two stations which would create the same key, and do they handle those collisions accordingly? Because otherwise, if you don't do this fall back to the slow case, you would be very fast, but you would be incorrect because you don't deal correctly with all possible names. This was a bit of a trap, which I set up for myself, and it meant I always had to check for that and actually ask people in the pull request template, if you have a custom map implementation, where do you deal with collisions? Then we would have conversations like we see here. How do you deal with hash collisions? I don't, that's why it's so fast. Then he would go and rework it. A big foot trap for myself.</p>

<h2>GraalVM: JIT and AOT Compiler</h2>

<p>Those are three big things, parallelization, then all this parsing with SIMD and SWAR, and custom hashmapping for bookkeeping. Those were recurring themes I saw again. Then there were more specific tricks, and I just wanted to mention a few of them. I just want to give you some inspiration of what exists. One thing which exists is the Epsilon garbage collector, which is a very interesting garbage collector because it doesn't collect any garbage. It's a no-op implementation. If you have your regular Java application, that would be not a good idea. Because you keep allocating objects, and if you don't do any GC, you will run out of heap space at some point. Here, people realized, we can actually implement this in a way that we don't do any allocations on our processing loop. We'll do a few allocations initially when bootstrapping the program, but then later on, no more objects get created. We just have arrays which we can reuse, like mutable structures, which we can just update.</p>

<p>Then we don't need any garbage collection, and we don't need any CPU cycles to be spent on garbage collection, which means we just can be a bit faster. Again, I think that's an interesting thing. Maybe, for instance, if you work on some CLI tool, short-lived thing, could be an interesting option to just disable the garbage collector and see how that goes. The other thing, which you can see here is people used a lot GraalVM. GraalVM, it's two things, really. It's an ahead-of-time compiler, so it will take your Java program and emit a native binary out of it. This has two essential advantages. First of all, it uses less memory. Secondly, it's very fast to start because it doesn't have to do class loading and the compilation and everything, this all happens at build time. This is fast to start if you have this native binary. Now to the level of results we got here, this actually mattered.</p>

<p>Initially, I thought saving a few hundred milliseconds on startup won't make a difference for processing 13 gigabytes of file, but actually it does make a difference. The AOT compiler and most of the fastest implementations, they actually used the AOT compiler with GraalVM. There's also the possibility to use this as a replacement for the just-in-time compiler in your JVM. You just can use it as a replacement for the C2 compiler. I'm not saying you should always do this. It depends a little bit on your particular workload and what you do, whether it's advantageous or not. In this case, this problem, it lends itself very well to that. People just by using GraalVM as the JIT compiler in the JVM, this gave them a nice improvement of like 5% or so. It's something I can recommend for you to try out, because it's essentially free. You just need to make sure you use a JVM or a Java distribution which has GraalVM available as the C2 compiler replacement. Then it's just means of saying, that's the JIT compiler I want to use, and off you go. Either it does something for you or does not.</p>

<h2>Further Tricks and Techniques</h2>

<p>A few other things, like unsafe, what I found interesting is the construct here on the right-hand side, because if you look at that, this is our inner processing loop. We have a scanner object. We try to take next values. We try to process them, and so on. What we have here is we have the same loop three times in a program which is written up in a sequential way. If you look at it, you would say, those three loops, they run one after another. What actually happens is, as the CPUs have multiple execution units, the compiler will figure out, this can actually be parallelized, because there is no data dependencies between those loops. This is what happens, we can take those loops and run them concurrently. I found it very interesting. Why is it three times? Empirically determined.</p>

<p>Thomas, who came up with this, he tried it two times. He tried to have the loop four times and three times it was just the fastest on that particular machine. It could be different in other machines. Of course, you see already here with all those things, this creates questions around maintainability. Because I already can see the junior guys joining the team, and they're like, "That's duplication. It's like the same code three times. Let me go and refactor it. Let me clean it up", and your optimization would be out of the window. You would want to put a comment there, don't go and refactor it into one loop. That's the consideration. Are those things worth it? Should you do it for your particular context? That's what you need to ask. I found this super interesting, that this is a thing.</p>

<h2>The Results</h2>

<p>You are really curious then, how fast were we in the end? This is the leaderboard with those 8 CPU cores I initially had. I had 8 CPU cores because that was what I had with this virtual server initially. When I moved to the dedicated box, I tried to be in the same ballpark. With those 8 cores, we went down to 1.5 seconds. I would not have expected that you could go that fast with Java, processing 13 gigabytes of input in 1.5 seconds. I found that pretty impressive. It gets better because I had this beefy server with 32 cores and 64 threads with hyperthreading. Of course, I would like to see, how fast can we go there? Then we go down to 300 milliseconds. To me, it's like doubly mind blowing. Super impressive. Also, people did this, as I mentioned, in other languages, other platforms, and Java really is very much at the top, so you wouldn't be substantially better with other platforms.</p>

<p>The other thing, there was another evaluation, which there was, because I mentioned I had this data generator with those 400-something station names, and people optimized a lot for that by choosing specific hashing functions and so on. Some people realized that actually, this was not my intention. I wanted to see, how fast can this be in general? Some people agreed with that view of the world. For those, we had another leaderboard where we actually had 10k different station names. As you can see here now, it's actually a bit slower, because you really cannot optimize that much for that dataset. Also, it's different people at the top here. If I go back, here we have Thomas Würthinger, and people who teamed up with him for the regular key set, and then for the 10k key set, it's other people. It's different tradeoffs, and you see how much this gets specific for that particular dataset.</p>

<h2>It's a Long Journey</h2>

<p>People worked on that for a long time. Again, the challenge went through the entire month of January. I didn't do much besides running it really. People like Thomas, who was the fastest in the end, he sent 10 PRs. There were other people who sent even more. The nice thing was, it was a community effort. People teamed up. As I mentioned before, like the fastest one, it was actually an implementation by three people who joined forces and they took inspiration. When people came up with particular tricks, then very quickly, the others would also go and adopt them into their own implementation. It was a long journey with many steps, and I would very much recommend to check this out.</p>

<p>This is, again, the implementation from Roy Van Rijn, who was the first one, because he kept this very nice log of all the things he did. You see how he progressed over time. If you go down at the very bottom, you will see, he started to struggle a bit because he did changes, and actually they were slower than what he had before. The problem was he was running on his Arm MacBook, which obviously has a different CPU with different characteristics than the machine I was running this on. He saw improvements locally, but it was actually faster on the evaluation machine. You can see it at the bottom, he went and tried to get an Intel MacBook, to have better odds to do something locally, which then also performs better on that machine. I found it really surprising to see this happening with Java, that we get down to this level where the particular CPU and even its generation would make a difference here.</p>

<h2>Should You Do Any of This?</h2>

<p>Should you do any of this? I touched on this already. It depends. If you work on an enterprise application, I know you deal with database I/O most of the times. Going to that level and trying to avoid CPU cycles in your business code probably isn't the best use of your time. Whereas if you were to work on such a challenge, then it might be an interesting thing. What I would recommend is, for instance, check out this implementation, because this is one order of magnitude faster than my baseline. This would run 20 seconds or so. It's still very well readable, and that's what I observed, like improving by one order of magnitude. We have still very well readable code. It's maintainable.</p>

<p>You don't have any pitfalls in this. It just makes sense. You are very much faster than before. Going down to the next order of magnitude, so going down to 1.5 seconds, this is where you do all the crazy mid-level magic, and you should be very conscious whether you want to do it or not. Maybe not in your regular enterprise application. If you participate in a challenge you want to win a coffee mug, then it might be a good idea. Or if you want to be hired into the GraalVM team, I just learned this the other day, actually, some person who goes by the name of Mary Kitty in the competition, he actually got hired into the GraalVM compiler team at Oracle.</p>

<h2>Wrap-Up, and Lessons Learned</h2>

<p>This was impacting the Java community, but then also people in other ecosystems, databases, in Snowflake they had a One Trillion Row Challenge. This really blew up and kept people busy for quite a while. There was this show and tell in the GitHub repo. You can go there and take a look at all those implementations in Rust, and OCaml, and all the good things I've never heard about, to see what they did in a very friendly, competitive way. Some stats, you can go to my blog post there, you will see how many PRs, and 1900 workflow runs, so quite a bit of work, 187 lines of comment in Aleksey's implementation. Really interesting to see. In terms of lessons learned there, if I ever want to do this again, I would have to be really prescriptive in terms of rules, automate more, and work with the community as it happened already today. Is Java slow? I think we have debunked that. I wouldn't really say so. You can go very fast. Will I do it again next year? We will see. So far, I don't really have a good problem which would lend itself to doing that.</p>




<p><big><strong>See more <a href="https://www.infoq.com/transcripts/presentations/">presentations with transcripts</a></strong></big></p>



                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google CEO says more than a quarter of the company's new code is created by AI (106 pts)]]></title>
            <link>https://www.businessinsider.com/google-earnings-q3-2024-new-code-created-by-ai-2024-10</link>
            <guid>41991291</guid>
            <pubDate>Wed, 30 Oct 2024 02:09:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/google-earnings-q3-2024-new-code-created-by-ai-2024-10">https://www.businessinsider.com/google-earnings-q3-2024-new-code-created-by-ai-2024-10</a>, See on <a href="https://news.ycombinator.com/item?id=41991291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude" id="piano-inline-content-wrapper" data-piano-inline-content-wrapper="" data-track-content="" data-post-type="post">
                                  <ul><li>More than a quarter of new code at Google is made by AI and then checked by employees.</li><li>Google is doubling down on AI internally to make its business more efficient.</li><li>Business Insider previously reported that Google launched an internal AI model named "Goose."</li></ul><!-- Excluded mobile ad on desktop --><div id="formContainer" data-component-type="inline-newsletter-module" data-event-label="insider_today" data-newsletter-id="1" data-newsletter-title="Insider Today" data-acq-source="techinlinesignup" data-track-view="{&quot;product_field&quot;:&quot;bi_value_unassigned&quot;,&quot;element_name&quot;:&quot;inline_newsletter&quot;}">
                        
                        
                          <section>
                              
                        
                            
                            
                        
                            
                        
                            <div>
                              <p>Thanks for signing up!</p>
                              
                              <p>
                              Access your favorite topics in a personalized feed while you're on the go.
                                    </p>
                            </div>
                        
                            
                            <form id="emailCapture" action="javascript:void(0);" method="POST" novalidate="">
                              
                              
                              <p>
                                  By clicking “Sign Up”, you accept our <a href="https://www.insider-inc.com/terms" target="_blank">Terms of Service</a> and <a href="https://www.insider-inc.com/privacy-policy" target="_blank">Privacy Policy</a>. You can opt-out at any time by visiting our Preferences page or by clicking "unsubscribe" at the bottom of the email.
                              </p>
                            </form>
                          </section>
                        
                            <div>
                                <p><img src="https://www.businessinsider.com/public/assets/rebrand/newsletter-bull.png" data-old-src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1 1'%3E%3C/svg%3E" data-src="/public/assets/rebrand/newsletter-bull.png">
                              
                              
                              
                              </p>    </div>
                        
                          
                        </div><p><a target="_blank" href="https://www.businessinsider.com/google-third-quarter-earnings-report-q3-2024-2024-10" data-analytics-product-module="body_link" rel="">Google</a> is all in on AI — both inside and outside the company.</p><p>More than a quarter of new code created at Google is generated by AI, CEO Sundar Pichai said on Tuesday during the company's Q3 earnings call.</p><p>Pichai said using AI for coding was "boosting productivity and efficiency" within Google. After the code is generated, it's then checked and reviewed by employees, he added.</p><p>"This helps our engineers do more and move faster," Pichai said. "I'm energized by our progress and the opportunities ahead, and we continue to be laser-focused on building great products."</p><!-- Excluded mobile ad on desktop --><p><a target="_blank" href="https://www.businessinsider.com/google-goose-ai-model-language-ai-coding-2024-2" data-analytics-product-module="body_link" rel="">Business Insider reported in February</a> that the company had launched a new internal AI model named "Goose" to help employees code and build products.</p><p>Goose was trained on "25 years of engineering expertise at Google," according to internal documents seen by BI.</p><p>The new data from Pichai will surely have some employees wondering whether they're coding themselves out of a job, while other employees say AI has already <a target="_blank" href="https://www.businessinsider.com/how-google-technical-program-manager-uses-ai-gemini-2024-8" data-analytics-product-module="body_link" rel="">transformed their work</a>. Company leaders have previously promised that AI isn't taking Googlers' jobs (yet), but the over-25% figure is striking and underscores the benefits of improving this technology.</p><p><strong><em>Are you a current or former Google employee? Got something to share? You can reach the reporter Hugh Langley via the encrypted messaging app Signal (+1 628-228-1836) or email (</em></strong><a target="_blank" href="mailto:hlangley@businessinsider.com" data-analytics-product-module="body_link" rel=" nofollow"><strong><em><u>hlangley@businessinsider.com</u></em></strong></a><strong><em>).</em></strong></p>
                      </div></div>]]></description>
        </item>
    </channel>
</rss>