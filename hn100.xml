<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 23 Mar 2025 08:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Next.js version 15.2.3 has been released to address a security vulnerability (139 pts)]]></title>
            <link>https://nextjs.org/blog/cve-2025-29927</link>
            <guid>43448723</guid>
            <pubDate>Sat, 22 Mar 2025 21:19:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nextjs.org/blog/cve-2025-29927">https://nextjs.org/blog/cve-2025-29927</a>, See on <a href="https://news.ycombinator.com/item?id=43448723">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Next.js version 15.2.3 has been released to address a security vulnerability (<a href="https://github.com/advisories/GHSA-f82v-jwr5-mffw" rel="noopener noreferrer nofollow" target="_blank">CVE-2025-29927<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a>). Additionally, backported patches are available.</p>
<p>We recommend that all self-hosted Next.js deployments using <code>next start</code> and <code>output: 'standalone'</code> should <a href="https://nextjs.org/docs/app/building-your-application/upgrading">update</a> immediately.</p>
<p>Continue reading for more details on the CVE.</p>
<h2 id="timeline" data-docs-heading=""><a href="#timeline">Timeline<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<ul>
<li><code>2025-02-27T06:03Z</code>: Disclosure to Next.js team via GitHub private vulnerability reporting</li>
<li><code>2025-03-14T17:13Z</code>: Next.js team started triaging the report</li>
<li><code>2025-03-14T19:08Z</code>: Patch pushed for Next.js 15.x</li>
<li><code>2025-03-14T19:26Z</code>: Patch pushed for Next.js 14.x</li>
<li><code>2025-03-17T22:44Z</code>: Next.js 14.2.25 released</li>
<li><code>2025-03-18T00:23Z</code>: Next.js 15.2.3 released</li>
<li><code>2025-03-18T18:03Z</code>: CVE-2025-29927 issued by GitHub</li>
<li><code>2025-03-21T10:17Z</code>: Security Advisory published</li>
<li><code>2025-03-22T21:21Z</code>: Next.js 13.5.9 released</li>
</ul>
<p>We are also publishing a backport for v12. We will update this post as they are released.</p>
<h2 id="vulnerability-details" data-docs-heading=""><a href="#vulnerability-details">Vulnerability details<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<p>Next.js uses an internal header <code>x-middleware-subrequest</code> to prevent recursive requests from triggering infinite loops. The security report showed it was possible to skip running Middleware, which could allow requests to skip critical checks—such as authorization cookie validation—before reaching routes.</p>
<h2 id="impact-scope" data-docs-heading=""><a href="#impact-scope">Impact scope<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<h3 id="affected" data-docs-heading=""><a href="#affected">Affected<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h3>
<ul>
<li>Self-hosted Next.js applications using Middleware (<code>next start</code> with output: <code>standalone</code>)</li>
<li>This affects you if you rely on Middleware for auth or security checks, which are not then validated later in your application.</li>
<li>Applications using Cloudflare can turn on a <a href="https://developers.cloudflare.com/changelog/2025-03-22-next-js-vulnerability-waf/" rel="noopener noreferrer nofollow" target="_blank">Managed WAF rule<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a></li>
</ul>
<h3 id="not-affected" data-docs-heading=""><a href="#not-affected">Not affected<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h3>
<ul>
<li>Applications hosted on Vercel</li>
<li>Applications hosted on Netlify</li>
<li>Applications deployed as static exports (Middleware not executed)</li>
</ul>
<h2 id="patched-versions" data-docs-heading=""><a href="#patched-versions">Patched versions<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<ul>
<li>For Next.js 15.x, this issue is fixed in <code>15.2.3</code></li>
<li>For Next.js 14.x, this issue is fixed in <code>14.2.25</code></li>
<li>For Next.js 13.x, this issue is fixed in <code>13.5.9</code></li>
</ul>
<p>If patching to a safe version is infeasible, it is recommended that you prevent external user requests which contain the <code>x-middleware-subrequest</code> header from reaching your Next.js application.</p>
<p>We are also publishing a backport for v12. We will update this post as they are released.</p>
<h2 id="our-security-responsibility" data-docs-heading=""><a href="#our-security-responsibility">Our security responsibility<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<p>Next.js has published <a href="https://github.com/vercel/next.js/security/advisories?state=published" rel="noopener noreferrer" target="_blank">16 security advisories<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a> since 2016. Over time, we've continued to improve how we gather, patch, and disclose vulnerabilities.</p>
<p>GitHub Security Advisories and CVEs are industry-standard approaches to notifying users, vendors, and companies of vulnerabilities in software. While we have published a CVE, <strong>we missed the mark</strong> on partner communications.</p>
<p>To help us more proactively work with partners depending on Next.js, and other infrastructure providers, we are opening a partner mailing list. Please reach out to <a href="mailto:partners@nextjs.org"><code>partners@nextjs.org</code></a> to be included.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CEO of Kubient sentenced for fraud (158 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/</link>
            <guid>43448606</guid>
            <pubDate>Sat, 22 Mar 2025 21:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/">https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/</a>, See on <a href="https://news.ycombinator.com/item?id=43448606">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      
                      
          <p>In <a href="https://web.archive.org/web/20240511122402/https://kubient.com/">May 2024</a>, the website of ad-tech firm Kubient touted that the company was "a perfect blend" of ad veterans and developers, "committed to solving the growing problem of fraud" in digital ads. Like many corporate sites, it also linked old blog posts from its home page, including <a href="https://web.archive.org/web/20240315021936/https://kubient.com/ad-fraud/how-to-create-a-world-free-of-fraud-kubients-secret-sauce/">a May 2022 post</a> on "How to create a world free of fraud: Kubient's secret sauce."</p>
<p>These days, Kubient's website cannot be reached, the team is no more, and CEO Paul Roberts is <a href="https://www.justice.gov/usao-sdny/pr/former-ceo-kubient-inc-sentenced-prison-connection-accounting-fraud-scheme">due to serve one year and one day in prison</a>, having pled guilty Thursday to creating his own small world of fraud. Roberts, according to federal prosecutors, schemed to create $1.3 million in fraudulent revenue statements to bolster Kubient's initial public offering (IPO) and significantly oversold "KAI," Kubient's artificial intelligence tool.</p>
<p>The core of the case is an I-pay-you, you-pay-me gambit that Roberts initiated with an unnamed "Company-1," according to prosecutors. Kubient and this firm would each bill the other for nearly identical amounts, with Kubient purportedly deploying KAI to find instances of ad fraud in the other company's ad spend.</p>
<p>Roberts, prosecutors said, "directed Kubient employees to generate fake KAI reports based on made-up metrics and no underlying data at all." These fake reports helped sell the story to independent auditors and book the synthetic revenue in financial statements, according to Roberts' indictment.</p>

          
                      
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Vibe Coding" vs. Reality (193 pts)]]></title>
            <link>https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html</link>
            <guid>43448432</guid>
            <pubDate>Sat, 22 Mar 2025 20:35:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html">https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html</a>, See on <a href="https://news.ycombinator.com/item?id=43448432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body"><article><time datetime="2025-03-19T15:00:00.000Z" title="3/19/2025, 10:00:00 AM">Published Mar 19, 2025</time> - 11 min read - <a href="https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.txt">Text Only</a><div id="table-of-contents-container"><p>Table of contents</p><ul><li><a href="#title" data-id="title">"Vibe Coding" vs Reality</a></li><li><a href="#working-around-the-problem" data-id="working-around-the-problem">Working around the problem</a></li><li><a href="#conclusion" data-id="conclusion">Conclusion</a></li></ul></div><p>There's a trend on social media where many repeat <a href="https://x.com/karpathy/status/1886192184808149383">Andrej Karpathy's words</a> (<a href="https://archive.is/yNSTA">archived</a>): "give in to the vibes, embrace exponentials, and forget that the code even exists." This belief — like many flawed takes humanity holds — comes from laziness, inexperience, and self-deluding imagination. It is called "Vibe Coding."</p><div><p><img width="128" height="128" alt="head-empty" src="https://cendyne.dev/s/128/cendyne/head-empty" sizes="128px"></p><div><p>"Embrace the exponentials" sounds like it came from an NFT junkie.</p></div></div><div><p><img width="128" height="128" alt="shinji-cup" src="https://cendyne.dev/s/128/cendyne/shinji-cup" sizes="128px"></p><div><p>Like the NFT crowd, there is a bubble of unreality they cling to justifying their perception of the world.</p></div></div><p>Producing software is now more accessible as newer tools allow people to describe what they want in a natural language to a large language model (LLM). This idea is catching on because LLM agents are now accessible to anyone willing to subscribe to vendors like <a href="https://www.cursor.com/en">Cursor</a>, <a href="https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode">GitHub</a>, Windsurf, and others. These editors have an "agent" option where users can request something and in response changes are made to the appropriate files, rather than only the file currently in focus. Over time, the agent will request to run commands to run tests or even run scripts it previously wrote to the file system, much as you would if you were solving the problem.</p><p>In 2022, folks could copy code into <a href="https://en.wikipedia.org/wiki/ChatGPT">ChatGPT</a> and ask questions or for rewrites.</p><p>In 2023, folks could ask it to review and edit a single file with an IDE integration like Copilot.</p><p>In 2024 and 2025, folks could ask it to solve a specific problem in the project and have it find out what files to edit, edit them, then verify its own work, and correct any mistakes it made with feedback from linting errors and unit tests.</p><p>With LLM agents having so much capability, people can delegate the idea of refining their imprecise ideas to a precise implementation elaborated by an LLM through "Vibe Coding."</p><div><p><a href="https://twitter.com/a16z">@a16z</a> <a href="https://twitter.com/stuffyokodraws">@stuffyokodraws</a> First - what is vibe coding?</p><p>A concise definition from <a href="https://twitter.com/stuffyokodraws">@stuffyokodraws</a>, and then an exploration of how technical vs. non-technical users approach these tools.</p></div><p>If you open a blank folder and tell it to set up an initial project, it can do a lot at once. With no rules, no patterns to mimic, and no constraints, it can produce something that feels more tailored for you in minutes than <code>npx create-react-app</code> ever could.</p><p>With a simple instruction like "I want to create a website for my ski resort" and about ten minutes of having it massage errors of its own making, I can have just that.</p><p><img data-blurhash="MSQc#U~WxtIW%LM|t6t7WCt7^*9Zt7%LR*" data-width="645" data-height="423" data-ratio="true" src="https://cendyne.dev/c/XH7rgh3H?width=645" alt="A generated website about a ski resort with a phrase like 'Easy to Reach, Hard to Leave'" width="645" height="423"></p><p>These leaps of progress are what fuels the "Vibe Coding" idea. To go from nothing to something shareable and personal sounds incredible.</p><div><p><img width="128" height="128" alt="beat-saber" src="https://cendyne.dev/s/128/cendyne/beat-saber" sizes="128px"></p><div><p>This moment provided a thrill I hadn't experienced in a long time when coding. However, this excitement drained quickly the further I got from a blank canvas.</p></div></div><p>Agents, as a concept, aren’t new. <a href="https://www.youtube.com/watch?v=ijeXop674Dg">Google IO made up buzzwords</a> like <a href="https://www.theverge.com/2024/12/11/24317436/google-deepmind-project-astra-mariner-ai-agent">"agentic era"</a> (<a href="https://archive.is/dw8XE">archived</a>) to describe this concept. It has been realized through open technologies like <a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a>, <a href="https://github.com/OpenBMB/XAgent">XAgent</a>, and <a href="https://www.anthropic.com/news/model-context-protocol">more recently by Anthropic</a> with the <a href="https://modelcontextprotocol.io/introduction">Model Context Protocol</a> (MCP).</p><p>When the model can interact with more than just a person who proxies their outputs into different domains, it is autonomous. If it can perform searches on the web or in a codebase, it can enrich its own context with the information it needs to fulfill the current request. Further, when it can commit outputs and then gain immediate and automatic feedback on those outputs, it can refine its solution without a person intervening.</p><p>There are actions that do prompt the user for consent before proceeding, such as running commands in the console or deleting files. This consent can be pre approved with a mode called "YOLO."</p><p><img data-blurhash="E04ec*~qofxuoft7D%Rj?b-;xut7" data-width="645" data-height="149" data-ratio="true" src="https://cendyne.dev/c/E8IJPCCd?width=645" alt="Cursor settings YOLO mode, allows running commands automatically" width="645" height="149"></p><div><p><img width="128" height="127" alt="we-live-in-a-society" src="https://cendyne.dev/s/128/cendyne/we-live-in-a-society" sizes="128px"></p><div><p>A mode for "You Only Live Once"!? Really?</p></div></div><p>You can witness this autonomy for yourself today in Cursor.</p><p>The agent concept has merit and today can deliver proofs of concept that <a href="https://youtu.be/IACHfKmZMr8">VC firms like Y-Combinator</a> will invest in — proofs of concept that are trash by unskilled founders hoping to win the lottery while living the life of leisure.</p><div><p>I’ve cracked vibe coding, TrendFeed has almost hit its first 10k month, and Ai built the entire thing</p><p>Im just sitting here sipping coffee, coding with Ai + MCP</p><p>Also more time to shitpost on X haha</p></div><div><p><img width="128" height="128" alt="cheers" src="https://cendyne.dev/s/128/cendyne/cheers" sizes="128px" loading="lazy"></p><div><p>The optimal technical founder for a VC is not the 10x engineer. It is someone who'll deliver <em>enough</em> of a product to test its fitness in the market and then succeed in raising more investment money. Their execution on their vision and hiring prowess is more important than their technical skillset.</p></div></div><p>The execution of agents today is over-hyped and does not hold up to the needs of any functioning businesses which need experts to develop and maintain their technical capabilities instead of single points of failure on the internet.</p><div><p>babe, come to bed</p><p>i can't, i'm vibe coding</p></div><p>These models are trained on average sloppy code, wrong answers on Stack Overflow, and the junk that ends up on Quora. Despite the power and capability Claude 3.7 Sonnet has in small contexts, when faced with even a small codebase it makes constant silly mistakes that no normal developer would repeat and continue to repeat every hour of its operation.</p><div><p><span></span><span>Specific details on the mistakes, feel free to skip</span><span></span></p><div><ul><li>Regularly clones TypeScript interfaces instead of exporting the original and importing it.</li><li>Reinvents components all the time with the same structure without searching the code base for an existing copy of that component.</li><li>Writes trusted server side logic on the client side, using RPC calls to update the database.</li><li>As a feature develops, it prioritizes maintaining previous mistakes instead of re-evaluating its design, even when told to do so. You have to say the previous implementation is outright unusable for it to replace its design.</li><li>Cursor has some sort of <a href="https://www.reddit.com/r/ClaudeAI/comments/1i8n3wq/does_claude_have_stupid_mode_enabled_tonight/">"concise mode"</a> (<a href="https://archive.is/iU8gx">archived</a>) that they'll turn on when there is high load where the model will still be rated at the normal price but behaves in a useless manner. This mode will omit details, drop important findings, and corrupt the output that is being produced.</li><li>Cannot be trusted to produce unit tests with decent coverage.</li><li>Will often break the project's code to fit a unit test rather than fix the unit test when told to do so.</li><li>When told to fix styles with precise details, it will alter the wrong component entirely.</li><li>When told specifically where there are many duplicated components and instructed to refactor, will only refactor the first instance of that component in the file instead of all instances in all files.</li><li>When told to refactor code, fails to search for the breaks it caused even when told to do so.</li><li>Will merrily produce files over 1000 lines which exceed its context window over time, even when told to refactor early on.</li><li>Will regularly erase entire route handlers if not bound to the file hierarchy.</li></ul></div></div><p>As currently designed, these models cannot learn new information. They cannot do better than the dataset they were created with. Instead their capability is realized by how effective they can process tokens entering their context window.</p><p>If you ask Claude 3.7 Sonnet to develop a runtime schema for validating some domain specific language and then ask it to refactor the file — because it is too large for its context window to continue — it will degrade and output incoherent nonsense before finishing its work.</p><p><img data-blurhash="E042PB?bM{kCWBof%M%MRjRjtRay" data-width="645" data-height="244" data-ratio="true" src="https://cendyne.dev/c/l6YxDTlA?width=645" alt="Now that we've created all the schemado that: ... I'v schema files for each schema schemaschema schemaactored code?" width="645" height="244" loading="lazy"></p><div><p><img width="128" height="110" alt="wat" src="https://cendyne.dev/s/128/cendyne/wat" sizes="128px" loading="lazy"></p><div><p>It did not type "I've" correctly and conjoined the words "schema" and "refactored" into one.</p></div></div><div><p>my saas was built with Cursor, zero hand written code</p><p>AI is no longer just an assistant, it’s also the builder</p><p>Now, you can continue to whine about it or start building.</p><p>P.S. Yes, people pay for it</p></div><p>You cannot ask these tools today to develop a performant React application. You cannot ask these tools to implement a secure user registration flow. It will choose to execute functions like is user registered on the client instead of the server.</p><div><p><img width="128" height="128" alt="trash" src="https://cendyne.dev/s/128/cendyne/trash" sizes="128px" loading="lazy"></p><div><p>Others are learning this the hard way too.</p></div></div><div><p>guys, i'm under attack</p><p>ever since I started to share how I built my SaaS using Cursor</p><p>random thing are happening, maxed out usage on api keys, people bypassing the subscription, creating random shit on db</p><p>as you know, I'm not technical so this is taking me longer that usual to figure out</p><p>for now, I will stop sharing what I do publicly on X</p><p>there are just some weird ppl out there</p></div><p>Without expert intervention, the best these tools can do today is produce a somewhat functional mockup, where every future change beyond that risks destroying existing functionality.</p><p>I cannot — and would not — trust a team member who vibe codes in a production application. The constant negligence I observe when "Vibe Coding" is atrocious and unacceptable to a customer base of any size.</p><p>No available model demonstrates consistent and necessary attention to detail needed for a production environment. They are not yet equipped or designed to transform information involving multiple contexts inherent to producing a digital product.</p><p>These tools are optimized to produce solutions that fit in a single screen of markdown and are now being asked to do far more than they were trained for. As the context window overflows and the model degrades, it will fail to even format MCP calls correctly and upon reaching this point of no return, produces a log that comes across as being tortured. Like a robot losing a limb, it will try and try again to walk only to fall down until the editor pauses the conversation to save on resources.</p><p><img data-blurhash="L03+Dt~q%2ofM{WURjWBx]t7WUj[" data-width="645" data-height="628" data-ratio="true" src="https://cendyne.dev/c/xNtV9Ji3?width=645" alt="Let me try a different approach. Error calling tool." width="645" height="628" loading="lazy"></p><h2 id="working-around-the-problem">Working around the problem</h2><p>A modern <a href="https://en.wikipedia.org/wiki/Twitch_Plays_Pok%C3%A9mon">"Twitch plays Pokémon"</a> is going on right now: <a href="https://www.twitch.tv/claudeplayspokemon">Claude Plays Pokémon</a>. It mitigates this context window problem by starting a new context with seeded information provided by its previous incarnation in the form of many Markdown files, which it can then read as if new and search via MCP during its playthrough.</p><div><div><p>So, what makes this possible? Claude was given a knowledge base to store notes, vision to see the screen, and function calls which allow it to simulate button presses and navigate the game.</p><p>Together, they allow Claude to sustain gameplay with tens of thousands of interactions.</p></div><p><img data-blurhash="L04epF.7M-a1.6t6WDofI2VtxstR" data-width="645" data-height="645" data-ratio="true" src="https://cendyne.dev/c/a9W5MgiW?width=645" alt="Photo included with tweet" width="645" height="645" loading="lazy"></p></div><p>Even so, it can make bad assumptions and spend 43 hours intentionally blacking out over and over in Mt. Moon (an in-game route between story locations) making no effective progress towards achieving its next goal because by the time it could second guess itself, its context window is no longer fit to continue.</p><p><video poster="https://cendyne.dev/c/-PEsNE-L" preload="metadata" playsinline="" controls="" autoplay="" loop="" muted="" width="644" height="362"><source src="https://cendyne.dev/c-no-index/3RrkLRXm" type="video/mp4"><img data-blurhash="L98zorE19a~C^+IoIo-p9t%2xaE1" data-width="644" data-height="362" data-ratio="true" src="https://cendyne.dev/c/-PEsNE-L?width=645" alt="Claude plays pokemon going through a context clean up and restart" width="644" height="362" loading="lazy"></video></p><div><p><img width="128" height="128" alt="galaxy-brain2" src="https://cendyne.dev/s/128/cendyne/galaxy-brain2" sizes="128px" loading="lazy"></p><div><p>It did escape and progress, but only after the critic instance of the model suggested its assumption was incorrect.</p></div></div><p>After a context cleanup completes, which takes about five minutes (the video above is edited to the meaningful moments), the model proceeds to make the same mistakes its prior incarnation did. The notes it wrote are not meaningfully interpreted in context, I find the same happens too with the Cursor rules I write.</p><p>While increasing the length of the context window will improve some immediate experiences, this is a problem of scale that needs a different solution for agents to be more effective and, perhaps, move "Vibe Coding" closer to reality.</p><div><p><img width="107" height="128" alt="thinker" src="https://cendyne.dev/s/128/cendyne/thinker" sizes="128px" loading="lazy"></p><div><p>Would a formalized <a href="https://bulletjournal.com/">bullet journal</a> over MCP help a model be more complete in delivering more reliable results?</p></div></div><div><div><p>As long as the model correctly checks it before concluding its work is complete!</p></div><p><img width="128" height="123" alt="point-left" src="https://cendyne.dev/s/128/jacobi/point-left" sizes="128px" loading="lazy"></p></div><p><img data-blurhash="K5SF;MIV~q?vt7%Mxuxuj[" data-width="645" data-height="669" data-ratio="true" src="https://cendyne.dev/c/FD-gkvrg?width=645" alt="Bullet journal with ski examples" width="645" height="669" loading="lazy"></p><p>A bullet journal may be one of many tools that improve the reliability of the models we have today.</p><p>The next issue is that these models cannot ingest information from multiple concurrent real-time sources. In one terminal we may be running the server and in another some end-to-end tests. Both of these terminals were created at the agent's request. It either ignores or is not fed the stack trace logged by the server in the first terminal as it watches the output of the end-to-end tests fail and retry, fail and retry.</p><p>For agents to have the impact promised by the hype, LLMs need a robust mechanism to mimic the development of short and long term memory without fine-tuning the memories into the model.</p><p>Furthermore, for agents to contribute to a team, there must be a way to develop long-term memories bound to the organization and its products that seamlessly merge with and reconcile with memories personal to each team member.</p><p>And lastly, these memories have to be portable. As models improve and are integrated into our tools, domain specific memories must be usable by the next generation of large language models.</p><h2 id="conclusion">Conclusion</h2><p>"Vibe Coding" might get you 80% the way to a functioning concept. But to produce something reliable, secure, and worth spending money on, you’ll need experienced humans to do the hard work not possible with today’s models.</p><p>Agents do demonstrate enough capability that LinkedIn CEO influencers confidently spread the unreality that we can replace jobs with "agentic AI."</p><p>Agents do enable skilled people to create more independently than they ever have. For the time being, it will not replace those that can solve the hard problems that only experience and intuition can identify. Like other no-code solutions, agents do give the less skilled more capability than they had the day before. Until they develop their own competent skill set, "Vibe Coders" will not be able to release production quality software in this world, no matter how <em>exponential</em> the agent is over their own inferior skill set.</p><p>Keep an eye on how LLM agents develop and improve. For now, they are worth evaluating and discussing, but are not ready for us to delegate the precise task of creating reliable, secure, and scalable software that powers our society. "Vibe Coding" will not create the next big thing in 2025.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mathematical Methods for Physics [pdf] (107 pts)]]></title>
            <link>https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf</link>
            <guid>43448193</guid>
            <pubDate>Sat, 22 Mar 2025 19:58:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf">https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=43448193">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Italy demands Google poison DNS under strict Piracy Shield law (155 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/</link>
            <guid>43448112</guid>
            <pubDate>Sat, 22 Mar 2025 19:46:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/">https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/</a>, See on <a href="https://news.ycombinator.com/item?id=43448112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>Spotted by <a href="https://torrentfreak.com/court-orders-google-to-poison-public-dns-to-prevent-iptv-piracy-250321/">TorrentFreak</a>, AGCOM Commissioner Massimiliano Capitanio <a href="https://www.linkedin.com/feed/update/urn:li:activity:7308503541390741504/">took to LinkedIn</a> to celebrate the ruling, as well as the existence of the Italian Piracy Shield. "The Judge confirmed the value of AGCOM's investigations, once again giving legitimacy to a system for the protection of copyright that is unique in the world," said Capitanio.</p>
<p>Capitanio went on to complain that Google has routinely ignored AGCOM's listing of pirate sites, which are supposed to be blocked in 30 minutes or less under the law. He noted the violation was so clear-cut that the order was issued without giving Google a chance to respond, known as <em>inaudita altera parte</em> in Italian courts.</p>
<p>This decision follows a similar case against Internet backbone firm Cloudflare. In January, the Court of Milan found that Cloudflare's CDN, DNS server, and WARP VPN were facilitating piracy. The court threatened Cloudflare with fines of up to 10,000 euros per day if it did not begin blocking the sites.</p>
<p>Google could face similar sanctions, but AGCOM has had difficulty getting international tech behemoths to acknowledge their legal obligations in the country. We've reached out to Google for comment and will update this report if we hear back.</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NixOS and reproducible builds could have detected the xz backdoor (253 pts)]]></title>
            <link>https://luj.fr/blog/how-nixos-could-have-detected-xz.html</link>
            <guid>43448075</guid>
            <pubDate>Sat, 22 Mar 2025 19:39:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://luj.fr/blog/how-nixos-could-have-detected-xz.html">https://luj.fr/blog/how-nixos-could-have-detected-xz.html</a>, See on <a href="https://news.ycombinator.com/item?id=43448075">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h2 id="introduction">Introduction</h2>
<p>In March 2024, a backdoor was discovered in <code>xz</code>, a (de)-compression software that is regularly used at the core of Linux distributions to unpack source tarballs of packaged software. The backdoor had been covertly inserted by a malicious maintainer under the pseudonym of <em>Jia Tan</em> over a period of three years. This event deeply stunned the open source community as the attack was both of <strong>massive impact</strong> (it allowed <em>remote code execution</em> on all affected machines that had <code>ssh</code> installed) and <strong>extremely difficult to detect</strong>. In fact, it was only thanks to the diligence (and maybe luck) of Andres Freund – a Postgres developer working at Microsoft – that the catastrophe was avoided: while investigating a seemingly unrelated 500ms performance regression in <code>ssh</code> that he was experiencing on several <em>Debian unstable</em> machines, he was able to trace it back to the <code>liblzma</code> library, identify the backdoor and document it.</p>
<p>While it was already established that the open source supply chain was often the target of malicious actors, what is stunning is the amount of energy invested by <em>Jia Tan</em> to gain the trust of the maintainer of the <code>xz</code> project, acquire push access to the repository and then among other perfectly legitimate contributions insert – piece by piece – the code for a very sophisticated and obfuscated backdoor. This should be a wake up call for the OSS community. We should consider the open source supply chain a high value target for powerful threat actors, and to collectively find countermeasures against such attacks.</p>
<p>In this article, I’ll discuss the inner workings of the <code>xz</code> backdoor and how I think we could have mechanically detected it thanks to build reproducibility.</p>
<h2 id="how-does-the-attack-work">How does the attack work?</h2>
<p>The main intent of the backdoor is to allow for <em>remote code execution</em> on the target by hijacking the <code>ssh</code> program. To do that, it replaces the behavior of some of <code>ssh</code>’s functions (most importantly the <code>RSA_public_decrypt</code> one) in order to allow an attacker to execute arbitrary commands on a victim’s machine when some specific RSA key is used to log in. Two main pieces are combined to put together to install and activate the backdoor:</p>
<ol type="1">
<li><p><strong>A script to de-obfuscate and install a malicious object file as part of the <code>xz</code> build process.</strong>
Interestingly the backdoor was not comprehensively contained in the source code for <code>xz</code>. Instead, the malicious components were only contained in tarballs built and signed by the malicious maintainer <em>Jia Tan</em> and published alongside releases <code>5.6.0</code> and <code>5.6.1</code> of <code>xz</code>. This time the additional release tarball contained slight and disguised modifications to extract a malicious object file from the <code>.xz</code> files used as data for some test contained in the repository.</p></li>
<li><p><strong>A procedure to hook the <code>RSA_public_decrypt</code> function.</strong> The backdoor uses the <em>ifunc</em> mechanism of <code>glibc</code> to modify the address of the <code>RSA_public_function</code> when <code>ssh</code> is loaded, in case <code>ssh</code> links against <code>liblzma</code> through <code>libsystemd</code>.</p></li>
</ol>



<h2 id="a-script-to-de-obfuscate-and-install-a-malicious-object-file-as-part-of-the-xz-build-process">1. A script to de-obfuscate and install a malicious object file as part of the <code>xz</code> build process</h2>
<p>As explained above, the malicious object file is stored directly in the <code>xz</code> git repository, hidden in some test files. The project being a decompression software, test cases include <code>.xz</code> files to be decompressed, making it possible to hide some machine code into fake test files;
<strong>The backdoor is not active in the code contained in the git repository, it is only included by building <code>xz</code> from the tarball released by the project</strong>, which has a few differences with the actual contents of the repository, most importantly in the <code>m4/build-to-host.m4</code> file.</p>
<pre tabindex="0"><code><span><span>diff --git a/m4/build-to-host.m4 b/m4/build-to-host.m4</span></span>
<span><span>index f928e9ab..d5ec3153 100644</span></span>
<span><span>--- a/m4/build-to-host.m4</span></span>
<span><span>+++ b/m4/build-to-host.m4</span></span>
<span><span>@@</span><span> -1,4 +1,4 </span><span>@@</span></span>
<span><span>-</span><span># build-to-host.m4 serial 3</span></span>
<span><span>+</span><span># build-to-host.m4 serial 30</span></span>
<span><span> dnl Copyright (C) 2023-2024 Free Software Foundation, Inc.</span></span>
<span><span> dnl This file is free software; the Free Software Foundation</span></span>
<span><span> dnl gives unlimited permission to copy and/or distribute it,</span></span>
<span><span>@@</span><span> -37,6 +37,7 </span><span>@@</span><span> AC_DEFUN([gl_BUILD_TO_HOST],</span></span>
<span></span>
<span><span>   dnl Define somedir_c.</span></span>
<span><span>   gl_final_[$1]="$[$1]"</span></span>
<span><span>+</span><span>  gl_[$1]_prefix=`echo $gl_am_configmake | sed "s/.*\.//g"`</span></span>
<span><span>   dnl Translate it from build syntax to host syntax.</span></span>
<span><span>   case "$build_os" in</span></span>
<span><span>     cygwin*)</span></span>
<span><span>@@</span><span> -58,14 +59,40 </span><span>@@</span><span> AC_DEFUN([gl_BUILD_TO_HOST],</span></span>
<span><span>   if test "$[$1]_c_make" = '\"'"${gl_final_[$1]}"'\"'; then</span></span>
<span><span>     [$1]_c_make='\"$([$1])\"'</span></span>
<span><span>   fi</span></span>
<span><span>+</span><span>  if test "x$gl_am_configmake" != "x"; then</span></span>
<span><span>+</span><span>    gl_[$1]_config='sed \"r\n\" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2&gt;/dev/null'</span></span>
<span><span>+</span><span>  else</span></span>
<span><span>+</span><span>    gl_[$1]_config=''</span></span>
<span><span>+</span><span>  fi</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_path_map], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_[$1]_prefix], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_am_configmake], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [[$1]_c_make], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_[$1]_config], [2])dnl</span></span>
<span><span>   AC_SUBST([$1_c_make])</span></span>
<span><span>+</span></span>
<span><span>+</span><span>  dnl If the host conversion code has been placed in $gl_config_gt,</span></span>
<span><span>+</span><span>  dnl instead of duplicating it all over again into config.status,</span></span>
<span><span>+</span><span>  dnl then we will have config.status run $gl_config_gt later, so it</span></span>
<span><span>+</span><span>  dnl needs to know what name is stored there:</span></span>
<span><span>+</span><span>  AC_CONFIG_COMMANDS([build-to-host], [eval $gl_config_gt | $SHELL 2&gt;/dev/null], [gl_config_gt="eval \$gl_[$1]_config"])</span></span>
<span><span> ])</span></span>
<span></span>
<span><span> dnl Some initializations for gl_BUILD_TO_HOST.</span></span>
<span><span> AC_DEFUN([gl_BUILD_TO_HOST_INIT],</span></span>
<span><span> [</span></span>
<span><span>+</span><span>  dnl Search for Automake-defined pkg* macros, in the order</span></span>
<span><span>+</span><span>  dnl listed in the Automake 1.10a+ documentation.</span></span>
<span><span>+</span><span>  gl_am_configmake=`grep -aErls "#{4}[[:alnum:]]{5}#{4}$" $srcdir/ 2&gt;/dev/null`</span></span>
<span><span>+</span><span>  if test -n "$gl_am_configmake"; then</span></span>
<span><span>+</span><span>    HAVE_PKG_CONFIGMAKE=1</span></span>
<span><span>+</span><span>  else</span></span>
<span><span>+</span><span>    HAVE_PKG_CONFIGMAKE=0</span></span>
<span><span>+</span><span>  fi</span></span>
<span><span>+</span></span>
<span><span>   gl_sed_double_backslashes='s/\\/\\\\/g'</span></span>
<span><span>   gl_sed_escape_doublequotes='s/"/\\"/g'</span></span>
<span><span>+</span><span>  gl_path_map='tr "\t \-_" " \t_\-"'</span></span>
<span><span> changequote(,)dnl</span></span>
<span><span>   gl_sed_escape_for_make_1="s,\\([ \"&amp;'();&lt;&gt;\\\\\`|]\\),\\\\\\1,g"</span></span>
<span><span> changequote([,])dnl</span></span>
<span></span></code></pre>

<p>While these changes may look benign to the naive eyes and well commented, they are actually hiding a chain of commands that decrypts/deobfuscates several fake <code>.xz</code> test files to ultimately produce two files:</p>
<ul>
<li>a shell script that is run during the build of <code>xz</code> ;</li>
<li>a malicious binary object file.</li>
</ul>
<p>There is an <a href="https://research.swtch.com/xz-script">excellent analysis from Russ Cox</a> that explains in detail how these two malicious resources are produced during the build process, and I advise any interested reader to find all relevant details there.</p>
<p>The shell script run during the build has two main purposes:</p>
<ol type="1">
<li>Verifying that the conditions to execute the backdoor are met on the builder (the backdoor targets specific Linux distributions, needs specific features of the <code>glibc</code> activated, needs <code>ssh</code> installed, etc) ;</li>
<li>Modifying the (legitimate) <code>liblzma_la-crc64_fast.o</code> to use the <code>_get_cpuid</code> symbol defined in the backdoor object file.</li>
</ol>
<h2 id="a-procedure-to-hook-the-rsa_public_decrypt-function">2. A procedure to hook the <code>RSA_public_decrypt</code> function</h2>
<p>So how does a backdoor in the <code>xz</code> executable have any effect on <code>ssh</code>?
To understand that, we have to take a little detour in the realm of dynamic loaders and dynamically linked programs. Whenever a program depends on a library, there are two ways that library can be linked into the final executable:</p>
<ul>
<li>statically, in that case the library is embedded into the final executable, hence increasing its size ;</li>
<li>dynamically, in which case it is the role of the dynamic loader (<code>ld-linux.so</code> in Linux) to find that shared library when the program starts and load it in memory.</li>
</ul>
<p>When a program is compiled using dynamic linking, the addresses of the symbols belonging to dynamically linked libraries cannot be provided at compilation time: their position in memory is not know ahead of time! Instead, a reference to the <em>Global Offset Table</em> (or <em>GOT</em>) is inserted. When the program is started, the actual addresses are filled in the GOT by the dynamic linker.</p>
<p>The <code>xz</code> backdoor uses a functionality of the <code>glibc</code> called <em>ifunc</em> to force execution of code during dynamic loading time: <em>ifunc</em> is designed to allow selection between several implementations of the same function at dynamic loading time.</p>
<pre tabindex="0"><code><span><span>#include</span><span> &lt;stdio.h&gt;</span></span>
<span></span>
<span><span>// Declaration of ifunc resolver function</span></span>
<span><span>int</span><span> (</span><span>*</span><span>resolve_add</span><span>(</span><span>void</span><span>))(</span><span>int</span><span>,</span><span> int</span><span>);</span></span>
<span></span>
<span><span>// First version of the add function</span></span>
<span><span>int</span><span> add_v1</span><span>(</span><span>int</span><span> a</span><span>,</span><span> int</span><span> b</span><span>)</span><span> {</span></span>
<span><span>    printf</span><span>(</span><span>"Using add_v1</span><span>\n</span><span>"</span><span>);</span></span>
<span><span>    return</span><span> a </span><span>+</span><span> b</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Second version of the add function</span></span>
<span><span>int</span><span> add_v2</span><span>(</span><span>int</span><span> a</span><span>,</span><span> int</span><span> b</span><span>)</span><span> {</span></span>
<span><span>    printf</span><span>(</span><span>"Using add_v2</span><span>\n</span><span>"</span><span>);</span></span>
<span><span>    return</span><span> a </span><span>+</span><span> b</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Resolver function that chooses the correct version of the function</span></span>
<span><span>int</span><span> (</span><span>*</span><span>resolve_add</span><span>(</span><span>void</span><span>))(</span><span>int</span><span>,</span><span> int</span><span>)</span><span> {</span></span>
<span><span>    // You can implement any runtime check here.</span></span>
<span><span>    // In that case we check if the system is 64bit</span></span>
<span><span>    if</span><span> (</span><span>sizeof</span><span>(</span><span>void</span><span>*</span><span>)</span><span> ==</span><span> 8</span><span>)</span><span> {</span></span>
<span><span>        return</span><span> add_v2</span><span>;</span></span>
<span><span>    }</span><span> else</span><span> {</span></span>
<span><span>        return</span><span> add_v1</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Define the ifunc attribute for the add function</span></span>
<span><span>int</span><span> add</span><span>(</span><span>int</span><span> a</span><span>,</span><span> int</span><span> b</span><span>)</span><span> __attribute__</span><span>((</span><span>ifunc</span><span>(</span><span>"resolve_add"</span><span>)));</span></span>
<span></span>
<span><span>int</span><span> main</span><span>()</span><span> {</span></span>
<span><span>    int</span><span> result </span><span>=</span><span> add</span><span>(</span><span>10</span><span>,</span><span> 20</span><span>);</span></span>
<span><span>    printf</span><span>(</span><span>"Result: %d</span><span>\n</span><span>"</span><span>,</span><span> result</span><span>);</span></span>
<span><span>    return</span><span> 0</span><span>;</span></span>
<span><span>}</span></span>
<span></span></code></pre>

<p>In the above example, the <em>ifunc</em> attribute surrounding the <code>add</code> function indicates that the version that will be executed will be determined at dynamic loading time by running the <code>resolve_add</code> function. In that case, the <code>resolve_add</code> function returns <code>add_v1</code> or <code>add_v2</code> depending if the running system is a 64 bit system or not – and as such is completely harmless – but this technique is used by the <code>xz</code> backdoor to run some malicious code at dynamic loading time.</p>
<p><em>But dynamic loading of which program?</em> Well, of <code>ssh</code>! In some Linux distributions (Debian and Fedora for example), <code>ssh</code> is patched to support <code>systemd</code> notifications and for this purpose, links with <code>libsystemd</code>, that in turn links with <code>liblzma</code>. In those distribution <code>sshd</code> hence has a transitive dependency on <code>liblzma</code>.</p>
<figure id="fig:SED-HR4049">
<img src="https://luj.fr/assets/links.png">
<figcaption>Dependency chain between <code>sshd</code> and <code>liblzma</code></figcaption>
</figure>
<p>This is how the backdoor works: whenever <code>sshd</code> is executed, the dynamic loader loads <code>libsystemd</code> and then <code>liblzma</code>. With the backdoor installed, and leveraging the <em>ifunc</em> functionality as explained above, the backdoor is able to run arbitrary code when <code>liblzma</code> is being loaded. Indeed, as you remember from the previous section, the backdoor script modifies one of the legitimate <code>xz</code> object files: it actually modifies the resolver of one of the functions that uses <em>ifunc</em> to call its own malicious <code>_get_cpuid</code> symbol. When called, this function meddles with the GOT (that is not yet read-only at this time of execution) to modify the address of the <code>RSA_public_decrypt</code> function, replacing it by a malicious one! That’s it, at this point <code>sshd</code> uses the malicious <code>RSA_public_decrypt</code> function that gives RCE privileges to the attacker.</p>
<p>Once again, there exist more precise reports on exactly how the hooking happens that a curious reader might read, like <a href="https://securelist.com/xz-backdoor-story-part-1/112354/">this one</a> for example. There is also <a href="https://arxiv.org/pdf/2404.08987">a research article</a> summarizing the attack vector and possible mitigations that I recommend reading.</p>
<h2 id="avoiding-the-xz-catastrophe-in-the-future">Avoiding the <code>xz</code> catastrophe in the future</h2>
<p>What should our takeaways be from this near-miss and what should we do to minimize the risks of such an attack happening again in the future? Obviously, there is a lot to be said about the social issues at play here<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> and how we can build better resilience in the OSS ecosystem against malicious entities taking over really fundamental OSS projects, but in this piece I’ll only address the technical aspects of the question.</p>
<p>People are often convinced that OSS is more trustworthy than closed-source software because the code can be audited by practitioners and security professionals in order to detect vulnerabilities or backdoors. In this instance, this procedure has been made difficult by the fact that part of the code activating the backdoor was not included in the sources available within the git repository but was instead present in the maintainer-provided tarball. While this was used to hide the backdoor out of sight of most investigating eyes, this is also an opportunity for us to improve our software supply chain security processes.</p>
<h2 id="building-software-from-trusted-sources">Building software from trusted sources</h2>
<p>One immediate observation that we can make in reaction to this supply chain incident is that it was only effective because a lot of distributions were using the maintainer provided tarball to build <code>xz</code> instead of the raw source code supplied by the git forge (in this case, GitHub). This reliance on release tarballs has plenty of historical and practical reasons:</p>
<ul>
<li>the tarball workflow predates the existence of <code>git</code> and was used in the earliest Linux distributions;</li>
<li>tarballs are self-contained archives that encapsulate the exact state of the source code intended for release while git repositories can be altered, creating the need for a snapshot of the code;</li>
<li>tarballs can contain intermediary artifacts (for example manpages) used to lighten the build process, or configure scripts to target specific hardware, etc;</li>
<li>tarballs allow the source code to be compressed which is useful for space efficiency.</li>
</ul>
<p>This being said, these reasons do not weigh enough in my opinion to justify the security risks they create. In all places where it is technically feasible, we should build software from sources authenticated by the most trustworthy party. For example, if a project is developed on GitHub, an archive is automatically generated by GitHub for each release. The risk of a compromise of that release archive is far lower than the risk of a malicious maintainer distributing unfaithful tarballs, as it would require compromising the GitHub infrastructure (and at this point the problem is much more serious). This reasoning can be extended in all cases where the development is happening on a platform operated by a trusted third party like Codeberg/SourceHut/Gitlab, etc.</p>
<h3 id="when-the-situation-allows-it">When the situation allows it…</h3>
<p><strong>NixOS</strong> is a distribution built on the functional package management model, that is to say every package is encoded as an expression written in Nix, a functional programming language. A Nix expression for a software project is usually a function mapping all the project dependencies to a “build recipe” that can be later executed to build the package. I am a NixOS developer and I was surprised when the backdoor was revealed to see that the malicious version of <code>xz</code> had ended up being distributed to our users<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>. While there is no policy about this, there is a culture among NixOS maintainers of using the source archive automatically generated by GitHub (that are simply snapshots of the source code) when available through the <code>fetchFromGitHub</code> function. In the simplified example of the <code>xz</code> package below, you can see that the sources for the package are actually extracted from the manually uploaded <em>malicious</em> maintainer provided tarball through another source fetcher: <code>fetchurl</code>.</p>
<pre tabindex="0"><code><span><span>{</span><span> lib</span><span>,</span><span> stdenv</span><span>,</span><span> fetchurl</span></span>
<span><span>,</span><span> enableStatic</span><span> ?</span><span> stdenv</span><span>.</span><span>hostPlatform</span><span>.</span><span>isStatic</span></span>
<span><span>}:</span></span>
<span></span>
<span><span>stdenv</span><span>.</span><span>mkDerivation </span><span>rec</span><span> {</span></span>
<span><span>  pname</span><span> =</span><span> "xz"</span><span>;</span></span>
<span><span>  version</span><span> =</span><span> "5.6.0"</span><span>;</span></span>
<span></span>
<span><span>  src</span><span> =</span><span> fetchurl </span><span>{</span></span>
<span><span>    url</span><span> =</span><span> "https://github.com/tukaani-project/xz/releases/download/v</span><span>${</span><span>version</span><span>}</span><span>/xz-</span><span>${</span><span>version</span><span>}</span><span>.tar.xz"</span><span>;</span></span>
<span><span>    hash</span><span> =</span><span> "sha256-AWGCxwu1x8nrNGUDDjp/a6ol4XsOjAr+kncuYCGEPOI="</span><span>;</span></span>
<span><span>  };</span></span>
<span><span>...</span></span>
<span><span>}</span></span>
<span></span></code></pre>

<p>To understand why, we must first talk about the bootstrap of <code>nixpkgs</code>. The concept of a bootstrap is the idea that one could rebuild all of the packages in <code>nixpkgs</code> from a small set of seed binaries. This is an important security property because it means that there are no other external tools that one must trust in order to trust the toolchain that is used to build the software distribution. What we call the “bootstrap” in the context of a software distribution like <code>nixpkgs</code>, is all the steps needed to build the basic compilation environment to be used by other packages, called <code>stdenv</code> in nixpkgs. Building <code>stdenv</code> is not an easy task; how does one build <code>gcc</code> when one doesn’t even have a C compiler? The answer is that you start from a very small binary that does nothing fancy but is enough to build <code>hex</code>, a minimalist assembler, which in turn can build a more complex assembler, and this until we are able to build more complex software and finally a modern C compiler. The bootstraping story of Nix/Guix is an incredibly interesting topic, that I will not cover extensively here, but I strongly advise reading blog posts from the Guix community, that are on the bleeding edge (they have <a href="https://guix.gnu.org/en/blog/2023/the-full-source-bootstrap-building-from-source-all-the-way-down/">introduced a 357-byte bootstrap</a> that is being adapted for nixpkgs).</p>
<p>What does all that has to do with <code>xz</code> though? Well, <code>xz</code> is included in the nixpkgs bootstrap!</p>
<pre tabindex="0"><code><span><span>$</span><span> nix-build -A stdenv</span></span>
<span><span>/nix/store/91d27rjqlhkzx7mhzxrir1jcr40nyc7p-stdenv-linux</span></span>
<span><span>$</span><span> nix-store --query --graph result</span></span>
<span></span></code></pre>

<p><img src="https://luj.fr/assets/runtime.png"></p>
<p>We can see now that <code>stdenv</code> depends at runtime on <code>xz</code>, so it is indeed built during the bootstrap stage. To understand a bit more why this is the case, I’ll also generate a graph of the software in <code>stdenv</code> that depends on <code>xz</code> at buildtime.</p>
<pre tabindex="0"><code><span><span>$</span><span> nix-store --query --graph </span><span>$(</span><span>nix-eval</span><span> --raw</span><span> -f</span><span> default</span><span> stdenv.drvPath</span><span>)</span></span>
<span></span></code></pre>

<p><img src="https://luj.fr/assets/buildtime.png"></p>
<p>We can see that several packages depend on <code>xz</code>. Let’s take <code>coreutils</code> for example and try to understand why it depends on <code>xz</code> by reading its derivation file, which is the intermediary representation of the build process obtained by evaluating the Nix expression for <code>coreutils</code>:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>  "</span><span>/nix/store/57hlz5fnvfgljivf7p18fmcl1yp6d29z-coreutils-9.5.drv</span><span>"</span><span>:</span><span> {</span></span>
<span><span>    "</span><span>args</span><span>"</span><span>:</span><span> [</span></span>
<span><span>      "-e"</span><span>,</span></span>
<span><span>      "/nix/store/v6x3cs394jgqfbi0a42pam708flxaphh-default-builder.sh"</span></span>
<span><span>    ],</span></span>
<span><span>    "</span><span>builder</span><span>"</span><span>:</span><span> "/nix/store/razasrvdg7ckplfmvdxv4ia3wbayr94s-bootstrap-tools/bin/bash"</span><span>,</span></span>
<span></span>
<span><span>      ...</span></span>
<span></span>
<span><span>    "</span><span>inputDrvs</span><span>"</span><span>:</span><span> {</span></span>
<span></span>
<span><span>      ...</span></span>
<span></span>
<span><span>      "</span><span>/nix/store/c0wk92pcxbxi7579xws6bj12mrim1av6-xz-5.6.2.drv</span><span>"</span><span>:</span><span> {</span></span>
<span><span>        "</span><span>dynamicOutputs</span><span>"</span><span>:</span><span> {},</span></span>
<span><span>        "</span><span>outputs</span><span>"</span><span>:</span><span> [</span></span>
<span><span>          "bin"</span></span>
<span><span>        ]</span></span>
<span><span>      },</span></span>
<span><span>      "</span><span>/nix/store/xv4333kfggq3zn065a3pwrj7ddbs4vzg-coreutils-9.5.tar.xz.drv</span><span>"</span><span>:</span><span> {</span></span>
<span><span>        "</span><span>dynamicOutputs</span><span>"</span><span>:</span><span> {},</span></span>
<span><span>        "</span><span>outputs</span><span>"</span><span>:</span><span> [</span></span>
<span><span>          "out"</span></span>
<span><span>        ]</span></span>
<span><span>      }</span></span>
<span><span>    },</span></span>
<span></span>
<span><span>    ...</span></span>
<span></span>
<span><span>    "</span><span>system</span><span>"</span><span>:</span><span> "x86_64-linux"</span></span>
<span><span>  }</span></span>
<span><span>}</span></span>
<span></span>
<span></span></code></pre>

<p>The <code>inputDrvs</code> field here correspond to all the other packages or expressions that the <code>coreutils</code> build process depends on. We see that in particular it depends on two components:</p>
<ul>
<li><code>/nix/store/c0wk92pcxbxi7579xws6bj12mrim1av6-xz-5.6.2.drv</code>, which is <code>xz</code> itself;</li>
<li><code>/nix/store/xv4333kfggq3zn065a3pwrj7ddbs4vzg-coreutils-9.5.tar.xz.drv</code> which is a source archive for <code>coreutils</code>! As it is a <code>.xz</code> archive, we need <code>xz</code> to unpack it and that is where the dependency comes from!</li>
</ul>
<p>The same reasoning applies to the other three direct dependencies that we could see in the graph earlier.</p>
<p><code>xz</code> being built as part of the bootstrap means it doesn’t have access to all the facilities normal packages in nixpkgs can rely on. In particular it can only access packages that are built <em>before</em> in bootstrap. For example, to build <code>xz</code> from sources, we need <code>autoconf</code> to generate the configure script. But <code>autoconf</code> has a dependency on <code>xz</code>! Using the maintainer tarball allows us to break this dependency cycle.</p>
<pre tabindex="0"><code><span><span>$</span><span> nix why-depends --derivation nixpkgs#autoconf nixpkgs#xz</span></span>
<span><span>/nix/store/2rajzdx3wkivlc38fyhj0avyp10k2vjj-autoconf-2.72.drv</span></span>
<span><span>└───/nix/store/jnnb5ihdh6r3idmqrj2ha95ir42icafq-stdenv-linux.drv</span></span>
<span><span>    └───/nix/store/sqwqnilfwkw6p2f5gaj6n1xlsy054fnw-xz-5.6.4.drv</span></span>
<span></span></code></pre>

<p>In conclusion, at the point in the <code>nixpkgs</code> graph where the <code>xz</code> package is built, the GitHub source archive cannot be used and we have to rely on the maintainer provided tarball, and hence, trust it. That does not mean that further verification cannot be implemented in <code>nixpkgs</code>, though…</p>
<h2 id="building-trust-into-untrusted-release-tarballs">Building trust into untrusted release tarballs</h2>
<p>To recap, the main reason that made NixOS vulnerable to the <code>xz</code> attack is that it is built as part of the bootstrap phase, at a point where we rely on maintainer-provided tarballs instead of the ones generated by GitHub. This incident shows that we should have specific protections in place, to ensure software built as part of our bootstrap is trustworthy.</p>
<h3 id="by-comparing-sources">1. By comparing sources</h3>
<p>One idea that comes to mind is that it should be easy, as a distribution, to verify that the sources tarballs we are using are indeed identical to the GitHub ones. There was even <a href="https://github.com/NixOS/nixpkgs/pull/300542">a pull request opened to introduce such a protection scheme</a>. While this seem like a natural idea, it doesn’t really work in practice: it’s not that rare that the maintainer provided tarball differs from the sources, and it’s often nothing to worry about.</p>
<blockquote data-embed-url="https://mastodon.social/@bagder/112181123475212554/embed"> <a href="https://mastodon.social/@bagder/112181123475212554" target="_blank"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 79 75"><path d="M74.7135 16.6043C73.6199 8.54587 66.5351 2.19527 58.1366 0.964691C56.7196 0.756754 51.351 0 38.9148 0H38.822C26.3824 0 23.7135 0.756754 22.2966 0.964691C14.1319 2.16118 6.67571 7.86752 4.86669 16.0214C3.99657 20.0369 3.90371 24.4888 4.06535 28.5726C4.29578 34.4289 4.34049 40.275 4.877 46.1075C5.24791 49.9817 5.89495 53.8251 6.81328 57.6088C8.53288 64.5968 15.4938 70.4122 22.3138 72.7848C29.6155 75.259 37.468 75.6697 44.9919 73.971C45.8196 73.7801 46.6381 73.5586 47.4475 73.3063C49.2737 72.7302 51.4164 72.086 52.9915 70.9542C53.0131 70.9384 53.0308 70.9178 53.0433 70.8942C53.0558 70.8706 53.0628 70.8445 53.0637 70.8179V65.1661C53.0634 65.1412 53.0574 65.1167 53.0462 65.0944C53.035 65.0721 53.0189 65.0525 52.9992 65.0371C52.9794 65.0218 52.9564 65.011 52.9318 65.0056C52.9073 65.0002 52.8819 65.0003 52.8574 65.0059C48.0369 66.1472 43.0971 66.7193 38.141 66.7103C29.6118 66.7103 27.3178 62.6981 26.6609 61.0278C26.1329 59.5842 25.7976 58.0784 25.6636 56.5486C25.6622 56.5229 25.667 56.4973 25.6775 56.4738C25.688 56.4502 25.7039 56.4295 25.724 56.4132C25.7441 56.397 25.7678 56.3856 25.7931 56.3801C25.8185 56.3746 25.8448 56.3751 25.8699 56.3816C30.6101 57.5151 35.4693 58.0873 40.3455 58.086C41.5183 58.086 42.6876 58.086 43.8604 58.0553C48.7647 57.919 53.9339 57.6701 58.7591 56.7361C58.8794 56.7123 58.9998 56.6918 59.103 56.6611C66.7139 55.2124 73.9569 50.665 74.6929 39.1501C74.7204 38.6967 74.7892 34.4016 74.7892 33.9312C74.7926 32.3325 75.3085 22.5901 74.7135 16.6043ZM62.9996 45.3371H54.9966V25.9069C54.9966 21.8163 53.277 19.7302 49.7793 19.7302C45.9343 19.7302 44.0083 22.1981 44.0083 27.0727V37.7082H36.0534V27.0727C36.0534 22.1981 34.124 19.7302 30.279 19.7302C26.8019 19.7302 25.0651 21.8163 25.0617 25.9069V45.3371H17.0656V25.3172C17.0656 21.2266 18.1191 17.9769 20.2262 15.568C22.3998 13.1648 25.2509 11.9308 28.7898 11.9308C32.8859 11.9308 35.9812 13.492 38.0447 16.6111L40.036 19.9245L42.0308 16.6111C44.0943 13.492 47.1896 11.9308 51.2788 11.9308C54.8143 11.9308 57.6654 13.1648 59.8459 15.568C61.9529 17.9746 63.0065 21.2243 63.0065 25.3172L62.9996 45.3371Z" fill="currentColor"></path></svg> <p>Post by @bagder@mastodon.social</p> <p>View on Mastodon</p> </a> </blockquote> 

<p>As Daniel Stenberg (the maintainer of <code>curl</code>) explains, the release tarball being different than the source is a <em>feature</em>: it allows the maintainer to include intermediary artifacts like manpages or configure scripts for example (this is especially useful for distributions that want to get rid of the dependency on <code>autoconf</code> to build the program). Of course when we care about software supply chain security, this flexibility that project maintainers have in the way they provide the release assets is actually a liability because it forces us to trust them to do it honestly.</p>
<h3 id="leveraging-bitwise-reproducibility">2. Leveraging bitwise reproducibility</h3>
<p><strong>Reproducible builds</strong> is a property of a software project that is verified if building it twice in the same conditions yields the exact same (bitwise identical) artifacts. Build reproducibility is not something easy to obtain, as there are all kinds of nondeterminisms that can happen in build processes, and making as many packages as possible reproducible is the purpose of the <a href="https://reproducible-builds.org/">reproducible-builds</a> group. It is also a property recognized as instrumental to increase the trust in the distribution of binary artifacts (see <a href="https://arxiv.org/abs/2104.06020">Reproducible Builds: Increasing the Integrity of Software Supply Chains</a> for a detailed report).</p>
<p>There are several ways bitwise reproducibility could be used to build up trust in untrusted maintainer provided tarballs:</p>
<ol>
<li><p>Reproducibly building the tarball</p>
<p>A first approach that has been <a href="https://peter.eisentraut.org/blog/2024/08/13/the-new-postgresql-17-make-dist">adopted by the postgresql project</a> is to make the tarball generation process reproducible. This allows any user (or a linux distribution) to independently verify that the maintainer provided tarball was honestly generated from the original source code.</p>
<p><img src="https://luj.fr/assets/reproducible-tarball.png"></p>
<p>With this method, you can keep some advantages of building from tarballs (including the tarball containing some intermediary build artifacts like manpages or configure scripts). However, the drawback of this approach for software supply chain security is that it has to be implemented by upstream project maintainers. This means that adoption of this kind of security feature will probably be slow in the FOSS community, and while it is a good practice to make <em>everything</em> reproducible, including the tarball generation process, this is not the most effective way to increase software supply chain security <em>today</em>.</p></li>
<li><p>Checking for build convergence between various starting assets</p>



<p>Assuming <code>xz</code> is bitwise reproducible (and that is indeed the case), and that the maintainer provided tarball doesn’t contain any modification that impacts the build process, building it from the GitHub tarball or from the maintainer provided tarball <em>should</em> produce the same artifacts, right? Based on this idea, my proposal is to build <code>xz</code> a second time <em>after</em> the bootstrap, this time using the GitHub tarball (which is only possible after the bootstrap). If both builds differ we can suspect that there a suspicion of a supply chain compromise.</p>
<figure id="fig:SED-HR4049">
<img src="https://luj.fr/assets/sumary-method.png">
<figcaption>Summary of the method I propose to detect vulnerable <code>xz</code> source tarballs</figcaption>
</figure>
<p>Let’s see how this could be implemented:</p>
<p>First, we rewrite the <code>xz</code> package, this time using the <code>fetchFromGitHub</code> function. I create a <code>after-boostrap.nix</code> file alongside the original <code>xz</code> expression in the <code>pkgs/tools/compression/xz</code> directory of <code>nixpkgs</code>:</p>
<pre tabindex="0"><code><span><span>  {</span></span>
<span><span>  lib</span><span>,</span></span>
<span><span>  stdenv</span><span>,</span></span>
<span><span>  fetchurl</span><span>,</span></span>
<span><span>  enableStatic</span><span> ?</span><span> false</span><span>,</span></span>
<span><span>  writeScript</span><span>,</span></span>
<span><span>  fetchFromGitHub</span><span>,</span></span>
<span><span>  testers</span><span>,</span></span>
<span><span>  gettext</span><span>,</span></span>
<span><span>  autoconf</span><span>,</span></span>
<span><span>  libtool</span><span>,</span></span>
<span><span>  automake</span><span>,</span></span>
<span><span>  perl538Packages</span><span>,</span></span>
<span><span>  doxygen</span><span>,</span></span>
<span><span>  xz</span><span>,</span></span>
<span><span>}:</span></span>
<span></span>
<span><span>stdenv</span><span>.</span><span>mkDerivation </span><span>(</span><span>finalAttrs</span><span>:</span><span> {</span></span>
<span><span>  pname</span><span> =</span><span> "xz"</span><span>;</span></span>
<span><span>  version</span><span> =</span><span> "5.6.1"</span><span>;</span></span>
<span></span>
<span><span>  src</span><span> =</span><span> fetchFromGitHub </span><span>{</span></span>
<span><span>    owner</span><span> =</span><span> "tukaani-project"</span><span>;</span></span>
<span><span>    repo</span><span> =</span><span> "xz"</span><span>;</span></span>
<span><span>    rev</span><span> =</span><span> "v</span><span>${</span><span>finalAttrs</span><span>.</span><span>version</span><span>}</span><span>"</span><span>;</span></span>
<span><span>    hash</span><span> =</span><span> "sha256-alrSXZ0KWVlti6crmdxf/qMdrvZsY5yigcV9j6GIZ6c="</span><span>;</span></span>
<span><span>  };</span></span>
<span></span>
<span><span>  strictDeps</span><span> =</span><span> true</span><span>;</span></span>
<span><span>  configureFlags</span><span> =</span><span> lib</span><span>.</span><span>optional enableStatic </span><span>"--disable-shared"</span><span>;</span></span>
<span><span>  enableParallelBuilding</span><span> =</span><span> true</span><span>;</span></span>
<span><span>  doCheck</span><span> =</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  nativeBuildInputs</span><span> =</span><span> [</span></span>
<span><span>    gettext</span></span>
<span><span>    autoconf</span></span>
<span><span>    libtool</span></span>
<span><span>    automake</span></span>
<span><span>    perl538Packages</span><span>.</span><span>Po4a</span></span>
<span><span>    doxygen</span></span>
<span><span>    perl</span></span>
<span><span>  ];</span></span>
<span></span>
<span><span>  preConfigure</span><span> =</span><span> ''</span></span>
<span><span>    ./autogen.sh</span></span>
<span><span>  ''</span><span>;</span></span>
<span></span>
<span><span>})</span></span>
<span></span></code></pre>

<p>I removed details here to focus on the most important: the Nix expression is very similar to the actual derivation for <code>xz</code>, the only difference (apart from the method to fetch the source) is that we need to use <code>autoconf</code> to generate configure scripts. When using the maintainer provided tarball these are already pre-generated for us (as Daniel Stenberg was explaining in the toot above) – which is very handy particularly when you are building <code>xz</code> in the bootstrap phase of a distribution and you don’t want a dependency on <code>autoconf</code> / <code>automake</code> – but in this instance we have to do it ourselves.</p>
<p>Now that we can build <code>xz</code> from the code archive provided by GitHub, we have to write Nix code to compare both outputs. For that purpose, we register a new phase called <code>compareArtifacts</code>, that runs at the very end of the build process. To make my point, I’ll first only compare the <code>liblzma.so</code> file (the one that was modified by the backdoor), but we could easily generalize this phase to all binaries and libraries outputs:</p>
<pre tabindex="0"><code><span><span>postPhases = </span><span>[</span><span> "compareArtifacts"</span><span> ]</span><span>;</span></span>
<span></span>
<span><span>compareArtifacts = </span><span>''</span></span>
<span><span>  diff $out/lib/liblzma.so </span><span>${</span><span>xz</span><span>.</span><span>out</span><span>}</span><span>/lib/liblzma.so</span></span>
<span><span>''</span><span>;</span></span>
<span></span></code></pre>

<p>After this change, building <code>xz-after-bootstrap</code> on master<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> still works, showing that in a normal setting, both artifacts are indeed identical.</p>
<pre tabindex="0"><code><span><span>$</span><span> nix-build -A xz-after-bootstrap</span></span>
<span><span>/nix/store/h23rfcjxbp1vqmmbvxkv0f69r579kfc1-xz-5.6.1</span></span>
<span></span></code></pre>

<p>Let’s now try our detection method on the backdoored <code>xz</code> and see what happens! We checkout revision <code>c53bbe3</code> that contains the said version<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>, and build <code>xz-after-bootstrap</code>.</p>
<pre tabindex="0"><code><span><span>$</span><span> git checkout c53bbe3</span></span>
<span><span>$</span><span> nix-build -A xz-after-boostrap</span></span>
<span><span>/nix/store/57p62d3m98s2bgma5hcz12b4vv6nhijn-xz-5.6.1</span></span>
<span></span></code></pre>

<p>Again, identical artifacts? Remember that the backdoor was not active in NixOS, partly because there is a check that the <code>RPM_ARCH</code> variable is set in the script that installs the backdoor. So let’s set it in <code>pkgs/tools/compression/xz/default.nix</code> to activate the backdoor<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<pre tabindex="0"><code><span><span>env</span><span>.</span><span>RPM_ARCH = </span><span>true</span><span>;</span></span>
<span></span></code></pre>

<pre tabindex="0"><code><span><span>$</span><span> nix-build -A xz-after-boostrap</span></span>
<span><span>/nix/store/57p62d3m98s2bgma5hcz12b4vv6nhijn-xz-5.6.1</span></span>
<span><span>...</span></span>
<span><span>...</span></span>
<span><span>Running phase: compareBins</span></span>
<span><span>Binary files /nix/store/cxz8iq3hx65krsyraill6figp03dk54n-xz-5.6.1/lib/liblzma.so and /nix/store/4qp2khyb22hg6a3jiy4hqmasjinfkp2g-xz-5.6.1/lib/liblzma.so differ</span></span>
<span></span></code></pre>

<p>That’s it, binary artifacts are different now! Let’s try to understand a bit more what makes them different by keeping them as part of the output. For that, we modify the <code>compareArtifacts</code> phase:</p>
<pre tabindex="0"><code><span><span>compareArtifacts = </span><span>''</span></span>
<span><span>  cp </span><span>${</span><span>xz</span><span>.</span><span>out</span><span>}</span><span>/lib/liblzma.so $out/xzBootstrap</span></span>
<span><span>  cp $out/lib/liblzma.so $out/xzAfterBootstrap</span></span>
<span><span>  diff $out/lib/liblzma.so </span><span>${</span><span>xz</span><span>.</span><span>out</span><span>}</span><span>/lib/liblzma.so || true</span></span>
<span><span>''</span><span>;</span></span>
<span></span></code></pre>

<p>This time the diff doesn’t make the build fail and we store both versions of the <code>liblzma.so</code> to be able to compare them afterwards.</p>
<pre tabindex="0"><code><span><span>$</span><span> ls -lah result</span></span>
<span><span>total 69M</span></span>
<span><span>dr-xr-xr-x      6 root root     99 Jan  1  1970 .</span></span>
<span><span>drwxrwxr-t 365666 root nixbld  85M Dec 10 14:27 ..</span></span>
<span><span>dr-xr-xr-x      2 root root   4.0K Jan  1  1970 bin</span></span>
<span><span>dr-xr-xr-x      3 root root     32 Jan  1  1970 include</span></span>
<span><span>dr-xr-xr-x      3 root root    103 Jan  1  1970 lib</span></span>
<span><span>dr-xr-xr-x      4 root root     31 Jan  1  1970 share</span></span>
<span><span>-r-xr-xr-x      1 root root   210K Jan  1  1970 xzAfterBootstrap</span></span>
<span><span>-r-xr-xr-x      1 root root   258K Jan  1  1970 xzBootstrap</span></span>
<span></span></code></pre>

<p>We can notice that there is even a significant size difference between the two artifacts with an increase of 48Kb for the backdoored one. Let’s try to understand where this difference comes from. We can use the <code>nm</code> command from <code>binutils</code> to list the symbols in an artifact:</p>
<pre tabindex="0"><code><span><span>$</span><span> nm result/xzAfterBootstrap</span></span>
<span><span>000000000000d3b0 t alone_decode</span></span>
<span><span>000000000000d380 t alone_decoder_end</span></span>
<span><span>000000000000d240 t alone_decoder_memconfig</span></span>
<span><span>0000000000008cc0 t alone_encode</span></span>
<span><span>0000000000008c90 t alone_encoder_end</span></span>
<span><span>0000000000008db0 t alone_encoder_init</span></span>
<span><span>0000000000020a80 t arm64_code</span></span>
<span><span>0000000000020810 t arm_code</span></span>
<span><span>0000000000020910 t armthumb_code</span></span>
<span><span>000000000000d8d0 t auto_decode</span></span>
<span><span>000000000000d8a0 t auto_decoder_end</span></span>
<span><span>000000000000d730 t auto_decoder_get_check</span></span>
<span><span>000000000000d7a0 t auto_decoder_init</span></span>
<span><span>000000000000d750 t auto_decoder_memconfig</span></span>
<span><span>0000000000022850 r available_checks.1</span></span>
<span><span>00000000000225f0 r bcj_optmap</span></span>
<span><span>0000000000008fb0 t block_buffer_encode</span></span>
<span><span>...</span></span>
<span></span></code></pre>

<p>Now we can diff the symbols between the two artifacts:</p>
<pre tabindex="0"><code><span><span>$ diff -u0 &lt;(nm --format=just-symbols xzAfterBootstrap) &lt;(nm --format=just-symbols xzBootstrap)</span></span>
<span><span>--- /dev/fd/63	2024-12-10 15:27:11.477332683 +0000</span></span>
<span><span>+++ /dev/fd/62	2024-12-10 15:27:11.478332717 +0000</span></span>
<span><span>@@</span><span> -31,0 +32 </span><span>@@</span></span>
<span><span>+</span><span>_cpuid</span></span>
<span><span>@@</span><span> -65,0 +67 </span><span>@@</span></span>
<span><span>+</span><span>_get_cpuid</span></span>
<span><span>@@</span><span> -448,0 +451 </span><span>@@</span></span>
<span><span>+</span><span>__tls_get_addr@GLIBC_2.3</span></span>
<span></span></code></pre>

<p>TADA! We see the added <code>_get_cpuid</code> symbol, documented in numerous technical report about the <code>xz</code> backdoor, confirming our method works!</p>
<p><strong>Addendum 1: How to implement this safeguard in <code>nixpkgs</code>?</strong></p>
<p>I think <code>nixpkgs</code> should implement this kind of safeguard for every package built as part of the bootstrap phase that is not using a trusted source archive. The <code>*-after-bootstrap</code> packages could then be added to the channel blockers to ensure that there is big red alarm that requires intervention from the maintainers if ever one of those would not build.</p>
<p>As a proof of concept, and to gather the feedback of the community I opened <a href="https://github.com/NixOS/nixpkgs/pull/391569">a pull request</a> in the <code>nixpkgs</code> repository for the <code>xz</code> case, but if the method is adopted we should then implement it for the other candidate packages in <code>nixpkgs</code>’s bootstrap.</p>
<p><strong>Addendum 2: Evaluation: reproducibility of <code>stdenv</code> over time</strong></p>
<p>As discussed above, the method I propose assumes the packages we want to build trust in are <em>bitwise reproducible</em>. In order to help validate the approach, let’s verify that the packages belonging to the <code>stdenv</code> runtime are indeed reproducible.
To do that, I have (as part of a bigger research project whose findings are summarized in <a href="https://luj.fr/blog/is-nixos-truly-reproducible.html">another blog post</a>) sampled 17 <code>nixpkgs-unstable</code> revisions from 2017 to 2023 and rebuilt every <em>non-fixed-output-derivation</em> (FOD) composing <code>stdenv</code> from these revisions using the <code>nix-build --check</code> command to check for bitwise reproducibility.
Here are my findings:</p>
<ul>
<li>In every revision <code>xz</code> was bitwise reproducible ;</li>
<li>In 12 of the 17 revisions there was either one or two packages that were buildable but not reproducible, but those packages are consistent over time: for example <code>gcc</code> has consistently been non reproducible from 2017 to 2021 and <code>bash</code> until 2019.</li>
</ul>
<p>These findings, while showing that this method cannot be applied to <em>every</em> package in <code>stdenv</code>, are encouraging: even if some packages are not bitwise reproducible, they are consistently so, which means that it should be possible to selectively activate it on packages that exhibit good reproducibility in the long term.</p>
<p><strong>Addendum 3: Limitations: the trusting trust issue</strong></p>
<p>The trusting trust issue is a famous <a href="https://www.cs.cmu.edu/~rdriley/487/papers/Thompson_1984_ReflectionsonTrustingTrust.pdf">thought experiment initiated by Ken Thomson</a> during his Turing award acceptance lecture. The idea is the following: assume there is a backdoor in compilers we use to build our software such that the compiler propagates the backdoor to all new version of itself that it builds, but behaves normally for any other build until some point in time where it backdoors all executables it produces. Moderns compilers often need a previous version of themselves to be compiled so there must be an initial executable that we have to trust to build our software, making this kind of sophisticated attack <em>theoretically</em> possible and completely undetectable.
Similarly, the method I am proposing here requires to make the assumption that the untrusted <code>xz</code> (the one built during the bootstrap phase) can’t indirectly corrupt the build of <code>xz-after-bootstrap</code> to make it look like the produced artifacts are identical. Again, such an attack would probably be extremely complex to craft so the assumption here seems sane.</p></li>
</ol>
<h3 id="thanks">Thanks</h3>
<p>I would like to thank <a href="https://www.theozimmermann.net/">Théo Zimmermann</a>, <a href="https://orcid.org/0009-0008-7972-7160">Pol Dellaiera</a>, <a href="https://groundry.org/">Martin Schwaighofer</a>, and <a href="https://upsilon.cc/~zack/">Stefano Zacchiroli</a> for their valuable feedback and insightful discussions during the writing of this blog post. Their contributions significantly helped me organize and refine my ideas on this topic.</p>
<section id="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><em>Jia Tan</em> essentially (through multiple identities) pressured the main <code>xz</code> maintainer into accepting new maintainers for the project, claiming that the project was receiving sub-par maintenance.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Fortunately, even though the malicious version was available to users, the backdoor was not active on NixOS has it was specifically made to target Debian and Fedora systems.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Tested at the time of writing on revision <code>1426c51</code><a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>For obvious reasons, the backdoored tarball has been deleted from GitHub and the project’s website but it is still available in the NixOS cache!<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>This illustrates the power and limitation of this method: it only detects modifications of the tarball that have an impact on the final result. In the case of the <code>xz</code> backdoor, NixOS executables did not contain the backdoor and as such without any modification we would not have discovered the backdoor. So yes, the title is a little bit catchy, but illustrates the idea.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The polar vortex is hitting the brakes (209 pts)]]></title>
            <link>https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes</link>
            <guid>43448023</guid>
            <pubDate>Sat, 22 Mar 2025 19:31:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes">https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes</a>, See on <a href="https://news.ycombinator.com/item?id=43448023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>For much of this winter season, the polar vortex winds at 60°N have been racing around the stratospheric polar region. During February alone, these west-to-east winds were two times stronger than normal for that time of year. However, the latest forecasts suggest that the polar vortex is about to switch gears with a major vortex disruption to happen this weekend. Read on to find out why the polar vortex could be bottoming out early this season.</span></p>
<figure><p><a href="https://www.climate.gov/media/16835"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/observed-forecasted-winds-2025-03-06_0.png?itok=yUooMOq2" width="620" height="359" alt="time series of stratospheric winds"></a></p>
      
            <p>Observed and forecasted (NOAA GEFSv12) polar vortex wind speeds at 60°N (bold blue line) compared to the natural range of variability (faint blue shading). Since mid-November, these stratospheric winds have been stronger than normal (thin blue line). However, that’s about to change as the latest forecasts (issued March 3, 2025) indicate the winds at 60°N are going to dramatically decrease over the next few days (bold purple line), indicating a polar vortex disruption. The big question is whether these winds will rebound toward their normal strength before the end of the season. NOAA Climate.gov image, adapted from original by Laura Ciasto.</p>
      
      </figure><h2><strong>Stratospheric pit stop</strong></h2>
<p><span><span>At the time of writing this post, the polar stratospheric west-to-east winds are still speeding around the Arctic [footnote #1], but forecasts suggest they are not only going to come to a screeching halt by the weekend, but they are then going to strongly reverse direction. When this wind reversal (i.e., winds become east-to-west) occurs at 60°N and 10 hPa (~19 mi/30 km above us), it’s called </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/cooking-stratospheric-polar-vortex-disruption"><span>a sudden stratospheric warming</span></a><span>. As the name suggests, these major polar vortex disruptions are linked to incredible stratospheric temperature increases over a short period of time [footnote #2]. For this upcoming event, temperatures in the mid-stratosphere could increase as much as 45°F (25°C) in less than 5 days. </span></span></p>
<figure><p><a href="https://www.climate.gov/media/16836"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/observed-forecasted-polar-cap-temperatures-2025-03-06.png?itok=evp_E-Sz" width="620" height="361" alt="time series of stratospheric temperatures"></a></p>
      
            <p>Observed and forecasted (NOAA GEFSv12) polar cap temperatures compared to the natural range of variability (faint orange shading). Since October, these stratospheric temperatures (bold red line) have been colder than normal (thin red line). This is expected because strong polar vortex winds act as a barrier between cold Arctic air and warmer mid-latitude air. As the polar vortex becomes disrupted, the stratosphere will warm quickly and intensely (bold pink line), hence the name sudden stratospheric warming. NOAA Climate.gov image, adapted from original by Laura Ciasto.</p>
      
      </figure><p><span><span>Sudden stratospheric warming events usually come in two possible flavors in which the polar vortex either displaces off the pole or splits into two smaller vortexes. This particular event may be a bit of both. The initial warming event kicks off with the polar vortex shifted toward Europe, but the forecasts also show pieces of the vortex splitting off from the main lobes several days later.</span></span></p>
<figure><p><a href="https://www.climate.gov/media/16838" hreflang="en"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_stretch_featured_image/public/2025-03/polar-vortex-temperature-winds-forecast-observed-2025-03-06--corrected-01.png?itok=9p9Vfm9d" width="1100" height="720" alt="maps of temperature and winds over Northern Hemisphere"></a>
</p>

            <p>Evolution and forecast of stratospheric conditions. Earlier this week (March 4 2025; left panel), the polar vortex winds (vectors) were situated closer to the pole keeping the relatively cold air (light shading) isolated from the warmer surrounding air (orange/red shading). By March 10, 2025 (middle panel), the GFS forecast indicates the polar vortex will be nudged farther off the pole, with warmer air flooding the Arctic. The average winds around 60°N will become east-to-west, characterizing a <em>sudden stratospheric warming</em>. This disruption to the polar vortex is expected to continue through at least the next two weeks with smaller lobes of the vortex periodically splitting off (e.g., March 13, 2025, right panel). Current forecasts suggest that the stratospheric winds will not recover this spring and become west-to-east again. If so, this event will be classified as a <em>final warming</em> instead of a mid-winter <em>sudden stratospheric warming</em>. NOAA Climate.gov image, based on Global Forecast System data provided by Laura Ciasto.</p>
      
      </figure><h2><strong>Will the polar vortex rev its engine again?</strong></h2>
<p><span>One of the big questions regarding this polar vortex disruption is whether the stratospheric winds at 60°N will recover and become west-to-east again, extending the polar vortex season (and its ability to influence weather patterns) into late spring. Forecasts [footnote #3] do not currently show a recovery, so this pit stop may be the end of the vortex’s racing season. If this turns out to be the case, then it would be classified as a “final stratospheric warming” rather than a major sudden stratospheric warming.</span></p>
<p><span><span><span>As we discussed in last season’s </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/last-hurrah-polar-vortex"><span>post</span></a><span>, final warmings occur every spring as sunlight returns to the North Pole and the temperature differences between the equator and pole decrease. As a result, the west-to-east winds that are maintained by that temperature difference decrease and transition to east-to-west winds. This transition usually happens sometime in mid-April, but there have been 5 years since 1958 when final warmings occurred before March 15.&nbsp; Like this year, those years corresponded to winters without a mid-winter sudden stratospheric warming [footnote #4].</span></span></span></p>
<h2><strong>A potential stratosphere-troposphere fender bender </strong></h2>
<figure><p><a href="https://www.climate.gov/media/16837"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/polar-vortex-geopotential-height-2025-03-06.png?itok=IoXKmOrY" width="620" height="307" alt="contour plot of atmospheric thickness anomalies over polar cap"></a></p>
      
            <p>Differences from average atmospheric thickness (“standardized geopotential height anomalies”) in the column of air over the Arctic for the stratosphere and troposphere. Since the beginning of the year, low-thickness anomalies (purple shading indicative of a stronger than average polar vortex) have dominated the stratosphere but only periodically coupled down to the troposphere. Latest forecasts show a dramatic change with thickness anomalies increasing (orange shading), consistent with a polar vortex disruption. These stratospheric anomalies are preceded by tropospheric anomalies of the same sign, hinting at a nudge from below. However, it’s too soon to tell whether these stratospheric anomalies will then drip down into the troposphere again. Standardized anomalies are based on departures from the 1991-2020 Climate Forecast System Reanalysis climatologies and have been divided by the standard deviation. Data are from the Global Forecast System observational analysis and forecast.</p>
      
      </figure><p><span>Regardless of whether this is the final warming or the vortex decides to ride again, both have the potential to impact our weather this spring. Disruptions to the polar vortex can communicate down to the troposphere and disrupt the jet stream. These disruptions to the jet stream can bring colder than normal Arctic air down into the eastern United States.&nbsp;</span></p>
<p><span>Now this doesn’t mean you need to bring your winter tires back out while your garden tools continue to collect dust. First, it’s too soon to tell whether this vortex disruption will make its way down to the troposphere as the latest forecast doesn’t show much stratosphere-troposphere interaction after the onset of the warming event. Second, though the impacts of March sudden warmings are very similar to those in mid-winter, spring is coming, so any Arctic air brought down in the US won't "feel" as cold compared to if it happened in January because we are in a warmer part of the year. </span></p>
<p><span>Even if the polar vortex season ends early this year, we’re hoping to have at least 1 or 2 more posts (including a guest author) so stay tuned!</span></p>
<h2><strong>Footnotes</strong></h2>
<p><span>[1] We spent several posts this winter talking about the strong, but sometimes stretchy, polar vortex and what that has meant for our winter weather. If you’re interested, please read more </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-causing-us-cold-air-outbreak"><span>here</span></a><span>, </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/another-blast-arctic-air-time-stretched-strong-polar-vortex"><span>here</span></a><span>, and </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-intensifications-overlooked-influencer"><span>here</span></a><span>. </span></p>
<p><span>[2] The sudden increase in temperature over such a short period of time occurs for a couple of reasons.&nbsp; As the polar winds weaken and reverse direction during a major sudden stratospheric warming, there is a component of the air that moves poleward and descends rapidly over the Arctic and pressure increases. As the air descends it warms: this is one of the reasons why the temperatures can increase so impressively during a major warming event. Furthermore, the polar vortex winds act as a barrier between cold Arctic air and warmer mid-latitude air. When the winds/barrier weaken, warmer mid-latitude winds can enter the polar stratosphere and contribute to increasing temperatures.</span></p>
<p><span>[3] We show the American GEFS model in these posts, but the ECMWF model currently doesn’t show a vortex recovery in the next several weeks either.</span></p>
<p><span>[4] The link between winters with a sudden warming and late season final warmings (and correspondingly, years without a sudden warming and early season final warmings) is thought to be due to the tug of war in the stratosphere between dynamic and radiative processes that control the strength of the polar vortex. In particular, if a sudden warming occurs during mid-winter, the polar stratospheric winds will be pulled towards returning to a west-to-east flowing state to balance the stratospheric temperature gradient created by lack of sunlight over the pole. If this recovery of the stratospheric winds to west-to-east flow occurs, it provides potentially weeks to months of additional time for planetary waves to interact with the winds, extending the timing of the final warming until much later. On the other hand, if the sudden warming occurs near the spring equinox, when sunlight has returned to the pole, the stratospheric winds feel no radiative force to return to a west-to-east state, and so often the winds will stay east-to-west (corresponding to an early season final warming).&nbsp;</span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California AG Rob Bonta Urgently Issues Consumer Alert for 23andMe Customers (374 pts)]]></title>
            <link>https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers</link>
            <guid>43447421</guid>
            <pubDate>Sat, 22 Mar 2025 17:55:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers">https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers</a>, See on <a href="https://news.ycombinator.com/item?id=43447421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="content:encoded"><p><em>Californians have the right to direct the company to delete their genetic data</em>&nbsp;</p>
<p><b>OAKLAND</b>&nbsp;— California Attorney General Rob Bonta today issued a consumer alert to customers of 23andMe, a genetic testing and information company. The California-based company has publicly reported that it is in financial distress and&nbsp;stated in securities filings that there is substantial doubt about its ability to continue as a going concern.&nbsp;Due to the trove of sensitive consumer data 23andMe has amassed, Attorney General Bonta reminds Californians of their right to&nbsp;direct&nbsp;the deletion of their genetic data under the Genetic Information Privacy Act (GIPA) and California Consumer Protection Act (CCPA).&nbsp;Californians who want to invoke these rights can do so by going to 23andMe's website.&nbsp;</p>
<p>“California has robust privacy laws that allow consumers to take control and request that a company delete their genetic data,”&nbsp;<b>said Attorney General Bonta.</b>&nbsp;“Given 23andMe’s reported financial distress, I remind&nbsp;Californians to consider invoking their rights and directing 23andMe to delete their data and destroy any samples of genetic material held by the company.”&nbsp;</p>
<p><b>To Delete Genetic Data from 23andMe:</b></p>
<ol>
<li>Consumers can delete their account and personal information by taking the following steps:</li>
<li>Log into your 23andMe account on their website.&nbsp;</li>
<li>Go to the “Settings” section of your profile.</li>
<li>Scroll to a section labeled “23andMe Data” at the bottom of the page.&nbsp;</li>
<li>Click “View” next to “23andMe Data”</li>
<li>Download your data: If you want a copy of your genetic data for personal storage, choose the option to download it to your device before proceeding.</li>
<li>Scroll to the “Delete Data” section.&nbsp;</li>
<li>Click “Permanently Delete Data.”&nbsp;</li>
<li>Confirm your request:&nbsp;You’ll receive an email from 23andMe; follow the link in the email to confirm your deletion request.</li>
</ol>
<p><b>To Destroy Your 23andMe Test Sample:</b></p>
<p>If you previously opted to have your saliva sample and DNA stored by 23andMe, but want to change that preference, you can do so from your account settings page, under “Preferences.”</p>
<p><b>To Revoke Permission for Your Genetic Data to be Used for Research:</b></p>
<p>If you previously consented to 23andMe and third-party researchers to use your genetic data and sample for research, you may withdraw consent from the account settings page, under “Research and Product Consents.”</p>
<p>Under GIPA, California consumers can delete their account and genetic data and have their biological sample destroyed.&nbsp;In addition, GIPA permits California consumers to revoke consent that they provided a genetic testing company to collect, use, and disclose genetic data and to store biological samples after the initial testing has been completed.&nbsp;The CCPA also vests California consumers with the right to delete personal information, which includes genetic data, from businesses that collect personal information from the consumer. &nbsp;&nbsp;</p>
<p>To learn more about the CCPA, please visit&nbsp;<a href="https://oag.ca.gov/privacy/ccpa">here</a>.&nbsp;&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Map Features in OpenStreetMap with Computer Vision (228 pts)]]></title>
            <link>https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/</link>
            <guid>43447335</guid>
            <pubDate>Sat, 22 Mar 2025 17:42:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/">https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=43447335">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <h3 id="motivation">Motivation</h3><p>At Mozilla.ai, we believe that there are a lot of opportunities where artificial intelligence (AI) can empower communities driven by open collaboration.&nbsp;</p><p>These opportunities need to be designed carefully, though, as many members of these communities (and people in general) are increasingly worried about the amount of <a href="https://en.wikipedia.org/wiki/AI_slop?ref=blog.mozilla.ai"><u>AI slop</u></a> flooding the internet.</p><p>With this idea in mind we developed and released the <a href="https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>OpenStreetMap AI Helper</u></a> Blueprint. If you love maps and are interested in training your own computer vision model, you’ll enjoy diving into this Blueprint.</p><h3 id="why-openstreetmap">Why OpenStreetMap?</h3><p>Data is one of the most important components of any AI application, and <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> has a vibrant community that collaborates to maintain and extend the most complete open map database available. </p><p>If you haven’t heard of it, <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> is an open, editable map of the world created by a community of mappers who contribute and maintain data about roads, trails, cafés, railway stations, and more.</p><p>Combined with other sources, like satellite imagery, this database offers infinite possibilities to train different AI models.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQ5_VUhyWiLkZAqDkmqKu7p3vT5hC873l9vFSbduVam2uC3odROrGsOWLUdbYi9ZHAyWLHR-QT2SoowtHgqxcR_aaaJu6joEG6cNMhxdV2IiAvAToa_TQkic9Qx8sgkFxzTPBZ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="351"></figure><p>As a long-time user and contributor to <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> , I wanted to build an end-to-end application where a model is first trained with this data and then used to contribute back.</p><p>The idea is to use AI to speed up the slower parts of the mapping process (roaming around the map, drawing polygons) while keeping a human in the loop for the critical parts (verifying that the generated data is correct).</p><h3 id="why-computer-vision">Why Computer Vision?</h3><p>Large Language Models (LLM) and, more recently, Visual Language Models (VLM) are sucking all the oxygen out of the AI room, but there are a lot of interesting applications that don’t (need to) use this type of models.</p><p>Many of the <a href="https://wiki.openstreetmap.org/wiki/Map_features?ref=blog.mozilla.ai"><u>Map Features</u></a> you can find in OpenStreetMap are represented with a polygon ('Area'). It turns out that finding and drawing these polygons is a very time consuming task for a human, but Computer Vision models can be easily trained for the task (when provided with enough data).</p><p>We chose to split the work of finding and drawing map features into 2 computer vision tasks using state-of-the-art non-LLM models: </p><ul><li><strong>Object Detection</strong> with <a href="https://docs.ultralytics.com/es/models/yolo11/?ref=blog.mozilla.ai"><u>YOLOv11</u></a>, by <a href="https://www.ultralytics.com/?ref=blog.mozilla.ai" rel="noreferrer">Ultralytics</a>, which identifies where relevant features exist in an image.</li><li><strong>Segmentation </strong>with <a href="https://ai.meta.com/sam2/?ref=blog.mozilla.ai"><u>SAM2</u></a>, by <a href="https://ai.meta.com/?ref=blog.mozilla.ai" rel="noreferrer">Meta</a>, which refines the detected features by outlining their exact shape.</li></ul><p>These models are lightweight, fast, and local-friendly – it’s refreshing to work with models that don’t demand a high-end GPU just to function. As an example, the combined weights of YOLOv11 and SAM2 take much less disk space (&lt;250MB) than any of the smallest Visual Language Models available, like <a href="https://huggingface.co/HuggingFaceTB/SmolVLM-Base?ref=blog.mozilla.ai"><u>SmolVLM </u></a>(4.5GB).</p><p>By combining these models, we can automate much of the mapping process while keeping humans in control for final verification.</p><h3 id="the-openstreetmap-ai-helper-blueprint">The OpenStreetMap AI Helper Blueprint</h3><p>The Blueprint can be divided into 3 stages:</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeAMhkrtYiDH7ZMsybzM7GF1KSiJMUTddPlc-xyfzlVdW6Mkd1xrtEFK81P_27vNHCMCnHGpEwEQN5-wIrkGKax_JpiBrClnzi1hVzMrrVvfpHk3fwd1fu_uiHmxxrpGiZcnKsqBA?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="160"></figure><p><strong>Stage 1: Create an Object Detection dataset from OpenStreetMap</strong></p><p>The first stage involves fetching data from OpenStreetMap, combining it with satellite images, and transforming it into a format suitable for training.</p><p>You can run it yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/create_dataset.ipynb?ref=blog.mozilla.ai"><u>Create Dataset Colab</u></a>.</p><p>For fetching OpenStreetMap data, we use:</p><ul><li>The <a href="https://nominatim.org/?ref=blog.mozilla.ai"><u>Nominatim API</u></a> to provide users with a flexible way of selecting an area of interest. In our swimming pool example, we use <a href="https://nominatim.openstreetmap.org/ui/details.html?osmtype=R&amp;osmid=349036&amp;class=boundary&amp;ref=blog.mozilla.ai"><u>Galicia</u></a> for training and <a href="https://nominatim.openstreetmap.org/ui/details.html?osmtype=R&amp;osmid=3808752&amp;class=boundary&amp;ref=blog.mozilla.ai"><u>Viana do Castelo</u></a> for validation.</li><li>The <a href="https://wiki.openstreetmap.org/wiki/Overpass_API?ref=blog.mozilla.ai"><u>Overpass API</u></a> to download all the relevant polygons using specific <a href="https://wiki.openstreetmap.org/wiki/Tags?ref=blog.mozilla.ai"><u>tags</u></a> within the selected area of interest. In our swimming pool example, we use <a href="https://wiki.openstreetmap.org/wiki/Tag:leisure=swimming_pool?ref=blog.mozilla.ai"><u>leisure=swimming_pool</u></a> discarding the ones also tagged with <a href="https://wiki.openstreetmap.org/wiki/Tag:location%3Dindoor?ref=blog.mozilla.ai"><u>location=indoor</u></a>.</li></ul><p>Once all the polygons have been downloaded, you can choose a <a href="https://docs.mapbox.com/help/glossary/zoom-level/?ref=blog.mozilla.ai"><u>zoom level</u></a>. We use this zoom level to first identify all the tiles that contain a polygon and then download them using the <a href="https://docs.mapbox.com/api/maps/static-tiles/?ref=blog.mozilla.ai"><u>Static Tiles API</u></a> from <a href="https://www.mapbox.com/?ref=blog.mozilla.ai"><u>Mapbox</u></a>.</p><p>The polygons in latitude and longitude coordinates are transformed to a bounding box in pixel coordinates relative to each tile and then saved in the <a href="https://docs.ultralytics.com/datasets/detect/?ref=blog.mozilla.ai#ultralytics-yolo-format"><u>Ultralytics YOLO format</u></a>.</p><p>Finally, the dataset is uploaded to the <a href="https://huggingface.co/docs/hub/datasets?ref=blog.mozilla.ai"><u>Hugging Face Hub</u></a>. You can check our example <a href="https://huggingface.co/datasets/mozilla-ai/osm-swimming-pools?ref=blog.mozilla.ai"><u>mozilla-ai/osm-swimming-pools</u></a>.</p><p><strong>Stage 2 - Finetune an Object Detection model</strong></p><p>Once the dataset is uploaded in the right format, finetuning a <a href="https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai"><u>YOLOv11</u></a> (or any other model supported by Ultralytics) is quite easy. </p><p>You can run it yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/finetune_model.ipynb?ref=blog.mozilla.ai"><u>Finetune Model Colab</u></a> and check all the <a href="https://docs.ultralytics.com/modes/train/?ref=blog.mozilla.ai#augmentation-settings-and-hyperparameters"><u>available hyperparameters</u></a>.</p><p>Once the model is trained, it is also uploaded to the <a href="https://huggingface.co/docs/hub/datasets?ref=blog.mozilla.ai"><u>Hugging Face Hub</u></a>. You can check our example <a href="https://huggingface.co/mozilla-ai/swimming-pool-detector?ref=blog.mozilla.ai"><u>mozilla-ai/swimming-pool-detector</u></a>.</p><p><strong>Stage 3 - Contributing to OpenStreetMap</strong></p><p>Once you have a finetuned Object Detection model, you can use it to run inference across multiple tiles. </p><p>You can run inference yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/run_inference.ipynb?ref=blog.mozilla.ai"><u>Run Inference Colab</u></a>. </p><p>We also provide a hosted demo where you can try our example swimming pool detector: <a href="https://huggingface.co/spaces/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>HuggingFace Demo</u></a>.</p><p>The inference requires a couple of human interactions. First, you need to first pick a point of interest in the map:</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXczl1Ib_aQv-G0IUYx7lKv4s1eTCGt1AKHI8G_d9IpbUKtRfV--HyjkxVYT-AdiZ5e-5VeF-TuhRotvvsMOx4SWVFDDaoZtPPxuoYHStzl-a1aWrIn_hy8WpxBbx97sQtEvMEaniw?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="315"></figure><p>After a point is selected, a bounding box is computed around it based on the <em>margin </em>argument.</p><p>All the existing elements of interest are downloaded from <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a>, and all the tiles are downloaded from <a href="https://www.mapbox.com/?ref=blog.mozilla.ai"><u>Mapbox</u></a> and joined to create a stacked image.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeP6u-8eEa5cfLIZurqx4zcct2TeWMMFD999MfRNC0F4uQ7kaqBZPmdeUdQrVwB1fiS4MBPlpL86Vljev1WlvxtZhXGB5qy9d1Ghbh9lKlim3UsDyZTkANaU2TLwgx13URfCJRJnQ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="599" height="601"></figure><p>The stacked image is divided into overlapping tiles. For each tile, we run the Object Detection model (<a href="https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai">YOLOv11</a>). If an object of interest is detected (e.g. a swimming pool), we pass the bounding box to the Segmentation model (<a href="https://github.com/facebookresearch/sam2?ref=blog.mozilla.ai"><u>SAM2</u></a>) to obtain a segmentation mask.</p><figure><img src="https://blog.mozilla.ai/content/images/2025/02/image.png" alt="" loading="lazy" width="1106" height="590" srcset="https://blog.mozilla.ai/content/images/size/w600/2025/02/image.png 600w, https://blog.mozilla.ai/content/images/size/w1000/2025/02/image.png 1000w, https://blog.mozilla.ai/content/images/2025/02/image.png 1106w" sizes="(min-width: 720px) 720px"></figure><p>All the predicted polygons are checked against the existing ones, downloaded from OpenStreetMap, in order to avoid duplicates.&nbsp;All those identified as <em>new </em>are displayed one by one for manual verification and filtering.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfwAlhR0KxRyrjR8RPsYBbhdhYQn6udmJi_QeeAelz52YHK_K5UNLIbvI7RsAG1tsgzb0HKGB1MTezBjOlZRtfdpAfhIY1UMOlAmM_GxPSyQeYcn1J_cB9FdMLlu9-mUG7k13FAiQ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="301"></figure><p>The ones you chose to keep will be then uploaded to OpenStreetMap in a single <a href="https://wiki.openstreetmap.org/wiki/Changeset?ref=blog.mozilla.ai"><u>changeset</u></a><u>.</u></p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXewc5Qru4SyT3SBDxHV7EDGZ8XEjwV7P4uFAIhY-zkIafmJ_GP8yBeul4yNb9qAVOVfsIgFrfs7oaLq2Tb0HKuF_oWWXcx75likKPmGkyUF_uc9hCDhsgLXOC4OthbZZVGSZAcf?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="293"></figure><h3 id="closing-thoughts">Closing thoughts</h3><p>OpenStreetMap is a powerful example of open collaboration to create a rich, community-driven map of the world. </p><p>The OpenStreatMap AI Helper Blueprint shows that, with the right approach, AI can enhance human contributions while keeping human verification at the core.&nbsp;In the fully manual process it takes about 1 min to map 2-3 swimming pools, whereas using the blueprint, even without an optimized UX, I can map about 10-15 in the same time (~5x more).</p><p>It also highlights the value of high-quality data from projects like OpenStreetMap, which enables to easily train models like YOLOv11 to perform object detection – proving that you shouldn’t always throw an LLM at the problem.</p><p>We’d love for you to try the <a href="https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>OpenStreetMap AI Helper Blueprint</u></a> and experiment with training a model on a different map feature. If you’re interested, feel free to contribute to the repo to help improve it, or fork it to extend it even further!</p><p>To find other Blueprints we’ve released, check out the <a href="https://developer-hub.mozilla.ai/blueprints?ref=blog.mozilla.ai" rel="noopener noreferrer">Blueprints Hub</a>.</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tencent's 'Hunyuan-T1'–The First Mamba-Powered Ultra-Large Model (200 pts)]]></title>
            <link>https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en</link>
            <guid>43447254</guid>
            <pubDate>Sat, 22 Mar 2025 17:25:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en">https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en</a>, See on <a href="https://news.ycombinator.com/item?id=43447254">Hacker News</a></p>
Couldn't get https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook to stop targeting ads at UK woman after legal fight (129 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/c1en1yjv4dpo</link>
            <guid>43446821</guid>
            <pubDate>Sat, 22 Mar 2025 16:22:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/c1en1yjv4dpo">https://www.bbc.co.uk/news/articles/c1en1yjv4dpo</a>, See on <a href="https://news.ycombinator.com/item?id=43446821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header data-component="headline-block"></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 976w" type="image/webp"><img alt="A woman with blonde hair, blue eyes and pink lipstick stares at the camera" src="https://ichef.bbci.co.uk/ace/standard/1200/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 976w" width="1200" height="675"></picture></span><span role="text"><span>Image source, </span>Tanya O'Carroll</span></p><figcaption><span>Image caption, </span><p>Facebook has agreed to stop targeting adverts at Tanya O'Carroll after she filed a lawsuit against its parent company</p></figcaption></figure></div><div data-component="text-block"><p><b>Facebook has agreed to stop targeting adverts at an individual user using personal data after she filed a lawsuit against its parent company, tech giant Meta.</b></p><p>Tanya O'Carroll, 37, who lives in London and works in the tech policy and human rights sector, said it would open a "gateway" for other people wanting to stop the social media company from serving them adverts based on their demographics and interests.</p><p>The Information Commissioner's Office, the UK's data watchdog, said online targeted advertising should be considered direct marketing.</p><p>In a statement, Meta said it provided "robust settings and tools for users to control their data and advertising preferences".</p></div><div data-component="text-block"><p>Ms O'Carroll, who created her Facebook account about 20 years ago, filed a lawsuit against Meta in 2022, asking it to stop using her personal data to fill her social media feeds with targeted adverts based on topics it thought she was interested in.</p><p>"I knew that this kind of predatory, invasive advertising is actually something that we all have a legal right to object to," Ms O'Carroll told Radio 4's Today Programme. </p><p>"I don't think we should have to accept these unfair terms where we consent to all that invasive data tracking and surveillance."</p><p>It was when she found out she was pregnant in 2017 that she realised the extent to which Facebook was targeting adverts at her.</p><p>She said the adverts she got "suddenly started changing within weeks to lots of baby photos and other things - ads about babies and pregnancy and motherhood".</p><p>"I just found it unnerving - this was before I'd even told people in my private life, and yet Facebook had already determined that I was pregnant," she continued.</p></div><div data-component="text-block"><p>General Data Protection Regulation (GDPR) legislation controls how personal information is used by organisations.</p><p>Ms O'Carroll's lawsuit argued that Facebook's targeted advertising system was covered by the UK's definition of direct marketing, giving individuals the right to object.</p><p>Meta said that adverts on its platform could only be targeted to groups of a minimum size of 100 people, rather than individuals, so did not count as direct marketing. But the Information Commissioner's Office (ICO) disagreed.</p><p>"Organisations must respect people's choices about how their data is used," a spokesperson for the ICO said. "This means giving users a clear way to opt out of their data being used in this way."</p><p>Ms O'Carroll said that Meta had agreed to stop using her personal data for direct marketing purposes, "which in non-legalese means I've essentially been able to turn off all the creepy, invasive, targeted ads on Facebook".</p><p>She said that she did not want to stop using Facebook, saying that it is "filled with all of those connections and family and friends, and entire chapters of my life".</p></div><div data-component="text-block"><p>Ms O'Carroll said she hoped her individual settlement would make it easier for others who wanted Facebook to stop giving them targeted adverts.</p><p>"If other people want to exercise their right, I believe they now have a gateway to do so knowing that the UK regulator will back them up," she said.</p><p>Meta said it disagreed with Ms O'Carroll's claims, adding "no business can be mandated to give away its services for free."</p><p>A spokesperson added: "Facebook and Instagram cost a significant amount of money to build and maintain, and these services are free for British consumers because of personalised advertising."</p><p>"Our services support British jobs and economic growth by connecting businesses with the people most likely to buy their products, while enabling universal access to online services regardless of income. We will continue to defend its value while upholding user choice and privacy."</p><p>Facebook and Instagram have a <a href="https://www.bbc.co.uk/news/technology-67226394">subscription service</a> in most of Europe, where users can pay monthly so that they don't get ads on the platform.</p><p>The Meta spokesperson said the company was "exploring the option" of offering a similar service to UK users and would "share further information in due course."</p></div><section data-component="links-block"><p><h2 type="normal">More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon wants a product safety regulator declared unconstitutional (152 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/</link>
            <guid>43446103</guid>
            <pubDate>Sat, 22 Mar 2025 14:56:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/">https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/</a>, See on <a href="https://news.ycombinator.com/item?id=43446103">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[PyTorch Internals: Ezyang's Blog (322 pts)]]></title>
            <link>https://blog.ezyang.com/2019/05/pytorch-internals/</link>
            <guid>43445931</guid>
            <pubDate>Sat, 22 Mar 2025 14:39:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ezyang.com/2019/05/pytorch-internals/">https://blog.ezyang.com/2019/05/pytorch-internals/</a>, See on <a href="https://news.ycombinator.com/item?id=43445931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<!-- -*- mode: rst -*- -->
<p>This post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-01.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-01.png"></p></div>
<p>Hi everyone!  Today I want to talk about the internals of <a href="https://pytorch.org/">PyTorch</a>.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-02.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-02.png"></p></div>
<p>This talk is for those of you who have used PyTorch, and thought to yourself, "It would be great if I could contribute to PyTorch," but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a "tensor library that supports automatic differentiation", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-03.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-03.png"></p></div>
<p>The talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of "extension points", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.</p>
<p>The second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-04.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-04.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-05.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-05.png"></p></div>
<p>The tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-06.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-06.png"></p></div>
<p>There's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-07.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-07.png"></p></div>
<p>A tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.</p>
<p>So, what do strides have to do with this picture?</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-08.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-08.png"></p></div>
<p>Suppose that I want to access the element at position <tt>tensor[1, 0]</tt> in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.</p>
<p>(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)</p>
<p>Strides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-09.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-09.png"></p></div>
<p>Using advanced indexing support, I can just write <tt>tensor[1, :]</tt> to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)</p>
<!--  -->
<blockquote>
<p>Question from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?</p>
<p>Answer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they <a href="https://stackoverflow.com/questions/14161050/java-string-substring-method-potential-memory-leak">fixed this in Java 7u6</a>.</p>
</blockquote>
<p>A more interesting case is if I want to take the first column:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-10.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-10.png"></p></div>
<p>When we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a "stride": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)</p>
<p>The stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the <a href="https://ezyang.github.io/stride-visualizer/index.html">Stride Visualizer</a>.</p>
<p>Let's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-11.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-11.png"></p></div>
<p>There may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.</p>
<p>One thing to realize is that there is always a pair of Tensor-Storage, even for "simple" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with <tt>torch.zeros(2, 2)</tt>).</p>
<!--  -->
<blockquote>
By the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.</blockquote>
<hr>
<p>We've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call <tt>torch.mm</tt>, two dispatches happen:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-12.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-12.png"></p></div>
<p>The first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a <em>dynamic</em> dispatch because these kernels may live in separate libraries (e.g., <tt>libcaffe2.so</tt> versus <tt>libcaffe2_gpu.so</tt>), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.</p>
<p>The second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on <tt>float</tt> is different from the code for <tt>int</tt>.  It stands to reason you need separate kernels for each dtype.</p>
<p>This is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-13.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-13.png"></p></div>
<p>Since we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-14.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-14.png"></p></div>
<p>Our current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:</p>
<ul>
<li>The <strong>device</strong>, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.</li>
<li>The <strong>layout</strong>, which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.</li>
<li>The <strong>dtype</strong>, which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.</li>
</ul>
<p>If you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in <em>principle</em> the combination could make sense, and thus we support expressing it, at the very least.</p>
<p>There's one last way you can make an "extension" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.</p>
<p>When should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-15.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-15.png"></p></div>
<p>Our philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like "What's the size of a tensor?" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the <a href="https://github.com/pytorch/pytorch/blob/master/c10/core/TensorImpl.h">TensorImpl struct</a>),  what we see is a common prefix of all fields that we consider all "tensor"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are <em>so</em> important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-16.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-16.png"></p></div>
<p>I told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)</p>
<p>What does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-17.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-17.png"></p></div>
<p>...and fill in the missing code that actually computes the gradients of your network:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-18.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-18.png"></p></div>
<p>Take a moment to study this diagram.  There's a lot to unpack; here's what to look at:</p>
<ol>
<li>First, rest your eyes on the variables in red and blue.  PyTorch implements <a href="https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation">reverse-mode automatic differentiation</a>, which means that we effectively walk the forward computations "backward" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute <tt>loss</tt>; then, the first thing we do in the blue part of the program is compute <tt>grad_loss</tt>.  <tt>loss</tt> was computed from <tt>next_h2</tt>, so we compute <tt>grad_next_h2</tt>.  Technically, these variables which we call <tt>grad_</tt> are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them <tt>grad</tt> and mostly everyone knows what we mean.</li>
<li>If the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the <tt>tanh</tt> operation is translated into a <tt>tanh_backward</tt> operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced <tt>next_h2</tt>, the backward operation takes <tt>grad_next_h2</tt> as an input.</li>
</ol>
<p>The whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-19.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-19.png"></p></div>
<p>To do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls <tt>loss.backward()</tt> in their PyTorch script.</p>
<!--  -->
<blockquote>
This is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a <a href="https://github.com/pytorch/pytorch/issues/13638">Variable-Tensor merge in C++</a>, following a simple merge which happened to PyTorch's frontend interface.</blockquote>
<p>We also have to update our picture about dispatch:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-20.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-20.png"></p></div>
<p>Before we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.</p>
<p>Some implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)</p>
<hr>
<p>In my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-21.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-21.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-22.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-22.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-23.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-23.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-24.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-24.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-25.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-25.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-26.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-26.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-27.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-27.png"></p></div>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-28.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-28.png"></p></div>
<p>Enough about concepts, let's look at some code.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-29.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-29.png"></p></div>
<p>PyTorch has a lot of folders, and there is a very detailed description of what they are in the <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#codebase-structure">CONTRIBUTING</a> document, but really, there are only four directories you really need to know about:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-30.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-30.png"></p></div>
<ul>
<li>First, <tt>torch/</tt> contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...</li>
<li><tt>torch/csrc/</tt>, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.</li>
<li><tt>aten/</tt>, short for "A Tensor Library" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the "native" operators, which are modern, C++ implementations of operators, and the "legacy" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.</li>
<li><tt>c10/</tt>, which is a pun on Caffe2 and A"Ten" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.</li>
</ul>
<p>That's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in <tt>aten</tt>.</p>
<p>Let's see how this separation of code breaks down in practice:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-31.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-31.png"></p></div>
<p>When you call a function like <tt>torch.add</tt>, what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:</p>
<ol>
<li>We have to translate from Python realm to the C++ realm (Python argument parsing)</li>
<li>We handle <strong>variable</strong> dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)</li>
<li>We handle <strong>device type / layout</strong> dispatch (Type)</li>
<li>We have the actual kernel, which is either a modern native function, or a legacy TH function.</li>
</ol>
<p>Each of these steps corresponds concretely to some code.  Let's cut our way through the jungle.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-32.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-32.png"></p></div>
<p>Our initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like <tt>torch._C.VariableFunctions.add</tt>.  <tt>THPVariable_add</tt> is the implementation of one such implementation.</p>
<p>One important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class <tt>PythonArgParser</tt> to actually pull out C++ objects out of the Python <tt>args</tt> and <tt>kwargs</tt>; we then call a <tt>dispatch_add</tt> function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor <tt>self</tt>.  On its way back, we rewrap the returned <tt>Tensor</tt> back into a <tt>PyObject</tt>.</p>
<p>(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-33.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-33.png"></p></div>
<p>When we call the <tt>add</tt> method on the <tt>Tensor</tt> class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a "Type" object.  This method is the actual virtual method (this is why I say Type is just a "gadget" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named <tt>TypeDefault</tt>.  This happens to be because we have an implementation of <tt>add</tt> that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like <tt><span>CPUFloatType::add</span></tt>.  It is this implementation of the virtual method that finally gets us to the actual kernel code.</p>
<!--  -->
<blockquote>
Hopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing <tt>Type</tt> dispatch with another mechanism which will help us better support PyTorch on mobile.</blockquote>
<p>It's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-34.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-34.png"></p></div>
<p>It's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-35.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-35.png"></p></div>
<p>PyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-36.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-36.png"></p></div>
<p>We generally think of a kernel in PyTorch consisting of the following parts:</p>
<ol>
<li>First, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.</li>
<li>Once you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)</li>
<li>Next, we generally have to allocate the result tensor which we are going to write the output into.</li>
<li>Time for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)</li>
<li>Most performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are "implicitly" parallelized, since their programming model is built on top of massive parallelization).</li>
<li>Finally, you need to access the data and do the computation you wanted to do!</li>
</ol>
<p>In the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-37.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-37.png"></p></div>
<p>To take advantage of all of the code generation which PyTorch brings, you need to write a <em>schema</em> for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the <a href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md">README in native</a> is for more information about this format.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-38.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-38.png"></p></div>
<p>You also may need to define a derivative for your operation in <a href="https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml">derivatives.yaml</a>.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-39.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-39.png"></p></div>
<p>Error checking can be done by way of either a low level or a high level API.  The low level API is just a macro, <tt>TORCH_CHECK</tt>, which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of <tt>operator&lt;&lt;</tt>, and most important data types in PyTorch have <tt>operator&lt;&lt;</tt> implementations.</p>
<p>The high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each <tt>Tensor</tt> into a <tt>TensorArg</tt>, which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g., <tt>checkDim()</tt> tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the <tt>TensorArg</tt> metadata.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-40.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-40.png"></p></div>
<p>One important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write <em>three</em> operators: <tt>abs_out</tt>, which operates on a preallocated output (this implements the <tt>out=</tt> keyword argument), <tt>abs_</tt>, which operates inplace, and <tt>abs</tt>, which is the plain old functional version of an operator.</p>
<p>Most of the time, <tt>abs_out</tt> is the real workhorse, and <tt>abs</tt> and <tt>abs_</tt> are just thin wrappers around <tt>abs_out</tt>; but sometimes writing specialized implementations for each case are warranted.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-41.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-41.png"></p></div>
<p>To do dtype dispatch, you should use the <tt>AT_DISPATCH_ALL_TYPES</tt> macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.</p>
<p>This macro doesn't just "do dispatch", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want <tt>AT_DISPATCH_ALL_TYPES</tt>, but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in <a href="https://github.com/pytorch/pytorch/blob/21ef4cc615a7d9d772ade52a5023900718b09e92/aten/src/ATen/Dispatch.h#L62">Dispatch.h</a> for how to select the correct one for your use-case.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-43.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-43.png"></p></div>
<p>On CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-42.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-42.png"></p></div>
<p>At some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.</p>
<ol>
<li>If you just want to get a value at some specific location, you should use <tt>TensorAccessor</tt>.  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like <tt>x.accessor&lt;float, <span>3&gt;();</span></tt>, we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a <tt>PackedTensorAccessor</tt>, which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha: <tt>TensorAccessor</tt> defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)</li>
<li>If you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the <tt>TensorIterator</tt>.   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.</li>
<li>For true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The <tt>Vec256</tt> class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like <tt>binary_kernel_vec</tt> then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.</li>
</ol>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-44.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-44.png"></p></div>
<p>A lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?</p>
<ol>
<li>It's written in C style, no (or very little) use of C++.</li>
<li>It's manually refcounted (with manual calls to <tt>THTensor_free</tt> to decrease refcounts when you're done using tensors), and</li>
<li>It lives in <tt>generic/</tt> directory, which means that we are actually going to compile the file multiple times, but with different <tt>#define scalar_t</tt>.</li>
</ol>
<p>This code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-45.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-45.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-46.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-46.png"></p></div>
<p>To wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, <strong>you will have a bad time</strong>: it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.</p>
<p>How to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: "It's hard to work on PyTorch."</p>
<ol>
<li>If you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!</li>
<li>Our CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can <a href="https://github.com/pytorch/ossci-job-dsl">download and run the Docker images locally</a></li>
<li>The <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#use-ccache">CONTRIBUTING guide explains how to setup ccache</a>; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.</li>
<li>At the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.</li>
</ol>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-47.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-47.png"></p></div>
<p>So that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.</p>
<p>Where should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled <strong>triaged</strong> mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22high+priority%22+label%3Atriaged">high priority</a> or look up issues specific to some module, e.g., <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3A%22module%3A+autograd%22">autograd</a> or find issues which we think are <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3Asmall">small</a> (word of warning: we're sometimes wrong!)</p>
<p>Even if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I <em>love</em> merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding R1-Zero-Like Training: A Critical Perspective (114 pts)]]></title>
            <link>https://github.com/sail-sg/understand-r1-zero</link>
            <guid>43445894</guid>
            <pubDate>Sat, 22 Mar 2025 14:35:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sail-sg/understand-r1-zero">https://github.com/sail-sg/understand-r1-zero</a>, See on <a href="https://news.ycombinator.com/item?id=43445894">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">Understanding R1-Zero-Like Training: A Critical Perspective</h2><a id="user-content-understanding-r1-zero-like-training-a-critical-perspective" aria-label="Permalink: Understanding R1-Zero-Like Training: A Critical Perspective" href="#understanding-r1-zero-like-training-a-critical-perspective"></a></p>
<p dir="auto"><a href="https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf"><img src="https://camo.githubusercontent.com/b8b7a1f24d0115db199a318f8baba58ee41a4e62682fe54b18361fc57a7acceb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617065722d3843413141463f6c6f676f3d72656164746865646f6373266c6f676f436f6c6f723d7768697465" alt="Paper" data-canonical-src="https://img.shields.io/badge/Paper-8CA1AF?logo=readthedocs&amp;logoColor=white"></a></p>
<p dir="auto"><a href="https://github.com/sail-sg/understand-r1-zero"><img src="https://camo.githubusercontent.com/254c537f0b39d9dd4f7190bc33f014ebac3cc91df480eb0cf914615a568a8612/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f556e6465727374616e6425323052312532305a65726f2d3030303030303f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d303030266c6f676f436f6c6f723d7768697465" alt="Github" data-canonical-src="https://img.shields.io/badge/Understand%20R1%20Zero-000000?style=for-the-badge&amp;logo=github&amp;logoColor=000&amp;logoColor=white"></a>  <a href="https://huggingface.co/collections/sail/oat-zero-understanding-r1-zero-like-training-67dcdb07b9f3eb05f1501c4a" rel="nofollow"><img src="https://camo.githubusercontent.com/4de185a217b7c8b03b23d64603118bd1247c2be4b8d46529d0141177fc31fe2e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f64656c5f436f6c6c656374696f6e2d6663643032323f7374796c653d666f722d7468652d6261646765266c6f676f3d68756767696e6766616365266c6f676f436f6c6f723d303030" alt="Hugging Face Collection" data-canonical-src="https://img.shields.io/badge/Model_Collection-fcd022?style=for-the-badge&amp;logo=huggingface&amp;logoColor=000"></a></p>

</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Updates</h2><a id="user-content-updates" aria-label="Permalink: Updates" href="#updates"></a></p>
<ul dir="auto">
<li>21/03/2025: 🎉 We release our paper, models and codebase. Our R1-Zero training is implemented with 🌾 <a href="https://github.com/sail-sg/oat">Oat</a>, a highly modular, research-friendly and efficient LLM RL framework.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Links</h2><a id="user-content-links" aria-label="Permalink: Links" href="#links"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Understanding R1-Zero-Like Training</strong></p>
<ul dir="auto">
<li>📄 <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf">Paper</a></li>
<li>🤗 <a href="https://huggingface.co/collections/sail/oat-zero-understanding-r1-zero-like-training-67dcdb07b9f3eb05f1501c4a" rel="nofollow">Models</a></li>
</ul>
</li>
<li>
<p dir="auto"><strong>There May Not Be Aha Moment in R1-Zero-like Training — A Pilot Study</strong></p>
<ul dir="auto">
<li>📄 <a href="https://oatllm.notion.site/oat-zero" rel="nofollow">Blog</a></li>
<li>💻 <a href="https://github.com/sail-sg/oat-zero">Code</a></li>
</ul>
</li>
<li>
<p dir="auto"><strong>OAT: A research-friendly framework for LLM online alignment</strong></p>
<ul dir="auto">
<li>💻 <a href="https://github.com/sail-sg/oat">Codebase</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">TL;DR</h2><a id="user-content-tldr" aria-label="Permalink: TL;DR" href="#tldr"></a></p>
<p dir="auto">To understand R1-Zero-like training, we critically examine two core components: <strong>base models</strong>
and <strong>reinforcement learning</strong>. We highlight our findings below.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">On base models:</h3><a id="user-content-on-base-models" aria-label="Permalink: On base models:" href="#on-base-models"></a></p>
<ol dir="auto">
<li><strong>DeepSeek-V3-Base already exhibit "Aha moment"</strong>.</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/deepseek-base-aha.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/deepseek-base-aha.png" width="70%/"></a>
</p>
<ol start="2" dir="auto">
<li>As the popular choice for R1-Zero-like training, Qwen2.5 base models demonstrate strong reasoning capabilities
even <strong>without</strong> prompt templates: the average benchmark scores improve by <strong>~60%</strong> (compared to the traditional 4-shot prompting)!</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/qwen-math-base-scores.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/qwen-math-base-scores.png" width="70%/"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">On reinforcement learning:</h3><a id="user-content-on-reinforcement-learning" aria-label="Permalink: On reinforcement learning:" href="#on-reinforcement-learning"></a></p>
<ol start="3" dir="auto">
<li>GRPO leads to <strong>biased</strong> optimization! We propose a simple fix that improves token efficiency
while maintaining reasoning performance, termed as Dr. GRPO (GRPO <strong>D</strong>one <strong>R</strong>ight).</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/drgrpo.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/drgrpo.png" width="80%/"></a>
</p>
<ol start="4" dir="auto">
<li>In R1-Zero-like training, the template and the question set perform a duet to affect the RL dynamics
<ul dir="auto">
<li>(Left Plot) For Qwen2.5-Math-1.5B, a mismatched template (e.g., R1 template) in fact <strong>destructs the reasoning capabilities before RL reconstructing it</strong>. This makes the improvement impressive on the surface.</li>
<li>(Middle Plot) However, if a template does not deviate from the pretraining distribution too far, even a small and completely o.o.d. question set (e.g., GSM8K) could induce the reasoning ability equally well, by reinforcing correct reasoning behaviors instead of infusing new knowledge.</li>
</ul>
</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/template-data-duet.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/template-data-duet.png" width="80%/"></a>
</p>
<ol start="5" dir="auto">
<li>Beyond Qwen, Llama can also be RL-tuned from base models. In this case, domain-specific pretraining will improves RL ceiling.
<ul dir="auto">
<li>(Right Plot) GRPO can even make Llama with math knowledge "Aha" by increasing the output length; however, it is likely due to its length bias, which can be removed by Dr. GRPO.</li>
</ul>
</li>
</ol>
 <p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/llama-r1-zero.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/llama-r1-zero.png" width="70%/"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Our minimalist R1-Zero recipe:</h3><a id="user-content-our-minimalist-r1-zero-recipe" aria-label="Permalink: Our minimalist R1-Zero recipe:" href="#our-minimalist-r1-zero-recipe"></a></p>
<p dir="auto">Our analysis suggests a minimalist recipe for R1-Zero-like training:</p>
<p dir="auto">We RL-tune Qwen2.5-
Math-7B using the (unbiased) Dr. GRPO algorithm on MATH level 3-5 questions with the Qwen-Math template, and achieve state-of-the-art performance with only 27 hours compute on 8× A100 GPUs.</p>
 <p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/benchmark.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/benchmark.png" width="90%/"></a>
</p>
<p dir="auto">If you are interested in more details, please check out our <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf">paper</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install</h3><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto">We recommend a clean <code>python==3.10</code> environment for development.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install vllm &amp; oat, the LLM RL framework we developed r1-zero training on.
pip install vllm==0.7.2 &amp;&amp; pip install oat-llm==0.0.9

# Install this package locally to use the math grader.
git clone git@github.com:sail-sg/understand-r1-zero.git &amp;&amp; cd understand-r1-zero
pip install -e ."><pre><span><span>#</span> Install vllm &amp; oat, the LLM RL framework we developed r1-zero training on.</span>
pip install vllm==0.7.2 &amp;&amp; pip install oat-llm==0.0.9

<span><span>#</span> Install this package locally to use the math grader.</span>
git clone git@github.com:sail-sg/understand-r1-zero.git &amp;&amp; cd understand-r1-zero
pip install -e .</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Training</h3><a id="user-content-training" aria-label="Permalink: Training" href="#training"></a></p>
<p dir="auto">We implement R1-Zero training by extending Oat's Learner and Actor components. Please see <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/train_zero_math.py">train_zero_math.py</a> for a step-by-step guide.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Patch LD_LIBRARY_PATH to avoid dependency errors:
export LD_LIBRARY_PATH=$(python -c &quot;import sysconfig; print(sysconfig.get_config_var('LIBDIR'))&quot;):$LD_LIBRARY_PATH

# Run the experiment (tested on 8 x A100-40G) with Dr. GRPO:
# (change to `--critic_type grpo` for running GRPO)
python train_zero_math.py \
    --critic_type drgrpo \
    --gpus 8 \
    --enable_prefix_caching \
    --collocate \
    --vllm_sleep \
    --vllm_gpu_ratio 0.35 \
    --gradient-checkpointing \
    --flash-attn \
    --bf16 \
    --rnd-seed \
    --learning_rate 0.000001 \
    --lr_scheduler constant \
    --num_ppo_epochs 1 \
    --beta 0 \
    --oracle_type reward \
    --oracle math \
    --pretrain Qwen/Qwen2.5-Math-1.5B \
    --prompt_template r1 \
    --zero-stage 2 \
    --ref_offload \
    --prompt_data ./datasets/train/math_12k \
    --train_split train \
    --input_key problem \
    --output_key answer \
    --max-train 9999999 \
    --num_prompt_epoch 20 \
    --prompt_max_length 1024 \
    --num_samples 8 \
    --temperature 1 \
    --top_p 1 \
    --generate_max_length 3000 \
    --save_steps -1 \
    --train_batch_size 128 \
    --rollout_batch_size 128 \
    --rollout_batch_size_per_device 16 \
    --pi_buffer_maxlen_per_device 128 \
    --eval_batch_size 200 \
    --eval_steps 16 \
    --eval_temperature 0 \
    --eval_generate_max_length 3000 \
    --eval_data ./datasets/evaluation_suite \
    --eval_input_key input \
    --use-wb \
    --wb-run-name qwen2.5-Math-1.5b-r1-zero \
    --wb_project oat-zero"><pre><span><span>#</span> Patch LD_LIBRARY_PATH to avoid dependency errors:</span>
export LD_LIBRARY_PATH=$(python -c "import sysconfig; print(sysconfig.get_config_var('LIBDIR'))"):$LD_LIBRARY_PATH

<span><span>#</span> Run the experiment (tested on 8 x A100-40G) with Dr. GRPO:</span>
<span><span>#</span> (change to `--critic_type grpo` for running GRPO)</span>
python train_zero_math.py \
    --critic_type drgrpo \
    --gpus 8 \
    --enable_prefix_caching \
    --collocate \
    --vllm_sleep \
    --vllm_gpu_ratio 0.35 \
    --gradient-checkpointing \
    --flash-attn \
    --bf16 \
    --rnd-seed \
    --learning_rate 0.000001 \
    --lr_scheduler constant \
    --num_ppo_epochs 1 \
    --beta 0 \
    --oracle_type reward \
    --oracle math \
    --pretrain Qwen/Qwen2.5-Math-1.5B \
    --prompt_template r1 \
    --zero-stage 2 \
    --ref_offload \
    --prompt_data ./datasets/train/math_12k \
    --train_split train \
    --input_key problem \
    --output_key answer \
    --max-train 9999999 \
    --num_prompt_epoch 20 \
    --prompt_max_length 1024 \
    --num_samples 8 \
    --temperature 1 \
    --top_p 1 \
    --generate_max_length 3000 \
    --save_steps -1 \
    --train_batch_size 128 \
    --rollout_batch_size 128 \
    --rollout_batch_size_per_device 16 \
    --pi_buffer_maxlen_per_device 128 \
    --eval_batch_size 200 \
    --eval_steps 16 \
    --eval_temperature 0 \
    --eval_generate_max_length 3000 \
    --eval_data ./datasets/evaluation_suite \
    --eval_input_key input \
    --use-wb \
    --wb-run-name qwen2.5-Math-1.5b-r1-zero \
    --wb_project oat-zero</pre></div>
<p dir="auto">Please see <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/examples">here</a> for more example scripts.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Evaluation</h3><a id="user-content-evaluation" aria-label="Permalink: Evaluation" href="#evaluation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Evaluate our models:
python evaluate_model.py --model_name sail/Qwen2.5-Math-7B-Oat-Zero
python evaluate_model.py --model_name sail/Qwen2.5-Math-1.5B-Oat-Zero
python evaluate_model.py --model_name sail/Llama-3.2-3B-Oat-Zero --template r1

# Evaluate baseline models:
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-1.5B
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-7B
python evaluate_model.py --model_name hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zero
python evaluate_model.py --model_name PRIME-RL/Eurus-2-7B-PRIME-Zero
python evaluate_model.py --model_name Open-Reasoner-Zero/Open-Reasoner-Zero-7B"><pre><span><span>#</span> Evaluate our models:</span>
python evaluate_model.py --model_name sail/Qwen2.5-Math-7B-Oat-Zero
python evaluate_model.py --model_name sail/Qwen2.5-Math-1.5B-Oat-Zero
python evaluate_model.py --model_name sail/Llama-3.2-3B-Oat-Zero --template r1

<span><span>#</span> Evaluate baseline models:</span>
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-1.5B
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-7B
python evaluate_model.py --model_name hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zero
python evaluate_model.py --model_name PRIME-RL/Eurus-2-7B-PRIME-Zero
python evaluate_model.py --model_name Open-Reasoner-Zero/Open-Reasoner-Zero-7B</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you find our work useful for your research, please consider citing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{liu2025understanding,
  title={Understanding R1-Zero-Like Training: A Critical Perspective},
  author={Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin},
  year={2025},
  howpublished={\url{https://github.com/sail-sg/understand-r1-zero}},
}"><pre><span>@misc</span>{<span>liu2025understanding</span>,
  <span>title</span>=<span><span>{</span>Understanding R1-Zero-Like Training: A Critical Perspective<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
  <span>howpublished</span>=<span><span>{</span>\url{https://github.com/sail-sg/understand-r1-zero}<span>}</span></span>,
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgement</h2><a id="user-content-acknowledgement" aria-label="Permalink: Acknowledgement" href="#acknowledgement"></a></p>
<ul dir="auto">
<li>This work is supported by <a href="https://sail.sea.com/" rel="nofollow">Sea AI Lab</a> for computing resources.</li>
<li>The training codes are built on <a href="https://github.com/sail-sg/oat">Oat</a>, which employs <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a> and <a href="https://github.com/google-deepmind/launchpad">launchpad</a>.</li>
<li>The base models are from <a href="https://huggingface.co/Qwen/Qwen2.5-Math-7B" rel="nofollow">Qwen2.5-Math</a>, <a href="https://huggingface.co/meta-llama/Llama-3.2-3B" rel="nofollow">Llama</a>, and <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base" rel="nofollow">DeepSeek</a>.</li>
<li>We thank Qingfeng Lan for his time in thoroughly reviewing our code.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: FastOpenAPI – automated docs for many Python frameworks (123 pts)]]></title>
            <link>https://github.com/mr-fatalyst/fastopenapi</link>
            <guid>43445720</guid>
            <pubDate>Sat, 22 Mar 2025 14:10:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mr-fatalyst/fastopenapi">https://github.com/mr-fatalyst/fastopenapi</a>, See on <a href="https://news.ycombinator.com/item?id=43445720">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mr-fatalyst/fastopenapi/master/logo.png"><img src="https://raw.githubusercontent.com/mr-fatalyst/fastopenapi/master/logo.png" alt="Logo"></a>
</p>
<p dir="auto">
  <b>FastOpenAPI</b> is a library for generating and integrating OpenAPI schemas using Pydantic and various frameworks.
</p>
<p dir="auto">
  This project was inspired by <a href="https://fastapi.tiangolo.com/" rel="nofollow">FastAPI</a> and aims to provide a similar developer-friendly experience.
</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/95207bc5a4752edda90d15266a572544a3dbb84deb37be8db3f51ec6a8b7d47a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d722d666174616c7973742f666173746f70656e617069"><img src="https://camo.githubusercontent.com/95207bc5a4752edda90d15266a572544a3dbb84deb37be8db3f51ec6a8b7d47a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d722d666174616c7973742f666173746f70656e617069" data-canonical-src="https://img.shields.io/github/license/mr-fatalyst/fastopenapi"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/mr-fatalyst/fastopenapi/actions/workflows/master.yml/badge.svg"><img src="https://github.com/mr-fatalyst/fastopenapi/actions/workflows/master.yml/badge.svg"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5c66ef23261df1a6bde8781d2cf6f4d04e2fbd46e5776cab7b6ffb384f83f4ff/68747470733a2f2f636f6465636f762e696f2f67682f6d722d666174616c7973742f666173746f70656e6170692f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d55534852314930434a42"><img src="https://camo.githubusercontent.com/5c66ef23261df1a6bde8781d2cf6f4d04e2fbd46e5776cab7b6ffb384f83f4ff/68747470733a2f2f636f6465636f762e696f2f67682f6d722d666174616c7973742f666173746f70656e6170692f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d55534852314930434a42" data-canonical-src="https://codecov.io/gh/mr-fatalyst/fastopenapi/branch/master/graph/badge.svg?token=USHR1I0CJB"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/59e6735443dc41ae21b13a0392b65b11928e6f239981db11985ca745ab0c23b9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f666173746f70656e617069"><img src="https://camo.githubusercontent.com/59e6735443dc41ae21b13a0392b65b11928e6f239981db11985ca745ab0c23b9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f666173746f70656e617069" data-canonical-src="https://img.shields.io/pypi/v/fastopenapi"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a8e0850dc525c2d40b3cf3ba469031461d1cc9f23f40f2d79f3df0c5d9674709/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f666173746f70656e617069"><img src="https://camo.githubusercontent.com/a8e0850dc525c2d40b3cf3ba469031461d1cc9f23f40f2d79f3df0c5d9674709/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f666173746f70656e617069" data-canonical-src="https://img.shields.io/pypi/pyversions/fastopenapi"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/37049c06136b234af5254472216d00bb66a6e2436a06246a0036e2ef25f75da2/68747470733a2f2f7374617469632e706570792e746563682f62616467652f666173746f70656e617069"><img src="https://camo.githubusercontent.com/37049c06136b234af5254472216d00bb66a6e2436a06246a0036e2ef25f75da2/68747470733a2f2f7374617469632e706570792e746563682f62616467652f666173746f70656e617069" alt="PyPI Downloads" data-canonical-src="https://static.pepy.tech/badge/fastopenapi"></a>
</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📦 Installation</h2><a id="user-content--installation" aria-label="Permalink: 📦 Installation" href="#-installation"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Install only FastOpenAPI:</h4><a id="user-content-install-only-fastopenapi" aria-label="Permalink: Install only FastOpenAPI:" href="#install-only-fastopenapi"></a></p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Install FastOpenAPI with a specific framework:</h4><a id="user-content-install-fastopenapi-with-a-specific-framework" aria-label="Permalink: Install FastOpenAPI with a specific framework:" href="#install-fastopenapi-with-a-specific-framework"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install fastopenapi[falcon]"><pre>pip install fastopenapi[falcon]</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="pip install fastopenapi[flask]"><pre>pip install fastopenapi[flask]</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="pip install fastopenapi[sanic]"><pre>pip install fastopenapi[sanic]</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="pip install fastopenapi[starlette]"><pre>pip install fastopenapi[starlette]</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="pip install fastopenapi[tornado]"><pre>pip install fastopenapi[tornado]</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛠️ Quick Start</h2><a id="user-content-️-quick-start" aria-label="Permalink: 🛠️ Quick Start" href="#️-quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Step 1. Create an application</h3><a id="user-content-step-1-create-an-application" aria-label="Permalink: Step 1. Create an application" href="#step-1-create-an-application"></a></p>
<ul dir="auto">
<li>Create the <code>main.py</code> file</li>
<li>Copy the code from an example</li>
<li>For some examples uvicorn is required (<code>pip install uvicorn</code>)</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Examples:</h4><a id="user-content-examples" aria-label="Permalink: Examples:" href="#examples"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3d06f6d932d933ba71c0c56c68a06e5a89e2a2774b1e12ba41aa0f5358d7b320/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f46616c636f6e2d3435623864383f7374796c653d666c6174266c6f676f3d66616c636f6e266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/3d06f6d932d933ba71c0c56c68a06e5a89e2a2774b1e12ba41aa0f5358d7b320/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f46616c636f6e2d3435623864383f7374796c653d666c6174266c6f676f3d66616c636f6e266c6f676f436f6c6f723d7768697465" alt="Falcon" data-canonical-src="https://img.shields.io/badge/Falcon-45b8d8?style=flat&amp;logo=falcon&amp;logoColor=white"></a></p>
<details>
  <summary>Click to expand the Falcon Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="import falcon.asgi
import uvicorn
from pydantic import BaseModel

from fastopenapi.routers import FalconRouter

app = falcon.asgi.App()
router = FalconRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
async def hello(name: str):
    &quot;&quot;&quot;Say hello from Falcon&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Falcon!&quot;)


if __name__ == &quot;__main__&quot;:
    uvicorn.run(app, host=&quot;127.0.0.1&quot;, port=8000)"><pre><span>import</span> <span>falcon</span>.<span>asgi</span>
<span>import</span> <span>uvicorn</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span> <span>import</span> <span>FalconRouter</span>

<span>app</span> <span>=</span> <span>falcon</span>.<span>asgi</span>.<span>App</span>()
<span>router</span> <span>=</span> <span>FalconRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>async</span> <span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Falcon"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Falcon!"</span>)


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>uvicorn</span>.<span>run</span>(<span>app</span>, <span>host</span><span>=</span><span>"127.0.0.1"</span>, <span>port</span><span>=</span><span>8000</span>)</pre></div>
</details>
</li>
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/68c5bb6d17237e3c5de59740254e6ec055b6d781415ac1b8712bdb588660e266/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d466c61736b2d4545454545453f7374796c653d666c6174266c6f676f3d666c61736b266c6f676f436f6c6f723d626c61636b"><img src="https://camo.githubusercontent.com/68c5bb6d17237e3c5de59740254e6ec055b6d781415ac1b8712bdb588660e266/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d466c61736b2d4545454545453f7374796c653d666c6174266c6f676f3d666c61736b266c6f676f436f6c6f723d626c61636b" alt="Flask" data-canonical-src="https://img.shields.io/badge/-Flask-EEEEEE?style=flat&amp;logo=flask&amp;logoColor=black"></a></p>
<details>
  <summary>Click to expand the Flask Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="from flask import Flask
from pydantic import BaseModel

from fastopenapi.routers import FlaskRouter

app = Flask(__name__)
router = FlaskRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
def hello(name: str):
    &quot;&quot;&quot;Say hello from Flask&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Flask!&quot;)


if __name__ == &quot;__main__&quot;:
    app.run(port=8000)"><pre><span>from</span> <span>flask</span> <span>import</span> <span>Flask</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span> <span>import</span> <span>FlaskRouter</span>

<span>app</span> <span>=</span> <span>Flask</span>(<span>__name__</span>)
<span>router</span> <span>=</span> <span>FlaskRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Flask"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Flask!"</span>)


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>app</span>.<span>run</span>(<span>port</span><span>=</span><span>8000</span>)</pre></div>
</details>
</li>
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/40014866e3e1b3170af04182fddb5675874cd752bf04329133b7092d971b68fc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d51756172742d3439393744303f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/40014866e3e1b3170af04182fddb5675874cd752bf04329133b7092d971b68fc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d51756172742d3439393744303f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465" alt="Quart" data-canonical-src="https://img.shields.io/badge/-Quart-4997D0?style=flat&amp;logo=python&amp;logoColor=white"></a></p>
<details>
  <summary>Click to expand the Quart Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="from pydantic import BaseModel
from quart import Quart

from fastopenapi.routers import QuartRouter

app = Quart(__name__)
router = QuartRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
async def hello(name: str):
    &quot;&quot;&quot;Say hello from Quart&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Quart!&quot;)


if __name__ == &quot;__main__&quot;:
    app.run(port=8000)"><pre><span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>
<span>from</span> <span>quart</span> <span>import</span> <span>Quart</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span> <span>import</span> <span>QuartRouter</span>

<span>app</span> <span>=</span> <span>Quart</span>(<span>__name__</span>)
<span>router</span> <span>=</span> <span>QuartRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>async</span> <span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Quart"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Quart!"</span>)


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>app</span>.<span>run</span>(<span>port</span><span>=</span><span>8000</span>)</pre></div>
</details>
</li>
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/aa08690e4c157edae7675b8b72b3797aae7c461734c01d79f9f50dabda226d4f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d53616e69632d3030626666663f7374796c653d666c6174266c6f676f3d73616e6963266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/aa08690e4c157edae7675b8b72b3797aae7c461734c01d79f9f50dabda226d4f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d53616e69632d3030626666663f7374796c653d666c6174266c6f676f3d73616e6963266c6f676f436f6c6f723d7768697465" alt="Sanic" data-canonical-src="https://img.shields.io/badge/-Sanic-00bfff?style=flat&amp;logo=sanic&amp;logoColor=white"></a></p>
<details>
  <summary>Click to expand the Sanic Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="from pydantic import BaseModel
from sanic import Sanic

from fastopenapi.routers import SanicRouter

app = Sanic(&quot;MySanicApp&quot;)
router = SanicRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
async def hello(name: str):
    &quot;&quot;&quot;Say hello from Sanic&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Sanic!&quot;)


if __name__ == &quot;__main__&quot;:
    app.run(host=&quot;0.0.0.0&quot;, port=8000)"><pre><span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>
<span>from</span> <span>sanic</span> <span>import</span> <span>Sanic</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span> <span>import</span> <span>SanicRouter</span>

<span>app</span> <span>=</span> <span>Sanic</span>(<span>"MySanicApp"</span>)
<span>router</span> <span>=</span> <span>SanicRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>async</span> <span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Sanic"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Sanic!"</span>)


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>app</span>.<span>run</span>(<span>host</span><span>=</span><span>"0.0.0.0"</span>, <span>port</span><span>=</span><span>8000</span>)</pre></div>
</details>
</li>
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7d050f9c2c3198ebf9345a40bb456e217b6c2184bfeae53de6d26ab02bcae5b7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d537461726c657474652d3442303038323f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/7d050f9c2c3198ebf9345a40bb456e217b6c2184bfeae53de6d26ab02bcae5b7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d537461726c657474652d3442303038323f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465" alt="Starlette" data-canonical-src="https://img.shields.io/badge/-Starlette-4B0082?style=flat&amp;logo=python&amp;logoColor=white"></a></p>
<details>
  <summary>Click to expand the Starlette Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="import uvicorn
from pydantic import BaseModel
from starlette.applications import Starlette

from fastopenapi.routers import StarletteRouter

app = Starlette()
router = StarletteRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
async def hello(name: str):
    &quot;&quot;&quot;Say hello from Starlette&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Starlette!&quot;)

if __name__ == &quot;__main__&quot;:
    uvicorn.run(app, host=&quot;127.0.0.1&quot;, port=8000)"><pre><span>import</span> <span>uvicorn</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>
<span>from</span> <span>starlette</span>.<span>applications</span> <span>import</span> <span>Starlette</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span> <span>import</span> <span>StarletteRouter</span>

<span>app</span> <span>=</span> <span>Starlette</span>()
<span>router</span> <span>=</span> <span>StarletteRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>async</span> <span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Starlette"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Starlette!"</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>uvicorn</span>.<span>run</span>(<span>app</span>, <span>host</span><span>=</span><span>"127.0.0.1"</span>, <span>port</span><span>=</span><span>8000</span>)</pre></div>
</details>
</li>
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e6db5c55534d9a6d47361d7ce0d82d1141ab481e3bf08d5c106f0be85fe0c94d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d546f726e61646f2d3239383042393f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/e6db5c55534d9a6d47361d7ce0d82d1141ab481e3bf08d5c106f0be85fe0c94d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d546f726e61646f2d3239383042393f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465" alt="Tornado" data-canonical-src="https://img.shields.io/badge/-Tornado-2980B9?style=flat&amp;logo=python&amp;logoColor=white"></a></p>
<details>
  <summary>Click to expand the Tornado Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio

from pydantic import BaseModel
from tornado.web import Application

from fastopenapi.routers.tornado import TornadoRouter

app = Application()

router = TornadoRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
def hello(name: str):
    &quot;&quot;&quot;Say hello from Tornado&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Tornado!&quot;)


async def main():
    app.listen(8000)
    await asyncio.Event().wait()


if __name__ == &quot;__main__&quot;:
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>

<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>
<span>from</span> <span>tornado</span>.<span>web</span> <span>import</span> <span>Application</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span>.<span>tornado</span> <span>import</span> <span>TornadoRouter</span>

<span>app</span> <span>=</span> <span>Application</span>()

<span>router</span> <span>=</span> <span>TornadoRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Tornado"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Tornado!"</span>)


<span>async</span> <span>def</span> <span>main</span>():
    <span>app</span>.<span>listen</span>(<span>8000</span>)
    <span>await</span> <span>asyncio</span>.<span>Event</span>().<span>wait</span>()


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
</details>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Step 2. Run the server</h3><a id="user-content-step-2-run-the-server" aria-label="Permalink: Step 2. Run the server" href="#step-2-run-the-server"></a></p>
<p dir="auto">Launch the application:</p>

<p dir="auto">Once launched, the documentation will be available at:</p>
<p dir="auto">Swagger UI:</p>
<div data-snippet-clipboard-copy-content="http://127.0.0.1:8000/docs"><pre><code>http://127.0.0.1:8000/docs
</code></pre></div>
<p dir="auto">ReDoc UI:</p>
<div data-snippet-clipboard-copy-content="http://127.0.0.1:8000/redoc"><pre><code>http://127.0.0.1:8000/redoc
</code></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">⚙️ Features</h2><a id="user-content-️-features" aria-label="Permalink: ⚙️ Features" href="#️-features"></a></p>
<ul dir="auto">
<li><strong>Generate OpenAPI schemas</strong> with Pydantic v2.</li>
<li><strong>Data validation</strong> using Pydantic models.</li>
<li><strong>Supports multiple frameworks:</strong> Falcon, Flask, Quart, Sanic, Starlette, Tornado.</li>
<li><strong>Proxy routing provides FastAPI-style routing</strong></li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📖 Documentation</h2><a id="user-content--documentation" aria-label="Permalink: 📖 Documentation" href="#-documentation"></a></p>
<p dir="auto">Explore the <a href="https://github.com/mr-fatalyst/fastopenapi/blob/master/docs/en/index.md">Docs</a> for an overview of FastOpenAPI, its core components, and usage guidelines. The documentation is continuously updated and improved.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📂 Advanced Examples</h2><a id="user-content--advanced-examples" aria-label="Permalink: 📂 Advanced Examples" href="#-advanced-examples"></a></p>
<p dir="auto">Examples of integration and detailed usage for each framework are available in the <a href="https://github.com/mr-fatalyst/fastopenapi/tree/master/examples"><code>examples</code></a> directory.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📊 Quick &amp; Dirty Benchmarks</h2><a id="user-content--quick--dirty-benchmarks" aria-label="Permalink: 📊 Quick &amp; Dirty Benchmarks" href="#-quick--dirty-benchmarks"></a></p>
<p dir="auto">Fast but not perfect benchmarks. Check the <a href="https://github.com/mr-fatalyst/fastopenapi/tree/master/benchmarks"><code>benchmarks</code></a> directory for details.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">✅ Development Recommendations</h2><a id="user-content--development-recommendations" aria-label="Permalink: ✅ Development Recommendations" href="#-development-recommendations"></a></p>
<ul dir="auto">
<li>Use Pydantic models for strict typing and data validation.</li>
<li>Follow the project structure similar to provided examples for easy scalability.</li>
<li>Regularly update dependencies and monitor library updates for new features.</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛠️ Contributing</h2><a id="user-content-️-contributing" aria-label="Permalink: 🛠️ Contributing" href="#️-contributing"></a></p>
<p dir="auto">If you have suggestions or find a bug, please open an issue or create a pull request on GitHub.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 <strong>License</strong></h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">This project is licensed under the terms of the MIT license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Landrun: Sandbox any Linux process using Landlock, no root or containers (317 pts)]]></title>
            <link>https://github.com/Zouuup/landrun</link>
            <guid>43445662</guid>
            <pubDate>Sat, 22 Mar 2025 13:56:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Zouuup/landrun">https://github.com/Zouuup/landrun</a>, See on <a href="https://news.ycombinator.com/item?id=43445662">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">landrun</h2><a id="user-content-landrun" aria-label="Permalink: landrun" href="#landrun"></a></p>
<p dir="auto">A lightweight, secure sandbox for running Linux processes using Landlock LSM. Think firejail, but with kernel-level security and minimal overhead.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🔒 Kernel-level security using Landlock LSM</li>
<li>🚀 Lightweight and fast execution</li>
<li>🛡️ Fine-grained access control for directories</li>
<li>🔄 Support for read and write paths</li>
<li>⚡ Optional execution permissions for allowed paths</li>
<li>🌐 TCP network access control (binding and connecting)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Zouuup/landrun/blob/main/demo.gif"><img src="https://github.com/Zouuup/landrun/raw/main/demo.gif" alt="landrun demo" width="700" data-animated-image=""></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Linux kernel 5.13 or later with Landlock LSM enabled</li>
<li>Linux kernel 6.8 or later for network restrictions (TCP bind/connect)</li>
<li>Go 1.18 or later (for building from source)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Install</h3><a id="user-content-quick-install" aria-label="Permalink: Quick Install" href="#quick-install"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="go install github.com/zouuup/landrun/cmd/landrun@latest"><pre>go install github.com/zouuup/landrun/cmd/landrun@latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">From Source</h3><a id="user-content-from-source" aria-label="Permalink: From Source" href="#from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/zouuup/landrun.git
cd landrun
go build -o landrun cmd/landrun/main.go
sudo cp landrun /usr/local/bin/"><pre>git clone https://github.com/zouuup/landrun.git
<span>cd</span> landrun
go build -o landrun cmd/landrun/main.go
sudo cp landrun /usr/local/bin/</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Basic syntax:</p>
<div dir="auto" data-snippet-clipboard-copy-content="landrun [options] <command> [args...]"><pre>landrun [options] <span>&lt;</span>command<span>&gt;</span> [args...]</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Options</h3><a id="user-content-options" aria-label="Permalink: Options" href="#options"></a></p>
<ul dir="auto">
<li><code>--ro &lt;path&gt;</code>: Allow read-only access to specified path (can be specified multiple times)</li>
<li><code>--rw &lt;path&gt;</code>: Allow read-write access to specified path (can be specified multiple times)</li>
<li><code>--exec</code>: Allow executing files in allowed paths</li>
<li><code>--bind-tcp &lt;port&gt;</code>: Allow binding to specified TCP port (can be specified multiple times)</li>
<li><code>--connect-tcp &lt;port&gt;</code>: Allow connecting to specified TCP port (can be specified multiple times)</li>
<li><code>--best-effort</code>: Use best effort mode, falling back to less restrictive sandbox if necessary [default: enabled]</li>
<li><code>--log-level &lt;level&gt;</code>: Set logging level (error, info, debug) [default: "error"]</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Important Notes</h3><a id="user-content-important-notes" aria-label="Permalink: Important Notes" href="#important-notes"></a></p>
<ul dir="auto">
<li>You must explicitly add the path to the command you want to run with the <code>--ro</code> flag</li>
<li>For system commands, you typically need to include <code>/usr/bin</code>, <code>/usr/lib</code>, and other system directories</li>
<li>When using <code>--exec</code>, you still need to specify the directories containing executables with <code>--ro</code></li>
<li>Network restrictions require Linux kernel 6.8 or later with Landlock ABI v5</li>
<li>The <code>--best-effort</code> flag allows graceful degradation on older kernels that don't support all requested restrictions</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Environment Variables</h3><a id="user-content-environment-variables" aria-label="Permalink: Environment Variables" href="#environment-variables"></a></p>
<ul dir="auto">
<li><code>LANDRUN_LOG_LEVEL</code>: Set logging level (error, info, debug)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<ol dir="auto">
<li>Run a command with read-only access to a directory:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls /path/to/dir"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls /path/to/dir</pre></div>
<ol start="2" dir="auto">
<li>Run a command with write access to a directory:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --rw /path/to/dir touch /path/to/dir/newfile"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --rw /path/to/dir touch /path/to/dir/newfile</pre></div>
<ol start="3" dir="auto">
<li>Run a command with execution permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --exec /usr/bin/bash"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --exec /usr/bin/bash</pre></div>
<ol start="4" dir="auto">
<li>Run with debug logging:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --log-level debug --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls"><pre>landrun --log-level debug --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls</pre></div>
<ol start="5" dir="auto">
<li>Run with network restrictions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --bind-tcp 8080 --connect-tcp 53 /usr/bin/my-server"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --bind-tcp 8080 --connect-tcp 53 /usr/bin/my-server</pre></div>
<p dir="auto">This will allow the program to only bind to TCP port 8080 and connect to TCP port 53.</p>
<ol start="6" dir="auto">
<li>Run a DNS client with appropriate permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /etc/resolv.conf --connect-tcp 53 dig example.com"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /etc/resolv.conf --connect-tcp 53 dig example.com</pre></div>
<p dir="auto">This allows DNS resolution by granting access to /etc/resolv.conf and permitting connections to port 53 (DNS).</p>
<ol start="7" dir="auto">
<li>Run a web server with selective network permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /var/www --rw /var/log --bind-tcp 80 --bind-tcp 443 /usr/bin/nginx"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /var/www --rw /var/log --bind-tcp 80 --bind-tcp 443 /usr/bin/nginx</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">landrun uses Linux's Landlock LSM to create a secure sandbox environment. It provides:</p>
<ul dir="auto">
<li>File system access control</li>
<li>Directory access restrictions</li>
<li>Execution control</li>
<li>TCP network restrictions</li>
<li>Process isolation</li>
</ul>
<p dir="auto">Landlock is an access-control system that enables processes to securely restrict themselves and their future children. As a stackable Linux Security Module (LSM), it creates additional security layers on top of existing system-wide access controls, helping to mitigate security impacts from bugs or malicious behavior in applications.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Landlock Access Control Rights</h3><a id="user-content-landlock-access-control-rights" aria-label="Permalink: Landlock Access Control Rights" href="#landlock-access-control-rights"></a></p>
<p dir="auto">landrun leverages Landlock's fine-grained access control mechanisms, which include:</p>
<p dir="auto"><strong>File-specific rights:</strong></p>
<ul dir="auto">
<li>Execute files (<code>LANDLOCK_ACCESS_FS_EXECUTE</code>)</li>
<li>Write to files (<code>LANDLOCK_ACCESS_FS_WRITE_FILE</code>)</li>
<li>Read files (<code>LANDLOCK_ACCESS_FS_READ_FILE</code>)</li>
<li>Truncate files (<code>LANDLOCK_ACCESS_FS_TRUNCATE</code>) - Available since Landlock ABI v3</li>
</ul>
<p dir="auto"><strong>Directory-specific rights:</strong></p>
<ul dir="auto">
<li>Read directory contents (<code>LANDLOCK_ACCESS_FS_READ_DIR</code>)</li>
<li>Remove directories (<code>LANDLOCK_ACCESS_FS_REMOVE_DIR</code>)</li>
<li>Remove files (<code>LANDLOCK_ACCESS_FS_REMOVE_FILE</code>)</li>
<li>Create various filesystem objects (char devices, directories, regular files, sockets, etc.)</li>
<li>Refer/reparent files across directories (<code>LANDLOCK_ACCESS_FS_REFER</code>) - Available since Landlock ABI v2</li>
</ul>
<p dir="auto"><strong>Network-specific rights</strong> (requires Linux 6.8+ with Landlock ABI v5):</p>
<ul dir="auto">
<li>Bind to specific TCP ports (<code>LANDLOCK_ACCESS_NET_BIND_TCP</code>)</li>
<li>Connect to specific TCP ports (<code>LANDLOCK_ACCESS_NET_CONNECT_TCP</code>)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Limitations</h3><a id="user-content-limitations" aria-label="Permalink: Limitations" href="#limitations"></a></p>
<ul dir="auto">
<li>Landlock must be supported by your kernel</li>
<li>Network restrictions require Linux kernel 6.8+ with Landlock ABI v5</li>
<li>Some operations may require additional permissions</li>
<li>Files or directories opened before sandboxing are not subject to Landlock restrictions</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Kernel Compatibility Table</h2><a id="user-content-kernel-compatibility-table" aria-label="Permalink: Kernel Compatibility Table" href="#kernel-compatibility-table"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Feature</th>
<th>Minimum Kernel Version</th>
<th>Landlock ABI Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>Basic filesystem sandboxing</td>
<td>5.13</td>
<td>1</td>
</tr>
<tr>
<td>File referring/reparenting control</td>
<td>5.17</td>
<td>2</td>
</tr>
<tr>
<td>File truncation control</td>
<td>6.1</td>
<td>3</td>
</tr>
<tr>
<td>Network TCP restrictions</td>
<td>6.8</td>
<td>5</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">If you receive "permission denied" or similar errors:</p>
<ol dir="auto">
<li>Ensure you've added all necessary paths with <code>--ro</code> or <code>--rw</code></li>
<li>Try running with <code>--log-level debug</code> to see detailed permission information</li>
<li>Check that Landlock is supported and enabled on your system:
<div dir="auto" data-snippet-clipboard-copy-content="grep -E 'landlock|lsm=' /boot/config-$(uname -r)"><pre>grep -E <span><span>'</span>landlock|lsm=<span>'</span></span> /boot/config-<span><span>$(</span>uname -r<span>)</span></span></pre></div>
You should see <code>CONFIG_SECURITY_LANDLOCK=y</code> and <code>lsm=landlock,...</code> in the output</li>
<li>For network restrictions, verify your kernel version is 6.8+ with Landlock ABI v5:

</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Details</h2><a id="user-content-technical-details" aria-label="Permalink: Technical Details" href="#technical-details"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Implementation</h3><a id="user-content-implementation" aria-label="Permalink: Implementation" href="#implementation"></a></p>
<p dir="auto">This project uses the <code>landlock-lsm/go-landlock</code> package for sandboxing, which provides both filesystem and network restrictions. The current implementation supports:</p>
<ul dir="auto">
<li>Read/write/execute restrictions for files and directories</li>
<li>TCP port binding restrictions</li>
<li>TCP port connection restrictions</li>
<li>Best-effort mode for graceful degradation on older kernels</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Best-Effort Mode</h3><a id="user-content-best-effort-mode" aria-label="Permalink: Best-Effort Mode" href="#best-effort-mode"></a></p>
<p dir="auto">When using <code>--best-effort</code> (enabled by default), landrun will gracefully degrade to using the best available Landlock version on the current kernel. This means:</p>
<ul dir="auto">
<li>On Linux 6.8+: Full filesystem and network restrictions</li>
<li>On Linux 6.1-6.7: Filesystem restrictions including truncation, but no network restrictions</li>
<li>On Linux 5.17-6.0: Basic filesystem restrictions including file reparenting, but no truncation control or network restrictions</li>
<li>On Linux 5.13-5.16: Basic filesystem restrictions without file reparenting, truncation control, or network restrictions</li>
<li>On older Linux: No restrictions (sandbox disabled)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future Features</h2><a id="user-content-future-features" aria-label="Permalink: Future Features" href="#future-features"></a></p>
<p dir="auto">Based on the Linux Landlock API capabilities, we plan to add:</p>
<ul dir="auto">
<li>🔒 Enhanced filesystem controls with more fine-grained permissions</li>
<li>🌐 Support for UDP and other network protocol restrictions (when supported by Linux kernel)</li>
<li>🔄 Process scoping and resource controls</li>
<li>🛡️ Additional security features as they become available in the Landlock API</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the GNU General Public License v2</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Differential Geometry: A First Course in Curves and Surfaces [pdf] (132 pts)]]></title>
            <link>https://math.franklin.uga.edu/sites/default/files/users/user317/ShifrinDiffGeo.pdf</link>
            <guid>43445614</guid>
            <pubDate>Sat, 22 Mar 2025 13:46:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://math.franklin.uga.edu/sites/default/files/users/user317/ShifrinDiffGeo.pdf">https://math.franklin.uga.edu/sites/default/files/users/user317/ShifrinDiffGeo.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=43445614">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
    </channel>
</rss>