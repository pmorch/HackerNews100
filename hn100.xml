<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 01 Apr 2025 19:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A man powers home for eight years using a thousand old laptop batteries (194 pts)]]></title>
            <link>https://techoreon.com/a-man-powers-home-8-years-laptop-batteries/</link>
            <guid>43548217</guid>
            <pubDate>Tue, 01 Apr 2025 15:49:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techoreon.com/a-man-powers-home-8-years-laptop-batteries/">https://techoreon.com/a-man-powers-home-8-years-laptop-batteries/</a>, See on <a href="https://news.ycombinator.com/item?id=43548217">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
<p><strong>A man has managed to power his home for eight years with a system using more than 1,000 recycled laptop batteries</strong>. This ingenious project, based on the use of electronic waste, has proven to be an environmentally friendly and economical solution, without the need to even replace batteries over the years.</p>



<p>This system also uses solar panels, which were the origin of his renewable energy project that he started a long time ago and which has been enough for him to live during this time.</p>



<h3><mark>How Does This DIY Power System Work?</mark></h3>



<p>The project began in November 2016, when the creator, known with the alias <em>Glubux</em> on online forums, began sharing his plans in the <a href="https://secondlifestorage.com/index.php?threads%2Fglubuxs-powerwall.126%2F=" target="_blank" rel="noopener"><em>Second Life Storage community</em></a>. From the outset, his goal was clear: to generate energy for his home without relying on the electrical grid, through a combination of solar panels and recycled batteries.</p>




<p>In its early stages, it used a basic 1.4 kW solar panel system, along with an old 24V 460Ah forklift battery, charge controllers, and a 3 kVA inverter. However, its vision was to <strong>expand the system and take it beyond</strong> what it had initially achieved.</p>



<p>The centerpiece of their system is more than 1,000 secondhand laptop batteries. For many, old computer batteries are considered waste, but for Glubux, they represented an opportunity to create a completely independent as well as renewable energy source.</p>



<p>Reusing these batteries is a great idea and is an example of how it’s possible to give a second life to electronic waste, a sector in which the UN has noted that <strong>less than a quarter</strong> of the e-waste generated globally is properly collected and recycled.</p>




<p>The system was initially modest, but over time, Glubux began adding more and more recycled batteries. Soon,&nbsp;his installation grew from a small setup to a self-powered system consisting of 650 batteries.</p>



<p>This growth forced the creator to build a separate warehouse, located about 50 meters from his home, to store the batteries and the new charge controllers and inverters. The warehouse became a workshop where he assembled the battery packs, grouping them together to create blocks with a capacity of approximately 100 Ah each.</p>



<p>At first, he faced some obstacles. The battery discharge rates were uneven due to differences in the battery cells used, <strong>causing some to drain faster than others</strong>. However, the solution came with rearranging and adjusting the cells to ensure the packs worked more efficiently.</p>



<p>Glubux even began disassembling entire laptop batteries, removing individual cells and organizing them into custom racks. This task, which likely required a great deal of manual labor and technical knowledge, was key to making the system work effectively and sustainably.</p>



<h3><mark>How this System Has Lasted Eight Years?</mark></h3>



<p><strong>The most amazing thing about this project</strong> is that, despite the initial difficulties and the experimental nature of the system, it has continued to operate uninterruptedly today. In its eight years of operation, not a single battery cell has needed to be replaced, a remarkable achievement considering the operating conditions and the nature of the recycled batteries.</p>



<p>In addition, over the years, Glubux has improved and expanded its solar panel system. Currently, its installation features 24 solar panels, each measuring 440W, allowing it to generate sufficient power even during the coldest months.</p>




<p><strong>Despite being an unusual system</strong>, with recycled and homemade components, no major problems have been reported, such as fires or swollen batteries, which is a common issue with some second-hand electronic devices.</p>



<p>Glubux, for its part, continues to operate with complete confidence in its installation, which has not only been able to supply all of its home’s electricity, but also allows the operation of equipment such as the washing machine.</p>



<hr>







<!-- CONTENT END 1 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Why hasn’t AMD made a viable CUDA alternative? (136 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43547309</link>
            <guid>43547309</guid>
            <pubDate>Tue, 01 Apr 2025 14:37:54 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43547309">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="43547461"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547461" href="https://news.ycombinator.com/vote?id=43547461&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>There is more than one way to answer this.</p><p>They have made an alternative to the CUDA language with HIP, which can do most of the things the CUDA language can.</p><p>You could say that they haven't released supporting libraries like cuDNN, but they are making progress on this with AiTer for example.</p><p>You could say that they have fragmented their efforts across too many different paradigms but I don't think this is it because Nvidia also support a lot of different programming models.</p><p>I think the reason is that they have not prioritised support for ROCm across all of their products. There are too many different architectures with varying levels of support. This isn't just historical. There is no ROCm support for their latest AI Max 395 APU. There is no nice cross architecture ISA like PTX. The drivers are buggy. It's just all a pain to use. And for that reason "the community" doesn't really want to use it, and so it's a second class citizen.</p><p>This is a management and leadership problem. They need to make using their hardware easy. They need to support all of their hardware. They need to fix their driver bugs.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547568"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547568" href="https://news.ycombinator.com/vote?id=43547568&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>This ticket, finally closed after being open for <i>2 years</i>, is a pretty good micocosm of this problem:</p><p><a href="https://github.com/ROCm/ROCm/issues/1714" rel="nofollow">https://github.com/ROCm/ROCm/issues/1714</a></p><p>Users complaining that the docs don't even specify which cards work.</p><p>But it goes deeper - a valid complaint is that "this only supports one or two consumer cards!" A common rebuttal is that it works fine on lots of AMD cards if you set some environment flag to force the GPU architecture selection.  The fact that this is <i>so close</i> to working on a wide variety of hardware, and yet doesn't, is exactly the vibe you get with the whole ecosystem.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549097"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549097" href="https://news.ycombinator.com/vote?id=43549097&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>What I don't get is why they don't at least assign a dev or two to make the poster child of this work: llama.cpp</p><p>It's the first thing anyone tries when trying to dabble in AI or compute on the gpu, yet it's a clusterfuck to get to work. A few blessed cards work, with proper drivers and kernel; others just crash, perform horribly slow, or output GGGGGGGGGGGGGG to every input (I'm not making this up!) Then you LOL, dump it and go buy nvidia et voila, stuff works first try.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548203"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548203" href="https://news.ycombinator.com/vote?id=43548203&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I suspect part of it is also that Nvidia actually does a lot of things in firmware that can be upgraded. The new Nvidia Linux drivers (the "open" ones) support Turing cards from 2018. That means chips that old already do much of the processing in firmware.</p><p>AMD keeps having issues because their drivers talk to the hardware directly so their drivers are massive bloated messes, famous for pages of auto-generated register definitions. Likely it's much more difficult to fix anything.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547940"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547940" href="https://news.ycombinator.com/vote?id=43547940&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Geez. If I were Berkshire Hathaway looking to invest in the GPU market, this would be a major red flag in my fundamentals analysis.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547700"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547700" href="https://news.ycombinator.com/vote?id=43547700&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I had a similar (I think) experience when building LLVM from source a few years ago.</p><p>I kept running into some problem with LLVM's support for HIP code, even though I had not interest in having that functionality.</p><p>I realize this isn't exactly an AMD problem, but IIRC it was they were who contributed the troublesome code to LLVM, and it remained unfixed.</p><p>Apologies if there's something unfair or uninformed in what I wrote, it's been a while.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547988"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547988" href="https://news.ycombinator.com/vote?id=43547988&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>That reeks of gross incompetence somewhere in the organization. Like a hosting company that has a customer dealing with very poor performance, over pays greatly to avoid it while the whole time nobody even thinks to check what the linux swap file is doing.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547799"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547799" href="https://news.ycombinator.com/vote?id=43547799&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt;This is a management and leadership problem.</p><p>It's easy (and mostly correct) to blame management for this, but it's such a foundational issue that even if everyone up to the CEO pivoted on every topic, it wouldn't change anything. They simply don't have the engineering talent to pull this off, because they somehow concluded that making stuff open source means someone else will magically do the work for you. Nvidia on the other hand has accrued top talent for more than a decade and carefully developed their ecosystem to reach this point. And there are only so many talented engineers on the planet. So even if AMD leadership wakes up tomorrow, they won't go anywhere for a looong time.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547827"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547827" href="https://news.ycombinator.com/vote?id=43547827&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; This is a management and leadership problem. They need to make using their hardware easy. They need to support all of their hardware. They need to fix their driver bugs.</p><p>Yes. This kind of thing is unfortunately endemic in hardware companies, which don't "get" software. It's cultural and requires (a) a leader who does Get It and (b) one of those Amazon memos stating "anyone who does not Get With The Program will be fired".</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547675"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547675" href="https://news.ycombinator.com/vote?id=43547675&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>It is a little bit more complicated than ROCm simply not having support because ROCm has at a point claimed support, and they've had to walk it back painfully (multiple times). Its not a driver issue, nor a hardware issue on their side.</p><p>There has been a long-standing issue between AMD and its mainboard manufacturers. The issue has to do with features required for ROCm, namely PCIe Atomics. AMD has been unable or unwilling to hold the mainboard manufacturers to account for advertising features the mainboard does not support.</p><p>The CPU itself must support this feature, but the mainboard must as well (in firmware).</p><p>One of the reasons why ROCm hasn't worked in the past is because the mainboard manufacturers have claimed and advertised support for PCIe Atomics, and the support they've claimed has been shown to be false, and the software fails in non-deterministic ways when tested. This is nightmare fuel for the few AMD engineers tasked with ROCm.</p><p>PCIe Atomics requires non-translated direct IO to operate correctly, and in order to support the same CPU models from multiple generations they've translated these IO lines in firmware.</p><p>This has left most people that query their system to check this showing PCIAtomics is supported, while when actual tests that rely on that support are done they fail, in chaotic ways. There is no technical specification or advertising that the mainboard manufacturers provide showing whether this is supported. Even the boards with multiple x16 slots and the many technologies related to it such as Crossfire/SLI/mGPU brandings these don't necessarily show whether PCIAtomics is properly supported.</p><p>In other words, the CPU is supported, the firmware/mainboard fail with no way to differentiate between the two at the upper layers of abstraction.</p><p>All in all. You shouldn't be blaming AMD for this. You should be blaming the three mainboard manufacturers who chose to do this. Some of these manufacturers have upper end boards where they actually did do this right they just chose to not do this for any current gen mainboard costing less than ~$300-500.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549200"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549200" href="https://news.ycombinator.com/vote?id=43549200&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>There are so many hardware certification programs out there, why doesn't AMD run one to fix this?</p><p>Create a "ROCm compatible" logo and a list of criteria. Motherboard manufacturers can send a pre-production sample to AMD along with a check for some token amount (let's say $1000). AMD runs a comprehensive test suite to check actual compatibility, if it passes the mainboard is allowed to be advertised and sold with the previously mentioned logo. Then just tell consumers to look for that logo if they want to use ROCm. If things go wrong on a mainboard without the certification, communicate that it's probably the mainboard's fault.</p><p>Maybe add some kind of versioning scheme to allow updating requirements in the future</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43549341"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549341" href="https://news.ycombinator.com/vote?id=43549341&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>AIUI, AMD documentation claims that the requirement for PCIe Atomics is due to ROCm being based on Heterogeneous System Architecture, <a href="https://en.wikipedia.org/wiki/Heterogeneous_System_Architecture" rel="nofollow">https://en.wikipedia.org/wiki/Heterogeneous_System_Architect...</a> which allows for a sort of "unified memory" (strictly speaking, a unified address space) across CPU and GPU RAM.  Other compute API's such as CUDA, OpenCL, SYCL or Vulkan Compute don't have HSA as a strict requirement but ROCm apparently does.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547796"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547796" href="https://news.ycombinator.com/vote?id=43547796&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Look, this sounds like a frustrating nightmare, but the way it seems to us consumers is that AMD chose to rely on poorly implemented and supported technology, and Nvidia didn't. I can't blame AMD for the poor support by motherboards manufacturers but I can and will blame AMD for relying on it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548795"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43548795" href="https://news.ycombinator.com/vote?id=43548795&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>While we won't know for sure, unless someone from AMD comments on this; in fairness there may not have been any other way.</p><p>Nvidia has a large number of GPU related patents.</p><p>The fact that AMD chose to design their system this way, in such a roundabout and brittle manner, which is contrary to how engineer's approach things, may have been a direct result of being unable to design such systems any other way because of broad patents tied to the interface/GPU.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549136"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43549136" href="https://news.ycombinator.com/vote?id=43549136&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I feel like this issue is to at least some extent a red herring. Even accepting that  ROCm doesn't work on some motherboards, this can't explain why so few of AMD's GPUs have official ROCm support.</p><p>I notice that at one point there was a ROCm release which said it didn't require atomics for gfx9 GPUs, but the requirement was reintroduced in a later version of ROCm. Not sure what happened there but this seems to suggest AMD might have had a workaround at some point (though possibly it didn't work).</p><p>If this really is due to patent issues AMD can likely afford to licence or cross-license the patent given potential upside.</p><p>It would be in line with other decisions taken by AMD if they took this decision because it works well with their datacentre/high-end GPUs, and they don't (or didn't) really care about offering GPGPU to the mass/consumer GPU market.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549331"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43549331" href="https://news.ycombinator.com/vote?id=43549331&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; I feel like this issue is to at least some extent a red herring.</p><p>I don't see that, these two issues adequately explain why so few GPUs have official support. They don't want to get hit with a lawsuit, as a result of issues outside their sphere of control.</p><p>&gt; If this really is due to patent issues AMD can likely afford to license or cross-license the patent given potential upside.</p><p>Have you ever known any company willing to cede market dominance and license or cross-license a patent letting competition into a market that they hold an absolute monopoly over, let alone in an environment where antitrust is non-existent and fang-less?</p><p>There is no upside for NVIDIA to do that. If you want to do serious AI/ML work you currently need to use NVIDIA hardware, and they can charge whatever they want for that.</p><p>The moment you have a competitor, demand is halved at a bare minimum depending on how much the competitor undercuts you by. Any agreement on coordinating prices leads to price-fixing indictments.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43547751"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547751" href="https://news.ycombinator.com/vote?id=43547751&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>How does NVIDIA manage this issue? I wonder whether they have a very different supply chain or just design software that puts less trust in the reliability of those advertised features.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549279"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43549279" href="https://news.ycombinator.com/vote?id=43549279&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I should point out here, if nobody has already; Nvidia's GPU designs are <i>extremely</i> complicated compared to what AMD and Apple ship. The "standard" is to ship a PCIe card with display handling drivers and some streaming multiprocessor hardware to process your framebuffers. Nvidia goes even further by adding additional accelerators (ALUs by way of CUDA core and tensor cores), onboard RTOS management hardware (what Nvidia calls GPU System Processor), and more complex userland drivers that very well might be able to manage atomics without any PCIe standards.</p><p>This is also one of the reasons AMD and Apple can't simply turn their ship around right now. They've both invested heavily in simplifying their GPU and removing a lot of the creature-comforts people pay Nvidia for. 10 years ago we could at least all standardize on OpenCL, but these days it's all about proprietary frameworks and throwing competitors under the bus.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547816"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547816" href="https://news.ycombinator.com/vote?id=43547816&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Its an open question they have never answered afaik.</p><p>I would speculate that their design is self-contained in hardware.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547777"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547777" href="https://news.ycombinator.com/vote?id=43547777&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>So .. how's Nvidia dealing with this? Or do they benefit from motherboard manufacturers doing preferential integration testing?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43549371"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549371" href="https://news.ycombinator.com/vote?id=43549371&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I made a post here a while back suggesting an investment strategy of spending one billion on AMD shares and one billion on software developers to 3rd party write a quality support stack for their hardware. I'm still not sure if its a crazy idea.</p><p>Actually it might be better to spend 1B on shares and 10x 100M on development and take ten attempts in parallel and use the best of them.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547919"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547919" href="https://news.ycombinator.com/vote?id=43547919&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I want to argue that graphics cards are really 3 markets: integrated, gaming (dedicated), and compute. Not only do these have different hardware (fixed function, ray tracing cores, etc.) but also different programming and (importantly) distribution models. NVIDIA went from 2 to 3. Intel went from 1 to 2, and bought 3 (trying to merge). AMD started with 2 and went to 1 (around Llano) and attempted the same thing as NVIDIA via GCN (please correct me if I'm wrong).</p><p>My understanding is that the reason is that the real market for 3 (GPUs for compute) didn't show up until very late, so AMD's GCN bet didn't pay off. Even in 2021, NVIDIA's revenue from gaming was above data center revenue (a segment they basically had no competition in, and 100% of their revenue was from CUDA). AMD meanwhile won the battle for Playstation and Xbox consoles, and was executing a turnaround in data centers with EPYC and CPUs (with Zen). So my guess as to why they might have underinvested is basically: for much of the 2010s they were just trying to survive, so they focused on battles they could win that would bring them revenue.</p><p>This high level prioritization would explain a lot of "misexecution", e.g. if they underhired for ROCm, or prioritized APU SDK experience over data center, their testing philosophy ("does this game work ok? great").</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547462"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547462" href="https://news.ycombinator.com/vote?id=43547462&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>They likely haven't put even close to enough money behind it. This isn't a unique situation - you'll see in corporate america a lot of CEOs who say "we are investing in X" and they really believe they are. But the required size is billions (like, hundreds of really insanely talented engineers being paid 500k-1m, lead by a few being paid $3-10m), and they are instead investing low 10's of millions.</p><p>They can't bring themselves to put so much money into it that it would be an obvious fail if it didn't work.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547563"><td></td></tr>
                <tr id="43547645"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547645" href="https://news.ycombinator.com/vote?id=43547645&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>The big players are all investing in building chips themselves.</p><p>And probably not putting enough money behind it... it takes enormous courage as a CEO to walk into a boardroom and say "I'm going to spend $50 billion, I think it will probably work, I'm... 60% certain".</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547778"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547778" href="https://news.ycombinator.com/vote?id=43547778&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>You're probably correct, but I feel like I have to raise the issue of Zuckerberg spending a comparable amount on VR which was much more speculative.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548018"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548018" href="https://news.ycombinator.com/vote?id=43548018&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Zuck is founder and owner. So is Huang (Nvidia CEO). They call all the shots.</p><p>Whereas AMD's CEO was appointed, and can be fired. Huge difference in their risk appetite.</p><p>I'm reminded of pg's article "founder mode": <a href="https://paulgraham.com/foundermode.html" rel="nofollow">https://paulgraham.com/foundermode.html</a></p><p>I think some companies simply aren't capable of taking big risks and innovating in big ways, for this reason.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547864"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43547864" href="https://news.ycombinator.com/vote?id=43547864&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Zuckerberg owns Facebook though. It’s a lot easier to make bold decisions when you’re the majority shareholder.</p><p>Edit: though emphasis should be put on “easIER” because it’s still far from easy.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547935"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43547935" href="https://news.ycombinator.com/vote?id=43547935&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>This. Without knowing the guy, he seems to be a) very comfortable taking a lot of risk and b) it's actually not that risky for him to blow $20 billion.</p><p>There aren't many cases like this. Larry/Sergey were more than comfortable risking $10 billion here and there.</p></div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="43547934"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547934" href="https://news.ycombinator.com/vote?id=43547934&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>It amazes me how much these companies make actually gets spent on R&amp;D, you see the funnel charts on reddit and I am like what the hell. Microsoft only spends ~6bn USD on R&amp;D with a total 48bn of revenue and 15bn in profits?</p><p>What the hell is going on, they should be able to keep an army of PhDs doing pointless research even if only one paper in 10 years comes to a profitable product. But instead they are cutting down workforce like there is no tomorrow...</p><p>(I know, I know, market dynamics, value extraction, stock market returns)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549373"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549373" href="https://news.ycombinator.com/vote?id=43549373&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>R and D in the financial statements I've seen basically covers the entire product, engineering etc org. Lots and lots of people, but not what regular people consider RnD.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548046"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548046" href="https://news.ycombinator.com/vote?id=43548046&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>well, look at Meta... they're spending Billions with a capital B on stuff and they get slaughtered every earnings call because it hasn't paid off yet. if Zuckerberg wasn't the majority share holder it probably wouldn't be sustainable.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43549332"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549332" href="https://news.ycombinator.com/vote?id=43549332&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I've been telling people for years that NVIDIA is actually a software company, but nobody ever listens.  My argument is that their silicon is nothing special and could easily be replicated by others, and therefore their real value is in their driver+CUDA layer.</p><p>(Maybe "nothing special" is a little bit strong, but as a chip designer I've never seen the actual NVIDIA chips as all that much of a moat.  What makes it hard to find alternatives to NVIDIA is their driver and CUDA stack.)</p><p>Curious to hear others' opinions on this.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43549300"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549300" href="https://news.ycombinator.com/vote?id=43549300&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>If AMD developers use AI deployed on nvidia hardware to create tools that complete against nvidia as a company but overall improves outcomes because of competition, would this be an example of co evolution observable in human time standards... I feel like ai is evolving, taking a stable form in this complex multi dimension multi paradigm sweet spot of an environment we have created, on top of this technical, social and governmental infrastructure and we're watching it live on discovery tech filtered into a 2d video narrated by some idiot who has no right to be as confident as he sounds. I'm sorry I'm on withdrawal from quitting mass media and I'm very bored.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549395"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43549395" href="https://news.ycombinator.com/vote?id=43549395&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt;I'm sorry I'm on withdrawal from quitting mass media and I'm very bored.</p><p>Good choice!  So many people doing that these days.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547586"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547586" href="https://news.ycombinator.com/vote?id=43547586&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>CUDA is an entire ecosystem - not a single programming language extension (C++) or a single library, but a collection of libraries &amp; tools for specific use cases and optimizations (cuDNN, CUTLASS, cuBLAS, NCCL, etc.). There is also tooling support that Nvidia provides, such as profilers, etc. Many of the libraries build on other libraries. Even if AMD had the decent, reliable language extensions for general-purpose GPU programming, they still don't have the libraries and the supporting ecosystem to provide anything to the level that CUDA provides today, which is a decade plus of development effort from Nvidia to build.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548095"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43548095" href="https://news.ycombinator.com/vote?id=43548095&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The counter point is they could make a higher level version of CUDA which wouldn't necessitate all the other supporting libraries. The draw of cuBLAS is that CUDA is a confusing pain. It seems reasonable to think they could write a better, higher level language (in the same vein as triton) and not have to write as many support libraries</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548849"><td></td></tr>
                        <tr id="43547594"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547594" href="https://news.ycombinator.com/vote?id=43547594&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Back in 2015, they were a quarter or two from bankruptcy, saved by the XBOX and Playstation contracts.  Those years saw several significant layoffs, and talent leaving for greener pastures.    Lisa Su has done a great job at rebuilding the company.   But not in a position to hire 2000 engineers x few million comp (~$4 billion annually) even if there were people readily available.</p><p>"it'd still be a good investment." - that's definitely not a sure thing.  Su isn't a risk taker, seems to prefer incremental growth, mainly focused on the CPU side.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548052"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43548052" href="https://news.ycombinator.com/vote?id=43548052&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Where does the idea that engineers cost "a few million" come from? You might pay that much to senior engineering management, big names who can attract other talent, but normal engineers cost much less than a million dollars a year.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548486"><td></td></tr>
            <tr id="43547672"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547672" href="https://news.ycombinator.com/vote?id=43547672&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>This is the difference between Jensen and Su. It’s not that Jensen is a risk taker. No. Jensen focused on incremental growth of the core business while slowly positioning the company for growth in other verticals as well should the landscape change.</p><p>Jensen never said… hey I’m going to bet it all on AI and cuda. Let’s go all in. This never happened. Both Jensen and Su are not huge risk takers imo.</p><p>Additionally there’s a lot of luck involved with the success of NVIDIA.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549302"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549302" href="https://news.ycombinator.com/vote?id=43549302&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I think this broaches the real matter, which is that nVidia's core business is GPUs while AMD's core business is CPUs. And, frankly, AMD has lately been doing a great job at its core business. The problem is that GPUs are now much more profitable than CPUs, both in terms of unit economics and growth potential. So they are winning a major battle (against Intel) even as they are losing a different one (against nVidia). I'm not sure there's a strategy they could have adopted to win both at the same time.</p><p>However, the next big looming problem for them is likely to be the shrinking market for x86 vs. the growing market for Arm etc. So they might very well have demonstrated great core competence, that ends up being completely swept away by not just one but two major industry shifts.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43549345"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549345" href="https://news.ycombinator.com/vote?id=43549345&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>In turn I will raise you the following: Why are GPU ISA trade secrets at all? Why not open them up like CPU ISAs, get rid of specialized cores and let compiler writers port their favorite languages to compile into native GPU programs? Everyone will be happy. Game devs will be happy with more control over the hardware, Compiler devs will be happy to run haskell or prolog natively on GPUs, ML devs will be happier, NVIDIA/AMD will be happier with taking the MainStage.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548054"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43548054" href="https://news.ycombinator.com/vote?id=43548054&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>CUDA isn't the moat people think it is. NVIDIA absolutely has the best dev ergonomics for machine learning, there's no question about that. Their driver is also far more stable than AMD's. But AMD is also improving, they've made some significant strides over the last 12-18 months.</p><p>But I think more importantly, what is often missed in this analysis is that most programmers doing ML work aren't writing their own custom kernels. They're just using pytorch (or maybe something even more abstracted/multi-backend like keras 3.x) and let the library deal with implementation details related to their GPU.</p><p>That doesn't mean there aren't footguns in that particular land of abstraction, but the delta between the two providers is not nearly as stark as its often portrayed. At least not for the average programmer working with ML tooling.</p><p>(EDIT: also worth noting that the work being done in the MLIR project has a role to play in closing the gap as well for similar reasons)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548642"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43548642" href="https://news.ycombinator.com/vote?id=43548642&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; But I think more importantly, what is often missed in this analysis is that most programmers doing ML work aren't writing their own custom kernels. They're just using pytorch (or maybe something even more abstracted/multi-backend like keras 3.x) and let the library deal with implementation details related to their GPU.</p><p>That would imply that AMD could just focus on implementing good PyTorch support on their hardware and they would be able to start taking market share. Which doesn't sound like much work compared with writing a full CUDA competitor. But that does not seem to be the strategy, which implies it is not so simple?</p><p>I am not an ML engineer so don't have first hand experience, but those I have talked to say they depend on a lot more than just one or two key libraries. But my sample size is small. Interested in other perspectives...</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549088"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549088" href="https://news.ycombinator.com/vote?id=43549088&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; But that does not seem to be the strategy, which implies it is not so simple?</p><p>That is exactly what has been happening [1], and not just in pytorch. Geohot has been very dedicated in working with AMD to upgrade their station in this space [2]. If you hang out in the tinygrad discord, you can see this happening in real time.</p><p>&gt; those I have talked to say they depend on a lot more than just one or two key libraries.</p><p>Theres a ton of libraries out there yes, but if we're talking about python and the libraries in question are talking to GPUs its going to be exceedingly rare that theyre not using one of these under the hood: pytorch, tensorflow, jax, keras, et al.</p><p>There are of course exceptions to this, particularly if you're not using python for your ML work (which is actually common for many companies running inference at scale and want better runtime performance, training is a different story). But ultimately the core ecosystem does work just fine with AMD GPUs, provided you're not doing any exotic custom kernel work.</p><p>(EDIT: just realized my initial comment unintentionally borrowed the "moat" commentary from geohot's blog. A happy accident in this case, but still very much rings true for my day to day ML dev experience)</p><p>[1] <a href="https://github.com/pytorch/pytorch/pulls?q=is%3Aopen+is%3Apr+label%3A%22module%3A+rocm%22" rel="nofollow">https://github.com/pytorch/pytorch/pulls?q=is%3Aopen+is%3Apr...</a></p><p>[2] <a href="https://geohot.github.io//blog/jekyll/update/2025/03/08/AMD-YOLO.html" rel="nofollow">https://geohot.github.io//blog/jekyll/update/2025/03/08/AMD-...</a></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547535"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547535" href="https://news.ycombinator.com/vote?id=43547535&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I can't contribute much to this discussion due to bias and NDAs, but I just wanted to mention, technically HIP is our CUDA competitor. ROCm is the foundation that HIP is being built on.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547554"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547554" href="https://news.ycombinator.com/vote?id=43547554&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I wonder what the purpose is behind creating a whole new API? Why not just focus on getting Vulkan compute on AMD GPUs to have the data throughput of CUDA?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548619"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548619" href="https://news.ycombinator.com/vote?id=43548619&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I don’t know answer to your question, but I recalled something relevant. Some time ago, Microsoft had a tech which compiled almost normal looking C++ into Direct3D 11 compute shaders: <a href="https://learn.microsoft.com/en-us/cpp/parallel/amp/cpp-amp-overview?view=msvc-170" rel="nofollow">https://learn.microsoft.com/en-us/cpp/parallel/amp/cpp-amp-o...</a> The compute kernels are integrated into CPU-running C++ in the similar fashion CUDA does.</p><p>As you see, the technology deprecated in Visual Studio 2022. I don’t know why but I would guess people just didn’t care. Maybe because it only run on Windows.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547492"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547492" href="https://news.ycombinator.com/vote?id=43547492&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>AMD have actually made several attempts at it.</p><p>The first time, they went ahead and killed off their effort to consolidate on OpenCL. OpenCL went terribly (in no small part because NVIDIA held out on OpenCL 2 support) and that set AMD back a long ways.</p><p>Beyond that, AMD does not have a strong software division or one with the teeth to really influence hardware to their needs . They have great engineers but leadership doesn’t know how to get them to where they need to be.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547900"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547900" href="https://news.ycombinator.com/vote?id=43547900&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>This is it, it's an organisational skill issue. To be fair, being a HW company and a SW company at the same time is very difficult.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548029"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548029" href="https://news.ycombinator.com/vote?id=43548029&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>It is but you have to be.</p><p>It’s been key to the success of their peers. NVIDIA and Apple are the best examples but even Intel to a smaller degree.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547545"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547545" href="https://news.ycombinator.com/vote?id=43547545&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The idea that CUDA is the main reason behind Nvidia dominance seems strange to me. If most of the money is coming from Facebook and Microsoft they have their own teams writing code at a lower level than CUDA anyway. Even deepseek was writing stuff lower than that.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549250"><td></td></tr>
                  <tr id="43547853"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547853" href="https://news.ycombinator.com/vote?id=43547853&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>AMD <i>was</i> investing in a drop-in CUDA compatibility layer &amp; cross-compiler!</p><p>Perhaps in keeping with the broader thread here, they had only ever funded a single contract developer working on it, and then discontinued the project (for who-knows-what legal or political reasons). But the developer had specified that he could open-source the pre-AMD state if the contract was dissolved, and he did exactly that! The project is active with an actively contributing community, and is rapidly catching up to where it was.</p><p><a href="https://www.phoronix.com/review/radeon-cuda-zluda" rel="nofollow">https://www.phoronix.com/review/radeon-cuda-zluda</a></p><p><a href="https://vosen.github.io/ZLUDA/blog/zludas-third-life/" rel="nofollow">https://vosen.github.io/ZLUDA/blog/zludas-third-life/</a></p><p><a href="https://vosen.github.io/ZLUDA/blog/zluda-update-q4-2024/" rel="nofollow">https://vosen.github.io/ZLUDA/blog/zluda-update-q4-2024/</a></p><p>IMO it's vital that even if NVIDIA's future falters in some way, the (likely) collective millennia of research built on top of CUDA will continue to have a path forward on other constantly-improving hardware.</p><p>It's frustrating that AMD will benefit from this without contributing - but given the entire context of this thread, maybe it's best that they <i>aren't</i> actively managing the thing that gives their product a future!</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547906"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547906" href="https://news.ycombinator.com/vote?id=43547906&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>ZLUDA is built on HIP which is built on ROCm.   Both of the latter are significant efforts that AMD is pouring significant resources into.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43548633"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43548633" href="https://news.ycombinator.com/vote?id=43548633&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>CUDA is over a decade of investment. I left CUDA toolkit team in 2014 and it was probably around 10 years old back then. Can't build something comparable fast.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547506"><td></td></tr>
            <tr id="43547335"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547335" href="https://news.ycombinator.com/vote?id=43547335&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The answer is in the question, because if they had the foresight to do such a thing the tech would already be here, instead they thought 1 dimensionally about their product, were part of the group that fumbled OpenCL and now they're a decade behind playing catch up.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547379"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547379" href="https://news.ycombinator.com/vote?id=43547379&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>A good group can catch up significantly in 2 years. They will still be behind, but if they are cheaper (or just you can buy them) that would still go a long way.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547589"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547589" href="https://news.ycombinator.com/vote?id=43547589&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I think even with the trashy api and drivers if they release graphic cards with 4x the memory of the nvidia equivalents the community would put the effort to make them work.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547800"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547800" href="https://news.ycombinator.com/vote?id=43547800&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Yeah. Easier said than done, I know, but they need to not just catch up to nVidia but leapfrog them somehow.</p><p>I would have said that releasing cards with 32GB+ of onboard RAM, or better yet 128GB, would have gotten things moving. They'd be able to run/train models that nVidia's consumer cards couldn't.</p><p>But I think nVidia closed that gap with their "Project Digits" (or whatever the final name is) PCs.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547724"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547724" href="https://news.ycombinator.com/vote?id=43547724&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I think that it is really hard to be cheaper in the ways that really matter. Performance per watt matters a lot here, and NVidia is excellent at this. It doesn't seem like anyone else will be able to compete within at least the next couple of years.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547858"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547858" href="https://news.ycombinator.com/vote?id=43547858&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>"good group" is carrying a lot of weight here.   You can't buy that.    You can buy good small groups, but AMD needs a good large group, and that can't be bought.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547882"><td></td></tr>
                  <tr id="43547416"><td></td></tr>
            <tr id="43548746"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43548746" href="https://news.ycombinator.com/vote?id=43548746&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Another possible reason might be outreach. NVIDIA spends big money on getting people to use their products. I have worked at two HPC centers and at both we had NVIDIA employees stationed there, whose job it was to help us get the most out of the hardware. Besides that, they also organize Hackatrons and they have dedicated software developer programs for each common application, be it LLMs, Weather Prediction or Protein Folding, not to mention dedicated libraries for pretty much every domain.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43549019"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549019" href="https://news.ycombinator.com/vote?id=43549019&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Maybe this is an overly cynical response but the answer is simply that they cannot (at least not immediately). They have not invested enough into engineering talent with this specific goal in mind.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547831"><td></td></tr>
            <tr id="43547510"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547510" href="https://news.ycombinator.com/vote?id=43547510&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; The delta between NVIDIA's value and AMD's is bigger than the annual GDP of Spain.</p><p>Nvidia is massively overvalued right now. AI has rocketed them into absolute absurdity, and it's not sustainable. Put aside the actual technology for a second and realize that public image of AI is at rock bottom. Every single time a company puts out AI-generated materials, they receive immense public backlash. That's not going away any time soon and it's only likely to get worse.</p><p>Speaking as someone that's not even remotely anti-AI, I wouldn't touch the shit with a 10 foot pole because of how bad the public image is. The moment that capital realizes this, that bubble is going to pop and it's going to pop hard.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547590"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547590" href="https://news.ycombinator.com/vote?id=43547590&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Interesting perspective, I haven't noticed much if any public backlash against AI generation. What are some examples?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547761"><td></td></tr>
                <tr id="43548218"><td></td></tr>
                  <tr id="43547884"><td></td></tr>
                  <tr id="43547711"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547711" href="https://news.ycombinator.com/vote?id=43547711&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; AI has rocketed them into absolute absurdity, and it's not sustainable</p><p>Why isn't it sustainable? Their biggest customers all have strong finances and legitimate demand. Google and Facebook would happily run every piece of user generated content through an LLM if they had enough GPUs. Same with Microsoft and every enterprise document.</p><p>The VC backed companies and Open AI are more fragile, but they're comparatively small customers.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548050"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548050" href="https://news.ycombinator.com/vote?id=43548050&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>IMO the closest analogue for Nvidia now is Cisco during the dot-com boom. Cisco sold the physical infrastructure required for Internet companies to operate. Investors all bought in because they figured it was a safe bet. Individual companies may come and go, but if the Internet keeps growing, companies will always need to buy networking equipment. Despite the Internet being way bigger than it was in 2000, and Cisco being highly profitable, Cisco's share price has never exceeded the peak it was at during the dot-com boom.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548097"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548097" href="https://news.ycombinator.com/vote?id=43548097&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Google may well want to run more of their content through an LLM, but they will not be using Nvidia hardware to do it, they'll be using their TPUs.</p><p>Amazon are on their third generation of in-house AI chips and Anthropic will be using those chips to train the next generation of Claude.</p><p>In other words, their biggest customers are looking for cheaper alternatives and are already succeeding in finding them.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547801"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547801" href="https://news.ycombinator.com/vote?id=43547801&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; Google and Facebook would happily run every piece of user generated content through an LLM if they had enough GPUs. Same with Microsoft and every enterprise document.</p><p>.. But how much actual value derives from this?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547917"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547917" href="https://news.ycombinator.com/vote?id=43547917&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Youtube could conceivably put multi language subtitles on every video. Potentially even dub them.</p><p>But the "real value" would come from making adverts better targeted and more interactive. It's hard to quantity as a person outside of the companies, but the intuition for a positive value is pretty strong.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548093"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548093" href="https://news.ycombinator.com/vote?id=43548093&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; Youtube could conceivably put multi language subtitles on every video.</p><p>They already do this, it's opt-in.</p><p>&gt; But the "real value" would come from making adverts better targeted and more interactive.</p><p>Is there any evidence to suggest that a transformer would be better at collaborative filtering than the current deep learning system that was custom engineered and built for this?</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43547559"><td></td></tr>
                <tr id="43547716"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547716" href="https://news.ycombinator.com/vote?id=43547716&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I see this form of argument sometimes here but I really don’t get it.</p><p>Lots of people don’t play the stock market or just invest in funds. It seems like just a way of challenging somebody that looks vaguely clever, or calls them out in a “put your money where your mouth is” sense, but actually presents no argument.</p><p>Anyway, if you want to short Nvidia you have to know when their bubble is going to pop to get much benefit out of it, right? The market can remain stupid for longer than you can remain solvent or whatever.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547942"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547942" href="https://news.ycombinator.com/vote?id=43547942&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Spot on on the timing being important. I don't think you need to fine-tune it that much; short and hold until the pop happens. If you hold off for a <i>the pop could happen at an indefinite time; maybe very far from now</i>, then I think that invalidates the individual prediction.</p><p>One frustrating aspect of investing is that confident information is tough to come by. It's my take that if you have any (I personally rarely do), you should act on it. So, when someone claims confidently (e.g. with adjectives that imply confidence) that something's going to happen, then that's better than the default.</p><p>I don't have the insight the claimer does; my thought is: "I am jealous. I with I could be that confident about a stock's trajectory. I would act on it."</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548338"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548338" href="https://news.ycombinator.com/vote?id=43548338&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I was a student up until 2009; watching people talk about buying houses for 50K and selling them for 100K, everyone talking about easy money.</p><p>I knew things were bad when a friend of my sister was complaining that her father(a building framer) was not able to get a loan for a 500K house, something that his colleagues had been able to get. It took another 6 months before the collapse started to hit and the banks when up.</p><p>Timing is hard.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547659"><td></td></tr>
                <tr id="43547867"><td></td></tr>
                <tr id="43548013"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548013" href="https://news.ycombinator.com/vote?id=43548013&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; But they didn't say soon.</p><p>Setting an indefinite timeline devalues any claim. You could prove this to yourself using <i>Reductio ad absurdum</i>, or by applying it to various general cases.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547984"><td></td></tr>
                        <tr id="43547619"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547619" href="https://news.ycombinator.com/vote?id=43547619&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>You imply you either would believe his word or would short nvidia yourself if he said he was.  If not, why not?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547856"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547856" href="https://news.ycombinator.com/vote?id=43547856&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Close - If I had the degree of confidence that post implies about Nvidia being overvalued, I would take an aggressive short position.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547996"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43547996" href="https://news.ycombinator.com/vote?id=43547996&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; Lots of very smart people have lost a lot of money by being completely right about the destination, but wrong about the path and how long it will take to get there.</p><p>If you make a habit of this and still lose money, then either you statistically were very unlucky, or did not have a history of being right.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547944"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43547944" href="https://news.ycombinator.com/vote?id=43547944&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p><i>I would take an aggressive short position.</i></p><p>Lots of very smart people have lost a lot of money by being completely right about the destination, but wrong about the path and how long it will take to get there.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547847"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547847" href="https://news.ycombinator.com/vote?id=43547847&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I forbid myself from speculative trading as a consequence of idiosyncratic principles that I live my life by. One of many symbolic rejections of toxic profiteering that infests our neo-mercantile society. I have enough digits in my bank account that adding any more would be unambiguously greedy and distasteful, so in the end it would be violating my principles simply to debase myself. No thanks.</p><p>Anyways you'd need some kind of window of when a stock is going to collapse to short it. Good luck predicting this one.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547885"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547885" href="https://news.ycombinator.com/vote?id=43547885&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I respect, and adore your philosophy.</p><p>For a short, I think you don't need that strong of a window. For an options combination, yes.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547803"><td></td></tr>
                  <tr id="43547630"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547630" href="https://news.ycombinator.com/vote?id=43547630&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>One thing I've learned the hard way is that industry trends -- and the stock valuations that go with them -- can stay irrational far longer than you can imagine.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548927"><td></td></tr>
                  <tr id="43547728"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547728" href="https://news.ycombinator.com/vote?id=43547728&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>HIP is definitely a viable option. In fact with some effort you can port large CUDA projects to be compilable with the HIP/ AMD-clang toolchain. This way you don’t have to rewrite the world from scratch in a new language but still be able to run GPU workloads on AMD hardware.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547450"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547450" href="https://news.ycombinator.com/vote?id=43547450&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I don't think its that bad. The focus will turn to inference going forward and that eventually means a place for AMD and maybe even Intel. Eventually it will be all about the efficiency of inference in watts.</p><p>That switch will reduce the NVIDIA margins by a lot. NVIDIA probably has 2 years left of being the only one with golden shovels.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547549"><td></td></tr>
                  <tr id="43547627"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547627" href="https://news.ycombinator.com/vote?id=43547627&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>What about Rust for GPU programming? I wonder why AMD doesn't back such kind of effort as an alternative.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547340"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547340" href="https://news.ycombinator.com/vote?id=43547340&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Leadership. At the end of the day, the buck stops with leadership.</p><p>If they wanted to prioritize this, they would. They're simply not taking it seriously.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547363"><td></td></tr>
                <tr id="43547469"><td></td></tr>
            <tr id="43547604"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547604" href="https://news.ycombinator.com/vote?id=43547604&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Why, because 90% of her job is talking to and appeasing shareholders, grand standing with fat whales, and what else.. what do you think a CEO at these companies actually does? They aren't in the trenches of each subdivision nurturing and cracking whips. She likely attends a 2 hour briefing with a line item: CUDA parity project: on schedule release date not set</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547560"><td></td></tr>
            <tr id="43547520"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547520" href="https://news.ycombinator.com/vote?id=43547520&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>HIP is now somewhat viable (and ROCm is now all HIP).</p><p>But — too late. First versions of ROCm were terrible. Too much boilerplate. 1200 lines of template-heavy C++ for a simple FFT. Can't just start hacking around.</p><p>Since then, the CUDA way is cemented in minds of developers. Intel now has oneAPI, and it is not too bad, and hackable, but there is no hardware and no one will learn it. And HIP is "CUDA-like", so why not CUDA, unless you _have to_ use AMD hardware.</p><p>Tl;dr first versions of ROCm were bad. Now they are better, but it is too late.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547893"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547893" href="https://news.ycombinator.com/vote?id=43547893&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>AMD's CEO is the cousin of Nvidias CEO.</p><p>Neither will encroach too much on the others turf.    The two companies don't want to directly compete on the things that really drive the share price.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547347"><td></td></tr>
                <tr id="43547359"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547359" href="https://news.ycombinator.com/vote?id=43547359&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The question was why don't they have anything as good as CUDA, not why don't they adopt CUDA itself.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547402"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547402" href="https://news.ycombinator.com/vote?id=43547402&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Isn't the "goodness" of CUDA really down to its mass adoption -- and therefore its community and network effects -- not strictly its technical attributes?</p><p>If I recall, there are various "GPU programming" and "AI" efforts that have existed for AMD GPUs, but none of them have had the same success in large part because they're simply non-"standard?"</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547524"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547524" href="https://news.ycombinator.com/vote?id=43547524&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I thought OpenCL was supposed to be the "standard"?  From the Wikipedia page, it's largely vendor neutral and not that much younger than CUDA (initial release Aug 2009 vs Feb 2007).  Maybe some more knowledgeable people can comment why it seems to have been outcompeted by the proprietary CUDA?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548063"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548063" href="https://news.ycombinator.com/vote?id=43548063&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>CUDA has a comparatively nicer user experience. If you would like to understand tacitly and have an nvidia GPU, try writing a simple program using both. (Something highly parallel, like nbody, for example)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                                    </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bletchley code breaker Betty Webb dies aged 101 (241 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c78jd30ywv8o</link>
            <guid>43546236</guid>
            <pubDate>Tue, 01 Apr 2025 12:55:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c78jd30ywv8o">https://www.bbc.com/news/articles/c78jd30ywv8o</a>, See on <a href="https://news.ycombinator.com/item?id=43546236">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span>Aida Fofana</span></p><p><span>BBC News, West Midlands<!-- --></span></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp" loading="eager" alt="BBC Pictured is Betty Webb with short curly white hair and in a purple coast with army pin badges on the right lapel. She is slightly smiling and looking towards the camera."><span>BBC</span></p></div><p data-component="caption-block"><figcaption>Bletchley Park code breaker Betty Webb has died at the age of 101<!-- --></figcaption></p></figure><div data-component="text-block"><p>A decorated World War Two code breaker who spent her youth deciphering enemy messages at Bletchley Park has died at the age of 101.<!-- --></p><p>Charlotte "Betty" Webb MBE - who was among the last surviving Bletchley code breakers - died on Monday night, the Women's Royal Army Corps Association confirmed.<!-- --></p><p>Mrs Webb, from Wythall in Worcestershire, joined operations at the Buckinghamshire base at the age of 18, later going on to help with Japanese codes at The Pentagon in the US. She was awarded France's highest honour - the Légion d'Honneur - in 2021. <!-- --></p><p>The Women's Royal Army Corps Association described Mrs Webb as a woman who "inspired women in the Army for decades".<!-- --></p></div><div data-component="text-block"><p>Bletchley Park Trust CEO Iain Standen said Mrs Webb will not only be remembered for her work but "also for her efforts to ensure that the story of what she and her colleagues achieved is not forgotten."<!-- --></p><p>"Betty's passion for preserving the history and legacy of Bletchley Park has undoubtedly inspired many people to engage with the story and visit the site," he said in a statement.<!-- --></p><p>Tributes to Mrs Webb have begun to be posted on social media, including one from historian and author Dr Tessa Dunlop who said she was with her in her final hours.<!-- --></p><p>Describing Mrs Webb as "the very best", she said on X: "She is one of the most remarkable woman I have ever known."<!-- --></p><p>Mrs Webb told the BBC in 2020 that she had "never heard of Bletchley", Britain's wartime code-breaking centre, before starting work there as a member of the ATS, the Auxiliary Territorial Service.<!-- --></p><p>She had been studying at a college near Shrewsbury, Shropshire, when she volunteered as she said she and others on the course felt they "ought to be serving our country rather than just making sausage rolls".<!-- --></p><p>Her mother had taught her to speak German as a child and ahead of her posting remembered being "taken into the mansion [at Bletchley] to read the Official Secrets Act".<!-- --></p><p>"I realised that from then on there was no way that I was going to be able to tell even my parents where I was and what I was doing until 1975 [when restrictions were lifted]," she recalled.<!-- --></p><p>She would tell the family with whom she lodged that she was a secretary.<!-- --></p></div><p data-component="caption-block"><figcaption>Listen on BBC Sounds: Mrs Webb went to work at Bletchley Park when she was 18<!-- --></figcaption></p><div data-component="text-block"><p>When the War ended in Europe in May of 1945, she went to work at the Pentagon after spending four years at Bletchley, which with its analysis of German communications had served as a vital cog in the Allies' war machine.<!-- --></p><p>At the Pentagon she would paraphrase and transcribe already-decoded Japanese messages. She said she was the only member of the ATS to be sent to Washington, describing it as a "tremendous honour".<!-- --></p><p>Mrs Webb, in 2020, recalled she had had no idea the Americans planned to end the conflict by dropping atomic weapon on Japanese cities, describing the weapons' power as "utterly awful"<!-- --></p><p>After the Allies' final victory, it took Mrs Webb several months to organise return passage to the UK, where she worked as a secretary at a school in Shropshire.<!-- --></p><p>The head teacher there had also worked at Bletchley so knew of her professionalism, whereas other would-be employers, she recalled, were left stumped by her being unable to explain - due to secrecy requirements - her previous duties.<!-- --></p><p>More than half a century later, in 2021, Mrs Webb was one of 6,000 British citizens to receive the Légion d'Honneur, following a decision by President François Hollande in 2014 to recognise British veterans who helped liberate France.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp" loading="lazy" alt="PA Media Mrs Webb sat in the front row in a red skirt suit surrounded by other people in large hats, floral print dresses and trouser suits at the Kings Coronation."><span>PA Media</span></p></div><p data-component="caption-block"><figcaption>Betty Webb, seen in the front row in a red suit, was invited to the Coronation<!-- --></figcaption></p></figure><div data-component="text-block"><p>In 2023, she and her niece were among 2,200 people from 203 countries invited to Westminster Abbey to see King Charles III's coronation.<!-- --></p><p>The same year she celebrated her 100th birthday at Bletchley Park with a party. <!-- --></p><p>She and her guests were treated to a fly-past by a Lancaster bomber. She said at the time: "It was for me - it's unbelievable isn't it? Little me."<!-- --></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why F#? (259 pts)]]></title>
            <link>https://batsov.com/articles/2025/03/30/why-fsharp/</link>
            <guid>43546004</guid>
            <pubDate>Tue, 01 Apr 2025 12:34:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://batsov.com/articles/2025/03/30/why-fsharp/">https://batsov.com/articles/2025/03/30/why-fsharp/</a>, See on <a href="https://news.ycombinator.com/item?id=43546004">Hacker News</a></p>
<div id="readability-page-1" class="page"><section itemprop="text">
        
        <p>If someone had told me a few months ago I’d be playing with .NET again after a
15+ years hiatus I probably would have laughed at this.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> Early on in my
career I played with .NET and Java, and even though .NET had done some things
better than Java (as it had the opportunity to learn from some early Java
mistakes), I quickly settled on Java as it was a truly portable environment.</p>

<p>I guess everyone who reads my blog knows that in the past few years I’ve been
playing on and off with OCaml and I think it’s safe to say that it has become
one of my favorite programming languages - alongside the likes of Ruby and
Clojure. My work with OCaml drew my attention recently to F#, an ML targeting
.NET, developed by Microsoft. The functional counterpart of the
(mostly) object-oriented C#. The newest ML language created…</p>

<h2 id="what-is-f">What is F#?</h2>

<blockquote>
  <p>Unfortunately, no one can be told what the Matrix is. You have to see it for yourself.</p>

  <p>– Morpheus, The Matrix</p>
</blockquote>

<p>Before we start discussing F#, I guess we should answer first the question
“What is F#?”. I’ll borrow a bit from the <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/what-is-fsharp">official page</a> to answer it.</p>

<p>F# is a universal programming language for writing succinct, robust and performant code.</p>

<p>F# allows you to write uncluttered, self-documenting code, where your focus remains on your problem domain, rather than the details of programming.</p>

<p>It does this without compromising on speed and compatibility - it is open-source, cross-platform and interoperable.</p>

<div><pre><code><span>open</span> <span>System</span> <span>// Gets access to functionality in System namespace.</span>

<span>// Defines a list of names</span>
<span>let</span> <span>names</span> <span>=</span> <span>[</span> <span>"Peter"</span><span>;</span> <span>"Julia"</span><span>;</span> <span>"Xi"</span> <span>]</span>

<span>// Defines a function that takes a name and produces a greeting.</span>
<span>let</span> <span>getGreeting</span> <span>name</span> <span>=</span> <span>$</span><span>"Hello, {name}"</span>

<span>// Prints a greeting for each name!</span>
<span>names</span>
<span>|&gt;</span> <span>List</span><span>.</span><span>map</span> <span>getGreeting</span>
<span>|&gt;</span> <span>List</span><span>.</span><span>iter</span> <span>(</span><span>fun</span> <span>greeting</span> <span>-&gt;</span> <span>printfn</span> <span>$</span><span>"{greeting}! Enjoy your F#"</span><span>)</span>
</code></pre></div>

<p><strong>Trivia:</strong> F# is the language that made the pipeline operator (<code>|&gt;</code>) popular.</p>

<p>F# has numerous features, including:</p>

<ul>
  <li>Lightweight syntax</li>
  <li>Immutable by default</li>
  <li>Type inference and automatic generalization</li>
  <li>First-class functions</li>
  <li>Powerful data types</li>
  <li>Pattern matching</li>
  <li>Async programming</li>
</ul>

<p>A full set of features are documented in the <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/language-reference/">F# language guide</a>.</p>

<p>Looks pretty promising, right?</p>

<p>F# 1.0 was officially released in May 2005 by Microsoft Research. It was
initially developed by Don Syme at Microsoft Research in Cambridge and evolved
from an earlier research project called “Caml.NET,” which aimed to bring OCaml
to the .NET platform.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup> F# was officially moved from Microsoft Research to
Microsoft (as part of their developer tooling division) in 2010 (timed
with the release of F# 2.0).</p>

<p>F# has been steadily evolving since those early days and the most recent release
<a href="https://learn.microsoft.com/en-us/dotnet/fsharp/whats-new/fsharp-9">F# 9.0</a> was
released in November 2024.  It seems only appropriate that F# would come to my
attention in the year of its 20th birthday!</p>

<p>There were several reasons why I wanted to try out F#:</p>

<ul>
  <li>.NET became open-source and portable a few years ago and I wanted to check the progress on that front</li>
  <li>I was curious if F# offers any advantages over OCaml</li>
  <li>I’ve heard good things about the F# tooling (e.g. Rider and Ionide)</li>
  <li>I like playing with new programming languages</li>
</ul>

<p>Below you’ll find my initial impressions for several areas.</p>

<h2 id="the-language">The Language</h2>

<p>As a member of the ML family of languages, the syntax won’t surprise
anyone familiar with OCaml. As there are quite few people familiar with
OCaml, though, I’ll mention that Haskell programmers will also feel right at
home with the syntax. And Lispers.</p>

<p>For everyone else - it’d be fairly easy to pick up the basics.</p>

<div><pre><code><span>// function application</span>
<span>printfn</span> <span>"Hello, World!"</span>

<span>// function definition</span>
<span>let</span> <span>greet</span> <span>name</span> <span>=</span>
    <span>printfn</span> <span>"Hello, %s!"</span> <span>name</span>

<span>greet</span> <span>"World"</span>

<span>// whitespace is significant, like in Python</span>
<span>let</span> <span>foo</span> <span>=</span>
    <span>let</span> <span>i</span><span>,</span> <span>j</span><span>,</span> <span>k</span> <span>=</span> <span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>)</span>

    <span>// Body expression:</span>
    <span>i</span> <span>+</span> <span>2</span> <span>*</span> <span>j</span> <span>+</span> <span>3</span> <span>*</span> <span>k</span>

<span>// conditional expressions</span>
<span>let</span> <span>test</span> <span>x</span> <span>y</span> <span>=</span>
  <span>if</span> <span>x</span> <span>=</span> <span>y</span> <span>then</span> <span>"equals"</span>
  <span>elif</span> <span>x</span> <span>&lt;</span> <span>y</span> <span>then</span> <span>"is less than"</span>
  <span>else</span> <span>"is greater than"</span>

<span>printfn</span> <span>"%d %s %d."</span> <span>10</span> <span>(</span><span>test</span> <span>10</span> <span>20</span><span>)</span> <span>20</span>

<span>// Looping over a list.</span>
<span>let</span> <span>list1</span> <span>=</span> <span>[</span> <span>1</span><span>;</span> <span>5</span><span>;</span> <span>100</span><span>;</span> <span>450</span><span>;</span> <span>788</span> <span>]</span>
<span>for</span> <span>i</span> <span>in</span> <span>list1</span> <span>do</span>
   <span>printfn</span> <span>"%d"</span> <span>i</span>

<span>// Looping over a sequence of tuples</span>
<span>let</span> <span>seq1</span> <span>=</span> <span>seq</span> <span>{</span> <span>for</span> <span>i</span> <span>in</span> <span>1</span> <span>..</span> <span>10</span> <span>-&gt;</span> <span>(</span><span>i</span><span>,</span> <span>i</span><span>*</span><span>i</span><span>)</span> <span>}</span>
<span>for</span> <span>(</span><span>a</span><span>,</span> <span>asqr</span><span>)</span> <span>in</span> <span>seq1</span> <span>do</span>
  <span>printfn</span> <span>"%d squared is %d"</span> <span>a</span> <span>asqr</span>

<span>// A simple for...to loop.</span>
<span>let</span> <span>function1</span> <span>()</span> <span>=</span>
  <span>for</span> <span>i</span> <span>=</span> <span>1</span> <span>to</span> <span>10</span> <span>do</span>
    <span>printf</span> <span>"%d "</span> <span>i</span>
  <span>printfn</span> <span>""</span>

<span>// A for...to loop that counts in reverse.</span>
<span>let</span> <span>function2</span> <span>()</span> <span>=</span>
  <span>for</span> <span>i</span> <span>=</span> <span>10</span> <span>downto</span> <span>1</span> <span>do</span>
    <span>printf</span> <span>"%d "</span> <span>i</span>
  <span>printfn</span> <span>""</span>

<span>// Records</span>

<span>// Labels are separated by semicolons when defined on the same line.</span>
<span>type</span> <span>Point</span> <span>=</span> <span>{</span> <span>X</span><span>:</span> <span>float</span><span>;</span> <span>Y</span><span>:</span> <span>float</span><span>;</span> <span>Z</span><span>:</span> <span>float</span> <span>}</span>

<span>// You can define labels on their own line with or without a semicolon.</span>
<span>type</span> <span>Customer</span> <span>=</span>
    <span>{</span> <span>First</span><span>:</span> <span>string</span>
      <span>Last</span><span>:</span> <span>string</span>
      <span>SSN</span><span>:</span> <span>uint32</span>
      <span>AccountNumber</span><span>:</span> <span>uint32</span> <span>}</span>

<span>let</span> <span>mypoint</span> <span>=</span> <span>{</span> <span>X</span> <span>=</span> <span>1</span><span>.</span><span>0</span><span>;</span> <span>Y</span> <span>=</span> <span>1</span><span>.</span><span>0</span><span>;</span> <span>Z</span> <span>=</span> <span>-</span><span>1</span><span>.</span><span>0</span> <span>}</span>

<span>// Discriminated Union</span>
<span>type</span> <span>Shape</span> <span>=</span>
    <span>|</span> <span>Circle</span> <span>of</span> <span>radius</span><span>:</span> <span>float</span>
    <span>|</span> <span>Rectangle</span> <span>of</span> <span>width</span><span>:</span> <span>float</span> <span>*</span> <span>height</span><span>:</span> <span>float</span>

<span>// Functing using pattern matching</span>
<span>let</span> <span>area</span> <span>shape</span> <span>=</span>
    <span>match</span> <span>shape</span> <span>with</span>
    <span>|</span> <span>Circle</span> <span>radius</span> <span>-&gt;</span> <span>System</span><span>.</span><span>Math</span><span>.</span><span>PI</span> <span>*</span> <span>radius</span> <span>*</span> <span>radius</span>
    <span>|</span> <span>Rectangle</span> <span>(</span><span>width</span><span>,</span> <span>height</span><span>)</span> <span>-&gt;</span> <span>width</span> <span>*</span> <span>height</span>

<span>let</span> <span>circle</span> <span>=</span> <span>Circle</span> <span>5</span><span>.</span><span>0</span>
<span>let</span> <span>rectangle</span> <span>=</span> <span>Rectangle</span><span>(</span><span>4</span><span>.</span><span>0</span><span>,</span> <span>3</span><span>.</span><span>0</span><span>)</span>

<span>printfn</span> <span>"Circle area: %f"</span> <span>(</span><span>area</span> <span>circle</span><span>)</span>
<span>printfn</span> <span>"Rectangle area: %f"</span> <span>(</span><span>area</span> <span>rectangle</span><span>)</span>
</code></pre></div>

<p>Nothing shocking here, right?</p>

<p>Here’s another slightly more involved example:</p>

<div><pre><code><span>open</span> <span>System</span>

<span>// Sample data - simple sales records</span>
<span>type</span> <span>SalesRecord</span> <span>=</span> <span>{</span> <span>Date</span><span>:</span> <span>DateTime</span><span>;</span> <span>Product</span><span>:</span> <span>string</span><span>;</span> <span>Amount</span><span>:</span> <span>decimal</span><span>;</span> <span>Region</span><span>:</span> <span>string</span> <span>}</span>

<span>// Sample dataset</span>
<span>let</span> <span>sales</span> <span>=</span> <span>[</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>15</span><span>);</span> <span>Product</span> <span>=</span> <span>"Laptop"</span><span>;</span> <span>Amount</span> <span>=</span> <span>1200</span><span>m</span><span>;</span> <span>Region</span> <span>=</span> <span>"North"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>);</span>  <span>Product</span> <span>=</span> <span>"Phone"</span><span>;</span>  <span>Amount</span> <span>=</span> <span>800</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"South"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>20</span><span>);</span> <span>Product</span> <span>=</span> <span>"Tablet"</span><span>;</span> <span>Amount</span> <span>=</span> <span>400</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"North"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>2</span><span>,</span> <span>18</span><span>);</span> <span>Product</span> <span>=</span> <span>"Laptop"</span><span>;</span> <span>Amount</span> <span>=</span> <span>1250</span><span>m</span><span>;</span> <span>Region</span> <span>=</span> <span>"East"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>);</span>  <span>Product</span> <span>=</span> <span>"Phone"</span><span>;</span>  <span>Amount</span> <span>=</span> <span>750</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"West"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>2</span><span>,</span> <span>12</span><span>);</span> <span>Product</span> <span>=</span> <span>"Tablet"</span><span>;</span> <span>Amount</span> <span>=</span> <span>450</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"North"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>);</span> <span>Product</span> <span>=</span> <span>"Laptop"</span><span>;</span> <span>Amount</span> <span>=</span> <span>1150</span><span>m</span><span>;</span> <span>Region</span> <span>=</span> <span>"South"</span> <span>}</span>
<span>]</span>

<span>// Quick analysis pipeline</span>
<span>let</span> <span>salesSummary</span> <span>=</span>
    <span>sales</span>
    <span>|&gt;</span> <span>List</span><span>.</span><span>groupBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Product</span><span>)</span>                          <span>// Group by product</span>
    <span>|&gt;</span> <span>List</span><span>.</span><span>map</span> <span>(</span><span>fun</span> <span>(</span><span>product</span><span>,</span> <span>items</span><span>)</span> <span>-&gt;</span>                          <span>// Transform each group</span>
        <span>let</span> <span>totalSales</span> <span>=</span> <span>items</span> <span>|&gt;</span> <span>List</span><span>.</span><span>sumBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Amount</span><span>)</span>
        <span>let</span> <span>avgSale</span> <span>=</span> <span>totalSales</span> <span>/</span> <span>decimal</span> <span>(</span><span>List</span><span>.</span><span>length</span> <span>items</span><span>)</span>
        <span>let</span> <span>topRegion</span> <span>=</span>
            <span>items</span>
            <span>|&gt;</span> <span>List</span><span>.</span><span>groupBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Region</span><span>)</span>                   <span>// Nested grouping</span>
            <span>|&gt;</span> <span>List</span><span>.</span><span>maxBy</span> <span>(</span><span>fun</span> <span>(_,</span> <span>regionItems</span><span>)</span> <span>-&gt;</span>
                <span>regionItems</span> <span>|&gt;</span> <span>List</span><span>.</span><span>sumBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Amount</span><span>))</span>
            <span>|&gt;</span> <span>fst</span>

        <span>(</span><span>product</span><span>,</span> <span>totalSales</span><span>,</span> <span>avgSale</span><span>,</span> <span>topRegion</span><span>))</span>
    <span>|&gt;</span> <span>List</span><span>.</span><span>sortByDescending</span> <span>(</span><span>fun</span> <span>(_,</span> <span>total</span><span>,</span> <span>_,</span> <span>_)</span> <span>-&gt;</span> <span>total</span><span>)</span>      <span>// Sort by total sales</span>

<span>// Display results</span>
<span>salesSummary</span>
<span>|&gt;</span> <span>List</span><span>.</span><span>iter</span> <span>(</span><span>fun</span> <span>(</span><span>product</span><span>,</span> <span>total</span><span>,</span> <span>avg</span><span>,</span> <span>region</span><span>)</span> <span>-&gt;</span>
    <span>printfn</span> <span>"%s: $%M total, $%M avg, top region: %s"</span>
        <span>product</span> <span>total</span> <span>avg</span> <span>region</span><span>)</span>
</code></pre></div>

<p>Why don’t you try saving the snippet above in a file called <code>Sales.fsx</code> and running it like this:</p>



<p>Now you know that F# is a great choice for ad-hoc scripts! Also, running <code>dotnet fsi</code> by itself
will pop an F# REPL where you can explore the language at your leisure.</p>

<p>I’m not going to go into great details here, as much of what I wrote about OCaml
<a href="https://batsov.com/articles/2022/08/29/ocaml-at-first-glance/">here</a> applies to F# as well.
I’d also suggest this quick <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/tour">tour of F#</a>
to get a better feel for its syntax.</p>

<p><strong>Tip:</strong> Check out the <a href="https://fsprojects.github.io/fsharp-cheatsheet/">F# cheatsheet</a>
if you’d like to see a quick syntax reference.</p>

<p>One thing that made a good impression to me is the focus of the language designers on
making F# approachable to newcomers, by providing a lot of small quality of life improvements
for them. Below are few examples, that probably don’t mean much to you, but would mean something
to people familiar with OCaml:</p>

<div><pre><code><span>// line comments</span>
<span>(* the classic ML comments are around as well *)</span>

<span>// mutable values</span>
<span>let</span> <span>mutable</span> <span>x</span> <span>=</span> <span>5</span>
<span>x</span> <span>&lt;-</span> <span>6</span>

<span>// ranges and slices</span>
<span>let</span> <span>l</span> <span>=</span> <span>[</span><span>1</span><span>..</span><span>2</span><span>..</span><span>10</span><span>]</span>
<span>name</span><span>[</span><span>5</span><span>..]</span>

<span>// C# method calls look pretty natural</span>
<span>let</span> <span>name</span> <span>=</span> <span>"FOO"</span><span>.</span><span>ToLower</span><span>()</span>

<span>// operators can be overloaded for different types</span>
<span>let</span> <span>string1</span> <span>=</span> <span>"Hello, "</span> <span>+</span> <span>"world"</span>
<span>let</span> <span>num1</span> <span>=</span> <span>1</span> <span>+</span> <span>2</span>
<span>let</span> <span>num2</span> <span>=</span> <span>1</span><span>.</span><span>0</span> <span>+</span> <span>2</span><span>.</span><span>5</span>

<span>// universal printing</span>
<span>printfn</span> <span>"%A"</span> <span>[</span><span>1</span><span>..</span><span>2</span><span>..</span><span>100</span><span>]</span>
</code></pre></div>

<p>I guess some of those might be controversial, depending on whether you’re a ML
language purist or not, but in my book anything that makes ML more popular is a
good thing.</p>

<p>Did I also mention it’s easy to work with unicode strings and regular expressions?</p>

<p>Often people say that F# is mostly a staging ground for future C# features, and perhaps that’s true.
I haven’t observed both languages long enough to have my own opinion on the subject, but I was impressed
to learn that <code>async/await</code> (of C# and later JavaScript fame) originated in… F# 2.0.</p>

<blockquote>
  <p>It all changed in 2012 when C#5 launched with the introduction of what has now
become the popularized <code>async/await</code> keyword pairing. This feature allowed you to
write code with all the benefits of hand-written asynchronous code, such as not
blocking the UI when a long-running process started, yet read like normal
synchronous code. This <code>async/await</code> pattern has now found its way into many
modern programming languages such as Python, JS, Swift, Rust, and even C++.</p>

  <p>F#’s approach to asynchronous programming is a little different from <code>async/await</code>
but achieves the same goal (in fact, <code>async/await</code> is a cut-down version of F#’s
approach, which was introduced a few years previously, in F#2).</p>

  <p>– Isaac Abraham, F# in Action</p>
</blockquote>

<p>Time will tell what will happen, but I think it’s unlikely that C# will ever be able to fully replace F#.</p>

<p>I’ve also found this <a href="https://www.reddit.com/r/fsharp/comments/xlegmc/why_doesnt_microsoft_use_f/">encouraging comment from 2022</a> that Microsoft might be willing to invest more in F#:</p>

<blockquote>
  <p>Some good news for you. After 10 years of F# being developed by 2.5 people
internally and some random community efforts, Microsoft has finally decided to
properly invest in F# and created a full-fledged team in Prague this
summer. I’m a dev in this team, just like you I was an F# fan for many years
so I am happy things got finally moving here.</p>
</blockquote>

<p>Looking at the changes in F# 8.0 and F 9.0, it seems the new full-fledged team
has done some great work!</p>

<h2 id="ecosystem">Ecosystem</h2>

<p>It’s hard to assess the ecosystem around F# after such a brief period, but overall it seems to
me that there are fairly few “native” F# libraries and frameworks out there and most people
rely heavily on the core .NET APIs and many third-party libraries and frameworks geared towards C#.
That’s a pretty common setup when it comes to hosted languages in general, so nothing surprising here as well.</p>

<p>If you’ve ever used another hosted language (e.g. Scala, Clojure, Groovy) then you probably know what
to expect.</p>

<p><a href="https://github.com/fsprojects/awesome-fsharp">Awesome F#</a> keeps track of popular F# libraries, tools and frameworks. I’ll highlight here the web development and data science libraries:</p>

<p><strong>Web Development</strong></p>

<ul>
  <li><strong>Giraffe</strong>: A lightweight library for building web applications using ASP.NET Core. It provides a functional approach to web development.</li>
  <li><strong>Suave</strong>: A simple and lightweight web server library with combinators for routing and task composition. (Giraffe was inspired by Suave)</li>
  <li><strong>Saturn</strong>: Built on top of Giraffe and ASP.NET Core, it offers an MVC-style framework inspired by Ruby on Rails and Elixir’s Phoenix.</li>
  <li><strong>Bolero</strong>: A framework for building client-side applications in F# using WebAssembly and Blazor.</li>
  <li><strong>Fable</strong>: A compiler that translates F# code into JavaScript, enabling integration with popular JavaScript ecosystems like React or Node.js.</li>
  <li><strong>Elmish</strong>: A model-view-update (MVU) architecture for building web UIs in F#, often used with Fable.</li>
  <li><strong>SAFE Stack</strong>: An end-to-end, functional-first stack for building cloud-ready web applications. It combines technologies like Saturn, Azure, Fable, and Elmish for a type-safe development experience.</li>
</ul>

<p><strong>Data Science</strong></p>

<ul>
  <li><strong>Deedle</strong>: A library for data manipulation and exploratory analysis, similar to pandas in Python.</li>
  <li><strong>DiffSharp</strong>: A library for automatic differentiation and machine learning.</li>
  <li><strong>FsLab</strong>: A collection of libraries tailored for data science, including visualization and statistical tools.</li>
</ul>

<p>I haven’t played much with any of them at this point yet, so I’ll reserve any
feedback and recommendations for some point in the future.</p>

<h2 id="documentation">Documentation</h2>

<p>The official documentation is pretty good, although I find it kind of weird that
some of it is hosted on <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/what-is-fsharp">Microsoft’s site</a>
and the rest is on <a href="https://fsharp.org/">https://fsharp.org/</a> (the site of the F# Software Foundation).</p>

<p>I really liked the following parts of the documentation:</p>

<ul>
  <li><a href="https://learn.microsoft.com/en-us/dotnet/fsharp/style-guide/">F# Style Guide</a></li>
  <li><a href="https://github.com/fsharp/fslang-design">F# Design</a> - a repository of RFCs (every language should have one of those!)</li>
  <li><a href="https://fsharp.github.io/fsharp-core-docs/">F# Standard Library API</a></li>
</ul>

<p><a href="https://fsharpforfunandprofit.com/">https://fsharpforfunandprofit.com/</a> is another good learning resource. (even if it seems a bit dated)</p>



<p>F# has a somewhat troubled dev tooling story, as historically
support for F# was great only in Visual Studio, and somewhat subpar
elsewhere. Fortunately, the tooling story has improved a lot in the past
decade:</p>

<blockquote>
  <p>In 2014 a technical breakthrough was made with the creation of the
FSharp.Compiler.Service (FCS) package by Tomas Petricek, Ryan Riley, and Dave
Thomas with many later contributors. This contains the core implementation of
the F# compiler, editor tooling and scripting engine in the form of a single
library and can be used to make F# tooling for a wide range of
situations. This has allowed F# to be delivered into many more editors,
scripting and documentation tools and allowed the development of alternative
backends for F#. Key editor community-based tooling includes Ionide, by
Krzysztof Cieślak and contributors, used for rich editing support in the
cross-platform VSCode editor, with over 1M downloads at time of writing.</p>

  <p>– Don Syme, The Early History of F#</p>
</blockquote>

<p>I’ve played with the F# plugins for several editors:</p>

<ul>
  <li>Emacs (<code>fsharp-mode</code>)</li>
  <li>Zed (third-party plugin)</li>
  <li>Helix (built-in support for F#)</li>
  <li>VS Code (<a href="https://ionide.io/">Ionide</a>)</li>
  <li>Rider (JetBrains’s .NET IDE)</li>
</ul>

<p>Overall, Rider and VS Code provide the most (and the most polished) features,
but the other options were quite usable as well.  That’s largely due to the fact
that the F# LSP server <code>fsautocomplete</code> (naming is hard!) is quite robust and
any editor with good LSP support gets a lot of functionality for free.</p>

<p>Still, I’ll mention that I found the tooling lacking in some regards:</p>

<ul>
  <li><code>fsharp-mode</code> doesn’t use TreeSitter (yet) and doesn’t seem to be very actively developed (looking at the code - it seems it was derived from <code>caml-mode</code>)</li>
  <li>Zed’s support for F# is quite spartan</li>
  <li>In VS Code shockingly the expanding and shrinking selection is broken, which is quite odd for what is supposed to be the flagship editor for F#</li>
</ul>

<p>I’m really struggling with VS Code’s keybindings (too many modifier keys and functions keys for my taste) and editing model, so I’ll likely stick with Emacs going forward. Or I’ll finally spend more quality time with neovim!</p>

<p>It seems that everyone is using the same code formatter (<code>Fantomas</code>), including the F# team, which is great!
The linter story in F# is not as great (seems the only popular linter <a href="https://fsprojects.github.io/FSharpLint/">FSharpLint</a> is abandonware these days), but when your
compiler is so good, you don’t really need a linter as much.</p>

<p>Oh, well… It seems that Microsoft are not really particularly invested in
supporting the tooling for F#, as pretty much all the major projects in this
space are community-driven.</p>

<p>Using AI coding agents (e.g. Copilot) with F# worked pretty well, but I didn’t
spend much time on this front.</p>

<p>In the end of the day any editor will likely do, as long as you’re using LSP.</p>

<p>By the way, I had an interesting observation while programming in F# (and OCaml for that matter) -
that when you’re working with a language with a really good type system you don’t really need that much
from your editor. Most the time I’m perfectly happy with just some inline type information (e.g. something like CodeLenses), auto-completion and the ability to easily send code to <code>fsi</code>. Simplicity continues
to be the ultimate sophistication…</p>

<p>Other tools that should be on your radar are:</p>

<ul>
  <li><a href="https://fsprojects.github.io/Paket/">Paket</a> - Paket is a dependency manager for .NET projects. Think of it as something like <code>bundler</code>, <code>npm</code> or <code>pip</code>, but for .NET’s NuGet package ecosystem.</li>
  <li><a href="https://fake.build/">FAKE</a> -  A DSL for build tasks and more, where you can use F# to specify the tasks. Somewhat similar to Ruby’s <code>rake</code>. Some people claim that’s the easiest way to sneak F# into an existing .NET project.</li>
</ul>

<h2 id="use-cases">Use Cases</h2>

<p>Given the depth and breath of .NET - I guess that sky is the limit for you!</p>

<p>Seems to me that F# will be a particularly good fit for data analysis and manipulation, because
of features like <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/tutorials/type-providers/">type providers</a>.</p>

<p>Probably a good fit for backend services and even full-stack apps, although I haven’t really played
with the F# first solutions in this space yet.</p>

<p>Fable and Elmish make F# a viable option for client-side programming and might offer
another easy way to sneak F# into your day-to-day work.</p>

<p><strong>Note:</strong> Historically, Fable has been used to target JavaScript but since Fable
4, you can also target other languages such as TypeScript, Rust, Python, and
more.</p>

<p>Here’s how easy it is to transpile an F# codebase into something else:</p>

<div><pre><code><span># If you want to transpile to JavaScript</span>
dotnet fable

<span># If you want to transpile to TypeScript</span>
dotnet fable <span>--lang</span> typescript

<span># If you want to transpile to Python</span>
dotnet fable <span>--lang</span> python
</code></pre></div>

<p>Cool stuff!</p>



<p>My initial impression of the community is that it’s fairly small, perhaps even
smaller than that of OCaml.  The F# Reddit and Discord (the one listed on
Reddit) seem like the most active places for F# conversations. There’s supposed
to be some F# Slack as well, but I couldn’t get an invite for it. (seems the
automated process for issuing those invites has been broken for a while)</p>

<p>I’m still not sure what’s the role Microsoft plays in the community, as I
haven’t seen much from them overall.</p>

<p>For a me a small community is not really a problem, as long as the community is
vibrant and active. Also - I’ve noticed I always feel more connected to smaller
communities. Moving from Java to Ruby back in the day felt like night and day as
far as community engagement and sense of belonging go.</p>

<p>I didn’t find many books and community sites/blogs dedicated to F#, but I didn’t
really expect to in the first place.</p>

<p>The most notable community initiatives I discovered were:</p>

<ul>
  <li><a href="https://amplifyingfsharp.io/">Amplifying F#</a> - an effort to promote F# and to get more businesses involved with it</li>
  <li><a href="https://fsharpforfunandprofit.com/">F# for Fun and Profit</a> - a collection of tutorials and essays on F#</li>
  <li><a href="https://fslab.org/">F# Lab</a> - The community driven toolkit for datascience in F#</li>
  <li><a href="https://sergeytihon.com/category/f-weekly/">F# Weekly</a> - a weekly newsletter about the latest developments in the world of F#</li>
</ul>

<p>Seems to me that more can be done to promote the language and engage new programmers and businesses
with it, although that’s never easy 20 years into the existence of some project. I continue to be
somewhat puzzled as to why Microsoft doesn’t market F# more, as I think it could be a great
marketing vehicle for them.</p>

<p>All in all - I don’t feel qualified to comment much on the F# community at this point.</p>

<h2 id="the-popularity-contest">The Popularity Contest</h2>

<p>Depending on the type of person you are you may or may not care about a a programming language’s
“popularity”. People often ask my why I spent a lot of time with languages that are unlikely to
ever result in job opportunities for me, e.g.:</p>

<ul>
  <li>Emacs Lisp</li>
  <li>Clojure</li>
  <li>OCaml</li>
  <li>F#</li>
</ul>

<p>Professional opportunities are important, of course, but so are:</p>

<ul>
  <li>having fun (and the F in F# stands for “fun”)</li>
  <li>learning new paradigms and ideas</li>
  <li>challenging yourself to think and work differently</li>
</ul>

<p>That being said, F# is not a popular language by most conventional metrics. It’s not highly ranked
on TIOBE, StackOverflow or most job boards. But it’s also not less popular than most “mainstream”
functional programming languages. The sad reality is that functional programming is still not
mainstream and perhaps it will never be.</p>

<p>A few more resources on the subject:</p>

<ul>
  <li><a href="https://medium.com/@lanayx/about-f-popularity-c9b78ed89252">About F#’s popularity</a></li>
  <li><a href="https://hamy.xyz/blog/2024-11_fsharp-popularity">How Popular is F# in 2024</a>
    <ul>
      <li>Here’s also a <a href="https://www.youtube.com/watch?v=JioaHcy_QE0&amp;t=1s">video</a> for the article above</li>
    </ul>
  </li>
</ul>

<h2 id="f-vs-ocaml">F# vs OCaml</h2>

<blockquote>
  <p>The early conception of F# was simple: to bring the benefits of OCaml to .NET and .NET to OCaml: a
marriage between strongly typed functional programming and .NET. Here “OCaml” meant both the
core of the language itself, and the pragmatic approach to strongly-typed functional programming
it represented. The initial task was relatively well-defined: I would re-implement the core of the
OCaml language and a portion of its base library to target the .NET Common Language Runtime.
The implementation would be fresh, i.e. not using any of the OCaml codebase, for legal clarity.</p>

  <p>– Don Syme, creator of F#, The Early History of F#</p>
</blockquote>

<p>F# was derived from OCaml, so the two languages share a lot of DNA. Early on
F# made some efforts to support as much of OCaml’s syntax as possible, and it
even allowed the use of <code>.ml</code> and <code>.mli</code> file extensions for F# code. Over time
the languages started to diverge a bit, though.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>Creating a language that’s independent from OCaml, of course, was something
intended from the very beginning. That’s also reflected in the decision
to chose the name F#, even if early versions of the language were called “Caml.NET”:</p>

<blockquote>
  <p>Although the first version of F# was initially presented as “Caml-for-.NET”,
in reality it was always a new language, designed for .NET from day 1. F# was
never fully compatible with any version of OCaml, though it shared a compatible
subset, and it took Caml-Light and OCaml as its principal sources of design
guidance and inspiration.</p>

  <p>– Don Syme, The Early History of F#</p>
</blockquote>

<p>If you ask most people about the pros and cons of F# over OCaml you’ll probably
get the following answers.</p>

<p><strong>F# Pros</strong></p>

<ul>
  <li>Runs on .NET
    <ul>
      <li>Tons of libraries are at disposal</li>
    </ul>
  </li>
  <li>Backed by Microsoft</li>
  <li>Arguably it’s a bit easier to learn by newcomers (especially those who have only experience with OO programming)
    <ul>
      <li>The syntax is slightly easier to pick up (I think)</li>
      <li>The compiler errors and warnings are “friendlier” (easier to understand)</li>
      <li>It’s easier to debug problems (partially related to the previous item)</li>
    </ul>
  </li>
  <li>Strong support for <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/tutorials/async">async programming</a></li>
  <li>Has some cool features, absent in OCaml, like:
    <ul>
      <li>Anonymous Records</li>
      <li>Active Patterns</li>
      <li>Computational expressions</li>
      <li>Sequence comprehensions</li>
      <li>Type Providers</li>
      <li>Units of measure</li>
    </ul>
  </li>
</ul>

<p><strong>F# Cons</strong></p>

<ul>
  <li>Runs on .NET
    <ul>
      <li>The interop with .NET influenced a lot of language design decisions (e.g. allowing <code>null</code>)</li>
    </ul>
  </li>
  <li>Backed by Microsoft
    <ul>
      <li>Not everyone likes Microsoft</li>
      <li>Seems the resources allocated to F# by Microsoft are modest</li>
      <li>It’s unclear how committed Microsoft will be to F# in the long run</li>
    </ul>
  </li>
  <li>Naming conventions: I like <code>snake_case</code> way more than <code>camelCase</code> and <code>PascalCase</code></li>
  <li>Misses some cool OCaml features
    <ul>
      <li>First-class modules and functors</li>
      <li>GADTs</li>
    </ul>
  </li>
  <li>Doesn’t have a friendly camel logo</li>
  <li>The name F# sounds cool, but is a search and filename nightmare (and you’ll see FSharp quite often in the wild)</li>
</ul>

<p>Both F# and OCaml can also target JavaScript runtimes as well - via <a href="https://fable.io/">Fable</a> on
the F# side and Js_of_ocaml and Melange on the OCaml side. Fable seems like a
more mature solution at a cursory glance, but I haven’t used any of the three
enough to be able to offer an informed opinion.</p>

<p>In the end of the day both remain two fairly similar robust, yet niche,
languages, which are unlikely to become very popular in the future. I’m guessing
working professionally with F# is more likely to happen for most people, as .NET
is super popular and I can imagine it’d be fairly easy to sneak a bit of F# here
in there in established C# codebases.</p>

<p>One weird thing I’ve noticed with F# projects is that they still use XML project
manifests, where you have to list the source files manually in the order in
which they should be compiled (to account for the dependencies between them). I
am a bit shocked that the compiler can’t handle the dependencies automatically,
but I guess that’s because in F# there’s not direct mapping between source files
and modules. At any rate - I prefer the OCaml compilation process (and Dune) way
more.</p>

<p>As my interest in MLs is mostly educational I’m personally leaning towards OCaml, but if I had to build
web services with an ML language I’d probably pick F#. I also have a weird respect for every language
with its own runtime, as this means that it’s unlikely that the runtime will force some compromises
on the language.</p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>All in all I liked F# way more than I expected to! In a way it reminded me of my
experience with Clojure back in the day in the sense that Clojure was the most
practical Lisp out there when it was released, mostly because of its great
interop with Java.</p>

<p>I have a feeling that if .NET was portable since day 1 probably ClojureCLR would have become
as popular as Clojure, and likely F# would have developed a bigger community and
broader usage by now. I’m fairly certain I would have never dabbled in .NET again
if it hadn’t been for .NET Core, and I doubt I’m the only one.</p>

<p>Learning OCaml is definitely not hard, but I think that people interested to learn some ML
dialect might have an easier time with F#. And, as mentioned earlier, you’ll probably have an
easier path to “production” with it.</p>

<p>I think that everyone who has experience with .NET will benefit from learning F#.
Perhaps more importantly - everyone looking to do more with an ML family language
should definitely consider F#, as it’s a great language in its own right, that gives
you access to one of the most powerful programming platforms out there.</p>

<p>Let’s not forget about <a href="https://fable.io/">Fable</a>, which makes it possible for you leverage
F# in JavaScript, Dart, Rust and Python runtimes!</p>

<p>So, why F#? In the F# community there’s the saying that the “F” in F# stands for
“Fun”. In my brief experience with F# I found this to be very true! I’ll go a
step further and make the claim that F# is both seriously <strong>fun</strong> and seriously
practical!</p>

<p>Also if your code compiles - it will probably work the way you expect it to. I
hear that’s generally considered a desirable thing in the world of programming!</p>

<p>That’s all I have for you today. Please, share in the comments what do you love about F#!</p>

<p>In sane type systems we trust!</p>

<h2 id="discussions">Discussions</h2>

<ul>
  <li><a href="https://news.ycombinator.com/item?id=43546004">Hacker News</a></li>
  <li><a href="https://lobste.rs/s/kubso9/why_f">Lobsters</a></li>
</ul>



        
      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electron Band Structure in Germanium, My Ass (542 pts)]]></title>
            <link>https://pages.cs.wisc.edu/~kovar/hall.html</link>
            <guid>43545917</guid>
            <pubDate>Tue, 01 Apr 2025 12:25:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pages.cs.wisc.edu/~kovar/hall.html">https://pages.cs.wisc.edu/~kovar/hall.html</a>, See on <a href="https://news.ycombinator.com/item?id=43545917">Hacker News</a></p>
<div id="readability-page-1" class="page">
<base target="top">
<basefont size="3">

<b></b>
<p><b> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Abstract: The exponential dependence of resistivity on temperature 
in germanium is found to be a great big lie.  My careful theoretical modeling and painstaking experimentation reveal 1) that my equipment 
is crap, as are all the available texts on the subject and 2) that this whole exercise was a complete waste of my 
time.
</b>
 </p>

<h3><u>Introduction</u></h3> 

<p> 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  Electrons in germanium are confined to 
well-defined energy bands that are separated by "forbidden regions" of zero charge-carrier density.  You can 
read about it yourself if you want to, although I don't recommend it.  You'll have to wade through an obtuse, convoluted discussion about considering an arbitrary number of 
non-coupled harmonic-oscillator potentials and taking limits and so on.  The upshot is that if you heat up a sample of germanium, electrons will jump from a
non-conductive energy band to a conductive one, thereby creating a measurable change in resistivity.  This 
relation between temperature and resistivity can be shown to be exponential in certain temperature regimes 
by waving your hands and chanting "to first order".  
 </p>
<h3><u>Experiment procedure</u></h3> 

<p>
&nbsp; &nbsp; &nbsp; I sifted through the box of germanium crystals and chose the one that appeared 
to be the least cracked.  Then I soldered wires onto the crystal in the spots shown in figure 2b of Lab Handout 
32.  Do you have any idea how hard it is to solder wires to germanium?  I'll tell you: real goddamn hard.  The 
solder simply won't stick, and you can forget about getting any of the grad students in the solid state labs to 
help you out. 
<br> 
&nbsp; &nbsp; &nbsp;  Once the wires were in place, I attached them as appropriate to the second-rate  
equipment I scavenged from the back of the lab, none of which worked properly.  I soon wised up and swiped 
replacements from the well-stocked research labs.   This is how they treat undergrads around here: they give 
you broken tools and then don't understand why  you don't get any results.
 <br>

<table>
<tbody><tr>
<td rowspan="3">&nbsp;</td>
<td>
<img src="https://pages.cs.wisc.edu/~kovar/fittedHall.gif" width="351" height="285">
</td></tr>
<tr><td></td></tr>
<tr><td>
<b><span face="Arial, Geneva, Helvetica, sans-serif" size="2">
Fig. 1: Check this shit out.
</span></b>
</td></tr></tbody></table>


&nbsp; &nbsp; &nbsp;  In order to control the temperature of the germanium, I attached the crystal to a 
copper rod, the upper end of which was attached to a heating coil and the lower end of which was dipped in 
a thermos of liquid nitrogen.   Midway through the project, the thermos began leaking.  That's right: I pay a cool ten grand a quarter to 
come here, and yet they can't spare the five bucks to ensure that I have a working thermos.  
</p>
<h3><u>Results</u></h3> 

<p>
&nbsp; &nbsp; &nbsp; Check this shit out (Fig. 1).  That's bonafide, 100%-real data, my friends.  I took it 
myself over the course of two weeks.  And this was not a leisurely two weeks, either; I busted my
ass day and night in order to provide you with nothing but the best data possible.   Now, let's look a bit more closely
at this data, remembering that it is absolutely first-rate.  Do you see the exponential dependence?  I sure don't.  I see a bunch of crap.<br>
&nbsp; &nbsp; &nbsp; Christ, this was such a waste of my time. <br>
&nbsp; &nbsp; &nbsp;  Banking on my hopes that whoever grades this will just look at the pictures, I drew an 
exponential through my noise.  I believe the apparent legitimacy is enhanced by the fact that I used a complicated computer program
to make the fit.  I understand this is the same process by which the top quark was discovered.
</p>
<h3><u>Conclusion</u></h3> 

<p>
&nbsp; &nbsp; &nbsp; Going into physics was the biggest mistake of my life.  I should've declared CS.  I still 
wouldn't have any women, but at least I'd be rolling in cash.
</p>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[CERN scientists find evidence of quantum entanglement in sheep (209 pts)]]></title>
            <link>https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep</link>
            <guid>43545349</guid>
            <pubDate>Tue, 01 Apr 2025 11:08:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep">https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep</a>, See on <a href="https://news.ycombinator.com/item?id=43545349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <figure id="CERN-PHOTO-201706-157-2"><a href="https://cds.cern.ch/images/CERN-PHOTO-201706-157-2" title="View on CDS"><img alt="Life at CERN" src="https://cds.cern.ch/images/CERN-PHOTO-201706-157-2/file?size=large"></a><figcaption>The CERN flock of sheep on site in 2017.<span> (Image: CERN)</span></figcaption></figure>
<p>Quantum entanglement is a fascinating phenomenon where two particles’ states are tied to each other, no matter how far apart the particles are. In 2022, the <a href="https://home.cern/news/news/knowledge-sharing/cern-congratulates-winners-2022-nobel-prize-physics">Nobel Prize in Physics</a> was awarded to Alain Aspect, John F. Clauser and Anton Zeilinger for groundbreaking experiments involving entangled photons. These experiments confirmed the predictions for the manifestation of entanglement that had been made by the <a href="https://home.cern/news/news/physics/fifty-years-bells-theorem">late CERN theorist John Bell</a>. This phenomenon has so far been observed in a wide variety of systems, such as in top quarks at <a href="https://home.cern/news/press-release/physics/lhc-experiments-cern-observe-quantum-entanglement-highest-energy-yet">CERN’s Large Hadron Collider</a> (LHC) in 2024. Entanglement has also found several important societal applications, such as quantum cryptography and quantum computing. Now, it also explains the famous herd mentality of sheep.</p>

<p>A flock of sheep (ovis aries) has roamed the CERN site during the spring and summer months <a href="https://cds.cern.ch/record/970008?ln=en">for over 40 years</a>. Along with the CERN shepherd, they help to maintain the vast expanses of grassland around the LHC and are part of the Organization’s long-standing <a href="https://home.cern/news/news/cern/environmental-awareness-exploring-cerns-biodiversity">efforts to protect the site’s biodiversity</a>. In addition, their <a href="https://www.nature.com/articles/s41567-022-01769-8">flocking behaviour</a> has been of great interest to CERN's physicists. It is well known that sheep <a href="https://physicsworld.com/a/field-work-the-physics-of-sheep-from-phase-transitions-to-collective-motion/">behave like particles</a>: their stochastic behaviour has been studied by zoologists and physicists alike, who noticed that a flock’s ability to quickly change phase is similar to that of atoms in a solid and a liquid. Known as the Lamb Shift, this can cause them to get themselves into bizarre situations, such as walking in a circle for <a href="https://www.abc.net.au/news/science/2022-11-22/sheep-circling-mystery-could-have-simple-explanation/101682672">days on end.</a></p>

<p>Now, new research has shed light on the reason for these extraordinary abilities. Scientists at CERN have found evidence of quantum entanglement in sheep. Using sophisticated modelling techniques and specialised trackers, the findings show that the brains of individual sheep in a flock are quantum-entangled in such a way that the sheep can move and vocalise simultaneously, no matter how far apart they are. The evidence has several ramifications for ovine research and has set the baa for a new branch of quantum physics.</p>

<p>“The fact that we were having our lunch next to the flock was a shear coincidence,” says Mary Little, leader of the <a href="https://greybook.cern.ch/experiment/detail?id=HERD">HERD collaboration</a>, describing how the project came about. “When we saw and herd their behaviour, we wanted to investigate the movement of the flock using the technology at our disposal at the Laboratory.”</p>

<p>Observing the sheep’s ability to simultaneously move and vocalise together caused one main question to aries: since the sheep behave like subatomic particles, could quantum effects be the reason for their behaviour?</p>

<p>“Obviously, we couldn’t put them all in a box and see if they were dead or alive,” said Beau Peep, a researcher on the project. “However, by assuming that the sheep were spherical, we were able to model their behaviour in almost the exact same way as we model subatomic particles.”</p>

<p>Using sophisticated trackers, akin to those in the LHC experiments, the physicists were able to locate the precise particles in the sheep’s brains that might be the cause of this entanglement. Dubbed “moutons” and represented by the Greek letter lambda, l, these particles are leptons and are close relatives of the muon, but fluffier.</p>

<p>The statistical significance of the findings is 4 <a href="https://home.cern/resources/faqs/five-sigma">sigma</a>, which is enough to show evidence of the phenomenon. However, it does not quite pass the baa to be classed as an observation.</p>

<p>“More research is needed to fully confirm that this was indeed an observation of ovine entanglement or a statistical fluctuation,” says Ewen Woolly, spokesperson for the HERD collaboration. “This may be difficult, as we have found that the research makes physicists become inexplicably drowsy.”</p>

<p>“While entanglement is now the leading theory for this phenomenon, we have to take everything into account,” adds Dolly Shepherd, a CERN theorist. “Who knows, maybe further variables are hidden beneath their fleeces. Wolves, for example.”</p>

<figure id="CERN-HOMEWEB-PHO-2025-028-1"><a href="https://cds.cern.ch/images/CERN-HOMEWEB-PHO-2025-028-1" title="View on CDS"><img alt="home.cern,Life at CERN" src="https://cds.cern.ch/images/CERN-HOMEWEB-PHO-2025-028-1/file?size=large"></a>
<figcaption>Theoretical physicist John Ellis, pioneer of the penguin diagram, with its updated sheep version. Scientists at CERN find evidence of quantum entanglement in sheep in 2025, the year declared by the United Nations as the <a href="https://home.cern/news/news/knowledge-sharing/official-launch-quantum-year">International Year of Quantum Science and Technology.</a>&nbsp;<span>(Image: CERN)</span></figcaption></figure>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-Hosting like it's 2025 (189 pts)]]></title>
            <link>https://kiranet.org/self-hosting-like-its-2025/</link>
            <guid>43544979</guid>
            <pubDate>Tue, 01 Apr 2025 10:11:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kiranet.org/self-hosting-like-its-2025/">https://kiranet.org/self-hosting-like-its-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=43544979">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[US accidentally sent Maryland father to Salvadorian prison, can't get him back (164 pts)]]></title>
            <link>https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html</link>
            <guid>43544534</guid>
            <pubDate>Tue, 01 Apr 2025 09:14:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html">https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html</a>, See on <a href="https://news.ycombinator.com/item?id=43544534">Hacker News</a></p>
Couldn't get https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The April Fools joke that might have got me fired (335 pts)]]></title>
            <link>http://oldvcr.blogspot.com/2025/04/the-april-fools-joke-that-might-have.html</link>
            <guid>43543743</guid>
            <pubDate>Tue, 01 Apr 2025 07:11:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://oldvcr.blogspot.com/2025/04/the-april-fools-joke-that-might-have.html">http://oldvcr.blogspot.com/2025/04/the-april-fools-joke-that-might-have.html</a>, See on <a href="https://news.ycombinator.com/item?id=43543743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-6562913850949730620" itemprop="description articleBody"><p>
Everyone should pull one great practical joke in their lifetimes. This one was mine, and I think it's past the statute of limitations. The story is true. Only the names are redacted to protect the guilty.
</p><p>
My first job out of college was a database programmer, even though my undergraduate degree had nothing to do with computers and my current profession still mostly doesn't. The reason was that the University I worked for couldn't afford competitive wages, but they did offer various fringe benefits, and they were willing to train someone who at least had decent working knowledge. I, as a newly minted graduate of the august University of California system, had decent working knowledge at least of BSD/386 and SunOS, but more importantly also had the glowing recommendation of my predecessor who was being promoted into a new position. I was hired, which was their first mistake.
</p><p>
<a name="more"></a>
The system I was hired to work on was an HP 9000 K250, one of Hewlett-Packard's big PA-RISC servers. I wish I had a photograph of it, but all I have are a couple bad scans of some bad Polaroids of my office and none of the server room. The server room was downstairs from my office back in the days when server rooms were on-premises, complete with a swipe card lock and a halon system that would give you a few seconds of grace before it flooded everything. The K250 hulked in there where it had recently replaced what I think was an Encore mini of some sort (probably a Multimax, since it was a few years old and the 88K Encores would have been too new for the University), along with the AIX RS/6000s that provided student and faculty shell accounts and E-mail, the bonded T1 lines, some of the terminal servers, the massive Cabletron routers and a lot of the telco stuff. One of the tape reels from the Encore hangs on my wall today as a memento.
</p><p>
The K250 and the Encore it replaced (as well as the L-Class that later replaced the K250 when I was a consultant) ran an all-singing, all-dancing student information system called CARS. CARS is still around, renamed <a href="https://jenzabar.com/">Jenzabar</a>, though I suspect that many of its underpinnings remain if you look under the table. In those days CARS was a massive overlay that was loaded atop the operating system and database, which when I started were, respectively, HP/UX 10.20 and Informix. (I'm old.) It used Informix tables, screens and stored procedures plus its own text UI libraries to run code written variously as Perform screens, SQL, C-shell scripts and plain old C or ESQL/C. Everything was tracked in RCS using overgrown <tt>Makefile</tt>s. I had the admin side (resource management, financials, attendance trackers, etc.) and my office partner had the academic side (mostly grades and faculty tracking). My job was to write and maintain this code and shortly after to help the University create custom applications in CARS' brand-spanking new web module, which chose the new hotness in scripting languages, i.e., Perl. Fortuitously I had learned Perl in, appropriately enough, a computational linguistics course.
</p><p>
CARS also managed most of the printers on campus except for the few that the RS/6000s controlled directly. Most of the campus admin printers were HP LaserJet 4 units of some derivation equipped with JetDirect cards for networking. These are great warhorse printers, some of the best laser printers HP ever made. I suspect there were line printers other places, but those printers were largely what existed in the University's offices.
</p><p>
It turns out that the <tt>READY</tt> message these printers show on their VFD panels is changeable. I don't remember where I read this, probably idly paging through the manual over a lunch break, but initially the only fun things I could think of to do was to have the printer say hi to my boss when she sent jobs to it, stuff like that (whereupon she would tell me to get back to work). Then it dawned on me: because I had access to the printer spools on the K250, and the spool directories were conveniently named the same as their hostnames, I knew where each and every networked LaserJet on campus was. I was young, rash and motivated. This was a hack I just couldn't resist. It would be even better than what had been my favourite joke at my alma mater, where campus services, notable for posting various service suspension notices, posted one April Fools' Day that gravity itself would be suspended to various buildings. I felt sure this hack would eclipse that too.
</p><p>
The plan on April Fools' Day was to get into work at OMG early o'clock and iterate over every entry in the spool, sending it a sequence that would change the <tt>READY</tt> message to <tt>INSERT 5 CENTS</tt>. This would cause every networked LaserJet on campus to appear to ask for a nickel before you printed anything. The script was very simple (this is the actual script, I saved it):
</p><div><pre>#!/bin/csh -f

cd /opt/carsi/spool
foreach i (*)
        echo '^[%-12345X@PJL RDYMSG DISPLAY="INSERT 5 CENTS"' | netto $i 9100
end
</pre></div>
<p>
The <tt>^[</tt> was a literal ASCII 27 ESCape character, and <tt>netto</tt> was a simple <tt>netcat</tt>-like script I had written in these days before <tt>netcat</tt> was widely used. That's it.

Now, let me be clear: the printer was <em>still</em> ready! The effect was merely cosmetic! It would still print if you sent jobs to it! Nevertheless, to complete the effect, this message was sent out on the campus-wide administration mailing list (which I also saved):
</p><div><pre>To: xxx@xxx.xxx
Date: xxx, 1 Apr xxxx 05:41:34 -0800 (PST)
Subject: IMPORTANT NOTE ON PRINTER POLICY

Due to the increasing costs of service commitments for campus printers,
all printers on campus will be reprogrammed for pay-per-page service
to defray these mounting expenses, effective immediately.

Most printers will now require a 5 cent deposit per page for printing. This
may be paid on account or through special coin acceptors to be installed
on the unit by technicians through the end of this week. If your office has
not yet established an account, your printer will automatically request you to 
insert 5 cents into the slot per page to be printed. Please check your
printer's LCD [sic] display to see if your printer requires the 5 cents per
page before using your printer.

Additional printers will be retrofitted as soon as possible. Technicians
will be contacting departments with specific details.

All accounts will be maintained on CARS. Do not call the Helpdesk. To
establish or verify your department's printer account, please call me at
xxxx.

Please also direct all questions regarding this new policy to me as well.

We apologise for the inconvenience and hope that the new cost requirement
will not adversely affect your department's productivity.
</pre></div>
<p>
At the end of the day I would reset everything back to <tt>READY</tt>, smile smugly, and continue with my menial existence. That was the plan.
</p><p>
Having sent this out, I fielded a few anxious calls, who laughed uproariously when they realized, and I reset their printers manually afterwards. The people who knew me, knew I was a practical joker, took note of the date, and sent approving replies. One of the best was sent to me later in the day by intercampus mail, printed on their laser printer, with a nickel taped to it.
</p><p>
Unfortunately, not everybody on campus knew me, and those who did not not only did <em>not</em> call me, but instead called university administration directly. By 8:30am it was chaos in the main office and this filtered up to the head of HR, who most definitely <em>did</em> know me, and told me I'd better send a retraction before the CFO got in or I was in big trouble. That went wrong also, because my retraction said that campus administration was not considering charging per-page fees when in fact they actually were, so I had to retract it and send a <em>new</em> retraction that didn't call attention to that fact. I also ran the script to reset everything early. Eventually the hubbub finally settled down around noon. Everybody in the office thought it was very funny. Even my boss, who officially disapproved, thought it was somewhat funny.
</p><p>
The other thing that went wrong, as if all that weren't enough, was that the director of IT — which is to say, my boss's boss — was away on vacation when all this took place. (Read E-mail remotely? Who does <em>that</em>?) I compounded this situation with the tactical error of going skiing over the coming weekend and part of the next week, most of which I spent snowplowing down the bunny slopes face first, so that he discovered all the angry E-mail in his box without me around to explain myself. (My office partner remembers him coming in wide-eyed asking, "what did he <em>do</em>??") When I returned, it was icier in the office than it had been on the mountain. The assistant director, who thought it was funny, was in trouble for not putting a lid on it, and I was in really big trouble for doing it in the first place. I was appropriately contrite and made various apologies and was an uncharacteristically model employee for an unnaturally long period of time.
</p><p>
The Ice Age eventually thawed and the incident was officially dropped except for a "poor judgment" on my next performance review and the satisfaction of what was then considered the best practical joke ever pulled on campus. Indeed, everyone agreed it was much more technically accomplished than the previous award winner, where someone had supposedly gotten it around the grounds that the security guards at the entrance would be charging a nominal admission fee per head. Years later they still said it was legendary.
</p><p>
I like to think they still do.
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Nue – Apps lighter than a React button (600 pts)]]></title>
            <link>https://nuejs.org/blog/large-scale-apps/</link>
            <guid>43543241</guid>
            <pubDate>Tue, 01 Apr 2025 05:47:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nuejs.org/blog/large-scale-apps/">https://nuejs.org/blog/large-scale-apps/</a>, See on <a href="https://news.ycombinator.com/item?id=43543241">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    <article>
      <header>
  <time datetime="2025-04-01T00:00:00.000Z">April 1, 2025</time>

  

  

</header>
      <p>On this release we're showing what you can do by taking the modern web standards — HTML, CSS, and JS — to their absolute peak:</p>

<p>The entire app is significantly lighter than a React button:</p>
<figure><picture><source srcset="https://nuejs.org/img/react-button-vs-nue-spa.png" media="(max-width: 750px)" type="image/png">
<source srcset="https://nuejs.org/img/react-button-vs-nue-spa-big.png" media="(min-width: 750px)" type="image/png">
<img loading="lazy" src="https://nuejs.org/img/react-button-vs-nue-spa-big.png" width="704" height="394"></picture></figure>
<p>See benchmark and details <a href="https://nuejs.org/docs/react-button-vs-nue.html">here ›</a></p>
<h2 id="going-large-scale"><a href="#going-large-scale" title="Going large-scale"></a>Going large-scale</h2>
<p>Here’s the same app, now with a <strong>Rust</strong> computation engine and <strong>Event Sourcing</strong> for instant search and other operations over <strong>150,000</strong> records — far past where JavaScript (and React) would crash with a stack overflow error:</p>
<p>

  

  <bunny-player custom="bunny-player">
  
</bunny-player>

  <figcaption>
    Instant operations across 150.000 records with Rust/WASM
  </figcaption>

  

</p>
<p>See this demo <a href="https://mpa.nuejs.org/app/?rust">live ›</a></p>

<p>Nue crushes HMR and build speed records and sets you up with a millisecond feedback loop for your day-to-day VSCode/Sublime file-save operations:</p>
<p>

  

  <bunny-player custom="bunny-player">
  
</bunny-player>

  <figcaption>
    Immediate feedback for design and component updates, preserving app state
  </figcaption>

  

</p>
<video src="https://nuejs.org/img/mpa-build.mp4" type="video/mp4" autoplay="" loop="" muted="" width="350"></video>

<p>Here's what this means:</p>
<h3 id="for-rust-go-and-js-engineers"><a href="#for-rust-go-and-js-engineers" title="For Rust, Go, and JS engineers"></a>For Rust, Go, and JS engineers</h3>
<p>This is a wake-up call for Rust, Go, and JS engineers stuck wrestling with React idioms instead of leaning on timeless software patterns. Nue emphasizes a model-first approach, delivering modular design with simple, testable functions, true static typing, and minimal dependencies. Nue is a liberating experience for system devs whose skills can finally shine in a separated model layer.</p>
<h3 id="for-design-engineers"><a href="#for-design-engineers" title="For Design Engineers"></a>For Design Engineers</h3>
<p>This is a wake-up call for design engineers bogged down by React patterns and <a href="https://github.com/shadcn-ui/ui/tree/main/apps/v4/registry/new-york-v4">40,000+ line</a> design systems. Build radically simpler systems with modern CSS (@layers, variables, calc()) and take control of your typography and whitespace.</p>
<h3 id="for-ux-engineers"><a href="#for-ux-engineers" title="For UX Engineers"></a>For UX Engineers</h3>
<p>This is a wake-up call for UX engineers tangled in React hooks and utility class walls instead of owning the user experience. Build apps as light as a React button to push the web—and your skills—forward.</p>
<h2 id="faq-wth-is-nue"><a href="#faq-wth-is-nue" title="FAQ: WTH is Nue?"></a>FAQ: WTH is Nue?</h2>
<p>Nue is a web framework focused on web standards, currently in active development. We aim to reveal the hidden complexity that’s become normalized in modern web development. When a single button outweighs an entire application, something’s fundamentally broken.</p>
<p>Nue drives the inevitable shift. We’re rebuilding tools and frameworks from the ground up with a cleaner, more robust architecture. Our goal is to restore the joy of web development for all key skill sets: frontend architects, design engineers, and UX engineers.</p>
      
    </article>

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Duolingo-style exercises but with real-world content like the news (363 pts)]]></title>
            <link>https://app.fluentsubs.com/exercises/daily</link>
            <guid>43543235</guid>
            <pubDate>Tue, 01 Apr 2025 05:46:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://app.fluentsubs.com/exercises/daily">https://app.fluentsubs.com/exercises/daily</a>, See on <a href="https://news.ycombinator.com/item?id=43543235">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[An 'Administrative Error' Sends a Maryland Father to a Salvadoran Prison (110 pts)]]></title>
            <link>https://www.theatlantic.com/politics/archive/2025/03/an-administrative-error-sends-a-man-to-a-salvadoran-prison/682254/</link>
            <guid>43542333</guid>
            <pubDate>Tue, 01 Apr 2025 02:51:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/politics/archive/2025/03/an-administrative-error-sends-a-man-to-a-salvadoran-prison/682254/">https://www.theatlantic.com/politics/archive/2025/03/an-administrative-error-sends-a-man-to-a-salvadoran-prison/682254/</a>, See on <a href="https://news.ycombinator.com/item?id=43542333">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true">The Trump administration acknowledged in a court <a data-event-element="inline link" href="https://storage.courtlistener.com/recap/gov.uscourts.mdd.578815/gov.uscourts.mdd.578815.11.0.pdf">filing</a> Monday that it had grabbed a Maryland father with protected legal status and mistakenly deported him to El Salvador, but said that U.S. courts lack jurisdiction to order his return from the megaprison where he’s now locked up.</p><p data-flatplan-paragraph="true">The case appears to be the first time the Trump administration has admitted to errors when it sent three planeloads of Salvadoran and Venezuelan deportees to El Salvador’s grim “Terrorism Confinement Center” on March 15. Attorneys for several Venezuelan deportees have said that the Trump administration falsely labeled their clients as gang members because of their tattoos. Trump officials have disputed those claims.</p><p data-flatplan-paragraph="true">But in Monday’s court filing, attorneys for the government admitted that the Salvadoran man, Kilmar Abrego Garcia, was deported accidentally. “Although ICE was aware of his protection from removal to El Salvador, Abrego Garcia was removed to El Salvador because of an administrative error,” the government told the court. Trump lawyers said the court has no ability to bring him back now that Abrego Garcia is in Salvadoran custody.</p><p data-flatplan-paragraph="true">Simon Sandoval-Moshenberg, Abrego Garcia’s attorney, said he’s never seen a case in which the government knowingly deported someone who had already received protected legal status from an immigration judge. He is asking the court to order the Trump administration to ask for Abrego Garcia’s return and, if necessary, to withhold payment to the Salvadoran government, which says it’s charging the United States $6 million a year to jail U.S. deportees.</p><p data-flatplan-paragraph="true">Trump administration attorneys told the court to dismiss the request on multiple grounds, including that Trump’s “primacy in foreign affairs” outweighs the interests of Abrego Garcia and his family.</p><p data-flatplan-paragraph="true">“They claim that the court is powerless to order any relief,’’ Sandoval-Moshenberg told me. “If that’s true, the immigration laws are meaningless—all of them—because the government can deport whoever they want, wherever they want, whenever they want, and no court can do anything about it once it’s done.”</p><p data-flatplan-paragraph="true">Court <a data-event-element="inline link" href="https://storage.courtlistener.com/recap/gov.uscourts.mdd.578815/gov.uscourts.mdd.578815.1.0.pdf">filings</a> show Abrego Garcia came to the United States at age 16 in 2011 after fleeing gang threats in his native El Salvador. In 2019 he received a form of protected legal status known as “withholding of removal” from a U.S. immigration judge who found he would likely be targeted by gangs if deported back.</p><p data-flatplan-paragraph="true">Abrego Garcia, who is married to a U.S. citizen and has a 5-year-old disabled child who is also a U.S. citizen, has no criminal record in the United States, according to his attorney. The Trump administration does not claim he has a criminal record, but called him a “danger to the community” and an active member of MS-13, the Salvadoran gang that Trump has declared a Foreign Terrorist Organization.</p><p data-flatplan-paragraph="true">Sandoval-Moshenberg said those charges are false, and the gang label stems from a 2019 incident when Abrego Garcia and three other men were detained in a Home Depot parking lot by a police detective in Prince George’s County, Maryland. During questioning, one of the men told officers Abrego Garcia was a gang member, but the man offered no proof and police said they didn’t believe him, filings show. Police did not identify him as a gang member.</p><p data-flatplan-paragraph="true">Abrego Garcia was not charged with a crime, but he was handed over to U.S. Immigration and Customs Enforcement after the arrest to face deportation. In those proceedings, the government claimed that a reliable informant had identified him as a ranking member of MS-13. Abrego Garcia and his family hired an attorney and fought the government’s attempt to deport him. He received “withholding of removal” six months later, a protected status.</p><p data-flatplan-paragraph="true">It is not a path to permanent U.S. residency, but it means the government won’t deport him back to his home country because he’s more likely than not to face harm there.</p><p data-flatplan-paragraph="true">Abrego Garcia has had no contact with any law enforcement agency since his release, according to his attorney. He works full time as a union sheetmetal apprentice, has complied with requirements to check in annually with ICE, and cares for his five-year-old son, who has autism and a hearing defect, and is unable to communicate verbally.</p><p data-flatplan-paragraph="true">On March 12 Abrego Garcia had picked up his son after work from the boy’s grandmother’s house when ICE officers stopped the car, saying his protected status had changed. Officers waited for Abrego Garcia’s wife to come to the scene and take care of the boy, then drove him away in handcuffs. Within two days he had been transferred to an ICE staging facility in Texas, along with other detainees the government was preparing to send to El Salvador. Trump had invoked the Alien Enemies Act of 1798, and the government planned to deport two planeloads of Venezuelans along with a separate group of Salvadorans.</p><p data-flatplan-paragraph="true">Abrego Garcia’s family has had no contact with him since he was sent to the megaprison in El Salvador, known as the CECOT. His wife spotted her husband in news photographs released by Salvadoran President Nayib Bukele on the morning of March 16, after a U.S. District Judge had told the Trump administration to halt the flights.</p><p data-flatplan-paragraph="true">“Oopsie,” Bukele <a data-event-element="inline link" href="https://x.com/nayibbukele/status/1901238762614517965">wrote</a> on social media, taunting the judge.</p><p data-flatplan-paragraph="true">Abrego Garcia’s wife recognized her husband’s decorative arm tattoo and scars, according to the court filing. The image showed Salvadoran guards in black ski masks frog-marching him into the prison, with his head shoved down toward the floor. The CECOT is the same prison Department of Homeland Security Secretary Kristi Noem visited last week, recording videos for social media while standing in front of a cell packed with silent detainees.</p><p data-flatplan-paragraph="true">If the government wants to deport someone with protected status, the standard course would be to reopen the case and introduce new evidence arguing for deportation. The deportation of a protected status holder has even stunned some government attorneys I’ve been in touch with who are tracking the case, who declined to be named because they weren’t authorized to speak to the press. “What. The. Fuck,” one texted me.</p><p data-flatplan-paragraph="true">Sandoval-Moshenberg told the court he believes Trump officials deported his client “through extrajudicial means because they believed that going through the immigration judge process took too long, and they feared that they might not win all of their cases.’’</p><p data-flatplan-paragraph="true">Officials at ICE and the Department of Homeland Security did not respond to a request for comment. The Monday court filing by the government indicates officials knew Abrego Garcia had legal protections shielding him from deportation to El Salvador.</p><p data-flatplan-paragraph="true">“ICE was aware of this grant of withholding of removal at the time [of] Abrego Garcia’s removal from the United States. Reference was made to this status on internal forms,” the government told the court in its filing.</p><p data-flatplan-paragraph="true">Abrego Garcia was not on the initial manifest of the deportation flight, but was listed “as an alternate,” the government attorneys explained. As other detainees were removed from the flight for various reasons, Abrego Garcia “moved up the list.’’</p><p data-flatplan-paragraph="true">The flight manifest “did not indicate that Abrego-Garcia should not be removed,’’ the attorneys said. “Through administrative error, Abrego-Garcia was removed from the United States to El Salvador. This was an oversight.” But despite this, they told the court that Abrego Garcia’s deportation was carried out ‘’in good faith.’’</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Get the hell out of the LLM as soon as possible (303 pts)]]></title>
            <link>https://sgnt.ai/p/hell-out-of-llms/</link>
            <guid>43542259</guid>
            <pubDate>Tue, 01 Apr 2025 02:34:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sgnt.ai/p/hell-out-of-llms/">https://sgnt.ai/p/hell-out-of-llms/</a>, See on <a href="https://news.ycombinator.com/item?id=43542259">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <figure>
  <img src="https://sgnt.ai/hell.jpeg" alt="Get out of there">
</figure>
<p>Don’t let an LLM make decisions or implement business logic: they suck at that. I build NPCs for an online game, and I get asked a lot “How did you get ChatGPT to do that?” The answer is invariably: “I didn’t, and also you shouldn’t”.</p>
<p>In most applications, the LLM should be the user-interface only between the user and an API into your application logic. The LLM shouldn’t be implementing any logic. Get the hell out of the LLM as soon as possible, and stay out as long as you can.</p>
<h2 id="y-tho">Y Tho?</h2>
<p>This is best illustrated by a contrived example: you want to write a chess-playing bot you access over WhatsApp. The user sends a description of what they want to do (“use my bishop to take the knight”), and the bot plays against them.</p>
<p>Could you get the LLM to be in charge of maintaining the state of the chess board and playing convincingly? <a href="https://dynomight.net/chess/">Possibly, maybe</a>. Would you? Hell no, for some intuitive reasons:</p>
<ul>
<li><strong>Performance</strong>: It’s impressive that LLMs might be able to play chess at all, but they suck at it (as of 2025-04-01). A specialized chess engine is always going to be a faster, better, cheaper chess player. Even modern chess engines like Stockfish that incorporate neural networks are still purpose-built specialized systems with well-defined inputs and evaluation functions - not general-purpose language models trying to maintain game state through text.</li>
<li><strong>Debugging and adjusting</strong>: It’s impossible to reason about and debug <em>why</em> the LLM made a given decision, which means it’s very hard to change <em>how</em> it makes those decisions if you need to tweak them. You don’t understand the journey it took through the high-dimensional semantic space to get to your answer, and it’s really poor at explaining it too. Even purpose-built neural networks like those in chess engines can be challenging for observability, and a general LLM is a nightmare, despite Anthropic’s <a href="https://www.anthropic.com/research/tracing-thoughts-language-model">great strides in this area</a></li>
<li><strong>And the rest…</strong>: testing LLM outputs is much harder than unit-testing known code-paths; LLMs are much worse at math than your CPU; LLMs are insufficiently good at picking random numbers; version-control and auditing becomes much harder; monitoring and observability gets painful; state management through natural language is fragile; you’re at the mercy of API rate limits and costs; and security boundaries become fuzzy when everything flows through prompts.</li>
</ul>
<h2 id="examples"><strong>Examples</strong></h2>
<p>The chess example illustrates the fundamental problem with using LLMs for core application logic, but this principle extends far beyond games. In any domain where precision, reliability, and efficiency matter, you should follow the same approach:</p>
<ol>
<li>The user says they want to attack player X with their vorpal sword? The LLM shouldn’t be the system figuring out is the user has a vorpal sword, or what the results of that would be: the LLM is responsible for translating the free-text the user gave you into an API call <em>only</em> and translating the result into text for the user</li>
<li>You’re building a negotiation agent that should respond to user offers? The LLM isn’t in charge of the negotiation, just in charge of packaging it up, passing it off to the negotiating engine, and telling the user about the result</li>
<li>You need to make a random choice about how to respond to the user? The LLM doesn’t get to choose</li>
</ol>
<h2 id="reminder-of-what-llms-are-good-at"><strong>Reminder of what LLMs are good at</strong></h2>
<p>While I’ve focused on what LLMs shouldn’t do, it’s equally important to understand their strengths so you can leverage them appropriately:</p>
<p>LLMs excel at transformation and at categorization, and have a pretty good grounding in “how the world works”, and this is where you in your process you should be deploying them.</p>
<p>The LLM is good at taking “hit the orc with my sword” and turning it into <code>attack(target="orc", weapon="sword")</code>. Or taking <code>{"error": "insufficient_funds"}</code> and turning it into “You don’t have enough gold for that.”</p>
<p>The LLM is good at figuring out what the hell the user is trying to do and routing it to the right part of your system. Is this a combat command? An inventory check? A request for help?</p>
<p>Finally, the LLM is good at knowing about human concepts, and knowing that a “blade” is probably a sword and “smash” probably means attack.</p>
<p>Notice that all these strengths involve transformation, interpretation, or communication—not complex decision-making or maintaining critical application state. By restricting LLMs to these roles, you get their benefits without the pitfalls described earlier.</p>
<h2 id="the-future"><strong>The future</strong></h2>
<p>What LLMs can and can’t do is ever-shifting and reminds me of the “<a href="https://en.wikipedia.org/wiki/God_of_the_gaps">God of the gaps</a>”. a term from theology where each mysterious phenomenon was once explained by divine intervention—until science filled that gap. Likewise, people constantly identify new “human-only” tasks to claim that LLMs aren’t truly intelligent or capable. Then, just a few months later, a new model emerges that handles those tasks just fine, forcing everyone to move the goalposts again, examples <em>passim</em>. It’s a constantly evolving target, and what seems out of reach today may be solved sooner than we expect.</p>
<p>And so like in our chess example, we will probably soon end up with LLMs that can handle all of our above examples reasonably well. I suspect however that most of the drawbacks won’t go away: your non-LLM logic that you pass off to is going to be easier to reason about, easier to maintain, cheaper to run, and more easily version-controlled.</p>
<p>Even as LLMs continue to improve, the fundamental architectural principle remains: use LLMs for what they’re best at—the interface layer—and rely on purpose-built systems for your core logic.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The case against conversational interfaces (256 pts)]]></title>
            <link>https://julian.digital/2025/03/27/the-case-against-conversational-interfaces/</link>
            <guid>43542131</guid>
            <pubDate>Tue, 01 Apr 2025 02:14:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://julian.digital/2025/03/27/the-case-against-conversational-interfaces/">https://julian.digital/2025/03/27/the-case-against-conversational-interfaces/</a>, See on <a href="https://news.ycombinator.com/item?id=43542131">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
 		
<p><span>01</span> Intro</p>



<p>Conversational interfaces are a bit of a meme. Every couple of years a shiny new AI development emerges and people in tech go <em>“This is it! The next computing paradigm is here! We’ll only use natural language going forward!”</em>. But then nothing actually changes and we continue using computers the way we always have, until the debate resurfaces a few years later.</p>



<p>We’ve gone through this cycle a couple of times now: Virtual assistants (Siri), smart speakers (Alexa, Google Home), chatbots (<a href="https://medium.com/chris-messina/conversational-commerce-92e0bccfc3ff">“conversational commerce”</a>), <a href="http://julian.digital/2020/04/19/airpods-as-a-platform/">AirPods-as-a-platform</a>, and, most recently, large language models.</p>



<p>I’m not entirely sure where this obsession with conversational interfaces comes from. Perhaps it’s a type of anemoia, a nostalgia for a future we saw in StarTrek that never became reality. Or maybe it’s simply that people look at the term <em>“<strong>natural</strong> language”</em> and think <em>“well, if it’s <strong>natural</strong> then it must be the logical end state”</em>.</p>



<p>I’m here to tell you that it’s not.</p>



<p><span>02</span> Data transfer mechanisms</p>



<p>When people say <em>“natural language”</em> what they mean is written or verbal communication. Natural language is a way to exchange ideas and knowledge between humans. In other words, it’s a data transfer mechanism.</p>



<p>Data transfer mechanisms have two critical factors: speed and lossiness.</p>



<p>Speed determines how quickly data is transferred from the sender to the receiver, while lossiness refers to how accurately the data is transferred. In an ideal state, you want data transfer to happen at maximum speed (instant) and with perfect fidelity (lossless), but these two attributes are often a bit of a trade-off. </p>



<p>Let’s look at how well natural language does on the speed dimension:</p>



<p><img decoding="async" src="https://julian.digital/wp-content/uploads/2025/02/datatransfer.png"></p><p>The first thing I should note is that these data points are <a href="https://www.researchgate.net/publication/332380784_How_many_words_do_we_read_per_minute_A_review_and_meta-analysis_of_reading_rate">very</a>, <a href="https://irisreading.com/what-is-the-average-reading-speed/">very</a> <a href="https://virtualspeech.com/blog/average-speaking-rate-words-per-minute">simplified</a> <a href="https://en.wikipedia.org/wiki/Words_per_minute">averages</a>. The important part to take away from this table is not the accuracy of individual numbers, but the overall pattern: We are significantly faster at receiving data (reading, listening) than sending it (writing, speaking). This is why we can listen to podcasts at 2x speed, but not record them at 2x speed.</p>



<p>To put the writing and speaking speeds into perspective, <strong>we form thoughts at 1,000-3,000 words per minute</strong>. Natural language might be natural, but it’s a bottleneck.</p>



<p>And yet, if you think about your day-to-day interactions with other humans, most communication feels really fast and efficient. That’s because natural language is only one of many data transfer mechanisms available to us.</p>



<p>For example, instead of saying <em>“I think what you just said is a great idea”</em>, I can just give you a thumbs up. Or nod my head. Or simply smile. </p>



<p>Gestures and facial expressions are effectively data compression techniques. They encode information in a more compact, but lossier, form to make it faster and more convenient to transmit.</p>



<p><img decoding="async" src="https://julian.digital/wp-content/uploads/2025/01/thumbsup.png"></p><p>Natural language is great for data transfer that requires high fidelity (or as a data storage mechanism for async communication), but whenever possible we switch to other modes of communication that are faster and more effortless. Speed and convenience always wins.</p>



<p>My favorite example of truly effortless communication is a memory I have of my grandparents. At the breakfast table, my grandmother never had to ask for the butter – my grandfather always seemed to pass it to her automatically, because after 50+ years of marriage he just sensed that she was about to ask for it. It was like they were communicating telepathically.  </p>



<p>*That* is the type of relationship I want to have with my computer!</p>



<p><span>03</span> Human Computer Interaction</p>



<p>Similar to human-to-human communication, there are different data transfer mechanisms to exchange information between humans and computers. In the early days of computing, users interacted with computers through a command line. These text-based commands were effectively a natural language interface, but required precise syntax and a deep understanding of the system.</p>



<p>The introduction of the GUI primarily solved a discovery problem: Instead of having to memorize exact text commands, you could now navigate and perform tasks through visual elements like menus and buttons. This didn’t just make things easier to discover, but also more convenient: It’s faster to click a button than to type a long text command.</p>



<p>Today, we live in a productivity equilibrium that combines graphical interfaces with keyboard-based commands.</p>



<p>We still use our mouse to navigate and tell our computers what to do next, but routine actions are typically communicated in form of quick-fire keyboard presses: <span>⌘</span><span>b</span> to format text as bold, <span>⌘</span><span>t</span> to open a new tab, <span>⌘</span><span>c</span>/<span>v</span> to quickly copy things from one place to another, etc.</p>



<p>These shortcuts are not natural language though. They are another form of data compression. Like a thumbs up or a nod, they help us to communicate faster.</p>



<p>Modern productivity tools take these data compression shortcuts to the next level. In tools like Linear, Raycast or Superhuman every single command is just a keystroke away. Once you’ve built the muscle memory, the data input feels completely effortless. It’s almost like being handed the butter at the breakfast table without having to ask for it.</p>



<p>Touch-based interfaces are considered the third pivotal milestone in the evolution of human computer interaction, but they have always been more of an augmentation of desktop computing rather than a replacement for it. Smartphones are great for “away from keyboard” workflows, but important productivity work still happens on desktop.</p>



<p><a href="https://x.com/blakeir/status/1838365114312872320">
  <img decoding="async" src="https://julian.digital/wp-content/uploads/2025/03/blake-1.png">
</a></p><p>That’s because text is not a mobile-native input mechanism. A physical keyboard can feel like a natural extension of your mind and body, but typing on a phone is always a little awkward – and it shows in data transfer speeds: <a href="https://userinterfaces.aalto.fi/typing37k/">Average typing speeds on mobile are just 36 words-per-minute</a>, notably slower than the ~60 words-per-minute on desktop.</p>



<p>We’ve been able to replace natural language with mobile-specific data compression algorithms like emojis or Snapchat selfies, but we’ve never found a mobile equivalent for keyboard shortcuts. Guess why we still don’t have a truly mobile-first productivity app after almost 20 years since the introduction of the iPhone?</p>



<p><img decoding="async" src="https://julian.digital/wp-content/uploads/2025/02/emojis.png"></p><p><em>“But what about speech-to-text,”</em> you might say, pointing to <a href="https://www.npr.org/2023/04/16/1170232936/voice-notes-messages-trend">reports</a> about increasing usage of voice messaging. It’s true that speaking (150wpm) is indeed a faster data transfer mechanism than typing (60wpm), but that doesn’t automatically make it a better method to interact with computers.</p>



<p>We keep telling ourselves that previous voice interfaces like Alexa or Siri didn’t succeed because the underlying AI wasn’t smart enough, but that’s only half of the story. The core problem was never the quality of the output function, but the inconvenience of the input function: A natural language prompt like <em>“Hey Google, what’s the weather in San Francisco today?”</em> just takes 10x longer than simply tapping the weather app on your homescreen.</p>



<p>LLMs don’t solve this problem. The quality of their output is improving at an astonishing rate, but the input modality is a step backwards from what we already have. Why should I have to describe my desired action using natural language, when I could simply press a button or keyboard shortcut? Just pass me the goddamn butter.</p>



<p><span>04</span> Conversational UI as Augmentation</p>



<p>None of this is to say that LLMs aren’t great. I love LLMs. I use them all the time. In fact, I wrote this very essay with the help of an LLM. </p>



<p>Instead of drafting a first version with pen and paper (my preferred writing tools), I spent an entire hour walking outside, talking to ChatGPT in Advanced Voice Mode. We went through all the fuzzy ideas in my head, clarified and organized them, explored some additional talking points, and eventually pulled everything together into a first outline.</p>



<p>This wasn’t just a one-sided “<em>Hey, can you write a few paragraphs about x</em>” <a href="https://x.com/julianlehr/status/1855858599156932773">prompt</a>. It felt like a genuine, in-depth conversation and exchange of ideas with a true thought partner. Even weeks later, I’m still amazed at how well it worked. It was one of those rare, magical moments where software makes you feel like you’re living in the future.</p>



<p>In contrast to typical human-to-computer commands, however, this workflow is not defined by speed. Like writing, my ChatGPT conversation is a thinking process – not an interaction that happens post-thought.</p>



<p>It should also be noted that ChatGPT does not substitute any existing software workflows in this example. It’s a completely new use case.</p>



<p>This brings me to my core thesis: The inconvenience and inferior data transfer speeds of conversational interfaces make them an unlikely replacement for existing computing paradigms – but what if they complement them?</p>



<p>The most convincing conversational UI I have seen to date was at a hackathon where a team turned <a href="https://upsidelab.io/blog/design-voice-user-interface-starcraft">Amazon Alexa into an in-game voice assistant for StarCraft II</a>. Rather than replacing mouse and keyboard, voice acted as an <em>additional</em> input mechanism. It increased the bandwidth of the data transfer.</p>



<p><img decoding="async" src="https://julian.digital/wp-content/uploads/2025/03/starcraft-2.png"></p><p>You could see the same pattern work for any type of knowledge work, where voice commands are available <em>while</em> you are busy doing other things. We will not replace Figma, Notion, or Excel with a chat interface. It’s not going to happen. Neither will we forever continue the status quo, where we constantly have to switch back and forth between these tools and an LLM.</p>



<p>Instead, AI should function as an always-on command meta-layer that spans across all tools. Users should be able to trigger actions from anywhere with simple voice prompts without having to interrupt whatever they are currently doing with mouse and keyboard.</p>



<p>For this future to become an actual reality, AI needs to work at the OS level. It’s not meant to be an interface for a single tool, but an interface across tools. <a href="https://kwokchain.com/2019/08/16/the-arc-of-collaboration/">Kevin Kwok famously wrote</a> that <em>“productivity and collaboration shouldn’t be two separate workflows”</em>. And while he was referring to human-to-human collaboration, the statement is even more true in a world of human-to-AI collaboration, where the lines between productivity and coordination are becoming increasingly more blurry. </p>



<p><img decoding="async" src="https://julian.digital/wp-content/uploads/2025/03/metalayer-1.png"></p><p>The second thing we need to figure out is how we can compress voice input to make it faster to transmit. What’s the voice equivalent of a thumbs-up or a keyboard shortcut? Can I prompt Claude faster with simple sounds and whistles? Should ChatGPT have access to my camera so it can change its answers in realtime based on my facial expressions?</p>



<p>Even as a secondary interface, speed and convenience is all that matters.</p>



<p><span>05</span> Closing thoughts</p>



<p>I admit that the title of this essay is a bit misleading (made you click though, didn’t it?). This isn’t really a case against conversational interfaces, it’s a case against zero-sum thinking.</p>



<p>We spend too much time thinking about AI as a substitute (for interfaces, workflows, and jobs) and too little time about AI as a complement. Progress rarely follows a simple path of replacement. It unlocks new, previously unimaginable things rather than merely displacing what came before.</p>



<p>The same is true here. The future isn’t about replacing existing computing paradigms with chat interfaces, but about enhancing them to make human-computer interaction feel effortless – like the silent exchange of butter at a well-worn breakfast table.</p>







<p><em>Thanks to Blake Robbins, Chris Paik, Jackson Dahl, Johannes Schickling, Jordan Singer, and signüll for reading drafts of this post.</em></p>




 	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Netflix's Media Production Suite (230 pts)]]></title>
            <link>https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22</link>
            <guid>43541759</guid>
            <pubDate>Tue, 01 Apr 2025 01:02:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22">https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22</a>, See on <a href="https://news.ycombinator.com/item?id=43541759">Hacker News</a></p>
Couldn't get https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Everything is Ghibli (162 pts)]]></title>
            <link>https://carly.substack.com/p/everything-is-ghibli</link>
            <guid>43540326</guid>
            <pubDate>Mon, 31 Mar 2025 21:44:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://carly.substack.com/p/everything-is-ghibli">https://carly.substack.com/p/everything-is-ghibli</a>, See on <a href="https://news.ycombinator.com/item?id=43540326">Hacker News</a></p>
Couldn't get https://carly.substack.com/p/everything-is-ghibli: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Go Optimization Guide (418 pts)]]></title>
            <link>https://goperf.dev/</link>
            <guid>43539585</guid>
            <pubDate>Mon, 31 Mar 2025 20:29:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://goperf.dev/">https://goperf.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=43539585">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  


  
  


<h2 id="patterns-and-techniques-for-writing-high-performance-applications-with-go">Patterns and Techniques for Writing High-Performance Applications with Go<a href="#patterns-and-techniques-for-writing-high-performance-applications-with-go" title="Permanent link">¶</a></h2>
<p>The <strong>Go App Optimization Series</strong> is a collection of technical articles aimed at helping developers write faster, more efficient Go applications. Whether you're building high-throughput APIs, microservices, or distributed systems, this series offers practical patterns, real-world use cases, and low-level performance insights to guide your optimization efforts.</p>
<p>While Go doesn’t expose as many knobs for performance tuning as languages like C++ or Rust, it still provides <strong>plenty of opportunities</strong> to make your applications significantly faster. From memory reuse and allocation control to efficient networking and concurrency patterns, Go offers a pragmatic set of tools for writing high-performance code.</p>
<p>We focus on <strong>concrete techniques</strong> with <strong>mesurable impact</strong> you can apply immediately—covering everything from core language features to advanced networking strategies.</p>
<h2 id="whats-covered-so-far"><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 13c.7 0 1.37.13 2 .35V9l-6-6H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h8.35c-.22-.63-.35-1.3-.35-2 0-3.31 2.69-6 6-6m-5-8.5 5.5 5.5H14zm8.5 12.75L17.75 22 15 19l1.16-1.16 1.59 1.59 3.59-3.59z"></path></svg></span> What’s Covered So Far<a href="#whats-covered-so-far" title="Permanent link">¶</a></h2>
<h3 id="common-go-patterns-for-performance"><a href="https://goperf.dev/01-common-patterns/">Common Go Patterns for Performance</a><a href="#common-go-patterns-for-performance" title="Permanent link">¶</a></h3>
<p>In this first article, we explore a curated set of high-impact performance patterns every Go developer should know:</p>
<ul>
<li>Using <code>sync.Pool</code> effectively</li>
<li>Avoiding unnecessary allocations</li>
<li>Struct layout and memory alignment</li>
<li>Efficient error handling</li>
<li>Zero-cost abstractions with interfaces</li>
<li>In-place sorting and slices reuse</li>
</ul>
<p>Each pattern is grounded in practical use cases, with benchmarks and examples you can copy into your own codebase.</p>
<hr>
<h2 id="whats-coming-next"><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 2.03v2.02c4.39.54 7.5 4.53 6.96 8.92-.46 3.64-3.32 6.53-6.96 6.96v2c5.5-.55 9.5-5.43 8.95-10.93-.45-4.75-4.22-8.5-8.95-8.97m-2 .03c-1.95.19-3.81.94-5.33 2.2L7.1 5.74c1.12-.9 2.47-1.48 3.9-1.68zM4.26 5.67A9.9 9.9 0 0 0 2.05 11h2c.19-1.42.75-2.77 1.64-3.9zM15.5 8.5l-4.88 4.88-2.12-2.12-1.06 1.06 3.18 3.18 5.94-5.94zM2.06 13c.2 1.96.97 3.81 2.21 5.33l1.42-1.43A8 8 0 0 1 4.06 13zm5.04 5.37-1.43 1.37A10 10 0 0 0 11 22v-2a8 8 0 0 1-3.9-1.63"></path></svg></span> What’s Coming Next<a href="#whats-coming-next" title="Permanent link">¶</a></h2>
<h3 id="high-performance-networking-in-go">High-Performance Networking in Go<a href="#high-performance-networking-in-go" title="Permanent link">¶</a></h3>
<p>In our upcoming deep dive into networking, we'll focus on building high-throughput network services with Go’s standard library and beyond. This includes:</p>
<ul>
<li>Efficient use of <code>net/http</code> and <code>net.Conn</code></li>
<li>Managing large volumes of concurrent connections</li>
<li>Performance tuning with epoll/kqueue and <code>GOMAXPROCS</code></li>
<li>Load testing techniques and bottleneck diagnostics</li>
<li>TBD...</li>
</ul>
<p>We'll also explore when to drop down to lower-level libraries like <code>fasthttp</code>, and how to balance performance with maintainability.</p>
<hr>
<h2 id="who-this-is-for"><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.03 6.03 20 7l2-5-5 2 .97.97-1.82 1.82C10.87 2.16 3.3 3.94 2.97 4L2 4.26l.5 1.94.79-.2 6.83 6.82L6.94 16H5l-3 3 2 1 1 2 3-3v-1.94l3.18-3.18L18 20.71l-.19.79 1.93.5.26-.97c.06-.33 1.84-7.9-2.79-13.18zM4.5 5.78c2.05-.28 6.78-.5 10.23 2.43l-3.91 3.91zM18.22 19.5l-6.34-6.32 3.91-3.91c2.93 3.45 2.71 8.18 2.43 10.23"></path></svg></span> Who This Is For<a href="#who-this-is-for" title="Permanent link">¶</a></h2>
<p>This series is ideal for:</p>
<ul>
<li>Backend engineers optimizing Go services in production</li>
<li>Developers working on latency-sensitive systems</li>
<li>Teams migrating to Go and building performance-critical paths</li>
<li>Anyone curious about Go’s performance model and trade-offs</li>
</ul>
<hr>
<p>Stay tuned—more articles, code samples, and tools are on the way. You can bookmark this page to follow the series as it evolves.</p>







  
    
  
  
    
  


  





                
              </article>
            </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[KOReader: Open-Source eBook Reader (361 pts)]]></title>
            <link>https://github.com/koreader/koreader</link>
            <guid>43539103</guid>
            <pubDate>Mon, 31 Mar 2025 19:52:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/koreader/koreader">https://github.com/koreader/koreader</a>, See on <a href="https://news.ycombinator.com/item?id=43539103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://koreader.rocks/" rel="nofollow"><img src="https://raw.githubusercontent.com/koreader/koreader.github.io/master/koreader-logo.png" alt="KOReader"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">KOReader is a document viewer primarily aimed at e-ink readers.</h4><a id="user-content-koreader-is-a-document-viewer-primarily-aimed-at-e-ink-readers" aria-label="Permalink: KOReader is a document viewer primarily aimed at e-ink readers." href="#koreader-is-a-document-viewer-primarily-aimed-at-e-ink-readers"></a></p>
<p dir="auto"><a href="https://github.com/koreader/koreader/blob/master/COPYING"><img src="https://camo.githubusercontent.com/5bb38e76b63285b53eefb8b5ec6047b0ea2e12c3e4aeb04358366b5c8a22266b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6b6f7265616465722f6b6f726561646572" alt="AGPL Licence" data-canonical-src="https://img.shields.io/github/license/koreader/koreader"></a>
<a href="https://github.com/koreader/koreader/releases"><img src="https://camo.githubusercontent.com/f0e1aa8bfa9e1a000beef74d8a3c6c82e5730eb272790a5640b442ef481b19d2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6b6f7265616465722f6b6f7265616465722e737667" alt="Latest release" data-canonical-src="https://img.shields.io/github/release/koreader/koreader.svg"></a>
<a href="https://gitter.im/koreader/koreader" rel="nofollow"><img src="https://camo.githubusercontent.com/7513ddfad8bc40022e947a027b821ea79e33194c3c01f540c0d541c385cacca0/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6b6f7265616465722f6b6f7265616465723f636f6c6f723d726564" alt="Gitter" data-canonical-src="https://img.shields.io/gitter/room/koreader/koreader?color=red"></a>
<a href="http://www.mobileread.com/forums/forumdisplay.php?f=276" rel="nofollow"><img src="https://camo.githubusercontent.com/93aec7413392831b748bcae5e985b60b91edf256d39015cee75ba483038396e7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f666f72756d2d6f6e5f6d6f62696c65726561642d6c6967687467726579" alt="Mobileread" data-canonical-src="https://img.shields.io/badge/forum-on_mobileread-lightgrey"></a>
<a href="https://circleci.com/gh/koreader/koreader" rel="nofollow"><img src="https://camo.githubusercontent.com/c1ab248628b1eba39c7bd6231da7d4c4d2f09df39cce5da94e234331e1eea826/68747470733a2f2f636972636c6563692e636f6d2f67682f6b6f7265616465722f6b6f7265616465722e7376673f7374796c653d736869656c64" alt="Build Status" data-canonical-src="https://circleci.com/gh/koreader/koreader.svg?style=shield"></a>
<a href="https://codecov.io/gh/koreader/koreader" rel="nofollow"><img src="https://camo.githubusercontent.com/ba8273e053b14d0b09437a11b82e8c0025ed5b42488798e7737e70e3049fa431/68747470733a2f2f636f6465636f762e696f2f67682f6b6f7265616465722f6b6f7265616465722f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage Status" data-canonical-src="https://codecov.io/gh/koreader/koreader/branch/master/graph/badge.svg"></a>
<a href="https://hosted.weblate.org/engage/koreader/?utm_source=widget" rel="nofollow"><img src="https://camo.githubusercontent.com/daac1cb6aa37b411df148843881e1144e1a4f7c5387114db23e928a911aed718/68747470733a2f2f686f737465642e7765626c6174652e6f72672f776964676574732f6b6f7265616465722f2d2f6b6f7265616465722f7376672d62616467652e737667" alt="Weblate Status" data-canonical-src="https://hosted.weblate.org/widgets/koreader/-/koreader/svg-badge.svg"></a></p>
<p dir="auto"><a href="https://github.com/koreader/koreader/releases">Download</a> •
<a href="http://koreader.rocks/user_guide/" rel="nofollow">User guide</a> •
<a href="https://github.com/koreader/koreader/wiki">Wiki</a> •
<a href="http://koreader.rocks/doc/" rel="nofollow">Developer docs</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Main features</h2><a id="user-content-main-features" aria-label="Permalink: Main features" href="#main-features"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>portable</strong>: runs on embedded devices (Cervantes, Kindle, Kobo, PocketBook, reMarkable), Android and Linux computers. Developers can run a KOReader emulator in Linux and MacOS.</p>
</li>
<li>
<p dir="auto"><strong>multi-format documents</strong>: supports fixed page formats (PDF, DjVu, CBT, CBZ) and reflowable e-book formats (EPUB, FB2, Mobi, DOC, RTF, HTML, CHM, TXT). Scanned PDF/DjVu documents can also be reflowed with the built-in K2pdfopt library. <a href="https://github.com/koreader/koreader/wiki/ZIP">ZIP files</a> are also supported for some formats.</p>
</li>
<li>
<p dir="auto"><strong>full-featured reading</strong>: multi-lingual user interface with a highly customizable reader view and many typesetting options. You can set arbitrary page margins, override line spacing and choose external fonts and styles. It has multi-lingual hyphenation dictionaries bundled into the application.</p>
</li>
<li>
<p dir="auto"><strong>integrated</strong> with <em>calibre</em> (search metadata, receive ebooks wirelessly, browse library via OPDS), <em>Wallabag</em>, <em>Wikipedia</em>, <em>Google Translate</em> and other content providers.</p>
</li>
<li>
<p dir="auto"><strong>optimized for e-ink devices</strong>: custom UI without animation, with paginated menus, adjustable text contrast, and easy zoom to fit content or page in paged media.</p>
</li>
<li>
<p dir="auto"><strong>extensible</strong>: via plugins</p>
</li>
<li>
<p dir="auto"><strong>fast</strong>: on some older devices, it has been measured to have less than half the page-turn delay as the built in reading software.</p>
</li>
<li>
<p dir="auto"><strong>and much more</strong>: look up words with StarDict dictionaries / Wikipedia, add your own online OPDS catalogs and RSS feeds, over-the-air software updates, an FTP client, an SSH server, …</p>
</li>
</ul>
<p dir="auto">Please check the <a href="http://koreader.rocks/user_guide/" rel="nofollow">user guide</a> and the <a href="https://github.com/koreader/koreader/wiki">wiki</a> to discover more features and to help us document them.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><a href="https://github.com/koreader/koreader-artwork/raw/master/koreader-menu.png"><img src="https://github.com/koreader/koreader-artwork/raw/master/koreader-menu-thumbnail.png" alt="" width="200px"></a>
<a href="https://github.com/koreader/koreader-artwork/raw/master/koreader-footnotes.png"><img src="https://github.com/koreader/koreader-artwork/raw/master/koreader-footnotes-thumbnail.png" alt="" width="200px"></a>
<a href="https://github.com/koreader/koreader-artwork/raw/master/koreader-dictionary.png"><img src="https://github.com/koreader/koreader-artwork/raw/master/koreader-dictionary-thumbnail.png" alt="" width="200px"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Please follow the model specific steps for your device:</p>
<p dir="auto"><a href="https://github.com/koreader/koreader/wiki/Installation-on-Android-devices">Android</a> •
<a href="https://github.com/koreader/koreader/wiki/Installation-on-BQ-devices">Cervantes</a> •
<a href="https://github.com/koreader/koreader/wiki/Installation-on-Kindle-devices">Kindle</a> •
<a href="https://github.com/koreader/koreader/wiki/Installation-on-Kobo-devices">Kobo</a> •
<a href="https://github.com/koreader/koreader/wiki/Installation-on-desktop-linux">Linux</a> •
<a href="https://github.com/koreader/koreader/wiki/Installation-on-PocketBook-devices">Pocketbook</a> •
<a href="https://github.com/koreader/koreader/wiki/Installation-on-Remarkable">reMarkable</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><a href="https://github.com/koreader/koreader/blob/master/doc/Building.md">Setting up a build environment</a> •
<a href="https://github.com/koreader/koreader/blob/master/doc/Collaborating_with_Git.md">Collaborating with Git</a> •
<a href="https://github.com/koreader/koreader/blob/master/doc/Building_targets.md">Building targets</a> •
<a href="https://github.com/koreader/koreader/blob/master/doc/Porting.md">Porting</a> •
<a href="http://koreader.rocks/doc/" rel="nofollow">Developer docs</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">KOReader is developed and supported by volunteers all around the world. There are many ways you can help:</p>
<ul dir="auto">
<li><a href="https://github.com/koreader/koreader/issues?q=is%3Aopen+is%3Aissue+label%3Abug">fix bugs</a> and <a href="https://github.com/koreader/koreader/issues?q=is%3Aopen+is%3Aissue+label%3Aenhancement">implement new features</a></li>
<li><a href="https://hosted.weblate.org/engage/koreader/?utm_source=widget" rel="nofollow">translate the program into your language</a> or improve an existing translation</li>
<li>document lesser-known features on the <a href="https://github.com/koreader/koreader/wiki">wiki</a></li>
<li>help others with your knowledge on the <a href="http://www.mobileread.com/forums/forumdisplay.php?f=276" rel="nofollow">forum</a></li>
</ul>
<p dir="auto">Right now we only support <a href="https://liberapay.com/KOReader" rel="nofollow">liberapay</a> donations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<p dir="auto"><a href="https://github.com/koreader/koreader/commits/master"><img src="https://camo.githubusercontent.com/67345c78507e396cc6c9462876617c5de9d889023620822527a68bea7eca06dd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f6b6f7265616465722f6b6f7265616465723f636f6c6f723d6f72616e6765" alt="Last commit" data-canonical-src="https://img.shields.io/github/last-commit/koreader/koreader?color=orange"></a>
<a href="https://github.com/koreader/koreader/pulse"><img src="https://camo.githubusercontent.com/1f46f03040612f161709c63f954f32574d180ae5c7c499457ebe654374d91977/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f6b6f7265616465722f6b6f726561646572" alt="Commit activity" data-canonical-src="https://img.shields.io/github/commit-activity/m/koreader/koreader"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JEP Draft: Prepare to Make Final Mean Final (198 pts)]]></title>
            <link>https://openjdk.org/jeps/8349536</link>
            <guid>43538919</guid>
            <pubDate>Mon, 31 Mar 2025 19:35:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openjdk.org/jeps/8349536">https://openjdk.org/jeps/8349536</a>, See on <a href="https://news.ycombinator.com/item?id=43538919">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="Summary">Summary</h2>
<p>Issue warnings about uses of <em>deep reflection</em> to mutate <code>final</code> fields. The warnings aim to prepare developers for a future release that ensures <a href="https://openjdk.org/jeps/8305968">integrity by default</a> by restricting <code>final</code> field mutation; this makes Java programs safer and potentially faster. Application developers can avoid both current warnings and future restrictions by selectively enabling the ability to mutate <code>final</code> fields where essential.</p>
<h2 id="Goals">Goals</h2>
<ul>
<li>Prepare the Java ecosystem for a future release that, by default, disallows the mutation of <code>final</code> fields by deep reflection. As of that release, application developers will have to explicitly enable the capability to do so at startup.</li>
<li>Align <code>final</code> fields in normal classes with the components of record classes, which cannot be mutated by deep reflection.</li>
<li>Allow serialization libraries to continue working with <code>Serializable</code> classes, even those with <code>final</code> fields.</li>
</ul>
<h2 id="Non-Goals">Non-Goals</h2>
<ul>
<li>It is not a goal to deprecate or remove any part of the Java Platform API.</li>
<li>It is not a goal to prevent the mutation of <code>final</code> fields by serialization libraries during deserialization.</li>
</ul>
<h2 id="Motivation">Motivation</h2>
<p>Java developers rely on <code>final</code> fields to represent immutable state. Once assigned in a constructor (for <code>final</code> instance fields) or in a class initializer (for <code>static final</code> fields), a <code>final</code> field cannot be reassigned; its value, whether a primitive value or a reference to an object, is immutable. The expectation that a <code>final</code> field cannot be reassigned in far-flung parts of the program, whether deliberately or accidentally, is often crucial when developers reason about correctness. Furthermore, many classes exist <em>only</em> to represent immutable state, so <a href="https://openjdk.org/jeps/395">records</a> were introduced in JDK 16 to provide a concise way to declare a class where all fields are <code>final</code>, making it easy to reason about correctness.</p>
<p>The expectation that a <code>final</code> field cannot be reassigned is also important for performance. The more the JVM knows about the behavior of a class, the more optimizations it can apply. For example, being able to trust that <code>final</code> fields are never reassigned makes it possible for the JVM to perform <em>constant folding</em>, an optimization that elides the need to load a value from memory since the value can instead be embedded in the machine code emitted by the JIT compiler. Constant folding is often the first step in a chain of optimizations that together provide a massive speed-up.</p>
<p>Unfortunately, the expectation that a <code>final</code> field cannot be reassigned is <strong>false</strong>. The Java Platform provides <a href="https://openjdk.org/jeps/8305968#Undermining-integrity">a number of APIs</a> that allow <code>final</code> fields to be reassigned at any time by any code in the program, undermining all reasoning about correctness and invalidating important optimizations. The most prevalent of these APIs is <a href="https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/lang/reflect/AccessibleObject.html#setAccessible(boolean)"><em>deep reflection</em></a>. Here is an example that uses deep reflection to mutate a <code>final</code> field at will:</p>
<pre><code>class C {
    final int x;
    C() { x = 100; }
}

// Perform deep reflection over C's final field
java.lang.reflect.Field f = C.class.getDeclaredField("x");
f.setAccessible(true);

// Create an object of class C
C obj = new C();
System.out.println(obj.x);  // Prints 100

// Mutate the final field in the object
f.set(obj, 200);
System.out.println(obj.x);  // Prints 200
f.set(obj, 300);
System.out.println(obj.x);  // Prints 300</code></pre>
<p>Accordingly, a <code>final</code> field is as mutable as a non-<code>final</code> field. Developers are unable to use <code>final</code> fields to construct the deeply immutable graphs of objects that would enable the JVM to deliver the best performance optimizations.</p>
<p>It might seem absurd for the Java Platform to provide an API that undermines the meaning of <code>final</code>. However, after JDK 5 introduced the <a href="https://docs.oracle.com/javase/specs/jls/se23/html/jls-17.html#jls-17.4">Java Memory Model</a> that led to widespread use of <a href="https://docs.oracle.com/javase/specs/jls/se23/html/jls-17.html#jls-17.5"><code>final</code> fields</a>, such an API was deemed necessary to support serialization libraries. In retrospect, offering such unconstrained functionality was a poor choice because it sacrificed integrity. When we introduced <a href="https://openjdk.org/jeps/371">hidden classes</a> in JDK 15 and <a href="https://openjdk.org/jeps/395">record classes</a> in JDK 16, we constrained deep reflection to <a href="https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/lang/reflect/Field.html#set(java.lang.Object,java.lang.Object)">disallow mutation of <code>final</code> fields in hidden and record classes</a>.
We constrained deep reflection further when we <a href="https://openjdk.org/jeps/403">strongly encapsulated JDK internals</a> in JDK 17. In JDK 24, <a href="https://openjdk.org/jeps/498">we started a process</a> to remove methods in <code>sun.misc.Unsafe</code> that, like deep reflection, allow mutation of <code>final</code> fields.</p>
<p>Relatively little code mutates <code>final</code> fields, but the mere existence of APIs for doing so makes it impossible for developers or the JVM to trust the value of <em>any</em> <code>final</code> field. This compromises safety and performance in <em>all</em> programs. In line with the policy of <a href="https://openjdk.org/jeps/8305968"><em>integrity by default</em></a>, we plan to enforce the immutability of <code>final</code> fields so that code cannot use deep reflection to reassign them at will. We will support one special case -- serialization libraries that need to mutate <code>final</code> fields during deserialization -- via a limited-purpose API.</p>
<h2 id="Description">Description</h2>
<p>In JDK 5 and later releases, you can mutate <code>final</code> fields via deep reflection (the <a href="https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/lang/reflect/Field.html#setAccessible(boolean)"><code>setAccessible</code></a> and <a href="https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/lang/reflect/Field.html#set(java.lang.Object,java.lang.Object)"><code>set</code></a> methods in <code>java.lang.reflect.Field</code>). In JDK XX, we will restrict deep reflection so that mutating a <code>final</code> field also causes a warning to be issued at run time by default. It will not be possible to avoid the warning simply by using <code>--add-opens</code> to enable deep reflection of classes with <code>final</code> fields.</p>
<p>We refer to restrictions on mutating <code>final</code> fields as <em><code>final</code> field restrictions</em>. We will strengthen the effect of <code>final</code> field restrictions over time. Rather than issue warnings, a future JDK release will throw exceptions by default when Java code uses deep reflection to mutate <code>final</code> fields. The intent is to ensure that applications and the Java Platform have <a href="https://openjdk.org/jeps/8305968">integrity by default</a>.</p>
<h3 id="Enabling-final-field-mutation">Enabling <code>final</code> field mutation</h3>
<p>Application developers can avoid warnings (and in the future, exceptions) by enabling <code>final</code> field mutation for selected Java code at startup. Enabling <code>final</code> field mutation acknowledges the application's need to mutate <code>final</code> fields and lifts the <code>final</code> field restrictions.</p>
<p>Under the policy of integrity by default, it is the application developer (or perhaps deployer, on the advice of the application developer) who enables <code>final</code> field mutation, not library developers. Library developers who rely on reflection to mutate <code>final</code> fields should inform their users that they will need to enable <code>final</code> field mutation using one of the methods below.</p>
<p>To enable <code>final</code> field mutation by any code on the class path, regardless of where the <code>final</code> fields are declared, use the following command-line option:</p>
<pre><code>java --enable-final-field-mutation=ALL-UNNAMED ...</code></pre>
<p>To enable <code>final</code> field mutation by specific modules on the module path, again regardless of where the <code>final</code> fields are declared, pass a comma-separated list of module names:</p>
<pre><code>java --enable-final-field-mutation=M1,M2 ...</code></pre>
<p>Most application developers who wish to allow <code>final</code> field mutation will pass <code>--enable-final-field-mutation</code> directly to the <code>java</code> launcher in a startup script, but other techniques are available:</p>
<ul>
<li>You can pass <code>--enable-final-field-mutation</code> to the launcher indirectly, by setting the environment variable <code>JDK_JAVA_OPTIONS</code>.</li>
<li>You can put <code>--enable-final-field-mutation</code> in an argument file that is passed to the launcher by a script or an end user, e.g., <code>java @config</code></li>
<li>You can add <code>Enable-Reflective-Final-Mutation</code> to the manifest of an executable JAR file, i.e., a JAR file that is launched via <a href="https://docs.oracle.com/en/java/javase/22/docs/specs/man/java.html#synopsis"><code>java -jar</code></a>. (The only supported value for the <code>Enable-Reflective-Final-Mutation</code> manifest entry is <code>ALL-UNNAMED</code>; other values cause an exception to be thrown.)</li>
<li>If you create a custom Java runtime for your application, you can pass the <code>--enable-final-field-mutation</code> option to <code>jlink</code> via the <code>--add-options</code> option, so that reflective final field mutation is enabled in the resulting runtime image.</li>
<li>The <a href="https://docs.oracle.com/en/java/javase/22/docs/specs/jni/invocation.html">JNI Invocation API</a> allows a native application to embed a JVM in its own process. A native application which uses the JNI Invocation API can enable <code>final</code> field mutation for modules in the embedded JVM by passing the <code>--enable-final-field-mutation</code> option when <a href="https://docs.oracle.com/en/java/javase/22/docs/specs/jni/invocation.html#jni_createjavavm">creating the JVM</a>.</li>
</ul>
<h3 id="API-changes-in-JDK-XX">API changes in JDK XX</h3>
<p>The behavior of <code>Field::setAccessible</code> is unchanged. This means that when code calls <code>f.setAccessible(true)</code> on a <code>Field</code> object <code>f</code>, the code must either be in the same module as the field reflected by <code>f</code>, or, if the code is in a different module, the field reflected by <code>f</code> must be accessible to the caller via <code>exports</code> or <code>opens</code>. The call throws <code>InaccessibleObjectException</code> if these conditions are not met.</p>
<p>The behavior of <code>Field::set</code> is changed to have an additional condition:</p>
<pre>If the underlying field is final, this Field object has write access
if and only if the following conditions are met:

    setAccessible(true) has succeeded for this Field object; and
    final field mutation is enabled for the caller's module
        and the package of the field reflected by this Field object is open to the caller's module; and
    the field is non-static; and
    the field's declaring class is not a hidden class; and
    the field's declaring class is not a record class.

If any of the above checks is not met, this method throws an IllegalAccessException. 

Note that if this Field object reflects a field which is declared in a module other than the caller's module, then it is not sufficient for the field to be declared as public in an exported package. The field must be declared in a package that is open.

Note that the caller might not be the same as the caller of setAccessible(true).
</pre>
<p>For reference, package <code>p</code> in module <code>M</code> is <em>open</em> to module <code>N</code> if:</p>
<ul>
<li><code>N</code> is <code>M</code> itself, or</li>
<li><code>M</code>'s <code>module-info</code> contains <code>opens p</code> or <code>opens p to N</code>, or</li>
<li><code>M</code> is an <em>automatic module</em>, i.e., its classes are placed on the module path but it has no <code>module-info</code>, or</li>
<li><code>M</code> is an <em>unnamed module</em> (all classes on the class path are in an unnamed module), or</li>
<li>The application was started with the command-line option <code>--add-opens M/p=N</code>, or the application was launched as an executable JAR file whose manifest contains an appropriate <code>Add-Opens</code> attribute.</li>
</ul>
<p>Because every module is open to itself, code in a module may use deep reflection to mutate <code>final</code> fields of any class in the same module, provided that <code>final</code> final mutation is enabled at startup for that module.</p>
<p>The behavior of other relevant methods is as follows:</p>
<ul>
<li>
<p>The behavior of <a href="https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/lang/invoke/MethodHandles.Lookup.html#unreflectSetter(java.lang.reflect.Field)"><code>MethodHandles.Lookup::unreflectSetter</code></a> is changed along similar lines as <code>Field::set</code>.</p>
</li>
<li>
<p>The <a href="https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/lang/Module.html#addOpens(java.lang.String,java.lang.Module)"><code>Module::addOpens</code></a> method allows a caller in module <code>M</code> to open a package in module <code>N</code> to another module <code>O</code> at run time, provided that the package is already open to <code>M</code> itself. Calling this method will <em>not</em> enable <code>O</code> to mutate <code>final</code> fields in the package, even if <code>final</code> field mutation was enabled for <code>O</code> at startup, because the JVM already decided to trust the <code>final</code> fields in the package based on it not being open to <code>O</code> at startup.</p>
<p>The same applies to <a href="https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/lang/ModuleLayer.Controller.html#addOpens(java.lang.Module,java.lang.String,java.lang.Module)"><code>ModuleLayer.Controller::addOpens</code></a> and <a href="https://docs.oracle.com/en/java/javase/23/docs/api/java.instrument/java/lang/instrument/Instrumentation.html#redefineModule(java.lang.Module,java.util.Set,java.util.Map,java.util.Map,java.util.Set,java.util.Map)"><code>Instrumentation.redefineModule</code></a>.</p>
</li>
<li>
<p>The <a href="https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/lang/System.html#method-detail"><code>System::setIn</code>, <code>System::setOut</code>, and <code>System::setErr</code> methods</a> exist to mutate, respectively, the <code>final</code> fields <a href="https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/lang/System.html#field-detail"><code>System.in</code>, <code>System.out</code>, and <code>System.err</code></a>. These fields have always been <a href="https://docs.oracle.com/javase/specs/jls/se23/html/jls-17.html#jls-17.5.4">write-protected</a>, which means they can be mutated <em>only</em> by calling the corresponding methods in <code>System</code>. It has never been possible to mutate these fields via deep reflection. In JDK XX, there is no change of any kind to these fields and their corresponding methods.</p>
</li>
</ul>
<h3 id="Controlling-the-effect-of-final-field-restrictions">Controlling the effect of <code>final</code> field restrictions</h3>
<p>If <code>final</code> field mutation <em>is not</em> enabled for a module then it is illegal for code in the module to mutate any <code>final</code> field via deep reflection. That is, given a <code>Field</code> object <code>f</code> that reflects a <code>final</code> field, it may be legal for code in the module to call <code>f.setAccessible(true)</code> (depending on whether the field's package is open to the caller), but it is illegal for code in the module to call <code>f.set(..., ...)</code>.</p>
<p>If <code>final</code> field mutation <em>is</em> enabled for a module, but some <code>final</code> field is in a package is not open to the module, then it is illegal for code in the module to mutate that <code>final</code> field via deep reflection. This scenario can occur when code in one module, to which the field's package is open, calls <code>f.setAccessible(true)</code> and then passes <code>f</code> to code in a different module, for which <code>final</code> field mutation is enabled but to which the field's package is not open. It is illegal for the code that receives <code>f</code> to call <code>f.set(..., ...)</code>.</p>
<p>What action the Java runtime takes when an illegal <code>final</code> field mutation is attempted is controlled by a new command-line option, <code>--illegal-reflective-final-mutation</code>. This is similar in spirit and form to the <code>--illegal-access</code> option introduced by <a href="https://openjdk.org/jeps/261#Relaxed-strong-encapsulation">JEP 261</a> in JDK 9 and to <code>--illegal-native-access</code> introduced by <a href="https://openjdk.org/jeps/472">JEP 472</a> in JDK 24. It works as follows:</p>
<ul>
<li>
<p><code>--illegal-final-final-mutation=allow</code> allows the mutation to proceed without warning.</p>
</li>
<li>
<p><code>--illegal-final-final-mutation=warn</code> allows the mutation but issues a warning the first time that illegal <code>final</code> field mutation occurs in a particular module. At most one warning per module is issued.</p>
<p>This mode is the default in JDK XX. It will be phased out in a future release and, eventually, removed.</p>
</li>
<li>
<p><code>--illegal-final-final-mutation=deny</code> will result in <code>Field::set</code> throwing an <code>IllegalAccessException</code> for every illegal <code>final</code> field mutation.</p>
<p>This mode will become the default in a future release.</p>
</li>
</ul>
<p>When <code>deny</code> becomes the default mode, <code>allow</code> will be removed but <code>warn</code> will remain supported for at least one release.</p>
<p>To prepare for the future, we recommend running existing code with the <code>deny</code> mode to identify code that mutates <code>final</code> fields via deep reflection.</p>
<h3 id="Warnings-on-mutation-of-final-fields">Warnings on mutation of <code>final</code> fields</h3>
<p>When <code>Field::set</code> on a <code>final</code> field is called from a module for which <code>final</code> field mutation is not enabled, the mutation will succeed but the Java runtime will, by default, issue a warning that identifies the caller:</p>
<pre><code>WARNING: Final field f in p.C has been [mutated/unreflected for mutation] by class com.foo.Bar.caller in module N (file:/path/to/foo.jar)
WARNING: Use --enable-reflective-final-mutation=N to avoid a warning
WARNING: Mutating final fields will be blocked in a future release unless final field mutation is enabled</code></pre>
<p>At most one such warning is issued for any particular module, and only if a warning has not yet been issued for that module. The warning is written to the standard error stream.</p>
<h3 id="Libraries-should-not-use-deep-reflection-to-mutate-final-fields">Libraries should not use deep reflection to mutate <code>final</code> fields</h3>
<p>The ability to mutate <code>final</code> fields via deep reflection was added in JDK 5 so that serialization libraries could provide functionality on par with the JDK's own serialization facilities. In particular, the JDK can deserialize objects from an input stream even if the object's class declares <code>final</code> fields. The JDK bypasses the class's constructors that ordinarily assign instance fields, and instead <a href="https://docs.oracle.com/en/java/javase/23/docs/specs/serialization/input.html#the-objectinputstream-class">assigns values from the input stream to instance fields directly</a> -- even if they are <code>final</code>. Third-party serialization libraries use deep reflection to do the same.</p>
<p>When <code>final</code> field restrictions are strengthened in a future JDK release, serialization libraries will no longer be able to use deep reflection out of the box. Rather than asking users to enable <code>final</code> field mutation on the command line, the developers of serialization libraries should serialize and deserialize objects using the <a href="https://github.com/openjdk/jdk/blob/master/src/jdk.unsupported/share/classes/sun/reflect/ReflectionFactory.java"><code>sun.reflect.ReflectionFactory</code></a> class, which is <a href="https://openjdk.org/jeps/260#Critical-internal-APIs-not-encapsulated-in-JDK-9">supported for this purpose</a>. Its deserialization methods can mutate <code>final</code> fields even if called from code in modules that are not enabled for <code>final</code> field mutation.</p>
<p>The <code>sun.reflect.ReflectionFactory</code> class only supports deserialization of objects whose classes implement <code>java.io.Serializable</code>. We believe this limitation balances the interests of developers using serialization libraries with the wider interest of all developers in having correct and efficient execution. First, it limits the impact of any security exploit that utilizes deserialization, since it is not possible to craft malicious objects of arbitrary classes. Second, it ensures that the JVM, when performing optimizations such as constant folding, is not unduly constrained in the assumptions it can make about <code>final</code> fields. While the JVM has to treat <code>final</code> fields in <code>Serializable</code> objects as potentially mutable, it can assume that <code>final</code> fields in all other objects (the vast majority) are permanently immutable.</p>
<p>Distinct from serialization libraries, frameworks for dependency injection, unit testing, and mocking <a href="https://openjdk.org/jeps/8305968#Restrictions-on-standard-unsafe-APIs">use deep reflection</a> to manipulate objects, including mutating <code>final</code> fields. The maintainers of such frameworks should only ask users to enable <code>final</code> field mutation on the command line as a last resort. Instead, maintainers should <a href="https://openjdk.org/jeps/8305968#Embracing-integrity-by-default">find architectural approaches</a> that avoid the need to mutate <code>final</code> fields (and access <code>private</code> fields) altogether. For example, most dependency injection frameworks now forbid the injection of <code>final</code> fields, and all discourage it, instead recommending constructor injection.</p>
<h3 id="Mutating-final-fields-from-native-code">Mutating <code>final</code> fields from native code</h3>
<p>Native code can mutate Java fields by calling the <a href="https://docs.oracle.com/en/java/javase/22/docs/specs/jni/functions.html#settypefield-routines"><code>Set&lt;Type&gt;Field</code> functions</a> or the <a href="https://docs.oracle.com/en/java/javase/22/docs/specs/jni/functions.html#setstatictypefield-routines"><code>SetStatic&lt;Type&gt;Field</code> functions</a> defined in the <a href="https://docs.oracle.com/en/java/javase/22/docs/specs/jni/index.html">Java Native Interface</a> (JNI).</p>
<p>The behavior of these functions on <code>final</code> fields is <a href="https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html">undefined</a>. This means that the function could mutate the field to the desired value, or mutate it to a different value, or not mutate it at all, or, e.g., mutate it correctly in 999 out of 1000 executions but cause a JVM crash in 1 out of 1000 executions. As we enhance the JVM's catalog of optimizations to exploit the <code>final</code> field restrictions placed on Java code, the chance of oddball outcomes due to undefined behavior in native code becomes more likely.</p>
<p>There are already <a href="https://openjdk.org/jeps/472">restrictions on executing native code</a> due to the possibility of undefined behavior, so by default the JVM can assume that these functions are not called. However, if <a href="https://openjdk.org/jeps/472#Enabling-native-access">native access is enabled</a>, then this JEP proposes new diagnostics to mitigate the risks of oddball outcomes from mutating <code>final</code> fields via JNI:</p>
<ul>
<li>
<p>If the application is started with <a href="https://openjdk.org/jeps/158">unified logging</a> enabled for native code (<code>-Xlog:jni=debug</code>), calling any of the functions mentioned above on a <code>final</code> field will cause a message to be logged:</p>
<pre><code>[0.20s][debug][jni] Set&lt;Type&gt;Field of final instance field C.f</code></pre>
<p>or</p>
<pre><code>[0.20s][debug][jni] SetStatic&lt;Type&gt;Field of final static field C.f</code></pre>
</li>
<li>
<p>If the application is started with <a href="https://docs.oracle.com/en/java/javase/23/docs/specs/man/java.html#extra-options-for-java">additional checking of JNI functions</a> (<code>-Xcheck:jni</code>), calling any of the functions mentioned above on a <code>final</code> field will cause the JVM to terminate with an error message.</p>
</li>
</ul>
<p>In a future JDK release, the functions mentioned above may be changed so that they always return successfully when called on <code>final</code> fields, but never actually effect any mutation.</p>
<p>There are no diagnostics for when Java code mutates <code>final</code> fields via the <code>sun.misc.Unsafe</code> class. Such mutation may cause strange bugs or JVM crashes.</p>
<h2 id="Risks-and-Assumptions">Risks and Assumptions</h2>
<ul>
<li>
<p>The ability to mutate <code>final</code> fields has been part of the Java Platform since JDK 5, so there is a risk that existing applications will be impacted by the <code>final</code> field restrictions.</p>
</li>
<li>
<p>We assume that developers whose applications rely directly or indirectly on mutating <code>final</code> fields will be able to configure the Java runtime to enable the capability via <code>--enable-final-final-mutation</code>. This is similar to how they can already configure the Java runtime to disable strong encapsulation for modules via <code>--add-opens</code>.</p>
</li>
</ul>
<h2 id="Alternatives">Alternatives</h2>
<ul>
<li>
<p>Rather than enforce the immutability of <code>final</code> fields, the Java runtime could rely on speculation and optimistically assume that <code>final</code> fields are not mutated, detecting when they are, and deoptimizing code when that happens. While speculative optimizations are the bread-and-butter of the JVM's JIT compiler, they may not suffice in this case as future planned optimizations may wish to rely not only on immutability within the lifetime of the process, but also on the immutability of fields from one run of the application to the next.</p>
</li>
<li>
<p>Instead of specifying the modules whose code can mutate <code>final</code> fields, we could specify the modules that allow their classes' <code>final</code> fields to be mutated. However, since the mutation of <code>final</code> fields is generally undesirable, it is better for command-line options to record which modules should be changed to no longer perform mutation. Specifying the modules whose <code>final</code> fields can be mutated would make it hard to know for what purpose they allow their fields to be mutated, and by whom.</p>
</li>
<li>
<p>Requiring <code>--enable-final-field-mutation</code> to specify <em>both</em> sides — the module performing the mutation <em>and</em> the module containing the mutated field — is unnecessarily burdensome. In many practical cases, <code>--enable-final-field-mutation</code> will be specified in conjunction with <code>--add-opens</code>, which already specifies both sides of the reflective access.</p>
</li>
</ul>
</div></div>]]></description>
        </item>
    </channel>
</rss>