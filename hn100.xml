<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 07 Dec 2025 02:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Kilauea erupts, destroying webcam [video] (118 pts)]]></title>
            <link>https://www.youtube.com/watch?v=TK2N99BDw7A</link>
            <guid>46177645</guid>
            <pubDate>Sat, 06 Dec 2025 23:39:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=TK2N99BDw7A">https://www.youtube.com/watch?v=TK2N99BDw7A</a>, See on <a href="https://news.ycombinator.com/item?id=46177645">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Screenshots from developers: 2002 vs. 2015 (2015) (171 pts)]]></title>
            <link>https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</link>
            <guid>46176905</guid>
            <pubDate>Sat, 06 Dec 2025 21:55:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/">https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</a>, See on <a href="https://news.ycombinator.com/item?id=46176905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

	<article>

		

		<div>
			<p>In 2002 I asked a number of developers/Unix people for screenshots of their desktops. I recently <a href="https://anders.unix.se/2015/10/28/screenshots-from-developers--unix-people-2002/">republished them</a>, and, seeing <a href="https://news.ycombinator.com/item?id=10469824">the</a> <a href="https://www.reddit.com/r/linux/comments/3qlyf6/screenshots_from_developers_unix_people_2002/">interest</a> this generated, I thought it’d be fun to ask the same people* again 13 years later. To my delight I managed to reach many of them.</p>

<p><small>* Sans Dennis Ritchie and itojun, who are no longer with us.</small></p>

<p>So, without further ado:</p>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/bwk_desktop.jpg">
        <img src="https://anders.unix.se/images/bwk_desktop.jpg">
    </a>
    
    <figcaption>
        July 2002
        
    </figcaption>
    
</figure>


<blockquote><p>my desktop is pretty boring, since it consists of xterm windows to whatever unix system i am using at the moment.  the machine itself is likely to be running some x-window server like exceed on some flavor of windows, though for many years i just used an x terminal.</p></blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_bwk_2015.png">
        <img src="https://anders.unix.se/images/desktop_bwk_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        October 2015
        
    </figcaption>
    
</figure>


<blockquote><p>If you thought it was boring last time, check this out!</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<p>2002:</p>
<blockquote><p>
I don’t know how to make a screenshot, because I normally use my computer in text-mode. I have X and GNOME installed, but I use them only occasionally.
</p></blockquote>

</div>
<div>
<p>2015:</p>
<blockquote><p>
Under X, I use the standard environment of Trisquel, but mostly I type at Emacs in a console.
</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_b_moolenaar.png">
        <img src="https://anders.unix.se/images/desktop_b_moolenaar.png.thumb.jpg">
    </a>
    
    <figcaption>
        September 2002
        
    </figcaption>
    
</figure>


<blockquote><p>
Well, my desktop is quite boring. I mostly work with four xterms and a few Netscape windows. The KDE bar hides automatically, you can only see a thin grey line at the bottom.
</p></blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_b_moolenaar_2015.png">
        <img src="https://anders.unix.se/images/desktop_b_moolenaar_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        November 2015
        
    </figcaption>
    
</figure>


<blockquote><p>
Here is the new one.  You'll see that, like before, I have lots of xterms where I work on Vim, Zimbu and email.  Now using the Chrome browser, showing off the Zimbu homepage.  But clearly everything has become bigger!
</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_rasmus_lerdorf.png">
        <img src="https://anders.unix.se/images/desktop_rasmus_lerdorf.png.thumb.jpg">
    </a>
    
    <figcaption>
        September 2002
        
    </figcaption>
    
</figure>


<blockquote><p>
Linux (2.4.20-pre5), Gnome2, vim, Pine.
</p></blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_rasmus_lerdorf_2015.png">
        <img src="https://anders.unix.se/images/desktop_rasmus_lerdorf_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        October 2015
        
    </figcaption>
    
</figure>


<blockquote><p>
Not that much has changed in 13 years. Still using Linux. Still just a browser window and a ton of terminals hiding behind them.  The main change is that switched from Pine to Thunderbird for email at some point. The OS on my laptop here is Ubuntu with Unity although there are a lot of Debian packages installed so it is a bit of a hybrid at this point. Oh, and yes, my son Carl is a lot older now.
</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_warren_toomey.gif">
        <img src="https://anders.unix.se/images/desktop_warren_toomey.gif">
    </a>
    
    <figcaption>
        August 2002
        
    </figcaption>
    
</figure>


<blockquote>
<p>Ah, my desktop is pretty boring, I used fvwm 1.24 as my window manager and I try to have no more than 1 or 2 windows open per virtual desktop.  I use FreeBSD 4-STABLE as my operating system. I first came across Unix when I got an account on a Pyramid 90x running OSx. This had a dual-universe setup: both AT&amp;T and BSD-style environments, chosen by an environment variable. Initially I was given the AT&amp;T environment, but my friends convinced me to ``come over” to BSD. Since then I’ve been a BSD afficionado.</p>

<p>After OSx, SunOS 3.5 and later SunOS releases, until 386BSD 0.1 came out and I started to run BSD at home. Then when 386BSD transmogrified to FreeBSD, I went with FreeBSD.</p>

<p>In terms of desktop, I’m a command-line guy, always will be. My favourite editor is vi, my favourite shell is tcsh (but kudos to rc for elegance).  So I don’t really feel the need for GUI things like Gnome or KDE :-)</p>
</blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_warren_toomey_2015.jpg">
        <img src="https://anders.unix.se/images/desktop_warren_toomey_2015.jpg">
    </a>
    
    <figcaption>
        October 2015
        
    </figcaption>
    
</figure>


<blockquote>
<p>How things have (and have not changed). I'm still a command-line junkie with at least two xterm windows open. I'm still using a 3x3 virtual desktop. However, instead of fvwm, it is now LXDE. I've also switched from FreeBSD to Linux and I'm running Lubuntu as my distribution.</p>

<p>There are a lot of indispensable GUI tools that I use. These include Firefox, lyx, Gimp, KeepassX, Shutter, viking, dia, Wireshark, calibre, audacity, Handbrake and VLC. But where possible I still prefer to script things. My main development languages are still shell, Perl and C.</p>

<p>My shell is now bash. The vi keystrokes are burned into my fingertips and, as long as vim can be ported to new systems, that will be my text editor until I pass on. My mail client is now mutt (definitely not a web client) and my mail is stored locally, not on someone else's server.</p>

<p>The only issue I have is that, since a job change, I now have to deal with Windoze things. Thus, I have VirtualBox, libreoffice and Wine to help me do that.</p>

<p>I started with Unix on a Pyramid 90x. I now have a smart phone that blows the 90x out of the water on performance, RAM and storage. But I'm so very happy that, somewhere down underneath, there is still a Bourne shell and an operating system that does open(), close(), read(), write(), fork() and exec()!</p>
</blockquote>

</div>
</div>

<hr>

<div>
<p><a href="https://en.wikipedia.org/wiki/Jordan_Hubbard">Jordan Hubbard</a> (FreeBSD co-founder, later Director of UNIX Technology at Apple; now CTO of iXsystems):
</p>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_jordan_hubbard.jpg">
        <img src="https://anders.unix.se/images/desktop_jordan_hubbard.jpg">
    </a>
    
    <figcaption>
        July 2002
        
    </figcaption>
    
</figure>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_jordan_hubbard_2015.png">
        <img src="https://anders.unix.se/images/desktop_jordan_hubbard_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        November 2015
        
    </figcaption>
    
</figure>


<blockquote>
<p>You’ll probably be sad (or perhaps not) to hear that my desktop hasn’t really changed much at all - still OS X, though because OS X has virtual desktops now I have multiple “desktops” (6 of them) where Mail.app runs on one, Safari on another, Calendar, Slack, etc - all on separate desktops.  This makes it a bit boring, but here’s the one I probably spend the most time in - the terminal window desktop. :)</p>
</blockquote>

</div>
</div>

<hr>



<hr>

<p>Discussion: <a href="https://news.ycombinator.com/item?id=10722536">Hacker News</a>; reddit: <a href="https://www.reddit.com/r/programming/comments/3wg48k/screenshots_from_developers_2002_vs_2015/">/r/programming</a>, <a href="https://www.reddit.com/r/linux/comments/3w83ta/screenshots_from_developers_2002_vs_2015/">/r/linux</a></p>

		</div>

		 


	</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The general who refused to crush Tiananmen's protesters (112 pts)]]></title>
            <link>https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters</link>
            <guid>46176072</guid>
            <pubDate>Sat, 06 Dec 2025 19:47:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters">https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters</a>, See on <a href="https://news.ycombinator.com/item?id=46176072">Hacker News</a></p>
Couldn't get https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[OMSCS Open Courseware (138 pts)]]></title>
            <link>https://sites.gatech.edu/omscsopencourseware/</link>
            <guid>46175826</guid>
            <pubDate>Sat, 06 Dec 2025 19:14:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sites.gatech.edu/omscsopencourseware/">https://sites.gatech.edu/omscsopencourseware/</a>, See on <a href="https://news.ycombinator.com/item?id=46175826">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
	<p><a href="#content">Skip to content</a></p><nav id="navbar-secondary" aria-label="secondary">
	<div>
				
			<p><a href="http://www.gatech.edu/">
					<img alt="Georgia Institute of Technology" src="https://sites.gatech.edu/omscsopencourseware/wp-content/themes/gatech-flex/img/gt-logo-oneline-white.svg" width="245px" height="42px">
				</a>
			</p>

			</div>
</nav>
	
		<!-- #wrapper-navbar end -->

	
		<header id="hero-main" aria-label="page title and basic information">
		

		
						<p><img width="850" height="478" src="https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1.png" alt="" decoding="async" fetchpriority="high" srcset="https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1.png 850w, https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1-300x169.png 300w, https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1-768x432.png 768w" sizes="(max-width: 850px) 100vw, 850px">						</p>
						
						
	</header>
	
<div id="page-wrapper">

			<main id="main">
									
<article class="page" id="post-7">
			<!-- .page-header -->

		
	<div>
		
		
<p>Georgia Tech’s Online Master of Science in Computer Science (OMSCS) program is proud to make the course content* for many of its courses publicly available through Ed Lessons. Select a course below to view the public content for that course.</p>



<p>Note that students enrolled in OMSCS should access their course content through Canvas, as the for-credit versions of these courses may include graded components or recent content updates not available through OMSCS Open Courseware.</p>



<p>*<em>Course content typically includes things such as lecture videos and exercises; it will not include things like homeworks, projects quizzes, exams, or other graded assignments.</em></p>

























































		
			</div><!-- .entry-content -->

	</article><!-- #post-## -->

												</main><!-- #main -->

			<!-- Do the right sidebar check -->
			
</div><!-- #page-wrapper -->


<!-- wrapper end -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perl's decline was cultural (197 pts)]]></title>
            <link>https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical</link>
            <guid>46175112</guid>
            <pubDate>Sat, 06 Dec 2025 17:42:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical">https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical</a>, See on <a href="https://news.ycombinator.com/item?id=46175112">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

		      
			  
		      


		      <div id="content">
			    <h3>According to the Discourse, somebody killed perl</h3><p>There's been a flurry of discussion <a href="https://news.ycombinator.com/item?id=45977900">on Hacker News</a> and <a href="https://lobste.rs/s/0m6yln/what_killed_perl">other tech forums</a> about what killed Perl. I wrote a lot of Perl in the mid 90s and subsequently worked on some of the most trafficked sites on the web in mod_perl in the early 2000s, so I have some thoughts. My take: it was mostly baked into the culture. Perl grew amongst a reactionary community with conservative values, which prevented it from evolving into a mature general purpose language ecosystem. Everything else filled the gap. </p><h3>I remember Perl</h3><p>Something to keep in mind, is that although this is my personal take, and therefore entirely an opinion piece, I <em>was</em> there at the time. I stopped doing Perl properly when I left Amazon, I think this would have been around 2005. It's based on the first hand impressions of somebody who was very deeply involved in Perl in its heyday, and moved on. I have a lot of experience, from both inside and outside the tent. </p><h3>Perl's roots are sysadmin</h3><p>What culture? Perl always had a significant amount of what you might call "BOFH" culture, which came from its old UNIX sysadmin roots. All of those passive aggressive idioms and in jokes like <em>"RTFM"</em>, <em>"lusers"</em>, <em>"wizards"</em>, <em>"asking for help the wrong way"</em> etc.  None of this is literally serious, but it does encode and inform social norms that are essentially tribal and introverted. There implicitly is a privileged population, with a cost of entry to join. Dues must be paid. Cultural conservatism as a first principle. </p><p>This stems from the old locked-down data centre command culture. When computer resource was expensive, centralised, fragile, and manually operated, it was rigidly maintained by gatekeepers, defending against inappropriate use. I started my career as an apprentice programmer at the very end of this era, (late 80s) pre-web, and before microcomputers had made much inroads, and this really was the prevailing view from inside the fort. (This is a drawback about fort-building. Once you live in a fort, it's slightly too easy to develop a siege mentality). Computers are special, users are inconvenient, disruption is the main enemy. </p><p>An unfortunate feedback loop in this kind of "perilous" environment is that it easily turns prideful. It's difficult to thrive here, if you survive and do well you are skilled; you've performed feats; you <em>should</em> mark your rites of passage. This can become a dangerous culture trap. If you're not careful about it, you may start to think of the hazards and difficulties, the "foot guns", as <em>necessary</em> features - they teach you those essential survival skills that mark you out. More unkindly, they keep the stupid folk out, and help preserve the high status of those who survived long enough to be assimilated. Uh-oh, now you've invented class politics. </p><p>The problem with this thinking is that it's self-reinforcing. Working hard to master system complexities was genuinely rewarding - you really were doing difficult things and doing them well. This is actually the same mechanism behind what eventually became known as 'meritocracy'<sup id="ref1"><a href="#fn1">1</a></sup>, but the core point is simpler - if difficulty itself becomes a badge of honour, you've created a trap: anything that makes the system more approachable starts to feel like it's cheapening what you achieved. You become invested in preserving the barriers you overcame. </p><p>(This is the same mentality that built leetcode interview pipelines BTW, but let's leave that sidebar alone for now) </p><p>So the UNIX operator culture tended to operate as a tribal meritocracy (as opposed to the UNIX <em>implementer</em> culture, which fell out of a different set of cultural norms, quite an interesting side bar itself<sup id="ref2"><a href="#fn2">2</a></sup>), a cultural priesthood, somewhat self-regarding, rewarding of cleverness and knowledge hoarding, prone to feats of bravado, full of lore, with a defensive mentality of keeping the flame aloft, keeping the plebs happy and fed, and warding off the barbarians. As we entered the 90s it was already gently in decline, because centralised computing was giving way to the rise of the microcomputer, but the sudden explosive growth of the WWW pulled internet / Unix culture suddenly back into the mainstream with an enormous and public opportunity vacuum. Everyone suddenly has an urgent need to write programs that push text off UNIX file-systems (and databases) and into web pages, and Perl is uniquely positioned to have a strong first-mover advantage in this suddenly vital, novel ecosystem. But it's culture and values are very much pulled across from this previous era. </p><p>(Springing out of this, Perl had an, at best grudging, tolerance for 'difficult genius' types, alongside this baseline culture. Unfortunately, this kind of toxic personality tends to thrive in the type of culture I've described, and they do set to help the tone. I'm not here to call out people specifically, because I'm trying to make a point rather than feed a culture war, or dig up gossip, but there were several significant examples, you can probably find lore if you like. I think the kindest way I can describe the compounding effect of this is that there was a strong cultural norm along the lines of "It's OK to be rude, as long as it's for a good cause".) </p><h3>A fort within a fort</h3><p>I remember this tension as always being tangibly there. Perl IRC and mailing lists were quite cliquey and full of venerated experts and in-jokes, rough on naivety, keen on robust, verbose debate, and a little suspicious of newcomers. And very cult-like. The "<a href="https://perl.fandom.com/wiki/TIMTOWTDI">TIMTOWTDI</a>" rule, although ostensibly liberal, literally means 'there is more than one way to do it <em>in Perl</em>' - and you can perhaps infer from that that there's little to no reason to do it using anything else. Elevating extreme flexibility like this is paradoxically also an engine of conservatism. If Perl can already do anything, flexibly, in multiple ways, then the language itself doesn't need to change - 'we already have one of those here, we don't need new things'. This attitude determined how Perl intended to handle evolution: the core language would remain stable (a fort inside a fort, only accessible to high level wizards), while innovation was pushed outward to CPAN. You could add features outside of core by writing and consuming third party libraries, you could bend language behaviour with pragmas without modifying Perl itself. The very best CPAN modules could theoretically be promoted into core, allowing the language to evolve conservatively from proven, widely-used features. </p><p>On paper, this sounds reasonable. In practice, I think it encoded a fundamental conflict of interest into the community early on, and set the stage for many of the later growth problems.  I'm not going to pretend that Perl <em>invented</em> dependency hell, but I think it turned out to be another one of those profound misfeatures that their cultural philosophy lead them to mistake for virtue, and embrace. </p><p>An interesting thing I think has been missed discussing the context of the original blog piece, about whether Perl 6 significantly impacted Perl growth, is the fact that Perl 6 itself manifested out of ongoing arguments. Perl 6 is a schism. Here's a oft-cited note from Larry Wall himself about the incident that sparked Perl 6, at <strike> YAPC</strike>  <a href="https://whitecamel.org/p/jon_orwant.html">OSCON 2000</a> </p><blockquote><p>We spent the first hour gabbing about all sorts of political and organizational issues of a fairly boring and mundane nature. Partway through, Jon Orwant comes in, and stands there for a few minutes listening, and then he very calmly walks over to the coffee service table in the corner, and there were about 20 of us in the room, and he picks up a coffee mug and throws it against the other wall and he keeps throwing coffee mugs against the other wall, and he says "we are f-ed unless we can come up with something that will excite the community, because everyone's getting bored and going off and doing other things". </p></blockquote><p>(Pause a second and ask yourself about the sort of social culture that both allows this kind of behaviour at public events, and then chooses to embrace it as a key piece of cultural lore) </p><h3>The impact of Perl 6</h3><p>Perl 6 was really a <em>schism</em>. Perl was already under a great amount of strain trying to accommodate the modernising influx of post dot-com mainstream web application building, alongside the entrenched conservatism of the core maintainers, and the maintenance burden of a few years exponential growth of third-party libraries, starting to build a fractal mess of slightly differentiating, incompatible approaches of those multiple ways to do things that were effectively now table-stakes language features, as the deployment landscape started to tiptoe towards a more modern, ubiquitous WWW<sup id="ref3"><a href="#fn3">3</a></sup>. </p><p>So, while I agree that it's wrong to generalise that 'Perl 6 killed Perl', I would say that Perl 6 was a symptom of the irreconcilable internal forces that killed Perl. Although, I also intend to go on to point out that Perl isn't dead, nothing has actually <em>killed</em> Perl. Killed Perl is a very stupid way to frame the discussion, but here we are. </p><p>So... Perl 6 is created as a valve to offset that pressure, and it kind of works. Up to a point. Unfortunately I think the side effect really is that the two branches of the culture, in the process of forking, double down on their encoded norms. Perl 5.x beds down as the practical, already solved way to do all the same things, with little need to change. Any requirements for more <em>modern</em> application patterns that are emerging in the broader web development environment, like idk, Unicode, REST clients, strict data structures, asynchronous I/O, whatever? That can <em>either</em> wait for Perl6 or you can pull things together using the CPAN if you want to move right now. Perl 6 leans the other way - they don't need to ship immediately, we have Perl 5 already here for doing things, Perl 6 is going to innovate on <em>everything</em>, and spend it's time getting there, designing up-front.<sup id="ref4"><a href="#fn4">4</a></sup> They spend at least two years writing high level requirement specs. They even spin out a side-project trying to build a universal virtual machine to run all dynamic programming languages that never delivers<sup id="ref5"><a href="#fn5">5</a></sup> </p><p>This is the landscape where Perl's central dominance of 'back end' web programming continues to slip. Unfortunately, alongside the now principled bias toward cultural conservatism, Perl 5 has an explicit excuse for it. The future is over there, and exciting, and meanwhile we're working usefully, and getting paid, and getting stuff done. Kind of OK from inside the fort. Some day we'll move to the newer fort, but right now <em>this is fine</em>. Not very attractive to newcomers though, really. And this is also sort of OK, because Perl doesn't really want those sort of newcomers, does it? The kind that turns up on IRC or forums and asks basic questions about Perl 6 and sadly often gets treated with open contempt. </p><h3>Meanwhile, over there</h3><p>Ruby has sprouted "Ruby on Rails", and it's taken the dynamic web building world by storm. Rails is a second generation web framework, that's proudly an 'opinionated web framework'. Given that the web application architecture is starting to stabilise into a kind of three-tier system , with a client as a web browser, a middle tier as a monolithic application server, and a persistence layer as a relational database , and a split server architecture serving static and dynamic content from different routes, here is just one way to do that, with hugely developer friendly tooling turning this into a cookie-cutter solution for the 80% core, and a plugin and client-side decoration approach that allows for the necessary per-site customisation. </p><p>Ruby is interesting as well. Ruby is kind of a Perl6 really. More accurately it's <a href="https://ruby-doc.org/docs/ruby-doc-bundle/FAQ/FAQ.html">a parallel universe Perl5</a> Ruby comes from Japan, and has developed as an attempt to build something similar to Perl, but it's developed much later, by programming language enthusiasts, and for the first ten years or so, it's mostly only used in Japan. To my line of thinking this is probably important. Ruby does not spring from decades of sysadmin or sysop culture. Ruby is a language for programmers, and is at this point an sensible candidate for building something like Rails with - a relatively blank canvas for dynamic programming, with many of the same qualities as Perl, with less legacy cruft, and more modern niceties, like an integrated object system, exceptions, straightforward data structures. Ruby also has adopted 'friendliness' as a core value, and the culture over there adopts a principled approach to aggressively welcoming newcomers, promoting easiness, and programmer happiness and convenience as strong first class principles. </p><p>Rails is a <em>huge</em> hit. At this point, which is around about the time I stopped significantly using Perl (2004-2005) (because I quit my job, not out of any core animosity toward it, in fact, in my day, I was really quite a Perl <em>fan</em>), Rails is the most appealing place to start as a new web programmer. Adoption rate is high, community is great, velocity of development is well-paced, and there's a lovely , well-lit, onboarding pipeline for how to start. You don't even really need to know ruby. It has a one-shot install tool, and generates working websites from templates, almost out of the box. It's an obvious starting point. </p><p>Perl being Perl, develops several analogue frameworks to Rails, all of them interdependently compatible and incompatible with each other and each other's dependencies, all of them designed to be as customisable and as user configurable as they possibly can be<sup id="ref6"><a href="#fn6">6</a></sup> </p><h3>PHP</h3><p>There are also the other obvious contenders. PHP has been there all along, and it's almost coming up from entirely the opposite cultural background of Perl. PHP is <em>a users language</em>. It's built to be deployed by copying script files to your home directory, with minimal server side impact or privileges. It's barely designed at all, but it encounters explosive growth all the way through the first (and through into the second) web era, almost entirely because it makes the barrier to onboarding so low as to be non-existent. PHP gets a couple of extra free shots in the arm </p><ol><li>Because it's architecture is so amenable to shared-server hosting, it is adopted as the primary implementation language of the blogging boom. An entire generation of web developers is born of installing and customising WordPress and text-pattern et. al by installing it directly into your home directory on a rented CPanel host account. It's the go-to answer for 'I'm not a programmer really but how do I get a personal web site'<sup id="ref7"><a href="#fn7">7</a></sup> This zero gate-keeping approach keeps the PHP stack firmly on the table of 'basic' web programmers all through the history of the web up to the current day. </li><li>Because of these initially lightweight deployment targets, PHP scales like little else, mostly because it's execution model leans strongly towards idempotent execution, with each web request tearing up and tearing down the whole environment. In a sense, this is slower than keeping hot state around, but it does lend itself extremely well to shared-nothing horizontal scaling, which as the web user base increases gigantically throughout the 2000s era, is the simplest route to scaling out. Facebook famously, is built in PHP at this point in time. </li></ol><h3>Python</h3><p>There is of course one other big horse in the race in this era, and it's a particularly interesting one in many ways, certainly when contrasted with Perl. This is of course, Python. Python is a close contemporary of Perl's but once again, it's roots are somewhere very different. Python doesn't come from UNIX culture either. Python comes from academia, and programming language culture. It's kind of a forgotten footnote, but Python was originally built for the <a href="https://en.wikipedia.org/wiki/Amoeba_(operating_system">Amoeba operating system</a>, and it's intention was to be a straightforward programming language for scripting this<sup id="ref8"><a href="#fn8">8</a></sup>. The idea was to build a language that could be the 'second programming language' for programmers. Given that this is the 1980s, early 1990s, the programmers would be expected to be mostly using C / C++ ,perhaps Pascal. Python was intended to allow faster development for lighter weight programs or scripting tasks. I suppose the idea was to take something that you might want to build in a shell script, but provide enough high level structured support that you could cleanly build the kind of things that quickly become a problem in shell scripts. So, it emphasises data structures, and scoped variables, and modules, and prioritises making it possible to extend the language with modules. Typical things that experienced programmers would want to use. The language was also designed to be portable between the different platforms programmers would use, running on the desktops of the day, but also on the server. As a consequence, it had a broad standard library of common <em>portable</em> abstractions around standard system features - file-systems, concurrency, time, FFI. For quite a long time, one of python's standard mottoes was 'batteries included'. </p><p>Python never set the world on fire at any particular moment, but it remained committed to a clear evolutionary incremental development, and clean engineering principles. Again, I think the key element here is cultural tone. Python is kind of boring, not trying to be anyone's best language, or even a universal language. Python was always a little fussy, maybe snobby, slightly abstracted away from the real world. It's almost as old as Perl and it just kept incrementally evolving, picking up users, picking up features, slowly broadening the standard library. The first time I saw Python pick up an undeniable mainstream advantage would also have been around the early 2000s, when Google publicly adopted it as one of their house standard languages. Never radical, just calmly evolving in it's environs. </p><h3>Nature abhors a vacuum</h3><p>When I sketch out this landscape, I remain firmly convinced that most of Perl's impedance to continued growth were cultural. Perl's huge moment of relevance in the 90s was because it cross-pollinated two diverging user cultures. Traditional UNIX / database / data-centre maintenance and admin users, and enthusiastic early web builders and scalers. It had a cultural shock phase from extremely rapid growth, the centre couldn't hold, and things slowly fell apart. </p><p>Circling back though, it's time to address the real elephant in the room. Perl manifestly did not die. It's here right now. It's installed I think by default, on almost every single computer I own and operate, without me doing a single thing to make that happen. It's still used every day by millions of people on millions of systems (even if that isn't deliberate). It's still used by many people <em>entirely deliberately</em> for building software, whether that's because they know it and like it and it works, or because they're interfacing with or working on legacy Perl systems (of which there are still many), or maybe they're using it still in it's original intentional role - A capable POSIX-native scripting language, with much better performance and a broader feature-set than any shell or awk. I still occasionally break it out myself, for small scripts I would like to use more than once, or as parts of CLI pipelines. </p><p>What I don't do any more is reach for Perl <em>first</em> to make anything new. In my case, it's just because I typically am spoilt for options that are a better fit for most tasks, depending on whatever it is I'm trying to achieve. By the time I came to Perl, (1998-ish), I was already on my third career phase, I had a strong UNIX background, and had already built real things in lisp, java, pascal, visual basic and C++. My attitude to languages was already informed by picking a tool to fit the task at hand. Boy did I love Perl for a few years. The product/market-fit for those early web days was just beautiful. The culture did have too much of the negative tropes I've been pointing at, but that wasn't really a problem personally for me, I'd grown up amongst the BOFHs inside the data centres already, it wasn't too hard for me to assimilate, nor pick up the core principles. I did occasionally bounce off a couple of abrasive characters in the community, but mostly this just kept me loosely coupled, I enjoyed how the language solved the problems I needed solving quickly, I enjoyed the flexibility, and I also enjoyed the way that it made me feel smart, and en-route to my wizard's robes and hat, when i used it to solve harder problems in creative ways, or designed ways around bugs and gremlins. For a good 3-4 years I would have immediately picked it as my favourite language. </p><p>So as I say, I didn't fall out of it with any sense of pique, I just naturally moved to different domains, and picked up tools that best fit. After Amazon, I spent t a lot of time concentrating on OS X and audio programming, and that involved a lot of objective C, C++. The scripting tools in that domain were often in ruby, sometimes python. For personal hacking, I picked up lisp again<sup id="ref9"><a href="#fn9">9</a></sup> (which I'd always enjoyed in school). I dipped in and out of Perl here and there for occasional contract work, but I tended to gravitate more towards larger database stuff, where I typically found C, java and python. The next time I was building web things, it was all Rails and ruby, and then moving towards the web services / REST / cloud era, the natural fits were go, and of course node and JavaScript or Typescript. I've always been a polyglot, and I've always been pretty comfortable moving between programming languages. The truth of the matter is, that the majority of programming work is broadly similar, and the specific implementation details of the language you use don't matter all that much, if it's a good fit for the circumstances. </p><p>I can't imagine Perl disappearing entirely in my lifetime. I can remember entire programming environments and languages that are much, much deader than I can ever see Perl becoming. </p><ul><li>Pascal used to be <em>huge</em> for teaching and also for desktop development in the 8/16 bit era</li><li>Objective C - only really useful inside the Apple ecosystem, and they're hell bent on phasing it out.</li><li>Before I got into the Internet, I used to build application software for 16 bit Windows (3.11) which was a vast market, in a mixture of database 4GLs (like PowerBuilder, Gupta/Centura SQLWindows) and Win16 C APIs. This entire universe basically no longer exists, and is fully obsolete. There must be many similar cases.</li><li>I mean who the hell realistically uses common lisp any more outside of legacy or enthusiast markets? Less people than Perl I'm sure.</li></ul><p>Perl also got to be if not first, then certainly early to dominate a new market paradigm. Plenty of things never manage that. It's hard to see Perl as anything other than an enormous success on these terms. Perl innovated and influenced languages that came after in some truly significant ways. </p><ul><li>Tightly embedding regular expressions and extending regular expressions (the most commonly used regular expression dialect in other tools is Perl)</li><li>CPAN, for package/library distribution via the internet, with dependency resolution - and including important concepts like supply chain verification with strong package signatures</li><li>A huge emphasis on testing, automated test harnesses, and CI. Perl test format (TAP) is also widely found in other CI/harness systems</li><li>Blending the gap between shell / scripting / and system programming in a single tool. I suppose this is debatable, but the way Perl basically integrated all the fundamental POSIX/libc as native built-ins with broadly the same semantics, but with managed memory and shell conventions was really revolutionary. Before this, most languages I had ever seen broadly tended to sit in one box, afterwards, most languages tended to span across several.</li><li>Amazing integrated documentation, online, in-tool and also man pages. POD is maybe the most successful ever implementation of literate programming ideas (although most of the real docs don't intertwingle the documentation very much iirc)</li></ul><p>Just these points, and I'm sure there are many others that could be made, are enough of a legacy to be proud of. </p><p>Counterfactuals are stupid (but also fun). If I squint, I can imagine that a Perl with a less reactionary culture, and a healthier acceptance of other ideas and environmental change might have been able to evolve alongside the other tools in the web paradigm shift, and still occupy a more central position in today's development landscape. That's not the Perl we have though, and that didn't happen. And I'm very confident that without the Perl we did have, the whole of modern software practice would be differently shaped. I do think Perl now lives in a legacy role, with a declining influence, but that's really nothing to feel shame or regret for. Nobody is going to forcibly take Perl away as long as POSIX exists, and so far as I can see, that means forever. In 2025 too, I can see the invisible hand creeping up on some of these other systems I've mentioned. Rust is slowly absorbing C and C++. Ruby (and of course Rails) is clearly in decline, in a way that probably consigns it to become a similar legacy state. From a certain angle, it looks a lot like Typescript is slowly supplanting Python. I won't be entirely surprised if that happens, although at my age I kind of doubt I'll live to see the day. </p><h3>Footnotes</h3><p><a id="fn1" href="#ref1">1</a> : Meritocracy is a fun word. It was originally coined as a pejorative term to describe a dystopian mechanism by which modern i.e. Western / British society entrenches and justifies an unfair and unequal distribution of privilege </p><p><a id="fn2" href="#ref2">2</a> : The UNIX <em>implementer</em> culture, is scientific/academic and fell out of Bell Labs. I guess you could extend this school of thought as a cultural sweep towards building abstracted cloud operations, toward plan 9/ Inferno / go </p><p><a id="fn3" href="#ref3">3</a> : Web 2.0 was first <a href="https://www.webdesignmuseum.org/web-design-history/web-2-0-1999">defined in 1999</a> by <a href="http://darcyd.com/fragmented_future.pdf">Darcy DiNucci in a print article</a> , the term didn't become mainstream until it was picked up and promoted by Tim O'Reilly (then owner/operator of perl.com, trivia fans), an astute inside observer of the forces driving web development </p><p><a id="fn4" href="#ref4">4</a>: Another unfortunate bit of luck here. Right at the point of time that <em>'agile'</em> starts getting some traction as a more natural way to embrace software development - i.e. iterating in small increments against a changing environment and requirements, Perl 6 decides to do perhaps the most waterfall open source development process ever attempted. . It is fifteen years before Perl 6 ships something resembling a usable programming language.<br>
</p><p><a id="fn5" href="#ref5">5</a> : <a href="http://www.parrot.org/">The Parrot VM</a>, a lovely quixotic idea, which sadly fizzled out, after even Perl 6 stopped trying to target it. Interestingly enough, both python and ruby both made relatively high profile ports to the JVM that were useful enough to be used for production deploys in certain niches. </p><p><a id="fn6" href="#ref6">6</a> : A side effect of this degree of abstraction, is that as well as being very hard to get started, it's easy to fall foul of performance overhead. </p><p><a id="fn7" href="#ref7">7</a> : This ubituitious ecosystem of small footprint wordpress custom installs gives birth to the web agency model of commercial website building / small ecommerce sites, which thrives and is suprisingly healthy today. Recent, and slighly optimistic surveys have pitched WordPress as powering over 40% of all websites today. Now this is certainly inflated, but even if the realistic number is half of that, that's still pretty damn healthy. </p><p><a id="fn8" href="#ref8">8</a> : It's often repeated that Python was designed as a teaching language, but as far as I know, that's not actually the case. The designer of Python, Guido Van Rossum <a href="https://www.artima.com/articles/the-making-of-python">was previously working on a project</a> that <em>was</em> a intended as training language, called ABC, and many of ABC's syntax and structural features influenced or made their way into Python. </p><p><a id="fn9" href="#ref9">9</a> : Common lisp is a better answer to an infinitely flexible 'everything' chainsaw language than perl, IMHO </p>
			    <section>
			      <span>posted by <a rel="me" href="https://www.beatworm.co.uk/">cms</a></span><span> on <time datetime="2025-11-20">2025-11-20</time></span>
			    </section>
			  <p>tagged as</p>
		      </div>

              </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HTML as an Accessible Format for Papers (206 pts)]]></title>
            <link>https://info.arxiv.org/about/accessible_HTML.html</link>
            <guid>46173825</guid>
            <pubDate>Sat, 06 Dec 2025 14:59:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://info.arxiv.org/about/accessible_HTML.html">https://info.arxiv.org/about/accessible_HTML.html</a>, See on <a href="https://news.ycombinator.com/item?id=46173825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  

  <a href="https://github.com/arXiv/arxiv-docs/blob/develop/source/about/accessible_HTML.md" title="Edit this page">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"></path></svg>
  </a>


<h2 id="html-as-an-accessible-format-for-papers">HTML as an accessible format for papers</h2>
<p>Accessibility barriers in research are not new, but they are urgent. The message we have heard from our community is that arXiv can have the most impact in the shortest time by offering HTML papers alongside the existing PDF.</p>
<p>arXiv has successfully launched papers in HTML format. We are gradually backfilling HTML for arXiv's corpus of over 2 million papers over time. Not every paper can be successfully converted, so a small percentage of papers will not have an HTML version. We will work to improve conversion over time.</p>
<p>The link to the HTML format will appear on abstract pages below the existing PDF download link. Authors will have the opportunity to preview their paper’s HTML as a part of the submission process.</p>
<p>The beta rollout is just the beginning. We have a long way to go to improve HTML papers and will continue to solicit feedback from authors, readers, and the entire arXiv community to improve conversions from LaTeX.</p>
<h2 id="why-experimental-html">Why "experimental" HTML?</h2>
<p>Did you know that 90% of submissions to arXiv are in TeX format, mostly LaTeX? That poses a unique accessibility challenge: to accurately convert from TeX—a very extensible language used in myriad unique ways by authors—to HTML, a language that is much more accessible to screen readers and text-to-speech software, screen magnifiers, and mobile devices. In addition to the technical challenges, the conversion must be both rapid and automated in order to maintain arXiv’s core service of free and fast dissemination.</p>
<p>Because of these challenges we know there will be some conversion and rendering issues. We have decided to launch in beta with “experimental” HTML because:</p>
<ol>
<li>Accessible papers are needed now. We have talked to the arXiv community, especially researchers with accessibility needs, and they overwhelmingly asked us not to wait.</li>
<li>We need your help. The obvious work is done. Reports from the community will help us identify issues we can track back to specific LaTeX packages that are not converting correctly.</li>
</ol>
<h2 id="error-messages-you-may-see-in-html-papers">Error messages you may see in HTML papers</h2>
<p>HTML papers on arXiv.org are a work in progress and will sometimes display errors. As we work to improve accessibility we share with you the causes of these errors and what authors can do to help minimize them. <a href="https://info.arxiv.org/about/accessibility_html_error_messages.html">Learn more about error messages you may see in HTML papers</a></p>
<h2 id="ways-to-help">Ways to help</h2>
<h3 id="1-read-html-papers-and-report-issues">1) Read HTML papers and report issues</h3>
<p>We encourage the community to try out HTML papers in your field:</p>
<h4 id="report-an-issue">Report an issue</h4>
<ul>
<li>Go to the abstract page for a paper you are interested in reading.</li>
<li>Look in the section where you find the link to the PDF download, and click the new link for HTML.</li>
<li>Report issues by either <strong>a)</strong> clicking on the Open Issue button <strong>b)</strong> selecting text and clicking on the Open Issue for Selection button or <strong>c)</strong> use <code>Ctrl+?</code> on your keyboard. If you are using a screen reader, use <code>Alt+y</code> to toggle accessible reporting buttons per paragraph.</li>
</ul>
<p><strong>Please do not create reports that the HTML paper doesn't look exactly like the PDF paper</strong></p>
<p>Our primary goal for this project is to make papers more accessible, so the focus during the beta phase will value function over form. HTML layouts that are incorrect or are illegible are important to report. But we do expect the HTML papers to present differently than the same paper rendered in PDF. Line breaks will occur in different places and there is likely to be more white space. In general, the HTML paper won't present as compactly. Intricate typographic layouts will not be rendered so intricately. This is by design.</p>
<p>HTML is a different medium and brings its own advantages versus PDF. In addition to being much more compatible with assistive technologies, HTML does a far better job adapting to the characteristics of the device you are reading on, including mobile devices.</p>
<h3 id="2-help-improve-the-conversion-from-latex">2) Help improve the conversion from LaTeX</h3>
<p>If you are an author you can help us improve conversions to HTML by following our guide to <a href="https://info.arxiv.org/help/submit_latex_best_practices.html">LaTeX Markup Best Practices for Successful HTML Papers</a>.</p>
<p>If you are a developer and have free development cycles, help us improve conversions! Our collaborators at LaTeXML maintain a <a href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML">list of issues</a> and welcome feedback and developer contributions.</p>
<p>If you are a publisher, member of a society, or conference organizer you can help us improve conversions to HTML by reviewing the .cls files your organization recommends to authors for unsupported packages. Providing .cls files that use supported packages is an easy way to support and sow accessibility in the scientific community. </p>
<h2 id="thank-you-to-our-collaborators">Thank you to our collaborators</h2>
<p>First, we want to share a special thank you to all the scientists with disabilities who have generously shared their insights, expertise, and guidance throughout this project.</p>
<p>We want to thank two organizations without which HTML papers on arXiv would not be possible: The <a href="https://www.latex-project.org/">LaTeX Project</a>, and the <a href="https://math.nist.gov/~BMiller/LaTeXML/">LaTeXML</a> team from NIST. We deeply thank each member of these teams for their knowledge, incredible work, and commitment to accessibility.</p>





                
              </article>
            </div>
        
      </main>
      






    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tiny Core Linux: a 23 MB Linux distro with graphical desktop (362 pts)]]></title>
            <link>http://www.tinycorelinux.net/</link>
            <guid>46173547</guid>
            <pubDate>Sat, 06 Dec 2025 14:18:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.tinycorelinux.net/">http://www.tinycorelinux.net/</a>, See on <a href="https://news.ycombinator.com/item?id=46173547">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        <h3>Welcome to The Core Project - Tiny Core Linux</h3>
        
        <p>The Core Project is a highly modular based system with community build extensions.
</p><p>
 It starts with a recent Linux kernel, vmlinuz, and our root filesystem and start-up scripts packaged with a basic set of kernel modules in core.gz.
Core (11MB) is simply the kernel + core.gz - this is the foundation for user created desktops, servers, or appliances.
TinyCore is Core + Xvesa.tcz + Xprogs.tcz + aterm.tcz + fltk-1.3.tcz + flwm.tcz + wbar.tcz
</p><p>
TinyCore becomes simply an example of what the Core Project can produce, an 16MB FLTK/FLWM desktop.
</p><p>
CorePlus ofers a simple way to get started using the Core philosophy with its included community packaged
extensions enabling easy embedded frugal or pendrive installation of the user's choice of supported desktop, while
maintaining the Core principal of mounted extensions with full package management.
</p><p>

It is not a complete desktop nor is all hardware completely supported. It represents only the core needed to boot into a very minimal X desktop typically with wired internet access.</p><p>

The user has complete control over which applications and/or additional hardware to have supported, be it for a desktop, a netbook, an appliance, or server, selectable by the user by installing additional applications from online repositories, or easily compiling most anything you desire using tools provided.</p>

<p>The latest version: <b>16.2</b></p>

<h3>News</h3>



<h3>About Our Project</h3>

<p>Our goal is the creation of a nomadic ultra small graphical desktop operating system capable of booting from cdrom, pendrive, or frugally from a hard drive. The desktop boots extremely fast and is able to support additional applications and hardware of the users choice. While Tiny Core always resides in ram, additional applications extensions can either reside in ram, mounted from a persistent storage device, or installed into a persistent storage device.</p>

<p>We invite interested users and developers to explore Tiny Core. Within our forums we have an open developement model. We encourage shared knowledge. We promote community involvement and community built application extensions. Anyone can contribute to our project by packaging their favorite application or hardware support to run in Tiny Core. The Tiny Core Linux Team currently consists of eight members who peruse the forums to assist from answering questions to helping package new extensions.
</p><p>
Join us here and on IRC Freenode <a href="irc://irc.freenode.net/tinycorelinux">#tinycorelinux</a>.
</p><p>
Learn. Share. Grow your knowledge of Linux.
</p><p>
Robert Shingledecker, December 01, 2008 </p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GrapheneOS is the only Android OS providing full security patches (475 pts)]]></title>
            <link>https://grapheneos.social/@GrapheneOS/115647408229616018</link>
            <guid>46173407</guid>
            <pubDate>Sat, 06 Dec 2025 13:58:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grapheneos.social/@GrapheneOS/115647408229616018">https://grapheneos.social/@GrapheneOS/115647408229616018</a>, See on <a href="https://news.ycombinator.com/item?id=46173407">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[How I discovered a hidden microphone on a Chinese NanoKVM (377 pts)]]></title>
            <link>https://telefoncek.si/2025/02/2025-02-10-hidden-microphone-on-nanokvm/</link>
            <guid>46173383</guid>
            <pubDate>Sat, 06 Dec 2025 13:54:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://telefoncek.si/2025/02/2025-02-10-hidden-microphone-on-nanokvm/">https://telefoncek.si/2025/02/2025-02-10-hidden-microphone-on-nanokvm/</a>, See on <a href="https://news.ycombinator.com/item?id=46173383">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">
	<div itemprop="articleBody">
		<p>NanoKVM is a <strong>hardware KVM switch</strong> developed by the Chinese company Sipeed. Released last year, it enables remote control of a computer or server using a virtual keyboard, mouse, and monitor. Thanks to its compact size and low price, it quickly gained attention online, especially when the company promised to release its code as open-source. However, as we’ll see, the device has some serious security issues. But first, let’s start with the basics.</p>

<h2 id="how-does-the-device-work">How Does the Device Work?</h2>

<p>As mentioned, NanoKVM is a KVM switch designed for remotely controlling and managing computers or servers. It features an HDMI port, three USB-C ports, an Ethernet port for network connectivity, and a special serial interface. The package also includes a small accessory for managing the power of an external computer.</p>

<p>Using it is quite simple. First, you connect the device to the internet via an Ethernet cable. Once online, you can access it through a standard web browser (though <em>JavaScript JIT</em> must be enabled). The device supports Tailscale VPN, but with some effort (read: hacking), it can also be configured to work with your own VPN, such as WireGuard or OpenVPN server. Once set up, you can control it from anywhere in the world via your browser.</p>

<div>
<p><a href="https://telefoncek.si/static/2025/02/NanoKVM.jpg">
<img src="https://telefoncek.si/static/2025/02/NanoKVM.jpg" alt="NanoKVM"></a></p><p>
NanoKVM
</p>
</div>

<p>The device could be connected to the target computer using an HDMI cable, capturing the video output that would normally be displayed on a monitor. This allows you to view the computer’s screen directly in your browser, essentially acting as a virtual monitor.</p>

<p>Through the USB connection, NanoKVM can also emulate a keyboard, mouse, CD-ROM, USB drive, and even a USB network adapter. This means you can remotely control the computer as if you were physically sitting in front of it - but all through a web interface.</p>

<p>While it functions similarly to remote management tools like RDP or VNC, it has one key difference: there’s no need to install any software on the target computer. Simply plug in the device, and you’re ready to manage it remotely. NanoKVM even allows you to enter the BIOS, and with the additional accessory for power management, you can remotely turn the computer on, off, or reset it.</p>

<p>This makes it incredibly useful - you can power on a machine, access the BIOS, change settings, mount a virtual bootable CD, and install an operating system from scratch, just as if you were physically there. Even if the computer is on the other side of the world.</p>

<p>NanoKVM is also quite affordable. The fully-featured version, which includes all ports, a built-in mini screen, and a case, costs just over €60, while the stripped-down version is around €30. By comparison, a similar RaspberryPi-based device, PiKVM, costs around €400. However, PiKVM is significantly more powerful and reliable and, with a KVM splitter, can manage multiple devices simultaneously.</p>

<p>As mentioned earlier, the announcement of the device caused quite a stir online - not just because of its low price, but also due to its compact size and minimal power consumption. In fact, it can be powered directly from the target computer via a USB cable, which it also uses to simulate a keyboard, mouse, and other USB devices. So you have only one USB cable - in one direction it powers NanoKVM, on the other it helps it to simulate keyboard mouse and other devices on a computer you want to manage.</p>

<p>The device is built on the open-source RISC-V processor architecture, and the manufacturer eventually did release the device’s software under an open-source license at the end of last year. (To be fair, one part of the code remains closed, but the community has already found a suitable open-source replacement, and the manufacturer has promised to open this portion soon.)</p>

<p><strong>However, the real issue is security.</strong></p>

<p>Understandably, the company was eager to release the device as soon as possible. In fact, an early version had a minor hardware design flaw - due to an incorrect circuit cable, the device sometimes failed to detect incoming HDMI signals. As a result, the company recalled and replaced all affected units free of charge. Software development also progressed rapidly, but in such cases, the primary focus is typically on getting basic functionality working, with security taking a backseat.</p>

<p>So, it’s not surprising that the developers made some serious missteps - rushed development often leads to stupid mistakes. But some of the security flaws I discovered in my quick (and by no means exhaustive) review are genuinely concerning.</p>

<p>One of the <a href="https://www.hackster.io/news/security-researcher-warns-on-sipeed-s-nanokvm-finds-vulnerabilities-and-a-cat-in-the-firmware-e1157a9ff0f4">first security analysis revealed numerous vulnerabilities</a> - and some rather bizarre discoveries. For instance, a security researcher even found an image of a cat embedded in the firmware. While the Sipeed developers acknowledged these issues and relatively quickly fixed at least some of them, many remain unresolved.</p>

<div>
<p><a href="https://telefoncek.si/static/2025/02/device.jpg">
<img src="https://telefoncek.si/static/2025/02/device.jpg" alt="NanoKVM"></a></p><p>
NanoKVM
</p>
</div>

<p>After purchasing the device myself, I ran a quick security audit and found several alarming flaws. The device initially came with a default password, and <code>SSH</code> access was enabled using this preset password. I reported this to the manufacturer, and to their credit, they fixed it relatively quickly. However, many other issues persist.</p>

<p>The user interface is riddled with security flaws - there’s no CSRF protection, no way to invalidate sessions, and more. Worse yet, the encryption key used for password protection (when logging in via a browser) is <strong>hardcoded and identical</strong> across all devices. This is a major security oversight, as it allows an attacker to easily decrypt passwords. More problematic, this needed to be explained to the developers. Multiple times.</p>

<p>Another concern is the device’s reliance on Chinese DNS servers. And configuring your own (custom) DNS settings is quite complicated. Additionally, the device communicates with Sipeed’s servers in China - downloading not only updates but also the closed-source component mentioned earlier. For this closed source component it needs to verify an identification key, which is stored on the device in plain text. Alarmingly, the device does not verify the integrity of software updates, includes a strange version of the WireGuard VPN application (which does not work on some networks), and runs a heavily stripped-down version of Linux that lacks <code>systemd</code> and <code>apt</code>. And these are just a few of the issues.</p>

<p>Were these problems simply oversights? Possibly. But what additionally raised red flags was the presence of <code>tcpdump</code> and <code>aircrack</code> - tools commonly used for network packet analysis and wireless security testing. While these are useful for debugging and development, they are also <strong>hacking tools</strong> that can be dangerously exploited. I can understand why developers might use them during testing, but they have absolutely no place on a production version of the device.</p>

<p>A Hidden Microphone</p>

<p>And then I discovered something even more alarming - <strong>a tiny built-in microphone that isn’t clearly mentioned in the official documentation</strong>. It’s a miniature SMD component, measuring just 2 x 1 mm, yet capable of recording surprisingly high-quality audio.</p>

<p>What’s even more concerning is that all the necessary recording tools are already installed on the device! By simply connecting via <code>SSH</code> (remember, the device initially used default passwords!), I was able to start recording audio using the amixer and arecord tools. Once recorded, the audio file could be easily copied to another computer. With a little extra effort, it would even be possible to stream the audio over a network, allowing an attacker to eavesdrop in real time.</p>

<div>
<p><a href="https://telefoncek.si/static/2025/02/hidden_microphone.jpg">
<img src="https://telefoncek.si/static/2025/02/hidden_microphone.jpg" alt="Hidden Microphone in NanoKVM"></a></p><p>
Hidden Microphone in NanoKVM
</p>
</div>

<p>Physically removing the microphone is possible, but it’s not exactly straightforward. As seen in the image, disassembling the device is tricky, and due to the microphone’s tiny size, you’d need a microscope or magnifying glass to properly desolder it.</p>

<p><strong>To summarize</strong>: the device is riddled with security flaws, originally shipped with default passwords, communicates with servers in China, comes preinstalled with hacking tools, and even includes a built-in microphone - fully equipped for recording audio - without clear mention of it in the documentation. Could it get any worse?</p>

<p>I am pretty sure these issues stem from extreme negligence and rushed development rather than malicious intent. However, that doesn’t make them any less concerning.</p>

<p>That said, these findings don’t mean the device is entirely unusable.</p>

<p>Since the device is open-source, it’s entirely possible to install custom software on it. In fact, <a href="https://github.com/scpcom/sophgo-sg200x-debian">one user has already begun porting his own Linux distribution</a> - starting with Debian and later switching to Ubuntu. With a bit of luck, this work could soon lead to official Ubuntu Linux support for the device.</p>

<p>This custom Linux version already runs the manufacturer’s modified KVM code, and within a few months, we’ll likely have a fully independent and significantly more secure software alternative. The only minor inconvenience is that installing it requires physically opening the device, removing the built-in SD card, and flashing the new software onto it. However, in reality, this process isn’t too complicated.</p>

<p>And while you’re at it, you might also want to remove the microphone… or, if you prefer, connect a speaker. In my test, I used an 8-ohm, 0.5W speaker, which produced surprisingly good sound - essentially turning the NanoKVM into a tiny music player. Actually, the idea is not so bad, because <a href="https://docs.pikvm.org/audio/">PiKVM also included 2-way audio support for their devices end of last year</a>.</p>

<div>
<p><a href="https://telefoncek.si/static/2025/02/speaker.jpg">
<img src="https://telefoncek.si/static/2025/02/speaker.jpg" alt="Basic board with speaker"></a></p><p>
Basic board with speaker
</p>
</div>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>All this of course raises an interesting question: How many similar devices with hidden functionalities might be lurking in your home, just waiting to be discovered? And not just those of Chinese origin. Are you absolutely sure none of them have built-in miniature microphones or cameras?</p>

<p>You can start with your iPhone - <a href="https://arstechnica.com/tech-policy/2025/01/apple-agrees-to-pay-95m-delete-private-conversations-siri-recorded/">last year Apple has agreed to pay $95 million to settle a lawsuit alleging that its voice assistant Siri recorded private conversations</a>. They shared the data with third parties and used them for targeted ads. “Unintentionally”, of course! Yes, that Apple, that cares about your privacy so much.</p>

<p>And Google is doing the same. They are facing a similar lawsuit over their voice assistant, but the litigation likely won’t be settled until this fall. So no, small Chinese startup companies are not the only problem. And if you are worried about Chinese companies obligations towards Chinese government, let’s not forget that U.S. companies also have obligations to cooperate with U.S. government. While Apple is publicly claiming they do not cooperate with FBI and other U. S. agencies (because thy care about your privacy so much), some media revealed that Apple was holding a series secretive Global Police Summit at its Cupertino headquarters <a href="https://www.forbes.com/sites/thomasbrewster/2024/10/09/apple-sells-privacy-to-consumers-but-its-quietly-helping-police-use-iphones-for-surveillance/">where they taught police how to use their products for surveillance and policing work</a>. And as one of the police officers pointed out - he has “<em>never been part of an engagement that was so collaborative</em>.”. Yep.</p>

<h3 id="ps-how-to-record-audio-on-nanokvm">P.S. How to Record Audio on NanoKVM</h3>

<p>If you want to test the built-in microphone yourself, simply connect to the device via <code>SSH</code> and run the following two commands:</p>

<ul>
  <li><code>amixer -Dhw:0 cset name='ADC Capture Volume 20'</code> (<em>this sets microphone sensitivity to high</em>)</li>
  <li><code>arecord -Dhw:0,0 -d 3 -r 48000 -f S16_LE -t wav test.wav &amp; &gt; /dev/null &amp;</code> (<em>this will capture the sound to a file named <code>test.wav</code></em>)</li>
</ul>

<p>Now, speak or sing (perhaps the Chinese national anthem?) near the device, then press <code>Ctrl + C</code>, copy the <code>test.wav</code> file to your computer, and listen to the recording.</p>

	</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Speed Matters (117 pts)]]></title>
            <link>https://lemire.me/blog/2025/12/05/why-speed-matters/</link>
            <guid>46172902</guid>
            <pubDate>Sat, 06 Dec 2025 12:46:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lemire.me/blog/2025/12/05/why-speed-matters/">https://lemire.me/blog/2025/12/05/why-speed-matters/</a>, See on <a href="https://news.ycombinator.com/item?id=46172902">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main">

		
<article id="post-22361">
	
		<p><img width="825" height="510" src="https://lemire.me/blog/wp-content/uploads/2025/12/Capture-decran-le-2025-12-06-a-14.08.31-825x510.png" alt="" decoding="async" fetchpriority="high">	</p><!-- .post-thumbnail -->

	
	<!-- .entry-header -->

	<div>
		<p>The one constant that I have observed in my professional life is that people underestimate the need to move fast.</p>
<p>Of course, doing good work takes time. I once spent six months writing a URL parser. But the fact that it took so long is not a feature, it is not a positive, it is a negative.</p>
<p>If everything is slow-moving around you, it is likely not going to be good. To fully make use of your brain, you need to move as close as possible to the speed of your thought.</p>
<p>If I give you two PhD students, one who completed their thesis in two years and one who took eight years… you can be almost certain that the two-year thesis will be much better.</p>
<p>Moving fast does not mean that you complete your projects quickly. Projects have many parts, and getting everything right may take a long time.</p>
<p>Nevertheless, you should move as fast as you can.</p>
<p>For multiple reasons:</p>
<p>1. A common mistake is to spend a lot of time—too much time—on a component of your project that does not matter. I once spent a lot of time building a podcast-like version of a course… only to find out later that students had no interest in the podcast format.</p>
<p>2. You learn by making mistakes. The faster you make mistakes, the faster you learn.</p>
<p>3. Your work degrades, becomes less relevant with time. And if you work slowly, you will be more likely to stick with your slightly obsolete work. You know that professor who spent seven years preparing lecture notes twenty years ago? He is not going to throw them away and start again, as that would be a new seven-year project. So he will keep teaching using aging lecture notes until he retires and someone finally updates the course.</p>
<p>What if you are doing open-heart surgery? Don’t you want someone who spends days preparing and who works slowly? No. You almost surely want the surgeon who does many, many open-heart surgeries. They are very likely to be the best one.</p>
<p>Now stop being so slow. Move!</p>
	</div><!-- .entry-content -->

	
<div>
	
	<p><img alt="" src="https://secure.gravatar.com/avatar/a0c6c34cf4a45ff5083d5ea541e7f27c5e4457ac393889e077af10688f6b3831?s=56&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/a0c6c34cf4a45ff5083d5ea541e7f27c5e4457ac393889e077af10688f6b3831?s=112&amp;d=mm&amp;r=g 2x" height="56" width="56" decoding="async">	</p><!-- .author-avatar -->

	<!-- .author-description -->
</div><!-- .author-info -->

	<!-- .entry-footer -->

</article><!-- #post-22361 -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Touching the Elephant – TPUs (159 pts)]]></title>
            <link>https://considerthebulldog.com/tte-tpu/</link>
            <guid>46172797</guid>
            <pubDate>Sat, 06 Dec 2025 12:29:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://considerthebulldog.com/tte-tpu/">https://considerthebulldog.com/tte-tpu/</a>, See on <a href="https://news.ycombinator.com/item?id=46172797">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><h4>Understanding the Tensor Processing Unit</h4><section><span></span>Reed<br><span>&nbsp;</span><time datetime="2025-12-01">December 1, 2025</time></section><p><img alt="latency hiding" src="https://considerthebulldog.com/assets/tpu/latency_hiding.jpeg"></p><h2 id="something-new">Something New</h2><p>There is mythological reverence for Google’s Tensor Processing Unit. While the world presently watches NVIDIA’s gravity drag more companies into its orbit, there sits Google, imperial and singular. Lots of companies participate in the “Cambrian-style explosion of new-interesting accelerators”<a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a> – Groq, Amazon, and Tenstorrent come to mind – but the TPU is the original existence proof. NVIDIA should take credit for the reemergence of deep learning, but the GPU wasn’t designed with deep learning in mind. What’s strange is that the TPU isn’t a secret. This research is indebted to Google’s public chest-thumping, but the devices themselves have long been exclusive to Google’s datacenters. That is over a decade of work on a hardware system sequestered behind their walls. That the TPU is so well documented yet without a true counterpart creates a strange asymmetry. Google is well positioned in the AI race because of their decision over a decade ago to build a hardware accelerator. It is because of the TPU.</p><p>On the back of DistBelief Google had gotten neural networks running at scale. In 2013 however they realized that they would need to double their datacenter capacity to meet the growing demand for these new services. “Even if this was economically reasonable, it would still take significant time, as it would involve pouring concrete, striking arrangements for windmill farm contracts, ordering and installing lots of computers, etc.” <a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a> The race against the clock began, and 15 months later the TPU was born. Fast forward to April of this year when Sundar Pichai announced the 7th generation TPU, Ironwood, at Google Cloud Next. The headline figures were eye-popping. 9,216 chips in a pod, 42.5 Exaflops, 10 MW <a href="https://considerthebulldog.com/tte-tpu/#ref-21">[21]</a>. In 12 years the TPU went from a research project to a goliath rack-scale system.</p><p>Perhaps reverence is warranted. The development of the TPU is set against the backdrop of a changing hardware scaling landscape. It used to be that to get better programs you just had to wait. With each new generation of chip Moore’s Law and Dennard Scaling brought enormous tailwinds in transistor density, power efficiency, and wall clock improvements. But in the aughts and 2010s there was no more sitting and no more waiting. The advancements in chip physics were not producing exponential returns as they once had, and workload demands continued growing.</p><p>Casting this as mythology however obscures the details and risks making the TPU seem like magic. The development of the TPU is the story of trade-offs and constraints and co-design. It touches hardware, software, algorithms, systems, network topology, and everything in between. It did not happen by accident, but through the deliberate process of design and iteration. When thinking about the TPU it’s natural to ask:</p><p>How did we get here?</p><h2 id="slowing-down">Slowing Down</h2><p>For decades the industry relied on Moore’s Law to pack more transistors into a smaller area and on Dennard Scaling to get more energy efficiency from those transistors. This netted out to smaller, faster, and more efficient devices. You didn’t need to change your software or architecture to realize significant gains, regardless of the domain. CPU performance doubled every 1.5 years from 1985-2003, and every 2 years from 2003-2010. The doubling speed since is closer to every 20 years <a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a>. The AlexNet moment in 2012 charted a course to the current renaissance in neural networks. Different hardware suddenly opened the door for new questions to be asked. The range of problems that neural networks were suited to solve, along with their appetite for bigger data and bigger models, meant that this algorithmic paradigm was taking off as our scaling paradigms began to languish.</p><p><img alt="Scaling Post Moore’s Law" src="https://considerthebulldog.com/assets/tpu/Post_Moores_Law.png"> <em>Degradation in the reliability of chip performance scaling under different regimes <a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a></em></p><p>The TPU falls into the broad classification of hardware accelerators, of which the marquee distinction is that it is specialized for certain computational domains, hence the name Domain Specific Accelerator. Whereas general purpose devices are designed to accommodate the maximum number of program shapes, specialized designs are defined as much by what they can do as what they can’t. They trade off generality for performance. If we can’t rely on Moore’s Law and Dennard Scaling, and there are new workloads demanding attention, the goal is to optimize for the characteristics of those workloads and to discard everything else. Specialization asks what the optimal way to spend a fixed transistor and energy budget is to squeeze out performance.</p><p>Linear algebra is ripe for specialization because a relatively small set of parallelizable operations dominate neural networks. For the TPU that meant a monastic focus on those primitives. Neural networks are simple compositions of Matrix-Vector, Matrix-Matrix, and Elementwise computations over large tensors. Consider that matrix multiplication has cubic complexity. While computationally expensive, this one class of operations is the spine for a large fraction of what is required for a neural network. This narrows the window of optimizations that need to be baked into silicon. Matrix multiplies have the property that as the size of inputs grow, the ratio of compute, O(n^3), to data access, O(n^2), improves <a href="https://considerthebulldog.com/tte-tpu/#ref-15">[15]</a>. If you can dedicate hardware to speeding up arithmetic and coordinating data movement you can exploit this, and the arithmetic properties are complemented by the runtime properties. Neural networks can be fully specified ahead of time. With clever planning a program can be entirely mapped out before an instruction is issued. There was rarely a need before to design, tape out, and deploy custom silicon. Free performance gains made the economics of simply waiting versus the cost of designing an ASIC a non-starter. The decline of hardware scaling made exploring these realities attractive.</p><p><img alt="Energy Per Operation of Common Operations" src="https://considerthebulldog.com/assets/tpu/Energy_Costs_Breakdown.png"> <em>Horowitz Energy per Operation <a href="https://considerthebulldog.com/tte-tpu/#ref-11">[11]</a></em></p><p>This opportunity is best exploited in the power budget. Compare the relative cost of arithmetic to control, memory access, and data movement. Horowitz <a href="https://considerthebulldog.com/tte-tpu/#ref-11">[11]</a> notes that over 50% of processor die energy is dissipated in caches and register files. These inefficiencies exist to mitigate the even greater inefficiency of large memory accesses. In <a href="https://considerthebulldog.com/tte-tpu/#ref-12">[12]</a> they cite that the energy to fetch and interpret instructions is 10-4000x more expensive than to perform simple operations. Moving and accessing data costs significantly more power, and what is required of deep learning is more arithmetic per unit control. Finding ways to circumvent relative power inefficiencies with specialization means rearchitecting chips to remove that waste.</p><h2 id="the-inference-chip">The Inference Chip</h2><p><img alt="TPU Block Diagram" src="https://considerthebulldog.com/assets/tpu/TPU_Block_Diagram.png"> <em>Block diagram of TPUv1 <a href="https://considerthebulldog.com/tte-tpu/#ref-1">[1]</a></em></p><p>Datacenter expansions plans are a hell of a drug. To stem the tide of models devouring datacenter capacity, the first ASIC needed to focus on inference. Inference only needs a forward pass through the neural network. A simple neural network layer might look like this:</p><p>$$ ReLU( (X \cdot W) + b ) $$</p><p>Where X and W are input data and model weights, ReLU is a non-linear activation function, and b is a bias term. A matrix multiply followed by some elementwise addition and an elementwise maximum function. Imagine that chaining a handful of these layers together forms the totality of an inference. This simplified view on early model architectures gives us the general template for designing TPUv1. Matrix multiply, some activation looking functions on that result, feed the results to storage, repeat. To meet the initial deadlines the TPU design exploited this loop-like behavior.</p><p>TPUv1 is a single-threaded co-processor connected over PCIe with a 24MiB software-controlled Unified Buffer, an 8-bit integer systolic array, and 8GiB DDR3 DRAM. The device runtime lays out tensors, plans memory transfers with a programmable DMA controller between the host and the Unified Buffer (on-chip SRAM), and tiles compute operands. The host sends 12-bit CISC instructions to the device’s instruction buffer which the in-order sequencer consumes to move data to DRAM and issue MXU ops. The datapath consumes ~2/3 of the die area of the chip <a href="https://considerthebulldog.com/tte-tpu/#ref-1">[1]</a>. Take care to notice what it is not. It is not a multi-level cache hierarchy. There is no multi-threading or branch prediction or prefetching or TLB. The systolic array executes arithmetic and the runtime eliminates control overhead. TPUv1 is a spartan device aimed at making inference fast.</p><p>The heart of the device is the Matrix Multiplication Unit (MXU). It is a 256x256, 2D weight-stationary systolic array of processing elements, in this case MACs. The MXU targets dense GEMMs to maximize arithmetic intensity. The TPU is designed to keep the MXU busy. You can find nice animated demonstrations of data moving through the systolic array <a href="https://fleetwood.dev/posts/domain-specific-architectures">here</a> or <a href="https://jax-ml.github.io/scaling-book/tpus/">here</a>.</p><p><img alt="MXU Cycle Timing" src="https://considerthebulldog.com/assets/tpu/MXU_Cycle_Timing.svg"> <em>MXU Cycle Timing</em></p><p>We’ll start with a simplified 4x4 systolic array. Although there are design variations of systolic execution <a href="https://considerthebulldog.com/tte-tpu/#ref-18">[18]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-36">[36]</a>, we are concerned with the 2D weight-stationary variant. The weights are pre-loaded into the array from the right hand side (the top in this diagram), and the inputs stream in from the left hand side (conveniently on the left). Once the weights are loaded they sit resident in the MACs, one weight per MAC. As the inputs flow from left to right, the MACs compute the product of the resident weight and the streamed input each cycle. The result of that computation is passed downward to the next processing element. If a MAC has one of these partial sums, it adds it to the result of the weight/input product and passes that new sum downward. At the bottom edge of the array there are no more computations and the result is passed to a 4096 row x 256-element bank of 32-bit accumulators.</p><p><img alt="MXU Double Buffering" src="https://considerthebulldog.com/assets/tpu/MXU_Double_Buffering.svg"> <em>MXU Double Buffering</em></p><p>Notice that weight pre-loading doesn’t happen all at once. It would waste cycles to wait for each MAC to have a resident weight before streaming in inputs. Weight pre-loading instead happens diagonally, with the left-most part of the systolic array receiving weights first. When the left column of processing elements has weights, the inputs begin streaming diagonally top to bottom. This imposes significant timing coordination for such a simple component. Much of the rest of the chips’ design can be thought of as accommodating these timing needs, and a particular instantiation of that is the liberal use of double buffering.</p><p>MXUs can perform immense amounts of arithmetic, but data movement/control stops at the edges of the systolic array. Between processing elements there is only result-passing with chains of two-input adders. If either weight or input data is not where it needs to be, stalls burn cycles that hurt MXU utilization. Spelling it out:</p><ul><li>The MXU holds two 64KiB tiles of weights with one reserved for double buffering</li><li>Four 64KiB weight tiles act as a FIFO queue to decouple memory accesses and weight loads between DRAM and the MXU</li><li>The Unified Buffer stores intermediate results from the accumulators and prepares new data to feed to the systolic array</li><li>The bank of accumulators logically splits 4096 rows into two chunks of 2048 rows, one to feed outputs and one to drain them</li></ul><details><summary>Sizing the MXU</summary> The number of processing elements that touch data before it reaches the accumulators grows quadratically with the array size which affects the speed of the computation. For a 256x256 array that is 65,536 MACs vs. 262,144 MACs in the 512x512 configuration. During fill/drain you pay an O(num_edges) cost to populate the buffers. Fewer edges better amortize this overhead. As arrays shrink they are penalized by wiring constraints. They perform less compute per data access and require running many wires between components. Sizing this device is a delicate balance between compute intensity and layout constraints, which we will see again in later generations.</details><p>The runtime knows how long each operation it issues should take, so it can intelligently overlap them with one another. During matrix multiplications the UB prepares the next batch of inputs, the fixed activation units operate on the results in the accumulators, and the Weight FIFO banks more weights. Matrix multiplies are relatively long latency, which leaves lots of cycles between when work starts and when work ends. The runtime schedules memory accesses, data movement and computation deterministically to minimize stop-the-world pauses rather than make coordination dependent on the MXU. Hiding latency with overlapping improves parallelism, improves data reuse, and conserves energy otherwise wasted in control flow.</p><p>The headline figures from their paper are anachronistic by now, but they help contextualize the accomplishment of the first gen chip. 25x as many MACs and 3.5x the on-chip memory of the K80 GPU. 15-30x the inference speed and 30-80x the perf/W of the K80 and the Haswell CPU <a href="https://considerthebulldog.com/tte-tpu/#ref-1">[1]</a>. The fixed-latency, software-managed design created a hardware accelerator that eschewed prevailing designs that spent energy in cache hierarchies and control overhead. Maniacal focus on mitigating inference bottlenecks with large SRAM and coordinated data movement proved that TPUv1 worked.</p><h2 id="the-training-chip">The Training Chip</h2><p>Neural networks need to be trained before they can be used for inference, and TPUv1 was not designed for training. Requirements include backpropagation to modify weights during execution, gradients with higher precision than int8, and support for diverse activation functions. This costs orders of magnitude more FLOPs <a href="https://considerthebulldog.com/tte-tpu/#ref-2">[2]</a>, and those FLOPs must be distributed over multiple devices while maintaining deterministic execution. TPUv1’s fixed activation units were not flexible enough for experimenting with new algorithms. The memory subsystem was not flexible enough to coordinate work between multiple devices. The UB was not flexible enough to tuck more Matrix-Vector work in behind the MXU. The whole device was too tightly coupled. Adding that flexibility, without reverting to a general-purpose processor, needed a radically different datapath.</p><p><img alt="TPUv2 Block Diagram" src="https://considerthebulldog.com/assets/tpu/TPUv2_ICI.png"> <em>TPUv2 Block Diagram <a href="https://considerthebulldog.com/tte-tpu/#ref-2">[2]</a></em></p><p>TPUv2 was animated from the bones of TPUv1, but only the MXU feels familiar. TPUv2 is a dual-core chip. Each core pairs a scalar controller with programmable vector units, local SRAM, a 128x128 MXU, and HBM. It adds inter-core interconnects (ICI) to communicate between the memory systems of each core and across chips. Two 128x128 MXUs combine to total the same 256x256 array from TPUv1 but simplify the circuit design. Unequal logic, wire, and SRAM scaling on smaller process nodes made arithmetic improvements comparatively free, enabling the chip designers to focus on the laggard scaling axes <a href="https://considerthebulldog.com/tte-tpu/#ref-2">[2]</a>. For the second generation MXUs that meant two efficiencies over their predecessor: BrainFloat16 and wire routing.</p><p><img alt="BF16 floating point format" src="https://considerthebulldog.com/assets/tpu/bf16.png"> <em>BF16 floating point format <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a></em></p><p>Dynamic range matters more than precision for neural network training. Gradients represented as integers don’t produce adequate convergence behavior; you need floating point numbers to make fine-grained weight updates. Accessing higher precision numerics however means sacrificing die area. Logic circuits need more adders to handle mantissa bits. Floating point adder arrays scale as (M+1) * (M+1), where M is the size of the mantissa, – 576 adders for fp32 and 121 adders for fp16 <a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a> – totalling more die area and more energy spent on arithmetic. Notice that although bf16 is the same number of bits as fp16, the proportion of exponent bits to mantissa bits is higher. bf16 only requires 64 adders in the MAC circuitry, and less circuitry means more MACs in the same package and power budget <a href="https://considerthebulldog.com/tte-tpu/#ref-2">[2]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a>.</p><p><img alt="MXU Sizing Considerations" src="https://considerthebulldog.com/assets/tpu/Why_128.png"> <em>MXU Sizing Considerations <a href="https://considerthebulldog.com/tte-tpu/#ref-32">[32]</a></em></p><p>Chip geometry considerations extend beyond individual processing elements. Big cores need long, global wires routed to/from functional units, FIFOs, and control units. Though wire diameters shrink on improved process nodes, their resistance and capacitance scale unevenly. Long wires are chunked into shorter segments connected with repeaters, but this induces signal delay making circuit timings more complex <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. MXU configurations with multiple smaller cores shorten average wire lengths but need wires routed all over the chip. The trade off is between compute bandwidth and array utilization. Compute utilization scales down quadratically with the array area, but smaller arrays use more energy-efficient wires. Splitting the die into two cores and running fewer, shorter wires to the vector and control units balances wiring scaling with utilization.</p><p><img alt="TPU Scalar Unit" src="https://considerthebulldog.com/assets/tpu/Scalar_Unit.png"> <em>TPU Scalar Unit <a href="https://considerthebulldog.com/tte-tpu/#ref-32">[32]</a></em></p><p>All those wires have to lead to somewhere. To drive the new datapath, TPUv2 introduces the scalar unit. When a user submits a program, the XLA compiler performs static analysis, lowering the program into 322-bit VLIW instruction bundles. XLA schedules DMAs, vector ops, and MXU work in a deterministic stream. The complexity of organizing program control flow is absorbed by software, keeping the scalar unit relatively simple. It is single-threaded and contains 4KB of scratchpad SRAM (SMEM), small instruction memory (IMEM), and a 32 element, 32-bit register file (SReg) connected to a dual-issue ALU. Sync registers flag when arithmetic and memory blocks are busy to explicitly synchronize execution. The host sends instructions over PCIe to HBM, where they are DMA’d into the Scalar Unit’s IMEM as overlays. Scalar instruction slots execute locally, and the vector/matrix slots are decoded and dispatched to the VPU/MXU <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>. There is no dynamic runtime scheduling, just instruction fetch, decode, and forward.</p><p>Two programmable vector processing units (VPU) consolidate the fixed function blocks from TPUv1. The VPU is a 2D SIMD processor designed to increase the ratio of vector operations to matrix operations. Each VPU has 128 vector lanes with 8 sublanes. Each sublane is connected to 32 dual-issue ALUs with lane-local register files (Vregs). The VPU is backed by 16MiB on-chip Vector Memory (VMEM) that mediates data movement to the MXU with pushes/pops onto a Result FIFO <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>. Each core’s VMEM has local access to half of the chip’s HBM, and DMAs to VMEM are strided to fetch contiguous tiles of data rather than issuing many small DMAs. The VPU accesses VMEM with explicit loads/stores to Vregs which remove the need for a cache hierarchy.</p><p>The simplicity of describing the rearchitected datapath belies the complexity that the subsystems represent. Whereas general purpose devices use branch predictors, TLBs, Out of Order execution, and a bevy of techniques to shuttle data and instructions, the TPU routes around a cache-centric design with software-managed execution. The aforementioned general purpose mechanisms alleviate runtime dependencies at the expense of more hardware and more energy. Control and caches consume massive amounts of the limited energy budget, so redesigning this subsystem is the difference between an economic chip and a renegotiated contract with power providers. When you know what operations you need, the order you need them in, and the operational characteristics of the hardware, you can move control flow to compile time. The VPU and Scalar Units are co-designed to leverage this operating paradigm, moving program orchestration to software.</p><p><img alt="VLIW Instruction Bundles" src="https://considerthebulldog.com/assets/tpu/VLIW_Bundle.svg"> <em>Sample VLIW Instructions</em></p><p>VLIW instructions expose this complexity. They contain slots for 2 scalar, 4 vector, 2 matrix, 1 miscellaneous, and 6 immediate instructions <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>. Slots map to scalar/vector/matrix arithmetic, loads and stores, DMAs, synchronization flags, and data literals. Though innocuously named, the miscellaneous slot controls heaven and earth. It is reserved for kernel launches, DMAs, and synchronization guards which we can think of as WAITs. Data dependencies must be carefully sequenced to ensure operation A finishes before operation B uses its results. XLA utilizes the misc slot to keep subsystems working while guarding against illegal instruction sequences. Operational latencies are known constants at compile time, and XLA can use those values to place WAIT instructions at exactly the right point in the VLIW stream to minimize stalls.</p><p><img alt="Simplified TPU Program" src="https://considerthebulldog.com/assets/tpu/TPU_Execution.svg"> <em>Simplified TPU Instruction Overlay</em></p><p>Subsystems operate with different latencies: scalar arithmetic might take single digit cycles, vector arithmetic 10s, and matrix multiplies 100s. DMAs, VMEM loads/stores, FIFO buffer fill/drain, etc. all must be coordinated with precise timing. The MXU might be busy executing a matrix multiply for 128 cycles, meanwhile the VPU is preparing the next tile of weights for the Result FIFO. While DMAs prepare new data for VMEM a DMA_OVERLAY instruction gets inserted to fetch new instructions for IMEM. When the MXU finishes a tile, the hardware sends a signal to clear the MXU_BUSY bit in the scalar unit’s sync registers. When the scalar unit evaluates a WAIT_MXU instruction it sees that the bit is unset and hops to the next instruction for decoding. The scalar unit JUMPs to the new VLIW bundle region and the program continues. Seamlessly overlapping the work of an arbitrary DAG requires extraordinary co-design between the device and the software.</p><p>Decoupling the hardware gave software the capacity to drive massive data and instruction level parallelism. VLIW slots can launch 8 operations per cycle. That is 2048 vector ALUs and two 128x128 systolic arrays with minimal control overhead. HBM, VMEM, Vregs, and the MXU all remain busy with the same pipelining and overlap philosophy from TPUv1, only now massively scaled up. XLA wrests power away from control and back into the arithmetic units with coordinated, deterministic execution. Determinism across devices requires explicit communication between chips.</p><p>ICI forms the backbone of the training pods. It creates a coherent communication fabric that lets chips operate locally while composing into a mesh of devices acting as one large core. Two on-chip ICI links route data between the HBM and VMEM of each core. Four 496Gbit/s bidirectional off-chip links connect a TPU to its neighbors in the rack with OSFP passive copper. RDMAs over this fabric let chips treat remote HBM as explicitly addressable endpoints. Racks arrange 256 chips as a 16x16 2D torus over ICI to form the full supercomputer pod. ICI removes frequent host communication, skipping the cost of network cards, switches, and communication delays. All this sacrifices 13% of the die area for gains in distributing computations <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>.</p><p><img alt="1D Torus" src="https://considerthebulldog.com/assets/tpu/1D_Torus.svg"> <em>One dimensional torus wraparound</em></p><p>Let’s imagine that we’re playing a game of telephone. You and 8 friends are arranged in a 3x3 grid, and you can only communicate with your adjacent neighbors. Your goal is to send a message from the person at (0,0) to the person at (2,2) in the fewest messages. Many paths achieve this, but the shortest one is always four. Now imagine that the people on the left edge of the grid can wrap messages around to people on the right edge of the grid. This is logically like mirroring you and all your friends over that wraparound axis. These 3 new connections make our shortest path 3 instead of 4.</p><p><img alt="2D Torus" src="https://considerthebulldog.com/assets/tpu/2D_Torus.svg"> <em>Logical mirroring in two dimensional torus wraparound, adapted from <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a></em></p><p>ICI plays this game of telephone in two dimensions. During backpropagation and optimizer state updates intermediate values accumulate across different partitions of the model located on different chips. Results must be broadcast to all the chips participating in the computation for synchronization. Whereas on-chip work is explicitly synchronized with hardware flags, work across chips is implicitly synchronized with MPI-style collectives (All-to-All, AllReduce, etc.). Torus topologies improve communication bandwidth and increase access to different communication patterns during synchronization.</p><p>32 wraparound links at 496Gbit/s enable 15.9Tbit/s of bisection bandwidth <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>, which tells us how much data can move through the network. In a 4x4 array, a cut down the middle would sever 4 connections. That same cut down the middle of a 2D torus severs 8 connections. Even if each connection carries the same amount of data, there are more paths for data to move through which helps reduce congestion. XLA absorbs the complexity of cross-device scheduling. Software can trust that RDMAs will reach their intended stacks of HBM traveling along the ICI interface.</p><p>The same DNA ostensibly runs through TPUv1, yet the chips look and feel utterly different. The microarchitecture, software, and networking each became independently sophisticated parts of a larger system. Subsystems decoupled from one another yet still composed neatly. Where TPUv1 tightly choreographed everything, TPUv2 divided components into independent, asynchronously operating units communicating through explicit queues and synchronization points. TPUv3 was a minor revision in comparison. It has two MXUs per core, an increased clock, double the HBM capacity with 30% higher bus speeds, higher ICI bandwidth, and scales up to a 1024 node liquid-cooled rack. The dies only increased 6% relative to TPUv2 because engineers learned how to better lay the chip out <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>. Scaling the system to meet the continued growth of neural networks pushed future designs into new territory.</p><h2 id="scaling-up">Scaling Up</h2><p>As the footprint of the system grew, so too did the complexity of operating it. Our focus up to now has emphasized chip-local comparisons, e.g. How expensive are these operations relative to one another? How does the memory hierarchy work? How do subsystems A and B communicate on-device? While the TPUs remain the atom of the supercomputer, as we zoom out we observe the crystalline structure of the racks and pods. The fourth generation TPU is better examined thinking about memory as one unified domain. Specialization forces care in the microarchitecture, but the questions change. Where are collectives slow? How are larger tensors handled? Can we scale the racks further? Viewing the world from low altitude we find that TPUv4’s design emphasizes system scaling and energy management.</p><p>Peeking behind the accounting curtain for a moment, they note that “most OpEx cost is for provisioning power and not for electricity use, so saving power already provisioned doesn’t improve TCO as much as one might hope” <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Total Cost of Ownership (TCO) tries to consider the all in cost of the pods. On the back of a napkin we break this out into CapEx (equipment, installation, etc.) and OpEx (personnel, maintenance, power, etc.). Initially CapEx might dominate ASIC design, but as the platform matures, thinking through operational requirements produces different sets of optimizations. The need for fast, power efficient devices remains but extends out into the unknowable future. As model demands increase, better economics need compositional scalability in an efficient power envelope.</p><p>A brief note: TPUv4 is the training design and TPUv4i is the inference design. The impetus was to keep training and inference chips nearly identical so that there weren’t two separate designs awkwardly diverging into separate projects <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. The relevant change is that the inference chip has one core while the training chip is dual-core.</p><p><img alt="4th Gen MXU" src="https://considerthebulldog.com/assets/tpu/4th_Gen_MXU.svg"> <em>Simplified Model of TPUv4 Systolic Execution</em></p><p>Fourth generation chips keep TPUv3’s MXU footprint, totaling 4 MXUs per core. In previous MXU designs partial sums moved downwards each cycle through a series of N two-input adders, where N is the size of the array, before reaching the output accumulators. TPUv4 batches groups of four products before passing them to custom 4-input adders. Batching products reduces the length of the adder chain from N to N/4, quartering the operational latency. Above we see 12 PEs bank four multiplies to reduce hops from 12 to 3. The specific implementation of these circuits isn’t clear from the paper, but this should provide enough motivation to understand the change. This circuit design decreases die area 40% and reduces peak power 12% <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>.</p><p><img alt="CMEM Speed Ups" src="https://considerthebulldog.com/assets/tpu/CMEM_SpeedUp.png"> <em>CMEM Speed Ups <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a></em></p><p>Accessing DRAM is still expensive, and inference workloads underutilize chips. TPUv4 adds 128MiB shared CMEM that is like an L3 cache but with the niceties of software-managed programmability. CMEM helps to keep all 4 MXUs busy with computations at the cost of 28% of the TPUv4 die area. On the 7nm process node, SRAM memory accesses are 20x more energy-efficient than DRAM accesses <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. CMEM’s memory bandwidth sits in between HBM and VMEM, but unlike HBM it can both read and write data. Expanding the memory hierarchy and keeping data closer to the arithmetic units allows XLA to cut out expensive trips to DRAM. During inference, prefetching model weights into SRAM for multi-tenancy drives higher utilization of chip resources that may otherwise be sitting idle. The ability to swap weights out from SRAM rather than DRAM makes paying the context switching cost feasible. All that die area and upfront CapEx gets amortized over the life of the chip in TCO so long as XLA can effectively leverage it.</p><details><summary>SparseCores</summary> <p><img alt="SparseCore Block Diagram" src="https://considerthebulldog.com/assets/tpu/SparseCore.png"> <em>SparseCore Block Diagram <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a></em></p> <p>Contrary to the prevailing LLMs-everywhere paradigm, ad serving and recommendation models (DLRMs) run the world. SparseCores (SC) are built to accelerate these models at the cost of 5% die area and power <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. The key features of these models are their usage of embeddings. Embeddings map data into enormous sparse matrices. Efficiently handling these sparse matrices requires clever strategies to shard tensors across devices and to make looking up the correct slice of data fast. Unstructured sparsity suffers massive memory traffic and imbalances between compute, communication, and data-dependent execution. The MXU is ill-suited to make progress on sparse workloads because they waste cycles on empty computations and don’t directly manage communication.</p> <p>SparseCores address this class of models with a “Sea of Cores” architecture designed to accelerate collectives and memory accesses <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. SCs are segmented into 16 individual compute elements (tiles) near DRAM that support multiple outstanding memory accesses <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. Tiles communicate over a data crossbar with one another and over the on-chip fabric to the rest of the device. A stream of CISC instructions enabling data-dependent communication gets issued by the processor’s core sequencer. The Fetch unit (8-wide SIMD vector processor, scVPU) reads data from HBM into 2.5MiB of sparse memory (Spmem), and the Flush Unit writes data out to HBM. Five on-board cross channel units (XPU) perform embedding specific operations. When embeddings are distributed across remote devices SCs leverage the existing ICI bandwidth to access remote memory. The dataflow looks as follows:</p> <ul><li>HBM DMA issued and read by Fetch Unit to Spmem</li><li>scVPU and XPUs operate on data</li><li>Flush unit writes data out to HBM (or remote HBM via RDMAs)</li></ul> <p>SCs alleviate the need for the MXU to handle computation and memory traffic on sparse data. They remove the CPU/DRAM bottleneck and shift sparse phases off the MXU path. The cores issue many RDMAs across the global address space of the TPUv4 pods, speeding up embeddings based models 30.1x versus CPUs <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. Dedicating a small amount of die area to the gather/scatter intensive DLRMs allows the device to be flexible and efficient under multiple algorithmic regimes.</p></details><p>Cores are getting crowded: MXUs, SparseCores, VPUs, HBM, and ICI routers. We see this component management pressure in the VLIW bundles. Driving the additional MXUs and CMEM required the VLIW bundle size to expand ~25% <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Adding new subsystems to the microarchitecture adds efficiencies that bubble up to system level performance, but lurking behind each of these changes is the specter of wiring. Fitting more efficient work onto the package with point-to-point connections became too great a tax. Training racks need to be close to one another in the datacenter to amortize the cost of cooling infrastructure, and this physical constraint forces the usage of optical fiber. ICI cabling in TPUv2/v3 coupled rack deployments so that a supercomputer couldn’t go into operation until the full pod was deployed <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. To realize the TCO and energy wins of the microarchitecture system scaling needed to decouple and compose.</p><p><img alt="TPUv4i Floorplan" src="https://considerthebulldog.com/assets/tpu/TPUv4i_Floor_Plan.png"> <em>TPUv4i Floorplan <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a></em></p><p>The ICI needed to breathe. Previous revisions of ICI handled both on-chip communication and off-chip communication. More wires needed to be routed to/from the ICI interface as the number of components grew. This circuit layout pressure was complemented by the equally frustrating reality that handling on-chip and off-chip communication increased contention for ICI bandwidth. TPUv4 separates these concerns by adding a dedicated on-chip interconnect (OCI) fabric. The OCI interface handles data movement on-chip so that ICI can solely route traffic across chips. Notice in the fourth generation floorplan how much die area is reserved for OCI <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Shorter wires run between components and OCI rather than point-to-point. The OCI interface acts as the mailman. The Scalar Unit drops a message off at the OCI to submit a DMA to DRAM, and the OCI routes it to the memory controller. It tucks subsystem communication behind a unified data exchange interface that shortens wire routes and opens a path to flexible scaling in future designs.</p><p>Arbitrating memory accesses between HBM, VMEM, IMEM, SMEM and now CMEM meant maintaining too many sets of independent lanes. OCI uses 512B-wide native data paths segmented into four, 128B-wide groups across the memory hierarchy. Each group serves a quarter of the total HBM bandwidth (153GB/s) so that independent transfers don’t serialize behind one another <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Transferring small IMEM overlays shouldn’t have to wait on the completion of a long-latency tensor DMA. This partitioning strategy gives software more flexibility when scheduling work across a device. The full HBM bandwidth is available to each group, but software can schedule multiple concurrent transfers instead of funneling everything through one contested path. XLA plans large transfers to CMEM, CMEM feeds the arithmetic units, OCI handles message passing, and ICI routes and manages RDMAs. OCI and CMEM jointly help to improve spatial locality and reduce trips to HBM.</p><details><summary>4D Tensor (R)DMAs</summary> <p>TPUv2/v3 used two-dimensional, relaxed order DMAs to stride along two axes when moving data. This forced XLA to decompose complex tensor reshapes into multiple DMA operations. TPUv4(i) uses four-dimensional DMAs that stride along three axes moving 512-byte chunks <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Operations that previously required multiple round-trips to memory now happen in a single DMA. The architecture distributes DMA engines throughout the chip rather than centralizing them. Each engine acts as a co-processor that can decode and execute tensor operations independently. The unified design works identically for on-chip transfers, cross-chip transfers, and host transfers. XLA inserts explicit synchronization, but in exchange it gets predictable performance and the freedom to schedule data movement aggressively. The compiler knows the latency and pipelines around it.</p></details><p>TPUv3 had already resorted to optical fiber across racks to enable the full 2D torus, but the 1024 node supercomputer could not expand its physical footprint. Rigid ICI wiring constraints meant individual racks couldn’t be used until each pod was deployed, and the system topology was fixed as configured unless a technician recabled the pod. Rack maintenance brought the whole pod offline with it. Optical Circuit Switching (OCS) infrastructure was the cure. Even though optical solutions are expensive, OCS optical components represent less than five percent of both system and power costs <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-10">[10]</a>. Centralizing cross-rack communications inserted massive programmability into the system. Substituting the cross-rack links with a programmable OCS provided massive gains in “scale, availability, utilization, modularity, deployment, security, power, and performance” <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>, unlocking a new scaling paradigm.</p><p><img alt="OCS Logical Diagram" src="https://considerthebulldog.com/assets/tpu/TPUv4_OCS.png"> <em>OCS Logical Diagram</em></p><p>Each rack in TPUv4 is a 4x4x4 cube, where this cube configuration is chosen to optimize all-to-all communications. Previous pod sizes (16x16 in v2, up to 128x32 in v3) were topology-limited. Devices could communicate between racks over ICI, but the system topology was statically programmed by the cabling. OCS removed these hard limits by centralizing cross-rack communication over an optical switching fiber. OCS offloads link establishment to an array of MEMS mirrors that dynamically configure links between devices in milliseconds <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. New system topologies can be programmed on the fly by software, placing workloads on idle, non-contiguous machines. Dynamically reconfiguring the OCS improves system availability, tolerating outages in 0.1% - 1.0% of the CPU hosts <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a>. TPUv4 pods scale up to 8x8 racks totaling a 4096 node cluster connected over OCS.</p><p>The OCSes isolate scaling complexity. Each rack contains 64 chips laid out logically as a cube. With 6 cube faces (+/- X/Y/Z), and 16 (4x4) chips per face, 96 optical links go to the OCS per rack. In the full 64 (8x8) rack pod, that is 6,144 uplinks to the OCS. This requires 48 OCSes that have 128 active ports to connect all the uplinks <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. Moving cross-rack interconnects to a dedicated optical panel at this scale enabled programmable topologies, eased deployment by decoupling racks, and allowed software to effectively use OCS as a “plugboard” to route around node and link failures.</p><details><summary>Mirror, Mirror on the Wall</summary> <p><img alt="MEMS Mirrors" src="https://considerthebulldog.com/assets/tpu/MEMS_Mirrors.png"> <em>MEMS Mirrors <a href="https://considerthebulldog.com/tte-tpu/#ref-10">[10]</a></em></p> <p>OCSes use micro-electro-mechanical systems (MEMS) mirrors that tilt in three dimensions to steer optical beams. Each OCS contains two arrays of 136 mirrors. Each mirror has four voltage-controlled actuators that rotate it along two axes, steering light from any input port to any output port with sub-degree accuracy. Rather than monitoring each of the 136 mirrors with a dedicated photodetector, OCS uses a single camera per array with an 850nm monitoring laser. Image processing algorithms optimize the high-voltage driver signals to minimize insertion loss across the entire array. Once positioned, each mirror draws 10s of milliwatts to maintain alignment <a href="https://considerthebulldog.com/tte-tpu/#ref-10">[10]</a>.</p> <p><img alt="Circulators" src="https://considerthebulldog.com/assets/tpu/OCS_Circulators.png"> <em>Circulators <a href="https://considerthebulldog.com/tte-tpu/#ref-10">[10]</a></em></p> <p>Circulators double the OCS’s effective capacity by enabling bidirectional communication. A circulator is a three-port optical device. Light entering port 1 exits port 2, light entering port 2 exits port 3. This cyclic property means a single fiber and a single OCS port can carry traffic in both directions simultaneously halving the required fiber count and OCS ports <a href="https://considerthebulldog.com/tte-tpu/#ref-10">[10]</a>.</p></details><p>Full connectivity of the OCS across the pods meant that the torus topologies of the previous generations could now add a third wraparound dimension. The distance between racks was no longer a constraint, and since the OCS can program chip-to-chip connections on the fly a path to new topologies emerged. Not only could the connections between racks wrap around the z-dimension, they could twist.</p><p><img alt="Example Twisted Tori" src="https://considerthebulldog.com/assets/tpu/Twisted_Torus.svg"> <em>Sample 1D Twisted Tori</em></p><p>We’ll make one modification to our previous wraparound topology diagram. Instead of wraparounds connecting only to the other side of their respective row/column, OCS programmability means that these connections can be offset. Adding twists to the wraparounds is an option not a requirement. Having the option to twist the network topology allows for new questions, e.g. given the communication pattern of this model, how should data be sent between participating chips? Twists make algorithmic experimentation and optimization two independently tractable targets and broadens the horizon of available efficiencies. Even without twisted topologies a third wraparound dimension adds bisection bandwidth to the network. The bisection bandwidth of 2D tori scales with the side length of the interconnects, N^(1/2). Adding the additional wraparound dimension scales bisection bandwidth with the area of the interconnects, N^(2/3). More paths in the topology shorten hops between participating nodes and alleviate system congestion along busy routes during synchronization. OCS better utilizes available devices and diversifies achievable topologies.</p><p>TPUv4(i) requires our thinking to broaden. We shouldn’t forget the impacts that microarchitecture improvements drive, but we need to consider the economics of the system holistically. Building warehouse scale solutions requires thinking about power provisioning, rack availability, interconnects, network topology, and accounting. Energy efficiency is still the overarching principle, but at datacenter scale. The message is simple: Target TCO over CapEx <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Adding CMEM is more expensive now but less expensive over time. Optical interconnects are expensive now but cost &lt;3% of the fully operational system <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. The duration of the design trade-offs became smeared into the future. All the same apparitions motivating TPUv1 go bump in the night, but they cast shorter shadows. TCO implies a system that requires operation, and the software that keeps the system available is an equal part of TPU’s development.</p><h2 id="island-hopping">Island Hopping</h2><p>Up to now we have enjoyed the quiet refuge of spreadsheet analysis, but the world is imperfect. Hardware dies, electricity spikes, and networks suffer congestion. The triumph of composing the system into decoupled, single responsibility units is not trivial, but infrastructure needs to serve real users. A cast of supporting software must keep chips available. Rock solid hardware relies on software to rationalize TCO obsession. The software is as much a part of the TPU story as the hardware.</p><p>We want to train a model. We decide which devices we need, pay rent, and start gawking at loss curves. When we submit our job for execution we don’t worry about the thousands of eager folks just like us. This mass of users vying for a fixed number of TPUs in sporadic intervals presents a problem. As the infrastructure provider what matters is that users don’t experience downtime. Components regularly fail and workloads are hard to predict. Once power has been provisioned every second of idle chip time or suboptimal workload allocation works against your best TCO approximations. Whether by underutilization or oversubscription, wasted resources are the enemy. Outer loop software that manages TPUs coordinates with XLA to find available nodes, check resource health, and configure ICI/OCS <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a>. XLA needs to know which TPUs the computation will run on as well as the requested network topology because device placement is part of the program. Optimizing the system for high availability means dealing with the constraints imposed by ahead of time scheduling.</p><p><img alt="TPU Resource Fragmentation" src="https://considerthebulldog.com/assets/tpu/TPU_Fragmentation.png"> <em>TPU Fragmentation <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a></em></p><p>Slices, Single Program Multiple Data (SPMD), and gang scheduling undergird TPU execution. Most workloads don’t consume an entire pod. Slices are declarations in code that allow developers to request an &lt;X,Y,Z&gt; device mesh which XLA uses to partition and shard models. This abstraction squirrels away both topology size and communication patterns. Pipeline parallelism may want a 2x2x1024 slice while data parallelism wants a 16x16x16 slice. The topology choice optimizes which communications are fast and which are slow. Mapping communications to a slice topology gives developers the freedom to experiment with parallelism strategies.</p><p>ICI coupling in TPUv3 meant the scheduler needed to find contiguous, healthy chips for workload placement. OCS lifted that restriction in TPUv4, but in both generations once a set of devices is selected the topology remains static for the duration of the program. A program owns the devices that it runs on until the program exits <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a>. Concurrent users submitting unknowable slice sizes makes assigning devices like Tetris. The scheduler must place new jobs onto devices as old jobs pop in and out of existence. It needs mechanisms to rebalance suboptimal device allocations.</p><p>A single executable distributed to each participating device runs an identical program. SPMD encapsulates this many devices, single program framework. Developers write models as if they are running on one giant device, and the complexity of managing device-level data placement disappears from view. XLA’s partitioner rewrites every operation in the model to work on local tensor shards, inserting an AllReduce where gradients need to sync, scattering data where it needs to spread, and gathering results where they need to combine <a href="https://considerthebulldog.com/tte-tpu/#ref-7">[7]</a>. The single logical program becomes thousands of coordinated physical programs each operating on its local slice of data. Control is synchronized explicitly on-device with VLIW barriers and implicitly between devices by collectives. Gang scheduled execution means that each device launches the program all at once, trading off runtime resilience for performance. When a fault crops up during execution the job must be checkpointed and relocated <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a>. The hardware stays simple, the software stays deterministic, but the orchestration layer must handle outages, link failures, and maintenance.</p><p><img alt="TPU Job Lifecycle" src="https://considerthebulldog.com/assets/tpu/TPU_Job_Lifecycle.png"> <em>TPU Job Lifecycle <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a></em></p><p>Software must anticipate failures to juggle pre-allocated workloads. In <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a> they note “To train a model, all TPU processes must be simultaneously up to synchronously update their weights via ICI collectives. A single failed, or interrupted process will interrupt the whole training process.” When a user submits a job, the cluster management client Borg queues it. If resources are fragmented or a job fails, Borg can preempt running workloads to shuffle them to different devices. When a job is ready to be scheduled, Borg selects a subset of devices and publishes an xconnect to the Pod Manager. The PM discovers pending xconnects and sends commands to the appropriate OCSes to connect the requested ICI channels. Once ICI connections stabilize, libtpunet configures the device’s ICI and programs its forwarding tables. XLA consumes the topology built by libtpunet to shard the model. Once execution begins, each device has its compiled program in local memory, knows its neighbors via ICI routing tables, and has its slice of the model weights in HBM. Thousands of devices execute in lockstep, synchronizing through collectives, without a single global runtime controller. The user does not see any of this background orchestration.</p><details><summary>Fault Tolerant Routing</summary> <p><img alt="ICI Interface" src="https://considerthebulldog.com/assets/tpu/ICI_Interface.png"> <em>ICI Interface <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a></em></p> <p>Packets hop through a path of ICI switches and optical fibers to arbitrary pairs of TPUs determined by libtpunet once during setup. xconnects initiate mirror configuration in the OCS, triggering on-chip device managers to initialize physical connections between ICIs. When libtpunet issues an ICI session start it clears and rightsizes the ICI buffers in the data layer for new RDMAs. Routing is handled by forwarding tables that provide a simple abstraction to locate destination TPUs. XLA emits sets of RDMA operations called transactions for collective communications. On-chip DMA engines read data from HBM and send the data to the ICI’s transaction layer to send over the network <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a>. All the required hardware for training drags down MTBF <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a>, so the system needs to be resilient to outages without bringing everything down.</p> <p><img alt="TPU Fault Tolerance" src="https://considerthebulldog.com/assets/tpu/TPU_Fault_Tolerance.png"> <em>TPU Fault Tolerance <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a></em></p> <p>The system manages faulty links with fault tolerant routing. An offline integer linear program simulates link outages and frames the route selection as a max flow problem, using an all-to-all collective as the canonical use case. The results from the ILP are cached and accessible by libtpunet. Fault tolerant routing uses Wild First Routing as its heuristic. Packets can take a wild hop around faulty links before reverting to fault free routing. Though using fault tolerant routing may induce network congestion, TPU availability benefits <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a>.</p></details><p>Getting the whole system to cooperate at scale needs clear boundaries and hand-offs. Borg, PM, and libtpunet bless the configuration of the workload before triggering execution. When TCO skews towards operation, getting these pieces right is as important as systolic arrays and memory hierarchies. But this presentation of how the software works is also subject to the constant evolution of the TPU. Cores communicate over OCI. Chips communicate over ICI. Racks connect remote ICI links over OCS. That leaves us with one final communication frontier: the datacenter network.</p><p><img alt="Mixture of Experts Routing" src="https://considerthebulldog.com/assets/tpu/MoE_Layer.png"> <em>Mixture of Experts Routing <a href="https://considerthebulldog.com/tte-tpu/#ref-38">[38]</a></em></p><p>SPMD assumes every device can communicate over ICI with predictable latency, which constrains developers to slice sizes that fit on a single pod. Islands of accelerators <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a> leave idle capacity stranded across pods, and under contention, jobs struggle to get the right-shaped device allocation. Individual pods also constrain algorithmic flexibility. Unlike traditional transformers, Mixture-of-Experts models include runtime data dependencies. The gating mechanism in MoEs introduces non-deterministic routing during execution. The SPMD model has to be stretched to express the fine-grained, data-dependent control flow these models need. If you want to shard experts across pods there is no natural way to do so. Without the DCN there is no dynamic routing, resource sharing, or use of idle chips across pods.</p><p>The datacenter network (DCN) connects islands using Google’s Jupiter fabric <a href="https://considerthebulldog.com/tte-tpu/#ref-9">[9]</a>. From the TPU’s point of view it is the communication that doesn’t occur over ICI. Extending the many cores, one logical system scaling approach gets complicated by varying latency and bandwidth characteristics. Two solutions emerged from these limitations. Multislice extends SPMD across pod boundaries. It is a conservative but compatible approach with existing code. Pathways abandoned synchronous execution for asynchronous dataflow. It is more complex but necessary for true heterogeneity.</p><p><img alt="Multislice over DCN logical Diagram" src="https://considerthebulldog.com/assets/tpu/MultiSlice.png"> <em>Logical diagram of Multislice over DCN <a href="https://considerthebulldog.com/tte-tpu/#ref-26">[26]</a></em></p><p>Multislice extends existing SPMD code across pod boundaries with minimal changes. Pod boundaries are treated as just another level in the communication hierarchy. SPMD still uses gang-scheduled execution, but XLA understands that some collectives happen over ICI and others happen over slower DCN. The familiar declarative slice syntax adds a parameter to select devices across islands. The compiler optimizes collective placement to minimize cross-pod traffic. Multislice expands the number of devices available for training by providing access to resources across pods <a href="https://considerthebulldog.com/tte-tpu/#ref-26">[26]</a>.</p><p><img alt="Pathways System Overview" src="https://considerthebulldog.com/assets/tpu/PW_System_Overview.png"> <em>Pathways System Overview <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a></em></p><p>Pathways is a plug-in replacement for JAX’s backend that virtualizes the datacenter <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a>. Instead of one giant SPMD program running in lockstep, it models execution as a DAG of compiled functions distributed across islands. Gang scheduling still happens within each island, but between islands coordination is asynchronous. There’s no single global runtime controller for the whole job. Mixture-of-Experts models can route activations dynamically to experts on different pods, and pipeline parallel stages can span multiple islands connected over DCN. Multiple programs can time-multiplex accelerators without context-switching overhead. Users request devices and the client compiles programs into a device-agnostic Pathways IR. XLA analyzes the program, the resource manager assigns physical TPUs, and the system inserts data movement operations between shards. Orchestration is complete by the time execution begins. Each device knows its program, its neighbors, and its slice of model weights.</p><p>Pathways uses a sharded dataflow model built on Plaque <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a>. Each node represents a compiled function executing across thousands of TPU shards. The system uses parallel asynchronous dispatch. Pathways pipelines host side work in parallel instead of waiting for computation A to finish before preparing computation B. A control-plane scheduler per island enforces gang scheduling across programs. Between islands, Pathways uses centralized dispatch to coordinate placement and data movement. Data moves directly between accelerators over ICI within islands and DCN between islands. Pathways matches multi-controller performance by front-loading coordination work, even though cross-island dispatch is mediated by the control plane rather than issued independently by each host. This execution model performs as well as JAX and lifts restrictions on algorithmic expressibility <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a>.</p><p>A dedicated upstart could reproduce the hardware design philosophy, but the software co-design makes the TPU a mammoth. Borg allocates resources and preempts jobs. The Pod Manager configures optical switches. libtpunet knows every ICI routing edge case and manages fault tolerance. XLA compiles with full knowledge of topology and latencies. SPMD partitions models while maintaining the illusion of one giant device. Multislice extends that illusion across pods. Pathways rethinks distributed execution and virtualizes the datacenter as one programmable pool. Schedulers, compilers, and coordination systems all play one long song. Building a TPU competitor needs generations of hard earned experience points. Each new design reconsiders which approaches were dead ends. Admitting you were wrong and doubling back is the game. Thinking about the TPU is thinking about Everything Else.</p><h2 id="ceci-n-est-pas-une-tpu">Ceci n’est pas une TPU</h2><p>After TPUv4 the well of detailed microarchitecture papers runs dry. You can still find information scattered across the internet, but not in the same succinct, curated way. Maybe more papers will be released publicly and we’ll be able to study these designs in greater detail, but until then we have to cobble together an understanding of our own. TPUv4 and v4i are followed by TPUv5p (performance) and v5e (efficiency), Trillium (v6e), and Ironwood (v7). We know that the inference (e) optimized designs retain a single-core architecture and use 2D tori instead of 3D tori. We know the interconnect and HBM performance numbers for the fifth, sixth, and seventh generation chips. We know that Trillium and Ironwood revert to 256x256 systolic arrays. We know that Ironwood scales up to 9,216 chips for training and 256 for inference with 1.77PB HBM that delivers 42.5 FP8 ExaFlops (6x Perf/W improvement over TPUv4) with a chiplet design for next generation reasoning and MoE workloads <a href="https://considerthebulldog.com/tte-tpu/#ref-16">[16]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-20">[20]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-21">[21]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-23">[23]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-24">[24]</a>.</p><p>And I know that all of this fails to capture the totality of the enhancements since TPUv4. But a spec sheet like the one <a href="https://www.nextplatform.com/2025/04/09/with-ironwood-tpu-google-pushes-the-ai-accelerator-to-the-floor/">here</a> or a primer like the one <a href="https://jax-ml.github.io/scaling-book/tpus/">here</a> could have told us that. The subsequent papers have focused on the system, but discussions of the system hide the simple origins of the device behind a hodgepodge of specs and new thundering heights. The essence of the thing becomes a folklorish amalgam of TPU lore. Myths are about meaning. Moore’s Law was never free in the literal sense. It required diligent engineering and enduring frustration, but decade after decade the compounding continued. The idea of Moore’s Law cast a spell that actualized its reality.</p><p>By nature the TPU is what it is not. The thrust and posturing of papers, talks, slides, and internet chatter focus on the technical minutiae, but the seams that hold this constellation of facts and figures together are the ordinary and the human. They are long emails and oscilloscopes in equal measure. How many of these choices go unseen? Hand-wringing about the system internals helps us to glimpse the creative act, but we mistake the painting for the paint chemistry. In this new world where nothing is free, every decision comes at an intentional, excruciating cost. The weight of the space of possibilities grows heavier knowing that each decision may foreclose another. Each choice is an act of reinvention in the face of a future that folds onto itself.</p><p>The TPU is an artifact born out of the quiet solace of steady hands doing careful engineering. AI DSAs are unlikely to be self-fulfilling in the same infinite feeling way as Moore’s Law. They will be five hundred ordinary decisions that compose into something greater. Can we make it smaller? Can we make it bigger? Can we make it easier to use? When we skim specs like the ones strewn above we notice the changes and feel the weight of what they represent. As new pressures get applied new entities emerge. For a moment we sense each decision branching into some unknown. Our new AI-obsessed world brings with it the demands of new ways of thinking. It is a reminder that the future is always at hand, and that if we participate in the myth-making we find that there are dragons after all.</p><hr><h2 id="references">References:</h2><p><a id="ref-1" target="_blank"></a>[1]: <a href="https://arxiv.org/ftp/arxiv/papers/1704/1704.04760.pdf">In-Datacenter Performance Analysis of a Tensor Processing Unit​</a></p><p><a id="ref-2"></a>[2]: <a href="https://gwern.net/doc/ai/scaling/hardware/2021-norrie.pdf">The Design Process for Google’s Training Chips: TPUv2 and TPUv3</a></p><p><a id="ref-3"></a>[3]: <a href="https://dl.acm.org/doi/pdf/10.1145/3360307">A Domain-Specific Supercomputer for Training Deep Neural Networks</a></p><p><a id="ref-4"></a>[4]: <a href="https://arxiv.org/pdf/2304.01433">TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings</a></p><p><a id="ref-5"></a>[5]: <a href="https://gwern.net/doc/ai/scaling/hardware/2021-jouppi.pdf">Ten Lessons From Three Generations Shaped Google’s TPUv4i</a></p><p><a id="ref-6"></a>[6]: <a href="https://www.usenix.org/system/files/nsdi24spring_prepub_zu.pdf">Resiliency at Scale: Managing Google’s TPUv4 Machine Learning Supercomputer</a></p><p><a id="ref-7"></a>[7]: <a href="https://arxiv.org/pdf/2105.04663">GSPMD: General and Scalable Parallelization for ML Computation Graphs</a></p><p><a id="ref-8"></a>[8]: <a href="https://arxiv.org/pdf/2203.12533">PATHWAYS: ASYNCHRONOUS DISTRIBUTED DATAFLOW FOR ML</a></p><p><a id="ref-9"></a>[9]: <a href="https://dl.acm.org/doi/pdf/10.1145/3544216.3544265">Jupiter Evolving: Transforming Google’s Datacenter Network via Optical Circuit Switches and Software-Defined Networking</a></p><p><a id="ref-10"></a>[10]: <a href="https://arxiv.org/pdf/2208.10041">Mission Apollo: Landing Optical Circuit Switching at Datacenter Scale</a></p><p><a id="ref-11"></a>[11]: <a href="https://gwern.net/doc/cs/hardware/2014-horowitz-2.pdf">Computing’s Energy Problem</a></p><p><a id="ref-12"></a>[12]: <a href="https://dl.acm.org/doi/pdf/10.1145/3361682">Domain-Specific Hardware Accelerators</a></p><p><a id="ref-13"></a>[13]: <a href="https://parallel.princeton.edu/papers/wall-hpca19.pdf">The Accelerator Wall: Limits of Chip Specialization</a></p><p><a id="ref-14"></a>[14]: <a href="https://arxiv.org/pdf/1911.05289">The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design</a></p><p><a id="ref-15"></a>[15]: <a href="https://fleetwood.dev/posts/domain-specific-architectures">Domain specific architectures for AI inference</a></p><p><a id="ref-16"></a>[16]: <a href="https://jax-ml.github.io/scaling-book/tpus/">How to Think About TPUs – Chapter 2</a></p><p><a id="ref-17"></a>[17]: <a href="https://henryhmko.github.io/posts/tpu/tpu.html">TPU Deep Dive</a></p><p><a id="ref-18"></a>[18]: <a href="https://www.telesens.co/2018/07/30/systolic-architectures/">Understanding Matrix Multiplication on a Weight-Stationary Systolic Architecture</a></p><p><a id="ref-19"></a>[19]: <a href="https://www.nextplatform.com/2017/04/05/first-depth-look-googles-tpu-architecture/">First in-depth look at Google’s TPU Architecture</a></p><p><a id="ref-20"></a>[20]: <a href="https://www.nextplatform.com/2025/04/09/with-ironwood-tpu-google-pushes-the-ai-accelerator-to-the-floor/">WITH “IRONWOOD” TPU, GOOGLE PUSHES THE AI ACCELERATOR TO THE FLOOR</a></p><p><a id="ref-21"></a>[21]: <a href="https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/">Ironwood: The first Google TPU for the age of inference</a></p><p><a id="ref-22"></a>[22]: <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">TPU Architecture – Google Documentation</a></p><p><a id="ref-23"></a>[23]: <a href="https://www.servethehome.com/google-ironwood-tpu-swings-for-reasoning-model-leadership-at-hot-chips-2025/">Google Ironwood TPU Swings for Reasoning Model Leadership at Hot Chips 2025</a></p><p><a id="ref-24"></a>[24]: <a href="https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus">Announcing Trillium, the sixth generation of Google Cloud TPU</a></p><p><a id="ref-25"></a>[25]: <a href="https://openxla.org/xla/sparsecore">A deep dive into SparseCore for Large Embedding Models</a></p><p><a id="ref-26"></a>[26]: <a href="https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads">How to scale AI training to up to tens of thousands of Cloud TPU chips with Multislice</a></p><p><a id="ref-27"></a>[27]: <a href="https://hc2023.hotchips.org/assets/program/conference/day2/ML%20training/HC2023.Session5.ML_Training.Google.Norm_Jouppi.Andy_Swing.Final_2023-08-25.pdf">A Machine Learning Supercomputer With An Optically Reconfigurable Interconnect and Embeddings Support – HotChips Slides</a></p><p><a id="ref-28"></a>[28]: <a href="https://hc33.hotchips.org/assets/program/tutorials/HC2021.Google.Sameer%20Kumar.pdf">Challenges in large scale training of Giant Transformers on Google TPU machines – HotChips Slides</a></p><p><a id="ref-29"></a>[29]: <a href="https://hc32.hotchips.org/assets/program/tutorials/HC2020.Google.SameerKumarDehaoChen.v02.pdf">Exploring Limits of ML Training on Google TPUs – HotChips Slides</a></p><p><a id="ref-30"></a>[30]: <a href="https://old.hotchips.org/hc31/HC31_T3_Cloud_TPU_Codesign.pdf">Cloud TPU: Codesigning Architecture and Infrastructure – HotChips Slides</a></p><p><a id="ref-31"></a>[31]: <a href="https://pages.cs.wisc.edu/~markhill/seminar2020/jouppi2020_10_tpu-v2-v3.pdf">A DOMAIN-SPECIFIC TPU SUPERCOMPUTER FOR TRAINING DEEP NEURAL NETWORKS – Slides</a></p><p><a id="ref-32"></a>[32]: <a href="https://www.hc32.hotchips.org/assets/program/conference/day2/HotChips2020_ML_Training_Google_Norrie_Patil.v01.pdf">Google’s Training Chips Revealed: TPUv2 and TPUv3 – Slides</a></p><p><a id="ref-33"></a>[33]: <a href="https://chips-compilers-mlsys-22.github.io/assets/slides/10%20Lessons%204%20TPU%20gens%20%2B%20CO2e%2045%20minutes.pdf">A Decade of Machine Learning Accelerators: Lessons Learned and Carbon Footprint – MLSys Slides</a></p><p><a id="ref-34"></a>[34]: <a href="https://tnm.engin.umich.edu/wp-content/uploads/sites/353/2020/08/2020.6.sparse-tpu_ics2020.pdf">Sparse-TPU: Adapting Systolic Arrays for Sparse Matrices</a></p><p><a id="ref-35"></a>[35]: <a href="https://www.eecs.harvard.edu/htk/static/files/1978-cmu-cs-report-kung-leiserson.pdf">Systolic Array For VLSi</a></p><p><a id="ref-36"></a>[36]: <a href="https://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf">Why Systolic Architectures?</a></p><p><a id="ref-37"></a>[37]: <a href="https://dl.acm.org/doi/pdf/10.5555/800052.801897">Doubly Twisted Torus Networks for VLSI Processor Arrays</a></p><p><a id="ref-38"></a>[38]: <a href="https://huggingface.co/blog/moe">Mixture of Experts Explained</a></p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autism's confusing cousins (227 pts)]]></title>
            <link>https://www.psychiatrymargins.com/p/autisms-confusing-cousins</link>
            <guid>46172443</guid>
            <pubDate>Sat, 06 Dec 2025 11:18:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psychiatrymargins.com/p/autisms-confusing-cousins">https://www.psychiatrymargins.com/p/autisms-confusing-cousins</a>, See on <a href="https://news.ycombinator.com/item?id=46172443">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!gvh6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!gvh6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 424w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 848w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!gvh6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg" width="1152" height="384" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:384,&quot;width&quot;:1152,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37942,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.psychiatrymargins.com/i/180764157?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!gvh6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 424w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 848w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><blockquote><p>“I think that these days what we mean by “autism” is basically “weird person disease.””</p></blockquote><p><strong>Sorbie Richner</strong><span>, </span><a href="https://www.psychiatrymargins.com/p/rich-girl-rehab" rel="">Rich Girl Rehab</a></p><blockquote><p>“Accurate diagnosis requires consideration of multiple diagnoses. Sometimes, different diagnoses can overlap with one another and can only be differentiated in subtle and nuanced ways, but particular diagnoses vary considerably in levels of public awareness. As such, an individual may meet the diagnostic criteria for one diagnosis but self-diagnoses with a different diagnosis because it is better known.”</p></blockquote><p><strong>Sam Fellowes</strong><span>, </span><a href="https://www.cambridge.org/core/journals/royal-institute-of-philosophy-supplements/article/abs/selfdiagnosis-in-psychiatry-and-the-distribution-of-social-resources/91A981A3908EE250DE19CF597277F197" rel="">Self-Diagnosis in Psychiatry and the Distribution of Social Resources</a></p><p>Unsurprisingly, these days I meet many people in the psychiatric clinic who are convinced that they have autism, or suspect (with various degrees of confidence) that they have autism, or report being diagnosed with autism at some point in their lives by some clinician. And for a fair number of such individuals, I cannot say with reasonable certitude that they have autism. The reasons they give for considering autism vary widely, but tend to be along the lines of…</p><ul><li><p>“Eye contact makes me very uncomfortable.”</p></li><li><p>“I suck at small talk.”</p></li><li><p>“I have rigid routines.”</p></li><li><p>“I hyper-focus on my hobbies.”</p></li><li><p>“I am always fidgeting.”</p></li><li><p>“Social interaction exhausts me.”</p></li><li><p>“I really bad at making friends.”</p></li><li><p>“I don’t fit in; people find me weird.”</p></li></ul><p>What’s interesting about many of the items above is that the number one diagnostic possibility in my mind is an anxiety disorder of some sort. I remember seeing a woman who was a classic example of someone with high neuroticism, poor self-esteem, and severe social anxiety, and she had believed for much of her life that she was autistic because some random doctor somewhere at some point (she couldn’t even remember who or what sort of assessment this involved) had told her that she had autism, and she believed it because it fit in with her experience of being awkward-shy-weird.</p><p>It is common for me to meet individuals who think they have autism and find myself thinking, “schizoid,” “obsessive compulsive,” “cluster B,” “social anxiety,” “generalized anxiety,” “trauma,” “socially awkward,”… None of these, however, have the mimetic virality of autism.</p><p>I don’t want to come across as being skeptical of the reality of autism as a diagnosis or as asserting that most people are misdiagnosed. Autism exists, to the extent that any psychiatric disorder exists. Not everyone is misdiagnosed, perhaps even most people.  I am not trying to say, “autism is bullshit.” It’s not. I offer the diagnosis of autism as a clinician perhaps as often as I find myself doubting it.</p><p><span>What intrigues me is that people are drawn to autism as a diagnosis because it seems to offer recognition of something they’ve lived with: they may be deeply awkward, terribly shy, or bad with people, they may struggle with social interactions, they may find other people annoying, other people may find them weird, they may have a hard time connecting to others, they may have been bullied, and they may have directed their loneliness or introversion towards peculiar interests or hobbies. Autism seems to them to capture all that. It seems like an apt and appealing narrative. But autism may also be the only relevant diagnosis they’ve heard of or are familiar with. They haven’t seen any cool TikToks about being schizoid. No one’s offering them quizzes about being schizotypal. A random pediatrician or primary care doc is not going to tell them they have an obsessive-compulsive style of personality. So when some professional doubts that they have “autism,” they see it as a dismissal or rejection of their “lived experience.” </span><em>Of course, I am weird-anxious-awkward. How can you say otherwise?</em><span> What they don’t know is that the choice is not between autism or nothing, but rather between autism and about a dozen other diagnostic possibilities.</span></p><p>So for the sake of our collective sanity, let’s consider a few of them…</p><p><span>To be diagnosed with autism spectrum disorder according to DSM-5, a person must have ongoing </span><strong>difficulties in social communication and interaction</strong><span> in all three areas: trouble with back-and-forth social connection, problems with nonverbal communication like eye contact and body language, and difficulty making or keeping friendships. They also must show at least two types of </span><strong>repetitive or restricted behaviors</strong><span>, such as repetitive movements or phrases, needing things to stay the same, having very intense focused interests, or being unusually sensitive (or under-sensitive) to things like sounds, textures, or lights. These patterns must have been </span><strong>present since early childhood</strong><span> (even if they weren’t noticed until later when life got more complicated), lead to substantial </span><strong>impairment in functioning</strong><span>, and can’t simply be explained by intellectual disability (or other psychiatric disorders).</span></p><p>To “have” autism is simply to demonstrate this cluster of characteristics at the requisite level of severity and pervasiveness. It doesn’t mean that the person has a specific type of brain attribute or a specific set of genes that differentiates them from non-autistics. No such internal essence exists for the notion as currently conceptualized.</p><p><span>Autism spectrum is wide enough to have very different prototypes within it. On one end we have profound autism, representing someone with severe autistic traits who is completely dependent on others for care and has substantial intellectual disability or very limited language ability. At the other end, we have successful nerdy individuals with autistic traits and superior intelligence, often seen in science or academia, à la Sheldon Cooper. (Holden Thorp, editor-in-chief of the </span><em>Science</em><span> journals and former UNC chancellor, for example, has publicly disclosed his own autism diagnosis.) This wide range is confusing enough on its own, even without considering other conditions that can present with autism-like features.</span></p><p><span>Autism cannot be identified via medical “tests.” It is identified via clinical information in the form of history, observation, and interaction, and the less information available or the more unreliable the information provided is, the more uncertain we’ll be. To </span><em>have</em><span> autism is basically a judgment call that one is a good match to a descriptive prototype. We can get this judgment wrong, and we sometimes do get it wrong. (There is nothing wrong with this fallibility as such, as long as we recognize it. Lives have been built on foundations less sturdy.)</span></p><p><span>Autism as a category or identity has taken on a life of its own. I am aware that not everyone in the neurodiversity crowd accepts the legitimacy of clinician judgments or clinical criteria as outlined in the diagnostic manuals, such as the DSM and ICD. There are other ways to ground the legitimacy of self-diagnoses, </span><a href="https://www.tandfonline.com/doi/full/10.1080/09515089.2024.2327823" rel="">in theoretically virtuous accounts or pragmatic uses</a><span>, which require distinct considerations of their own; I don’t reject that. But here, I am concerned with autism as a clinical diagnosis and the accuracy of autism understood in terms of alignment with clinical diagnosis. Would competent and knowledgeable clinicians with access to all relevant clinical information concur that the person’s presentation meets diagnostic criteria for autism? If you don’t really care about that, this post is not for you.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!NXb7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!NXb7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 424w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 848w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!NXb7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg" width="720" height="472" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:472,&quot;width&quot;:720,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:97251,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.psychiatrymargins.com/i/180764157?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!NXb7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 424w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 848w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Lascaux Cave</figcaption></figure></div><p>Schizoid personality describes people who have little desire for close relationships and prefer solitary activities. Unlike people who are simply shy or socially anxious, individuals with schizoid personality style genuinely don’t find relationships rewarding or necessary. They typically appear emotionally detached or cold, show restricted emotional expression, seem indifferent to praise or criticism, and have few if any close friends or confidants. They often live quietly on the margins of society, pursuing solitary interests or jobs. They keep their inner worlds (which can be quite rich) private and don’t seek emotional intimacy with others.</p><p>In autism, social difficulties stem from genuine challenges with processing social information: difficulty reading facial expressions, understanding implied meanings, picking up on social cues, knowing unwritten social rules, etc. In schizoid personality, the person typically understands social conventions but simply isn’t motivated to engage with them. They withdraw from genuine disinterest. Schizoid personality also lacks the additional features of autism (repetitive or restricted behaviors, various sensory sensitivities).</p><p>Schizotypal personality describes people who have odd or eccentric beliefs, unusual perceptual experiences, and difficulties with close relationships. Unlike schizoid personality (which involves simple disinterest in relationships), schizotypal includes strange ways of thinking and perceiving the world. People with schizotypal personality might believe in telepathy, feel they have special powers, think random events have special meaning for them personally, or have unusual perceptual experiences (like feeling a presence in the room or hearing whispers). They typically have few close friends, experience social anxiety that doesn’t improve with familiarity, and may appear paranoid or suspicious of others’ motives. Both schizotypal personality and autism can involve social difficulties and odd or eccentric behavior, but in schizotypal personality, the peculiarity comes from magical thinking, paranoid ideas, and perceptual distortions.</p><p>Obsessive-compulsive personality describes people who are preoccupied with orderliness, perfectionism, and control. These individuals are rigid rule-followers who want things to be done “the right way,” have difficulty delegating tasks, and get caught up in details and lists to the point where they lose sight of the main goal. They tend to be workaholics who neglect leisure and friendships, are inflexible about matters of morality or ethics, and are often stubborn and controlling. Both obsessive-compulsive personality and autism can involve rigid adherence to routines, rules, and specific ways of doing things. In obsessive-compulsive personality, the inflexibility comes from anxiety about loss of control. The person is trying to, consciously or unconsciously, manage anxiety through control and perfectionism. In autism, the need for sameness and routine serves different functions. It provides predictability in a world that feels confusing or it helps with sensory regulation rather than anxiety-driven perfectionism.</p><p>Severe social anxiety is an intense, persistent fear of social situations where a person might be judged, embarrassed, or humiliated. Social anxiety disorder involves overwhelming fear that interferes with daily life. People with this condition worry excessively about saying something stupid, looking foolish, or being rejected. They often avoid social situations entirely, which can lead to isolation, difficulty maintaining employment, and problems forming relationships. Both social anxiety and autism involve social difficulties and withdrawal. Social anxiety usually improves significantly in comfortable, safe environments (like with close family or friends), while autistic social differences tend to be more consistent across all contexts.</p><p>Borderline personality disorder involves intense emotional instability, unstable relationships, fear of abandonment, and a shifting sense of self, with people experiencing rapid mood swings and chaotic relationships that alternate between idealization and devaluation of others. While it can resemble autism through social difficulties, emotional dysregulation, rigid thinking, and feeling different from others, the key distinctions are that borderline centers on intense relationship preoccupations and emotional chaos, whereas autism involves genuine difficulty understanding social cues and communication; borderline features rapidly shifting identity and relationship-triggered mood swings, while autism includes stable self-concept, sensory sensitivities, restricted interests, and literal communication that aren’t present in borderline; and borderline symptoms fluctuate dramatically with relationship stability while autistic traits remain consistent across contexts.</p><p>Social communication disorder is a condition in DSM-5 where someone has significant, ongoing difficulty using verbal and nonverbal communication appropriately in social contexts. People with social communication disorder struggle with the “pragmatic” aspects of language, that is, knowing how to use language effectively in social situations. They may have trouble understanding when to take turns in conversation, knowing how much detail to give, adjusting their speaking style for different situations, understanding implied meanings or hints, picking up on nonverbal cues like body language and facial expressions, and knowing how to start, maintain, or end conversations naturally. This makes forming friendships and relationships difficult and affects life functioning. The social communication problems in social communication disorder look nearly identical to the “Criterion A” features of autism. However, unlike autism, people with social communication disorder don’t show repetitive behaviors, restricted interests, sensory sensitivities, or the need for sameness and routine.</p><p>Social communication disorder is rarely diagnosed in favor of autism primarily because autism provides access to critical services, insurance coverage, educational support, and legal protections that social communication disorder does not reliably offer, creating strong practical incentives for families and clinicians to prefer the autism diagnosis. Additionally, autism has an established evidence base, validated assessment tools, clear intervention protocols, and a large supportive community with a neurodiversity-affirming culture, while social communication disorder has none of these. It has no community, minimal research, no specific treatments, and little professional awareness since it was only introduced in the DSM in 2013. Service delivery, insurance, and educational systems are built entirely around autism rather than social communication disorder, and since both conditions require similar interventions for social-communication difficulties, there’s little practical incentive to make the diagnostic distinction, especially when the boundary between them (whether restricted/repetitive behaviors are truly absent or just subtle) is often unclear and clinicians are often unsure the distinction really matters.</p><p>Trauma-related disorders, particularly from early developmental trauma, severe neglect, or disrupted attachment, can mimic autism through social withdrawal and avoidance of eye contact (defensive protection rather than social processing difficulties), communication delays and difficulties (from lack of language exposure or trauma’s impact on brain development), emotional dysregulation and meltdowns (from emotional dysregulation rather than sensory overload), repetitive self-soothing behaviors (anxiety management rather than stimming), sensory sensitivities (hypervigilance rather than sensory processing differences), and rigid need for routine (anxiety-driven safety-seeking rather than cognitive processing style). </p><p>Severe early deprivation can create “quasi-autistic” patterns that can be genuinely difficult to distinguish. The critical distinctions are that trauma-related difficulties typically improve significantly in safe, nurturing environments and with adequate psychological treatment, show more variability across contexts (worse with triggers), are tied to identifiable adverse experiences rather than present from earliest infancy, and lack the restricted interests and genuine social communication processing deficits of autism.</p><p>Social awkwardness refers to social ineptness without meaningful impairment that falls within what is considered normal or typical human variation. This can be mistaken for autism because both may involve limited friendships, preference for solitude, conversation difficulties, reduced eye contact, and intense interests, particularly fueled by online self-diagnosis culture and broad autism awareness. The key distinctions are that socially awkward individuals understand what they should do socially but find it difficult or uninteresting (versus genuinely not understanding unwritten rules), show significant improvement with practice and maturity, are more comfortable in specific contexts, lack the sensory sensitivities and restricted/repetitive behaviors required for autism diagnosis, and generally achieve life goals despite awkwardness rather than experiencing clinically significant impairment.</p><p>Selective Mutism, Intellectual Disability (without autism), Stereotypic Movement Disorder, Attention-Deficit/Hyperactivity Disorder (ADHD), Schizophrenia Spectrum Disorders, Avoidant Personality Disorder, Attachment Disorders, Generalized Anxiety Disorder, Obsessive-Compulsive Disorder, and Rett Syndrome (a characteristic pattern of developmental regression after initial normal development, typically 6-18 months).</p><p>Comorbidity is possible and expected. Someone can be autistic and have maladaptive personality patterns, trauma histories, or anxiety disorders that complicate the presentation. Developmental context, response to relationships, and subjective experiences are all very important in looking beyond the surface presentation to understanding the meaning and functions of behaviors.</p><p><em>See also:</em></p><div data-component-name="DigestPostEmbed"><a href="https://www.psychiatrymargins.com/p/it-is-not-ludicrous-for-mildly-and" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!m4UH!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b0ee8e-8905-4305-bd23-9a32a74a0fc0_736x414.jpeg"><img src="https://substackcdn.com/image/fetch/$s_!m4UH!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b0ee8e-8905-4305-bd23-9a32a74a0fc0_736x414.jpeg" sizes="100vw" alt="It is not ludicrous for mildly and severely impaired to have the same diagnosis" width="140" height="140"></picture></div></a></div><div data-component-name="DigestPostEmbed"><a href="https://www.psychiatrymargins.com/p/the-overdiagnosis-confusion" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!pyp8!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb66d64d-46e4-4555-b163-1871fe295d11_742x550.jpeg"><img src="https://substackcdn.com/image/fetch/$s_!pyp8!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb66d64d-46e4-4555-b163-1871fe295d11_742x550.jpeg" sizes="100vw" alt="The “Overdiagnosis” Confusion" width="140" height="140"></picture></div></a></div><p data-attrs="{&quot;url&quot;:&quot;https://www.psychiatrymargins.com/p/autisms-confusing-cousins?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.psychiatrymargins.com/p/autisms-confusing-cousins?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux Instal Fest Belgrade (155 pts)]]></title>
            <link>https://dmz.rs/lif2025_en</link>
            <guid>46172167</guid>
            <pubDate>Sat, 06 Dec 2025 10:20:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dmz.rs/lif2025_en">https://dmz.rs/lif2025_en</a>, See on <a href="https://news.ycombinator.com/item?id=46172167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

        <h2>Where and when</h2>

        <p>Linux Install Fest will be held on December 9, 2025 in the JAG3 classroom of the Faculty of Mathematics, at
            <a href="https://www.openstreetmap.org/node/3807078606">Jagićeva 5, Belgrade</a>. Entry to the classroom is possible from 6 pm to 9 pm.</p>

        <p>Jagićeva street is located between the  <a href="https://www.openstreetmap.org/node/6670711291"><em>Pijaca
                    Đeram</em></a> station where trams 5, 6, 7L and 14 stop, and the <a href="https://www.openstreetmap.org/node/1693535022"><em>Crveni krst</em></a> station where buses 21 and 83 stop, as well as trolleybuses 19, 22 and 29.</p>

        <h2>Program schedule</h2>

        <p>The goal of the gathering is to help interested install the Linux operating system on laptops. Several people with working Linux experience will be present at the event. In addition, depending on the interest of those present, short trainings related to the command line, git, web services, C programming, etc. can be held.</p>

        <p>After 9 p.m., we can continue socializing in one of the nearby bars.</p>

        <h2>Linux distributions</h2>

        <p>Linux is the core of the operating system, on which other programs are installed. All of these together make up a particular <em>Linux distribution</em>. There are many distributions, but we recommend the ones with a long tradition like the following:
        </p>

        <ul>
            <li><strong>The Debian</strong> distribution is probably the most suitable for Linux beginners. Known derivatives of Debian are Ubuntu, Mint and Zorin.</li>
            <li><strong>Fedora</strong> is also suitable for Linux beginners. It differs from the Debian distribution by the faster release of new versions, which in practice means that users have newer versions of the program.</li>
            <li><strong>Arch</strong> is a Linux distribution that allows the user to easily configure all parts of the system. This distribution is intended for people with significant Linux experience.</li>
        </ul>

        <p>If you are a beginner and haven't decided which distribution you want to install, we recommend Fedora or Debian. Regardless of which distribution you have, you will be able to run all programs intended for Linux.</p>

        <h2>End of 10</h2>

        <p>This year's Linux Install Fest is organized as part of the global <a href="https://endof10.org/">End of 10</a>
            campaign, which promotes the Linux operating system as a replacement for Windows 10.</p>

        <p>For a long time now, the Windows operating system has become increasingly unfriendly to users. On the contrary, many Linux distributions have improved the user experience to the maximum, and today we can claim that Linux enables significantly more pleasant work, regardless of the user's technical knowledge.</p>

        <p>Windows imposes on users functionalities that users do not want to use, such as: cloud integrations, AI, advertisements, mandatory accounts, and the like. These functionalities serve above all to increase Microsoft's profits, and have no benefit for most end users. Also, basic programs such as calendars, calculators or text editors have become slow and full of bugs. With useless functionalities, Windows becomes more demanding every year and requires the purchase of better hardware, leading to an increase in electronic waste. Unlike Windows, the latest Linux distributions work very well on computers that are more than a decade old.</p>

        <p>The choice of an operating system is no longer just a technical decision, but also an environmental attitude.</p>

        <h2>Installation methods</h2>

        <p>We can install Linux in three ways:</p>

        <ol>
            <li><strong>Inside a virtual machine on Windows.</strong> In this way, the user retains his existing operating system and the data on it. Linux in a virtual machine will be significantly slower than an installation without virtualization.
            </li>
            <li><strong>In addition to the existing operating system.</strong> If it is possible to shrink one of your partitions and free up at least 10GB of space, you can install a Linux operating system in addition to Windows. When booting the computer, the user will be able to choose whether to boot Windows or Linux. With such an installation, there is a certain risk that one of the subsequent Windows updates will reset the bootloader settings, after which a small intervention is required to make the Linux system accessible again.</li>
            <li><strong>By completely removing the Windows system.</strong> In place of the Windows partition, a new partition with the Linux distribution will be placed. Additional partitions that exist may or may not be removed.</li>
        </ol>

        <h2>Before arrival</h2>

        <p>In order for the installation to be effective, before coming to the Linux Instal Fest, it is necessary to make a backup of the data from the system partition if you decide on the second or third installation option. If you have two partitions (for example, C and D), move the data from the system partition (C:) that you want to keep to the non-system partition (D:). If you don't have an additional partition, you can use a USB flash drive. Pay attention to the files inside the user directory (Desktop, Downloads, Documents,... ), and export bookmarks and passwords from the browser.</p>

        <p>Also, before your arrival, you can familiarize yourself with the appearance and way of functioning of various Linux distributions. You can try some Linux distributions through the browser, without any installation, on the
        <a href="https://distrosea.com/">DistroSea</a> website (sometimes it is necessary to wait a short time to free up resources on the site). Please note that the operating system on this site is many times slower than the system installed on your computer.
        </p>

        <h2>Organizer</h2>

        <p>The organizer of the event is <a href="https://dmz.rs/en/">Decentrala</a> - a group of enthusiasts gathered around the ideas of decentralization and free dissemination of knowledge. So far, we have organized more than <a href="https://dmz.rs/en/events_archive">300 events</a>, and we regularly announce the next events on the <a href="https://dmz.rs/en/events">Events</a> page.
        </p>

        <p>In the following period, two more events for Linux beginners will be held at the same location (classroom JAG3):</p>
        <ul>
            <li><strong>Tuesday December 16</strong> -  Introduction to the Linux command line</li>
            <li><strong>Tuesday, December 23</strong> - Introduction to Git</li>
        </ul>
        <p>Events start at 6pm.</p>

        <h2>Ponovo</h2>
        <p>You can bring defective devices to the Linux install fest: laptops, phones, desktop computers, monitors... We will deliver them to the organization <a href="https://ponovo.rs/">Ponovo</a> in Kikinda during January. This organization will repair these devices and thereby prevent the increase of electronic waste.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Schizophrenia sufferer mistakes smart fridge ad for psychotic episode (481 pts)]]></title>
            <link>https://old.reddit.com/r/LegalAdviceUK/comments/1pc7999/my_schizophrenic_sister_hospitalised_herself/</link>
            <guid>46171425</guid>
            <pubDate>Sat, 06 Dec 2025 07:31:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LegalAdviceUK/comments/1pc7999/my_schizophrenic_sister_hospitalised_herself/">https://old.reddit.com/r/LegalAdviceUK/comments/1pc7999/my_schizophrenic_sister_hospitalised_herself/</a>, See on <a href="https://news.ycombinator.com/item?id=46171425">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Need some advice. She's not really capable of organising most of her own affairs.</p>

<p>She spent 2 days under monitoring. This isn't the first time she's been kept in as she has had previous psychotic episodes once every two years or so.</p>

<p>During this time her medications were adjusted.</p>

<p>She also rang me during this time to tell me that "someone was trying to communicate with her through her fridge." She booked a taxi to A&amp;E and was driven there.</p>

<p>I've finally got her back home a few days ago. However, when I was scrolling on Facebook today I saw an advert on a smart fridge which stated the words, "WE'RE SORRY WE UPSET YOU, CAROL." It was set against a creepy yellow background and was very ominous. Upon closer inspection it was an advert for some TV show.</p>

<p>That's my sister's name. Carol. I sent her the photo and asked if this was what she saw. She confirmed it.</p>

<p>Some creepy advert in a place where an advert shouldn't usually go has sent her to the bloody hospital and triggered a review of the efficiacy of her antipyshotics.</p>

<p>Is this even legal in the UK? Running creepy adverts like that on a smart fridge with absolutely no way of knowing who could've seen them?</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wolfram Compute Services (220 pts)]]></title>
            <link>https://writings.stephenwolfram.com/2025/12/instant-supercompute-launching-wolfram-compute-services/</link>
            <guid>46171394</guid>
            <pubDate>Sat, 06 Dec 2025 07:21:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://writings.stephenwolfram.com/2025/12/instant-supercompute-launching-wolfram-compute-services/">https://writings.stephenwolfram.com/2025/12/instant-supercompute-launching-wolfram-compute-services/</a>, See on <a href="https://news.ycombinator.com/item?id=46171394">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><img title="Instant Supercompute: Launching Wolfram Compute Services" src="https://content.wolfram.com/sites/43/2025/11/sw11252025heroimg1.png" alt="Instant Supercompute: Launching Wolfram Compute Services" width="620" height="540"></p>
<div id="gpt-stripe">
<p>To immediately enable Wolfram Compute Services in Version 14.3 Wolfram Desktop systems, run</p>
<div data-c2c-file="https://content.wolfram.com/sites/43/2025/12/sw12022025banner-2img1_copy.txt" data-c2c-type="text/html" id="writtings-c2c_above"><tt>RemoteBatchSubmissionEnvironment["WolframBatch"]</tt><p>.</p></div>
<p>(The functionality is automatically available in the <a href="https://www.wolfram.com/cloud/">Wolfram Cloud</a>.)</p>
</div>
<h2 id="scaling-up-your-computations">Scaling Up Your Computations</h2>
<p>Let’s say you’ve done a computation in <a href="https://www.wolfram.com/language/">Wolfram Language</a>. And now you want to scale it up. Maybe 1000x or more. Well, <a href="https://www.wolfram.com/compute-services/">today we’ve released</a> an extremely streamlined way to do that. Just wrap the scaled up computation in <tt><a href="http://reference.wolfram.com/language/ref/RemoteBatchSubmit.html">RemoteBatchSubmit</a></tt> and off it’ll go to our new <a href="https://www.wolfram.com/compute-services/">Wolfram Compute Services system</a>. Then—in a minute, an hour, a day, or whatever—it’ll let you know it’s finished, and you can get its results.</p>
<p>For decades I’ve often needed to do big, crunchy calculations (<a href="https://writings.stephenwolfram.com/all-by-date/">usually for science</a>). With large volumes of data, millions of cases, rampant <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility">computational irreducibility</a>, etc. I probably have more compute lying around my house than most people—these days about 200 cores worth. But many nights I’ll leave all of that compute running, all night—and I still want much more. Well, as of today, there’s an easy solution—for everyone: just seamlessly send your computation off to Wolfram Compute Services to be done, at basically any scale.</p>
<p>For nearly 20 years we’ve had built-in functions like <tt><a href="http://reference.wolfram.com/language/ref/ParallelMap.html">ParallelMap</a></tt> and <tt><a href="http://reference.wolfram.com/language/ref/ParallelTable.html">ParallelTable</a></tt> in Wolfram Language that make it immediate to parallelize subcomputations. But for this to really let you scale up, you have to have the compute. Which now—thanks to our new Wolfram Compute Services—everyone can immediately get.<span id="more-71678"></span></p>
<p>The <a href="https://reference.wolfram.com/language/guide/RemoteBatchJobs.html">underlying tools</a> that make Wolfram Compute Services possible have existed in the Wolfram Language for several years. But what Wolfram Compute Services now does is to pull everything together to provide an extremely streamlined all-in-one experience. For example, let’s say you’re working in a notebook and building up a computation. And finally you give the input that you want to scale up. Typically that input will have lots of dependencies on earlier parts of your computation. But you don’t have to worry about any of that. Just take the input you want to scale up, and feed it to <tt>RemoteBatchSubmit</tt>. Wolfram Compute Services will automatically take care of all the dependencies, etc. </p>
<p>And another thing: <tt>RemoteBatchSubmit</tt>, like every function in Wolfram Language, is dealing with symbolic expressions, which can represent anything—from numerical tables to images to graphs to user interfaces to videos, etc. So that means that the results you get can immediately be used, say in your Wolfram Notebook, without any importing, etc.</p>
<p>OK, so what kinds of machines can you run on? Well, Wolfram Compute Services gives you a <a href="https://www.wolfram.com/compute-services/#machine-instance-categories">bunch of options</a>, suitable for different computations, and different budgets. There’s the most basic 1 core, 8 GB option—which you can use to just “get a computation off your own machine”. You can pick a machine with larger memory—currently up to about 1500 GB. Or you can pick a machine with more cores—currently up to 192. But if you’re looking for even larger scale parallelism Wolfram Compute Services can deal with that too. Because <tt><a href="http://reference.wolfram.com/language/ref/RemoteBatchMapSubmit.html">RemoteBatchMapSubmit</a></tt> can map a function across any number of elements, running on any number of cores, across multiple machines. </p>
<h2 id="a-simple-example">A Simple Example</h2>
<p>OK, so here’s a very simple example—that happens to come from <a href="https://writings.stephenwolfram.com/2023/11/aggregation-and-tiling-as-multicomputational-processes/#polygonal-shapes">some science I did a little while ago</a>. Define a function <tt>PentagonTiling</tt> that randomly adds nonoverlapping pentagons to a cluster:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025simpleimg1.png" alt="" title="" width="407" height="74"> </p>

<p>For 20 pentagons I can run this quickly on my machine:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025simpleimg2.png" alt="" title="" width="205" height="153"> </p>

<p>But what about for 500 pentagons? Well, the computational geometry gets difficult and it would take long enough that I wouldn’t want to tie up my own machine doing it. But now there’s another option: use Wolfram Compute Services!</p>
<p>And all I have to do is feed my computation to <tt>RemoteBatchSubmit</tt>:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025simpleimg3.png" alt="" title="" width="600" height="108"> </p>

<p>Immediately, a job is created (with all necessary dependencies automatically handled). And the job is queued for execution. And then, a couple of minutes later, I get an email: </p>
<p><img src="https://content.wolfram.com/sites/43/2025/11/sw11252025simpleimg4.png" alt="Email confirming batch job is starting" title="Email confirming batch job is starting" width="360" height="287"></p>
<p>Not knowing how long it’s going to take, I go off and do something else. But a while later, I’m curious to check how my job is doing. So I click the link in the email and it takes me to a dashboard—and I can see that my job is successfully running:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/11/sw11252025simpleimg5.png" alt="Wolfram Compute Services dashboard" title="Wolfram Compute Services dashboard" width="619" height="295"></p>
<p>I go off and do other things. Then, suddenly, I get an email:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/11/sw11252025simpleimg6.png" alt="Email confirming batch job success" title="Email confirming batch job success" width="611" height="657"></p>
<p>It finished! And in the mail is a preview of the result. To get the result as an expression in a Wolfram Language session I just evaluate a line from the email: </p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025simpleimg7.png" alt="" title="" width="673" height="380"> </p>

<p>And this is now a computable object that I can work with, say computing areas</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025simpleimg8.png" alt="" title="" width="322" height="43"> </p>

<p>or counting holes:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025simpleimg9.png" alt="" title="" width="293" height="93"> </p>

<h2 id="large-scale-parallelism">Large-Scale Parallelism</h2>
<p>One of the great strengths of Wolfram Compute Services is that it makes it easy to use large-scale parallelism. You want to run your computation in parallel on hundreds of cores? Well, just use Wolfram Compute Services! </p>
<p>Here’s an example that came up in some recent work of mine. I’m searching for a cellular automaton rule that generates a pattern with a “lifetime” of exactly 100 steps. Here I’m testing 10,000 random rules—which takes a couple of seconds, and doesn’t find anything:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025largeimg1.png" alt="" title="" width="516" height="91"> </p>

<p>To test 100,000 rules I can use <tt><a href="http://reference.wolfram.com/language/ref/ParallelSelect.html">ParallelSelect</a></tt> and run in parallel, say across the 16 cores in my laptop:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025largeimg2.png" alt="" title="" width="516" height="91"> </p>

<p>Still nothing. OK, so what about testing 100 million rules? Well, then it’s time for Wolfram Compute Services. The simplest thing to do is just to submit a job requesting a machine with lots of cores (here 192, the maximum currently offered): </p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025largeimg3.png" alt="" title="" width="660" height="179"> </p>

<p>A few minutes later I get mail telling me the job is starting. After a while I check on my job and it’s still running:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/11/sw11252025largeimg4.png" alt="Email confirming batch job is starting" title="Email confirming batch job is starting" width="360" height="287"></p>
<p>I go off and do other things. Then, after a couple of hours I get mail telling me my job is finished. And there’s a preview in the email that shows, yes, it found some things:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/11/sw11252025largeimg5.png" alt="Email confirming batch job success" title="Email confirming batch job success" width="360" height="461"></p>
<p>I get the result:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025largeimg6.png" alt="" title="" width="684" height="115"> </p>

<p>And here they are—rules plucked from the hundred million tests we did in the computational universe:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025largeimg7.png" alt="" title="" width="518" height="244"> </p>

<p>But what if we wanted to get this result in less than a couple of hours? Well, then we’d need even more parallelism. And, actually, Wolfram Compute Services lets us get that too—using <tt>RemoteBatchMapSubmit</tt>. You can think of <tt>RemoteBatchMapSubmit</tt> as a souped up analog of <tt>ParallelMap</tt>—mapping a function across a list of any length, splitting up the necessary computations across cores that can be on different machines, and handling the data and communications involved in a scalable way. </p>
<p>Because <tt>RemoteBatchMapSubmit</tt> is a “pure <tt><a href="http://reference.wolfram.com/language/ref/Map.html">Map</a></tt>” we have to rearrange our computation a little—making it run 100,000 cases of selecting from 1000 random instances:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025largeimg8.png" alt="" title="" width="630" height="219"> </p>

<p>The system decided to distribute my 100,000 cases across 316 separate “child jobs”, here each running on its own core. How is the job doing? I can get a dynamic visualization of what’s happening:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025largeimg9.png" alt="" title="" width="359" height="225"> </p>

<p>And it doesn’t take many minutes before I’m getting mail that the job is finished:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/11/sw11252025largeimg12.png" alt="Email providing job details" title="Email providing job details" width="454" height="430"></p>
<p>And, yes, even though I only had to wait for 3 minutes to get this result, the total amount of computer time used—across all the cores—is about 8 hours. </p>
<p>Now I can retrieve all the results, using <tt><a href="http://reference.wolfram.com/language/ref/Catenate.html">Catenate</a></tt> to combine all the separate pieces I generated:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025large-aimg1.png" alt="" title="" width="503" height="267"> </p>

<p>And, yes, if I wanted to spend a little more, I could run a bigger search, increasing the 100,000 to a larger number; <tt>RemoteBatchMapSubmit</tt> and Wolfram Compute Services would seamlessly scale up.</p>
<h2 id="its-all-programmable">It’s All Programmable!</h2>
<p>Like everything around Wolfram Language, Wolfram Compute Services is fully programmable. When you submit a job, there are lots of options you can set. We already saw the option <tt><a href="https://reference.wolfram.com/language/ref/RemoteMachineClass.html">RemoteMachineClass</a></tt> which lets you choose the type of machine to use. Currently the choices range from <tt>"</tt><span>Basic1x8</span><tt>"</tt> (1 core, 8 GB) through <tt>"</tt><span>Basic4x16</span><tt>"</tt> (4 cores, 16 GB) to “parallel compute” <tt>"</tt><span>Compute192x384</span><tt>"</tt> (192 cores, 384 GB) and “large memory” <tt>"</tt><span>Memory192x1536</span><tt>"</tt> (192 cores, 1536 GB).</p>
<p>Different classes of machine cost different numbers of credits to run. And to make sure things don’t go out of control, you can set the options <tt><a href="http://reference.wolfram.com/language/ref/TimeConstraint.html">TimeConstraint</a></tt> (maximum time in seconds) and <tt><a href="https://reference.wolframcloud.com/language/ref/CreditConstraint.html">CreditConstraint</a></tt> (maximum number of credits to use). </p>
<p>Then there’s notification. The default is to send one email when the job is starting, and one when it’s finished. There’s an option <tt><a href="https://reference.wolfram.com/language/ref/RemoteJobName.html">RemoteJobName</a></tt> that lets you give a name to each job, so you can more easily tell which job a particular piece of email is about, or where the job is on the web dashboard. (If you don’t give a name to a job, it’ll be referred to by the UUID it’s been assigned.)</p>
<p>The option <tt><a href="https://reference.wolfram.com/language/ref/RemoteJobNotifications.html">RemoteJobNotifications</a></tt> lets you say what notifications you want, and how you want to receive them. There can be notifications whenever the status of a job changes, or at specific time intervals, or when specific numbers of credits have been used. You can get notifications either by email, or by text message. And, yes, if you get notified that your job is going to run out of credits, you can always go to the <a href="https://account.wolfram.com/login/oauth2/sign-in" target="_blank" rel="noopener">Wolfram Account portal</a> to top up your credits.</p>
<p>There are many properties of jobs that you can query. A central one is <tt>"EvaluationResult"</tt>. But, for example, <tt>"<a href="http://reference.wolfram.com/language/ref/EvaluationData.html">EvaluationData</a>"</tt> gives you a whole association of related information:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025programmableimg1.png" alt="" title="" width="657" height="221"> </p>

<p>If your job succeeds, it’s pretty likely <tt>"EvaluationResult"</tt> will be all you need. But if something goes wrong, you can easily drill down to study the details of what happened with the job, for example by looking at <tt>"JobLogTabular"</tt>.</p>
<p>If you want to know all the jobs you’ve initiated, you can always look at the web dashboard, but you can also get symbolic representations of the jobs from: </p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025programmableimg3-a.png" alt="" title="" width="547" height="214"> </p>

<p>For any of these job objects, you can ask for properties, and you can for example also apply <tt><a href="https://reference.wolfram.com/language/ref/RemoteBatchJobAbort.html">RemoteBatchJobAbort</a></tt> to abort them.</p>
<p>Once a job has completed, its result will be stored in Wolfram Compute Services—but only for a limited time (currently two weeks). Of course, once you’ve got the result, it’s very easy to store it permanently, for example, by putting it into the Wolfram Cloud using <tt><a href="https://reference.wolfram.com/language/ref/CloudPut.html">CloudPut</a></tt>[<i>expr</i>]. (If you know you’re going to want to store the result permanently, you can also do the <tt>CloudPut</tt> right inside your <tt>RemoteBatchSubmit</tt>.) </p>
<p>Talking about programmatic uses of Wolfram Compute Services, here’s another example: let’s say you want to generate a compute-intensive report once a week. Well, then you can put together several very high-level Wolfram Language functions to deploy a scheduled task that will run in the Wolfram Cloud to initiate jobs for Wolfram Compute Services:</p>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/11/sw11252025programmableimg4.png" alt="" title="" width="485" height="14"> </p>

<p>And, yes, you can initiate a Wolfram Compute Services job from any Wolfram Language system, whether on the desktop or in the cloud. </p>
<h2 id="and-theres-more-coming">And There’s More Coming…</h2>
<p>Wolfram Compute Services is going to be very useful to many people. But actually it’s just part of a much larger constellation of capabilities aimed at broadening the ways Wolfram Language can be used.</p>
<p>Mathematica and the Wolfram Language <a href="https://www.wolfram.com/mathematica/scrapbook/">started—back in 1988</a>—as desktop systems. But even at the very beginning, there was a capability to run the notebook front end on one machine, and then have a “<a href="https://reference.wolfram.com/language/howto/ConnectToARemoteKernel.html">remote kernel</a>” on another machine. (In those days we supported, among other things, communication via phone line!) In 2008 we introduced built-in parallel computation capabilities like <tt>ParallelMap</tt> and <tt>ParallelTable</tt>. Then in 2014 we introduced the <a href="https://www.wolframcloud.com/">Wolfram Cloud</a>—both replicating the core functionality of <a href="https://www.wolfram.com/notebooks/">Wolfram Notebooks</a> on the web, and providing services such as <a href="https://reference.wolfram.com/language/guide/CreatingAnInstantAPI.html">instant APIs</a> and <a href="https://reference.wolfram.com/language/ref/ScheduledTask.html">scheduled tasks</a>. Soon thereafter, we introduced the <a href="https://www.wolfram.com/enterprise-private-cloud/">Enterprise Private Cloud</a>—a private version of Wolfram Cloud. In 2021 we introduced <a href="https://www.wolfram.com/application-server/">Wolfram Application Server</a> to deliver high-performance APIs (and it’s what we now use, for example, for <a href="https://www.wolframalpha.com/">Wolfram|Alpha</a>). Along the way, in 2019, we introduced <a href="https://www.wolfram.com/engine/">Wolfram Engine</a> as a streamlined server and command-line deployment of Wolfram Language. Around Wolfram Engine we built <a href="https://www.wolfram.com/wstpserver/">WSTPServer</a> to serve Wolfram Engine capabilities on local networks, and we introduced <a href="https://www.wolfram.com/wolframscript/">WolframScript</a> to provide a deployment-agnostic way to run command-line-style Wolfram Language code. In <a href="https://writings.stephenwolfram.com/2020/12/launching-version-12-2-of-wolfram-language-mathematica-228-new-functions-and-much-more/#big-computations-send-them-to-a-cloud-provider">2020 we then introduced</a> the first version of <tt>RemoteBatchSubmit</tt>, to be used with cloud services such as <a href="https://reference.wolfram.com/language/ref/batchcomputationprovider/AWSBatch.html">AWS</a> and <a href="https://reference.wolfram.com/language/ref/batchcomputationprovider/AzureBatch.html">Azure</a>. But unlike with Wolfram Compute Services, this required <a href="https://reference.wolfram.com/language/workflow/SetUpTheAWSBatchComputationProvider.html">“do it yourself” provisioning</a> and licensing with the cloud services. And, finally, now, that’s what we’ve automated in Wolfram Compute Services.</p>
<p>OK, so what’s next? An important direction is the forthcoming Wolfram HPCKit—for organizations with their own large-scale compute facilities to set up their own back ends to <tt>RemoteBatchSubmit</tt>, etc. <tt>RemoteBatchSubmit</tt> is built in a very general way, that allows different “<a href="https://reference.wolfram.com/language/guide/RemoteBatchJobs.html#179238631">batch computation providers</a>” to be plugged in. Wolfram Compute Services is initially set up to support just one standard batch computation provider: <tt>"WolframBatch"</tt>. HPCKit will allow organizations to configure their own compute facilities (often with our help) to serve as batch computation providers, extending the streamlined experience of Wolfram Compute Services to on-premise or organizational compute facilities, and automating what is often a rather fiddly job process of submission (which, I must say, personally reminds me a lot of the mainframe job control systems I used in the 1970s). </p>
<p>Wolfram Compute Services is currently set up purely as a batch computation environment. But within the Wolfram System, we have the capability to support synchronous remote computation, and we’re planning to extend Wolfram Compute Services to offer this—allowing one, for example, to seamlessly run a remote kernel on a large or exotic remote machine. </p>
<p>But this is for the future. Today we’re launching the first version of Wolfram Compute Services. Which makes “supercomputer power” immediately available for any Wolfram Language computation. I think it’s going to be very useful to a broad range of users of Wolfram Language. I know I’m going to be using it a lot.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nook Browser (108 pts)]]></title>
            <link>https://browsewithnook.com</link>
            <guid>46170402</guid>
            <pubDate>Sat, 06 Dec 2025 03:32:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://browsewithnook.com">https://browsewithnook.com</a>, See on <a href="https://news.ycombinator.com/item?id=46170402">Hacker News</a></p>
<div id="readability-page-1" class="page"> <astro-island uid="Z1OI2LO" component-url="/_astro/FloatingBanner.D8UsFlNT.js" component-export="default" renderer-url="/_astro/client.svelte.BSMze0vR.js" props="{&quot;latest&quot;:[0,{&quot;id&quot;:[0,1],&quot;title&quot;:[0,&quot;First launch: Nook Browser alpha&quot;],&quot;slug&quot;:[0,&quot;first-launch-nook-browser-alpha&quot;],&quot;version&quot;:[0,&quot;v1.0.1&quot;],&quot;tag&quot;:[0,&quot;New&quot;],&quot;publishedAt&quot;:[0,&quot;2025-10-06T07:00:00.000Z&quot;],&quot;summary&quot;:[0,&quot;The very first public alpha of Nook — Browse. It's yours.&quot;],&quot;image&quot;:[0,{&quot;id&quot;:[0,1],&quot;alt&quot;:[0,&quot;v1.0.1&quot;],&quot;updatedAt&quot;:[0,&quot;2025-10-12T19:50:08.229Z&quot;],&quot;createdAt&quot;:[0,&quot;2025-10-12T19:50:08.229Z&quot;],&quot;url&quot;:[0,&quot;/api/media/file/a4d901bf9ab760bbb8b04a6dca741c1f73191e47-1724x1080.avif&quot;],&quot;thumbnailURL&quot;:[0,null],&quot;filename&quot;:[0,&quot;a4d901bf9ab760bbb8b04a6dca741c1f73191e47-1724x1080.avif&quot;],&quot;mimeType&quot;:[0,&quot;image/avif&quot;],&quot;filesize&quot;:[0,126357],&quot;width&quot;:[0,1724],&quot;height&quot;:[0,1080]}],&quot;body&quot;:[0,{&quot;root&quot;:[0,{&quot;children&quot;:[1,[[0,{&quot;children&quot;:[1,[[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;We’re live! 🎉&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}]]],&quot;direction&quot;:[0,null],&quot;format&quot;:[0,&quot;&quot;],&quot;indent&quot;:[0,0],&quot;type&quot;:[0,&quot;heading&quot;],&quot;version&quot;:[0,1],&quot;tag&quot;:[0,&quot;h2&quot;]}],[0,{&quot;children&quot;:[1,[]],&quot;direction&quot;:[0,null],&quot;format&quot;:[0,&quot;&quot;],&quot;indent&quot;:[0,0],&quot;type&quot;:[0,&quot;paragraph&quot;],&quot;version&quot;:[0,1],&quot;textFormat&quot;:[0,0],&quot;textStyle&quot;:[0,&quot;&quot;]}],[0,{&quot;children&quot;:[1,[[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;Nook is an experiment in building a browser that feels &quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,2],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;calm&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;, &quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,2],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;local-first&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;, and &quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,2],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;open by design&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;.&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}]]],&quot;direction&quot;:[0,null],&quot;format&quot;:[0,&quot;&quot;],&quot;indent&quot;:[0,0],&quot;type&quot;:[0,&quot;paragraph&quot;],&quot;version&quot;:[0,1],&quot;textFormat&quot;:[0,0],&quot;textStyle&quot;:[0,&quot;&quot;]}],[0,{&quot;children&quot;:[1,[]],&quot;direction&quot;:[0,null],&quot;format&quot;:[0,&quot;&quot;],&quot;indent&quot;:[0,0],&quot;type&quot;:[0,&quot;paragraph&quot;],&quot;version&quot;:[0,1],&quot;textFormat&quot;:[0,0],&quot;textStyle&quot;:[0,&quot;&quot;]}],[0,{&quot;children&quot;:[1,[[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;- &quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,1],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;Spaces&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;: organize work and hobbies without tab sprawl.&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}]]],&quot;direction&quot;:[0,null],&quot;format&quot;:[0,&quot;&quot;],&quot;indent&quot;:[0,0],&quot;type&quot;:[0,&quot;paragraph&quot;],&quot;version&quot;:[0,1],&quot;textFormat&quot;:[0,0],&quot;textStyle&quot;:[0,&quot;&quot;]}],[0,{&quot;children&quot;:[1,[[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;- &quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,1],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;Local-first&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;: data stays on your machine.&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}]]],&quot;direction&quot;:[0,null],&quot;format&quot;:[0,&quot;&quot;],&quot;indent&quot;:[0,0],&quot;type&quot;:[0,&quot;paragraph&quot;],&quot;version&quot;:[0,1],&quot;textFormat&quot;:[0,0],&quot;textStyle&quot;:[0,&quot;&quot;]}],[0,{&quot;children&quot;:[1,[[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;- &quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,1],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;Community roadmap&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}],[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;: shape the journey with us.&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}]]],&quot;direction&quot;:[0,null],&quot;format&quot;:[0,&quot;&quot;],&quot;indent&quot;:[0,0],&quot;type&quot;:[0,&quot;paragraph&quot;],&quot;version&quot;:[0,1],&quot;textFormat&quot;:[0,0],&quot;textStyle&quot;:[0,&quot;&quot;]}],[0,{&quot;children&quot;:[1,[]],&quot;direction&quot;:[0,null],&quot;format&quot;:[0,&quot;&quot;],&quot;indent&quot;:[0,0],&quot;type&quot;:[0,&quot;paragraph&quot;],&quot;version&quot;:[0,1],&quot;textFormat&quot;:[0,0],&quot;textStyle&quot;:[0,&quot;&quot;]}],[0,{&quot;children&quot;:[1,[[0,{&quot;detail&quot;:[0,0],&quot;format&quot;:[0,0],&quot;mode&quot;:[0,&quot;normal&quot;],&quot;style&quot;:[0,&quot;&quot;],&quot;text&quot;:[0,&quot;Thank you for being here at day one.&quot;],&quot;type&quot;:[0,&quot;text&quot;],&quot;version&quot;:[0,1]}]]],&quot;direction&quot;:[0,null],&quot;format&quot;:[0,&quot;&quot;],&quot;indent&quot;:[0,0],&quot;type&quot;:[0,&quot;paragraph&quot;],&quot;version&quot;:[0,1],&quot;textFormat&quot;:[0,0],&quot;textStyle&quot;:[0,&quot;&quot;]}]]],&quot;direction&quot;:[0,null],&quot;format&quot;:[0,&quot;&quot;],&quot;indent&quot;:[0,0],&quot;type&quot;:[0,&quot;root&quot;],&quot;version&quot;:[0,1]}]}],&quot;links&quot;:[1,[[0,{&quot;id&quot;:[0,&quot;68ec062b635845ffed811b20&quot;],&quot;url&quot;:[0,&quot;https://github.com/nook-browser/Nook/releases/tag/v1.0.1&quot;]}]]],&quot;hidden&quot;:[0,false],&quot;updatedAt&quot;:[0,&quot;2025-10-12T19:50:11.091Z&quot;],&quot;createdAt&quot;:[0,&quot;2025-10-12T19:50:11.091Z&quot;]}]}" ssr="" client="load" opts="{&quot;name&quot;:&quot;FloatingBanner&quot;,&quot;value&quot;:true}" await-children=""><!--[--><!--[--><!--]--><!--]--><!--astro:end--></astro-island> <header>  </header>  <section data-reveal-parent=""> <h2 data-reveal=""> <span>Browse. It's yours.</span><br> <span>Open-source, Private, Forever.</span> </h2> <p data-reveal="" data-reveal-delay="90">
The memory of web browsing being quick and easy lives with us all.
        Fewer pop-ups, less clutter, and finding what you need ASAP.
</p>  </section> <section data-reveal-parent=""> <h2 data-reveal="">
Frequently asked questions
</h2>  </section> <!-- <div class="flex justify-center mt-[clamp(28px,6vw,56px)]">
			<figure class="w-[min(1120px,92vw)] rounded-[18px] border border-black/10 bg-white shadow-[0_24px_60px_rgba(0,0,0,0.12),_0_6px_16px_rgba(0,0,0,0.06)] overflow-hidden">
				<img src="/browser-mock.png" alt="Product preview mock" class="block w-full h-auto" loading="eager" width="2240" height="1260" />
			</figure>
		</div> --> <!-- <div
      class="w-full flex justify-center mt-[clamp(28px,6vw,56px)] px-[clamp(16px,2vw,28px)]"
    >
      <img
        src="/browser-mock.png"
        alt="Product preview mock"
        class="block h-auto w-full max-w-[min(1120px,92vw)] object-contain"
        loading="eager"
      />
    </div> -->  <div data-reveal-parent=""> <div data-reveal="">  <h3>
Fast by design
</h3> <p>
Minimal overhead. Less clutter. Pages feel instant, powered by
            WebKit.
</p> </div> <div data-reveal="" data-reveal-delay="70">  <h3>
Optional, Opt-In AI
</h3> <p>
When enabled, they provide helpful tools such as chat assistance,
            summaries, up-to-date web insights, and more.
</p> </div> <div data-reveal="" data-reveal-delay="140">  <h3>
Open-source forever
</h3> <p>
Transparent code, permissive license, and a community-driven
            roadmap.
</p> </div> </div> <!-- PRODUCT PREVIEW / GALLERY -->  <div data-reveal="" data-reveal-parent=""> <div> <h2>
Our pledge
</h2> <ul> <li> <span>No selling of browsing data. Ever.</span> </li> <li> <span>Features ship when they’re stable and accessible.</span> </li> <li> <span>We keep settings understandable and reversible.</span> </li> </ul>  </div> <p><img src="https://browsewithnook.com/browser-mock.png" alt="Browser"> </p> </div> <section data-reveal-parent="" data-faq=""> <h2 data-reveal="">
Frequently asked questions
</h2>  </section>    </div>]]></description>
        </item>
        <item>
            <title><![CDATA[PalmOS on FisherPrice Pixter Toy (188 pts)]]></title>
            <link>https://dmitry.gr/?r=05.Projects&amp;proj=27.%20rePalm#pixter</link>
            <guid>46170309</guid>
            <pubDate>Sat, 06 Dec 2025 03:17:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dmitry.gr/?r=05.Projects&#x26;proj=27.%20rePalm#pixter">https://dmitry.gr/?r=05.Projects&#x26;proj=27.%20rePalm#pixter</a>, See on <a href="https://news.ycombinator.com/item?id=46170309">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>




<p><span>rePalm</span>
<a href="https://photos.app.goo.gl/ubrrKSnMi0UoKEa03">Photo Album</a>(constantly updated)
</p><h2>Table of Contents</h2>
<ol type="1"><li><a href="#_TOC_d6004cc94a9c0596eceb597fffab171b">Blog-style updates</a><ol type="a"><li><a href="#_TOC_105d081c20da41387dcb640d7844e380">BLOG</a></li><li><a href="#_TOC_663662b91f36a398f3dd1022569cb7e1">Dec 5, 2025 - Pixter</a><ol type="I"><li><a href="#_TOC_317c392cf501b6b4320a3115280d293e">Getting started with Pixter</a></li><li><a href="#_TOC_ccf13f871bff4d85711b6c2982adaca7">Initial Slow Pixter Color Bringup</a></li><li><a href="#_TOC_a23bcb0f34045da3628fbb9c63dc9cd8">The Worst ARM SoC I've Seen Yet</a></li><li><a href="#_TOC_32a452a8a1d23aa3378b2fa796f23911">Pixter Memories</a></li><li><a href="#_TOC_c6090cdcd19a87ac82f0678f94ca5682">Pixter Displays</a></li><li><a href="#_TOC_15afaf19e9f1c29c9df05efc98623567">Making Pixter IrDA work</a></li><li><a href="#_TOC_6aaa83ffbda119675aa2b1a85d121186">Getting and Flashing the Pixter Carts</a></li><li><a href="#_TOC_81c94ecd14afaae13888291ae522ac30">Pixter Polish</a></li><li><a href="#_TOC_a23619cfc40ca570c4e32829136a083b">Battery State</a></li><li><a href="#_TOC_41361f59557a0b66300dbfbb53bf5e60">ARM7 quirks</a><ol type="A"><li><a href="#_TOC_55d2a6f8599932d97309a32336a88e3b">What exactly does ARM7 do with PC[1] in ARM mode?</a></li></ol></li><li><a href="#_TOC_a068d6e9b5f120e3dbc023accff99cfa">Pixter Multimedia</a></li><li><a href="#_TOC_d02db4cca336224e352d18884fb87bbf">Some More Pixter Polish</a></li><li><a href="#_TOC_db89a79a972ee06eda00fd6ec56af499">Pixter Results</a></li></ol></li><li><a href="#_TOC_4cf9ff9167b06c4f8aa708bac38bebd4">Nov 2, 2025 - summary of what you missed</a></li></ol></li><li><a href="#_TOC_4c49906dca074ffeea2a31a3da78a946">The original article about the start of the project</a><ol type="a"><li><a href="#_TOC_c38a777301478b17b8065df633c16073">PalmOS Architecture (and a bit of history)</a><ol type="I"><li><a href="#_TOC_16d2b386b2034b9488996466aaae0b57">History</a></li><li><a href="#_TOC_fe46700337bda9932cad30ebb5092816">Modules? Libraries? DALs? Drivers?</a></li></ol></li><li><a href="#_TOC_244123db096496b6518454bdcd8d458a">Towards the first unauthorized PalmOS port</a><ol type="I"><li><a href="#_TOC_7690ee3e298ae7406f138709ec026dce">So what's so hard?</a></li><li><a href="#_TOC_274e47df117091e2557aef0ecc391316">ROM formats are hard</a></li><li><a href="#_TOC_19186ee232b2454827d3f2865b5f6e69">So write a DAL and you're done!</a></li><li><a href="#_TOC_49caf94f20bd2e5f2db1081947789a60">Minimal DAL</a></li><li><a href="#_TOC_ef4090e17154674f80d64ad2111ea396">Drawing is hard</a></li><li><a href="#_TOC_b5690009716e72d140983f70b9681f50">Theft is a form of flattery, right?</a></li><li><a href="#_TOC_d6b64b427e5aaef1676407293aca6f38">Meticulously-performed imitation is also a form of flattery, no?</a></li><li><a href="#_TOC_353f99c16369230cb5399b32ba50446a">Virtual SD card</a></li><li><a href="#_TOC_353a28e51ebccb5135d5ba38c4d440e5">Which device ROM are you using?</a></li><li><a href="#_TOC_7af9e60baebf45eba73fb5a032144d71">So you're done, right? It works?</a></li></ol></li><li><a href="#_TOC_b8af925e2e0ccf5d333ef97c97cfa358">Towards the first pirate PalmOS device</a><ol type="I"><li><a href="#_TOC_f678de22decd6176f0c5a83b196ea618">A little bit about normal PalmOS 5.x devices, their CPUs, and the progress since...</a></li><li><a href="#_TOC_fa633dde76a9d5962249bd20bd69ed1d">ARMv7M</a></li><li><a href="#_TOC_a6683a2f1a734ad2e0c8266c58d6814c">My kingdom for an ARM!</a></li><li><a href="#_TOC_1576edec6568c60734a8c8f578946c2a">But what if we try?</a></li></ol></li><li><a href="#_TOC_0be92df30c61e06c706246f9f35b14fe">We need hardware, but developing on hardware is ... hard</a><ol type="I"><li><a href="#_TOC_1866711c0f60ccdc92f2b71753fadf1f">CortexEmu to the rescue</a></li><li><a href="#_TOC_fb27a2d36a554d080bd29f19319dd4ba">Waaaah! You promised real hardware</a></li></ol></li><li><a href="#_TOC_afaee39943998caadf5b45eb09ca52ff">Um, but now we need a kernel...</a><ol type="I"><li><a href="#_TOC_f4ed5002aeb6070729ce9b6df22517aa">Need a kernel? Why not Linux?</a></li></ol></li><li><a href="#_TOC_934bffe7eb7428a2ecd772ae407ba2bd">So, uh, what about all that pesky ARM code?</a><ol type="I"><li><a href="#_TOC_99433f6c0ccf629e79759495dd2a3895">The ARM code still was a problem</a></li><li><a href="#_TOC_321e6b87e6ca9bede1defae0487f6d3b">You do not mean...?</a></li><li><a href="#_TOC_811139ec6ba0b492d7e119279ce3fe81">But isn't writing an emulator in C kind of slow?</a></li><li><a href="#_TOC_7da84df7c7794732114e4f17174e44ae">So, is it fast enough now?</a></li></ol></li><li><a href="#_TOC_6157fbbda2d6ba3e17ac7f1d2dcb03bb">You do not mean...? (pt2)</a><ol type="I"><li><a href="#_TOC_3204170ab6cefc87ff0b9e8bbf15e3c3">Just in time: this</a></li><li><a href="#_TOC_6c1e3d4f3650586ef87c08dcb6a56032">JITs: how do we start?</a></li><li><a href="#_TOC_317e071be8e7e97a898025ca4f5d6a2d">Parlez-vous ARM?</a></li><li><a href="#_TOC_0bc5b46b49b772d9f98167f3d38c57cd">2 Thumbs do not make an ARM</a></li></ol></li><li><a href="#_TOC_7d2bad5e5df9b51e6561930af5281c4b">A JIT's job is never over</a><ol type="I"><li><a href="#_TOC_0bf44f2f0483b69f4f788aa7476bcfdf">LDM and STM, may they burn in hell forever!</a><ol type="A"><li><a href="#_TOC_154e4d4907e66b0615bf614b8b04b3f2">How LDM/STM work in ARM</a></li><li><a href="#_TOC_8bbe9e91149d059ee85d3694c813b8e7">How LDM/STM work in Thumb2</a></li><li><a href="#_TOC_e850194f609729b7a92dc19d57efffce">But wait, there's more ... pain</a></li><li><a href="#_TOC_6dae46dec6458f7e262d1dde02676949">Translating LDM/STM</a></li></ol></li><li><a href="#_TOC_1db9dab871ed72bf21793712aaa1571a">Slightly less hellish instructions</a></li><li><a href="#_TOC_0b703df619f55a061fa3080cd285e905">Conditional instructions</a></li><li><a href="#_TOC_12a7cbf0d95e0874700a526b0c606f24">Jumps &amp; Calls</a></li><li><a href="#_TOC_1845170175d6cd3ee7a45bcdcf60a99d">Translating a TU</a></li><li><a href="#_TOC_d8be03a7734ce336e0e95c12bed0d3d7">And if the TC is full?</a></li><li><a href="#_TOC_86491a45a2021ee669a55df54ecccd5d">Growing up</a></li><li><a href="#_TOC_1736a13ea992c6d293687480e5f8eba4">The Cortex-M0 backend</a><ol type="A"><li><a href="#_TOC_85e1b5d18cc909adc0ec42cefc752374">Why this is insane</a></li><li><a href="#_TOC_5f936d66726370eea0749c9a4d3cca5c">The basics</a></li><li><a href="#_TOC_3d1b169dc1c246143311667973d27191">Fault dispatching</a></li></ol></li></ol></li><li><a href="#_TOC_6edec3a664ff4b245910e38197c368f5">Is PACE fast enough?</a><ol type="I"><li><a href="#_TOC_8406e71f183218c42ea61761bec7f300">Those indirect jumps...</a></li><li><a href="#_TOC_eb78374ce478e8ce5e126397507be744">A special solution for a special problem</a></li><li><a href="#_TOC_53097c4149f9f99536f550859bf51298">Any 68k emulator...</a></li></ol></li><li><a href="#_TOC_04125366fb7868c0c90ac612ebd9446c">But, you promised hardware...</a><ol type="I"><li><a href="#_TOC_7dc6f1c77b85c0e75518885955ed41e3">Hardware has bugs</a></li><li><a href="#_TOC_430d6ee50c9ff1aa84546049d1a6a3aa">So why the 0x80000000 limit?</a></li><li><a href="#_TOC_d9c532a1d434bda469b4565b0fa78d0b">Two wrongs do not make a right, but do two nasty hacks?</a></li></ol></li><li><a href="#_TOC_fd61d91f4f762f98fe9d08c759298fac">Tales of more PalmOS reverse engineering</a><ol type="I"><li><a href="#_TOC_70c487f74203eb4f8470c3c652adec8e">SD-card Support</a></li><li><a href="#_TOC_0745eb1e96d7eecb7dca979ac0c5641d">Serial Port Support</a><ol type="A"><li><a href="#_TOC_f0938d2fb6ec341fc50c3cc32a68cf79">Yes, you can try it!</a></li></ol></li><li><a href="#_TOC_ca36ea3a859782b7e0e5a14e7999a0d2">Vibrate &amp; LED support</a></li><li><a href="#_TOC_09dd8d91380adf4a8376b97b0dbe8cbd">Networking support (WIP)</a><ol type="A"><li><a href="#_TOC_3a225f6b197aff0014031e11feafd006">False starts</a></li><li><a href="#_TOC_70f2cfa9e150ffd5e72c9dfa0b6a2e35">The scary way forward</a></li><li><a href="#_TOC_7cb40e1cb574360af15a7a7bfcab368a">Those who study history...</a></li><li><a href="#_TOC_4310dcbc6aabcc166c70891f443152b8">On to OS 5's Net.lib</a></li><li><a href="#_TOC_0f72c0cb93852eff2102a38c6867c3b0">I found a bug!</a></li><li><a href="#_TOC_0cedc678c13085bef12438ec8e144a5a">Well, that was easy...</a></li><li><a href="#_TOC_0ed61b57f60d1619623ad9529ec24522">NOT!</a></li><li><a href="#_TOC_b7ea5704d8c610fefee98b6e893f4285">More reverse engineering</a></li></ol></li><li><a href="#_TOC_0e86ad5bf27c5b7db90e96d69a6a20f1">1.5 density support</a><ol type="A"><li><a href="#_TOC_1dd41736817f8594b13c015f5d675061">Density basics</a></li><li><a href="#_TOC_3888d6ea493b8540d9b222eda870a34b">How does it all fall apart?</a></li><li><a href="#_TOC_6dc1fcfd40eac977ca8de78c302e161a">How do we fix it?</a></li><li><a href="#_TOC_81e1dca6c4086b8ca08fdd313aab38ef">And now, for some polish</a></li></ol></li><li><a href="#_TOC_fb8dffd581e4c5c2fb3a602c9968dfa1">Dynamic Input Area/Pen Input Manager Services support</a><ol type="A"><li><a href="#_TOC_55c4ffa0cef8df0ca7dada39de0c7edc">DIA/PINS basics</a></li><li><a href="#_TOC_f972738a6fc57f587b6b8217a3a8c3ff">How it works pre-garnet</a></li><li><a href="#_TOC_7a4af6319253fe4958ef467593058a76">The intricacies of writing a DIA implementation</a></li></ol></li><li><a href="#_TOC_9452c577c360e42af5e333b94545a583">Audio support</a><ol type="A"><li><a href="#_TOC_401cba25ab569a340588aba30df19598">PalmOS Audio basics</a></li><li><a href="#_TOC_7e67a64aa2db4bac832171b0201d5059">PalmOS sampled sudio support</a></li><li><a href="#_TOC_e29e5b41fd7910dd365d892a74032818">Why audio is hard &amp; how PalmOS makes it easy</a></li><li><a href="#_TOC_141eb8494d23c01acb07946b8dfae14d">How rePalm does audio mixing</a></li><li><a href="#_TOC_a6ab4fb079f26742eb2b15a40f8c415e">How do assembly and audio mix?</a></li><li><a href="#_TOC_c9a3edc13d1d9b624900782475cb37d0">rePalm's audio hw driver architecture</a></li><li><a href="#_TOC_2cc9187af12cfb288552a94945e2576f">Microphone</a></li></ol></li><li><a href="#_TOC_2d69b048c7fa078b50826e0bb14ef187">Zodiac support</a><ol type="A"><li><a href="#_TOC_9abd84aae09c84b126622be17538bb11">Tapwave Zodiac primer</a></li><li><a href="#_TOC_3ae834b010d019ad46839d53a26fc246">The reverse engineering</a></li><li><a href="#_TOC_488e310c3fbbea8ddc8055dce2fbd0eb">The "GPU"</a></li><li><a href="#_TOC_e5161cb0029a5c9e6b45aedc75ec628c">Other Tapwave APIs</a></li></ol></li></ol></li><li><a href="#_TOC_971499a21e41b5eede224c94b7f329cf">Real hardware: reSpring</a><ol type="I"><li><a href="#_TOC_b9dc6effb2e2905b150bdb2f7431d48b">The ultimate Springboard accessory</a></li><li><a href="#_TOC_523dfe92d22dbe6fd0c936743d86a610">Interfacing with the Visor</a></li><li><a href="#_TOC_d4b36e25c7ff304278d8f8203382ad65">Version 1</a></li><li><a href="#_TOC_140b7a05f413da05bcf245962e8ef816">Bringup of v1</a></li><li><a href="#_TOC_7d7efb1fd00e505e192d7de552b38a5c">Let's stick it into a Visor?</a><ol type="A"><li><a href="#_TOC_cc4796dbaf2dc5b25a09a60a68dce960">Getting recognized</a></li><li><a href="#_TOC_8c44784826fb6a797ca8f9ddef7f17d1">Saving valuable space</a></li><li><a href="#_TOC_cf948ca3c9fa8aa0d50b7af90489dbab">Communications</a></li><li><a href="#_TOC_a08fa204773f1401ecad0ad320f0e29f">Early Visor support</a></li></ol></li><li><a href="#_TOC_efa1a6bae85982abb5eb2f5b1cdfc635">Making it work well</a><ol type="A"><li><a href="#_TOC_60071961fe2964324372b9a5f34e819a">Initial data</a></li><li><a href="#_TOC_265eae2e494574477805623b3356ea35">Sending display data</a></li><li><a href="#_TOC_c77b699ee26bfeed9429d18ac6eaf913">Buttons, pen, brightness, contrast, and battery info</a></li><li><a href="#_TOC_eccf95f0e1bd185a785e1e8a17db8566">Microphone support</a></li></ol></li><li><a href="#_TOC_c730389bc8d99e59c867766babdd48b5">Polish</a><ol type="A"><li><a href="#_TOC_0fc38b86ec3a198dd384a33f1ba51090">Serial/IrDA</a></li><li><a href="#_TOC_b50a7cf91ffc6c447c18e7bc3e5d2ba5">Alarm LED</a></li><li><a href="#_TOC_c329847c5a9eb54120dcc6a37e7688cd">Software update</a></li></ol></li><li><a href="#_TOC_9b6a7a6a1e5e220ce79d2c1fb3055b4b">Onboard NAND</a><ol type="A"><li><a href="#_TOC_c822baa4a271b34914d0d52b7daf51fe">You wanted pain? Here's some NAND</a></li><li><a href="#_TOC_6b3b62f6a546e9fc2070dadae82f7fb6">To write an FTL...</a></li></ol></li><li><a href="#_TOC_c2a5006b48989e3cbd0a40a6c28ae0a5">One final WTF</a></li></ol></li><li><a href="#_TOC_9b23c2379c1d04a1e2c5f7bd55e5b11d">More real hardware</a><ol type="I"><li><a href="#_TOC_68d5b3fecf777afa589daf30f3003eed">rePalm-MSIO</a><ol type="A"><li><a href="#_TOC_b78ced7630516f39a9df67f8bff3fbcb">MCU selection</a></li><li><a href="#_TOC_63dbebc501303d2fbc119f39fd5fa3a9">The bugs...</a></li><li><a href="#_TOC_12902d19af5dcccb4930367282858fd6">MSIO low level</a></li><li><a href="#_TOC_b3a6a302f1a4b30ed9243ec16e09d331">MSIO high level</a></li><li><a href="#_TOC_8bef4bd1a59725a78a239166683e5005">MSIO performance</a></li><li><a href="#_TOC_0ef17952991bb23ceb3eff5d58852ff5">Other loose ends</a></li></ol></li><li><a href="#_TOC_7d49aaed1771a47099d2087b01e38be4">AximX3</a></li><li><a href="#_TOC_37411efaa3076380c4c52b43da1a7de7">STM32F469 Discovery Board</a></li><li><a href="#_TOC_0734fd58b98b17e23027547eec1258f5">RP2040</a><ol type="A"><li><a href="#_TOC_da19cdc85215569c48f1e5adbb0dcceb">It is possible!</a></li><li><a href="#_TOC_71fc1629af9b6f7f996c71b3442f6138">Memories</a></li><li><a href="#_TOC_a458b10a6c13066f378af7a655517e3f">PACE again</a></li></ol></li></ol></li><li><a href="#_TOC_8c65bb84ff7790f79c8b425f7bdc1116">So where does this leave us?</a></li><li><a href="#_TOC_4c200f344d32f6a87d65cd4615afc915">Source Code</a><ol type="I"><li><a href="#_TOC_30c25f0330c9b505cc348823769f06d6">Source intro</a></li><li><a href="#_TOC_dc2be8290c0302afb4d265bad618ed1c">Building basics</a></li><li><a href="#_TOC_0092c8837761d29c05d04f994e095d6d">Building PACE</a></li></ol></li><li><a href="#_TOC_e797f665d200ef27bdbf6f28bf3f764c">Article update history</a></li><li><a href="#_TOC_7e1e75c32bc9b275daf70df8cba8efb5">Comments...</a></li></ol></li></ol>






<h2>Blog-style updates</h2>
<h2>BLOG</h2>
<p>I have decided to change the format of this article to be more blog-like as further development is being done in parallel on many fronts and will be hard to follow if I just update the main (now-huge) article body. So what has transpired since? 
</p>

<h2>Dec 5, 2025 - Pixter</h2>
<p><a href="https://dmitry.gr/images/rePalm-PixterColorAboutScreenLarge.jpg"><img src="https://dmitry.gr/images/rePalm-PixterColorAboutScreenSmall.jpg" alt="Pixter Color showing PalmOS 5.2.1 infor panel"></a>
<a name="_TOC_317c392cf501b6b4320a3115280d293e"></a></p><h3>Getting started with Pixter</h3>
<p>Fisher-Price (owned by Mattel) produced some toys in the early 2000 under the <a href="https://en.wikipedia.org/wiki/Pixter">Pixter</a> brand. They were touchscreen-based drawing toys, with cartridge-based <a href="https://en.wikipedia.org/wiki/Pixter">extra games</a> one could plug in. Pixter devices of the first three generations ("classic", "plus", and "2.0") featured 80x80 black-and-white screens, which makes them of no interest for rePalm. The last two generations of Pixter ("color" and "multimedia") featured 160x160 color displays. Now, this was more like it! Pixter was quite popular, as far as kids' toys go, in USA in the early 2000s. A friend brought it to my attention a year ago as a potential rePalm target. The screen resolution was right and looking inside a "Pixter Color" showed an ARM SoC - a <a href="https://web.archive.org/web/20250206223428/https://www.keil.com/dd/docs/datashts/philips/lh75401_lh75411_ds.pdf">Sharp LH75411</a>. The device had sound (games made noises), and touch panel was resistive. In theory - a viable rePalm target indeed.
</p>
<p>My initial work involved figuring out how the last two generations of Pixter work and how to get code execution on them, which I <a href="https://dmitry.gr/?r=05.Projects&amp;proj=37.%20Pixter">wrote a separate article on</a> (which may not yet be publicly up -- I am but one man and editing takes time). The short of it is that the cartridge slot includes access to the full memory bus and two chip-select lines allowing one to connect two memories or memory-like things to the device. The first (seen at PA <span>0x48000000</span>) must connect to a 16-bit-wide ROM which would normally contain the game. I would put a PalmOS ROM there, of course. However, it would need to be formatted such that the Pixter boots it as a game, instead of assuming that the cartridge is invalid. Reverse engineering the Pixter ROM showed me the minimal way to make my ROM bootable. This requires a simple 44-byte header, with the following values at the following offsets: <span>u32@0x00 - 0xAA5566CC</span> (magic number), <span>u16@0x04 - 0x0001</span> (required version number), <span>u16@0x06 - 0x293c</span> (VM instruction to do a native callout to offset 0x28), <span>u32@0x10 - 0x48000006</span> (address where the first VM instr is to be seen, I use <span>0x48000006</span>), <span>u32@0x28 - 0x48??????</span> (address where Pixter OS will jump to in THUMB mode, where our actual execution will begin). I place some code before the <span>0x28</span> word to switch to ARM mode and disable interrupts, then jump to my PalmOS ROM which will start at offset <span>0x30</span> (for roundness). Thus, after this now-48-byte header, there can follow a normal PalmOS ROM. Pixter Color contains 128KB of RAM the motherboard, which is too little for PalmOS, so we'll use the second chip-select line to attach some RAM. Pixter Multimedia has 4MB of SDRAM onboard, which makes it able to run PalmOS without external RAM.
</p>
<h3>Initial Slow Pixter Color Bringup</h3>
<p>The pinout of the SoC on the Pixter Color was easy to work out since the chip is in an LQFP package and I could buzz-out the pin connections. The <a href="https://web.archive.org/web/20240719161636/https://www.keil.com/dd/docs/datashts/philips/lh754xx_um.pdf">User's Guide for Sharp LH75411</a> was available. Debugging on real hardware is hard, of course, so I wrote a Pixter Color emulator, as detailed in my <a href="https://dmitry.gr/?r=05.Projects&amp;proj=37.%20Pixter">Pixter article</a>. With this, I was able to bring up a minimal PalmOS image relatively quickly. Then, it was on to making it work on the real device. This was quite a bit more work. <a href="https://github.com/GeorgeRudolf">George</a> designed a board with a 1MB NOR flash for the OS and some RAM for PalmOS to use, and <a href="https://jlcpcb.com/">JLC</a> assembled a few for me. There were a few design decisions made during Pixter Color's design that complicated this project, unfortunately.
</p>
<p>Memories are connected to a SoC over a bus. A bus has a width, denominated in bits. For 32-bit ARM chips, external busses are usually 8, 16, or 32 bits wide. The wider the bus, the more bits can be sent over it in the same number of clocks, meaning that it is faster. Obviously, if you write a properly-aligned 32-bit word in your code, a 32-bit bus can transfer it to memory in one transfer. A 16-bit bus will need two -- one for the lower halfword, one for the higher. An 8-bit-wide bus will need 4 transfers to transfer the word, thus being 4 times slower. However, this does not mean that a narrower bus is always slower. Consider the case of writing a single byte. The 8-bit-wide bus can do this in a single transfer. What do the 16 and 32 bit busses do in this case? Guess!
</p>
<p>There are two guesses you could have come up with. The first is: read a bus-width-sized quantity of memory, modify the requisite byte, and then write a bus-width-sized quantity of memory back. This would require two bus transactions for both the 16 and the 32 bit wide busses. This is <em>not</em> what is done, for a variety of reasons which are quite out of scope here. What is actually done is that besides the access, data, and control lines, the wider busses also have a few extra lines, which are called "byte lane select" lines. They tell the memory which of the bytes in the addressed bus-width-sized memory location being addressed are active. So, to write a byte on a 32-bit-wide bus, only one of the byte lane select lines will be active, and the memory will not overwrite the other 3 bytes. This does mean that the memory chips need to support this sort of thing, and they do. Of course this is not an issue for reads - the unneeded 3 bytes of memory for a byte-sized read on a 32-bit-bus can just be ignored by the SoC. Easy!
</p>
<p>So, what were the design decisions in Pixter Color that made my life harder? Pixter Color's external cartridge slot exposes 24 bits of address and 16 bits of data. Since ROM is read-only, it needs no byte lane selects and indeed runs in 16-bit-wide mode. Sadly, byte lane select lines are <em>NOT</em> brought out to the cartridge slot. So, what would happen if I were to attach 16-bit RAM without them? Given the explanation above, it is clear -- reads would work fine. Word and halfword writes would work fine too. Byte writes would corrupt the neighboring byte. Clearly this is not going to work for booting PalmOS, which expects all RAM to be byte-addressable. What options are left? Just one -- RAM must be attached in 8-bit-wide mode. This does not require byte lane select lines and will correctly work for attaching RAM to Pixter Color via the cart slot. Sadly, as described earlier, this means that this memory if slower for larger access sizes, which are more common.
</p>
<p>There is more to consider here. When memory is accessed, it needs some time from being given an address and being asked to read it until it is expected to reply. Same applies for writes. To give it time, wait states are inserted. A normal bus access with no wait states might reasonably take two bus cycles to read a single bus-width-sized memory amount. The first cycle will present the address to the memory chip, and by the second, it is expected to have a reply ready to be read from the data lines of the bus. If the memory cannot reply that fast (in one cycle, basically), it will need wait states. What determines whether it can reply? Memories come in speed grades, which among other things, tell you how fast it could reply. For example on my Pixter Color cartridges, I use "-70" memory which can reply in 70 nanoseconds. Speed of light is also nonzero, and traces on boards and in connectors have inductance and capacitance, which, together, mean that the signals take time to travel from the SoC to the memory and back. Taken all together, one needs to configure the wait states such that the memory has enough time from truly seeing the control signals to the SoC truly seeing the replies. In Pixter Color's case at the rates I run the bus, this means the external memory runs with 2 wait states. The practical upshot of this is somewhat sad. Imagine a typical 32-bit read of external memory. Since the bus is 8-bits-wide, this will take 4 accesses. With 2 wait states, each access takes 4 cycles. This means the entire 32-bit-wide read takes as much as 4 x 4 = 16 cycles. Now, normally, the SoC's cache would absorb this slowness for reads and the write buffer would help on writes. Which brings us to...
</p>
<h3>The Worst ARM SoC I've Seen Yet</h3>
<p><a href="https://dmitry.gr/images/rePalm-PixterColorWarfareLarge.jpg"><img src="https://dmitry.gr/images/rePalm-PixterColorWarfareSmall.jpg" alt="Pixter Color playing Warfare Inc game"></a></p><p>The SoC in Pixter Color has the most minimal ARM7 configuration I've ever seen. The ARM7 CPU design is sold by ARM with a few configuration options that one decides on before instantiating it on a chip. One of the options is whether there is a cache, and of what size. Sharp went with "no thanks". Strike one! The next is whether there is an MMU. This is piece of hardware that allows very granular memory protection and mapping. Sharp went with "no". Strike two! Lacking that, there is an MPU option. This is a simpler memory protection unit - no mapping ability and limited number of regions of protection, but it is still better than nothing. The NintendoDS CPU uses this option, for example. This configuration is so simple, it basically costs no extra silicon at all -- no reason not to choose it. Sharp went with "nah". Strike three!
</p>
<p>But this gets even more fun, actually. ARM architectures before version 6 did not really support unaligned memory accesses. An unaligned write acted as if the lower address bits were zero, while an unaligned read would rotate the read word such that the "addressed" byte was at the bottom. Neither of those behaviours act like real unaligned memory access. That is to say that unaligned accesses were almost always a logic error. To catch them, ARM cores have a configuration bit to enable "alignment checking" which will cause an exception if an unaligned access is attempted. Since such accesses are almost always a bug, this checking should almost always be enabled. To configure whether it is or is not enabled, one uses coprocessor 15, which itself is optional. Sharp went with "ooh...optional, eh? NOPE!". Lacking a coprocessor 15, all configurable options become hardcoded to a set value with no ability to change them. In the case of the SoC in Pixter Color this means that alignment checking does not exist, since Sharp could not be bothered to enable it (at a cost of a dozen transistors, no more). Additionally, this means the exception vectors are always at <span>0x00000000</span>, since the ability to relocate them to <span>0xffff0000</span> is configured by cp15. This forces us to configure some memory to exist at address zero, which makes trapping NULL-pointer accesses impossible. There goes another error class we cannot trap. We're at five strikes by now... Jeez, Sharp!
</p>
<p>Without a cache, our 63MHz CPU ends up spending most of its time waiting on memory. Sharp did put in 16KB of <span>TCM</span> (tightly coupled memory) into the chip. This memory is accessible in a single cycle, making it rather fast. It can also appear anywhere in the address space (it is movable and overlays anything). But it is only 16KB which is very little. There is also 32KB of <span>eSRAM</span> (embedded SRAM) in the chip, which operates with no wait states and is 32 bits wide. This means that accessing it takes two cycles per word -- still quite fast. Pixter Color designers added 128KB of RAM onto the motherboard, as I had mentioned earlier. It is on a 16-bit-wide bus with one wait state. This means that for 32-bit reads, it takes 2 x 3 = 6 cycles per access, making it more than twice as fast the external RAM I put on my external cart. Sadly, 128KB is also not that much in PamOS 5's terms. It does give me a place to put the framebuffer and kernel globals. Better than nothing I guess.
</p>
<h3>Pixter Memories</h3>
<p>Given the complete lack of an MMU or an MPU, how can we protect the PalmOS storage heap from unintended or accidental modification? There is no obvious way. It is not strictly mandatory, of course, but highly desired. An idea came suddenly, while brainstorming how to connect more RAM to the device. Recall those byte lane select lines I explained earlier. They are only meaningful for writes, since for reads, the SoC can just ignore data it does not need. But what do memory chips <em>actually</em> do with those lines on reads? Turns out that they do not ignore them, they use them to mask output lanes. This means that a 16-bit-wide RAM can be used as an 8-bit-wide-ram of double the size by connecting its lower 8 data lines to the higher 8 data lines, connecting an unused address line to byte lane select, and the same address line through an inverter to another byte lane select. Think about it (or look at the schematic below).
</p>
<p>This scheme can be expanded further to use a 2-to-4 decoder to connect two x16 RAMS as a x8 RAM with 4x the size. Why am I telling you this? Because the largest PSRAM that could be located for this project was 16x4M, meaning that each chip of it has 4mega words of 16 bit-wide memory (8MB). Two such chips would make 16MB of memory, which is as much as Pixter's 24 external address lines would allow addressing. The 2-to-4 decoder would make this possible. Now, back to protection. Say, we decide up front to use the first 1/4 of the external ram as the dynamic heap, and the last 3/4 as the storage heap. The logical OR of the top address bits would be one for any storage access and zero for dynamic memory access. Add one more gate and a GPIO pin, and we have ability to ignore write to the storage area by blocking the "write enable" signal. Now, this will not <em>tell us</em> that an access was blocked - the Pixter Cart slot lacks an ability for us to send back an error to the SoC, but at least the erroneous write would be ignored. This scheme was implemented, tested, and found working! Cool!
</p>
<p>Why was PSRAM used? Pixter Color's SoC lacks any support for dynamic memory, which is what we use nowadays. Real SRAM (static memory), does not come in megabyte sizes, at least not on the budget I had in mind. PSRAM is a nice middle ground. It is dynamic memory with internal mechanisms to refresh itself. Externally it pretends to be static memory. It is not as cheap as dynamic memory, but when you need huge SRAMs, PSRAM might be all you can realistically get.
</p>
<p><a href="https://dmitry.gr/images/rePalm-PixterColorWithCartsLarge.jpg"><img src="https://dmitry.gr/images/rePalm-PixterColorWithCartsSmall.jpg" alt="Pixter Color showing rePalm boot screen surrounded by 8 rePalm Pixter Color cartridges"></a></p><p>The first revision boards had just 1MB of flash, as I had mentioned. This is rather little to squeeze in a full PalmOS 5 image. I did manage, with a lot of effort, but it was tight and I had to make some tough decisions and even rewrite one library in assembly to save ten bytes! Needless to say, revision 2 boards featured a much more roomy 8MB flash chip. This allowed for inclusion of all the standard PalmOS PIM apps as well as some games and utilities. There is even 2MB still free in ROM. The only issue was that this part was not stocked by JLC, forcing me to order it separately, and wait for them to receive it before they could assemble the boards for me. As the PSRAM and the Flash are both BGA-packaged chips, assembling at home was a non-starter.
</p>
<h3>Pixter Displays</h3>
<p>The first Pixter Color I got my hands on (and, really, most of the Pixter Color devices produced) featured an STN color display of such poor quality, that I struggled to call it "color". If you recall color laptop displays from the early 1990s, you can imagine this one too. The colors shifted with the slightest head movement, and the contrast slider allowed free adjustment from "muddy washed out dark browns" to "muddy washed out light greys" without any good middle "passable colors" state. Well, you play with what you have. STN displays need their controller to work hard to show gradations of color. This is done by temporal dithering (quickly alternating a pixel between on and off to create the illusion of a middle state). The ditherrer in the SoC allowed 15 brightness values per color channel. Yes, not 16. Indeed there are 16 values, but the middle two produce the same brightness, as is clearly documented in the SoC's user guide. This means that with this SoC, this display could display 15 x 15 x 15 = 3375 colors. 
</p>
<p>The display controller supports direct color mode, but sadly not in the normal RGB565 mode, but in the who-the-hell-asked-for-this XRGB1555 mode which PalmOS (and literally every other piece of software to ever use 16-bits-per-pixel displays) has no use for. Oh well, not like this display could display enough colors to make the 16-bits-per-pixel mode worth it. I decided to just support the 1, 2, and 4 bits-per-pixel greyscale modes and the 8-bits-per-pixel paleted color mode. This should be enough to run most PalmOS 5 software and, given the shittiness of this device, one should grade on a curve! When PalmOS sets a palete entry, I pick the closest of the 3375 colors to the requested RGB888 triple. 
</p>
<h3>Making Pixter IrDA work</h3>
<p><a href="https://dmitry.gr/images/rePalm-PixterColorIrDA.jpg"><img src="https://dmitry.gr/images/rePalm-PixterColorIrDA.jpg" alt="Pixter Color recieving a hand-drawn note over InfraRed"></a></p><p>Most SoCs' UARTs support IrDA SIR modulation, allowing one to simply connect an IrDA transceiver to the pins and immediately send and receive bytes via InfraRed. Of course the minimum-spec SoC in the Pixter Color does not have this option. I bet they saved a whole 0.0001 square millimeters of silicon by not having this option, the stingy bastards! I wanted InfraRed to work, though. There are chips that simply convert normal serial port signals to IrDA SIR modulation and back. This would be the simple solution, but due to how they work, they also need a stable clock input at the precise rate of 16x the current baudrate. As making IrDA work properly requires ability to negotiate a variable baudrate between 9600bps and 115,200bps, this means I'd need ability to drive out a stable variable clock on a cartridge pin. While this SoC can output a given clock, none of the pins capable of it connect to the cartridge slot. No, this approach would not work. What alternatives are there?
</p>
<p>Well, I did say that most SoCs' UARTs support IrDA SIR modulation. This is also true of small cheap microcontrollers, and even chinese clones of small cheap microcontrollers. Thus, the new plan was to use a simple microcontroller to talk IrDA to an InfraRed transceiver and normal serial port protocol to the SoC, over the cart slot. Luckily, among the various pins connected to the cart slot, there are two complete serial ports available for functional assignment to some of the pins. Score! One can be used for serial debugging and the other -- for this. A thought occurs, however. We need to not only send data to and from this microcontroller, but also control signals, eg to tell it to adjust the baudrate, or to update its firmware. This means that we need to talk to it at a higher rate than InfraRed ever would use, to provide for the extra overhead of whatever protocol I invent to make this all work. I decided on 2x the max IrDA rate - 230,400bps. The microcontroller chosen was the very cheap <a href="https://global.geehy.com/product/fifth/APM32F003">APM32F003F6U6</a> from Geehy. It had two serial ports with IrDA abilities, could be clocked from an external oscillator at a frequency quite amenable to generating UART clocks (11.0592 MHz), and was available as a stock part at JLCPCB. I figured that it was just like any other cheap Cortex-M0 and I would be able to find a common language with it. This turned out to be true, and it took only an hour to get CortexProg to program it.
</p>
<p>Getting this microcontroller to do UART was harder. The documentation was rather sparse, and I searched in vain for any way to assign a given pin to be a GPIO or a function pin. This is typical in most microcontrollers, including other families of MCUs from Geehy. But not this one, evidently. Eventually, I figured out that if you enable a peripheral, it simply takes over the requisite pins on this chip. This, however, did not explain why I could get UART1 working, but not UART3. Eventually, I realized that while UART1 had simply an enable bit, UART2 has <em>that</em> plus an extra "ENABLE" register which needs to be set to enable it, while UART3 has <em>that</em> plus an extra-extra "IO ENABLE" register that also needs to be set. Docs were not at all clear about this. This got me to another impasse. UART3 receive worked fine, but transmit did not, pin just sat at zero volts. It is, of course, at this point that I noted that the pin that UART3 used for TX is a hardware open-collector pin, meaning that It simply cannot source any current, only sink it. In human terms, this means: it needs a pull-up resistor to be of any use at all whatsoever. So, I enabled the pull-up on the Pixter Color's SoC side of that wire, and I had bidirectional communication!
</p>
<p>Designing protocols over UARTs is a bit of a pain. Almost any noise on an otherwise-idle line will turn into a <span>0xFF</span> byte. Any character can be lost to a framing error if noise causes its stop bit to appear low. And any character can be corrupted by noise during its data bits. Parity can be used to add some resilience to this, allowing, at least, likely detection of corrupted bytes. But parity support is not always present and does not always work. Since any byte can also go missing, how does one design a resilient protocol? If you send a length byte, and it gets corrupted, the reciever might be waiting for a lot more data than you intend to send, and thus get stuck. Conversely, the reciver might think the packet ended sooner than it really did and interpret the next byte of data at a packet header -- not good. Many ways can be invented to resolve this. A typical one is to simply somehow mark "start of packet" allowing the reciever to resynchronize in case of a sync-loss. A special byte can be used, but then that byte is not allowed in the packet contents. It must be escaped somehow. And what if the packet being sent happens to be made of just that byte? Escaping it might blow the packet size up by a factor of two. Another common method is to use the UART in 9-bit-mode, and just use the top (8th) bit as a "start of packet" marker. This has the benefit of not needing any escaping. The issue is that 9-bit-character support is not uniform among all the UARTs our there. Pixter SoC's UARTs, for example, do not support this. Not good. A third method is using a <span>BREAK</span>. This is when the data line for the UART is low for a full character length, including the stop bit. Most UARTs support recieving this and noting it as such. Sending it is a bit harder. Some UARTs, like the one in the APM32F003F6U6, can send a proper-length break simply by setting a bit and waiting for it to self-clear. This is not common. Most commonly, there is simply a "SEND BREAK" bit that lowers the TX line, and it is up to you to make sure you keep it low long enough. Annoying. This is what the Pixter SoC can do. At least this is what it advertises being able to do. In reality, I found that it worked unreliably. Sometimes using this feature would place the UART into a weird state where it could not transmit again. I found a workaround: I can reconfigure the TX pin as a GPIO and literally just take it low, wait, then reconfigure it back. The UART unit need not even know, and it does not get wedged! Win!
</p>
<p>The protocol I designed is simple but not symmetric, since while Pixter might have a lot of control data to send to the microcontroler (configuration, updates), the microcontroller rarely has much to say to the Pixter other than what InfraRed data it got. From microcontroller to Pixter, it is as follows: any byte received is an InfraRed data byte, unless preceded by a <span>BREAK</span>. If it was, the top 2 bits determine what it is. 00 means that it is a start byte of a longer packet, the lower 6 bits give the packet type. Each packet type has a fixed length. 01 means that it is a lower nibble of a non-first byte of a longer packet, bits 4 and 5 must be zero. 10 means that it is a higher nibble of a non-first byte of a longer packet, bits 4 and 5 must be zero. 11 means it is a one-byte control packet. You can see that "longer packets" thus get blown up in size by a factor of 4. This is fine since this is rare, the only such packet defined is the "version info packet". Actual IrDA data arriving can interrupt transmission of such packets, since any byte not preceded by a <span>BREAK</span> is treated entirely differently. The one-byte control packets allow for flow control. This is needed since this interface is at 230,400bps while IrDA is at 115,200 max. Pushback ability is needed from the microcontroller to PalmOS to keep it from overflowing the microcontroller's TX buffer. This range is also used to signal various framing errors in received IrDA data. For more details you can see "pixterComms.h".
</p>
<p>The protocol from the Pixter to the MCU is different. Here, a <span>BREAK</span> is sent before the start of a packet. Then comes a byte that describes the packet. The top 2 bits determine packet type. 00 - simple command where bits 0..5 determine command type, each has a fixed length. Examples are: "get version info", "reset", "set IrDA config". 01 - IrDA data. Bits 0..5 give data length minus one. That many bytes of data to send follow. 10 - firmware update data. Same length encoding as for IrDA data. 11 - reserved for future use. Firmware data is further decoded based on the first few bytes. Again, for details see "pixterComms.h". When PalmOS starts trying to send IrDA data, a packet is sent off to the microcontroller right away, no waiting. This means that usually it just contains one byte of data. By the time it is sent, PalmOS might have added 5 or 6 bytes more to the TX buffer, and those are sent in a longer packet, by the time that is sent, much more data has been added to the TX buffer, and maximum-length packets can be sent to the MCU. Keep in mind also that Pixter-to-MCU comms are <em>at least</em> 2x as fast as IrDA comms are, which helps here. This design minimizes delays to <em>start</em> getting the data out. This matters since IrDA protocol timeouts give a limited amount of time to <em>START</em> recieving data, with more time available once the data starts coming in.
</p>
<p><a href="https://dmitry.gr/images/rePalm-PixterInfraRedDebugLarge.png"><img src="https://dmitry.gr/images/rePalm-PixterInfraRedDebugSmall.png" alt="A screenshot of Saleae Logic software debugging InfraRed communications with two lines depicting UART traffic and two depicting IrDA traffic, one inverted"></a></p><p>Debugging IrDA was a <em>huge</em> pain in the posterior, exacerbated by the fact that there exist no good working IrDA SIR decoders for <a href="https://saleae.com/">Saleae Logic</a>. Without my Logic 16 PRO and various analyzers, I'd be very lost. Seriously, this thing is a huge force multiplier, if you do not have one, you are developing on hard mode for no reason. I do not get paid to say this, I just really love this thing! In any case, since there was no analyzer for IrDA SIR, I wrote one. It properly decodes all bit lengths, parity settings, marks start and stop bits, shows errors, and supports both inverted (RX) and normal (TX) signaling. Most importantly, it allowed me to debug a few issues I had caused. As I <a href="https://dmitry.gr/?r=05.Projects&amp;proj=35.%20Linux4004">had done in the past</a>, I sent my analyzer's source to the good people at Saleae for consideration for inclusion in the Logic software, so that no others will ever need to suffer the indignity of decoding IrDA SIR one bit at a time by hand.
</p>
<p>The practical upshot of all of this is that it all works! IrDA communications work. MCU firmware updates also work. For that last one to work, there is a tiny (400 bytes) bootloader in the MCU that copies an uploaded validated image to the main flash area on boot if the version field differs. If the image was not fully uploaded, it will not be seen as valid. If the copy is interrupted, it'll resume on next boot. There is way to brick the MCU as long as the bootloader is not touched!
</p>
<p>There was one more thing the MCU needed to do. There is a pin on the cart slot that needs to be high for the Pixter Color to believe that a cart is inserted. After this check, the pin is usable for ... whatever. I ended up not using it for anything, but it is wired to the MCU. This does mean that soon after boot, the MCU needs to raise it and keep it high until rePalm takes over from Pixter's OS. It does this. Without this code, Pixter Color will boot-loop as long as the cart is inserted, neither booting nor giving up, forever. Curiously, Pixter Multimedia does not care about this pin and never checks it.
</p>
<h3>Getting and Flashing the Pixter Carts</h3>
<p><a href="https://dmitry.gr/images/rePalm-PixterColor-CartSchem.pdf"><img src="https://dmitry.gr/images/rePalm-PixterColor-CartSchemSmall.png" alt="Pixter Color cart Schematic"></a></p><p>Since the cartridge boards feature a parallel 16-bit-wide NOR flash, I needed some way to program them initially. <a href="https://github.com/GeorgeRudolf">George</a> designed and JLCPCB manufactured a flasher board for me, based on the wonderful <a href="https://www.raspberrypi.com/products/rp2350/">RP2350</a>, which is pretty much the best microcontroller you can get today (not merely an opinion, a true fact, fight me!). This board also has a cart slot similar to the one in Pixter, the <em>VERY</em> not cheap <a href="https://www.digikey.com/en/products/detail/edac-inc/302-060-221-201/21847827">302-060-221-201</a>. I use this to program each Pixter Color cart once. I then use CortexProg to program the microcontroller. After this, self-firmware-update from inside PalmOS can be used for flashing, as long as you do not accidentally flash a broken image!
</p>
<p>I wrote a PalmOS updater that loads the update (<span>/ROM.BIN</span>) from an SD card into RAM and then disables interrupts (since various drivers might be part of the OS image which we are about to slowly partially erase and overwrite), and then flashes the NOR flash with the new image. Before doing this, it also updates the MCU firmware (<span>/FIRMWARE.BIN</span>), if the replacement firmware has a higher version number. The entire process takes slightly more than four minutes, making it much faster than manual flashing with the flashing tool described above. Also, this brings updates to the users of these carts who do not have a flashing tool I described above.
</p>
<p>Did I say <em>users</em>? Yes! Fifteen of these were manufactured for those who wanted them and are now with their happy users. The cost to manufacture them ended up being around $50 each, making them a bit more expensive than a used Pixter Color on eBay. There is a chance that I'll run another production run, so if you want one, <a href="mailto:pixtercarts@dmitry.gr">email me</a>. Alternatively, you can have your own boards made and assembled using <a href="https://dmitry.gr/images/rePalm-PixterColorCartGerbers.zip">these files</a>. Board thickness should be 1.2mm. Initial flashing is left as an exercise to the reader.
</p>
<h3>Pixter Polish</h3>
<p><a href="https://dmitry.gr/images/rePalm-PixterColorPalmkedexLarge.jpg"><img src="https://dmitry.gr/images/rePalm-PixterColorPalmkedexSmall.jpg" alt="Pixter Color running Palmkedex showing Marill's page"></a></p><p>Now that basic PalmOS 5 worked (slowly), it was time for some polish. First of all, those buttons below the screen initially did nothing. But why not make them do something? The first one looked like a pencil. I wired it up to send a special unused keycode, and then wrote a tiny hack called <span>PixterEnabler</span> that catches this key and toggles onscreen writing. Since there was no documented API to control on-screen writing, I had to reverse-engineer the <span>GrafitiAnywhere</span> module. While doing that, I found that it had an unused-ever-before capability to change the ink color. I went with bright green.
</p>
<p>The third from the right button was used in Pixter's native OS to bring up settings, which include contrast adjustment. I wired this up to bring up the PalmOS contrast adjustment dialog. Reverse engineering how Pixter Color controls display contrast took some work. It is weird. It uses an R-2R resistor ladder and 4 GPIO pins to create one of 16 voltage levels that are then fed as an input to the display driver. Figuring it out took a while, wiring it up to PalmOS took all of a few seconds. Cool! This would do for now. More later.
</p>
<p>Pixter Color's CPU is simply not fast enough to do sampled audio playback. Lacking a real codec with a DAC, one would have to use the PWM unit, and take an interrupt every sample to reset it to a new value. Given the slowness of the CPU and memory subsystem, this would not work. I did try it. 44,100Hz uncompressed WAV playback used about 98% of the CPU cycles. This means that games with audio would be too slow to play and realtime MP3 decoding is a fevered dream of a madman. Given this, I decided to instead support the "simple sound" API of PalmOS. You may know it as "the beeps and the boops" that the earlier devices used. This can be done by simply programming the PWM unit once as "tone start" and again at "tone end". This allows for simple tunes, alarm sounds, and UI clicks to work. Good enough.
</p>
<p>Pixter devices also have an internal melody chip, as <a href="https://dmitry.gr/?r=05.Projects&amp;proj=37.%20Pixter">my main Pixter article mentions</a>. I thought that it would be cool to allow starting and stopping melody playback from PalmOS. The timing on the control interface is rather tight, forcing me to write the code in ARM assembly and use rePalm-specific high-resolution timer API. Nonetheless, it worked and you can indeed start and stop melody playback using the <span>PixterMelodyCtl</span> app. Since the playback is entirely independent of the OS, it will continue until stopped, including across firmware updates. I did code <span>PixterMelodyCtl</span> to send the "stop melody" command on PalmOS reset, so that at least it would stop on reboot. A video of this is in the rePalm photos album linked-to above.
</p>
<p>Pixter Color actually has one physical button. It is the pinhole on the back that the native Pixter OS uses to cause pen recalibration. This makes sense since a messed-up calibration would make tapping on-screen buttons impossible, so a real button is needed. I wired this up in PalmOS as hard button #1, and it can be mapped to any application using the usual Buttons Prefs Panel. I considered wiring this up as a soft reset button, as it is reminiscent of those, but the device has a perfectly working power switch on the side, toggling which causes a perfectly good reset. Actually making this button work was nontrivial. You see, it is not wired to any pin that can cause an interrupt to the CPU. Instead, in the timer-overflow handler which runs in FIQ mode (for speed) at around 120Hz, I check its state, do some quick debouncing, and if it changed state, enqueue a normal low-priority interrupt that will later be handled to deal with it. The same check-and-debounce-in-periodic-FIQ method is used to detect SD card insert/remove, for the same reason.
</p>
<p>There is, of course, no SDIO support in Pixter Color's SoC. There is SPI support, but none of the pins available on the cart slot are connected to the SPI unit in the SoC, so that would be of no use either. I ended up bit-banging the SPI interface for SD card support in assembly. You'll recall that the CPU in Pixter Color is super slow, and so is the memory. I spent a little bit of my fast TCM to keep these SPI bit-banging routines fast. The final result is that my code reaches access speeds around 3.8Mbit/s, which is not all that terrible. Of course, this uses the CPU so nothing else can really transpire while this goes on. Oh well. It does work, allowing backups to card and loading games from card!
</p>
<h3>Battery State</h3>
<p><a href="https://dmitry.gr/images/rePalm-PixterColorBatteryVoltageLarge.jpg"><img src="https://dmitry.gr/images/rePalm-PixterColorBatteryVoltageSmall.jpg" alt="Pixter Color showing a battery voltage of 5.34V at a charge state of 73%"></a></p><p>Luckily, converting a voltage to an approximate state of charge for alkaline batteries is trivial. Once I figured out how the battery voltage was hooked up to the SoC's ADC and at what scale (0.25, evidently), I was able to measure battery voltage. A conversion of battery voltage occurs at every pen down, pen movement, or every 500ms. These values are smoothed and converted to a percentage that is properly handed to PalmOS. Curiously, in PalmOS 5, there is no official or even unofficial API to get battery voltage, only percent charge. This is actually not unreasonable, since battery technologies evolve and user-level applications have no business trying to understand voltages. Current battery state of charge is enough for applications. That being said, in PalmOS 4, there was such an API. In PalmOS 5, for compatibility it still exists, but in a fake way. It will read the current state of charge and map it linearly onto 3.7V - 4.2V range. I decided that it would be hilarious to expose the true battery voltage to applications that ask for it, so I added a small hack in my DAL to do so. Now applications using PalmOS 4 APIs can query and properly display the true battery voltage. The reason this is funny is because Pixter runs on 4 series-connected AA batteries, which means it'll see around 6V when full. No Palm OS device before had ever run on such a high battery voltage and I was curious what applications would do with this, and whether anything would break. Nothing did.
</p>
<h3>ARM7 quirks</h3>
<p>The ARM7 core used by Pixter's SoC implements ARM architecture version 4T, which is, in theory, good enough for PalmOS 5.x. You could have guessed this based on the whole story above - I got it to work afterall, right? Well, PalmOS ran on a number of ARMv4T processors, but all of them were ARM9 CPU or later. ARM7 CPU design is a bit older and a bit slower, which is not a disaster and you are probably tired of hearing about the slowness already, but it has a few other quirks which would turn out to become quite a pain when it came time to run my favourite PalmOS game - <a href="https://palmdb.net/app/warfare-inc">Warfare, Inc.</a>.
</p>
<p>As mentioned elsewhere in this increasingly long article, ARMv4T processors can execute instructions in one of two formats. ARM instructions are always 4 bytes long and occur only at memory addresses divisibly by 4 (this is called "self-aligned"). Thumb instructions are always 2 bytes long (do not believe anyone who tells you that the <span>BL</span> instruction is 4 bytes long, in ARMv4T, <span>BL</span> is actually two instructions, each of which can be executed independently and each is two bytes long), occurring at memory locations divisible by 2 (also self-aligned). These instructions cannot be freely mixed, since the CPU would not know how to interpret the next bytes. Instead, the CPU has an internal method to track which instruction set mode it is in (bit 5 in <span>CPSR</span>, if you are curious). There are a few ways to switch this mode. In ARMv4T, there are precisely two ways. One of them is returning from an exception. This is only used by the OS kernel and not by any normal user code. The second is the <span>BX</span> (branch and exchange) instruction. This instruction takes a register as a parameter and jumps to the address it contains. Since both ARM and Thumb instructions occur at even addresses, the lowest bit of the address register is by-definition not meaningful. The CPU uses that bit to decide what mode to switch to - ARM if it is zero, Thumb if it is one. Good so far. Let us analyze all 4 possible cases of the lower 2 bits of the register passed to <span>BX</span>. "01" and "11" are both valid options, both go to Thumb code either at an address that is even but not divisible by 4, or to an address that is divisible by 4. "00" is also a valid option. This will go to ARM code at an address divisible by 4, as ARM instructions ought to be. Quite clear. It is the last case -- the "10" case that is of most interest to us.
</p>
<p>ARM architecture reference manual says "If Rm[1:0] == 0b10, the result is UNPREDICTABLE, as branches to non word-aligned addresses are impossible in ARM state." OK, fine. Most often <span>BX</span> is used to return from functions. Clearly the return address should always be valid and this case should not come up. The second-most common use case of <span>BX</span> is to call a function via a function pointer. This should also only use valid pointers with proper alignment and nothing should ever be the matter. Fine. But, there is a third case. Say you are executing in Thumb mode, but wish to call an ARM function. You cannot directly <span>BL</span> to it, since that will leave you in Thumb mode. You could calculate its address and <span>BX</span> to the register containing it, but this is a lot of cycles. There is a third method, and a common one. You <span>BL</span> to a tiny thunk containing a single Thumb instruction: <span>BX PC</span>. Since when it is read, PC never has the low bit set, and since in Thumb mode it reads as the address of the current instruction plus 4, this will execute a <span>BX</span> with a value with the lowest bit clear and the rest of the bits pointing 4 bytes past this instruction's start (2 bytes past its end). This will cause a switch to ARM mode and continuation of execution at that address in ARM mode. There, one places an ARM <span>B</span> instruction to jump to the desired function. When that function returns (using <span>BX LR</span>), it will jump back to Thumb mode at the call site just past the <span>BL</span>, since the <span>BL</span> had set up the <span>LR</span> register thusly, as is its job. Did you spot a potential issue?
</p>
<p>This will all work wonderfully as long as the <span>BX PC</span> instruction is at an address divisible by 4. If it is not, we end up with the above-mentioned "10" case which is, I quote again "UNPREDICTABLE". Does the ARM ARM tell us anything more about this precise case? It does (in the section on the <span>BX</span> instruction)! "Register 15 can be specified for &lt;Rm&gt;. If this is done, R15 is read as normal for Thumb code, that is, it is the address of the BX instruction itself plus 4. If the BX instruction is at a word-aligned address, this results in a branch to the next word, executing in ARM state. However, if the BX instruction is not at a word-aligned address, this means that the results of the instruction are UNPREDICTABLE (because the value read for R15 has bits[1:0]==0b10)." Well, that is pretty clear, this case is unpredictable and nobody should do this. Fine!
</p>
<p>The issue is, some PalmOS games that were compiled with an antique version of ARM gcc <em>DO</em> do this. I ran into this while writing the main article on the project, and <a href="#bxpc">mentioned the special handling I had to do for it</a>. Somehow, this never broke on any PalmOS 5 device. What gives? It turns out that on ARMv5 and later, whenever the CPU is in ARM mode, the lower 2 bits of <span>PC</span> are forced to zero immediately on any write. So the <span>BX PC</span> at an address that is not divisible by 4 will simply jump to an address 2 less, which is divisible by 4. This seems to be what the old ARM gcc version expected and relied on. However, PalmOS 5 ran on ARMv4T as well. How did it ever work there? Well, it seems that ARM9 CPUs do the same thing. All PalmOS 5 devices on ARMv4T CPUs used ARM9 cores. No PalmOS 5 device ever ran on an ARM7 core. <b>I made the first one!</b> So, what does ARM7 do in this case?
</p>
<h4>What exactly does ARM7 do with PC[1] in ARM mode?</h4>
<p>This investigation took quite a bit of time, since I wanted to make sure I understood the behaviour entirely so that I could emulate it properly in <a href="https://github.com/uARM-Palm/uARM">uARM</a> for simplified debugging in the future. I found no information on this anywhere, so this might be the first documentation on the subject. ARM7 CPUs do not force <span>PC</span>[bit 1] to 0 when PC is written. You can write <span>PC</span> using any method you choose with that bit set, and nothing bad will befall you ... at least not immediately. Instruction fetches in ARM mode do not send <span>PC</span>[bits 0..1] on the bus, so instructions will continue to be fetched and execute as expected. If an exception is taken, the value of <span>PC</span> seen by the exception handler will reflect the true value of <span>PC</span>[bit 1], and a return from exception will properly restore it. The value of <span>PC</span>[bit 1] will survive a function call and return as well, causing no ill effects. Reading <span>PC</span> directly will also show the true value of <span>PC</span>[bit 1]. This is where you're likely to hit your first problem. You see, ARM instructions make it rather difficult to load large immediate values into registers, so it is common to load them from a "literal pool" - literally a set of word-sized constants at the end of the current function. Such a load usually takes the form of a PC-relative load instruction, like this: <span>LDR Rx, [PC, #0x124]</span>. Since <span>PC</span> is expected to always be word-aligned, the offsets used also are, producing a word-aligned address whence a word will be loaded. What happens if our <span>PC</span>[bit 1] is set? The produced address will not be word aligned. What happens then? If your CPU has alignment checking enabled, you take an exception due to a misaligned load. And what if your CPU, like the one in Pixter Color's SoC, has no alignment checking ability, or if you simply turned alignment checking off? ARM ARM quoth: "Load single word ARM instructions are architecturally defined to rotate right the word-aligned data transferred by a non word-aligned address one, two or three bytes depending on the value of the two least significant address bits." So, you'll simply load the immediate value you intended to load, except rotated right by 16 bits (swapping the lower and the upper halfwords). I'll let you imagine the havoc that doing this to all constants would cause.
</p>
<p>Curiously, there is another place this can cause issues. A typical way to call an OS kernel is a <span>SWI</span> instruction, which, in ARM mode, encodes a 24-bit immediate in its lower 24 bits. A kernel would usually read the immediate to figure out what the requested syscall number is. Since in the exception handler, <span>LR</span> is expected to point just past the <span>SWI</span> instruction, a typical way to get this immediate is <span>LDR R0, [LR, #-4];  BIC R0, #0xFF000000</span>. See the issue here? If <span>PC</span> was misaligned, your kernel would have just taken an alignment fault, or (if alignment checking is off) simply read the wrong value. A kernel aware of this quirk would instead do something like this: <span>BIC R0, LR, #3;  LDR R0, [R0, #-4];  BIC R0, #0xFF000000</span>. Fun story: <a href="https://elixir.bootlin.com/linux/v6.18/source/arch/arm/kernel/entry-common.S#L199">Looking at what Linux does</a>, it looks like a <b>possible user-space DoS on Linux in just two instructions</b>. Would that be a record? If the kernel was configured to support OABI (exclusively or together with EABI), the following two-instr binary will simply crash the kernel if the core has alignment checking: <span>SUB PC, PC, #2;  SWI 0</span>. I am not sure how common such configs are, but someone should maybe fix that? 
</p>
<p>But OK, back to my favourite game. Since ARM code execution is unimpeded by <span>PC</span>[bit 1], the faulty code crashes after an arbitrary delay following <span>PC</span>[bit 1] being set, or maybe does not crash at all, but malfunctions. If I had alignment checking, I could detect the most likely cause of crash (unaligned literal load) and fix it. Lacking that, what could I do? I decided on a complex, partial, and heuristic-full solution. To call into ARM-native code, PalmOS applications use an OsCall called <span>PceNativeCall</span>. It gets a function pointer to jump to in native ARM mode, and a parameter to pass to the code. I patched this function with my own wrapper that does the following: First, determine which memory heap the code pointer is in. Second, manually walk the heap structures to find which memory chunk the pointer is in. Third, assume that the entire memory chunk is ARM code and apply the heuristic to it. The heuristic produces no false positives or negatives across all the games I tested, so I am satisfied with it. It is this: (1) A valid thumb <span>BL</span> at a proper 2-byte boundary pointing to somewhere inside the chunk at a 2 but not 4 byte boundary, (2)  A <span>BX PC</span> at that location, (3) The <span>BX PC</span> is followed by a valid ARM <span>B</span> with a target somewhere inside the chunk, and (4) The target instruction is unconditional, making it a likely first instruction in a valid function.
</p>
<p>OK, so, say I find the problematic <span>BX PC</span>. What now? It is not like I can fix it. To fix it requires two bytes of extra space that I do not have, and editing of all the callsites. Instead, I simply replace the <span>BX PC</span> with an invalid instruction in a special format. My kernel has a handler for the invalid instruction trap that checks for Thumb-mode execution of that exact instruction. It will <em>correctly</em> adjust <span>PC</span> and return in ARM mode to the ARM <span>B</span> instruction, allowing it to continue with <span>PC</span>[bit 1] properly cleared. This does mean that (1) I am editing the game binary in RAM and some game might detect this and get upset, and (2) depending on how often this callsite is called, a whole lot of exceptions might be being taken, costing a lot of performance.  The first case is simple - seemingly no games get upset because usually they do self-checking before calling the code. The second case is addressed by making the handler as simple and light as possible, minimizing the penalty. This is the best I can do, and it works! Since the issue is found in the ARM7TDMI core, I named by hack to work around it <span>LEG7IMDT</span>, of course.
</p>
<h3>Pixter Multimedia</h3>
<p>The last generation of Pixter was the "Pixter Multimedia". This one was even fancier - it had some buttons (a directional pad and A/B buttons) and a better SoC: <a href="https://web.archive.org/web/20240812134113/https://www.nxp.com/docs/en/data-sheet/LH79524_525_N.pdf">Sharp LH79524</a>. It also supported some fancier multimedia game carts, some featuring rudimentary video playback. Inside, it sported a real DAC (<href="https: web.archive.org="" web="" 20170305140713="" http:="" www.ti.com="" general="" docs="" lit="" getliterature.tsp?genericpartnumber="tlv320dac26&amp;fileType=pdf&quot;">TLV320DAC26), connected by I2S to the SoC. There was also 4 whole megabytes of SDRAM in the device. Up front, and most noticeable of all, was the TFT screen. Still 160x160 but now with a better contrast ratio than the Pixter Color's STN screen's 1:2. Of course I wanted to support this one as well. What would that take?
</href="https:></p>
<p>The SoC uses the same ARM7 core, but now in much better configuration: it now had an MMU and 8KB of cache. The TCM is gone, however. This is a worthy trade. With an MMU, a number of things get better: NULL pointers can be caught, real memory protection is possible for the storage heap, and a simpler solution to the ARM7 quirks might be possible instead of <span>LEG7IMDT</span>. With a cache, much of the memory latency can be hidden for tasks with a small working set. Overall this device performs <em>significantly</em> better!
</p>
<p>Audio support was actually rather simple. Once I figured out how the SPI interface of the codec was wired to the SoC (it was bit-banged using some GPIOs), it was simply a matter of configuring the DMA for the data and configuring the DAC for the proper sample rate. I made it build-time-configurable in the source, but settled on 44.1KHz - a perfectly good sample rate. The codec supports driving a single speaker (as is present in the Pixter Multimedia) or a set of headphones in glorious full stereo. As I designed rePalm to make supporting new hardware easy, it took only a few hours of work to hook up audio support and hear it work. This device is fast enough to play uncompressed audio and even do so while a game is running, making playing Warfare, Inc even more fun, with the units calling out "on my way, sir!" when you direct them somewhere. Same as in Pixter Color, I hooked up the battery sense to the OS (here the scale was 0.27). There is also a volume knob on the side of Pixter Multimedia. As the DAC has no analog "gain" input, I was not quite sure where it could possibly be hooked up to work. The mystery was solved after some investigation. It is an analog input to the SoC's ADC, nothing more. It is up to software to do anything with this information. I decided to save this for later, but maybe I'll convert it to a jog-wheel-like thing. Anyways, simple game soundbites and uncompressed audio were not the extent of my aspirations -- I wanted real MP3 playback from SD card to work!
</p>
<p>Everything I said about SD card support on Pixter Color still held here - I was bit-banging SPI to talk to the card. The SoC in Pixter Multimedia had a different clock tree, and I played around with a lot of options, finally settling on a rather significant CPU overclock of 102MHz (documented max is 75MHz) while keeping the AHB speed at 51MHz. This provided stability and just barely enough cycles to decode mono 96Kbps MP3s. Higher clock rates allow higher quality music, but not all tested Pixter Multimedia units could clock higher than 110MHz.
</p>
<p>Pixter Multimedia display proved to be a pain point, however. It is indeed 160x160, but for some reason stock Pixter software was configuring it for 162x160. It took me very little time to figure out why - the display eats the first two columns of data. This is despite any configuration change. It does not mater if you adjust the HBP or HFP or HSYNC length. Unfortunately, losing the <em>FIRST</em> pixels of a row is very very bad for us! Why? Many parts of PalmOS, assume that every display line begins at a 2-byte boundary. My blitter does as well, for efficiency. There is no assumption that every line follows the previous one in memory, so in theory we can simply have a 160x160 display with a 162x160 framebuffer in memory, and claim that the framebuffer starts 2 pixels in. Will it work? Let's math! SoC hardware forces the display data to start at a 4-byte boundary. At 16bpp, two pixels are 4 bytes, so an address two bytes into a line is 4-byte aligned -- a superset of being 2 bytes aligned. Good. At 8bpp, two pixels are 2 bytes, so an address two bytes into a line is 2-byte aligned - good enough. Things begin to fall apart at 4bpp and below. At 4 bpp, two pixels are a single byte and the blitter will be quite unhappy at a line not starting at a two byte boundary. At 2 and 1 bpp, the line does not even start at a byte boundary. No good! What could I do?
</p>
<p>Had I had no MMU, the game would be over right there, but I did have one! I decided to do the same thing I had <a href="#timeddisplay">done before</a> for another reason. The short of it is: create a fake framebuffer, aligned as the OS wants it. Protect it using the MMU. Anytime a write is attempted, take a fault, unprotect it, and start a 60Hz timer to convert the data to the proper format and alignment and transfer to the real framebuffer. After a few such copies, re-protect the original framebuffer and disable the timer. In turn, that allows for fast refreshes while drawing is ongoing and allows us to stop the CPU waste when this is no longer needed. This allows for 1/2/4bpp modes to work and only wastes CPU on drawing when actual drawing is ongoing. I wrote the transfer funcs in assembly for speed. This also allows us to use 16bpp mode. You'll recall that I mentioned that these Sharp SoCs use the idiotic XRGB1555 mode, while PalmOS needs and assumes the common-and-sane RGB565. Well, now that I had an ability to "convert" data on each draw, why not support 16bpp as well? I did and it is glorious! Photo-viewing apps worked now, even if only using 32768 colors
</p>
<p>As foreshadowed earlier, <span>LEG7IMDT</span> is not needed on Pixter Multimedia. Any sane code running with <span>PC</span>[bit 1] set would either run fine to completion or hit an alignment fault when it attempted to load an immediate from the literal pool. My alignment fault handler simply checks if the CPU was in ARM mode with <span>PC</span>[bit 1] set, clears it, and returns. If this fixes the issue - good. If not, we trap again and this time it is fatal since the <span>PC</span>[bit 1] being set was clearly not the issue. This is indeed simpler than walking memory heaps and patching random executables live.
</p>

<h3>Some More Pixter Polish</h3>
<p><a href="https://dmitry.gr/images/rePalm-PixterMultiWrongDeviceLarge.jpg"><img src="https://dmitry.gr/images/rePalm-PixterMultiWrongDeviceSmall.jpg" alt="Pixter Multimedia showing a simple white message on black background that it is the wrong device to boot this cart"></a></p><p>Since both Pixter Color and Pixter Multimedia use the same cart slot, the same cart can be used in both, hardware-wise. But since rePalm kernel builds rather differently for MMU and MMU-less systems, I did not want to try to make a universal build. Instead, you can use the self-update mechanism to flash one of the two images to switch between them. Of course, if you only have a Pixter Color, you would not want to flash the Pixter Multimedia image since you'd then be unable to boot to flash back. I did want to be a bit more user-friendly. Luckily, long ago I added a capability to run some code very early in rePalm boot. On Pixter, I used it to check the SoC type before boot. What good is that? If it does not match the current build, I can use rePalm's simple fixed-width character renderer on the framebuffer still enabled from Pixter's OS's boot and show a message. Here you can see what it looks like.
</p>
<p>At some point during the project, I saw a weird Pixter Color. It seemed to have a much better screen than others. It also did not boot my Pixter Color image. To be more precise, it booted fine, based on the serial console, but the display was off. Some investigation revealed that there was a small production run of Pixter Color device with the Pixter Multimedia's TFT screen. I changed my code to detect screen type (based on how Pixter's OS had set it up) and handle both. The good news is that the TFT display on Pixter Color can display the full 4096 color-palette that 12 bits per pixel would allow, rather than the 3375 colors the STN could. There was bad news too, though. Being the same display as Pixter Multimedia, it still ate the first two columns of pixels. Pixter Color had not yet sprouted an MMU so my old tricks would not work. Initially I simply disabled 1/2/4 bpp modes. This did not seem to break any applications, but it confused many since very few actually check for errors when they call <span>WinScreenMode</span> to set screen depth. I decided that a low-performing solution is better than one that confuses apps, so I added a 60Hz interrupt that copies the data in the proper format from a fake framebuffer to a real one. Basically, this is the same as what I did for Pixter Multimedia, but without the ability to stop doing it when the display stops being changed by the app. I'd estimate the performance cost of this to be around 20% of Pixter Color's CPU budget. Luckily, when running at 8bpp, this is not an issue. I then did the same thing to enable 16bpp on both the STN and TFT displays. The cost is immense (30% CPU on TFT, 46% on STN due to needing to apply STN correction curves). Due to this I have the device boot in 8bpp mode which has color and performs well, but if any app requests 16bpp, it is available. After some more thought about how cruel it is to steal 46% of an already slow CPU, I changed this to a 30Hz interrupt, halving the cost.
</p>
<p><a href="https://dmitry.gr/images/rePalm-PixterMultiButtonsPanelLarge.jpg"><img src="https://dmitry.gr/images/rePalm-PixterMultiButtonsPanelSmall.jpg" alt="Pixter Multimedia showing the 'Buttons' control panel with cons matchng Pixter's silkscreened buttons"></a></p><p>I wanted to make a good use of ALL the silkscreened buttons under the display, not just the three I had assigned before. I mapped them all to a purpose, and even took the time to draw pixel-perfect icons for them to integrate into the <span>Buttons</span> Prefs Panel on both devices. The mapping is the same on both devices, even though the button spacing is not the same and required individual silkscreen resource files. The first button toggles on-screen writing, the next 4 act like the normal application buttons on palm devices. The next one (that looks like an explosion) opens the menu. The one after that, which looks like a magic wand, opens the contrast adjustment dialog. Why? Original Pixter OS used it for that and I desired some consistency. The one after (folder) brings up the find dialog. And, of course, the home icon opens the app launcher. Overall sane, I think.
</p>

<h3>Pixter Results</h3>
<p>This is the first primary-battery-powered color PalmOS device. This is the first primary-battery-powered PalmOS 5 device. Pixter Color is also the worst-performing PalmOS device ever. But it does work... There are a lot of photos and videos in the rePalm photo album linked-to on top of this page.
</p>
<p>I did some benchmarks and found that Pixter Multimedia performs approximately on par with Palm Tungsten T. Pixer Color ... looks cute trying, but the benchmark results are comical -- it is 6% as fast as a T|T. But for basic PIM and many games this is plenty. Warfare Inc works! What more could you ask for? To download the latest update images, <a href="https://dmitry.gr/images/rePalm-PixterBinaries.zip">click here</a>. You can use them to flash boards you make or to update boards you got from me.
</p>


<h2>Nov 2, 2025 - summary of what you missed</h2>
<p>
I have made builds for Pimoroni Presto, the DEFCON32 badge, and worked on PalmCard - a replacement memory card for Palm Pilot classic that uses RP2040 to run rePalm and makes a terminal out of the Palm. Lately I've been working on supporting Fisher-Price Pixter Color. All of this can be seen in the photo album. Future updates will be more detailed, but I am too lazy to write about the last few years of development here since it really was mostly just new device support and bug fixes. Soeone who is not me also did some work on rePalm - there is now a working nintendo DS port. I helped only a little, most of the work was not mine, and this is awesome!
</p>
<h2>The original article about the start of the project</h2>
<h2>PalmOS Architecture (and a bit of history)</h2>
<h3>History</h3>
<p>PalmOS before 5.4 kept all data in RAM in databases. They came in two types: record databases (what you'd imagine it to be) and resource databases (similar to MacOS classic resources). Each database had a type and a creator ID, each a 32-bit integer, customarily with each 8-bit piece being an ascii char. Most commonly any application would create databases with their creator ID set to its. Certain types also had meaning, like for example <span>appl</span> was an appliction and <span>panl</span> was a preference panel.
</p>
<p>PalmOS started out on Motorola 68k processors and ran on them from first development all the way to version 4.x. For version 5, Palm Inc chose to switch to ARM processors, as they allowed a lot more speed (which is always a plus). But what to do about all the software? Lots of PalmOS apps were written for OS 4.x and compiled for m68k processor. Palm Inc introduced <span>PACE</span> - Palm Application Compatibility Extension. <span>PACE</span> intercepted the OsCall <span>SysAppLaunch</span> (and a number of others) and emulated m68k processor, allowing all the old software to run. When m68k apps called an OsCall, <span>PACE</span> would translate the parameters and call the ARM Native OsCall. This meant that while the app's logic was running in emulation, all OsCalls were native ARM and fast. Combine this with the fact that PalmOS 4.x devices usually ran at 33MHz, and PalmOS 5.x devices usually ran at hundreds, there was almost no slowdown, most old apps compiled for PalmOS 4.x ran at a perfectly good speed. It was even good enough for Palm Inc, since most built-in apps (like calendar and contacts were still m68k apps, not ARM). There was also PalmOS 6.x (Cobalt) but it never really saw the light of day and is beyond the scope of this document.
</p>
<p>Palm Inc never documented how to write full Native ARM applications on PalmOS 5.x. It as possible, but not documented. The best official way to get the full speed of the new ARM processors was to use the OsCall <span>PceNativeCall</span> to jump into a small bit of native ARM code that Palm Inc called "ARMlet"s and later "PNOlet"s. Palm said that only the hottest pieces of code should be treated this way, and it was rather hard to call OsCalls from these bits of native ARM code (you had to call back into <span>PACE</span>, which would marshal the parameters for the native API, and then call it. The ways to call the real Native OsCalls were also not documented.

</p>
<p>PalmOS 5.x kept a lot of the design of PalmOS 4.x, including the shared heap, lack of protected memory, and lack of proper documented multithreading. A new thing was that PalmOS 5.x supported loadable modules. In fact, every Native ARM application or library in PalmOS 5.x is a module. Each module has a module ID, which is required to be system-unique and exist in the range of 0..1023. This is probably why Palm Inc never documented how to produce full Native applications - they could never allow more than 1024 of them to exist.
</p>
<p>PalmOS licensees (sony, handspring, etc) got the sources to the OS and all of this knowledge of course. They were able to customize the OS as needed and then shipped it, but the architecture was always mostly the same. This also aids us a lot.
</p>
<h3>Modules? Libraries? DALs? Drivers?</h3>
<p>The kernel of the OS, memory management, most of the drivers, and low level CPU wrangling is done by the <span>DAL</span>. <span>DAL</span>(Module ID 0) exports about 200 OsCalls, give or take based on the PalmOS version. These are low level things like getting battery state, raw access to screen drawing primitives, module loading and unloading, memory map management, interrupt management, etc. Basically these are functions that no user-facing app would ever need to use. On top of the <span>DAL</span> lives <span>Boot</span>. <span>Boot</span>(Module ID 1) provides a lot of the lower-level user-facing OsCalls. Implemented here are things like the DataManager, MemoryManager, AlarmManager, ExchangeManager, BitmapManager, and WindowManager. Feel free to refer to the PalmOS SDK for details on all of those. On top of <span>Boot</span> lives <span>UI</span>. <span>UI</span>(Module ID 2) provides all of the UI primites to the user. These are things like controls (buttons, sliders, etc), forms, menus, tables, and so on. These three modules together make up the core of PalmOS. You could, in fact, almost boot a ROM containing just these three files.
</p>
<p>These first three modules are actually somewhat special, being the core of the OS. They are always loaded, and their exported functions are always accessible via a special shortcut. For modules 0, 1, and 2, you can call an exported function number N by executing these two instructions: <span>LDR R12, [R9, #-4 * (module_ID + 1)]; LDR PC, [R12, #4 * func_no]</span>. This shortcut exists for easy calls to OsCalls by native modules and only works because these modules are always loaded. This is not a general rule, and this will <em>NOT</em> work for any other modules. You might ask if one can also write to these tables of function pointers to replace them. Yes, yes you can and this was often done by what were called "hacks" and also is liberally used by the OS itself (but not via direct writes but via an OsCall: <span>SysPatchEntry</span>).
</p>
<p>PalmOS lacks any memory protection, any user code can access hardware. PalmOS actually uses this - things like SD card drivers, and drivers for other peripherals are usually separate modules and not part of the <span>DAL</span>. The <span>Boot</span> module will load all PalmOS resource databases of certain types at boot, allowing them to initialize. An incomplete list of these types is: <span>libs</span>(slot driver), <span>libf</span>(filesystem driver), <span>vdrv</span>(serial port driver), <span>aext</span>(system extension), <span>aexo</span>(OEM extension). These things being separate is actually very convenient, since that means that they can be easily removed/replaced. There are of course corner cases, since PalmOS developers never anticipated this. For example, if <em>NO</em> serial drivers are loaded, the OS will crash as it never expected this. Luckily, this is also easy to work around.
</p>
<p>Anytime a module is loaded, the entry point is called with a special code, and the module is free to initialize, set up hardware, etc. When it is unloaded, it gets another code, and can deinitialize. There is another special code modules can get and that is from <span>PACE</span>. If you remember, I said that <span>PACE</span> marshals parameters from m68k apps to OsCalls and back, but <span>PACE</span> cannot possibly know about parameters that a random native library takes, so the marshalling there must be done by the library itself. This special code is used to tell the library to: read parameters from the m68k emulated stack, process them, and put the result unto the emulated m68k registers (<span>PACE</span> exports functions to actually manage the emulated state, so the libraries do not need to know of its insides).
</p>

<h2>Towards the first unauthorized PalmOS port</h2>
<h3>So what's so hard?</h3>
<p>As I mentioned, none of the native API of PalmOS 5.x was ever documented. There was a small number of people who figured out some parts of it, but nobody really got it all, or even close to it. To start with, because large parts are not useful to an app developer, and thus attracted no interest. This is a problem, however, if one wants to make a new device. So I had to actually do a lot of reverse engineering for this project - a lot of boring reverse engineering of very boring APIs that I still had to implement. Oh, and I needed a kernel, and actual hardware to run on.
</p>
<h3>ROM formats are hard</h3>
<p>To start with, I wrote a tool to split apart and put back together working PalmOS ROM images. The format is rather convoluted, and changed between versions, but after a lot of work the "splitrom" tool can now successfully split a PalmOS ROM from pre-release pre-v.1.0 PalmOS devices all the way to the PalmOS 6.0 cobalt ROMs. The "mkrom" tool can now produce valid PalmOS 5.x images - I never bothered to actually make it produce other versions as I did not need it. At this point I took a detour from the project to collect PalmOS ROMs. I now have one from almost every device and prototype. I'll share them with the world later. I tested this by pulling apart a T|T3 ROM, replacing some files, putting it back together, and reflashing my T|T3. It booted! Cool!
</p>
<h3>So write a DAL and you're done!</h3>
<p>I had no hardware to test on, no kernel to use, and a lot more "maybe"s than I was willing to live with, so it was time for action. The quickest way I could think of to try it was to use a real ARM processor and an existing kernel - linux. Since my desktop uses an x86 processor and not ARM, qemu was used. I wrote a basic rudimentary DAL that simply logged any function called and then crashed on purpose. At boot, it did same as PalmOS's <span>DAL</span> does: load <span>Boot</span> and in a new thread call <span>PalmOSMain</span> OsCall. I then wrote a simple "runner" app that used mmap() to map an area of memory at a particular location backed by "rom.bin" and another by "ram.bin" and tried to boot it. I got some logged messages and a crash, as expected. Cool! I guess the concept could work. So, what is the minimum number of functions my <span>DAL</span> needs to boot? Turns out that most of them! Sad day...
</p>
<h3>Minimal DAL</h3>
<p>It took months, but I got most of the <span>DAL</span> implemented, and it ran inside my "runner" inside qemu. It was a very scary setup. Since it was all a userspace app under Linux, I had to call back out to the "runner" to request things like thread creation, etc. It was a mess. Current <b>rePalm</b> code still supports this mode, but I do not expect to use it much, for a variety of reasons. To start with, Linux kernel lacks some API that PalmOS simply needs, for example ability to disable and re-enable task switching. Yup... PalmOS sometimes asks for preemption to be disabled. Linux lacks that ability. PalmOS also needs ability to remotely pause and resume a thread, without the thread's consent. The pthreads library lacks such ability as well. I hacked together some hacks using ptrace, but it was a mess. Fun story: since my machine is multi-core, and I never set any affinities, this was the first time ever that PalmOS ran on a multi-core device. I did not realize it till much later, but that is kind of cool, no?
</p>
<h3>Drawing is hard</h3>
<p>There was one problem. For some reason, things like drawing line, rectangles, circles, and bitmaps were all part of the <span>DAL</span>. Now, it is not hard to draw a line, but things like "draw a rounded rectangle with foreground color of X and a background color of Y, using drawing mode 'mask' on this canvas" or "draw this compresed 16-bit full-color 144ppi image on this 4-bits-per-pixel 108ppi canvas with dithering, respecting transparency colors, and using 'invert' mode" or even "print string 'Preferences' with background color X, foreground Y, text color Z, dotted-underlined, using this low-density font on this 1.5 density canvas" get convoluted quickly. And yes, the <span>DAL</span> is expected to handle this all. Oh, and none of this was ever documented of course! This was a nightmare. At first I treated all drawing functions as NOPs and just logged the drawn text to know how far my boot has gotten. This allowed me to implement many of the other OsCalls that <span>DAL</span> must provide, but eventually I had to face having to draw. My first approach was to just implement things myself, based on function names and some reverse engineering. This approach failed quickly - the matrix of possibilities was simply too large. There are 8 drawing modes, 3 supported densities, 4 image compression formats, 5 supported color depths, and two font formats. It was not possible to think of everything, especially with no way to be sure I had it right. I am not sure if some of these modes ever got exercised by any software in existence at all, but it did not matter - it had to be pixel exact! What to do?
</p>
<h3>Theft is a form of flattery, right?</h3>
<p>I decided on a stopgap measure. I disassembled the Zire72 <span>DAL</span>. And I copied each of the necessary functions, and all the functions they called, and all of the functions those functions called, and so on. I then cleaned up their direct references to Zire <span>DAL</span>'s globals, and to each other, and I stuck it all into a giant "drawing.S" file. It was over 30,000 lines long, and I mostly had no idea how it worked. Or if it worked...
</p>
<p>It did! Not right away, of course, but it did. Colors were messed up, artifacts everywhere, but I saw the touchscreen calibration screen after boot! Success, yes? Well, not even remotely. To start with, it turns out that in the interest of optimization, PalmOS's drawing code happily sticks its fingers into the display driver's globals. My display "driver" at this point was just an area of memory backed by an SDL surface. It took a lot of work (throwaway work - the worst kind) to figure out what it was looking for and give it to it. But after a few more weeks, Zire72's <span>DAL</span>'s drawing code happily ran under <b>rePalm</b> and I was able to see things drawn correctly. After hooking up rudimentary fake touchscreen support, I was even able to interact with the virtual device and see the home screen. Great, but this was all a waste. I do not own that code and cannot ship it. I also cannot improve it, expand it, fix it, or even claim to entirely understand it. This was not a path forward.
</p>
<h3>Meticulously-performed imitation is also a form of flattery, no?</h3>
<p>The time had come. I rewrote the drawing code. Function by function. Line by line. Assembly statement by assembly statement. I tested it after replacing every function as best as I could. Along the way I gained the understanding of how PalmOS draws, what shortcuts for what common cases there are, etc. This effort took two months, after them, 30,000 lines of uncommented assembly turned into 8,000 lines of C. <b>rePalm</b> finally was once again purely my own code! Along the way I optimized a few things and added support for one-and-a-half density, something that the Zire72 <span>DAL</span> never supported. Of all the parts of this project, this was the hardest to slog through, because at the end of every function decoded, understood, and rewritten, there was no noticeable movement forward - the goal was just to not break anything, and there were always dozens of thousands of lines of code to disasemble, understand, and rewrite in C.
</p>
<h3>Virtual SD card</h3>
<p>For testing it would be convenient to be able to load programs easier into the device than baking them into the ROM. I wrote a custom slot driver that did nothing, but only allowed you to use my custom filesystem. That filesystem used hypercalls to reach code in the "runner" to perform filesystem ops on the host. Basically this created a shared folder between my <span>PC</span> and <b>rePalm</b>. I used this to verify that most software and games worked as expected
</p>
<h3>Which device ROM are you using?</h3>
<p>ANY! I tested pre-production Tungsten T image, I tested LifeDrive image, even Sony TH55 ROM boots! Yes, there were custom per-device and per-OS-version tweaks, but I was able to get them to apply automatically at runtime. For example, determining which OS version is running is easily done by examining the number of exported entrypoints of <span>Boot</span>. And determining if the ROM is a Sony device is easy by looking for <span>SonyDAL</span> module. We then refuse to load it, and fake-export equivalent functions ourselves. Why does the <span>DAL</span> need to know the OS version? Some <span>DAL</span> entrypoints changed between PalmOS 5.0 and PalmOS 5.2, and PalmOS 5.4 or later expect a few extra behaviours out of existing funcs that we need to support.
</p>
<h3>So you're done, right? It works?</h3>
<p>At this point, <b>rePalm</b> sort of worked. It was a window on my desktop that ran <em>REAL UNMODIFIED PalmOS</em> with only a single file in the ROM replaced - the <span>DAL</span>. Time to call it done, and pick a new project, right? Well, not quite. Like I said, Linux was not an ideal kernel for this, and making a slightly-more-open PalmOS simulator was not my goal. I wanted to make a device...
</p>

<h2>Towards the first pirate PalmOS device</h2>
<h3>A little bit about normal PalmOS 5.x devices, their CPUs, and the progress since...</h3>
<p>In order to understand the difficulties I faced, it is necessary to explain some more about how PalmOS 5.x devices usually worked. PalmOS 5.x targetted ARMv4T or ARMv5 CPUs. They had 4-32MB of flash or ROM to contain the ROM, and 8-128MB or RAM for runtime allocations and data storage. PalmOS 5.4 added NVFS, which I shall for now pretend does not exist (as we all wished we could when NVFS first came out). ARMv4T and ARMv5 CPUs implement two separate instruction sets: ARM and Thumb. ARM instructions are each exactly 4 bytes, and are the original instruction set for ARM CPUs. Thumb was added in v4T as a method of improving code density. It is a set of 2-byte long instructions that implement the most common operations the code might want to do, and by being half the size improve code density. Obviously, you do not get something for nothing. In the CPUs back then, Thumb instructions had one extra pipeline stage, so this caused them to be slower in code with a lot of jumps. Also, as the instructions themselves were simpler, sometimes it took more of them to do the same thing. Thumb instructions, in most cases, also only have access to half as many registers as ARM instructions, further leading to slightly less optimal code. But, in general Thumb code was smaller, and speed was not a factor, so large parts of PalmOS were compiled in Thumb mode. (Sony bucks this trend, having splurged for larger flash chips and compiling the entire OS in ARM mode). Some things could also not at all be done in Thumb, for example, 32x32-&gt;64 bit multiply, and some were very suboptimal to do in Thumb (like a lot of the drawing code with a lot of complex bit shifts and addressing). These speed-critical pieces were always compiled in ARM mode in PalmOS. Also all library entry points were always in ARM mode with no other options, so even libraries entirely compiled as Thumb, had small thunks from ARM to Thumb mode on each entrypoint.
</p>
<p>How does one actually switch modes between ARM and Thumb in ARMv5? Certain, but not all, instructions that change control flow perform the change. Since all ARM instructions are 4-bytes long and always aligned on a 4-byte boundary, any valid ARM instruction's address has the low two bits cleared. Thumb instructions are 2 bytes long, and thus have the bottom one bit cleared. 32-bit-long Thumb2 instructions are also aligned on a 2-byte boundary. This means that for any instruction in any mode, the lower bit of its address is always clear. ARM used this fact for mode switching. The <span>BX</span> instruction would now look at the bottom bit of the register you're jumping to, and if it was 1, treat the destination as Thumb, else as ARM. Any instruction that loads <span>PC</span> with a word will do the same: <span>POP</span>, <span>LDM</span>, <span>LDR</span> instructions. Arithmetic done on <span>PC</span> in Thumb mode does not change to ARM mode ever (low bit ignored) and arithmetic done on <span>PC</span> in ARM mode is undefined if the lower 2 bits produced are nonzero (<em>CAUTION</em>: this is one of the things that ARMv7 changed: this now has defined behaviour). Also an extra instruction was added for easy calls between modes: <span>BLX</span>. There is a form of it that takes a relative offset encoded in the instruction itself, which basically acts like a <span>BL</span>, but also switches modes to whatever <em>NOT</em> the current mode is. There is also a register mode of it that combines what a <span>BX</span> does with saving the return address. Of course to make sure that returns to Thumb mode work as expected, Thumb instructions that save a return address, namely <span>BL</span> and <span>BLX</span> set the lower bit of <span>LR</span>.
</p>
<p>ARMv5 at this point in time is ancient history. ARM architecture is up to v8.x by now, with 64-bit-wide-registers and a completely different instruction set. ARMv7 is still often seen around (v8 can also run in v7 mode) and is actually an almost perfect (but actually not entirely so) superset of ARMv5. So I could basically take a dev board for any ARMv7 chip, which are abundant and cheap, and use that as my base, right? Technically yes, but I did not go this way. To start with, few of these CPUs are documented well, so unless you use linux kernel, you'll never get them up - writing your own kernel and drivers for them is not feasible (I am looking at you, allwinner). "But," you might object, "what about Raspberry Pi, isn't its CPU fully documented?" I considered it, but discarded the idea - RasPi is terribly unstable, and I had no desire to build on such a shaky platform. Launch firefox on your RasPi, open dailymail or some other complex site, and go away, come back in 2 weeks, I guarantee you'll be greeted by a hung screen and a kernel panic on the serial console. If even Linux kernel developers cannot make this thing work stably, I had no desire to try. No thanks. So what then?
</p>
<h3>ARMv7M</h3>
<p>The other option was to use a microcontroller - they are plentiful, documented, cheap, and available. ARM designs and sells a large number of small cores under the Cortex brand. Cortex-M0/M0+/M1 are cores based on the ARMv6M spec - basically they run the same Thumb instruction set that ARMv5 CPUs did, with a few extra instructions to allow them to manage privileged state (<span>MRS</span>/<span>MSR</span>/<span>CPS</span>). Cortex-M23 is their successor, which adds a few extra instructions (<span>DIV</span>/<span>CBZ</span>/<span>CBNZ</span>/<span>MOVW</span>/<span>MOVT</span>/<span>B.W</span>) which makes it a bit less of a pain in the ass, but it still is very much a pain for complex work. Cortex-M3/M4/M7 implement ARMv7M spec, which has a very expanded Thumb2 instruction set. It is the same instruction set that ARM introduced into the ARM cores back in the day with ARMv6T2 architecture CPUs. These instructions are a mix of 2 and 4-byte long pieces and are actually pretty good for complex code, supporting long multiplies, complex control flow, and bitfield operations. They can also address all registers and not just half of them like the Thumb instruction set of yore. Cortex-M33 is the successor to these, adding a few more things we do not currently care about. Optionally, these cores can also include an FPU for hardware floating point support. We also do not care about that. There is only one problem: <em>None of these CPUs support ARM instuctions</em>. They all only run Thumb/Thumb2. This means we can run most of PalmOS's <span>Boot</span> and <span>UI</span>, but many other things will fail. Not acceptable. Well, actually, since every library has to be entered in ARM mode, nothing will run...
</p>
<h3>My kingdom for an ARM!</h3>
<p>It is at this point that I decided to extend PalmOS's module format to support direct entry into Thumb mode and converted my <span>DAL</span> to this now format. I also taught my module loader to understand when an library's entry point points to a simple ARM-to-Thumb thunk, and to resolve this directly. This allowed an almost complete boot without needing ARM. But this was not a solution. Large parts of the OS were still in ARM mode (things like <span>MemMove</span>, <span>MemCmp</span>, division routines), and if the goal was to run an unmodified OS and apps, editing everything everywhere was not an option. Some things we could just patch via <span>SysPatchEntry</span>. This I did to the abovementioned <span>MemMove</span> and <span>MemCmp</span> for speed, providing optimal Thumb2 implementations. Other things I could do nothing about - things like integer division (which ARMv5 has no instruction for) were scattered in almost every library, and could not be patched away as they were not exported. We really did need something that ran ARM instructions.
</p>
<h3>But what if we try?</h3>
<p>What exactly will happen if we try to switch an ARMv7M microcontroller into ARM mode? The manual luckily is very clear on that. It <em>WILL</em> switch, clear the status bit that indicated we're in Thumb mode, and then when it tries to execute the next instruction, it will take a <span>UsageFault</span> since it cannot execute in this mode. The Thumb <span>BLX</span> instruction of the form that always switches modes is undefined in ARMv7M, and if executed, the CPU will take a <span>UsageFault</span> as well, indicating in invalid instruction. This all sounds grim, but this is actually fantastic news! We can catch a <span>UsageFault</span>... If you see where I am going with this, and are appropriately horrified, thanks for paying attention! We'll come back to this story arc later, to give everyone a chance to catch up.
</p>

<h2>We need hardware, but developing on hardware is ... hard</h2>
<h3>CortexEmu to the rescue</h3>
<p>I thought I could make this all work on a Cortex-M class chip, but I did not want to develop on one - too slow and painful. I also did not find any good emulators for Cortex-M class chips. At this point, I took a two-week-long break from this project to write CortexEmu. It is a fully functional Cortex-M0/M3/M23 emulator that faithfully emulates real Cortex hardware. It has a GDB stub so I can attach GDB to it to debug the running code, It has rudimentary hardware emulated to show a screen, and support an RTC, a console, and a touchscreen. It supports privileged and unprivileged mode, and emulates the memory protection unit (MPU) as well. CortexEmu remains the best way to develop <b>rePalm</b>.
</p>
<h3>Waaaah! You promised real hardware</h3>
<p>Yes, yes, we'll get to that, and a lot more later, but that is still months later in the story, so be patient!
</p>

<h2>Um, but now we need a kernel...</h2>
<h3>Need a kernel? Why not Linux?</h3>
<p>PalmOS needs a kernel with a particular set of primitives. We already discussed some (but definitely not all) reasons why Linux is a terrible choice. Add to that the fact that Cortex-M3 compatible linux is slow <em>AND</em> huge, it was simply not an option. So, what is?
</p>
<p>I ended up writing my own kernel. It is simple, and works well. It will run on any Cortex-M class CPU, supports multithreading with priorities, precise timers, mutexes, semaphores, event groups, mailboxes, and all the primitives PalmOS wants like ability to force-pause threads, and ability to disable task switching. It also takes advantage of the MPU to add some basic safety like stack guards. Also, there is great (&amp; fast) support for thread local storage, which comes in handy later. Why write my own kernel, aren't there enough out there? None of the ones out there really had the primitives I needed and bolting them on would take just as long.
</p>

<h2>So, uh, what about all that pesky ARM code?</h2>
<h3>The ARM code still was a problem</h3>
<p>PalmOS still would not boot all the way to UI because of the ARM code. But, if you remember, as few paragraphs ago I pointed out that we can trap attempts to get into ARM mode. I wrote a <span>UsageFault</span> handler that did that, and then...I emulated it
</p>
<h3>You do not mean...?</h3>
<p>Oh, but I do. I wrote an ARM emulator that would read each instruction and execute it, until the code exited ARM mode, at which point I'd exit the emulation and resume native execution. The actual details of how this works are interesting since the emulator needs its own stack and cannot run on the stack of the emulated code. There also needs to be a place to stash the emulated registers since we cannot just keep them in the real registers (not enough registers for both). Exiting emulation is also kind of fun since you need to load ALL register and status register as well all at once atomically. Not actually trivial on Cortex-M. Well, in any case, "emu.c" and "emuC.c" have the code - go wild and explore.
</p>
<h3>But isn't writing an emulator in C kind of slow?</h3>
<p>You have no idea! The emulator was slow. I instrumented CortexEmu to count cycles, and came up with an average of 170 cycles of host CPU to emulate a single ARM instruction. Not good enough. Not even remotely. It is well known that emulators written in C are slow. C compilers kind of suck at optimizing emulator code. So what next? Well, I went ahead and rewrote the emulator core in assembly. Actually I did it twice. Once for ARMv7M (Cortex-M3 target) and once for ARMv6M (Cortex-M0 target). The speed improved a lot. Now for the M3 core I was averaging 14 cycles per cycle, and for the M0 it was 19. A very respectable emulator performance if I do say so myself.
</p>
<h3>So, is it fast enough now?</h3>
<p>As mentioned before, on original PalmOS devices, ARM code was generally faster than Thumb, so most of the hottest, tightest, fastest code was written in ARM. For us, ARM is 14x slower than Thumb. So the code that was meant to be fastest is slow. But let us take an inventory of this code and see what it really is. Division routines are part of it. ARMv7M implements division in hardware, but ARMv5 did not (nor does ARMv6M). These routines are a hundred cycles or so in ARM mode. <span>MemMove</span>, <span>MemMSet</span> and <span>MemCmp</span> We spoke about already, and we do not care because we replaced them, but lots of libraries had their own internal copies we cannot replace. My guess is that the compiler prefers to inline its own "memset" and "memcpy" in most cases. That made up a large part of the boot process's ARM code usage. Luckily, all of these functions are the same everywhere...
</p>
<p>So, can we pattern-match some of these in the emulator code and execute faster native routines? I did this and boot process did go faster. The average per-instr overhead rose due to matching, but boot time shrank. Cool. But what happens <em>after</em> boot? After boot we meet the real monster... <span>PACE</span>'s m68k emulator is written in ARM. 60 kilobytes of what is clearly hand-written assembly with lots of clever tricks. Clever tricks suck when you're stuck emulating them... So this means that every single m68k application (which is most of them) is now running under double emulation. Gross... Oh, also: slow. Something had to be done. I considered rewriting <span>PACE</span>, but that is a poor solution - there are a lot of ARM libraries and I cannot rewrite them all. Plus, in what way can I claim to be running an unmodified OS if I replace every bit of it?</p>
<p>There is one more way to make non-native code fast...</p>
<h2>You do not mean...? (pt2)</h2>
<h3>Just in time: this</h3>
<p><span>PACE</span> contains a lot of hot code that is static. On real devices it lives in ROM and does not change. Most libraries are the same. So, what can we do to make it run faster? Translate it to what we can run natively, of course. Most people would not take on a task of writing a just-in-time translator alone. But that is just because they are wimps :) (Or maybe they reasonably assume that it is a huge time sink with more corner cases than one could shake a stick at)
</p>
<h3>JITs: how do we start?</h3>
<p>Basically the same way we did for the emulator. We create a per-thread translation cache (TC) which will hold our translations. Why per thread? Because this avoids the problem of one thread flushing the cache while another is running in it with no end in sight. The TC will contain translation units (TU) each of which represents some translated code. Each TU contains its original "source" ARM address, and then just valid Thumb2 code. There will also be a hashtable which will map source "ARM" addresses to a bucket where the first TU for that hash value is stored. Each bucket is a linked list, and 4096 buckets are used. This is configurable. A fast &amp; simple hash is used. Tested on a representative sample of addresses it gave good distribution. Now, whenever we take a <span>UsageFault</span> that indicates an attempted entry to ARM mode, we lookup the desired address in the hashtable. If we get a hit, we simply replace the <span>PC</span> in the exception frame with the "code" pointer of the matching TU and return. The CPU proceeds to execute native code quickly. Wonderful! What if we do not get a hit? We then save the state and replace the <span>PC</span> in the exception frame with the address of the translation code (we do not want to translate in kernel mode).
</p>
<h3>Parlez-vous ARM?</h3>
<p>The front end of a JIT basically just needs to ingest ARM instructions and understand them. We'll trap on any we do not understand, and try to translate all those that we do. Here we hit our first snag. Some games use instructions that are not valid. Bejeweled, I am looking at you! The game "Bejeweled" has some ARM code included in it and it likes to return by executing <span>LDMDB R11, {R0-R12, SP, PC}^</span>. Ignoring the fact that R0-R2 and R12 do not need to be saved and they are being inefficient, that is also not a valid instruction to execute in user mode at all. That little caret at the end means "also transfer <span>SPSR</span> to <span>CPSR</span>". That request is invalid in user mode and ARM architecture reference manual is very clear that executing this in user mode will have undefined effects. This explains why Bejeweled did not run under <b>rePalm</b> under QEMU. QEMU correctly refused to execute this insanity. Well, I dragged out a Palm device out of a drawer and tested to see what actually happens if you execute this. Turns out that it is just ignored. Well, I guess my JIT will do that too. My emulator cores had no trouble with this instr since as this instr is undefined, treating it like it has no caret was safe, and thus they never even checked the bit that indicated it.
</p>
<p>Luckily for us, ARM only has a few instruction formats. Unluckily for us they are all pretty complex. Luckily, decoding is easy. Almost every ARM instruction is conditional and the top 4 bits determine if it executes at all or does not. Data Processing operations are always 3-operand. Destination reg, Source reg, and "Operand" which is ARM's addressing mode 1. It can be an immediate of certain forms, a register, a register shifted by an immediate, or a register shifted by a register. Say what?! Yup, you can do things like <span>ADD R0, R1, R2, ROR R3</span>. Be scared. Be very scared! Setting flags is optional. Loading/storing bytes or words uses addressing mode 2, which allows a use of a register plus/minus an immediate, or register plus/minus register, or register plus/minus register shifted by an immediate. All of these modes can be index, postindex, or index-with-writeback, so scary things like <span>LDR R0, [R1], R2, LSL #12</span> can be concocted. Loading/storing halfwords or signed data uses addressing mode 3, which is just like mode 2 except no register shifts are available. This mode is also used for <span>LDRD</span> and <span>STRD</span> instructions that some ARMv5 cores implement (this is part of the optional DSP extension). Addressing mode 4 is used for <span>LDM</span> and <span>STM</span> instructions, which are terrifying in their complexity and number of corner cases. They can load or store any subset of registers to a given base address with pre-or-post increment-or-decrement and optional writeback. They are used for stack ops. And last, but not least, there are branches which are all encoded simply and decode easily. Phew...
</p>
<h3>2 Thumbs do not make an ARM</h3>
<p>Initially the thought was that the translation cannot be all that hard? The instructions look similar, and it shouldn't be all that bad. Then reality hit. Hard. Thumb2 has a lot of restrictions on operands, like for example <span>SP</span> cannot at all be treated like a general register, and <span>LR</span> and <span>PC</span> cannot ever be loaded together. It also lacks anything equalling addressing mode 1's ability to shift a register by a register as a third operand to an ALU operation. It lacks ability to shift a third register by more than 3, like mode 2 can in ARM. I am not even going to talk about <span>LDM</span> and <span>STM</span>! Oh, and then there is the issue of not letting the translated code know it is being translated. This means that it must still think it is running from original place, and if it reads itself, see ARM instructions. This means that we cannot ever leak PC's real value into any executable state. The practical upshot of that is that we can never emit a <span>BL</span> instruction, and whenever <span>PC</span> is read, we must instead produce an immediate value which is equal to what <span>PC</span> would have been, had the actual ARM code run from its actual place in memory. Not fun...
</p>
<p>Thumb2's <span>LDM</span>/<span>STM</span> actually lack half the modes that ARM has (modes <span>ID</span> and <span>DA</span>) so we'd have to expand those instructions to a lot more code. Oh, and Thumb has limits on writeback that do not match ARM's (more strict) and also you can never use <span>SP</span> in the register set, nor can you ever store <span>PC</span> this way in Thumb2. At this point it becomes abundantly clear that this will not be an easy instruction in -&gt; instruction out job. We'll need places to store temporary immediates, we'll need to rewrite lots of instructions, and we'll need to do it all without causing side effects. Oh, and it should be fast too!
</p>

<h2>A JIT's job is never over</h2>
<h3>LDM and STM, may they burn in hell forever!</h3>
<h4>How LDM/STM work in ARM</h4>
<p>ARM has two multiple-register ops: <span>LDM</span> and <span>STM</span>. Each has a few addressing modes. First is the order: up or down in addresses (that is, does the base register address where to store the lowest-numbered register or highest. Next is whether the base register itself is to be used, or should it be incremented/decremented first. This gives us the four basic modes: <span>IA</span>("increment after"), <span>IB</span>("increment before"), <span>DA</span>("decrement after"), <span>DB</span>("decrement before"). Besides that, it is optional to writeback the updated base address to the base register. There are of course corner cases, like what value gests stored if base register with writeback is stored, or what value the base register will have if loaded, while writeback is also specified. ARM spec explicitly defines some of these cases as having unpredictable consequences.
</p>
<p>For stack, ARM uses a full-descending stack. That means that at any point, the <span>SP</span> register points to the last ALREADY USED stack position. So, to pop a value, you load it from <span>[SP]</span>, and then increment <span>SP</span> by 4. This would be done using an <span>LDM</span> instruction with an <span>IA</span> addressing mode. To push a value unto the stack, one should first decrement <span>SP</span> by 4, and then store the desired value into <span>[SP]</span>. This corresponds to an <span>STM</span> instruction with an <span>DB</span> addressing mode. <span>IB</span> and <span>DA</span> modes are not used for stack in normal ARM code. 
</p>
<h4>How LDM/STM work in Thumb2</h4>
<p>So why did I tell you all this? Well, while designing the Thumb2 instruction set, ARM decided what to support and what not to. This basically meant that uncommon things did not get carried forward. Yup...you see where this is going. Thumb2 does not support <span>IB</span> and <span>DA</span> modes. At all. Not cool. But there is more. Thumb2 forbids using <span>PC</span> or <span>SP</span> registers in the list of registers to be stored for <span>STM</span>. Thumb2 also forbids ever loading <span>SP</span> using <span>LDM</span>, also if an <span>LDM</span> loads <span>PC</span>, it may not also load <span>LR</span>, and if it loads <span>LR</span>, it may not also load <span>PC</span>. There is more yet... <span>PC</span> is not allowed as the base register, and the register list must be at least two registers long. This is a somewhat-complete list of what Thumb2 is missing compared to ARM.
</p>
<p>But wait, there is more. Even the instrutions that map nicely from ARM to Thumb2 and comply with all the restrictions of Thubm2 are not that simple to translate. For example, storing <span>PC</span>, is as always hard - we need a spare register to store the expected PC value so we can push it. But, registers are pushed in order, so depending on what register we pick as our temporary reg, it might be out of other relative to others, we might need to split the store into a few stores. But, there is more yet. What if the store was to <span>SP</span> or included <span>SP</span>? We changed SP by pushing our temp reg, so we need to adjust for that. But what if this was a <span>STMDB SP!</span>(aka: <span>PUSH</span>). Then we cannot pre-push a temp register that easily...
</p>
<h4>But wait, there's more ... pain</h4>
<p>There is another complication. <span>LDM</span>/<span>STM</span> is expected to act as an atomic instruction to userspace. It is either aborted or resumable at system level. But in Thumb2 in Cortex-M chips, <span>SP</span> is special since the exception frame gets stored there. This means that <span>SP</span> must always be valid, and any data stored BELOW <span>SP</span> is not guaranteed to ever persist (since an interrupt may happen anytime). Luckily, on ARM it was also discouraged to store data below <span>SP</span> and this was rarely done. There is one common piece of PalmOS code that does this: the code around <span>SysLinkerStub</span> that is used to lazy-load libraries. For other reasons rePalm replaced this code anyways though. In all other cases the JIT will emit a warning if an attempt is made to load/store below <span>SP</span>.
</p>
<p>As you see, this is very very very complex. In fact, the complete code to translate <span>LDM</span>/<span>STM</span> ended up being just over four thousand lines long and the worst-case translation can be 60-ish bytes. Luckily this is only for very weird instructions the likes of which I have never seen in real code. "So," you might ask, "how could this be tested if no code uses it?" I actually used a modified version of my <a href="https://github.com/uARM-Palm/uARM">uARM</a> emulator to emulate both orignal code and translated code to verify that each destination address is loaded/stored once exactly and with proper vales only, and then made a test program that would generate a lot of random valid <span>LDM</span>/<span>STM</span> instructions. It was then left to run over a few weeks. All bugs were exterminated with extreme prejudice, and I am now satisfied that it works. So here is how the JIT handles it, in general (look in "emuJit.c" for details).
</p>
<h4>Translating LDM/STM</h4>
<ol><li>Check if the instruction triggers any undefined behaviour, or is otherwise not defined to act in a particular way as per the ARM Architecture Reference Manual. If so, log an error and bail out.</li><li>Check if it can be emitted as a Thumb2 <span>LDM</span>/<span>STM</span>, that is: does it comply with ALL the restrictions Thumb2 imposes, and if so, and also if <span>PC</span> is not being stored, emit a Thumb2 <span>LDM</span>/<span>STM</span></li><li>Check if it can be emitted as a <span>LDR</span>/<span>STR</span>/<span>LDRD</span>/<span>STRD</span> while complying with Thumb2 limits on those. If so, that is emitted.</li><li>A few special fast cases to emit translations for common cases that are not covered by the above (for example ADS liked to use <span>STMIB</span> for storing function parameters to stack)</li><li>For unsupported modes <span>IB</span> and <span>DA</span>, if no writeback is used, they can be rewritten in terms of the supported modes.</li><li>If instruction loads <span>SP</span>, it is impossible to emit a valid translation due to ohw ARMv7-M uses <span>SP</span>. For this one special case, the JIT emits a special undefined instruction and we trap it and emulate it. Luckily no common code uses this ever!</li><li>Finally, the generic slow path is taken:<ol><li>Generate a list of registers to be loaded/stored, and at what addresses.</li><li>Calculate writeback if needed.</li><li>If needed, allocate a temporary register or two (we need two if storing PC and SP) and spill their contents to stack</li><li>For all registers left to be loaded/stored, see how many we can load/store at once, and do so. This involves emitting a set of instructions: <span>LDR</span>/<span>STR</span>/<span>LDRD</span>/<span>STRD</span>/<span>LDM</span>/<span>STM</span> until all is done.</li><li>If we had allocated temporary registers, restore them</li></ol></li></ol>

<h3>Slightly less hellish instructions</h3>
<p>Addressing mode 1 was hard as well. Basically thanks to those rotate-by-register modes, we need a temporary register to calculate that value, so we can then use it. If the destination register is not used, we can use that as temp storage, since it is about to be overwritten anyways by the result, unless it is also one of the other source operands..or <span>SP</span>...or <span>PC</span>... oh god, this is becoming a mess. Now what if <span>PC</span> is also an operand? We need a temporary register to load the "fake" <span>PC</span> value into before we can operate on it. But once again we have no temporary registers. This got messy very quickly. Feel free to look in "emuJit.c" for details. Long story short: we do our best to not spill things to stack but sometimes we do have to.
</p>
<p>The same applies to some complex addressing modes. Thumb2 optimized its instructions for common cases, which makes uncommon cases very hard to translate. Here it is even harder to find temporary registers, because if we push anything, we might need to account for that if our base register is <span>SP</span>. Once again: long story, scary story, see "emuJit.c". Basically: common things get translated efficiently, uncommon ones are not. Special case is PC-based loads. These are used to load constant data. In most cases we inline the constant data into the produced translations for speed. 
</p>
<h3>Conditional instructions</h3>
<p>Thumb2 does have ways to make conditional instructions: the <span>IT</span> instruction that makes the next 1-4 instructions conditional. I chose not to use it due to the fact that it also changes how flags get set by 2-byte Thumb instructions and I did not want to special case it. Also sometimes 4 instructions are not enough for a translation. Eg: some <span>STMDA</span> instructions expand to 28 instructions or so. I just emit a branch of opposite polarity (condition) over the translation. This works since these branches are also just 2 bytes long for all possible translation lengths.
</p>
<h3>Jumps &amp; Calls</h3>
<p>This is where it gets interesting. Basically there are two type of jumps/calls. Those whose destinations are known at translation time, and those whose are not. Those whose addresses are known at translation time are pretty simple to handle. We look up the destination address in our TC. If it is found, we literally emit a direct jump to that TU. This makes hot loops fast - no exit from translated code is needed. Indirect or computed jumps are not common, so one would think that they are not that important. This is wrong because there is one type of such jump that happens a lot: function return. We do not, at translation time, know where the return is going to go to. So how do we handle it? Well, if the code directly loads <span>PC</span>, everything will work as expected. Either it will be an ARM address and our <span>UsageFault</span> handler will do its thing or it will be a Thumb address and our CPU will jump to it directly. An optimization exists in case an actual <span>BX LR</span> instruction is seen. We then emit a direct jump to a function that looks up <span>LR</span> in the hash - this saves us the time needed to take an exception and return from it (~60 cycles). Obviously more optimizations are possible, and more will be added, but for now, this is how it is. So what do we do for a jump whose destination is known and we haven't yet translated it? We leave ourselves a marker, namely an instruction we know is undefined, and we follow that up with the target address. This way if the jump is ever actually taken (not all are), we'll take the fault, translate, and then replace that undefined instr and the word following it with an actual jump. Next time that jump will be fast, taking no faults.
</p>
<h3>Translating a TU</h3>
<p>The process is easy: translate instructions until we reach one that we decide is terminal. What is terminal? An unconditional branch is terminal. A call is too (conditional or not). Why? Because someone might return from it, and we'd rather have the return code be in a new TU so we can then find it when the return happens. An unconditional write to <span>PC</span> of any sort is terminal as well. There is a bit of cleverness also for jumps to nearby places. As we translate a TU, we keep track of the last few dozen instructions we translated and where their translations ended up. This way if we see a short jump backwards, we can literally inline a jump to that translation right in there, thus creating a wonderfully fast translation of this small loop. But what about short jumps forward? We remember those as well, and if before we reach our terminal instr we translate an address we remembered a past jump to from this same TU, we'll go back and replace that jump with a short one to here.
<a name="_TOC_d8be03a7734ce336e0e95c12bed0d3d7"></a></p><h3>And if the TC is full?</h3>
<p>You might notice that I said we emit jumps between TUs. "Doesn't this mean," you might ask, "that you cannot just delete a single TU?" This is correct. Turns out that keeping track of which TUs are used a lot and which are not is too much work, and the benefits of inter-TU jumps are too big to ignore. So what do we do when the TC is full? We flush it - literally throw it all away. This also helps make sure that old translations that are no longer needed eventually do get tossed. Each thread's TC grows up to a maximum size. Some threads never run a lot of ARM and end up with small TCs. The TC of the main UI thread will basically always grow to the maximum (currently 32KB).
</p>
<h3>Growing up</h3>
<p>After the JIT worked, I <em>rewrote it</em>. The initial version was full of magic values and holes (cases that could happen in legitimate code but would be mistranslated). It also sometimes emitted invalid opcodes that Cortex-M4 would still execute (despite docs saying they were not allowed). The JIT was split into two pieces. The first was the frontend that ingested ARM instructions, maintained the TC, and kept track of various other state. The second was the backend. The backend had a function for each possible ARMv5 addressing mode or instruction format, and given <b>ANY</b> valid ARMv5 instruction, it could produce a sequence of ARMv7M instructions to perform the same task. For common cases the sequence was well optimized, for uncommon ones, it was not. However, the backend handles <em>ANY</em> possible valid ARMv5 request, even insane things like, for example,  <span>RSBS PC, SP, PC, ROR SP</span>. No sane person would ever produce this instruction, but the backend will properly translate it. I wrote tests and ran them automatically to verify that all possible inputs are handled, and correctly so. I also optimized the hottest path in the whole system - the emulation of the <span>BLX</span> instruction in thumb. It is now a whopping 50 cycles faster, which noticeably impacted performance. As an extra small optimization, I noticed that oftentimes Thumb code would use a <span>BLX</span> simply to jump to an OsCall (which due to using R12 and R9 cannot be written in Thumb mode). The new <span>BLX</span> handler detects this and skips emulation by calling the requisite OsCall directly.
</p>
<p>I then wrote a sub-backend for the EDSP extension (ARMv5E instructions) since some Sony apps use them. The reason for a separate sub-backend is that ARMv7E (Cortex-M4) has instructions we can use to translate EDSP instructions very well, while ARMv7 (Cortex-M3) does not, and requires longer instruction sequences to do the same work. rePalm supports both.
</p>
<p>Later, I went back and, despite it being a huge pain, worked out a way to use the <span>IT</span> instruction on Cortex-M3+. This resulted in a huge amount of code refactoring - basically pushing "condition code" to every backend function and expecting it to conditionalize itself however it wishes. This produced a change with an over-4000-line diff but it workes very well and resulted in a noticeable speed icnrease!
</p>
<h3>The Cortex-M0 backend</h3>
<h4>Why this is insane</h4>
<p>It was quite an endeavor, but I wanted to see if I could make a working Cortex-M0 backend for my JIT. Cortex-M0 executes the ARMv6-m instruction set. This is basically just Thumb-1, with a few minor additions. Why is this scary? In Thumb-1, most instructions only have access to half the registers (r0..r7). Only three instructions have access to high registers: <span>CMP</span>, <span>MOV</span>, and <span>ADD</span>. Almost all Thumb-1 instructions always set flags. There are also no long-multiply instructions in Thumb-1. And, there is no <span>RRX</span> rotation mode at all. The confluence of all these issues makes attempting a one-to-one instruction-to-instruction translation from ARM to Thumb-1 a non-starter.
</p>
<p>To make it all work, we'll need some temporary working space: a few registers. It is all doable with three with a lot of work, and comfortable with four. So I decided to use four work registers. We'll also need a register to point to our context (the place where we'll store extra state). And, for speed, we'll want a reg to store the virtual status register. Why do we need one of those? Because almost all of our Thumb-1 instructions clobber flags, whereas the ARM code we're translating expects flags to stick around during long instruction sequences. So our total is: 6. We need 6 registers. They need to be low registers since, as we had discussed, high registers are basically useless in Thumb-1. 
</p>
<h4>The basics</h4>
<p>Registers r0 through r3 are temporary work registers for us. The r4 register is where we keep our virtual status register, and r5 points to our context. We use r12 as another temporary. Yes it is a high-reg but sometimes we really just need to store something, so only being able to <span>MOV</span> something in and out of it is enough. So, what's in a context? Well, then state of the virtual r0 through r5 registers, as well as the virtual r12 and the virtual lr register. There, obviously, needs to be a separate context for every thread, since they may each run different ARM code. We allocate one the first time a thread runs ARM (it is actually part of the JIT state, and we copy it if we reallocate the JIT state). 
</p>
<p>"But," you might say, "if PalmOS's Thumb code expects register values in registers, and our translated ARM code keeps some of them in a weird context structure, how will they work together?" This is actually complex. Before every translation unit, we emit a prologue. It will save the registers from our real registers into the context. At the end of every translation unit, we emit an epilogue that restores registers from the context into the real registers. When we generate jumps between translation units, we jump past these pieces of code, so as long as we are running in the translated code, we take no penalty for saving/restoring contexts. We only need to take that penalty when switching between translated code and real Thumb code. Actually, it turns out that the prologue and epilogue are large enough that emitting then inside every TU is a huge waste of space, so we just keep a copy of each inside a special place in the context, and have each TU just call them as needed. A later speed improvement I added was to have multiple epilogues, based on whether we know that the code is jumping to ARM code, Thumb code, or "not sure which". This allows us to save a few cycles on exiting translated code. Every cycle counts!
</p>
<h4>Fault dispatching</h4>
<p>There is just one more problem: Those <span>BLX</span> instructions in Thumb mode. If you remember, I wrote about how they do not exist in ARMv7-m. They also do not exist in ARMv6-m. So we also need to emulate them. But, unlike ARMv7-m, ARMv6-m has no real fault handling ability. All faults are considered unrecoverable and cause a <span>HardFault</span> to occur. Clearly something had to be done to work around that. This actually led to a rather large side-project, which I published separately: <a href="https://dmitry.gr/?r=05.Projects&amp;proj=27.%20m0FaultDispatch">m0FaultDispatch</a>. In short: I found a way to completely and correctly determine the fault cause on the Cortex-M0, and recover as needed from many types of faults, including invalid memory accesses, unaligned memory accesses, and invalid instructions. With this final puzzle piece found, the Cortex-M0 JIT was functional.
</p>


<h2>Is PACE fast enough?</h2>
<h3>Those indirect jumps...</h3>
<p>Unfortunately, emulation almost always involves a lot of indirect jumps. Basically that is how one does instruction decoding. 68k being a CISC architecture with variable-length instructions means that the decoding stage is complex. <span>PACE</span>'s emulator is clearly hand-written in assembly, with some tricks. It is all ARM. It is actualy the same instruction-for-instruction from PalmOS 5.0 to PalmOS 5.4. The surrounding code changed, but the emulator core did not. This is actually good news - means it was good as is. My JIT properly and correctly handles translating <span>PACE</span>, as evidenced by the fact that rePalm works on ARMv7-M. The main problem is that every instruction emulated requires at least one indirect jump (for common instructions), two for medium-comonness ones, and up to three some some rare ones. Due to how my JIT works, each indirect jump that is not a function return requires an exception to be taken (14 cycles in, 12 out), some glue code (~30 cycles), and a hash lookup (~20 cycles). So even in case that the target code has been translated, this adds 70-ish cycles to each indirect jump. This puts a ceiling on the efficiency of the 68k emulator at 1/70th the speed. Not great. <span>PACE</span> usually is about 1/15 the speed of the native code, so that is quite a slowdown. I considered writing better translation just for <span>PACE</span>, but it is quite nontrivial to do fast. Simply put, there isn't a simple fast way to translate something like <span>LDR R0, [R11, R1, LSL #2]; ADD PC, R11, R0</span>. There simply is no way to know where that jump will go, or that even R11 points to a location that is immutable. Sadly that is what <span>PACE</span>'s top level dispatch looks like.
</p>
<h3>A special solution for a special problem</h3>
<p>I had already fulfilled my goal of running PalmOS unmodified - <span>PACE</span> does work with my JIT, and the OS is usable and not slow, but I wanted a better solution and decided that <span>PACE</span> is a unique-enough problem to warrant it. The code emulator in <span>PACE</span> has a single entry point, and only calls out to other code in a 10 clear cases: Line1010 (instruction starting with 0xA), Line1111 (instruction starting with 0xF), TRAP0, TRAP8, TRAPF (OsCall), Division By Zero, Illegal instrction, Unimplemented instruction, Trace Bit being set, and hitting a PC value of precisely 0xFFFFFFF0. So what to do? I wrote a tool "patchpace" that will take in a PACE.prc from any PalmOS device, analyze it to find where those handlers are in the binary, and find the main emulator core. It will then replace the core (in place if there is enough space, appended to the binary if not) with code you provide. The handler addresses will be inserted into your code at offsets the header provides, and a jump to your code will be placed where the old emulator core was. The header is very simple (see "patchpace.c") and just includes halfword offsets from the start of the binary to the entry, and to where to insert jumps to each of the abovementioned handlers as <span>BL</span> or <span>BLX</span> instructions). The only param to the emulator is the state. It is structured thusly: first word is free for emulator to use as it pleases, then 8 D-regs, then the 8 A-regs, then PC, and then SR. No further data is allowed (PACE uses data after here). This same state must be passed to all the handlers. TRAPF handler also needs the next word passed to it (OsCall number). Yes, you understand this correctly, this allows you to bring your own 68k emulator to the party. Any 68k emulator will do, it does not need to know anything about PalmOS at all. Pretty sweet!
</p>
<h3>Any 68k emulator...</h3>
<p>So where do we get us a 68k emulator? Well, anywhere? I wrote a simple one in C to test this idea, and it worked well, but really for this sort of thing you want assembly. I took PACE's emulator as a style guide, and did a <em>LOT</em> of work to produce a thumb2 68k emulator. It is much more efficient than PACE ever was. This is included in the "mkrom" folder as "PACE.0003.patch". As stated before, this is entirely optional and not required. But it does improve raw 68k speed by about 8.4x in the typical case.
</p>

<h2>But, you promised hardware...</h2>
<h3>Hardware has bugs</h3>
<p>I needed a dev board to play with. The STM32F429 discovery board seemed like a good start. It has 8MB of RAM which is enough, 2MB of flash which is good, a display with a touchscreen. Basically it is perfect on paper. Oh, if only I knew how imperfect the reality is. Reading the STM32F429 reference manual it does sound like the perfect chip for this project. And ST does not quite go out of their way to tell you where to find the problems. The errata sheet is damning. Basically if you make the CPU run from external memory, put the stack in external memory, and SDRAM FIFO is on, exceptions will crash the chip (incorrect vector address read). OK, I can work around that - just turn off the FIFO. Next erratum: Same story but if the FIFO is off, sometimes writes will be ignored and not actually write. Ouchy! Fine! I'll move my stacks to internal RAM. It is quite a rearchitecturing, but OK, fine! Still crashes. No errata about that! What gives? I removed <b>rePalm</b> and created a 20-line repro scenario. This is not in ST's errata sheet, but here is what I found: if <span>PC</span> points to external RAM, and <span>WFI</span> instruction is executed (to wait for interrupts in a low power mode), and then an interrupt happens after more than 60ms, the CPU will take a random interrupt vector instead of the correct one after waking up! Just imagine how long that took to figure out! How many sleepless nights ripping my hair out at random crashes in interrupt handlers that simply could not possibly be executing at that time! I worked around this by not using WFI. Power is obviously wasted this way, but this is ok for development for now, until I design a board with a chip that actually works!
</p>
<p>Next issue: RAM adddress. STM32F429 supports two banks of RAM 0 and 1. Bank 0 starts at <span>0xC0000000</span> and Bank 1 at <span>0xD0000000</span>. This is a problem because PalmOS needs both RAM and flash to be below <span>0x80000000</span>. Well, we're lucky. RAM Bank 0 is remappable to <span>0x00000000</span>. Sweet.... Until you realize that whoever designed this board hated us! The board only has one RAM chip connected, so logically it is Bank 0. Right? Nope! It is Bank 1, and that one is not remappable. Well, damn! Now we're stuck and this board is unusable to boot PalmOS. The <span>0x80000000</span> limit is rather set in stone.
</p>
<h3>So why the 0x80000000 limit?</h3>
<p>PalmOS has two types of memory chunks: movable and nonmovable. This is what an OS without access to an MMU does to avoid too much memory fragmentation. Basically when a movable chunk is not locked, the OS can move it, and one references it using a "handle". One can then lock it to get a pointer, use it, and then unlock when done. So what has this got to do with <span>0x80000000</span>? PalmOS uses the top bit of a pointer to indicate if it is a handle or an actual pointer. The top bit being set indicates a handle, clear indicates a pointer. So now you see that we cannot really live with RAM and ROM above <span>0x80000000</span>. But then again, maybe...
</p>
<h3>Two wrongs do not make a right, but do two nasty hacks?</h3>
<p>Given that I've already decided that this board was only for temporary development, why not go further? Handle-vs-pointer disambiguation is only done in a few places. Why not patch them to invert the condition? At least for now. No, not at runtime. I actually disassembled and hand-patched 58 places total. Most were in <span>Boot</span>, where the MemoryManager lives, a few were in <span>UI</span> since the code for text fields likes to find out of a pointer passed to it is a pointer (noneditable) or a handle (editable). There were also a few in <span><span>PACE</span></span> since m68k had a SysTrap to detemine the kind of pointer, which <span><span>PACE</span></span> implemented internally. Yes, this is not anymore "unmodified PalmOS" but this is only temporary, so I am willing to live with it! But, you might ask, didn't you also say that ROM <em>and</em> RAM both need to be below <span>0x80000000</span>? If we invert the condition, we need them both above. But flash is at <span>0x08000000</span>... Oops. Yup, we cannot use flash anymore. I changed the RAM layout again, carving out 2MB at <span>0xD0600000</span> to be the fake "ROM" and I copy the flash to it at boot. It works!
</p>


<h2>Tales of more PalmOS reverse engineering</h2>
<h3>SD-card Support</h3>
<p>Luckily, I had written a slot driver for PalmOS before, so writing an SD card driver was not hard. In fact, I reused some PowerSDHC source code! rePalm supports SD cards now on the STM32F469 dev board. On the STM32F429 board, they are also supported, but since the board lacks a slot, you need to wire them up yourself (CLK -&gt; C12, CMD -&gt; D2, DAT_0 -&gt; C8). Due to how the board is already wired, only one-bit-wide bus will work (DAT_1 and DAT_2 are used for other tthings and cannot be remapped to other pins), so that limits the speed. Also since your wires will be long and floppy, they maximum speed is also limited. This means that on the STM32F429 the speed is about 4Mbit/sec. On the STM32F469 board the speed is a much more respectable 37MBit/sec. Higher speeds could be reached with DMA, but this is good enough for now. While writing the SD card support for the STM32F4 chips, I found a hardware bug, one that was very hard to debug. The summary is this: SD bus allows the host to stop the clock anytime. So the controller has a function to stop it anytime it is not sending commands or sending/receiving data. Good so far. But that data lines can also be used to signal that the card is busy. Specifically, the DAT_0 line is used for that. The problem is that most cards use the clock line as a reference as to when they can change the state of the DAT lines. This means that if you do something that the card can be busy after, like a write, and then shut down the clock, the card will keep the DAT_0 line low forever, since it is waiting for the clock to tick to raise it. "So," you will ask, "why not enable clock auto-stopping except for this one command?" It does not work since clock auto-stopping cannot be easily flipped on and off. Somehow it confuses the module's internal state machine if it is flipped while the clock is running. So, why stop the clock at all? Minor power savings. Definitely not enough to warrant this mess, so I just disabled the auto-stopping function. A week to debug, and a one line fix! The slot driver can be seen in the "slot_driver_stm32" directory.
</p>
<h3>Serial Port Support</h3>
<p>Palm Inc did document how to write a serial port driver for PalmOS 4. There were two types: virtual drivers and serial drivers. The former was for ports that were not hardwired to the external world (like the port connected to the bluetooth chip or the Infra-red port), and the second for ports that were (like the cradle serial port). PalmOS 5 merged the two types into a unified "virtual" type. Sadly this was not documented. It borrowed from both port types in PalmOS 4. I had to reverse engineer the OS for a long time to figure it out. I produced a working idea of how this works on PalmOS 5, and you can see it in "vdrvV5.h" include file. This information is enough to produce a working driver for a serial port, IrDA SIR port, and USB for HotSync purposes.
</p>
<p>Actually making the serial port work on the STM32F4 hardwre was a bit hard. The hardware has only a single one-byte buffer. This means that to not lose any received data at high data rates, one needs to use hardware flow control or make the serial port interrupt the highest priority and hope for the best. This was unacceptable for me. I decided to use DMA. This was a fun chance to write my first PalmOS 5 library that can be used by other libraries. I wrote a DMA library for STM32F4-series chips. The code is in the "dma_driver_stm32" directory. With this, one would think that all would be easy. No. DMA needs to know how many bytes you expect to receive. In case of generic UART data receive, we do not know this. So how do we solve this? With cleverness. DMA can interrupt us when half of a transfer is done, and again when it is all done. DMA can be circular (restart from beginning when done). This gets us almost as far as we need to go. Basically as long as data keeps arriving, we'll keep getting one of these interrupts, and then the other in order. In our interrupt handler, we just need to see how far into the buffer we are, and report the bytes since last time we checked as new data. As long as our buffer is big enough that it does not overflow in the time it takes us to handle these interrupts we're all set, right? Not quite. What if we get just one byte? This is less than half a transfer so we'll never get an interrupt at all, and thus will never report this to the clients. This is unacceptable. How? STM32F4 UART has "IDLE detect" mode. This will interrupt us if after a byte has been RXed, four bit times have expired with no further character starting. This is basically just what we need. If we wire this interrupt to our previous handling code for the circular buffer, we'll always be able to receive data as fast as it comes, no matter the sizes. Cool! The Serial driver I produced does this, and can be seen in the "uart_driver_stm32" directory. I was able to successfully Hotsync over it! IrDA is supported too. It works well. See the photo album for a video demo!
</p>
<h4>Yes, you can try it!</h4>
<p>If you want to try, on the STM32F429 discovery board, the "RX" unpopulated 0.1 inch hole is the STM32's transmit (yes I know, weird label for a transmit pin). B7 is STM32's receive pin. If you connect a USB-to-serial adapter there, you can hotsync over serial. If you instead connect an IrDA SIR transceiver there, you'll get working IR. I used MiniSIR2 transceiver from Novalog, Inc. It is the same one as most Palm devices use.
</p>
<h3>Vibrate &amp; LED support</h3>
<p>Adding vibration and LED support was never documented, since those are hardware features that vendors handle. Luckily, I had reverse engineered this a long time ago, when I was <a href="http://www.palminfocenter.com/news/8274/adding-vibration-alarm-to-the-palm-tx/">adding vibration support to T|X</a>. Turns out that I almost got it all right back then. A bit more reverse engineering yielded a complete result of the proper API. LED follows the same API as vibrator: one "GetAttributes" function and one "SetAttributes" function. The settable things are the pattern, speed, delay in betweern repetitions, and number of repetitions. The OS uses them as needed and automatically adds "Vibrate" and "LED" settings to "Sounds and Alerts" preferences panel if it notices the hardware is supported. And rePalm now supports both! The code is in "halVibAndLed.c", feel free to peruse it at your leisure.
</p>
<h3>Networking support (WIP)</h3>
<h4>False starts</h4>
<p>I really wanted to add support for networking to rePalm. There were a few ways I could think of to do that, such that all existing apps would work. One could simply replace <span>Net.lib</span> with one with a similar interface but controlled by me. I could then wire it up to any interface I wanted to, and all would be magical. This is a poor approach. To start with, while large parts of <span>Net.lib</span> are documented, there are many parts that are not. Having to figure them out would be hard, and proving correctness and staying bug-compatible even more so. Then there is the issue with wanting to run an unmodified PalmOS. Replacing random libraries diminishes the ability to claim that. No, this approach would not work. The next possibility was to make a fake serial interface, and tell PalmOS to connect via it, via SLIP or PPP to a fake remote machine. The other end of this serial port could go to a thread that talks to our actual network interface. This can be made to work. There would be overhead of encoding and decoding PPP/SLIP frames, and the UI would be confusing and all wrong. Also, I'd need to find ways to make the config UI. This is also quite a mess. But at least this mess is achievable. But maybe there is a better approach?
</p>
<h4>The scary way forward</h4>
<p><em>Conceptually</em>, there is a better approach. PalmOS's <span>Net.lib</span> supports pluggable network interfaces (I call it a NetIF driver). You can see a few on all PalmOS devices: PPP, SLIP, Loopback. Some others also have one for WiFi or Cellular. So all I have to do is produce a NetIF driver. Sounds simple enough, no? Just as you'd expect, the answer is a strong, resounding, and unequivocal "no!" Writing NetIF drivers was never documented. And a network interface is a lot harder than a serial port driver (which was the previous plug-in driver interface of PalmOS that I had reverse engineered). Reverse engineering this would be hard.
</p>
<h4>Those who study history...</h4>
<p>I started with some PalmOS 4.x devices and looked at SLIP/PPP/Loopback NetIF drivers. Why? Like I had mentioned earlier, in 68k, the compiler tends to leave function names around in the binary unless turned off. This is a huge help in reverse engineering. Now, do not let this fool you, function names alone are not <em>that</em> much help. You still need to guess structure formats, parameters, etc. Thus despite the fact that <span>Net.lib</span> and NetIF driver interface both changed between PalmOS 4.x and PalmOS 5.x, figuring out how NetIF drivers worked in PalmOS 4.x would still provide some foundational knowledge. It took a few weeks until I thought I had that knowledge. Then I asked myself: "Was there a PalmOS 4.x device with WiFi?" Hm... There was. Alphasmart Dana Wireless had WiFi. Now that I thought I had a grip on the basics of how these NetIF drivers worked, it was time to look at a more complex one since PPP, SLIP, and Loopback are all very simple. Sadly, Alphasmart's developers knew how to turn off the insertion of function names into the binary. Their WiFi driver was still helpful, but it took weeks of massaging to make sense of it. It is approximately at this point that I realized that <span>Net.lib</span> had many versions and I had to look at others. I ended up disassembling each version of <span>Net.lib</span> that existed to see the evolution of the NetIF driver interface and <span>Net.lib</span> itself. Thus I looked at Palm V's version, Palm Vx's, Palm m505's, and Dana's. The most interesting changes were with v9, where support for ARP &amp; DHCP was merged into <span>Net.lib</span>, whereas previously each NetIF driver that needed those, embedded their own logic for them.
</p>
<h4>On to OS 5's Net.lib</h4>
<p>This was all nice and great, but I was not really in this to understand how NetIF drivers worked in PalmOS 4.x. Time had come to move on to reverse-engineering how PalmOS 5.x did it. I grabbed a copy of <span>Net.lib</span> from the T|T3, and started tracing out its functions, matching them up to their PalmOS 4.x equivalents. It took a few more weeks, but I more or less understood how PalmOS 5.x <span>Net.lib</span> worked.
</p>
<h4>I found a bug!</h4>
<p>Along the way I found an actual bug: a use-after-free in arp_close()
</p>
<p>NETLIB_T3:0001F580                 CMP             R4, #0        ; Linked list is empty?
NETLIB_T3:0001F584                 BEQ             loc_1F5A4     ; if so, lust skip this entire thing
NETLIB_T3:0001F588                 B               loc_1F590     ; else go free it one-by-one
NETLIB_T3:0001F58C
NETLIB_T3:0001F58C loc_1F58C:
NETLIB_T3:0001F58C                 BEQ             loc_1F598     ; this instr here is harmless, but makes no sense! We only get here on "NE" condition
NETLIB_T3:0001F590
NETLIB_T3:0001F590 loc_1F590:
NETLIB_T3:0001F590                 MOV             R0, R4        ; free the node
NETLIB_T3:0001F594                 BL              MemChunkFree  ; after this, memory pointed to by R4 is invalid (freed)
NETLIB_T3:0001F598
NETLIB_T3:0001F598 loc_1F598:
NETLIB_T3:0001F598                 LDR             R4, [R4]      ; load "-&gt;next" from now-invalid memory...
NETLIB_T3:0001F59C                 CMP             R4, #0        ; see if it is NULL
NETLIB_T3:0001F5A0                 BNE             loc_1F58C     ; and if not, loop to free that node too
NETLIB_T3:0001F5A4 loc_1F5A4:
</p>
<h4>Well, that was easy...</h4>
<p>Then I started disassembling PalmOS 5.x SLIP/PPP/Loopback NetIF drivers to see how they had changed from PalmOS 4.x. I assumed that nobody really changed their logic, so any changes I see could be hints on changed in the <span>Net.lib</span> and NetIF structure between PalmOS 4.x and PalmOS 5.x. It turned out that not that much had changed. Structures got realigned, a few attribute values got changed, but otherwise it was pretty close. It is at this point that I congratulated myself, and decided to start writing my own NetIF driver to test my understanding.
</p><h4>NOT!</h4>
<p>The self-congratulating did not last long. It turned out that in my notes I marked a few things I had thought inconsequential as "to do: look into this later". Well, it appears that they were not inconsequential. For example: the callback from DHCP to the NetIF driver to notify it of DHCP status was <em>NOT</em> purely informative as I had thought, and in fact a large amount of logic has to exist inside it. That logic, in turn, touches the insides of the DhcpState structure, half of which I had not fully understood since I thought it was opaque to the NetIF driver. Damn, well, back to IDA and more reverse engineering. At some point in time here, to understand what various callbacks between <span>Net.lib</span> and the NetIF driver did, I realized that I need to understand DHCP and ARP a lot better than I did. After sinking some hours into reading the DHCP and ARP RFCs, I dove back into the disassembled code. It all sort of made sense. I'll summarize the rest of the story: it took another three weeks to document every structure and function that ARP and DHCP code uses.
</p>
<h4>More reverse engineering</h4>
<p>There was just one more thing left. As the NetIF driver comes up, it is expected to show UI and call back into <span>Net.lib</span> at various times. Different NetIF drivers I disassembled did this in very different ways, so I was not clear as to what was the proper way to do this. At this point I went to my archive of all the PalmOS ROMs, and wrote a tool to find all the files with the type <span>neti</span>(NetIF drivers have this type), skip all that are PPP, SLIP, or Loopback, and copy the rest to a folder, after deduplicating them. I then disassembled them all, producing diagrams and notes about how each brought itself up and down, where UI was shown or hidden, and when each step was taken. While doing this, I saw some (but not much) logging in some of these drivers, so I was able to rename my own names for various values and structs to more proper ones that writers of those NetIF drivers were kind enough to leak in their log statements. I ended up disassembling: Sony's "CFEtherDriver" from the UX50, Hagiwara's WiFi memorystick driver "HNTMSW_neti", Janam's "WLAN NetIF" from the XP30, Sony's "CFEtherDriver" from the TH55, PalmOne's "PxaWiFi" from Tungsten C, PalmOne's "WiFiLib" from the TX, and PalmOne's "WiFiLib" from their WiFi SD card. Phew, that was a lot! Long story short: the reverse engineered NetIF interface is documented in "netIfaceV5.h" and it is enough that I think a working NetIF driver can be written using it.
</p>
<p>"You think?" you might ask, "have you not tested it?". Nope, I am still writing my NetIF driver so stay tuned...
</p>

<h3>1.5 density support</h3>
<h4>Density basics</h4>
<p><img src="https://dmitry.gr/images/rePalm1.5ddBad.png" alt="Bad rendered PalmOS"></p><p>PalmOS since version 4.2 has support for multiple screen densities. That is to say that one could have a device with a screen of the same size, but more pixels in it and still see things rendered at the same size, just with more detail. Sony did have high-res screens before Palm, and HandEra did before both of them, but Palm's solution was the first OS-scale one, so that is the one that PalmOS 5 used. The idea is simple. Each Bitmap/Window/Font/etc has a coordinate system associated with it, and all operations use that to decide how to scale things. 160x160 screens were termed 72ppi (no relation to actual points or inches), and the new 320x320 ones were 144ppi (double density). This made life easy - when the proper density image/font/etc was missing, one could pixel-double the low-res one. The reverse worked to. Pen coordinates also had to be adjusted of course since now the developer could request to work in a particular coordinate system, and the whole system API then had to.
</p>
<p>How was this implemented? A few coordinate systems are always in play: native (what the display is), standard (UI layout uses this), and active (what the user set using <span>WinSetCordinateSystem</span>). So given three systems, there are at any point in time 6 scaling factors to convert from any to any other. PalmOS 5.0 used just one. This was messy and we'll not talk about this further. Lets just say this solution did not stick. PalmOS 5.2 and later use 4 scaling factors, representing bidirectional transforms between active and native, and native and standard. Why not the third pair? It is used uncommonly enough that doing two transformations is OK. Since floating-point math is slow on ARMv5, fixed point numbers are used. Here there is a difference between PalmOS 5.2 and PalmOS 5.4. The former uses 16-bit fixed point numbers in 10.6 format, the latter uses 32-bit numbers in 16.16 format. I'll let you read up about fixed-point numbers on your own time, but the crux of the matter is that the number of fraction bits limits the precision of the number itself and the math you can do with it. Now, for precise powers of two, one does not need that many bits, so while there were only 72ppi an 144ppi screens, 10.6 was good enough, with scale factors always being 0x20 (x0.5), 0x40 (x1.0), and 0x80 (x2.0) . PalmOS 5.4 added support for one-and-a-half density due to the overabundance of cheap 320x240 displays at the time. This new resolution was specified as 108ppi, or precisely 1.5 times the standard resolution. Technically everything in PalmOS 5.2 will work as is, and if you give PalmOS 5.2 such a screen, it will more or less sort of work. To the right you can see what that looks like. Yes, not pretty. But it does not crash, and things sort of work as you'd expect. So why does it look like crap? Well, that scaling thing. Let's see what scale factors we might need now. First of all, PalmOS will not ever scale between 108 and 144ppi for bitmaps or fonts, so those scale factors are not necessary (rePalm will in one special case: to draw 144ppi bitmaps on 108ppi screen, when no 72ppi or 108ppi bitmap is available). So the only new scale factors introduced are between standard and 1.5 densities. From standard to 108ppi the scale factor is 1.5, which is representable as 0x60 in 10.6 fixed point format. So far so good, that is exact and math will work perfectly every time. But from 108ppi to 72ppi the scale factor is 2/3, which is <strong>NOT</strong> representable exactly in binary (no matter how many bits of precision you have). The simple rule with fixed-point math is that when your numbers are not representable exactly, your rounding errors will accumulate to more than one once the values you operate on are greater than one over your LSB. So for 10.6, the LSB is 1/64, so once we start working with numbers over 64, rounding will have errors of over one. This is a problem, since PalmOS routinely works with numbers over 64 when doing UI. Hell, the screen's standard-density width is 160. Oops... These accumulated rounding errors are what you see in that screenshot. Off by one here, off by one there, they add up to that mess. 108ppi density became officially supported in PalmOS 5.4. So what did they do to make it work? Switch to 16.16 format. The LSB there is 1/65536, so math on numbers up to 65536 will round correctly. This is good enough since all of PalmOS UI uses 16-bit numbers for coordinates.
</p>
<h4>How does it all fall apart?</h4>
<p>So why am I telling you all this? Well, PalmOS 5.4 has a few other things in it that make it undesirable for rePalm (rePalm can run PalmOS 5.4, but I am not interested in supporting it) due to NVFS, which is mandatory in 5.4. I wanted PalmOS 5.2 to work, but I also wanted 1.5 density support, since 320x240 screens still are quite cheap, and in fact my STM32F427 dev board sports one. We cannot just take Boot.prc from PalmOS 5.4 and move it, since that also brings NVFS. So what to do? I decided to take an inventory of every part of the OS that uses these scaling values. They are hidden inside the "Window" structure, so mostly this was inside <span>Boot</span>. But there are other ways to fuck up. For example in a few places in <span>UI</span>, sequences like this can be seen: <span>BmpGetDensity(</span> <span>WinGetBitmap(</span> <span>WinGetDisplayWindow()))</span>. This is clearly a recipe for trouble because code that was never written to see anything other than a 72 or a 144 as a reply is about to see a 108. But, some of that is harmless, if math is not being done with it. It can quite harmful, however, if it is used in math. I disassembled the <span>Boot</span> from a PalmOS 5.4 device (Treo 680) and one from a PalmOS 5.2 device (Tungsten T3). For each place I found in the T3 ROM that looked weird, I checked what the PalmOS 5.4 <span>Boot</span> did. That provided most of the places of worry. I then searched the PalmOS 5.4 ROM for any references to <span>0x6C</span> as that is 108 in hex, and a very unlikely constant to occur in code naturally for any other reason (luckily). I also looked at every single division to see if coordinate scaling was involved. This produced a complete list of all the places in the ROM that needed help. There were over 150...
</p>
<h4>How do we fix it?</h4>
<p>Patching this many places is doable, but what if tomorrow I decide to use the <span>Boot</span> from another device? No, this was not a good solution. I opted instead to write an OEM extension (a module that the OS will load at boot no matter what) and fix this. But how? If the ROM is read only, and we do not have an MMU to map a page over the areas we want to fix, how to fix them? Well, every such place is logically in a function. And every function is sometimes called. It may be called by a timer, a notification, be a thread, or be a part of what the user does. Luckily PalmOS only expect UI work form the UI thread, so <strong>ALL</strong> all them were only called from use-facing functions. Sadly some were buried quite deep. I got started writing replacement functions, basing them on what the <span>Boot</span> from PalmOS 5.4 did. For most functions I wrote <em>full</em> patches (that is my patch entirely replaces the original function in the dispatch table, never calling back to the original). I wrote 73 of those: <span>FntBaseLine</span>, <span>FntCharHeight</span>, <span>FntLineHeight</span>, <span>FntAverageCharWidth</span>, <span>FntDescenderHeight</span>, <span>FntCharWidth</span>, <span>FntWCharWidth</span>, <span>FntCharsWidth</span>, <span>FntWidthToOffset</span>, <span>FntCharsInWidth</span>, <span>FntLineWidth</span>, <span>FntWordWrap</span>, <span>FrmSetTitle</span>, <span>FrmCopyTitle</span>, <span>CtlEraseControl</span>, <span>CtlSetValue</span>, <span>CtlSetGraphics</span>, <span>CtlSetSliderValues</span>, <span>CtlHandleEvent</span>, <span>WinDrawRectangleFrame</span>, <span>WinEraseRectangleFrame</span>, <span>WinInvertRectangleFrame</span>, <span>WinPaintRectangleFrame</span>, <span>WinPaintRoundedRectangleFrame</span>, <span>WinDrawGrayRectangleFrame</span>, <span>WinDrawWindowFrame</span>, <span>WinDrawChar</span>, <span>WinPaintChar</span>, <span>WinDrawChars</span>, <span>WinEraseChars</span>, <span>WinPaintChars</span>, <span>WinInvertChars</span>, <span>WinDrawInvertedChars</span>, <span>WinDrawGrayLine</span>, <span>WinEraseLine</span>, <span>WinDrawLine</span>, <span>WinPaintLine</span>, <span>WinInvertLine</span>, <span>WinFillLine</span>, <span>WinPaintLines</span>, <span>WinGetPixel</span>, <span>WinGetPixelRGB</span>, <span>WinPaintRectangle</span>, <span>WinDrawRectangle</span>, <span>WinEraseRectangle</span>, <span>WinInvertRectangle</span>, <span>WinFillRectangle</span>, <span>WinPaintPixels</span>, <span>WinDisplayToWindowPt</span>, <span>WinWindowToDisplayPt</span>, <span>WinScaleCoord</span>, <span>WinUnscaleCoord</span>, <span>WinScalePoint</span>, <span>WinUnscalePoint</span>, <span>WinScaleRectangle</span>,
<span>WinUnscaleRectangle</span>, <span>WinGetWindowFrameRect</span>, <span>WinGetDrawWindowBounds</span>, <span>WinGetBounds</span>, <span>WinSetBounds</span>, <span>WinGetDisplayExtent</span>, <span>WinGetWindowExtent</span>, <span>WinGetClip</span>, <span>WinSetClip</span>, <span>WinClipRectangle</span>, <span>WinDrawBitmap</span>, <span>WinPaintBitmap</span>, <span>WinCopyRectangle</span>, <span>WinPaintTiledBitmap</span>, <span>WinCreateOffscreenWindow</span>, <span>WinSaveBits</span>, <span>WinRestoreBits</span>,
<span>WinInitializeWindow</span>. A few things were a bit too messy to replace entirely. An example of that was <span>PrvDrawControl</span> a function that makes up the guts of <span>CtlDrawControl</span>, but is also used in a lot of places like event handling for controls. What to do? Well, I can replace all callers of it: <span>FrmHandleEvent</span> and <span>CtlDrawControl</span>, but that does not help since <span>PrvDrawControl</span> itself has issues and is HUGE and complex. After tracing it very carefully, I realized that it only really cares about density in one special case, when drawing a frame of type <span>0x4004</span>, in which case it instead sets the coordinate system to native, and draws a frame manually, and then resets the coordinate system. So, what I did is set a special global before calling it if the frame type requested is that special one, and the frame drawing function, the one I had already rewritten (<span>WinDrawRectangleFrame</span>) then sees that flag and instead does this special one thing. The same had to be done for erasing frame type <span>0x4004</span>, and the same method was employed. The results? It worked!
</p>
<p><img src="https://dmitry.gr/images/rePalm1.5ddGood.png" alt="Well rendered PalmOS"></p><p>There was one more complex case left - drawing a window title. It was buried deep inside <span>FrmDrawForm</span> since a title is technically a type of a frame object. To intercept this without rewriting the entire function, before it runs, I converted a title object to a special king of a list object, and saved the original object in my globals. Why a list? <span>FrmDrawForm</span> will call <span>LstDrawList</span> on a list object, and will not peek inside. I then intercept <span>LstDrawList</span>, check for our magic pointer, if so, draw the title, else let the original <span>LstDrawList</span> function run. On the way out of <span>FrmDrawForm</span>, this is all undone. For form title setting functions, I just replaced them since they redraw the title manually, and I already had written a title drawing function. There was one small thing left: the little (i) icon on forms that have help associated with them. It looked bad when tapped. My title drawing function drew it perfectly, but the tap responce was handled by <span>FrmHandleEvent</span> - another behemoth I did not want to replace. I looked at it, and saw that the handling of the user taps on the help (i) icon was pretty early on. So, I duplicated that logic (and some that preceded it) in my patch for <span>FrmHandleEvent</span> and did not let the original function get that event. It worked perfectly! So thus we have four more partial patches: <span>LstDrawList</span>, <span>FrmDrawForm</span>, <span>FrmHandleEvent</span>, and <span>CtlDrawControl</span>.
</p>
<h4>And now, for some polish</h4>
<p>Still one thing was left to do: proper support for 1.5 density feature set as defined by the SDK. So: I modified the DAL to allow me to patch functions that do not exist in the current OS version at all, since some new ones were added after 5.2 to make this feature set work: <span>WinGetScalingMode</span> and <span>WinSetScalingMode</span>. Then I modified <span>PACE</span>'s 68k dispatch handler for sysTrapHighDensityDispatch to handle the new 68K trap selectors <span>HDSelectorWinSetScalingMode</span> and <span>HDSelectorWinGetScalingMode</span>, letting the rest of the old ones be handled by <span>PACE</span> as they were. I also got a hold of 108ppi fonts, and wrote some code to replace the system fonts with them, and I got a hold of 108ppi system images (like the alert icons) and made my extension put them in the right places.
</p>
<p>The result? The system looks pretty good! There are still things left to patch, technically, and "main.c" in the "Fix1.5DD" folder has a comment listing them, but they are all minor and the system looks great as is. The "Fix1.5DD" extension is part of the source code that I am releasing with rePalm, and you can see the comparison "after" screenshot just above to the right. It is about 4000 lines of code, in 77 patches and a bit of glue and install logic.
</p>

<h3>Dynamic Input Area/Pen Input Manager Services support</h3>
<h4>DIA/PINS basics</h4>
<p>PalmOS initially supported square screens. A few OEMS (Handera, Sony) did produce non-square screens, but this was not standard. Sony made quite a headway with their 320x480 Sony Clie devices. But their API was sony-only and was not adopted by others. When PalmOS 5.2 added support for non-square screens, Palm made an API that they called PINS (or alternatively DIA or AIA). It was not as good as Sony's API but it was official, and thus everyone migrated to it. Later sony devices were forced to support it too. Why was it worse? Sony's API was simple: collapse dynamic input area, or bring it back. Enable or disable the button to do so. Easy. Palm's API tries to be smart, with things like per-form policies, and a whole lot of mess. It also has the simple things: put area down or up, or enable or disable the button. But all those settings get randomly mutated/erased anytime a new form comes onscreen, which makes it a huge pain! Well, in any case. That is the public API. How does it all work? In PalmOS 5.4, this is all part of the OS proper, and integrated into <span>Boot</span>.
</p>
<h4>How it works pre-garnet</h4>
<p>But, as I had said, I was tergetting PalmOS 5.2. There, it was not a part of the OS, it was an extension. The DAL presents to the system a raw screen of whatever the actual resolution is (commonly 320x480) and the extension hides the bottom area from the apps and draws the dynamic input area on it. This requires some interception of some OS calls, like <span>FrmDrawForm</span> (to apply the new policy), <span>FrmSetActiveForm</span> (to apply policy to re-activated already drawn forms), <span>SysHandleEvent</span> (to handle events in the dynamic input area), and <span>UIReset</span> (to reset to defaults the settings on app switching). There are also some things we want to be notified about, like screen color depth change. When that happens, we may need to redraw the input area. That is the gist of it. There are a lot of small but significant specifics though.
</p>
<h4>The intricacies of writing a DIA implementation</h4>
<p>Before embarking on writing my own DIA implementation, I tried all the existing ones to see if they would support resolution other than 320x480. I do not want to write pointles code, afterall. None of them worked well. Even such simple things as 160x240 (direct 2x downscaling) were broken. Screens with different aspect ratios like the common 240x320 and 160x220 were even more broken. Why? I guess nobody ever writes generic code. It is simpler to just hack things up for "now" with no plan for "later". Well, I decided to write a DIA implementation that could support almost any resolution.
</p>
<p>When the DIA is collapsed, a status bar is shown. It shows small icons like the home button and menu button, as well as the button to unhide the input area. I tried to make everything as generic as possible. For every screen resolution possible, one can make a skin. A skin is a set of graphics depicting the DIA, as well as some integers describing the areas on it, and how they act (what key codes they send, what they do). The specifics are described in the code and comments and samples (3 skins designed to look similar to sony's UIs). They also define a "notification tray" area. Any app can add icons there. Even normal 68k apps can! I am including an example of this too. The clock you see in the status bar is actually a 68k app caled "NotifGeneral" and its source is provided as part of rePalm's source code! My sample DIA skins currently support 320x480 in double-density, 240x320 in 1.5 density, and 160x220 single density. The cool part? The same codebase supports all of these resolutions despite them having different aspect ratios. NotifGeneral also runs on all of those unmodified. Cool, huh? The source code for the DIA implementation is also published with rePalm, of course!
</p>


<h3>Audio support</h3>
<h4>PalmOS Audio basics</h4>
<p>Since PalmOS 1.0, there has been support for simple sound via a piezo speaker. That means simple beeps. The official API allows one to: play a MIDI file (one channel, square waves only), play a tone of a given volume and amplitude (in background or in foreground), and stop the tone. In PalmOS 5.0, the low level API that backs this simple sound API is almost the same as the high-level official API. <span>HALSoundPlay</span> is used to start a tone for a given duration. The tone runs in the background, the func itself returns directly and immediately. If another tone had previously been started, it is replaced with the new one. A negative duration value means that the tone will never auto-stop. <span>HALSoundOff</span> stops a currently-playing tone, if there is one. <span>HALPlaySmf</span> plays a MIDI tune. This one is actually optional. If the <span>DAL</span> returns an error, <span>Boot</span> will interpret the MIDI file itself, and make a series of calls to <span>HALSoundPlay</span>. This means that unless you have special hardware that can play MIDI better than simple one-channel square waves, it makes no sense to implement <span>HALPlaySmf</span> in your <span>DAL</span>.
</p>
<h4>PalmOS sampled sudio support</h4>
<p>Around the time PalmOS 5.0 came out, the sampled sound API made an appearance. Technically it does not require PalmOS 5.0, but I am not aware of any Palm OS 4 device that implement this API. There were previous vendor-specific audio APIs in older PalmOS releases, but they were nonstandard and generally depended on custom hardware accelerator chips, since 68k processor is not really fast enough to decode any complex audio formats. The sampled sound API is obviously more complex than the simple sound API, but it is easily explained with the concept of streams. One can create an input or output stream, set volume and pan for it, and get a callback when data is available (input) or needed (output). For output streams, the system is expected to mix them together. That means that more than one audio stream may play at the same time and they should all be heard. Simple sound API should also work concurrently. PalmOS never really required support for more than one input stream, so at least that is nice.
</p>
<p>A stream (in or out) has a few immutable properties. The three most important ones are the sample rate, the channel number, and the sample format. The sample rate is basically how many samples per second there are. CD audio uses 44,100 per second, most DVDs use 48,000 per second, and cheap voice recorders use 8,000 (approximately telephone quality). PalmOS support only two channel widths: 1 and 2. These are commonly known as "mono", and "stereo". Sample type is a representation of how each sample is represented in the data stream. PalmOS API documents the following sample types: signed and unsigned 8-bit values, signed 16-bit values of any endianness, signed 32-bit values of any endianness, single-precision floating point values of any endianness. As far as I can tell, the only formats ever supported by actual devices were the 8 and 16-bit ones.
</p>
<h4>Why audio is hard &amp; how PalmOS makes it easy</h4>
<p>Mixing audio is hard. Doing it in good quality is harder, and doing it fast is harder yet. Why? The audio hardware can only output one stream, so you need to mix multiple streams into one. Mixing may involve format conversion, for example if hardware needs signed 16-bit little-endian samples and one of the streams is in float format. Mixing almost certainly involves scaling since each stream has a volume and may have a pan applied. And, hardest of all, mixing may involve resampling. If, for example, the hardware runs at 48,000 samples per second, and a client requested to play a stream with 44,100 samples per second, more samples are needed than are provided - one needs to generate more samples. This is all pretty simple to do, if you have large buffers to work with, but that is also a bad idea, since that adds a lot of latency - the larger your buffer, the more time passes between the app providing audio data and the audio coming out the speaker. In the audio world, you are forced to work with relatively small buffers. Users will also notice if you are late delivering audio samples to the hardware (they'll hear it). This means that you are always on a very tight schedule when dealing with audio.
</p>
<p>What do existing PalmOS <span>DAL</span>s do to address all this difficulty? Mostly, they shamelessly cut corners. All existing <span>DAL</span>s have a very bad resampler - it simply duplicates samples as needed to upsample (convert audio to a higher sampling rates), and drops samples as needed to downsample (convert audio to a lower sampling rates). Why is this bad? Well, when resampling between sample rates that are close to each other in this manner, this method will introduce noticeable artifacts. What about format conversions? Well, only supporting four formats is pretty easy - the mixing code was duplicated four times in the <span>DAL</span>, once for each time.
</p>
<h4>How rePalm does audio mixing</h4>
<p>I wanted rePalm to produce good audio quality, and I wanted to support all the formats that PalmOS API claimed were supported. Actually, I ended up supporting even more formats: signed and unsigned 8, 16, and 32-bit integer, as well as single-precision floating-point samples in any endianness. For sample rates, rePalm's mixer supports: 8,000, 11,025, 16,000, 22,050, 24,000, 32,000, 44,100, and 48,000 samples per second. The format the output hardware uses is decided by the hardware driver at runtime in rePalm. Mono and stereo hardware is supported, any sample rate is supported, and any sample format is supported for native hardware output. If you now consider the matrix of all the possible stream input and output formats, sample rates, and channel numbers, you'll realize that it is a very large matrix. Clearly the PalmOS approach of duplicating the code 4 times will not work, since we'd have to duplicate it hundreds or thousands of times. The alternative approach of using generic code that switches based on the types is too slow (the switching logic simply wastes too many cycles per sample). No simple solutions here. But before we even get to resampling and mixing, we need to work out how to deal with buffering.
</p>
<p>The initial approach involved each channel having a single circular buffer that the client would write and the mixer would read. This turned out to be too difficult to manage in assembly. Why in assembly? We'll get to that soon. The final approach I settled on was actually simpler to manage. Each stream has a few buffers (buffer depth is currently defined to be four), and after any buffer is 100% filled, it is sent to the mixer. If there are no free buffers, the client blocks (as PalmOS expects). If the mixer has no buffers for a stream, the stream does not play, as PalmOS API specifies. This setup is easy to manage from both sides, since the mixer now never has to deal with partially-filled buffers or sorting out the circular-buffer wraparound criteria. A semaphore is used to block the client conveniently when there are no buffers to fill. "But," you might ask, "what if the client does not give a full buffer's worth of data?" Well, we do not care. Eventually if the client wants the audio to play, they'll have to give us more samples. And in any case, remember how above we discussed that we have to use small buffers? Any useful audio will be big enough to fill at least a few buffers.
</p>
<p>One mustn't forget that supporting sampled sound API does not absolve you from having to support simple sound functions. rePalm creates a sound stream for simple sound support, and uses it to play the required tones. They are generated from an interpolated sine wave at request time. To support doing this without any pesky callbacks, the mixer supports special "looped" channels. This means that once the data buffer is filled, it is played repeatedly until stopped. Since at least one complete wave must fit into the buffer, rePalm refuses to play any tones under 20Hz. This is acceptable to me.
</p>
<h4>How do assembly and audio mix?</h4>
<p>The problem of resampling, mixing, and format conversion loomed large over me. The naive approach of taking a sample from each stream, mixing it into the output stream, and then doing the same for the next stream is too slow, due to the constant "switch"ing required based on sample types and sample rates. Resampling is also complex if done in good (or at least passable) quality. So what does rePalm's <span>DAL</span> do? For resampling, a large number of tables are used. For upsampling, a table tells us how to linearly interpolate between input samples to produce output samples.  One such carefully-tuned table exists for each pair of frequencies. For downsampling, a table tells us how many samples to average and at what weight. One such table exists for each pair of frequencies. Both of these approaches are strictly better than what PalmOS does. But, if mixing was already hard, now we just made it harder. Let's try to split it into chewable chunks. First, we need an intermediate format - a format we can work with efficiently and quickly, without serious data loss. I picked signed 32-bit fixed point with 8 integer bits and 24 fraction bits. Since no PalmOS device ever produced audio at more than 24-bit resolution, this is acceptable. The flow is <em>conceptually</em> simple: first zero-fill an intermediate buffer. Then, for each stream for which we have buffers of data, mix said buffer(s) into the intermediate buffer, with resampling as needed. Then clip the intermediate buffer's samples, since mixing two loud streams can produce values over the maximum allowed. And, finaly, convert the intermediate buffer into the format hardware supports, and hand it off to the hardware. rePalm does not bother with a stereo intermediate buffer if the audio hardware is mono only. The intermediate buffer is only in stereo if the hardware is! How do we get this much flexibility? Because of how we mix things into it.
</p>
<p>The only hard part from above is that "mix buffers into the intermediate buffer with resampling" step. In fact, not only do we need to resample, but we also need to apply volume, pan, and possibly convert from mono to stereo or from stereo to mono. The most optimal approach is to write a custom well-tuned mix function for every possible combination of inputs and outputs. The number of combinations is dizzying. Input has 8 possible rates, 2 possible channel configs, and 12 possible sample types. Output has 8 possible rates and 2 possible channel configs. This means that there is a total of just over 3,000 combinations (8 * 2 * 12 * 8 * 2). I was not going to write 3072 functions by hand. In fact, even auto-generating them at build time (if I were to somehow do that) would bloat rePalm's <span>DAL</span>'s code size to megabytes. No, another approach was needed.
</p>
<p>I decided that I could reuse some things I learned while I was writing the JIT, and also reuse some of its code. That's right! When you create a stream, a custom mix function is created just for that stream's configuration, and for your hardware's output configuration. This custom assembly code uses all the registers optimally and, in fact, it manages to use no stack at all! The benefit is clear! The mixing code is always optimal since it is custom for your configuration. For example, if the hardware only supports mono output, the mixing code will downmix before upsampling (to do it to fewer samples), but will only downmix after downsampling (once again, so less math is needed). Since there are three major cases: upsampling, downsampling, and no-resampling, there are three paths through the codegen to produce mix functions. Each mix function matches a very simple prototype: <span>int32_t* (*MixInF)(int32_t* dst, const void** srcP, uint32_t maxOutSamples, void* resampleStateP, uint32_t volumeL, uint32_t volumeR, uint32_t numInSamples)</span>. It returns the pointer to the first intermediate buffer sample NOT written. <span>srcP</span> is updated to point to the first input audio sample not consumed, <span>maxOutSamples</span> limits how many audio samples may be produced, <span>numInSamples</span> limits how many audio samples may be consumed. Mix functions return when either limit is reached. Resampling logic may have long-lived state, so that is stored in a per-stream data structure (5 words), and passed in as <span>resampleStateP</span>. The actual resample table pointer is encoded in the function itself (for speed), since it will never change. Why? Because the stream's sample rate is constant, and the hardware will not magically grow ability to play at another sample rate at a later time. The stream's volume and pan, however, may be changed anytime, so they are not hardcoded into the function body. They are provided as parameters at mixing time. I actually considered hardcoding them in, and re-generating the mix function anytime the volume or pan changed, but the gain would have been too small to matter, so I decided against it. Instead we simply pre-calculate "left volume" and "right volume" from the user settings of volume" and "pan" and pass them to the mix function.
</p>
<p>Having a mix function that nice makes the rest of the mixer easy. Simply: call the mix function for each non-paused stream as long as there are buffers to consume and the output buffer is not full. If we fully consume a buffer, release it to the user. If not, just remember how many samples in there we haven't yet used for later. That is all! So does all this over-complex machinery work? Yes it does! The audio mixer is about 1,500 lines, BUT it can resample and mix streams realtime at under 3 million cycles per stream per second, which is much better than PalmOS did, and with better quality to boot! The code is in "audio.c".
</p>

<h4>rePalm's audio hw driver architecture</h4>
<p>rePalm's audio hardware layer is very simple. For simple sound support, one just provides the funcs for that and the sound layer clals them directly. For sampled audio, the audio init function tells the audio mixer the native channel number and sample rate. What about native sample format? The code provides an inline function to convert a sample from the mixer's intermediate format (8.24 signed integer) to whatever format the hardware needs. Thus, the hardware's native sample format is defined by this inline function. At init time the hw layer provides to the mixer all this info, as well as the size of the hardware audio buffer. This buffer is needed since interrupts have latency and we need the audio hw to always have some audio to play.
</p>
<p>On the STM32F429 board, audio output is on pin A5. The audio is generated using a PWM channel, running at 48,000 samples per second, in mono mode. Since the PWM clock runs at 192MHz, if we want to output 48,000 samples per second, the PWM unit will only be able to count to 4000. Yes, indeed, for this board, since it lacks any real audio output hardware, we're stuck with just about 12-bit precision. This is good enough for testing purposes and actually doesn't sound all that bad. The single-ended output directly from the pin of the microcontroller cannot provide much power, but with a small speaker, the sound is clear and sounds great! I will upload an image with audio support soon.
</p>
<p>On reSpring, the CPU clock (and thus PWM clock) is at 196.6MHz. Why this weird frequency? Because it is precisely 48,000 x 4096. This allows us to not need to scale audio in a complex fashion, like we do on the STM32F429 board. Just saturating it to 12 bits will work. Also, on reSpring, two pins are used to output audio, in opposite polarity, this gives us twice the voltage swing, producing louder sounds.
</p>
<h4>Microphone</h4>
<p>I did not implement a mixer/resampler for the microphone - PalmOS never supported more than one user of a microphone at a time, so why bother? - no apps will do so. Instead, whichever sampling rate was requested, I pass that to the hardware driver and have it actually run at that sampling rate. As for sample type, same as for audio out, a custom function is generated to convert the sample format from the input (16 bit little-endian mono), to whatever the requested format was. The generated code is pretty tight and works well!
</p>

<h3>Zodiac support</h3>
<h4>Tapwave Zodiac primer</h4>
<p><a href="https://en.wikipedia.org/wiki/Tapwave_Zodiac">Tapwave Zodiac</a> was a rather unusual PalmOS device released in 2003. It was designed for gaming and had some special hardware just for that: landscape screen, an analog stick, a Yamaha Midi chip, and an ATI Imageon W4200 graphics accelerator with dedicated graphics RAM. There was a number of Tapwave-exclusive titles released that used the new hardware well, including some fancy 3D games. Of course this new hardware needed OS support. Tapwave introduced a number of new APIs, and, luckily, documented them quite well. The new API was quite well designed and easy to follow. The documentation was almost perfect. Kudos, Tapwave! Of course, I wanted to support Tapwave games in rePalm.
</p>
<h4>The reverse engineering</h4>
<p>Tapwave's custom API were all exposed via a giant table of function pointers given to all Tapwave-targetting apps, after they pass the signature checks (Tapwave required approvals and app signing). But, of course, somewhere they had to go to some library or hardware. Digging in, it became clear that most of them go to <span>Tapwave Application Layer</span>(<span>TAL</span>). This module is special, in that on the Zodiac, like the <span>DAL</span>, <span>Boot</span>, and <span>UI</span>, the <span>TAL</span> can be accessed directly off of <span>R9</span> via <span>LDR R12, [R9, #-16]; LDR PC, [R12, #4 * tal_func_no]</span>. But, after spending a lot of time in the <span>TAL</span>, I realized that it was just a wrapper. All the other libraries were too: <span>Tapwave Midi Library</span> and <span>Tapwave Multiplayer Library</span>. All the special sauce was in the DAL. And, boy, was there a lot of special sauce. Normal PalmOS DALs have about 230 entrypoints. Tapwave's has 373!
</p>
<p>A lot of tracing through the <span>TAL</span>, and a lot of trawling through the CPU docs got me the names and params to most of the extra exported <span>DAL</span> funcs. I was able to deduce what all but 14 functions do! And as for those 14: I could find no uses of any of them anywhere in the device's software! The actual implementations underneath matter a bit less since I am just reimplementing them. My biggest worries were, of course, the graphics acceleration APIs. Turned out that that part was the easiest!
</p>
<h4>The "GPU"</h4>
<p>Zodiac's graphics accelerator was pretty fancy for a handheld device at the time, but it is also quite basic. It has 8MB of memory built in, and accelerates only 2D operations. Basically, it can: copy rectangles of image data, blend rectangles between layers with constant or parametric alpha blending, do basic bilinear resizing, and draw lines, rectangles, and points. It operates only on 16-bit RGB565LE layers. This was actually quite easy to implement. Of course doing this in software would not be fast, but for the purposes of my proof of concept, it was good enough. A few days of work, and ... it works! A few games ran.
</p>
<p>Next step is still in-progress: using the DMA2D unit in the STM32 to accelerate most of the things the ATI chip can do. Except for image resizing, it can do them all in one pass or two! For extra credit, it can also operate in the background like the ATI chip did to the CPU in the Zodiac. But that is for later...
</p>
<h4>Other Tapwave APIs</h4>
<p>Input subsystem in the Zodiac was quite special and required some work. Instead of the usual PalmOS methods of reading keys, touch, etc, they introduced a new "input queue" mechanism that allowed all of these events to be delivered all into one place. I had to reimplement this from nothing but the documented high level API and disassembly. It worked: rePalm now has a working implementation of TwInput and can be used as reference for anyone who also for some reason wants to implement it.
</p>
<p>TwMidi was mostly reverse engineered in a week. But I did not write a midi sequencer. I could and shall, but not yet. The API is known and that is as far as I needed to go to return proper error codes to allow the rest of the system to go on.
</p>

<h2>Real hardware: reSpring</h2>
<h3>The ultimate Springboard accessory</h3>
<p>Back when <a href="https://en.wikipedia.org/wiki/Handspring_(company)">Handspring</a> first released the Visor, its <a href="https://en.wikipedia.org/wiki/Springboard_Expansion_Slot">Springboard Expansion Slot</a> was one of the most revolutionary features. It allowed a few very cool expansion devices, like <a href="https://web.archive.org/web/20190921012909/https://www.zdnet.com/product/handspring-visorphone/">cellular phones</a>, <a href="https://web.archive.org/web/20110111075957/http://www.agentland.com/Store/129.html">GPS receivers</a>, <a href="https://web.archive.org/web/20080515225204/http://www.visorcentral.com/content/Stories/1329-1.htm">barcode readers</a>, <a href="https://web.archive.org/web/20160314022653/https://the-gadgeteer.com/2001/06/29/innopocket_flashplus_plug_and_play_compact_flash_springboard_adapter">expansion card readers</a>, and <a href="https://web.archive.org/web/20190403061919/https://the-gadgeteer.com/2000/04/07/eyemodule_visor_springboard_review/">cameras</a>. Springboard slot is cool because it is a literal direct connection to the CPU's data and address bus. This provides a lot of expansion opportunities. I decided that the first application of rePalm should be a Springboard accessory that will, when pluged in, upgrade a Visor to PalmOS 5. The idea is that reSpring will run rePalm on its CPU, and the Visor will act as the screen, touch, and buttons. I collaborated with <a href="mailto:george.rudolf.mezzomo@gmail.com">George Rudolf Mezzomo</a> on reSpring, with me setting the specs, him doing the schematics and layout, and me doing the software and drivers.
</p>
<h3>Interfacing with the Visor</h3>
<p>To the Visor, the sprinboard module looks like two memory areas (two chip select lines), each a few megabytes large at most. The first must have a valid ROM image for the Visor to find, structured like a PalmOS ROM memory, with a single heap. Usually that heap contains a single application - the driver for this module. The second chip select is usually used to interface to whatever hardware the Springboard unit has. For reSpring I decided to do things differently. There were a few reasons. The main reason was that a NOR flash to store the ROM would take up board space, but also because I really did not want to manage so many different flashable components on the board. There was a third reason too, but we'll need to get back to that in a bit.
</p>
<p>The Visor expects to interface with the Springboard by doing memory accesses to it (reads and writes) and the module is expected to basically behave like a synchronous memory device. That means that there is no "I am ready to reply" line, instead you have a fixed number of cycles to reply to any request. When a module is inserted, the Visor configured that number to be six, but it can then be lowered by the module's driver app. Trying to reply to requests coming in with a fixed (and very short) deadline would be a huge CPU load for our ARM CPU. I decided that the easiest way to accomplish this is to actually put a RAM there, and let the Visor access that. But, then, how will we access it, if the Visor can do so anytime? Well, there are special types of RAM that allow this.
</p>
<p>Yes, the elusive (and expensive) dual-ported RAM. I decided that reSpring would use a small amount of dual-ported RAM as a malbox between the Visor and rePalm's CPU.  This way the Visor could access it anytime, and so could rePalm. The Springboard slot also has two interrupt request lines, one to the Visor, one to the module. These can be used to signal when a message is in the mailbox. There are two problems. The first is that dual-ported RAMs are usually large, mostly due to the large number of pins needed. Since the Visor needs a 16-bit-wide memory in the Springboard slot, our hypotherical dual-ported RAM would need to be 16-bit wide. And then we need address lines, control lines, byte lane select lines, and chip select lines. If we were to use a 4KB memory, for example, we'd need 11 address lines, 16 data lines, 2 byte lane select lines, one chip select line, one output enable line, and one write enable line, PER PORT! Add in at least two power pins, and our hypothetical chip is a 66-pin monstrosity. Since 66-pin packages do not exist, we're all in for a 100-pin part. And 4KB is not even much. Ideally we'd like to fit our entire framebuffer in there to avoid complex piecewise transfers. Sadly, as the great philosopher Jagger once said, "You can't always get what you want." Dual-ported RAMs are <em>very</em> expensive. There are only two companies making them, and they charge <em>a lot</em>. I settled on the 4KB part purely based on cost. Even at this measly 4KB size, this one RAM is <em>by far</em> the most expensive component on the board at $25. Given that the costs of putting in a 64KB part (my preferred size) were beyond my imagination (and beyond my wallet's abilities), I decided to invent a complex messaging protocol and make it work over a 4KB RAM used as a bidirectional mailbox.
</p>
<p>But, let us get back to our need for a ROM to hold our driver program. Nowhere in the Sprinboard spec is there actually a requirement for a ROM, just a memory. So what does that mean? We can avoid that extra chip by having the reSpring CPU contain the ROM image inside it, and quickly write it into the dual-ported RAM on powerup. Since the Visor gives the module up to three seconds to produce a valid card header, we have plenty of time to boot up and write the ROM to our RAM. One chip fewer to buy and place on the board is wonderful!
</p>
<h3>Version 1</h3>
<p>I admit: there was a bit of feature creep, but the final hardware design for version 1 ended up being: 8MB of RAM, 128MB of NAND flash, a 192MHz CPU with 2MB of flash for the OS, a microSD card slot, a speaker for audio out, and an amplifier to use the in-Visor microphone for audio in. Audio out will be done the same way as on the STM32F429 board, audio in will be done via the real ADC. The main RAM is on a 32-bit wide bus running at 96MHz (384MB/s bandwidth). The NAND flash is on a QSPI bus at 96MHz (48MB/s bandwidth). The OS will be stored in the internal flash of the STM32F469 CPU. The onboard NAND is just an exploration I would like to do. It will either be an internal SD card, or maybe storage for something like NVFS(but not as unstable), when I've had time to write it.
</p>
<p>So, when is this happening? Five version 1 boards were delivered to me in late November 2019!
</p>
<h3>Bringup of v1</h3>
<p>Having hardware in-hand is great. It is greater yet when it work right the vey first time. Great like unicorns, and just as likely. Nope... nothing worked right away. The boards did not want to talk to the debugger at all, and after weeks of torture, I realized some pull ups and downs were missing from the boards. This was not an issue on STM's dev boards since they include these pull ups/downs. Once the CPU started talking to me, it became evident very quickly that it was very very unstable. It is specified to run at 180MHz (yes, this means that normally we are overclocking it by 9.2% to 196.6MHz). On the reSpring boards the CPU would not run with anystability over 140MHz. I checked power supply, and decoupling caps. All seemed to be in place, until... No VCAP1 and VCAP2. The CPU core runs at a lower voltage than 3.3V, so the CPU has an internal regulator. This regulator needs capacitors to stabilize its output in the face of variable consumption by the CPU. That is what VCAP1 and VCAP2 pins are for. Well, the board had no capacitors on VCAP1 and VCAP2. The internal regulator output was swinging wildly (+/- 600mV on a 1.8V supply is a <em>lot</em> of swing!). In fact, it is amazing that the CPU ran at all with such an unstable supply! Well, after another rework under the microscope with two capacitors were added, the board was stable. On to the next problem...
</p>
<p>The next issue was SDRAM. The main place the code runs from and data is stored. The interface seemed entirely borked. Any word that was written, the 15th bit would always read as 1, and 0th and 1st bits would always read as a zero. Needless to say, this is not acceptable for a RAM which I hoped to run code from. This was a giant pain to debug, but in the end it there out to be a typo in GPIO config not mapping the two lower bits to be SDRAM DQ0 and DQ1. This left only bit 15 stuck high to resolve. That issue did not replicate on other boards, so that was a local issue to one board. A lot of careful microscoping revealed a gob of solder under the pin left from PCBA, which was shorting to a nearby pin that was high. Lifting the pin, wicking the solder off, and reconnecting the pin to the PCB resolved this issue. SDRAM now worked. Since this SDRAM was quite different than the one on the STM32F429 discovery board, I had to dig up the configs to use for it, and translate between the timings STM uses and the RAM datasheet uses to come up with proper settings. The result was quite fast SDRAM which seems stable. Awesome!
</p>
<p>Of course this was not nearly the end of it. I could not access the dual-ported SRAM at all. A quick check with the board layout revelaed that its chip select pin was not at all wired to the STM. Out came the microscope and soldering iron, and a wire was added. Lo and behold, SRAM was accessible. More datasheet reading ensued to configure it properly. While doing that, I noticed that it's power consumption is listed as <em>"low"</em>, just 380 mW!!! So not only is this the most expensive chip on the board, it is also the most power hungry! It really needs to go!
</p>
<p>I can tell you of more reworks that followed after some in-Visor testing, just to keep all the rework story together. It turned out that the line to interrupt the visor was never connected anywhere, so I wired that up to PA4, so that reSpring could send an IRQ to the visor. Also it turned out that SRAM has a lot of "modes" and it was configured for the wrong one. Three separate pins had to be reworked to switch it from "master" mode into "slave" mode. These modes configure how multiple such SRAMs can be used together. As reSpring only has one, logically it was configured as master. This turns out to have been wrong. Whoops.
</p>
<h3>Let's stick it into a Visor?</h3>
<h4>Getting recognized</h4>
<p><img src="https://dmitry.gr/images/rePalm-visor-boot-1.jpg" alt="reSpring module recognized by the Visor"></p><p>So simple, right? Just stick it into the Visor and be done with it? Reading and re-reading the Handspring Springboard Development Guide provided almost all the info needed, in theory. Practice was different. For some reason, no matter how I formatted the fake ROM in the shared SRAM, the Visor would not recognize it. Finally I gave  up on this approach, and wrote a test app to just dump what the Visor sees to screen, in a series of messageboxes. Springboard ROM is always mapped at <span>0x28000000</span>. I quickly realized the issues. First, the visor Springboard byteswaps all accesses. This is because most of the world is little-endian, while the 68k CPU is big-endian. To allow peripheral designers to not worry, Handspring byteswaps the bus. "But," you might say, "what about non-word accesses?" There are no such accesses. Visor always accesses 16 bits at a time. There are no byte-select lines. For us this is actually kind of cool. As long as we communicate using only 16-bit quantities, no byteswapping in software is needed. There was another issue: the Visor saw <em>every other</em> word that reSpring wrote. This took some investigation, but the result was both hilarious and sad at the same time. Despite all accesses to Springboard being 16-bit-wide, address line 0 is wired to the Springboard connector. Why? Who knows? But it is always low. On reSpring board, Springboard connector's A0 was wired to RAM's A0. But since it is always 0, this means the Visor can only access every other word of RAM - the even addresses. <em>...sigh...</em> So we do not have 4K of shared RAM. We have 2K... But, now that we know all this, can we get the visor to recognize reSpring as a Springboard module? <em>YES!</em>. The image on the right was taken the first time the reSpring module was recognized by the Visor.
</p>
<h4>Saving valuable space</h4>
<p>Of course, this was only the beginning of the difficulties. Applications run right from the ROM of the module. This is good and bad. For us this is mostly bad. What does this mean? The ROM image we put in the SRAM must remain there, forever. So we need to make it as small as possible. I worked very hard to minimize the size, and got it down to about 684 bytes. Most of my attempts to overlap structures to save space did not work - the Visor code that validates the ROM on the Springboard module is merciless. The actual application is tiny. It implements the simplest possible messaging protocol (one word at a time) to communicate with the STM. It implements no graphics support and no pen support. So what <em>does</em> it do? It downloads a larger piece of code, one word at a time, from the STM. This code is stored in the Visor's RAM and can run from there. It then simply jumps to that code. Why? This allows us to save valuable SRAM space. So we end up with 2K - 684bytes = 1.3K of ram for sending data back and forth. Not much but probably passable.
</p>
<h4>Communications</h4>
<p>So, we have 1.3KB of shared RAM, an interrupt going each way, how do we communicate? I designed two communications protocols: a simple one and a complex one. The simple one is used only to bootstrap the larger code into Visor RAM. It sends a single 16-bit message and gets a single 16-bit response. The messages implemented are pretty basic: a request to reply - just to check comms, a few requests to get information on where in the shared memory the large mailboxes are for the complex protocol, a request for how big the downloaded code is, and the message to download the next word of code. Once the code is downloaded and knows what the locations and sizes of mailboxes are, it uses the complex protocol. How does it differ? A large chunk of data is placed in the mailbox, and then the simple protocol is used to indicate a request and get a response. The mailboxes are unidirectional, and sized very differently. The STM-to-Visor mailbox occupies about 85% of the space, while the mailbox in the other direction is tiny. The reason is obvious - screen data is large.
</p>
<p>All requests are always originated from the Visor and get a response from the reSpring module. If the module has something to tell the Visor, it will raise an IRQ, and the visor will send a request for the data. If the visor has nothing to send, it will simply send an empty NOP message. How does the Visor send a request? First, the data is written to the mailbox, then the message type is written to a special SRAM location, and then a special marker indicating that the message is done is written to another SRAM location. An IRQ is then raised to the module. The IRQ handler in the STM looks for this "message valid" marker, and if it is found the message is read and replied to: first the data is written to the mailbox, then message type is written to the shared SRAM location for message type, and then the "this is a reply" marker is written to the marker SRAM location. This whole time, the Visor is simply loop-reading the marker SRAM location waiting for it to change. Is this busy waiting a problem? No. The STM is so fast, and the code to handle the IRQ does so little processing that the replies often come in microseconds.
</p>
<p>A careful reading of the Handspring Springboard Development Guide might leave you with a question: "what exactly do you mean when you say 'interrupt to the module'? There are no pins that are there for that!" Indeed. There are, however, two chip-select lines going to the module. The first must address the ROM (SRAM for us). The chip-select line second is free for the module to use. Its base address in Visor's memory map is <span>0x29000000</span>. We use that as the IRQ to the STM, and simply access <span>0x29000000</span> to cause an interrupt to the STM.
</p>
<h4>Early Visor support</h4>
<p>At this point, some basic things could be tested, but they all failed on Visor Deluxe and Visor Solo. In fact, everything crashed shortly after the module was inserted. Why? Actually the reason is obvious - they run PalmOS 3.1, while all other Visors ran PalmOS 3.5. A surprising number of APIs one comes to rely on in PalmOS programming are simply not available on PalmOS 3.1. Such simple things like <span>ErrAlertCustom()</span>, <span>BmpGetBits()</span>, <span>WinPalette()</span>, and <span>WinGetBitmap()</span> simply do not exist. I had to write code to avoid using these in PalmOS 3.1. But some of them are needed. For example, how do I directly copy bits into the display framebuffer if I cannot get a pointer to the framebuffer via <span>BmpGetBits( WinGetBitmap( WinGetDisplayWindow ()))</span>? I attempted to just dig into the structures of windows and bitmaps myself, but it turns out that the display bitmap is not a valid bitmap in PalmOS 3.1 at all. At the end, I realized that PalmOS 3.1 only supported MC68EZ328 and MC68328 processors, and both of them configure the display controller base address in the same register, so I just read it directly. As for palette setting, it is not needed since PalmOS 3.1 does not support color or palettes. Easy enough.
</p>
<h3>Making it work well</h3>
<h4>Initial data</h4>
<p><img src="https://dmitry.gr/images/rePalm-visor-screen-1.jpg" alt="Visor showing garbled OS5.2 touch screen calibration dialog"></p><p>Some data is needed by rePalm before it can properly boot: screen resolution and supported depths, hardware flags (eg: whether screen has brightness or contrast adjustment), and whether the device as an alert LED (yes, you read that right, more on this later). Thus rePalm does not boot until it gets a "continue boot" message that is sent by the code on the Visor once it collects all this info.
</p>
<h4>Sending display data</h4>
<p>The highest-bandwidth data we need to transfer between the Visor and the reSpring module is the display data. For example for a 160x160 scren at 16 bits per pixel at 60 FPS, we'd need to transfer 160x160x16x60 = 23.44Mbps. Not a low data rate at all to attempt on a 33MHz 68k CPU. In fact, I do not think this is even possible. For 4 bits-per-pixel greyscale the numbers look a little better: 160x160x4x60 = 5.86Mbps. But there is a second problem. Each message needs a full round trip. We are limited by Visor's interrupt latency and our general round-trip latency. Sadly that latency is as high as 2-4ms. So we need to minimize the number of packets sent. We'll come back to this later. Initially I just sent the data piecewise and displayed it onscreen. Did it work the first time? Actually, almost. The image to the right shows the results. All it took was a single byteswap to get it to work perfectly!
</p>
<p>It was quite slow, however - about 2 frames per second. Looking into it, I realized that the call to MemMove was one of the reasons. I wrote a routine optimized to move the large chunks of data, given that it was not overlapped and always aligned. This improved the refresh rate to about 8 frames per second on the greyscale devices. More improvement was needed. The major issue was the round trip time of copying data, waiting, copying it out, and so on. How do we minimize the number of round trips? Yup - compress the data. I wrote a very very fast lossless image compressor on the STM. It works somewhat like LZ, with a hashtable to find previous occurrences of a data pattern. The compression rations were very very good, and refresh rates went up to 30-40 FPS on the greyscale devices. Color Bejeweled became playable even!
</p>
<p>Actually getting the display data was also quite interesting. PalmOS 5 expects the display to just be a framebuffer that may be written to freely. While there are API to draw, one may also just write to the framebuffer. This means that there isn't really a way to get notified when the image onscreen changes. We could send screen data constantly. In fact, this is what I did initially. This depletes the Visor battery at about two percent a minute since the CPU is constantly busy. Clearly this is not the way to go. But how can we get notified when someone draws? The solution is a fun one: we use the MPU. We can protect the framebuffer from writes. Reads are allowed but any write causes an exception. We handle the exception by setting a timer for 1/60 of a second later, and then permit the writes and return. The code that was drawing them resumes, none the wiser. When our timer fires, we re-lock the framebuffer, and request to transfer a screenful of data to Visor. This allows us to not send the same data over and over. Sometimes writes to screen also change nothing, so I later added a second layer where anytime we send a screenful of data, we keep a copy, and next time we're asked to send, we compare, and do nothing if the image is the same. Together with compression, these two techniques bring us to a reasonable power usage and screen refresh rate.
</p>
<h4>Buttons, pen, brightness, contrast, and battery info</h4>
<p>Since the Visor can send data to the reSpring module anytime it wishes, sending button and pen info is easy, just send a message with the data. For transferring data the other way, the design is also simple. If the module requests an IRQ, the visor will send a NOP message, in reply the module will send its request. There are requests for setting display palette, brightness, contrast, or battery info. Visor will perform the requested action, and perhaps reply (eg: for battery info).
</p>
<h4>Microphone support</h4>
<p>The audio amp turned out to be quite miswired on v1 boards, but after some complicated reworks, it was possible to test basic audio recording functionality. It worked! Due to how the reworks worked, the qulity was not stellar, but I could recognize my voice as I said "1 2 3 4 5 6 7" to the voice memo app. But, in reality, amplifying the visor mic is a huge pain - we need a 40dB gain to get anything useful out of the ADC. The analog components of doing this properly and noise-free are just too expensive and numerous, so for v2 it was decided to just populate a digital mic on the board - it is actually cheaper. Plus, <em>no</em> analog is the best amount of analog for a board!
</p>

<h3>Polish</h3>
<h4>Serial/IrDA</h4>
<p>I support forwarding the Visor's serial port to reSpring. What is this for? HotSync (works) and IR beaming (mostly works). This is actually quite a hard problem to solve. To start with, in order to support PalmOS 3.1, one must use the <span>Old Serial Manager</span> API. I had never used them since PalmOS 4.5 introduced the <span>New Serial Manager</span> and I had almost never written any code for PalmOS before 4.1. The APIs are actually similar, and both quite hostile to what we need. We need to be able to be told when data arrives, without busy-waiting for it. Seemingly there is no API for this. Repeatedly and constantly checking for data works, but wastes battery. Finally I figured out that by using the "receive window" and "wakeup handler" both of which are halfway-explained in the manual, I can get what I need - a callback when data arrives. I also found that, while lightly documented, there is a way to give the Serial manager a larger receive buffer. This allows us to not drop received data even if we take a few milliseconds to get it out of the buffer. I was able to use all of this to wire up Visor's serial port to a driver in reSpring. Sadly, beaming requires a rather quick response rate, which is hard to reach with our round-trip latency. Beaming works, but not every time. Hotsync does work, even over USB.
</p>
<h4>Alarm LED</h4>
<p>Since rePalm supports alarm LEDs and some Visors have LEDs (Pro, Prism, and Edge), I wanted to wire one up to the other. There are no public API for LED access in the Handspring devices. Some reverse engineering showed that Handspring HAL does have a function to set the LED state: <span>HalLEDCommand()</span>. It does precisely what I want, and can be called simply as <span>TRAP #1; dc.w 0xa014</span>. There is an issue. Earlier versions of Handspring HAL lack this function, and if you attempt to call it, they will crash. "Surely," you might say, "all devices that support the LED implement this function!" Nope... Visor Prism devices sold in the USA do not. The EFIGS version does, as do all later devices. This convenient hardware-independent function was not available to me thus. What to do? Well, there are only three devices that have a LED, and I can detect them. Let's go for direct hardware access then! On the visor edge the LED is on GPIO K4, on the Pro, it is K3, and on the Prism it is C7. We can write this GPUI directly and it works as expected.
</p>
<p><img src="https://dmitry.gr/images/rePalm-visor-update.jpg" alt="Visor showing garbled OS5.2 touch screen calibration dialog"></p><p>There are two driver modes for LED and vibrator in rePalm - simple and complex. Simple mode has rePalm give the LED/vibrator very simple "turn on now" "turn off now" commands. This is suitable for a directly wired LED/vibrator. In the reSpring case we actually prefer to use the complex driver, where the OS tells us "here is the LED/vibrator pattern, here is how fast to perform it, this many times, with this much time in between. This is suitable for when you have an external controller that drives the LED/vibrator. Here we do have one: the Visor is our external controller. So we simply send these commands to the Visor and our downloaded code performs the proper actions using a simple state machine.
</p>
<h4>Software update</h4>
<p>I wanted reSpring to be able to self-update from SD card. How could this be accomplished? Well, the flash in the STM32 can be written by code running on the STM32, so logically it should not be hard. A few complications exist: to start with, the entire PalmOS is running form flash, including drivers for various hardware pieces. Our comms layer to talk to the Visor is also in there. So to perform the update we need to stop the entire OS and disable all interrupts and drivers. OK, that is easy enough, but among those drivers are the drivers for the SD card, where our update is. We need that. Easy to solve: copy the update to RAM before starting the update - RAM needs no drivers. But how do we show the progress to the user - our framebuffer is not real, making visor show it requires a lot of code and working interrupts. There was no chance this would work as normal.
</p>
<p>I decided that the best way to do this was to have the Visor draw the update UI itself, and just use a single SRAM location to show progress. Writing a single SRAM location is something our update process can do with no issues since the SRAM needs no drivers - it is just memory mapped. The rest was easy: a program to load the update into RAM, send the "update now" message, and then flash the ROM, all the while writing to the proper SRAM location the "percent completed". This required exporting the "send a message" API from the rePalm DAL for applications to use. I did that.
</p>
<h3>Onboard NAND</h3>
<h4>You wanted pain? Here's some NAND</h4>
<p>The reSpring board has 256MB of NAND flash on a QSPI bus. Why? Because at the time it was designed, I thought it would be cool, and it was quite cheap. NAND is the storage technology underlying most modern storage - your SD cards, your SSD, and the storage in your phone. But, NAND is hard - it has a number of anti-features that make it rather difficult to use for storage. First, NAND may not properly store data - error correction is needed as it may occasionally flip a bit or two. Worse, more bit flips may accumulate over time, to a point where error correction may not be enough, necessitating moving data when such a time approaches. The smallest addressable unit of NAND is a page. That is the size of NAND that may be read or programmed. Programming only flips one bits to zero, not the reverse. The only way to get one bits back is an erase operation. But that operates on a block - a large collection of pages. Because you need error correcting codes, AND bits can only be flipped from one to zero, overwriting data is hard (since the ECC code you use almost certainly will need more ones). There are usually limits to how many times a page may be programmed between erases anyways. There are also usually requirements that pages in a block be programmed in order. And, for extra fun, blocks may go bad (failing to erase or program). In fact a NAND device may ship with bad blocks directly from the factory! Clearly this is not at all what you think of when you imagine block storage. NAND requires careful management to use for storage. Since blocks die due to wear, caused by erasing, you want to evenly wear across the entire device. This may in turn necessitate movinig more data. At the same time while you move data, power may go out so you need to be careful when and what is erased and where it is written. Keeping a consistent idea of what is stored where is hard. This is the job of an FTL - a flash translation layer. An FTL takes the mess that is nand and presents it as a normal block device with a number of sectors which maybe read and written to randomly, with no concern for things like error correction, erase counts, and page partial programming limits.
</p>
<h4>To write an FTL...</h4>
<p>I had written an FTL long ago, so I had some basic idea of the process involved. This was, however, more than a decade ago. It was fun to try to do it again, but better. This time I set out with a few goals. The number one priority was to absolutely never lose any data in face of random power loss since the module may be removed from the Visor randomly at any time. The FTL I produced will never lose any data, no matter when you randomly cut its power. A secondary priority was to minimize the amount of RAM used, since, afterall, reSpring only has 8MB of it!
</p>
<p>The pages in the NAND on reSpring are 2176 bytes in size. Of that, 4 are reserved for "bad block marker", 28 are free to use however you wish, with <em>no</em> error correction protection, and the rest is split into 4 equal parts of 536 bytes, which, if you desire, the chip can error-correct (by using the last 16 of those bytes for the ECC code). This means that per page we have 2080 error-corrected bytes and 28 non-error-corrected bytes. Blocks are 64 pages each, and the device has 2048 blocks, of which they promise at least 2008 will be good from the factory. Having the chip do the ECC for us is nice - it has a special hardware unit and can do it much faster then our CPU ever could in software. It will even report to us how many bits were corrected on each read. This information is vital because it tells us about the health of this page and thus informs our decision as to when to relocate the data before it becomes unreadable.
</p>
<p>I decided that I would like my FTL to present itself as a block device with 4K blocks. This is the cluster size FAT16 should optimally use on our device, and having larger blocks allows us to have a smaller mapping table (the map from virtual "sector number" to real "page number"). Thus we'd treat two pages together as one always. This means that each of our virtual pages will have 4160 bytes of error-corrected data and 56 bytes of non-erorr corrected data. Since our flash allows writing the same page twice, we'll use the un-error-corrected area ourselves with some handmade error corection to store some data we want to persist. This will be things like how many times this block has been erased, same for prev and next blocks, and the current generation counter to figure out how old the information is. The handmade ECC was trivial: hamming code to correct up to one bit of error, and then replicate the info plus the hamming code three times. This should provide enough protection. Since this only used the un-error-corrected part of the pages, we can then easily write error-correctd-data over this with no issues. Whenever we erase a page, we write this data to it immediately. If we are interrupted, the pages around it have the info we need and we can resume said write after power is back on.
</p>
<p>The error-corected data contains the user data (4096 bytes of it) and our service data, such as what vitual sector this data is for, generation counter, info on this and a few neighboring blocks, and some other info. This info allows us to rebuild the mapping table after a power cycle. But clearly reading the entire device each power on is slow and we do not want to do this. We thus support checkpoints. Whenever the device is powered off, or the FTL is unmounted, we write a checkpoint. It contains the mapping data and some other info that allows us to quickly resume operation without scanning the entire device. Of course in case of an unexpected power off we do need to do a scan. For those cases there is an optimization too - a directory at the end of each block tells us what it contains - this allows the scan to read only 1/32nd of the device instead of 100% of it - a 32x speedup!
</p>
<p>Read and write requests from PalmOS directly map to the FTL layer's read and write. Except there is a problem - PalmOS only supports block devices with sector sizes of 512 bytes. I wrote a simple translation layer that does read-modify-write as needed to map my 4K sectors to PalmOS's 512-byte sectors, if PalmOS's request did not perfectly align with the FTL's 4K sectors. This is not as scary or as slow as you imagine it, because PalmOS uses FAT16 to format the device. When it does, it asks the device about its preferred block size. We repy with 4K and from then on, PalmOS's FAT driver only writes complete 4K clusters - which align perfectly with out 4K FTL sectors. The runtime memory usage of the FTL is only 128KB - not bad at all, if I do say so myself! I wrote a very torturous set of tests for the FTL and ran it on my computer over a few nights. The test simulated data going bad, power off randomly, etc. The FTL passed. There is actually a lot more to this FTL, and you are free to go look at the source code to see more.
</p>
<h3>One final WTF</h3>
<p>Among all this work, rePalm worked well, mostly. Occasionally it would lose a message from the Visor to the module or vice-versa. I spent a lot of time debugging this and came to a startling realization. The dual-ported SRAM does not actually support simultaneous access to the same address by both ports at once. This is documented in its datasheet as a "helpful feature" but it is anything but. Now, it might be reasonable to not allow two simultaneous writes to the same word, sure. But two reads should work, and a read and a write should work too (with a read returning the old data or the new data, or even a mix of the two). This SRAM instead signals "busy" (which is otherwise never does) to one side. Since it is not supposed to ever be busy, and the Springboard slot does not even have a BUSY pin, these signals were wired nowhere. This is where I found this stuff in the footnote in the manual. It said that switching the chip to SLAVE mode and raising the BUSY pins (which are now inputs) to HIGH will allow simultaneous access. Well, it sort of does. There is no more busy signalling, but sometimes a write will be <em>DROPPED</em> if it is executed concurrently with a read. And a read will sometimes return <em>ZERO</em> if executed concurrently with another read or write, even if the old and new data were both not zero. There seems to be no way around this. Another company's dual-ported SRAM had the same nonsense limitation, leading me to believe that nobody in the industry makes REAL dual-ported SRAMs. This SRAM has something called "semaphores" which can be used to implement actual semaphores that are truly shared by both devices, but otherwise it is not true dual-ported RAM. Damn! 
</p>
<p>Using these semaphores would require significant rewiring: we'd need a new chip select line going to this chip, and need to invent a new way to interrupt the STM since the second chip select line would be now used to access semaphores. This was beyond my rework abilities, so I just beefed up the protocol to avoid these issues. Now the STM will write each data word that might be concurently read 64 times, and then read it back to verify it was written. The comms protocol was also modified to never ever use zeroes, and thus if a zero is seen, it is clear that a re-read was necessary. With these hacks the communication is stable, but in the next board rev rev I think we'll wire up the semaphores to avoid this nasty hack!
</p>

<h2>More real hardware</h2>
<h3>rePalm-MSIO</h3>
<p><a href="https://dmitry.gr/images/rePalm-MSIO_board.jpg"><img src="https://dmitry.gr/images/rePalm-MSIO_board.jpg" alt="rePalm-MSIO first board"></a></p><p>After <a href="https://dmitry.gr/?r=05.Projects&amp;proj=31.%20Memory%20Stick">documenting the Sony MemoryStick protocol</a>, an opportunity presented itself - why not a rePalm version on a MemoryStick? In theory, I could get a microcontroller to act as a MemoryStick device, load a program unto the host Sony PalmOS device, and then take over it, like reSpring did. That was the idea, of course. The space is tight, and timing requirement insane. The fact that the MemoryStick protocol is so much unlike any normal sane bus means that there will be no simple solutions. However, I was determined to make this work.
</p>
<h4>MCU selection</h4>
<p>STM32F429 and an SDRAM chip together would take up too much space to fit inside a MemoryStick slot. Instead, a 64-pin STM32H7 chip is used. It has 1.25MB of internal ram, which is a bit little for PalmOS. Luckily, it supports a rather rare thing: a read/write QSPI interface - perfect for interfacing with QSPI PSRAM chips like APS6404L from APMemory! This allows for 8MB of RAM without taking up a lot of board space or needing a boatload of pins! STM32H7 is also a Cortex-M7, which is quite an improvement from the Cortex-M4 core in the STM32F429. M7 is faster per-cycle, and has a cache! The fact that STM32F429 had no cache was a serious handicapping factor for it when running code from RAM, since the RAM was limited to half the core clock speed. With a small-enough working set, the M7 can operate at full speed from cache! Cool! There is also <span>TCM</span> - some memory near the core that always operates at full speed with no delay or wait-states!
</p>
<p>I laid out the board such that it would fit into the MemoryStick slot. It is a 4-layer board (which is apparently very cheap now). This makes routing easier and signal integrity better. With the proper board thickness, there is just enough space for the chips to fit. It all works, inserts, clicks, everything! Pretty amazing, actually. Of course, there were errors, but by the second revision of the board, only one bodge wire was needed, as you can see in the picture. The board is precisely the size of a MemoryStick. There is extra that sticks out, those are the debugging headers and it is break-away. I have one where I did break it away and it is amazing how well it fits inside.
</p>

<h4>The bugs...</h4>
<p>Of course, this being an STM chip, there were bugs. The chip would sometimes lock up entirely when executing from QSPI RAM. When consulted, ST suggested changing the MPU parameters to make the QSPI RAM uncacheable. This is an idiotic suggestion, because even if it worked (spoiler: it does not), it would make that RAM slow beyond any degree of usefulness. In any case, when I tried that, the RAM gets corrupted. I verified with bus traces and presented ot STM. Eventually they admitted that any writes to the QSPI interface that are not sequential and word-sized will cause corruption. Somehow, that info tells me precisely what was the only test they ever ran on this peripheral. Sigh...
</p>
<p>Luckily, with the cache on, the dirty cache-line eviction will always sequentially write an integer number of words, so there is hope. Sadly, the chip would work for a while, and then lock up. The lock up was very strange, my debugger would be unable to connect to the core in this state at all, but it could access the debug access port itself. This lead me to believe that it was not the core that locked up but the internal AHB fabric. I was able to confirm this by attaching to another debugger access port (the one on AHB3), where I could look around but have no access to the main AHB busses. STM had no ideas.
</p>
<p>Given what I knew about how AHB buses works, guesses on how ST likely designed the arbiters, and how ST likely wired up their QSPI unit to it all, I guessed at the issue, and a workaround the might work. After some prototyping, I can confirm that it does. The performance cost is about 20% (compared to no workaround enabled), but at least no more hangs. Why am I being so cagey about what the workaround is? Well, while denying the issue exists, STM asked for the precise details of my workaround once they heard I had found one. Apparently an actually-important client also hit this issue. I am currently refusing to disclose the workaround until they agree to admit the issue. So far it is a stalemate, which is fine - I am losing no sales over it. Them...?
</p>
<h4>MSIO low level</h4>
<p>The main signal that controls the protocol phases is <span>BS</span>, and it always leads the actual state transition by a cycle, which makes it very hard to use for anything. If only it were not one cycle early, I could use it (and its inverse) as chip-selects and try to use the hardware SPI bus units somehow. After some head-scratching, a solution became evident. Two flip flops will do. Running the BS signal through them will delay it a cycle. Finding a dual-negative-edge-triggered flip-flop turned out to be impossible, so an inverter was thrown into the mix, so that I could use an easily-available <span>SN74LVC74A</span>.
</p>
<p>With the BS signal delayed, it could be used as chip select for some SPI units. To make this work, I wired <em>THREE</em> SPI units together. The first edge of <span>BS</span> Triggers a DMA channel that enables three SPI units: one receives the <span>TPC</span>, and the second and third are ready to receive the data that follows. We'll have no time to validate the <span>TPC</span> in the meantime, so we prime the SPI unit to receive it no matter what. This is harmless. This first <span>BS</span> edge also triggers a software interrupt. Assuming not too many delays, we'll arrive into the IRQ after the TPC has already been received and, if the transaction is a write, the data is already on on the way coming in. If we are less lucky, data might have even already been entirely received. Here we can validate the <span>TPC</span> and check its direction. If this is a READ, we need to send the handshaking pattern immediately, so we use one of the SPI units to do that now. While that goes on, we find the data and queue it up for transmission, telling the SPI unit to also send the CRC after it. If this was a WRITE, we had two SPI units receiving the data. One copied the data to RAM, the second to the CRC unit (STM32H7 cannot CRC incoming data if we do not up front know the length). We quickly check the CRC and configure one of the SPI units to send the handshaking pattern to acknowledge the data.
</p>
<p>"Now, this all sounds very fragile," an astute observer would say. Yes! Very. It also means that we cannot ever disable interrupts for very long, since there is only a few cycles of leeway between the data being sent to us and a reply being needed to avoid the host timing out. I had to rearchitect rePalm kernel's interrupt handling a little bit, to allow some interrupts to <em>NEVER</em> be disabled, in return for some concessions from those interrupt handlers: they do not make any syscalls or modify any state shared with any other piece of code. So then how do we interface with them? When an MSIO transaction finishes, the data is placed into a shared buffer, and a software interrupt is triggered, which is handled normally by normal code with normal constraints. This can be disabled, prioritized, etc, since it is not time critical anymore. Of course, all the time-critical code must be run from the <span>ITCM</span> (the tightly-coupled instruction memory) to make the deadlines.
</p>
<p>When the STM32H7 runs at 320MHz, this works most of the time with newer palm devices, since they run the MSIO interface at 16MHz, giving me some breathing room. Older devices like the S500C are tougher. They run the MSIO bus at 20MHz, and the timings are very tight. Things work well, but if the core is waiting for instruction fetch from QSPI, it will not jump to the interrupt handler till that compltes, causing larger latency. Sometimes this causes an MSIO interrut handler to be late and miss the proper window to ACK some transaction. My host-side driver retries and papers over this. The real solution is a tiny FPGA to offload this from the main MCU. I'm looking into this.
</p>
<h4>MSIO high level</h4>
<p><a href="https://dmitry.gr/images/rePalm-MSIO_S500.jpg"><img src="https://dmitry.gr/images/rePalm-MSIO_S500.jpg" alt="rePalm-MSIO running on a PEG-S500C"></a></p><p>As there exist no MSIO drivers for rePalm, I had to write and provide them. But how would a user get them unto the device? In theory, as far as my reverse-engieering can tell, a MemoryStick may have multiple functions, possibly memory and one or more IO functions. No such stick was observed in the wild, so I set out to create the first. Why not? The logic of how it should work is rather simple - function 0xFF should be memory, and any other unused function number could be for rePalm IO. I picked the function number 0x64. Why pretend to be memory at all? To give the user the driver, of course!
</p>
<p>My code does the minimum to pretend to be a read-only MemoryStick with 4MB of storage. As MemorySticks are raw NAND devices, my code pretends to be a perfect one - no bad blocks, no error correction ever needed. The fake medum is "formatted" with FAT12 and contains a rather curious filesystem indeed. To support <em>ALL</em> the sony devices, the driver is needed in a few places. Anything with PalmOS 4.0 or later will show files in <span>/PALM/LAUNCHER</span> to the user, and will auto-launch <span>/PALM/START.prc</span> on insertion. Anything with earlier PalmOS versions will only allow the user to browse <span>/PALM/PROGRAMS/MSFILES</span>. All but the first Sony devices also had another way to auto-launch an executable on stick insertion - a Sony utiliy called "MS AutoRun". It reads a config file at <span>/DEFAULT.ARN</span> and loads the specified program to RAM on insertion. Auto-run is never triggered if the MemoryStick was aleady inserted at device boot, so we cannot rely on it. This is why we need the file to be itself visible and accessible to the user for manual launching. Let's count then, how many copies of the driver app our MemoryStick needs. One in <span>/PALM/LAUNCHER</span>, one in <span>/PALM/PROGRAMS/MSFILES</span>, and one as <span>/PALM/START.prc</span>. Three copies. Now, this will not do! If only FAT12 supported hard links...
</p>
<p>But, wait, if the filesystem is read-only, it <em>DOES</em> support hard links! More than one directory entry may reference the same cluster chain. This is only a problem when the file is deleted, which does not happen to a read-only filesystem. The filesystem thus contains a <span>PALM</span> directory in the root, That contains <span>DEFAULT.ARN</span> file, pointing to a cluster with its contents, a <span>PROGRAMS</span> directory, a <span>LAUNCHER</span> directory, and a directory entry with the name <span>START.PRC</span> pointing to the first cluster of our driver. <span>PROGRAMS</span> contains an <span>MSFILES</span> directory, which itself contains another directory entory pointing to the driver, this one with the name <span>DRIVER.PRC</span>. <span>/PALM/LAUNCHER</span> contains the third directory entry pointing to the driver, also named <span>DRIVER.PRC</span>. PalmOS does not do a file system check on read-only media, so no issue is ever hit - it all works.
</p>
<h4>MSIO performance</h4>
<p>Some Sony devices have actual exported MSIO API in their MemoryStick drivers which I was able to reverse engineer (<a href="https://www.reddit.com/r/Palm/comments/12li6t5/complete_reverseengineering_of_the_msio_api/?">and publish</a>). Some others did not, but Sony published updates that included such API. Usually these updates came with MSIO peripherals like the MemoryStick Bluetooth adapter or the MemoryStick Camera. And some devices never had any official MSIO suport at all. I wanted to support them all, and since I had already reverse engineered how the MemoryStick Host chip (MB86189) worked, I was able to just write my own drivers, talking to it directly. This worked for some devices. Others do not have direct access to the chip, since the DSP controls it. Sony DSP is not documented, the firmware is encrypted, and the key is not known. Here, I was stuck for a while. Eventually I was able to figure out just enough to be able to send and receive raw <span>TPC</span>s via the DSP. This worked well on almost all devices, except the N7xx series devices. Their DSP firmware was the oldest of all (as far as I can tell) and the best bandwidth I was able to coax out of it was 176Kbit/s. Needless to say that this is not quite good enough for live video (basically what rePalm does). It works, but the quality is not great.
</p>
<p>As MSIO allows transfers of no more than 512 bytes per transfer, transferring screen image data is complex. The same compression is used here as was used in reSpring. Even then, performance varies based on the device and screen configuration. On low-resolution devices, everything is fast. On high-resolution ones (except N7xx), 35 FPS is reachable in 16bits-per-pixel mode. It is faster on greyscale devices. The lone PalmOS 4 HiRes+ device (NR70V) lags behind at around 20FPS. This is because there is simply so much data to transfer each frame - 300KB.
</p>
<h4>Other loose ends</h4>
<p>Curiously, it seems that Asus licensed the MemoryStick IP from Sony, so the Asus PalmOS devices (s10 and s60 families) also use MemoryStick. I added support for them. For each device,  I wired up as much as possible to rePalm. Devices with a LED have it wired to the attention manager, devices with the vibrate motor have that wired up as well. Sound is a bit more complex. Some of these devices had a DSP for MP3 decoding, but the ability to play raw sampled sound is limited, since 68K was unlikely to be able to do it fast enough anyways. There exists a sony API to play 8KHz 4-bits-per-sapme ADPCM. I considered wiring that up to the sound output of rePalm, but did not get around to it. It is likely not worth it as the quality will be atrocious. I did consider the alternative - have rePalm encode its output as MP3, and somehow find a way to feed that to the DSP, but I was stymied in my efforts. In most of the devices, the DSP firmware reads the MP3 file directly from the MemoryStick, bypassing the OS entirely, leading me to believe that I may not find a way to inject MP3 data even if I made it.
</p>
<p>Initially, I did the development on STM32H7B0RB. This variant has only 128KB of flash, which is, of course, not enough to contain PalmOS. I used some of the RAM to contain a ROM image, which I loaded over SWD each time. This worked well enough, but was not really fun as it could not be used away from a computer. Luckily, I was able (with a lot of help from an unnamed source) to get some of the STM32H7 chips with 2MB of internal flash. This <em>IS</em> enough to fit PalmOS, so now I have variants that boot directly on insertion. The latest boards also have some onboard NAND flash that acts as a built-in storage device for user using my FTL, mentioned before. The photo album (linked above) has more photos and videos! <a href="https://photos.app.goo.gl/4NGYzd1ejggu1RV3A">Here is one</a>. Enjoy!
</p>
<h3>AximX3</h3>
<p><a href="https://dmitry.gr/images/rePalm-AximX3.jpg"><img src="https://dmitry.gr/images/rePalm-AximX3.jpg" alt="Axim X3 running PalmOS"></a></p><p>This was a fun target just for shits and giggles. As this runs an ARMv5T CPU, my kernel was forced to adapt to this world. It was not terribly difficult and it works now. Curiously, this device is rather similar internally to the Palm Tungsten T3, so this same rePalm build can run with few modifications on the T|T3 as well.
</p>
<p>I put a lot of work into this device. Luckily, a lot of the initial investigation of the hardware was already done as part of my <a href="https://github.com/uARM-Palm/uARM">uARM-Palm</a> project. Almost everything works. Audio in and out work, SD card works, infrared works, touch and buttons work, battery reporting works, and the screen works. Missing is only USB and sleep/wake. The first I see no point in, the second is complicated by the built-in bootloader. Initial builds of this used a WinCE loader I wrote to load the ROM into RAM and run from there. Further investigation of the device ROM indicated to me that there is a rather complete bootloader there, capable of flashing the device ROM from the SD card. I decided to exploit that, and with some changes, now rePalm can be flashed to ROM of the device and boot directly. Yes!
</p>
<p>How? The stock bootloader has a mode for this. If an image file is placed on the SD card as <span>/P16R_K0.NB0</span>, the card is inserted, jog wheel select and the second app button are held, and the device resetted, it'll flash the image to flash, right after the bootloader. This can be used to flash rePalm, or to reflash the stock image. Depending on the AximX3 version (there are three), the amount of flash and RAM differs. rePalm detects the available RAM and uses it all!
</p>
<h3>STM32F469 Discovery Board</h3>
<p><a href="https://dmitry.gr/images/rePalm-STM32F469DISCO.jpg"><img src="https://dmitry.gr/images/rePalm-STM32F469DISCO.jpg" alt="STM32F469DISCO board running PalmOS"></a></p><p>This was a quick little hack to see in real life PalmOS running on a 3x density display. No such device ever shipped. The <a href="https://www.st.com/en/evaluation-tools/32f469idiscovery.html">STM32F469DISCOVERY</a> board has a 480x800 display, of which 480x720 is used as a 3x density display with a dynamic input area. This board has a capacitive touch screen, which makes it ill-suited for PalmOS. Capacitive touch screens are very bad for precise tapping of small elements, since your finger would normally obscure whatever it is that you are trying to tap. This screen being rather large helps a little, but not really all that much. I got this board working well enough to see what it is like, but put little work into it afterwards. Screen, touch, and SD card are the only things supported. It does not help that just like the STM32F429, STM32F469 lacks any cache, making it rather slow when running out of SDRAM.
</p>

<h3>RP2040</h3>
<p><a href="https://dmitry.gr/images/rePalm-RP2040.jpg"><img src="https://dmitry.gr/images/rePalm-RP2040.jpg" alt="Raspberry Pi Pico running PalmOS"></a>
<a name="_TOC_da19cdc85215569c48f1e5adbb0dcceb"></a></p><h4>It is possible!</h4>
<p>How little RAM/CPU does PalmOS 5 really require? Since rePalm had support (at least in theory) for Cortex-M0, I wanted to try on real hardware, as previously the support was tested on CortexEmu only. There does happen to be one Cortex-M0 chip out there with enough ram - the RP2040 - the chip in the $4 <a href="https://www.raspberrypi.com/products/raspberry-pi-pico/">Raspberry Pi Pico</a>. I then sought out a display with a touchscreen that could be easily bought. There were actually not that many options, but <a href="https://www.waveshare.com/pico-restouch-lcd-2.8.htm">this one</a> seemed like a good fit. It turned out, after some investigation, that driving it properly and quickly will not be at all easy. RP2040's special sauce - the PIO - to the rescue! <a href="https://dmitry.gr/?r=06.%20Thoughts&amp;proj=09.ComplexPioMachines">I found a way to do it</a>. I switched the resistors on the screen's board from "SPI" to "SDIO" to enable the SD card, and I wired up the LED to be the alarm LED for PalmOS. Those were the easy things.
</p>
<p>As this project depends on some undocumented behaviour in the Cortex-M chips, it was always unknown what would happen in some cases. For example, Cortex-M3 causes a <span>UsageFault</span> when you jump to an address without the bottom bit set, indicating a switch to ARM mode. What would Cortex-M0 do? Turns out - it simply causes a <span>HardFault</span>. <a href="https://dmitry.gr/?r=05.Projects&amp;proj=27.%20m0FaultDispatch">m0FaultDispatch</a> to the rescue! It is able to categorize all the causes of a <span>HardFault</span> and wire them to the proper place. I did find one difference from the Cortex-M3. When the Cortex-M3 executes a <span>BX PC</span> instruction, it will execute a jump to the current address plus 4, in ARM mode. This differs from what ARMv5 chips do when you execute that same instruction in Thumb mode. They jump to the current address plus 4, rounded down to the nearest multiple of 4, in ARM mode. This difference my JIT and emulator code alrady handled. But Cortex-M0 does yet a third thing in this case. It actually seems to treat the actual instruction as invaild. PC is not changed, mode is not changed, and a <span>HardFault</span> is taken right on the instruction itself. Curiously, this does not happen if another non-PC register with the low bit clear is used. Well, in any case, I adjusted the JIT and the emulator code to handle this. I also modified CortexEmu to emulate this properly.
</p>
<h4>Memories</h4>
<p>RP2040 lacks any flash, it uses an external Q/D/SPI flash for code and data storage. This is convenient when you have a lot of data. For rePalm this means we can have a ROM as big as the biggest flash chip we can buy. The Pi Pico comes with a 2MB chip, so I targetted that. The RAM situation is much tighter. There is just 264KB of RAM in there. This is not much. The last PalmOS device to have this little RAM ran PalmOS 1.0. But it is worth trying. One of the largest RAM expenditures are graphics. The primary one is the framebuffer. PalmOS assumes that the display has a framebuffer that is directly accessible by the CPU. This means that if I wanted to use the entire 320x240 display in truecolor mode, the framebuffer would occupy 150Kb. Oof! Well, how much <em>IS</em> acceptable?
</p>
<p>Some experimentation followed. To boot successfully and to launch the launcher, preferences app, and the digitizer calibration panel successfully, approximately 128KB of dynamic RAM is necessary. The various default databases as well as PACE temporary databases in the storage heap mandate a storage heap of at least 50KB. A 64KB minimum storage heap size is preferred, really, so we do not immediately run out of space at boot. And rePalm's DAL needs at least 15KB of memory for its data structures and about 24KB for the kernel heap where stacks and various other data structures are allocated. Let's add those up. The sum is 231KB. that leaves at most 33KB for the framebuffer. There are a few options. We can use the whole screen at 2 bits per pixel (4 greys). This will need a 18.75KB framebuffer. We can use a square 240x240 screen at 4 bits per pixel, for a 28.125KB framebuffer. We can also use the standard low-density resolution of 160x160 at a whopping 8 bits per pixel (the only non-greyscale option).
</p>
<p>One might notice that the above memory areas did not include a JIT translation cache. This is correct. While my JIT does indeed support targetting the Cortex-M0, there simply is not enough space to make it worthwhile. I instead enabled the <span>asmM0</span> ARM emulator core since it needs no extra space of any sort. Not wonderful, but oh well. We knew all along that compromises would need to be made! As long as I'm just showing off, let's have a full-screen experience, with a dynamic input area and all! 320x240 it is! The second core of the RP2040 is not currently used (yet).
</p>
<h4>PACE again</h4>
<p>My previously-mentioned Cortex-M3-targetting patched <span>PACE</span> is of no use on a Cortex-M0. Combine this with the fact that I cannot use the JIT means that all the 68K code will be running under double emulation (68K emulated by ARM, ARM itself emulated in thumb). It was time to write a whole new 68k emulator, in Thumb-1 assembly, of course. I give you <span>PACE.m0</span>. It is actually rather fast, competing well with Palm's ARM PACE in performance, as tested on my Tungsten T3. It really helped make the RP2040 build usable. It is now no slower than a Tunsten T was.
</p>

<h2>So where does this leave us?</h2>
<p>There is still a lot to do: implement BT, WiFi, USB, debug NVFS some more, and probably many more things. However, I am releasing some little preview images to try, if you happen to have an STM32F429 discovery board, an AximX3, a raspberryPi Pico with the proper screen. No support for USB. Anyways if you want to play with it, here: <a href="https://drive.google.com/file/d/1u8zcfhuRAaH1dAfQWU3pHrjZMHxFsHBn/view?usp=share_link">LINK</a>. I am also continuing to work on the reSpring/MSIO/and ther hardware options and you might even be able to get your hands on one soon :) If you already have a reSpring module (you know who you are), the archive linked to above has an update to 1.3.0.0 for you too.
</p>

<h2>Source Code</h2>
<h3>Source intro</h3>
<p><a href="https://dmitry.gr/images/rePalm_sources_0000.tar.bz2">Version 0000 source download</a> is here. This is a very very very early release of the source code, just to allow people to browse this codebase and see what it is. The README explains the basic directory structure, and there is a LICENSE document in each directory. Building this requires a modern (read: mine) build of PilRC (included) and an ARM cross-gcc toolchain. Some builds require a PalmOS-specific 68k toolchain too, <a href="https://www.reddit.com/r/Palm/comments/p81m58/announce_new_gcc_or_palmos_again/">from here</a>, for example.
</p>
<h3>Building basics</h3>
<p>Building a working image is a multi-step process. First the DAL needs to be built. This is accomplished by running <span>make</span> in the <span>myrom/dal</span> directory. Some params need to be passed to it. For example, to build for rPI-Pico with the waveshare display, the command <span>make BUILD=RP2040_Waveshare</span> will do. For some cases, makefile itself will need to be edited. For the abovementioned build, for example, we do not want to use jit, preferring the emulator instead. To do this, you'll want to comment out the line <span>ENABLE_JIT		= yes</span> and uncomment the one that says <span>EMU_CORE	= asmM0</span>. This will build the DAL.prc. The next step is to build a full ROM image. This is done from the <span>myrom</span> directory. Again, <span>make</span> is used. The parameters now are the build type (which determines the ROM image parameters) and the directory of files to include in the ROM. For the RP2040_Waveshare build, the proper incantation is <span>make RP2040_Waveshare FILESDIR=files_RP2040_Waveshare</span>. The files directory given already contains some other things from rePalm, like PACE and rePalm information preferences panel.
</p>
<h3>Building PACE</h3>
<p>The PACE patch is a binary patch unto PACE. It is built in a few steps. First the patch itself is assembled using <span>make</span> in the <span>myrom/paceM0</span> directory. This will produce the patch as a ".bin" file. Then using the <span>patchpace</span> tool (which you must also build) you can apply this patch to an unmodified PACE.prc file (a copy of which can be found, for exmaple, in the <span>AximX3</span> directory). This patched pace can now replace the stock one in the destination files directory.
</p>


<h2>Article update history</h2>
<ol>
<li>image above was updated to v00001: jit is now on (much faster), RTC works (time), notepad added, touch response improved</li>
<li>image above was updated to v00002: grafitti area now drawn, grafitti works, more apps added (Bejeweled removed for space reasons)</li>
<li>image above was updated to v00003: ROM is now compressed to allow more things to be in it. This is ok since we unpack it to RAM anyways. some work done on SD card support</li>
<li>Explained how LDM/STM are translated</li>
<li>Wrote a bit about SD card support</li>
<li>Wrote a bit about serial port support</li>
<li>Wrote a bit about Vibrate &amp; LED support</li>
<li>Wrote the first part about NetIF drivers</li>
<li>image above was updated to v00004: some drawing issues fixed (underline under memopad text field), alert LED now works, SD card works (if you wire it up to the board)</li>
<li>image above was updated to v00005: some support for 1.5 density displays works so image now uses the full screen</li>
<li>Wrote the document section on 1.5-density display support</li>
<li>Wrote the document section on DIA support and uploaded v000006 image with it</li>
<li>Wrote a section on <span>PACE</span>, uploaded image v000007 with much faster 68k execution and some DIA fixes</li>
<li>Uploaded image v000008 with IrDA support</li>
<li>Wrote about audio support</li>
<li>Wrote about reSpring</li>
<li>Uploaded image v000009 with preliminary audio support</li>
<li>Uploaded image v000010 with new JIT backend and multiple JIT fixes</li>
<li>Uploaded image v000011 with an improved JIT backend and more JIT fixes, and an SD-card based updater. Wrote about the Cortex-M0 backend</li>
<li>Wrote a lot about reSpring hardware v1 bring up and current status</li>
<li>Uploaded STM32F429 discovery image v000012 with significant speedups and some fixes (grafiti, notepad)! (this corresponds to rePalm v 1.1.1.8)</li>
<li>Uploaded STM32F429 and, <b>for the first time ever</b>, reSpring images for v 1.3.0.0 with many speedups, wrote about mic support and Zodiac support</li>
<li>Apr 15, 2023: PACE for M0, rePalm hardware update: MSIO, AximX3, RP2040, new downloads</li>
<li>Sep 3, 2023: Source dode posted for the first time</li>
</ol>


<!--- We do not show this to the user, but ToC system will index this and we'll get a link to comments in the ToC -->










					
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Have I been Flocked? – Check if your license plate is being watched (290 pts)]]></title>
            <link>https://haveibeenflocked.com/</link>
            <guid>46170302</guid>
            <pubDate>Sat, 06 Dec 2025 03:16:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://haveibeenflocked.com/">https://haveibeenflocked.com/</a>, See on <a href="https://news.ycombinator.com/item?id=46170302">Hacker News</a></p>
Couldn't get https://haveibeenflocked.com/: Error: Request failed with status code 429]]></description>
        </item>
    </channel>
</rss>