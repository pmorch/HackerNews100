<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 30 Jan 2025 06:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Commercial jet collides with Black Hawk helicopter near Reagan airport (263 pts)]]></title>
            <link>https://www.mediaite.com/news/breaking-commercial-jet-collides-with-police-chopper-near-reagan-airport/</link>
            <guid>42874301</guid>
            <pubDate>Thu, 30 Jan 2025 02:56:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mediaite.com/news/breaking-commercial-jet-collides-with-police-chopper-near-reagan-airport/">https://www.mediaite.com/news/breaking-commercial-jet-collides-with-police-chopper-near-reagan-airport/</a>, See on <a href="https://news.ycombinator.com/item?id=42874301">Hacker News</a></p>
Couldn't get https://www.mediaite.com/news/breaking-commercial-jet-collides-with-police-chopper-near-reagan-airport/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Younger cannabis users have reduced brain function, finds largest study yet (228 pts)]]></title>
            <link>https://newatlas.com/brain/young-adult-cannabis-brain-function/</link>
            <guid>42873697</guid>
            <pubDate>Thu, 30 Jan 2025 01:23:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/brain/young-adult-cannabis-brain-function/">https://newatlas.com/brain/young-adult-cannabis-brain-function/</a>, See on <a href="https://news.ycombinator.com/item?id=42873697">Hacker News</a></p>
Couldn't get https://newatlas.com/brain/young-adult-cannabis-brain-function/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Advice for a friend who wants to start a blog (127 pts)]]></title>
            <link>https://www.henrikkarlsson.xyz/p/start-a-blog</link>
            <guid>42872276</guid>
            <pubDate>Wed, 29 Jan 2025 22:45:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.henrikkarlsson.xyz/p/start-a-blog">https://www.henrikkarlsson.xyz/p/start-a-blog</a>, See on <a href="https://news.ycombinator.com/item?id=42872276">Hacker News</a></p>
Couldn't get https://www.henrikkarlsson.xyz/p/start-a-blog: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[An update on Dart macros and data serialization (129 pts)]]></title>
            <link>https://medium.com/dartlang/an-update-on-dart-macros-data-serialization-06d3037d4f12</link>
            <guid>42871867</guid>
            <pubDate>Wed, 29 Jan 2025 22:08:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/dartlang/an-update-on-dart-macros-data-serialization-06d3037d4f12">https://medium.com/dartlang/an-update-on-dart-macros-data-serialization-06d3037d4f12</a>, See on <a href="https://news.ycombinator.com/item?id=42871867">Hacker News</a></p>
Couldn't get https://medium.com/dartlang/an-update-on-dart-macros-data-serialization-06d3037d4f12: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[From C++ to Clojure: Jank language promises best of both (119 pts)]]></title>
            <link>https://thenewstack.io/from-c-to-clojure-new-language-promises-best-of-both/</link>
            <guid>42871743</guid>
            <pubDate>Wed, 29 Jan 2025 21:56:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/from-c-to-clojure-new-language-promises-best-of-both/">https://thenewstack.io/from-c-to-clojure-new-language-promises-best-of-both/</a>, See on <a href="https://news.ycombinator.com/item?id=42871743">Hacker News</a></p>
Couldn't get https://thenewstack.io/from-c-to-clojure-new-language-promises-best-of-both/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Exposed DeepSeek database leaking sensitive information, including chat history (507 pts)]]></title>
            <link>https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak</link>
            <guid>42871371</guid>
            <pubDate>Wed, 29 Jan 2025 21:25:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak">https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak</a>, See on <a href="https://news.ycombinator.com/item?id=42871371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Wiz Research has identified a publicly accessible ClickHouse database belonging to DeepSeek, which allows full control over database operations, including the ability to access internal data. The exposure includes over a million lines of log streams containing chat history, secret keys, backend details, and other highly sensitive information. The Wiz Research team immediately and responsibly disclosed the issue to DeepSeek, which promptly secured the exposure.&nbsp;</p><p>In this blog post, we will detail our discovery and also consider the broader implications for the industry at large.&nbsp;&nbsp;&nbsp;</p><h2><a id="executive-summary-2"></a><strong>Executive Summary</strong>&nbsp;</h2><p>DeepSeek, a Chinese AI startup, has recently garnered significant media attention due to its groundbreaking AI models, particularly the DeepSeek-R1 reasoning model. This model rivals leading AI systems like OpenAI’s o1 in performance and stands out for its cost-effectiveness and efficiency.&nbsp;</p><p>As DeepSeek made waves in the AI space, the Wiz Research team set out to assess its external security posture and identify any potential vulnerabilities.&nbsp;</p><p>Within minutes, we found a publicly accessible ClickHouse database linked to DeepSeek, completely open and unauthenticated, exposing sensitive data. It was hosted at oauth2callback.deepseek.com:9000 and dev.deepseek.com:9000.&nbsp;</p><p>This database contained a significant volume of chat history, backend data and sensitive information, including log streams, API Secrets, and operational details. &nbsp;</p><p>More critically, the exposure allowed for full database control and potential <a href="https://www.wiz.io/academy/privilege-escalation">privilege escalation</a> within the DeepSeek environment, without any authentication or defense mechanism to the outside world.&nbsp;</p><figure></figure><figure></figure><h2><a id="exposure-walkthrough-10"></a><strong>Exposure Walkthrough</strong>&nbsp;&nbsp;</h2><p>Our reconnaissance began with assessing DeepSeek’s publicly accessible domains. By mapping the external attack surface with straightforward reconnaissance techniques (passive and active discovery of subdomains), we identified around 30 internet-facing subdomains. Most appeared benign, hosting elements like the chatbot interface, status page, and API documentation—none of which initially suggested a high-risk exposure.&nbsp;</p><p>However, as we expanded our search beyond standard HTTP ports (80/443), we detected two <strong>unusual, open ports (8123 &amp; 9000)</strong> associated with the following hosts:&nbsp;</p><ul><li><p><a rel="noreferrer noopener" target="_blank" href="http://oauth2callback.deepseek.com:8123/"><u>http://oauth2callback.deepseek.com:8123</u></a>&nbsp;&nbsp;</p></li><li><p><a rel="noreferrer noopener" target="_blank" href="http://dev.deepseek.com:8123/"><u>http://dev.deepseek.com:8123</u></a>&nbsp;&nbsp;</p></li><li><p><a rel="noreferrer noopener" target="_blank" href="http://oauth2callback.deepseek.com:9000/"><u>http://oauth2callback.deepseek.com:9000</u></a>&nbsp;&nbsp;</p></li><li><p><a rel="noreferrer noopener" target="_blank" href="http://dev.deepseek.com:9000/"><u>http://dev.deepseek.com:9000</u></a>&nbsp;</p></li></ul><p>Upon further investigation, these ports led to a <strong>publicly exposed ClickHouse database</strong>, accessible without any authentication at all – immediately raising red flags. &nbsp;</p><p>ClickHouse is an open-source, columnar database management system designed for fast analytical queries on large datasets. It was developed by Yandex and is widely used for real-time data processing, log storage, and big data analytics, which indicates such exposure as a very valuable and sensitive discovery.&nbsp;</p><figure></figure><p>By leveraging ClickHouse’s HTTP interface, we accessed the /play path, which <strong>allowed direct execution of arbitrary SQL queries</strong> via the browser. Running a simple SHOW TABLES; query returned a full list of accessible datasets.&nbsp;</p><figure><figcaption>Tables output from ClickHouse Web UI</figcaption></figure><p>Among them, one table stood out: log_stream, which contained extensive logs with <strong>highly sensitive data</strong>.&nbsp;</p><p>The log_stream table contained <strong>over 1 million log entries</strong>, with particularly revealing columns:&nbsp;</p><figure></figure><ul><li><p>timestamp – Logs dating from <strong>January 6, 2025</strong>&nbsp;</p></li><li><p>span_name – References to various internal <strong>DeepSeek API endpoints</strong>&nbsp;</p></li><li><p>string.values – <strong>Plaintext logs</strong>, including <strong>Chat History</strong>, <strong>API Keys, backend details, and operational metadata</strong>&nbsp;</p></li><li><p>_service – Indicating which <strong>DeepSeek service</strong> generated the logs&nbsp;</p></li><li><p>_source – Exposing the <strong>origin of log requests</strong>, containing <strong>Chat History, API Keys, directory structures, and chatbot metadata logs</strong>&nbsp;</p></li></ul><figure></figure><p>This level of access posed a critical risk to DeepSeek’s own security and for its end-users. Not only an attacker could retrieve sensitive logs and actual plain-text chat msgs, but they could also potentially <strong>exfiltrate plaintext passwords and local files</strong> along <strong>propriety information</strong> directly from the server using queries like: SELECT LOAD_FILE(‘{FileName}‘);&nbsp;&nbsp;</p><p><em>(Note: We did not execute intrusive queries beyond enumeration to preserve ethical research practices.)</em>&nbsp;</p><h2><a id="key-takeaways-26"></a><strong>Key Takeaways&nbsp;&nbsp;</strong>&nbsp;</h2><p>The rapid adoption of AI services without corresponding security is inherently risky. This exposure underscores the fact that the immediate <a href="https://www.wiz.io/academy/ai-security-risks">security risks for AI applications</a> stem from the infrastructure and tools supporting them.&nbsp;</p><p>While much of the attention around AI security is focused on futuristic threats, the real dangers often come from basic risks—like accidental external exposure of databases. These risks, which are fundamental to security, should remain a top priority for security teams.&nbsp;</p><p>As organizations rush to adopt AI tools and services from a growing number of startups and providers, it’s essential to remember that by doing so, we’re entrusting these companies with sensitive data. The rapid pace of adoption often leads to overlooking security, but protecting customer data must remain the top priority. It’s crucial that security teams work closely with AI engineers to ensure visibility into the architecture, tooling, and models being used, so we can safeguard data and prevent exposure.&nbsp;</p><h2><a id="conclusion-30"></a>Conclusion&nbsp;&nbsp;</h2><p>The world has never seen a piece of technology adopted at the pace of AI. Many AI companies have rapidly grown into critical infrastructure providers without the security frameworks that typically accompany such widespread adoptions. As AI becomes deeply integrated into businesses worldwide, the industry must recognize the risks of handling sensitive data and enforce security practices on par with those required for public cloud providers and major infrastructure providers.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parsing PDFs (and more) in Elixir using Rust (108 pts)]]></title>
            <link>https://www.chriis.dev/opinion/parsing-pdfs-in-elixir-using-rust</link>
            <guid>42871143</guid>
            <pubDate>Wed, 29 Jan 2025 21:05:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chriis.dev/opinion/parsing-pdfs-in-elixir-using-rust">https://www.chriis.dev/opinion/parsing-pdfs-in-elixir-using-rust</a>, See on <a href="https://news.ycombinator.com/item?id=42871143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Here's the thing about PDFs - they're complex beasts that require quite a bit of thinking to properly parse - they come in all shapes and sizes, and they can contain a lot of different types of data and formatting. 90% of the time, we just want to extract the text from the file, but that's not always easy - for the remaining 10%, well we won't be covering that in this blog post.</p>
<p>If you've been in the Elixir world for long enough, you'll probably have tried to parse a PDF file and realised that it's not as easy as it seems. A quick look on the <a href="https://elixirforum.com/t/parsing-pdf-file/23287">Elixir Forum</a> will quickly show you that there is no simple way to do it.</p>
<p>Most people will tell you to upload the file to S3 and use a Lambda to handle the contents. Offloading to AWS Lambda might seem elegant at first ("Look, Ma, no dependencies!"), but it comes with its own baggage:</p>
<ul>
<li><span></span>You're adding network latency to what should be a simple operation</li>
<li><span></span>AWS costs can spiral if you're processing lots of PDFs</li>
<li><span></span>You're now dependent on external services for core functionality</li>
<li><span></span>Debugging becomes a distributed systems problem</li>
</ul>
<p>These aren't ideal solutions - and software engineering is already made more complicated than it needs to be at times - we don't need to add more complexity to the mix.</p>
<p>We need a robust, native solution that plays nicely with the BEAM. So how do we do that?</p>
<h2>Enter the crabs!</h2>
<p>Elixir is my favourite language, but it can't do everything - web services, background jobs, and more are easy but sometimes we need a little help from our friends closer to the hardware for some of the tasks Elixir doesn't have a native solution for. That's where Rust and NIFs come in!</p>
<p>Rust is a systems programming language that is fast, safe, and easy to use. It's a great language for writing code that needs to be performant and reliable.</p>
<p>But Rust isn't just fast - it's "zero-cost abstractions" fast. What does that mean? You get high-level, ergonomic code that compiles down to something as efficient as hand-written C. For PDF parsing, where you're dealing with complex file formats and potentially large documents, this performance is a game-changer.</p>
<blockquote>
<p>What is a NIF?
A NIF (Native Implemented Function) is a way to call Rust code from Elixir - it's the BEAMs method of allowing processes to directly call native functions. It allows you to write code in Rust that can be called directly from Elixir, giving you the performance benefits of Rust without sacrificing the ease of use of Elixir.</p>
</blockquote>
<p>For this blog post, we're going to be using the <a href="https://github.com/yobix-ai/extractous">Extractous library</a> which provides fast and efficient unstructured data extraction in Rust. This combined with the NIFs in Elixir gives us a powerful combination for parsing PDFs.</p>
<h2>The Setup</h2>
<p>First things first, ensure you have Elixir and Rust installed on your machine.</p>
<p>Let's begin by creating a new LiveView Elixir application that will allow users to upload a PDF file and see a breakdown of the contents. We won't be needing any database functionality for this so we can use the <code>--no-ecto</code> flag to skip the database setup.</p>
<pre><code>mix phx<span>.</span>new elixir_pdf <span>--</span>no<span>-</span>ecto
</code></pre>
<p>We'll also need to add the <code>rustler</code> dependency to our <code>mix.exs</code> file so we can call Rust code from Elixir.</p>
<pre><code><span>defp</span> deps <span>do</span>
  <span>[</span>
    <span>{</span><span>:rustler</span><span>,</span> <span>"~&gt; 0.27.0"</span><span>}</span>
  <span>]</span>
<span>end</span>
</code></pre>
<p>Once we pull down our dependencies using <code>mix deps.get</code>, we can use <code>mix rustler.new</code> to generate our new Rust project in our code.</p>
<p>If you head to <code>lib/elixir_pdf/&lt;name_of_your_rust_project&gt;.ex</code>, you'll see that it's already generated a basic NIF for us. A default NIF implementation is provided for us, but we'll be implementing our own in the next step. I've named my Rust project <code>rustreader</code> for this example.</p>
<pre><code><span>defmodule</span> <span>RustReader</span> <span>do</span>
  <span>use</span> <span>Rustler</span><span>,</span> <span>otp_app:</span> <span>:elixir_pdf</span><span>,</span> <span>crate:</span> <span>"rustreader"</span>

  <span># Define the function that will be implemented in Rust</span>
  <span>def</span> <span>extract_pdf</span><span>(</span>_path<span>)</span><span>,</span> <span>do:</span> <span>:erlang</span><span>.</span><span>nif_error</span><span>(</span><span>:nif_not_loaded</span><span>)</span>
<span>end</span>

</code></pre>
<p>Now, let's grab the <code>extractous</code> library and add it to our <code>native/rustreader/Cargo.toml</code> file - this will allow us to use the <code>extractous</code> library in our Rust code.</p>
<pre><code><span>[</span>dependencies<span>]</span>
rustler <span>=</span> <span>"0.36.0"</span>
extractous <span>=</span> <span>"0.2.0"</span>
</code></pre>
<p>With this in place, we can run <code>cargo build</code> to build our Rust code - this will also pull down the <code>extractous</code> library and any other dependencies.</p>
<h2>The fun part - writing some code</h2>
<p>Next we need to actually write some Rust code to implement the <code>extract_pdf</code> function in our <code>native/rustreader/src/lib.rs</code> file.</p>
<pre><code><span>use</span> <span>extractous<span>::</span></span><span>Extractor</span><span>;</span>
<span>use</span> <span>rustler<span>::</span></span><span>{</span><span>Encoder</span><span>,</span> <span>Env</span><span>,</span> <span>NifResult</span><span>,</span> <span>Term</span><span>}</span><span>;</span>

<span>#[rustler::nif(schedule = <span>"DirtyCpu"</span>)]</span>
<span>fn</span> <span>extract_pdf</span><span>(</span>path<span>:</span> <span>String</span><span>)</span> <span>-&gt;</span> <span>NifResult</span><span>&lt;</span><span>(</span><span>String</span><span>,</span> <span>String</span><span>)</span><span>&gt;</span> <span>{</span>
    <span>let</span> extractor <span>=</span> <span>Extractor</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>

    <span>match</span> extractor<span>.</span><span>extract_file_to_string</span><span>(</span><span>&amp;</span>path<span>)</span> <span>{</span>
        <span>Ok</span><span>(</span><span>(</span>content<span>,</span> metadata<span>)</span><span>)</span> <span>=&gt;</span> <span>Ok</span><span>(</span><span>(</span>content<span>,</span> <span>format!</span><span>(</span><span>"{:?}"</span><span>,</span> metadata<span>)</span><span>)</span><span>)</span><span>,</span>
        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>Err</span><span>(</span><span>rustler<span>::</span></span><span>Error</span><span>::</span><span>Term</span><span>(</span><span>Box</span><span>::</span><span>new</span><span>(</span><span>format!</span><span>(</span><span>"Extraction failed: {}"</span><span>,</span> e<span>)</span><span>)</span><span>)</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>rustler<span>::</span></span><span>init!</span><span>(</span><span>"Elixir.RustReader"</span><span>,</span> <span>[</span>extract_pdf<span>]</span><span>)</span><span>;</span>
</code></pre>
<p>This code will define a new instance of the <code>Extractor</code> struct and use it to extract the contents of the PDF file. We'll then return the contents and the metadata as a tuple.</p>
<p>The magic of the <code>rustler::init!</code> macro is that it will automatically generate the necessary code to call the Rust function from Elixir.</p>
<p>Astute observers will note our use of the <code>DirtyCpu</code> schedule. This ingenious feature instructs Rustler and the BEAM to automatically schedule our task in a manner that prevents global blocking during execution. This functionality, known as a DirtyNif, significantly simplifies our work compared to the complexities of manual implementation in C.</p>

<p>Now we need to write a some simple LiveView Elixir code to allow users to upload a PDF file and then call our Rust function from the server.</p>
<pre><code><span>defmodule</span> <span>ElixirPdfWeb</span><span>.</span><span>HomeLive</span> <span>do</span>
  <span>use</span> <span>ElixirPdfWeb</span><span>,</span> <span>:live_view</span>

  <span>@impl</span> <span>true</span>
  <span>def</span> <span>mount</span><span>(</span>_params<span>,</span> _session<span>,</span> socket<span>)</span> <span>do</span>
    <span>{</span><span>:ok</span><span>,</span>
     socket
     <span>|&gt;</span> <span>assign</span><span>(</span><span>:uploaded_files</span><span>,</span> <span>[</span><span>]</span><span>)</span>
     <span>|&gt;</span> <span>allow_upload</span><span>(</span><span>:pdf</span><span>,</span>
       <span>accept:</span> <span>~w(.pdf)</span><span>,</span>
       <span>max_entries:</span> <span>1</span><span>,</span>
       <span># 10MB limit</span>
       <span>max_file_size:</span> <span>10_000_000</span><span>,</span>
       <span>chunk_size:</span> <span>64_000</span>
     <span>)</span><span>}</span>
  <span>end</span>

  <span>@impl</span> <span>true</span>
  <span>def</span> <span>handle_event</span><span>(</span><span>"validate"</span><span>,</span> _params<span>,</span> socket<span>)</span> <span>do</span>
    <span>{</span><span>:noreply</span><span>,</span> socket<span>}</span>
  <span>end</span>

  <span>@impl</span> <span>true</span>
  <span>def</span> <span>handle_event</span><span>(</span><span>"save"</span><span>,</span> _params<span>,</span> socket<span>)</span> <span>do</span>
    uploaded_files <span>=</span>
      <span>consume_uploaded_entries</span><span>(</span>socket<span>,</span> <span>:pdf</span><span>,</span> <span>fn</span> <span>%</span><span>{</span><span>path:</span> path<span>}</span><span>,</span> _entry <span>-&gt;</span>
        dest <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>[</span><span>"priv"</span><span>,</span> <span>"static"</span><span>,</span> <span>"uploads"</span><span>,</span> <span>Path</span><span>.</span><span>basename</span><span>(</span>path<span>)</span><span>]</span><span>)</span>
        <span>File</span><span>.</span><span>cp!</span><span>(</span>path<span>,</span> dest<span>)</span>
        <span>{</span><span>:ok</span><span>,</span> dest<span>}</span>
      <span>end</span><span>)</span>

    pdf_document <span>=</span>
      uploaded_files
      <span>|&gt;</span> <span>hd</span><span>(</span><span>)</span>

    <span>{</span><span>:noreply</span><span>,</span>
     socket
     <span>|&gt;</span> <span>assign</span><span>(</span><span>:pdf_document</span><span>,</span> pdf_document<span>)</span>
     <span>|&gt;</span> <span>update</span><span>(</span><span>:uploaded_files</span><span>,</span> <span>&amp;</span><span>(</span><span>&amp;1</span> <span>++</span> uploaded_files<span>)</span><span>)</span><span>}</span>
  <span>end</span>
<span>end</span>
</code></pre>
<p>Alongside this we need to add a little bit of code to our <code>router.ex</code> file to allow us to upload files.</p>
<pre><code>scope <span>"/"</span><span>,</span> <span>ElixirPdfWeb</span> <span>do</span>
  pipe_through <span>:browser</span>

  live <span>"/"</span><span>,</span> <span>HomeLive</span>
<span>end</span>
</code></pre>
<p>We also need a simple LiveView template to allow users to upload a PDF file and see the results.</p>
<pre><code>&lt;div class<span>=</span><span>"mx-auto max-w-2xl py-8"</span>&gt;
  <span>&lt;</span>div class<span>=</span><span>"flex flex-col items-center justify-center"</span>&gt;
    <span>&lt;</span>h1 class<span>=</span><span>"text-2xl font-bold mb-8"</span>&gt;<span>Upload</span> <span>PDF</span>&lt;<span>/</span>h1<span>&gt;</span>

    <span>&lt;</span>form phx<span>-</span>submit<span>=</span><span>"save"</span> phx<span>-</span>change<span>=</span><span>"validate"</span> class<span>=</span><span>"w-full"</span>&gt;
      <span>&lt;</span>div class<span>=</span><span>"flex flex-col items-center space-y-4 w-full"</span> phx<span>-</span>drop<span>-</span>target<span>=</span><span>{</span><span>@uploads</span><span>.</span>pdf<span>.</span>ref<span>}</span><span>&gt;</span>
        <span>&lt;</span>div class<span>=</span><span>"w-full border-2 border-dashed border-gray-300 rounded-lg p-12 text-center hover:border-gray-400 transition-colors"</span>&gt;
          <span>&lt;</span>div class<span>=</span><span>"space-y-2"</span>&gt;
            <span>&lt;</span>div class<span>=</span><span>"text-gray-600"</span>&gt;
              <span>Drag</span> <span>and</span> drop your <span>PDF</span> here <span>or</span>
              <span>&lt;</span>label class<span>=</span><span>"cursor-pointer text-blue-500 hover:text-blue-600"</span>&gt;
                browse <span>&lt;</span><span>.</span>live_file_input upload<span>=</span><span>{</span><span>@uploads</span><span>.</span>pdf<span>}</span> class<span>=</span><span>"hidden"</span> <span>/</span>&gt;
              <span>&lt;</span><span>/</span>label<span>&gt;</span>
            <span>&lt;</span><span>/</span>div<span>&gt;</span>
            <span>&lt;</span>p class<span>=</span><span>"text-xs text-gray-500"</span>&gt;<span>PDF</span> files only<span>,</span> up to 10MB<span>&lt;</span><span>/</span>p<span>&gt;</span>
          <span>&lt;</span><span>/</span>div<span>&gt;</span>
        <span>&lt;</span><span>/</span>div<span>&gt;</span>

        <span>&lt;</span><span>%</span><span>=</span> <span>for</span> entry <span>&lt;-</span> <span>@uploads</span><span>.</span>pdf<span>.</span>entries <span>do</span> <span>%</span><span>&gt;</span>
          <span>&lt;</span>div class<span>=</span><span>"w-full"</span>&gt;
            <span>&lt;</span>div class<span>=</span><span>"flex items-center justify-between p-4 bg-gray-50 rounded"</span>&gt;
              <span>&lt;</span>div class<span>=</span><span>"flex items-center space-x-2"</span>&gt;
                <span>&lt;</span>span class<span>=</span><span>"font-medium"</span>&gt;<span>{</span>entry<span>.</span>client_name<span>}</span><span>&lt;</span><span>/</span>span<span>&gt;</span>
                <span>&lt;</span>span class<span>=</span><span>"text-sm text-gray-500"</span>&gt;
                  <span>(</span><span>{</span>entry<span>.</span>client_size<span>}</span><span>B</span><span>)</span>
                <span>&lt;</span><span>/</span>span<span>&gt;</span>
              <span>&lt;</span><span>/</span>div<span>&gt;</span>

              <span>&lt;</span>button
                type<span>=</span><span>"button"</span>
                class<span>=</span><span>"text-red-500 hover:text-red-700"</span>
                phx<span>-</span>click<span>=</span><span>"cancel-upload"</span>
                phx<span>-</span>value<span>-</span>ref<span>=</span><span>{</span>entry<span>.</span>ref<span>}</span>
              <span>&gt;</span>
                <span>&amp;</span>times;
              <span>&lt;</span><span>/</span>button<span>&gt;</span>
            <span>&lt;</span><span>/</span>div<span>&gt;</span>

            <span>&lt;</span><span>%</span><span>=</span> <span>for</span> err <span>&lt;-</span> <span>upload_errors</span><span>(</span><span>@uploads</span><span>.</span>pdf<span>,</span> entry<span>)</span> <span>do</span> <span>%</span><span>&gt;</span>
              <span>&lt;</span>div class<span>=</span><span>"text-red-500 text-sm"</span>&gt;
                <span>{</span>err<span>}</span>
              <span>&lt;</span><span>/</span>div<span>&gt;</span>
            <span>&lt;</span><span>%</span> <span>end</span> <span>%</span><span>&gt;</span>
          <span>&lt;</span><span>/</span>div<span>&gt;</span>
        <span>&lt;</span><span>%</span> <span>end</span> <span>%</span><span>&gt;</span>

        <span>&lt;</span><span>%</span><span>=</span> <span>if</span> <span>length</span><span>(</span><span>@uploads</span><span>.</span>pdf<span>.</span>entries<span>)</span> <span>&gt;</span> <span>0</span> <span>do</span> <span>%</span><span>&gt;</span>
          <span>&lt;</span>button
            type<span>=</span><span>"submit"</span>
            class<span>=</span><span>"px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600 transition-colors"</span>
          <span>&gt;</span>
            <span>Upload</span>
          <span>&lt;</span><span>/</span>button<span>&gt;</span>
        <span>&lt;</span><span>%</span> <span>end</span> <span>%</span><span>&gt;</span>
      <span>&lt;</span><span>/</span>div<span>&gt;</span>
    <span>&lt;</span><span>/</span>form<span>&gt;</span>
  <span>&lt;</span><span>/</span>div<span>&gt;</span>
<span>&lt;</span><span>/</span>div<span>&gt;</span>
</code></pre>
<h2>Putting it all together</h2>
<p>So we can upload a PDF file - but let's call our Rust function and see what it returns.</p>
<p>In our handle_event function, we can call our Rust function as simply as this:</p>
<pre><code>  <span>@impl</span> <span>true</span>
  <span>def</span> <span>handle_event</span><span>(</span><span>"save"</span><span>,</span> _params<span>,</span> socket<span>)</span> <span>do</span>
    uploaded_files <span>=</span>
      <span>consume_uploaded_entries</span><span>(</span>socket<span>,</span> <span>:pdf</span><span>,</span> <span>fn</span> <span>%</span><span>{</span><span>path:</span> path<span>}</span><span>,</span> _entry <span>-&gt;</span>
        dest <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>[</span><span>"priv"</span><span>,</span> <span>"static"</span><span>,</span> <span>"uploads"</span><span>,</span> <span>Path</span><span>.</span><span>basename</span><span>(</span>path<span>)</span><span>]</span><span>)</span>
        <span>File</span><span>.</span><span>cp!</span><span>(</span>path<span>,</span> dest<span>)</span>
        <span>{</span><span>:ok</span><span>,</span> dest<span>}</span>
      <span>end</span><span>)</span>

    pdf_document <span>=</span>
      uploaded_files
      <span>|&gt;</span> <span>hd</span><span>(</span><span>)</span>
      <span>|&gt;</span> <span>RustReader</span><span>.</span><span>extract_pdf</span><span>(</span><span>)</span> <span>## This is where the magic happens!</span>

    <span>{</span><span>:noreply</span><span>,</span>
     socket
     <span>|&gt;</span> <span>assign</span><span>(</span><span>:pdf_document</span><span>,</span> pdf_document<span>)</span>
     <span>|&gt;</span> <span>update</span><span>(</span><span>:uploaded_files</span><span>,</span> <span>&amp;</span><span>(</span><span>&amp;1</span> <span>++</span> uploaded_files<span>)</span><span>)</span><span>}</span>
  <span>end</span>
</code></pre>
<p>We're grabbing the first uploaded file and calling our Rust function. The result is a tuple containing the contents of the PDF and the metadata.</p>
<p>Let's try it out with the <a href="https://liveviewcookbook.com/">LiveView Cookbook PDF</a>.</p>
<p><img src="https://i.imgur.com/dpyzduq.png" alt="Unstructured"></p>
<p>Success! We've now got a PDF parser that's fast, efficient, and written in Rust.</p>
<p>But that's quite hard to read so we're not done yet, let's make this a little nicer to work with.</p>
<p>Let's create a new module to handle the Jason encoding of the metadata.</p>
<pre><code><span>defmodule</span> <span>ElixirPdf</span><span>.</span><span>PdfDocument</span> <span>do</span>
  <span>@derive</span> <span>{</span><span>Jason</span><span>.</span><span>Encoder</span><span>,</span> <span>only:</span> <span>[</span><span>:content</span><span>,</span> <span>:metadata</span><span>]</span><span>}</span>
  <span>defstruct</span> <span>[</span><span>:content</span><span>,</span> <span>:metadata</span><span>]</span>

  <span>def</span> <span>from_rustler</span><span>(</span><span>{</span>content<span>,</span> metadata_json<span>}</span><span>)</span> <span>do</span>
    with <span>{</span><span>:ok</span><span>,</span> metadata<span>}</span> <span>&lt;-</span> <span>Jason</span><span>.</span><span>decode</span><span>(</span>metadata_json<span>)</span> <span>do</span>
      <span>%</span>__MODULE__<span>{</span>
        <span>content:</span> <span>String</span><span>.</span><span>trim</span><span>(</span>content<span>)</span><span>,</span>
        <span>metadata:</span> metadata
      <span>}</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre>
<p>This will allow us to encode the metadata to JSON and decode it back to a struct in Elixir to make it easier to work with.</p>
<p>All we have to do is pipe the result of our Rust function through this module and we're done!</p>
<pre><code><span>...</span>
<span>RustReader</span><span>.</span><span>extract_pdf</span><span>(</span><span>@pdf_document</span><span>)</span>
<span>|&gt;</span> <span>ElixirPdf</span><span>.</span><span>PdfDocument</span><span>.</span><span>from_rustler</span><span>(</span><span>)</span>
<span>...</span>
</code></pre>
<p>Now when we upload a PDF file, we'll see the metadata results in a much more readable format.</p>
<p><img src="https://i.imgur.com/QvG2QUV.png" alt="Structured"></p>
<p>Much better!</p>
<p>This approach is simple and effective - it's fast, efficient, and leverages the strengths of both Elixir and Rust to provide a robust solution for PDF parsing.</p>
<p>We're only talking about PDF files here but extractous supports a <a href="https://github.com/yobix-ai/extractous?tab=readme-ov-file#-supported-file-formats">wide range of file types</a> - so keep that in mind if you need to extract data from other file types.</p>
<h2>What about deployment?</h2>
<p>Keep in mind that this is a native extension and so you'll need to build the Rust code before deploying your application. This can be done in a CI/CD pipeline or manually.</p>
<p>If you're using Docker, you can update the <code>Dockerfile</code> to build the Rust code as part of the build process and update <code>config/prod.exs</code> to tell Rustler to skip compilation and load the compiled NIF from where it was built in the Docker image.</p>
<p>Check out the <a href="https://fly.io/phoenix-files/elixir-and-rust-is-a-good-mix/">Fly.io blog post</a> for more information on how to deploy an Elixir application with Rust NIFs.</p>
<h2>Shoutouts</h2>
<p>Some shoutouts are in order - firstly this blog post from <a href="https://fly.io/phoenix-files/elixir-and-rust-is-a-good-mix/">Fly.io's Phoenix Files</a> outlining how to use Rust with NIFs in Elixir. It was a key inspiration for this approach and gave me the idea to use Rust in the first place. Also check out Fly in general for some great Elixir hosting options - I use them for all my Elixir applications.</p>
<p>Also a shoutout for the excellent <a href="https://github.com/yobix-ai/extractous">Extractous library</a> which provides fast and efficient unstructured data extraction in Rust - it's also 25x faster than the very popular unstructured-io library.</p>
<p>Finally, a shoutout to the <a href="https://github.com/rusterlium/rustler">Rustler</a> library for providing a simple way to call Rust code from Elixir!</p>

<p>All the code for this blog post can be <a href="https://github.com/chrisgreg/elixir_pdf_tutorial">found here on my Github</a> if anyone wants to clone it and run it yourselves!</p>
<p>I hope you found this post useful, subscribe to my Substack below for similar content and  <a href="https://www.twitter.com/codestirring">follow me on Twitter</a> and <a href="https://bsky.app/profile/codestirring.bsky.social">Bluesky</a> for more Elixir (and general programming) tips.</p>
<p>If you're building a Phoenix project, I'd also encourage you to take a look at my open-source component library <a href="https://bloom-ui.fly.dev/">Bloom</a> to help you out even further or check out my <a href="https://mmbl.io/">new voice to notes application to automatically tag and sync your voice to your calendar</a>.</p><div><p><img src="https://www.liveviewcookbook.com/liveview-cook-book2.png"></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making the video that made Gorillaz (180 pts)]]></title>
            <link>https://animationobsessive.substack.com/p/making-the-video-that-made-gorillaz</link>
            <guid>42870990</guid>
            <pubDate>Wed, 29 Jan 2025 20:50:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://animationobsessive.substack.com/p/making-the-video-that-made-gorillaz">https://animationobsessive.substack.com/p/making-the-video-that-made-gorillaz</a>, See on <a href="https://news.ycombinator.com/item?id=42870990">Hacker News</a></p>
Couldn't get https://animationobsessive.substack.com/p/making-the-video-that-made-gorillaz: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Soviet Shoe Factory Principle (109 pts)]]></title>
            <link>https://wiki.c2.com/?SovietShoeFactoryPrinciple</link>
            <guid>42870690</guid>
            <pubDate>Wed, 29 Jan 2025 20:26:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.c2.com/?SovietShoeFactoryPrinciple">https://wiki.c2.com/?SovietShoeFactoryPrinciple</a>, See on <a href="https://news.ycombinator.com/item?id=42870690">Hacker News</a></p>
Couldn't get https://wiki.c2.com/?SovietShoeFactoryPrinciple: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Dead Games (144 pts)]]></title>
            <link>https://garry.net/posts/dead-games</link>
            <guid>42870230</guid>
            <pubDate>Wed, 29 Jan 2025 19:52:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garry.net/posts/dead-games">https://garry.net/posts/dead-games</a>, See on <a href="https://news.ycombinator.com/item?id=42870230">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-body" b-14kpe6t92i=""><article b-14kpe6t92i="">






<div b-yrzs0ob67i=""><p>You know when people spend 2 years making games and when they release, they don't sell anything, so they stop selling it and throw it all in then bin..</p><p>What happens to all those sounds and models and animations they made? Do they just go in the bin?</p><p>I guess there's reasons they don't just slap an open license on them and release them for free. Like time, storage, bandwidth, and permission. Maybe they use textures or sounds from a library and can't redistribute?</p><p>Well, I was just thinking, what a shame it all gets chucked in the bin, when it'd be nice to be able to use those assets in something like gmod/sbox.&nbsp;</p><p>If your game died and you have some cool assets, email me and tell me what you've got and what you want for them.. garrynewman@gmail.com</p></div></article></div><p>
        
            An error has occurred. This application may no longer respond until reloaded.
        
        
        <a href="">Reload</a>
        <a>🗙</a>
    </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Airflow – Stream media files directly from macOS to AirPlay devices (107 pts)]]></title>
            <link>https://airflow.app/</link>
            <guid>42870171</guid>
            <pubDate>Wed, 29 Jan 2025 19:47:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://airflow.app/">https://airflow.app/</a>, See on <a href="https://news.ycombinator.com/item?id=42870171">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h2>What if we say it's not like the others?</h2>
<p><img src="https://airflow.app/images/pipeline.svg"></p><p>Airflow is <strong>different</strong> We're not cutting any corners. This is <em>not yet another FFmpeg wrapper</em> like you might have seen elsewhere. Don't get us wrong, we love FFmpeg and use many of its parts under the hood, but our custom built video processing pipeline goes way beyond wrapping FFmpeg and calling it a day. We've been working on it for years it and <strong>it lets us do things that other similar software <span id="what_container">simply can't</span></strong>.</p>


<h2>...with a very particular set of skills...</h2>
<p><img src="https://airflow.app/images/device.png" srcset="https://airflow.app/images/device@2x.png 2x"></p><p>Airflow is a razor sharp focused software. It supports specific set of devices and it will pull every trick in the book to get the best possible results on these devices. It may not stream video to your smart fridge, but it will gladly <em>push your Chromecast, Apple TV and AirPlay 2 TVs to their limits</em>.</p>
<p>And yes, Airflow can handle pretty much <strong>any video format and codec you throw at it</strong>.</p>
</div>
<div>
<h2>Pixels, pixels everywhere!</h2>
<p>Airflow can stream full <strong>4K HDR</strong> HEVC files to Chromecast Ultra, Built-in, Apple TV 4K and AirPlay 2 enabled TVs. It will go out of its way not to touch the original video stream unless absolutely needed for compatibility reasons, ensuring <strong>best possible video quality</strong> with <strong>lowest CPU load</strong> (your computer fans will thank you). As far as we can tell, Airflow is still <em>the only desktop software that can natively stream HEVC videos to Apple TV and AirPlay 2 TVs</em>.</p>
<p>And for those pesky videos that are incompatible with your device - Airflow will handle that tranparently, with <em>hardware accelerated transcoding</em> if your computer supports it.</p>
<p><img src="https://airflow.app/images/settings.png" srcset="https://airflow.app/images/settings@2x.png 2x">
</p></div>
<div>
<h2>Audio pipeline that goes to eleven</h2>
<p><img src="https://airflow.app/images/audio-delay.png" srcset="https://airflow.app/images/audio-delay@2x.png 2x"></p><p>Full <strong>multichannel support</strong> including DD+ passthrough with Dolby Atmos? Of course.</p>
<p><strong>Advanced adaptive volume booster + limiter</strong> for late night watching when you don't want to disturb your neighbours with loud scenes but still want to hear the dialogue clearly? Check.</p>
<p><em>Spatial headphone downmix</em> for surround sound videos? Also check.</p>
<p>Detailed <em>A/V sync adjustment</em> where you can compensate for the delay of individual devices like bluetooth headphones? Airflow has it.</p>
</div>
<div>
<h2>And subtitle support to match it</h2>
<p><img src="https://airflow.app/images/subtitle-menu.png" srcset="https://airflow.app/images/subtitle-menu@2x.png 2x"></p><p>For both <strong>embedded and external subtitles</strong>. It's a bit of a secret that pretty much every other streaming software needs to extract embedded subtitle tracks before playing the video. That involves reading the entire file upfront! Crazy, right? <em>Airflow needs no such crude tricks</em>. Embedded or external, for our playback pipeline it's all the same. All widely used subtitle formats are supported, now including vobsub. Integrated <strong>opensubtitles.org</strong> search is a cherry on top.</p>
<p><img src="https://airflow.app/images/sub-search.png" srcset="https://airflow.app/images/sub-search@2x.png 2x"></p><h2>...with real time text recognition</h2>
<p>Some subtitles (DVD, Vobsub, Bluray) are stored as pictures. This means that the only way to render them when streaming is to burn them in the video. That's inconvenient to say the least. It massively increases CPU load (think fan noise and heat) and it's completely infeasible to do for 4K videos.</p>
<p>Enter our new <em>realtime subtitle text recognition (OCR)</em>. During playback Airflow will transparently extract the text from picture subtitles and render it on target device just like it would with regular text subtitles.</p>
<p><img src="https://airflow.app/images/vobsub.png">
<img src="https://airflow.app/images/down-arrow.svg"></p><p>
    We'll be in <i>bigger trouble</i><br>
    if the thief has it.
</p>
</div>
<div>
<h2>But wait, there's more!</h2>
<p><img src="https://airflow.app/images/speed-test.png" srcset="https://airflow.app/images/speed-test@2x.png 2x"></p><p>The "small" things, like the <strong>scrubbing preview</strong>, <em>beautiful polished user interface</em>, multiple playlists support, meticulous last position tracking, or the integrated <strong>Speed Test for Chromecast</strong>, which is invaluable when dealing with network connection issues. The list goes on.</p>

<p><img src="https://airflow.app/images/screenshot-remote-app.png" srcset="https://airflow.app/images/screenshot-remote-app@2x.png 2x"></p><p>Did we mention the <strong>remote control companion app</strong> for Android and iPhone? No? Well, it's pretty cool. It lets you control all Airflow features from the comfort of your couch. And it's completely free!</p>
<p><a href="https://apps.apple.com/us/app/airflow-remote/id1459536549?ls=1"><img src="https://airflow.app/images/appstore.svg"></a>
     <a href="https://play.google.com/store/apps/details?id=xyz.bitcave.airflow.remote"><img src="https://airflow.app/images/googleplay_alt.svg"></a>
</p>
</div>
</div><div>
<h2>F.A.Q.</h2>
<h3>Is Airflow a subscription of any kind?</h3>
<p>Nope. One time payment only. Airflow license does not expire.</p>
<h3>What if I have multiple computers? Do you I need multiple licenses?</h3>
<p>You only need one license. You can activate Airflow on all computers that you own using single license.</p>
<h3>I lost my license key</h3>
<p>No worries. You can always retreive your license <a href="https://license.airflow.app/my">here</a>.</p>
<h3>I'm having issue with Airflow</h3>
<p>Please see our <a href="https://airflow.app/troubleshooting">troubleshooting page</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo to test its autonomous driving technology in over 10 new cities (131 pts)]]></title>
            <link>https://www.reuters.com/business/autos-transportation/alphabets-waymo-test-its-autonomous-driving-technology-over-10-new-cities-2025-01-29/</link>
            <guid>42870056</guid>
            <pubDate>Wed, 29 Jan 2025 19:38:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/autos-transportation/alphabets-waymo-test-its-autonomous-driving-technology-over-10-new-cities-2025-01-29/">https://www.reuters.com/business/autos-transportation/alphabets-waymo-test-its-autonomous-driving-technology-over-10-new-cities-2025-01-29/</a>, See on <a href="https://news.ycombinator.com/item?id=42870056">Hacker News</a></p>
Couldn't get https://www.reuters.com/business/autos-transportation/alphabets-waymo-test-its-autonomous-driving-technology-over-10-new-cities-2025-01-29/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[SmolGPT: A minimal PyTorch implementation for training a small LLM from scratch (246 pts)]]></title>
            <link>https://github.com/Om-Alve/smolGPT</link>
            <guid>42868770</guid>
            <pubDate>Wed, 29 Jan 2025 18:09:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Om-Alve/smolGPT">https://github.com/Om-Alve/smolGPT</a>, See on <a href="https://news.ycombinator.com/item?id=42868770">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">SMOL-GPT 🦾</h2><a id="user-content-smol-gpt-" aria-label="Permalink: SMOL-GPT 🦾" href="#smol-gpt-"></a></p>
<p dir="auto">A minimal PyTorch implementation for training your own small LLM from scratch. Designed for educational purposes and simplicity, featuring efficient training, flash attention, and modern sampling techniques.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features ✨</h2><a id="user-content-features-" aria-label="Permalink: Features ✨" href="#features-"></a></p>
<ul dir="auto">
<li><strong>Minimal Codebase</strong>: Pure PyTorch implementation with no abstraction overhead</li>
<li><strong>Modern Architecture</strong>: GPT model with:
<ul dir="auto">
<li>Flash Attention (when available)</li>
<li>RMSNorm and SwiGLU</li>
<li>Efficient top-k/p/min-p sampling</li>
</ul>
</li>
<li><strong>Training Features</strong>:
<ul dir="auto">
<li>Mixed precision (bfloat16/float16)</li>
<li>Gradient accumulation</li>
<li>Learning rate decay with warmup</li>
<li>Weight decay &amp; gradient clipping</li>
</ul>
</li>
<li><strong>Dataset Support</strong>: Built-in TinyStories dataset processing</li>
<li><strong>Custom Tokenizer</strong>: SentencePiece tokenizer training integration</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation 🛠️</h2><a id="user-content-installation-️" aria-label="Permalink: Installation 🛠️" href="#installation-️"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install torch sentencepiece tqdm requests numpy"><pre>pip install torch sentencepiece tqdm requests numpy</pre></div>
<p dir="auto"><strong>Requirements</strong>:</p>
<ul dir="auto">
<li>Python 3.8+</li>
<li>PyTorch 2.0+ with CUDA</li>
<li>Modern GPU (recommended)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start 🚀</h2><a id="user-content-quick-start-" aria-label="Permalink: Quick Start 🚀" href="#quick-start-"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option 1: Full Training Cycle</h3><a id="user-content-option-1-full-training-cycle" aria-label="Permalink: Option 1: Full Training Cycle" href="#option-1-full-training-cycle"></a></p>
<ol dir="auto">
<li><strong>Prepare Dataset</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python preprocess.py prepare-dataset --vocab-size 4096"><pre>python preprocess.py prepare-dataset --vocab-size 4096</pre></div>
<ol start="2" dir="auto">
<li><strong>Start Training</strong></li>
</ol>

<ol start="3" dir="auto">
<li><strong>Generate Text</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python sample.py \
    --prompt &quot;Once upon a time&quot; \
    --num_samples 3 \
    --temperature 0.7 \
    --max_new_tokens 500"><pre>python sample.py \
    --prompt <span><span>"</span>Once upon a time<span>"</span></span> \
    --num_samples 3 \
    --temperature 0.7 \
    --max_new_tokens 500</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option 2: Use Pre-trained Model</h3><a id="user-content-option-2-use-pre-trained-model" aria-label="Permalink: Option 2: Use Pre-trained Model" href="#option-2-use-pre-trained-model"></a></p>
<ol dir="auto">
<li><strong>Download Assets</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Download tokenizer
wget https://huggingface.co/OmAlve/TinyStories-SmolGPT/resolve/main/tok4096.model -P data/

# Download pre-trained checkpoint
wget https://huggingface.co/OmAlve/TinyStories-SmolGPT/resolve/main/ckpt-v1.pt -P out/"><pre><span><span>#</span> Download tokenizer</span>
wget https://huggingface.co/OmAlve/TinyStories-SmolGPT/resolve/main/tok4096.model -P data/

<span><span>#</span> Download pre-trained checkpoint</span>
wget https://huggingface.co/OmAlve/TinyStories-SmolGPT/resolve/main/ckpt-v1.pt -P out/</pre></div>
<ol start="2" dir="auto">
<li><strong>Run Inference</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python sample.py \
    --prompt &quot;Once upon a time&quot; \
    --tokenizer_path data/tok4096.model \
    --ckpt_dir out/ \
    --num_samples 3 \
    --max_new_tokens 200 \
    --temperature 0.7"><pre>python sample.py \
    --prompt <span><span>"</span>Once upon a time<span>"</span></span> \
    --tokenizer_path data/tok4096.model \
    --ckpt_dir out/ \
    --num_samples 3 \
    --max_new_tokens 200 \
    --temperature 0.7</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pre-trained Model Details 🔍</h2><a id="user-content-pre-trained-model-details-" aria-label="Permalink: Pre-trained Model Details 🔍" href="#pre-trained-model-details-"></a></p>
<p dir="auto">The provided checkpoint was trained on the TinyStories dataset.</p>
<p dir="auto">Architecture:</p>
<ul dir="auto">
<li>4096-token vocabulary</li>
<li>8 heads</li>
<li>8-layer transformer</li>
<li>512 embedding dimension</li>
<li>Trained on <code>~4 Billion Tokens</code> for around <code>18.5</code> hours</li>
</ul>
<p dir="auto">Validation Loss - <code>1.0491</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Om-Alve/smolGPT/blob/main/assets/loss.png"><img src="https://github.com/Om-Alve/smolGPT/raw/main/assets/loss.png" alt="Loss Curve"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sample Outputs 📝</h2><a id="user-content-sample-outputs-" aria-label="Permalink: Sample Outputs 📝" href="#sample-outputs-"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 1</h3><a id="user-content-example-1" aria-label="Permalink: Example 1" href="#example-1"></a></p>
<div data-snippet-clipboard-copy-content="Prompt: One day, Lily met a unicorn

Output:
One day, Lily met a unicorn in the park. The unicorn had shiny fur and a pretty dress. Lily asked the unicorn, &quot;Where did you come from?&quot;
The unicorn replied, &quot;I came from the forest and wanted to meet you, Lily. I am here to make sure you are safe.&quot;
Lily and the unicorn played together and had lots of fun. But then, the unicorn started to act funny. Lily didn't know what was happening. Suddenly, the unicorn turned into a little girl and said, &quot;I was under a spell, and your kindness broke it. Thank you for breaking it.&quot;
Lily was surprised and happy. She gave the unicorn a big hug and they played together all day. From that day on, the unicorn would always come to play with Lily - her new friend."><pre lang="text"><code>Prompt: One day, Lily met a unicorn

Output:
One day, Lily met a unicorn in the park. The unicorn had shiny fur and a pretty dress. Lily asked the unicorn, "Where did you come from?"
The unicorn replied, "I came from the forest and wanted to meet you, Lily. I am here to make sure you are safe."
Lily and the unicorn played together and had lots of fun. But then, the unicorn started to act funny. Lily didn't know what was happening. Suddenly, the unicorn turned into a little girl and said, "I was under a spell, and your kindness broke it. Thank you for breaking it."
Lily was surprised and happy. She gave the unicorn a big hug and they played together all day. From that day on, the unicorn would always come to play with Lily - her new friend.
</code></pre></div>
<div data-snippet-clipboard-copy-content="Prompt: The dragon flew over the mountains

Output:
The dragon flew over the mountains, over the rivers and over the rivers. He was very brave and strong.
One day, the dragon saw something very strange. It was a big, shiny rock. He wanted to know what it was, so he flew down and touched it with his nose. Suddenly, the rock began to move!
The dragon was so surprised! He had never seen anything like it before. He looked around and saw that it was a little mouse! The mouse was very scared and started to run away.
The dragon was very sad. He wanted to help the mouse, so he decided to try and make friends. He flew around and around until he found the mouse. He said hello to the mouse and asked if he wanted to be friends.
The mouse was so happy! He said yes, and they played together all day long. From then on, the dragon and the mouse were the best of friends. They had lots of fun together and the dragon was never lonely again."><pre><code>Prompt: The dragon flew over the mountains

Output:
The dragon flew over the mountains, over the rivers and over the rivers. He was very brave and strong.
One day, the dragon saw something very strange. It was a big, shiny rock. He wanted to know what it was, so he flew down and touched it with his nose. Suddenly, the rock began to move!
The dragon was so surprised! He had never seen anything like it before. He looked around and saw that it was a little mouse! The mouse was very scared and started to run away.
The dragon was very sad. He wanted to help the mouse, so he decided to try and make friends. He flew around and around until he found the mouse. He said hello to the mouse and asked if he wanted to be friends.
The mouse was so happy! He said yes, and they played together all day long. From then on, the dragon and the mouse were the best of friends. They had lots of fun together and the dragon was never lonely again.
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration ⚙️</h2><a id="user-content-configuration-️" aria-label="Permalink: Configuration ⚙️" href="#configuration-️"></a></p>
<p dir="auto">Key parameters (modify in <code>config.py</code>):</p>
<p dir="auto"><strong>Model Architecture</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="GPTConfig(
    block_size=512,    # Context length
    n_layer=8,         # Number of transformer layers
    n_head=8,          # Number of attention heads
    n_embed=512,       # Embedding dimension
    dropout=0.2,       # Dropout rate
    bias=False         # Use bias in layers
)"><pre><span>GPTConfig</span>(
    <span>block_size</span><span>=</span><span>512</span>,    <span># Context length</span>
    <span>n_layer</span><span>=</span><span>8</span>,         <span># Number of transformer layers</span>
    <span>n_head</span><span>=</span><span>8</span>,          <span># Number of attention heads</span>
    <span>n_embed</span><span>=</span><span>512</span>,       <span># Embedding dimension</span>
    <span>dropout</span><span>=</span><span>0.2</span>,       <span># Dropout rate</span>
    <span>bias</span><span>=</span><span>False</span>         <span># Use bias in layers</span>
)</pre></div>
<p dir="auto"><strong>Training</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="TrainingConfig(
    batch_size=64,
    max_iters=30000,
    learning_rate=6e-4,
    weight_decay=0.1,
    grad_clip=1.0,
    warmup_iters=1000
)"><pre><span>TrainingConfig</span>(
    <span>batch_size</span><span>=</span><span>64</span>,
    <span>max_iters</span><span>=</span><span>30000</span>,
    <span>learning_rate</span><span>=</span><span>6e-4</span>,
    <span>weight_decay</span><span>=</span><span>0.1</span>,
    <span>grad_clip</span><span>=</span><span>1.0</span>,
    <span>warmup_iters</span><span>=</span><span>1000</span>
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">File Structure 📁</h2><a id="user-content-file-structure-" aria-label="Permalink: File Structure 📁" href="#file-structure-"></a></p>
<div data-snippet-clipboard-copy-content="om-alve-smolgpt/
├── config.py       - Model &amp; training configuration
├── dataset.py      - Data loading &amp; preprocessing
├── model.py        - GPT model implementation
├── preprocess.py   - Dataset preparation scripts
├── sample.py       - Text generation script
├── tokenizer.py    - Tokenizer wrapper
└── train.py        - Main training loop"><pre><code>om-alve-smolgpt/
├── config.py       - Model &amp; training configuration
├── dataset.py      - Data loading &amp; preprocessing
├── model.py        - GPT model implementation
├── preprocess.py   - Dataset preparation scripts
├── sample.py       - Text generation script
├── tokenizer.py    - Tokenizer wrapper
└── train.py        - Main training loop
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing 🤝</h2><a id="user-content-contributing-" aria-label="Permalink: Contributing 🤝" href="#contributing-"></a></p>
<p dir="auto">Contributions welcome! Please open an issue or PR for:</p>
<ul dir="auto">
<li>Bug fixes</li>
<li>Performance improvements</li>
<li>New features</li>
</ul>
<hr>
<p dir="auto"><strong>Note</strong>: This implementation is inspired by modern LLM training practices and adapted for educational purposes. For production use, consider scaling up model size and dataset.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Adding iodine to salt played a role in cognitive improvements: research (2013) (207 pts)]]></title>
            <link>https://www.discovermagazine.com/health/how-adding-iodine-to-salt-boosted-americans-iq</link>
            <guid>42868718</guid>
            <pubDate>Wed, 29 Jan 2025 18:06:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.discovermagazine.com/health/how-adding-iodine-to-salt-boosted-americans-iq">https://www.discovermagazine.com/health/how-adding-iodine-to-salt-boosted-americans-iq</a>, See on <a href="https://news.ycombinator.com/item?id=42868718">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p><span>Iodized salt is so commonplace in the U.S. today that you may never have given the additive a second thought. But new research finds that humble iodine has played a substantial role in cognitive improvements seen across the American population in the 20th century.</span></p><p><span>Iodine is a critical micronutrient in the human diet — that is, something our bodies can’t synthesize that we have to rely on food to obtain — and it’s been added to salt (in the form of potassium iodide) since 1924. Originally, iodization was adopted to reduce the incidence of goiter, an enlargement of the thyroid gland. But research since then has found that iodine also plays a crucial role in brain development, especially during gestation.</span></p><p><span>Iodine deficiency&nbsp;today is the leading cause of preventable mental retardation in the world. It’s estimated that nearly one-third of the world’s population has a diet with too little iodine in it, and the problem isn’t limited to developing countries — perhaps&nbsp;</span><a href="http://www.who.int/nutrition/publications/VMNIS_Iodine_deficiency_in_Europe.pdf" color="accent" target="_blank" rel="noopener"><span>one-fifth of those cases are in Europe</span></a><span>&nbsp;(pdf), where iodized salt is still not the norm.</span></p><p id="iodines-natural-experiment"><h2><span>Iodine’s Natural Experiment</span></h2></p><p><span>With this background, then, a group of economists saw a natural experiment: comparing the intelligence of children born just before 1924 — the year iodization began — and those born just after. James Freyer, David Weil and Dimitra Politi used military data from the early 1900s&nbsp;1920s, when World War II drove millions of men and women to enlist.</span></p><p><span>Recruits all took a standardized intelligence test as part of their enlistment. Researchers didn’t have access to the test scores themselves, but they had a clever substitute: smarter recruits were assigned to the Air Forces while the less bright ones went to the Ground Forces. This allowed the researchers to infer test scores depending on which branch a recruit was selected for.</span></p><p><span>Intelligence data were paired with birthdate and hometown, since iodine levels in the soil and water vary significantly from place to place. To estimate which regions were naturally high-iodine and which were low, the researchers referred to nationwide statistics&nbsp;collected after World War I on the prevalence of goiter.</span></p><p><span>In all, researchers had sufficient data on about 2 million male recruits born between 1921 and 1927.</span></p><p id="stark-improvements"><h2><span>Stark Improvements</span></h2></p><p><span>The economists found that in the lowest-iodine areas — the bottom quarter of the study population — the introduction of iodized salt had stark effects. Men from these regions born in 1924 or later were significantly more likely to get into the Air Force and had an average IQ that was 15 points higher than their predecessors.</span></p><p><span>Nationwide, that averages out to a 3.5-point rise in IQ because of iodization, the researchers </span><a href="http://www.nber.org/papers/w19233" color="accent" target="_blank" rel="noopener"><span>report</span></a><span> in a paper for the National Bureau of Economic Research.</span></p><p><span>The initiative wasn’t without its drawbacks — sudden iodine supplementation among people who are deficient can cause thyroid-related deaths. The researchers estimate that 10,000 deaths in the decades after 1924 were caused by salt iodization.</span></p><p><span>But on the positive side, iodine deficiency and its symptoms were vanquished almost overnight. And iodine’s mental benefits may even help explain the Flynn Effect, which observes that IQ rose about 3 points per decade in developed countries throughout the 20th century. It’s been thought that improved health and nutrition were the driving forces of the Flynn Effect. Now, it appears that iodine alone was responsible for roughly one decade of that remarkable climb. All the more reason, then, for the rest of the world to follow suit and relegate iodine deficiency to the history books.</span></p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No Man's Sky's update introduces billions of new stars, planets, and more (129 pts)]]></title>
            <link>https://blog.playstation.com/2025/01/29/no-mans-skys-latest-update-introduces-billions-of-new-stars-planets-and-more-today/</link>
            <guid>42868618</guid>
            <pubDate>Wed, 29 Jan 2025 17:59:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.playstation.com/2025/01/29/no-mans-skys-latest-update-introduces-billions-of-new-stars-planets-and-more-today/">https://blog.playstation.com/2025/01/29/no-mans-skys-latest-update-introduces-billions-of-new-stars-planets-and-more-today/</a>, See on <a href="https://news.ycombinator.com/item?id=42868618">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-400697">

				

				<div>

						<p><img fetchpriority="high" width="1088" height="612" src="https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart" alt="No Man’s Sky’s latest update introduces billions of new stars, planets, and more today" sizes="(min-width: 1170px) 936px, (min-width: 960px) 80vw, 100vw" decoding="async" srcset="https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=1 1088w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.5 544w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.38 413w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.31 337w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.25 272w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.21 228w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.16 174w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.13 141w"></p>

						
						
						<div>
							


<p>Hello! Today we are releasing one of our biggest updates for <a href="https://www.playstation.com/games/no-mans-sky/">No Man’s Sky</a>. We can’t wait for you to see what we’ve been working on.</p>



<p>Last year, we released No Man’s Sky Worlds Part I (5.0) for PS4, PS5, and PS VR2 players, and it was one of our most successful to date. We’ve been planning the changes in Worlds Part II for a long time, and the wait is finally over.&nbsp;</p>


	<div data-youtube-id="BGfnL4s38sc" data-title="">
	<p>
		<img src="https://img.youtube.com/vi/BGfnL4s38sc/maxresdefault.jpg" data-srcset="https://img.youtube.com/vi/BGfnL4s38sc/default.jpg 120w, https://img.youtube.com/vi/BGfnL4s38sc/mqdefault.jpg 320w, https://img.youtube.com/vi/BGfnL4s38sc/hqdefault.jpg 480w, https://img.youtube.com/vi/BGfnL4s38sc/sddefault.jpg 640w, https://img.youtube.com/vi/BGfnL4s38sc/maxresdefault.jpg 1280w" data-sizes="(min-width: 1170px) 936px, (min-width: 960px) 80vw, 100vw" data-src="https://img.youtube.com/vi/BGfnL4s38sc/maxresdefault.jpg" alt="No Man’s Sky’s latest update introduces billions of new stars, planets, and more today" srcset="https://img.youtube.com/vi/BGfnL4s38sc/default.jpg 120w, https://img.youtube.com/vi/BGfnL4s38sc/mqdefault.jpg 320w, https://img.youtube.com/vi/BGfnL4s38sc/hqdefault.jpg 480w, https://img.youtube.com/vi/BGfnL4s38sc/sddefault.jpg 640w, https://img.youtube.com/vi/BGfnL4s38sc/maxresdefault.jpg 1280w"></p>
</div>
	



<p>One of the biggest reasons people play No Man’s Sky is for that sense of adventure and discovery, that feeling of flying to a planet, and landing to explore, knowing no one has ever been there before. With Worlds Part II, we added billions of new star systems and trillions of new planets to the universe. This allows us to push the boundaries of our engine and technology without changing the things people love about the game already. If you settled on your home planet with a beautiful base that you lovingly crafted, that is safe – but now there are new worlds to explore with a level of variety no one has seen before.</p>



<p><a href="https://live.staticflickr.com/65535/54291870269_4e4f33a4a1_h.jpg" target="_blank" rel="noopener"><img decoding="async" src="https://live.staticflickr.com/65535/54291870269_4e4f33a4a1_h.jpg" alt=""></a></p><p>Going to these new systems travelers will notice new terrain, new biomes, new flora, and new fauna. There’s a new terrain system that I’ve been working on for a while now. There are huge mountains to climb, oceans that are kilometers deep to discover, and caverns and canyons at a scale that wasn’t possible before.</p>



<p>In these systems you may even find enormous Gas Giants. These huge worlds can be ten times bigger than any other planet you have explored previously. Gas Giants and deep oceans both require high-level technology to explore. Gas storms, deep sea pressure, and anomalies bring new hazards and challenges to No Man’s Sky.</p>



<p><a href="https://live.staticflickr.com/65535/54291632791_a3101e4947_h.jpg" target="_blank" rel="noopener"><img decoding="async" src="https://live.staticflickr.com/65535/54291632791_a3101e4947_h.jpg" alt=""></a></p><p>Water gameplay has much added depth too, including improvements to our submarine, The Nautilus, as well as improvements to fishing and deep sea diving systems. New ocean tech allows water to react physically to the world around it. Dimples appear when it rains and wake appears as ships fly over the surface, and creatures and players can wade through creating waves.&nbsp;</p>



<p>Our lighting system has been completely rewritten. Shadows show more details, sunlight and ambient occlusion are sharper, and sunlight sparkling through leaves and metals looks crisp and beautiful. Starry night skies and wispy clouds reflect in the water of lakes and oceans.</p>



<p><a href="https://live.staticflickr.com/65535/54292060960_3f430425a6_h.jpg" target="_blank" rel="noopener"><img decoding="async" src="https://live.staticflickr.com/65535/54292060960_3f430425a6_h.jpg" alt=""></a></p><p>A lot of this new technology comes from learnings and hard work on our next big fantasy game, Light No Fire, which is keeping our small team incredibly busy.</p>



<p>As well as new solar systems and new technology Worlds Part II brings a lot of new adventures. This update introduces a large strand of new quests and lore connecting up some of the storylines and mysteries we have been building to for a long time. Those who relish unearthing the collective knowledge of the No Man’s Sky universe have a lot to dig into.</p>



<p><a href="https://live.staticflickr.com/65535/54291632746_6d99f45fd4_h.jpg" target="_blank" rel="noopener"><img decoding="async" src="https://live.staticflickr.com/65535/54291632746_6d99f45fd4_h.jpg" alt=""></a></p><p>There is also a whole new expedition to accompany this major update which deliberately serves as a guided tour to some of the best new additions to Worlds Part II. The rewards for completing this awe-inspiring journey are really special too. Not least is a brand new spacecraft which is a cross between a living ship and a jet fighter. It’s pretty wild!&nbsp;</p>



<p>The Worlds Part II update is a signal to the PlayStation community that 2025 is going to be a very exciting year for No Man’s Sky. Whether you play on PS5, PS4, or PS VR2, there’s a lot to look forward to from this tiny but still energised team. We’re so thrilled to be able to keep working on this game we all love so much.</p>



<p>Our journey continues.</p>
						</div>
							
							
					</div>
			</article><section>
	<h2>Trending Stories</h2>

	
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Astral – "We're building a new static type checker for Python" (267 pts)]]></title>
            <link>https://twitter.com/charliermarsh/status/1884651482009477368</link>
            <guid>42868576</guid>
            <pubDate>Wed, 29 Jan 2025 17:56:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/charliermarsh/status/1884651482009477368">https://twitter.com/charliermarsh/status/1884651482009477368</a>, See on <a href="https://news.ycombinator.com/item?id=42868576">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Intel doesn't know how to be a foundry," Tim Cook reportedly told TSMC's CEO (153 pts)]]></title>
            <link>https://www.tomshardware.com/tech-industry/tsmc-founder-says-tim-cook-told-him-intel-did-not-know-how-to-be-a-foundry</link>
            <guid>42868531</guid>
            <pubDate>Wed, 29 Jan 2025 17:53:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/tech-industry/tsmc-founder-says-tim-cook-told-him-intel-did-not-know-how-to-be-a-foundry">https://www.tomshardware.com/tech-industry/tsmc-founder-says-tim-cook-told-him-intel-did-not-know-how-to-be-a-foundry</a>, See on <a href="https://news.ycombinator.com/item?id=42868531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg" alt="Morris Chang" srcset="https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY.jpg" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<figcaption>
<span>(Image credit: Walid Berrazeg/SOPA Images/LightRocket via Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<p>When Apple began to build its own processors for iPhones and iPads in 2009 – 2010, it initially used Samsung Foundry, but after custom silicon became a key advantage of iPhones over rivals in the early 2010s, the company began to explore other makers as Samsung was Apple's primary rival at the time. The company considered using Intel Custom Foundry (ICF) and Texas Instruments but quickly realized the ICF was not tailored for external customers at all, while TI did not have advanced process technologies. As a result, it chose TSMC as its exclusive supplier, according to Morris Chang, the founder of TSMC, who spoke to <a data-analytics-id="inline-link" href="https://www.youtube.com/watch?v=FZItbr4ZJnc" data-url="https://www.youtube.com/watch?v=FZItbr4ZJnc" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Acquired</a>.&nbsp;</p><p>"The [CEO] of Intel has approached Tim Cook and has asked Tim Cook to consider Intel, and at this time, Intel was the major supplier for Apple's Mac line," Chang reminisced. "I knew a lot of Intel's customer customers in Taiwan […] none of them liked Intel [as it] always acted like they were the the only guy [with] microprocessors. […] The Foundry business where TSMC […] does not compete with customers and even if Intel is trying to do business in good faith they do have the conflict [of interests."&nbsp;</p><p>When Intel's CEO Paul Otellini approached Tim Cook in early 2011, offering to manufacture Apple's chips, Apple paused discussions with TSMC for two months to evaluate the proposal.</p><p>Morris Chang, concerned about this pause, traveled to Apple's headquarters to check on the situation. In a private meeting, Tim Cook reassured Chang that Apple would not choose Intel.&nbsp;</p><p>"Intel just does not know how to be a foundry," Tom Cook reportedly told Chang.&nbsp;</p><p>The implication was that Intel lacked the customer-centric mindset required for a foundry business. Unlike TSMC, which tailors its process technologies to meet customer needs, Intel was used to designing and producing its own chips and struggled to adapt to servicing external clients. By contrast, Apple valued TSMC's ability to listen and respond to specific demands, something Intel historically did not do.&nbsp;</p><p>"When the customer asks a lot of things, we have learned to respond to every request," Chang said. "Some of them were crazy, some of them were irrational, [but] we respond to each request courteously. […] Intel has never done that, I knew a lot of customers of Intel's here in Taiwan and all [of them] wished that there were another supplier."&nbsp;</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-6S8ZwAVeqjYYXmY7mXf9rB"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><p>However, it is notable that Intel has worked to defray those concerns with its now-revamped Intel Foundry, which also now offers support for industry-standard design tools, a notable area it lacked with its first Intel Custom Foundry foray in the past.</p><p>Indeed, the very first encounter with Apple disrupted TSMC's roadmap. TSMC planned to move from 28nm planar to 16nm FinFET, but Apple wanted a custom 20nm-class planar node instead. At the time, TSMC did not have enough R&amp;D teams to develop two process technologies at once, so the company had to divert people working on CLN16FF to CLN20SOC to meet Apple's needs in 2014.&nbsp;</p><p>Although Apple dual-sourced its A8 and A9 processors at 20nm and 16nm-class process technologies from Samsung and TSMC, Apple eventually committed to TSMC for all future processors. The Apple Silicon strategy cemented TSMC's position as the exclusive supplier, as the company's system-on-chips for different applications share quite a lot of IP.</p><p>The decision to meet Apple's demands was critical in TSMC surpassing Intel as the world's most advanced semiconductor manufacturer. Apple's business gave TSMC predictable high-volume orders, helping justify massive CapEx and R&amp;D investments. As a result, TSMC has consistently outpaced Intel by introducing leading-edge nodes.</p>
</div>



<!-- Drop in a standard article here maybe? -->



<div id="slice-container-authorBio-6S8ZwAVeqjYYXmY7mXf9rB"><p>Anton Shilov is a contributing writer at Tom’s Hardware. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends.</p></div>
</section>





<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An analysis of DeepSeek's R1-Zero and R1 (465 pts)]]></title>
            <link>https://arcprize.org/blog/r1-zero-r1-results-analysis</link>
            <guid>42868390</guid>
            <pubDate>Wed, 29 Jan 2025 17:44:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arcprize.org/blog/r1-zero-r1-results-analysis">https://arcprize.org/blog/r1-zero-r1-results-analysis</a>, See on <a href="https://news.ycombinator.com/item?id=42868390">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
            <h2>ARC Prize remains undefeated.<br>New ideas still needed<span>.</span></h2>
        </p><div>
<h2 id="an-analysis-of-deepseeks-r1-zero-and-r1">An Analysis of DeepSeek's R1-Zero and R1</h2>
<h2 id="r1-zero-is-more-important-than-r1">R1-Zero is more important than R1</h2>

<blockquote>
  <p>Special thanks to <a href="https://x.com/tuhinone">Tuhin</a> and <a href="https://www.linkedin.com/in/abuqader/">Abu</a> from <a href="https://www.baseten.co/">Baseten</a> and <a href="https://x.com/yuchenj_uw">Yuchen</a> from <a href="https://hyperbolic.xyz/">Hyperbolic Labs</a> for hosting r1-zero for us. Hardly any providers are hosting this model variant, and its availability is important for research purposes.</p>
</blockquote>

<p>ARC Prize Foundation’s goal is to define, measure, and inspire new ideas towards AGI. To this end, we strive to create the strongest global innovation environment possible.</p>

<p>We do not have AGI yet and are still innovation constrained – scaling up pure LLM pretraining is not the path, despite this being the dominant AI industry narrative and mainstream public view as of last summer.</p>

<p>The reason narratives are important is they end up driving economic activity, like investment, research focus, funding, geopolitics, trade, etc. For example, in 2023-24 there was ~$20B invested into new LLM startups compared to only ~$200M into new AGI startups.</p>

<p>We <a href="https://arcprize.org/blog/launch">launched ARC Prize 2024 last June</a> to grow awareness of limits of scaling LLMs and promote a useful benchmark, ARC-AGI-1, towards a new direction that requires AI systems to adapt to novel, unseen problems instead of being able to rely strictly on memorization.</p>

<figure>
  <img src="https://arcprize.org/media/images/blog/r1-arch.jpg" alt="R1 Training Architecture">
  <figcaption>DeepSeek R1 architecture by <a href="https://x.com/SirrahChan/status/1881488738473357753" target="_blank">@SirrahChan</a>.</figcaption>
</figure>

<p>Last week, DeepSeek <a href="https://arxiv.org/abs/2501.12948">published</a> their new R1-Zero and R1 “reasoner” systems that is <a href="https://x.com/arcprize/status/1881761987090325517">competitive with OpenAI’s o1 system</a> on ARC-AGI-1. R1-Zero, R1, and o1 (low compute) all score around 15-20% – in contrast to <code>GPT-4o</code>’s 5%, the pinnacle of years of pure LLM scaling. Based on this week’s <a href="https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/">US market reaction</a>, the public is starting to understand the limits of scaling pure LLMs too. However, there is still broad public ignorance about impending inference demand.</p>

<p>In December 2024, OpenAI announced a <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">new breakthrough o3 system that we verified</a>. It scored 76% in a low compute mode and 88% in a high compute mode. The o3 system demonstrates the first practical, general implementation of a computer adapting to novel unseen problems.</p>

<p>Despite being <a href="https://www.techmeme.com/241220/h2200">huge tech news</a>, o3 beating ARC-AGI-1 <a href="https://x.com/benspringwater/status/1881507009184530449">went largely unnoticed and unreported</a> by mainstream press.</p>

<p>This is an incredibly important moment for the field of AI and for computer science and these systems demand study. But due to the closed nature of o1/o3, we’re forced to rely on speculation. Thanks to ARC-AGI-1 and now (nearly) open source R1-Zero and R1, we can add to our understanding. In particular, R1-Zero is significantly more important than R1.</p>

<blockquote>
  <p>“Nearly” because DeepSeek did not publish a reproducible way to generate their model weights from scratch</p>
</blockquote>

<h2 id="r1-zero-removes-the-human-bottleneck">R1-Zero removes the human bottleneck</h2>

<p>In our <a href="https://arcprize.org/blog/openai-o1-results-arc-prize">o1</a> and <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">o3 analysis</a>, we speculated how these reasoning systems work. The key ideas:</p>

<ol>
  <li>Generate chains-of-thought (CoT) for a problem domain.</li>
  <li>Label the intermediary CoT steps using a combination of human experts (“supervised fine tuning” or SFT) and automated machines (“reinforcement learning” or RL).</li>
  <li>Train base model using (2).</li>
  <li>At test time, iteratively inference from the process model.</li>
</ol>

<p>Techniques used to iterative sample, along with ARC-AGI-1 scores, are reviewed below:</p>

<div>
  <table>
    <thead>
      <tr>
        <th>System</th>
        <th>ARC-AGI-1</th>
        <th>Method</th>
        <th>Avg Tokens</th>
        <th>Avg Cost</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>r1-zero</td>
        <td>14%</td>
        <td>No SFT / no search</td>
        <td>11K</td>
        <td>$.11</td>
      </tr>
      <tr>
        <td>r1</td>
        <td>15.8%</td>
        <td>SFT / no search</td>
        <td>6K</td>
        <td>$.06</td>
      </tr>
      <tr>
        <td>o1 (low)</td>
        <td>20.5%</td>
        <td>SFT / no search</td>
        <td>7K</td>
        <td>$.43</td>
      </tr>
      <tr>
        <td>o1 (med)</td>
        <td>31%</td>
        <td>SFT / no search</td>
        <td>13K</td>
        <td>$.79</td>
      </tr>
      <tr>
        <td>o1 (high)</td>
        <td>35%</td>
        <td>SFT / no search</td>
        <td>22K</td>
        <td>$1.31</td>
      </tr>
      <tr>
        <td>o3 (low)</td>
        <td>75.7%</td>
        <td>SFT / search + sampling</td>
        <td>335K</td>
        <td>$20</td>
      </tr>
      <tr>
        <td>o3 (high)</td>
        <td>87.5%</td>
        <td>SFT / search + sampling</td>
        <td>57M</td>
        <td>$3.4K</td>
      </tr>
    </tbody>
  </table>
</div>

<p><em>Note: ARC-AGI-1 semi-private score shown.</em></p>

<p>With DeepSeek’s new published research, we can better inform our speculation. The key insight is that higher degrees of novelty adaptation (and reliability) for LLM reasoning systems are achieved along three dimensions:</p>

<ol>
  <li>Adding human labels aka SFT to CoT process model training</li>
  <li>CoT search instead of linear inference (parallel per-step CoT inference)</li>
  <li>Whole CoT sampling (parallel trajectory inference)</li>
</ol>

<p>Item (1) is bottlenecked by human data generation and constrains which domains these reasoning systems benefit most. For example, the <a href="https://openai.com/index/learning-to-reason-with-llms/">MMLU professional law category</a> is surprisingly much lower than the math and logic on o1.</p>

<p>Items (2) and (3) are bottlenecked by efficiency. o1 and o3 both <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">show logarithmic improvement</a> in benchmark accuracy on ARC-AGI-1 as they spend more inference compute at test time, while the different ways to spend that compute adjust the x-axis of the curve.</p>

<p>In my opinion, the most interesting thing DeepSeek has done is to publish R1-Zero separately. R1-Zero is a model which does not use SFT, the (1) item. Instead it relies purely on reinforcement learning.</p>

<p>R1-Zero and R1 show strong score agreement on  ARC-AGI-1, scoring 14% and 15% respectively. DeepSeeks’s own reported benchmark scores also show strong agreement between R1-Zero and R1, eg. on MATH AIME 2024 scores are 71% and 76% respectively (up from ~40% on the base DeepSeek V3).</p>

<p>In the paper, R1-Zero authors say “DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing” and <a href="https://www.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/">has been corroborated online</a>. However in our testing, we found little to no evidence of incoherence when testing R1-Zero on ARC-AGI-1 which is similar to the math and coding domains the system was RL’d on.</p>

<p>Taken together, these findings suggest:</p>

<ol>
  <li>SFT (eg. human expert labeling) is not necessary for accurate and legible CoT reasoning in domains with strong verification.</li>
  <li>The R1-Zero training process is capable of creating its own internal domain specific language (“DSL”) in token space via RL optimization.</li>
  <li>SFT is necessary for increasing CoT reasoning domain generality.</li>
</ol>

<p>This makes intuitive sense, as language itself is effectively a reasoning DSL. The exact same “words” can be learned in one domain and applied in another, like a program. The pure RL approach can not yet discover a broad shared vocabulary and I expect this will be a strong focus for future research.</p>

<p>Ultimately, R1-Zero demonstrates the prototype of a potential scaling regime with zero human bottlenecks – even in the training data acquisition itself.</p>

<p>Almost certainly DeepSeek has set its sights on OpenAI’s o3 system. It is important to watch whether SFT ends up being a requirement to add CoT search and sampling, or whether a hypothetical “R2-Zero” could exist along the same logarithmic accuracy vs inference scaling curve. Based on R1-Zero results, I believe SFT will not be required to beat ARC-AGI-1 in this hypothetical scaled up version.</p>

<h2 id="dollars-for-reliability">Dollars for reliability</h2>

<p>There are two major shifts happening in AI, economically speaking:</p>

<ol>
  <li>You can now spend more $ to get higher accuracy and reliability</li>
  <li>Training $ is moving to inference $</li>
</ol>

<p>Both are going to drive a massive amount of demand for inference and neither will curtail the demand for more compute. In fact, they will increase the demand for compute.</p>

<p>AI reasoning systems promise much greater returns than simply higher accuracy on benchmarks. The number one issue preventing more AI automation use (e.g. inference demand) is reliability. I’ve spoken with hundreds of Zapier’s customers trying to deploy AI agents in their businesses and the feedback is strongly consistent: “I don’t trust them yet because they don’t work reliably”.</p>

<p><a href="https://www.cognitiverevolution.ai/the-arc-prize-efficiency-intuition-and-agi-with-mike-knoop-co-founder-of-zapier/">Previously</a> I’ve argued that progress towards ARC-AGI would result in higher reliability. The challenge with LLM agents is they need strong local domain steering to work reliability. Stronger generalization capability requires the ability to adapt to unseen situations. We’re now starting to <a href="https://x.com/woj_zaremba/status/1882290021778313272">see evidence</a> this view is correct. And so it’s no surprise several companies are now introducing agents (Anthropic, OpenAI, Apple, …)</p>

<p>Agents will drive significant near-term demand inference due the reliability needs. More broadly, developers can choose to spend more compute to increase user trust in the system. More reliability does not mean 100% accuracy though – but you’d expect to be more <a href="https://commons.wikimedia.org/wiki/File:Statistical_bias_and_statistical_noise_illustration.png">consistently inaccurate</a>. This is okay because users and developers can now more confidently steer behavior via prompting when accuracy is low.</p>

<p>Problems that were impossible for computers previously now have dollar amounts attached to them. And as efficiency climbs, those dollar amounts will go down.</p>

<h2 id="inference-as-training">Inference as training</h2>

<p>The other major shift occurring is in the provenance of data going into LLM systems for pretraining. Previously, most data was either purchased, scraped, or synthetically generated from an existing LLM (eg. distilling or augmenting).</p>

<p>These reasoning systems offer a new option which is to generate “real” data as opposed to “synthetic”. The AI industry uses the term synthetic to identify low quality data that is typically recycled through an LLM to boost the overall amount of training data – with diminishing returns.</p>

<p>But now with reasoning systems and verifiers, we can create brand new legitimate data to train on. This can either be done offline where the developer pays to create the data or at inference time where the end user pays!</p>

<p>This is a fascinating shift in economics and suggests there could be a runaway power concentrating moment for AI system developers who have the largest number of paying customers. Those customers are footing the bill to create new high quality data … which improves the model … which becomes better and more preferred by users … you get the idea.</p>

<p>If we can break through the human expert CoT barrier and create an extremely efficient system to create new data via search/synthesis and verification, then we should expect a massive influx of compute to go into these inference systems as they quite literally get better just by inputting dollars and raw data. Eventually this type of AI training will eclipse pretraining on human generated data altogether.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We will continue to see market corrections as increased inference demand becomes clear. AI system efficiency is only going to drive more usage, not just due to <a href="https://en.wikipedia.org/wiki/Jevons_paradox">Jevons Paradox</a> but because new regimes of training are unlocked as efficiency increases.</p>

<p>With R1 being open and reproducible, more people and teams will be pushing CoT and search to the limits. This will more quickly tell us where the frontier actually lies and will fuel a wave of innovation that increases the chance of reaching AGI quickly.</p>

<p>Several people have already told me they plan to use R1-style systems for <a href="https://arcprize.org/blog/arc-prize-2025">ARC Prize 2025</a> and I’m excited to see the results.</p>

<p>The fact that R1 is open is a great thing for the world. DeepSeek has pushed the frontier of science forward.</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A major Postgres upgrade with zero downtime (144 pts)]]></title>
            <link>https://www.instantdb.com/essays/pg_upgrade</link>
            <guid>42867657</guid>
            <pubDate>Wed, 29 Jan 2025 16:57:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.instantdb.com/essays/pg_upgrade">https://www.instantdb.com/essays/pg_upgrade</a>, See on <a href="https://news.ycombinator.com/item?id=42867657">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
We’re Instant, a modern Firebase. <a href="https://www.instantdb.com/tutorial">Try out the demo</a>, you can spin up a database and make queries within a minute — no login required.
</p>

<p>Right before Christmas we discovered that our Aurora Postgres instance needed a major version upgrade. We found a great essay by the <a href="https://eng.lyft.com/postgres-aurora-db-major-version-upgrade-with-minimal-downtime-4e26178f07a0">Lyft team</a>, showing how they ran their upgrade with about 7 minutes of downtime.</p>
<p>We started with Lyft’s checklist but made some changes, particularly with how we switched masters. <strong>In our process we got to 0 seconds of downtime.</strong></p>
<p>Doing a major version upgrade is stressful, and reading other’s reports definitely helped us along the way. So we wanted to write an experience report of our own, in the hopes that it’s as useful to you as reading others were for us.</p>
<p>In this write-up we’ll share the path we took — from false starts, to gotchas, to the steps that ultimately worked. Fair warning, our system runs at a modest scale. We have less than a terabyte of data, we read about 1.8 million tuples per second, and write about 500 tuples per second as of this writing. If you run at a much higher scale, this may be less relevant to you.</p>
<p>With all that said, let’s get into the story!</p>
<h2 id="state-of-affairs">State of Affairs</h2>
<p>Let’s start with a brief outline of our system:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/the_system.png" alt=""></p>
<p>Browsers connect to sync servers. Sync servers keep track of active queries. Sync servers also listen to Postgres’ write-ahead log; they take transactions, find affected queries, and send novelty back to browsers. <sup id="marked-fnref-1"><a href="#marked-fn-1">[1]</a></sup> Crucially, all Instant databases are hosted under one Aurora Postgres instance. <sup id="marked-fnref-2"><a href="#marked-fn-2">[2]</a></sup></p>
<h2 id="trouble-erupts">Trouble Erupts</h2>
<p>After our open source launch in August <sup id="marked-fnref-3"><a href="#marked-fn-3">[3]</a></sup>, we experienced about a 100x increase in throughput. For the first 2 months, whenever we saw perf issues they usually lived in our Client SDK or the Sync Server. When we hit a new high in December though, our Aurora Postgres instance started to spike in CPU and stumble.</p>
<p>To give us breathing room, we kept upgrading the size of the machine, until we reached db.r6g.16xlarge. <sup id="marked-fnref-4"><a href="#marked-fn-4">[4]</a></sup> We had to do something about the queries we were writing.</p>
<h2 id="sometimes-new-is-better-than-old">Sometimes, new is better than old</h2>
<p>We started to reproduce slow queries locally and began to optimize them. Within the first hour we noticed something strange: one teammate constantly reported faster query results then the rest of us.</p>
<p>Turns out this teammate was running Postgres 16, while most of us (and our production instance) were running Postgres 13.</p>
<p>We did some more backtesting and realized that Postgres 16 improved many of the egregious queries by 30% or more. Not bad. There came our first learning: sometimes, just upgrading Postgres is a great way to improve perf. <sup id="marked-fnref-5"><a href="#marked-fn-5">[5]</a></sup></p>
<p>So we thought, let’s upgrade to Postgres 16. Now how do we go about it?</p>
<h2 id="false-starts">False Starts</h2>
<p>We were a team of 4 and we were in a crunch. If we could find a quick option we’d have been happy to take it. Here’s what we tried:</p>

<h2 id="1-in-place-upgradesbut-they-take-15-minutes">1) In-Place Upgrades...but they take 15 minutes</h2>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/in_place.png" alt=""></p>
<p>The easiest choice would have been to run an in-place upgrade. Put the database in maintenance mode, upgrade major versions, then turn it back on again. In RDS console you can do this with a <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_UpgradeDBInstance.PostgreSQL.MajorVersion.html#USER_UpgradeDBInstance.Upgrading.Manual:~:text=the%20RDS%20API.-,Console,-To%20upgrade%20the">few button clicks</a>.</p>
<p>The big problem is the downtime. Your DB is in maintenance mode for the entirety of the upgrade. The Lyft team said an in-place upgrade would have caused them a <a href="https://eng.lyft.com/postgres-aurora-db-major-version-upgrade-with-minimal-downtime-4e26178f07a0#4831">30 minute</a> outage.</p>
<p>We wanted to test this for ourselves though, in a case smaller database upgraded more quickly. So we cloned our production database and tested an in-place upgrade. Even with our smaller size, it took about 15 minutes for the clone to come back online.</p>
<p>Crunch or not, a 15-minute outage was off the table for us. Since launch we had folks sign up across the U.S, Europe and Asia; traffic ebbed and flowed, but there wasn’t a period where 15 minutes of downtime felt tolerable.</p>

<h2 id="2-blue-green-deploymentsbut-you-cant-have-active-replication-slots">2) Blue-Green Deployments...but you can’t have active replication slots</h2>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/blue_green.png" alt=""></p>
<p>Well, Aurora Postgres also has <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/blue-green-deployments-overview.html">blue-green deployments</a>. AWS spins up an upgraded replica for you, and you can switch masters with a button click. They promise about a minute of downtime.</p>
<p>With such little operational effort, a minute of downtime sounded like a great option for us.</p>
<p>So we cloned our DB and tested a blue-green deployment. Yup, the connection came back in a minute! It looked like we were done. Until we tried a full rehearsal.</p>
<p>We spun up a complete staging environment, this time with active sync servers and connected clients. Now the blue-green deployment would go on for 30 minutes, and then break with a configuration error:</p>
<blockquote>
<p>Creation of blue/green deployment failed due to incompatible parameter settings. See <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/blue-green-deployments-creating.html#blue-green-deployments-creating-preparing-postgres">link</a> to help resolve the issues, then delete and recreate the blue/green deployment.</p>
</blockquote>
<p>The next few hours was frustrating: we would change a setting, start again, wait 30 minutes, and invariably end up with the same error.</p>
<p>Once we exhausted the suggestions from in error message, we began a process of elimination: when did the upgrade work, and what change made it fail? Eliminating the sync servers revealed the issue: active replication slots.</p>
<p>Remember how our sync servers listen to Postgres’ write-ahead log? To do this, we opened <a href="https://www.postgresql.org/docs/current/logicaldecoding-explanation.html#LOGICALDECODING-REPLICATION-SLOTS">replication slots</a>. We couldn’t create a blue-green deployment when the master DB had active replication slots. The AWS docs did not mention this. <sup id="marked-fnref-6"><a href="#marked-fn-6">[6]</a></sup></p>
<p>At least this experience highlighted a learning: <em>always</em> run a rehearsal that’s as close to production as possible, you never know what you’ll find.</p>
<p>In order to stop using replication slots we’d have to disconnect our sync servers. But then we would lose reactivity, potentially for 30 minutes. Apps would appear broken if we queries were out of sync that long; blue-green deployments were off the table too.</p>
<h2 id="a-plan-for-going-manual">A Plan for Going Manual</h2>
<p>When the managed options don’t work, it’s time to go manual. We knew that a manual upgrade would have to involve three steps:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/going_manual.png" alt=""></p>
<p>First, we would stand up a new replica running Postgres 16 — Let’s call this machine "16". Once 16 was running, we could get our sync servers to subscribe to 16. The remaining step would be to switch writes "all in one go" (what this meant TBD) to 16. When that was done, migration done.</p>
<p>Now to figure out the steps</p>
<h2 id="1-replicate-to-16">1) Replicate to 16</h2>
<p>The first problem was to create our replica running Postgres 16.</p>
<h2 id="a-clone-upgrade-replicate-led-tolost-data">a) Clone-Upgrade-Replicate led to...lost data</h2>
<p>Lyft had a great <a href="https://eng.lyft.com/postgres-aurora-db-major-version-upgrade-with-minimal-downtime-4e26178f07a0#a7df">series of steps</a> to create a replica, so we tried to follow it. There were three stages:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/clone_upgrade_replicate.png" alt=""></p>
<p>First, we clone our database, then we upgrade our clone, and then we start replication. By the end, our clone would have become a replica running Postgres 16.</p>
<p>Steps 1 (clone) &amp; 2 (upgrade) worked great. The trouble started with step 3 (replicate).</p>
<h3 id="lost-pg-functions">Lost PG functions</h3>
<p>When we turned on replication, we saw this error:</p>
<pre><code><span>:ERROR: function is_jsonb_valid_timestamp(jsonb) does not exist at character 1</span>
</code></pre><p>That’s weird. We <em>did</em> have a custom Postgres function called <code>is_jsonb_valid_timestamp</code>. And the function existed on both machines; if we logged in with PSQL, we could write queries:</p>
<pre><code><span>select</span><span> is_jsonb_valid_timestamp</span><span>(</span><span>'1724344362000'</span><span>::jsonb</span><span>)</span><span>;</span>
</code></pre><pre><code><span> is_jsonb_valid_timestamp</span>
<span>--------------------------</span>
<span> t</span>
</code></pre><p>We thought maybe there was an error with our WAL level, or maybe some input worked in 13, but stopped working in 16.</p>

<h3 id="search-paths">Search paths</h3>
<p>So we went down a rabbit hole investigating and searching in <a href="https://www.postgresql.org/message-id/flat/D2B9F2A20670C84685EF7D183F2949E2373D64%40gigant.nidsa.net#8132cc2fa455dd1f1bb02c63cdd04678">PG’s mailing list.</a> Finally, we discovered the problem was <a href="https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PATH">search paths</a>. <sup id="marked-fnref-7"><a href="#marked-fn-7">[7]</a></sup></p>
<pre><code><span>show</span><span> search_path</span><span>;</span>
</code></pre><pre><code><span>   search_path</span>
<span>-----------------</span>
<span> "$user", public</span>
</code></pre><p>Postgres stores custom functions in a <a href="https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PUBLIC">schema</a>. When you write a function in your query, PG uses a <code>search_path</code> to decide which schema to look into. During replication, Postgres was having trouble finding our function. To get around this issue, we <a href="https://github.com/instantdb/instant/pull/593">wrote a PR</a> to add the <code>public</code> prefix explicitly in all our function definitions:</p>
<pre><code><span>-- Before:</span><span></span>
<span></span><span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> is_jsonb_valid_timestamp</span><span>(</span><span>value</span><span> jsonb</span><span>)</span><span></span>
<span></span><span>-- After:                   👇</span><span></span>
<span></span><span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>public</span><span>.</span><span>is_jsonb_valid_timestamp</span><span>(</span><span>value</span><span> jsonb</span><span>)</span>
</code></pre><p>Note to us: make sure to use <code>public</code> in all our function definitions. <sup id="marked-fnref-8"><a href="#marked-fn-8">[8]</a></sup></p>
<p>With PG functions working, 3) replicate ran smoothly! Or so we thought.</p>

<h3 id="missing-data">Missing data</h3>
<p>For all intents and purposes, our new clone looked like a functioning replica. But we wanted to absolutely make sure that we didn’t lose any data.</p>
<p>Thankfully, we had a special <code>transactions</code> table — it’s an immutable table we use internally <sup id="marked-fnref-9"><a href="#marked-fn-9">[9]</a></sup>:</p>
<pre><code><span>instant=&gt; \d transactions;</span>
<!-- -->
<span>   Column   |            Type             | -- ... </span>
<span>------------+-----------------------------+</span>
<span> id         | bigint                      |</span>
<span> app_id     | uuid                        |</span>
<span> created_at | timestamp without time zone |</span>
</code></pre><p>Since we never modify rows, we could also use the <code>transactions</code> table for quick sanity checks — was there any data lost in the table? Here’s the query we ran to do that:</p>
<pre><code><span>-- On 13</span><span></span>
<span></span><span>select</span><span> </span><span>max</span><span>(</span><span>id</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span>;</span><span></span>
<span></span><span>select</span><span> </span><span>count</span><span>(</span><span>*</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span> </span><span>where</span><span> id </span><span>&lt;</span><span> :max</span><span>-</span><span>id</span><span>;</span><span></span>
<!-- -->
<span></span><span>-- Wait for :max-id to replicate ...</span><span></span>
<span></span><span>-- On 16</span><span></span>
<span></span><span>select</span><span> </span><span>COUNT</span><span>(</span><span>*</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span> </span><span>where</span><span> id </span><span>&lt;</span><span> :max</span><span>-</span><span>id</span><span>;</span>
</code></pre><p>To our surprise...we found 13 missing transactions! That definitely stumped us. We weren’t quite sure where the data loss came from <sup id="marked-fnref-10"><a href="#marked-fn-10">[10]</a></sup></p>
<h2 id="b-create-replicateworked-great">b) Create, Replicate...worked great!</h2>
<p>So we went back to the drawing board. One problem with our replica checklist was that it had about 13 steps in it. If we could remove the number of steps, perhaps we could kill whatever caused this data loss.</p>
<p>So we cooked up an alternate approach:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/create_replicate.png" alt=""></p>
<p>Instead of creating, cloning, and then upgrading, we would start with a fresh database running Postgres 16, and replicate from scratch. Lyft chose to clone their DB, because they had over 30TB of data and could leverage <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html#Aurora.Clone.Overview">Aurora Cloning</a>. But we had less than a terabyte of data; starting replication from scratch wasn’t a big a deal for us. <sup id="marked-fnref-11"><a href="#marked-fn-11">[11]</a></sup></p>
<p>So we created a checklist and ended up with 7 steps:</p>

<div>

<h3>Checklist: Create an upgraded Replica</h3>

<div> 

<ol>
<li><p><strong>16: Create a new Postgres Aurora Database on Postgres 16.</strong></p>
<p> Make sure to set <code>wal_level = logical</code></p>
</li>
<li><p><strong>13: Extract the schema</strong></p>
<pre><code><span>pg_dump </span><span>${DATABASE_URL}</span><span> --schema-only -f dump.schema.sql</span>
</code></pre></li>
<li><p><strong>16: Import the schema into 16</strong></p>
<pre><code><span>psql </span><span>${NEW_DATABASE_URL}</span><span> -f dump.schema.sql</span>
</code></pre></li>
<li><p><strong>13: Create a publication</strong></p>
<pre><code><span>create</span><span> publication pub_all_table </span><span>for</span><span> </span><span>all</span><span> </span><span>tables</span><span>;</span>
</code></pre></li>
<li><p><strong>16: Create a subscription with copy_data = true</strong></p>
<pre><code><span>create</span><span> subscription pub_from_scratch </span>
<span>connection </span><span>'host=host_here dbname=name_here port=5432 user=user_here password=password_here'</span><span></span>
<span>publication pub_from_scratch</span>
<span></span><span>with</span><span> </span><span>(</span><span> </span>
<span>  copy_data </span><span>=</span><span> </span><span>true</span><span>,</span><span> create_slot </span><span>=</span><span> </span><span>true</span><span>,</span><span> enabled </span><span>=</span><span> </span><span>true</span><span>,</span><span> </span>
<span>  </span><span>connect</span><span> </span><span>=</span><span> </span><span>true</span><span>,</span><span> </span>
<span>  slot_name </span><span>=</span><span> </span><span>'pub_from_scratch'</span><span></span>
<span></span><span>)</span><span>;</span>
</code></pre></li>
<li><p><strong>Confirm that there’s no data loss</strong></p>
<pre><code><span> </span><span>-- On 13</span><span></span>
<span> </span><span>select</span><span> </span><span>max</span><span>(</span><span>id</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span>;</span><span></span>
<span> </span><span>select</span><span> </span><span>count</span><span>(</span><span>*</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span> </span><span>where</span><span> id </span><span>&lt;</span><span> :max</span><span>-</span><span>id</span><span>;</span><span></span>
<!-- -->
<span> </span><span>-- Wait for :max-id to replicate ...</span><span></span>
<span> </span><span>-- On 16</span><span></span>
<span> </span><span>select</span><span> </span><span>count</span><span>(</span><span>*</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span> </span><span>where</span><span> id </span><span>&lt;</span><span> :max</span><span>-</span><span>id</span><span>;</span>
</code></pre></li>
<li><p><strong>16: Run vaccum analyze</strong></p>
<pre><code><span> vacuum </span><span>(</span><span>verbose</span><span>,</span><span> </span><span>analyze</span><span>,</span><span> </span><span>full</span><span>)</span><span>;</span>
</code></pre></li>
</ol>
</div>

</div>


<p>We ran step 6 with bated breath...and it all turned out well! <sup id="marked-fnref-12"><a href="#marked-fn-12">[12]</a></sup> Now we had a replica running Postgres 16.</p>
<h2 id="2-switching-subscriptions">2) Switching Subscriptions</h2>
<p>Next step, to switch subscriptions. Let’s remind ourselves what we’re looking to do:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/switch_subs.png" alt=""></p>
<p>We’d need to get our sync servers to create replication slots in 16, rather than 13.</p>
<p>To do this, we added a <code>next-database-url</code> variable to our sync servers. During startup, if <code>next-database-url</code> was set, sync servers would subscribe from from there:</p>
<pre><code><span>;; invalidator.clj</span><span></span>
<span></span><span>;; `start` runs when the machine boots up</span><span></span>
<span></span><span>(</span><span>defn</span><span> start</span>
<span>  </span><span>(</span><span>[</span><span>process-id</span><span>]</span><span></span>
<span>    </span><span>; ...</span><span></span>
<span>    </span><span>(</span><span>wal/start-worker</span><span> </span><span>{</span><span>:conn-config</span><span></span>
<span>                      </span><span>(</span><span>or</span><span> </span><span>(</span><span>config/get-next-aurora-config</span><span>)</span><span></span>
<span>                          </span><span>;; Use the next db so that we don't</span><span></span>
<span>                          </span><span>;; have to worry about restarting the</span><span></span>
<span>                          </span><span>;; invalidator when failing over to a</span><span></span>
<span>                          </span><span>;; new db.</span><span></span>
<span>                          </span><span>(</span><span>config/get-aurora-config</span><span>)</span><span>)</span><span>}</span><span>)</span><span></span>
<span>    </span><span>; ...</span><span></span>
<span>    </span><span>)</span><span>)</span>
</code></pre><p>Once we deployed this change, sync servers replicated from 16. Phew, this was at least one step in the story that didn’t feel nerve-wracking!</p>
<h2 id="3-switching-writes">3) Switching Writes</h2>
<p>Now to worry about writes:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/switch_writes.png" alt=""></p>
<p>Ultimately, we needed to click some button and trigger a switch. To make the switch work, we’d need to follow two rules:</p>
<ol>
<li><p><strong>16 must be caught up</strong></p>
<p> If there are <em>any</em> writes in 13 that haven’t replicated to 16 yet, we can’t turn on writes to 16. Otherwise transactions would come in the wrong order</p>
</li>
<li><p><strong>Once caught up, all new writes must go to 16</strong></p>
<p> If <em>any</em> write accidentally goes to 13, we could lose data.</p>
</li>
</ol>
<p>So, how could we follow these rules?</p>
<h2 id="we-could-stop-the-worldbut-thats-downtime">We could stop the world...but that’s downtime</h2>
<p>The simplest way to switch writes would have been to stop the world:</p>
<ol>
<li>Turn off all writes.</li>
<li>Wait for 16 to catch up</li>
<li>Enable writes again — this time they all go to 16</li>
</ol>
<p>If we went with the ‘stop the world approach’, we’d have about the same kind of downtime as blue-green deployments: a minute or so.</p>
<p>We were okay with a minute of downtime. But we had already spent a day setting up our manual method, could we do better?</p>
<p>Since we were switching manually we had finer control over our connections. We realized that with just a little bit more work...we could have no downtime at all!</p>

<h2 id="or-we-could-write-an-algorithm-with-zero-downtime">Or we could write an algorithm with zero downtime!</h2>
<p>Our co-author Daniel shared an algorithm he used at his previous startup:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/no_downtime.png" alt=""></p>
<p>First, we pause all new transactions. Then, we wait for active transactions to complete and for 16 to catch up. Finally we unpause all transactions and have them go to 16. If we did this right, we could switch major versions without any downtime at all!</p>
<h3 id="the-benefits-of-being-small">The benefits of being small</h3>
<p>Sounds good in theory, but it can be hard to pull off. Unless of course you run at a modest scale.</p>
<p>Our switching algorithm hinges on being able to control all active connections. If you have tons of machines, how could you control all active connections?</p>
<p>Well, since our throughput was still modest, we could temporarily scale our sync servers down to just one giant machine. Clojure and java came handy here too. We had threads and the JVM is efficient, so we could take full advantage of the <a href="https://instances.vantage.sh/aws/ec2/m6a.16xlarge?region=us-east-1&amp;os=linux&amp;cost_duration=monthly&amp;reserved_term=Standard.noUpfront">m6a.16xlarge</a> sync server we moved to for the switch.</p>
<h3 id="writing-out-a-failover-function">Writing out a failover function</h3>
<p>So we went forward and translated our zero-downtime algorithm into code. Here’s how it looked:</p>
<pre><code><span>(</span><span>defn</span><span> do-failover-to-new-db </span><span>[</span><span>]</span><span></span>
<span>  </span><span>(</span><span>let</span><span> </span><span>[</span><span>prev-pool aurora/-conn-pool</span>
<span>        next-pool </span><span>(</span><span>start-new-pool</span><span> next-config</span><span>)</span><span></span>
<span>        next-pool-promise </span><span>(</span><span>promise</span><span>)</span><span>]</span><span></span>
<!-- -->
<span>    </span><span>;; 1. Make new connections wait</span><span></span>
<span>    </span><span>(</span><span>alter-var-root</span><span> </span><span>#</span><span>'aurora/conn-pool </span><span>(</span><span>fn</span><span> </span><span>[</span><span>_</span><span>]</span><span> </span><span>(</span><span>fn</span><span> </span><span>[</span><span>]</span><span> </span><span>@</span><span>next-pool-promise</span><span>)</span><span>)</span><span>)</span><span></span>
<!-- -->
<span>    </span><span>;; 2. Give existing transactions 2.5 seconds to complete.</span><span></span>
<span>    </span><span>(</span><span>Thread/sleep</span><span> </span><span>2500</span><span>)</span><span></span>
<span>    </span><span>;; Cancel the rest</span><span></span>
<span>    </span><span>(</span><span>sql/cancel-in-progress</span><span> sql/default-statement-tracker</span><span>)</span><span></span>
<!-- -->
<span>    </span><span>;; 3. Wait for 16 to catch up</span><span></span>
<span>    </span><span>(</span><span>let</span><span> </span><span>[</span><span>tx </span><span>(</span><span>transaction-model/create!</span><span> aurora/-conn-pool</span>
<span>                                        </span><span>{</span><span>:app-id</span><span> </span><span>(</span><span>config/instant-config-app-id</span><span>)</span><span>}</span><span>)</span><span>]</span><span></span>
<span>      </span><span>(</span><span>loop</span><span> </span><span>[</span><span>i </span><span>0</span><span>]</span><span></span>
<span>        </span><span>(</span><span>if-let</span><span> </span><span>[</span><span>row </span><span>(</span><span>sql/select-one</span><span> next-pool</span>
<span>                                      </span><span>[</span><span>"select * from transactions where app_id = ?::uuid and id = ?::bigint"</span><span></span>
<span>                                      </span><span>(</span><span>config/instant-config-app-id</span><span>)</span><span> </span><span>(</span><span>:id</span><span> tx</span><span>)</span><span>]</span><span>)</span><span>]</span><span></span>
<span>          </span><span>(</span><span>println</span><span> </span><span>"we are caught up!"</span><span>)</span><span></span>
<span>          </span><span>;; Still waiting...</span><span></span>
<span>          </span><span>(</span><span>do</span><span> </span><span>(</span><span>Thread/sleep</span><span> </span><span>50</span><span>)</span><span></span>
<span>              </span><span>(</span><span>recur</span><span> inc i</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span></span>
<!-- -->
<!-- -->
<span>    </span><span>;; 4 accept new connections!</span><span></span>
<span>    </span><span>(</span><span>deliver</span><span> next-pool-promise next-pool</span><span>)</span><span></span>
<span>    </span><span>(</span><span>alter-var-root</span><span> </span><span>#</span><span>'aurora/-conn-pool </span><span>(</span><span>fn</span><span> </span><span>[</span><span>_</span><span>]</span><span> next-pool</span><span>)</span><span>)</span><span>)</span><span>)</span>
</code></pre><p>We spun up staging again, ran our failover function...buut transactions failed again. We were getting unique constraint violations on our transactions table.</p>
<h3 id="dont-forget-sequences">Don’t forget sequences</h3>
<p>This time the fix was easy to catch: sequences. Postgres does not <a href="https://www.postgresql.org/docs/current/logical-replication-restrictions.html">replicate sequence</a> data. This meant that when a new <code>transaction</code> row was created, we were using ids that already existed.</p>
<p>To fix it, we incremented our sequences in the failover function:</p>
<pre><code><span>-</span><span>           (println "we are caught up!")</span>
<span></span><span>+</span><span>           (sql/execute! next-pool</span>
<span></span><span>+</span><span>                         ["select setval('transactions_id_seq', ?::bigint, true)"</span>
<span></span><span>+</span><span>                         (+ (:id row) 1000)])</span>
</code></pre><p>This time we ran the failover function...and it worked great!</p>
<p>If you’re curious, here’s how the actual failover <a href="https://github.com/instantdb/instant/blob/main/server/src/instant/jdbc/failover.clj#L25-L87">function</a> looked for production.</p>
<h3 id="running-in-prod">Running in Prod</h3>
<p>Now that we had a good practice run, we got ourselves ready, had our sparkling waters in hand, and began to ran our steps in production. </p>
<p>After about a 3.5 second pause <sup id="marked-fnref-13"><a href="#marked-fn-13">[13]</a></sup>, the failover function completed smoothly! We had a new Postgres instance serving requests, and best of all, nobody noticed. <sup id="marked-fnref-14"><a href="#marked-fn-14">[14]</a></sup></p>
<h3 id="future-improvements">Future Improvements</h3>
<p>Our <code>do-failover-to-new-db</code> worked at our scale, but will probably fail us in a few months. There are two improvements we plan to make:</p>
<ol>
<li>We paused <em>both</em> writes and reads. But technically we don’t need to pause reads. Daniel pushed <a href="https://github.com/instantdb/instant/pull/743">up a PR</a> to be explicit about read-only connections. In the future we can skip pausing them.</li>
<li>In December we were able to scale down to one big machine. We’re approaching the limits to one big machine today. <sup id="marked-fnref-15"><a href="#marked-fn-15">[15]</a></sup> We’re going to try to evolve this into a kind of <code>two-phase-commit</code>, where each machine reports their stage, and a coordinator progresses when all machines hit the same stage.</li>
</ol>
<h2 id="fin">Fin</h2>
<p>Aand that’s our story of how did our major version upgrade. We wanted to finish up with a summary of learnings, in the hopes that’s easier for you to get back to this essay when you’re considering an upgrade. Here’s what we wish we knew when we started:</p>
<ol>
<li>Sometimes, newer Postgres versions improve perf. Make sure to check this if you face perf issues.</li>
<li>If you need to upgrade<ol>
<li>Pick a buddy if you can, it’s a lot more fun (and less nerve-racking) to do this with a partner.</li>
<li>Before you do anything in production, do a full rehearsal. Use a staging environment that mimics production as closely as possible</li>
<li>If you are okay with 15 minutes of downtime, do an <a href="#in-place-upgrade">in-place upgrade</a></li>
<li>If you don’t have active replication slots and are okay with a minute of downtime, try a <a href="#blue-green-deployment">blue-green deployment</a></li>
<li>When you need to do a manual upgrade:<ol>
<li>If you can, skip cloning and create a replica from scratch. There are only <a href="#replica-checklist">7 steps</a></li>
<li>If you wrote custom pg functions, make sure to check your <a href="#search-paths">search_path</a></li>
<li>Do some sanity checks to make sure you don’t <a href="#missing-data">lose data</a></li>
<li>If you can get writes down to one machine, try our <a href="#zero-downtime-algo">algorithm for zero downtime</a></li>
</ol>
</li>
</ol>
</li>
</ol>
<p>Hopefully, this was a fun read for you :)</p>
<p><em>Thanks to Nikita Prokopov, Joe Averbukh, Martin Raison, Irakli Safareli, Ian Sinnott for reviewing drafts of this essay</em></p>
<p><a id="marked-fn-1" href="#marked-fnref-1">[1]</a>  Our sync strategy was inspired by Figma’s LiveGraph and Asana’s Luna. The LiveGraph team wrote a <a href="https://www.figma.com/blog/livegraph-real-time-data-fetching-at-figma/">great essay</a> that explains the sync strategy. You can read our original <a href="https://www.instantdb.com/essays/next_firebase">design essay</a> to learn more about Instant</p>
<p><a id="marked-fn-2" href="#marked-fnref-2">[2]</a>  You may be wondering: how do we host multiple "Instant databases", under one "Aurora database"? The short answer is that we wrote a query engine on top of Postgres. This lets us create a multi-tenant system., where we can "spin up" dbs on demand. I hope to share more about this in a separate essay.</p>
<p><a id="marked-fn-3" href="#marked-fnref-3">[3]</a>  All of the code (including this blog) is open sourced <a href="https://github.com/instantdb/instant">here</a>.</p>
<p><a id="marked-fn-4" href="#marked-fnref-4">[4]</a>  <a href="https://instances.vantage.sh/aws/rds/db.r6g.16xlarge">db.r6g.16xlarge</a> would cost us north of 6K per month. That was out of the question for the kind of traffic we were handling.</p>
<p><a id="marked-fn-5" href="#marked-fnref-5">[5]</a>  In case you were wondering, we also looked to optimize the queries. After we upgraded (took about a day and a half), we added a partial index that improved perf another 50% or so.</p>
<p><a id="marked-fn-6" href="#marked-fnref-6">[6]</a>  We did see a note about replication in <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/blue-green-deployments-switching.html#blue-green-deployments-switching-guardrails">"Switchover Guardrails"</a>, but this note is about the second step: after 1) creating a green deployment, we 2) run the switch.</p>
<p><a id="marked-fn-7" href="#marked-fnref-7">[7]</a>  The key to discovering this issue was our co-author Daniel’s sleuthing. He planned test upgrades locally: going from 13 → 14 → 15 → 16, to see where things broke. When Daniel tried 13 → 14, it failed. To sanity check things, he then tried a migration from 13 → 13…and that failed too! From there we knew something had to be up with our process.</p>
<p><a id="marked-fn-8" href="#marked-fnref-8">[8]</a>  An alternative would have been to enhance the dump file with the search path. We like the idea of being more explicit in our definitions though; especially if we can find a good linter.</p>
<p><a id="marked-fn-9" href="#marked-fnref-9">[9]</a>  Why do we have it? We use the transaction’s id column for record-keeping inside sync servers.</p>
<p><a id="marked-fn-10" href="#marked-fnref-10">[10]</a>  If you are curious, you can look at a slice of the checklist we used <a href="https://gist.github.com/stopachka/f05d3682223e206ed6465cafe3ec9f2a">here</a>. If you have a hunch where the data loss could have come from, let us know</p>
<p><a id="marked-fn-11" href="#marked-fnref-11">[11]</a>  Though even with 30TB, it would only take a week to transfer at a modest 50 mb/second.</p>
<p><a id="marked-fn-12" href="#marked-fnref-12">[12]</a>  You may be wondering — sure, the transactions table was okay, but what if there was data loss in other tables? We wrote a <a href="https://github.com/instantdb/instant/blob/main/server/src/instant/jdbc/failover.clj#L258">more involved script</a> to check for every table too. We really wanted to make sure there was no data loss.</p>
<p><a id="marked-fn-13" href="#marked-fnref-13">[13]</a>  About 2.5 seconds to let active queries complete, and about 1 second for the replica to catch up</p>
<p><a id="marked-fn-14" href="#marked-fnref-14">[14]</a>  You may be wondering, how did we run the function? Where’s the feature flag? That’s one more Clojure win: we could SSH into production, and execute this function in our REPL!</p>
<p><a id="marked-fn-15" href="#marked-fnref-15">[15]</a>  The big bottleneck is all the active websocket connections on one machine — it slows down the sync engine too much. If we improve perf, perhaps we can get to one big machine again!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On DeepSeek and export controls (127 pts)]]></title>
            <link>https://darioamodei.com/on-deepseek-and-export-controls</link>
            <guid>42866905</guid>
            <pubDate>Wed, 29 Jan 2025 16:19:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://darioamodei.com/on-deepseek-and-export-controls">https://darioamodei.com/on-deepseek-and-export-controls</a>, See on <a href="https://news.ycombinator.com/item?id=42866905">Hacker News</a></p>
Couldn't get https://darioamodei.com/on-deepseek-and-export-controls: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Case Study: ByteDance Uses eBPF to Enhance Networking Performance (129 pts)]]></title>
            <link>https://ebpf.foundation/case-study-bytedance-uses-ebpf-to-enhance-networking-performance/</link>
            <guid>42866572</guid>
            <pubDate>Wed, 29 Jan 2025 15:58:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ebpf.foundation/case-study-bytedance-uses-ebpf-to-enhance-networking-performance/">https://ebpf.foundation/case-study-bytedance-uses-ebpf-to-enhance-networking-performance/</a>, See on <a href="https://news.ycombinator.com/item?id=42866572">Hacker News</a></p>
Couldn't get https://ebpf.foundation/case-study-bytedance-uses-ebpf-to-enhance-networking-performance/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek proves the future of LLMs is open-source (495 pts)]]></title>
            <link>https://www.getlago.com/blog/deepseek-open-source</link>
            <guid>42866201</guid>
            <pubDate>Wed, 29 Jan 2025 15:37:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.getlago.com/blog/deepseek-open-source">https://www.getlago.com/blog/deepseek-open-source</a>, See on <a href="https://news.ycombinator.com/item?id=42866201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><a href="https://www.getlago.com/blog"><p>Blog</p></a><p>Why DeepSeek had to be open-source (and why it won't defeat OpenAI)</p></div><div><p>By now, you’ve heard of DeepSeek. It’s the Chinese AI lab that trained R1, an open-source reasoning model as good as OpenAI’s o1, but trained on inferior hardware for a fraction of the price. </p><p>‍</p><p>They achieved this with novel training methods that are more efficient than the ones OpenAI, Anthropic and other well-funded competitors use to train their proprietary models. But why would they open-source it? </p><p>‍</p><p>On the surface, it goes against every business textbook you’ve read: If you innovate to build a market-leading product at a fraction of the cost, you should exploit that advantage. Coca-Cola doesn’t open-source its recipe, right? </p><p>‍</p><p>Not in the world of LLMs. I believe DeepSeek almost had to open-source its models—and that open-source models will become more and more dominant as time goes on. </p><h3>‍<br>Why DeepSeek had to go open-source</h3><p>‍<br>DeepSeek is in a unique position. It’s a Chinese company, which probably makes businesses feel uneasy about building with them, especially when you start to deal with customer data—and even more so when you want to be HIPAA compliant or SOC2-certified. </p><p>‍</p><p>A Chinese AI API would likely receive skepticism in the West. But an open-source model instantly builds trust. You have full control if you self-host or use an AI vendor like Together AI. </p><p>‍</p><p>To gain a foothold in Western markets, DeepSeek had to open-source its models. But that’s not just an economic decision. But it’s not just a political decision. I recently heard the quote that “Open-source is not just a technological behavior, it’s also a cultural one”. </p><p>‍</p><p>And open-source companies (at least in the beginning) have to do more with less. It’s precisely because DeepSeek has to deal with export control on cutting-edge chips like Nvidia H100s and GB10s that they had to find more efficient ways of training models. </p><p>‍</p><p>OpenAI, Meta, Google etc. have billions of dollars, massive compute resources and world-class distribution. They don’t need to find a more efficient way to train models when their expensive solution is the only one. In fact, making it easier and cheaper to build LLMs would erode their advantages!</p><p>‍</p><p>Now that has changed. </p><h3>‍<br>Models are getting commoditized</h3><p>‍<br>It feels like a new GPT-4-level LLM gets released every week. In the AI apps I use, I can’t tell if I’m using LLaMa, GPT, Claude or Mistral models. They’re pretty equal in performance both in my personal experience and in benchmarks. </p><p>‍</p><p>OpenAI is still the leader. They were the first to release a reasoning model and the first to release GPT-4. But models are getting commoditized—and it’s worth asking whether it’s worth paying the premium the OpenAI API charges compared to open-source models. </p><p>‍</p><p>DeepSeek might be the starkest example of this. Compare $60 per million output tokens for OpenAI o1 to $7 per million output tokens on Together AI for DeepSeek R1. </p><p>‍</p><p>If your end user doesn’t know the difference, why would you pay that much more? This is especially important in infrastructure.</p><h3>‍<br>In infrastructure, open-source wins (eventually)</h3><p>‍<br>There’s often a tradeoff with using open-source and proprietary software: Open-source is cheaper and more customizable, but ties up more resources (and requires technical knowledge) because you have to maintain it yourself. Proprietary costs more, but offers a smoother (if more rigid) experience.</p><p>For many product categories, this tradeoff is not worth making for most companies. You don’t want to lose your knowledge base because your self-hosted Notion alternative made an error. </p><p>‍</p><p>But infrastructure is always custom. It always requires work from you. Even a proprietary Oracle database requires a ton of work to set up and maintain. This is why open-source databases are more and more popular. </p><p>‍</p><p>The advantage of proprietary software (No maintenance, no technical knowledge required, etc.) is much lower for infrastructure. It’s actually the opposite: The more technical a product, the better it is for the user (engineers) to work with open-source because they can audit the codebase.</p><p>‍</p><p>This is also why we’re building Lago as an open-source company. We know billing gets complex whether you build your own or use a vendor, so the engineers prefer to work with Lago.</p><p>‍</p><p>The same is true for LLMs. To build any useful product, you’ll be doing a lot of custom prompting and engineering anyway, so you may as well use DeepSeek’s R1 over OpenAI’s o1. </p><p>‍</p><p>This is why there are a lot of successful open-source infrastructure companies and almost no successful open source consumer companies. </p><p>‍</p><p>Does that mean proprietary AI is done? No. </p><h3>‍<br>OpenAI is far from over</h3><p>‍<br>There’s a lot of talk about how OpenAI will be obsolete because of DeepSeek R1 or other open-source models. But that’s not true. First, OpenAI has always been first to market, both with LLMs like GPT-4 and reasoning models like o1. </p><p>‍</p><p>Without OpenAI’s models, DeepSeek R1 and many other models wouldn’t exist (because of LLM distillation). This does beg the question of whether it’s still worth it to build new frontier models if you provide the breakthrough and someone else ships something similar for much cheaper. </p><p>‍</p><p>But R1 might also wake up the well-funded incumbents and force them to find more efficient methods—and who knows what they can build when they have both efficiency and all the resources in the world.</p><p>‍</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A story about restoring and upgrading a Commodore Amiga 1000 (128 pts)]]></title>
            <link>https://celso.io/posts/2025/01/26/the-first-perfect-computer/</link>
            <guid>42865867</guid>
            <pubDate>Wed, 29 Jan 2025 15:16:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://celso.io/posts/2025/01/26/the-first-perfect-computer/">https://celso.io/posts/2025/01/26/the-first-perfect-computer/</a>, See on <a href="https://news.ycombinator.com/item?id=42865867">Hacker News</a></p>
Couldn't get https://celso.io/posts/2025/01/26/the-first-perfect-computer/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Google Pixel 4a old firmware is gone, trapping users on the buggy battery update (261 pts)]]></title>
            <link>https://www.androidcentral.com/phones/google-pixel-4as-old-firmware-is-gone-trapping-users-on-the-buggy-battery-update</link>
            <guid>42865619</guid>
            <pubDate>Wed, 29 Jan 2025 14:59:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.androidcentral.com/phones/google-pixel-4as-old-firmware-is-gone-trapping-users-on-the-buggy-battery-update">https://www.androidcentral.com/phones/google-pixel-4as-old-firmware-is-gone-trapping-users-on-the-buggy-battery-update</a>, See on <a href="https://news.ycombinator.com/item?id=42865619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg" alt="Google Pixel 4a on a black metal bench" srcset="https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF.jpg" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<figcaption>
<span>(Image credit: Android Central)</span>
</figcaption>
</div>

<div id="article-body">
<h2 id="what-you-need-to-know-3">What you need to know</h2><ul><li>Google’s Pixel 4a battery update is a disaster, causing crazy battery drain for many users.</li><li>The update also wiped older firmware, leaving no way to roll back to the previous version.</li><li>Google has acknowledged the mess and offered compensation: a free battery replacement, $50 cash, or $100 credit toward a new Pixel.</li></ul><hr><p>Google recently <a data-analytics-id="inline-link" href="https://www.androidcentral.com/apps-software/older-pixel-4a-to-get-an-all-new-battery-performance-update" data-before-rewrite-localise="https://www.androidcentral.com/apps-software/older-pixel-4a-to-get-an-all-new-battery-performance-update">rolled out a battery update for the Pixel 4a</a>, and it’s been a bit of a disaster. A bunch of users are reporting crazy battery drain since installing it. To add insult to injury, Google wiped the older firmware as part of the update process, so there’s no official way to go back to the previous version. If you’re dealing with this mess, you’re pretty much stuck at the moment.</p><p>After discontinuing the Pixel 4a last year, Google had left the device without any software updates. But out of nowhere, about two weeks ago, the company released a surprise update meant to <a data-analytics-id="inline-link" href="https://www.androidcentral.com/phones/google-tips-pixel-smartphone-battery-health" data-before-rewrite-localise="https://www.androidcentral.com/phones/google-tips-pixel-smartphone-battery-health">improve battery performance</a>. Part of the Pixel 4a Battery Performance Program, it was supposed to help some units last longer on a charge. Instead, it ended up making battery problems worse for a ton of users.</p><p>In other words, the update was a total flop—lots of Pixel 4a owners are now stuck with awful battery life. And if you were thinking about rolling back to an older version to fix things, Google just shut that door by pulling the download links for previous firmware.</p><p>Someone on <a data-analytics-id="inline-link" href="https://old.reddit.com/r/GooglePixel/comments/1iajsu3/google_removed_pixel_4a_firmware_images_from/" target="_blank" data-url="https://old.reddit.com/r/GooglePixel/comments/1iajsu3/google_removed_pixel_4a_firmware_images_from/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Reddit</a> noticed that the only Android image you can get for the Pixel 4a is the new update that Google forced everyone to install in order to tackle unidentified battery issues (via <a data-analytics-id="inline-link" href="https://9to5google.com/2025/01/26/old-pixel-4a-updates/" target="_blank" data-url="https://9to5google.com/2025/01/26/old-pixel-4a-updates/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">9to5Google</a>).</p><p>The update might keep the Pixel 4a running, but it's essentially rendered it useless for many users.</p><p>One <a data-analytics-id="inline-link" href="https://www.reddit.com/r/Pixel4a/comments/1i6u7h8/comment/m8fbhnf/" target="_blank" data-url="https://www.reddit.com/r/Pixel4a/comments/1i6u7h8/comment/m8fbhnf/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Reddit user</a> reported their 4a draining from full to 2% in just five hours, despite minimal screen usage (under 20 minutes).</p><p><a data-analytics-id="inline-link" href="https://www.androidauthority.com/google-pixel-4a-firmware-removed-3520326/" target="_blank" data-url="https://www.androidauthority.com/google-pixel-4a-firmware-removed-3520326/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Android Authority</a> points out that Google rarely removes older firmware, implying a serious underlying issue with the Pixel 4a update.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-ZHxyhSCQ88Eymxod3bdfsN"><section><p>Get the latest news from Android Central, your trusted companion in the world of Android</p></section></div><p>Google is owning up to the battery disaster caused by the recent Pixel 4a update and is trying to make things right. The company is offering compensation to anyone impacted. To check if you’re eligible, hop over to this <a data-analytics-id="inline-link" href="https://support.google.com/pixelphone/workflow/15642495" target="_blank" data-url="https://support.google.com/pixelphone/workflow/15642495" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">support page</a>, type in your phone’s IMEI number, and follow the steps. You’ll get to pick between a free battery replacement, $50 cash, or a $100 credit toward a new Pixel.</p>
</div>


<div id="slice-container-authorBio-ZHxyhSCQ88Eymxod3bdfsN"><p>Jay Bonggolto always keeps a nose for news. He has been writing about consumer tech and apps for as long as he can remember, and he has used a variety of Android phones since falling in love with Jelly Bean. Send him a direct message via Twitter or LinkedIn.</p></div>
</section>





<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Complete hardware and software setup for running Deepseek-R1 locally. ($6000) (226 pts)]]></title>
            <link>https://twitter.com/carrigmat/status/1884244369907278106</link>
            <guid>42865575</guid>
            <pubDate>Wed, 29 Jan 2025 14:56:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/carrigmat/status/1884244369907278106">https://twitter.com/carrigmat/status/1884244369907278106</a>, See on <a href="https://news.ycombinator.com/item?id=42865575">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI Furious DeepSeek Might Have Stolen All the Data OpenAI Stole from Us (1281 pts)]]></title>
            <link>https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/</link>
            <guid>42865527</guid>
            <pubDate>Wed, 29 Jan 2025 14:52:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/">https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/</a>, See on <a href="https://news.ycombinator.com/item?id=42865527">Hacker News</a></p>
Couldn't get https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Cali's AG Tells AI Companies Almost Everything They're Doing Might Be Illegal (170 pts)]]></title>
            <link>https://gizmodo.com/californias-ag-tells-ai-companies-practically-everything-theyre-doing-might-be-illegal-2000555896</link>
            <guid>42865174</guid>
            <pubDate>Wed, 29 Jan 2025 14:21:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/californias-ag-tells-ai-companies-practically-everything-theyre-doing-might-be-illegal-2000555896">https://gizmodo.com/californias-ag-tells-ai-companies-practically-everything-theyre-doing-might-be-illegal-2000555896</a>, See on <a href="https://news.ycombinator.com/item?id=42865174">Hacker News</a></p>
Couldn't get https://gizmodo.com/californias-ag-tells-ai-companies-practically-everything-theyre-doing-might-be-illegal-2000555896: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[I do not want AI to "polish" me (387 pts)]]></title>
            <link>https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/</link>
            <guid>42864854</guid>
            <pubDate>Wed, 29 Jan 2025 13:50:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/">https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/</a>, See on <a href="https://news.ycombinator.com/item?id=42864854">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">

		<main id="main">

			
<article id="post-42430">

	

	
	<div>
		
<p>I was sending an email when a little magic wand popped up that said <strong>“Polish”</strong> and I thought that was weird because <em>why would I want to translate my email into Polish</em>?  </p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?ssl=1"><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="750" height="295" data-attachment-id="42432" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-27-41-pm/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?fit=1862%2C732&amp;ssl=1" data-orig-size="1862,732" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.27.41 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?fit=300%2C118&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?fit=750%2C295&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=750%2C295&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=1024%2C403&amp;ssl=1 1024w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=300%2C118&amp;ssl=1 300w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=768%2C302&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=1536%2C604&amp;ssl=1 1536w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=1200%2C472&amp;ssl=1 1200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=1568%2C616&amp;ssl=1 1568w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?w=1862&amp;ssl=1 1862w" sizes="(max-width: 750px) 100vw, 750px"></a></figure>



<p>I tried to click on it to make it go away but instead it changed the entire email because apparently it was saying that it needed to “polish” my email because I guess I’m too unsophisticated to use words:</p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?ssl=1"><img data-recalc-dims="1" decoding="async" width="750" height="943" data-attachment-id="42434" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-27-57-pm/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?fit=2044%2C2572&amp;ssl=1" data-orig-size="2044,2572" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.27.57 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?fit=238%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?fit=750%2C943&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=750%2C943&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=814%2C1024&amp;ssl=1 814w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=238%2C300&amp;ssl=1 238w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=768%2C966&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=1221%2C1536&amp;ssl=1 1221w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=1628%2C2048&amp;ssl=1 1628w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=1200%2C1510&amp;ssl=1 1200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=1568%2C1973&amp;ssl=1 1568w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?w=2044&amp;ssl=1 2044w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?w=1500&amp;ssl=1 1500w" sizes="(max-width: 750px) 100vw, 750px"></a></figure>



<p>There is no way in hell anyone who knows me would get that email and not think I’d been abducted so I deleted the suggested rewrite and updated my email:</p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?ssl=1"><img data-recalc-dims="1" decoding="async" width="750" height="758" data-attachment-id="42436" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-32-40-pm/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?fit=1938%2C1960&amp;ssl=1" data-orig-size="1938,1960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.32.40 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?fit=297%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?fit=750%2C758&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=750%2C758&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=1013%2C1024&amp;ssl=1 1013w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=297%2C300&amp;ssl=1 297w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=768%2C777&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=1519%2C1536&amp;ssl=1 1519w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=1200%2C1214&amp;ssl=1 1200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=1568%2C1586&amp;ssl=1 1568w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?w=1938&amp;ssl=1 1938w" sizes="(max-width: 750px) 100vw, 750px"></a></figure>



<p>But after I added the update gmail was like, “YOU’RE STILL DOING IT WRONG, IDIOT?” and the polish thing came up <em><strong>again</strong></em> and I was like, “Are you trying to AI fix a paragraph where I say how much I don’t want AI to fix shit?”  And turns out, yeah, that exactly what it meant because it gave me this:</p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="750" height="753" data-attachment-id="42438" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-28-15-pm/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?fit=2232%2C2240&amp;ssl=1" data-orig-size="2232,2240" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.28.15 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?fit=750%2C753&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=750%2C753&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=1020%2C1024&amp;ssl=1 1020w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=768%2C771&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=1531%2C1536&amp;ssl=1 1531w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=2041%2C2048&amp;ssl=1 2041w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=1200%2C1204&amp;ssl=1 1200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=1568%2C1574&amp;ssl=1 1568w" sizes="auto, (max-width: 750px) 100vw, 750px"></a></figure>



<p><strong>Jesus.</strong> Y’all, if you get an email from me it will be signed with HUGS, LOVE, FIGHT THE PATRIARCHY, DOWN WITH POWDERED GRAVY or SORRY I SUCK SO MUCH. It will be filled with typos and rambling parentheticals and apologies for answering several months too late. This is how you know it’s me and not a robot. My only hope is that my constant declining of the suggestions will make the AI learn from me and spread my terrible etiquette throughout the world.</p>



<p>Also, I just realized when I tried to insert these pictures into this blog about how much I hate AI my blog was suddenly like, “HEY I KNOW YOU JUST CLICKED A BUTTOM SAYING YOU WANT TO ADD A SPECIFIC PICTURE BUT HOW ABOUT WE JUST MAKE AI IMAGES FOR YOU INSTEAD?”  <strong><em>AM I ON CANDID CAMERA?</em></strong>  It’s like my whole computer is a toddler screaming “LET ME DO IT!” every time I try to create something.</p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="750" height="364" data-attachment-id="42446" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-46-17-pm/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?fit=1170%2C568&amp;ssl=1" data-orig-size="1170,568" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.46.17 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?fit=300%2C146&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?fit=750%2C364&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?resize=750%2C364&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?resize=1024%2C497&amp;ssl=1 1024w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?resize=300%2C146&amp;ssl=1 300w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?resize=768%2C373&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?w=1170&amp;ssl=1 1170w" sizes="auto, (max-width: 750px) 100vw, 750px"></a></figure>



<p>And as much as I hate AI, I had to see what the program thought it could do so much better than me so I gave it the prompt “please stop giving me AI” and…all apologies.  Clearly I <strong><em>did</em></strong> need help because…fucking wow.  Nailed it:</p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="750" height="644" data-attachment-id="42448" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-48-20-pm-2/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?fit=1712%2C1470&amp;ssl=1" data-orig-size="1712,1470" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.48.20 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?fit=300%2C258&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?fit=750%2C644&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=750%2C644&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=1024%2C879&amp;ssl=1 1024w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=300%2C258&amp;ssl=1 300w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=768%2C659&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=1536%2C1319&amp;ssl=1 1536w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=1200%2C1030&amp;ssl=1 1200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=1568%2C1346&amp;ssl=1 1568w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?w=1712&amp;ssl=1 1712w" sizes="auto, (max-width: 750px) 100vw, 750px"></a></figure>



<p>Anyway…this sucks.</p>



<p>Worst regards,</p>



<p>Jenny</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article><!-- #post-${ID} -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
<!-- #comments -->

		</main><!-- #main -->

		



	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Seagate: 'new' hard drives used for tens of thousands of hours (309 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/hdds/german-seagate-customers-say-their-new-hard-drives-were-actually-used-resold-hdds-reportedly-used-for-tens-of-thousands-of-hours</link>
            <guid>42864788</guid>
            <pubDate>Wed, 29 Jan 2025 13:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/hdds/german-seagate-customers-say-their-new-hard-drives-were-actually-used-resold-hdds-reportedly-used-for-tens-of-thousands-of-hours">https://www.tomshardware.com/pc-components/hdds/german-seagate-customers-say-their-new-hard-drives-were-actually-used-resold-hdds-reportedly-used-for-tens-of-thousands-of-hours</a>, See on <a href="https://news.ycombinator.com/item?id=42864788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1600-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1200-80.jpg" alt="" srcset="https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1600-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN.jpg" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<figcaption>
<span>(Image credit: Seagate)</span>
</figcaption>
</div>

<div id="article-body">
<p>German outlet Heise.de says it may have uncovered fraud after hearing from many of its readers who bought supposedly new <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/seagate" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/seagate">Seagate</a> hard drives, which turned out to be very much used.</p><p>Last week, the publication <a data-analytics-id="inline-link" href="https://www.heise.de/news/Gebrauchte-Seagate-Festplatten-als-Neuware-im-Umlauf-10254276.html" data-url="https://www.heise.de/news/Gebrauchte-Seagate-Festplatten-als-Neuware-im-Umlauf-10254276.html" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">relayed</a> the experience of one of Heise.de’s readers, who said he had bought a couple of 14TB Seagate Exos HDDs that seemed a little strange. The drives had some minor signs of wear on the outside, but after a quick look at the SMART stats, everything appeared normal. Later, though, the reader did a more thorough Field Accessible Reliability Metrics (FARM) test and discovered that one drive had already been used for 10,000 hours, and the other 15,000 hours.</p><p>Naturally, he returned the drives to the store he bought them from, an official Seagate retailer, and decided to replace them with two 16TB Exos HDDs purchased from a different store. These drives also turned out to be heavily used: 22,000 hours logged on each one.</p><p>Although both HDD sellers, neither of which Heise.de identified, claimed the Exos drives were simply brand-new retail models, Seagate told the publication that all four drives were actually OEM models. This meant that the normal five-year warranty did not apply like it would to typical drives bought at retail.</p><p>The initial retailer eventually stopped selling the 14TB and 16TB HDDs at some point and even canceled an order that Heise.de had anonymously placed. According to the report, Seagate is looking into how this happened, especially as one of the retailers has the storage corporation’s endorsement as an official retailer.</p><p>After this report was published, the floodgates opened, and over fifty other Heise.de readers <a data-analytics-id="inline-link" href="https://www.heise.de/news/Betrug-mit-Seagate-Festplatten-Dutzende-Leser-melden-Verdachtsfaelle-10258657.html" target="_blank" data-url="https://www.heise.de/news/Betrug-mit-Seagate-Festplatten-Dutzende-Leser-melden-Verdachtsfaelle-10258657.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">said</a> they experienced the exact same thing after buying apparently new Seagate HDDs. While 50 is a small sample size, the issue might be widespread since they bought their drives at a dozen different retailers, some of which are on Seagate’s official “where-to-buy” list. Some of the impacted retailers are quite large, such as <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/amazon" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/amazon">Amazon</a> and Mindfactory.</p><p>Most readers report having 16TB Exos drives, but others have the 12TB model, and a few have non-Exos HDDs ranging from 4 to 18TB. The time used ranges from 15,000 to 36,000 hours except for two 4TB HDDs, which were both used for about 50,000 hours. Heise.de checked a few drives at random to see when their warranties expired, and most of them were for 2026. Assuming a five-year warranty, that means they were first made and sold in 2021. All of the readers who reported receiving a used Seagate drive had bought it in the past few weeks, meaning the issue is relatively recent.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-yuaCUcXynTznQQrKVaUXa7"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><p>It’s hard to imagine this is just a simple mixup, not just because so many retailers are apparently involved but also because they’ve all had their SMART stats reset, which would be very useful to someone trying to pretend a used drive is new. Although it’s not entirely clear if actual fraud is happening here, something has definitely gone very wrong.</p><p>We reached out to Seagate for comment but haven’t received a reply yet.</p><p>Seagate does have a direct relationship with used hard drives. Nearly a year ago, the company <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/hdds/seagate-opens-an-ebay-store-to-sell-refurbished-hard-drives" target="_blank" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/hdds/seagate-opens-an-ebay-store-to-sell-refurbished-hard-drives">launched an official eBay store that sells refurbished drives. It</a> also has a Hard Drive Circularity Program to find as many refurbish-worthy drives as possible, including Exos models. However, this store only sells in the US, so it doesn’t seem likely that it has anything to do with the current situation.</p>
</div>



<!-- Drop in a standard article here maybe? -->



<div id="slice-container-authorBio-yuaCUcXynTznQQrKVaUXa7"><p>Matthew&nbsp;Connatser is a freelancing writer for Tom's Hardware US. He writes articles about CPUs, GPUs, SSDs, and computers in general.</p></div>
</section>





<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Our phones are killing our ability to feel sexy (2024) (206 pts)]]></title>
            <link>https://catherineshannon.substack.com/p/your-phone-is-why-you-dont-feel-sexy</link>
            <guid>42864595</guid>
            <pubDate>Wed, 29 Jan 2025 13:21:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://catherineshannon.substack.com/p/your-phone-is-why-you-dont-feel-sexy">https://catherineshannon.substack.com/p/your-phone-is-why-you-dont-feel-sexy</a>, See on <a href="https://news.ycombinator.com/item?id=42864595">Hacker News</a></p>
Couldn't get https://catherineshannon.substack.com/p/your-phone-is-why-you-dont-feel-sexy: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Asteroid Impact on Earth 2032 with Probability 1% and 8Mt Energy (153 pts)]]></title>
            <link>https://cneos.jpl.nasa.gov/sentry/details.html#?des=2024%20YR4</link>
            <guid>42864272</guid>
            <pubDate>Wed, 29 Jan 2025 12:47:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cneos.jpl.nasa.gov/sentry/details.html#?des=2024%20YR4">https://cneos.jpl.nasa.gov/sentry/details.html#?des=2024%20YR4</a>, See on <a href="https://news.ycombinator.com/item?id=42864272">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <td colspan="2">
            <h4>
              [[ objSummary.fullname ]] -- Earth Impact Risk Summary
              <small>(<a href="https://cneos.jpl.nasa.gov/sentry/[[%20risk_object_URL]][[%20des%20]]">orbit details</a>)</small>
            </h4>
          </td>
        </div><div>
          
          <td>
            <table>
              
<tbody>

  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="The maximum hazard rating according to the Torino impact hazard scale, based on the tabulated impact probability and impact energy. The Torino Scale is defined only for potential impacts less than 100 years in the future.">Torino Scale (maximum)</span></th>
    
      
      
    <td nowrap="">[[objSummary.ts_max]]</td>
      
    
  </tr>
  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="The maximum hazard rating according to the Palermo technical impact hazard scale, based on the tabulated impact date, impact probability and impact energy.">Palermo Scale (maximum)</span></th>
    
      
      
    <td nowrap="">[[objSummary.ps_max | number:2 ]]</td>
      
    
  </tr>
  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="The cumulative hazard rating according to the Palermo technical impact hazard scale, based on the tabulated impact date, impact probability and impact energy.">Palermo Scale (cumulative)</span></th>
    
      
      
    <td nowrap="">[[objSummary.ps_cum | number:2 ]]</td>
      
    
  </tr>
  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="The cumulative probability that the tabulated impact will occur. The probability computation is complex and depends on a number of assumptions that are difficult to verify. For these reasons the stated probability can easily be inaccurate by a factor of a few, and occasionally by a factor of ten or more.">Impact Probability (cumulative)</span></th>
    
    <td nowrap="" ng-bind-html="summary_ip_html"></td>
    
  </tr>
  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="Total number of potential impacts resulting from this analysis.">Number of Potential Impacts</span></th>
    
      
      
    <td nowrap="">[[objSummary.n_imp]]</td>
      
    
  </tr>
  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="Technique used to search for potential impacts. The technique can be based on a Monte Carlo (MC) sampling of the orbital uncertainty or can analyze the trajectories of virtual astroids along the Line of Variations (LOV)">Impact Search Technique</span></th>
    
      
      
    <td nowrap="">[[objSummary.method]]</td>
      
    
  </tr>
  


</tbody>


            </table>
          </td>
          <td rowspan="2">
            <table>
              
<tbody>

  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="Velocity at atmospheric entry.">V<sub>impact</sub></span></th>
    
      
      
    <td nowrap="">[[objSummary.v_imp  | number:2 ]] km/s</td>
      
    
  </tr>
  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="Relative velocity at atmospheric entry neglecting the acceleration caused by the Earth's gravity field, often called the hyperbolic excess velocity. (V<sub>infinity</sub><sup>2</sup> = V<sub>impact</sub><sup>2</sup> - V<sub>escape</sub><sup>2</sup>, where V<sub>escape</sub> = ~11.2 km/s is the Earth escape velocity.)">V<sub>infinity</sub></span></th>
    
      
      
    <td nowrap="">[[objSummary.v_inf | number:2 ]] km/s</td>
      
    
  </tr>
  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="Absolute Magnitude, a measure of the intrinsic brightness of the object.">H</span></th>
    
      
      
    <td nowrap="">[[objSummary.h | number:1 ]]</td>
      
    
  </tr>
  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="This is an estimate based on the absolute magnitude, usually assuming a uniform spherical body with visual albedo <i>p<sub>V</sub></i> = 0.154 (in accordance with the Palermo Scale) but sometimes using actual measured values if these are available.  Since the albedo is rarely measured, the diameter estimate should be considered only approximate, but in most cases will be accurate to within a factor of two.">Diameter</span></th>
    
      
      
    <td nowrap="">[[objSummary.diameter | number:3 ]] km</td>
      
    
  </tr>
  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="This estimate assumes a uniform spherical body with the computed diameter and a mass density of 2.6 g/cm<sup>3</sup>. The mass estimate is somewhat more rough than the diameter estimate, but generally will be accurate to within a factor of three.">Mass</span></th>
    
      
      
    <td nowrap="">[[objSummary.mass | exponentialFormat ]] kg</td>
      
    
  </tr>
  
    
  <tr>
    
    <th nowrap=""><span data-tooltip="" aria-haspopup="true" title="The kinetic energy at impact: 0.5 * Mass * V<sub>impact</sub><sup>2</sup>, measured in Megatons of TNT.">Energy</span></th>
    
      
      
    <td nowrap="">[[objSummary.energy | exponentialFormat ]] Mt</td>
      
    
  </tr>
  


  
  <tr><td colspan="2" nowrap="">All above are mean values<br>weighted by impact probability</td></tr>

</tbody>


            </table>
          </td>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jevons paradox (168 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Jevons_paradox</link>
            <guid>42863808</guid>
            <pubDate>Wed, 29 Jan 2025 11:34:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Jevons_paradox">https://en.wikipedia.org/wiki/Jevons_paradox</a>, See on <a href="https://news.ycombinator.com/item?id=42863808">Hacker News</a></p>
Couldn't get https://en.wikipedia.org/wiki/Jevons_paradox: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Bacteria (and Their Metabolites) and Depression (354 pts)]]></title>
            <link>https://www.science.org/content/blog-post/bacteria-and-their-metabolites-and-depression</link>
            <guid>42863262</guid>
            <pubDate>Wed, 29 Jan 2025 09:54:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/blog-post/bacteria-and-their-metabolites-and-depression">https://www.science.org/content/blog-post/bacteria-and-their-metabolites-and-depression</a>, See on <a href="https://news.ycombinator.com/item?id=42863262">Hacker News</a></p>
Couldn't get https://www.science.org/content/blog-post/bacteria-and-their-metabolites-and-depression: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[We got hit by an alarmingly well-prepared phish spammer (149 pts)]]></title>
            <link>https://utcc.utoronto.ca/~cks/space/blog/spam/WellPreparedPhishSpammer</link>
            <guid>42862873</guid>
            <pubDate>Wed, 29 Jan 2025 08:43:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://utcc.utoronto.ca/~cks/space/blog/spam/WellPreparedPhishSpammer">https://utcc.utoronto.ca/~cks/space/blog/spam/WellPreparedPhishSpammer</a>, See on <a href="https://news.ycombinator.com/item?id=42862873">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>We got hit by an alarmingly well-prepared phish spammer</h2>

	<p><small>January 28, 2025</small></p>
</div><div><p>Yesterday evening, <a href="https://support.cs.toronto.edu/">we</a> were hit
by a run of phish spam that I would call 'vaguely customized' for
us, for example the display name in the From: header was "U of T |
CS Dept" (but then the actual email address was that of the compromised
account elsewhere that was used to send the phish spam). The
destination addresses here weren't particularly well chosen, and
some of them didn't even exist. So far, so normal. One person here
fell for the phish spam that evening but realized it almost immediately
and promptly changed their password. Today that person got in touch with
us because they'd started receiving email bounces for (spam) email that
they hadn't sent. Investigation showed that the messages were being sent
through us, but in an alarmingly clever way.</p>

<p>We have a local VPN service for people, and this VPN service requires
a different password from your regular (Unix and IMAP and etc)
password. People connecting through our VPN have access to an
internal-only SMTP gateway machine that doesn't require SMTP
authentication. As far as we can tell, in the quite short interval
between when the person fell for the phish and then changed their
password, the phish spam attacker used the main password they'd
just stolen to register the person for our VPN and obtain a VPN
password (which we don't reset on Unix password changes). They then
connected to the VPN using their stolen credentials and used the
VPN to send spam email through our internal-only SMTP gateway
(initially last evening and then again today, at which point they
were detected).</p>

<p>Based on some log evidence, I think that the phish spammer first
tried to use authenticated SMTP but failed due to the password
change, then fell back on the VPN access. Even if VPN access hadn't
been their primary plan, they worked very fast to secure themselves
an additional access method. It seems extremely likely that the
attacker had already researched our mail and VPN environment before
they sent their initial phish spam, since they knew exactly where
to go and what to do.</p>

<p>If phish spammers are increasingly going to be this well prepared
and clever, we're going to have to be prepared for that on our side.
Until now, we hadn't really thought about the possibility of phish
spammers gaining VPN access; previous phish spammers have exploited
some combination of webmail and authenticated SMTP.</p>

<p>(We're also going to need to be more concerned about other methods
of obtaining persistent account access, such as adding new SSH
authorized keys to the Unix login. This attacker didn't attempt any
sort of SSH access.)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I still like Sublime Text in 2025 (896 pts)]]></title>
            <link>https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/</link>
            <guid>42862246</guid>
            <pubDate>Wed, 29 Jan 2025 06:43:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/">https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=42862246">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="ArticleBody text"><p>I still get people asking me why I use Sublime Text in 2025 given there are <em>soooo</em> many other great editors out there.</p><p>My response: there is? Because I still think Sublime Text holds up as a great editor.</p><p><em>Table Of Contents</em></p><ul><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#it-fast">It fast</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#lsp">LSP</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#snippets">Snippets</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#project-workspaces">Project workspaces</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#build-systems">Build systems</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#multiple-cursors">Multiple cursors</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#key-mouse-bindings">Key/mouse bindings</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#included-niceness">Included niceness</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#wish-list">Wish List</a></li></ul><p>I started with Sublime Text 2 back in 2010/2011 while I was in college. I mainly started using it because it was free, cross-platform, and came as a "portable app" that I could put on a USB and just use.</p><p>Back then, I had a really basic Toshiba laptop that dual booted Windows XP and Ubuntu (or maybe it was Mint?) so it was nice that it worked on both. I really liked how snappy it was compared to the tools our teacher suggested using. At that time it was Dreamweaver and maybe Notepad++.</p><p>Sublime, at that time, was pretty novel. It clearly took a lot of inspirations from <a rel="noopener nofollow noreferrer" href="https://github.com/textmate/textmate" target="_blank">TextMate</a>, another classic editor, considering that one is 4 years older than Sublime. It had multiple cursors, plugins, a build system. But the <a rel="noopener nofollow noreferrer" href="https://www.vendr.com/blog/consumer-dev-tools-command-palette" target="_blank">biggest claim to fame for Sublime, was the "command palette"</a>. I'm sure there is some other older app that had a precursor or similar feature to it, but generally speaking, it seems like that user experience pattern really kicked off with Sublime.</p><p>I built my web-dev chops on Sublime. The shortcuts are ingrained in my bones at this point. I'm not some key-combo-king, but I know a lot of the shortcuts that can help me get the UI and commands I need without thinking much at all.</p><blockquote><p>I have been, and continue to be, a Sublime user of about 15 years.</p></blockquote><p>So take all this with that in mind. I have been, and continue to be, a Sublime user of about 15 years.</p><p>So why do I keep using Sublime?</p><p>If you thought Sublime was dead, well you couldn't be more wrong! The latest build of Sublime as of this post is "4192" and was released 20th January 2025. So basically a week ago from this post. Not too bad.</p><p>It has regularly been updated with minor tweaks and fixes about a dozen times a year. I think the last major upgrade would be when Package Control (the plugin installer/manager) bumped to the next version which allowed plugins to install external dependencies.</p><p>You can nitpick here and say that Package Control is not part of Sublime. But most people won't use Sublime without it. So I am going to take some liberties and say it is part of it.</p><p>I think the thing to consider is how Sublime is basically "done" software. It has been around a <em>long</em> time. It was first released around 2008. It just passed it's 17-year anniversary actually. Congrats to them!</p><p>Before I dive into the details of <strong>why</strong> I still use it, consider this: if you are using a modern GUI-driven editor, it probably has taken inspiration from Sublime. So why not check out one of the OGs? You might find something you like.</p><p>Without further adieu, my reasons for still using Sublime in 2025:</p><h3 id="it-fast">It fast</h3><p>Sublime is fast. It starts instantly. Uses very few resources. Handles large files gracefully. Rarely crashes.</p><p>Nothing else to add here. A+ performance.</p><h3 id="lsp">LSP</h3><p><a rel="noopener nofollow noreferrer" href="https://github.com/sublimelsp" target="_blank">Sublime LSP</a> is really doing a lot for Sublime to keep it feeling modern and keeping up with other tools in the same class.</p><p>If you aren't aware of what an LSP is, this isn't the post to learn about it. But the gist is, it handles all that fancy code-aware completion and hover info you like from VS Code. If you want to learn more <a href="https://www.youtube.com/watch?v=LaS32vctfOY" rel="noopener nofollow noreferrer" target="_blank">give TJ 5 minutes to learn you</a>.</p><p>Some of the cool things about the Sublime LSP:</p><p><strong>Multiple servers per file</strong></p><p>You can enable as many LSP servers per file that you want. Restart them individually and configure them on a per-project basis (more on that later) which really helps bolster the capabilities of this already great editor.</p><p><strong>Detection on a scope level</strong></p><p>When configuring an LSP, outside of one installed with a plugin, you tell the LSP plugin which "scope" (think of this as an id for a syntax) to enable an LSP on.</p><p>Want your LSP to only turn on if you only open a file with a specific syntax? No problem. Want it to turn on only if a type of syntax is detected? Like a specific flavour of CSS? Sure. It is very configurable.</p><p><strong>Extensible configuration</strong></p><p>I know VS Code is the LSP king, which the tech originating with that editor, but I haven't seen the ability to just add an LSP installed as a binary in on your <code>usr/local/bin</code>.</p><p>There are a few "cutting-edge" LSPs that are installed via Cargo that are usually only targeting Neovim, but can easily be configured in Sublime with a simple JSON object.</p><p>Here is an example of configuring <a rel="noopener nofollow noreferrer" href="https://github.com/matkrin/md-lsp" target="_blank">md-lsp</a> (Markdown language server with support for GitHub flavored Markdown) in a few lines:</p><pre data-lang="json"><code data-lang="json"><span>// in the "LSP Settings" file, under "clients[]"
</span><span>"</span><span>md-lsp</span><span>": {
</span><span>  </span><span>// default enabled?
</span><span>  "</span><span>enabled</span><span>": </span><span>true</span><span>,
</span><span>  "</span><span>command</span><span>": [
</span><span>    </span><span>// as long as the system path is setup right, we can find this binary
</span><span>    "</span><span>md-lsp</span><span>",
</span><span>  ],
</span><span>  </span><span>// when we see this scope, the LSP will start this server
</span><span>  "</span><span>selector</span><span>": "</span><span>text.html.markdown</span><span>",
</span><span>},
</span></code></pre><p>It should be noted that md-lsp <strong>does not have a Sublime LSP plugin</strong> nor any mention of Sublime in their README. They only mention support for <em>Helix</em> and <em>Neovim</em>. Well, guess what? You support Sublime too!</p><h3 id="snippets">Snippets</h3><p>I write a lot of snippets. Right now, my snippets folder in Sublime has 123 snippets. The latest one was added <strong>today</strong>. It was a "TODO" snippet for Blade.</p><p>Sublime lets you create snippets from the <code>Tools &gt; Developer &gt; New Snippet</code> dropdown. They get sent to your "User" folder in the Sublime directory and are sourced on startup.</p><p><strong>Scope based</strong></p><p>Snippets are also scope-based. VS Code has scopes too, so there is nothing new here. I wonder where they got that from? 😮</p><p>A quick note on scopes: they can be very vague (like <code>source.txt</code>, targeting a whole <code>.txt</code> file) or super specific (like <code>text.html.basic.liquid text.html.basic meta.object.liquid</code> which targets a nested object in a liquid template) based on what you need.</p><p>I have found the Sublime scope integration to be straightforward to understand. A syntax defines scopes, and you can target those scopes in snippets, keybindings, and macros. More on those other two later.</p><p>I'm sure I didn't get the details perfect. But it doesn't really matter given the point I'm going to make: not all snippet systems work this way.</p><p>Specifically, I have found Helix, Neovim, and Zed snippets to be more based around "filetype" and not the scope of the where you are in the "syntax".</p><p>I'm sure this will change. Or perhaps I've missed something. From what I can see on the surface, snippets based on syntax-specific scopes seem to be the default in VS Code and Sublime.</p><p><strong>Tab stops with nesting, placeholders, and references</strong></p><p>Here is the snippet I made today:</p><pre data-lang="xml"><code data-lang="xml"><span>&lt;</span><span>snippet</span><span>&gt;
</span><span>    &lt;</span><span>content</span><span>&gt;&lt;![</span><span>CDATA</span><span>[
</span><span>&lt;!-- TODO: $1 --&gt;$0
</span><span>]]&gt;&lt;/</span><span>content</span><span>&gt;
</span><span>    &lt;</span><span>description</span><span>&gt;Blade todo comment&lt;/</span><span>description</span><span>&gt;
</span><span>    </span><span>&lt;!-- Optional: Set a tabTrigger to define how to trigger the snippet --&gt;
</span><span>    &lt;</span><span>tabTrigger</span><span>&gt;todo&lt;/</span><span>tabTrigger</span><span>&gt;
</span><span>    </span><span>&lt;!-- Optional: Set a scope to limit where the snippet will trigger --&gt;
</span><span>    &lt;</span><span>scope</span><span>&gt;text.html.blade&lt;/</span><span>scope</span><span>&gt;
</span><span>&lt;/</span><span>snippet</span><span>&gt;
</span></code></pre><p>Pretty simple. People who have written snippets before will recognize the syntax. The <code>$1</code> is where your caret is sent first, you can then type, then hit tab, and you get sent to <code>$0</code>.</p><p>Here is a TODO snippet I have for "javascript" files:</p><pre data-lang="xml"><code data-lang="xml"><span>&lt;</span><span>snippet</span><span>&gt;
</span><span>  &lt;</span><span>content</span><span>&gt;&lt;![</span><span>CDATA</span><span>[
</span><span>/** @todo ${1:this is my todo} */$0
</span><span>]]&gt;&lt;/</span><span>content</span><span>&gt;
</span><span>  </span><span>&lt;!-- Optional: Set a tabTrigger to define how to trigger the snippet --&gt;
</span><span>  &lt;</span><span>tabTrigger</span><span>&gt;todo&lt;/</span><span>tabTrigger</span><span>&gt;
</span><span>  &lt;</span><span>description</span><span>&gt;Insert a TODO JS comment&lt;/</span><span>description</span><span>&gt;
</span><span>  </span><span>&lt;!-- Optional: Set a scope to limit where the snippet will trigger --&gt;
</span><span>  &lt;</span><span>scope</span><span>&gt;source.js, source.ts, source.jsx meta.function.js meta.block.js meta.group.js meta.jsx.js meta.interpolation.js, source.tsx meta.function.js meta.block.js meta.group.js meta.jsx.js meta.interpolation.js&lt;/</span><span>scope</span><span>&gt;
</span><span>&lt;/</span><span>snippet</span><span>&gt;
</span></code></pre><p>Here you can see that the first tab stop has default content of "this is my todo". Here you can see a more complex scope setup that only expands this snippet under those conditions. Nothing really spectacular here.</p><p>But snippets in Sublime also support some transformations...</p><p><strong>Transformations (Vue component)</strong></p><p>Here is a much more complicated snippet:</p><pre data-lang="xml"><code data-lang="xml"><span>&lt;</span><span>snippet</span><span>&gt;
</span><span>  &lt;</span><span>content</span><span>&gt;&lt;![</span><span>CDATA</span><span>[
</span><span>&lt;!--
</span><span>  ${1:Namespace} Component
</span><span>  Usage:
</span><span>    &lt;${1/([a-zA-Z]+)(?:(\s+?)|\b)/\L\1(?2:\-)\E/g}&gt;&lt;/${1/([a-zA-Z]+)(?:(\s+?)|\b)/\L\1(?2:\-)\E/g}&gt;
</span><span>  ${2:Here is a description of my web component.}
</span><span>  @element ${3:div}
</span><span>  @fires change - This jsdoc tag makes it possible to document events.
</span><span>  @fires submit
</span><span>  @prop {String} title - You can use this jsdoc tag to document properties.
</span><span>  @slot - This is an unnamed slot (the default slot)
</span><span>  @slot start - This is a slot named "start".
</span><span>  @slot end
</span><span> --&gt;
</span><span>
</span><span>&lt;script setup lang="ts"&gt;
</span><span>import type { HTMLAttributes } from 'vue';
</span><span>export interface Props extends /* @vue-ignore */ HTMLAttributes {
</span><span>  ${4:title?: string;${5}}
</span><span>}
</span><span>
</span><span>withDefaults(defineProps&lt;Props&gt;(), {});
</span><span>&lt;/script&gt;
</span><span>
</span><span>&lt;template&gt;
</span><span>  &lt;${3:div} :key="${1/(\w+)(\W*)/\L\1\E(?2:\-)/g}" class="${1/(\w+)(\W*)/\L\1\E(?2:\-)/g}wrapper"&gt;
</span><span>    ${6:&lt;!-- content --&gt;}
</span><span>  &lt;/${3:div}&gt;
</span><span>&lt;/template&gt;
</span><span>]]&gt;&lt;/</span><span>content</span><span>&gt;
</span><span>  </span><span>&lt;!-- Optional: Set a tabTrigger to define how to trigger the snippet --&gt;
</span><span>  &lt;</span><span>tabTrigger</span><span>&gt;sfc&lt;/</span><span>tabTrigger</span><span>&gt;
</span><span>  </span><span>&lt;!-- Optional: Set a scope to limit where the snippet will trigger --&gt;
</span><span>  &lt;</span><span>scope</span><span>&gt;text.html.vue&lt;/</span><span>scope</span><span>&gt;
</span><span>  &lt;</span><span>description</span><span>&gt;Vue single-file component template&lt;/</span><span>description</span><span>&gt;
</span><span>&lt;/</span><span>snippet</span><span>&gt;
</span></code></pre><p>This snippet has some transformations in it. This means we can actually format what the content in the different tab stops will be.</p><p>I won't harp on what is going on too much. Just know that I can format the content in the "Usage" comment as <code>UpperCamelCase</code> and I can format the content in <code>:key</code> to be <code>lower-snake-case</code>. VS Code can do this. The syntax for it is a bit nicer. But I prefer authoring snippets in XML rather than JSON.</p><p>Obviously, I'm twisted.</p><h3 id="project-workspaces">Project workspaces</h3><p>Sublime supports the concept of workspaces under the banner of a "project". All without a plugin, by the way. You can open a folder and save that folder as a project.</p><p>This creates an empty <code>your-project-name.sublime-project</code> which you can really save wherever you like in your project as it has some features to target where the root of the project is.</p><p>This file is just a JSON file and contains editor settings that you are overriding for that specific project. You can target global settings, set rules for specific folders, create a build system, tweak/toggle LSP settings, etc. etc.</p><p>This is a lot like the <code>.vscode/settings.json</code> file from what I understand. I also believe you can do this in Vim with a <code>.vim</code> folder in the root of your project with a feature called "exrc". I haven't used it personally, so I can't speak to it much.</p><p>In my brief flirtations with Neovim and Helix, you need a plugin for this. In Zed, they also have a settings file that can be saved into a project root to get the same thing.</p><p><strong>Including/excluding files and folders</strong></p><p>I think all the editors I referenced above can do this. Not much to share. It is just nice to have an array of file configurations for a project that may or not be in that directory, can be matched with a glob, or just listed explicitly.</p><p>Here is an example of how I would use the project file in a Next.js site:</p><pre data-lang="json"><code data-lang="json"><span>{
</span><span>    "</span><span>folders</span><span>": [
</span><span>        {
</span><span>            "</span><span>file_exclude_patterns</span><span>": [
</span><span>                "</span><span>.gitkeep</span><span>",
</span><span>                "</span><span>*.min.*</span><span>",
</span><span>                "</span><span>*.snap</span><span>",
</span><span>                "</span><span>*.lock</span><span>",
</span><span>                "</span><span>*lock.json</span><span>"
</span><span>            ],
</span><span>            "</span><span>folder_exclude_patterns</span><span>": [
</span><span>                "</span><span>.sanity</span><span>",
</span><span>                "</span><span>.netlify</span><span>",
</span><span>                "</span><span>.next</span><span>",
</span><span>                "</span><span>.vercel</span><span>",
</span><span>                "</span><span>.cache</span><span>",
</span><span>                "</span><span>out</span><span>",
</span><span>                "</span><span>dist</span><span>",
</span><span>                "</span><span>node_modules</span><span>"
</span><span>            ],
</span><span>            "</span><span>path</span><span>": "</span><span>.</span><span>"
</span><span>        }
</span><span>    ],
</span><span>    "</span><span>build_systems</span><span>": [
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Build</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>pnpm run build</span><span>"
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Test</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>pnpm run test</span><span>"
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Open Test</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>find tests -print | grep $file_base_name.spec | sed -n 1p | xargs subl</span><span>
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Test Snapshots</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>pnpm run test:snapshots</span><span>"
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Test File</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>pnpm run test -- $file_base_name.spec</span><span>"
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Test File Snapshot</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>pnpm run test:snapshots -- $file_base_name.spec</span><span>"
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Format README.md</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>mdformat $file</span><span>"
</span><span>        }
</span><span>    ],
</span><span>    "</span><span>settings</span><span>": {
</span><span>        "</span><span>match_brackets_angle</span><span>": </span><span>true</span><span>,
</span><span>        "</span><span>tab_size</span><span>": </span><span>2</span><span>,
</span><span>        "</span><span>translate_tabs_to_spaces</span><span>": </span><span>true</span><span>,
</span><span>        "</span><span>jsdocs_return_tag</span><span>": "</span><span>@return</span><span>",
</span><span>        "</span><span>lsp_format_on_save</span><span>": </span><span>false</span><span>,
</span><span>        "</span><span>lsp_code_actions_on_save</span><span>": {
</span><span>            "</span><span>source.organizeImports</span><span>": </span><span>false</span><span>,
</span><span>            "</span><span>source.fixAll.eslint</span><span>": </span><span>true
</span><span>        },
</span><span>        "</span><span>LSP</span><span>": {
</span><span>            "</span><span>formatters</span><span>": {
</span><span>                "</span><span>source.ts</span><span>": "</span><span>LSP-biome</span><span>",
</span><span>                "</span><span>source.js</span><span>": "</span><span>LSP-biome</span><span>",
</span><span>                "</span><span>source.tsx</span><span>": "</span><span>LSP-biome</span><span>",
</span><span>                "</span><span>source.jsx</span><span>": "</span><span>LSP-biome</span><span>",
</span><span>                "</span><span>text.html.basic</span><span>": "</span><span>LSP-biome</span><span>"
</span><span>            },
</span><span>            "</span><span>LSP-eslint</span><span>": {
</span><span>                "</span><span>enabled</span><span>": </span><span>false
</span><span>            },
</span><span>            "</span><span>LSP-biome</span><span>": {
</span><span>                "</span><span>enabled</span><span>": </span><span>true
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre><p>I'm doing a lot here. Setting files to ignore, folders to exclude from indexing, setting build commands that use <code>pnpm</code> as well as <code>mdformat</code>, setting from editor setting for tab spacing, and finally a few LSP tweaks that make sense for a Next project.</p><p>VS Code can do all of what I've listed here. But they split it up into different files. Kinda annoying.</p><p><strong>Configure plugin settings per project</strong></p><p>You can also configure plugin settings per project. Here is how I would add project settings for the "syntax override" plugin. This plugin forces the editor to use a specific syntax for certain files that match a given pattern:</p><pre data-lang="json"><code data-lang="json"><span>{
</span><span>    </span><span>// everything is the same as above but this is added on the end
</span><span>    "</span><span>syntax_override</span><span>": {
</span><span>        "</span><span>\\</span><span>.env.*$</span><span>": [
</span><span>            "</span><span>ShellScript</span><span>",
</span><span>            "</span><span>Shell-Unix-Generic</span><span>"
</span><span>        ],
</span><span>        "</span><span>\\</span><span>.*rc$</span><span>": [
</span><span>            "</span><span>JSON</span><span>",
</span><span>            "</span><span>JSON</span><span>"
</span><span>        ],
</span><span>        "</span><span>\\</span><span>.ts.snap$</span><span>": [
</span><span>            "</span><span>JavaScript</span><span>",
</span><span>            "</span><span>TypeScript</span><span>"
</span><span>        ],
</span><span>        "</span><span>\\</span><span>.css$</span><span>": [
</span><span>            "</span><span>Tailwind CSS</span><span>",
</span><span>            "</span><span>Tailwind CSS</span><span>"
</span><span>        ]
</span><span>    }
</span><span>}
</span></code></pre><p>I don't always want all <code>.css</code> files to be highlighted with the Tailwind syntax. But in this project I do. So I can set it locally here, and when I open a <code>.css</code> file in this project, it will switch the syntax for me. Nice!</p><p><strong>Add build systems per project</strong></p><p>You can see in the example above that I have set build systems on this specific project. You can do this in VS Code with "tasks". Zed has this feature as well and calls it "tasks" too. I just find it annoying that they are in their own files. I guess that makes them more portable.</p><p>I just like my project configuration <em>in one central place</em>. I must be nuts.</p><h3 id="build-systems">Build systems</h3><p>I've touched on build systems a bit already. But in summation, they are just tasks you can run in your project.</p><p>Sometimes they call a global command (like <code>curl</code>), sometimes a local dependency is installed with a package manager (think some binary in <em>node_modules/.bin/</em>), or maybe just runs a command already setup in your other tools (like <code>composer run test</code> or <code>php artisan migrate:fresh</code>) that run in the root of the project but need some context.</p><p><strong>Can also be provided by a plugin</strong></p><p>A plugin can provide build systems. The neat thing is they are just Sublime files. JSON that ends in <code>.sublime-build</code>. Like snippets. So they are really portable too. Just like the other editors with their <code>tasks.json</code> file.</p><p><strong>Just a simple file</strong></p><p>Here is one I saved in my "User" directory called <code>dot-env-linter.sublime-build</code>:</p><pre data-lang="json"><code data-lang="json"><span>{
</span><span>  "</span><span>cmd</span><span>": ["</span><span>dotenv-linter</span><span>", "</span><span>$file</span><span>"],
</span><span>  "</span><span>selector</span><span>": "</span><span>source.shell</span><span>",
</span><span>  "</span><span>file_patterns</span><span>": ["</span><span>\\</span><span>.env.*$</span><span>"],
</span><span>  "</span><span>file_regex</span><span>": "</span><span>^(.*?)</span><span>\\</span><span>:(</span><span>\\</span><span>d+)(</span><span>\\</span><span>s)(.*+)</span><span>"
</span><span>}
</span></code></pre><p>You can see there are some special variables (like <code>$file</code>) that will expand based on context. In this case, that one is the full path to the currently active file.</p><p>Here is my <code>phpmd.sublime-build</code> with a bit more flavour:</p><pre data-lang="json"><code data-lang="json"><span>{
</span><span>  "</span><span>cmd</span><span>": ["</span><span>phpmd</span><span>", "</span><span>${file}</span><span>", "</span><span>text</span><span>", "</span><span>codesize,unusedcode,naming</span><span>"],
</span><span>  "</span><span>path</span><span>": "</span><span>${PATH}</span><span>",
</span><span>  "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>  "</span><span>selector</span><span>": "</span><span>source.php</span><span>"
</span><span>}
</span></code></pre><p>You can see some more vars here for the project path as well as a reference to my system PATH. Of course, we got a nice lil scope as well.</p><p><strong>Build systems with 🐍</strong></p><p><a rel="noopener nofollow noreferrer" href="https://www.sublimetext.com/docs/build_systems.html#advanced-example" target="_blank">You can actually write build systems in Python as well</a>. So if you need something more complicated, you can reach for that.</p><p>You don't even need to make any semblance of a plugin. You can toss a <code>.py</code> file in your "User" directory and implement a class that takes a <code>sublime_plugin.WindowCommand</code>.</p><h3 id="multiple-cursors">Multiple cursors</h3><p>Yep. Multiple cursors. I use them all the time. I know that the "vim" way is to start recording a macro, apply the changes on a single line, and then replay that macro on all the lines you want to change. Or do some <code>s//g</code> fu for a fancy find and replace. I get it. I just don't like it.</p><p>Most of the editors these days have multiple cursors. Including some terminal editors like Helix. I have tried Helix and I think it is a lot closer to what I would want from a modern editor than my previous terminal editor of Neovim + LazyVim.</p><h3 id="key-mouse-bindings">Key/mouse bindings</h3><p>The key and mouse bindings are what you would expect from a modern editor. It is basically the same as VS Code. Nothing exhilarating here. But I do like the way conceptual key bindings are handled.</p><p><strong>Contextual key bindings</strong></p><p>Like any good editor, Sublime supports contextual key bindings.</p><p>I think the best usage I have seen for contextual key bindings is in <a rel="noopener nofollow noreferrer" href="https://github.com/sublimehq/Packages/blob/master/JavaScript/Default.sublime-keymap#L11-L17" target="_blank">the JavaScript language support package</a> that comes with Sublime.</p><p>When you have an active selection, and that selection is not empty, and you are inside a string-like scope, then ` will wrap that selection with `. Basically, it will wrap your selection as a template string. Handy!</p><p><strong>Just a simple file</strong></p><p>Like the build systems and snippets, key bindings are just saved in a file that ends with <code>.sublime-keymap</code>. They can be in your "User" directory or in a plugin. Unfortunately, unlike build systems, they cannot be saved on a per-project basis - from what I can tell.</p><p>You can also have key bindings for different platforms. They are named as follows:</p><pre><code><span>Default (Windows).sublime-keymap
</span><span>Default (OSX).sublime-keymap
</span><span>Default (Linux).sublime-keymap
</span></code></pre><h3 id="included-niceness">Included niceness</h3><p>These are just some notable mentions of things I like:</p><p><strong>Python all the way down</strong></p><p>Given how Python is probably the most popular language, at least <a rel="noopener nofollow noreferrer" href="https://www.theregister.com/2024/11/05/python_dethrones_javascript_github/" target="_blank">the last time GitHub checked</a>, I'm surprised this isn't the go-to editor. I don't think you need to know python to use Sublime but it helps if you want to craft a nice plugin.</p><p>One more thing to add around authoring plugins, they are super simple. It is just a <code>.py</code> file in a folder. No build system, external dependencies, or ever-changing APIs to navigate.</p><p>You can see <a rel="noopener nofollow noreferrer" href="https://github.com/james2doyle/sublime_scratchpad/blob/master/Scratchpad.py" target="_blank">my recent fork of the Scratchpad plugin</a> and how the whole thing is mostly powered by a single python file.</p><p>This plugin has been trucking along for 11 years. I can't imagine any VS Code plugin lasting that long!</p><p><strong>Macros</strong></p><p>Yep you can record macros in Sublime. You can also save them to... a file! Then put that file (<code>.sublime-macro</code>) in a plugin, or, of course, your "User" folder. The macro is just an array of key presses. They also support scopes and can be bound to a key combo. Pretty sweet.</p><p>This is one feature that VS Code has not stolen - I mean implemented - and that requires an additional plugin to have. Of course the Vimmers have had this for decades.</p><p><strong>Diff hunks (revert or show)</strong></p><p>Sublime supports viewing inline diff hunks. Handy for when you don't want to dive into the diff of a file. You can just ask to see the diff hunk for that line or group of lines. You can also revert just as easily.</p><p><strong>Case conversion and line permute functions</strong></p><p>There are some handy case conversion functions that are built in. Nice with multiple cursors. VS Code has a couple of conversion choices. Sublime has 8 different case conversions built in.</p><p><strong>Package control and repo URLs for packages</strong></p><p>Package control packages can be installed from the central repository. But you can also install them from a git repo URL. You can also clone a repo to your "Packages" folder, and it will work too. No marketplace or anything like that is required.</p><p>This is really handy if you have forked a package and just want to install your fork.</p><p><strong>All the config and settings are plain files</strong></p><p>Since the whole of a Sublime setup is mostly plain files, that makes it really easy to sync your setup across multiple computers. I actually symlink my Sublime folder to Dropbox. So any changes I make will be shared across all my computers that use it.</p><p>This isn't unique to Sublime. I think it is just a benefit of having tools that use plain text-driven configuration.</p><p><strong>Distraction-free mode</strong></p><p>Sublime has a "distraction free mode" which will full-screen your editor and focus the content to the middle of the screen. I am using it to write this post right now!</p><h3 id="wish-list">Wish List</h3><p>Of course there could be a few things that could be better.</p><p><strong>Better docs</strong></p><p>I do find the docs for developing plugins to be sparse. There are doc sites. There are two big ones. One for the "official" docs that document the APIs. There is also another site that is tagged as the "unofficial" docs.</p><p>Usually when I want to know how to do something in a plugin, I just read the source of another plugin that does something I want to emulate. It isn't a bad way to learn, it is just a bit tedious.</p><p><strong>Plugin development DX</strong></p><p>Speaking of building plugins, there does not seem to be any "stubs" for the Sublime python API. I am by no means a python guru. I only use it in Sublime, and I usually forget everything once I finish what I am trying to do. But I think there could be some "plugin starter template" that could include a Pyright setup and some basic guide for how to get going.</p><p>I also find that getting plugins on the Package Control site to be quite a chore. You need to open a PR to a repo and put your repo into a list based on where it goes into an alphabet. It doesn't allow forks and plugins that are too similar. There is also no way to mark a package as "abandoned".</p><p>I like the way Composer/Packagist does packages. You create a repo, submit that URL to the Packagist site, and it will automatically keep track of it for you. You release a new version by just using git tags and releases.</p><p>NPM is also a bit nicer. But forcing an NPM account and having to juggle the mixing of git tags and the <code>package.json</code> "version" key to be a bit unclear at times.</p><p><strong>Key/Mouse bindings per project</strong></p><p>Simple one here. It would be nice to have key and mouse bindings on a project level. I don't know how often I would really use it, but it would be nice to have just for those projects that have tedious tasks or macros that I want to run under specific scopes.</p><h2 id="in-summation">In Summation</h2><p>I like Sublime. I think it is still incredibly capable in 2025. If you are in search of something a little snappier, classier, and not riddled with AI slop, then give it a try.</p><p>I doubt I can pry your Vim from your <code>HJKL</code> riddled right hand, but if you have been let down or uninspired by the latest offerings when it comes to editors, you might find Sublime still has a lot to offer.</p><p><em>Note: if any of my information is wrong or outdated, I will update it accordingly</em></p></div></div>]]></description>
        </item>
    </channel>
</rss>