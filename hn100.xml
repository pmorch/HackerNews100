<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 06 Apr 2025 17:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Self-Driving Teslas Are Fatally Rear-Ending Motorcyclists More Than Any Other (281 pts)]]></title>
            <link>https://fuelarc.com/news-and-features/self-driving-teslas-are-fatally-striking-motorcyclists-more-than-any-other-brand-new-analysis/</link>
            <guid>43601421</guid>
            <pubDate>Sun, 06 Apr 2025 13:51:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fuelarc.com/news-and-features/self-driving-teslas-are-fatally-striking-motorcyclists-more-than-any-other-brand-new-analysis/">https://fuelarc.com/news-and-features/self-driving-teslas-are-fatally-striking-motorcyclists-more-than-any-other-brand-new-analysis/</a>, See on <a href="https://news.ycombinator.com/item?id=43601421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="27ad23c5" data-element_type="widget" data-widget_type="theme-post-content.default">
					
<p>Among motorcyclists, there is a <a href="https://www.rideapart.com/news/608732/why-tesla-autopilot-ignores-bikes/">persistent rumor</a> that Teslas are dangerous to ride around in traffic. Whether it’s their silent electric drivetrain, extreme acceleration, or self-driving technology supposedly failing to see motorcycles, every biker seems to know someone who’s had a close call with a Tesla.</p>



<p>Our new analysis of data from the NHTSA on self-driving crashes proves this rumor to be true. Since 2022, Teslas in self-driving mode have been involved in five fatal motorcycle crashes (or more).</p>



<p>It’s not just that self-driving cars in general are dangerous for motorcycles, either: this problem is unique to Tesla. Not a single other automobile manufacturer or ADAS self-driving technology provider reported a single motorcycle fatality in the same time frame.</p>



<p>So, why are Teslas involved in self-driving crashes that kill motorcyclists? Let’s interrogate the data and get to the bottom of it.</p>



<h2><strong>TL;DR: Self-Driving Teslas Rear-End Motorcyclists, Killing at Least 5</strong></h2>



<p>Brevity is the spirit of wit, and I am just not that witty. This is a long article, here is the gist of it:</p>



<ul>
<li>The NHTSA’s self-driving crash data reveals that Tesla’s self-driving technology is, by far, the most dangerous for motorcyclists, with five fatal crashes <em>that we know of</em>.</li>



<li>This issue is unique to Tesla. Other self-driving manufacturers have logged zero motorcycle fatalities in the same time frame.</li>



<li>The crashes are overwhelmingly Teslas rear-ending motorcyclists.</li>
</ul>



<h2><strong>Self-Driving Teslas Are Fatally Striking Motorcyclists From Behind</strong></h2>



<p>I learned to ride a motorcycle on a military base, and the sergeant was brutally honest.</p>



<p>He said: <strong>“If you’re going to die on the bike, it’s going to be your own fault.”</strong></p>



<p>Welcome to being a motorcyclist. Our margin of error is slim, and mistakes are easily fatal. It’s rough out here on two wheels. Helmet and jacket, <em>strongly encouraged.</em></p>



<p>Rider error, while tragic, is a fact of life. So the first item to rule out is whether the motorcyclists themselves are causing these accidents.</p>



<p><strong>The good news:</strong> we can largely rule out rider error in these crashes. The NHTSA reports the crash location on the self-driving vehicle, so we can tell whether the Tesla was t-boned or rear-ended.</p>



<p>That’s not the case for the self-driving Tesla motorcycle fatalities reported to the NHTSA.</p>



<p>According to the <a href="https://static.nhtsa.gov/odi/ffdd/sgo-2021-01/SGO-2021-01_Incident_Reports_ADAS.csv">NHTSA data</a>:</p>



<ul>
<li>3 out of 5 cases show the Tesla sustained front-end impacts, or front and left-side impacts, or (grimly) front and bottom impacts.</li>



<li>2 out of 5 cases had crash direction marked as “unknown” in the manufacturer’s reporting to the government.</li>
</ul>



<p>Those two “unknown” cases are both in Florida. One of them, in Boca Raton, was a Tesla rear-ending a motorcycle stopping at a traffic light. The final Florida case has fewer details available.</p>



<p>That means that, in 100% of Tesla’s fatal self-driving motorcycle crashes where we know the direction of travel, the Tesla was behind the motorcyclist before impact. The self-driving Teslas are fatally striking the motorcyclists from behind.</p>



<h2>We’d Love to Tell You More, But Tesla’s Self-Driving Data Is Redacted</h2>



<p>It would be helpful to have the official crash narratives for context. The NHTSA requires that automobile manufacturers and self-driving technology providers offer that context in their governmental crash reporting.</p>



<p>Tesla is unique in that they request all of their narrative data be redacted, along with many other data points, citing “confidential business information” or “personally identifiable information” about the victim.</p>



<p><strong>Learn more about that here:</strong> <a href="https://fuelarc.com/tech/redacted-tesla-asked-for-redactions-on-every-self-driving-crash-reported-to-the-nhtsa-since-2021/">Tesla Asked for Redactions on Every Self-Driving Crash Reported to the NHTSA Since 2021</a></p>



<p>This limits journalists’ ability to see the full picture behind these crashes and what might be causing them.</p>



<p>Speaking of investigations… why did the Department of Government Efficiency (DOGE) <a href="https://www.reuters.com/world/us/doge-cuts-nearly-half-unit-overseeing-autonomous-vehicles-safety-washington-post-2025-02-21/">cut the NHTSA’s self-driving safety office staff by almost half</a>? Why cut back on the small office capable of investigating Tesla’s unredacted safety data? Seems like an odd place to start cutting costs.</p>



<p>Anyway, here is how we constructed our study.</p>



<h3><strong>A Note on Methodology</strong></h3>



<p>To be included in the NHTSA’s self-driving crash reporting, known as the “<a href="https://www.nhtsa.gov/laws-regulations/standing-general-order-crash-reporting">Standing General Order,</a>” the car’s self-driving technology must have been active within 30 seconds of the crash.</p>



<p>We don’t have access to internal Tesla data, which could tell us whether self-driving was still active at the moment of impact. We can only infer from what we do have. </p>



<p>This is good and bad: bad in that it is not as granular as we would like, but good in that it is standardized. Every manufacturer is held to the same standard.</p>



<p>For the rest, we’ve tracked down each crash in local news reporting. Read those narratives below.</p>



<h2><strong>Filling in Tesla’s Self-Driving Crash Redactions with Press Coverage</strong></h2>



<p>It’s hard to hide the truth in the 21st century. We have city names and months for each fatal motorcycle crash involving a Tesla in self-driving mode. Most of the other significant data is redacted at Tesla Inc’s request, but I’ve done more with less. That’s enough to start connecting the dots with public news reports and police reports.</p>



<p>Below, I’ve linked publicly available reporting for each case to support our conclusions above. The details are distressing, so if you’d rather avoid reading about fatal car accidents, <a href="#engineering">skip ahead to my conclusions</a>.</p>



<h3>April 2024, Snohomish WA:</h3>



<p><strong>NHTSA Report ID 13781-7567.</strong></p>



<p>The self-driving subject vehicle is a 2022 Tesla Model S. The Tesla fatally struck a motorcyclist from behind in low-speed stop and go traffic on a highway near Seattle.</p>



<p>The driver reports that he was looking at his cellphone with his Tesla’s Full-Self Driving mode engaged. Allegedly, the driver heard a bang, his Tesla suddenly accelerated, and he struck the unmoving motorcyclist in front of him from behind, pinning the motorcyclist under the Tesla.</p>



<p><a href="https://www.cnbc.com/2024/07/31/tesla-in-crash-that-killed-motorcyclist-was-using-self-driving-system-authorities-say.html">https://www.cnbc.com/2024/07/31/tesla-in-crash-that-killed-motorcyclist-was-using-self-driving-system-authorities-say.html</a></p>



<p>Interestingly, the Washington State Highway Patrol determined that Full-Self Driving was active not from a governmental collaboration with the NHTSA or similar, but by downloading the data directly from the Tesla’s computer after seeking a warrant.</p>



<p>The driver was arrested for vehicular homicide, due to being distracted by his cellphone. Documents here with witness statements in the police reports:</p>



<p><a href="https://www.opb.org/article/2025/01/15/tesla-may-face-less-accountability-for-crashes-under-trump">https://www.opb.org/article/2025/01/15/tesla-may-face-less-accountability-for-crashes-under-trump</a></p>



<p>Of further interest, the Tesla feature to brake after a crash has occurred (automatic emergency braking) can be disengaged by pressing the accelerator pedal. The Tesla’s onboard data reports that the accelerator pedal was pressed 95-100% continuously for 10 seconds after the collision. A witness reports that the Tesla’s front wheels were spinning while up in the air, which means that no brake was being applied, even after the impact where the motorcyclist was pinned beneath the vehicle.</p>



<p>This is a tragically easy mistake to make, for a driver dazed and confused in a crash situation. It’s amazing that no technical system enforces braking in a more difficult-to-override way when sensors detect a crash has occurred.</p>



<h3>August 2023, Green Cove Springs FL:</h3>



<p><strong>NHTSA Report ID 13781-6205.</strong></p>



<p>The self-driving subject vehicle is a 2023 Tesla Model Y. Details are extremely scant, just what is unredacted in the NHTSA reporting. I really wish we had access to the unredacted data!</p>



<p>This one, I do not include in our fatality totals. The injury severity was reported to the NHTSA as “unknown.” The airbags on the Tesla deployed during a crash involving a motorcycle, which is consistent with serious or fatal injuries to motorcycle riders, but we just can’t know for sure with the data redactions.</p>



<p>I couldn’t find any local news coverage for the crash in rural Florida in several hours of trying. If anybody knows more about this case, please let me know on the <a href="https://fuelarc.com/contact/">Contact</a> page!</p>



<p>I include this case to point out: there are countless Tesla self-driving crashes where the data is so heavily redacted that it is simply not possible to tell what is going on. That’s why I say there are “at least 5” self-driving Tesla accidents that have killed motorcyclists, I highly suspect that some of those “unknown” or redacted data points are motorcycle crashes.</p>



<h3>August 2022, Boca Raton FL:</h3>



<p><strong>NHTSA Report ID 13781-3713.</strong></p>



<p>The self-driving subject vehicle is a 2020 Tesla Model 3. The Tesla fatally struck a motorcyclist from behind at above 100MPH on a 45MPH speed limit road in suburban Boca Raton. The motorcyclist was properly stopping at a yellow light, according to police reports.</p>



<p><a href="https://www.palmbeachpost.com/story/news/crime/2025/02/12/boca-raton-attorney-richard-dorfman-who-killed-motorcyclist-ingrid-noon-tesla-gets-2-year-plea-deal/78411624007">https://www.palmbeachpost.com/story/news/crime/2025/02/12/boca-raton-attorney-richard-dorfman-who-killed-motorcyclist-ingrid-noon-tesla-gets-2-year-plea-deal/78411624007</a></p>



<p>The driver was arrested for vehicular homicide and DUI manslaughter charges just under a year after the fatal crash. He plead guilty, reducing a potential sentence of 30 years to just 2 years in prison. In his guilty plea, he specified that he was relying on Tesla’s self-driving technology while intoxicated.</p>



<p><a href="https://www.cnn.com/2022/10/17/business/tesla-motorcycle-crashes-autopilot/index.html">https://www.cnn.com/2022/10/17/business/tesla-motorcycle-crashes-autopilot/index.html</a></p>



<p>You might wonder how a vehicle with self-driving technology was traveling more than double the speed limit into a yellow light with a motorcyclist braking in front of them. This <a href="https://www.reddit.com/r/teslamotors/comments/p6m2q9/advanced_autopilot_question_what_does_the/">Reddit thread explains the feature</a>. </p>



<p>Certain Tesla self-driving technologies are speed capped, but others are not. Simply pressing the accelerator will raise your speed in certain modes, and as we saw in the police filings from the Washington State case, pressing the accelerator also cancels emergency braking. </p>



<p>That’s how you would strike a motorcyclist at such extreme speed, simply press the accelerator and all other inputs are apparently overridden.</p>



<h3>July 2022, Bluffdale UT:</h3>



<p><strong>NHTSA Report ID 13781-3488.</strong></p>



<p>The self-driving subject vehicle is a 2020 Tesla Model 3. The Tesla fatally struck a motorcyclist from behind on a Utah highway. Final speed of the Tesla prior to crashing into the motorcyclist was reported to the NHTSA was 61MPH.</p>



<p><a href="https://kjzz.com/news/local/motorcyclist-dies-after-being-hit-by-tesla-reportedly-on-autopilot">https://kjzz.com/news/local/motorcyclist-dies-after-being-hit-by-tesla-reportedly-on-autopilot</a></p>



<p>The driver reports that they did not see the motorcyclist, which will sound immediately familiar to any of my readers who are motorcyclists. As is typical in fatal motorcycle crashes with no exacerbating human factors like alcohol or distraction, we could find no record that charges were filed.</p>



<p>This accident was probed by the NHTSA’s investigation teams.</p>



<p><a href="https://www.insurancejournal.com/news/midwest/2022/08/08/679062.htm">https://www.insurancejournal.com/news/midwest/2022/08/08/679062.htm</a></p>



<h3>April 2022, reported to the NHTSA as an unknown city in Florida:</h3>



<p><strong>NHTSA Report ID 13781-3470.</strong></p>



<p>The self-driving subject vehicle is a 2020 Tesla Model Y.</p>



<p>I can find literally zero news reporting on this fatal accident involving a Tesla self-driving vehicle, so we need to rely on the NHTSA data to assemble the narrative. It’s starting to become apparent why this data is so heavily redacted, connecting the dots is difficult!</p>



<p>The Tesla was traveling straight on a highway when it was involved in a fatal crash with a motorcycle. No further details are available. This is the only motorcycle fatality reported to the NHTSA where we are not certain that they were rear-ended by the Tesla.</p>



<p>The address of the crash was redacted as “[MAY CONTAIN PERSONALLY IDENTIFIABLE INFORMATION]” which limits our ability to locate the narrative details.</p>



<h3>July 2022, Riverside CA:</h3>



<p><strong>NHTSA Report ID 13781-3332.</strong></p>



<p>The self-driving subject vehicle is a 2021 Tesla Model Y. The Tesla fatally struck a motorcyclist from behind in the HOV lane on a California freeway.</p>



<p><a href="https://techxplore.com/news/2022-08-agency-probes-tesla-motorcyclists.html">https://techxplore.com/news/2022-08-agency-probes-tesla-motorcyclists.html</a></p>



<p>This is an alarmingly common trend in our analysis. Not an edge-case, no difficult weather or lighting conditions, simply a self-driving Tesla killing a motorcyclist by rear-ending them at speed.</p>



<p>This accident was probed by the NHTSA’s investigation teams.</p>



<p><a href="https://www.insurancejournal.com/news/midwest/2022/08/08/679062.htm">https://www.insurancejournal.com/news/midwest/2022/08/08/679062.htm</a></p>







<h2>Engineering Failures and Human Error Cascade to Cause Fatal Self-Driving Accidents</h2>



<p>Self-driving Teslas developed such a reputation for ramming into parked emergency vehicles from behind that the <a href="https://static.nhtsa.gov/odi/rcl/2023/RCLRPT-23V838-8276.PDF">NHTSA launched an investigation in 2022</a>. Confusingly, the NHTSA is <em>currently </em>investigating whether the subsequent recall was effective. What the agency uncovered through close analysis of the unredacted self-driving crash data went far beyond firetrucks and police cruisers.</p>



<p>As part of its review, the Office of Defects Investigation (ODI) at the NHTSA found that Teslas had a general tendency to experience “frontal plane crashes.” Essentially, driving straight into obstacles in their path. </p>



<p>The issue wasn’t limited to emergency vehicles. Teslas using self-driving software were crashing into all sorts of things: barriers, stopped cars, <em>and motorcycles</em>.</p>



<p>The NHTSA extensively reviewed Tesla’s self-driving crash data, which it has unredacted access to. The findings? A vast majority of these crashes occurred in situations where a visible hazard should have been detected and avoided well before impact.</p>



<h4>Self-Driving Teslas Have Plenty of Time to Avoid Frontal-Plane Crashes… They Just Don’t</h4>



<p>Here’s a telling table from <a href="https://static.nhtsa.gov/odi/rcl/2023/RCLRPT-23V838-8276.PDF">the NHTSA report</a>, showing that <strong>fully 93% of hazards</strong> in crashes would have been visible to a human in time to either avoid the accident entirely or at least take action to mitigate the severity of the accident:</p>



<figure><img fetchpriority="high" decoding="async" width="624" height="207" src="https://fuelarc.com/wp-content/uploads/2025/04/frontal-plane-visible-hazard-table.png" alt="" srcset="https://fuelarc.com/wp-content/uploads/2025/04/frontal-plane-visible-hazard-table.png 624w, https://fuelarc.com/wp-content/uploads/2025/04/frontal-plane-visible-hazard-table-300x100.png 300w" sizes="(max-width: 624px) 100vw, 624px"></figure>



<p>The government’s report has this damning quote, couched in dense bureaucratic language:</p>



<blockquote>
<p>“In certain circumstances when Autosteer is engaged, if a driver misuses the SAE Level 2 advanced driver-assistance feature such that they fail to maintain continuous and sustained responsibility for vehicle operation and are unprepared to intervene, fail to recognize when the feature is canceled or not engaged, and/or fail to recognize when the feature is operating in situations where its functionality may be limited, there may be an increased risk of a collision.”</p>
</blockquote>



<p>In the case of the motorcycle crashes, the pattern is clear: Tesla’s self-driving technology either fails to detect the motorcyclist or does not apply the brakes once the motorcycle is detected. The human driver, meanwhile, is responsible for stepping in and braking manually… a task they fail to complete, because they are not paying attention, a situation which the Tesla’s attention detection system has also failed to recognize. When both the technology and the human backup fail, the result is a fatal collision.</p>



<p>This is a natural <em>and enragingly obvious</em> outcropping of Level 2 self-driving technology. <em>Of course</em> drivers will get bored of watching their car drive itself for hours on end. Simply having a hand on the wheel is not a sufficient measure of the driver paying attention, and Tesla’s other technical measures apparently do not work reliably. Motorcyclists pay for that stupid design decision with their lives.</p>



<h2>Watch a Tesla Freak Out in FSD Mode Behind a Motorcyclist</h2>



<p>Sometimes, seeing the technology in action helps illustrate what’s going wrong. In this video, a Tesla running a recent version of its <em>unfortunately named </em>“Full Self-Driving” (FSD) software behaves erratically when following a motorcycle.</p>



<figure><p>
<iframe title="Did Tesla FSD v12.3.6 Forget How to Drive Behind a Motorcycle?! Very Weird..." width="800" height="450" src="https://www.youtube.com/embed/0G6dofV3Jh8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p><a href="https://www.youtube.com/watch?v=0G6dofV3Jh8" target="_blank" rel="noreferrer noopener">Watch the clip here.</a></p>



<p>The Tesla appears to struggle with maintaining a consistent follow distance. At times, it leaves an absurdly long gap between itself and the motorbike, which could be an indication that the system is compensating for weak sensor performance when detecting range on motorcycles.</p>



<p>The video creator even calls attention to the odd behavior, and the comment section is full of people reporting similar experiences… along with the usual Tesla defenders who downplay or dismiss the issue, or attack the video as a fraud. Whether those defenders are diehard fans, bots, or a mix of both is up for debate.</p>



<p>The bottom line? Tesla’s self-driving technology has serious issues with detecting and responding safely to motorcycles. When the system struggles, it leaves human drivers to pick up the slack when they have been lulled into inattention by a false sense of competency from the self-driving system. That’s a deadly combination, as evidenced by five (or more) fatal Tesla collisions with motorcyclists.</p>



<p>Of course, all of us are watching Tesla’s planned rollout of self-driving taxis <em>with no human driver</em>, <a href="https://x.com/elonmusk/status/1906727530607808546">slated for Austin in less than 60 days</a>. If you’re a motorcycle rider like me, you might avoid the city limits of Austin TX for a minute, just to see how things shake out.</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How the Atlantic's Jeffrey Goldberg Got Added to the White House Signal Chat (110 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2025/apr/06/signal-group-chat-leak-how-it-happened</link>
            <guid>43601213</guid>
            <pubDate>Sun, 06 Apr 2025 13:12:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2025/apr/06/signal-group-chat-leak-how-it-happened">https://www.theguardian.com/us-news/2025/apr/06/signal-group-chat-leak-how-it-happened</a>, See on <a href="https://news.ycombinator.com/item?id=43601213">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><a href="https://www.theguardian.com/us-news/donaldtrump" data-link-name="in body link">Donald Trump</a>’s national security adviser Mike Waltz included a journalist in the <a href="https://www.theguardian.com/us-news/signal-group-chat-leak" data-link-name="in body link">Signal group chat</a> about plans for US strikes in Yemen after he mistakenly saved his number months before under the contact of someone else he intended to add, according to three people briefed on the matter.</p><p>The mistake was one of several missteps that came to light in the White House’s internal <a href="https://www.theguardian.com/us-news/2025/apr/03/pete-hegseth-signal-chat-dod-investigation" data-link-name="in body link">investigation</a>, which showed a series of compounding slips that started during the 2024 campaign and went unnoticed until Waltz created the group chat last month.</p><p>Trump briefly considered firing Waltz over the episode, more angered by the fact that Waltz had the number of <a href="https://www.theguardian.com/us-news/2025/mar/27/who-is-jeffrey-goldberg-atlantic-signal-chat-leak" data-link-name="in body link">Jeffrey Goldberg</a>, the editor of the Atlantic – a magazine he despises – than the fact that the military operation discussion took place on an unclassified system such as Signal.</p><figure id="bba94387-2f9c-4a24-bcef-61ab5a98123a" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:3,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Trump fires six national security staffers after meeting with far-right activist Laura Loomer&quot;,&quot;elementId&quot;:&quot;bba94387-2f9c-4a24-bcef-61ab5a98123a&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2025/apr/03/laura-loomer-trump-meeting&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>But <a href="https://www.theguardian.com/us-news/2025/mar/27/trump-fire-waltz-signal-atlantic-leak" data-link-name="in body link">Trump decided against firing him</a> in large part because he did not want the Atlantic and the news media more broadly to have the satisfaction of forcing the ouster of a top cabinet official weeks into his second term. Trump was also mollified by the findings of the internal investigation.</p><p>The disclosures nonetheless triggered a “forensic review” by the White House information technology office, which found that Waltz’s phone had saved Goldberg’s number as part of an unlikely series of events that started when Goldberg emailed the Trump campaign last October.</p><p>According to three people briefed on the internal investigation, Goldberg had emailed the campaign about a story that criticized Trump for his attitude towards wounded service members. To push back against the story, the campaign enlisted the help of Waltz, their national security surrogate.</p><p>Goldberg’s email was forwarded to then Trump spokesperson Brian Hughes, who then copied and pasted the content of the email – including the signature block with Goldberg’s phone number – into a text message that he sent to Waltz, so that he could be briefed on the forthcoming story.</p><p>Waltz did not ultimately call Goldberg, the people said, but in an extraordinary twist, inadvertently ended up saving Goldberg’s number in his iPhone – under the contact card for Hughes, now the spokesperson for the national security council.</p><p>A day after that <a href="https://www.theatlantic.com/politics/archive/2024/10/trump-military-generals-hitler/680327/" data-link-name="in body link">Goldberg story was published</a>, on 22 October, Waltz appeared on CNN to defend Trump. “Don’t take it from me, take it from the 13 Abbey Gate Gold Star families, some of whom stood on a stage in front of a 30,000 person crowd and said how he helped them heal,” Waltz said.</p><p>According to the White House, the number was erroneously saved during a “contact suggestion update” by Waltz’s iPhone, which one person described as the function where an iPhone algorithm adds a previously unknown number to an existing contact that it detects may be related.</p><p>The mistake went unnoticed until last month when Waltz sought to add Hughes to the Signal group chat – but ended up adding Goldberg’s number to the 13 March message chain named “Houthi PC small group”, where several top US officials discussed plans for strikes against the Houthis.</p><p>Waltz said in the immediate aftermath of the incident that he had never met or communicated with Goldberg. He also suggested on Fox News that Goldberg’s number had been “sucked” into his phone, seemingly in reference to how his iPhone had saved Goldberg’s number.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-13">skip past newsletter promotion</a><p id="EmailSignup-skip-link-13" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>The White House did not comment on this story, and the investigation did not resolve the extent of Waltz’s relationship with Goldberg, if any. Reached by phone on Saturday, Goldberg said: “I’m not going to comment on my relationship with Mike Waltz beyond saying I do know him and have spoken to him.”</p><figure id="9a921bb7-7ce1-4c0e-931f-a29319abbbf1" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:15,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Mike Waltz’s team set up at least 20 Signal chats for national security work – report&quot;,&quot;elementId&quot;:&quot;9a921bb7-7ce1-4c0e-931f-a29319abbbf1&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2025/apr/02/trump-mike-waltz-signal-groups&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>Trump was briefed on the findings of the forensic review last week around the time he decided to keep Waltz, a person familiar with the matter said. Trump accepted Waltz’s mea culpa and has <a href="https://www.theguardian.com/us-news/2025/mar/29/trump-signalgate-media-lapse-us-administration" data-link-name="in body link">publicly defended him</a> in recent weeks since the group chat situation became public.</p><p>When Trump left the White House on Thursday, he was joined aboard Marine One by his chief of staff, Susie Wiles, his personnel chief, Sergio Gor, and Waltz, which aides took as a show of support for the embattled national security adviser.</p><p>Waltz also appears to have also engendered some sympathy from inside Trump’s orbit over the group chat because the White House had authorized the use of Signal, largely because there is no alternative platform to text in real time across different agencies, two people familiar with the matter said.</p><p>Previous administrations, including the Biden White House, did not develop an alternative platform to Signal, one of the people said. As a temporary solution, the Trump White House told officials to use Signal as they had done during the transition instead of regular text-message chains.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The "S" in MCP Stands for Security (310 pts)]]></title>
            <link>https://elenacross7.medium.com/%EF%B8%8F-the-s-in-mcp-stands-for-security-91407b33ed6b</link>
            <guid>43600192</guid>
            <pubDate>Sun, 06 Apr 2025 09:42:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elenacross7.medium.com/%EF%B8%8F-the-s-in-mcp-stands-for-security-91407b33ed6b">https://elenacross7.medium.com/%EF%B8%8F-the-s-in-mcp-stands-for-security-91407b33ed6b</a>, See on <a href="https://news.ycombinator.com/item?id=43600192">Hacker News</a></p>
Couldn't get https://elenacross7.medium.com/%EF%B8%8F-the-s-in-mcp-stands-for-security-91407b33ed6b: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators (115 pts)]]></title>
            <link>https://machinelearning.apple.com/research/seedlm-compressing</link>
            <guid>43599967</guid>
            <pubDate>Sun, 06 Apr 2025 08:53:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://machinelearning.apple.com/research/seedlm-compressing">https://machinelearning.apple.com/research/seedlm-compressing</a>, See on <a href="https://news.ycombinator.com/item?id=43599967">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main"><div><p><span>Authors</span>Rasoul Shafipour, David Harrison, Maxwell Horton, Jeffrey Marker, Houman Bedayat, Sachin Mehta†, Mohammad Rastegari†, Mahyar Najibi, Saman Naderiparizi</p></div><div><p>Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of a pseudo-random generator to encode and compress model weights. Specifically, for each block of weights, we
find a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. Unlike state-of-the-art methods that rely on calibration data, our approach is data-free and generalizes well across diverse tasks. Our experiments with
Llama3 70B, which is particularly challenging, show zero-shot accuracy retention at 4- and  3-bit compression to be on par with or better than state-of-the-art methods, while maintaining performance comparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases, approaches a 4x speed-up over an FP16 Llama 2/3 baseline.</p>
<p>† Meta</p></div><section><p><h2>Related readings and updates.</h2></p><div><div data-testid="card-compress-compare"><p>*Equal Contributors
To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support…</p><p><a href="https://machinelearning.apple.com/research/compress-compare" aria-label="See full paper details regarding Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments">See paper details</a></p></div><div data-testid="card-lossless-compression"><p>This paper was accepted at the ICML 2021 conference as well as the Theory and Practice of Differential Privacy workshop at the ICML 2021 conference.
Locally Differentially Private (LDP) Reports are commonly used for collection of statistics and machine learning in the federated setting. In many cases the best known LDP algorithms require sending prohibitively large messages from the client device to the server (such as when constructing…</p><p><a href="https://machinelearning.apple.com/research/lossless-compression" aria-label="See full paper details regarding Lossless Compression of Efficient Private Local Randomizers">See paper details</a></p></div></div></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Video footage appears to contradict Israeli account of Gaza medic killings (117 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c4g2z103nqxo</link>
            <guid>43599864</guid>
            <pubDate>Sun, 06 Apr 2025 08:32:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c4g2z103nqxo">https://www.bbc.com/news/articles/c4g2z103nqxo</a>, See on <a href="https://news.ycombinator.com/item?id=43599864">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p>Mobile phone footage has emerged that appears to contradict Israel's account of why soldiers opened fire on a convoy of ambulances and a fire truck on March 23, <!-- --><a target="_self" href="https://www.bbc.co.uk/news/articles/crkxm1rg6k1o">killing 15 rescue workers<!-- --></a>. <!-- --></p><p>The video, published by the Palestine Red Crescent Society (PRCS), shows the vehicles moving in darkness with headlights and emergency flashing lights switched on - before coming under fire. The PRCS said the video was obtained from the phone of a paramedic who was killed.<!-- --></p><p>The Israel Defense Forces (IDF) initially denied the vehicles had their headlights or emergency signals on. <!-- --></p></div><div data-component="text-block"><p>But in response to the new video, the IDF told the BBC: "All claims, including the documentation circulating about the incident, will be thoroughly and deeply examined to understand the sequence of events and the handling of the situation".<!-- --></p><p>A surviving paramedic previously <!-- --><a target="_self" href="https://www.bbc.co.uk/news/articles/ckgere1y740o">told the BBC<!-- --></a> that the ambulances were clearly marked and had their internal and external lights on. <!-- --></p><p>The latest video, which the PRCS said had been shown to the UN Security Council, shows the marked vehicles drawing to a halt on the edge of the road, lights still flashing, and at least two emergency workers stepping out wearing reflective clothing.<!-- --></p><p>The windscreen of the vehicle being filmed from is cracked and shooting can then be heard lasting for several minutes as the person filming says prayers. He is understood to be one of the dead paramedics.<!-- --></p><p>The footage was found on his phone after his body was recovered from a shallow grave one week after the incident. The bodies of the eight paramedics, six Gaza Civil Defence workers and one UN employee were found buried in sand, along with their wrecked vehicles. It took international organisations days to negotiate safe access to the site.<!-- --></p><p>Israel claimed a number of Hamas and Islamic Jihad militants had been killed in the incident, but it has not provided any evidence or further explained the threat to its troops.<!-- --></p><p>Israel's Foreign Minister Gideon Saar earlier this week echoed the army account, saying "the IDF did not randomly attack an ambulance".<!-- --></p><p>The IDF promised to investigate the circumstances after a surviving paramedic questioned its account. <!-- --></p><p>In an interview with the BBC, <!-- --><a target="_self" href="https://www.bbc.co.uk/news/articles/ckgere1y740o">paramedic Munther Abed said<!-- --></a>: "During day and at night, it's the same thing. External and internal lights are on. Everything tells you it's an ambulance vehicle that belongs to the Palestinian Red Crescent. All lights were on until the vehicle came under direct fire."<!-- --></p><p>He also denied he or his team had any militant connections.<!-- --></p><p>"All crews are civilian. We don't belong to any militant group. Our main duty is to offer ambulance services and save people's lives. No more, no less," he said.<!-- --></p><p>Speaking at the United Nations yesterday the President of the PRCS, Dr Younis Al-Khatib, referred to the video recording, saying: "I heard the voice of one of those team members who was killed. His last words before being shot…'forgive me mum, I just wanted to help people. I wanted to save lives'. It's heartbreaking".<!-- --></p><p>He called for "accountability" and "an "independent and thorough investigation" of what he called an "atrocious crime".<!-- --></p><p>One paramedic is still unaccounted for following the 23 March incident.<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Standard Ebooks: liberated ebooks, carefully produced for the true book lover (640 pts)]]></title>
            <link>https://standardebooks.org</link>
            <guid>43599637</guid>
            <pubDate>Sun, 06 Apr 2025 07:36:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://standardebooks.org">https://standardebooks.org</a>, See on <a href="https://news.ycombinator.com/item?id=43599637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<h2>Free and liberated ebooks,<br> carefully produced for the true book lover.</h2>
	<picture>
		<source srcset="https://standardebooks.org/images/devices@2x.avif 2x, https://standardebooks.org/images/devices.avif 1x" type="image/avif">
		<source srcset="https://standardebooks.org/images/devices@2x.png 2x, https://standardebooks.org/images/devices.png 1x" type="image/png">
		<img src="https://standardebooks.org/images/devices@2x.png" alt="Ereaders with a Standard Ebook open.">
	</picture>
	<p>Standard Ebooks is a volunteer-driven project that produces new editions of public domain ebooks that are lovingly formatted, open source, free of U.S. copyright restrictions, and free of cost.</p>
	<p>Ebook projects like <a href="https://www.gutenberg.org/">Project Gutenberg</a> transcribe ebooks and make them available for the widest number of reading devices. Standard Ebooks takes ebooks from sources like Project Gutenberg, formats and typesets them using a carefully designed and professional-grade style manual, fully proofreads and corrects them, and then builds them to create a new edition that takes advantage of state-of-the-art ereader and browser technology.</p>
	<p>Standard Ebooks aren’t just a beautiful addition to your digital library—they’re a high quality standard to build your own ebooks on.</p><a href="https://standardebooks.org/ebooks">Browse our library of free ebooks</a>
	<section>
		<h2>What makes Standard Ebooks different?</h2>
		<section>
			<h3>Modern &amp; consistent typography</h3>
			<div>
				<div>
					<p>Other free ebooks don’t put much effort into professional-quality typography: they use "straight" quotes instead of “curly” quotes, they ignore details like em- and en-dashes, and they look more like early-90’s web pages instead of actual books.</p>
					<p>Standard Ebooks applies a rigorous and modern <a href="https://standardebooks.org/manual">style manual</a> when developing each and every ebook to ensure they meet a professional-grade and consistent typographical standard. Our ebooks look <em>good</em>.</p>
				</div>
				<figure>
					<picture>
						<source srcset="https://standardebooks.org/images/typography-bad@2x.avif 2x, https://standardebooks.org/images/typography-bad.avif 1x" type="image/avif">
						<source srcset="https://standardebooks.org/images/typography-bad@2x.png 2x, https://standardebooks.org/images/typography-bad.png 1x" type="image/png">
						<img src="https://standardebooks.org/images/typography-bad@2x.png" alt="An example of bad typography." height="150" width="300">
					</picture>
					<picture>
						<source srcset="https://standardebooks.org/images/typography-good@2x.avif 2x, https://standardebooks.org/images/typography-good.avif 1x" type="image/avif">
						<img src="https://standardebooks.org/images/typography-good@2x.png" alt="An example of Standard Ebooks typography." height="150" width="300">
					</picture>
					<img alt="An arrow pointing from bad typography to good typography." src="https://standardebooks.org/images/arrow-down.png">
				</figure>
			</div>
		</section>
		<section>
			<h3>Full proofing with careful corrections</h3>
			<div>
				<div>
					<p>Transcriptions from other sources are often filled with typos or suffer from issues like inconsistent spelling, missing accent marks, or missing punctuation. Submitting corrections to such sources can be difficult or impossible, so errors are rarely fixed.</p>
					<p>At Standard Ebooks, we do a careful and complete readthrough of each ebook before releasing it, checking it against a scan of the original pages to fix as many typos as possible. Even if we <em>do</em> miss something, our ebooks are stored in the hugely popular Git source control system, allowing anyone to easily submit a correction.</p>
				</div>
				<figure>
					<picture>
						<source srcset="https://standardebooks.org/images/proofreading@2x.avif 2x, https://standardebooks.org/images/proofreading.avif 1x" type="image/avif">
						<source srcset="https://standardebooks.org/images/proofreading@2x.png 2x, https://standardebooks.org/images/proofreading.png 1x" type="image/png">
						<img src="https://standardebooks.org/images/proofreading@2x.png" alt="A text with proofreader’s marks." height="200" width="400">
					</picture>
				</figure>
			</div>
		</section>
		<section>
			<h3>Rich &amp; detailed metadata</h3>
			<div>
				<p>Our ebooks include complete, well-researched, and consistent metadata, including original, detailed book blurbs and links to encyclopedia sources. Perfect for machine processing or for extra-curious, technically-minded readers.</p>
				<figure>
					<picture>
						<source srcset="https://standardebooks.org/images/metadata@2x.avif 2x, https://standardebooks.org/images/metadata.avif 1x" type="image/avif">
						<source srcset="https://standardebooks.org/images/metadata@2x.png 2x, https://standardebooks.org/images/metadata.png 1x" type="image/png">
						<img src="https://standardebooks.org/images/metadata@2x.png" alt="Ebook metadata displayed in an ereader program." height="200" width="400">
					</picture>
				</figure>
			</div>
		</section>
		<section>
			<h3>State-of-the-art technology</h3>
			<div>
				<div>
					<p>Each Standard Ebook takes full advantage of the latest ereader technology, including:</p>
					<ul>
						<li>
							<p>Hyphenation support,</p>
						</li>
						<li>
							<p>Popup footnotes,</p>
						</li>
						<li>
							<p>High-resolution and scalable vector graphics,</p>
						</li>
						<li>
							<p>Ereader-compatible tables of contents,</p>
						</li>
					</ul>
					<p>and more. One of our goals is to ensure our ebooks stay up-to-date with the best reading experience technology can provide. Just because it’s a classic doesn’t mean it has to use old technology.</p>
				</div>
				<figure>
					<picture>
						<source srcset="https://standardebooks.org/images/endnote@2x.avif 2x, https://standardebooks.org/images/endnote.avif 1x" type="image/avif">
						<source srcset="https://standardebooks.org/images/endnote@2x.png 2x, https://standardebooks.org/images/endnote.png 1x" type="image/png">
						<img src="https://standardebooks.org/images/endnote@2x.png" alt="A screenshot of a popup endnote." height="150" width="300">
					</picture>
					<picture>
						<source srcset="https://standardebooks.org/images/toc@2x.avif 2x, https://standardebooks.org/images/toc.avif 1x" type="image/avif">
						<source srcset="https://standardebooks.org/images/toc@2x.png 2x, https://standardebooks.org/images/toc.png 1x" type="image/png">
						<img src="https://standardebooks.org/images/toc@2x.png" alt="A screenshot of an ebook’s table of contents." height="150" width="300">
					</picture>
				</figure>
			</div>
		</section>
		<section>
			<h3>Quality covers</h3>
			<div>
				<div>
					<p>Everyone knows a book is judged by its cover, but most free ebooks leave it to your ereader software to generate a drab default cover.</p>
					<p>Standard Ebooks draws from a vast collection of public domain fine art to create attractive, unique, appropriate, and consistent covers for each of our ebooks.</p>
				</div>
				<figure>
					<picture>
						<source srcset="https://standardebooks.org/images/covers@2x.avif 2x, https://standardebooks.org/images/covers.avif 1x" type="image/avif">
						<source srcset="https://standardebooks.org/images/covers@2x.jpg 2x, https://standardebooks.org/images/covers.jpg 1x" type="image/jpg">
						<img src="https://standardebooks.org/images/covers@2x.jpg" alt="An ebookshelf featuring Standard Ebooks covers." height="125" width="400">
					</picture>
				</figure>
			</div>
		</section>
		<section>
			<h3>Clean code &amp; semantic markup</h3>
			<div>
				<div>
					<p>Our strict coding standards allow technologists and ebook producers to use Standard Ebooks files as reliable, easy to read, and robust bases for their own work—not to mention as models of what well-crafted ebook files look like. Common code patterns are repeated through different ebooks, so the code never surprises you.</p>
					<p>Each ebook is also enhanced with careful standards-based semantic markup that opens the gateway for exciting new kinds of machine processing.</p>
				</div>
				<figure>
					<picture>
						<source srcset="https://standardebooks.org/images/code@2x.avif 2x, https://standardebooks.org/images/code.avif 1x" type="image/avif">
						<source srcset="https://standardebooks.org/images/code@2x.png 2x, https://standardebooks.org/images/code.png 1x" type="image/png">
						<img src="https://standardebooks.org/images/code@2x.png" alt="The source code for an ebook." height="200" width="400">
					</picture>
				</figure>
			</div>
		</section>
		<section>
			<h3>Free, open-source, &amp; public domain</h3>
			<div>
				<div>
					<p>We use the popular Git source control system to track each and every change made to our ebooks. Anyone can easily see a history of changes, or contribute their own changes with the click of a mouse.</p>
					<p>And while all of the ebooks we feature and the cover art we draw from are <em>already</em> believed to be in the public domain in the U.S., Standard Ebooks releases all of the work we put in to each ebook into the public domain too. That makes each and every one of our ebook files not just free, but <a href="https://en.wikipedia.org/wiki/Gratis_versus_libre">libre</a> too—because the world deserves more unrestricted culture.</p>
				</div>
				<figure>
					<img alt="The Git SCM logo." src="https://standardebooks.org/images/git.svg" height="150" width="150">
					<img alt="The no-copyright symbol." src="https://standardebooks.org/images/no-copyright.svg" height="150" width="150">
					<img alt="The anti-DRM symbol." src="https://standardebooks.org/images/no-drm.svg" height="137" width="150">
				</figure>
			</div>
		</section>
	</section>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple's Darwin OS and XNU Kernel Deep Dive (386 pts)]]></title>
            <link>https://tansanrao.com/blog/2025/04/xnu-kernel-and-darwin-evolution-and-architecture/</link>
            <guid>43597778</guid>
            <pubDate>Sat, 05 Apr 2025 23:46:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tansanrao.com/blog/2025/04/xnu-kernel-and-darwin-evolution-and-architecture/">https://tansanrao.com/blog/2025/04/xnu-kernel-and-darwin-evolution-and-architecture/</a>, See on <a href="https://news.ycombinator.com/item?id=43597778">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-gjtny2mx="">    <article data-astro-cid-gjtny2mx="">  <p>This post is the result of me going down a several week long XNU rabbit-hole
after reading <a href="https://www.theregister.com/2025/03/08/kernel_sanders_apple_rearranges_xnu/">this post by Thomas Claburn on
Exclaves</a>,
more on that later. I’ve tried my best to condense all the information into a
single blog post. I’ve also tried to keep sections self-contained so you can
skip around using the table of contents, this does come at the cost of
repeating myself in some places, so thanks in advance for your patience. While
I’m confident of my understanding on this topic, some errors are inevitable
when dealing with content this dense, if you spot any errors, assume them to be
mine and please reach out so I can correct it, also let me know your thoughts
by reaching out via email or mastodon. Thanks in advance and let’s begin!</p>
<h2 id="introduction">Introduction</h2>
<p>Apple’s Darwin operating system is the Unix-like core underpinning macOS, iOS,
and all of Apple’s modern OS platforms. At its heart lies the XNU kernel – an
acronym humorously standing for “X is Not Unix.” XNU is a unique <strong>hybrid
kernel</strong> that combines a Mach microkernel core with components of BSD Unix.
This design inherits the rich legacy of Mach (originating from 1980s
microkernel research) and the robust stability and POSIX compliance of BSD. The
result is a kernel architecture that balances modularity and performance by
blending microkernel message-passing techniques with a monolithic Unix kernel
structure. We’ll go through a chronological exploration of Darwin and XNU’s
evolution – from Mach and BSD origins to the modern kernel features in macOS on
Apple Silicon and iOS on iPhones. We’ll follow this with a deep dive into the
architectural milestones, analyze XNU’s internal design (Mach-BSD interaction,
IPC, scheduling, memory management, virtualization), and examine how the kernel
and key user-space components have adapted to new devices and requirements over
time.</p>
<h2 id="darwin-and-xnu-development-history">Darwin and XNU Development History</h2>
<h3 id="mach-microkernel-origins-19851996">Mach Microkernel Origins (1985–1996)</h3>
<p>Darwin’s story begins with <strong>Mach</strong>, a project at Carnegie Mellon University
(1985) led by Richard Rashid and Avie Tevanian. Mach was envisioned as a
next-generation <strong>microkernel</strong> to address the growing complexity of UNIX
kernels. Instead of a single large kernel binary, Mach provided only
fundamental low-level functions – <strong>memory management</strong> (virtual memory,
address spaces), <strong>CPU scheduling</strong> (threads and tasks), and <strong>inter-process
communication</strong> (IPC via message passing). Higher-level services (file systems,
networking, device drivers, etc.) were intended to run as user-space <em>servers</em>
on top of Mach. This separation promised improved reliability (a crashed driver
wouldn’t crash the whole system) and flexibility (multiple OS personalities
could run concurrently). In fact, Mach’s design allowed running several
“personalities” – for example, UNIX and another OS – on one microkernel, a
concept analogous to modern virtualization.</p>
<p>By 1990, Mach had progressed to <strong>Mach 2.5</strong>, which was a microkernel but still
co-located some BSD kernel code in kernel space for performance. The true
microkernel version, <strong>Mach 3.0</strong>, arrived in 1991–1994. Mach’s <strong>virtual
memory (VM) system</strong> was influential beyond the project – it was adopted by
4.4BSD and later FreeBSD as their memory management subsystem. Importantly,
Mach introduced the concept of <strong>tasks</strong> (encapsulating an address space and
resources, roughly equivalent to a process) and <strong>threads</strong> (unit of CPU
execution) as first-class kernel objects. It also implemented an efficient VM
with copy-on-write and memory object abstractions, and a message-based IPC
mechanism using <strong>Mach ports</strong>.</p>
<p>Parallel to Mach’s development, <strong>NeXT Computer</strong> (founded by Steve Jobs in
1985) needed a modern OS for its workstations. NeXT adopted Mach early:
<strong>NeXTSTEP</strong>, released in 1989, was built on a Mach 2.5 kernel with a 4.3BSD
Unix subsystem layered on top. Crucially, NeXTSTEP’s kernel (later named
<strong>XNU</strong>) was not a pure microkernel system with user-space servers; instead, it
took Mach and <strong>integrated the BSD code into the kernel address space</strong> for
speed. In other words, NeXT used Mach’s abstractions (tasks, threads, IPC, VM)
and ran a BSD kernel <em>in kernel mode</em> on top of Mach primitives. This hybrid
approach sacrificed some of Mach’s extreme modularity in favor of performance:
it avoided the heavy context-switching and messaging overhead that plagued
fully microkernel systems of the era. NeXTSTEP’s kernel also included an
object-oriented driver framework called <strong>DriverKit</strong> (written in Objective-C)
to develop device drivers as objects, reflecting NeXT’s preference for
higher-level languages.</p>
<p>By the mid-1990s, Apple’s original Mac OS (classic Mac OS) was aging and lacked
modern OS features like proper multitasking and memory protection. In 1996,
Apple sought an existing OS as its foundation for the future. The company
acquired NeXT in December 1996, choosing NeXTSTEP as the core of the new <strong>Mac
OS X</strong>. With this acquisition, NeXT’s Mach/BSD hybrid kernel came to Apple,
bringing along the engineering leadership of Avie Tevanian (Mach co-author) as
Apple’s VP of Software. Apple named the new OS project <strong>Rhapsody</strong>, which
would later become Mac&nbsp;OS&nbsp;X.</p>
<h3 id="rhapsody-to-mac-os-x-integrating-mach-30-and-bsd-19972005">Rhapsody to Mac OS X: Integrating Mach 3.0 and BSD (1997–2005)</h3>
<p>After acquiring NeXT, Apple set out to merge the NeXTSTEP kernel with
additional features and hardware support needed for Macs. The kernel was
further updated with newer Mach and BSD technology. Notably, Apple incorporated
code from <strong>OSFMK 7.3</strong>, the Open Software Foundation’s Mach 3.0 kernel, into
XNU. This meant the Mach portion of XNU now drew from Mach 3.0’s true
microkernel lineage (including contributions from University of Utah’s Mach 4
research). On the BSD side, the NeXTSTEP kernel’s 4.3BSD subsystem was upgraded
with <strong>4.4BSD and FreeBSD</strong> code. This brought in a more modern BSD
implementation with features like improved networking and a robust filesystems
infrastructure. By combining Mach 3.0 and FreeBSD elements, Apple shaped XNU
into a powerful hybrid: Mach provided the low-level kernel architecture and
abstractions, while BSD provided the <strong>Unix APIs and services</strong> on top.</p>
<p>Apple also replaced NeXT’s old DriverKit with a new driver framework called
<strong>I/O Kit</strong>, written in a subset of C++. I/O Kit introduced a object-oriented
device driver model within the kernel, supporting features like dynamic device
matching and hot-plugging in a robust way. The choice of C++ (minus exceptions
and multiple inheritance, using Embedded C++ subset) for I/O Kit was likely to
improve performance and avoid the runtime overhead of Objective-C in the
kernel. By the late 1990s, XNU was thus composed of three core parts: the Mach
microkernel layer (now OSFMK 7.3 based), the BSD layer (largely
FreeBSD-derived), and the I/O Kit for drivers.</p>
<p>Apple delivered the first developer previews of Mac OS X in 1999, and in 2000
released the open source Darwin 1.0, which exposed the XNU kernel and basic
Unix userland to developers. The commercial debut, <strong>Mac&nbsp;OS&nbsp;X 10.0 (Cheetah)</strong>,
came in early 2001 (Darwin 1.3.1). While the initial releases were rough in
performance, they cemented the architectural paradigm. Key early milestones
included:</p>
<ul>
<li><strong>Mac OS X 10.1 (Puma, 2001)</strong> – Improved performance in threading and added
missing Unix features. Darwin 1.4.1 in 10.1 introduced faster thread
management and real-time threads support.</li>
<li><strong>Mac OS X 10.2 (Jaguar, 2002)</strong> – Darwin 6.0 brought the synchronicity of
the BSD layer with FreeBSD 4.4/5, plus large new features: IPv6 and IPSec
networking, the new <code>mDNSResponder</code> service for discovery
(Bonjour/Rendezvous), and journaling in HFS+ file system. It also upgraded
the toolchain (GCC3) and added modern Unix utilities.</li>
<li><strong>Mac OS X 10.3 (Panther, 2003)</strong> – Darwin 7.0/7.1 integrated <strong>FreeBSD 5</strong>
kernel improvements. This brought <strong>fine-grained kernel locking</strong> (moving
away from the earlier giant-lock model) to better utilize multiprocessors.
Panther’s kernel also introduced <strong>integrated BFS</strong> (basic firewall) and
other performance tuning like improved VM and I/O.</li>
</ul>
<p>Throughout these releases, XNU remained a 32-bit kernel (with limited 64-bit
user process support introduced in 10.4 for specific tasks). Apple maintained
support for PowerPC the Mac CPU architecture of choice in the early days while
also quietly keeping the Intel x86 compatibility (inherited from NeXTSTEP’s x86
support) in the source, preparing for future transitions.</p>
<p>A major architectural change arrived in <strong>Mac&nbsp;OS&nbsp;X 10.4 (Tiger, 2005)</strong>. This
was the first version where Apple declared OS&nbsp;X to be <strong>UNIX&nbsp;03 certified</strong>,
meaning the system conformed to the Single UNIX Specification and could legally
use the UNIX name. Darwin 8 (Tiger’s core) achieved this UNIX certification by
virtue of the robust BSD layer integrated in XNU. Tiger also introduced new
kernel features like <strong>kqueue/kevent</strong> (from FreeBSD, for scalable event
handling), and laid groundwork for Intel Macs by keeping XNU cross-platform.
Apple then announced in 2005 the switch to Intel x86 processors for Macs. XNU’s
Mach foundations made such platform adaptability easier, as Mach abstracted
many low-level hardware details behind a portability layer. In early 2006,
Apple released <strong>Mac&nbsp;OS&nbsp;X 10.4.4 for Intel</strong>, demonstrating XNU running on
x86_32 with much of the code shared with the PowerPC build.</p>
<h3 id="transition-to-64-bit-multi-core-and-iphone-os-20052010">Transition to 64-bit, Multi-Core and iPhone OS (2005–2010)</h3>
<p>By the mid-2000s, computing had shifted to multi-core 64-bit architectures, and
Apple’s OS had to evolve accordingly. <strong>Mac&nbsp;OS&nbsp;X 10.5 Leopard (2007)</strong>, based
on Darwin 9, was a landmark release for XNU. It introduced extensive 64-bit
support: while earlier versions could run 64-bit user applications in limited
form, Leopard’s kernel itself could run in 64-bit mode on appropriate hardware
(x86-64) and support 64-bit drivers. Leopard also dropped official support for
older architectures like PowerPC G3 and brought in stronger security and
performance features: <strong>address space layout randomization (ASLR)</strong> to thwart
exploits, an advanced <strong>sandbox</strong> facility for restricting processes, and the
<strong>DTrace</strong> instrumentation framework from Solaris for low-level tracing.
Notably, Leopard was the last Mac OS X version to fully support PowerPC – Apple
was transitioning its entire lineup to Intel by this time.</p>
<p>In 2007, Apple also debuted the <strong>iPhone</strong> with “iPhone OS” (later named iOS),
which was built on Darwin as well. The first iPhone OS was based on Darwin 9
(same core as Leopard). This demonstrated the versatility of XNU: within the
same kernel version, Apple could target high-end PowerPC and x86 servers,
consumer Intel laptops, and resource-constrained ARM mobile devices. The kernel
gained support for the ARM architecture and tailor-made modifications for
mobile. For example, because early iPhones had very limited RAM and no swap,
the kernel’s memory management had to incorporate aggressive <strong>memory-pressure
handling</strong>. Apple introduced a <strong>Jetsam</strong> mechanism in iPhone OS, which
monitored low-memory conditions and killed background apps to free memory
(since traditional swapping to disk was not feasible on flash storage). iPhone
OS also ran all third-party apps in a <em>sandbox</em> by design and required strict
code signing for binaries – security measures facilitated by XNU’s Mach and BSD
layers (Mach’s task port and codesign enforcement in the kernel, with help from
user-space daemons like <code>amfid</code> for signature validation).</p>
<p><strong>Mac&nbsp;OS&nbsp;X 10.6 Snow Leopard (2009)</strong> marked the maturation of XNU on 64-bit
Intel. Snow Leopard (Darwin 10) discontinued support for PowerPC entirely,
making XNU a dual-architecture kernel (x86_64 and i386 for Intel Macs). It
also was the first to ship with an <em>optional</em> fully 64-bit kernel on capable
Macs (most defaulted to 32-bit kernel with 64-bit userland, except Xserve).
Snow Leopard brought major concurrency advances: the introduction of <strong>Grand
Central Dispatch (libdispatch)</strong> for user-space task parallelization and kernel
support for <strong>dispatch queues</strong>. While <code>libdispatch</code> is a user-space library,
it works closely with the kernel, which provides the underlying thread pools
and scheduling for dispatch queues. Another addition was <strong>OpenCL</strong> for GPU
computing, again requiring tight integration between user frameworks and kernel
drivers. Snow Leopard’s streamlined focus on Intel and multi-core optimizations
made XNU more efficient.</p>
<p>On the mobile side, <strong>iPhone OS 3 (2009)</strong> and <strong>iOS 4 (2010)</strong> (renamed “iOS”
in 2010) evolved alongside, adding support for the Apple A4/A5 ARM chips and
features like multitasking. XNU’s scheduler was adapted in iOS 4 to handle the
concept of background apps with different priority bands (foreground,
background, etc.), and to support <strong>multi-core ARM SoCs</strong> as they appeared
(e.g., the Apple A5 in 2011 was dual-core). iOS and macOS kernels remained
largely unified, with conditional code for platform differences. By <strong>OS&nbsp;X 10.7
Lion (2011)</strong>, XNU dropped support for 32-bit Intel kernels entirely – it
required a 64-bit CPU on Mac, reflecting the industry’s move beyond 32-bit.
Lion (Darwin 11) also improved sandboxing and added full support for new
features like <strong>Automatic Reference Counting (ARC)</strong> in Obj-C (with compiler
and runtime changes reflected in the system).</p>
<h3 id="modern-macos-and-ios-evolution-20112020">Modern macOS and iOS Evolution (2011–2020)</h3>
<p>From 2011 onward, Apple’s OS releases came in a yearly cadence, and Darwin
continued to get incremental but significant enhancements to support new
hardware and features:</p>
<ul>
<li><strong>OS&nbsp;X 10.8 Mountain Lion (2012)</strong> and <strong>10.9 Mavericks (2013)</strong> (Darwin 12
and 13) introduced power- and memory-optimizations in the kernel. Mavericks
added <strong>Compressed Memory</strong>, a kernel feature where inactive pages are
compressed in RAM to avoid swapping to disk. This was in line with iOS
techniques to cope with low RAM, and it benefited Macs by improving
responsiveness under memory pressure. Mavericks also implemented <strong>Timer
Coalescing</strong>, where the kernel aligns wake-ups from idle to reduce CPU power
usage. These changes show how the kernel adapted to energy-efficiency
demands, influenced by mobile design philosophies. Additionally, around this
time, Apple introduced <strong>App Nap</strong> and increased use of Quality-of-Service
(QoS) classes for threads, which required kernel scheduling awareness to
throttle or prioritize threads based on QoS hints (e.g., background vs
user-initiated tasks). XNU’s scheduler evolved to support these multiple
priority bands and energy-efficient scheduling.</li>
<li><strong>OS&nbsp;X 10.10 Yosemite (2014)</strong> and <strong>10.11 El Capitan (2015)</strong> (Darwin 14 and
15) continued the trend. A major security addition in El Capitan was <strong>System
Integrity Protection (SIP)</strong>. SIP (also called “rootless”) is enforced by the
kernel’s security framework, preventing even root user processes from
tampering with critical system files and processes. Implemented via the BSD
layer’s Mandatory Access Control (MAC) framework, SIP hardened the OS by
moving more trust into the kernel and away from user space. For iOS (iOS 9 in
2015), similar “rootless” concepts were applied. Darwin 15 also saw Apple
unifying the code base for OS X and iOS further, as they introduced
<strong>watchOS</strong> and <strong>tvOS</strong> (both also Darwin-based) – XNU had to accommodate
running on tiny Apple Watch hardware (S1 chip) up to powerful Mac Pros, with
scalable scheduling, memory, and I/O capabilities. By now, XNU supported
ARM64 (64-bit ARMv8, first used in iPhone 5s in 2013) and would go on to drop
32-bit ARM support for iOS by <strong>iOS 11 (2017)</strong>.</li>
<li><strong>macOS 10.12 Sierra (2016)</strong>, <strong>10.13 High Sierra (2017)</strong>, <strong>10.14 Mojave
(2018)</strong> (Darwin 16–18) brought filesystem evolution and further security.
High Sierra introduced <strong>APFS (Apple File System)</strong> as the new default
filesystem, replacing HFS+. APFS required kernel support for snapshots,
cloning, and encryption at the container level. XNU’s VFS layer (in the BSD
component) was extended to accommodate APFS’s advanced features and
performance characteristics. During this era, kext (kernel extension)
security was tightened – macOS High Sierra requires user approval for loading
third-party kexts, and macOS Mojave introduced stricter code signing checks
and hardened runtime for user-space processes that also influence how the
kernel validates and allows certain operations. Another adaptation was
graphics and external device support, High Sierra’s eGPU support via
Thunderbolt required hot-plug handling improvements in I/O Kit and scheduling
of external PCIe devices.</li>
<li><strong>macOS 10.15 Catalina (2019)</strong> (Darwin 19) was a significant modernization
step for XNU. Catalina was the first to <strong>deprecate most 32-bit code</strong> (only
64-bit apps, and the kernel had been 64-bit only for years already). More
notably, Apple introduced a new approach for device drivers: <strong>DriverKit</strong>,
reviving the name of NeXT’s old driver framework but with a new design.
DriverKit in modern macOS allows many drivers to run in user space as
<strong>Driver Extensions (dexts)</strong>, outside of the kernel. This is a shift towards
microkernel philosophy for third-party code – by moving drivers (USB,
network, etc.) to user-space processes, Apple improved system stability and
security (a buggy driver can’t crash the kernel if it’s outside it). XNU was
adapted to facilitate this: the kernel provides user-space drivers with
controlled access to hardware (via IPC and shared memory) instead of loading
their code as kexts. At the same time, Catalina split the OS filesystem into
a read-only system volume, reinforcing the kernel’s SIP protections (the
kernel now treats system files as immutable during runtime). These changes
show how even decades after its birth, XNU’s architecture can pivot to
incorporate more user-space responsibilities when beneficial, leveraging the
Mach IPC mechanisms to do so safely.</li>
</ul>
<h3 id="apple-silicon-era-2020present">Apple Silicon Era (2020–Present)</h3>
<p>In 2020, Apple undertook another monumental transition: moving the Mac lineup
from Intel CPUs to Apple’s custom <strong>ARM64 SoCs</strong> (the <strong>Apple Silicon</strong> chips,
starting with M1). Darwin had long supported ARM due to iOS, but running macOS
on ARM64 introduced new challenges and opportunities. <strong>macOS 11 Big Sur
(2020)</strong>, corresponding to Darwin 20, was the first release for Apple Silicon
Macs. XNU was already cross-platform, but it now had to support a heterogeneous
<strong>big.LITTLE CPU architecture</strong>: Apple Silicon chips combine high-performance
cores and energy-efficient cores. The scheduler was enhanced to be
<strong>heterogeneity-aware</strong>, ensuring high-priority and heavy threads run on
performance cores, while background and low-QoS threads can be scheduled on
efficiency cores to save power. Apple likely utilizes the thread <strong>QoS
classes</strong> (which had been introduced in earlier macOS/iOS) to map threads to
appropriate core types – this is an extension of Mach scheduling concepts to a
new domain of asymmetric multiprocessing.</p>
<p>Another aspect of Apple Silicon is the unified memory architecture (shared
memory between CPU/GPU). While largely abstracted by frameworks, the kernel’s
memory manager works with the GPU drivers (which are now Apple’s own,
integrated via I/O Kit) to manage buffer sharing without expensive copies. The
Mach VM abstraction fits well here – memory objects can be shared between
user-space and the GPU with VM remapping rather than duplication. Additionally,
Apple Silicon brought hardware features like <strong>Pointer Authentication (PAC)</strong>
and <strong>Memory Tagging Extension (MTE)</strong> for security. XNU’s ARM64 backend had to
support PAC (which it does by using PAC keys in exception frames and system
pointers to mitigate ROP<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> attacks) and potentially MTE to detect memory
errors – these are deep architecture-specific enhancements in the kernel to
improve security on new hardware.</p>
<p>On the virtualization front, Apple Silicon prompted a reevaluation of
virtualization strategy. On Intel Macs, XNU has long supported virtualization
via the <strong>Hypervisor framework</strong> (introduced in macOS 10.10 Yosemite) which
allows user-space programs to run VMs using hardware VT-x support. With Apple
Silicon, macOS 11 introduced a new <strong>Virtualization framework</strong> built on top of
an in-kernel hypervisor for ARM64 (taking advantage of the ARM VMM features).
Notably, while the <strong>open-source XNU</strong> code does not include the Apple Silicon
hypervisor, the shipped kernel does initialize hypervisor support if running on
the appropriate Apple chip. This allows macOS on M1/M2 to run lightweight
virtual machines (for Linux, macOS guests, etc.) entirely from user-space
controllers, similar to Linux KVM. On iOS devices, Apple has kept the
hypervisor disabled or restricted (no public API), but the hardware capability
appeared with A14 chips. Enthusiasts quickly found that on jailbroken A14
devices, the hypervisor could be enabled to run Linux VMs<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>.</p>
<p>Beyond CPU and virtualization, Apple Silicon Macs run many of the same daemons
and services as iOS, indicating a convergence in system architecture. The XNU
kernel now powers everything from servers (macOS), personal computers, phones,
watches, TVs, and even the <strong>bridgeOS</strong> (a variant of Darwin running on the
Apple T2/M1 auxiliary processors for device management). Darwin’s <strong>flexibility
and scalability</strong> stem from the Mach foundation: it abstracts hardware
specifics in a platform layer, so adding a new CPU architecture (PowerPC → x86
→ ARM64) or scaling down to limited hardware largely requires implementing the
Mach low-level interfaces (like pmap for MMU, thread context switches, etc.)
and leaving higher-level kernel logic untouched. This design has paid dividends
in Apple’s transitions.</p>
<p>In summary, over two decades, XNU has undergone major transformations while
retaining its core identity. <strong>Table 1</strong> highlights a timeline of Darwin/XNU
milestones and architectural changes:</p>































































































<table><thead><tr><th><strong>Year</strong></th><th><strong>Release (Darwin ver.)</strong></th><th><strong>Key Kernel Developments</strong></th></tr></thead><tbody><tr><td><strong>1989</strong></td><td>NeXTSTEP 1.0 (Mach 2.5 + 4.3BSD)</td><td>NeXT’s XNU kernel hybrid introduced: Mach microkernel with BSD in kernel space for performance. Drivers via Obj-C DriverKit.</td></tr><tr><td><strong>1996</strong></td><td>NeXT acquired by Apple</td><td>Rhapsody OS development begins, based on OpenStep. Mach 2.5 + 4.3BSD XNU to be upgraded with Mach 3 and FreeBSD.</td></tr><tr><td><strong>1999</strong></td><td>Mac&nbsp;OS&nbsp;X Server 1.0 (Darwin 0.x)</td><td>First Darwin releases (0.1–0.3) as Apple integrates OSFMK Mach 3.0 (OSF/1) and FreeBSD into XNU.</td></tr><tr><td><strong>2001</strong></td><td>Mac&nbsp;OS&nbsp;X 10.0 (Darwin 1.3)</td><td>Darwin 1.x: Core OS X launched with hybrid kernel, BSD userland, Cocoa APIs. Early performance tuning of Mach/BSD integration.</td></tr><tr><td><strong>2003</strong></td><td>Mac&nbsp;OS&nbsp;X 10.3 (Darwin 7)</td><td>XNU sync with FreeBSD 5, bringing SMP scalability (fine-grained locking).</td></tr><tr><td><strong>2005</strong></td><td>Mac&nbsp;OS&nbsp;X 10.4 (Darwin 8)</td><td>UNIX&nbsp;03 certified kernel. Intel x86 support readied (Mach portability layer leveraged).</td></tr><tr><td><strong>2006</strong></td><td>Mac&nbsp;OS&nbsp;X on Intel (Darwin 8.x)</td><td>Apple transitions Macs to x86. XNU supports <strong>Universal Binary</strong> drivers and Rosetta translation (user-space emulation of PowerPC on x86).</td></tr><tr><td><strong>2007</strong></td><td>Mac&nbsp;OS&nbsp;X 10.5 (Darwin 9)</td><td>64-bit support in kernel (on x86_64); last PowerPC support. Security: NX support, ASLR, code signing, sandbox introduced. <strong>iPhone OS 1</strong> (Darwin 9) released on ARM, with XNU scaled to mobile (no swap, sandbox always on).</td></tr><tr><td><strong>2009</strong></td><td>Mac&nbsp;OS&nbsp;X 10.6 (Darwin 10)</td><td>Intel-only (drops PowerPC). Fully 64-bit kernel on capable Macs; Grand Central Dispatch (kernel task queues); OpenCL support. iPhone OS -&gt; <strong>iOS 3</strong> (Darwin 10) adds improved power management.</td></tr><tr><td><strong>2011</strong></td><td>Mac&nbsp;OS&nbsp;X 10.7 (Darwin 11)</td><td>Drops 32-bit kernel support on Mac; Requires x86_64. Expands sandboxing, FileVault 2 encryption (kernel crypto). <strong>iOS 5</strong> brings dual-core scheduling.</td></tr><tr><td><strong>2013</strong></td><td>OS&nbsp;X 10.9 (Darwin 13)</td><td>Power optimizations: compressed memory, timer coalescing in kernel. Improved multicore scheduling with QoS introduction.</td></tr><tr><td><strong>2015</strong></td><td>OS&nbsp;X 10.11 (Darwin 15)</td><td><strong>System Integrity Protection</strong> (kernel-enforced security). Enhanced AMFI (Apple Mobile File Integrity) for code signing in kernel and user helper (amfid). iOS 9 / watchOS debut (Darwin 15) on new device categories, kernel runs on Apple Watch (ARM Cortex-A7).</td></tr><tr><td><strong>2017</strong></td><td>macOS 10.13 (Darwin 17)</td><td>New APFS filesystem default on Mac (already in iOS 10). Kernel changes for cloning, snapshots. Kext loading requires user approval. iOS 11 drops 32-bit ARM, fully 64-bit kernel.</td></tr><tr><td><strong>2019</strong></td><td>macOS 10.15 (Darwin 19)</td><td>Legacy I/O Kit model shifts: <strong>DriverKit</strong> introduced for user-space drivers. System extensions modularize networking and endpoint security features out of kernel. macOS split system volume (read-only) to strengthen kernel’s protection of OS files.</td></tr><tr><td><strong>2020</strong></td><td><strong>macOS 11.0 (Darwin 20)</strong></td><td><strong>Apple Silicon support</strong> – XNU on ARM64 Mac (M1). Kernel adapts to heterogeneous cores, unified memory. Rosetta 2 translation tier (user-space JIT, with kernel enforcing memory protections for translated code). <strong>iOS 14</strong> – exposes new virtualization features for developers (e.g., running lightweight VMs on iPadOS).</td></tr><tr><td><strong>2022</strong></td><td>macOS 13 (Darwin 22)</td><td>Continued refinement for Apple Silicon (e.g., high-power mode on M1 Max, kernel scheduling tweaks). <strong>iOS 16</strong> – XNU adds support for virtualizing iOS/macOS guests (used in Xcode Simulator and Developer Mode features).</td></tr><tr><td><strong>2024</strong></td><td>macOS 14 (Darwin 23)</td><td>Ongoing improvements (Memory tagging support and fine-tuning for M2/M3 chips). Darwin remains the common core for <strong>visionOS</strong> (Apple Vision Pro AR headset) as well.</td></tr></tbody></table>
<p><strong>Table 1:</strong> Timeline of Darwin/XNU evolution with selected kernel milestones and architectural changes.</p>
<p>This timeline shows how XNU’s Mach/BSD core proved to be a stable foundation
that Apple could incrementally enhance: adding 64-bit support, embracing
multicore, tightening security, and porting to new architectures, all while
retaining backward compatibility. Next, we delve into the internal architecture
of XNU – the hybrid kernel design that made all of this possible.</p>
<h2 id="xnu-kernel-architecture-and-design">XNU Kernel Architecture and Design</h2>
<p><img alt="MacOS-Architecture.png" width="2169" height="2048" loading="lazy" decoding="async" src="https://tansanrao.com/_astro/MacOS-Architecture.B0qOydhL_Z1GQHHd.webp"></p>
<blockquote>
<p>File: Diagram of Mac OS X architecture.svg. (2024, December 29). <em>Wikimedia
Commons</em>. Retrieved 22:59, April 3, 2025 from
<a href="https://commons.wikimedia.org/w/index.php?title=File:Diagram_of_Mac_OS_X_architecture.svg&amp;oldid=976998015">https://commons.wikimedia.org</a>.</p>
</blockquote>
<h3 id="hybrid-kernel-design-mach--bsd-integration">Hybrid Kernel Design: Mach + BSD Integration</h3>
<p>XNU’s kernel design is often described as a <strong>hybrid kernel</strong>, because it
merges characteristics of microkernels (Mach) and monolithic kernels (BSD). In
a traditional microkernel, the kernel provides minimal services (IPC,
scheduling, VM) and everything else runs as user-space servers. In a monolithic
UNIX kernel, all OS services run in kernel mode as one large program. XNU
attempts to get “the best of both”: it uses Mach to modularize and abstract
low-level functions, but co-locates the critical BSD services in kernel space
for efficiency.</p>
<p>In XNU, the Mach component and the BSD component <strong>run as a single kernel
entity</strong> – they are linked into one binary and share the same address space.
There is no Mach vs BSD protection boundary; Mach functions and BSD functions
call each other via normal function calls within the kernel, not via IPC
messages. This co-location avoids the significant context-switch overhead that
a pure Mach system would incur (where a Unix system call would require
messaging a user-space BSD server). As a result, standard UNIX system calls
(file I/O, socket operations, etc.) in XNU perform comparably to other
monolithic Unix kernels, since the BSD code executes directly in kernel mode.
For instance, when a process calls <code>read()</code>, it traps into the kernel and the
BSD file system code is invoked directly; there’s no Mach message to a separate
process as would happen in a Mach 3.0 microkernel with an external BSD server.</p>
<p><strong>Mach’s Role:</strong> Mach in XNU provides the core kernel <strong>infrastructure and
abstractions</strong>. Mach manages CPU <em>threads</em> and task address spaces, implements
low-level scheduling, and handles virtual memory management (memory mapping,
paging). It also provides the fundamental <strong>IPC mechanism</strong> – Mach messages
sent over Mach <em>ports</em> (communication endpoints). In XNU, every process (BSD
process) is backed by a Mach <strong>task</strong> and every thread by a Mach thread. The
Mach layer is responsible for creating and terminating tasks/threads, context
switching threads on the CPU, and implementing primitives like locks, timers,
and scheduling queues. It also implements the VM system: each task has a
virtual address map, memory regions are backed by Mach <strong>memory objects</strong>, and
Mach can perform copy-on-write copy optimizations and map propagation. Notably,
Mach supports <strong>IPC-based memory sharing</strong> – one task can send a memory object
(or a port right to it) to another, enabling efficient shared memory or
transfer of large buffers without copying.</p>
<p><strong>BSD’s Role:</strong> The BSD component sits logically “on top” of Mach and provides
the traditional <strong>OS personality</strong> and services. This includes managing
<strong>processes</strong> (the BSD process table, PID allocation, user IDs, signals, etc.),
<strong>POSIX threads</strong> (which are mapped to Mach threads), and the entire set of
UNIX system calls (file systems, networking, IPC, device I/O, etc.). The BSD
kernel in XNU is derived primarily from FreeBSD (with substantial
OpenBSD/NetBSD influences and custom Apple modifications). It handles things
like:</p>
<ul>
<li><strong>VFS and File Systems:</strong> XNU’s BSD layer implements a VFS (Virtual File
System) and supports many file systems (HFS+, APFS, NFS, etc.). The file
system code runs in the kernel and interacts with storage drivers via I/O
Kit. Mach VM and BSD file systems meet when implementing memory-mapped files
– Mach calls into BSD to fetch pages from files on disk (via the vnode
pager).</li>
<li><strong>Network Stack:</strong> The entire TCP/IP stack (and other protocols like UDP,
ICMP, as well as higher-level sockets API) is in the BSD kernel. This code
came from BSD and is updated with modern standards. It interfaces with
network drivers (in I/O Kit) for packet send/receive.</li>
<li><strong>UNIX IPC:</strong> Besides Mach IPC, XNU provides traditional Unix IPC (signals,
pipes, SysV IPC, POSIX message queues, etc.) through the BSD layer. Signals,
for example, are implemented by the BSD kernel, but interestingly signals are
delivered using Mach exceptions under the hood – Mach exceptions are the
low-level mechanism, and the BSD code translates them to Unix signals for
processes as needed.</li>
<li><strong>Security and Credentials:</strong> The BSD layer manages user IDs, permissions,
access control, and integrates several security frameworks. For instance,
<strong>KAuth</strong> (Kernel Authorization) and the MAC Framework (Mandatory Access
Control) operate in the BSD layer. Features like the Sandbox, SIP, code
signing enforcement involve cooperation between BSD security modules and Mach
task port restrictions. The sandbox (Seatbelt) in macOS/iOS uses the
TrustedBSD MAC framework – when a system call is made, the MAC policy can vet
it. This happens in the BSD layer, though the sandbox’s configuration is set
from user space by launchd or other daemons.</li>
<li><strong>POSIX APIs and Environment:</strong> The BSD layer is what makes Darwin a UNIX. It
provides <code>/dev</code> management (which often links to I/O Kit devices), system
call table for standard C library calls, process forking (<code>fork()</code> is
implemented partly by Mach (VM copy-on-write) and partly by BSD (duplicating
file descriptors, etc.), and execve (loading binaries, setting up Mach task
states and BSD process states).</li>
</ul>
<p>In essence, one can think of Mach as the <strong>core kernel supervisor</strong> in XNU, and
BSD as a high-level kernel server that depends on Mach. The two are tightly
coupled – e.g., when a new BSD process is created via <code>fork()</code>, the kernel
internally calls Mach to create a new task and thread, then BSD code populates
the process structure and file descriptors. The BSD code calls Mach kernel
functions directly (not via message) using an internal API. Conversely, Mach
relies on some BSD services; for example, Mach has an abstraction called
“default pager” for managing swap. In XNU, the default pager is implemented
partly in user space (the <code>dynamic_pager</code> daemon) which uses Mach VM APIs to
create and manage swap files, but the BSD layer is involved in the actual file
I/O to the swap file. This shows a cooperative multi-tier design rather than
strictly separated layers.</p>
<p><strong>I/O Kit:</strong> The third pillar of XNU is the <strong>I/O Kit</strong>, Apple’s
object-oriented driver framework. I/O Kit runs in kernel space (as part of the
XNU kernel), but it is written in a restricted form of C++ for type-safety and
code reuse. The I/O Kit defines a class hierarchy for devices (buses, storage,
network, display, etc.) and drivers subclass these to implement support for
specific hardware. Drivers in I/O Kit live as C++ objects within the kernel,
but they interact with user space through well-defined interfaces. For
instance, an I/O Kit driver can publish properties accessible via the I/O
Registry, and can provide <strong>user client</strong> interfaces that allow user-space
applications or daemons to call into the driver in a controlled way. I/O Kit
also supports limited <strong>user-space drivers</strong> historically (via user clients),
but in practice, until recent DriverKit, most drivers ran in kernel. The Mach
component provides threading and synchronization primitives used by I/O Kit
(like locks and workloops), while the BSD component interacts with I/O Kit for
networking and disk I/O (e.g., the BSD filesystem code calls an I/O Kit disk
driver to read a block). The decision to use C++ in kernel (contrary to the
“not written in C++” myth; the core kernel logic is C, but drivers are C++
classes) was made to improve extensibility. By eliminating multiple inheritance
and exceptions, Apple ensured the kernel would not suffer C++ runtime overhead.
Many drivers can be loaded and unloaded dynamically as <strong>Kexts</strong> (kernel
extensions), which are essentially loadable bundles of I/O Kit C++ classes or
additional BSD/Mach code. XNU’s modularity in this sense is reminiscent of
other OS kernels that allow loadable modules, but Mach’s abstractions also help
here (each kext is essentially a Mach-O image loaded into kernel memory and
linked).</p>
<p><strong>Mach IPC and Message Passing:</strong> Even though XNU does not use Mach messages
for Unix <em>system calls</em>, Mach IPC is still heavily used throughout the system
for what we might call “RPC”-style interactions and for connecting
user-space services to kernel or to each other. Mach ports are the
foundation of various high-level features:</p>
<ul>
<li>Many kernel abstractions are represented as Mach ports to user space. For
example, each task (process) has a Mach port (the task port) that represents
its control handle. The kernel holds the rights, but certain privileged tasks
(like <code>launchd</code> or debugging tools) can obtain send rights to manipulate
other tasks (to start/stop them, inspect memory, etc.).</li>
<li>Mach <strong>notifications</strong> are used for event delivery. The WindowServer
(graphics system) receives user input events from the kernel via Mach
messages. Likewise, higher-level APIs like Grand Central Dispatch under the
hood use Mach ports to sleep threads waiting for events, leveraging Mach’s
port-set and message mechanism. The <code>kqueue/kevent</code> mechanism in BSD is
integrated: an event queue can wait on Mach port messages as well as file
descriptors, unifying the event sources.</li>
<li><strong>Inter-process Communication</strong> for system services: Apple’s entire <strong>XPC</strong>
framework (used by modern macOS/iOS for lightweight IPC between apps and
services) is built on Mach messages. Each XPC connection is essentially a
Mach port behind the scenes. The reason Mach IPC is chosen is its security
model – Mach ports have an associated rights system and live in the kernel,
so the kernel mediates who can send to whom. This allows checks like “is the
sender task entitled to send this message?” which is used in services like
the Keychain (securityd) to validate callers. Mach messages also support
carrying out-of-line memory (shared memory regions) and port rights, which is
extremely powerful for building higher-level RPC: you can send a file
descriptor (which is a BSD concept) as a Mach port right in a message,
enabling UNIX domain socket semantics via Mach. Under the hood, the file
descriptor send uses a Mach port representing that file in the receiving
task’s space.</li>
<li><strong>Remote Procedure Calls:</strong> Mach introduced MIG (Mach Interface Generator),
which is used to define interfaces where one side is in kernel and the other
in user. For example, the bootstrap server (launchd) and various system
servers use MIG to auto-generate code for sending/receiving messages. The
macOS <code>notify</code> system (for system-wide notifications) and many daemon APIs
are implemented with MIG definitions.</li>
</ul>
<p>Therefore, Mach IPC is a backbone for the macOS/iOS architecture beyond the
kernel boundary. It’s how user-space components talk to each other and to the
kernel in many cases. Even certain device drivers use Mach port notifications
(e.g., I/O Kit user clients might deliver an asynchronous event to a client via
a Mach message). The hybrid kernel thus uses Mach messaging where appropriate
(for asynchronous, out-of-band communication), and uses direct function calls
for in-kernel interactions. This hybrid approach retains Mach’s <strong>modularity</strong>
benefits – for instance, one can imagine refactoring a component to run in
user space with Mach messages without changing the other parts, since they
might already use a Mach port interface to talk to it. In fact, Apple did
exactly this with DriverKit: they moved certain drivers to user space and
replaced their in-kernel part with a Mach IPC conduit. The
performance-critical path (e.g., actual packet sending) might still be in
kernel, but higher-level policy or USB logic can be in a user process
communicating via Mach.</p>
<h3 id="scheduler-and-thread-management">Scheduler and Thread Management</h3>
<p>XNU’s scheduling is rooted in Mach’s scheduler, which was originally a
priority-based round-robin scheduler with support for threads and processor
sets. Over time, Apple has modified the scheduler significantly to meet the
needs of desktop and mobile. The scheduler manages threads (Mach threads)
across CPU cores (XNU supports SMP and on Apple Silicon, asymmetric cores). Key
points of XNU scheduling:</p>
<ul>
<li><strong>Priority Levels:</strong> Mach defines a range of thread priorities (0–127,
historically) with certain bands reserved for real-time, kernel, and normal
threads. Apple uses these priorities along with abstractions called
<strong>“sched_pri”</strong> and <strong>“base_pri”</strong> for each thread. Time-sharing threads have
priorities that can float based on usage (to implement favoring I/O-bound
threads), whereas real-time threads have fixed priorities. The highest
priorities are for critical kernel threads or timers.</li>
<li><strong>Run Queues:</strong> In classic Mach, each processor or processor-set had a run
queue for threads. XNU has per-CPU run queues for efficiency. It also has
mechanisms for <strong>scheduler interrupts</strong> to load-balance or preempt when
necessary.</li>
<li><strong>Extended Policies:</strong> Apple added features like <strong>container-level
prioritization</strong>. When iOS introduced App Sandbox with backgrounding, the
scheduler got a concept of a “task role” or “priority group”. In Darwin 9
(Leopard/iPhone OS), an “improved hierarchical process scheduling model” was
noted, which suggests that threads were grouped by tasks or by “workload”,
possibly to enforce limits on background tasks. This is likely the origin of
<strong>boosts and throttles</strong> that iOS uses to ensure the foreground app gets more
CPU than background apps.</li>
<li><strong>Quality of Service (QoS):</strong> In iOS 8 / OS X 10.10 and beyond, Apple
introduced QoS classes (user-interactive, user-initiated, default, utility,
background, etc.) for threads. The kernel scheduler integrates QoS by mapping
them to priority bands and scheduling deadlines. Threads created by Grand
Central Dispatch or NSThreads inherit a QoS that influences their scheduling
priority and which core they run on. This was further refined on Apple
Silicon where the scheduler might steer “background QoS” threads to
efficiency cores. Internally, XNU’s <code>sched_prim.c</code> and <code>sched_perf.c</code> (for
performance controller) handle these decisions. There is also an interface
for the kernel to ask the power management firmware about energy vs
performance (used in macOS’s power management QoS).</li>
<li><strong>Realtime and Multimedia:</strong> macOS supports realtime threads for audio or
critical tasks. The scheduler has a realtime queue and will preempt other
work to run RT threads to meet latency requirements. Also, since Mac OS X
10.4, XNU has <strong>scheduler deadlines</strong> for real-time threads (used for audio
playback, etc.), which is an EDF-like (Earliest deadline first) feature.</li>
<li><strong>Idle and Power:</strong> On mobile devices, the scheduler cooperates with the
power management to aggressively idle cores. Mach scheduler invokes an idle
thread when no work is available, and in iOS, if all cores are idle, the
system can enter low-power states quickly. Timer coalescing (10.9 Mavericks)
means the scheduler tries to batch wakeups – effectively, if several threads
have timers expiring, it aligns them to let CPU sleep longer intervals.</li>
</ul>
<p>Overall, XNU’s scheduler has evolved from Mach’s general-purpose design to one
aware of <strong>multi-core</strong> and <strong>heterogeneous</strong> cores, <strong>energy vs performance</strong>
trade-offs, and <strong>workload groupings</strong> (like apps vs system daemons vs kernel
threads). It still uses Mach’s thread data structures, but many scheduling
algorithms have been tuned by Apple (sometimes influenced by developments in
FreeBSD or other OSes).</p>
<h3 id="memory-management-and-virtual-memory">Memory Management and Virtual Memory</h3>
<p>Memory management in XNU is primarily handled by Mach’s VM subsystem, which is
one of Mach’s strongest components. Key aspects:</p>
<ul>
<li><strong>Virtual Address Space:</strong> Each Mach task has a virtual address space
represented by a set of <strong>VM maps</strong> and <strong>VM regions</strong>. When a process (task)
is created, it starts with a copy of the parent’s address space. Mach’s VM is
inherently copy-on-write – <code>fork()</code> doesn’t duplicate all memory immediately;
instead, both parent and child share pages marked copy-on-write until either
writes, then a fault triggers an actual copy. This makes <code>fork()</code> efficient
despite potentially large processes.</li>
<li><strong>Memory Objects and Pagers:</strong> Mach introduces the concept of <em>memory
objects</em> to represent a backing store for memory (like a file or the swap
area) and <em>pagers</em> which supply data to those memory objects on demand. In
XNU, the <strong>default pager</strong> (for anonymous memory) is implemented by the
<code>dynamic_pager</code> user-space daemon which manages swap files. That is, when the
kernel decides to evict a page from RAM, Mach will send a message to the
default pager indicating the page should be written to swap. The
<code>dynamic_pager</code> then writes to the swap file (via normal file I/O). This is a
classic microkernel design: the <strong>pager runs in user space</strong>, meaning the
policy of how to manage swap space is not fixed in kernel. By adjusting or
replacing dynamic_pager, one could change swapping behavior (e.g., macOS’s
dynamic_pager can create multiple swap files on demand). File memory is
managed by a different pager: the <strong>vnode pager</strong> inside the kernel (this one
is not user-space, but part of XNU’s BSD layer) which interacts with file
system code to read/write file data for memory mapped files. Having this
modular pager design made features like <strong>compressed memory</strong> feasible to
implement – in Mavericks, an in-kernel compression pager was added: when
pressure is high, instead of immediately paging to disk, XNU can compress
inactive pages and keep them in a reserved VM object (the “compressor pool”)
in RAM. The VM considers that as a form of pseudo-swapping (faster than
disk). Only if compression is insufficient does it resort to disk swap via
dynamic_pager.</li>
<li><strong>Physical Memory Management:</strong> XNU abstracts physical memory operations in a
machine-dependent layer called <strong>pmap</strong> (physical map). The pmap manages page
tables or equivalent structures on each architecture. When Mach allocates a
new VM region, it uses pmap to map virtual pages to physical frames. On
ARM64, pmap also integrates with security features (like marking pages with
the appropriate permission keys for PAC, or handling aliasing issues with
caches). The kernel uses a <strong>zone allocator</strong> for many kernel memory
structures (zones are pools for objects of fixed size, like VM map entries,
IPC port structures, etc.). There’s also a general-purpose <strong>kernel malloc</strong>
(kmem) for variable sizes. Notably, the XNU kernel employs strategies to
mitigate fragmentation and has guard pages for certain allocations to detect
overruns (on debug kernels, typically).</li>
<li><strong>Shared Memory and Map Inheritance:</strong> Mach VM allows tasks to share regions
– either explicitly via Mach IPC (sending a port for a memory object) or via
inheritance (a child can inherit memory from parent on fork with
copy-on-write or shared semantics). This is how the dynamic linker works in
macOS: the shared cache of frameworks is mapped into every process at launch
via a shared memory region provided by <code>dyld</code>. Mach makes this efficient by
mapping the same physical pages into all tasks read-only.</li>
<li><strong>Kernel Virtual Memory:</strong> The kernel itself has a virtual address space.
Mach manages kernel memory similarly to user memory, but there are
differences: the kernel uses a single map for all of kernel space, and some
regions are wired (non-pageable). XNU historically had a 32-bit kernel for
which it used tricky schemes like a shared address space with user processes
(in early OS X on 32-bit, kernel and user shared space to avoid costly
segment switches, but on 64-bit this was not an issue). Modern XNU (64-bit)
uses a separate address space for kernel, with portions (like the shared
cache, comm page) mapped into user for communication.</li>
<li><strong>Memory Protection:</strong> Mach’s design enforces that one task cannot access
another task’s memory unless explicitly allowed. This is basic memory
protection via separate address spaces. The only controlled sharing is via
Mach VM APIs or if the kernel (or a privileged daemon) maps memory into
another task (used by things like the debugger or by the system frameworks to
implement features like XPC shared memory). The kernel also zero-fills memory
on allocation to avoid leakage between processes.</li>
<li><strong>Evolution for Apple Silicon:</strong> On Apple Silicon, with large physical memory
and unified memory, XNU’s VM had to consider not having distinct GPU memory.
Instead, I/O Kit drivers for GPU allocate from general memory with certain
attributes (e.g., contiguous, protected). The pmap might have optimizations
for the extremely large virtual address space (ARMv8.5 supports 52-bit VA).
Also, memory tagging (MTE) if used would mean the kernel must manage tag
bits in pointers and memory; Apple hasn’t announced using it, but the
hardware is there on M2. If enabled, the kernel would tag allocations and
check tags on load/store, catching use-after-free or overflow.</li>
</ul>
<p>XNU’s VM is regarded as quite advanced due to its Mach heritage – it was built
for 64-bit from the start (Mach had 64-bit addressing on 32-bit hardware via
abstractions), and it’s relatively decoupled from the rest of kernel logic,
which is why Apple could plug in new features (compressor, different
pagers) without massive overhaul.</p>
<h3 id="virtualization-support">Virtualization Support</h3>
<p>While Mach’s original vision could be seen as a form of virtualization
(multiple OS personalities on one kernel), modern hardware virtualization is a
different beast. XNU did not originally include a hypervisor in early releases,
but as virtualization became important, Apple added support. On <strong>Intel Macs</strong>,
XNU gained the ability to manage the hardware virtualization extensions (Intel
VT-x) around 2014. Apple provided a <strong>Hypervisor.framework</strong> API to developers
from OS&nbsp;X 10.10 onward, enabling user-space virtualization without third-party
kernel extensions. Under the hood, a kernel extension (or part of XNU) would
configure VMX operations, allowing a user-space process to act as a virtual
machine monitor. This was used by tools like <strong>xhyve</strong> (a port of FreeBSD’s
bhyve to macOS), and by virtualization apps when running without their own
drivers.</p>
<p>On <strong>Apple Silicon (ARM64)</strong>, the approach is similar conceptually: XNU on
these devices can act as a Type-2 hypervisor using ARM’s virtualization
extensions (EL2 on ARM). Apple introduced a more full-featured
<strong>Virtualization.framework</strong> in macOS 11, which builds on an in-kernel
hypervisor to let developers run Linux or even macOS VMs in user space. One
design decision on Apple Silicon was to not allow arbitrary third-party
hypervisors in kernel; instead, the hypervisor is part of XNU but only
accessible via Apple’s frameworks with proper entitlements (to maintain
security).</p>
<p>From a kernel perspective, the XNU hypervisor functionality includes: managing
<strong>guest physical memory</strong>, handling <strong>trap and emulate</strong> for sensitive
instructions, and exposing virtual CPU interfaces to user space (for instance,
allowing a user program to set register state and run a vCPU until the next VM
exit). The Mach scheduling is leveraged to schedule vCPUs (which are just
threads from the host’s point of view). The memory subsystem is used to map
guest memory. On Apple Silicon, features like Stage 2 page tables for guests
are managed likely by an in-kernel hypervisor module.</p>
<p>Additionally, XNU supports containers and emulation in various ways (not full
virtualization but worth noting): the <strong>XNU kernel supports multiple user-space
“personalities”</strong> only in a limited sense now (for example, the Rosetta 2
x86_64 code translator on ARM is not a separate OS but it does require the
kernel to manage alternate CPU context for x86 state). The kernel includes an
<strong>x86 emulator</strong> or at least support for handling x86 code segmentation, etc.,
when Rosetta translates x86 code to ARM64 – primarily done in user space by
Rosetta’s JIT, the kernel might assist e.g. with syscall translation or by
providing an x86_64 syscall ABI on ARM.</p>
<p>The design choices around virtualization emphasize security and performance:
Apple’s approach is to keep the hypervisor simple and lean and to strongly
isolate guest OSes from the host (different Mach task, limited communication).
As of iOS 15, Apple even allows <strong>virtualization on iOS</strong> (to run Linux inside
an iPad app, for example, which some developers have demoed), indicating the
XNU hypervisor is capable on mobile as well, though subject to entitlement.</p>
<p>In summary, virtualization in XNU spans from the conceptual Mach
multi-personality support (not widely used in products outside early Classic
environment on OS&nbsp;X which ran Mac OS 9 in a para-virtualized setup), to robust
hardware-assisted virtualization on modern Macs.</p>
<h3 id="secure-computing">Secure Computing</h3>
<p>MacOS uses two complementary but distinct isolation mechanisms—<strong>Secure
Enclaves</strong> and the more recently introduced <strong>exclaves</strong>—to protect sensitive
operations and data.</p>
<p>The <strong>Secure Enclave</strong> is a dedicated, hardened subsystem integrated into
Apple’s SoCs (found in iPhones, iPads, Macs with T2 or Apple Silicon, etc.). It
runs its own microkernel‐based operating system (historically a variant of L4
called sepOS) and is used to manage and protect cryptographic keys, biometric
data, and other sensitive information. Its design isolates critical data even
if the main application processor or kernel is compromised. In short, it’s a
“trusted box” built into the hardware that handles security‑critical tasks
independently.</p>
<p><strong>Exclaves</strong>, by contrast, are a newer security innovation (first appearing in
macOS 14.4 and iOS 17) that further subdivide the operating system’s
privileges. Instead of having all sensitive operations run in the same
privileged domain as the main XNU kernel, Apple is now isolating key resources
into separate, “externally located” domains. These resources—such as Apple ID
services for virtual machines, audio buffers, sensor data, and even components
that manage indicator lights—are pre‑configured at boot and are managed by
specialized kernel extensions (e.g., ExclaveKextClient.kext,
ExclaveSEPManagerProxy.kext, and ExclavesAudioKext.kext) along with private
frameworks.</p>
<p>In geographical terms, an enclave is a territory entirely enclosed within
another, which aptly describes the Secure Enclave’s containment within the SoC.
An exclave, on the other hand, is a fragment that is separated from the main
territory yet still associated with it—mirroring how these isolated resources
exist outside the main XNU kernel while remaining tightly integrated with the
overall system. This separation means that even if the main kernel is
compromised, the operations running in exclaves remain insulated, offering
additional defense in depth.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Darwin and XNU offer a fascinating case study of an operating system that is
<em>neither fully microkernel nor monolithic</em>, but a judicious mix. Its evolution
illustrates trade-offs in OS design: performance vs. modularity, generality vs.
specialization. XNU’s Mach-based core, once considered a performance liability,
has proven to be a strength in adapting to new architectures and enabling
system-wide features (like seamless multi-OS integration on Apple Silicon, or
fine-grained sandboxing). Meanwhile, the BSD layer ensured that developers and
applications have a rich, POSIX-compliant environment, with all the expected
UNIX tools and APIs, greatly smoothing the adoption and software portability
for the platform.</p>
<p>In the modern era, as hardware trends move towards specialized processors and
increased parallelism, XNU continues to incorporate new techniques (e.g.,
dispatch queues, QoS scheduling, and direct support for machine learning
accelerators through drivers) while maintaining robustness. The Darwin OS,
through open source releases, also provides researchers a window into a
commercial-grade hybrid kernel (albeit not a very good window), inspiring
efforts in OS architecture that blend ideas from both camps of the classic
microkernel debate.</p>
<p>Apple’s Darwin has thus grown from a niche NeXTSTEP OS to the core of millions
of devices, all the while <strong>tracing a line of continuity back to Mach and
BSD</strong>. Each major transition – be it new CPU architectures (PowerPC→Intel→ARM),
new device categories, or new security paradigms – has been met by XNU with an
architectural answer: <em>extend</em> (not rewrite) the kernel, <em>integrate</em> components
tightly when needed, and <em>isolate</em> through Mach IPC when possible. This
balanced evolution of Darwin’s kernel showcases a successful long-term OS
design, one that remains at the forefront of commercial operating systems while
rooted in decades of operating systems research.</p>
<p><strong>References:</strong></p>
<ul>
<li>Singh, Amit. <em>Mac OS X Internals: A Systems Approach</em>. Addison-Wesley, 2006
(for historical and architectural insights).</li>
<li>Apple Developer Documentation – Kernel Architecture Overview (<a href="https://developer.apple.com/library/archive/documentation/Darwin/Conceptual/KernelProgramming/Architecture/Architecture.html">Kernel
Architecture
Overview</a>).</li>
<li>Wikipedia: <em>XNU Kernel</em> (<a href="https://en.wikipedia.org/wiki/XNU">XNU -
Wikipedia</a>), <em>Darwin OS</em> release history
(<a href="https://en.wikipedia.org/wiki/Darwin_(operating_system)">Darwin (operating system) -
Wikipedia</a>).</li>
<li>Chisnall, David. “What Is Mac OS X?” InformIT (2010) (<a href="https://www.informit.com/articles/article.aspx?p=1552774">What Is Mac OS X? | A
Mach-O System |
InformIT</a>).</li>
<li>24C3: <em>Inside the Mac OS X Kernel</em> (Ilja van Sprundel, 2007) (<a href="https://fahrplan.events.ccc.de/congress/2007/Fahrplan/attachments/986_inside_the_mac_osx_kernel.pdf">Inside the Mac
OS X
Kernel</a>).</li>
<li>Mazurek, Karol. “Snake&amp;Apple X.NU” Medium (2024) (<a href="https://karol-mazurek.medium.com/snake-apple-x-nu-0bc5c36170da">Snake&amp;Apple X.NU |
Medium</a>).
(Security-focused kernel internals series).</li>
</ul>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p><a href="https://en.wikipedia.org/wiki/Return-oriented_programming">Return-oriented
programming</a> is
a computer security exploit technique that allows an attacker to execute
code in the presence of security defenses such as executable-space
protection and code signing. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p><a href="https://worthdoingbadly.com/hv/">Hardware-accelerated virtual machines on jailbroken iPhone 12 / iOS 14.1
| Worth Doing Badly</a> <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
</ol>
</section>   </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The ADHD Body Double: A Unique Tool for Getting Things Done (296 pts)]]></title>
            <link>https://add.org/the-body-double/</link>
            <guid>43597425</guid>
            <pubDate>Sat, 05 Apr 2025 22:45:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://add.org/the-body-double/">https://add.org/the-body-double/</a>, See on <a href="https://news.ycombinator.com/item?id=43597425">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>Can something as simple as another person’s presence make it easier to stay on task?</p>



<p>While there’s no research to prove its effectiveness, ADHD body doubling is helping many people get things done – starting with my client, David.</p>



<p>When I first met David, he was a retired vice president of a major corporation. In his “retirement,” he ran three businesses out of his home office, two of them overseas.</p>



<p>Observing David in his workspace, I found him to be quite organized. He wasn’t sitting eyeball-high in papers. In fact, he hardly had any papers around him at all.</p>



<p>He had working systems in place, but felt terribly disorganized and distracted.</p>



<p>David contacted me because I specialize in coaching <a href="https://add.org/adhd-in-adults/">adults with Attention Deficit Hyperactivity Disorder (ADHD)</a>. During my twenty-three years of coaching, I have gathered many tools and strategies for helping clients accomplish desired change and create order out of chaos.</p>



<p>The tool I want to share with you today is one born of the unique mix of client and coach inventing together. I call it <em>the body double.</em></p>



<p><em><strong>Originally published in 1996, this article was republished on February 20th, 2025.</strong></em></p>



<figure><img decoding="async" width="1920" height="1280" src="https://add.org/wp-content/uploads/2025/02/A-man-reading-an-article-online.jpg" alt="A man reading an article online" srcset="https://add.org/wp-content/uploads/2025/02/A-man-reading-an-article-online.jpg 1920w, https://add.org/wp-content/uploads/2025/02/A-man-reading-an-article-online-300x200.jpg 300w, https://add.org/wp-content/uploads/2025/02/A-man-reading-an-article-online-1024x683.jpg 1024w, https://add.org/wp-content/uploads/2025/02/A-man-reading-an-article-online-768x512.jpg 768w, https://add.org/wp-content/uploads/2025/02/A-man-reading-an-article-online-800x533.jpg 800w, https://add.org/wp-content/uploads/2025/02/A-man-reading-an-article-online-1536x1024.jpg 1536w, https://add.org/wp-content/uploads/2025/02/A-man-reading-an-article-online-600x400.jpg 600w" sizes="(max-width: 1920px) 100vw, 1920px"></figure>



<h2><strong>What Is ADHD Body Doubling?</strong></h2>



<p>The methodology of ADHD body doubling is a productivity strategy used by individuals with ADHD to finish possibly annoying jobs while having another person beside them. This person is the body double. The body double’s duty is to keep the individual with ADHD focused on the task at hand to reduce potential distractions and increase motivation.</p>



<p>David had been surprised by a recent diagnosis of ADD, which explained his lifelong difficulty accomplishing mundane tasks that others seemed to handle with ease.</p>



<p>As a VP in the corporate world, he had always had an executive assistant to connect the dots and pull loose ends together. Now, however, he had the time to do these tasks. He even had the will to do them. He just couldn’t stay on track.</p>



<p>David’s wife also ran a business out of their home and had her own well-organized office. She offered to advise him, but they both quickly agreed this just didn’t work.</p>



<p>That’s when they asked for help.</p>



<p>First, I helped David modify his storage systems. Then, together, we honed his time management and prioritization systems.</p>



<p>Nothing, however, seemed to address the problem of his inattention and distraction. There were days, too many days, when he easily got off track or found it hard to transition from one task to another.</p>



<p>Frustrated, puzzled, and somewhat embarrassed, he confessed, “You know, it seems that, sometimes, if I just have my wife sitting in a chair nearby, I can accomplish more than if I’m alone.”</p>



<p>Though reluctantly, David had identified a strategy that really worked for him. I instantly realized that I had seen this same effect before with other clients.</p>



<p>There were times when just having someone in proximity (not advising, sorting, or strategizing) brought clarity and focus. I felt it. I knew my clients felt it, too.</p>



<p>This phenomenon of <em>just being there</em>, which David first described out loud, I named the body double.</p>



<figure><img decoding="async" width="1280" height="853" src="https://add.org/wp-content/uploads/2024/04/Working-productively-with-a-body-double.jpg" alt="Working productively with a body double" srcset="https://add.org/wp-content/uploads/2024/04/Working-productively-with-a-body-double.jpg 1280w, https://add.org/wp-content/uploads/2024/04/Working-productively-with-a-body-double-300x200.jpg 300w, https://add.org/wp-content/uploads/2024/04/Working-productively-with-a-body-double-1024x682.jpg 1024w, https://add.org/wp-content/uploads/2024/04/Working-productively-with-a-body-double-768x512.jpg 768w, https://add.org/wp-content/uploads/2024/04/Working-productively-with-a-body-double-800x533.jpg 800w, https://add.org/wp-content/uploads/2024/04/Working-productively-with-a-body-double-600x400.jpg 600w" sizes="(max-width: 1280px) 100vw, 1280px"></figure>



<h2><strong>How Does Body Doubling Work?</strong></h2>



<p>For many people with ADHD, finding the motivation to get the ball rolling can be quite a challenge. This can lead to procrastination. <a href="https://www.tandfonline.com/doi/full/10.1080/00050067.2023.2218540#abstract" target="_blank" rel="noreferrer noopener"><sup>[1]</sup></a> They may also find themselves easily distracted by unrelated thoughts or activities<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10421702/" target="_blank" rel="noreferrer noopener"><sup>[2]</sup></a></p>



<p>This is where body doubling comes into play!&nbsp;</p>



<p>To start body doubling, all you need is a family member, colleague, or peer. It can be done physically or virtually, as long as someone is present while you work. You and your body double will agree on a set time and block the session out on your calendar.&nbsp;</p>



<p>The aim of a body doubling session is for you and your body double to work alongside each other. You don’t have to do the same thing. What’s most important is that both of you are working on <em>something</em>. That could be doing the laundry, paying bills, exercising, or completing a work project.&nbsp;</p>



<p>It also helps to share your goals with your body double at the start of the session. This allows them to hold you accountable for what you need or want to achieve.&nbsp;</p>



<p>Body doubling is effective because it helps create a strong sense of accountability. That extra bit of pressure from being watched can go a long way toward holding you responsible. That way, you’re more likely to follow through on your actions.&nbsp;</p>



<p>Body doubling can also help to mix things up. It adds freshness to a boring routine. This can be helpful since the ADHD brain craves novelty.<a href="https://link.springer.com/article/10.1007/s40806-024-00400-8" target="_blank" rel="noreferrer noopener"><sup>[3]</sup></a></p>



<h2><strong>Why Does Body Doubling Work?</strong></h2>



<p>There are a few possible explanations as to why a body double works as a strategy for getting through challenging or boring tasks. In the moment, it can counter <a href="https://add.org/signs-of-adhd/">ADHD symptoms</a> that sap motivation, focus, and energy.</p>



<h3><strong>A Supportive Presence Adds Motivation</strong></h3>



<p>The simplest is that the body double serves as a physical anchor for the distracted individual who feels more focused by the presence of another person in their space.</p>



<p>The distracted person feels responsible to and for the body double.</p>



<p>This perception translates as­–<em>I can’t waste this gift of time.</em></p>



<p>Another explanation might be that the body double serves as a kind of <em>mirror – </em>a calm reflection for the individual with ADHD of how their <a href="https://add.org/sensory-overload-adhd/">over-stimulated mind and body</a> would like to be at the moment.</p>



<p>This concept is called ADHD mirroring. It happens when someone with ADHD follows the behaviors of other people, often unconsciously, to fit in. In many cases, ADHD mirroring can be draining, as it takes energy to mask symptoms of ADHD by copying others.&nbsp;</p>



<p>However, in the case of body doubling, the unconscious act of mirroring can be beneficial. Watching someone else stay focused on a task can naturally encourage the person with ADHD to do the same.&nbsp;</p>



<figure><img loading="lazy" decoding="async" width="1920" height="1280" src="https://add.org/wp-content/uploads/2024/04/People-with-ADHD-practicing-the-body-double-technique.jpg" alt="People with ADHD practicing the body double technique" srcset="https://add.org/wp-content/uploads/2024/04/People-with-ADHD-practicing-the-body-double-technique.jpg 1920w, https://add.org/wp-content/uploads/2024/04/People-with-ADHD-practicing-the-body-double-technique-300x200.jpg 300w, https://add.org/wp-content/uploads/2024/04/People-with-ADHD-practicing-the-body-double-technique-1024x683.jpg 1024w, https://add.org/wp-content/uploads/2024/04/People-with-ADHD-practicing-the-body-double-technique-768x512.jpg 768w, https://add.org/wp-content/uploads/2024/04/People-with-ADHD-practicing-the-body-double-technique-800x533.jpg 800w, https://add.org/wp-content/uploads/2024/04/People-with-ADHD-practicing-the-body-double-technique-1536x1024.jpg 1536w, https://add.org/wp-content/uploads/2024/04/People-with-ADHD-practicing-the-body-double-technique-600x400.jpg 600w" sizes="(max-width: 1920px) 100vw, 1920px"></figure>



<h3><strong>It Provides a Model of Calm Focus</strong></h3>



<p>The body double becomes a model of control and a mirror, confidently reflecting the message: <em>I can concentrate. I am working. I am focused</em>.</p>



<p>This theory might actually carry some weight. In the 1980s, neuroscientist Giacomo Rizzolatti, MD, along with colleagues at the University of Parma, made a serendipitous discovery while researching macaque monkeys. They observed that specific neurons in the macaque’s brain fired when watching another monkey, or a human, reach for a peanut. They named these neurons “mirror neurons.”<a href="http://blogs.scientificamerican.com/guest-blog/whats-so-special-about-mirror-neurons/" target="_blank" rel="noreferrer noopener"><sup>[4]</sup></a></p>



<p>The researchers theorized that mirror neurons also existed in humans and were the likely explanation for our ability to emulate and empathize with others.</p>



<p>As much as this makes sense and sounds like the perfect validation for the body double effect, please note that the scientific community is somewhat divided about whether humans actually have mirror neurons.</p>



<p>So, the mirror neuron theory does not give us a definitive answer.</p>



<h3><strong>Adds Positive Energy to the Workspace</strong></h3>



<p>I have one more explanation to offer as to why the body double might work.</p>



<p>In Eastern cultures, energy is referred to as chi (or qi). It is viewed as being either in or out of balance in the human body and the surrounding environment.</p>



<p>Acupuncturists move chi, or energy, to recreate balance and promote health. There are many forms of exercise and meditation, tai chi and chi gong, to name a few, which are about the management and flow of energy.</p>



<p>Feng shui (fung shway) is the 5,000-year-old art of balancing positive and negative chi in the space around us, with the goal of optimizing healthy energy in one’s living and working space.</p>



<p>What would chi have to do with a body double? The body double might be a chi balancer or protective barrier helping to contain and calm the energy in and around the person with ADHD. The body double might also be a buffer against distracting energy from the outside, ready to bombard the vulnerable <a href="https://add.org/adhd-brain/">ADHD brain</a>.</p>



<p>Whichever explanation you prefer, there’s no denying the effectiveness of the body double.</p>



<p>Consider this strategy a gift from David and the many other ADHDers who have experienced its magic. If a task requires your attention and seems impossible to complete alone – beg, borrow, or pay for a body double to sit in the chair next to you.</p>



<figure><img loading="lazy" decoding="async" width="1280" height="853" src="https://add.org/wp-content/uploads/2024/04/Colleagues-working-together.jpg" alt="Colleagues working together" srcset="https://add.org/wp-content/uploads/2024/04/Colleagues-working-together.jpg 1280w, https://add.org/wp-content/uploads/2024/04/Colleagues-working-together-300x200.jpg 300w, https://add.org/wp-content/uploads/2024/04/Colleagues-working-together-1024x682.jpg 1024w, https://add.org/wp-content/uploads/2024/04/Colleagues-working-together-768x512.jpg 768w, https://add.org/wp-content/uploads/2024/04/Colleagues-working-together-800x533.jpg 800w, https://add.org/wp-content/uploads/2024/04/Colleagues-working-together-600x400.jpg 600w" sizes="(max-width: 1280px) 100vw, 1280px"></figure>



<h2><strong>Benefits of Body Doubling</strong></h2>



<p>Body doubling can be a powerful tool, especially if you struggle with focus and motivation.&nbsp;</p>



<p>If you’re not already convinced to give it a try, here are the possible benefits of body doubling:&nbsp;</p>



<ul>
<li><strong>Improved focus:</strong> Having a body double helps you stay accountable. This technique creates positive reinforcement by gently nudging you to focus and stay on track.&nbsp;</li>



<li><strong>Increased motivation:</strong> The presence of someone else can create a sense of accountability and novelty, increasing your motivation to get the ball rolling.&nbsp;</li>



<li><strong>Reduced procrastination:</strong> By planning and setting aside time to work alongside someone else, you’re more likely to get started when the time comes. This helps you avoid postponing and delaying your tasks.&nbsp;</li>



<li><strong>Improved time management:</strong> You can dedicate a specific time block in your day to body doubling sessions. This reduces wasted time due to delays and can save time by minimizing distractions.&nbsp;</li>



<li><strong>Improved mood:</strong> Having someone around you when you work can create a positive atmosphere and reduce stress. Your body double can also serve as a source of support, cheering you on as you reach your session goals.&nbsp;</li>



<li><strong>Boosted productivity:</strong> When focused and motivated, you’re bound to get more done in less time.&nbsp;</li>
</ul>



<p>Of course, if you want to reap these benefits and get the best results, it’s essential to find a suitable body double. The right body double won’t distract you or create <em>too much</em> pressure. Instead, their presence should help you feel encouraged, empowered, and ready to tackle your to-do list!&nbsp;</p>



<h2><strong>What to Look for in a Body Double</strong></h2>



<p>Who and what should you look for in a body double?</p>



<p>Find someone who can be quiet and independent. They can sit, read, knit, or work quietly on a laptop. Their job is to not engage with you.</p>



<p>It requires energy to instruct, supervise, or be interrupted by another person, and that expenditure of energy equates to distraction.</p>



<p>This is not to say that you do not need to hire outside help – a professional organizer or office assistant.</p>



<p>You may also consider finding an <a href="https://add.org/how-to-find-an-adhd-coach/">ADHD coach</a> to help you identify the best strategies for you.</p>



<p>I encouraged David to hire a student, retiree, or fellow church member to sit and keep him company. His wife may not be the best person for the job, but he could use her in a pinch.</p>



<p>Following our discovery, David occasionally hired short-term office support help, who sometimes served as a body double and other times as office assistants. Knowing when to hire someone to help with office work or with just paying the bills is a valuable skill in getting things done.</p>



<p>There are also many different ways to try this technique. Some people prefer body doubling for longer periods, while others find shorter sessions with small breaks in between more effective.</p>



<p>You can also choose between in-person groups, online sessions with your camera and microphone on, or virtual focus groups with a chat-only feature.</p>



<p>Additionally, try to experiment with different times of the day. Some ADHDers find that body doubling boosts their productivity in the morning but doesn’t work as well in the evening, or vice versa.</p>



<p>Ultimately, not all body-doubling techniques work for everyone. To find what works best for you, you can test it with different people, groups, timings, and structures.</p>



<p>Regardless of the approach, body doubling can be an effective accountability tool for hitting your daily goals.</p>



<figure><img loading="lazy" decoding="async" width="1920" height="1280" src="https://add.org/wp-content/uploads/2025/02/Colleagues-focused-on-their-work.jpg" alt="Colleagues focused on their work" srcset="https://add.org/wp-content/uploads/2025/02/Colleagues-focused-on-their-work.jpg 1920w, https://add.org/wp-content/uploads/2025/02/Colleagues-focused-on-their-work-300x200.jpg 300w, https://add.org/wp-content/uploads/2025/02/Colleagues-focused-on-their-work-1024x683.jpg 1024w, https://add.org/wp-content/uploads/2025/02/Colleagues-focused-on-their-work-768x512.jpg 768w, https://add.org/wp-content/uploads/2025/02/Colleagues-focused-on-their-work-800x533.jpg 800w, https://add.org/wp-content/uploads/2025/02/Colleagues-focused-on-their-work-1536x1024.jpg 1536w, https://add.org/wp-content/uploads/2025/02/Colleagues-focused-on-their-work-600x400.jpg 600w" sizes="(max-width: 1920px) 100vw, 1920px"></figure>



<h2><strong>Where to Find a Body Double?</strong></h2>



<p>One of the best places to find a suitable body double is in your own community. You can get a friend or family member to work alongside you physically or virtually.</p>



<p>Alternatively, you can consider hiring someone to spend time sitting beside you as you get things done.</p>



<p>Virtual body doubling can also work wonders. You can work with a group or partner from anywhere in the world at any time. For this, you’d typically be matched through an online platform.</p>



<p>Various websites and apps can connect you to a body double from their own community. For instance, FocusMate allows you to connect with a virtual body double from anywhere in the world to co-work and get things done.</p>



<p>Another example is the <a href="https://add.org/adda-virtual-peer-support-group-studyhall-pomodoro-style/">ADDA Productivity PowerHour+ support group</a>, which combines body doubling with another time management strategy called the Pomodoro technique. You’d participate in these sessions with a group of ADDA members.</p>



<h2><strong>Using Body Doubling for ADHD</strong></h2>



<p>In lieu of any proven scientific explanation, the body double is a chair holder, space taker-upper, karmic anchor, or a wedge between you and the door.</p>



<p>Hopefully, it might be a helpful tool in your magic bag of tricks to use at just the right time to get a job done and stay on track.</p>



<h2><strong>Using External Accountability to Boost Your Productivity</strong></h2>



<p>Because of how the ADHD brain is wired, being productive can be easier said than done. The good news is that you don’t have to rely on willpower alone!&nbsp;</p>



<p>External accountability is one of many practical tools for boosting productivity. Body doubling enables you to find this external accountability in an easy and structured way.&nbsp;</p>



<p>If you’d like to learn more about adult ADHD, <a href="https://add.org/adda-plus/">ADDA+</a> offers 200+ webinars, peer support groups, work groups, and much more.</p>



<p>Linda Anderson, MA, MCC, SCAC, is a master certified coach, a leader in the field of ADHD coaching, and the founder of <a href="https://gettingclear.com/">Getting Clear</a>.</p>



<h2><strong>References</strong></h2>



<p>[1] Ruth Netzer Turgeman, &amp; Pollak, Y. (2023). Using the temporal motivation theory to explain the relation between ADHD and procrastination. Using the Temporal Motivation Theory to Explain the Relation between ADHD and Procrastination, 58(6), 1–9. <a href="https://doi.org/10.1080/00050067.2023.2218540" target="_blank" rel="noreferrer noopener">https://doi.org/10.1080/00050067.2023.2218540</a></p>



<p>‌[2] Osborne, J. B., Zhang, H., Carlson, M., Shah, P., &amp; Jonides, J. (2023). The association between different sources of distraction and symptoms of attention deficit hyperactivity disorder. Frontiers in psychiatry, 14, 1173989. <a href="https://doi.org/10.3389/fpsyt.2023.1173989" target="_blank" rel="noreferrer noopener">https://doi.org/10.3389/fpsyt.2023.1173989</a></p>



<p>[3] Anne-Laure Le Cunff. (2024). Distractibility and Impulsivity in ADHD as an Evolutionary Mismatch of High Trait Curiosity. Evolutionary Psychological Science. <a href="https://doi.org/10.1007/s40806-024-00400-8" target="_blank" rel="noreferrer noopener">https://doi.org/10.1007/s40806-024-00400-8</a></p>



<p>[4] Thomas, B. (November 6, 2012) What’s So Special about Mirror Neurons? Scientific American. Retrieved May 1, 2016, from <a href="http://blogs.scientificamerican.com/guest-blog/whats-so-special-about-mirror-neurons/" target="_blank" rel="noreferrer noopener">http://blogs.scientificamerican.com/guest-blog/whats-so-special-about-mirror-neurons/</a></p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ten Rules for Negotiating a Job Offer (396 pts)]]></title>
            <link>https://haseebq.com/my-ten-rules-for-negotiating-a-job-offer/</link>
            <guid>43596864</guid>
            <pubDate>Sat, 05 Apr 2025 21:15:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://haseebq.com/my-ten-rules-for-negotiating-a-job-offer/">https://haseebq.com/my-ten-rules-for-negotiating-a-job-offer/</a>, See on <a href="https://news.ycombinator.com/item?id=43596864">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p>When <a href="https://haseebq.com/farewell-app-academy-hello-airbnb-part-i/">the story of how I landed a job at Airbnb</a> went viral, I was surprised at how infatuated people were with my negotiations. Media stories portrayed me as some kind of master negotiator—a wily ex-poker-player who was able to con the tech giants into a lucrative job offer.</p>

<p>This is silly. It’s silly for a lot of reasons, but one of the main ones is that in reality, my negotiation skills are nothing special. There are lots of job candidates who are better negotiators than I, to speak nothing of recruiters and other professional negotiators.</p>

<p>It just so happens that most people don’t negotiate at all, or if they do, they just negotiate just enough to satisfy themselves that they did.</p>

<p>Worse yet, most of the advice out there on negotiation is borderline useless. Almost anything you read on the subject will be a vague and long-winded exhortation to “make sure you negotiate” and “never say the first number.” Beyond those two morsels of advice, you’re pretty much on your own.</p>

<p>I thought to myself: why is there so little actionable advice out there about negotiation? I suspect it’s because deep down, many people believe that negotiation is inexplicable, that it’s something some people can do and others can’t, and that there’s no real way to break it down so anyone can learn it.</p>

<p>I say that’s bullshit. Negotiation is a skill that can be learned just like any other, and I don’t believe it’s particularly elusive or hard to understand. So I’m going to try to explain how anyone can do it.</p>

<p>Three caveats.</p>

<p>First: I’m not an expert. There are people who really are experts at this, and when my advice contradicts theirs, you should assume I’m wrong.</p>

<p>Second: negotiation is tricky to generalize about because it’s deeply intertwined with social dynamics and power. The appropriate advice for an Asian male in Silicon Valley may not be appropriate for a black woman in Birmingham, Alabama. Racial, sexual, and political dynamics accompany you to the negotiating table.</p>

<p>At the same time, I want to caution against overemphasizing these factors. Being afraid to negotiate out of fear of discrimination can often be just as deleterious as discrimination itself.</p>

<p>Ceteris paribus, negotiate aggressively.</p>

<p>Third: I’m the first to admit that negotiation is stupid. It’s a practice that inherently benefits those who are good at it, and is an absurd axis on which to reward people. But it’s a reality of our economic system. And like most collective action problems, we’re probably not going to be able to abolish it any time soon. In which case, you might as well improve at it.</p>

<p>So here’s my guide to negotiation. It’s going to be split into two parts: this first part will be about conceptualizing the negotiating process, about how to begin the process and set yourself up for maximal success. The second part will be advice on the actual back-and-forth portion of negotiating and how to ask for what you want.</p>

<p>Let’s take it from the top.</p>

<h2 id="what-it-means-to-get-a-job">What it means to “get a job”</h2>

<p>In our culture we call entering the employment market “trying to get a job.” This is an unfortunate turn of phrase. “Getting a job” implies that jobs are a resource out in the world, and you’re attempting to secure one of these resources. But that’s completely backwards. What you are actually doing is selling your labor, and a company is bidding for it.</p>

<p><strong>Employment is just striking a mutual deal in the labor market.</strong></p>

<p>Like any market, the labor market only functions well if it’s competitive. This is the only way to ensure fair and equitable pricing. Imagine you were a farmer selling watermelons. Would you just sell your watermelons to the first buyer who agreed to purchase them? Or would you survey the marketplace of buyers, see the best price (and business partner) you could get, and then make an informed decision on which buyer to sell to?</p>

<p>And yet, when people talk about the labor market, they think “oh, a company wants to <em>give me a job</em>! What a relief!” As though having a job were in itself some special privilege for which a company is the gatekeeper.</p>

<p>Dispel yourself of this mindset.</p>

<p>A job is just a deal. It is a deal between you and a company to exchange labor for money (and other things you value).</p>

<p>This might sound like an abstract point, but you should absolutely approach negotiation from this perspective.</p>

<h2 id="the-role-of-negotiation">The role of negotiation</h2>

<p>Negotiating is a natural and expected part of the process of trying to make a deal. It’s also a signal of competence and seriousness. Companies generally respect candidates who negotiate, and most highly attractive candidates negotiate (if for no other reason, because they often have too many options to choose from).</p>

<p>At the risk of spouting truisms: always, always negotiate. Doesn’t matter how good or bad you think you are. You never damage a relationship by negotiating.</p>

<p>In all my time as an instructor at App Academy, out of hundreds of offers negotiated, only once or twice were offers ever rescinded in negotiations. It basically never happens. And when it does, usually the candidate was being an unconscionable asshole, or the company was imploding and needed an excuse to rescind the offer.</p>

<p>You might think to yourself: “<em>well, I don’t want to set high expectations, and the offer is already generous, so I ought to just take it.</em>“</p>

<p><strong>No. Negotiate.</strong></p>

<p>Or maybe: “<em>I don’t want to start off on the wrong foot and look greedy with my future employer.</em>“</p>

<p><strong>No. Negotiate.</strong></p>

<p>“<em>But this company is small and—</em>“</p>

<p><strong>No. Shut up. Negotiate.</strong></p>

<p>We’ll talk more in the next section about why a lot of these objections are bullshit, and fundamentally misapprehend the dynamics of hiring. But for now, just trust me that you should always negotiate.</p>



<p>I’ve tried to boil down negotiation to ten rules. The rules, in order of appearance, are:</p>

<ol>
  <li>Get everything in writing</li>
  <li>Always keep the door open</li>
  <li>Information is power</li>
  <li>Always be positive</li>
  <li>Don’t be the decision maker</li>
  <li>Have alternatives</li>
  <li>Proclaim reasons for everything</li>
  <li>Be motivated by more than just money</li>
  <li>Understand what they value</li>
  <li>Be winnable</li>
</ol>

<p>We’ll only get through some of these in this blog post, and the rest will appear in the second part. But I’ll explain each rule as we get to it.</p>

<p>So let’s start from the top and try to walk through a negotiation process from the very beginning. For most, that starts when you receive an offer.</p>

<h2 id="the-offer-conversation">The offer conversation</h2>

<p>You’ve just received the phone call: your interview went well, and after much deliberation they decided they like you. They want to make you an offer. Congratulations!</p>

<p>Don’t get too excited though. The fun is just getting started.</p>

<p>Thank your recruiter. Sound excited—hopefully this won’t be hard. Before jumping into details, try to ask for specific feedback on your interview performance. If they give it to you, this will help you gauge how much they want you, as well as tell you things you can improve on in your next interview(s).</p>

<p>Now time to explore the offer.</p>

<p><strong>Rule #1 of negotiating: have everything in writing.</strong></p>

<p>Eventually, they’ll give you information about the offer. Write it all down. Doesn’t matter if they’re going to send you a written version later, <strong>write everything down</strong>. Even if there are things that are not directly monetary, if they relate to the job, write them down. If they tell you “we’re working on porting the front-end to Angular,” write that down. If they say they have 20 employees, write that down. You want as much information as you can. You’ll forget a lot of this stuff, and it’s going to be important in informing your final decision.</p>

<p>Depending on the company, they’ll also tell you about the equity package. We’ll look more specifically at equity in part II, but be sure to write everything down.</p>

<p>The rule from here on out is that everything significant you discuss will have some kind of a paper trail. Often, the company won’t even send you an official offer letter until a deal is finalized. So it falls to you to confirm all of the important details in subsequent e-mails.</p>

<p>So yadda yadda, lots of details, writing stuff down, oh there’s a joke, time to laugh. Now the recruiter is done talking and you’re done asking all of your questions.</p>

<p>Your recruiter will now say something along the lines of “<em>so what do you think?</em>“</p>

<p>This seems innocuous, but your reply here is critical, because there’s a lot you can say to weaken your position. This is your first decision point.</p>

<p>A decision point is a moment in the negotiation where your interlocutor wants to compel you to make a decision. If they succeed in tying you to a position, they will close the door on further negotiating. Of course “what do you think?” is a subtle prod. But it is the beginning of many attempts to get you to make a premature commitment.</p>

<p><strong>This leads to rule #2 of negotiating: always keep the door open.</strong> Never give up your negotiating power until you’re absolutely ready to make an informed, deliberate final decision.</p>

<p>This means your job is to traverse as many of these decision points as possible without giving up the power to continue negotiating. Very frequently, your interlocutor will try to trick you into making a decision, or tie you to a decision you didn’t commit to. You must keep verbally jiu-jitsu-ing out of these antics until you’re actually ready to make your final decision.</p>

<h2 id="protecting-information">Protecting information</h2>

<p>There’s an uncomfortable silence by now, and their “<em>what do you think?</em>” is hanging in the air.</p>

<p>If you say “<em>yes, that sounds amazing, when do I start?</em>” you implicitly accept the offer and completely close the door on the negotiation. This is your recruiter’s number one favorite thing to hear. It stands to reason you probably shouldn’t do this.</p>

<p>But their second favorite thing to hear you say is “<em>can you do 90K instead of 85K?</em>” This also closes the door, but for a different and more subtle reason. And it’s the number one reason why most people suck at negotiation.</p>

<p><strong>Rule #3 of negotiating: information is power.</strong> To protect your power in the negotiation, you must protect information as much as possible.</p>

<p>A company doesn’t give you insight into what it’s thinking. It doesn’t tell you its price range, how much it paid the previous candidate with your experience, or anything like that. It intentionally obfuscates those things. But it wants you not to do the same.</p>

<p>A company wants to be like a bidder in a secret auction. But unlike the other bidders, it wants to know exactly how high all of the other bids are. It then openly intends to exploit that knowledge, often by bidding one cent more than the second highest bid.</p>

<p>Yeah, no. Screw that. It’s a silent auction, and to keep it that way, you must protect information.</p>

<p>In many situations, the only reason why you have any negotiating power at all is because the employer doesn’t actually know what you’re thinking. They might not know how good your other offers are, or how much you were making in your last job, or how you weigh salary vs equity, or even how rational you are as a decision-maker. Bottom line, you want them to be uncertain on exactly what it would take to sign you.</p>

<p>When you say “<em>can you do 90K instead of 85K,</em>” you’ve told them exactly what it will take to make you sign. The sheet’s pulled back, the secret auction is up, and they’re going to bid 90K (or more likely, 87K). And they know there’s almost no risk in doing so, because you’ll probably accept.</p>

<p>What if you were the kind of person who wouldn’t even consider an offer below 110K? Or the kind of person who wouldn’t consider an offer below 120K? If you were, you wouldn’t ask for 90K, and if they offered it as conciliation, you’d tell them to stop wasting your time.</p>

<p>By staying silent, <em>they don’t actually know which of those kinds of people you are.</em> In their mind, you could be any of the three.</p>

<p>A corollary of this rule is that you should not reveal to companies what you’re currently making. There are some exceptions, but as a rule you should assume this. If you must divulge what you’re making, you should be liberal in noting the total value of your package (incorporate bonuses, unvested stock, nearness to promotion etc.), and always mention it in a context like “<em>[XYZ] is what I’m currently making, and I’m definitely looking for a step up in my career for my next role.</em>“</p>

<p>Companies will ask about your current compensation at different stages in the process—some before they ever interview you, some after they decide to make you an offer. But be mindful of this, and protect information.</p>

<p>So given this offer, don’t ask for more money or equity or anything of the sort. Don’t comment on any specific details of the offer except to clarify them.</p>

<p>Give away nothing. Retain your power.</p>

<p>Say instead: “<em>Yeah, [COMPANY_NAME] sounds great! I really thought this was a good fit, and I’m glad that you guys agree. Right now I’m talking with a few other companies so I can’t speak to the specific details of the offer until I’m done with the process and get closer to making a decision. But I’m sure we’ll be able to find a package that we’re both happy with, because I really would love to be a part of the team.</em>“</p>

<p>Think like the watermelon farmer. This offer is just is the first businessman who’s stopped by your watermelon patch, glanced over your crops, and announced “I’ll take all of these right now for $2 a melon.”</p>

<p>Cool. It’s a big market, and you’re patient—you’re a farmer after all. Just smile and tell them you’ll keep their offer in mind.</p>

<p>And this is super important: always be unequivocally positive.</p>

<h2 id="the-importance-of-positivity">The importance of positivity</h2>

<p><strong>Staying positive is rule #4 of negotiation</strong>. Even if the offer is shit, it’s extremely important to remain positive and excited about the company. This is because <em>your excitement is one of your most valuable assets in a negotiation.</em></p>

<p>A company is making you an offer because they think you’ll do hard work for them if they pay you. If you lose your excitement for the company during the interview process, then they’ll lose confidence that you’ll actually want to work hard or stay there for a long time. Each of those makes you less attractive as an investment. Remember, you are the product! If you become less excited, then the product you’re selling actually loses value.</p>

<p>Imagine you were negotiating with someone over buying your watermelons, but the negotiation took so long that by the time you’d reached an agreement, your watermelons had gone bad.</p>

<p>Companies are terrified of that. They don’t want their candidates to go bad during a negotiation. Hence why they hire professional recruiters to manage the process and make sure they remain amicable. You and the recruiter share the same interest in that regard. If a company feels like you’ve gone bad, suddenly they’re a lot less willing to pay for you.</p>

<p>So despite whatever is happening in the negotiation, give the company the impression that 1) you still like the company, and that 2) you’re still excited to work there, even if the numbers or the money or the timing is not working out. Generally the most convincing thing to signal this is to reiterate you love the mission, the team, or the problem they’re working on, and really want to see things work out.</p>

<h2 id="dont-be-the-decision-maker">Don’t be the decision-maker</h2>

<p>You can wrap up the conversation now by saying:</p>

<blockquote>
  <p>I’ll look over some of these details and discuss it with my [FAMILY/CLOSE_FRIENDS/SIGNIFICANT_OTHER]. I’ll reach out to you if I have any questions. Thanks so much for sharing the good news with me, and I’ll be in touch!</p>
</blockquote>

<p>So not only are you ending the conversation with the power all in your hands, but note there’s another important move here: you’re roping in other decision-makers.</p>

<p><strong>Rule #5 of negotiation: don’t be the decision-maker.</strong> Even if you don’t particularly care what your friends/family/husband/mother thinks, by mentioning them, you’re no longer the only person the recruiter needs to win over. There’s no point in them trying to bully and intimidate you; the “true decision-maker” is beyond their reach.</p>

<p>This is a classic technique in customer support and remediation. It’s never the person on the phone’s fault, they’re just some poor schmuck doing their job. It’s not their decision to make. This helps to defuse tension and give them more control of the situation.</p>

<p>It’s much harder to pressure someone if they’re not the final decision-maker. So take advantage of that.</p>

<p>Okay!</p>

<p>We have our first offer. Send a follow-up e-mail confirming all of the details you discussed with your recruiter so you have a paper trail. Just say “<em>just wanted to confirm I had all the details right.</em>“</p>

<p>Groovy. Next step is to leverage this to land other offers and find the best deal we can find in the job market.</p>

<h2 id="getting-other-offers">Getting other offers</h2>

<p>Turns out, it doesn’t matter that much where your first offer is from, or even how much they’re offering you. Just having an offer in hand will get the engine running.</p>

<p>If you’re already in the pipeline with other companies (which you should be if you’re doing it right), you should proactively reach out and let them know that you’ve just received an offer. Try to build a sense of urgency. Regardless of whether you know the expiration date, all offers expire at some point, so take advantage of that.</p>

<blockquote>
  <p>Hello [PERSON],</p>

  <p>I just wanted to update you on my own process. I’ve just received an offer from [COMPANY] which is quite strong. That said, I’m really excited about [YOUR AMAZING COMPANY] and really want to see if we can make it work. Since my timeline is now compressed, is there anything you can do to expedite the process?</p>
</blockquote>

<p>Should you specifically mention the company that gave you an offer? Depends. If it’s a well-known company or a competitor, then definitely mention it. If it’s a no-name or unsexy company, you should just say you received an offer. If it’s expiring soon, you should mention that as well.</p>

<p>Either way, send out a letter like this to every single company you’re talking to. No matter how hopeless or pointless you think your application is, you want to send this signal to everyone who is considering you in the market.</p>

<p>Second, if there are any other companies you are looking to apply to (whether through referral or cold application), or even companies at which you’ve already applied but haven’t heard back, I would also follow up with a similar e-mail.</p>

<p>So why do this? Isn’t this tacky, annoying, or even desperate?</p>

<p>None of the above. It is the oldest method in history to galvanize a marketplace—show that supplies are limited and build urgency. Demand breeds demand. Not every company will respond to this, but many will.</p>

<p>Isn’t it stupid that companies respond to this though?</p>

<h2 id="why-companies-care-about-other-offers">Why companies care about other offers</h2>

<p><a href="https://haseebq.com/farewell-app-academy-hello-airbnb-part-i/">When I wrote about the story of my own job search</a>, I mentioned how having an offer from Google made companies turn around and expedite me through their funnels. Many commentators lamented at the capriciousness of these companies. If Uber or Twitch only talked to me because of Google and until then weren’t willing to look at me, what did that say about their hiring processes? What legitimately are they evaluating, if anything at all?</p>

<p>I think this response is totally backwards. The behavior of tech companies here is actually very rational, and you would do well to understand it.</p>

<p>First, you must realize what a company’s goal is. A company’s goal is to hire someone who will become an effective employee and produce more value than their cost. How do you figure out who will do that? Well, you can’t know for certain without actually hiring them, but there are a few proxies. Pedigree is the strongest signal; if they did it at other companies, they can probably do it at yours. And if someone trusted within the organization can vouch for them, that’s often a strong signal as well.</p>

<p>But turns out, almost everything else is a weak signal. Weak in the sense that it’s just not very reliable. Interviews, if you think about it, are long, sweaty, uncomfortable affairs that only glancingly resemble actual employment. They’re weird and can’t tell you that much about whether an individual will be a good at their job. There’s no way around this. There are a few stronger signals, like bringing someone in for a week or two on a contract-to-hire position, but strong candidates won’t consider this. So candidates as a whole have effectively forced companies to assume almost all of the risk in hiring.</p>

<p>The truth is, knowing that someone has passed your interview just doesn’t say <em>that much</em> about whether they’ll be a good employee. It’s as though you knew nothing about a student other than their SAT score. It’s just not a lot of data to go off.</p>

<p>Nobody has solved this problem. Not Google nor anyone else.</p>

<p>And this is precisely why it’s rational for companies to care that you’ve received other offers. They care because each company knows that their own process is noisy, and the processes of most other companies are also noisy. But a candidate having multiple offers means that they have multiple weak signals in their favor. Combined, these converge into a much stronger signal than any single interview. It’s like knowing that a student has a strong SAT score, and GPA, and won various scholarships. Sure, it’s still possible that they’re a dunce, but it’s much harder for that to be true.</p>

<p>This is not to say that companies respond proportionally to these signals, or that they don’t overvalue credentials and brands. They do. But caring about whether you have other offers and valuing you accordingly is completely rational.</p>

<p>So this is all to say—tell other companies that you’ve received offers. Give them more signal so that they know you’re a valued and compelling candidate. And understand why this changes their mind about whether to interview you.</p>

<p>As you continue interviewing, remember to keep practicing your interview skills. The single strongest determinant of your final offer will be the number and strength of offers that you receive.</p>

<h2 id="some-advice-on-timing">Some advice on timing</h2>

<p>You want to be strategic about the timing of your offers. Generally, you should try to start interviewing at larger companies earlier. Their processes are slower and their offer windows are wider (meaning they allow you more time to decide). Startups are the other way around.</p>

<p>Your goal should be to have as many offers overlapping at the same time as possible. This will maximize your window for negotiating.</p>

<p>When you receive an offer, often the first thing you should ask for is more time to make your decision. Especially in your first offer, more time is by far the most valuable thing you can ask for. It’s time that enables you to activate other companies and end up with the strongest possible offer. So be prepared to fight for time.</p>

<h2 id="how-to-approach-exploding-offers">How to approach exploding offers</h2>

<p>Hoo boy.</p>

<p>Exploding offers are offers that expire within 24-72 hours. You won’t see this much at big companies, but they’re becoming increasingly common among startups and mid-sized companies.</p>

<p>Exploding offers suck, and I share most people’s disdain for this practice. But I do understand it. Exploding offers are a natural weapon for employers to combat a strong hiring market for tech workers. Companies know exactly what they’re doing with exploding offers—they play on fear and limit your ability to seek out counteroffers.</p>

<p>In a sense, it’s unsurprising that if startups have more difficulty attracting and securing talent, they’d resort to this practice. What I don’t like is the dishonesty about it. Employers often justify this by saying “<em>If you need more time than this, then that’s a sign you’re not the kind of person we’re looking for.</em>“</p>

<p>Please don’t buy this crap or feel guilty over it. They’re simply doing this to improve their chance of closing candidates. Needing more than three days to make a life decision isn’t a sign of anything other than thoughtfulness.</p>

<p>So what should you do if you receive an exploding offer?</p>

<p>Exploding offers are anathema to your ability to effectively navigate the labor market. Thus, there is only one thing to do. Treat the offer as a non-offer unless the expiration window is widened.</p>

<p>In no uncertain terms, convey that if the offer is exploding, it’s useless to you.</p>

<p>Example conversation:</p>

<blockquote>
  <p>I have one big concern. You mentioned that this offer explodes in 48 hours. I’m afraid this doesn’t work at all for me. There’s no way that I can make a decision on this offer within a 48 hour window. I’m currently wrapping up my interview process at a few other companies, which is likely to take me another week or so. So I’m going to need more time to make an informed decision.</p>
</blockquote>

<p>If they push back and say this is the best they can do, then politely reply:</p>

<blockquote>
  <p>That’s really unfortunate. I like [YOUR COMPANY] and was really excited about the team, but like I said, there’s no way I can consider this offer. 48 hours just too unreasonable of a window. The next company I join will be a big life decision for me, and I take my commitments very seriously. I also need to consult with my [EXTERNAL_DECISION_MAKER]. There’s no way that I can make a decision I’m comfortable with in this short an amount of time.</p>
</blockquote>

<p>Pretty much any company will relent at this point. If they persist, don’t be afraid to walk away over it. (They probably won’t let that happen, and will come grab you as you’re walking out the door. But if they don’t, then honestly, screw ‘em.)</p>

<p>I was given several exploding offers during my job search. And every time, I did essentially this. Every single offer immediately widened to become more reasonable, sometimes by several weeks.</p>

<p>I want to emphasize, lest I be misunderstood here—what I’m saying is not to just silently let an exploding offer expire, and assume that everything will be fine and they’ll still hire you. They won’t. For exploding offers to be a credible weapon, a company has to have a reputation of enforcing them. I’m saying explicitly call this out as an issue when they make the offer.</p>

<p>Don’t let a company bully you into giving away your negotiating power.</p>

<h2 id="the-negotiating-mindset">The Negotiating Mindset</h2>

<p>Before we enter into the actual back-and-forth, I want to examine the mindset you should have as a negotiator. This applies not just to how you approach the conversation, but also to how you think about the company.</p>

<p>Do not fall into the trap of valuing companies solely along one dimension. That means don’t just value companies based on salary, equity, or even on prestige. Those are all important dimensions, but so are cultural fit, the challenge of the work, learning potential, later career options, quality of life, growth potential, and just overall happiness. None of these inherently trump any of the other. Anyone who tells you “just choose wherever you think you’ll be happiest” is being just as simplistic than someone who says “just choose the one that offers the most money.” All of these things matter, and your decision should be genuinely multi-dimensional.</p>

<p>Be open to being surprised as you explore different companies.</p>

<p>It’s also important to understand that companies don’t all value you along the same dimension either. That is, different companies are genuinely looking for different skills, and there are some companies at which you will be more and less valuable. Even at peer companies this is true, especially so if you have a specialized skill-set.</p>

<p>The more companies you talk to, the more likely you are to find a company to which you are significantly more valuable than the rest. Chances are this is where you’ll be able to negotiate your strongest offer. It might surprise you which company this turns out to be; keep an open mind, and remember that a job search is a 2-sided process.</p>

<p>One of the most valuable things you can do for yourself in this process is to really try to understand how employers think and what motivates them. Understanding your interlocutor is extremely important in negotiation, and we’ll be exploring that a lot in the next blog post.</p>

<p>But most of all I want to emphasize: be curious about the other side. Try to understand why employers think the way they do. Be sympathetic toward them. Care about what they want and help them try to get it. Adopting this mindset will make you a much stronger negotiator, and accordingly, a much better employee and team member.</p>

<p>Okay. That’s as far as we’re going for today. In the next blog post, I’m going to cover the last four rules of negotiation. I’ll also go over the actual back-and-forth process—how to ask for what you want, how to strengthen offers, and how to dismantle the tricks that companies will try to pull on you. Also a lot more on the theory of negotiation, which I really dig.</p>

<p>Do share this post if you found it useful! And <a href="https://twitter.com/intent/follow?original_referer=http%3A%2F%2Fhaseebq.com%2F%3Fp%3D2393%26preview%3Dtrue&amp;ref_src=twsrc%5Etfw&amp;region=follow_link&amp;screen_name=hosseeb&amp;tw_p=followbutton">follow me on Twitter</a>.</p>

<p><a href="https://haseebq.com/how-not-to-bomb-your-offer-negotiation/">You can read part 2 here!</a></p>

<p>Until next time,</p>

    <p>Haseeb</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We are still using 88x31 buttons (129 pts)]]></title>
            <link>https://ultrasciencelabs.com/lab-notes/why-we-are-still-using-88x31-buttons</link>
            <guid>43596570</guid>
            <pubDate>Sat, 05 Apr 2025 20:26:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ultrasciencelabs.com/lab-notes/why-we-are-still-using-88x31-buttons">https://ultrasciencelabs.com/lab-notes/why-we-are-still-using-88x31-buttons</a>, See on <a href="https://news.ycombinator.com/item?id=43596570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="introduction">introduction</h2>

<p>If you surf the modern "small web" to any extent you've encountered 88x31 buttons - a staple of late 90s and early 2000's websites<sup id="fnref-1"><a href="https://ultrasciencelabs.com/lab-notes/why-we-are-still-using-88x31-buttons#fn-1" role="doc-noteref">1</a></sup> undergoing a bit of a revival. The <a href="https://neocities.org/">Neocities</a> community especially seems to have truly embraced them. 88x31 buttons have a long history on the web, straddling the worlds of advertising and the personal web. Much like IRL jacket pins and buttons - they're small, colorful, collectible, easy to make and trade, and at a glance can confer just enough information to characterize the website displaying them.</p>

<p>Some examples of sites sharing some thematic elements spanning over 25 years:<br>
<a href="http://thombs.com/Dann_1996-06/noframes.htm">Dann's Page</a><br>
<a href="https://billsworld.neocities.org/">Bill's World</a><br>
<a href="https://paintkiller.neocities.org/new/links">Paintkiller's links page</a><br>
<a href="https://solaria.neocities.org/">Solaria's Webspace</a><br>
<a href="https://gothicpossum.neocities.org/">Ellie's Magical Website</a><br>
<a href="http://bytemoth.nfshost.com/cd5k-net/www/pcp/">ByteMoth's Perfectly Cooked Pages</a></p>

<p>They all feature 88x31 buttons in some capacity and those buttons reflect the website and it's designer in some way.</p>

<!--more-->

<p>Despite their persistence into the 2020's (or maybe because of it), there seems to be some debate on the origin of the 88x31 format - some point to early <a href="https://thehistoryoftheweb.com/an-ode-to-geocities/">Geocities</a> websites, others Netscape's infamously ubiquitous "Now"  buttons. <a href="https://neonaut.neocities.org/cyber/88x31">Neonaut's 88x31 page</a> compares the 1996 Geocities.com and Netscape.com snapshots on the Wayback Machine to draw that conclusion. However, the snapshot shows a button advertising the new 3.0 release<sup id="fnref-2"><a href="https://ultrasciencelabs.com/lab-notes/why-we-are-still-using-88x31-buttons#fn-2" role="doc-noteref">2</a></sup>. But the "now" button goes back earlier.</p>

<h2 id="when-did-the-88x31-emerge">when did the 88x31 emerge?</h2>

<p>The <a href="https://www.versionmuseum.com/history-of/netscape-browser">Version Museum list of Netscape screenshots</a> has an image of Navigator version 1.2 (probably 1.22) dated from 1995 showing the Netscape homepage and the earliest instance of the 'Netscape Now' button which had no version number (Netscape 1.22 was released in 1995). <a href="https://www.webdesignmuseum.org/software/netscape-navigator-1-0-in-1994">Screenshots of the Netscape site from 1994</a><sup id="fnref-3"><a href="https://ultrasciencelabs.com/lab-notes/why-we-are-still-using-88x31-buttons#fn-3" role="doc-noteref">3</a></sup> do not show any 88x31 buttons.</p>

<p>The earliest version of the GeoCities homepage I can find is on <a href="https://www.webdesignmuseum.org/gallery/geocities-1995">Web Design Museum</a> and dates from 1995. It doesn't show any 88x31 buttons, but it does have a full banner, and two buttons that appear to be more like 88x40. The <a href="https://web.archive.org/web/19961022173245/http://www.geocities.com/">October 22 1996 snapshot</a> of the re-designed homepage shows several 88x31 buttons on the sidebar.</p>

<p>So the 88x31 button format seems to have emerged in sometime in 1995 on the Netscape website as part of promotion of their Navigator browser. I suspect the promotion effort stemmed from <a href="https://en.wikipedia.org/wiki/Netscape">Netscape's highly anticipated and successful 1995 IPO</a>. By 1996 the 88x31 button format was prevalent. It's easy to find snapshots and screenshots of many websites showing 88x31 buttons including GeoCities from 1996 onward.</p>

<p>So we have a rough idea of where the 88x31 button format originated from and when. But why the 88x31 size?</p>

<h2 id="why-is-the-88x31-button-88x31">why is the 88x31 button 88x31?</h2>

<p>On their <a href="https://web.archive.org/web/19961230181301/http://www.netscape.com/comprod/mirror/netscape_now_program.html">NETSCAPE NOW PROGRAM</a> page, Netscape mentions the following:</p>

<blockquote>
  <p>By joining the Netscape Now program, you will receive an authorized logo from Netscape for display on your site. Simply fill out and submit an application, and we will send you the corresponding image and usage guidelines.</p>
</blockquote>

<p>Why would you need an "authorized" image? And why would Netscape bother sending you one? Because further down the page there is a significant benefit for official program participants "...we will display your company or product logo in the Netscape Customer Showcase - free of charge - and will give you a 10 percent partner discount on advertising on the Netscape Internet site." But how would Netscape know if you were using an authorized image?</p>

<p>On the <a href="https://web.archive.org/web/19961230204530/http://www.netscape.com/comprod/mirror/usage.html">guideline page for usage of the "Netscape Now" button</a> for Netscape Now program participants you find the following in the list of guidelines:</p>

<blockquote>
  <p>Minimum Size The minimum size of the Netscape Now button is 88 by 32 pixels. The button may be displayed at a larger size, but all elements of the button must be increased in size proportionally.</p>
</blockquote>

<p>Did you notice something? The minimum size of the button is <em>88x32</em>. Not 31. But every instance of the button on their site is 88x31.</p>

<p>I suspect Netscape used 88x31 "sample" sized buttons to promote their "Now" rewards program and browser. But then they released "official" 88x32 buttons to registered rewards program participants. It would be a quick and easy way to verify if your site was using an "authorized" image.</p>

<p>But if all you wanted was a "Now" button or if you wanted to modify or remix it, well you'd just grab the unofficial 88x31 "sample" size buttons off the Netscape site and riff. And riff people did. I also suspect this usage guideline "No Alteration Allowed - The Netscape Now button must not be altered in ANY way. Do not shrink it; take it apart; change its proportions, color, or font; or otherwise alter it from the Netscape-supplied version." did little to discourage people and probably outright encouraged them just for spite - y'know because the Internet. By the end of the decade and well into the 2000's <em>everyone</em> used 88x31 buttons - from software giants like Microsoft, advertisers, media outlets, technology sites, to Geocities homesteaders - everybody. But why has it stuck around so long?</p>

<h2 id="why-is-the-88x31-so-durable">why is the 88x31 so durable?</h2>

<p>While diving into the origins of the 88x31 button, I came across an article by Yequari titled <a href="https://yequari.com/blog/2023/10/no-more-88x31-buttons/">"Why Are We Still Using 88x31 Buttons?"</a>. It's a good read and makes some good points, but I don't know if it answers it's own premise- why are we still using 88x31 buttons?</p>

<p>Yequari writes "I get it. 88x31 buttons are a fun way to express yourself and show off what your website is about. They were an essential piece of the web 1.0 culture which many are trying to revive. And to be clear, creating fun graphics is awesome! Creating art out of nostalgia for a time gone by is rad as hell. But the 88x31 format just ain’t it. It’s incredibly limiting and impractical. Computer screens today have over double the screen space than those for which the 88x31 button was designed."</p>

<p>I agree- 88x31 buttons aren't terribly practical. So something else must be at play to explain their longevity. 88x31 certainly were popular but how popular? But there were many popular button and banner formats from the 90's and 00's Internet that have gone by the wayside. If only we had a way of measuring "popularity" over time - a way to gauge the staying power of a particular format. Thankfully, we do. But first, more history.</p>

<p>In 1994 the HotWired website launched with the first "banner" ads. Sized at 468x60, these Hotwired banners became a "standard" size due simply to <a href="https://thehistoryoftheweb.com/ads/">being first and popular</a>. By 1996 banners were everywhere. <a href="https://web.archive.org/web/19961221003110/http://www.geocities.com/BHI/prodspecs.html">GeoCites listed the following ad standards</a>:</p>

<ul>
<li>GIF or JPG file format</li>
<li>468x60 pixels</li>
<li>&lt;=7.5 K file size</li>
<li>we can alternate among multiple banners</li>
<li>we can update with new banners weekly</li>
</ul>

<p>And the full banner was one of many sizes of buttons and banners in play. By 1998 there were banners, half banners, and any number of other square and rectangular buttons in a range of sizes (including the 88x31 button):</p>

<p>This 1997 snapshot of <a href="https://web.archive.org/web/19971212072743/http://www.hotwired.com/">Hotwired.com shows 2 half-wdith banners and a weird 144x72 button</a> at the top of the page. Another <a href="https://web.archive.org/web/19980208154258/http://www.hotwired.com:80/">1997 Hotwired snapshot</a> has a button measuring 127x127 on right, but squeezed into a 125x125 area. This <a href="https://web.archive.org/web/19980208162033/http://www.hotwired.com/info/">1998 snapshot</a> has two half-width banners and a 120x60 button at the top of the page.</p>

<p>In <a href="https://web.archive.org/web/19961226182558/http://www3.geocities.com:80/">1996 Geocities</a> had some 117x40 buttons on left in addition to a banner ad.</p>

<p>In <a href="https://web.archive.org/web/19981212020325/http://pcworld.com:80/">1998 PCWorld</a> had multiple 88x31 buttons and a 125x125 square button on the left.</p>

<p><a href="https://web.archive.org/web/19980207080656/http://tipworld.com/">TipWorld in 1998</a> Had 4 125x125 square buttons in a stack.</p>

<p>Yahoo had a banner area measuring 230x33 at the top of the page. And you can find examples of banners sized natively for that odd size like in this <a href="https://web.archive.org/web/19970605005803/http://www.yahoo.com/">1997 snapshot</a> or natively sized at 468x60 and <a href="https://web.archive.org/web/19970227062641/http://www5.yahoo.com:80/">squeezed down to fit</a> the Yahoo front page banner area.</p>

<p>As advertising spread through the web, it quickly became big business. In 1995 the creators of <a href="https://wiki.preterhuman.net/DoubleClick">DoubleClick</a> developed the concept on inserting targeted ads into sites on behalf of advertisers and brokers only a year after the first banner ads appeared. But the wide variety of sizes and formats was a challenge for site owners and advertisers. In 1997 to bring order to the chaos the newly formed IAB (Internet Advertising Bureau, later Interactive Advertising Bureau), an association of advertising, technology, ISP, and Internet media companies, contracted an <a href="https://web.archive.org/web/19980211025122/http://www.iab.net/advertise/content/adeff3.html">Advertising Effectiveness Study</a> using data from 12 "top" websites and produced a <a href="https://web.archive.org/web/19980211023701/http://www.iab.net/advertise/content/adintro.html">list of effective ad banner and button sizes</a> in 1998 and released them as recommendations:</p>

<p>Size (pixels):<br>
468 x 60 Full Banner<br>
392 x 72 Full Banner with Vertical Navigation Bar<br>
234 x 60 Half Banner<br>
125 x 125 Square Button<br>
120 x 90 Button #1<br>
120 x 60 Button #2<br>
88 x 31 Micro Button<br>
120 x 240 Vertical Banner</p>

<p>And along side its more famous older sibling, the "full banner", is the 88x31 "Micro Button" (by 2004 when I bookmarked the <a href="https://web.archive.org/web/20040610074507/http://www.iab.net/standards/adunits.asp">IAB site</a> it became the "Micro Bar")<sup id="fnref-4"><a href="https://ultrasciencelabs.com/lab-notes/why-we-are-still-using-88x31-buttons#fn-4" role="doc-noteref">4</a></sup>. But standard or not how durable could the little 88x31 button be? After all by the end of the decade 1024x768 and larger screen resolutions were becoming common and website designers were moving toward larger ad formats to take advantage of that increased screen real estate. The 2004 IAB recommendations included several new and larger ad sizes (with more arriving in later years):</p>

<p>300 x 250 IMU - (Medium Rectangle)<br>
250 x 250 IMU - (Square Pop-Up)<br>
240 x 400 IMU - (Vertical Rectangle)<br>
336 x 280 IMU - (Large Rectangle)<br>
180 x 150 IMU - (Rectangle)<br>
728 x 90 IMU - (Leaderboard)<br>
160 x 600 IMU - (Wide Skyscraper)<br>
120 x 600 IMU - (Skyscraper)<br>
300 x 600 IMU - (Half Page Ad)</p>

<p>Well 14 years after their first ad format recommendations and over a decade of ad-buy, click-through, and engagement data in hand the <a href="https://web.archive.org/web/20120726235824/http://www.iab.net/guidelines/508676/508767/displayguidelines">IAB de-listed</a> a number of ad banner sizes, including the near ubiquitous full banner of the 90's and 2000's widely regarded as <a href="https://www.mirrorreview.com/the-internet-ad-revolution/">kicking off the Internet ad revolution</a>:</p>

<p>250 x 250   Square Pop-Up<br>
240 x 400   Vertical Rectangle<br>
336 x 280   Large Rectangle<br>
300 x 100   3:1 Rectangle<br>
720 x 300   Pop-Under<br>
468 x 60    Full Banner<br>
234 x 60    Half Banner<br>
120 x 90    Button 1<br>
120 x 240   Vertical Banner<br>
125 x 125   Square Button<br>
120 x 600   Skyscraper</p>

<p>But the venerable 88x31 micro bar? It was <a href="https://web.archive.org/web/20120726235824/http://www.iab.net/guidelines/508676/508767/displayguidelines">still listed in the guidelines</a>. Despite it's display limitations it was still attractive and engaging enough to advertisers and users to maintain it's status as a standard ad format. And I think that's why it has staying power it maintained engagement despite its limitations- it's a matter of fun over function.</p>

<h2 id="alternatives-to-the-88x31-button">alternatives to the 88x31 button</h2>

<p>But what if 88x31 buttons aren't for you? 88x31 buttons, especially of the animated variety, are often employed to invoke a certain flavor of Internet kitsch. Maybe that's not what you're aiming for. And concerns about visibility and accessibility are valid - especially if your site is intended to be viewed by a wider audience. Yequari takes issue with the 88x31 mainly in terms of visibility/accessibility issues and suggests a larger 200x40 format:</p>

<blockquote>
  <p>The 200x40 format is is simply superior to 88x31. It’s still small enough to not worry much about the filesize, but it’s large enough to have text readable to my deteriorating eyes alongside a fun graphic. Stacking a bunch of them together is way less overwhelming to look at, especially if they’re animated.</p>
</blockquote>

<p>A 200x40 banner is definitely better in terms of visibility/readability on high-res screens- but the format is odd. I found <a href="http://www.stripeymaney.com/banners.html">one instance of a 200x40 banner</a> in the wild while researching this article. I found a 205x53 button on this 2005 snapshot of the Wired website. But that's the closest I could find.</p>

<p>Instead of 200x40, I would suggest one of the depreciated banner formats from the IAB's old recommendations like the <a href="http://www.multi-banners.com/234x60ads.htm">234 x 60 Half Banner</a> or the 120 x 60 Button #2. Why? Since the half banner and 120x60 button survived into the early 2010s you have over two decades of banners and buttons in archive.org snapshots to riff from or include outright for "flavor".</p>

<p>Here are a few examples:</p>

<p>120x60</p>

<p><img src="https://ultrasciencelabs.com/media/images//120x60/120x60_button_defalt.gif" width="120" height="60" alt="visto" title="visto">
<img src="https://ultrasciencelabs.com/media/images//120x60/bargain_120x60.gif" width="120" height="60" alt="barnes and noble" title="barnes and noble">
<img src="https://ultrasciencelabs.com/media/images//120x60/sick_120.gif" width="120" height="60" alt="suck.com" title="suck.com"></p>



<p>234x60</p>

<p><img src="https://ultrasciencelabs.com/media/images//234x60/Vis2a234.gif" width="234" height="60" alt="neoplanet" title="neoplanet">
<img src="https://ultrasciencelabs.com/media/images//234x60/rampnet_hban_ramp3_ad.gif" width="234" height="59" alt="web ramp" title="web ramp"><br>
<img src="https://ultrasciencelabs.com/media/images//234x60/ns61002.gif" width="234" height="60" alt="netscape" title="netscape">
<img src="https://ultrasciencelabs.com/media/images//234x60/speed234.gif" width="234" height="60" alt="amazon" title="amazon"></p>



<h2 id="conclusions">conclusions</h2>

<p>Will I be using 88x31 buttons on my own site? Maybe. Will I try some alternate formats? Also maybe. I've been reworking old icons to add some flavor to my pages, but I haven't decided on other graphics yet.</p>

<p>In the end though I have to conclude that, love them or hate them, 88x31 buttons are good. They occupy a unique space defined by their prevalence on the early web that wasn't due to a cash-fueled advertising rush<sup id="fnref-5"><a href="https://ultrasciencelabs.com/lab-notes/why-we-are-still-using-88x31-buttons#fn-5" role="doc-noteref">5</a></sup> forced on Internet surfers, but was because of their origin as freely released buttons that could be easily co-opted, remixed, and spread organically. The Internet embraced them early on, and I don't see any sign they'll be let go any time soon.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[North America Is Dripping from Below, Geoscientists Discover (123 pts)]]></title>
            <link>https://www.jsg.utexas.edu/news/2025/04/north-america-is-dripping-from-below-geoscientists-discover/</link>
            <guid>43596546</guid>
            <pubDate>Sat, 05 Apr 2025 20:24:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jsg.utexas.edu/news/2025/04/north-america-is-dripping-from-below-geoscientists-discover/">https://www.jsg.utexas.edu/news/2025/04/north-america-is-dripping-from-below-geoscientists-discover/</a>, See on <a href="https://news.ycombinator.com/item?id=43596546">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
					
												<p>April 1, 2025</p>
												
						<figure id="attachment_25112" aria-describedby="caption-attachment-25112"><a href="https://www.jsg.utexas.edu/news/files/Figure4_new-scaled.jpg"><img decoding="async" src="https://www.jsg.utexas.edu/news/files/Figure4_new-840x451.jpg" alt="Figure4 New" width="640" height="344" srcset="https://www.jsg.utexas.edu/news/files/Figure4_new-840x451.jpg 840w, https://www.jsg.utexas.edu/news/files/Figure4_new-400x215.jpg 400w, https://www.jsg.utexas.edu/news/files/Figure4_new-768x412.jpg 768w, https://www.jsg.utexas.edu/news/files/Figure4_new-1536x824.jpg 1536w, https://www.jsg.utexas.edu/news/files/Figure4_new-2048x1099.jpg 2048w" sizes="(max-width: 640px) 100vw, 640px"></a><figcaption id="caption-attachment-25112">A figure from the study showing rock dripping from the craton. The researchers hypothesize that the dripping is caused by the remnants of the subducting Farallon slab below the craton. Credit: Hua et al., Nature Geoscience.</figcaption></figure>
<p>Researchers have discovered that the underside of the North American continent is dripping away in blobs of rock — and that the remnants of a tectonic plate sinking in the Earth’s mantle may be the reason why.</p>
<p>A paper published in <a href="https://doi.org/10.1038/s41561-025-01671-x"><em>Nature Geoscience</em></a> describes the phenomenon, which was discovered at The University of Texas at Austin. It’s the first time that “cratonic thinning” may be captured in action.</p>
<p>“We made the observation that there could be something beneath the craton,” said the study’s lead author Junlin Hua, who conducted the research during a postdoctoral fellowship at UT’s Jackson School of Geosciences. “Luckily, we also got the new idea about what drives this thinning.”</p>
<p>Cratons are very old rocks that are part of Earth’s continents. They’re known for their stability and ability to persist for billions of years. But sometimes cratons undergo changes that can affect their stability or that remove entire rock layers.</p>
<p>For example, the North China Craton lost its deepest root layer millions of years ago.</p>
<p>What makes the discovery of cratonic dripping special, said the researchers, is that it’s happening right now. This allows scientists to observe the cratonic thinning process as it occurs.</p>
<p>The dripping is concentrated over the Midwest of the United States. There’s no need to worry about the continent hollowing out or the dripping changing the landscape anytime soon, the researchers assure. The mantle processes driving the dripping can influence how tectonic plates evolve over time – but they’re very slow going.&nbsp; What’s more, the dripping is expected to eventually stop as the remnants of the tectonic plate sinks deeper into the mantle and its influence over the craton fades.</p>
<p>The discovery is most important to geoscientists who study continents over their entire lifespan, said co-author Thorsten Becker, a professor at the Jackson School’s Department of Earth and Planetary Sciences and Institute for Geophysics.</p>
<p>“This sort of thing is important if we want to understand how a planet has evolved over a long time,” Becker said. “It helps us understand how do you make continents, how do you break them, and how do you recycle them [into the Earth.]”</p>
<p>The dripping discovery came out of a larger project led by Hua, who is now a professor at the University of Science and Technology of China, that created a new full-waveform seismic tomographic model for North America using an approach developed by co-author Stephen Grand, who is now a professor emeritus at the Jackson School, and his team. This computer model, which uses seismic data collected by the <a href="https://www.earthscope.org/">EarthScope</a> project, revealed new details about the geologic processes happening in the crust and mantle underlying North America.</p>
<figure id="attachment_25113" aria-describedby="caption-attachment-25113"><a href="https://www.jsg.utexas.edu/news/files/Figure1.png"><img loading="lazy" decoding="async" src="https://www.jsg.utexas.edu/news/files/Figure1-837x600.png" alt="Figure1" width="640" height="459" srcset="https://www.jsg.utexas.edu/news/files/Figure1-837x600.png 837w, https://www.jsg.utexas.edu/news/files/Figure1-400x286.png 400w, https://www.jsg.utexas.edu/news/files/Figure1-768x551.png 768w, https://www.jsg.utexas.edu/news/files/Figure1-1536x1102.png 1536w, https://www.jsg.utexas.edu/news/files/Figure1.png 1945w" sizes="auto, (max-width: 640px) 100vw, 640px"></a><figcaption id="caption-attachment-25113">Seismic waves pass through different geological features at different speeds. This map shows seismic speed in the Earth’s crust at 200 kilometers depth across the continental United States and portions of Central America and Canada. The North American craton (outlined in black dashes) has a high seismic velocity compared to its surroundings. Credit: Hua et al., Nature Geoscience.</figcaption></figure>

<p>“Because of the use of this full-waveform method, we have a better representation of that important zone between the deep mantle and the shallower lithosphere where we would expect to get clues on what’s happening with the lithosphere,” Becker said.</p>
<p>This model brought the drips into view for the first time in this way. It also helped the researchers deduce that the Farallon Plate, an oceanic tectonic plate that has been subducting under North America for about the past 200 million years, could be driving the process despite being separated from the craton by about 600 kilometers.</p>
<p>The plate, which was first seismically imaged in the 1990s by Grand, played an important role in shaping the North American plate. The researchers think that it is now wearing away at the continent from below by redirecting the flow of mantle material so that it shears the bottom of the craton and by releasing volatile compounds that weaken its base.</p>
<p>Although the dripping is concentrated in one area of the craton, Hua said that the plate appears to be interacting with material from across the entire craton, which covers most of the United States and Canada.</p>
<p>“A very broad range is experiencing some thinning,” Hua said.</p>
<p>When the researchers built a computer model of these dynamics, the model craton dripped when the Farallon Plate was present. When the plate was removed, the dripping stopped.</p>
<p>Becker acknowledges that computer models have limitations. But the model’s resemblance to the data is a good sign, he said.</p>
<p>“You look at a model and say, ‘Is it real, are we overinterpreting the data or is it telling us something new about the Earth?,’” Becker said. “But it does look like in many places that these blobs come and go, that it’s [showing us] a real thing.”</p>
<p>The research was funded by the National Science Foundation and the Jackson School of Geosciences and involved colleagues at University of Hawai’i at Mānoa, University of Nevada, Reno, and Southern University of Science and Technology.</p>
<p><strong>For more information, contact:&nbsp;</strong><strong><a href="mailto:anton.caputo@jsg.utexas.edu">Anton Caputo</a>, Jackson School of Geosciences, 210-602-2085;&nbsp;<a href="mailto:mkortsha@jsg.utexas.edu">Monica Kortsha</a>, Jackson School of Geosciences, 512-471-2241;&nbsp;<a href="mailto:costa@ig.utexas.edu">Constantino Panagopulos</a>, University of Texas Institute for Geophysics</strong><strong>.</strong></p>
						
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Llama 4 Now Live on Groq (107 pts)]]></title>
            <link>https://groq.com/llama-4-now-live-on-groq-build-fast-at-the-lowest-cost-without-compromise/</link>
            <guid>43596470</guid>
            <pubDate>Sat, 05 Apr 2025 20:13:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://groq.com/llama-4-now-live-on-groq-build-fast-at-the-lowest-cost-without-compromise/">https://groq.com/llama-4-now-live-on-groq-build-fast-at-the-lowest-cost-without-compromise/</a>, See on <a href="https://news.ycombinator.com/item?id=43596470">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="63e34e6" data-element_type="container" data-elementor-type="wp-post" data-elementor-id="6435" data-elementor-post-type="post" data-widget_type="theme-post-content.default">
				<div data-id="722ca0c" data-element_type="widget" data-widget_type="text-editor.default">
				<p>
									<h4><span>Meta’s Llama 4 Scout and Maverick models are live today on GroqCloud™, giving developers and enterprises day-zero access to the most advanced open-source AI models available.<br></span></h4>								</p>
				</div>
				<div data-id="1ab7589" data-element_type="widget" data-widget_type="text-editor.default">
				<p>Today, Meta released the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences. With Llama 4 Scout and Llama 4 Maverick available on GroqCloud today to its free users and paid customers, developers can run cutting-edge multimodal workloads while keeping costs low and latency predictable.</p>
				</div>
				
				<div data-id="345d7c5" data-element_type="widget" data-widget_type="text-editor.default">
				<p>
									<h4><span>Groq Performance &amp; Pricing<br></span></h4>								</p>
				</div>
				<div data-id="3568efc" data-element_type="widget" data-widget_type="text-editor.default">
									<p><span>Our vertically integrated GroqCloud and inference-first architecture deliver unmatched performance and price. With Llama 4 models, developers can run cutting-edge multimodal workloads while keeping costs low and latency predictable.</span></p>
<p><span>Llama 4 Scout is currently running at over 460</span><span>&nbsp;tokens/s</span><span> while Llama 4 Maverick is coming today</span><span>.&nbsp;</span><span>Stay tuned for official 3rd party benchmarks from Artificial Analysis.&nbsp;</span></p>
<p><span>Groq is offering the first of the Llama 4 model herd at the following pricing:</span></p>								</div>
				<div data-id="355d301" data-element_type="widget" data-widget_type="text-editor.default">
									<ul><li aria-level="1"><b>Llama 4 Scout: </b><span>$0.11 / M input tokens and $0.34 / M output tokens</span></li><li aria-level="1"><strong>Llama 4 Maverick:</strong> $0.50 / M input tokens and $0.77 / M output tokens</li></ul>								</div>
				
				<div data-id="1ee4919" data-element_type="widget" data-widget_type="text-editor.default">
				<p>
									<h4><span>About Llama 4 <br></span></h4>								</p>
				</div>
				<div data-id="f519fef" data-element_type="widget" data-widget_type="text-editor.default">
				<p>The new Llama 4 models are Meta’s first models that use a Mixture of Experts (MoE) architecture. In MoE models, a single token activates only a fraction of the total parameters. MoE architectures are more compute efficient for model training and inference and, given a fixed training FLOPs budget, deliver higher quality models compared to dense architectures.<br>Llama 4 models are designed with native multimodality, incorporating early fusion to seamlessly integrate text and vision tokens into a unified model backbone. <br>Meta aims to develop the most helpful, useful models for developers while protecting against and mitigating the most severe risks. This includes integrating mitigations at each layer of model development from pre-training to post training and tunable system-level mitigations that shield developers from adversarial users. In doing so, Meta is helping empower developers to create helpful, safe, and adaptable experiences for their Llama supported applications.</p>
				</div>
				
				<div data-id="e9fc04b" data-element_type="widget" data-widget_type="text-editor.default">
				<p>
									<h4><span>Llama 4 Scout &amp; Maverick<br></span></h4>								</p>
				</div>
				<div data-id="9a49798" data-element_type="widget" data-widget_type="text-editor.default">
									<p><span>These latest Llama models from Meta include smaller and larger options to accommodate a range of use cases and developer needs.</span></p><p><span>Llama 4 Scout is a leading multimodal model and is more powerful than the Llama 3 models. It contains 17 billion active parameters, 16 experts, and 109 billion total parameters; it delivers state-of-the-art performance for its class.&nbsp;</span></p><p><span>Llama 4 Maverick contains 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B. It offers unparalleled, industry-leading performance in image and text understanding with support for 12 languages, enabling the creation of sophisticated AI applications that bridge language barriers. As the workhorse model for general assistant and chat use cases, Llama 4 Maverick is great for precise image understanding and creative writing. For developers, it offers state-of-the-art intelligence with high speed, optimized for best response quality on tone, and refusals.</span></p>								</div>
				
				<div data-id="531d7d8" data-element_type="widget" data-widget_type="text-editor.default">
				<p>
									<h4><span>Build Fast with Llama 4 on GroqCloud<br></span></h4>								</p>
				</div>
				
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Llama4 (1146 pts)]]></title>
            <link>https://www.llama.com/llama4/</link>
            <guid>43595585</guid>
            <pubDate>Sat, 05 Apr 2025 18:33:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.llama.com/llama4/">https://www.llama.com/llama4/</a>, See on <a href="https://news.ycombinator.com/item?id=43595585">Hacker News</a></p>
Couldn't get https://www.llama.com/llama4/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: iPhone 2005 weird "Blob Keyboard" simulator (126 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43595442</link>
            <guid>43595442</guid>
            <pubDate>Sat, 05 Apr 2025 18:20:49 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43595442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="43595442">
      <td><span></span></td>      <td><center><a id="up_43595442" href="https://news.ycombinator.com/vote?id=43595442&amp;how=up&amp;goto=item%3Fid%3D43595442"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=43595442">Show HN: iPhone 2005 weird "Blob Keyboard" simulator</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_43595442">101 points</span> by <a href="https://news.ycombinator.com/user?id=juliendorra">juliendorra</a> <span title="2025-04-05T18:20:49 1743877249"><a href="https://news.ycombinator.com/item?id=43595442">8 hours ago</a></span> <span id="unv_43595442"></span> | <a href="https://news.ycombinator.com/hide?id=43595442&amp;goto=item%3Fid%3D43595442">hide</a> | <a href="https://hn.algolia.com/?query=Show%20HN%3A%20iPhone%202005%20weird%20%22Blob%20Keyboard%22%20simulator&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=43595442&amp;auth=73eff720157eb1fabbb5a96d6d3aa1dc7ef9a0e8">favorite</a> | <a href="https://news.ycombinator.com/item?id=43595442">34&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hi HN,</p><p>I teach tech design history, and one of the key stories I cover is the development of the original iPhone keyboard by Ken Kocienda. Reading about it in his book "Creative Selection" is great, but I wanted my students (and now you!) to actually <i>feel</i> this step in the process.</p><p>So, I built a web simulator of the "Blob Keyboard", Kocienda's very first attempt at a touchscreen keyboard that actually works, from September 2005:</p><p>Try the Blob Keyboard: <a href="https://juliendorra.github.io/blob-keyboard-simulator/blob-keyboard-simulator.html" rel="nofollow">https://juliendorra.github.io/blob-keyboard-simulator/blob-k...</a></p><p>- Tap for the middle letter</p><p>- Swipe left or right for the side letters</p><p>More on the github repo: <a href="https://github.com/juliendorra/blob-keyboard-simulator">https://github.com/juliendorra/blob-keyboard-simulator</a></p><p>The Blob Keyboard prototype emerged during a UX crisis for iPhone team (their software keyboard just didn't work at all, fingers being too big, and the Newton failure loomed over them), highlighting how innovation is rarely a straight path. It was developed on a tethered touchscreen display codenamed "Wallaby".</p><p>To make this simulator as authentic as possible, I referenced images from Kocienda's book and even got direct feedback and guidance from Ken Kocienda himself on Bluesky.</p><p>What to expect (or… what not to expect):</p><p>This is a reconstruction of a very early prototype with limitations reflecting that specific moment. The goal was to test first if typing with accuracy was even possible, as all the rest was moot if it failed!</p><p>It's NOT QWERTY: They were still hoping to get us out of QWERTY, but then familiarity won.</p><p>No Backspace: You can't delete.</p><p>No Cursor Movement: The text field is just a simple display.</p><p>No Caps or Numbers: Only lowercase letters.</p><p>No Smooth Animations: Keys just "pop" instantly when pressed. Kocienda noted that your eye fills in the gaps, giving a sense of movement.</p><p>Best Experience:</p><p>While it works with a mouse/trackpad on desktop, it's designed for touchscreens to better replicate the original Wallaby hardware interaction. Use it on your phone!</p><p>This project aims to provide a tangible glimpse into a turning point moment in iPhone development and the iterative nature of design. It's like stepping back in time and trying out that early demo on Kocienda's desk.</p><p>I would love to hear your reactions and thoughts on experiencing this piece of UI history! What other significant prototype do you wish you could experience?</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA's Project Scientist Faces Painful Choices as Voyager Mission Nears Its End (106 pts)]]></title>
            <link>https://gizmodo.com/keeping-voyager-alive-nasas-project-scientist-faces-painful-choices-as-the-iconic-mission-nears-its-end-2000580634</link>
            <guid>43595293</guid>
            <pubDate>Sat, 05 Apr 2025 18:01:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/keeping-voyager-alive-nasas-project-scientist-faces-painful-choices-as-the-iconic-mission-nears-its-end-2000580634">https://gizmodo.com/keeping-voyager-alive-nasas-project-scientist-faces-painful-choices-as-the-iconic-mission-nears-its-end-2000580634</a>, See on <a href="https://news.ycombinator.com/item?id=43595293">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              
              
              <p>In 1977, two probes launched less than a month apart on a mission to the great beyond. The twin Voyager spacecraft were to travel where no other mission had gone before, exploring what lies outside the vast bubble that surrounds our solar system, beyond the influence of our host star.</p> <p>Voyager 1 reached the beginning of interstellar space in 2012, while Voyager 2 reached the boundary in 2018, traveling beyond the protective bubble surrounding the solar system known as the heliosphere. The Voyager probes were the first spacecraft to cross into interstellar space and have been exploring the unfamiliar region for nearly 48 years. But all good things must come to an end, and the iconic mission is gradually losing steam as it approaches oblivion.</p> <p>The Voyagers are powered by heat from decaying plutonium, which is converted into electricity. Each year, the aging spacecraft lose about 4 watts of power. In an effort to conserve power, the <a href="https://gizmodo.com/nasa-shuts-off-voyager-science-instrument-more-power-cuts-ahead-to-keep-both-probes-going-2000572202">mission team has turned off any systems that were deemed unnecessary</a>, including a few science instruments. Each Voyager spacecraft began with 10 instruments, but now have just three each. The two spacecraft now have enough power to operate for another year or so before engineers are forced to turn off two more instruments.</p> <p>The Voyager team, some of whom have worked on the mission since it first began, are forced to make these tough decisions to keep the mission going, in addition to <a href="https://gizmodo.com/nasa-pulls-off-delicate-thruster-swap-keeping-voyager-1-mission-alive-2000497434">coming up with creative solutions to resolve menacing glitches</a> that affect the spacecraft as they weather the harsh space environment.</p> <p>Linda Spilker, the Voyager mission’s project scientist, spoke to Gizmodo about the challenges that come with operating the outdated spacecraft, and passing on the knowledge of the Voyagers to the newer generations of scientists and engineers who have joined the mission.</p>

 <p>This interview has been lightly edited for clarity and length.</p> <p><strong>Passant Rabie, Gizmodo: How long have you worked on the Voyager mission?</strong></p> <p><strong>Spilker:</strong> I started working on Voyager in 1977, it was my first job out of college, and I had a choice between the Viking extended mission or the Voyager mission. I, of course, hadn’t heard of Voyager. So I said, where’s Voyager going? And they said, well, Jupiter and Saturn and onto Uranus and Neptune with Voyager 2 if all goes well. And I thought, oh my goodness—I remember in third grade, I got a little telescope I used to use to look at the Moon and look at Jupiter and Saturn, and look for little moons around Jupiter and see if I could spot the rings around Saturn. So the thought of a chance to go visit these worlds that were really only tiny dots in my little telescope, I said, “sign me up.”</p> <p><strong>Gizmodo: How has the mission evolved over the years?</strong></p> <p><strong>Spilker:&nbsp;</strong>The number of people that are working on and flying Voyager is a whole lot smaller than it was in the planetary days. We’ve turned off a lot of instruments on Voyager. We had some pretty big teams with the remote sensing instruments, the cameras, the spectrometers, etc, that are out on a boom on the end of the spacecraft. As the instruments turned off, the mission got smaller.</p>

 <p>There really was the thinking after Neptune, that Voyager would probably only last a few more years and so they had a very small team, and they kind of were, in a sense, basically neatening up everything and putting Voyager in a mode that could operate longterm. A lot of the engineers, as well as the scientists, were rolling off the mission, leaving just a very small operations team for what we call the Voyager interstellar mission.</p> <p>The challenge was, can we reach the heliopause? We didn’t know where it was, we had no idea how far away it was. We got to Neptune, and then we thought, “well, maybe it’s just another 10 [astronomical units] or so, a little bit further, a little bit further.” And so every time we got a little bit further, the modelers would go back, scratch their heads and say, “ah, it could be a little bit more, a little bit farther away,” and so on and on that continued, until finally, Voyager 1 crossed the heliopause in 2012. If you think about that, that’s like 21 years after the start of the mission. And then, six years after that, Voyager 2 crossed the heliopause, and ever since then, they’ve been flying in interstellar space, making unique measurements about the particles in interstellar space, the cosmic ray abundance, the magnetic field. Basically, it’s a chance to explore—once you cross that boundary, there’s a whole new region, a whole new realm out there in interstellar space.</p> <p><strong>Gizmodo: Is it an emotional decision to turn off Voyager’s instruments?</strong></p> <p><strong>Spilker:&nbsp;</strong> I was talking to the cosmic ray instrument lead, and I said, “Wow, this must really be tough for you to see your instrument turned off.” He helped build the instrument in the early 1970s. This instrument that’s been sending you data, and that’s been part of your life for over 50 years now. And he said, it was hard to think about turning it off for the whole team. It’s kind of like losing a best friend, or someone that’s been a part of your life for so many years, and then suddenly, it’s silent.</p>

 <p>At the same time, there’s this pride that you were part of that, and your instrument got so much great data—so it’s a mix of emotions.</p> <p><strong>Gizmodo: What are the challenges that come with operating a mission for this long?</strong></p> <p><strong>Spilker:&nbsp;</strong> The spacecraft was built in the 1970s, and so that’s the technology that we had in those days. And we didn’t have very much computer memory, so we had to be very careful and think through what we could do with this tiny amount of computer memory.</p>

 <p>So the challenge with these aging components is, how long until a key piece fails? We’re well past the warranty of four years. We also have less power every year, about 4 watts less power so we have to find 4 watts per year to turn off on the spacecraft. The spacecraft had a lot of redundancy on it, so that means two of every computer and two of all the key components. We’ve been able to turn off those backup units, but we’re now at the point where, to really get a significant amount of power, all that’s left are some of the science instruments to turn off. So, that’s where we’re at.</p> <p>Then, of course, if you have less power, the temperature goes down inside. There’s something called a bus that has all the electronics inside, and that’s getting colder and colder. Along the outside of the bus are these tiny lines of hydrazine that go to the thrusters, so we started to worry about the thermal constraints. How cold can the lines get before they freeze? How cold can some of these other components get before they stop working? So that’s another challenge.</p>  <p>Then there are individual tiny thrusters that align the spacecraft and keep that antenna pointed at the Earth so we can send the data back, and they’re very slowly clogging up with little bits of silica, and so their puffs are getting weaker and weaker. That’s another challenge that we’re going through to balance.</p>

 <p>But we’re hopeful that we can get one, possibly two, spacecraft to the 50th anniversary in 2027. Voyager’s golden anniversary, and perhaps even into the early 2030s with one, maybe two, science instruments.</p> <p><strong>Gizmodo: What about the language that the spacecraft use?</strong></p> <p><strong>Spilker:&nbsp;</strong> They use something called machine language, and I think it’s a language that’s unique to Voyager’s program. There are three different computers, an attitude control computer, another computer for commands, and another computer that basically configures the data and sends it back to the ground.</p> <p>So you have to configure these very tiny memories, and it’s in a machine language that nobody really uses anymore. We got some experts to come back and help us solve some of the problems we’ve had on the spacecraft, or other engineers who have had to learn the machine language. We had a chip failure on one of the computers, so we had to reprogram that computer and so we brought in some experts, and they really enjoyed it, trying to troubleshoot and figure out what’s wrong. And it was like a detective story, you know, what can we do? And they figured it out, and it worked.</p> <p>With Voyager, what often happens is, everything looks really good and then something goes wrong on the spacecraft. And in this case, all of a sudden we went from data coming back every day to just a tone, a signal that said the spacecraft is still there.</p>

 <p>One good analogy is going from getting letters from Voyager—you open them up and read about what’s happening every day—to now getting a letter, opening it, and finding it blank. You have no information coming back from Voyager. Imagine your computer fails, and the screen is dark</p> <p>We were sending up commands and trying to figure out what happened, and ultimately got something called a memory readout, and we found that a chip had failed. We knew which parts of the computer programs were on that chip, and then it was a matter of taking those pieces and then finding enough free space on the rest of the computer to reprogram it and get it to work again. But in bringing in those people, where do you start? In the 70s, we didn’t have the computers we do today. A lot of Voyager material is in memos, and sometimes the memos are scanned in a PDF file. And so you have to go on, literally, a sort of a hunting, like, which would be the most useful for me to look at. Some of the engineers had a big diagram up on the wall of what the computer looked like and all the paths that it had to go through to figure it all out. And they just stuck sticky notes all over as they were figuring it out.</p>

 <p>It was a mix of bringing in people who really knew and understood that computer—one of the retirees really understands the flight data system computer—and subject matter experts, and we would get them up to speed and have them work with the Voyager team. Meanwhile, the scientists are patiently waiting for their data to come back.</p> <p><strong>Gizmodo: You&nbsp;</strong><b>mentioned that the team has gotten smaller over the years. Is it basically the same people that have been working on the mission all along or do you have to bring in new people and fill them in?</b></p>

 <p><strong>Spilker:</strong> As you can imagine, most of the people are new. There are really only a handful that helped build the instruments in the 1970s, and a few of the scientists that are left have worked on the mission from the beginning until now.</p> <p>We’ve actually brought back some people who retired, who were there in that time frame of building and coding Voyager, so they have come back and now work part time. Retirees are very happy to come back and help us. And then, of course, a lot of younger people that have come on and bring their own experiences, and so we’ve been training several new people recently into the roles that we need to operate.</p> <p>On the science side, there’s a series of guest investigators—basically modelers and theorists—who work with the scientists on the Voyager teams to help pass that knowledge forward. In other words, to mentor the next generation of scientists who might want to work with the data in the future.</p> <p><strong>Gizmodo: As a scientist, what have been the most important things that you’ve learned from the Voyager mission?</strong></p>

 <p><strong>Spilker:&nbsp;</strong>Voyager left breadcrumbs, clues for future missions to come. One of Voyager’s goals was to see through to the surface of Saturn’s Moon, Titan. We didn’t know if it could have liquid oceans on the surface, or what the surface looked like. During Voyager’s close flyby of Titan, we found that none of its instruments or camera filters could penetrate through the haze. It looked like a bad day in a smoggy city.</p> <p>It was Voyager’s discovery, or non-discovery, of not being able to see the surface of Titan, that led to the Cassini mission. After Voyager’s flyby, NASA and the European Space Agency got together and said, “we need to go back.”</p> <p>I had a chance to go work on Cassini. I got in very early, and helped formulate the mission concept. I spent around 30 years on Cassini, and then the mission ended in 2017. At that point, I was thinking of retiring but then I got the opportunity to go back to Voyager and work with Edward Stone [who served as project scientist for Voyager from 1972 to 2022] and the science team, and go back to the mission where I first started.</p> <p>I went home and I told my husband, “I don’t think I’m going to retire.”</p>

 <figure id="attachment_2000584812" aria-describedby="caption-attachment-2000584812"><img decoding="async" src="https://gizmodo.com/app/uploads/2025/04/LUCKYPEANUTS.jpeg" alt="Luckypeanuts" width="1920" height="1280" srcset="https://gizmodo.com/app/uploads/2025/04/LUCKYPEANUTS.jpeg 1920w, https://gizmodo.com/app/uploads/2025/04/LUCKYPEANUTS-300x200.jpeg 300w, https://gizmodo.com/app/uploads/2025/04/LUCKYPEANUTS-1024x683.jpeg 1024w, https://gizmodo.com/app/uploads/2025/04/LUCKYPEANUTS-768x512.jpeg 768w, https://gizmodo.com/app/uploads/2025/04/LUCKYPEANUTS-336x224.jpeg 336w, https://gizmodo.com/app/uploads/2025/04/LUCKYPEANUTS-1400x932.jpeg 1400w, https://gizmodo.com/app/uploads/2025/04/LUCKYPEANUTS-680x453.jpeg 680w, https://gizmodo.com/app/uploads/2025/04/LUCKYPEANUTS-896x597.jpeg 896w, https://gizmodo.com/app/uploads/2025/04/LUCKYPEANUTS-1792x1195.jpeg 1792w" sizes="(max-width: 1023px) calc(100vw - 2rem), (max-width: 1279px) calc(100vw - 26rem), 680px"><figcaption id="caption-attachment-2000584812">Spilker explains the tradition of lucky peanuts, which date back to the Ranger Project in the 1960’s, at a gathering in Von Karman Auditorium at NASA’s Jet Propulsion Laboratory in Pasadena, California. Credit: NASA/JPL-Caltech</figcaption></figure> <p><strong>Gizmodo: How does it feel now that the mission is approaching its end?</strong></p> <p><strong>Spilker:&nbsp;</strong>We’re hoping to get one or both spacecraft to Voyager’s golden anniversary, and that’s going to be in 2027. As we get closer to the end of the mission, for me personally, it’s kind of like wrapping up my career in a way—because I’ll probably retire once the Voyager mission ends. I’m just really, really happy to have been a part of it.</p> <p><strong>Gizmodo:</strong> <strong>There’s always this debate of whether we should launch another interstellar probe. I’m wondering how you feel about that?</strong></p> <p><strong>Spilker:&nbsp;</strong>I think it would be a great idea, it could even go further than Voyager.</p> <p>We know that material mostly comes from supernova explosions, and that those explosions create bubbles in space filled with material that came from the exploding star. Earth and the rest of the planets are inside this heliopause [the outer edge of the bubble that surrounds our solar system]. But there are other bubbles.</p>

 <p>You can imagine, every time you have a supernova, you get a new bubble, and those bubbles are all there in space. How far do you have to keep going to reach another bubble? And what is it like to get farther and farther away from the Sun? One of the questions of the Voyager mission is, how far does the Sun’s influence continue into interstellar space?</p> <p>We’re still working and thinking about an interstellar probe that would go much, much farther than Voyager. You’re talking about a multi-generation mission.</p> <p><strong>Gizmodo: Should we have already launched one?</strong></p> <p><strong>Spilker:&nbsp;</strong>There’s so many interesting places to go. Prior to Voyager, we had no idea what the heliopause was like. Then getting this sort of taste of interstellar space makes us want to go back.</p> <p>It’s like going to so many places, you get to answer all these questions and make tremendous discoveries, but you leave behind a list of questions that’s much longer than the ones you answered.</p>

 <p><strong>Gizmodo: Do you worry that we won’t be able to recreate a mission like Voyager again under the current circumstances at NASA?</strong></p> <p><strong>Spilker:&nbsp;</strong>We’re entering a new and interesting era. You have the private industry wanting to play a bigger role in getting us to space. In a certain sense, some of these bigger rockets could deliver a mission to Uranus or Neptune in a much shorter time.</p> <p>I see hopeful signs, but it’s always tough when you have budgets to balance and other things to look out for. But if you look at when I started at NASA’s Jet Propulsion Laboratory to now, the number of missions that are flying in space— whether they’re missions to planets or to study our Sun—there are so many more missions today. There’s just been sort of a blossoming of scientific missions and our understanding of our place in the universe.</p> <p>So I’m hopeful, there’s always tough times to weather. We’ve been through tough times before, and I think we’ll weather this one.</p>
                          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Faster interpreters in Go: Catching up with C++ (207 pts)]]></title>
            <link>https://planetscale.com/blog/faster-interpreters-in-go-catching-up-with-cpp</link>
            <guid>43595283</guid>
            <pubDate>Sat, 05 Apr 2025 17:59:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://planetscale.com/blog/faster-interpreters-in-go-catching-up-with-cpp">https://planetscale.com/blog/faster-interpreters-in-go-catching-up-with-cpp</a>, See on <a href="https://news.ycombinator.com/item?id=43595283">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>By <!-- -->Vicent Martí<!-- --> | <time datetime="2025-03-20">March 20, 2025</time></p><p>The SQL evaluation engine that ships with Vitess, <a href="https://github.com/vitessio/vitess">the open-source database that powers PlanetScale</a>, was originally implemented as an AST evaluator that used to operate directly on the SQL AST generated by our parser. Over this past year, we've gradually replaced it with a Virtual Machine which, despite being written natively in Go, performs similarly to the original C++ evaluation code in MySQL. Most remarkably, the new Virtual Machine has repeatedly proven itself easier to maintain than the original Go interpreter, even though it's orders of magnitude faster. Let's review the implementation choices we've made to get these surprising results.</p><h2 id="whats-a-sql-evaluation-engine"><a href="#whats-a-sql-evaluation-engine">What's a SQL evaluation engine?</a></h2><p>Vitess has been designed for unlimited horizontal scaling. To accomplish this, all queries to a Vitess cluster must go through a <code>vtgate</code>. Since you can deploy as many <code>vtgate</code> instances as you want, because they’re essentially stateless, this allows you to grow the capacity of your cluster linearly.  The job of each gate is the most complex part of the whole distributed system. It parses the SQL of the incoming queries and creates a shard-aware query plan, which we evaluate in one or many of the shards of the cluster. Then, we aggregate the results of these evaluations, and return them to the user.</p><p>One of the reasons why Vitess works so well in practice (in both performance and in ease of adoption) is that every shard in a cluster is backed by a real MySQL instance. Even the more complex SQL queries can be decomposed into simpler statements that are evaluated in the underlying MySQL database. Hence, the results of these queries always match what you’d expect from querying MySQL directly.</p><p>However, SQL queries in the real world can get <em>really wild</em>. We need to support pretty much every kind of query that a normal MySQL instance supports, but we need to evaluate it across several MySQL instances. This means that sometimes, we don’t get to fall back to MySQL to evaluate all our SQL expressions.</p><p>Think of a rather simple query such as this:</p><pre data-language="sql">SELECT inventory.item_id, SUM(inventory.count), AVG(inventory.price) AS avg_price
FROM inventory
WHERE inventory.state = 'available' AND inventory.warehouse IN ? 
GROUP BY inventory.item_id
HAVING  avg_price &gt; 100;
</pre><p>Assuming this query is executed in a sharded Vitess cluster, the inventoried items can exist in any of the shards. Hence, our query planner will prepare a plan that queries all shards in parallel, <a href="https://planetscale.com/blog/grouping-and-aggregations-on-vitess">pushing down part of the aggregation to MySQL</a>, and then we'll perform the aggregations (<code>SUM</code> and <code>AVG</code>) locally in the <code>vtgate</code>. The <code>state</code> and <code>warehouse</code> checks in the <code>WHERE</code> clause can and will be executed directly on the MySQL instance that powers each shard. But the last expression, <code>avg_price &gt; 100</code>, applies to the result of the aggregation, which is only available inside Vitess. This is where the Vitess evaluation engine comes in.</p><p>Our evaluation engine is an interpreter that supports the majority of the scalar expressions in the SQL dialect used by MySQL. This does not include high level constructs such as performing a <code>JOIN</code>,  the grouping of a <code>GROUP BY</code>, etc (these are performed directly by the planner, as we’ve seen), but the actual sub-expressions that you’d see as the condition of a <code>WHERE</code> clause, or a <code>GROUP BY</code> clause. Any piece of SQL that cannot be lowered to be executed in MySQL by the planner is evaluated locally in Go by the engine.</p><p>Of course, these SQL sub-expressions are not arbitrarily complex. They are not even Turing complete (as they cannot loop!), so you may think that a statement like <code>avg_price &gt; ?</code> would be trivial to evaluate, but as in most engineering problems, there’s a wealth of nuance when doing these things in the real world.</p><p>SQL is an incredibly dynamic language full of quirks, and the SQL in MySQL, doubly so. We have spent an inordinate amount of time getting every single corner case of SQL evaluation to match exactly MySQL’s behavior. In fact, our <a href="https://github.com/vitessio/vitess/blob/main/go/vt/vtgate/evalengine/testcases/cases.go">test</a> <a href="https://github.com/vitessio/vitess/blob/main/go/vt/vtgate/evalengine/testcases/inputs.go">suite</a> and <a href="https://github.com/vitessio/vitess/tree/main/go/vt/vtgate/evalengine/integration">fuzzer</a> are so comprehensive that we routinely find bugs in the original MySQL evaluation engine, which we have to fix upstream (<a href="https://github.com/mysql/mysql-server/pull/602">like this collation bug</a>, <a href="https://github.com/mysql/mysql-server/pull/517">this issue in the <code>insert</code> SQL function</a> or <a href="https://github.com/mysql/mysql-server/pull/515">this bug when searching substrings</a>). Nonetheless, being fully accurate is not enough. For most queries, these expressions are evaluated once or even more than once <em>for every returned row</em>, so in order to not introduce additional overhead, evaluation needs to be as quick as possible.</p><p>As discussed earlier, the first version of the evaluation engine in Vitess was an AST-based interpreter, operating directly on top of the SQL AST generated by our parser. This was a very straightforward design that allowed us to focus on <em>accuracy</em>, at the expense of performance. Let's discuss our new design for replacing this interpreter with a fully fledged virtual machine which is both faster and easier to maintain. Starting with the basics.</p><h2 id="the-shapes-of-an-interpreter"><a href="#the-shapes-of-an-interpreter">The shapes of an interpreter</a></h2><p>For those new to programming language implementations, there are roughly 3 ways to execute a <em>dynamic</em> language at runtime. In increasing level of complexity <em>and</em> performance:</p><ol><li>An AST-based interpreter, where the syntax of the language is parsed into an AST and evaluation is performed by recursively walking each node of the AST and computing the results. <em>(this is the way the <code>evalengine</code> in Vitess used to work!)</em></li><li>A bytecode VM, where the AST is compiled into binary bytecode that can be evaluated by a virtual machine — a piece of code that simulates a CPU, but with higher-level instructions. <em>(this is what we've recently shipped!)</em></li><li>A JIT compiler, in which the bytecode is compiled directly into the host platform's native instructions, so it can be executed directly by the CPU without being interpreted by a Virtual Machine. <em>(we'll talk about this later!)</em></li></ol><p>The first thing to consider here is whether the upgrade from an AST interpreter to a virtual machine makes sense from a performance point of view. Here’s an intuition: SQL expressions are incredibly dynamic (when it comes to typing), very high level (when it comes to each primitive operation), and with very little control flow (when it comes to evaluation -- SQL expressions don't really loop, and conditionals are rare; their flow is always lineal!). This can lead us to believe that there's no performance to be squeezed from translating the AST-based evaluation engine into bytecode. The AST is already well suited for high level operations and type-switching!</p><p>This is only <em>superficially</em> true. Lots of programming languages are highly dynamic and they manage to run in bytecode VMs much more efficiently than with an AST interpreter. A clear example of this is the now ancient transition that Ruby did from its original AST interpreter in MRI to YARV, a bytecode VM. Python also did a similar switch very early on. And you can bet that literally no JavaScript engines are using AST evaluation: even though the goal of these engines is to start running JS as soon as possible, they still compile to (very efficient) bytecode interpreters before JIT compilation kicks in.</p><p>So where’s the advantage of a virtual machine versus an AST interpreter? A lot of it boils down to instruction dispatching, which can be made very fast (more on this later!). But it is true that for SQL expressions, we’re actually going to execute very few instructions. Hence, to squeeze performance out of the VM, we’re going to have to come up with new tricks.</p><p>The initial approach I had in mind for our SQL virtual machine was based on <a href="https://publications.sba-research.org/publications/dls10.pdf">Efficient Interpretation using Quickening</a> by Stefan Brunthaler. The idea behind this paper is that dynamic programming languages are very hard to execute efficiently because of the lack of information about types. A simple expression such as <code>a + 1</code> must be interpreted in a completely different way depending on whether <code>a</code> is an integer, a floating point number of even a string. To optimize these operations in practice, the paper suggests rewriting the bytecode from more generic instructions (e.g. the sum operator that needs to figure out the types of the two operands to know how to sum them) into specific static instructions which are specialized for the types they operate on at runtime (e.g. the sum operator that knows that both operands are integers and can sum them right away).</p><p>To do that, a quickening VM needs to figure out <em>at runtime</em> the types of the expressions being evaluated and incrementally rewrite the bytecode into instructions that operate on them. This is very hard to do in practice! But after implementing a good chunk of specialized instructions for the different types of operators in SQL and attempting to runtime rewrite them, I noticed an opportunity to take the idea even further by making it more efficient and, crucially, simpler.</p><p>It turns out that the semantic analysis we perform in Vitess is advanced enough that, through careful integration with the upstream MySQL server and its information schema, it can be used to <em>statically type the AST of the SQL expressions we were executing</em>. This took a lot of effort to implement, but resulted in a big win: since the planner knows the types of the actual inputs that will be used to evaluate each SQL expression, we can derive from those the types of all sub-expressions at compilation time, resulting in byte-code that is already specialized without requiring runtime rewriting.</p><p>Now we just need to implement a Virtual Machine to efficiently interpret the specialized bytecode!</p><h2 id="an-efficient-virtual-machine-in-go"><a href="#an-efficient-virtual-machine-in-go">An efficient Virtual Machine in Go</a></h2><p>Implementing a VM usually involves a lot of complexity. As we’ve explained, you have to write a compiler that processes the input expression AST and generates the corresponding binary instructions (you have to come up with an encoding even!) and <em>afterwards</em> you have to implement the actual VM, which decodes each instruction and performs the corresponding operation. And you have to constantly keep these in sync! Any mismatches between the compiler that emits the bytecode and the VM that executes it are often catastrophic and very hard to debug.</p><p>Historically, a bytecode VM has always been implemented the same way: a big-ass switch statement. You decode an instruction, and switch on its type to jump to the operation that needs to be performed. This is often a performance advantage against AST interpreters, because switching in practice is quite fast (particularly when implemented in C or C++ like most VMs are), and allows execution to happen linearly, without recursion.</p><p>This design, however, also has its fair share of shortcomings. Mike Pall, JIT-master extraordinaire and author of LuaJIT, gives a very insightful rundown of these issues on <a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html">this mailing list post from 2011</a>. Allow me to summarize for this blog: Besides the fact that the VM's instructions need to be kept in-sync with the compiler, the actual performance of the main VM loop in a language with many instructions is not great in practice because compilers usually struggle when compiling massive functions, and these functions <em>are</em> massive. They spill registers all over the place on each branch of the switch, because it's hard to tell which branches are hot and which ones are cold. With all the pushing and popping, the jump into the switch's branch often looks more like a function call, so a lot of the performance benefits of the virtual machine dissipate.</p><p>Mike was discussing C compilers in that post, but it's safe to assume that these problems are the same for a virtual machine implemented in Go. After a lot of testing, I can assure you that they are actually <em>much worse</em> because the Go compiler is not great at optimization. There’s always a trade-off between optimization and fast compile times, and the Go authors have historically opted for the latter.</p><p>One key issue for Go is that often the different branches of the switch statement are jumped to via <em>binary search</em> instead of a jump table. Switch jump table optimization was <a href="https://go-review.googlesource.com/c/go/+/357330">implemented surprisingly late on the compiler</a>, and in practice it is <em>very fiddly</em>, without any way to enforce it. You have to tweak the way the VM's instructions are encoded carefully to ensure that you're jumping in the VM's main loop, and you have no way to reliably check whether your virtual machine’s dispatch code has been properly optimized besides reviewing the generated assembly yourself.</p><p>Clearly, switch-based VM loops are not the state of the art for writing efficient interpreters, neither in Go nor in any other programming language. So what <em>is</em> the state of the art then? Well, when it comes to Go it turns out that there's nobody doing fast interpreters right now (at least nobody I can find). The people who are doing interesting work here, such as the <a href="https://github.com/tetratelabs/wazero"><code>wazero</code> WASM implementation</a> are focusing their performance efforts on JIT. So we’re going to have to innovate!</p><p>Outside of Go, the most interesting approach for interpreters implemented in C or C++ is <strong>continuation-style evaluation loops</strong>, as seen <a href="https://blog.reverberate.org/2021/04/21/musttail-efficient-interpreters.html">in this report from 2021 that implements this technique for parsing Protocol Buffers</a>. This involves implementing all the opcodes for the VM as freestanding functions that operate on the VM as an argument, with the return of the function being <em>a callback to the next step of the computation</em>. It does sound like something expensive and, huh, recursive, but the trick is that newer versions of LLVM allow us to mark functions as <em>forcefully</em> tail-called (see: https://en.wikipedia.org/wiki/Tail_call), so the resulting code is not recursively calling the VM loop but instead <em>jumping</em> between the operations and using the free-standing functions as an abstraction to control register placement and spillage. The most recent release of Python 3.14 actually <a href="https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-tail-call">ships an interpreter based on this design</a>, boasting up to 30% improvement when executing Python code.</p><p>Unfortunately, this is not something we can do in Go because as we discussed earlier, the Go compiler is allergic to optimization. It <em>can</em> sometimes emit tail calls, but it needs to be tickled in just the right way, and this implementation simply does not work in practice unless the tail-calls are guaranteed at compilation time. But what if we keep the same design with free-standing functions for each instruction and instead of tail-calling, we forcefully return control to the evaluation loop after each one? This could be implemented very easily by not emitting our compiled program as “byte code”, but instead emitting <strong>a slice of function pointers to each instruction</strong>. The design may be a bit counter-intuitive, but it has a lot of very interesting properties.</p><p>First, the VM becomes trivial! It's just a few lines of code, and it doesn't have to worry about optimizing any large switch statements. It's just repeatedly calling functions one after the other! Here’s a simplified example, but if you check <a href="https://github.com/vitessio/vitess/blob/b05df12741adf1314839694e489e687e7ec6c6ea/go/vt/vtgate/evalengine/vm.go#L73-L98">the actual implementation in Vitess</a> you’ll see that a real virtual machine implementation is hardly more complicated than this.</p><pre data-language="go">func (vm *VirtualMachine) execute(p *Program) (eval, error) {
	code := p.code
	ip := 0

	for ip &lt; len(code) {
		ip += code[ip](vm)
		if vm.err != nil {
			return nil, vm.err
		}
	}
	if vm.sp == 0 {
		return nil, nil
	}
	return vm.stack[vm.sp-1], nil
}
</pre><p>All we need to return when executing each instruction is the offset for the instruction pointer <code>ip</code>. Most functions return <code>1</code>, which causes the next instruction to be executed, but by returning negative or positive values, you can implement all control flow, including loops and conditionals.</p><p>Besides the greatly simplified virtual machine, the second advantage of this approach is that the compiler <em>also</em> becomes trivial, because there is no bytecode! Instead, the compiler emits the individual instructions directly by pushing "callbacks" into a slice. There are no instruction opcodes to keep track off, no encoding to perform and nothing to keep in sync with the VM. <strong>Developing the compiler means developing the VM simultaneously</strong>, which greatly improves iteration speed and prevents a whole class of bugs that happen often when developing virtual machines.</p><pre data-language="go">func (c *compiler) emitPushNull() {
	c.emit(func(vm *VirtualMachine) int {
		vm.stack[vm.sp] = nil
		vm.sp++
		return 1
	})
}
</pre><p>As you may notice, there’s a bit of a hiccup here when it comes to modeling the instructions for a non-trivial language: if there's no instruction encoding, then we cannot have instructions with arguments.</p><p>This is a big problem in a language like C (traditionally used to implement most programming language interpreters), which is why this technique is never seen there. But it’s actually not a problem for us,  because the Go compiler actually supports <em>closures</em>! We can emit any instruction we want and the Go compiler will automatically capture its arguments inside the callback. We don't have to think about how to encode our arguments in the bytecode, and in fact, our arguments can be as complex as they need to be: the resulting callback will contain a copy of them created by the Go compiler. It's essentially a poor man's JIT, aided by the compiler, and it works amazingly well in practice, both performance-wise and for ergonomics.</p><p>Check out this compiler method that generates an instruction to push a <code>TEXT</code> SQL object from the input rows into the stack:</p><pre data-language="go">func (c *compiler) emitPushColumn_text(offset int, col collations.TypedCollation) {
	c.emit(func(vm *VirtualMachine) int {
		vm.stack[vm.sp] = newEvalText(vm.row[offset].Raw(), col)
		vm.sp++
		return 1
	})
}
</pre><p>Both the offset in the input <code>rows</code> array <em>and</em> the collation for the text are statically baked into the generated instruction!</p><h2 id="almost-statically-typed"><a href="#almost-statically-typed">Almost statically typed</a></h2><p>With the fully static typing for SQL expressions (derived from the type information in the planner) we get to design an extremely efficient virtual machine where every single instruction is specialized for the type of the operands it executes on. This is both the optimal and the simplest design for a VM because we never have to do type switching during evaluation. But we’re dealing with SQL here (or, more accurately, the SQL dialect of MySQL), so not everything is rainbows and unicorns. Very often it’s quite the opposite.</p><p>Let’s consider this wildly complex SQL expression: <code>-inventory.price</code>. That is, the negation of each of the values in the <code>inventory.price</code> column of our query. We know (thanks to our semantic analysis, and the schema tracker) that the type of the <code>inventory.price</code> column is <code>BIGINT</code>. So what could be the type of <code>-inventory.price</code>? Naive readers without experience in the magical world of SQL may believe the resulting type is <code>BIGINT</code>, but that’s not the case in practice!</p><p>The vast majority of the time, the negation of a <code>BIGINT</code> yields indeed another <code>BIGINT</code> value. But when the actual value of the <code>BIGINT</code> is -9223372036854775808 (i.e. the smallest value that can be represented in 64 bits), negating it promotes the value into a <code>DECIMAL</code>, instead of silently truncating it, or returning an error. You can see how this can easily throw a wrench in our statically compiled instructions for our virtual machine. Suddenly the static type checking we’ve computed is no longer valid because the types of the expression no longer depend on the types of the inputs, but on the actual <em>values</em> of the inputs. In order to continue evaluating the result of this negation, we’d always have to type-check again at runtime, defeating the whole point of static typing to begin with.</p><p>To work around this issue, we’re <em>not</em> introducing more type switches at runtime. We’re using a classic trick which can be seen all the time in JIT compiled code and very rarely, if ever, in virtual machines: <strong>de-optimization</strong>. There’s a small list of expressions where corner cases (e.g. overflow) can result in dynamic typing at runtime. Whenever this happens, we simply bail out of executing in our virtual machine and fall back to executing on the old AST evaluator, which has always performed type switching at runtime. This is very similar to what JIT compilers do when they detect that the runtime type of a value no longer matches the generated code they’ve emitted; they fall back from the native code to the virtual machine. In our case, we’re one step behind, falling back from the virtual machine to the AST interpreter, but the performance implications are the same. This design allows us to keep our interpreter executing statically typed code without any type switches at runtime. Here's an example of what integer negation looks like when compiled:</p><pre data-language="go">func (c *compiler) emitNeg_i() {
	c.emit(func(vm *VirtualMachine) int {
		arg := vm.stack[env.vm.sp-1].(*evalInt64)
		if arg.i == math.MinInt64 {
			vm.err = errDeoptimize
		} else {
			arg.i = -arg.i
		}
		return 1
	})
}
</pre><p>There is one significant drawback with this approach, however: the code for the AST interpreter can never be removed from Vitess. But this is, overall, not a bad thing. Just like most advanced language runtimes keep their virtual machine interpreter despite having a JIT compiler, having access to our classic AST interpreter gives us versatility. It can be used when we detect that an expression will be evaluated just <em>once</em> (e.g. when we use the evaluation engine to perform constant folding on a SQL expression). In those cases, the overhead of compiling and then executing on the VM trumps a single-pass evaluation on the AST. Lastly, when it comes to accuracy, being able to fuzz both the AST interpreter and the VM against each other has resulted in an invaluable tool for detecting bugs and corner cases.</p><h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2><p>This technique for virtual machine implementation is not fully novel (I’ve seen it used before for a rules-based authorization engine in the wild!), but as far as I can tell it has never been used in Go. Given the constraints of the language and the compiler, the technique yields spectacular results: the new SQL interpreter in Vitess is just <em>faster</em>. Faster to write, faster to maintain and faster to execute. The benchmarks speak for themselves:</p><h3 id="evalengine-performance-in-vitess-over-time"><a href="#evalengine-performance-in-vitess-over-time">Evalengine performance in Vitess over time</a></h3><p><picture><source srcset="https://planetscale-images.imgix.net/assets/benchmark-results-D588Aj2C.png?auto=compress%2Cformat" media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><source srcset="https://planetscale-images.imgix.net/assets/benchmark-results-darkmode-BmvxW3Md.png?auto=compress%2Cformat" media="(prefers-color-scheme: dark)"><img alt="Benchmark results" height="1295" loading="lazy" src="https://planetscale-images.imgix.net/assets/benchmark-results-D588Aj2C.png?auto=compress%2Cformat" width="2361"></picture></p><p>Here we have a performance comparison of 5 different queries (ranging from very complex to very simple) between three implementations:</p><ol><li><strong>old</strong>, which is the original AST-based dynamic implementation of the <code>evalengine</code>.</li><li><strong>ast</strong>, which is the result of adding static type checking to the virtual machine and using them to partially optimize the AST evaluator.</li><li><strong>vm</strong>, which is the callback-based virtual machine implementation as discussed in this post.</li></ol><h3 id="recent-results-compared-with-mysql"><a href="#recent-results-compared-with-mysql">Recent results compared with MySQL</a></h3><p><picture><source srcset="https://planetscale-images.imgix.net/assets/benchmark-results-with-mysql-CvMaQMIM.png?auto=compress%2Cformat" media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><source srcset="https://planetscale-images.imgix.net/assets/benchmark-results-with-mysql-darkmode-CDCX_2gs.png?auto=compress%2Cformat" media="(prefers-color-scheme: dark)"><img alt="Benchmark results With MySQL" height="1295" loading="lazy" src="https://planetscale-images.imgix.net/assets/benchmark-results-with-mysql-CvMaQMIM.png?auto=compress%2Cformat" width="2361"></picture></p><p>This is the current performance of our evaluation engine pitted against the native C++ implementation in MySQL. Note that measuring the time that MySQL spends in evaluation is very tricky; these are not the total reponse times for a query, but the result of manual instrumentation in the <code>mysqld</code> server to ensure a fair comparison.</p><details><summary>Raw benchmark data</summary><div><pre>                                      │     ast      │                 vm                  │                  mysql                   │
                                      │    sec/op    │   sec/op     vs base                │    sec/op     vs base                    │
CompilerExpressions/complex_arith-32    162.75n ± 1%   50.77n ± 1%  -68.81% (p=0.000 n=10)   49.40n ±  5%  -69.64% (p=0.000 n=10+184)
CompilerExpressions/comparison_i64-32    30.30n ± 2%   16.95n ± 1%  -44.08% (p=0.000 n=10)   26.93n ± 22%  -11.12% (p=0.000 n=10+11)
CompilerExpressions/comparison_u64-32    30.57n ± 3%   17.49n ± 1%  -42.78% (p=0.000 n=10)   18.80n ±  9%  -38.53% (p=0.000 n=10+16)
CompilerExpressions/comparison_dec-32    70.75n ± 1%   52.58n ± 2%  -25.68% (p=0.000 n=10)   46.59n ±  5%  -34.14% (p=0.000 n=10+14)
CompilerExpressions/comparison_f-32      53.05n ± 1%   25.65n ± 1%  -51.64% (p=0.000 n=10)   27.75n ± 23%  -47.69% (p=0.000 n=10)
geomean                                  56.30n        28.94n       -48.60%                  31.76n        -43.58%

                                      │    ast     │                   vm                    │
                                      │    B/op    │    B/op     vs base                     │
CompilerExpressions/complex_arith-32    96.00 ± 0%    0.00 ± 0%  -100.00% (p=0.000 n=10)
CompilerExpressions/comparison_i64-32   16.00 ± 0%    0.00 ± 0%  -100.00% (p=0.000 n=10)
CompilerExpressions/comparison_u64-32   16.00 ± 0%    0.00 ± 0%  -100.00% (p=0.000 n=10)
CompilerExpressions/comparison_dec-32   64.00 ± 0%   40.00 ± 0%   -37.50% (p=0.000 n=10)
CompilerExpressions/comparison_f-32     16.00 ± 0%    0.00 ± 0%  -100.00% (p=0.000 n=10)

                                      │    ast     │                   vm                    │
                                      │ allocs/op  │ allocs/op   vs base                     │
CompilerExpressions/complex_arith-32    9.000 ± 0%   0.000 ± 0%  -100.00% (p=0.000 n=10)
CompilerExpressions/comparison_i64-32   1.000 ± 0%   0.000 ± 0%  -100.00% (p=0.000 n=10)
CompilerExpressions/comparison_u64-32   1.000 ± 0%   0.000 ± 0%  -100.00% (p=0.000 n=10)
CompilerExpressions/comparison_dec-32   3.000 ± 0%   2.000 ± 0%   -33.33% (p=0.000 n=10)
CompilerExpressions/comparison_f-32     2.000 ± 0%   0.000 ± 0%  -100.00% (p=0.000 n=10)
</pre></div></details><p>The results are stark: the pre-compiled SQL expressions when ran in the new VM are up to 20x times faster than the first implementation of SQL evaluation in Vitess, and for most cases, we've caught up with the performance of the C++ implementation in MySQL. One further detail which is not shown on the graphs, but can be seen on the raw benchmark data, is that the new virtual machine <strong>does not allocate memory</strong> to perform evaluation — a very nice side effect of the fully specialized instructions thanks to the static type checking.</p><p>Overall, we consider getting in the same performance ballpark as MySQL's C++ evaluation engine as a huge engineering success, particularly when the resulting implementation is so easy to maintain.<!-- --> <!-- -->There will always be a performance gap between Go and C++, arising from the trade-off of quality vs compilation speed in the Go compiler, and from the semantics of the language itself, but as we show here, this gap is not insurmountable. With expertise and careful design, it is possible to reap the many benefits of developing and deploying Go services without paying the performance penalty inherent in the language. In this specific case, we got there by having the capacity to perform semantic analysis and statically typing SQL expressions (something which MySQL does not do), and by choosing an efficient virtual machine design that uses the strengths of Go instead of fighting its limitations.</p><h3 id="addendum-so-why-not-jit"><a href="#addendum-so-why-not-jit">Addendum: So why not JIT?</a></h3><p>Inquiring minds may be wondering: what's next? Are we doing JIT compilation next? The answer is no. Although this design for a compiler and VM looks like an exceptional starting point for implementing a full JIT compiler <em>in theory</em>, in practice the trade-off between optimization and complexity doesn't make sense. JIT compilers are important for programming languages where their bytecode operations can be optimized into a very low level of abstraction (e.g. where an "add" operator only has to perform a native x64 <code>ADD</code>). In these cases, the overhead of dispatching instructions becomes so dominant that replacing the VM's loop with a block of JITted code makes a significant performance difference. However, for SQL expressions, and even after our specialization pass, most of the operations remain extremely high level (things like "match this JSON object with a path" or "add two fixed-width decimals together"). The overhead of instruction dispatch, as measured in our benchmarks, is less than 20% (and can possibly be optimized further in the VM's loop). 20% is not the number you're targetting before you start messing around with raw assembly for a JIT. So at this point my intuition is that JIT compilation would be a needlessly complex dead optimization.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What If We Made Advertising Illegal? (1503 pts)]]></title>
            <link>https://simone.org/advertising/</link>
            <guid>43595269</guid>
            <pubDate>Sat, 05 Apr 2025 17:57:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simone.org/advertising/">https://simone.org/advertising/</a>, See on <a href="https://news.ycombinator.com/item?id=43595269">Hacker News</a></p>
Couldn't get https://simone.org/advertising/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Dynamic Register Allocation on AMD's RDNA 4 GPU Architecture (119 pts)]]></title>
            <link>https://chipsandcheese.com/p/dynamic-register-allocation-on-amds</link>
            <guid>43595223</guid>
            <pubDate>Sat, 05 Apr 2025 17:51:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/dynamic-register-allocation-on-amds">https://chipsandcheese.com/p/dynamic-register-allocation-on-amds</a>, See on <a href="https://news.ycombinator.com/item?id=43595223">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Modern GPUs often make a difficult tradeoff between occupancy (active thread count) and register count available to each thread. Higher occupancy provides more thread level parallelism to hide latency with, just as more SMT threads help hide latency on a CPU. But while a CPU can use all of its SMT threads regardless of what code it's running, the same doesn't apply to GPUs. GPU ISAs offer a large number of very wide vector registers. Storing all registers for all thread slots would be impractical because register files must balance capacity with speed and die area usage.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp" width="824" height="545" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:545,&quot;width&quot;:824,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:17100,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ad0e36-cde9-4af8-b104-5aeae0ae836e_824x545.webp 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>For example, RDNA 4's ISA lets instructions address up to 256 vector general purpose registers (VGPRs). Each register is 1024 bits wide in wave32 mode, and each RDNA 4 SIMD has 16 thread slots. The SIMD would need a 512 KB register file to hold 256 registers for all 16 threads. In practice register requirements vary across different GPU workloads. RDNA 4, like many other GPUs, uses a smaller register file and allocates depending on what threads require. Code that needs a lot of registers can do so at the cost of less thread-level parallelism, while code that uses fewer registers can run more active threads and be less sensitive to latency. RDNA 4 desktop GPUs have a 192 KB register file per SIMD, so a GPU kernel can use all thread slots (achieve maximum occupancy) if it uses 96 or fewer vector registers.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp" width="1026" height="498" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:498,&quot;width&quot;:1026,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:8620,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702a8b22-ac95-4368-a706-3f4e3878e5a7_1026x498.webp 1456w" sizes="100vw"></picture></div></a></figure></div><p>A bigger register file obviously improves the occupancy and register usage tradeoff situation. RDNA increased SIMD register file capacity to 128 KB, up from 64 KB on GCN. RDNA 3 introduced a 192 KB register file configuration for high end GPUs, where die area is likely less of a concern. But that strategy isn’t efficient for raytracing.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp" width="1456" height="969" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:969,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1253978,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F656c63e2-c55c-4c92-af1e-8f9d8bbf7a51_2163x1440.webp 1456w" sizes="100vw"></picture></div></a></figure></div><p>AMD notes that ray traversal and hit/miss handling have different VGPR requirements. AMD uses an inline raytracing model where all raytracing stages run within the same thread. A raytracing shader’s VGPR allocation has to be set to the maximum that any stage requires, because a thread’s register allocation remains static throughout its lifetime. Even if code that needs a lot of registers only accounts for a small part of execution time, that high VGPR allocation will limit active thread count for the duration of the workload. Raytracing is particularly latency sensitive, and AMD would like to run as many threads (rays) in parallel as possible to help absorb latency.</p><p><span>Therefore RDNA 4 introduces a new dynamic VGPR allocation mode. In this mode, a thread starts with a minimum VGPR allocation and changes it throughout it’s lifetime. Rather than specify how many VGPRs a shader will use, the driver tells GPU to launch it in dynamic VGPR mode. A chip-wide </span><code>SQ_DYN_VGPR</code><span> register directly sets active thread count per SIMD, or occupancy, rather than having that inferred from shader VGPR usage. </span><code>SQ_DYN_VGPR</code><span> also controls other dynamic VGPR mode parameters, like VGPR allocation block size and deadlock avoidance mode.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp" width="337" height="280" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:280,&quot;width&quot;:337,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4692,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8314d90c-5b1d-463f-9b3c-c854e48373f0_337x280.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>As defined in Linux kernel code. I couldn’t find references/usages in either Linux or LLVM, so I’m guessing what each field does</figcaption></figure></div><p><span>Each enabled thread slot gets a single reserved VGPR block, and a newly launched thread starts with just that VGPR block active. When the thread needs more registers, it requests a new VGPR count using a </span><code>s_alloc_vgpr</code><span> instruction. </span><code>s_alloc_vgpr</code><span> attempts to allocate more registers if called with a value higher than the current allocation, or frees registers if called with a lower value. Changing VGPR allocation affects the upper end of the usable VGPR range, just like with non-dynamic VGPR allocation. Hardware hands out VGPRs in blocks of 16 or 32, depending on how the driver sets up </span><code>SQ_DYN_VGPR</code><span>. A thread can allocate up to eight blocks, so the driver must select the larger block size and give up some allocation granularity if a thread needs to use more than 128 VGPRs.</span></p><p><span>Allocation requests don’t always succeed. </span><code>s_alloc_vgpr</code><span> sets the Scalar Condition Code (SCC) to indicate success, or clears it on failure. SCC is analogous to a flag register on CPUs, and is used for branching and add-with-carry. Shader code has to check SCC to determine if an allocation request succeeded. If an allocation request fails, a shader could in theory try to find other useful work to do while periodically retrying the allocation. But doing so would be quite complex, so in practice a shader will busy-wait until allocation succeeds.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp" width="1365" height="726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1365,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:29748,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e369252-8055-4b9f-afb3-af9ba65b16d0_1365x726.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Example of dynamic register allocation used in the DirectX procedural geometry example</figcaption></figure></div><p>Therefore dynamic VGPR mode turns the occupancy question on its head. A SIMD can have as many active threads as the driver feels like, regardless of register allocation. But theoretical occupancy doesn’t tell the whole story. Threads can still get blocked waiting on VGPR allocation. A SIMD could have all thread slots filled, but some of those threads could be busy-waiting on VGPR allocation rather than making useful progress.</p><p>Busy-waiting can become more than a performance inconvenience. Dynamic VGPR allocation can lead to deadlock. AMD knows this, and describes how that can happen in RDNA 4’s ISA manual.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp" width="855" height="141" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:141,&quot;width&quot;:855,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:18458,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa68b488d-2f6e-4362-bbd9-5a46bbff3236_855x141.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I think a deadlock case can be more general than what AMD describes. If every thread in a SIMD needs to allocate more registers, but hardware doesn’t have enough free registers to satisfy any request, every thread will get stuck forever. That’s a deadlock, even if there are technically registers available.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp" width="810" height="439" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:439,&quot;width&quot;:810,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:6376,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bcbba48-90cc-48ef-a7c7-505eca11b196_810x439.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>AMD mitigates some deadlock scenarios with a deadlock avoidance mode. The ISA manual is light on details, only saying it reserves just enough VGPRs for one thread to reach maximum VGPR allocation at all times. Each thread can allocate up to eight VGPR blocks, and one block comes reserved with the thread slot, so deadlock avoidance mode would reserve 7 VGPR blocks. I believe deadlock avoidance mode works by only allowing one thread to allocate registers from the reserved pool at a time. In short:</p><ol><li><p>Base case: No reserved registers allocated. Any request can proceed</p></li><li><p>From (1), any combination of allocation requests from all threads will allow at least one thread (say thread A) to succeed</p></li><li><p>From (2), no other thread can allocate from the reserved pool, allowing thread A to increase its register allocation to the maximum should it need to.</p></li><li><p>Eventually A will leave its high register usage code section, or terminate completely, and thus free up registers for other threads to do the same.</p></li></ol><p>Needless to say, that situation isn’t great for performance because it can serialize useful work across threads. But getting to the finish line slowly is better than not getting there at all.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp" width="552" height="334" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:334,&quot;width&quot;:552,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:6912,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a40ca-920e-4308-96dc-b4d78e7f60f5_552x334.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Deadlock avoidance mode isn’t foolproof. If the programmer manages to meet three conditions:</p><ol><li><p>Two threads need to allocate registers</p></li><li><p>The high register usage sections of both threads depend on each other, for example in a producer consumer model</p></li><li><p>No other thread can give up their registers until the two threads above make progress</p></li></ol><p>Then they can run into a deadlock even with deadlock avoidance mode enabled. Programmers should probably avoid cross-thread dependencies in dynamic VGPR mode, unless they’re confident threads only wait on each other in low VGPR usage sections.</p><p>As with many new features, dynamic VGPR mode isn’t a one-size-fits-all solution. It’s narrowly targeted to start, and can only be used with wave32 compute shaders. Graphics shaders like pixel and vertex shaders can only use the regular non-dynamic launch mode. The same goes for wave64 shaders of any type.</p><p>A workgroup of threads launched in dynamic VGPR mode will “take over” the equivalent of a GPU core. That would be a Workgroup Processor (WGP) in WGP mode, or a Compute Unit (CU) in CU mode. Thus dynamic and non-dynamic threads can’t coexist on the same GPU core.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp" width="325" height="240" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:240,&quot;width&quot;:325,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:6238,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5a1bb28-cde0-43c1-b1f0-0858498cb05e_325x240.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Registers used to specify various compute program launch parameters</figcaption></figure></div><p><span>Dynamic VGPR mode may be less efficient at using register file capacity. Each enabled thread slot gets a reserved VGPR block, regardless of whether a thread is actually running in that slot. A workload that doesn’t have enough parallelism to fill all enabled thread slots would waste those reserved registers. Deadlock avoidance mode would set aside more registers that could have been easily allocated in non-dynamic mode. Drivers can reduce reserved register count by disabling deadlock avoidance mode or reducing thread slot count. Both of those options come with obvious downsides. In wave32 mode, non-dynamic register mode can allocate up to 256 registers in 24 entry blocks</span><sup>a</sup><span> on current RDNA 4 GPUs. That offers finer granularity than the 32 entry blocks needed to give a thread 256 registers in dynamic VGPR mode.</span></p><p><span>AMD isn’t the only GPU maker that lets a thread adjust register allocation mid-execution. Nvidia introduced a </span><code>setmaxnreg</code><span> PTX instruction in Hopper, and that’s carried forward to Blackwell consumer GPUs. </span><code>setmaxnreg</code><span> superficially acts like AMD’s </span><code>s_alloc_vgpr</code><span>, letting the calling thread request a different register allocation. However Nvidia’s dynamic register allocation works very differently from AMD’s, and is probably better called register reassignment. Nvidia for their part never gave this mechanism a name.</span></p><p><span>Nvidia doesn’t use a separate launch mode. Kernels always launch the regular way, with a specified register allocation that also determines how many threads they can run concurrently. For example a compute shader that uses 96 registers on Blackwell will only be able to run 5 concurrent threads in each SM sub-partition. After threads launch, they can call </span><code>setmaxnreg</code><span> to shift registers between threads in the same workgroup. Unlike </span><code>s_alloc_vgpr</code><span>, </span><code>setmaxnreg</code><span>‘s register pool is whatever the workgroup started out with. If every thread calls </span><code>setmaxnreg</code><span> and requested register count across threads is greater than what the workgroup started with, they will deadlock regardless of how much free space the register file may have.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp" width="724" height="274" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:274,&quot;width&quot;:724,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:5532,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c69e9a-d8ed-4e8e-ad9b-c8331939f2ad_724x274.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>As an aside, </span><code>setmaxnreg</code><span> is a PTX instruction. PTX in an intermediate level programming language for Nvidia GPUs with an assembly-like syntax. It isn’t assembly, which Nvidia calls SASS. However PTX is meant to give more control over emitted instructions than a C-like high level language. Therefore PTX instructions often have similarities with SASS instructions, and can offer hints about the underlying ISA.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png" width="1062" height="382" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:382,&quot;width&quot;:1062,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:60623,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e6374a-f9cc-4617-af23-729cb6be2bbd_1062x382.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The semantics around </span><code>setmaxnreg</code><span> suggest Nvidia’s mechanism is geared towards tightly orchestrated register swapping between threads. It’s not like AMD’s free-flowing dynamic allocation behavior where different threads can be out-of-phase with each other, so to speak. Nvidia’s “warpgroup” likely refers to threads sharing the same SM sub-partition, and thus the same register file.</span></p><blockquote><p><span>The same </span><code>setmaxnreg</code><span> instruction must be executed by all warps in a warpgroup. After executing a </span><code>setmaxnreg</code><span> instruction, all warps in the warpgroup must synchronize explicitly before executing subsequent setmaxnreg instructions. If a </span><code>setmaxnreg</code><span> instruction is not executed by all warps in the warpgroup, then the behavior is undefined</span></p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#miscellaneous-instructions-setmaxnreg" rel="">Miscallenous Instructions, Parallel Thread Execution ISA Version 8.7</a></p></blockquote><p><span>A determined developer could emulate AMD’s initial dynamic VGPR state on Nvidia by with a workgroup that allocates all register file capacity in a SM, then immediately has every thread trim its allocation down to the minimum. But after that, synchronization requirements on Nvidia would make it difficult to emulate AMD’s independent allocation behavior. </span><code>setmaxnreg</code><span>‘s scalar-only input makes it harder to look up a desired allocation value from memory. Of course difficult doesn’t mean impossible. A register input can be emulated with a sufficient application of conditional branches, but let’s not think about that too much.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp" width="1456" height="969" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:969,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:825958,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbab9e37-ba96-4cfb-b7e8-17571e981aec_2163x1440.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Not Hopper or Blackwell, but have a Nvidia related image to spice things up. I’m sick of seeing AI generated images everywhere, so I’m going to start taking more pictures with my DSLR and post them</figcaption></figure></div><p>In exchange for less flexibility, Nvidia should have no problem mixing “dynamic” and regular threads on the same SM. Nvidia can also adjust register allocation with finer granularity than AMD. The latter can be especially important because Nvidia has smaller 64 KB register files, and waste from “slack” register file usage can be even more painful.</p><p>Nvidia’s register reassignment mechanism isn’t well suited for AMD’s raytracing use case. However, Nvidia’s raytracing design likely doesn’t need it. Nvidia hardware uses a DXR 1.0 raytracing model. If it works like Intel, raytracing stages execute as separate thread launches on the SMs. Regular vector register allocation that happens at each thread launch would already solve the problem AMD faces with all-in-one raytracing shaders.</p><p>Intel’s documentation explicitly states that raytracing stages execute as separate thread launches. But even if they didn’t, Intel would benefit less from dynamic register allocation than AMD. Intel GPUs used fixed register allocation until very recently. Each thread gets 128 registers whether it needs them or not. More recent GPUs like Battlemage add a “large GRF” mode that cuts occupancy in half to give each thread 256 registers. There’s no option in between.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp" width="1456" height="969" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:969,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:153496,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faec9a9ca-4273-4bbb-8ddb-00d2dc91d8e4_2163x1440.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Intel’s Arc B580</figcaption></figure></div><p>Therefore Intel can maintain full occupancy with a higher per-thread register count than either AMD or Nvidia. Dynamic VGPR allocation is only useful if it helps increase occupancy in the first place – that is, the GPU can’t achieve full occupancy with non-dynamic VGPR allocation. If Intel were to dynamically allocate registers, the very coarse register allocation granularity may result in a more threads getting blocked than on AMD.</p><p>AMD’s dynamic VGPR allocation mode is an exciting new feature. It addresses a drawback with AMD’s inline raytracing technique, letting AMD keep more threads in flight without increasing register file capacity. That in turn makes RDNA 4 less latency sensitive in raytracing workloads, likely with minimal power and area cost. Raytracing shaders that use more than 96 VGPRs are attractive targets for the dynamic VGPR feature.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp" width="1456" height="789" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:789,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:95692,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ba099a-d6c3-4b1f-98b8-7a5dd1e43cb5_1920x1040.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Profiling Quake 2 RTX under Radeon Graphics Profiler. AMD chose to use fully inlined raytracing shaders, and no dynamic VGPR allocation, despite the shader being limited to 9 threads (out of 16 slots) due to VGPR usage.</figcaption></figure></div><p>Raytracing shaders on AMD can either inline all raytracing stages, or use an “indirect” mode where different stages are executed in separate function calls. So far, I’ve only seen AMD use dynamic VGPR allocation in indirect mode. Raytracing stages all take place within the same thread in both modes, but perhaps function call sites provide a convenient place to adjust VGPR allocation. After all, a function has clearly defined entry and exit points. AMD often prefers to inline raytracing stages to avoid function call overhead. I have not seen dynamic VGPR mode used when raytracing stages are inlined, even when raytracing shader occupancy is VGPR limited.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp" width="1456" height="358" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:358,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:40152,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e20d08-8e9b-417c-8622-f0ab5860bb52_2000x492.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The RX 9070 provided by AMD</figcaption></figure></div><p><span>Certainly </span><code>s_alloc_vgpr</code><span> isn’t limited to function call sites, so I wonder if future AMD drivers will be more trigger-happy with dynamic VGPR mode. Conversely, AMD uses dynamic VGPR allocation in indirect mode even when non-dynamic allocation could have achieved full occupancy. Doing so shouldn’t hurt performance, but it does suggest driver decisions aren’t so fine grained at the moment.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp" width="933" height="873" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:873,&quot;width&quot;:933,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:27596,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/160652329?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd581fedf-9944-4276-b294-03fdde3e6d2f_933x873.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Setting “Disable raytracing shader inlining” using AMD’s tools makes the driver use raytracing shaders with function calls, which also use dynamic register allocation. Done here to illustrate effect on occupancy</figcaption></figure></div><p><span>Generic compute workloads could benefit from dynamic VGPR mode too, assuming AMD does work to expose the feature through various toolchains. </span><a href="https://github.com/NVIDIA/cutlass/issues/2007" rel="">Some </a><span>of Nvidia’s GPGPU libraries take advantage of </span><code>setmaxnreg</code><span>, so there’s probably compute applications for AMD’s dynamic VGPR feature too.</span></p><p><span>At a higher level, features like dynamic VGPR allocation paint a picture where AMD’s GPU efforts are progressing at a brisk pace. It doesn’t feel like an easy feature to implement. Thread register allocation could be non-contiguous in the physical register file, complicating register addressing under the hood. Features like deadlock avoidance would demand additional work. With regards to raytracing, dynamic VGPR allocation shows there’s plenty of progress to be made within AMD’s single-shader raytracing model. Along with </span><a href="https://chipsandcheese.com/p/rdna-4s-out-of-order-memory-accesses" rel="">breaking false cross-wave memory dependencies</a><span>, AMD seems determined to keep stamping out performance limiters with each generation.</span></p><p><span>If you like the content then consider heading over to the </span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span> or </span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span> if you want to toss a few bucks to Chips and Cheese. Also consider joining the </span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p><ol><li><p><a href="https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/rdna4-instruction-set-architecture.pdf" rel="">“RDNA 4” Instruction Set Architecture Reference Guide</a></p></li><li><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#miscellaneous-instructions-setmaxnreg" rel="">Nvidia setmaxreg</a></p></li></ol><p>a. RDNA 4’s ISA manual indicates the 24 register allocation granularity only applies to devices with 1536 VGPRs per SIMD, or 192 KB register files. Other RDNA 4 devices allocate VGPRs in blocks of 16 registers, and likely have a 128 KB register file. RDNA 3 used smaller 128 KB register files in lower end devices, reserving 192 KB register files for the highest end SKUs. As RDNA 4 SKUs with non-192 KB register files do not exist at the time of writing, there is no need to discuss them in the article proper. However, such devices may launch in the future and it’s something to be aware of.</p></div></div>]]></description>
        </item>
    </channel>
</rss>