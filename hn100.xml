<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 20 May 2025 11:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Finland announces migration of its rail network to international gauge (222 pts)]]></title>
            <link>https://www.trenvista.net/en/news/rnhs/finland-migration-standard-gauge/</link>
            <guid>44038835</guid>
            <pubDate>Tue, 20 May 2025 07:40:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.trenvista.net/en/news/rnhs/finland-migration-standard-gauge/">https://www.trenvista.net/en/news/rnhs/finland-migration-standard-gauge/</a>, See on <a href="https://news.ycombinator.com/item?id=44038835">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<main id="main">
			
<article id="post-90722" itemtype="https://schema.org/CreativeWork" itemscope="">
	<div itemprop="text">
			
<p>The Finnish government has announced the conversion of its rail network from Russian gauge (1,524 mm) to European standard (1,435 mm). This historic decision reinforces its integration with the European Union and NATO.</p><p>The change, presented by Transport Minister Lulu Ranne at a meeting of Nordic ministers in Helsinki, is in response to the need to improve military mobility and regional security, especially in the wake of Finland’s NATO membership and growing tensions with Russia.</p><p>The project, which will start in the north of the country near Oulu, aims to remove technical obstacles to transporting troops and goods between Finland, Sweden and Norway.</p><p>The government is expected to make the final decision by July 2027, with construction starting around 2032. European funding will be key: the EU could cover up to 50% of planning costs and 30% of construction<a href="https://english.news.cn/europe/20250514/23904bc05bd6438b8c54979b8f3b1520/c.html" target="_blank" rel="noreferrer noopener"></a><a href="http://www.china.org.cn/world/Off_the_Wire/2025-05/14/content_117873438.shtml" target="_blank" rel="noreferrer noopener"></a>.</p><p>The transition, which will cost billions of euros, affect more than 9,200 km of track, and take decades, symbolises a geopolitical and strategic shift for Finland, which, with this decision, fully aligns itself with European infrastructure.</p><div data-id="46103"><p><a href="https://t.me/trenvista" target="_blank" rel="noopener"><img alt="Trenvista en Telegram" src="https://www.trenvista.net/wp-content/uploads/2022/03/Trenvista-Telegram.png" width="500" height="250" data-old-src="data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-src="https://www.trenvista.net/wp-content/uploads/2022/03/Trenvista-Telegram.png"></a></p>
<p><a href="https://t.me/railnewsvista" target="_blank" rel="noopener"><img alt="Trenvista en Telegram" src="https://www.trenvista.net/wp-content/uploads/2025/01/Anuncio-Telegram-eng.png" width="500" height="250" data-old-src="data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-src="https://www.trenvista.net/wp-content/uploads/2025/01/Anuncio-Telegram-eng.png"></a></p></div>
		</div>
</article>

			

					</main>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making Video Games (Without an Engine) in 2025 (212 pts)]]></title>
            <link>https://noelberry.ca/posts/making_games_in_2025/</link>
            <guid>44038209</guid>
            <pubDate>Tue, 20 May 2025 05:54:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://noelberry.ca/posts/making_games_in_2025/">https://noelberry.ca/posts/making_games_in_2025/</a>, See on <a href="https://news.ycombinator.com/item?id=44038209">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>It's 2025 and I am still making video games, which <a href="https://web.archive.org/web/20110902045531/http://noelberry.ca/">according to archive.org</a> is 20 years since I started making games! That's a pretty long time to be doing one thing...</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/2005.jpeg" alt="Screenshot of my website circa 2011">
<em></em></p><center><em>Screenshot of my website circa 2011</em></center>
<p>When I share stuff I'm working on, people frequently ask how I make games and are often surprised (and sometimes concerned?) when I tell them I don't use commercial game engines. There's an assumption around making games without a big tool like Unity or Unreal that you're out there hand writing your own assembly instruction by instruction.</p>
<p>I genuinely believe making games without a big "do everything" engine can be easier, more fun, and often less overhead. I am not making a "do everything" game and I do not need 90% of the features these engines provide. I am very particular about how my games feel and look, and how I interact with my tools. I often find the default feature implementations in large engines like Unity so lacking I end up writing my own anyway. Eventually, my projects end up being mostly my own tools and systems, and the engine becomes just a vehicle for a nice UI and some rendering...</p>
<p>At which point, why am I using this engine? What is it providing me? Why am I letting a tool potentially destroy my ability to work when they suddenly make <a href="https://www.theverge.com/2023/9/12/23870547/unit-price-change-game-development">unethical and terrible business decisions</a>? Or push out an update that they require to run my game on consoles, that also happens to break an entire system in my game, forcing me to rewrite it? Why am I fighting this thing daily for what essentially becomes a glorified asset loader and editor UI framework, by the time I'm done working around their default systems?</p>
<p>The obvious answer for me is to just not use big game engines, and write my own small tools for my specific use cases. It's more fun, and I like controlling my development stack. I know when something goes wrong I can find the problem and address it, instead of submitting a bug report and 3 months later hearing back it "won't be fixed". I like knowing that in another two decades from now I will still be able to compile my game without needing to pirate an ancient version of a broken game engine.</p>
<p>Obviously this is my personal preference - and it's one of someone who has been making indie games for a long time. I used engines like Game Maker for years before transitioning to more lightweight and custom workflows. I also work in very small teams, where it's easy to make one-off tools for team members. But I want to push back that making games "from scratch" is some big impossible task - especially in 2025 with the state of open source frameworks and libraries. A <a href="https://github.com/TerryCavanagh/VVVVVV">lot of</a> <a href="https://gamefromscratch.com/balatro-made-with-love-love2d-that-is/">popular</a> <a href="https://store.steampowered.com/app/813230/ANIMAL_WELL/">indie</a> <a href="https://www.stardewvalley.net/">games</a> <a href="https://store.steampowered.com/app/504230/Celeste/">are made</a> <a href="https://github.com/flibitijibibo/RogueLegacy1">in small</a> frameworks like FNA, Love2D, or SDL. Making games "without an engine" doesn't literally mean opening a plain text editor and writing system calls (unless you want to). Often, the overhead of learning how to implement these systems yourself is just as time consuming as learning the proprietary workflows of the engine itself.</p>
<p>With that all said, I think it'd be fun to talk about my workflow, and what I actually use to make games.</p>
<h2>Programming Languages</h2>
<p>Most of my career I've worked in C#, and aside from a <a href="https://github.com/noelfb/blah">short stint in C++</a> a few years ago, I've settled back into a modern C# workflow.</p>
<p>I think sometimes when I mention C# to non-indie game devs their minds jump to what it looked like circa 2003 - a closed source, interpreted, verbose, garbage collected language, and... the language has <em>greatly</em> improved since then. The C# of 2025 is vastly different from the C# of even 2015, and many of those changes are geared towards the performance and syntax of the langauge. You can allocate dynamically sized arrays on the stack! <code>C++</code> can't do that (<em>although <code>C99</code> can ;) ...</em>).</p>
<p>The dotnet developers have also implemented hot reload in C# (which works... <em>most of the time</em>), and it's pretty fantastic for game development. You can launch your project with <code>dotnet watch</code> and it will live-update code changes, which is amazing when you want to change how something draws or the way an enemy updates.</p>
<p>C# also ends up being a great middle-ground between running things fast (which you need for video games) and easy to work with on a day-to-day basis. For example, I have been working on <a href="https://cityofnone.com/">City of None</a> with my brother <a href="https://liamberry.ca/">Liam</a>, who had done very little coding when we started the project. But over the last year he's slowly picked up the language to the point where he's programming entire boss fights by himself, because C# is just that accessible - and fairly foot-gun free. For small teams where everyone wears many hats, it's a really nice language.</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/bossfight2.gif" alt="A boss fight that Liam coded" width="90%"></p><center><i>A boss fight that Liam coded</i></center>
<p>And finally, it has built in reflection... And while I wouldn't use it for release code, being able to quickly reflect on game objects for editor tooling is very nice. I can easily make live-inspection tools that show me the state of game objects without needing any custom meta programming or in-game reflection data. After spending a few years making games in C++ I really like having this back.</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/reflection.jpeg" alt="Inspecting an object with reflection in Dear ImGui">
<em></em></p><center><em>Inspecting an object with reflection in Dear ImGui</em></center>
<h2>Windows... Input... Rendering... Audio?</h2>
<p>This is kind of the big question when writing "a game from scratch", but there are a lot of great libraries to help you get stuff onto the screen - from <a href="https://www.libsdl.org/">SDL</a>, to <a href="https://www.glfw.org/">GLFW</a>, to <a href="https://www.love2d.org/">Love2D</a>, to <a href="https://www.raylib.com/">Raylib</a>, etc.</p>
<p>I have been using <a href="https://wiki.libsdl.org/SDL3/FrontPage">SDL3</a> as it does everything I need as a cross-platform abstraction over the system - from windowing, to game controllers, to rendering. It works on Linux, Windows, Mac, Switch, PS4/5, Xbox, etc, and as of SDL3 there is a <a href="https://wiki.libsdl.org/SDL3/CategoryGPU">GPU abstraction</a> that handles rendering across DirectX, Vulkan, and Metal. It just <em>works</em>, is open source, and is used by a lot of the industry (ex. Valve). I started using it because <a href="https://fna-xna.github.io/">FNA</a>, which Celeste uses to run on non-Windows platforms, uses it as its platform abstraction.</p>
<p>That said, I have written <a href="https://github.com/FosterFramework/Foster">my own C# layer</a> on top of SDL for general rendering and input utilities I share across projects. I make highly opinionated choices about how I structure my games so I like having this little layer to interface with. It works really well for my needs, but there are full-featured alternatives like <a href="https://github.com/MoonsideGames/MoonWorks">MoonWorks</a> that fill a similar space.</p>
<p>Before SDL3's release with the GPU abstraction, I was writing my own OpenGL and DirectX implementations - which isn't trivial! But it was a <a href="https://learnopengl.com/">great learning experience</a>, and not as bad as I expected it to be. I am however, very greatful for SDL GPU as it is a very solid foundation that will be tested across millions of devices.</p>
<p>Finally, for Audio we're using <a href="https://www.fmod.com/">FMOD</a>. This is the last proprietary tool in our workflow, which I don't love (especially <a href="https://www.reddit.com/r/linux_gaming/comments/1ijcfnt/celeste_not_finding_libfmodstudioso10/">when something stops working</a> and you have to hand-patch their library), but it's the best tool for the job. There are more lightweight open source libraries if you just want to play sounds, but I work with audio teams that want finite control over dynamic audio, and a tool like FMOD is a requirement.</p>
<h2>Assets</h2>
<p>I don't have much to say about assets, because when you're rolling your own engine you just load up what files you want, when you need them, and move on. For all my pixel art games, I load the whole game up front and it's "fine" because the entire game is like 20mb. When I was working on <a href="https://exok.com/games/earthblade/">Earthblade</a>, which had larger assets, we would register them at startup and then only load them on request, disposing them after scene transitions. We just went with the most dead-simple implementation that accomplished the job.</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/assets.jpeg" alt="Assets loading in 0.4 seconds">
<em></em></p><center><em>All the assets for City of None loading in 0.4 seconds</em></center>
<p>Sometimes you'll have assets that need to be converted before the game uses them, in which case I usually write a small script that runs when the game compiles that does any processing required. That's it.</p>
<h2>Level Editors, UI...</h2>
<p>Some day I'll write a fully procedural game, but until then I need tools to design the in-game spaces. There are a lot of really great existing tools out there, like <a href="https://ldtk.io/">LDtk</a>, <a href="https://www.mapeditor.org/">Tiled</a>, <a href="https://trenchbroom.github.io/">Trenchbroom</a>, and so on. I have used many of these to varying degrees and they're easy to set up and get running in your project - you just need to write a script to take the data they output and instantiate your game objects at runtime.</p>
<p>However, I usually like to write my own custom level editors for my projects. I like to have my game data tie directly into the editor, and I never go that deep on features because the things we need are specific but limited.</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/postcard.png" alt="A small custom level editor for City of None using Dear ImGui">
<em></em></p><center><em>A small custom level editor for City of None using Dear ImGui</em></center>
<p>But I don't want to write the actual UI - coding textboxes and dropdowns isn't something I'm super keen on. I want a simple way to create fields and buttons, kind of like when <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/editor-CustomEditors.html">you write your own small editor utilities</a> in the Unity game engine.</p>
<p>This is where <a href="https://github.com/ocornut/imgui/">Dear ImGui</a> comes in. It's a lightweight, cross-platform, immediate-mode GUI engine that you can easily drop in to any project. The editor screenshot above uses it for everything with the exception of the actual "scene" view, which is custom as it's just drawing my level. There are more full-featured (and heavy-duty) alternatives, but if it's good enough for <a href="https://github.com/ocornut/imgui/wiki/Software-using-dear-imgui">all these games</a> including <a href="https://github.com/ocornut/imgui/issues/7503#issuecomment-2308380962">Tears of the Kingdom</a> it's good enough for me.</p>
<p>Using ImGui makes writing editor tools extremely simple. I like having my tools pull data directly from my game, and using ImGui along with C# reflection makes that very convenient. I can loop over all the Actor classes in C# and have them accessible in my editor with a few lines of code! For more complicated tools it's sometimes overkill to write my own implementation, which is where I fall back to using existing tools built for specific jobs (like <a href="https://trenchbroom.github.io/">Trenchbroom</a>, for designing 3D environments).</p>
<h2>Porting Games ... ?</h2>
<p>The main reason I learned C++ a few years ago was because of my concerns with portability. At the time, it was not trivial to run C# code on consoles because C# was "just in time" compiled, which isn't something many platforms allow. Our game, Celeste, used a tool called <a href="http://brute.rocks/">BRUTE</a> to transpile the C# <a href="https://en.wikipedia.org/wiki/Common_Intermediate_Language">IL</a> (intermediate language binaries) to C++, and then recompiled that for the target platform. Unity <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/scripting-backends-il2cpp.html">has a very similar tool</a> that does the same thing. This worked, but was not ideal for me. I wanted to be able to just compile our code for the target platform, and so learning C++ felt like the only real option.</p>
<p>Since then, however, C# has made incredible progress with their <a href="https://learn.microsoft.com/en-us/dotnet/core/deploying/native-aot/">Native-AOT</a> toolchain (which basically just means all the code is compiled "ahead of time" - what languages like C++ and Rust do by default). It is now possible to compile C# code for all the major console architectures, which is amazing. The <a href="https://fna-xna.github.io/docs/appendix/Appendix-B%3A-FNA-on-Consoles/">FNA project</a> has been extremely proactive with this, leading to the release of games across all major platforms, while using C#.</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/platforms.jpeg" alt="Platforms supported by SDL3">
<em></em></p><center><em>Platforms supported by SDL3</em></center>
<p>And finally, SDL3 has console ports for all the major platforms. Using it as your platform abstraction layer (as long as you're careful about how you handle system calls) means a lot of it will "just work".</p>
<h2>Goodbye, Windows</h2>
<p>Finally, to wrap all this up ... I no longer use Windows to develop my games (aside from testing). I feel like this is in line with my general philosophy around using open source, cross-platform tools and libraries. I have found Windows <a href="https://www.howtogeek.com/739837/fyi-windows-11-home-will-require-a-microsoft-account-for-initial-setup/">increasingly frustrating to work</a> with, their <a href="https://bdsmovement.net/microsoft">business practices gross</a>, and their OS generally lacking. I grew up using Windows, but I switched to Linux full time around 3 years ago. And frankly, for programming video games, I have not missed it at all. It just doesn't offer me anything I can't do faster and more elegantly than on Linux.</p>
<p>There are of course certain workflows and tools that do not work on Linux, and that is just the current reality. I'm not entirely free of Microsoft either - I use vscode, I write my games in C#, and I host my projects on github... But the more people use Linux daily, the more pressure there is to support it, and the more support there is for open source alternatives.</p>
<p>(as a fun aside, I play most of my games on my steam deck these days, which means between my PC, game console, web server, and phone, I am always on a Linux platform)</p>
<h2>Miscellaneous thoughts</h2>
<ul>
<li><p><strong>What about Godot?</strong><br>If you're in the position to want the things a larger game engine provides, I definitely think <a href="https://godotengine.org/">Godot</a> is the best option. That it is open-source and community-maintained eliminates a lot of the issues I have with other proprietary game engines, but it still isn't usually the way I want to make games. I do intend to play around with it in the future for some specific ideas I have.</p>
</li>
<li><p><strong>What about 3D?</strong><br>I think that using big engines definitely have more of a place for 3D games - but even so for any kind of 3D project I want to do, I would roll my own little framework. I want to make highly stylized games that do not require very modern tech, and I have found that to be fairly straight forward (for example, we made <a href="https://github.com/exok/celeste64">Celeste 64</a> without very much prior 3D knowledge in under 2 weeks).</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/celeste64.jpeg" alt="Celeste 64 screenshot">
<em></em></p><center><em>Celeste 64 Screenshot</em></center>
</li>
<li><p><strong>I need only the best fancy tech to pull off my game idea</strong><br>Then use Unreal! There's nothing wrong with that, but my projects don't require those kinds of features (and I would argue most of the things I do need can usually be learned fairly quickly).</p>
</li>
<li><p><strong>My whole team knows [Game Engine XYZ]</strong><br>The cost of migrating a whole team to a custom thing can be expensive and time consuming. I'm definitely talking about this from the perspective of smaller / solo teams. But that said, speaking from experience, I know several middle-sized studios moving to custom engines because they have determined the potential risk of using proprietary engines to be too high, and the migration and learning costs to be worth it. I think using custom stuff for larger teams is easier now than it has been in a long time.</p>
</li>
<li><p><strong>Game-specific workflows</strong></p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/aseprite.jpeg" alt="Screeshot of Aseprite">
<em></em></p><center><em>Aseprite assets are loaded in automatically</em></center>
<p>I load in <a href="https://www.aseprite.org/">Aseprite</a> files and have my City of None engine automatically turn them into game animations, using their built in tags and frame timings. The format is <a href="https://github.com/aseprite/aseprite/blob/main/docs/ase-file-specs.md">surprisingly straight forward</a>. When you write your own tools it's really easy to add things like this!</p>
</li>
</ul>
<h2>Alright!</h2>
<p>That's it from me! That's how I make games in 2025!</p>
<p>Do I think you should make games without a big engine? My answer is: If it sounds fun.</p>
<p>-Noel</p>

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I got fooled by AI-for-science hype–here's what it taught me (185 pts)]]></title>
            <link>https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres</link>
            <guid>44037941</guid>
            <pubDate>Tue, 20 May 2025 04:57:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres">https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres</a>, See on <a href="https://news.ycombinator.com/item?id=44037941">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em><span>I’m excited to publish this guest post by </span><a href="https://x.com/NMcGreivy/" rel="">Nick McGreivy</a><span>, a physicist who last year earned a PhD from Princeton. Nick used to be optimistic that AI could accelerate physics research. But when he tried to apply AI techniques to real physics problems the results were disappointing.</span></em></p><p><em><span>I’ve </span><a href="https://www.understandingai.org/p/six-principles-for-thinking-about" rel="">written before</a><span> about the Princeton School of AI Safety, which holds that the impact of AI is likely to be similar to that of past general-purpose technologies such as electricity, integrated circuits, and the Internet. I think of this piece from Nick as being in that same intellectual tradition.</span></em></p><p><em>—Timothy B. Lee</em></p><p><span>In 2018, as a second-year PhD student at Princeton studying </span><a href="https://en.wikipedia.org/wiki/Plasma_(physics)" rel="">plasma physics</a><span>, I decided to switch my research focus to machine learning. I didn’t yet have a specific research project in mind, but I thought I could make a bigger impact by using AI to accelerate physics research. (I was also, quite frankly, motivated by the </span><a href="https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html?action=click&amp;module=RelatedCoverage&amp;pgtype=Article&amp;region=Footer" rel="">high</a><span> </span><a href="https://www.nytimes.com/2018/04/19/technology/artificial-intelligence-salaries-openai.html" rel="">salaries</a><span> in AI.)</span></p><p><span>I eventually chose to study what AI pioneer Yann LeCun later </span><a href="https://x.com/ylecun/status/1581648953275473921" rel="">described</a><span> as a “pretty hot topic, indeed”: using AI to solve partial differential equations (PDEs). But as I tried to build on what I thought were impressive results, I found that AI methods performed much worse than advertised.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png" width="1456" height="963" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:963,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2532708,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.understandingai.org/i/163736413?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>The author, Nick McGreivy.</figcaption></figure></div><p><span>At first, I tried applying a widely-cited AI method called PINN to some fairly simple PDEs, but found it to be unexpectedly brittle. Later, though dozens of papers had claimed that AI methods could solve PDEs faster than standard numerical methods—in some cases as much as a </span><a href="https://iopscience.iop.org/article/10.1088/1741-4326/ad313a" rel="">million times faster</a><span>—I discovered that a large majority of these comparisons were unfair. When </span><a href="https://www.nature.com/articles/s42256-024-00897-5" rel="">I compared</a><span> these AI methods on equal footing to state-of-the-art numerical methods, whatever narrowly defined advantage AI had usually disappeared.</span></p><p><span>This experience has led me to question the idea that AI is poised to “</span><a href="https://www.youtube.com/watch?v=yxAJohm0l_g&amp;ab_channel=TheRoyalSwedishAcademyofSciences" rel="">accelerate</a><span>” or even “</span><a href="https://www.youtube.com/watch?v=PKN95I93iGE&amp;ab_channel=TheEconomist" rel="">revolutionize</a><span>” science. Are we really about to enter what DeepMind </span><a href="https://deepmind.google/public-policy/ai-for-science/" rel="">calls</a><span> “a new golden age of AI-enabled scientific discovery,” or has the overall potential of AI in science been exaggerated—much like it was in my subfield?</span></p><p><span>Many others have identified similar issues. For example, in 2023 DeepMind </span><a href="https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/" rel="">claimed</a><span> to have discovered 2.2 million crystal structures, </span><a href="https://www.nature.com/articles/s41586-023-06735-9" rel="">representing</a><span> “an order-of-magnitude expansion in stable materials known to humanity.” But when </span><a href="https://x.com/Robert_Palgrave/status/1744383962913394758" rel="">materials</a><span> </span><a href="https://journals.aps.org/prxenergy/abstract/10.1103/PRXEnergy.3.011002" rel="">scientists</a><span> </span><a href="https://pubs.acs.org/doi/10.1021/acs.chemmater.4c00643" rel="">analyzed these compounds</a><span>, they found it was “</span><a href="https://www.aisnakeoil.com/p/scientists-should-use-ai-as-a-tool" rel="">mostly junk</a><span>” and “respectfully” suggested that the paper “does not report any new materials.”</span></p><p><span>Separately, Princeton computer scientists </span><a href="https://www.cs.princeton.edu/~arvindn/" rel="">Arvind Narayanan</a><span> and </span><a href="https://www.cs.princeton.edu/~sayashk/" rel="">Sayash Kapoor</a><span> have </span><a href="https://reproducible.cs.princeton.edu/" rel="">compiled a list</a><span> of 648 papers across 30 fields that all make a methodological error called </span><a href="http://en.wikipedia.org/wiki/Leakage_(machine_learning)" rel="">data leakage</a><span>. In each case data leakage leads to overoptimistic results. They argue that AI-based science is facing a “reproducibility crisis.”</span></p><p><span>Yet AI adoption in scientific research has been </span><a href="https://doi.org/10.1038/s41562-024-02020-5" rel="">rising sharply</a><span> </span><a href="https://arxiv.org/abs/2405.15828" rel="">over the last decade</a><span>. Computer science has seen the biggest impacts, of course, but other disciplines—physics, chemistry, biology, medicine, and the social sciences—have also seen rapidly increasing AI adoption. Across all scientific publications, </span><a href="https://www.csiro.au/en/research/technology-space/ai/artificial-intelligence-for-science-report" rel="">rates of AI usage grew</a><span> from 2 percent in 2015 to </span><a href="https://www.nature.com/articles/d41586-023-02980-0" rel="">almost 8 percent in 2022</a><span>. It’s harder to find data about the last few years, but there’s every reason to think that </span><a href="https://trends.google.com/trends/explore?date=all&amp;q=ai%20for%20science&amp;hl=en" rel="">hockey stick growth has continued</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>To be clear, AI </span><em>can</em><span> drive scientific breakthroughs. My concern is about their magnitude and frequency. Has AI really shown enough potential to justify such a massive shift in talent, training, time, and money away from existing research directions and towards a single paradigm?</span></p><p><span>Every field of science is experiencing AI differently, so we should be cautious about making generalizations. I’m convinced, however, that </span><em>some</em><span> of the lessons from my experience are broadly applicable across science:</span></p><ul><li><p><span>AI adoption is exploding among scientists less because it benefits science and more </span><a href="https://arxiv.org/abs/2412.07727" rel="">because it benefits the scientists themselves</a><span>.</span></p></li><li><p><span>Because AI researchers almost never publish negative results, AI-for-science is experiencing </span><a href="https://en.wikipedia.org/wiki/Survivorship_bias" rel="">survivorship bias</a><span>.</span></p></li><li><p>The positive results that get published tend to be overly optimistic about AI’s potential.</p></li></ul><p>As a result, I’ve come to believe that AI has generally been less successful and revolutionary in science than it appears to be.</p><p><span>Ultimately, I don’t know whether AI will reverse the decades-long trend of </span><a href="https://mattsclancy.substack.com/p/science-is-getting-harder" rel="">declining scientific productivity</a><span> and stagnating (or even decelerating) rates of </span><a href="https://substack.com/@aisnakeoil/note/c-92421948" rel="">scientific progress</a><span>. I don’t think anyone does. But barring major (and in my opinion unlikely) breakthroughs in advanced AI, I expect AI to be much more a </span><a href="https://knightcolumbia.org/content/ai-as-normal-technology" rel="">normal</a><span> tool of incremental, uneven scientific progress than a revolutionary one.</span></p><p><span>In the summer of 2019, I got a first taste of what would become my dissertation topic: solving PDEs with AI. </span><a href="https://en.wikipedia.org/wiki/Partial_differential_equation" rel="">PDEs</a><span> are mathematical equations used to model a wide range of physical systems, and solving (i.e., simulating) them is an extremely important task in computational physics and engineering. My lab uses PDEs to </span><a href="https://www.pppl.gov/research/computational-sciences" rel="">model</a><span> the behavior of plasmas, such as inside fusion reactors and in the interstellar medium of outer space.</span></p><p><span>The AI models being used to solve PDEs are custom deep learning models, much more analogous to </span><a href="https://deepmind.google/technologies/alphafold/" rel="">AlphaFold</a><span> than ChatGPT.</span></p><p><span>The first approach I tried was something called the physics-informed neural network. PINNs had recently been introduced in an </span><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=yZ0-ywkAAAAJ&amp;citation_for_view=yZ0-ywkAAAAJ:eLRq4zTgah0C" rel="">influential paper</a><span> that had already racked up hundreds of citations.</span></p><p>PINNs were a radically different way of solving PDEs compared to standard numerical methods. Standard methods represent a PDE solution as a set of pixels (like in an image or video) and derive equations for each pixel value. In contrast, PINNs represent the PDE solution as a neural network and put the equations into the loss function.</p><p>As a naive grad student who didn’t even have an advisor yet, there was something incredibly appealing to me about PINNs. They just seemed so simple, elegant, and general.</p><p><span>They also seemed to have good results. The </span><a href="https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125" rel="">paper</a><span> introducing PINNs found that their “effectiveness” had been “demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction-diffusion systems, and the propagation of nonlinear shallow-water waves.” If PINNs had solved all these PDEs, I figured, then surely they could solve some of the plasma physics PDEs that </span><a href="https://en.wikipedia.org/wiki/Princeton_Plasma_Physics_Laboratory" rel="">my lab</a><span> </span><a href="https://ammar-hakim.org/sj/je/je17/je17-hasegawa-wakatani.html" rel="">cared</a><span> </span><a href="https://en.wikipedia.org/wiki/Gyrokinetics" rel="">about</a><span>.</span></p><p><span>But when I replaced one of the </span><a href="https://github.com/maziarraissi/PINNs" rel="">examples from</a><span> that influential first paper (</span><a href="https://en.wikipedia.org/wiki/Burgers%27_equation" rel="">1D Burgers’</a><span>) with a different, but still extremely simple, PDE (</span><a href="https://ammar-hakim.org/sj/je/je14/je14-vlasov-fixed-pot.html" rel="">1D Vlasov</a><span>), the results didn’t look anything like the exact solution. Eventually, after extensive tuning, I was able to get something that looked correct. However, when I tried slightly more complex PDEs (such as </span><a href="https://en.wikipedia.org/wiki/Vlasov_equation#The_Vlasov%E2%80%93Poisson_equation" rel="">1D Vlasov-Poisson</a><span>), no amount of tuning could give me a decent solution.</span></p><p>After a few weeks of failure, I messaged a friend at a different university, who told me that he too had tried using PINNs, but hadn’t been able to get good results.</p><p>Eventually, I realized what had gone wrong. The authors of the original PINN paper had, like me, “observed that specific settings that yielded impressive results for one equation could fail for another.” But because they wanted to convince readers of how exciting PINNs were, they hadn’t shown any examples of PINNs failing.</p><p>This experience taught me a few things. First, to be cautious about taking AI research at face value. Most scientists aren’t trying to mislead anyone, but because they face strong incentives to present favorable results, there’s still a risk that you’ll be misled. Moving forward, I would have to be more skeptical, even (or perhaps especially) of high-impact papers with impressive results.</p><p><span>Second, people rarely publish papers about when AI methods fail, only when they succeed. The authors of the original PINN paper didn’t publish about the PDEs their method hadn’t been able to solve. I didn’t publish my unsuccessful experiments, presenting only a </span><a href="https://github.com/nickmcgreivy/PINN/blob/master/APS-Poster-McGreivy-2019.pdf" rel="">poster</a><span> at an obscure conference. So very few researchers heard about them. In fact, despite the huge popularity of PINNs, it took four years for anyone to publish </span><a href="https://proceedings.neurips.cc/paper/2021/hash/df438e5206f31600e6ae4af72f2725f1-Abstract.html" rel="">a paper about</a><span> their failure modes. That paper now has almost a thousand citations, suggesting that many other scientists tried PINNs and found similar issues.</span></p><p><span>Third, I concluded that PINNs weren’t the approach I wanted to use. They were simple and elegant, sure, but they were also far </span><a href="https://arxiv.org/abs/2205.14249" rel="">too unreliable</a><span>, </span><a href="https://arxiv.org/abs/2306.00230" rel="">too finicky</a><span>, and </span><a href="https://academic.oup.com/imamat/article/89/1/143/7680268" rel="">too slow</a><span>.</span></p><p><span>As of today, six years later, the original PINN paper has a whopping </span><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=yZ0-ywkAAAAJ&amp;citation_for_view=yZ0-ywkAAAAJ:eLRq4zTgah0C" rel="">14,000 citations</a><span>, making it the most cited numerical methods paper of the 21st century (and, by my count, a year or two away from becoming the second most-cited numerical methods paper of all time).</span></p><p><span>Though it’s now widely accepted that PINNs generally aren’t competitive with standard numerical methods for </span><em>solving</em><span> PDEs, there remains debate over how well PINNs perform for a different class of problems known as </span><em>inverse problems</em><span>. Advocates claim that PINNs are “</span><a href="https://www.nature.com/articles/s42254-021-00314-5" rel="">particularly effective</a><span>” for inverse problems, but some researchers have </span><a href="https://academic.oup.com/pnasnexus/article/3/1/pgae005/7516080" rel="">vigorously contested</a><span> that idea.</span></p><p><span>I don’t know which side of the debate is right. I’d like to think that </span><a href="https://x.com/shoyer/status/1532278186901327872" rel="">something useful has come</a><span> from all this PINN research, but I also wouldn’t be surprised if one day we look back on PINNs as simply a massive citation bubble.</span></p><p>For my dissertation, I focused on solving PDEs using deep learning models that, like traditional solvers, treated the PDE solution as a set of pixels on a grid or a graph.</p><p><span>Unlike PINNs, this approach had shown a lot of promise on the complex, time-dependent PDEs that my lab cared about. Most impressively, </span><a href="https://arxiv.org/abs/2010.08895" rel="">paper</a><span> </span><a href="https://www.nature.com/articles/s42256-021-00302-5" rel="">after</a><span> </span><a href="https://openreview.net/forum?id=roNqYL0_XP" rel="">paper</a><span> </span><a href="https://www.pnas.org/doi/abs/10.1073/pnas.2101784118" rel="">had</a><span> </span><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13619" rel="">demonstrated</a><span> the ability to solve PDEs faster—often orders of magnitude faster—than standard numerical methods.</span></p><p><span>The examples that excited </span><a href="https://www.pppl.gov/people/ammar-hakim" rel="">my advisor</a><span> and me the most were PDEs from fluid mechanics, such as the </span><a href="https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419/" rel="">Navier-Stokes equations</a><span>. We thought we might see similar speedups because the PDEs we cared about—equations describing </span><a href="https://ammar-hakim.org/sj/je/je17/je17-hasegawa-wakatani.html" rel="">plasmas in</a><span> </span><a href="https://hal.science/hal-03974985/file/Gyrokinetics_fundamentals_XG_ML_23.pdf" rel="">fusion reactors</a><span>, for example—have a </span><a href="https://arxiv.org/abs/1908.01814" rel="">similar mathematical structure</a><span>. In theory, this could allow scientists and engineers like us to simulate larger systems, more rapidly optimize existing designs, and ultimately accelerate the pace of research.</span></p><p>By this point, I was seasoned enough to know that in AI research, things aren’t always as rosy as they seem. I knew that reliability and robustness might be serious issues. If AI models give faster simulations, but those simulations are less reliable, would that be worth the trade-off? I didn’t know the answer and set out to find out.</p><p><span>But as I tried—and </span><a href="https://arxiv.org/abs/2303.16110" rel="">mostly failed</a><span>—to make these models more reliable, I began to question how much promise AI models had really shown for accelerating PDEs.</span></p><p><span>According to a number of </span><a href="https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419/" rel="">high-profile papers</a><span>, AI had solved the Navier-Stokes equations orders of magnitude faster than standard numerical methods. I eventually discovered, however, that the baseline methods used in these papers were not the fastest numerical methods available. When I compared AI to more advanced numerical methods, I found that AI was no faster (or at most, only slightly faster) than the stronger baselines.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png" width="1422" height="574" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:574,&quot;width&quot;:1422,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>When AI methods for solving PDEs were compared to strong baselines, whatever narrowly defined advantage AI had usually disappeared.</figcaption></figure></div><p><span>My advisor and I eventually </span><a href="https://www.nature.com/articles/s42256-024-00897-5" rel="">published</a><span> a systematic review of research using AI to solve PDEs from fluid mechanics. We found that 60 out of the 76 papers (79 percent) that claimed to outperform a standard numerical method had used a weak baseline, either because they hadn’t compared to more advanced numerical methods, or because they weren’t comparing them on an equal footing. Papers with large speedups </span><em>all</em><span> compared to weak baselines, suggesting that the more impressive the result, the more likely the paper had made an unfair comparison.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png" width="1300" height="700" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:700,&quot;width&quot;:1300,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Results from a systematic review of research comparing AI methods for solving PDEs from fluid mechanics to standard numerical methods. Very few papers reported negative results, while those reporting positive results mostly compared to weak baselines.</figcaption></figure></div><p><span>We also found evidence, once again, that researchers tend not to report negative results, an effect known as </span><a href="https://en.wikipedia.org/wiki/Reporting_bias" rel="">reporting bias</a><span>. We ultimately </span><a href="https://www.nature.com/articles/s42256-024-00897-5" rel="">concluded</a><span> that AI-for-PDE-solving research is overoptimistic: “weak baselines lead to overly positive results, while reporting biases lead to under-reporting of negative results.”</span></p><p><span>These findings </span><a href="https://www.nature.com/articles/s42256-025-00989-w" rel="">sparked a debate</a><span> about AI in computational science and engineering:</span></p><ul><li><p><a href="https://engineering.gwu.edu/lorena-barba" rel="">Lorena Barba</a><span>, a professor at GWU who has previously discussed poor research practices in what she </span><a href="https://lorenabarba.com/figshare/anti-patterns-of-scientific-machine-learning-to-fool-the-massesa-call-for-open-science/" rel="">has called</a><span> “Scientific Machine Learning to Fool the Masses,” </span><a href="https://x.com/LorenaABarba/status/1839729358044574158" rel="">saw</a><span> our results as “solid evidence supporting our concerns in the computational science community over the hype and unscientific optimism” of AI.</span></p></li><li><p><a href="https://stephanhoyer.com/" rel="">Stephan Hoyer</a><span>, the lead of a </span><a href="https://arxiv.org/abs/2207.00556" rel="">team</a><span> at Google Research that independently reached </span><a href="https://x.com/shoyer/status/1362301955243057154" rel="">similar conclusions</a><span>, </span><a href="https://x.com/shoyer/status/1839195637474332850" rel="">described</a><span> our paper as “a nice summary of why I moved on from [AI] for PDEs” to weather prediction and climate modeling, applications of AI that seem </span><a href="https://www.nature.com/articles/d41586-024-02391-9" rel="">more promising</a><span>.</span></p></li><li><p><a href="https://brandstetter-johannes.github.io/" rel="">Johannes Brandstetter</a><span>, a professor at JKU Linz and co-founder of a </span><a href="https://www.emmi.ai/about" rel="">startup</a><span> that provides “AI-driven physics simulations”, </span><a href="https://www.nature.com/articles/s42256-024-00962-z" rel="">argued</a><span> that AI might achieve better results for more complex industrial applications and that “the future of the field remains undeniably promising and brimming with potential impact.”</span></p></li></ul><p><span>In </span><a href="https://www.nature.com/articles/s42256-025-00989-w" rel="">my opinion</a><span>, AI might eventually prove useful for certain applications related to solving PDEs, but I currently don’t see much reason for optimism. I’d like to see a lot more focus on trying to match the reliability of numerical methods and on </span><a href="https://cset.georgetown.edu/article/what-does-ai-red-teaming-actually-mean/" rel="">red teaming</a><span> AI methods; right now, they have neither the </span><a href="https://arxiv.org/abs/2303.16110" rel="">theoretical guarantees</a><span> nor empirically validated robustness of standard numerical methods.</span></p><p><span>I’d also like to see funding agencies incentivize scientists to create challenge problems for PDEs. A good model could be </span><a href="https://en.wikipedia.org/wiki/CASP" rel="">CASP</a><span>, a biennial protein folding competition that helped to motivate and focus research in this area over the last 30 years.</span></p><p><span>Besides </span><a href="https://www.nobelprize.org/prizes/chemistry/2024/press-release/" rel="">protein folding</a><span>, the canonical example of a scientific breakthrough from AI, a few examples of scientific progress from AI include:</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-163736413" href="https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres#footnote-1-163736413" target="_self" rel="">1</a></span></p><ul><li><p><span>Weather forecasting, where </span><a href="https://www.ecmwf.int/en/about/media-centre/news/2025/ecmwfs-ai-forecasts-become-operational" rel="">AI forecasts</a><span> have had up to 20% higher accuracy (though still lower resolution) compared to traditional physics-based forecasts.</span></p></li><li><p><span>Drug discovery, where </span><a href="https://www.sciencedirect.com/science/article/pii/S135964462400134X?via%3Dihub" rel="">preliminary data</a><span> suggests that AI-discovered drugs have been more successful in Phase I (but not Phase II) clinical trials. If the trend holds, this would imply a nearly twofold increase in end-to-end drug approval rates.</span></p></li></ul><p><span>But </span><a href="https://deepmind.google/public-policy/ai-for-science/" rel="">AI</a><span> </span><a href="https://ai.google/applied-ai/science/" rel="">companies</a><span>, </span><a href="https://royalsociety.org/news-resources/projects/science-in-the-age-of-ai/" rel="">academic</a><span> and </span><a href="https://www.anl.gov/ai/reference/AI-for-Science-Energy-and-Security-Report-2023" rel="">governmental</a><span> organizations, and </span><a href="https://www.npr.org/2023/10/16/1198908289/ai-proteins-batteries-artificial-intelligence-scientific-discoveries" rel="">media outlets</a><span> increasingly present AI not only as a </span><a href="https://www.nature.com/articles/d41586-025-01069-0" rel="">useful scientific tool</a><span>, but </span><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-ai-for-science/" rel="">one that</a><span> “will have a transformational impact” on science.</span></p><p><span>I don’t think we should necessarily dismiss these statements. While current LLMs, </span><a href="https://deepmind.google/public-policy/ai-for-science/" rel="">according to DeepMind</a><span>, “still struggle with the deeper creativity and reasoning that human scientists rely on”, </span><a href="https://helentoner.substack.com/p/long-timelines-to-advanced-ai-have" rel="">hypothetical advanced AI systems</a><span> might one day be capable of </span><a href="https://sakana.ai/ai-scientist/" rel="">fully</a><span> </span><a href="https://www.futurehouse.org/" rel="">automating</a><span> the scientific process. I don’t expect that to happen anytime soon—if ever. But if such systems are created, there’s no doubt they would transform and accelerate science.</span></p><p>However, based on some of the lessons from my research experience, I think we should be pretty skeptical of the idea that more conventional AI techniques are on pace to significantly accelerate scientific progress.</p><p><span>Most narratives about AI accelerating science come from AI companies or scientists working on AI who benefit, directly or indirectly, from those narratives. For example, NVIDIA CEO Jensen Huang </span><a href="https://blogs.nvidia.com/blog/supercomputing-24/" rel="">talks about how</a><span> “AI will drive scientific breakthroughs” and “</span><a href="https://www.youtube.com/watch?v=heshd3L6Kdk" rel="">accelerate science by a million-X</a><span>.” NVIDIA, whose </span><a href="https://www.technologyreview.com/2016/04/07/161131/the-man-selling-shovels-in-the-machine-learning-gold-rush/" rel="">financial conflicts of interest</a><span> make them a particularly unreliable narrator, regularly makes hyperbolic statements about AI in science.</span></p><p><span>You might think that the rising adoption of AI by scientists is </span><a href="https://deepmind.google/public-policy/ai-for-science/" rel="">evidence</a><span> </span><a href="https://arxiv.org/abs/2405.15828" rel="">of</a><span> </span><a href="https://www.nature.com/articles/s41562-024-02020-5" rel="">AI’s</a><span> </span><a href="https://www.csiro.au/en/research/technology-space/ai/artificial-intelligence-for-science-report" rel="">usefulness</a><span> </span><a href="https://bojan.substack.com/p/ai-is-eating-the-research-world" rel="">in science</a><span>. After all, if AI usage in scientific research is growing exponentially, it must be because scientists find it useful, right?</span></p><p><span>I’m not so sure. In fact, I suspect that scientists are switching to AI less because it benefits science, and more because it benefits them.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-163736413" href="https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres#footnote-2-163736413" target="_self" rel="">2</a></span></p><p><span>Consider my motives for switching to AI in 2018. While I sincerely thought that AI might be useful in plasma physics, I was mainly motivated by higher salaries, better job prospects, and academic prestige. I also noticed that higher-ups at my lab usually seemed more interested in the </span><a href="https://www.energy.gov/science/articles/department-energy-announces-68-million-funding-artificial-intelligence-scientific" rel="">fundraising potential</a><span> of AI than technical considerations.</span></p><p><span>Later research found that scientists who use AI are </span><a href="https://www.nature.com/articles/d41586-024-03355-9" rel="">more likely to publish top-cited papers</a><span> and receive on average </span><a href="https://arxiv.org/abs/2412.07727" rel="">three times as many citations</a><span>. With such strong incentives to use AI, it isn’t surprising that so many scientists are doing so.</span></p><p><span>So even when AI achieves genuinely impressive results </span><em>in</em><span> science, that doesn’t mean that AI has done something useful </span><em>for</em><span> science. More often, it reflects only the </span><em>potential</em><span> of AI to be useful down the road.</span></p><p><span>This is because scientists working on AI (myself included) often work backwards. Instead of identifying a problem and then trying to find a solution, we start by assuming that AI will be the solution and then looking for problems to solve. But because it’s difficult to identify open scientific challenges that can be solved using AI, this “</span><a href="https://x.com/MilesCranmer/status/1879542350541635882" rel="">hammer in search of a nail</a><span>” style of science means that researchers will often tackle problems which are suitable for using AI but which either have already been solved or don't create new scientific knowledge.</span></p><p>To accurately evaluate the impacts of AI in science, we need to actually look at the science. But unfortunately, the scientific literature is not a reliable source for evaluating the success of AI in science.</p><p><span>One issue is </span><a href="https://en.wikipedia.org/wiki/Survivorship_bias" rel="">survivorship bias</a><span>. Because AI research, </span><a href="https://research-information.bris.ac.uk/ws/portalfiles/portal/437692523/methods_failing_the_data.pdf" rel="">in the words of</a><span> one researcher, has “nearly complete non-publication of negative results,” we usually only see the successes of AI in science and not the failures. But without negative results, our attempts to evaluate the impacts of AI in science typically get distorted.</span></p><p><span>As anyone who’s studied the </span><a href="https://en.wikipedia.org/wiki/Replication_crisis" rel="">replication crisis</a><span> knows, survivorship bias is a </span><a href="https://en.wikipedia.org/wiki/Science_Fictions" rel="">major issue</a><span> in science. Usually, the culprit is a </span><a href="https://www.cambridge.org/core/journals/psychological-medicine/article/cumulative-effect-of-reporting-and-citation-biases-on-the-apparent-efficacy-of-treatments-the-case-of-depression/71D73CADE32C0D3D996DABEA3FCDBF57" rel="">selection process</a><span> in which results that are not statistically significant are filtered from the scientific literature.</span></p><p><span>For example, the distribution of </span><a href="https://x.com/JohnHolbein1/status/1903173893222711795" rel="">z-values from medical research</a><span> is shown below. A z-value between -1.96 and 1.96 indicates that a result is not statistically significant. The sharp discontinuity around these values suggests that many scientists either didn’t publish results between these values or massaged their data until they cleared the threshold of statistical significance.</span></p><p>The problem is that if researchers fail to publish negative results, it can cause medical practitioners and the general public to overestimate the effectiveness of medical treatments.</p><p><span>Something similar has been happening in AI-for-science, though the selection process is based not on statistical significance but on </span><em>whether the proposed method outperforms other approaches or successfully performs some novel task</em><span>. This means that AI-for-science researchers almost always report successes of AI, and rarely publish results when AI isn’t successful.</span></p><p><span>A second issue is that pitfalls often cause the successful results that do get published to reach overly optimistic conclusions about AI in science. The details and </span><a href="https://www.aisnakeoil.com/p/scientists-should-use-ai-as-a-tool" rel="">severity</a><span> seem to differ </span><a href="https://journals.aps.org/prxenergy/abstract/10.1103/PRXEnergy.3.011002" rel="">between</a><span> </span><a href="https://x.com/Robert_Palgrave/status/1744383962913394758" rel="">fields</a><span>, but </span><a href="https://www.nature.com/articles/d41586-019-02307-y#ref-CR2" rel="">pitfalls mostly</a><span> have fallen into one of </span><a href="https://arxiv.org/abs/2407.12220" rel="">four categories</a><span>: </span><a href="https://reproducible.cs.princeton.edu/" rel="">data leakage</a><span>, </span><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0184604" rel="">weak</a><span> </span><a href="https://x.com/tunguz/status/1853545690565058723" rel="">baselines</a><span>, </span><a href="https://news.ycombinator.com/item?id=36231147" rel="">cherry-picking</a><span>, and </span><a href="https://pubs.acs.org/doi/10.1021/acs.chemmater.4c00643" rel="">misreporting</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The same people who evaluate AI models also benefit from those evaluations.</figcaption></figure></div><p><span>While the causes of this tendency towards overoptimism are complex, the core issue appears to be a </span><a href="https://arxiv.org/abs/2407.12220" rel="">conflict of interest</a><span> in which the same people who evaluate AI models also benefit from those evaluations.</span></p><p><span>These issues seem to be </span><a href="https://www.aisnakeoil.com/p/scientists-should-use-ai-as-a-tool" rel="">bad enough</a><span> that I encourage people to treat impressive results in AI-for-science the same way we treat surprising results in nutrition science: with </span><a href="https://www.theatlantic.com/magazine/archive/2023/05/ice-cream-bad-for-you-health-study/673487/" rel="">instinctive</a><span> </span><a href="https://www.cbsnews.com/news/how-the-chocolate-diet-hoax-fooled-millions/" rel="">skepticism</a><span>.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What are people doing? Live-ish estimates based on global population dynamics (161 pts)]]></title>
            <link>https://humans.maxcomperatore.com/</link>
            <guid>44036900</guid>
            <pubDate>Tue, 20 May 2025 01:36:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://humans.maxcomperatore.com/">https://humans.maxcomperatore.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44036900">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="utc-time-display">
      <svg id="utc-analog-clock" viewBox="0 0 100 100">
        <circle cx="50" cy="50" r="48"></circle><line id="hour-hand" x1="50" y1="50" x2="50" y2="25"></line><line id="minute-hand" x1="50" y1="50" x2="50" y2="15"></line><line id="second-hand" x1="50" y1="50" x2="50" y2="10"></line><circle cx="50" cy="50" r="3"></circle>
      </svg>
      <p><span id="currentTime">Coordinated Universal Time (UTC): 00:00:00</span>
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A shower thought turned into a Collatz visualization (114 pts)]]></title>
            <link>https://abstractnonsense.com/collatz/</link>
            <guid>44036716</guid>
            <pubDate>Tue, 20 May 2025 01:04:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abstractnonsense.com/collatz/">https://abstractnonsense.com/collatz/</a>, See on <a href="https://news.ycombinator.com/item?id=44036716">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p>
        I recently went on a nice long SCUBA diving trip with my wife and daughters. Lots of diving implies lots of showers, and
        lots of showers means lots of shower-thoughts! <span>[1]</span> An especially interesting one I had turned into a nice way to visualize
        some aspects of the Collatz Conjecture.
    </p>
    <p>
        The Collatz Conjecture defines a very simple function on all positive integers as follows:
        </p><ul>
            <li>If the number is even, divide it by 2</li>
            <li>If the number is odd, multiply it by 3 and then add 1</li>
        </ul>
    
    <p>
        If you can prove that repeated applications of this function, starting from any positive integer, will eventually reach 1 then you have proved the Collatz Conjecture
        and won a million dollars and plenty of fame and glory! <span>[2]</span>
    </p>
    <p>
        Individual inputs are trivial to verify. For example, if we start with 6, we get:
        </p><ul>
            <li>6 is even, so 6 / 2 → 3</li>
            <li>3 is odd, so 3 × 3 + 1 → 10</li>
            <li>10 is even, so 10 / 2 → 5</li>
            <li>5 is odd, so 5 × 3 + 1 → 16</li>
            <li>16 is even, so 16 / 2 → 8</li>
            <li>8 is even, so 8 / 2 → 4</li>
            <li>4 is even, so 4 / 2 → 2</li>
            <li>2 is even, so 2 / 2 → 1</li>
        </ul>
        <br>
        But the real work is proving that this happens for <i>all</i> positive integers. So far, no one has been able to do that.
    
    <p>
        I'm not going to solve the Collatz Conjecture. Somehow I'm turning 50 this year, and my PhD was more than two decades ago (and in a branch of math that's unlikely
        to impact the conjecture). But I love that it's just there, waiting and inspiring people. I hope to see it solved in my lifetime.
    </p>
    <p>
        Anyway, my shower-thought was that it would be nice to visualize this repeated application of the Collatz function
        for many positive integers all at once, rather than just one at a time. To do that, I thought, why not
        keep track of the sequence of branches taken for each input and then since there are only two branches,
        why not treat them as binary digits?! <span>[3]</span> We could use those binary digit sequences to make a fraction from
        each input, by summing <i>2<sup>-n</sup>b<sub>n</sub></i> for each bit <i>b<sub>n</sub></i> in the sequence,
        making it really easy to graph them and perhaps more easily see what the Collatz process is doing.
    </p>
    <p>
        After a little more thinking I decided this approach would be a little too naive. The output of the Collatz function at each step seems
        like it would be biased towards producing even numbers, making the bit-sequences more full of one binary digit than the other, and maybe obscuring any interesting features
        we could otherwise see. I decided I would fix that by <i>immediately</i> dividing by 2 at the end of the 3n + 1 step (since we know 3n + 1 will be even if n is odd).
        It turns out this idea is well-known, as the "shortcut" Collatz function.
    </p>
    <p>
        With that tweak, here's a simple JavaScript implementation of this idea:
        </p><pre><span>function</span> collatzBits(n) { 
    <span>let</span> bits = [];
    <span>while</span> (n !== 1) {
        <span>if</span> (n % 2 === 0) {
            bits.push(1);
            n = n / 2;
        } <span>else</span> {
            bits.push(0);
            n = (3 * n + 1) / 2;
        }
    }
    <span>return</span> bits;
}

<span>function</span> bitsToFraction(bits) { 
    <span>let</span> fraction = 0;
    <span>for</span> (<span>let</span> i = 0; i &lt; bits.length; i++) {
        fraction += bits[i] / Math.pow(2, i + 1);
    }
    <span>return</span> fraction;
}
</pre>
    
    <p>
        Here's an interactive version you can play with. Try putting in a few numbers to see how the Collatz process gets encoded into binary fractions.
        <br>
        </p><p>
            Choose an input n =
            
            <br>
            The corresponding sequence of bits, interpreted as a binary fraction, is <span id="interactiveBits">0.0</span><sub>2</sub>
            <br>
            which is <span id="interactiveOutput">0</span> as a decimal.
        </p>
    
    <p>
        Something fun to think about: How long of a bit-sequence can you construct? Can you make an arbitrarily long one, by choosing the right starting number?
    </p>
    
    <p>
        Of course, now that we have a mapping from positive integers to fractions, we can also plot them.
    </p>
    <p>
        Here's a plot of the Collatz fractions for the first N positive integers. You can adjust N to see more points. Try some big
        values of N to get a sense of the distribution of the Collatz fractions.
        N = 
    </p>
    <canvas id="collatz1dCanvas"></canvas>
    <p>
        The points look quite uniformly distributed to me. If I squint, then <i>maybe</i> I can see some structure, but it's hard to describe
        and I could be imagining it.
    </p>
    <p>
        After making this plot I remembered a nice trick I read about as a teenager in James Gleick's fantastic book "Chaos"
        (I think the idea might have been attributed to Feigenbaum). The trick is that, when you have a sequence of numbers
        that seem random, you should try treating subsequent pairs of numbers as coordinates in a 2D plot. For our sequence of "Collatz fractions",
        f<sub>n</sub>, we would plot the points (f<sub>n</sub>, f<sub>n+1</sub>) for n = 1, 2, 3, ... N.
    </p>
    <p>
        I did that, and was so surprised by the result I first thought I must have made a mistake in my code. But I hadn't, the patterns are real.
        They look almost like some kind of alien "writing" to me, and there's so much beautiful self-similarity in them.
        To dig deeper into the structure, I added a way to color points that match simple javascript rules. Here's an interactive version of that,
        with a few of my colorizing rules. You can add more of your own too!
    </p>
    <ul id="color-rules">
        <li>
            
            
            
        </li>
        <li>
            
            
            
        </li>
        <li>
            
            
            
        </li>
        <li>
            
             <!-- sixty-ten! -->
            
        </li>
        <li>
            
            
            
        </li>
    </ul>
    <p>
        Here's the plot for the first N integers, where N =  If you zoom far enough to see gaps between points, increase N (but the plot will be slower!)
        </p>
        <canvas id="collatz2dCanvas"></canvas>
    
    <p>
        I find this incredibly fun to play with, zooming deep in and adding new color rules and seeing how they change the plot.
        Depending on what device you're viewing this on, you should be able to pinch or scroll to zoom in and out, and drag it around to see different parts of it.
    </p>
    <p>
        I had already done a quick literature search to see if anyone else had done something similar, but hadn't found anything so far.
        I wrote down some ideas I thought might possibly turn into a proof of the self-similarity, but it didn't look trivial to me.
        So, before really getting to work on that, I decided to try again with the literature search. This time I used ChatGPT's new "Deep Research" feature.
        It thought about it for a long time, doing a bunch of searches, and eventually replied with a list of papers it thought might be promising.
        Most of them actually weren't, but one of them was an exact match. <a href="https://arxiv.org/abs/1805.00133">This 2019 paper by French mathematician
        Olivier Rozier</a> contains a plot that looks exactly the same as mine! It was really fun to see it again in a different context, and Rozier does prove some
        self-similarity results. Rozier's paper also cites a
        <a href="https://www.researchgate.net/publication/255633359_A_fractal_set_associated_with_the_Collatz_problem">2007 paper by Yukihiro Hashimoto</a>
        that has the same plot again.
        Neither Rozier's nor Hashimoto's plot is constructed the same way as mine, even though they look the same. Both of these papers build the plot
        starting with 2-adic numbers and only later map those to fractions. I would guess the 2-adic approach probably makes their proofs nicer,
        but jumping straight to fractions is likely to be easier if you've never seen p-adic numbers before (or if, like me, it's been long enough that
        you've forgotten most of what you learned about them!).
    </p>
    <p>
        If you find something interesting in the plot — a nice pattern, a structure, something weird — please <a href="https://abstractnonsense.com/">let me know</a>! I'd love for this to spark
        your imagination and maybe even lead to some new discoveries. I think this plot is beautiful, and I hope you enjoy it too.
    </p>
    
    <p>
        Thanks to Tatiana Moorier and Emmett Shear for reading drafts of this.
    </p>
    
    <p>
        <span>[1]</span> I've been telling people for years if businesses want employees to have better ideas, they should have more showers in their offices. So far everyone seems
        to think I'm joking. I'm not.
    </p>
    <p>
        <span>[2]</span> A reasonable question at this point is "Why do mathematicians care about proving the Collatz Conjecture?"
        One answer is just "because it's there!"
        But there are more serious answers too. Here's one: After the proof of Fermat's Last Theorem in 1995, the Collatz Conjecture is arguably the best example of a problem
        that's very easy to state, and so far completely impossible to prove. Often when mathematicians have encountered problems like this in the past,
        they've had to invent new kinds of "mathematical machinary" to solve them. In high school we learn some basic kinds of machinary -- proof by induction,
        proof by contradiction, and so on. Later, in college, we might learn Cantor's diagonal argument, or the epsilon-delta approach to limits.
        It seems likely, since we still haven't solved the Collatz Conjecture, that we will need some new kind of machinary to solve it.
        I think it's the anticipation of the discovery of that new machinary that keeps mathematicians interested in the Collatz Conjecture.
    </p>
    <p>
        <span>[3]</span> I've seen other people keep track of the branches and use them to draw some really pretty pictures,
        e.g. turning left or right at each step depending on which branch was taken.
        The problem with that is you can't use it to see the branching structure for huge numbers of inputs all at the same time.
    </p>
    <br>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DDoSecrets publishes 410 GB of heap dumps, hacked from TeleMessage (471 pts)]]></title>
            <link>https://micahflee.com/ddosecrets-publishes-410-gb-of-heap-dumps-hacked-from-telemessages-archive-server/</link>
            <guid>44036647</guid>
            <pubDate>Tue, 20 May 2025 00:52:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://micahflee.com/ddosecrets-publishes-410-gb-of-heap-dumps-hacked-from-telemessages-archive-server/">https://micahflee.com/ddosecrets-publishes-410-gb-of-heap-dumps-hacked-from-telemessages-archive-server/</a>, See on <a href="https://news.ycombinator.com/item?id=44036647">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
                    <header>
                        
                            <figure>
        <img srcset="https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=300 300w,
                    https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=720 720w,
                    https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=960 960w,
                    https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1200 1200w,
                    https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000 2000w,
                    https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000" sizes="(max-width: 1200px) 100vw, 1200px" src="https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1200" alt="DDoSecrets publishes 410 GB of heap dumps, hacked from TeleMessage's archive server">
            <figcaption><span>Photo by </span><a href="https://unsplash.com/@katertottz?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Katie Rodriguez</span></a><span> / </span><a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Unsplash</span></a></figcaption>
    </figure>
                    </header>

                <section>
                    <p>This morning, Distributed Denial of Secrets <a href="https://ddosecrets.com/article/telemessage" rel="noreferrer">published</a> 410 GB of data hacked from TeleMessage, the Israeli firm that makes modified versions of Signal, WhatsApp, Telegram, and WeChat that centrally archive messages. Because the data is sensitive and full of PII, DDoSecrets is only sharing it with journalists and researchers.</p><p>There's a lot of background, so here's a quick timeline of events with relevant links:</p><ul><li>March: Then-national security advisor Mike Waltz <a href="https://web.archive.org/web/20250325174744/https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/" rel="noreferrer">invited</a> a journalist into a Signal group where they planned <a href="https://zeteo.com/p/signal-chat-war-crimes-revealed-yemen-trump-admin" rel="noreferrer">war crimes</a>. This led to Congressional <a href="https://www.pbs.org/newshour/politics/watch-ratcliffe-gabbard-patel-testify-to-senate-after-war-plans-revealed-to-journalist-in-chat" rel="noreferrer">hearings</a> about Trump officials using Signal groups to discuss classified information.</li><li>May 1: Waltz (the day he was demoted from position of national security advisor) was photographed using TM SGNL, a modified version of Signal made by TeleMessage. He had texts up with Tusli Gabbard, JD Vance, and Marco Rubio.</li><li>May 3: I <a href="https://micahflee.com/heres-the-source-code-for-the-unofficial-signal-app-used-by-trump-officials/" rel="noreferrer">published</a> the source code of the TM SGNL to GitHub.</li><li>May 4: TeleMessage got hacked, which I <a href="https://micahflee.com/the-signal-clone-the-trump-admin-uses-was-hacked/" rel="noreferrer">reported</a> in 404 Media with Joseph Cox. </li><li>May 5: TeleMessage got hacked again by someone else, as NBC News <a href="https://www.nbcnews.com/tech/security/telemessage-suspends-services-hackers-say-breached-app-rcna204925" rel="noreferrer">reported</a>.</li><li>May 6: I <a href="https://micahflee.com/despite-misleading-marketing-israeli-company-telemessage-used-by-trump-officials-can-access-plaintext-chat-logs/" rel="noreferrer">published analysis</a> of the TM SGNL source code, along with some of the hacked data, that prove the TeleMessage lied about its products supporting end-to-end encryption.</li><li>May 18: I <a href="https://micahflee.com/how-the-knock-off-signal-app-used-by-trump-officials-got-hacked-in-20-minutes/" rel="noreferrer">published details</a> about the TeleMessage server's vulnerability in WIRED. <em>TLDR: if anyone on the internet loaded the URL <strong>archive.telemessage.com/management/heapdump</strong>, they would download a Java heap dump from TeleMessage's archive server, containing plaintext chat logs, among other things.</em></li></ul><p>Now, DDoSecrets has published 410 GB of these TeleMessage heap dumps. Here's the DDoSecrets description of <a href="https://ddosecrets.com/article/telemessage" rel="noreferrer">the release</a>:</p><blockquote>Thousands of heap dumps taken May 4, 2025 from TeleMessage, which produces software used to archive encrypted messaging apps such as Signal and WhatsApp. The service came to public notice in 2025 when it was reported that former national security adviser Mike Waltz used TeleMessage while communicating with members of the Trump administration, including Vice President JD Vance and Director of National Intelligence Tulsi Gabbard. TeleMessage has been used by the federal government since at least February 2023.<p>Some of the archived data includes plaintext messages while other portions only include metadata, including sender and recipient information, timestamps, and group names. To facilitate research, Distributed Denial of Secrets has extracted the text from the original heap dumps.</p></blockquote><p>It seems that the SignalGate saga of staggering incompetence is not yet complete. I'm digging into this data right now. It's bonkers.</p><p><em>Note: I'm a member of the DDoSecrets collective. If you can, </em><a href="https://donorbox.org/ddosecrets" rel="noreferrer"><em>donate</em></a><em>! DDoSecrets operates on a shoestring budget and does incredibly impactful work.</em></p>
                    
                </section>

                    

                
            </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[is-even-ai – Check if a number is even using the power of AI (255 pts)]]></title>
            <link>https://www.npmjs.com/package/is-even-ai</link>
            <guid>44036438</guid>
            <pubDate>Tue, 20 May 2025 00:19:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npmjs.com/package/is-even-ai">https://www.npmjs.com/package/is-even-ai</a>, See on <a href="https://news.ycombinator.com/item?id=44036438">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header></header><main id="main"> <div id="top"><div><p><span>1.0.5<!-- -->&nbsp;•&nbsp;</span><span>Public</span><span>&nbsp;•&nbsp;Published <time datetime="2024-10-19T03:05:20.686Z" title="10/19/2024, 3:05:20 AM">7 months ago</time></span></p></div><ul role="tablist" aria-owns="package-tab-readme package-tab-code package-tab-dependencies package-tab-dependents package-tab-versions"><li role="presentation"><a href="https://www.npmjs.com/package/is-even-ai?activeTab=readme" aria-selected="true" role="tab" aria-controls="tabpanel-readme" id="package-tab-readme" tabindex="0"><span> Readme</span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/is-even-ai?activeTab=code" aria-selected="false" role="tab" aria-controls="tabpanel-explore" id="package-tab-code" tabindex="-1"><span>Code <span><span>Beta</span></span></span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/is-even-ai?activeTab=dependencies" aria-selected="false" role="tab" aria-controls="tabpanel-dependencies" id="package-tab-dependencies" tabindex="-1"><span>1 Dependency</span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/is-even-ai?activeTab=dependents" aria-selected="false" role="tab" aria-controls="tabpanel-dependents" id="package-tab-dependents" tabindex="-1"><span>0 Dependents</span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/is-even-ai?activeTab=versions" aria-selected="false" role="tab" aria-controls="tabpanel-versions" id="package-tab-versions" tabindex="-1"><span>7 Versions</span></a></li></ul><p><span><div id="tabpanel-readme" aria-labelledby="package-tab-readme" role="tabpanel" data-attribute=""><article><div id="readme"><div><h2>is-even-ai</h2></div>
<p><a href="https://www.npmjs.com/package/is-even-ai" rel="nofollow"><img src="https://camo.githubusercontent.com/5ef7ab75d332b2a150f7abd92543b02f404a373538618c481d65e0878bafbce4/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f69732d6576656e2d61692e7376673f7374796c653d666c6174" alt="NPM Version" data-canonical-src="https://img.shields.io/npm/v/is-even-ai.svg?style=flat"></a>
<a href="https://github.com/Calvin-LL/is-even-ai/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/a534d8ca992ccfede6a1ec046afa59d5527e48b05af7e0e3d33ac804adbfe044/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f6c2f69732d6576656e2d61692e7376673f7374796c653d666c6174" alt="NPM License" data-canonical-src="https://img.shields.io/npm/l/is-even-ai.svg?style=flat"></a>
<a href="https://www.npmjs.com/package/is-even-ai" rel="nofollow"><img src="https://camo.githubusercontent.com/ee5ff62aeacf7bd48fd4fb5abd217a318db042c59dcfb82b2b9e9be79a3c0a1d/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f64742f69732d6576656e2d61692e7376673f7374796c653d666c6174" alt="NPM Downloads" data-canonical-src="https://img.shields.io/npm/dt/is-even-ai.svg?style=flat"></a></p>
<p>Check if a number is even using the power of ✨AI✨.</p>
<p>Uses OpenAI's GPT-3.5-turbo model under the hood to determine if a number is even.</p>
<p>For all those who want to use AI in their product but don't know how.</p>
<p>Inspired by the famous <a href="https://www.npmjs.com/package/is-even" rel="nofollow"><code>is-even</code></a> npm package and <a href="https://twitter.com/erenbali/status/1766602689863950658" rel="nofollow">this tweet</a>.</p>
<div><h2>Installation</h2></div>
<p><a href="https://www.npmjs.com/package/is-even-ai" rel="nofollow">This package is on npm.</a></p>

<div><h2>Usage</h2></div>
<div><pre><span>import</span> <span>{</span>
  <span>areEqual</span><span>,</span>
  <span>areNotEqual</span><span>,</span>
  <span>isEven</span><span>,</span>
  <span>isGreaterThan</span><span>,</span>
  <span>isLessThan</span><span>,</span>
  <span>isOdd</span><span>,</span>
  <span>setApiKey</span><span>,</span>
<span>}</span> <span>from</span> <span>"is-even-ai"</span><span>;</span>

<span>// won't need this if you have OPENAI_API_KEY in your environment</span>
<span>setApiKey</span><span>(</span><span>"YOUR_API_KEY"</span><span>)</span><span>;</span>

<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEven</span><span>(</span><span>2</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEven</span><span>(</span><span>3</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isOdd</span><span>(</span><span>4</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isOdd</span><span>(</span><span>5</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>areEqual</span><span>(</span><span>6</span><span>,</span> <span>6</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>areEqual</span><span>(</span><span>6</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>areNotEqual</span><span>(</span><span>6</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>areNotEqual</span><span>(</span><span>7</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isGreaterThan</span><span>(</span><span>8</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isGreaterThan</span><span>(</span><span>7</span><span>,</span> <span>8</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isLessThan</span><span>(</span><span>9</span><span>,</span> <span>8</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isLessThan</span><span>(</span><span>8</span><span>,</span> <span>9</span><span>)</span><span>)</span><span>;</span> <span>// true</span></pre></div>
<p>for more advance usage like changing which model to use and setting the temperature, use <code>IsEvenAiOpenAi</code> instead</p>
<div><pre><span>import</span> <span>{</span> <span>IsEvenAiOpenAi</span> <span>}</span> <span>from</span> <span>"is-even-ai"</span><span>;</span>

<span>const</span> <span>isEvenAiOpenAi</span> <span>=</span> <span>new</span> <span>IsEvenAiOpenAi</span><span>(</span>
  <span>{</span>
    <span>// won't need this if you have OPENAI_API_KEY in your environment</span>
    <span>apiKey</span>: <span>"YOUR_API_KEY"</span><span>,</span>
  <span>}</span><span>,</span>
  <span>{</span>
    <span>model</span>: <span>"gpt-3.5-turbo"</span><span>,</span>
    <span>temperature</span>: <span>0</span><span>,</span>
  <span>}</span>
<span>)</span><span>;</span>

<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isEven</span><span>(</span><span>2</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isEven</span><span>(</span><span>3</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isOdd</span><span>(</span><span>4</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isOdd</span><span>(</span><span>5</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>areEqual</span><span>(</span><span>6</span><span>,</span> <span>6</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>areEqual</span><span>(</span><span>6</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>areNotEqual</span><span>(</span><span>6</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>areNotEqual</span><span>(</span><span>7</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isGreaterThan</span><span>(</span><span>8</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isGreaterThan</span><span>(</span><span>7</span><span>,</span> <span>8</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isLessThan</span><span>(</span><span>8</span><span>,</span> <span>9</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isLessThan</span><span>(</span><span>9</span><span>,</span> <span>8</span><span>)</span><span>)</span><span>;</span> <span>// false</span></pre></div>
<div><h2>Supported AI platforms</h2></div>
<p>Feel free to make a PR to add more AI platforms.</p>
<ul>
<li>[x] <a href="https://openai.com/" rel="nofollow">OpenAI</a> via <code>IsEvenAiOpenAi</code>
</li>
</ul>
<div><h2>Supported methods</h2></div>
<ul>
<li><code>isEven(n: number)</code></li>
<li><code>isOdd(n: number)</code></li>
<li><code>areEqual(a: number, b: number)</code></li>
<li><code>areNotEqual(a: number, b: number)</code></li>
<li><code>isGreaterThan(a: number, b: number)</code></li>
<li><code>isLessThan(a: number, b: number)</code></li>
</ul>
</div></article></div></span><span aria-live="polite"></span></p></div> </main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Have I Been Pwned 2.0 is Now Live (613 pts)]]></title>
            <link>https://www.troyhunt.com/have-i-been-pwned-2-0-is-now-live/</link>
            <guid>44035158</guid>
            <pubDate>Mon, 19 May 2025 21:37:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.troyhunt.com/have-i-been-pwned-2-0-is-now-live/">https://www.troyhunt.com/have-i-been-pwned-2-0-is-now-live/</a>, See on <a href="https://news.ycombinator.com/item?id=44035158">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>This has been a <em>very</em> long time coming, but finally, after a marathon effort, <a href="https://haveibeenpwned.com/?ref=troyhunt.com" rel="noreferrer">the brand new Have I Been Pwned website is now live</a>!</p><p><img src="https://www.troyhunt.com/content/images/2025/05/2025-05-15_08-25-12.jpg" alt="" loading="lazy"></p>
<p><a href="https://github.com/HaveIBeenPwned/ux-rebuild/commit/38853b79597d3d22afad4ff6822ceaa94b6c49f9?ref=troyhunt.com" rel="noreferrer">Feb last year is when I made the first commit</a> to the public repo for the rebranded service, and <a href="https://www.troyhunt.com/soft-launching-and-open-sourcing-the-have-i-been-pwned-rebrand/" rel="noreferrer">we soft-launched the new brand in March of this year</a>. Over the course of this time, we've completely rebuilt the website, changed the functionality of pretty much every web page, added a heap of new features, and today, we're even launching a merch store 😎</p><p>Let me talk you through just some of the highlights, strap yourself in!</p><h2 id="the-search">The Search</h2><p>The signature feature of HIBP is that big search box on the front page, and now, it's even better - it has confetti!</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-16.png" alt="" loading="lazy" width="1689" height="820" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-16.png 600w, https://www.troyhunt.com/content/images/size/w1000/2025/05/image-16.png 1000w, https://www.troyhunt.com/content/images/size/w1600/2025/05/image-16.png 1600w, https://www.troyhunt.com/content/images/2025/05/image-16.png 1689w"></figure><p>Well, not for everyone, only about half the people who use it will see a celebratory response. There's a reason why this response is intentionally jovial, let me explain:</p><p>As Charlotte and I have travelled and spent time with so many different users of the service around the world, a theme has emerged over and over again: HIBP is a bit playful. It's not a scary place emblazoned with hoodies, padlock icons, and fearmongering about "the dark web". Instead, we aim to be more consumable to the masses and provide factual, actionable information without the hyperbole. Confetti guns (yes, there are several, and they're animated) lighten the mood a bit. The alternative is that you get the red response:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-21.png" alt="" loading="lazy" width="1343" height="1136" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-21.png 600w, https://www.troyhunt.com/content/images/size/w1000/2025/05/image-21.png 1000w, https://www.troyhunt.com/content/images/2025/05/image-21.png 1343w"></figure><p>There was a very brief moment where we considered a more light-hearted treatment on this page as well, but somehow a bit of sad trombone really didn't seem appropriate, so we deferred to a more demure response. But now it's on a timeline you can scroll through in reverse chronological order, with each breach summarising what happened.  And if you want more info, we have an all-new page I'll talk about in a moment.</p><p>Just one little thing first - we've dropped username and phone number search support from the website. Username searches were introduced in 2014 for <a href="https://haveibeenpwned.com/Breach/Snapchat?ref=troyhunt.com" rel="noreferrer">the Snapchat incident</a>, and phone number searches in 2021 for <a href="https://haveibeenpwned.com/Breach/Facebook?ref=troyhunt.com" rel="noreferrer">the Facebook incident</a>. And that was it. That's the only time we ever loaded those classes of data, and there are several good reasons why. Firstly, they're both painful to parse out of a breach compared to email addresses, which we simply use a regex to extract (<a href="https://github.com/HaveIBeenPwned/EmailAddressExtractor?ref=troyhunt.com" rel="noreferrer">we've open sourced the code that does this</a>). Usernames are a string. Phone numbers are, well, it depends. They're not just numbers because if you properly internationalise them (like they were in the Facebook incident), they've also got a plus at the front, but they're frequently all over the place in terms of format. And we can't send notifications because nobody "owns" a username, and phone numbers are <em>very </em>expensive to send SMSs to compared to sending emails. Plus, every other incident in HIBP other than those two has had email addresses, so if we're asking "have I been pwned?" we can always answer that question without loading those two hard-to-parse fields, which usually aren't present in most breaches anyway. When the old site offered to accept them in the search box, it created confusion and support overhead: "why wasn't my number in the [whatever] breach?!". That's why it's gone <em>from the website</em>, but we've kept it supported on the API to ensure we don't break anything... just don't expect to see more data there.</p><h2 id="the-breach-page">The Breach Page</h2><p>There are many reasons we created this new page, not least of which is that the search results on the front page were getting too busy, and we wanted to palm off the details elsewhere. So, now we have a dedicated page for each breach, <a href="https://haveibeenpwned.com/Breach/AshleyMadison?ref=troyhunt.com" rel="noreferrer">for example</a>:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-19.png" alt="" loading="lazy" width="1328" height="944" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-19.png 600w, https://www.troyhunt.com/content/images/size/w1000/2025/05/image-19.png 1000w, https://www.troyhunt.com/content/images/2025/05/image-19.png 1328w"></figure><p>That's largely information we had already (albeit displayed in a much more user-friendly fashion), but what's unique about the new page is much more targeted advice about what to do <em>after</em> the breach:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-20.png" alt="" loading="lazy" width="883" height="716" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-20.png 600w, https://www.troyhunt.com/content/images/2025/05/image-20.png 883w"></figure><p><a href="https://www.troyhunt.com/after-the-breach-finding-new-partners-with-solutions-for-have-i-been-pwned-users/" rel="noreferrer">I recently wrote about this section</a> and how we plan to identify other partners who are able to provide appropriate services to people who find themselves in a breach. Identity protection providers, for example, make a lot of sense for many data breaches.</p><p>Now that we're live, we'll also work on fleshing this page out with more breach and user-specific data. For example, if the service supports 2FA, then we'll call that out specifically rather than rely on the generic advice above. Same with passkeys, and we'll add a section for that. A recent discussion with the NCSC while we were in the UK was around adding localised data breach guidance, for example, showing folks from the UK the NCSC logo and a link to <a href="https://www.troyhunt.com/after-the-breach-finding-new-partners-with-solutions-for-have-i-been-pwned-users/" rel="noreferrer">their resource on the topic</a> (which recommends checking HIBP 🙂).</p><p>I'm sure there's much more we can do here, so if you've got any great ideas, drop me a comment below.</p><h2 id="the-dashboard">The Dashboard</h2><p>Over the course of many years, we introduced more and more features that required us to know who you were (or at least that you had access to the email address you were using). It began with <a href="https://www.troyhunt.com/heres-how-im-going-to-handle-ashley/" rel="noreferrer">introducing the concept of a sensitive breach</a> during the Ashley Madison saga of 2015, which meant the only way to see your involvement in that incident was to receive an email to the address before searching. (Sidenote: <a href="https://www.troyhunt.com/the-ethics-of-running-a-data-breach-search-service/" rel="noreferrer">There are many good reasons why we don't do that on every breach</a>.) In 2019, when <a href="https://www.troyhunt.com/authentication-and-the-have-i-been-pwned-api/" rel="noreferrer">I put an auth layer around the API to tackle abuse</a> (which it did <em>beautifully!</em>) I required email verification first before purchasing a key. And more things followed: a dedicated domain search dashboard, managing your paid subscription and earlier this year, viewing stealer logs for your email address.</p><p>We've now unified all these different places into one central dashboard:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-15.png" alt="" loading="lazy" width="1342" height="1085" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-15.png 600w, https://www.troyhunt.com/content/images/size/w1000/2025/05/image-15.png 1000w, https://www.troyhunt.com/content/images/2025/05/image-15.png 1342w"></figure><p>From a glance at the nav on the left, you can see a lot of familiar features that are pretty self-explanatory. These combine relevant things for the masses and those that are more business-oriented. They're now all behind the one "Sign In" that verifies access to the email address before being shown. In the future, we'll also add passkey support to avoid needing to send an email first.</p><p>The dashboard approach isn't just about moving existing features under one banner; it will also give us a platform on which to build new features in the future that require email address verification first. For example, we've often been asked to provide people with the ability to subscribe their family's email addresses to notifications, yet have them go to a different address. Many of us play tech support for others, and this would be a genuinely useful feature that makes sense to place at a point where you've already verified your email address. So, stay tuned for that one, among many others.</p><h2 id="the-domain-search-feature">The Domain Search Feature</h2><p>More time went into this one feature than most of the other ones combined. There's a lot we've tried to do here, starting with a much cleaner list of verified domains:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-25.png" alt="" loading="lazy" width="1325" height="508" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-25.png 600w, https://www.troyhunt.com/content/images/size/w1000/2025/05/image-25.png 1000w, https://www.troyhunt.com/content/images/2025/05/image-25.png 1325w"></figure><p>The search results now give a much cleaner summary and add filtering by both email address and a hotly requested new feature - just the latest breach (it's in the drop-down):</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-27.png" alt="" loading="lazy" width="983" height="673" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-27.png 600w, https://www.troyhunt.com/content/images/2025/05/image-27.png 983w"></figure><p>All those searches now just return JSON from APIs and the whole dashboard acts as a single-page app, so everything is <em>really</em> snappy. The filtering above is done purely client-side against the full JSON of the domain search, an approach we've tested with domains of over a quarter million breached email addresses and still been workable (although arguably, you really want that data via the API rather than scrolling through it in a browser window).</p><p>Verification of domain ownership has also been completely rewritten and has a much cleaner, simpler interface:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-26.png" alt="" loading="lazy" width="824" height="841" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-26.png 600w, https://www.troyhunt.com/content/images/2025/05/image-26.png 824w"></figure><p>We still have work to do to make the non-email verification methods smoother, but that was the case before, too, so at least we haven't regressed. That'll happen shortly, promise!</p><h2 id="the-api">The API</h2><p>First things first: there have been no changes to the API itself. <strong>This update doesn't break anything!</strong></p><p>There's a discussion over on the UX rebuild GitHub repo about <a href="https://github.com/HaveIBeenPwned/ux-rebuild/discussions/52?ref=troyhunt.com" rel="noreferrer">the right way to do API documentation</a>. The general consensus is OpenAPI and we started going down that route using <a href="https://scalar.com/?ref=troyhunt.com" rel="noreferrer">Scalar</a>. In fact, you can even see the work Stefan did on this here at <a href="https://haveibeenpwned.com/scalar/?ref=troyhunt.com">haveibeenpwned.com/scalar</a>:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-28.png" alt="" loading="lazy" width="1049" height="388" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-28.png 600w, https://www.troyhunt.com/content/images/size/w1000/2025/05/image-28.png 1000w, https://www.troyhunt.com/content/images/2025/05/image-28.png 1049w"></figure><p>It's very cool, especially the way it documents samples in all sorts of different languages and even has a test runner, which is effectively Postman in the browser. Cool, but we just couldn't finish it in time. As such, we've kept <a href="https://haveibeenpwned.com/API/v3?ref=troyhunt.com" rel="noreferrer">the old documentation</a> for now and just styled it so it looks like the rest of the site (which I reckon is still pretty slick), but we do intend to roll to the Scalar implementation when we're not under the duress of such a big launch.</p><h2 id="the-merch-store">The Merch Store</h2><p>You know what else is awesome? Merch! No, seriously, we've had <em>so </em>many requests over the years for HIBP branded merch and now, here we are:</p><figure><a href="https://merch.haveibeenpwned.com/?ref=troyhunt.com"><img src="https://www.troyhunt.com/content/images/2025/05/Artboard-1@2000x-100.jpg" alt="" loading="lazy" width="2000" height="728" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/Artboard-1@2000x-100.jpg 600w, https://www.troyhunt.com/content/images/size/w1000/2025/05/Artboard-1@2000x-100.jpg 1000w, https://www.troyhunt.com/content/images/size/w1600/2025/05/Artboard-1@2000x-100.jpg 1600w, https://www.troyhunt.com/content/images/2025/05/Artboard-1@2000x-100.jpg 2000w"></a></figure><p>We actually now have a real-life merch store at <a href="https://merch.haveibeenpwned.com/?ref=troyhunt.com">merch.haveibeenpwned.com</a>! This was probably the worst possible use of our time, considering how much mechanical stuff we had to do to make all the new stuff work, but it was a bit of a passion project for Charlotte, so yeah, now you can actually buy HIBP merch. It's all done through Teespring (<a href="https://haveibeenpwned.com/Breach/Teespring?ref=troyhunt.com" rel="noreferrer">where have I heard that name before?!</a>) and everything listed there is at cost price - we make absolutely zero dollars, it's just a fun initiative for the community 🙂</p><p>We did try out their option for stickers too, but they fell well short of what we already had up with <a href="https://www.stickermule.com/haveibeenpwned?ref=troyhunt.com" rel="noreferrer">our little one-item store on Sticker Mule</a> so for now, that remains the go-to for laptop decorations. Or just go and grab <a href="https://github.com/HaveIBeenPwned/Branding/tree/main/Merch/Stickers?ref=troyhunt.com" rel="noreferrer">the open source artwork</a> and get your own printed from wherever you please.</p><h2 id="the-nerdy-bits">The Nerdy Bits</h2><p>We still run the origin services on Microsoft Azure using a combination of the App Service for the website, "serverless" Functions for most APIs (there are still a few async ones there that are called as a part of browser-based features), SQL Azure "Hyperscale" and storage account features like queues, blobs and tables. Pretty much all the coding there is C# with .NET 9.0 and ASP.NET MVC on .NET Core for the web app. Cloudflare still plays a <em>massive</em> role with a lot of code in workers, data in R2 storage and all their good bits around WAF and caching. We're also now exclusively using their Turnstile service for anti-automation and have ditched Google's reCAPTCHA completely - big yay!</p><p>The front end is now latest gen Bootstrap and we're using SASS for all our CSS and TypeScript for all our JavaScript. Our (other) man in Iceland <a href="https://www.linkedin.com/in/ingiber/?ref=troyhunt.com" rel="noreferrer">Ingiber</a> has just done an absolutely outstanding job with the interfaces and exceeded all our expectations by a massive margin. What we have now goes far beyond what we expected when we started this process, and a big part of that has been Ingiber's ability to take a simple requirement and turn it into a thing of beauty 😍 I'm very glad that Charlotte, Stefan and I got to spend time with him in Reykjavik last month and share some beers.</p><p>We also made some measurable improvements to website performance. For example, I ran a <a href="https://tools.pingdom.com/?ref=troyhunt.com" rel="noreferrer">Pingdom website speed test</a> just before taking the old one offline:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-31.png" alt="" loading="lazy" width="726" height="235" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-31.png 600w, https://www.troyhunt.com/content/images/2025/05/image-31.png 726w" sizes="(min-width: 720px) 720px"></figure><p>And then ran it over the new one:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-30.png" alt="" loading="lazy" width="722" height="228" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-30.png 600w, https://www.troyhunt.com/content/images/2025/05/image-30.png 722w" sizes="(min-width: 720px) 720px"></figure><p>So we cut out 28% of the page size and 31% of the requests. The load time is much of a muchness (and it's highly variable at that), but having solid measures for all the values in the column on the right is a very pleasing result. Consider also the commentary anyone in web dev would have seen over the years about how much bigger web pages have become, and here we are shaving off solid double-digit percentages 11 years later!</p><p>Finally, anything that could remotely be construed as tracking or ad bloat just isn't there, because we simply don't do any of that 🙂 In fact, the only real traffic stats we have are based on what Cloudflare sees when the traffic flows through their edge nodes. And that 1Password product placement is, as it's always been, just text and an image. We don't even track outbound clicks, that's up to them if they want to capture that on the landing page we link to. This actually makes discussions such as we're having with identity theft companies that want product placement much harder as they're used to getting the sorts of numbers that invasive tracking produces, but we wouldn't have it any other way.</p><h2 id="the-ai">The AI</h2><p>I wanted to make a quick note of this here, as AI seems to be either constantly overblown or denigrated. Either it's going to solve the world's problems, or it just produces "slop". I used Chat GPT in particular <em>really</em> extensively during this rebuild, especially in the final days when time got tight and my brain got fried. Here are some examples where it made a big difference:</p>
<!--kg-card-begin: html-->
<pre><code>I'm using Bootstrap icons from here: https://icons.getbootstrap.com/

What's a good icon to illustrate a heading called "Index"?</code></pre>
<!--kg-card-end: html-->
<p>This was right at the 11th hour when we realised we didn't have time to implement Scalar properly, and I needed to quickly migrate all the existing API docs to the new template. There are over 2,000 icons on that page, and this approach meant it took about 30 seconds to find the right one, each and every time.</p><p>We killed off some pages on the old site, but before rolling it over, I wanted to know exactly what was there:</p>
<!--kg-card-begin: html-->
<pre><code>Write me a PowerShell script to crawl haveibeenpwned.com and write out each unique URL it finds</code></pre>
<!--kg-card-end: html-->
<p>And then:</p>
<!--kg-card-begin: html-->
<pre><code>Now write a script to take all the paths it found and see if they exist on stage.haveibeenpwned.com
</code></pre>
<!--kg-card-end: html-->
<p>It found good stuff too, like the security.txt file I'd forgotten to migrate. It also found stuff that never existed, so it's the usual "trust, but verify" situation.</p><p>And just a gazillion little things where every time I needed anything from some CSS advice to configuring Cloudflare rules to idiosyncrasies in the .NET Core web app, the correct answer was seconds away. I'd say it was right 90% of the time, too, and if you're not using AI aggressively in your software development work now (and I'm sure there are much better ways, too) I'm pretty confident in saying "you're doing it wrong".</p><h2 id="the-journey-here">The Journey Here</h2><p>It's hard to explain how much has gone into this, and that goes well beyond just what you see in front of you on the website today. It's seemingly little things, like minor revisions to the terms of use and privacy policy, which required many hours of time and thousands of dollars with lawyers (just minor updates to how we process data and a reflection of new services such as the stealer logs). </p><p>We pushed out the new site in the wee hours of Sunday morning my time, and <em>almost </em>everything went well:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-22.png" alt="" loading="lazy" width="995" height="764" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-22.png 600w, https://www.troyhunt.com/content/images/2025/05/image-22.png 995w"></figure><p>One or two little glitches that we've fixed and pushed quickly, that's it. I've actually waited until now, 2 days after going live, to publish this post just so we could iron out as much stuff as possible first. We've pushed more than a dozen new releases already since that time, just to keep iterating and refining quickly. TBH, it's been a bit intense and has been an <em>enormously</em> time-consuming effort that's dominated our focus, especially over the last few weeks leading up to launch. And just to drive that point home, I literally got a health alert first thing Monday morning:</p><figure><img src="https://www.troyhunt.com/content/images/2025/05/image-24.png" alt="" loading="lazy" width="800" height="385" srcset="https://www.troyhunt.com/content/images/size/w600/2025/05/image-24.png 600w, https://www.troyhunt.com/content/images/2025/05/image-24.png 800w"></figure><p>Nothing like empirical data to make a point! That last weekend when we went live was especially brutal; I don't think I've devoted that much high-intensity time to a software release for decades.</p><p>Have I Been Pwned has been a passion for a quarter of my life now. What I built in 2013 was never intended to take me this far or last this long, and I'm kinda shocked it did if I'm honest. I feel that what we've built with this new site and new brand has elevated this little pet project into a serious service that has a new level of professionalism. But I hope that in reading this, you see that it has maintained everything that has always been great about the service, and I'm so glad to still be here writing about it today in <a href="https://www.troyhunt.com/tag/have-i-been-pwned-3f/" rel="noreferrer">the 205th blog post with that tag</a>. Thanks for reading, <a href="https://haveibeenpwned.com/?ref=troyhunt.com" rel="noreferrer">now go and enjoy the new website</a> 😊</p>

            <section>
                <a href="https://www.troyhunt.com/tag/have-i-been-pwned-3f/">Have I Been Pwned</a>
            </section>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jules: An Asynchronous Coding Agent (361 pts)]]></title>
            <link>https://jules.google/</link>
            <guid>44034918</guid>
            <pubDate>Mon, 19 May 2025 21:12:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jules.google/">https://jules.google/</a>, See on <a href="https://news.ycombinator.com/item?id=44034918">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><p><span>1</span></p><div><p>Select your GitHub repository and branch. Write a detailed prompt for Jules.</p><p><span>*coming soon*</span> use "assign-to-jules" label in an issue to assign a task directly in GitHub.</p></div></div><div><p><span><span>@kathy/flipdisc </span><svg xmlns="http://www.w3.org/2000/svg" xml:space="preserve" x="0" y="0" version="1.1" viewBox="0 0 12 12" shape-rendering="crispEdges"><path fill="currentColor" d="M2 4v2h1v1h1v1h1v1h2V8h1V7h1V6h1V4z"></path></svg></span><span><span>main</span><svg xmlns="http://www.w3.org/2000/svg" xml:space="preserve" x="0" y="0" version="1.1" viewBox="0 0 12 12" shape-rendering="crispEdges"><path fill="currentColor" d="M2 4v2h1v1h1v1h1v1h2V8h1V7h1V6h1V4z"></path></svg></span></p><p>Can you bump the version of next.js to v15 and convert the project to use app directory?</p></div></div><div><div><p><span>2</span></p><p>Jules fetches your repository, clones it to a Cloud VM, and develops a plan utilizing the latest <a href="https://deepmind.google/technologies/gemini/pro/">Gemini 2.5 Pro model</a>.</p></div><div><div><p><img src="https://jules.google/jules-avatar-profile.png" width="30" height="30" alt="Jules Avatar"></p><div><p>Here is my plan:</p><p>I plan to update the following files to the new app directory structure.</p></div></div><div><div><p>That looks good. Continue!</p></div><p><img src="https://jules.google/you-avatar.png" width="30" height="30" alt="Your Avatar"></p></div></div></div><div><div><p><span>3</span></p><p>Jules provides a diff of the changes. Quickly browse and approve code edits.</p></div><div><pre> <p><span>9</span><span> </span><code></code></p><p><span>10</span><span> </span><code>  "dependencies": {</code></p><p><span>11</span><span>-</span><code>    "next": "10.2.3",</code></p><p><span>11</span><span>+</span><code>    "next": "15.0.1",</code></p><p><span>12</span><span> </span><code>    "react": "18.3.1",</code></p><p><span>13</span><span> </span><code>    "react-dom": "18.3"</code></p><p><span>14</span><span> </span><code>  }</code></p></pre></div></div><div><p><span>4</span></p><div><p>Jules creates a PR of the changes. Approve the PR, merge it to your branch, and publish it on GitHub.</p><p>Also, you can get caught up <span>fast</span>. Jules creates an audio summary of the changes. </p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FCC Chair Brendan Carr is letting ISPs merge–as long as they end DEI programs (105 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/05/fcc-chair-brendan-carr-is-letting-isps-merge-as-long-as-they-end-dei-programs/</link>
            <guid>44034536</guid>
            <pubDate>Mon, 19 May 2025 20:35:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/05/fcc-chair-brendan-carr-is-letting-isps-merge-as-long-as-they-end-dei-programs/">https://arstechnica.com/tech-policy/2025/05/fcc-chair-brendan-carr-is-letting-isps-merge-as-long-as-they-end-dei-programs/</a>, See on <a href="https://news.ycombinator.com/item?id=44034536">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>Verizon's letter said that because of the "changing landscape," the firm "has been evaluating its DEI-related programs, HR processes, supplier programs, training programs and materials, and other initiatives." Among other changes, Verizon said it "will no longer have a team or any individual roles focused on DEI" and will reassign DEI-focused employees to "HR talent objectives."</p>
<p>"Verizon recognizes that some DEI policies and practices could be associated with discrimination," the letter said.</p>
<p>T-Mobile sent a <a href="https://www.fcc.gov/ecfs/document/1032785606628/1">similar letter</a> to Carr on March 27, saying it "is fully committed to identifying and rooting out any policies and practices that enable such discrimination, whether in fulfillment of DEI or any other purpose," and is thus "conducting a comprehensive review of its DEI policies, programs, and activities." One day later, the FCC <a href="https://docs.fcc.gov/public/attachments/DA-25-283A1.pdf">approved</a> a T-Mobile joint venture to <a href="https://www.t-mobile.com/news/business/t-mobile-eqt-close-lumos-fiber-jv">acquire fiber provider Lumos</a>.</p>
<p>With the Verizon and T-Mobile deals approved, Carr has another opportunity to make demands on a major telecom company. On Friday, Charter announced a <a href="https://corporate.charter.com/newsroom/charter-communications-and-cox-communications-announce-definitive-agreement-to-combine-companies">$34.5 billion merger with Cox</a> that would make it the largest home Internet provider in the US, passing Comcast. Several <a href="https://www.fierce-network.com/broadband/what-charter-cox-deal-means-broadband-workers">Charter and Cox programs</a> could be on the chopping block because of Carr's animosity toward diversity initiatives.</p>

<h2>Verizon criticized as “cowardly”</h2>
<p>Media advocacy group Free Press criticized Verizon for agreeing to Carr's demands.</p>
<p>"Verizon's cowardly decision to modify or kill its diversity, equity and inclusion practices is the latest shameful episode in a litany of surrenders to appease our authoritarian president," Free Press Vice President of Policy Matt Wood <a href="https://www.freepress.net/news/verizon-cowardly-capitulates-trump-and-carrs-racist-bullying">said</a>. "The government alleges no specific instances of unlawful employment discrimination, and Verizon admits none. Yet to win a merger approval and the prospect of a few extra dollars, the company meekly suggests that some of its 'DEI policies and practices could be associated with discrimination'—lawyer-speak for we've done nothing wrong, but we can see which way the political winds are blowing."</p>
<p>Wood said that Carr "once defended his agency's independence from the White House when a Democrat was in charge" but is "now gleefully carrying out the president's orders to roll back civil-rights protections and equal-opportunity gains at all costs."</p>

          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kilo: A text editor in less than 1000 LOC with syntax highlight and search (151 pts)]]></title>
            <link>https://github.com/antirez/kilo</link>
            <guid>44034459</guid>
            <pubDate>Mon, 19 May 2025 20:28:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/antirez/kilo">https://github.com/antirez/kilo</a>, See on <a href="https://news.ycombinator.com/item?id=44034459">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Kilo</h2><a id="user-content-kilo" aria-label="Permalink: Kilo" href="#kilo"></a></p>
<p dir="auto">Kilo is a small text editor in less than 1K lines of code (counted with cloc).</p>
<p dir="auto">A screencast is available here: <a href="https://asciinema.org/a/90r2i9bq8po03nazhqtsifksb" rel="nofollow">https://asciinema.org/a/90r2i9bq8po03nazhqtsifksb</a></p>
<p dir="auto">Usage: kilo <code>&lt;filename&gt;</code></p>
<p dir="auto">Keys:</p>
<div data-snippet-clipboard-copy-content="CTRL-S: Save
CTRL-Q: Quit
CTRL-F: Find string in file (ESC to exit search, arrows to navigate)"><pre><code>CTRL-S: Save
CTRL-Q: Quit
CTRL-F: Find string in file (ESC to exit search, arrows to navigate)
</code></pre></div>
<p dir="auto">Kilo does not depend on any library (not even curses). It uses fairly standard
VT100 (and similar terminals) escape sequences. The project is in alpha
stage and was written in just a few hours taking code from my other two
projects, load81 and linenoise.</p>
<p dir="auto">People are encouraged to use it as a starting point to write other editors
or command line interfaces that are more advanced than the usual REPL
style CLI.</p>
<p dir="auto">Kilo was written by Salvatore Sanfilippo aka antirez and is released
under the BSD 2 clause license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The forbidden railway: Vienna-Pyongyang (2008) (203 pts)]]></title>
            <link>http://vienna-pyongyang.blogspot.com/2008/04/how-everything-began.html</link>
            <guid>44033310</guid>
            <pubDate>Mon, 19 May 2025 18:45:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://vienna-pyongyang.blogspot.com/2008/04/how-everything-began.html">http://vienna-pyongyang.blogspot.com/2008/04/how-everything-began.html</a>, See on <a href="https://news.ycombinator.com/item?id=44033310">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-7076948956751293997" itemprop="description articleBody">
<p><span>ABOUT ME</span></p><p>


My name is Helmut and I was born in 1980 in Graz (Austria). Trains were my hobby since childhood. My 2nd hobby is travelling. During my trips I always combine those hobbies. I also like nature, mountains, hiking and so on.</p><p>

My hobby is also my profession - since 2007 I work at the <a href="http://www.oebb.at/">Austrian Federal Railway</a><a href="http://www.oebb.at/">s</a>.</p><p>


I had the idea to travel to North Korea by train already since some years. I especially like long, transcontinental train trips, and the idea to start such a trip at the next train station to my home and then reach a distant place only by using trains (or sometimes also other means of public ground/sea transportation).</p><p>

During such train trips over thousands of kilometers I have the time to slowly adapt to another country, to another culture and mentaility - without the hassle of crowded airports, jet-lag and the discomfort of long-distance flights.<br>
I like the feeling of having a sleeper-train-compartment as my living- and bed-room for some days. It gives me a different sense of time and it is the perfect way to escape from the hectic everyday life. Time get's a new meaning.<br>
In our Western everyday life time is scarce. Time is luxury. And my trips for me represent some kind of luxury - the luxury of having time, whereas I'm not interested in the luxury of 5-star-hotels. A decent hostel is OK.</p><p>

Of course just sitting on the train for days isn't enough. An interesting destination and some interesting intermediate stops are also necessary for a good trip.</p><p>

My preferred destinations are not the usual tourist hot spots. I like to go to places, which are not so well known.<br>
I'm especially interested in Eastern Europe and the countries of the former USSR. Also Asian countries raise my interest and I'm also planning trips to the Middle East.<br>
Of course I also have the idea to travel across the other continents by train somewhen in future, but right now there are still much places to discover on Eurasia.</p><p>






<span>THE IDEA - NORTH KOREA BY TRAIN</span></p><p>

I don't know exactly when North Korea came into my mind for the 1st time. There are so many countries in the world, of which you only hear sometimes in the news or which you find on a map and don't have an idea what it is like. Usually you just don't think too much about it.</p><p>

In 1998 I bought my 1st <a href="http://www.thomascookpublishing.com/series.htm?series=Timetables">Thomas Cook Overseas Timetable</a>, a comprehensive guide to rail services over the world.<br>
I only started to explore the internet at that time, so this timetable book (very much recommended) gave me a 1st overview about passenger train services all over the world. At that time I had only travelled by train to some Central/Western European countries, even the so close Eastern European countries were exotic to me at that time.<br>
I remember that I found the timetables for international trains for North Korea in that book. I was surprised to find out that even two different direct train services link Moscow and Pyongyang. One via China and one via the North Korean/Russian border at Tumangan. At that time I only knew that North Korea was a communist country, but I didn't further investigate on possibilities to go there.</p><p>

During the following years I collected a lot of travel experience. Each trip leaded to more exotic destinations than the one before. In 1998 I interrailed across Western/Northern Europe, in 1999 I started to explore the former communist states like Poland.<br>
In 2001 I organized the 1st trip into the Former Soviet Union: Moscow, St. Petersburg and the Baltic states.<br>
With every trip the self confidence grew. In 2002 I travelled with the Transsiberian railway to Irkutsk and back - the 1st transcontinental train trip. 2005 was the year of a <a href="http://eurasia2005.blogspot.com/">3-months-trip</a> across Russia, Mongolia, China, Kazakhstan and Uzbekistan.</p><p>

At around this time my interest into North Korea grew. Im 2004 I read a <a href="http://groups.google.com/group/misc.transport.rail.europe/browse_thread/thread/c127ba911243286a/913a9d7ee1aa95bd?lnk=st&amp;q&amp;rnum=1&amp;hl=de&amp;pli=1">travelogue</a> of David Eerdmanns, who travelled by train from Europe to Pyongyang (via Beijing).</p><p>

In 2005 I saw a documentary on TV about a special trip to North Korea for railway enthusiasts. The company "<a href="http://www.farrail.net/">Farrail</a>" sometimes organizes such trips for groups.<br>
Of course my interest into North Korea wasn't only rail-related, also the political situation and the isolated society drew my attention. I haven't been to the Soviet Union or to Eastern Europe prior the fall of the Iron Curtain at around 1990. It would have been very interesting to see these countries. But I was too late.<br>
North Korea still offers the possibility to see an isolated communist country. Of course as a tourist you only see what the government wants you to see, but nevertheless I thought that it must be an unique experience to go there.</p><p>



Then, in february 2006 I read this <a href="http://groups.google.com/group/misc.transport.rail.europe/browse_thread/thread/ac0d39d25981e4c2/ab80e93ec8f1f20e?lnk=st&amp;q=&amp;rnum=3&amp;hl=de#ab80e93ec8f1f20e">fascinating post</a> by Roderick Smith at the usenet group misc.transport.rail.europe:</p><p>

<span>The Pyongyang trip was 1993-94, marking the 10th anniversary of my trip via </span><span>Mongolia. </span><span>The North Korean visa could be ordered in advance from Australia, but had to </span><span>be collected in Beijing (same as the Mongolian one 10 years earlier). </span><span>The rules were: compulsory guide, at USD100 per day. This was rather </span><span>expensive for a solo traveller; it would have been the same price for a </span><span>group of four. </span><span>Leaving Beijing: Chinese train. I was in a cabin with three Mongolian </span><span>businessmen bringing in huge quantities of consumer goods not available in </span><span>their own country. They spoke English, and shared their crate of beer with </span><span>me. </span><span>The train ran hours late from the border. We arrived in Pyongyang in mid </span><span>evening, too late for my promised visit to a railway workshop. The guide </span><span>took me to a huge hotel, and stayed on for dinner. I seemed to be the only </span><span>guest. </span><span>Next morning he collected me in a car with a driver. We followed some tram </span><span>routes, then had a short metro ride (same style as Moskva, with ornate </span><span>stations), then headed to the station for my departure (IIRC 10.30). </span><span>My through carriage was attached to a long Korean train. Meals were brought </span><span>to me in my compartment. </span><span>This train also ran late. </span><span>Next day, the through car was detached from the train (at Tuman'gang?)and </span><span>taken to the bogie-change yard, but too late to be changed that day. We </span><span>spent the night in the yard. The explanation of this was given to me by the </span><span>wife of an automative engineer from North Korea. His family was travelling </span><span>to a Volvo factory in Sweden. </span><span>Next morning, the carriage had its bogies changed. Nobody stopped me from </span><span>taking photos. </span><span>The carriage was shunted to Hasan, with no connection to Russia that day. </span><span>We spent 24 h there. I had USD to change: there was a food shop beside the </span><span>station, and a wine shop in the station. The hot water samovar was no cold, </span><span>and the carriage had no lighting. This was a dreary wait. </span><span>We continued next day, and were attached to a 'Russia' 2 days later than the </span><span>scheduled one. A couple of other carriages were attached at other stops </span><span>during the first day. </span></p><p>
About four years ago, the agent which handles my bookings to these countries<br>
told me that westerners can no longer use the Hasan crossing. I am not sure<br>
about the crossing for the Pyongyang - Manchuria - Moskva route.</p>
<p><span>Regards, </span><span>Roderick B Smith </span><span>Rail News Victoria Editor </span></p><p>


This message was fateful... it was the only information about the Tumangan-route on the internet I could find at that time. Since then the "Tumangan"-route was in some way engraved in my brain.<br>
I already found out before, that this route is served by a sleeping-wagon of the North Korean railways which only runs twice monthly. Also this fact makes it more interesting than the other Moscow-Pyongyang route via China, which is served by two Russian sleeping-wagons.</p><p>

Two months later at a German railway forum <a href="http://www.drehscheibe-foren.de/foren/read.php?30,138804,138806#msg-138806">a discussion about the Khasan/Tumangan border area</a> (in German) started. Someone posted a question about how to get from Vladivostok to the border - just to have a look over to North Korea.<br>
<a href="http://www.drehscheibe-foren.de/foren/read.php?30,3113329,3113329#msg-3113329">Further discussion about the Tumangan-route</a> (in German) followed, but everyone reported that travel agencies said that this route is currently not offered for tourist trips.</p><p>

In autumn 2006 decided to try a small experiment connected to this route: As I planned a trip to Siberia in winter, I thought: "Why not using this exotic sleeping-wagon of the North Korean railways for a domestic trip inside Russia?"</p><p>

In 2006 no photos of this vehicle were available on the internet yet, so even the idea of being a passenger in it was quite attractive. It departs Moscow only twice monthly, but it was no problem to adapt my travel plan to it's timetable (dep. Moscow on the 11th and 25th of each month).</p><p>

Of course this vagon is not intended for domestic trips inside Russia and tickets are only sold for international trips.<br>
Alexander, a friend of mine in Moscow, whom I know since my 2005 Eurasia trip and who works at a railway ticket agency, bought me a ticket to the 1st station in North Korea. To Tumangan!!!</p><p>

<a href="https://imageshack.com/i/f0M1PnYoj" target="_blank"></a><a href="https://imageshack.com/i/ey1LdVuij" target="_blank"><img src="http://imagizer.imageshack.us/v2/800x600q90/538/1LdVui.jpg"></a></p><p>


My plan was to get off already somewhere near Irkutsk (5000 km east of Moscow, still more than 4000 km away from the North Korean border).</p><p>

Departure from Moscow was on the 25th december 2006. I arrived in Moscow in the morning of the same day, after having spent Christmas evening on the Budapest - Moscow train.<br>
My friend brought me to the station by car and due to a traffic jam I nearly missed my train with the North Korean wagon. We arrived only 10 minutes before departure.<br>
Hint: If you need to be somewhere in Moscow at a certain time - forget about the car, use the metro!</p><p>

The North Korean conductors were quite surprised about a passenger from Europe, but they let me in. I explained that I only go to Irkutsk.</p><p>


<a href="https://imageshack.com/i/f0M1PnYoj" target="_blank"></a><a href="https://imageshack.com/i/f0ko8OSlj" target="_blank"><img src="http://imagizer.imageshack.us/v2/800x600q90/540/ko8OSl.jpg"></a></p><p>


I then spent 4 days in the North Korean sleeping car. For the 1st 24 hours I had my own compartment, but at Yekaterinburg three North Korean men, who worked in Russia and travelled back home, and their enormous quantities of luggage occupied the other three places in the 4-bed-compartment.<br>
We had a good and interesting time together, for me it was exciting to communicate with North Koreans (they spoke a little bit Russian) and for them it was also interesting to talk to a Westerner. We had a friendly relationsship and also shared a bottle of Austrian schnapps together... so, North Korean people are also just ordinary people as everyone in the world.</p><p>

Some more impressions of that trip:</p><p>

<a href="https://imageshack.com/i/f0M1PnYoj" target="_blank"></a><a href="https://imageshack.com/i/ipLphk6Nj" target="_blank"><img src="http://imagizer.imageshack.us/v2/800x600q90/673/Lphk6N.jpg"></a></p><p>

<a href="https://imageshack.com/i/ipEUm4Fpj" target="_blank"><img src="http://imagizer.imageshack.us/v2/800x600q90/673/EUm4Fp.jpg"></a></p><p>

<a href="https://imageshack.com/i/ex8Dqjksj" target="_blank"><img src="http://imagizer.imageshack.us/v2/800x600q90/537/8Dqjks.jpg"></a> <br>
<a href="http://imageshack.us/photo/my-images/802/jl50.jpg/" target="_blank"><br></a>
<a href="http://imageshack.us/photo/my-images/849/n3iz.jpg/" target="_blank"><br></a>
<a href="http://imageshack.us/photo/my-images/208/xgx6.jpg/" target="_blank"><br></a>
A short video from inside the Korean carriage:<br>
<a href="http://www.youtube.com/watch?v=t1wMqNCeG0w">http://www.youtube.com/watch?v=t1wMqNCeG0w</a></p><p>



After this trip the next aim was clear: Going with this wagon over the border to North Korea!!! This was just the logical consequence.</p><p>

But there was still the obstacle, that this route was officially closed for tourists. Travel agencies continued to claim that this routes doesn't exist and the train had been cancelled. I knew that that was wrong, as the Russian train timetable website <a href="http://train.mza.ru/">http://train.mza.ru/</a> showed the sleeping wagon Moscow - Pyongyang via Tumangan and also showed real-time place availability for departure dates within the 44-days-booking period. Another story told by travel agencies was, that this route is too unrealiable due to power-failures inside North Korea and that nobody could say how many days late the train actually would arrive in Pyongyang.</p><p>

On the German "Drehscheibe"-Forum <a href="http://www.drehscheibe-foren.de/foren/read.php?30,3424479,3424479#msg-3424479">discussions about possibilities to enter at Tumangan </a>continued in august 2007.<br>
A result of this discussions was, that Tumangan is by default listed on every North Korean visa, despite the fact that KITC doesn't offer this route to tourists.</p><p>

This was an important information. At least it is not totally illegal to enter at Tumangan...</p><p>

Together with Oliver, a good friend of mine working at the Swiss railways, I further discussed the possibilities to try a trip via Tumangan somewhen in 2008.<br>
So if Tumangan is listed on the visa - it could be possible to just book an ordinary trip to North Korea via Sinujiu but then in reality arrive via Tumangan.... hmmmm</p><p>

We both had time for such a trip in september 2008. The decision to actually make the trip to North Korea was done in spring 2008, but the final decision over the route (via Tumangan or not) was postponed. As you can imagine, it was not easy to decide whether we should take the risks of such a trip....<br>
Of course the idea of a potential routing of our trip via Tumangan was top-secret, only people whom we knew very well were introduced.</p><p>

We booked an individual DPRK-trip with a travel agency specialized on North Korea. Of course we told them that we would go via Sinujiu...<br>
Some weeks after I booked and payed the trip and submitted all data for the further processing, I got a call by a friendly employee of the North Korean embassy in Vienna. He said me, that I could pick up my visa now.</p><p>

I went to the embassy (here you can see the building in a quiet residence area in Vienna: <a href="http://www.buergmann.net/penzing/suchen.php?suchtext=zichy">http://www.buergmann.net/penzing/suchen.php?suchtext=zichy</a>) with my passport and rang at the door. A Korean man in jogging clothes and slippers let me in, I gave him the passport, the visa application form with a passport-photo and the 10 EUR visa fee.<br>
Inside the building there were the typical pictures of the two Kims and so on. The man disappeared with the stuff I gave him, came back 5 minutes later, gave me the passport with the visa inside, guided me back to the door and wished my a nice trip.</p><p>

That was it. 5 minutes. No questions, no bureacratic nightmare (as written on <a href="http://wikitravel.org/en/North_Korea#Get_in">http://wikitravel.org/en/North_Korea#Get_in</a>) at all!</p><p>


<span><a href="http://vienna-pyongyang.blogspot.com/2008/09/vienna-moscow.html"><span>Tumangan, we are coming!!!</span></a></span><span></span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Code SDK (361 pts)]]></title>
            <link>https://docs.anthropic.com/en/docs/claude-code/sdk</link>
            <guid>44032777</guid>
            <pubDate>Mon, 19 May 2025 18:04:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.anthropic.com/en/docs/claude-code/sdk">https://docs.anthropic.com/en/docs/claude-code/sdk</a>, See on <a href="https://news.ycombinator.com/item?id=44032777">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Claude Code SDK allows developers to programmatically integrate Claude Code into their applications. It enables running Claude Code as a subprocess, providing a way to build AI-powered coding assistants and tools that leverage Claude’s capabilities.</p>
<p>The SDK currently support command line usage. TypeScript and Python SDKs are coming soon.</p>
<h2 id="basic-sdk-usage"><span>Basic SDK usage</span></h2>
<p>The Claude Code SDK allows you to use Claude Code in non-interactive mode from your applications. Here’s a basic example:</p>

<h2 id="advanced-usage"><span>Advanced usage</span></h2>
<h3 id="multi-turn-conversations"><span>Multi-turn conversations</span></h3>
<p>For multi-turn conversations, you can resume conversations or continue from the most recent session:</p>

<h3 id="custom-system-prompts"><span>Custom system prompts</span></h3>
<p>You can provide custom system prompts to guide Claude’s behavior:</p>

<p>You can also append instructions to the default system prompt:</p>

<h3 id="mcp-configuration"><span>MCP Configuration</span></h3>
<p>The Model Context Protocol (MCP) allows you to extend Claude Code with additional tools and resources from external servers. Using the <code>--mcp-config</code> flag, you can load MCP servers that provide specialized capabilities like database access, API integrations, or custom tooling.</p>
<p>Create a JSON configuration file with your MCP servers:</p>

<p>Then use it with Claude Code:</p>

<p>Note: When using MCP tools, you must explicitly allow them using the <code>--allowedTools</code> flag. MCP tool names follow the pattern <code>mcp__&lt;serverName&gt;__&lt;toolName&gt;</code> where:</p>
<ul>
<li><code>serverName</code> is the key from your MCP configuration file</li>
<li><code>toolName</code> is the specific tool provided by that server</li>
</ul>
<p>This security measure ensures that MCP tools are only used when explicitly permitted.</p>
<h2 id="available-cli-options"><span>Available CLI options</span></h2>
<p>The SDK leverages all the CLI options available in Claude Code. Here are the key ones for SDK usage:</p>
<table><thead><tr><th>Flag</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td><code>--print</code>, <code>-p</code></td><td>Run in non-interactive mode</td><td><code>claude -p "query"</code></td></tr><tr><td><code>--output-format</code></td><td>Specify output format (<code>text</code>, <code>json</code>, <code>stream-json</code>)</td><td><code>claude -p --output-format json</code></td></tr><tr><td><code>--resume</code>, <code>-r</code></td><td>Resume a conversation by session ID</td><td><code>claude --resume abc123</code></td></tr><tr><td><code>--continue</code>, <code>-c</code></td><td>Continue the most recent conversation</td><td><code>claude --continue</code></td></tr><tr><td><code>--verbose</code></td><td>Enable verbose logging</td><td><code>claude --verbose</code></td></tr><tr><td><code>--max-turns</code></td><td>Limit agentic turns in non-interactive mode</td><td><code>claude --max-turns 3</code></td></tr><tr><td><code>--system-prompt</code></td><td>Override system prompt (only with <code>--print</code>)</td><td><code>claude --system-prompt "Custom instruction"</code></td></tr><tr><td><code>--append-system-prompt</code></td><td>Append to system prompt (only with <code>--print</code>)</td><td><code>claude --append-system-prompt "Custom instruction"</code></td></tr><tr><td><code>--allowedTools</code></td><td>Comma/space-separated list of allowed tools (includes MCP tools)</td><td><code>claude --allowedTools "Bash(npm install),mcp__filesystem__*"</code></td></tr><tr><td><code>--disallowedTools</code></td><td>Comma/space-separated list of denied tools</td><td><code>claude --disallowedTools "Bash(git commit),mcp__github__*"</code></td></tr><tr><td><code>--mcp-config</code></td><td>Load MCP servers from a JSON file</td><td><code>claude --mcp-config servers.json</code></td></tr><tr><td><code>--permission-prompt-tool</code></td><td>MCP tool for handling permission prompts (only with <code>--print</code>)</td><td><code>claude --permission-prompt-tool mcp__auth__prompt</code></td></tr></tbody></table>
<p>For a complete list of CLI options and features, see the <a href="https://docs.anthropic.com/en/docs/claude-code/cli-usage">CLI usage</a> documentation.</p>
<h2 id="output-formats"><span>Output formats</span></h2>
<p>The SDK supports multiple output formats:</p>
<h3 id="text-output-default"><span>Text output (default)</span></h3>
<p>Returns just the response text:</p>

<h3 id="json-output"><span>JSON output</span></h3>
<p>Returns structured data including metadata:</p>

<p>Response format:</p>

<h3 id="streaming-json-output"><span>Streaming JSON output</span></h3>
<p>Streams each message as it is received:</p>

<p>Each conversation begins with an initial <code>init</code> system message, followed by a list of user and assistant messages, followed by a final <code>result</code> system message with stats. Each message is emitted as a separate JSON object.</p>
<h2 id="message-schema"><span>Message schema</span></h2>
<p>Messages returned from the JSON API are strictly typed according to the following schema:</p>

<p>We will soon publish these types in a JSONSchema-compatible format. We use semantic versioning for the main Claude Code package to communicate breaking changes to this format.</p>
<h2 id="examples"><span>Examples</span></h2>
<h3 id="simple-script-integration"><span>Simple script integration</span></h3>

<h3 id="processing-files-with-claude"><span>Processing files with Claude</span></h3>

<h3 id="session-management"><span>Session management</span></h3>

<h2 id="best-practices"><span>Best practices</span></h2>
<ol>
<li>
<p><strong>Use JSON output format</strong> for programmatic parsing of responses:</p>

</li>
<li>
<p><strong>Handle errors gracefully</strong> - check exit codes and stderr:</p>

</li>
<li>
<p><strong>Use session management</strong> for maintaining context in multi-turn conversations</p>
</li>
<li>
<p><strong>Consider timeouts</strong> for long-running operations:</p>

</li>
<li>
<p><strong>Respect rate limits</strong> when making multiple requests by adding delays between calls</p>
</li>
</ol>
<h2 id="real-world-applications"><span>Real-world applications</span></h2>
<p>The Claude Code SDK enables powerful integrations with your development workflow. One notable example is the <a href="https://docs.anthropic.com/en/docs/claude-code/github-actions">Claude Code GitHub Actions</a>, which uses the SDK to provide automated code review, PR creation, and issue triage capabilities directly in your GitHub workflow.</p>

<ul>
<li><a href="https://docs.anthropic.com/en/docs/claude-code/cli-usage">CLI usage and controls</a> - Complete CLI documentation</li>
<li><a href="https://docs.anthropic.com/en/docs/claude-code/github-actions">GitHub Actions integration</a> - Automate your GitHub workflow with Claude</li>
<li><a href="https://docs.anthropic.com/en/docs/claude-code/tutorials">Tutorials</a> - Step-by-step guides for common use cases</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft's ICC blockade: digital dependence comes at a cost (265 pts)]]></title>
            <link>https://www.techzine.eu/news/privacy-compliance/131536/microsofts-icc-blockade-digital-dependence-comes-at-a-cost/</link>
            <guid>44032717</guid>
            <pubDate>Mon, 19 May 2025 17:59:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techzine.eu/news/privacy-compliance/131536/microsofts-icc-blockade-digital-dependence-comes-at-a-cost/">https://www.techzine.eu/news/privacy-compliance/131536/microsofts-icc-blockade-digital-dependence-comes-at-a-cost/</a>, See on <a href="https://news.ycombinator.com/item?id=44032717">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" role="main">
        <article id="post-131536">
            
          	<div itemprop="articleBody" id="primary">
                    
<p><strong>In February, the United States imposed sanctions on the International Criminal Court (ICC) in The Hague. As a result, Chief Prosecutor Karim Khan has no access to the emails on his Microsoft account. The incident once again demonstrates the risks of dependence on US IT services.</strong></p>



<p>To make matters worse, Khan’s bank accounts have also been frozen, according to the <a href="https://apnews.com/article/icc-trump-sanctions-karim-khan-court-a4b4c02751ab84c09718b1b95cbd5db3" target="_blank">Associated Press</a>. If he takes a flight to the US, he will likely be arrested upon arrival. According to the Associated Press, the ICC has been paralyzed by the forced Microsoft blockade. The conflict between the ICC and the US arose in November, when the former issued an arrest warrant for Israeli Prime Minister Benjamin Netanyahu. This incident tells bystanders more than just how applicable it is to this specific situation. Anyone who does not want to follow the geopolitical stance of the US exactly must have a plan B when it comes to software.</p>



<h2 id="h-good-until-it-isn-t">Good until it isn’t</h2>



<p>European governments may consider the risks of using Microsoft acceptable. That <a href="https://www.techzine.nl/blogs/security/555877/landsbelang-in-geding-overheid-vindt-risicos-microsoft-365-aanvaardbaar/">was the position</a> taken by the Dutch government in October last year, for example. Uncertainty about the sovereignty of Microsoft’s cloud services was not seen as a deal breaker: Azure, 365, and other Microsoft services were judged to offer all kinds of advantages that could mitigate any potential issues. At least, that was how it looked in 2024.</p>



<p>Under President Trump, the US has taken a decidedly less friendly stance toward the European Union and its member states. These geopolitical tensions mean that it is up to the business community to smooth things over. Microsoft, for example, <a href="https://www.techzine.eu/news/infrastructure/83677/microsoft-introduces-sovereign-cloud-for-european-governments/">is adamant</a> that it will defend itself in court if Washington demands access to European citizens’ data. In fact, the encryption and access management of sovereign cloud services should make access from outside the continent technically impossible. Even with the clearest explanation, this may be revoked or secretly circumvented. And if Microsoft loses its case in court, it will still have to change course. Every non-US government will have to keep this scenario in mind when organizing its digital affairs.</p>



<p>In other words, relying solely on Microsoft services for critical purposes has its risks. What if the Dutch government refuses to go along with changed US policy on ASML’s chip machines, to name one example? Do all civil servants have email accounts and bank accounts that are not linked to Microsoft and cannot be blocked by the US? If not, these services may continue to run smoothly until it is too late.</p>



<h2 id="h-contracts-are-worth-little">Contracts are worth little</h2>



<p>The most important achievement is that national security should not depend on the honor of an SLA. On top of that, the ICC is currently looking for European alternatives to the lost services, a smaller version of the larger concerns about Europe’s digital autonomy. There are <a href="https://european-alternatives.eu/" target="_blank">plenty of services</a> that could in principle provide the same functionality as the established names; the question is, however, whether they are enterprise-ready, secure, and fully sovereign.</p>
                </div><!-- #primary -->
                    
            </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Windows 98 themed website in 1 HTML file for my post punk band (187 pts)]]></title>
            <link>https://corp.band</link>
            <guid>44032470</guid>
            <pubDate>Mon, 19 May 2025 17:39:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://corp.band">https://corp.band</a>, See on <a href="https://news.ycombinator.com/item?id=44032470">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <!-- Desktop Icons -->
      <div>
        <!-- Events Icon -->
        <div data-window="events">
          <p><img src="https://corp.band/assets/calendar.png" alt="Events">
          </p>
          <p><span>Events</span>
        </p></div>

        <div data-window="social">
          <p><img src="https://corp.band/assets/people.png" alt="Social Media">
          </p>
          <p><span>Social</span>
        </p></div>

        <!-- Merch Icon -->
        <div data-window="merch">
          <p><img src="https://corp.band/assets/spider.png" alt="Merch">
          </p>
          <p><span>Merch</span>
        </p></div>

        <!-- Music Icon -->
        <div data-window="music">
          <p><img src="https://corp.band/assets/music.png" alt="Music">
          </p>
          <p><span>Music</span>
        </p></div>

        <!-- Mailing List Icon -->
        <div data-window="mailing">
          <p><img src="https://corp.band/assets/mail.png" alt="Mailing List">
          </p>
          <p><span>Mailing List</span>
        </p></div>

        <!-- EPK Icon -->
        <div data-window="booking">
          <p><img src="https://corp.band/assets/computer.png" alt="Computer">
          </p>
          <p><span>Booking</span>
        </p></div>

        <!-- Trash Icon -->
        <div data-window="trash">
          <p><img src="https://corp.band/assets/recycle.png" alt="Recycle Bin">
          </p>
          <p><span>Recycle Bin</span>
        </p></div>
      </div>

      <!-- Windows -->
      <!-- Computer Window -->
      <div id="computer">
        
        <p>My Computer contents would go here.</p>
      </div>

      <!-- Music Window -->
      <div id="music">
          <h2>Watch CORP.</h2>
          <iframe src="https://www.youtube.com/embed/OumeCXiSUuU?si=lKZBSALydi3YYRCH" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
          <p><a href="https://www.youtube.com/@corp.812" target="_blank">Watch More CORP.</a></p><h2>Listen To CORP.</h2>
          <p>
            It's your favorite band. It always has been.
            <img src="https://corp.band/assets/rock-on.gif" alt="CORP">
          </p>
          
          
        </div>

      <!-- Events Window -->
      

      <!-- Social Window -->
      

      <!-- Merch Window -->
      <div id="merch">
          <h2>Official CORP Merch</h2>
          <p>Be the coolest drone at the water cooler:</p>
          <ul>
            <li>Financial Accounting Services - $25</li>
            <li>Human Resource - $15</li>
            <li>Premium .mp3 - $30</li>
            <li>Insider Trading Tip - $20</li>
          </ul>
          <p><a href="https://corpband.bandcamp.com/" target="_blank">Shop on Bandcamp</a>
        </p></div>

      <!-- Mailing List Window -->
      <div id="mailing">
          <h2>Join Our Mailing List</h2>
          <p>Or become fodder for the machine:</p>
          

          <!-- <div class="mb-4">
            <label class="block mb-1">Email:</label>
            <input
              type="email"
              class="w-full p-1 border border-win98-dark shadow-win98-in"
              placeholder="your@email.com"
            />
          </div>
          <div class="mb-4">
            <label class="block mb-1">Name:</label>
            <input
              type="text"
              class="w-full p-1 border border-win98-dark shadow-win98-in"
              placeholder="Your Name"
            />
          </div>
          <button class="win98-btn">Subscribe</button> -->
        </div>

      <!-- EPK Window -->
      <div id="booking">
          <h2>Book CORP</h2>
          <p>
            Interested in booking CORP for your venue or event? Contact us via
            email:
          </p>
          
          <p>
            Check out our
            <a href="https://sites.google.com/view/corp-epk/home" target="_blank">Electronic Press Kit</a>
            for investment opportunities and tech specs.
          </p>
        </div>

      <!-- Trash Window -->
      <div id="trash">
          <p>The Recycle Bin is empty. Empty for kicks?</p>
          </div>

      <!-- Classified Window -->
      

      <!-- Links Window -->
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dilbert creator Scott Adams says he will die soon from same cancer as Joe Biden (379 pts)]]></title>
            <link>https://www.thewrap.com/dilbert-scott-adams-prostate-cancer-biden/</link>
            <guid>44031917</guid>
            <pubDate>Mon, 19 May 2025 17:00:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thewrap.com/dilbert-scott-adams-prostate-cancer-biden/">https://www.thewrap.com/dilbert-scott-adams-prostate-cancer-biden/</a>, See on <a href="https://news.ycombinator.com/item?id=44031917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<p>“Dilbert” creator Scott Adams said on Monday morning that he expects to die soon from prostate cancer, the same disease former President Joe Biden announced he is battling.</p>



<p>Adams made the jarring revelation during the latest episode of “Coffee With Scott Adams,” the Rumble show he hosts during weekday mornings. </p>	
	



<p>“I have the same cancer that Joe Biden has. I also have prostate cancer that has also spread to my bones, but I’ve had it longer than he’s had it – well, longer than he’s admitted having it,” Adams said. “So my life expectancy is maybe this summer. I expect to be checking out from this domain sometime this summer.” </p>	
	



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Scott Adams — I have the exact same cancer as Joe Biden, and it has spread to my bones. I expect to die this summer.<a href="https://t.co/uBrYMihTg5">pic.twitter.com/uBrYMihTg5</a></p>— Citizen Free Press (@CitizenFreePres) <a href="https://twitter.com/CitizenFreePres/status/1924477866915315722?ref_src=twsrc%5Etfw">May 19, 2025</a></blockquote>
</div></figure>



<p>The 67-year-old Adams first gained fame as the creator of “Dilbert,” the satirical comic strip that focused on corporate office inanity, in 1989. </p>



<p>Adams has written dozens of books since then, and has become more outspoken about politics in the last decade, sharing views that are mostly pro-Donald Trump and critical of Democrats, on social media. His Rumble show has 38,000 followers, and on X, he has 1.2 million followers. </p>



<p>Before revealing he also had prostate cancer on Monday’s show, Adams shared his thoughts on the disease. </p>	
	



<p> “If it’s localized and it hasn’t left your prostate, it is 100% curable. But, if it leaves your prostate and spreads to other parts of your body — in this case, Joe Biden has it in his bones — it is not curable. “</p>	
	



<p>His comments came the morning after it was announced Biden was battling an <a href="https://www.thewrap.com/president-joe-biden-diagnosed-aggressive-prostate-cancer/">“aggressive” form of prostate cancer. </a></p>



<p>“I’d like to extend my respect and compassion and sympathy for the ex president and his family, because they’re going to be going through an especially tough time,” Adams added. </p>


<div>
		<p><a href="https://www.thewrap.com/morning-joe-biden-had-cancer-as-president-doctor-says/">
							<img loading="lazy" decoding="async" width="300" height="169" src="https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?fit=300%2C169&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?w=1200&amp;quality=80&amp;ssl=1 1200w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=300%2C169&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=1024%2C576&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=768%2C432&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=320%2C180&amp;quality=80&amp;ssl=1 320w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=640%2C360&amp;quality=80&amp;ssl=1 640w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=380%2C214&amp;quality=80&amp;ssl=1 380w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=760%2C428&amp;quality=80&amp;ssl=1 760w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=394%2C222&amp;quality=80&amp;ssl=1 394w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=788%2C444&amp;quality=80&amp;ssl=1 788w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=778%2C438&amp;quality=80&amp;ssl=1 778w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=976%2C549&amp;quality=80&amp;ssl=1 976w, https://i0.wp.com/www.thewrap.com/wp-content/uploads/2025/05/morningjoepic.png?resize=990%2C557&amp;quality=80&amp;ssl=1 990w" sizes="auto, (max-width: 300px) 100vw, 300px" data-portal-copyright="TheWrap" data-has-syndication-rights="1">					</a></p>
	</div>




	
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Game theory illustrated by an animated cartoon game (309 pts)]]></title>
            <link>https://ncase.me/trust/</link>
            <guid>44031535</guid>
            <pubDate>Mon, 19 May 2025 16:28:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ncase.me/trust/">https://ncase.me/trust/</a>, See on <a href="https://news.ycombinator.com/item?id=44031535">Hacker News</a></p>
<div id="readability-page-1" class="page">
	<p>loading...</p> <!-- TRANSLATE THIS -->
	
	
















<!-- Core Engine -->













<!-- Simulations -->






<!-- Slides -->











<!-- Main Code -->

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Edit is now open source (237 pts)]]></title>
            <link>https://devblogs.microsoft.com/commandline/edit-is-now-open-source/</link>
            <guid>44031529</guid>
            <pubDate>Mon, 19 May 2025 16:27:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/commandline/edit-is-now-open-source/">https://devblogs.microsoft.com/commandline/edit-is-now-open-source/</a>, See on <a href="https://news.ycombinator.com/item?id=44031529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            
                    </header><!-- .entry-header -->
                <div>
                    <p><img src="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2022/09/Christopher-Nguyen-Photo-96x96.png" alt="Christopher Nguyen">
                    </p>
                    <div>
                                                

                        <p>Product Manager II, Windows Terminal</p>                    </div>
                </div>
    </div><div id="single-wrapper">
    
    <article data-clarity-region="article" id="post-10694">
        <div data-bi-area="body_article" data-bi-id="post_page_body_article">
            <h2>What is Edit?</h2>
<p>Edit is a new command-line text editor in Windows. Edit is open source, so you can <a href="https://github.com/microsoft/edit?tab=readme-ov-file#build-instructions">build</a> the code or <a href="https://github.com/microsoft/edit?tab=readme-ov-file#installation">install</a> the latest version from <a href="https://github.com/microsoft/edit">GitHub</a>!</p>
<p>This CLI text editor will be available to preview in the <a href="https://www.microsoft.com/windowsinsider/">Windows Insider Program</a> in the coming months. After that, it will ship as part of Windows 11!</p>
<h2>How to use Edit</h2>
<p>Open Edit by running <code>edit</code> in the command line or running <code>edit &lt;your-file-name&gt;</code>. With this, you will be able to edit files directly in the command line without context switching.</p>
<p><a href="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/Edit.gif"><img fetchpriority="high" decoding="async" data-src="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/Edit.gif" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABbIAAAOfAQMAAAAnyaccAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAvElEQVR4nO3BgQAAAADDoPlT3+AEVQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPAMmmYAAVAEGq0AAAAASUVORK5CYII=" alt="Edit image" width="1458" height="927"></a></p>
<h2>What are Edit’s features?</h2>
<p>Edit is still in an early stage, but it has several features out of the box. Here are some highlights!</p>
<h2>Lightweight</h2>
<p>Edit is a small, lightweight text editor. It is less than 250kB, which allows it to keep a small footprint in the Windows 11 image.</p>
<h2>Mouse Mode Support</h2>
<p>As a modeless editor with a Text User Interface (TUI), all the menu options in Edit have keybindings (which you can see next to the menu options).</p>
<p><a href="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/MouseModeSupport.gif"><img decoding="async" data-src="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/MouseModeSupport.gif" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABbIAAAOfAQMAAAAnyaccAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAvElEQVR4nO3BgQAAAADDoPlT3+AEVQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPAMmmYAAVAEGq0AAAAASUVORK5CYII=" alt="Mouse Mode Support image" width="1458" height="927"></a></p>
<h2>Open Multiple Files</h2>
<p>You can open multiple files in Edit and switch between them with <kbd>Ctrl+P</kbd> (or by clicking the file list on the lower-right).</p>
<p><a href="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/MultiFileSupport.gif"><img decoding="async" data-src="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/MultiFileSupport.gif" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABZgAAAOfAQMAAAB/tjQFAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAuUlEQVR4nO3BAQ0AAADCoPdPbQ43oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4NsAi+oAAdera20AAAAASUVORK5CYII=" alt="Multi File Support image" width="1432" height="927"></a></p>
<h2>Find &amp; Replace</h2>
<p>You can find and replace text with <kbd>Ctrl+R</kbd> or select Edit &gt; Replace in the TUI menu. There is also Match Case and Regular Expression support as well.</p>
<p><a href="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/Replace.png"><img decoding="async" data-src="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/Replace.png" src="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/Replace.png" alt="Replace image" width="1721" height="333" srcset="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/Replace.png 1721w, https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/Replace-300x58.png 300w, https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/Replace-1024x198.png 1024w, https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/Replace-768x149.png 768w, https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/Replace-1536x297.png 1536w" sizes="(max-width: 1721px) 100vw, 1721px"></a></p>
<h2>Word Wrap</h2>
<p>Edit supports word wrapping. To use Word Wrap, you can use <kbd>Alt+Z</kbd> or select View &gt; Word Wrap on the TUI menu.</p>
<p><a href="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/WordWrapMode.gif"><img decoding="async" data-src="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2025/05/WordWrapMode.gif" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABbIAAAOfAQMAAAAnyaccAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAvElEQVR4nO3BgQAAAADDoPlT3+AEVQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPAMmmYAAVAEGq0AAAAASUVORK5CYII=" alt="Word Wrap Mode image" width="1458" height="927"></a></p>
<h2>Why build another CLI editor?</h2>
<p>What motivated us to build Edit was the need for a default CLI text editor in 64-bit versions of Windows. 32-bit versions of Windows ship with the MS-DOS Edit or, but 64-bit versions do not have a CLI editor installed inbox. From there, we narrowed down our options…</p>
<p>Many of you are probably familiar with the “How do I exit vim?” meme. While it is relatively simple to learn the magic exit incantation, it’s certainly not a coincidence that this often turns up as a stumbling block for new and old programmers.</p>
<p>Because we wanted to avoid this for a built-in default editor, we decided that we wanted a modeless editor for Windows (versus a modal editor where new users would have to remember different modes of operation and how to switch between them).</p>
<p>This unfortunately limited our choices to a list of editors that either had no first-party support for Windows or were too big to bundle them with every version of the OS. As a result, Edit was born.</p>
<h2>Happy Editing!</h2>
<p>Edit will be rolling out to the <a href="https://www.microsoft.com/windowsinsider/">Windows Insider Program</a> in the coming months. Edit is now open source, so you can <a href="https://github.com/microsoft/edit?tab=readme-ov-file#build-instructions">build the code</a> or <a href="https://github.com/microsoft/edit?tab=readme-ov-file#installation">install it</a> from our GitHub repository.</p>
<p>If you have any feedback or questions, please reach out to the team on the official <a href="https://github.com/microsoft/edit">Edit repository</a>!</p>
        </div><!-- .entry-content -->

        <!-- AI Disclaimer -->
            </article>
    
</div><div><!-- Author section -->
            <h2>Author</h2>
            <div><p><img src="https://devblogs.microsoft.com/commandline/wp-content/uploads/sites/33/2022/09/Christopher-Nguyen-Photo-96x96.png" alt="Christopher Nguyen"></p><div><p>Product Manager II, Windows Terminal</p></div></div>        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub Copilot Coding Agent (459 pts)]]></title>
            <link>https://github.blog/changelog/2025-05-19-github-copilot-coding-agent-in-public-preview/</link>
            <guid>44031432</guid>
            <pubDate>Mon, 19 May 2025 16:17:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.blog/changelog/2025-05-19-github-copilot-coding-agent-in-public-preview/">https://github.blog/changelog/2025-05-19-github-copilot-coding-agent-in-public-preview/</a>, See on <a href="https://news.ycombinator.com/item?id=44031432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
<p>Backlog getting you down? Drowning in technical debt? Delegate issues to Copilot so you can focus on the creative, complex, and high-impact work that matters most. Copilot coding agent makes this possible.</p>
<p>Simply assign an issue (or multiple issues) to Copilot just as you would another developer. You can do this from github.com, <a href="https://github.com/mobile">GitHub Mobile</a>, or the <a href="https://cli.github.com/">GitHub CLI</a>. Copilot works in the background, using its own secure cloud-based development environment powered by GitHub Actions. Copilot explores the repository, makes changes, and even validates its work with your tests and linter before it pushes.</p>
<p>Once Copilot is done, it’ll tag you for review. You can ask Copilot to make changes by leaving comments in the pull request. Or, check out the branch locally and continue work in your IDE, with Copilot at your side.</p>

<p>Copilot excels at low-to-medium complexity tasks in well-tested codebases, from adding features and fixing bugs to extending tests, refactoring, and improving documentation. You can even assign multiple issues to Copilot at the same time.</p>
<p>Copilot coding agent is available now for Copilot Pro+ and Copilot Enterprise subscribers. If you’re on Copilot Enterprise, an administrator will need to enable the new <strong>Copilot coding agent</strong> policy before you can get access. Using the agent consumes <a href="https://docs.github.com/en/billing/managing-billing-for-your-products/managing-billing-for-github-actions/about-billing-for-github-actions">GitHub Actions minutes</a> and <a href="https://docs.github.com/en/copilot/managing-copilot/monitoring-usage-and-entitlements/about-premium-requests">Copilot premium requests</a>, starting from entitlements included with your plan.</p>
<p>To get started and see our top tips for getting the best results with Copilot, check out <a href="https://docs.github.com/en/copilot/using-github-copilot/using-copilot-coding-agent-to-work-on-tasks/about-assigning-tasks-to-copilot">our Copilot coding agent documentation</a>.</p>
<p>Starting June 4th, Copilot coding agent will use <a href="https://docs.github.com/en/copilot/managing-copilot/monitoring-usage-and-entitlements/about-premium-requests">one premium request</a> per model request the agent makes. This is a preview feature, and may be changed in the future.</p>
<p>We’d love to hear from you. Join <a href="https://github.com/orgs/community/discussions/159068">the discussion</a> to share your thoughts and ask any questions.</p>
<p><em>Disclaimer: The UI for features in public preview is subject to change.</em></p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[xAI's Grok 3 comes to Microsoft Azure (138 pts)]]></title>
            <link>https://techcrunch.com/2025/05/19/xais-grok-3-comes-to-microsoft-azure/</link>
            <guid>44031387</guid>
            <pubDate>Mon, 19 May 2025 16:14:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/05/19/xais-grok-3-comes-to-microsoft-azure/">https://techcrunch.com/2025/05/19/xais-grok-3-comes-to-microsoft-azure/</a>, See on <a href="https://news.ycombinator.com/item?id=44031387">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<div>
		<figure><img width="1024" height="683" src="https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?w=1024" alt="Satya Nadella, chief executive officer of Microsoft Corp." decoding="async" fetchpriority="high" srcset="https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg 3200w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=150,100 150w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=300,200 300w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=768,512 768w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=680,453 680w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=1200,800 1200w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=1280,853 1280w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=430,287 430w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=720,480 720w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=900,600 900w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=800,533 800w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=1536,1024 1536w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=2048,1365 2048w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=668,445 668w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=563,375 563w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=926,617 926w, https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-681557990.jpg?resize=708,472 708w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><strong>Image Credits:</strong>David Ryder/Bloomberg / Getty Images</figcaption></figure>	</div>
	<div>
						<p><time datetime="2025-05-19T09:00:00-07:00">9:00 AM PDT · May 19, 2025</time></p>											</div>
</div><div>
		
		<div>
			<div>
<p id="speakable-summary">Microsoft on Monday became one of the first hyperscalers to provide managed access to Grok, the AI model developed by billionaire Elon Musk’s AI startup, xAI. </p>

<p>Available through Microsoft’s Azure AI Foundry platform, Grok — specifically <a href="https://techcrunch.com/2025/04/09/elon-musks-ai-company-xai-launches-an-api-for-grok-3/">Grok 3 and Grok 3 mini</a> — will “have all the service-level agreements&nbsp;Azure customers expect from any Microsoft product,” says Microsoft. They’ll also be billed directly by Microsoft, as is the case with the other models hosted in Azure AI Foundry.</p>







<p>When Musk announced Grok several years ago, he pitched the AI model as edgy, unfiltered, and anti-“woke” — in general, willing to answer controversial questions other AI systems simply won’t. He delivered on some of that promise. Told to be vulgar, for example, Grok will happily oblige, spewing colorful language you likely wouldn’t hear from&nbsp;<a href="https://techcrunch.com/2024/11/22/chatgpt-everything-to-know-about-the-ai-chatbot/">ChatGPT</a>.</p>

<p>According to SpeechMap, a benchmark comparing how different models treat sensitive subjects, Grok 3 is <a href="https://techcrunch.com/2025/04/16/theres-now-a-benchmark-for-how-free-an-ai-chatbot-is-to-talk-about-controversial-topics/">among the more permissive models</a>.</p>

<p>Grok, which powers a number of features on X, Musk’s social network, has been the subject of much controversy lately. <a href="https://www.pcmag.com/news/gross-elon-musks-grok-ai-will-undress-photos-of-women-on-x-if-you-ask" target="_blank" rel="noreferrer noopener nofollow">A recent report</a>&nbsp;found that Grok would undress photos of women when asked. In February, Grok&nbsp;<a href="https://techcrunch.com/2025/02/23/grok-3-appears-to-have-briefly-censored-unflattering-mentions-of-trump-and-musk/">briefly censored</a>&nbsp;unflattering mentions of Donald Trump and Musk. And just last week, an “unauthorized modification” caused Grok to&nbsp;<a href="https://techcrunch.com/2025/05/14/grok-is-unpromptedly-telling-x-users-about-south-african-genocide/">repeatedly refer</a> to&nbsp;white genocide in South Africa when invoked in certain contexts. </p>

<p>The Grok 3 and Grok 3 mini models in Azure AI Foundry are decidedly more locked down than the Grok models on X. They also come with additional data integration, customization, and governance capabilities not necessarily offered by xAI through <a href="https://techcrunch.com/2025/04/09/elon-musks-ai-company-xai-launches-an-api-for-grok-3/">its API</a>.</p>
</div>

			

			


			
			
			

			




			
			
			

			



			
<div>
	
	
	
	

	
<div>
	<p>
		Kyle Wiggers is TechCrunch’s AI Editor. His writing has appeared in VentureBeat and Digital Trends, as well as a range of gadget blogs including Android Police, Android Authority, Droid-Life, and XDA-Developers. He lives in Manhattan with his partner, a music therapist.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/kyle-wiggers/" data-event="button" href="https://techcrunch.com/author/kyle-wiggers/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div>


		</div>
		

		
		<div id="wp-block-techcrunch-most-popular-posts__heading">
<h2 id="h-most-popular">Most Popular</h2>

</div>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Windows Subsystem for Linux is now open source (1296 pts)]]></title>
            <link>https://blogs.windows.com/windowsdeveloper/2025/05/19/the-windows-subsystem-for-linux-is-now-open-source/</link>
            <guid>44031385</guid>
            <pubDate>Mon, 19 May 2025 16:14:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.windows.com/windowsdeveloper/2025/05/19/the-windows-subsystem-for-linux-is-now-open-source/">https://blogs.windows.com/windowsdeveloper/2025/05/19/the-windows-subsystem-for-linux-is-now-open-source/</a>, See on <a href="https://news.ycombinator.com/item?id=44031385">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-index="0" data-js="panel" data-type="wysiwyg-with-aside" data-modular-content="" data-modular-content-collection="">
<p>Today we’re very excited to announce the open-source release of the Windows Subsystem for Linux. This is the result of a multiyear effort to prepare for this, and a great closure to the first ever issue raised on the Microsoft/WSL repo: <a href="https://github.com/microsoft/WSL/issues/1">Will this be Open Source? · Issue #1 · microsoft/WSL</a>.</p>
<p>That means that the code that powers WSL is now available on GitHub at <a href="https://github.com/microsoft/WSL/releases/tag/2.5.7">Microsoft/WSL</a> and open sourced to the community! You can download WSL and build it from source, add new fixes and features and participate in WSL’s active development.</p>
<h3>WSL component overview</h3>
<p>WSL is made of a set of distribution components. Some run in Windows, and some run inside the WSL 2 virtual machine. Here’s an overview of WSL’s architecture:</p>
<figure><p><img fetchpriority="high" decoding="async" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/05/wsl-architecture.png" alt="Windows Subsystem for Linux architecture." width="1000" height="934"></p></figure>
<p>WSL’s code can be broken up into these main areas:</p>
<ul>
<li>Command line executables that are the entry points to interact with WSL
<ul>
<li>wsl.exe, wslconfig.exe and wslg.exe</li>
</ul>
</li>
<li>The WSL service that starts the WSL VM, starts distros, mounts file access shares and more
<ul>
<li>wslservice.exe</li>
</ul>
</li>
<li>Linux init and daemon processes, binaries that run in Linux to provide WSL functionality
<ul>
<li>init for start up, gns for networking, localhost for port forwarding, etc.</li>
</ul>
</li>
<li>File sharing Linux files to Windows with WSL’s plan9 server implementation
<ul>
<li>plan9</li>
</ul>
</li>
</ul>
<p>Head over to <a href="https://wsl.dev/">https://wsl.dev</a> to learn more about each component.</p>
<p>This comes as an addition to the already open sourced WSL components:</p>
<ul>
<li><a href="https://github.com/microsoft/wslg">microsoft/wslg: Enabling the Windows Subsystem for Linux to include support for Wayland and X server related scenarios</a></li>
</ul>
<ul>
<li><a href="https://github.com/microsoft/WSL2-Linux-Kernel">microsoft/WSL2-Linux-Kernel: The source for the Linux kernel used in Windows Subsystem for Linux 2 (WSL2)</a></li>
</ul>
<p>The following components are still part of the Windows image and are not open sourced at this time:</p>
<ul>
<li>Lxcore.sys, the kernel side driver that powers WSL 1</li>
</ul>
<ul>
<li>P9rdr.sys and p9np.dll, which runs the “\\wsl.localhost” filesystem redirection (from Windows to Linux)</li>
</ul>
<h3>Why open source now? A bit of history…</h3>
<p>WSL was first announced at BUILD back in 2016 and first shipped with the Windows 10&nbsp;Anniversary update.</p>
<p>At that time WSL was based on a pico process provider, lxcore.sys, which enabled Windows to natively run ELF executables, and implement Linux syscalls inside the Windows kernel. This eventually became what we today know as “WSL 1”, which WSL still supports.</p>
<p>Over time it became clear that the best way to provide optimal compatibility with native Linux was to rely on the Linux kernel itself. WSL 2 was born, and first announced in 2019.</p>
<p>As the community behind WSL grew, WSL gained more features such as GPU support, graphical applications support (via wslg) and support for systemd.</p>
<p>It eventually became clear that to keep up with the growing community and feature requests, WSL had to move faster, and ship separately from Windows. That’s why in 2021 we separated WSL from the Windows codebase, and moved it to its own codebase. This new WSL first shipped as version 0.47.1 to the Microsoft Store, in July 2021. At the time, only Windows 11 was supported, and the package was marked as preview, only recommended to users that wanted to experience the latest and greatest of WSL.</p>
<p>We continued to develop this new “WSL package” until it was ready for general availability. That happened November of 2022, with WSL 1.0.0, which added support for Windows 10 and was the first “stable” release of this new WSL.</p>
<p>From there we kept on improving WSL, with the objective of fully transitioning all users to this new WSL package, and away from the WSL component that shipped with Windows. Windows 11 24H2 was the first Windows build that moved users from the “built-in” WSL to the “new” WSL package. We kept wsl.exe in the Windows image, so it could download the latest package on demand to make the transition easier.</p>
<p>As we kept on improving WSL, we eventually hit another milestone: WSL 2.0.0 (What are the three hardest problems in computer science? Off by one errors and naming things!).</p>
<p>WSL 2.0.0 introduced major improvements such as mirrored networking, DNS tunneling, session 0 support, proxy support, firewall support and more.</p>
<p>And that’s the milestone we’re still building on today! At the time of writing this article, WSL <a href="https://github.com/microsoft/WSL/releases/tag/2.5.7">2.5.7</a> is the latest available version out of our <a href="https://github.com/microsoft/WSL/releases/tag/2.5.7">nine pages of Github releases</a> since 0.47.1 4 years ago !</p>
<h3>The community behind WSL</h3>
<p>Over the years we’ve been incredibly lucky to have a strong community supporting WSL from day 1. We’ve been blessed with people sharing their knowledge, and spending countless hours to help track down bugs, find the best ways to implement new features and improve WSL.</p>
<p>WSL could never have been what it is today without its community. Even without access to WSL’s source code, people have been able to make major contributions that lead to what WSL is now.</p>
<p>This is why we’re incredibly excited to open-source WSL today. We’ve seen how much the community has contributed to WSL without access to the source code, and we can’t wait to see how WSL will evolve now that the community can make direct code contributions to the project.</p>
<h3>Contributing to WSL</h3>
<p>Are you interested in learning how WSL works? Would you like to see how a specific feature works, or make a change? Head over to <a href="https://github.com/microsoft/WSL">microsoft/WSL</a> to learn more!</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Open Sources Copilot (105 pts)]]></title>
            <link>https://code.visualstudio.com/blogs/2025/05/19/openSourceAIEditor</link>
            <guid>44031344</guid>
            <pubDate>Mon, 19 May 2025 16:09:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://code.visualstudio.com/blogs/2025/05/19/openSourceAIEditor">https://code.visualstudio.com/blogs/2025/05/19/openSourceAIEditor</a>, See on <a href="https://news.ycombinator.com/item?id=44031344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
        <!-- left nav -->
        <div>
            <nav id="docs-navbar" aria-label="Blog posts">
            	<h4>Blog posts</h4>
            	<ul>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2025/05/19/openSourceAIEditor" aria-label="Current Page: Open Source AI Editor">Open Source AI Editor</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2025/05/12/agent-mode-meets-mcp">Adding MCP in VS Code</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2025/04/07/agentMode">Agent mode available to all users</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2025/03/26/custom-instructions">Better AI results with custom instructions</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode">Copilot Agent Mode (preview)</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2025/02/12/next-edit-suggestions">Copilot Next Edit Suggestions (preview)</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2024/12/18/free-github-copilot">Announcing Copilot Free in VS Code</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2024/11/15/introducing-github-copilot-for-azure">GitHub Copilot for Azure</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2024/11/12/introducing-copilot-edits">Introducing Copilot Edits</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2024/06/24/extensions-are-all-you-need">Copilot extensions are all you need</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2024/06/07/wasm-part2">VS Code Extensions and WebAssembly - Part Two</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2024/05/08/wasm">VS Code Extensions and WebAssembly</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2024/04/15/vscode-day">VS Code Day 2024</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2023/11/13/vscode-copilot-smarter">Pursuit of wicked smartness in VS Code</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2023/07/20/mangling-vscode">Shrinking VS Code with name mangling</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2023/06/05/vscode-wasm-wasi">VS Code and WebAssemblies</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2023/04/13/vscode-day">VS Code Day</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2023/03/30/vscode-copilot">VS Code and Copilot</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2022/12/07/remote-even-better">Remote Development Even Better</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2022/11/28/vscode-sandbox">VS Code Sandboxing</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2022/10/04/vscode-community-discussions">VS Code Community Discussions</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2022/09/15/dev-container-features">Dev Container Features</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2022/08/16/markdown-language-server">Markdown Language Server</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2022/07/07/vscode-server">The VS Code Server</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2022/05/18/dev-container-cli">Dev container CLI</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2022/04/04/increase-productivity-with-containers">Moving from Local to Remote Development</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2022/03/08/the-tutorial-problem">The problem with tutorials</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2021/11/08/custom-notebooks">Custom Notebooks</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2021/10/20/vscode-dev">vscode.dev</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2021/10/11/webview-ui-toolkit">Webview UI Toolkit</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2021/09/29/bracket-pair-colorization">Bracket Pair Colorization</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2021/08/05/notebooks">Notebooks</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2021/07/06/workspace-trust">Workspace Trust</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2021/06/10/remote-repositories">Remote Repositories</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2021/06/02/build-2021">Build 2021</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2021/02/16/extension-bisect">Extension bisect</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2020/12/03/chromebook-get-started">VS Code on Chromebook</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2020/07/27/containers-edu">Development Containers in Education</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2020/07/01/containers-wsl">Dev Containers in WSL 2</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2020/06/09/go-extension">The Go experience</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2020/05/14/vscode-build-2020">VS Code at Build</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2020/05/06/github-issues-integration">GitHub Issues Integration</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2020/03/02/docker-in-wsl2">Docker in WSL 2</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2020/02/24/custom-data-format">Custom Data Format</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2020/02/18/optimizing-ci">Improving CI Build Times</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2019/10/31/inspecting-containers">Inspecting Containers</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2019/10/03/remote-ssh-tips-and-tricks">SSH Tips and Tricks</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2019/09/03/wsl2">WSL 2</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2019/07/25/remote-ssh">Remote SSH</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2019/05/23/strict-null">Strict null checking</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2019/05/02/remote-development">Remote Development</a>
            			</li>
            		
            			<li>
            				<a href="https://code.visualstudio.com/blogs/2019/02/19/lsif">Language Server Index Format</a>
            			</li>
            		
            	</ul>
            </nav>
            <nav id="small-nav" aria-label="Blog posts">
            	<label for="small-nav-dropdown">Blogs</label>
            	
            </nav>        </div>

        <!-- small right nav -->
        
        <p>
            <nav aria-labelledby="small-right-nav-label">
                <label for="small-right-nav-dropdown" id="small-right-nav-label">In this blog post</label>
                
            </nav>
        </p>
        

        <!-- main content -->
        <main>
            
<p>May 19th, 2025 by the VS Code team</p>
<p>We believe that the future of code editors should be open and powered by AI. For the last decade, VS Code has been one of the <a href="https://github.blog/news-insights/octoverse/octoverse-2024/#the-state-of-open-source" target="_blank">most successful OSS projects on GitHub</a>. We are grateful for our vibrant community of contributors and users who choose VS Code because it is open source. As AI becomes core to the developer experience in VS Code, we intend to stay true to our founding development principles: open, collaborative, and community-driven.</p>
<p>We will open source the code in the <a href="https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat" target="_blank">GitHub Copilot Chat extension</a> under the MIT license, then carefully refactor the relevant components of the extension into VS Code core. This is the next and logical step for us in making <strong>VS Code an open source AI editor</strong>. It’s a reflection that AI-powered tools are core to how we write code; a reaffirmation of our belief that working in the open leads to a better product for our users and fosters a diverse ecosystem of extensions.</p>
<h2 id="_why-open-source-now" data-needslink="_why-open-source-now">Why open source now?</h2>
<p>Over the last few months, we’ve observed shifts in AI development that motivated us to transition our AI development in VS Code from closed to open source:</p>
<ul>
<li>Large language models have significantly improved, mitigating the need for “secret sauce” prompting strategies.</li>
<li>The most popular and effective UX treatments for AI interactions are now common across editors. We want to enable the community to refine and build on these common UI elements by making them available in a stable, open codebase.</li>
<li>An ecosystem of open source AI tools and VS Code extensions has emerged. We want to make it easier for these extension authors to build, debug, and test their extensions. This is especially challenging today without access to the source code in the Copilot Chat extension.</li>
<li>We’ve gotten a lot of questions about the data that is collected by AI editors. Open sourcing the Copilot Chat extension enables you to see the data we collect, increasing transparency.</li>
<li>Malicious actors are increasingly targeting AI developer tools. Throughout VS Code’s history as OSS, community issues and PRs have helped us find and fix security issues quickly.</li>
</ul>
<h2 id="_next-steps" data-needslink="_next-steps">Next steps</h2>
<p>In the coming weeks, we will work to open source the code in the <a href="https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat" target="_blank">GitHub Copilot Chat extension</a> and refactor AI features from the extension into VS Code core. Our core priorities remain intact: delivering great performance, powerful extensibility, and an intuitive, beautiful user interface.</p>
<p>Open source works best when communities build around a stable, shared foundation. Thus, our goal is to make contributing AI features as simple as contributing to any part of VS Code. The stochastic nature of large language models makes it especially challenging to test AI features and prompt changes. To ease this, we will also make our prompt test infrastructure open source to ensure that community PRs can build and pass tests.</p>
<p>As usual, you can follow along on <a href="https://github.com/microsoft/vscode/issues/248627" target="_blank">our iteration plan</a>, where we will provide more information on this work. We will also keep <a href="https://code.visualstudio.com/docs/supporting/FAQ">our FAQ</a> updated with answers to questions from the community. <a href="https://github.com/microsoft/vscode/issues" target="_blank">We welcome your feedback</a> as we bring this vision to life.</p>
<p>We’re excited to shape the future of development as an open source AI editor - and we hope you’ll join us on this journey to build in the open.</p>
<p>Happy coding!</p>
<p>The VS Code team</p>

        </main>

        <!-- medium right nav -->
        
        <!-- end of page connect widget -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[European Investment Bank to inject €70B in European tech (286 pts)]]></title>
            <link>https://ioplus.nl/en/posts/european-investment-bank-to-inject-70-billion-in-european-tech</link>
            <guid>44031297</guid>
            <pubDate>Mon, 19 May 2025 16:05:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ioplus.nl/en/posts/european-investment-bank-to-inject-70-billion-in-european-tech">https://ioplus.nl/en/posts/european-investment-bank-to-inject-70-billion-in-european-tech</a>, See on <a href="https://news.ycombinator.com/item?id=44031297">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The European Investment Bank (EIB) will invest <a href="https://www.eib.org/en/press/news/president-calvino-tech-firms-innovators-handelsblatt">€70 billion</a> into Europe's technology sector by 2027, aiming to close the innovation gap with the United States. This initiative seeks to strengthen Europe's position in emerging technologies like artificial intelligence and military drones and to draw increased private investment, potentially unlocking €250 billion for the sector. EIB President Nadia Calviño emphasizes the bank's willingness to take more risks, notably speeding up the venture capital financing process, which could be pivotal for startups in a fast-moving market</p><p>The EIB plans to launch this initiative, dubbed TechEU, later this year, creating a centralized hub for financing requests from researchers and companies. This initiative aims to streamline the process, making EU funding processes faster and simpler. A quicker decision-making process can differentiate between the survival and failure of startups navigating tight cash flows and competitive markets.</p><p>In an <a href="https://www.handelsblatt.com/politik/international/eu-foerderbank-eib-will-70-milliarden-fuer-tech-firmen-und-forscher-bereitstellen/100127812.html">interview</a> with German business newspaper <em>Handelsblatt,</em> Calviño has emphasized a newfound willingness to embrace risk within the EIB's financing strategies. The bank aims to process startup financing applications within six months, significantly improving from the current 18-month timespan. Calviño describes this accelerated timeline as a 'gamechanger,' pointing out that the high-paced nature of tech innovation requires nimble response times to keep up with market dynamics.</p><h2>Investment climate in Europe</h2><p>Drawing on the current geopolitical landscape, Calviño sees the uncertainty generated by US President Donald Trump's economic policies as an opportunity for Europe. This environment increasingly attracts international investors interested in the stability and potential offered by the European market. The EIB aims to position itself as a beacon of stability and innovation, leveraging Europe's large market and academic prowess to bolster technological advancement.</p><p>Furthermore, the EIB has prioritized defense and security within its portfolio, recognizing its synergies with technological advancements. This approach acknowledges that investments in these sectors can stimulate technological development and fortify Europe’s technological agenda. As the EIB funds projects across various tech domains, it develops a comprehensive ecosystem where tech innovation is both shielded and nurtured.</p><p>With plans to co-invest with private investors, the EIB aspires to inspire confidence and mitigate risk through its backing, potentially catalyzing up to €250 billion in the European tech ecosystem. As the EIB awaits approval from the 27 EU finance ministers, this initiative underscores Europe's commitment to closing the innovation gap with the US and asserting its role as a global tech leader. The approval is expected to take place next month.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Too Much Go Misdirection (163 pts)]]></title>
            <link>https://flak.tedunangst.com/post/too-much-go-misdirection</link>
            <guid>44031009</guid>
            <pubDate>Mon, 19 May 2025 15:40:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flak.tedunangst.com/post/too-much-go-misdirection">https://flak.tedunangst.com/post/too-much-go-misdirection</a>, See on <a href="https://news.ycombinator.com/item?id=44031009">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Poking through layers of indirection in <b>go</b> trying to recover some efficiency.</p><p>Many functions in <b>go</b> take an <code>io.Reader</code> interface as input. This is a sensible default, allowing for streaming of data, instead of loading the entirety into memory. In some cases, though, we need the bytes. Which we may already have, in which case it would be great to simply use those bytes. Alas, this can be quite difficult.</p><h3 id="context">context</h3><p>I’m decoding some images. I’m using <i>libavif</i> and <i>libheif</i> via C bindings. For reasons primarily motivated by simplicity, I’m using the simple memory interfaces for these libraries, which makes it much easier to get the data from go into C. The streaming interface is much more work, and anyway the libraries would then just buffer the data internally, making another copy. Not every decoder fully works in a streaming fashion.</p><p>So the primary do the work function takes a <code>[]byte</code> and passes it to C, and there’s a wrapper that does things the go way with an <code>io.Reader</code>, which does a full read into a temporary buffer before sending it along. Now, as it happens, my application also uses <code>[]byte</code> internally because that’s what I’m getting out of <i>libsqlite3</i> (because again, the streaming interface is much trickier to wire up) and also because that’s what you get when doing RPC with <i>encoding/gob</i>. I think this is not an unusual scenario.</p><h3 id="bytes">bytes</h3><p>What I would like is for my image decoding function to notice that the <code>io.Reader</code> it has been given is in fact a <code>bytes.Reader</code> so we can skip the copy. Anyone who’s spent any time looking around in the go standard library has noticed that similar shortcuts are commonplace. Interfaces are type checked against specific implementations, and then optimized code paths are taken. Well, we can do the check, but it doesn’t immediately help, because <code>bytes.Reader</code> doesn’t expose its internal byte slice.</p><p>But it’s in there somewhere and I will not be denied.</p><pre><code>    <span>if</span> br<span>,</span> ok <span>:=</span> r<span>.</span><span>(</span><span>*</span>bytes<span>.</span>Reader<span>)</span>; ok <span>{</span>
        data <span>=</span> <span>*</span><span>(</span><span>*</span>[]<span>byte</span><span>)</span><span>(</span>unsafe<span>.</span>Pointer<span>(</span>br<span>)</span><span>)</span>
    <span>}</span> <span>else</span> <span>{</span>
        <span>var</span> buf bytes<span>.</span>Buffer
        io<span>.</span>Copy<span>(</span>&amp;buf<span>,</span> r<span>)</span>
        data <span>=</span> buf<span>.</span>Bytes<span>(</span><span>)</span>
    <span>}</span></code></pre><p>This seems to work in simple tests, but not when using the <code>image.Decode</code> function. A copy is still made. What’s wrong?</p><pre><code><span>func</span> Decode<span>(</span>r io<span>.</span>Reader<span>)</span> <span>(</span>Image<span>,</span> <span>string</span><span>,</span> error<span>)</span> <span>{</span>
    rr <span>:=</span> asReader<span>(</span>r<span>)</span>
<span>}</span>
<span>func</span> asReader<span>(</span>r io<span>.</span>Reader<span>)</span> reader <span>{</span>
    <span>if</span> rr<span>,</span> ok <span>:=</span> r<span>.</span><span>(</span>reader<span>)</span>; ok <span>{</span>
        <span>return</span> rr
    <span>}</span>
    <span>return</span> bufio<span>.</span>NewReader<span>(</span>r<span>)</span>
<span>}</span>
<span>type</span> reader <span>interface</span> <span>{</span>
    io<span>.</span>Reader
    Peek<span>(</span><span>int</span><span>)</span> <span>(</span>[]<span>byte</span><span>,</span> error<span>)</span>
<span>}</span></code></pre><p>Turns out the go image library does its own type inspection, looking for a <code>Peek</code> function, and if it’s not found, wraps the reader in a <code>bufio.Reader</code> instead. So the <code>bytes.Reader</code> never makes it into our function as is.</p><p>Now, why doesn’t <code>bytes.Reader</code> implement <code>Peek</code>? It’s just a byte slice, it’s definitely possible to peek ahead without altering stream state. But it was overlooked, and instead this workaround is applied.</p><p>Just knowing that we have a <code>bufio.Reader</code> isn’t sufficient, because, again, it doesn’t expose the underlying reader to us. It’s fine, okay, whatever. I am a master of unception.</p><pre><code><span>type</span> bufioReader <span>struct</span> <span>{</span>
    buf []<span>byte</span>
    rd  io<span>.</span>Reader
<span>}</span>
    <span>if</span> br<span>,</span> ok <span>:=</span> r<span>.</span><span>(</span><span>*</span>bufio<span>.</span>Reader<span>)</span>; ok <span>{</span>
        insides <span>:=</span> <span>(</span><span>*</span>bufioReader<span>)</span><span>(</span>unsafe<span>.</span>Pointer<span>(</span>br<span>)</span><span>)</span>
        r <span>=</span> insides<span>.</span>rd
    <span>}</span></code></pre><p>The new procedure is to look for a <code>bufio.Reader</code> and if so, unpack the inner reader. And then, as before, if it’s a <code>bytes.Reader</code>, we extract the bytes. The zero copy dream is alive.</p><h3 id="trees">trees</h3><p>The <code>bufio.Reader</code> should <i>probably</i> expose the underling reader.</p><p>The <code>bytes.Reader</code> should really implement <code>Peek</code>. I’m pretty sure the reason it doesn’t is because this is the only way of creating read only views of slices. And a naughty user could peek at the bytes and then modify them. Sigh. People hate const poisoning, but I hate this more.</p><p><code>bytes.Buffer</code> provides a <code>Bytes</code> function, but still not <code>Peek</code>, so even if you know that’s required or useful, it’s not a simple swap.</p><h3 id="forest">forest</h3><p>I’ve said this before, but the way <b>go</b> does structural typing, and the way the standard library uses it, creates these shadow APIs where blessed types work better than others. It’s almost never documented what the secret requirements are. But can you blame me for wanting to join the party?</p><p>I think two interpretations are possible. Casting was added to the language, and it’s used throughout the standard library, thus proving the feature is useful. Or pessimistically, every cast is a design oversight. The approach (in general, not my specific wizardry) only scales to the extent people stick with the standard types. It’s obviously not going to be feasible to specialize on third party types.
</p></div><p>
Posted 19 May 2025 14:45 by tedu Updated: 19 May 2025 14:45 
<br>Tagged: <a href="https://flak.tedunangst.com/t/go">go</a> <a href="https://flak.tedunangst.com/t/programming">programming</a>
</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discover is now part of Capital One (102 pts)]]></title>
            <link>https://www.discover.com/faqs/merger/</link>
            <guid>44031000</guid>
            <pubDate>Mon, 19 May 2025 15:39:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.discover.com/faqs/merger/">https://www.discover.com/faqs/merger/</a>, See on <a href="https://news.ycombinator.com/item?id=44031000">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content-rwd" role="main">

<div>


 <h2><b>Frequently asked questions</b></h2>
<p>We’re pleased to announce that on May 18, 2025, Discover Bank merged into Capital One, N.A. (“Capital One”). If you have any questions about credit cards, online banking accounts, or loans, we’re here to answer them.</p>
 
</div>
<div>


 <p>We’ll continue to keep you updated, and you can always check back here for more information. If you still have questions, our same friendly U.S.-based service is here to help by phone or online chat.</p>
 
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[23andMe Sells Gene-Testing Business to DNA Drug Maker Regeneron (201 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-05-19/23andme-sells-gene-testing-business-to-dna-drug-maker-regeneron</link>
            <guid>44030873</guid>
            <pubDate>Mon, 19 May 2025 15:27:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-05-19/23andme-sells-gene-testing-business-to-dna-drug-maker-regeneron">https://www.bloomberg.com/news/articles/2025-05-19/23andme-sells-gene-testing-business-to-dna-drug-maker-regeneron</a>, See on <a href="https://news.ycombinator.com/item?id=44030873">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-05-19/23andme-sells-gene-testing-business-to-dna-drug-maker-regeneron: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Zod 4 (604 pts)]]></title>
            <link>https://zod.dev/v4</link>
            <guid>44030850</guid>
            <pubDate>Mon, 19 May 2025 15:24:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zod.dev/v4">https://zod.dev/v4</a>, See on <a href="https://news.ycombinator.com/item?id=44030850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>After a year of active development: Zod 4 is now stable! It's faster, slimmer, more <code>tsc</code>-efficient, and implements some long-requested features.</p>
<div><p>❤️</p><div><p>Huge thanks to <a href="https://go.clerk.com/zod-clerk" rel="noreferrer noopener" target="_blank">Clerk</a>, who supported my work on Zod 4 through their extremely generous <a href="https://clerk.com/blog/zod-fellowship" rel="noreferrer noopener" target="_blank">OSS Fellowship</a>. They were an amazing partner throughout the (much longer than anticipated!) development process.</p></div></div>
<p>To simplify the migration process both for users and Zod's ecosystem of associated libraries, Zod 4 is being published alongside Zod 3 as part of the <code>zod@3.25</code> release. To upgrade:</p>
<figure tabindex="0"></figure>
<p>Then import Zod 4 from the <code>"/v4"</code> subpath:</p>
<figure tabindex="0"></figure>
<p>Refer to the <a href="https://zod.dev/v4/changelog">Migration guide</a> for a complete list of breaking changes. This page covers the new features and improvements.</p>
<!-- -->

<p>Zod v3.0 was released in May 2021 (!). Back then Zod had 2700 stars on GitHub and 600k weekly downloads. Today it has 37.8k stars and 31M weekly downloads (up from 23M when the beta came out 6 weeks ago!). After 24 minor versions, the Zod 3 codebase had hit a ceiling; the most commonly requested features and improvements require breaking changes.</p>
<p>Zod 4 fixes a number of long-standing design limitations of Zod 3 in one fell swoop, paving the way for several long-requested features and a huge leap in performance. It closes 9 of Zod's <a href="https://github.com/colinhacks/zod/issues?q=is%3Aissue%20state%3Aopen%20sort%3Areactions-%2B1-desc" rel="noreferrer noopener" target="_blank">10 most upvoted open issues</a>. With luck, it will serve as the new foundation for many more years to come.</p>
<p>For a scannable breakdown of what's new, see the table of contents. Click on any item to jump to that section.</p>

<p>You can run these benchmarks yourself in the Zod repo:</p>
<figure tabindex="0"></figure>
<p>Then to run a particular benchmark:</p>
<figure tabindex="0"></figure>
<h3 id="14x-faster-string-parsing"><a data-card="" href="https://zod.dev/v4?id=14x-faster-string-parsing">14x faster string parsing</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<figure tabindex="0"></figure>
<h3 id="3x-faster-array-parsing"><a data-card="" href="https://zod.dev/v4?id=3x-faster-array-parsing">3x faster array parsing</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<figure tabindex="0"></figure>
<h3 id="65x-faster-object-parsing"><a data-card="" href="https://zod.dev/v4?id=65x-faster-object-parsing">6.5x faster object parsing</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>This runs the <a href="https://moltar.github.io/typescript-runtime-type-benchmarks/" rel="noreferrer noopener" target="_blank">Moltar validation library benchmark</a>.</p>
<figure tabindex="0"></figure>

<p>Consider the following simple file:</p>
<figure tabindex="0"></figure>
<p>Compiling this file with <code>tsc --extendedDiagnostics</code> using <code>"zod/v3"</code> results in &gt;25000 type instantiations. With <code>"zod/v4"</code> it only results in ~175.</p>
<div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><path d="M12 16v-4"></path><path d="M12 8h.01"></path></svg><div><p>The Zod repo contains a <code>tsc</code> benchmarking playground. Try this for yourself using the compiler benchmarks in <code>packages/tsc</code>. The exact numbers may change as the implementation evolves.</p><figure tabindex="0"></figure></div></div>
<p>More importantly, Zod 4 has redesigned and simplified the generics of <code>ZodObject</code> and other schema classes to avoid some pernicious "instantiation explosions". For instance, chaining <code>.extend()</code> and <code>.omit()</code> repeatedly—something that previously caused compiler issues:</p>
<figure tabindex="0"></figure>
<p>In Zod 3, this took <code>4000ms</code> to compile; and adding additional calls to <code>.extend()</code> would trigger a "Possibly infinite" error. In Zod 4, this compiles in <code>400ms</code>, <code>10x</code> faster.</p>
<div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><path d="M12 16v-4"></path><path d="M12 8h.01"></path></svg><div><p>Coupled with the upcoming <a href="https://github.com/microsoft/typescript-go" rel="noreferrer noopener" target="_blank"><code>tsgo</code></a> compiler, Zod 4's editor performance will scale to vastly larger schemas and codebases.</p></div></div>

<p>Consider the following simple script.</p>
<figure tabindex="0"></figure>
<p>It's about as simple as it gets when it comes to validation. That's intentional; it's a good way to measure the <em>core bundle size</em>—the code that will end up in the bundle even in simple cases. We'll bundle this with <code>rollup</code> using both Zod 3 and Zod 4 and compare the final bundles.</p>
<div><table><thead><tr><th>Package</th><th>Bundle (gzip)</th></tr></thead><tbody><tr><td><code>zod/v3</code></td><td><code>12.47kb</code></td></tr><tr><td><code>zod/v4</code></td><td><code>5.36kb</code></td></tr></tbody></table></div>
<p>The core bundle is ~57% smaller in Zod 4 (2.3x). That's good! But we can do a lot better.</p>

<p>Zod's method-heavy API is fundamentally difficult to tree-shake. Even our simple <code>z.boolean()</code> script pulls in the implementations of a bunch of methods we didn't use, like <code>.optional()</code>, <code>.array()</code>, etc. Writing slimmer implementations can only get you so far. That's where <code>zod/v4-mini</code> comes in.</p>
<figure tabindex="0"></figure>
<p>It's a Zod variant with a functional, tree-shakable API that corresponds one-to-one with <code>zod</code>. Where Zod uses methods, <code>zod/v4-mini</code> generally uses wrapper functions:</p>

<p>Not all methods are gone! The parsing methods are identical in <code>zod/v4</code> and <code>zod/v4-mini</code>.</p>
<figure tabindex="0"></figure>
<p>There's also a general-purpose <code>.check()</code> method used to add refinements.</p>

<p>The following top-level refinements are available in <code>zod/v4-mini</code>. It should be fairly self-explanatory which methods they correspond to.</p>
<figure tabindex="0"></figure>
<p>This more functional API makes it easier for bundlers to tree-shaking the APIs you don't use. While <code>zod/v4</code> is still recommended for the majority of use cases, any projects with uncommonly strict bundle size constraints should consider <code>zod/v4-mini</code>.</p>
<h3 id="66x-reduction-in-core-bundle-size"><a data-card="" href="https://zod.dev/v4?id=66x-reduction-in-core-bundle-size">6.6x reduction in core bundle size</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>Here's the script from above, updated to use <code>"zod/v4-mini"</code> instead of <code>"zod"</code>.</p>
<figure tabindex="0"></figure>
<p>When we build this with <code>rollup</code>, the gzipped bundle size is <code>1.88kb</code>. That's an 85% (6.6x) reduction in core bundle size compared to <code>zod@3</code>.</p>
<div><table><thead><tr><th>Package</th><th>Bundle (gzip)</th></tr></thead><tbody><tr><td><code>zod/v3</code></td><td><code>12.47kb</code></td></tr><tr><td><code>zod/v4</code></td><td><code>5.36kb</code></td></tr><tr><td><code>zod/v4-mini</code></td><td><code>1.88kb</code></td></tr></tbody></table></div>
<p>Learn more on the dedicated <a href="https://zod.dev/packages/mini"><code>zod/v4-mini</code></a> docs page. Complete API details are mixed into existing documentation pages; code blocks contain separate tabs for <code>"Zod"</code> and <code>"Zod Mini"</code> wherever their APIs diverge.</p>

<p>Zod 4 introduces a new system for adding strongly-typed metadata to your schemas. Metadata isn't stored inside the schema itself; instead it's stored in a "schema registry" that associates a schema with some typed metadata. To create a registry with <code>z.registry()</code>:</p>
<figure tabindex="0"></figure>
<p>To add schemas to your registry:</p>
<figure tabindex="0"></figure>
<p>Alternatively, you can use the <code>.register()</code> method on a schema for convenience:</p>
<!-- -->
<figure tabindex="0"></figure>
<h3 id="the-global-registry"><a data-card="" href="https://zod.dev/v4?id=the-global-registry">The global registry</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>Zod also exports a global registry <code>z.globalRegistry</code> that accepts some common JSON Schema-compatible metadata:</p>
<figure tabindex="0"></figure>
<h3 id="meta"><a data-card="" href="https://zod.dev/v4?id=meta"><code>.meta()</code></a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>To conveniently add a schema to <code>z.globalRegistry</code>, use the <code>.meta()</code> method.</p>
<!-- -->
<figure tabindex="0"></figure>
<div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><path d="M12 16v-4"></path><path d="M12 8h.01"></path></svg><div><p>For compatibility with Zod 3, <code>.describe()</code> is still available, but <code>.meta()</code> is preferred.</p><figure tabindex="0"></figure></div></div>

<p>Zod 4 introduces first-party JSON Schema conversion via <code>z.toJSONSchema()</code>.</p>
<figure tabindex="0"></figure>
<p>Any metadata in <code>z.globalRegistry</code> is automatically included in the JSON Schema output.</p>
<figure tabindex="0"></figure>
<p>Refer to the <a href="https://zod.dev/json-schema">JSON Schema docs</a> for information on customizing the generated JSON Schema.</p>

<p>This was an unexpected on. After years of trying to crack this problem, I finally <a href="https://x.com/colinhacks/status/1919286275133378670" rel="noreferrer noopener" target="_blank">found a way</a> to properly infer recursive object types in Zod. To define a recursive type:</p>
<figure tabindex="0"></figure>
<p>You can also represent <em>mutually recursive types</em>:</p>
<figure tabindex="0"></figure>
<p>Unlike the Zod 3 pattern for recursive types, there's no type casting required. The resulting schemas are plain <code>ZodObject</code> instances and have the full set of methods available.</p>
<figure tabindex="0"></figure>

<p>To validate <code>File</code> instances:</p>
<figure tabindex="0"></figure>

<p>Zod 4 introduces a new <code>locales</code> API for globally translating error messages into different languages.</p>
<figure tabindex="0"></figure>
<p>At the time of this writing only the English locale is available; There will be a call for pull request from the community shortly; this section will be updated with a list of supported languages as they become available.</p>

<p>The popularity of the <a href="https://www.npmjs.com/package/zod-validation-error" rel="noreferrer noopener" target="_blank"><code>zod-validation-error</code></a> package demonstrates that there's significant demand for an official API for pretty-printing errors. If you are using that package currently, by all means continue using it.</p>
<p>Zod now implements a top-level <code>z.prettifyError</code> function for converting a <code>ZodError</code> to a user-friendly formatted string.</p>
<figure tabindex="0"></figure>
<p>This returns the following pretty-printable multi-line string:</p>
<figure tabindex="0"></figure>
<p>Currently the formatting isn't configurable; this may change in the future.</p>

<p>All "string formats" (email, etc.) have been promoted to top-level functions on the <code>z</code> module. This is both more concise and more tree-shakable. The method equivalents (<code>z.string().email()</code>, etc.) are still available but have been deprecated. They'll be removed in the next major version.</p>
<figure tabindex="0"></figure>
<h3 id="custom-email-regex"><a data-card="" href="https://zod.dev/v4?id=custom-email-regex">Custom email regex</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>The <code>z.email()</code> API now supports a custom regular expression. There is no one canonical email regex; different applications may choose to be more or less strict. For convenience Zod exports some common ones.</p>
<figure tabindex="0"></figure>

<p>Zod 4 implements <code>z.templateLiteral()</code>. Template literal types are perhaps the biggest feature of TypeScript's type system that wasn't previously representable.</p>
<figure tabindex="0"></figure>
<p>Every Zod schema type that can be stringified stores an internal regex: strings, string formats like <code>z.email()</code>, numbers, boolean, bigint, enums, literals, undefined/optional, null/nullable, and other template literals. The <code>z.templateLiteral</code> constructor concatenates these into a super-regex, so things like string formats (<code>z.email()</code>) are properly enforced (but custom refinements are not!).</p>
<p>Read the <a href="https://zod.dev/api#template-literals">template literal docs</a> for more info.</p>

<p>New numeric "formats" have been added for representing fixed-width integer and float types. These return a <code>ZodNumber</code> instance with proper minimum/maximum constraints already added.</p>
<figure tabindex="0"></figure>
<p>Similarly the following <code>bigint</code> numeric formats have also been added. These integer types exceed what can be safely represented by a <code>number</code> in JavaScript, so these return a <code>ZodBigInt</code> instance with the proper minimum/maximum constraints already added.</p>
<figure tabindex="0"></figure>

<p>The existing <code>z.coerce.boolean()</code> API is very simple: falsy values (<code>false</code>, <code>undefined</code>, <code>null</code>, <code>0</code>, <code>""</code>, <code>NaN</code> etc) become <code>false</code>, truthy values become <code>true</code>.</p>
<p>This is still a good API, and its behavior aligns with the other <code>z.coerce</code> APIs. But some users requested a more sophisticated "env-style" boolean coercion. To support this, Zod 4 introduces <code>z.stringbool()</code>:</p>
<figure tabindex="0"></figure>
<p>To customize the truthy and falsy values:</p>
<figure tabindex="0"></figure>
<p>Refer to the <a href="https://zod.dev/api#stringbools"><code>z.stringbool()</code> docs</a> for more information.</p>

<p>The majority of breaking changes in Zod 4 involve the <em>error customization</em> APIs. They were a bit of a mess in Zod 3; Zod 4 makes things significantly more elegant, to the point where I think it's worth highlighting here.</p>
<p>Long story short, there is now a single, unified <code>error</code> parameter for customizing errors, replacing the following APIs:</p>
<p>Replace <code>message</code> with <code>error</code>. (The <code>message</code> parameter is still supported but deprecated.)</p>
<figure tabindex="0"></figure>
<p>Replace <code>invalid_type_error</code> and <code>required_error</code> with <code>error</code> (function syntax):</p>
<figure tabindex="0"></figure>
<p>Replace <code>errorMap</code> with <code>error</code> (function syntax):</p>
<figure tabindex="0"></figure>

<p>Discriminated unions now support a number of schema types not previously supported, including unions, pipes, and nested objects:</p>
<figure tabindex="0"></figure>
<p>Perhaps most importantly, discriminated unions now <em>compose</em>—you can use one discriminated union as a member of another.</p>
<figure tabindex="0"></figure>

<p>The <code>z.literal()</code> API now optionally supports multiple values.</p>
<figure tabindex="0"></figure>

<p>In Zod 3, they were stored in a <code>ZodEffects</code> class that wrapped the original schema. This was inconvenient, as it meant you couldn't interleave <code>.refine()</code> with other schema methods like <code>.min()</code>.</p>
<figure tabindex="0"></figure>
<p>In Zod 4, refinements are stored inside the schemas themselves, so the code above works as expected.</p>
<figure tabindex="0"></figure>
<h3 id="overwrite"><a data-card="" href="https://zod.dev/v4?id=overwrite"><code>.overwrite()</code></a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>The <code>.transform()</code> method is extremely useful, but it has one major downside: the output type is no longer <em>introspectable</em> at runtime. The transform function is a black box that can return anything. This means (among other things) there's no sound way to convert the schema to JSON Schema.</p>
<figure tabindex="0"></figure>
<p>Zod 4 introduces a new <code>.overwrite()</code> method for representing transforms that <em>don't change the inferred type</em>. Unlike <code>.transform()</code>, this method returns an instance of the original class. The overwrite function is stored as a refinement, so it doesn't (and can't) modify the inferred type.</p>
<figure tabindex="0"></figure>
<div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><path d="M12 16v-4"></path><path d="M12 8h.01"></path></svg><div><p>The existing <code>.trim()</code>, <code>.toLowerCase()</code> and <code>.toUpperCase()</code> methods have been reimplemented using <code>.overwrite()</code>.</p></div></div>

<p>While this will not be relevant to the majority of Zod users, it's worth highlighting. The addition of <code>zod/v4-mini</code> necessitated the creation of a shared sub-package <code>zod/v4/core</code> which contains the core functionality shared between <code>zod/v4</code> and <code>zod/v4-mini</code>.</p>
<p>I was resistant to this at first, but now I see it as one of Zod 4's most important features. It lets Zod level up from a simple library to a fast validation "substrate" that can be sprinkled into other libraries.</p>
<p>If you're building a schema library, refer to the implementations of <code>zod/v4</code> and <code>zod/v4-mini</code> to see how to build on top of the foundation <code>zod/v4/core</code> provides. Don't hesitate to get in touch in GitHub discussions or via <a href="https://x.com/colinhacks" rel="noreferrer noopener" target="_blank">X</a>/<a href="https://bsky.app/profile/colinhacks.com" rel="noreferrer noopener" target="_blank">Bluesky</a> for help or feedback.</p>

<p>I'm planning to write up a series of additional posts explaining the design process behind some major features like <code>zod/v4-mini</code>. I'll update this section as those get posted.</p>
<p>For library authors, there is now a dedicated <a href="https://zod.dev/v4/library-authors">For library authors</a> guide that describes the best practices for building on top of Zod. It answers common questions about how to support Zod 3 &amp; Zod 4 (including Mini) simultaneously.</p>
<figure tabindex="0"></figure>
<p>Happy parsing!<br>
— Colin McDonnell <a href="https://x.com/colinhacks" rel="noreferrer noopener" target="_blank">@colinhacks</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Better Auth (YC X25) – Authentication Framework for TypeScript (183 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44030492</link>
            <guid>44030492</guid>
            <pubDate>Mon, 19 May 2025 14:48:03 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44030492">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="44030492">
      <td><span></span></td>      <td><center><a id="up_44030492" href="https://news.ycombinator.com/vote?id=44030492&amp;how=up&amp;goto=item%3Fid%3D44030492"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44030492">Launch HN: Better Auth (YC X25) – Authentication Framework for TypeScript</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_44030492">117 points</span> by <a href="https://news.ycombinator.com/user?id=bekacru">bekacru</a> <span title="2025-05-19T14:48:03 1747666083"><a href="https://news.ycombinator.com/item?id=44030492">3 hours ago</a></span> <span id="unv_44030492"></span> | <a href="https://news.ycombinator.com/hide?id=44030492&amp;goto=item%3Fid%3D44030492">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Better%20Auth%20%28YC%20X25%29%20%E2%80%93%20Authentication%20Framework%20for%20TypeScript&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44030492&amp;auth=2c12b1e23efe4e13e17e25e81675a26506cfdb68">favorite</a> | <a href="https://news.ycombinator.com/item?id=44030492">41&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hi HN! We’re Bereket and KinfeMichael of Better Auth (<a href="https://www.better-auth.com/">https://www.better-auth.com/</a>), a comprehensive authentication framework for TypeScript that lets you implement
everything from simple auth flows to enterprise-grade systems directly on your own database, embedded in your backend.</p><p>To be clear—we’re not building a 3rd party auth service. Our goal is to make rolling your own auth so ridiculously easy that you’ll never need one.</p><p>Here are some YouTube videos explaining how it works (we did make our own video but weren’t happy with it and these videos do a great job):</p><p><a href="https://www.youtube.com/watch?v=hFtufpaMcLM" rel="nofollow">https://www.youtube.com/watch?v=hFtufpaMcLM</a> - a really good overview</p><p><a href="https://www.youtube.com/watch?v=QurjwJHCoHQ" rel="nofollow">https://www.youtube.com/watch?v=QurjwJHCoHQ</a> - also a good overview and dives a little deeper into the code</p><p><a href="https://www.youtube.com/watch?v=RKqHrE0KyeE" rel="nofollow">https://www.youtube.com/watch?v=RKqHrE0KyeE</a> - short and clear</p><p><a href="https://www.youtube.com/watch?v=Atev8Nxpw7c" rel="nofollow">https://www.youtube.com/watch?v=Atev8Nxpw7c</a> - with TanStack framework</p><p><a href="https://www.youtube.com/watch?v=n6rP9d3RWo8" rel="nofollow">https://www.youtube.com/watch?v=n6rP9d3RWo8</a> - a full-on 2 hour tutorial</p><p>Auth has been a pain point for many developers in the TypeScript ecosystem for a while. Not because there aren’t options but because most fall into 2 buckets: (1) Third-party services like Auth0 which own your user data, lock you into a black-box solution and are often super expensive; or (2) open source libraries like NextAuth that cover the basics but leave you stitching your own solution together from there.</p><p>For Better Auth. the kick off moment was building a web analytics platform and wanting to add an organization feature - things like workspaces, teams, members, and granular permissions. I assumed there’d be something out there I could plug in to NextAuth (the popular and kind of the only library), but there wasn’t. The only options were to build everything from scratch or switch to a 3rd party auth provider. I even tried hacking together a wrapper around NextAuth to support those features, but it was hacky. That’s when we decided to take a step back and build a proper auth library from the ground up with a plugin ecosystem that lets you start simple and scale as needed. That frustration turned into Better Auth.</p><p>Better Auth lets you roll your own auth directly on your backend and database, with support for everything from simple auth flows to enterprise-grade systems without relying on 3rd party services.</p><p>It comes with built-in features for common auth flows, and you can extend it as needed through a plugin ecosystem whether that’s 2FA, passkeys, organizations, multi-session, SSO, or even billing integration with Stripe.</p><p>Unlike 3rd party auth providers, we’re just a library you install in your own project. It’s free forever, lives entirely in your codebase, and gives you full control. You get all the features you’d expect from something like Auth0 or Clerk plus even more through our plugin system, including things like billing integrations with Stripe or Polar. Most libraries stop at the basics but Better Auth is designed to scale with your needs while keeping things simple when you don’t need all the extras.</p><p>We’re currently building an infrastructure layer that works alongside the framework to offer features that are hard to deliver as just a library—e.g. an admin dashboard with user analytics, bot/fraud/abuse detection, secondary session storage, and more. This will be our commercial offering. For this, there’s a waitlist at <a href="https://www.better-auth.build/" rel="nofollow">https://www.better-auth.build</a>. However, this is only optional infrastructure for teams that need these capabilities. The library is free and open source and will remain so.</p><p>We’d love your feedback!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fabric Is Just Plain Unreliable, and Microsoft's Hiding It (103 pts)]]></title>
            <link>https://www.brentozar.com/archive/2025/05/fabric-is-just-plain-unreliable-and-microsofts-hiding-it/</link>
            <guid>44029566</guid>
            <pubDate>Mon, 19 May 2025 13:19:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.brentozar.com/archive/2025/05/fabric-is-just-plain-unreliable-and-microsofts-hiding-it/">https://www.brentozar.com/archive/2025/05/fabric-is-just-plain-unreliable-and-microsofts-hiding-it/</a>, See on <a href="https://news.ycombinator.com/item?id=44029566">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text"><p>Last week, <a href="https://www.reddit.com/r/MicrosoftFabric/comments/1km4sxh/fabric_down_again/">Microsoft Fabric went down yet again</a> for hours on multiple continents.</p>
<p>Oh, you didn’t hear about it? Let’s talk about why.</p>
<p><strong>First, Fabric’s status page is fabricated bullshit.</strong> The link <a href="https://aka.ms/fabricsupport">https://aka.ms/fabricsupport</a> takes you to a localized status page that almost always shows all green checkmarks – even when the service is on fire. During <a href="https://www.reddit.com/r/MicrosoftFabric/comments/1k9mmm8/fabric_practically_down/">last month’s 12+hour overnight outage</a>, people were screaming on Reddit overnight that things were down, but the status dashboard was showing all green. When Microsoft employees woke up, they asked if people were still having problems – and then eventually got around to updating the status page to reflect the outage when it was clear that things were really borked.</p>
<p>Redditors have resorted to relying on <a href="https://statusgator.com/services/microsoft-fabric">reporting Fabric outages to Statusgator</a>, who then tracks the time gap between a burst of user outage reports, to the time Microsoft actually updates their status page – and it ain’t pretty:</p>
<p><a href="https://statusgator.com/services/microsoft-fabric"><img fetchpriority="high" decoding="async" src="https://www.brentozar.com/wp-content/uploads/2025/05/status-fabric-600x169.jpg" alt="Microsoft Fabric on StatusGator" width="600" height="169" srcset="https://www.brentozar.com/wp-content/uploads/2025/05/status-fabric-600x169.jpg 600w, https://www.brentozar.com/wp-content/uploads/2025/05/status-fabric-250x70.jpg 250w, https://www.brentozar.com/wp-content/uploads/2025/05/status-fabric-768x216.jpg 768w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p><strong>Second, the post-mortems are just as fabricated.</strong> After last month’s outage, the team <a href="https://www.reddit.com/r/MicrosoftFabric/comments/1kfzigz/comment/mr43att/">posted on Reddit</a>, and opened with this whopper:</p>
<blockquote><p>Fabric/Power BI is deployed in 58+ regions worldwide and serve approximately 400,000 organizations, and 30 million+ business users every month. This outage impacted 4 regions in Europe and the US for about 4 hours.</p></blockquote>
<p>See what they did there? They used big giant numbers to talk about the subscriber base, and then switched units of measure to talk about the affected population (just “the US”.) That’s like saying, “We served over 30 billion hamburgers last month, but unfortunately, just 1 country (the US) came down with food poisoning.” Gimme a break. Furthermore, the 4-hour thing is just&nbsp;<em>wildly</em> incorrect, as evidenced by the people screaming on Reddit overnight and into the morning.</p>
<blockquote><p>The combination of factors that triggered this issue did not occur until we hit specific regions and usage patterns. This was caught at that point through automated alerting, and our incident management team initiated a rollback.</p></blockquote>
<p>Specific regions like, uh, Europe and the United States. You know, small places. Villages, practically.</p>
<p>I absolutely love the second sentence as a world-class example of fabrication. Microsoft is accidentally admitting that their own&nbsp;<em>internal</em> alerting showed that Fabric was broken – but not their&nbsp;<em>external</em> alerting, aka their status page. They’re accidentally showing their cards that the status dashboard just doesn’t show the truth.</p>
<p><strong>Next, Microsoft hides the Fabric outage history as quickly as they can.</strong> The status dashboard has no list of recent outages. I feel genuinely sorry for Fabric admins who struggle troubleshooting failed Fabric processes that were supposed to run overnight. They think it’s their own problem, not realizing that there was an overnight outage that Microsoft has simply swept under the rug as quickly as possible. The admin checks the status page, sees nothing, and continues troubleshooting, thinking it’s their problem.</p>
<p>Contrast this with the <a href="https://azure.status.microsoft/en-us/status">overall Azure status page</a>, which has a prominent link to <a href="https://azure.status.microsoft/en-us/status/history/">Azure status history</a>, publicly calling out major outages and their post-mortems. Microsoft knows how to do this – but the Fabric team ain’t doin’ it.</p>
<p><a href="https://www.brentozar.com/wp-content/uploads/2024/04/Brent_Ozar_Shouting_Left.jpg"><img decoding="async" src="https://www.brentozar.com/wp-content/uploads/2024/04/Brent_Ozar_Shouting_Left-250x245.jpg" alt="" width="250" height="245" srcset="https://www.brentozar.com/wp-content/uploads/2024/04/Brent_Ozar_Shouting_Left-250x245.jpg 250w, https://www.brentozar.com/wp-content/uploads/2024/04/Brent_Ozar_Shouting_Left-408x400.jpg 408w, https://www.brentozar.com/wp-content/uploads/2024/04/Brent_Ozar_Shouting_Left-600x589.jpg 600w, https://www.brentozar.com/wp-content/uploads/2024/04/Brent_Ozar_Shouting_Left-768x754.jpg 768w" sizes="(max-width: 250px) 100vw, 250px"></a>I don’t understand why the Fabric team is so secretive about the outages.</p>
<p>It’s not like Microsoft Fabric even has a service level agreement.</p>
<p>It’s not like they’re giving refunds when your data is gone for hours at a time.</p>
<p>Oh you didn’t realize that?</p>
<p>That brings me to the only reason I can think of that someone would recommend Microsoft Fabric as a critical part of a company’s infrastructure today: <em>ignorance</em>. That’s where the blog post comes in, dear reader – I don’t want you to be ignorant.</p>

<p id="jp-relatedposts">
	<h3><em>Related</em></h3>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diffusion Models Explained Simply (121 pts)]]></title>
            <link>https://www.seangoedecke.com/diffusion-models-explained/</link>
            <guid>44029435</guid>
            <pubDate>Mon, 19 May 2025 13:06:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/diffusion-models-explained/">https://www.seangoedecke.com/diffusion-models-explained/</a>, See on <a href="https://news.ycombinator.com/item?id=44029435">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header></header><section><p>Transformer-based large language models are relatively easy to understand. You break language down into a finite set of “tokens” (words or sub-word components), then train a neural network on millions of token sequences so it can predict the next token based on all the previous ones. Despite some clever tricks (mainly about how the model processes the previous tokens in the sequence), the core mechanism is relatively simple.</p>
<p>It’s harder to build the same kind of intuition about diffusion models (in part because the papers are much harder to read). But diffusion models are almost as big a part of the AI revolution as transformers. High-quality image generation has driven a lot of user interest in AI, particularly ChatGPT’s recent upgraded image generation.</p>
<p>Even if you don’t care much about images, there are also some fairly capable text-based diffusion models - not yet competitive with frontier transformer models, but it’s certainly possible that we’d someday see a diffusion language model that’s state-of-the-art in its niche.</p>
<h3>The core intuition</h3>
<p>So what are diffusion models? How are they different from transformers? What is the animating intuition that makes sense of how diffusion models work?</p>
<p>Imagine a picture of a dog. You could slowly add randomly-colored pixels to that picture - the visual equivalent of “white noise” - until it just looks like noise. You could do the same for any possible image. All those possible images look very different, but the eventual noise looks the same. That means that <strong>for any possible image, there is a gradient of steps between that image and “pure noise”</strong>.</p>
<p><span>
      <a href="https://www.seangoedecke.com/static/b626e5866ab86470fd5e640f73ad085b/a19d2/gaussian-noise.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="gaussian noise" title="gaussian noise" src="https://www.seangoedecke.com/static/b626e5866ab86470fd5e640f73ad085b/1c72d/gaussian-noise.jpg" srcset="https://www.seangoedecke.com/static/b626e5866ab86470fd5e640f73ad085b/a80bd/gaussian-noise.jpg 148w,
https://www.seangoedecke.com/static/b626e5866ab86470fd5e640f73ad085b/1c91a/gaussian-noise.jpg 295w,
https://www.seangoedecke.com/static/b626e5866ab86470fd5e640f73ad085b/1c72d/gaussian-noise.jpg 590w,
https://www.seangoedecke.com/static/b626e5866ab86470fd5e640f73ad085b/a19d2/gaussian-noise.jpg 812w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<p>What if you could train a model to understand that gradient?</p>
<h3>Training and inference</h3>
<p>To train a diffusion model, you take a large set of images, each expressed as a big tensor, and a caption for each image, each expressed as a normal text-model embedding. At each step in the training, for the current image, you add a little bit of random noise. Then you pass that noisy image and caption to the model, and ask it to predict exactly what noise was added to the image (e.g. which pixels changed from what color to what color). Unlike a language model, there’s no “tokens” - every model step takes a full image as input and produces a “noise report” as output.  Finally, you reward the model<sup id="fnref-1"><a href="#fn-1">1</a></sup> based on how close the model’s prediction was. </p>
<p>It’s important to train on noisy images, all the way from a little bit of noise to images that are indistinguishable from static. Typically that’s done by adding increasing amounts of noise to images in the training set during training (on a fixed schedule). Eventually your model gets really good at identifying the last layer of noise, even from images that just look like the “pure noise” image above.</p>
<p>At inference time, that’s exactly what you do: start with pure noise and a user-provided caption, then run the model to identify the “top” layer of noise. Remove that layer, then keep running the model and removing layers until you’re left with the “original” image. In reality, that image was entirely generated by the model. <strong>This process of identifying a layer of noise and reversing it is called “denoising”.</strong></p>
<p>There are lots of tricks that get used in this process, but the two most important ones are variational auto-encoders and classifier-free guidance.</p>
<h4>Variational auto-encoders</h4>
<p>Expressing an image (or a video) as a big tensor is very expensive. Images have a lot of pixels! In practice, diffusion models operate on a <em>compressed</em> version of each image, kind of like how text models operate on strings of tokens rather than individual letters of bytes. How is that compressed version generated?</p>
<p>Typically with a variational autoencoder (VAE) model that is trained first. That model learns to turn a big image tensor into a smaller random-looking tensor, while still being able to convert it back into the original image. Why use a VAE rather than an existing well-known compression like JPEG?</p>
<ul>
<li>It’s important that the compressed representation be random-looking (i.e. Gaussian-shaped) so the denoising process works properly. JPEG compression is highly structured</li>
<li>The compressed representation must always be the same size, which current compression algorithms don’t do</li>
<li>It’s OK for the VAE to discard some details (e.g. camera noise) which JPEG compression will retain</li>
</ul>
<p>So the usual strategy for training and inference is to run a VAE over your image tensor, add noise, denoise on that, and then decode it back to an original full-size image. Note that there are some models that don’t use VAE, like DALLE-3, but it’s much slower and more expensive.</p>
<h4>Classifier-free guidance</h4>
<p>There’s a common trick to make sure the model is actually learning to generate images based on the caption, instead of just any possible image. During training, you zero out the caption for some images, so the model learns two functions: not just how to remove the noise for a caption, but how to remove the noise for any possible image. During inference, you run once with a caption and once without, and blend the predictions (magnifying the difference between those two vectors). That makes sure the model is paying a lot of attention to the caption.</p>
<h3>Key differences from transformers</h3>
<p>The fundamental operation here is totally different from transformer-based language models, so many of your intuitions about transformers won’t apply. For instance:</p>
<ul>
<li>At each inference step, transformers keep generating new tokens, while diffusion models go from a (e.g.) 256x256 pixel image to a different 256x256 pixel image.</li>
<li>Transformers start with nothing but the prompt, but diffusion models need a “blank canvas” of pure noise to work from.</li>
<li>Transformers don’t “edit” previously generated tokens - once they’re outputted, they’re locked in - but diffusion models can and do change previous output as they go. </li>
<li>If you stop a transformer early, you probably don’t get the answer you were looking for. If you stop a diffusion model early, you get a noisy version of the image you wanted.</li>
</ul>
<p>That last point indicates an interesting capability that diffusion models have: you get a kind of built-in quality knob. If you want fast inference at the cost of quality, you can just run the model for less time and end up with more noise in the final output<sup id="fnref-2"><a href="#fn-2">2</a></sup>. If you want high quality and you’re happy to take your time getting there, you can keep running the model until it’s finished removing noise.</p>
<h3>Why does it work?</h3>
<p>Transformers work because (as it turns out) the structure of human language contains a functional model of the world. If you train a system to predict the next word in a sentence, you therefore get a system that “understands” how the world works at a surprisingly high level. All kinds of exciting capabilities fall out of that - long-term planning, human-like conversation, tool use, programming, and so on.</p>
<p>What is the equivalent animating intuition for diffusion models? I don’t really know, but it’s probably something about the relationship between noise and data - if you can train a system to tell the difference between them, you’re necessarily encoding a model of the world into that system? I bet there’s a much nicer way of articulating this, or a better intuition that could be teased out here.</p>
<p>The same principles that work for images work for other kinds of data: video, audiom, and even text.</p>
<h3>Diffusion video models</h3>
<p>So far this has all been about image diffusion models. What about diffusion models that generate video? As far as I can tell, there are lots of different approaches, but the simplest one is to treat the entire video as a single noisy input. Instead of having your input be a tensor that represents a single picture, your input is a (much larger) tensor that represents all the frames in a video clip. As the model learns to identify noise, it’s also learning each frame relates to the other frames in the clip (object permanence, cause and effect, and so on).</p>
<p>I find it very cool that you can run effectively the same approach for video that you do for single images. It suggests that the fundamental mechanism here is very powerful. It also sheds some light on why the current video diffusion models (like OpenAI’s Sora or Google’s VEO) only generate clips and can’t just “keep going” like a text-based transformer model can.</p>
<p>Incidentally, audio generation works the same way, just with a big audio tensor instead of a big video tensor.</p>
<h3>Diffusion text models</h3>
<p>What about diffusion models that generate text? Text-based diffusion models are really strange, because you can’t just add noisy pixels to text in the same way that you can to images or video. The <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10909201/#sec12">main strategy</a> seems to be adding noise to the text <em>embeddings</em><sup id="fnref-3"><a href="#fn-3">3</a></sup>. At inference time, you start with a big block of pure-noise embeddings (presumably just random numbers) then denoise until it becomes actual decodable text.</p>
<p>How do you turn embeddings back into text? There’s no obvious way. If you just try and look up the “closest” token to each embedding, you often end up with gibberish. If you use a separate decoder model to translate the embeddings, that works but feels a bit like cheating - at that point your diffusion model is really just generating a plan for your real text-generation model.</p>
<h3>Summary</h3>
<ul>
<li>Diffusion models are trained to identify small amounts of noise in images, based on a caption embedding</li>
<li>That means you can start with pure noise and a user-provided caption and just keep chipping away layers of noise until you get to what the model thinks the original image should look like</li>
<li>The operating model is very different from transformers: not sequence-based, operates on previous outputs, and can in principle be sped up or stopped early</li>
<li>Video diffusion works the same way as image diffusion, but it’s harder for the model to learn because it requires tracking consistency over time</li>
<li>Text diffusion is weird because you can’t easily add noise to language, and if you convert to embeddings before adding noise it’s hard to reliably convert back</li>
</ul>
</section><p>If you liked this post, consider <a href="https://buttondown.com/seangoedecke" target="_blank">subscribing</a> to email updates about my new posts.</p><p>May 19, 2025<!-- -->&nbsp;│ Tags: <a href="https://www.seangoedecke.com/tags/ai/">ai</a>, <a href="https://www.seangoedecke.com/tags/explainers/">explainers</a></p><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ClawPDF – Open-Source Virtual/Network PDF Printer with OCR and Image Support (170 pts)]]></title>
            <link>https://github.com/clawsoftware/clawPDF</link>
            <guid>44029142</guid>
            <pubDate>Mon, 19 May 2025 12:31:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/clawsoftware/clawPDF">https://github.com/clawsoftware/clawPDF</a>, See on <a href="https://news.ycombinator.com/item?id=44029142">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">clawPDF - Virtual PDF/OCR/Image (Network) Printer</h2><a id="user-content-clawpdf---virtual-pdfocrimage-network-printer" aria-label="Permalink: clawPDF - Virtual PDF/OCR/Image (Network) Printer" href="#clawpdf---virtual-pdfocrimage-network-printer"></a></p>
<p dir="auto">ClawPDF may seem like yet another Virtual PDF/OCR/Image Printer, but it actually comes packed with features that are typically found in enterprise solutions. With clawPDF, you can create documents in various formats, including PDF/A-1b, PDF/A-2b, PDF/A-3b, PDF/X, PDF/Image, OCR, SVG, PNG, JPEG, TIF, and TXT. You also have easy access to metadata and can remove it before sharing a document. In addition, you can protect your documents with a password and encrypt them with up to 256-bit AES.</p>
<p dir="auto">ClawPDF offers a scripting interface that lets you automate processes and integrate it into your application. Moreover, you can install clawPDF on a print server and print documents over the network, not just locally.</p>
<p dir="auto">ClawPDF is open-source and compatible with all major Windows client and server operating systems (x86/x64/ARM64), and it even supports multi-user environments!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<p dir="auto"><a href="https://github.com/clawsoftware/clawPDF/releases/download/0.9.3/clawPDF_0.9.3_setup.msi">https://github.com/clawsoftware/clawPDF/releases/download/0.9.3/clawPDF_0.9.3_setup.msi</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Print to PDF, PDF/A-1b, PDF/A-2b, PDF/A-3b, PDF/X, PDF/Image, OCR, SVG, PNG, JPEG, TIF and TXT</li>
<li>Print 100% valid <a href="https://github.com/clawsoftware/clawPDF/raw/master/docs/pdfa_valid/PDFA-1b.pdf">PDF/A-1b</a>, <a href="https://github.com/clawsoftware/clawPDF/raw/master/docs/pdfa_valid/PDFA-2b.pdf">PDF/A-2b</a> and <a href="https://github.com/clawsoftware/clawPDF/raw/master/docs/pdfa_valid/PDFA-3b.pdf">PDF/A-3b</a></li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/Optical-Character-Recognition-(OCR)">Optical Character Recognition (OCR)</a></li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/Scripting-Interface">Scripting Interface (Python, Powershell, VBScript...)</a></li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/Install-as-Network-Printer">Shared Network Printing</a></li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/SVG-Export">SVG Export</a></li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/Drag-and-Drop">Drag and Drop Support</a></li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/Merge-Files">Merge Files</a></li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/Command-Line-Commands">Command Line Support</a></li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/Silent-Printing">Silent Printing</a></li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/(Custom)-Paper-Sizes">Custom Paper Sizes / Standard Paper Sizes</a></li>
<li>256-bit AES encryption</li>
<li>Light/Dark Theme</li>
<li>ARM64 Support</li>
<li>Full Unicode Support</li>
<li>Multiple Profiles</li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/Post-Actions">Post Actions</a></li>
<li>Create additional printers with assigned profile</li>
<li><a href="https://github.com/clawsoftware/clawPDF/wiki/Translations">24 translations. Add yours!</a></li>
<li>Easy to deploy (MSI-Installer &amp; Config)</li>
<li>Many settings</li>
<li>Easy to use</li>
<li>No adware, spyware and nagware</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Optical Character Recognition (OCR)</h2><a id="user-content-optical-character-recognition-ocr" aria-label="Permalink: Optical Character Recognition (OCR)" href="#optical-character-recognition-ocr"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/clawsoftware/clawPDF/blob/master/docs/images/ImageOCR.gif?raw=true"><img src="https://github.com/clawsoftware/clawPDF/raw/master/docs/images/ImageOCR.gif?raw=true" alt="OCR" title="OCR" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/clawsoftware/clawPDF/blob/master/docs/com_examples/Powershell/CreatePDFwithPassword.ps1">Scripting Interface</a></h2><a id="user-content-scripting-interface" aria-label="Permalink: Scripting Interface" href="#scripting-interface"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/clawsoftware/clawPDF/blob/master/docs/images/ScriptingInterface.gif?raw=true"><img src="https://github.com/clawsoftware/clawPDF/raw/master/docs/images/ScriptingInterface.gif?raw=true" alt="ScriptingInterface" title="Scripting Interface" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Print a PDF and protect it with a password</h2><a id="user-content-print-a-pdf-and-protect-it-with-a-password" aria-label="Permalink: Print a PDF and protect it with a password" href="#print-a-pdf-and-protect-it-with-a-password"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/clawsoftware/clawPDF/blob/master/docs/images/PrintPDFwithPassword.gif?raw=true"><img src="https://github.com/clawsoftware/clawPDF/raw/master/docs/images/PrintPDFwithPassword.gif?raw=true" alt="PrintPDFwithPassword" title="PrintPDFwithPassword" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Merge multiple documents</h2><a id="user-content-merge-multiple-documents" aria-label="Permalink: Merge multiple documents" href="#merge-multiple-documents"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/clawsoftware/clawPDF/blob/master/docs/images/MergeFiles.gif?raw=true"><img src="https://github.com/clawsoftware/clawPDF/raw/master/docs/images/MergeFiles.gif?raw=true" alt="Merge" title="Merge" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tested under</h2><a id="user-content-tested-under" aria-label="Permalink: Tested under" href="#tested-under"></a></p>
<ul dir="auto">
<li>Windows Server 2022 RDS</li>
<li>Windows Server 2019 RDS</li>
<li>Windows Server 2016 RDS</li>
<li>Windows 11 x64/ARM64</li>
<li>Windows 10 x86/x64/ARM64</li>
<li>Windows 8 x86/x64</li>
<li>Windows 7 x86/x64</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Command Line</h2><a id="user-content-command-line" aria-label="Permalink: Command Line" href="#command-line"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Batch Printing</h2><a id="user-content-batch-printing" aria-label="Permalink: Batch Printing" href="#batch-printing"></a></p>
<div data-snippet-clipboard-copy-content="The GUID for the Profile parameter is located under: HKEY_CURRENT_USER\Software\clawSoft\clawPDF\Settings\ConversionProfiles\[id]\Guid

clawPDF.exe /PrintFile=D:\example.docx /profile=f81ea998-3a76-4104-a574-9a66d6f3039b
clawPDF.exe /PrintFile=D:\example.pdf /profile=JpegGuid
clawPDF.exe /PrintFile=D:\example.pdf /profile=JpegGuid /OutputPath=D:\batchjob

clawPDF.exe /PrintFile=D:\example.txt /printerName=clawPDF2
clawPDF.exe /PrintFile=D:\example.docx /printerName=clawJPG"><pre><code>The GUID for the Profile parameter is located under: HKEY_CURRENT_USER\Software\clawSoft\clawPDF\Settings\ConversionProfiles\[id]\Guid

clawPDF.exe /PrintFile=D:\example.docx /profile=f81ea998-3a76-4104-a574-9a66d6f3039b
clawPDF.exe /PrintFile=D:\example.pdf /profile=JpegGuid
clawPDF.exe /PrintFile=D:\example.pdf /profile=JpegGuid /OutputPath=D:\batchjob

clawPDF.exe /PrintFile=D:\example.txt /printerName=clawPDF2
clawPDF.exe /PrintFile=D:\example.docx /printerName=clawJPG
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overwrite Config</h2><a id="user-content-overwrite-config" aria-label="Permalink: Overwrite Config" href="#overwrite-config"></a></p>
<div data-snippet-clipboard-copy-content="- To deploy a default configuration in an enterprise environment.
- To export a configuration select &quot;Application Settings -> Debug -> Save settings to file&quot;.

clawPDF.exe /Config=D:\clawPDF.ini"><pre><code>- To deploy a default configuration in an enterprise environment.
- To export a configuration select "Application Settings -&gt; Debug -&gt; Save settings to file".

clawPDF.exe /Config=D:\clawPDF.ini
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Printer Managment</h2><a id="user-content-printer-managment" aria-label="Permalink: Printer Managment" href="#printer-managment"></a></p>
<div data-snippet-clipboard-copy-content="SetupHelper.exe /Printer=Add /Name=ExamplePrinter
SetupHelper.exe /Printer=Remove /Name=ExamplePrinter"><pre><code>SetupHelper.exe /Printer=Add /Name=ExamplePrinter
SetupHelper.exe /Printer=Remove /Name=ExamplePrinter
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">ManagePrintJobs</h2><a id="user-content-manageprintjobs" aria-label="Permalink: ManagePrintJobs" href="#manageprintjobs"></a></p>
<div data-snippet-clipboard-copy-content="clawPDF.exe /ManagePrintJobs"><pre><code>clawPDF.exe /ManagePrintJobs
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Changelog</h2><a id="user-content-changelog" aria-label="Permalink: Changelog" href="#changelog"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">v0.9.3 (2023.05.16)</h2><a id="user-content-v093-20230516" aria-label="Permalink: v0.9.3 (2023.05.16)" href="#v093-20230516"></a></p>
<ul dir="auto">
<li>[bugfix] Fixed a bug where in some cases only administrators could use the shared network printer function</li>
<li>[bugfix] Fixed Windows 7 issues caused since version 0.9.1</li>
</ul>
<p dir="auto"><a href="https://github.com/clawsoftware/clawPDF/wiki/Changelog">more</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>.Net Framework 4.6.2+</li>
<li><a href="https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist#visual-studio-2015-2017-2019-and-2022" rel="nofollow">Visual C++ Redistributable 14</a> (Download: <a href="https://aka.ms/vs/17/release/vc_redist.x86.exe" rel="nofollow">x86</a>/<a href="https://aka.ms/vs/17/release/vc_redist.x64.exe" rel="nofollow">x64</a>)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build</h2><a id="user-content-build" aria-label="Permalink: Build" href="#build"></a></p>
<ul dir="auto">
<li>Visual Studio 2022</li>
</ul>
<p dir="auto"><a href="https://github.com/clawsoftware/clawPDF/wiki/Build-it-yourself">more</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Third-party</h2><a id="user-content-third-party" aria-label="Permalink: Third-party" href="#third-party"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">clawPDF uses the following licensed software or parts of the source code:</h2><a id="user-content-clawpdf-uses-the-following-licensed-software-or-parts-of-the-source-code" aria-label="Permalink: clawPDF uses the following licensed software or parts of the source code:" href="#clawpdf-uses-the-following-licensed-software-or-parts-of-the-source-code"></a></p>
<ul dir="auto">
<li>PDFCreator (<a href="https://github.com/pdfforge/PDFCreator">https://github.com/pdfforge/PDFCreator</a>), licensed under AGPL v3 license.</li>
<li>Pdftosvg.net (<a href="https://github.com/dmester/pdftosvg.net">https://github.com/dmester/pdftosvg.net</a>), licensed under MIT license.</li>
<li>iText7 (<a href="https://github.com/itext/itext7-dotnet">https://github.com/itext/itext7-dotnet</a>), licensed under AGPL v3 license.</li>
<li>Nlog (<a href="https://github.com/NLog/NLog">https://github.com/NLog/NLog</a>), licensed under BSD 3-Clause.</li>
<li>PdfScribe (<a href="https://github.com/stchan/PdfScribe">https://github.com/stchan/PdfScribe</a>), licensed under AGPL v3 license.</li>
<li>clawmon (<a href="https://github.com/clawsoftware/clawPDF/tree/master/src/clawmon">https://github.com/clawsoftware/clawPDF/tree/master/src/clawmon</a>), licensed under GPL v2 license.</li>
<li>Microsoft Postscript Printer Driver (<a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/print/microsoft-postscript-printer-driver" rel="nofollow">https://docs.microsoft.com/en-us/windows-hardware/drivers/print/microsoft-postscript-printer-driver</a>), copyright (c) Microsoft Corporation. All rights reserved.</li>
<li>Ghostscript (<a href="https://www.ghostscript.com/download/gsdnld.html" rel="nofollow">https://www.ghostscript.com/download/gsdnld.html</a>), licensed under AGPL v3 license.</li>
<li>SystemWrapper (<a href="https://github.com/jozefizso/SystemWrapper">https://github.com/jozefizso/SystemWrapper</a>), licensed under Microsoft Public license.</li>
<li>Ftplib (<a href="https://archive.codeplex.com/?p=ftplib" rel="nofollow">https://archive.codeplex.com/?p=ftplib</a>), licensed under MIT license.</li>
<li>DataStorage.dll, licensed under pdfforge Freeware License.</li>
<li>DynamicTranslator.dll, licensed under pdfforge Freeware License.</li>
<li>Appbar_save (<a href="http://modernuiicons.com/" rel="nofollow">http://modernuiicons.com/</a>), licensed under Attribution-NoDerivs 3.0 Unported.</li>
<li>Appbar_cogs (<a href="http://modernuiicons.com/" rel="nofollow">http://modernuiicons.com/</a>), licensed under Attribution-NoDerivs 3.0 Unported.</li>
<li>Appbar_page_file_pdf (<a href="http://modernuiicons.com/" rel="nofollow">http://modernuiicons.com/</a>), licensed under Attribution-NoDerivs 3.0 Unported.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">clawPDF is licensed under AGPL v3 license<br>
Copyright (C) 2023 // Andrew Hess // clawSoft</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>