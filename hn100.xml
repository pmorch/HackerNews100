<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 01 Sep 2025 16:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Cloudflare Radar: AI Insights (128 pts)]]></title>
            <link>https://radar.cloudflare.com/ai-insights</link>
            <guid>45093090</guid>
            <pubDate>Mon, 01 Sep 2025 14:49:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radar.cloudflare.com/ai-insights">https://radar.cloudflare.com/ai-insights</a>, See on <a href="https://news.ycombinator.com/item?id=45093090">Hacker News</a></p>
Couldn't get https://radar.cloudflare.com/ai-insights: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Turns out Google made up an elaborate story about me (226 pts)]]></title>
            <link>https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z</link>
            <guid>45092925</guid>
            <pubDate>Mon, 01 Sep 2025 14:27:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z">https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z</a>, See on <a href="https://news.ycombinator.com/item?id=45092925">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[CocoaPods Is Deprecated (193 pts)]]></title>
            <link>https://blog.cocoapods.org/CocoaPods-Specs-Repo/</link>
            <guid>45091493</guid>
            <pubDate>Mon, 01 Sep 2025 10:39:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cocoapods.org/CocoaPods-Specs-Repo/">https://blog.cocoapods.org/CocoaPods-Specs-Repo/</a>, See on <a href="https://news.ycombinator.com/item?id=45091493">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        <p><strong>TLDR: In two years we plan to turn CocoaPods trunk to be read-only. At that point, no new versions or pods will be added to trunk.</strong> - Note, this post has been updated in May 2025.</p>

<p>Last month I wrote about how CocoaPods is currently being maintained, I also noted that we were discussing converting the main CocoaPods spec repo "trunk" to be read-only:</p>

<blockquote>
<p>We are discussing that on a very long, multi-year, basis we can drastically simplify the security of CocoaPods trunk by converting the Specs Repo to be read-only. Infrastructure like the Specs repo and the CDN would still operate as long as GitHub and jsDelivr continue to exist, which is pretty likely to be a very long time. <strong>This will keep all existing builds working</strong>.</p>
</blockquote>

<p>I plan to implement the read-only mode so that when someone submits a new Podspec to CocoaPods, it will always be denied at the server level. I would then convert the "CocoaPods/Specs" repo to be marked as "Archived" on GitHub which should cover all of our bases.</p>

<p>Making the switch will not break builds for people using CocoaPods in 2026 onwards, but at that point, you're not getting any more updates to dependencies which come though CocoaPods trunk. This shouldn't affect people who use CocoaPods with their own specs repos, or have all of their dependencies vendored (e.g. they all come from npm.)</p>

<p><em>May 2025</em> Update: Since this post was originally written, we've had enough security researchers abusing scripting capabilities in CocoaPods that we are now introducing a block on allowing new CocoaPods to use the <a href="https://guides.cocoapods.org/syntax/podspec.html#prepare_command"><code>prepare_command</code></a> field in a Podspec. Any existing Pods using <a href="https://guides.cocoapods.org/syntax/podspec.html#prepare_command"><code>prepare_command</code></a> are hard-coded to bypass this check.</p>

<h2 id="timeline">Timeline</h2>

<p>My goal is to send 2 very hard-to-miss notifications en-masse, and then do a test run a month before the final shutdown.</p>

<h3 id="may-2025">May 2025</h3>

<p>We are stopping new CocoaPods from being added which use the <a href="https://guides.cocoapods.org/syntax/podspec.html#prepare_command"><code>prepare_command</code></a> field</p>

<h3 id="mid-late-2025">Mid-late 2025</h3>

<p>I will email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post.</p>

<h3 id="september-october-2026">September-October 2026</h3>

<p>I will, again, email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post, noting that they have roughly a month before we do a test run of going read-only.</p>

<h3 id="november-1-7th-2026">November 1-7th 2026</h3>

<p>I will trigger a test run, giving automation a chance to break early</p>

<h3 id="december-2nd-2026">December 2nd 2026</h3>

<p>I will switch trunk to not accept new Podspecs permanently. This is a Wednesday after American Thanksgiving, so I think folks won't be in rush mode.</p>



<p>These dates are not set in stone, and maybe someone out there has a good reason for us to amend the timeline. I don't think I'm amenable to moving it forwards, but within reason there's space for backwards.</p>

<p>If you have questions, you can contact the team via <a href="https://blog.cocoapods.org/cdn-cgi/l/email-protection#0f666169604f6c606c606e7f606b7c21607d68"><span data-cfemail="7a13141c153a191519151b0a151e095415081d">[email&nbsp;protected]</span></a>, me personally at <a href="https://blog.cocoapods.org/cdn-cgi/l/email-protection#d3b0bcb0bcb2a3bcb7a093bca1a7b2fdbabc"><span data-cfemail="e3808c808c82938c8790a38c919782cd8a8c">[email&nbsp;protected]</span></a> or reach out to me via Bluesky: <a href="https://bsky.app/profile/orta.io/">@orta.io</a>.</p>

      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nintendo Switch 2 Dock USB-C Compatibility (265 pts)]]></title>
            <link>https://www.lttlabs.com/blog/2025/08/30/nintendo-switch-2-dock</link>
            <guid>45087971</guid>
            <pubDate>Sun, 31 Aug 2025 23:21:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lttlabs.com/blog/2025/08/30/nintendo-switch-2-dock">https://www.lttlabs.com/blog/2025/08/30/nintendo-switch-2-dock</a>, See on <a href="https://news.ycombinator.com/item?id=45087971">Hacker News</a></p>
Couldn't get https://www.lttlabs.com/blog/2025/08/30/nintendo-switch-2-dock: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Lewis and Clark marked their trail with laxatives (221 pts)]]></title>
            <link>https://offbeatoregon.com/2501d1006d_biliousPills-686.077.html</link>
            <guid>45087815</guid>
            <pubDate>Sun, 31 Aug 2025 22:54:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://offbeatoregon.com/2501d1006d_biliousPills-686.077.html">https://offbeatoregon.com/2501d1006d_biliousPills-686.077.html</a>, See on <a href="https://news.ycombinator.com/item?id=45087815">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<header>
		<a href="http://offbeatoregon.com/"><img src="https://offbeatoregon.com/assets-misc/header-2212.png" alt="Offbeat Oregon decorative banner" width="1274" height="237"></a>
				</header>
		
		
		<article>
        <p>
			<h4>ASTORIA, CLATSOP COUNTY; 1800s: </h4>
			
    </p> <!-- Closing "barHed" -->
    <div><section>
			  
			  			  <p>
				  <h6> Audio version is not yet available</h6>
				  <!--
            <h6 align="center"><em><strong>Audio version:</strong> <a href="PODCASTMP3URLPODCASTMP3PODCASTMP3">Download MP3</a> or use  controls below:</em> <br />

                <audio title="Audio verison of this article" controls>
                  <source src="PODCASTMP3URLPODCASTMP3URLPODCASTMP3URL" type="audio/mpeg" />
                  Your browser does not support the audio element. </audio>
            </h6>
-->
            </p>


		              <h5>By Finn J.D. John<br>
			                <em>January 26, 2025</em>
                            <figure>
                        </figure>
		              </h5>
		              <p><strong>AS LEWIS AND CLARK’S</strong> Corps of Discovery made its way across the continent to Oregon, the men (and woman) of the party probably weren’t thinking much about their place in history. So they weren’t taking any particular pains to document their every movement.</p>
            <p>There were, however, some particular pains they were experiencing with every movement, so to speak ... as a result of a relentlessly low-fiber diet: Everyone was constipated, all the time.</p>
            <p>Luckily, they had something that helped with that — a lot. The Corps of Discovery left on its journey with a trove of 600 giant pills that the men called “thunder-clappers,” which the soldiers and travelers used to jump-start things when they got bound up. And everyone used them pretty regularly.</p>
            <figure>
              <div> <p><a href="https://offbeatoregon.com/assets-2010/1006d_BiliousPills/fort_clatsop_1800x1013.jpg"><img src="https://offbeatoregon.com/assets-2010/1006d_BiliousPills/fort_clatsop_400.jpg" alt="" width="236" height="133"></a></p><figcaption>
                  <h6>The reproduction of Fort Clatsop, built at or near the site of the Corps of Expedition's original buildings. Dr. Rush's Bilious Pills have not been particularly helpful in locating the original Fort Clatsop, long since rotted away — either because it hasn’t been found yet, or because the site of the old pit latrine has been disturbed by farming or logging activities in the years since. (Image: National Parks Service)</h6>
                </figcaption>
              </div>
            </figure>
            <p>And, strange as it seems, that fact is why we know several of their campsites along the way. The main active ingredient in “thunder-clappers” was a mercury salt, which is a pretty stable compound. Archaeologists simply have to search for dimples in the ground — which is what old latrine pits often end up looking like, hundreds of years later, after Nature has partly filled them in — and take samples of the dirt in them. </p>
            <p>If it comes up with an off-the-charts reading for mercury, well, that’s a Corps of Discovery pit toilet — and the layout of the rest of the campsite can be extrapolated with considerable precision by consulting the military manuals they used to lay out their camps.</p>
            <p><br>
              <strong>THESE PILLS WERE</strong> the pride and joy of Dr. Benjamin Rush, one of the Founding Fathers and a signer of the Declaration of Independence. Rush was also the man President Thomas Jefferson considered the finest physician in the republic. </p>
            <p>In that opinion, Jefferson was probably alone, or at least in a small minority. Dr. Rush’s style of “heroic medicine” had caused his star to fall quite a bit by this time — especially after the Philadelphia yellow fever epidemic of 1793, when his patients died at a noticeably higher rate than untreated sufferers. </p>
            <p>At the time, of course, very little was known about how the human body worked. Physicians were basically theorists, who made educated guesses and did their best. </p>
            <p>The problem was, the education on which those educated guesses were based varied pretty wildly depending on what school you came from. Homeopathic physicians theorized that giving patients a tiny amount of something that mimicked their symptoms would stimulate the body to cure itself. Eclectic physicians sought cures from herbs and folk remedies. Hydropathic physicians believed hot and cold water, applied externally or internally, was all that was needed. </p>
            <p>Dr. Rush wasn’t from one of these schools. He was from the school of mainstream medicine — also known as allopathic medicine (although that term is a perjorative today).</p>
            <p>Allopathic medical theory, in the early 1800s, dated from the second century A.D., courtesy of a Roman doctor named Galen. </p>
            <p>Galen theorized that the human body ran on four different fluids, which he called “humours”: Blood, phlegm, yellow bile, and black bile. All disease, he claimed, stemmed from an imbalance in these humours.</p>
            <p>Thus, too much blood caused inflammation and fever; the solution was to let a pint or two out. Too much bile caused problems like constipation; the solution was to administer a purgative and let the patient blow out some black bile into a handy chamber-pot, or vomit up some yellow bile — or both.</p>
            <p>These interventions sometimes helped, but most of the time they had little or no good effect. So by Rush’s time, a number of physicians were going on the theory that what was needed was a doubling-down on their theory — in a style of practice that they called “heroic medicine.”</p>
            <p>If a sensible dose of a purgative didn’t get a patient’s bile back in balance, a “heroic” dose might. If a cup or two of blood didn’t get the fever down, four or five surely would.          </p>
          </section>
            <blockquote>&nbsp;</blockquote>
            <blockquote>
              <p><i><strong>[EDITOR'S NOTE: </strong>In "reader view" some phone browsers truncate the story here, algorithmically "assuming" that the second column is advertising. (Most browsers do not recognize this page as mobile-device-friendly; it is designed to be browsed on any device without reflowing, by taking advantage of the "double-tap-to-zoom" function.) If the story ends here on your device, you may have to exit "reader view" (sometimes labeled "Make This Page Mobile Friendly Mode") to continue reading. We apologize for the inconvenience.<strong>]</strong></i></p>
            </blockquote>
            <p><strong>—<a href="#TopCol02">(Jump to top of next column)</a>—</strong></p></div>
    <!-- closes "barBod" -->
<section>
        <h6 id="TopCol02">
          <figure> <a href="https://offbeatoregon.com/assets-2010/1006d_BiliousPills/ft-clatsop-drawing-1800.jpg"><img src="https://offbeatoregon.com/assets-2010/1006d_BiliousPills/ft-clatsop-drawing-436.jpg" alt="" width="436" height="298"></a>
            <figcaption>A sketch of Fort Clatsop as it would have appeared in 1805. (Image: Oregon Historical Society)</figcaption>
          </figure>
        </h6>
        <p><br>
          You can imagine what the result of this philosophy was, when applied to an actual sick person.</p>
        <p>“Some people have stated that the Lewis and Clark Expedition would have been better off if they had taken a trained physician along to care for the numerous problems that they encountered. I totally disagree,” says physician and historian David Peck. “I think a trained physician would have been overly confident and possibly would have been much more aggressive in their treatment of illnesses, often times to the detriment of the patient.”</p>
        <p>In lieu of a trained physician, the Corps of Discovery’s leaders got some basic medical training, along with a bag full of the tools of allopathic intervention: lancets for bleeding patients, blister powder for inducing “heat,” opium products for relieving pain and inducing sleep — and purgatives.</p>
        <p>Those purgatives are the heroes of our story today. They came in the form of beefy pills, about four times the size of a standard aspirin tablet, which Rush called “Dr. Rush’s Bilious Pills.” They contained about 10 grains of calomel and 10 to 15 grains of jalap.</p>
        <figure>
          <div> <p><a href="https://offbeatoregon.com/assets-2010/1006d_BiliousPills/BillPillFormula-1200.jpg"><img src="https://offbeatoregon.com/assets-2010/1006d_BiliousPills/BillPillFormula-236.jpg" alt="" width="236" height="120"></a></p><figcaption>
              <h6>This recipe for a milder version of Rush's Bilious Pills comes from the National Formulary in 1945. This image appears in the Lewis and Clark Fort Mandan Foundation's Web site, at which there's a lot more information about the ingredients in this compound. Mercury was still being used as an internal medicine in the 1960s and as a topical antiseptic (chiefly as Mercurochrome) into the 1990s.</h6>
            </figcaption>
          </div>
        </figure>
        <p>Jalap, the powdered root of a Mexican variety of morning glory, is a natural laxative of considerable power. </p>
        <p>And calomel ... ah, calomel. Calomel was the wonder drug of the age. Its chemical name is mercury chloride. In large doses (and they don’t get much larger than 10 grains, or 20 if a fellow takes two of them, as Dr. Rush recommended!) it functions as a savage purgative, causing lengthy and productive sessions in the outhouse and leaving a patient thoroughly depleted and hopefully in full restoration of his bile balance. </p>
        <p>Calomel also was the only thing known to be effective against syphilis, which was always an issue with military outfits. Whether picked up from a friendly lady in a waterfront St. Louis “sporting house” before the journey, or from an equally friendly Native lady met along the way, syphilis went with soldiers like ice cold milk with an Oreo cookie.</p>
        <p>When symptoms broke out, the patient would be dosed with “thunder clappers” and slathered with topical mercury ointments until he started salivating ferociously, which was a symptom of mild mercury poisoning but at the time was considered a sure sign that the body was purging the sickness out of itself. </p>
        <p>And yes, a few of the men did end up needing treatment for syphilis. But everyone in the party needed a good laxative “on the regular” (sorry about that). Week after week, hunting parties went out and brought back animals to eat. The explorers lived on almost nothing but meat.</p>
        <p>And this low-fiber diet had predictable results.</p>
        <p>It had another result, too, which was less predictable — although highly convenient for later historians. The fact is, mercury chloride is only slightly soluble in human digestion. Plus, the reason it works is, it irritates the tissues of the digestive tract severely, causing the body to expel it just as fast as it possibly can before more damage can be done. So, most of the calomel in any given “bilious pill” gets blown out post-haste in the ensuing “purge.”</p>
        <p>Then, once out of the body and in the earth, it lasts literally for centuries without breaking down or dissolving away.</p>
        <p>So as Lewis and Clark and their crew made their way across the continent, and across Oregon, they were unknowingly depositing a trail of heavy-metal laxatives along the way — a trail that historians and scientists have been able to detect and use to document almost their every, uh, movement.        </p>
        <blockquote>
          <p><br>
            (Sources: Class lecture in History of American Medicine, October 2009, Univ. of Oregon, by Dr. James Mohr; Or Perish in the Attempt: Wilderness Medicine in the Lewis and Clark Expedition, a book by David J. Peck published in 2002 by Farcountry Press; “Following Lewis and Clark’s Trail of Mercurial Laxatives,” an article by Marisa Sloan published in the Jan. 29, 2022, issue of Discover Magazine.)</p>
          <p>TAGS: #Archaeology #HeroicMedicine #DavidPeck #Jalap #Syphilis #CorpsOfDiscovery #BenjaminRush #Humours #Medicine #FrontierDoctors #Galen #FortClatsop #Calomel #MercuryPoisoning #Thunderclappers #Constipation #DrJamesMohr #OregonTrail #DrRush's #BiliousPills #Bile #COLUMBIAgorge #CLATSOPcounty</p>
        </blockquote>

		  
          

          
          

      </section>
<!-- closes "BarBod" -->
    </article> <!-- closes "Bar2" -->
    
		<div>
		    <h5>Background image is a postcard, a hand-tinted photograph of Crown Point and the Columbia Gorge Scenic Highway. Here is a <a href="https://offbeatoregon.com/2407c-1012b.sam-lancaster-columbia-river-highway-101.659.html">link to the Offbeat Oregon article</a> about it, from 2024.</h5>
		    <h5>Scroll sideways to move the article aside for a better view.</h5>
		    
		    <h2>Looking for more?</h2>
            <p>On our <a href="https://offbeatoregon.com/index-all.htm"><strong>Sortable Master Directory</strong></a> you can search by keywords, locations, or historical timeframes. Hover your mouse over the headlines to read the first few paragraphs (or a summary of the story) in a pop-up box.</p>
            <h2>... or ...		    </h2>
		    <p><a href="https://offbeatoregon.com/"><strong>Home</strong></a></p>
		    <p><a href="https://offbeatoregon.com/index-all.htm"><img src="https://offbeatoregon.com/assets-misc/1803.header-image.jpg" alt="Listeners" name="Listeners" width="280" height="279" usemap="#ListenersMap" id="Listeners"></a></p>
	      </div>

    <!-- end .content -->
    
  
    <!-- end .footer -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Linux version of the Procmon Sysinternals tool (147 pts)]]></title>
            <link>https://github.com/microsoft/ProcMon-for-Linux</link>
            <guid>45087748</guid>
            <pubDate>Sun, 31 Aug 2025 22:43:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/ProcMon-for-Linux">https://github.com/microsoft/ProcMon-for-Linux</a>, See on <a href="https://news.ycombinator.com/item?id=45087748">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">Process Monitor for Linux (Preview) <a href="https://dev.azure.com/sysinternals/Tools/_build/latest?definitionId=342&amp;repoName=Sysinternals%2FProcMon-for-Linux&amp;branchName=main" rel="nofollow"><img src="https://camo.githubusercontent.com/6862512e9e97defaaa6e8d2d6a0caf2d0437cafc37156673c4e9e22ba2a00f0a/68747470733a2f2f6465762e617a7572652e636f6d2f737973696e7465726e616c732f546f6f6c732f5f617069732f6275696c642f7374617475732f537973696e7465726e616c732e50726f634d6f6e2d666f722d4c696e75783f7265706f4e616d653d537973696e7465726e616c7325324650726f634d6f6e2d666f722d4c696e7578266272616e63684e616d653d6d61696e" alt="Build Status" data-canonical-src="https://dev.azure.com/sysinternals/Tools/_apis/build/status/Sysinternals.ProcMon-for-Linux?repoName=Sysinternals%2FProcMon-for-Linux&amp;branchName=main"></a></h2><a id="user-content-process-monitor-for-linux-preview-" aria-label="Permalink: Process Monitor for Linux (Preview) " href="#process-monitor-for-linux-preview-"></a></div>
<p dir="auto">Process Monitor (Procmon) is a Linux reimagining of the classic Procmon tool from the Sysinternals suite of tools for Windows.  Procmon provides a convenient and efficient way for Linux developers to trace the syscall activity on the system.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/ProcMon-for-Linux/blob/main/procmon.gif"><img src="https://github.com/microsoft/ProcMon-for-Linux/raw/main/procmon.gif" alt="Procmon in use" title="Procmon in use" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation &amp; Usage</h2><a id="user-content-installation--usage" aria-label="Permalink: Installation &amp; Usage" href="#installation--usage"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>OS: Ubuntu 18.04 lts</li>
<li><code>cmake</code> &gt;= 3.14 (build-time only)</li>
<li><code>libsqlite3-dev</code> &gt;= 3.22 (build-time only)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install Procmon</h2><a id="user-content-install-procmon" aria-label="Permalink: Install Procmon" href="#install-procmon"></a></p>
<p dir="auto">Please see installation instructions <a href="https://github.com/microsoft/ProcMon-for-Linux/blob/main/INSTALL.md">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build Procmon</h2><a id="user-content-build-procmon" aria-label="Permalink: Build Procmon" href="#build-procmon"></a></p>
<p dir="auto">Please see build instructions <a href="https://github.com/microsoft/ProcMon-for-Linux/blob/main/BUILD.md">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="Usage: procmon [OPTIONS]
   OPTIONS
      -h/--help                Prints this help screen
      -p/--pids                Comma separated list of process IDs to monitor
      -e/--events              Comma separated list of system calls to monitor
      -c/--collect [FILEPATH]  Option to start Procmon in a headless mode
      -f/--file FILEPATH       Open a Procmon trace file
      -l/--log FILEPATH        Log debug traces to file"><pre>Usage: procmon [OPTIONS]
   OPTIONS
      -h/--help                Prints this help screen
      -p/--pids                Comma separated list of process IDs to monitor
      -e/--events              Comma separated list of system calls to monitor
      -c/--collect [FILEPATH]  Option to start Procmon in a headless mode
      -f/--file FILEPATH       Open a Procmon trace file
      -l/--log FILEPATH        Log debug traces to file</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">The following traces all processes and syscalls on the system:</p>

<p dir="auto">The following traces processes with process id 10 and 20:</p>

<p dir="auto">The following traces process 20 only syscalls read, write and open at:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo procmon -p 20 -e read,write,openat"><pre>sudo procmon -p 20 -e read,write,openat</pre></div>
<p dir="auto">The following traces process 35 and opens Procmon in headless mode to output all captured events to file <code>procmon.db</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo procmon -p 35 -c procmon.db"><pre>sudo procmon -p 35 -c procmon.db</pre></div>
<p dir="auto">The following opens a Procmon <code>tracefile</code>, <code>procmon.db</code>, within the Procmon TUI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo procmon -f procmon.db"><pre>sudo procmon -f procmon.db</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feedback</h2><a id="user-content-feedback" aria-label="Permalink: Feedback" href="#feedback"></a></p>
<ul dir="auto">
<li>Ask a question on Stack Overflow (tag with ProcmonForLinux)</li>
<li>Request a new feature on GitHub</li>
<li>Vote for popular feature requests</li>
<li>File a bug in GitHub Issues</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">If you are interested in fixing issues and contributing directly to the code base, please see the <a href="https://github.com/microsoft/ProcMon-for-Linux/blob/main/CONTRIBUTING.md">document How to Contribute</a>, which covers the following:</p>
<ul dir="auto">
<li>How to build and run from the source</li>
<li>The development workflow, including debugging and running tests</li>
<li>Coding Guidelines</li>
<li>Submitting pull requests</li>
</ul>
<p dir="auto">Please see also our <a href="https://github.com/microsoft/ProcMon-for-Linux/blob/main/CODE_OF_CONDUCT.md">Code of Conduct</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright (c) Microsoft Corporation. All rights reserved.</p>
<p dir="auto">Licensed under the MIT License.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We should have the ability to run any code we want on hardware we own (1557 pts)]]></title>
            <link>https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html</link>
            <guid>45087396</guid>
            <pubDate>Sun, 31 Aug 2025 21:46:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html">https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html</a>, See on <a href="https://news.ycombinator.com/item?id=45087396">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><article>
  
  <div><p>Sideloading has been a hot topic for the last decade. Most recently, Google has <a href="https://9to5google.com/2025/08/25/android-apps-developer-verification/">announced</a> further restrictions on the practice in Android. Many hundreds of comment threads have discussed these changes over the years. One point in particular is always made: “I should be able to run whatever code I want on hardware I own”. I agree entirely with this point, but within the context of this discussion it’s moot.</p>

<blockquote>
  <p>“I should be able to run whatever code I want on hardware I own”</p>
</blockquote>

<p>When Google restricts your ability to install certain applications they aren’t constraining what you can do with the hardware you own, they are constraining what you can do using the software they provide with said hardware. It’s through this control of the operating system that Google is exerting control, not at the hardware layer. You often don’t have full access to the hardware either and building new operating systems to run on mobile hardware is impossible, or at least much harder than it should be. This is a separate, and I think more fruitful, point to make. Apple is a better case study than Google here. Apple’s success with iOS partially derives from the tight integration of hardware and software. An iPhone without iOS is a very different product to what we understand an iPhone to be. Forcing Apple to change core tenets of iOS by legislative means would undermine what made the iPhone successful.</p>

<p>You shouldn’t take away from this that I am some stalwart defender of the two behemoths Apple and Google, far from it. However, our critique shouldn’t be of the restrictions in place in the operating systems they provide – rather, it should focus on the ability to truly run any code we want on hardware we own. In this context this would mean having the ability and documentation to build or install alternative operating systems on this hardware. It should be possible to run Android on an iPhone and manufacturers should be required by law to provide enough technical support and documentation to make the development of new operating systems possible. If you want to play Playstation games on your PS5 you must suffer Sony’s restrictions, but if you want to convert your PS5 into an emulator running Linux that should be possible.</p>

</div>
  
    
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What to do with C++ modules? (205 pts)]]></title>
            <link>https://nibblestew.blogspot.com/2025/08/we-need-to-seriously-think-about-what.html</link>
            <guid>45086210</guid>
            <pubDate>Sun, 31 Aug 2025 19:22:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nibblestew.blogspot.com/2025/08/we-need-to-seriously-think-about-what.html">https://nibblestew.blogspot.com/2025/08/we-need-to-seriously-think-about-what.html</a>, See on <a href="https://news.ycombinator.com/item?id=45086210">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-3421824919025258501" itemprop="description articleBody">
<p><i><b>Note:</b> Everything that follows is purely my personal opinion as an individual. It should not be seen as any sort of policy of the Meson build system or any other person or organization. It is also not my intention to throw anyone involved in this work under a bus. Many people have worked to the best of their abilities on C++ modules, but that does not mean we can't analyze the current situation with a critical eye.</i></p><p>The lead on this post is a bit pessimistic, so let's just get it out of the way.</p><blockquote><p>If C++ modules can not show a 5× compilation time speedup (preferably 10×) on multiple existing open source code base, modules should be killed and taken out of the standard. Without this speedup pouring any more resources into modules is just feeding the sunk cost fallacy.&nbsp;</p></blockquote><p>That seems like a harsh thing to say for such a massive undertaking that promises to make things so much better. It is not something that you can just belt out and then mic drop yourself out. So let's examine the whole thing in unnecessarily deep detail. You might want to grab a cup of <span>$beverage</span> before continuing, this is going to take a while.</p><h2>What do we want?</h2><p>For the average developer the main visible advantages would be the following, ordered from the most important to the least.</p><ol><li>Much faster compilation times.</li></ol><p>If you look at old presentations and posts from back in the day when modules were voted in (approximately 2018-2019), this is the big talking point. This makes perfect sense, as the "header inclusion" way is an <i>O(N²) </i>algorithm and parsing C++ source code is slow. Splitting the code between source and header files is busywork one could do without. The core idea behind modules is that if you can store the "headery" bit in a preprocessed binary format that can be loaded from disk, things become massively faster.</p><p>Then, little by little, build speed seems to fall by the wayside and the focus starts shifting towards "build isolation". This means avoiding bugs caused by things like macro leakage, weird namespace lookup issues and so on. Performance is still kind of there, but the numbers are a lot smaller, spoken aloud much more rarely and often omitted entirely. Now, getting rid of these sorts of bugs is fundamentally a good thing. However it might not be the most efficient use of resources. Compiler developer time is, sadly, a zero sum game so we should focus their skills and effort on things that provide the best results.</p><p>Macro leakage and other related issues are&nbsp;<i>icky</i>&nbsp;but they are on average fairly rare. I have encountered a bug caused by them maybe once or twice a year. They are just not that common for the average developer. Things are probably different for people doing deep low level metaprogramming hackery, but they are a minuscule fraction of the total developer base. On the other hand slow build times are the bane of existence of <i>every single</i>&nbsp;C++ developer <i>every single</i>&nbsp;day. It is, without question, the narrowest bottleneck for developer productivity today and is the main issue modules were designed to solve. They don't seem to be doing that nowadays.</p><h2>How did we end up here in the first place?</h2><p>C++ modules were a C++ 20 feature. If a feature takes over five years of implementation work to get even somewhat working, you might ponder how it was accepted in the standard in the first place. As I was not there when it happened, I do not really know. However I have spoken to people who were present at the actual meetings where things were discussed and voted on. Their comments have been enlightening to say the least.</p><p>Apparently there were people who knew about the implementation difficulty and other fundamental problems and were quite vocal that modules as specified are borderline unimplementable. They were shot down by a group of "higher up" people saying that "modules are such an important feature that we <b>absolutely must</b> have them in C++ 20".</p><p>One person who was present told me: "that happened seven years ago [there is a fair bit of lead time in ISO standards] and [in practice] we still have nothing. In another seven years, if we are very lucky, we might have something that sort of works".</p><h2>The integration task from hell</h2><p>What sets modules apart from almost all other features is that they require very tight integration between compilers and build systems. This means coming up with schemes for things like what do module files actually contain, how are they named, how are they organized in big projects, how to best divide work between the different tools. Given that the ISO standard does not even acknowledge the fact that source code might reside in a file, none of this is in its purview. It is not in anybody's purview.</p><p>The end result of all that is that everybody has gone in their own corner, done the bits that are the easiest for them and hoping for the best. To illustrate how bad things are, I have been in discussions with compiler developers about this. In said discussion various avenues were considered on how to get things actually working, but one compiler developer replied "we do not want to turn the compiler into a build system" to <i>every single proposal, no matter what it was</i>. The experience was not unlike talking to a brick wall. My guess is that the compiler team in question did not have resources to change their implementation so vetoing everything became the sensible approach for them (though not for the module world in general).</p><p>The last time I looked into adding module support to Meson, things were so mind-bogglingly terrible, that you needed to create, during compilation time, additional compiler flags, store them in temp files and pass them along to compilation commands. <a href="https://nibblestew.blogspot.com/2023/12/even-more-breakage-in-c-module-world.html">I wish I was kidding but I am not</a>. It's quite astounding that the module work started basically from Fortran modules, which are simple and work (in production even), and ended up in their current state, a kafkaesque nightmare of complexity which does not work.</p><p>If we look at the whole thing from a project management viewpoint, the reason for this failure is fairly obvious. This is a big change across multiple isolated organizations. The only real way to get those done is to have a product owner who a) is extremely good at their job b) is tasked with and paid to get the thing done properly c) has sufficient stripes to give orders to the individual teams and d) has no problems slapping people on metaphorical wrists if they try to weasel out of doing their part.</p><p>Such a person does not exist in the modules space. It is arguable whether such a person could exist even in theory. Because of this modules can never become good, which is a reasonable bar to expect a foundational piece of technology to reach.</p><h2>The design that went backwards</h2><p>If there is one golden rule of software design, it is "Do not do a grand design up front". This is mirrored in the C++ committee's guideline of "standardize existing practice".</p><p>C++ modules may be the grandest up-frontest design the computing world has ever seen. There were no implementations (one might argue there still aren't, but I digress), no test code, no prototypes, nothing. Merely a strong opinion of "we need this and we need it yesterday".</p><p>For the benefit of future generations, one better way to approach the task would have gone something like this. First you implement enough in the compiler to be able to produce one module file and then consume it in a different compilation unit. Keep it as simple as possible. It's fine to only serialize a subset of functionality and error out if someone tries to go outside the lines. Then take a build system that runs that. Then expand that to support a simple project, say, one that has ten source files and produces one executable. Implement features in the module file until you can compile the whole thing. Then measure the output. If you do not see performance increases, stop further development until you either find out why that is or you can fix your code to work better. Now you update the API so that no part of the integration makes people's eyes bleed of horror. Then scale the prototype to handle project with 100 sources. Measure again. Improve again. Then do two 100 source pairs, one that produces a library and one that creates an executable that uses the library. Measure again. Improve again. Then do 1000 sources in 10 subprojects. Repeat.</p><p>If the gains are there, great, now you have base implementation that has been proven to work with real world code and which can be expanded to a full implementation. If the implementation can't be made fast and clean, that is a sign that there is a fundamental design flaw somewhere. Throw your code away and either start from scratch or declare the problem too difficult and work on something else instead.</p><p>Hacking on an existing C++ compiler is really difficult and it takes months of work to even get started. If someone wants to try to work on modules but does not want to dive into compiler development, I have implemented a "<a href="https://nibblestew.blogspot.com/2024/01/c-module-tooling-emulator-playground.html">module playground</a>", which consists of a fake C++ compiler, a fake build system and a fake module scanner all in ~300 lines of Python.</p><h2>The promise of import std</h2><p>There is a second way of doing modules in an iterative fashion and it is actually being pursued by C++ implementers, namely <span>import std</span>. This is a very good approach in several different ways. First of all, the most difficult part of modules is the way compilations must be ordered. For the standard library this is not an issue, because it has no dependencies and you can generate all of it in one go. The second thing is the fact that most of the slowness of most of C++ development comes from the standard library. For reference, merely doing an <span>#include&lt;vector&gt;</span> brings in 27 000 lines of code and that is fairly small amount compared to many other common headers.</p><p>What sort of an improvement can we expect from this on real world code bases? Implementations are still in flux, so let's estimate using information we have. The way <span>import std</span> is used depends on the compiler but roughly:</p><ol><li>Replace all <span>#include</span> statements for standard library headers with <span>import std</span>.</li><li>Run the compiler in a special mode.</li><li>The compiler parses headers of the standard library and produces some sort of a binary representation of them</li><li>The representation is written to disk.</li><li>When compiling normally, add compiler flags that tell the compiler to load the file in question before processing actual source code</li></ol><p>If you are thinking "wait a minute, if we remove step #1, this is exactly how precompiled headers work", you are correct. Conceptually it is pretty much the same and I have been told (but have not verified myself) that in GCC at least module files are just repurposed precompiled headers with all the same limitations (e.g. you must use all the same compiler flags to use a module file as you did when you created it).</p><p>Barring a major breakthrough in compiler data structure serialization, the expected speedup should be roughly equivalent to the speedup you get from precompiled headers. Which is to say, maybe 10-20% with Visual Studio and a few percentage points on Clang and GCC. OTOH if such a serialization improvement has occurred, it could probably be adapted to be usable in precompiled headers, too. Until someone provides verifiable measurements proving otherwise, we must assume that is the level of achievable improvement.</p><p>For reference, <a href="https://www.reddit.com/r/cpp/comments/1hv0yl6/success_stories_about_compilation_time_using/">here is a Reddit thread</a> where people report improvements in the 10-20% range.</p><h2>But why 5×?</h2><p>A reasonable requirement for the speedup would be "better than can be achieved using currently available tools and technologies". As an experiment I wrote a custom standard library (not API compatible with the ISO one on purpose) whose main design goal was to be fast to compile. I then took an existing library, converted that to use the new library and measured. The code compiled four times faster. In addition the binary it produced was smaller and, unexpectedly, ran faster. Details can be found <a href="https://nibblestew.blogspot.com/2025/06/a-custom-c-standard-library-part-4.html">in this blog post</a>.</p><p>Given that 4× is already achievable (though, granted, only tested on one project, not proven in general), 5× seems like a reasonable target.</p><h2>But what's in it for <i>you</i>?</h2><p>The C++ standard committee has done a lot of great (and highly underappreciated) work to improve the language. On several occasions Herb Sutter has presented new functionality with "all you have to do is to recompile your code with a new compiler and the end result runs faster and is safer". It takes a ton of work to get these kinds of results, and it is exactly where you want to be.</p><p>Modules are not there. In fact they are in the exact opposite corner.</p><p>Using modules brings with it the following disadvantages:</p><ol><li>Need to rewrite (possibly refactor) your code.</li><li>Loss of portability.</li><li>Module binary files (with the exception of MSVC) are not portable so you need to provide header files for libraries in any case.</li><li>The project build setup becomes more complicated.</li><li>Any toolchain version except the newest one does not work (at the time of writing Apple's module support is listed as "<a href="https://en.cppreference.com/w/cpp/compiler_support.html">partial</a>")<br></li></ol><p>In exchange for all this you, the regular developer-about-town, get the following advantages:</p><ol><li>Nothing.</li></ol><br>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Eternal Struggle (597 pts)]]></title>
            <link>https://yoavg.github.io/eternal/</link>
            <guid>45086020</guid>
            <pubDate>Sun, 31 Aug 2025 19:04:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yoavg.github.io/eternal/">https://yoavg.github.io/eternal/</a>, See on <a href="https://news.ycombinator.com/item?id=45086020">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
  
  <div id="main"><p><a id="theme" href="#" onclick="changeBackground()">change background</a></p></div>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Code Is Debt (112 pts)]]></title>
            <link>https://tornikeo.com/code-is-debt/</link>
            <guid>45085318</guid>
            <pubDate>Sun, 31 Aug 2025 17:58:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tornikeo.com/code-is-debt/">https://tornikeo.com/code-is-debt/</a>, See on <a href="https://news.ycombinator.com/item?id=45085318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  

  <article>
    <p>“Tornike, what do you think of AI coding tools?”</p>

<p>I like to answer this frequent question by way of an example. An example of two companies. It goes something like this:</p>

<p>Imagine two very similar companies. Both companies generate similar revenue and produce a similar software product. The only difference between these companies is that Company A uses 1 million lines of code and Company B uses 100 thousand lines of code. Which company is <em>better off</em>?</p>

<p>Clearly, the company with fewer lines of code is better off. They have fewer lines of code and so they can understand and modify their code more quickly. All other things being equal, less code is better. Put another way code is a form of debt. If you use an AI to generate code, you are effectively getting a debt – a debt of code.</p>

<p>Is it worth going into code debt? It depends. Debt can be both good or bad, it might have interest or be interest-free. Debt can also allow faster growth or it can cause your project to <a href="https://www.bbc.com/news/articles/ce87rer52k3o" target="_blank" rel="noopener noreferrer">implode</a>. In all cases it is important to have easy access to these debt-generating tools. It is still up to you to generate the code debt responsibly.</p>

<p><em>Thanks to <a href="https://www.linkedin.com/in/anitalakhadze/" target="_blank" rel="noopener noreferrer">Ani Talakhadze</a> for reading drafts of this</em></p>

  </article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How is Ultrassembler so fast? (114 pts)]]></title>
            <link>https://jghuff.com/articles/ultrassembler-so-fast/</link>
            <guid>45085156</guid>
            <pubDate>Sun, 31 Aug 2025 17:42:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jghuff.com/articles/ultrassembler-so-fast/">https://jghuff.com/articles/ultrassembler-so-fast/</a>, See on <a href="https://news.ycombinator.com/item?id=45085156">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
<p>
  How is Ultrassembler so fast?
</p>
<p>Published 2025-08-30</p>
<p><a href="https://github.com/Slackadays/Chata/tree/main/ultrassembler">Ultrassembler</a> is a superfast and complete RISC-V assembler library that I'm writing as a component of the bigger <a href="https://github.com/Slackadays/Chata">Chata signal processing</a> project. </p>
<p>Assemblers take in a platform-dependent assembly language and output that platform's native machine code which runs directly on the processor.</p>
<p><img src="https://jghuff.com/RISC-V-assembly-safe.svg" alt="Infographic showing the RISC-V assembly process for addi"></p>
<p><span>"Why would you want to do this?"</span> you might ask. First, existing RISC-V assemblers that conform the the entirety of the specification, <code>as</code> and <code>llvm-mc</code>, ship as binaries that you run as standalone programs. This is normally not an issue. However, in Chata's case, it needs to access a RISC-V assembler from within its C++ code. The alternative is to use some ugly C function like <code>system()</code> to run external software as if it were a human or script running a command in a terminal. </p>
<p>Here's an example of what I'm talking about:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>#include </span><span>&lt;</span><span>iostream</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>string</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>
</span><span>int </span><span>main</span><span>() {
</span><span>    std::string command = "</span><span>riscv64-linux-gnu-as code.s -o code.bin</span><span>";
</span><span>
</span><span>    </span><span>int</span><span> res = std::</span><span>system</span><span>(command.</span><span>data</span><span>());
</span><span>
</span><span>    </span><span>if </span><span>(res != </span><span>0</span><span>) {
</span><span>        std::cerr &lt;&lt; "</span><span>Error executing command: </span><span>" &lt;&lt; command &lt;&lt; std::endl;
</span><span>    }
</span><span>    </span><span>return</span><span> res;
</span><span>}
</span></code></pre>
<p>It gets even worse once you realize you need temporary files and possibly have to artisanally craft the command beforehand. Additionally, invoking the assembler in this manner incurs a significant performance overhead on embedded systems which lack significant processing power. There must be a better way. </p>
<p>Enter Ultrassembler.</p>
<p>With these two goals of speed and standard conformance in mind, I wrote Ultrassembler as a completely standalone library with GNU <code>as</code> as the speed and standard conformity benchmark. </p>
<p>The results are nothing short of staggering. </p>
<p>After months of peformance optimization, Ultrassembler can assemble a test file with about 16 thousand RISC-V instructions over 10 times faster than <code>as</code>, and around 20 times faster than <code>llvm-mc</code>. To put it another way, it only takes about 1000 CPU instructions (+-50% depending on platform) to assemble one RISC-V instruction, while it takes 10,000 for <code>as</code> and 20,000 for <code>llvm-mc</code>. This happens with plain old C++ code only and no platform-specific assembly code, although integrating assembly could crank up the speed even further.</p>
<p>Such performance ensures a good user experience on the platforms where Chata runs, but also as a consequence of this lack of overhead, you could also combine Ultrassembler with fantastic libraries like <a href="https://github.com/libriscv/libriscv">libriscv</a> to implement low-to-no-cost RISC-V scripting in things like games, or maybe even in your JIT programming language!</p>
<p>Let's look at some of the ways I made Ultrassembler this fast so that you can reap the benefits too.</p>
<p><span>WARNING!</span> &nbsp; The code you're about to see here is only current as of this article's publishing. The actual code Ultrassembler uses could be different by the time you read this in the future!</p>
<h2 id="exceptions">Exceptions</h2>
<p>Exceptions, C++'s first way of handling errors, are slow. Super duper slow. Mega slow. So slow, in fact, that many Programming Furus©️®️™️ say you should never ever use them. They'll infect your code with their slowness and transform you into a slow old hunchback in no time. </p>
<p>Or so you would think.</p>
<p>C++ exceptions, despite being so derided, are in fact zero-overhead. Huh? Didn't I just say they were super duper slow? Let me explain.</p>
<p>It's not clear when exactly exceptions are slow. I had to do some research here. As it turns out, GCC's <code>libstdc++</code> uses a so-called "zero-overhead" exception system, meaning that in the ideal normal case where the C++ code calls zero exceptions, there is zero performance penalty. But when it does call an exception, it could become very slow depending on how the code is laid out. Most programmers, not knowing this, frequently use exceptions in their normal cases, and as a result, their programs are slow. Such mysterious behavior caught the attention of Programming Furus©️®️™️ and has made exceptions appear somewhat of a curse.</p>
<p>This tragic curse turns out to be a heavenly blessing for Ultrassembler. In the normal case, there are zero errors to report as a result of proper usage of RISC-V instructions. But if there's some error somewhere, say somebody put in the wrong register, then Ultrassembler sounds the alarm. Since such mistakes only occur as a result of human error (ex bugs in codegen and Ultrassembler itself) the timeframe in which to report the error can expand to that of a human. As a result, even if an exception triggered by a mistake took a full 1 second (about a million times slower than it does in reality), it doesn't matter because the person percepting the error message can only do so in approximately that second timeframe.</p>
<p><span>"But hold on!"</span> you exclaim. <span>"What about std::expected?"</span> In response to some programs which frequently need to handle errors not seen by humans, C++ added a system to reduce the overhead of calling errors, <code>std::expected</code>. I tried this in Ultrassembler and the results weren't pretty. It trades off exception speed for normal case speed. Since the normal case is the norm in Ultrassembler, <code>std::expected</code> incurred at least a 10% performance loss due to the way the <code>std::expected</code> object wraps two values (the payload and the error code) together. <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p2544r0.html">See this C++ standard document for the juicy details.</a></p>
<p>The end result of the use of exceptions is that there is zero performance penalty to optimize out.</p>
<h2 id="fast-data-structures">Fast data structures</h2>
<p>Between all of the RISC-V instruction set extensions, there are 2000+ individual "instructions" (many instructions are identical to one another with a slight numerical change). There are also hundreds of CSRs and just under a hundred registers. This requires data structures large enough to store the properties of thousands of entries. How do you do that? It's tricky. So, how about I just show you what Ultrassembler uses as of this writing:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>struct </span><span>rvregister {
</span><span>    RegisterType type; </span><span>//1B
</span><span>    RegisterID id; </span><span>//1B
</span><span>    uint8_t encoding;
</span><span>    uint8_t padding;
</span><span>};
</span><span>
</span><span>const</span><span> std::array&lt;rvregister, </span><span>96</span><span>&gt; registers;
</span><span>
</span><span>struct </span><span>rvinstruction {
</span><span>    RVInstructionID id; </span><span>//2B
</span><span>    RVInstructionFormat type; </span><span>//1B
</span><span>    uint8_t opcode;
</span><span>    uint16_t funct;
</span><span>    RVInSetMinReqs setreqs; </span><span>//1B
</span><span>    rreq regreqs = reg_reqs::any_regs; </span><span>//1B
</span><span>    special_snowflake_args ssargs = </span><span>special_snowflake_args</span><span>(); </span><span>//2B
</span><span>};
</span><span>
</span><span>// We use a strong typedef to define both rreq and ssflag, but the underlying is a uint8_t in both cases
</span><span>
</span><span>namespace </span><span>ssarg {
</span><span>
</span><span>constexpr</span><span> ssflag get_imm_for_rs = </span><span>ssflag</span><span>(</span><span>0b00000001</span><span>);
</span><span>constexpr</span><span> ssflag use_frm_for_funct3 = </span><span>ssflag</span><span>(</span><span>0b00000010</span><span>);
</span><span>constexpr</span><span> ssflag special_handling = </span><span>ssflag</span><span>(</span><span>0b00000100</span><span>);
</span><span>constexpr</span><span> ssflag swap_rs1_rs2 = </span><span>ssflag</span><span>(</span><span>0b00001000</span><span>);
</span><span>constexpr</span><span> ssflag use_funct_for_imm = </span><span>ssflag</span><span>(</span><span>0b00010000</span><span>);
</span><span>constexpr</span><span> ssflag no_rs1 = </span><span>ssflag</span><span>(</span><span>0b00100000</span><span>);
</span><span>constexpr</span><span> ssflag has_custom_reg_val = </span><span>ssflag</span><span>(</span><span>0b01000000</span><span>);
</span><span>
</span><span>}
</span><span>
</span><span>struct </span><span>special_snowflake_args {
</span><span>    uint8_t custom_reg_val = </span><span>0</span><span>;
</span><span>    ssflag flags; </span><span>//1B
</span><span>};
</span><span>
</span><span>const</span><span> std::array&lt;rvinstruction, </span><span>2034</span><span>&gt; instructions;
</span></code></pre>
<p>Let's go over what each <code>struct</code> does.</p>
<h2 id="rvregister"><code>rvregister</code></h2>
<p><code>rvregister</code> is how Ultrassembler stores the data for all the RISC-V registers. What describes a register? You have its friendly name (like x0 or v20), an alias (like zero or fa1), what kind of register it is (integer, float, or vector?), and what raw encoding it looks like in instructions. You can get away with single bytes to represent the type and encoding. And, that's what we use here to keep data access simple. You could squeeze everything into one or two bytes through clever bitmasking, but after doing so, I couldn't find much of a speedup. This could be situational and so you should not dismiss such a trick.</p>
<p>Why not store the name and alias strings? Ultrassembler does not actually reference the name nor the alias anywhere in its code. Why? Strings are very expensive. This fact is not obvious if you have not made software at the level of Ultrassembler, where string comparison and manipulation grind computation to a crawl. So we just don't use strings anywhere. In spite of this, the initializers of <code>const std::array&lt;rvregister, 96&gt; registers</code> do contain both the name and alias, but the constructors silently discard these data. Such inclusion enables external scripts to look at the array and generate code around it. We'll look at that in the next section. But for now, know that we hate strings.</p>
<h2 id="rvinstruction"><code>rvinstruction</code></h2>
<p><code>rvinstruction</code> follows a similar idea, with the biggest differences being that it's much bigger, 2000+ entries versus 96, and that there is more information to store per entry. This necessitates some extra memory saving magic because there are so many different instructions. We first need an ID for each instruction to do specific checks if needed. We have almost more than 2048 instructions (subject to future expansion) but less than 4196, so we'll need 2 bytes. There are fewer than 256 "types" of instructions (R, I, S, B, U, J, etc.), so 1 byte is good. Same idea with all the other fields. Similarly to <code>rvregister</code>, it would be possible to use bitmasking to compress everything, but this might not result in a significant speedup.</p>
<h2 id="special-snowflake-args"><code>special_snowflake_args</code></h2>
<p>In RISC-V, many instructions require special attention because they have a special encoding, do something special, or are otherwise different from the rest of the herd. To avoid hardcoding behavior handling as much as possible, <code>special_snowflake_args</code> encodes specific properties that many of these special instructions share, such as getting an immediate value instead of a register, swapping the <code>rs1</code> and <code>rs2</code> registers (or <code>vs1</code> and <code>vs2</code>), or omitting a register entirely. We can encode all these properties in a binary way so we use a custom bitmask system to save all the properties in a single byte. <code>custom_reg_val</code>, however, is a separate 1-byte field because registers use 5 bits, and only exists in tandem with <code>has_custom_reg_val</code>.</p>
<p>All together, we are able to use only 20kB of memory to save all the instructions, not withstanding future entries. This fits nicely into many CPU data caches.</p>
<h2 id="preallocated-memory-pools">Preallocated memory pools</h2>
<p>In C++, by default, containers that dynamically allocate memory do so through the heap. The underlying OS provides the heap through assignment of a certain section of its virtual memory to the program requesting the heap memory. Heap allocation happens transparently most of the time. Unfortunately for us, it matters where exactly that memory is. Memory located far away from everything else (often the case with heap memory) unnecessarily clogs up the CPU's memory cache. Additionally, in C++, requesting that heap memory also requires a syscall every time the container geometrically changes size (roughly speaking, 1B -&gt; 2B -&gt; 4B -&gt; 8B -&gt; ... -&gt; 1MB). Syscalls drastically slow down code execution (more so than yo mama is big) because the OS needs to save all the registers, swap in the kernel's, and run the kernel code, all while clogging up the CPU cache again. Therefore, we need a way to allocate memory close to our variables with zero syscalls. </p>
<p>The solution? </p>
<p>Preallocated memory pools.</p>
<p>C++ offers a totally neato way to use the containers you know and love with a custom crafted memory allocator of your choice. </p>
<p>Here's how Ultrassembler does it.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>constexpr </span><span>size_t memory_pool_size = </span><span>33554432</span><span>;
</span><span>
</span><span>template </span><span>&lt;</span><span>class</span><span> T&gt;
</span><span>class </span><span>MemoryBank</span><span>;
</span><span>
</span><span>typedef</span><span> std::basic_string&lt;</span><span>char</span><span>, std::char_traits&lt;</span><span>char</span><span>&gt;, MemoryBank&lt;</span><span>char</span><span>&gt;&gt; ultrastring;
</span><span>
</span><span>template </span><span>&lt;</span><span>typename</span><span> T&gt;
</span><span>using </span><span>ultravector = std::vector&lt;T, MemoryBank&lt;T&gt;&gt;;
</span><span>
</span><span>class </span><span>GlobalMemoryBank </span><span>{
</span><span>    </span><span>inline static</span><span> std::array&lt;std::byte, memory_pool_size&gt; pool;
</span><span>    </span><span>inline static </span><span>size_t used </span><span>= </span><span>0</span><span>; 
</span><span>    </span><span>inline static long</span><span> pagesize </span><span>= </span><span>sysconf</span><span>(_SC_PAGE_SIZE); </span><span>// This only happens once :)
</span><span>
</span><span>public</span><span>:
</span><span>    </span><span>void</span><span>* </span><span>grab_some_memory</span><span>(size_t </span><span>requested</span><span>);
</span><span>
</span><span>    </span><span>void </span><span>reset</span><span>();
</span><span>}</span><span>;
</span><span>
</span><span>extern</span><span> GlobalMemoryBank memory_bank;
</span><span>
</span><span>template </span><span>&lt;</span><span>class</span><span> T&gt;
</span><span>class </span><span>MemoryBank </span><span>{
</span><span>public</span><span>:
</span><span>    </span><span>using </span><span>value_type </span><span>=</span><span> T;
</span><span>
</span><span>    </span><span>MemoryBank</span><span>() </span><span>= </span><span>default</span><span>;
</span><span>
</span><span>    [[nodiscard]] T</span><span>* </span><span>allocate</span><span>(size_t requested) {
</span><span>        std::size_t bytes </span><span>=</span><span> requested </span><span>* sizeof</span><span>(T);
</span><span>        </span><span>return </span><span>reinterpret_cast</span><span>&lt;T</span><span>*</span><span>&gt;(memory_bank.</span><span>grab_some_memory</span><span>(bytes));
</span><span>    }
</span><span>
</span><span>    </span><span>void </span><span>deallocate</span><span>(T</span><span>* </span><span>ptr</span><span>, size_t </span><span>requested</span><span>) { </span><span>return</span><span>; }
</span><span>
</span><span>    </span><span>bool </span><span>operator==</span><span>(</span><span>const</span><span> MemoryBank</span><span>&amp;</span><span>) </span><span>const </span><span>{ </span><span>return </span><span>true</span><span>; }
</span><span>}</span><span>;
</span><span>
</span><span>// In another file...
</span><span>
</span><span>void</span><span>* GlobalMemoryBank::</span><span>grab_some_memory</span><span>(size_t </span><span>requested</span><span>) {
</span><span>    </span><span>if </span><span>(requested + used &gt; pool.</span><span>size</span><span>()) {
</span><span>        </span><span>throw </span><span>UASError</span><span>(OutOfMemory, "</span><span>Out of memory!</span><span>");
</span><span>    }
</span><span>    </span><span>void</span><span>* ptr = reinterpret_cast&lt;</span><span>void</span><span>*&gt;(pool.</span><span>data</span><span>() + used);
</span><span>    used += requested;
</span><span>    </span><span>return</span><span> ptr;
</span><span>}
</span><span>
</span><span>void </span><span>GlobalMemoryBank::</span><span>reset</span><span>() {
</span><span>    used = </span><span>0</span><span>;
</span><span>}
</span></code></pre>
<p>Let's go through this section by section.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>constexpr </span><span>size_t memory_pool_size = </span><span>33554432</span><span>;
</span><span>
</span><span>template </span><span>&lt;</span><span>class</span><span> T&gt;
</span><span>class </span><span>MemoryBank</span><span>;
</span><span>
</span><span>typedef</span><span> std::basic_string&lt;</span><span>char</span><span>, std::char_traits&lt;</span><span>char</span><span>&gt;, MemoryBank&lt;</span><span>char</span><span>&gt;&gt; ultrastring;
</span><span>
</span><span>template </span><span>&lt;</span><span>typename</span><span> T&gt;
</span><span>using </span><span>ultravector = std::vector&lt;T, MemoryBank&lt;T&gt;&gt;;
</span></code></pre>
<p>This is boilerplate defining <em>how big our memory pool is</em> (in bytes), declaring <em>the regular memory pool class</em> (annoying!), what our <em>special memory pool string</em> is an alias of (a standard string but with the regular memory pool allocator), and the same creation of <em>a vector using the regular memory pool</em>.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>class </span><span>GlobalMemoryBank </span><span>{
</span><span>    </span><span>inline static</span><span> std::array&lt;std::byte, memory_pool_size&gt; pool;
</span><span>    </span><span>inline static </span><span>size_t used </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>inline static long</span><span> pagesize </span><span>= </span><span>sysconf</span><span>(_SC_PAGE_SIZE);
</span><span>
</span><span>public</span><span>:
</span><span>    </span><span>void</span><span>* </span><span>grab_some_memory</span><span>(size_t </span><span>requested</span><span>);
</span><span>
</span><span>    </span><span>void </span><span>reset</span><span>();
</span><span>}</span><span>;
</span><span>
</span><span>extern</span><span> GlobalMemoryBank memory_bank;
</span></code></pre>
<p>This class defines the <em>memory pool wrapper</em> that the actual allocator uses. Why? This has to do with how C++ uses custom allocators. When you use a container with a custom allocator, each declaration of that container creates a separate instance of that container <em>and the allocator class</em>. Therefore, if you added the memory pool array as a member of this custom allocator class, each declaration of the container would result in separate instantiations of the underlying memory pool object. This is UNACCEPTABLE for Ultrassembler. Therefore, we instead use a helper class that the allocators call to. As a consequence, it allows us to add memory pool functionality controlled independently of the containers through calls to the helper <code>GlobalMemoryBank</code> class in the future.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>template </span><span>&lt;</span><span>class</span><span> T&gt;
</span><span>class </span><span>MemoryBank </span><span>{
</span><span>public</span><span>:
</span><span>    </span><span>using </span><span>value_type </span><span>=</span><span> T;
</span><span>
</span><span>    </span><span>MemoryBank</span><span>() </span><span>= </span><span>default</span><span>;
</span><span>
</span><span>    [[nodiscard]] T</span><span>* </span><span>allocate</span><span>(size_t requested) {
</span><span>        std::size_t bytes </span><span>=</span><span> requested </span><span>* sizeof</span><span>(T);
</span><span>        </span><span>return </span><span>reinterpret_cast</span><span>&lt;T</span><span>*</span><span>&gt;(memory_bank.</span><span>grab_some_memory</span><span>(bytes));
</span><span>    }
</span><span>
</span><span>    </span><span>void </span><span>deallocate</span><span>(T</span><span>* </span><span>ptr</span><span>, size_t </span><span>requested</span><span>) { </span><span>return</span><span>; }
</span><span>
</span><span>    </span><span>bool </span><span>operator==</span><span>(</span><span>const</span><span> MemoryBank</span><span>&amp;</span><span>) </span><span>const </span><span>{ </span><span>return </span><span>true</span><span>; }
</span><span>}</span><span>;
</span></code></pre>
<p>This is the actual <em>custom allocator</em> object that we pass to C++ containers. The definition of a custom allocator in C++ is simply a class that provides the <code>allocate</code> and <code>deallocate</code> functions publicly. That's literally it. There are in fact more potential functions that you could add to handle specific uses, but <code>allocate</code> and <code>deallocate</code> are all we need for Ultrassembler. We define this class as a template because the return value of the <code>allocate</code> function must match the underlying type of the container using the allocator class. We furthermore define the <code>==</code> operator because C++ requires that two objects using allocators match their allocators. You'll normally never notice this because the default allocator for all C++ containers, <code>std::allocator</code>, provides all the allocator functions and operator comparison functions, and as a result, handles all comparisons transparently. Ultrassembler only uses equality. Finally, we provide a default constructor <code>MemoryBank() = default;</code> as this is what the C++ standard expects too from allocator classes.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>void</span><span>* GlobalMemoryBank::</span><span>grab_some_memory</span><span>(size_t </span><span>requested</span><span>) {
</span><span>    </span><span>if </span><span>(requested + used &gt; pool.</span><span>size</span><span>()) {
</span><span>        </span><span>throw </span><span>UASError</span><span>(OutOfMemory, "</span><span>Out of memory!</span><span>");
</span><span>    }
</span><span>    </span><span>void</span><span>* ptr = reinterpret_cast&lt;</span><span>void</span><span>*&gt;(pool.</span><span>data</span><span>() + used);
</span><span>    used += requested;
</span><span>    </span><span>return</span><span> ptr;
</span><span>}
</span><span>
</span><span>void </span><span>GlobalMemoryBank::</span><span>reset</span><span>() {
</span><span>    used = </span><span>0</span><span>;
</span><span>}
</span></code></pre>
<p>These functions implement <em>allocating the memory</em> and <em>resetting the memory bank</em>. Allocating should be obvious. However, resetting might not. As it stands, the memory pool simply gives up if it runs out of memory to allocate. We don't deallocate because such an operation would add extra overhead and subjects us to the issue of memory fragementation. Memory fragmentation is when you deallocate a small object from a large area of allocated memory, leaving a small area of unallocated memory laying in the otherwise allocated area. If you want to allocate a new object, tough luck, you probably can't fit it in this small area. You need to wait for the other objects to deallocate first. This cycle continues until your memory usage looks like Swiss cheese and doesn't support allocating any more objects, leading to a system crash. Normally, the OS kernel handles this problem transparently. Linux for example uses a "buddy allocator" to help deal with it. Memory fragmentation is also less of an issue with huge swaths of memory on modern systems. Our memory pool unfortunately lacks those luxuries of large memory and processing power for buddy allocators. Therefore, we provide the <code>reset</code> function to start everything over if the software using Ultrassembler receives an <code>OutOfMemory</code> exception.</p>
<p>Our memory pool trick lets Ultrassembler enjoy optimal memory locality and predefined memory usage while also completely eliminating syscalls (almost) and memory leaks, notwithstanding occasional memory bank resets.</p>
<h2 id="value-speculation">Value speculation</h2>
<p>A while ago, I read <a href="https://mazzo.li/posts/value-speculation.html">this fascinating article on something called L1 value speculation</a>. The basic idea is to free the branch predictor by giving it extra work to do guessing the next value in the linked list. If it's right (usually it is) then you get a free speedup.</p>
<p>Ultrassembler does something similar. Instead of a linked list, we iterate through an array checking for specific combinations of characters that define the end of a sequence to copy. </p>
<pre data-lang="cpp"><code data-lang="cpp"><span>auto</span><span> ch = [&amp;]() {
</span><span>    </span><span>return</span><span> data[i];
</span><span>};
</span><span>
</span><span>volatile char</span><span> preview;
</span><span>while </span><span>(i &lt; data.</span><span>size</span><span>() &amp;&amp; </span><span>not_at_end</span><span>(</span><span>ch</span><span>()) &amp;&amp; !</span><span>is_whitespace</span><span>(</span><span>ch</span><span>())) {
</span><span>    c.</span><span>inst</span><span>.</span><span>push_back</span><span>(</span><span>ch</span><span>());
</span><span>    i++;
</span><span>    preview = </span><span>ch</span><span>();
</span><span>}
</span></code></pre>
<p>As built-in strings in C++ are super duper mega slow even with custom allocators, we spend a lot of time on <code>c.inst.push_back(ch());</code>. There's fortunately a workaround. If the CPU knows that we'll be accessing the next character in the target string, why not queue it up first? This is exactly what <code>volatile char preview;</code> and <code>preview = ch();</code> accomplish. We already have an opportunity for speculation with the <code>i++;</code> and <code>i &lt; data.size();</code>. Although I'm not 100% sure, my hypothesis on why <code>preview</code> provides a speedup is that the branch predictor can only handle <code>i &lt; data.size()</code> and not additionally the character loading of <code>ch()</code>. Therefore, we should preemptively load <code>ch()</code> during <code>c.inst.push_back(ch());</code>. </p>
<p>Eagle eyed readers will notice how there is an opportunity for memory overflow if we are at the end of a string and <code>i++;</code> then <code>preview = ch();</code> loads a character past the string <code>data</code>. However, Ultrassembler accounts for this by preemptively adding an extra null character to the input string <code>data</code> earlier in the code, ensuring that such illegal memory accesses are impossible by definition. </p>
<p>This optimization sped up parsing of the instruction names enough that the overall Ultrassembler performance increased by about 10%.</p>
<h2 id="super-smart-searches">(Super) smart searches</h2>
<p>Here's one weird trick I haven't seen anywhere else.</p>
<p>Imagine I provided you these words: apple, apricot, avocado, and banana. </p>
<p>Now, what if I told you a mystery word I was looking for among the ones I provided was 7 letters long. You would immediately discard "apple" and "banana" because they're not 7 letters long. Now, I tell you that it starts with "a." You wouldn't discard any at this point because both "apricot" and "avocado" start with the letter a. Finally, I tell you that the second letter is "v." Immediately we know "avocado" is the mystery word because no other word remaining starts with "av."</p>
<p>This is the basic idea behind the instruction, register, CSR, and pseudoinstruction lookup systems in Ultrassembler. There's a rub, though. The code for these lookups looks something like this:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>const </span><span>uint16_t </span><span>fast_instr_search</span><span>(</span><span>const</span><span> ultrastring&amp; </span><span>inst</span><span>) {
</span><span>    </span><span>const auto</span><span> size = inst.</span><span>size</span><span>();
</span><span>
</span><span>    </span><span>if </span><span>(size == </span><span>2</span><span>) {
</span><span>        </span><span>if </span><span>(inst[</span><span>0</span><span>] == '</span><span>s</span><span>') {
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == '</span><span>d</span><span>') </span><span>return </span><span>44</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == '</span><span>w</span><span>') </span><span>return </span><span>17</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == '</span><span>b</span><span>') </span><span>return </span><span>15</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == '</span><span>h</span><span>') </span><span>return </span><span>16</span><span>;
</span><span>        }
</span><span>        </span><span>if </span><span>(inst[</span><span>0</span><span>] == '</span><span>o</span><span>') {
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == '</span><span>r</span><span>') </span><span>return </span><span>35</span><span>;
</span><span>        }
</span><span>        </span><span>if </span><span>(inst[</span><span>0</span><span>] == '</span><span>l</span><span>') {
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == '</span><span>d</span><span>') </span><span>return </span><span>43</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == '</span><span>w</span><span>') </span><span>return </span><span>12</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == '</span><span>b</span><span>') </span><span>return </span><span>10</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == '</span><span>h</span><span>') </span><span>return </span><span>11</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>if </span><span>(size == </span><span>3</span><span>) {
</span><span>        etc...
</span></code></pre>
<p>Clearly, there's a lot of work to do if you've got thousands of entries like the instructions array does. There's a fix for that though! </p>
<p>Enter codegen. </p>
<p>Ultrassembler uses artisan-crafted Python scripts to traverse through the listings and extract the string names for each instruction, register, CSR, and pseudoinstruction. Then, these scripts generate C++ code which performs these precomputed lookups. </p>
<p>Here's what the instruction search script looks like. <span>WARNING!</span> If this script looks ugly, it's because Python is one of the worst programming languages out there for anything more than mere supportive, throwaway software like this.</p>
<pre data-lang="python"><code data-lang="python"><span>input </span><span>= "</span><span>src/instructions.cpp</span><span>"
</span><span>output = "</span><span>src/generated/instruction_search.cpp</span><span>"
</span><span>
</span><span>import </span><span>re
</span><span>
</span><span>content = ""
</span><span>with </span><span>open</span><span>(</span><span>input</span><span>, "</span><span>r</span><span>") </span><span>as </span><span>file:
</span><span>    content = file.</span><span>read</span><span>()
</span><span>
</span><span>regex = "</span><span>(?&lt;={)</span><span>\"</span><span>([\w.]+)</span><span>\"</span><span>"
</span><span>
</span><span>instructions = re.</span><span>findall</span><span>(regex, content)
</span><span>
</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>len</span><span>(instructions)):
</span><span>    instructions[i] = (instructions[i], i, </span><span>len</span><span>(instructions[i]))
</span><span>
</span><span>instructions.</span><span>sort</span><span>()
</span><span>
</span><span>print</span><span>(instructions)
</span><span>
</span><span>min_len = </span><span>min</span><span>([i[</span><span>2</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions])
</span><span>
</span><span>max_len = </span><span>max</span><span>([i[</span><span>2</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions])
</span><span>
</span><span>depth = </span><span>0
</span><span>
</span><span>current_instr = ""
</span><span>
</span><span>code = "</span><span>// SPDX-License-Identifier: MPL-2.0</span><span>\n</span><span>"
</span><span>code += "</span><span>// The generate_instruction_search.py script automatically generated this code. DO NOT MODIFY!</span><span>\n</span><span>"
</span><span>code += "</span><span>#include </span><span>\"</span><span>../instructions.hpp</span><span>\"\n</span><span>"
</span><span>code += "</span><span>#include </span><span>\"</span><span>../ultrassembler.hpp</span><span>\"\n\n</span><span>"
</span><span>code += "</span><span>namespace ultrassembler_internal {</span><span>\n\n</span><span>"
</span><span>code += "</span><span>const uint16_t fast_instr_search(const ultrastring&amp; inst) {</span><span>\n</span><span>"
</span><span>code += "</span><span>    const auto size = inst.size();</span><span>\n\n</span><span>"
</span><span>
</span><span>def </span><span>ind</span><span>():
</span><span>    </span><span>return </span><span>"    " * (depth + </span><span>2</span><span>)
</span><span>
</span><span>def </span><span>instr_exists</span><span>(</span><span>instr</span><span>, </span><span>length</span><span>):
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>instructions:
</span><span>        </span><span>if </span><span>i[</span><span>0</span><span>] == instr and i[</span><span>2</span><span>] == length:
</span><span>            </span><span>return </span><span>True
</span><span>    </span><span>return </span><span>False
</span><span>    
</span><span>def </span><span>prefix_exists</span><span>(</span><span>prefix</span><span>, </span><span>length</span><span>):
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>instructions:
</span><span>        </span><span>if </span><span>i[</span><span>0</span><span>].</span><span>startswith</span><span>(prefix) and i[</span><span>2</span><span>] == length:
</span><span>            </span><span>return </span><span>True
</span><span>    </span><span>return </span><span>False
</span><span>
</span><span>potentialchars = ""
</span><span>
</span><span>for </span><span>instr </span><span>in </span><span>instructions:
</span><span>    </span><span>for </span><span>char </span><span>in </span><span>instr[</span><span>0</span><span>]:
</span><span>        </span><span>if </span><span>char not in potentialchars:
</span><span>            potentialchars += char
</span><span>
</span><span>def </span><span>process_depth</span><span>(</span><span>current_len</span><span>):
</span><span>    </span><span>global </span><span>code, current_instr, depth
</span><span>    </span><span>for </span><span>letter </span><span>in </span><span>potentialchars:
</span><span>        </span><span>if </span><span>instr_exists</span><span>(current_instr + letter, current_len):
</span><span>            code += </span><span>ind</span><span>() + </span><span>f</span><span>"</span><span>if (inst[</span><span>{depth}</span><span>] == '</span><span>{letter}</span><span>') return </span><span>{instructions[[i[</span><span>0</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions].</span><span>index</span><span>(current_instr + letter)][</span><span>1</span><span>]}</span><span>;</span><span>\n</span><span>"
</span><span>        </span><span>elif </span><span>prefix_exists</span><span>(current_instr + letter, current_len):
</span><span>            code += </span><span>ind</span><span>() + </span><span>f</span><span>"</span><span>if (inst[</span><span>{depth}</span><span>] == '</span><span>{letter}</span><span>') </span><span>{{\n</span><span>"
</span><span>            current_instr += letter
</span><span>            depth += </span><span>1
</span><span>            </span><span>process_depth</span><span>(current_len)
</span><span>            depth -= </span><span>1
</span><span>            current_instr = current_instr[:-</span><span>1</span><span>]
</span><span>            code += </span><span>ind</span><span>() + "</span><span>}</span><span>\n</span><span>"
</span><span>
</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(min_len, max_len + </span><span>1</span><span>):
</span><span>    code += </span><span>f</span><span>"</span><span>    if (size == </span><span>{i}</span><span>) </span><span>{{\n</span><span>"
</span><span>    </span><span>process_depth</span><span>(i)
</span><span>    code += "</span><span>    }</span><span>\n\n</span><span>"
</span><span>
</span><span>code += "</span><span>    return instr_search_failed;</span><span>\n</span><span>"
</span><span>code += "</span><span>}</span><span>\n\n</span><span>"
</span><span>code += "</span><span>} // namespace ultrassembler_internal</span><span>"
</span><span>
</span><span>print</span><span>(code)
</span><span>
</span><span>with </span><span>open</span><span>(output, "</span><span>w</span><span>") </span><span>as </span><span>file:
</span><span>    file.</span><span>write</span><span>(code)
</span></code></pre>
<p>Let's go through it section by section.</p>
<pre data-lang="python"><code data-lang="python"><span>input </span><span>= "</span><span>src/instructions.cpp</span><span>"
</span><span>output = "</span><span>src/generated/instruction_search.cpp</span><span>"
</span><span>
</span><span>import </span><span>re
</span><span>
</span><span>content = ""
</span><span>with </span><span>open</span><span>(</span><span>input</span><span>, "</span><span>r</span><span>") </span><span>as </span><span>file:
</span><span>    content = file.</span><span>read</span><span>()
</span></code></pre>
<p>This simply tells the script what file to read and where to generate the code, imports the regex package, and reads the input file.</p>
<pre data-lang="python"><code data-lang="python"><span>regex = "</span><span>(?&lt;={)</span><span>\"</span><span>([\w.]+)</span><span>\"</span><span>"
</span><span>
</span><span>instructions = re.</span><span>findall</span><span>(regex, content)
</span><span>
</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>len</span><span>(instructions)):
</span><span>    instructions[i] = (instructions[i], i, </span><span>len</span><span>(instructions[i]))
</span><span>
</span><span>instructions.</span><span>sort</span><span>()
</span><span>
</span><span>print</span><span>(instructions)
</span></code></pre>
<p>This regex searches for all instances of quotes in the instruction C++ code. That code looks like this:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>const</span><span> std::array&lt;rvinstruction, </span><span>2034</span><span>&gt; instructions = {
</span><span>        {{"</span><span>lui</span><span>", LUI, U, op_LUI, </span><span>0b000</span><span>, RVI, int_reg},
</span><span>         {"</span><span>auipc</span><span>", AUIPC, U, op_AUIPC, </span><span>0b000</span><span>, RVI, int_reg},
</span><span>         {"</span><span>jal</span><span>", JAL, J, op_JAL, </span><span>0b000</span><span>, RVI, int_reg}, etc...
</span></code></pre>
<p>Then, it creates a new array with the instruction name, what position it is in the array, and its length. This might seem redundant at first, but it's helpful later. We then sort all the insructions alphabetically (also important!) and show all of them for debugging/status purposes.</p>
<pre data-lang="python"><code data-lang="python"><span>min_len = </span><span>min</span><span>([i[</span><span>2</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions])
</span><span>
</span><span>max_len = </span><span>max</span><span>([i[</span><span>2</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions])
</span><span>
</span><span>depth = </span><span>0
</span><span>
</span><span>current_instr = ""
</span><span>
</span><span>code = "</span><span>// SPDX-License-Identifier: MPL-2.0</span><span>\n</span><span>"
</span><span>code += "</span><span>// The generate_instruction_search.py script automatically generated this code. DO NOT MODIFY!</span><span>\n</span><span>"
</span><span>code += "</span><span>#include </span><span>\"</span><span>../instructions.hpp</span><span>\"\n</span><span>"
</span><span>code += "</span><span>#include </span><span>\"</span><span>../ultrassembler.hpp</span><span>\"\n\n</span><span>"
</span><span>code += "</span><span>namespace ultrassembler_internal {</span><span>\n\n</span><span>"
</span><span>code += "</span><span>const uint16_t fast_instr_search(const ultrastring&amp; inst) {</span><span>\n</span><span>"
</span><span>code += "</span><span>    const auto size = inst.size();</span><span>\n\n</span><span>"
</span><span>
</span><span>def </span><span>ind</span><span>():
</span><span>    </span><span>return </span><span>"    " * (depth + </span><span>2</span><span>)
</span><span>
</span><span>def </span><span>instr_exists</span><span>(</span><span>instr</span><span>, </span><span>length</span><span>):
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>instructions:
</span><span>        </span><span>if </span><span>i[</span><span>0</span><span>] == instr and i[</span><span>2</span><span>] == length:
</span><span>            </span><span>return </span><span>True
</span><span>    </span><span>return </span><span>False
</span><span>    
</span><span>def </span><span>prefix_exists</span><span>(</span><span>prefix</span><span>, </span><span>length</span><span>):
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>instructions:
</span><span>        </span><span>if </span><span>i[</span><span>0</span><span>].</span><span>startswith</span><span>(prefix) and i[</span><span>2</span><span>] == length:
</span><span>            </span><span>return </span><span>True
</span><span>    </span><span>return </span><span>False
</span><span>
</span><span>potentialchars = ""
</span><span>
</span><span>for </span><span>instr </span><span>in </span><span>instructions:
</span><span>    </span><span>for </span><span>char </span><span>in </span><span>instr[</span><span>0</span><span>]:
</span><span>        </span><span>if </span><span>char not in potentialchars:
</span><span>            potentialchars += char
</span></code></pre>
<p>This is a lot of boilerplate for the algorithm later to come. We find the shortest and longest instructions. We add the first parts of the generated file. We define an indentation helper for nice formatting. We define additional helper functions to check if a whole instruction exists with a given name and length or if there is an instruction with the provided prefix and length. Finally, we assemble an array with all the characters to search for that the instructions use to avoid unnecessary computation later.</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>process_depth</span><span>(</span><span>current_len</span><span>):
</span><span>    </span><span>global </span><span>code, current_instr, depth
</span><span>    </span><span>for </span><span>letter </span><span>in </span><span>potentialchars:
</span><span>        </span><span>if </span><span>instr_exists</span><span>(current_instr + letter, current_len):
</span><span>            code += </span><span>ind</span><span>() + </span><span>f</span><span>"</span><span>if (inst[</span><span>{depth}</span><span>] == '</span><span>{letter}</span><span>') return </span><span>{instructions[[i[</span><span>0</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions].</span><span>index</span><span>(current_instr + letter)][</span><span>1</span><span>]}</span><span>;</span><span>\n</span><span>"
</span><span>        </span><span>elif </span><span>prefix_exists</span><span>(current_instr + letter, current_len):
</span><span>            code += </span><span>ind</span><span>() + </span><span>f</span><span>"</span><span>if (inst[</span><span>{depth}</span><span>] == '</span><span>{letter}</span><span>') </span><span>{{\n</span><span>"
</span><span>            current_instr += letter
</span><span>            depth += </span><span>1
</span><span>            </span><span>process_depth</span><span>(current_len)
</span><span>            depth -= </span><span>1
</span><span>            current_instr = current_instr[:-</span><span>1</span><span>]
</span><span>            code += </span><span>ind</span><span>() + "</span><span>}</span><span>\n</span><span>"
</span><span>
</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(min_len, max_len + </span><span>1</span><span>):
</span><span>    code += </span><span>f</span><span>"</span><span>    if (size == </span><span>{i}</span><span>) </span><span>{{\n</span><span>"
</span><span>    </span><span>process_depth</span><span>(i)
</span><span>    code += "</span><span>    }</span><span>\n\n</span><span>"
</span></code></pre>
<p>Here's where the magic happens. We process one instruction length depth at a time. Like the algorithm we talked about at the beginning of this section, we start with the shortest possible "words" and work our way to the longest. Each depth step works through a search of all the possible characters and first checks if we have already found an instruction. If there is such an instruction, we add it to the code. Alternatively, if there is no such instruction but there is in fact an instruction that starts with the current sequence, we go down a depth level because we know that eventually, we will find an instruction with an exact match. Once we've gone through all of the possible instructions and depths, we exit the <code>for</code> loop.</p>
<pre data-lang="python"><code data-lang="python"><span>code += "</span><span>    return instr_search_failed;</span><span>\n</span><span>"
</span><span>code += "</span><span>}</span><span>\n\n</span><span>"
</span><span>code += "</span><span>} // namespace ultrassembler_internal</span><span>"
</span><span>
</span><span>print</span><span>(code)
</span><span>
</span><span>with </span><span>open</span><span>(output, "</span><span>w</span><span>") </span><span>as </span><span>file:
</span><span>    file.</span><span>write</span><span>(code)
</span></code></pre>
<p>This completes the generated search function, shows it all for debugging/status purposes, and finally writes the generated code to the output file path.</p>
<p>There are no other instances of this kind of codegen that I know of. That's surprising, because codegen allows us to perform lookup of thousands of instructions with near-zero overhead. I estimate each instruction lookup takes on the order of 10 instructions to complete.</p>
<p>Here's what the resulting compiled assembly looks like on my x86 PC:</p>
<pre><code><span>0000000000029340 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE&gt;:
</span><span>   29340:	f3 0f 1e fa          	endbr64 
</span><span>   29344:	48 8b 47 08          	mov    0x8(%rdi),%rax
</span><span>   29348:	48 83 f8 02          	cmp    $0x2,%rax
</span><span>   2934c:	0f 84 c6 00 00 00    	je     29418 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0xd8&gt;
</span><span>   29352:	48 83 f8 03          	cmp    $0x3,%rax
</span><span>   29356:	75 28                	jne    29380 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x40&gt;
</span><span>   29358:	48 8b 17             	mov    (%rdi),%rdx
</span><span>   2935b:	0f b6 0a             	movzbl (%rdx),%ecx
</span><span>   2935e:	80 f9 61             	cmp    $0x61,%cl
</span><span>   29361:	0f 84 79 2b 00 00    	je     2bee0 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x2ba0&gt;
</span><span>   29367:	80 f9 64             	cmp    $0x64,%cl
</span><span>   2936a:	0f 85 58 10 00 00    	jne    2a3c8 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x1088&gt;
</span><span>   29370:	80 7a 01 69          	cmpb   $0x69,0x1(%rdx)
</span><span>   29374:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
</span><span>   29379:	0f 84 09 2f 00 00    	je     2c288 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x2f48&gt;
</span><span>   2937f:	c3                   	ret
</span><span>   # There are thousands more lines of this!
</span></code></pre>
<p>And RISC-V:</p>
<pre><code><span>000000000007c33c &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE&gt;:
</span><span>   7c33c:	7179                	addi	sp,sp,-48
</span><span>   7c33e:	f406                	sd	ra,40(sp)
</span><span>   7c340:	e42a                	sd	a0,8(sp)
</span><span>   7c342:	6522                	ld	a0,8(sp)
</span><span>   7c344:	00089317          	auipc	t1,0x89
</span><span>   7c348:	afc33303          	ld	t1,-1284(t1) # 104e40 &lt;_ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcEN22ultrassembler_internal10MemoryBankIcEEE4sizeEv@@Base+0xad9c4&gt;
</span><span>   7c34c:	9302                	jalr	t1
</span><span>   7c34e:	ec2a                	sd	a0,24(sp)
</span><span>   7c350:	6762                	ld	a4,24(sp)
</span><span>   7c352:	4789                	li	a5,2
</span><span>   7c354:	22f71c63          	bne	a4,a5,7c58c &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x250&gt;
</span><span>   7c358:	4581                	li	a1,0
</span><span>   7c35a:	6522                	ld	a0,8(sp)
</span><span>   7c35c:	00089317          	auipc	t1,0x89
</span><span>   7c360:	c6433303          	ld	t1,-924(t1) # 104fc0 &lt;_ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcEN22ultrassembler_internal10MemoryBankIcEEEixEm@@Base+0xaaef8&gt;
</span><span>   7c364:	9302                	jalr	t1
</span><span>   # Also thousands more lines of this!
</span></code></pre>
<h2 id="compile-time-templates">Compile-time templates</h2>
<p>This is similar to script codegen but with native C++ only.</p>
<p>One of the verification steps in Ultrassembler involves checking that the immediate value of an instruction (for example, <code>addi t0, t1, 100</code>) fits within some known range. C++ allows us to both cleanly invoke this check for an arbitrary range and do so with little to no runtime overhead to calculate that range.</p>
<p>Here's how it works.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>template </span><span>&lt;</span><span>auto</span><span> bits&gt;
</span><span>void </span><span>verify_imm</span><span>(</span><span>const auto</span><span>&amp; </span><span>imm</span><span>) {
</span><span>    </span><span>using </span><span>T = </span><span>decltype</span><span>(bits);
</span><span>    </span><span>if constexpr </span><span>(std::is_signed_v&lt;T&gt;) {
</span><span>        </span><span>if </span><span>(imm &lt; -(</span><span>1 </span><span>&lt;&lt; (bits - </span><span>1</span><span>)) || imm &gt;= (</span><span>1 </span><span>&lt;&lt; (bits - </span><span>1</span><span>))) {
</span><span>            </span><span>throw </span><span>UASError</span><span>(ImmOutOfRange, "</span><span>Immediate </span><span>" + </span><span>to_ultrastring</span><span>(imm) + "</span><span> is out of range [</span><span>" + </span><span>to_ultrastring</span><span>(-(</span><span>1 </span><span>&lt;&lt; (bits - </span><span>1</span><span>))) + "</span><span>, </span><span>" + </span><span>to_ultrastring</span><span>((</span><span>1 </span><span>&lt;&lt; (bits - </span><span>1</span><span>))) + "</span><span>)</span><span>", </span><span>0</span><span>, </span><span>0</span><span>);
</span><span>        }
</span><span>    } </span><span>else if constexpr </span><span>(std::is_unsigned_v&lt;T&gt;) {
</span><span>        </span><span>if </span><span>(imm &lt; </span><span>0 </span><span>|| imm &gt;= (</span><span>1</span><span>u </span><span>&lt;&lt; bits)) {
</span><span>            </span><span>throw </span><span>UASError</span><span>(ImmOutOfRange, "</span><span>Immediate </span><span>" + </span><span>to_ultrastring</span><span>(imm) + "</span><span> is out of range [0, </span><span>" + </span><span>to_ultrastring</span><span>((</span><span>1</span><span>u </span><span>&lt;&lt; bits)) + "</span><span>)</span><span>", </span><span>0</span><span>, </span><span>0</span><span>);
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Each invocation looks something like <code>verify_imm&lt;5u&gt;(imm)</code>. We supply a numeric literal and the immediate variable to check. C++'s template facilities then check whether we've supplied a signed or unsigned numeric literal, as RISC-V instruction can vary whether they expect signed or unsigned numbers only. We then calculate the lowest possible number (<code>-(1 &lt;&lt; (bits - 1))</code> for signed and <code>0</code> for unsigned) and the highest possible number (<code>(1 &lt;&lt; (bits - 1))</code> for signed and <code>(1u &lt;&lt; bits)</code> for unsigned) and check the input against that. We then throw an error if it doesn't fit these calculated constraints or return silently if it does.</p>
<p>The <code>if constexpr</code> tells the compiler to generate each signed or unsigned execution path at compile time depending on what numeric literal we've provided, allowing us to make each function call as pretty and fast as possible.</p>
<h2 id="fast-string-comparisons">Fast string comparisons</h2>
<p>For the times where we can't or don't want to use a precomputed string search, Ultrassembler uses an optimized string comparison function to minimize overhead.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>bool </span><span>fast_eq</span><span>(</span><span>const auto</span><span>&amp; </span><span>first</span><span>, </span><span>const</span><span> std::string_view&amp; </span><span>second</span><span>) {
</span><span>    </span><span>if </span><span>(first.</span><span>size</span><span>() != second.</span><span>size</span><span>()) { 
</span><span>        </span><span>return </span><span>false</span><span>;
</span><span>    }
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; first.</span><span>size</span><span>(); i++) {
</span><span>        </span><span>if </span><span>(first[i] != second[i]) {
</span><span>            [[likely]] </span><span>return </span><span>false</span><span>;
</span><span>        } </span><span>else </span><span>{
</span><span>            [[unlikely]] </span><span>continue</span><span>;
</span><span>        }
</span><span>    }
</span><span>    </span><span>return </span><span>true</span><span>;
</span><span>}
</span></code></pre>
<p>How does this work? First, we check to make sure the input strings are the same length. It's impossible by definition for them to be the same if they have different lengths. Then, we compare them character by character. Here, we use C++20's <code>[[likely]]</code> and <code>[[unlikely]]</code> tags to help the compiler optimize the positioning of each comparison. It's statistically more likely to have a comparison failure than a success because we are usually comparing one input string against many possible options but it can only match with up to one.</p>
<h2 id="reference-bigger-than-fundamental-objects-in-function-arguments">Reference bigger-than-fundamental objects in function arguments</h2>
<p>This one surprised me.</p>
<p>When you call a C++ function, you can choose to pass your arguments <em>by value</em>, or <em>by reference</em>. By default, C++ uses <em>by value</em>, which means the code internally makes a copy of the argument and provides that copy to the function. If you add a <code>&amp;</code> to make it a reference instead (there are other ways to do this too) then the code generates a pointer to that original object and passes that pointer to the function. However, unlike pointers, references handle referencing and dereferencing transparently. As an aside, this also means Ultrassembler technically doesn't use pointers... anywhere! Pointers are horrible.</p>
<p>One of the most common pieces of C++ optimization advice is to use references whenever possible to avoid the copy overhead incurred by value references. It might surprise you, then, to find out that the following code is vastly faster due to the use of a value argument:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>size_t </span><span>parse_this_line</span><span>(size_t </span><span>i</span><span>, </span><span>const</span><span> ultrastring&amp; </span><span>data</span><span>, assembly_context&amp; </span><span>c</span><span>) {
</span><span>    </span><span>// code that does "i++;" a lot
</span><span>}
</span><span>
</span><span>// later, in a different function:
</span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; data.</span><span>size</span><span>();) {
</span><span>    i = </span><span>parse_this_line</span><span>(i, data, c);
</span><span>    </span><span>// etc...
</span><span>}
</span></code></pre>
<p>If we had applied the Programming Furus©️®️™️'s advice to pass <code>i</code> by reference, it would have looked like:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>void </span><span>parse_this_line</span><span>(size_t&amp; </span><span>i</span><span>, </span><span>const</span><span> ultrastring&amp; </span><span>data</span><span>, assembly_context&amp; </span><span>c</span><span>) {
</span><span>    </span><span>// code that does "i++;" a lot
</span><span>}
</span><span>
</span><span>// later, in a different function:
</span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; data.</span><span>size</span><span>();) {
</span><span>    </span><span>parse_this_line</span><span>(i, data, c);
</span><span>    </span><span>// etc...
</span><span>}
</span></code></pre>
<p>So why is the first one faster? Here's why.</p>
<p>Under the hood of all programming languages, you have assembly code which translates to the CPU's machine code. There are also no variables. Instead, you've got registers which hold raw data and raw memory. In most application processors today, the registers are 64 bits wide, and maybe wider for special vector operations which don't matter here. 64 bits happens to match the maximum width of so-called <em>fundamental types</em> in C and C++ which are integers and most common floats. Therefore, we can fit at least one fundamental type into each register.</p>
<p>Quick refresher of the registers in RISC-V:</p>
<p><img src="https://jghuff.com/RISC-V-registers-safe.svg" alt="Infographic of the integer RISC-V registers"></p>
<p>Assembly also has little concept of a function call. Internally, all function calls do is clear out the current registers, load them with the function parameters, then jump to the function's address. This means all function calls involve at least one copy per argument, whether it's a fundamental type or a pointer to a fundamental type or a pointer to something else.</p>
<pre><code><span># Here's what this looks like in RISC-V assembly.
</span><span># Say we have a number in register t0, like 69.
</span><span>
</span><span>addi t0, x0, 69
</span><span>
</span><span># We also have a function foobar that takes a single integer argument (like "void foobar(size_t arg)" in C/C++)
</span><span># We can copy that register (and therefore its value) to argument register a0 before calling foobar
</span><span>
</span><span>addi a0, t0, 0
</span><span>
</span><span>jal foobar
</span><span>
</span><span># The copying of this value only took one step!
</span></code></pre>
<p>You can see where we're going. If our goal is to minimize copying, it would be better to copy a fundamental type once than to generate a pointer, copy that, then dereference that pointer to get the underlying value. That is the crux of this subtle optimization trick. The cost to copy one register is less than the cost to copy a register holding a pointer. </p>
<p>Note how I've only talked about fundamental types. Any type which does not fit in a single register, AKA many structs, containers, or anything else that isn't a fundamental type, costs more to copy by value in multiple registers than it does to copy a single register holding a pointer. I don't know of any Programming Furu©️®️™️ that makes this distinction clear.</p>
<h2 id="don-t-do-insertions-or-deletions">Don't do insertions or deletions</h2>
<p>One of the steps to assemble a jump operation in RISC-V assembly is to calculate the offset of bytes to the jump target. However, this is often impossible unless all other instructions are already assembled. Ultrassembler does its best to avoid insertions or deletions through a clever trick to assemble jump instructions with a placeholder jump offset and then insert the correct offset in-place at the end.</p>
<p>Here's how it works:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>void </span><span>solve_label_offsets</span><span>(assembly_context&amp; </span><span>c</span><span>) {
</span><span>    </span><span>using </span><span>enum RVInstructionFormat;
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; c.</span><span>label_locs</span><span>.</span><span>size</span><span>(); i++) {
</span><span>        </span><span>if </span><span>(!c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>is_dest</span><span>) {
</span><span>            </span><span>for </span><span>(size_t j = </span><span>0</span><span>; j &lt; c.</span><span>label_locs</span><span>.</span><span>size</span><span>(); j++) {
</span><span>                </span><span>if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(j).</span><span>is_dest </span><span>&amp;&amp; c.</span><span>label_locs</span><span>.</span><span>at</span><span>(j).</span><span>id </span><span>== c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>id</span><span>) {
</span><span>                    uint32_t inst = </span><span>0</span><span>;
</span><span>
</span><span>                    </span><span>if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>i_bytes </span><span>== </span><span>2</span><span>) {
</span><span>                        inst = reinterpret_cast&lt;uint16_t&amp;&gt;(c.</span><span>machine_code</span><span>.</span><span>at</span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>loc</span><span>));
</span><span>                    } </span><span>else if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>i_bytes </span><span>== </span><span>4</span><span>) {
</span><span>                        inst = reinterpret_cast&lt;uint32_t&amp;&gt;(c.</span><span>machine_code</span><span>.</span><span>at</span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>loc</span><span>));
</span><span>                    }
</span><span>
</span><span>                    int32_t offset = c.</span><span>label_locs</span><span>.</span><span>at</span><span>(j).</span><span>loc </span><span>- c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>loc</span><span>;
</span><span>
</span><span>                    </span><span>if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>format </span><span>== Branch) {
</span><span>                        inst &amp;= </span><span>0b00000001111111111111000001111111</span><span>;
</span><span>                        inst |= ((offset &gt;&gt; </span><span>11</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>7</span><span>;      </span><span>// Add imm[11]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>1</span><span>) &amp; </span><span>0b1111</span><span>) &lt;&lt; </span><span>8</span><span>;    </span><span>// Add imm[4:1]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>5</span><span>) &amp; </span><span>0b111111</span><span>) &lt;&lt; </span><span>25</span><span>; </span><span>// Add imm[10:5]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>12</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>31</span><span>;     </span><span>// Add imm[12]
</span><span>                    } </span><span>else if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>format </span><span>== J) {
</span><span>                        inst &amp;= </span><span>0b00000000000000000000111111111111</span><span>;
</span><span>                        inst |= ((offset &gt;&gt; </span><span>12</span><span>) &amp; </span><span>0b11111111</span><span>) &lt;&lt; </span><span>12</span><span>;  </span><span>// Add imm[19:12]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>11</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>20</span><span>;         </span><span>// Add imm[11]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>1</span><span>) &amp; </span><span>0b1111111111</span><span>) &lt;&lt; </span><span>21</span><span>; </span><span>// Add imm[10:1]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>20</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>31</span><span>;         </span><span>// Add imm[20]
</span><span>                    } </span><span>else if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>format </span><span>== CJ) {
</span><span>                        inst &amp;= </span><span>0b1110000000000011</span><span>;
</span><span>                        inst |= ((offset &gt;&gt; </span><span>5</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>2</span><span>;   </span><span>// Add offset[5]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>1</span><span>) &amp; </span><span>0b111</span><span>) &lt;&lt; </span><span>3</span><span>; </span><span>// Add offset[3:1]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>7</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>6</span><span>;   </span><span>// Add offset[7]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>6</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>7</span><span>;   </span><span>// Add offset[6]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>10</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>8</span><span>;  </span><span>// Add offset[10]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>8</span><span>) &amp; </span><span>0b11</span><span>) &lt;&lt; </span><span>9</span><span>;  </span><span>// Add offset[9:8]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>4</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>11</span><span>;  </span><span>// Add offset[4]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>11</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>12</span><span>; </span><span>// Add offset[11]
</span><span>                    } </span><span>else if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>format </span><span>== CB) {
</span><span>                        inst &amp;= </span><span>0b1110001110000011</span><span>;
</span><span>                        inst |= ((offset &gt;&gt; </span><span>5</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>2</span><span>;   </span><span>// Add offset[5]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>1</span><span>) &amp; </span><span>0b11</span><span>) &lt;&lt; </span><span>3</span><span>;  </span><span>// Add offset[2:1]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>6</span><span>) &amp; </span><span>0b11</span><span>) &lt;&lt; </span><span>5</span><span>;  </span><span>// Add offset[7:6]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>3</span><span>) &amp; </span><span>0b11</span><span>) &lt;&lt; </span><span>10</span><span>; </span><span>// Add offset[4:3]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>8</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>12</span><span>;  </span><span>// Add offset[8]
</span><span>                    }
</span><span>
</span><span>                    </span><span>if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>i_bytes </span><span>== </span><span>2</span><span>) {
</span><span>                        reinterpret_cast&lt;uint16_t&amp;&gt;(c.</span><span>machine_code</span><span>.</span><span>data</span><span>()[c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>loc</span><span>]) = inst;
</span><span>                    } </span><span>else if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>i_bytes </span><span>== </span><span>4</span><span>) {
</span><span>                        reinterpret_cast&lt;uint32_t&amp;&gt;(c.</span><span>machine_code</span><span>.</span><span>data</span><span>()[c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>loc</span><span>]) = inst;
</span><span>                    }
</span><span>                }
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>When we find a jump instruction that needs later TLC, we save its location and some other attributes to a special array. Then, after the rest of the code is done assembling, we go back through each jump instruction and calculate the correct offset and insert that offset in-place in the correct instruction format.</p>
<p>I believe this is faster than what some other assemblers do for instructions which jump to a location reachable within the constraints of the offset's size. However, it's not useful for far jumps, which require a separate helper instruction to extend the jump. Ultrassembler doesn't support those yet.</p>
<h2 id="more-optimizations">More optimizations</h2>
<p>Here's a few more optimization tricks that aren't quite significant enough for their own sections but deserve a mention anyway.</p>
<h2 id="memory-padding">Memory padding</h2>
<p>There are a few strings which Ultrassembler frequently reads and writes. To insure against runtime memory pool allocation overhead, we preemptively allocate a good amount of memory.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>c.</span><span>inst</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg1</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg2</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg3</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg4</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg5</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg6</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg_extra</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>machine_code</span><span>.</span><span>reserve</span><span>(</span><span>128000</span><span>);
</span></code></pre>
<p>I found that 32 bytes gave the biggest speedup for small strings, and sizes above a few kB are more appropriate for the machine code output.</p>
<h2 id="inline-some-functions">Inline some functions</h2>
<p>Sometimes, functions are faster when you mark them <code>inline</code> to suggest that the code have a copy for each invocation. This tends to work better for smaller functions.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>inline const </span><span>uint8_t </span><span>decode_encoding_length</span><span>(</span><span>const </span><span>uint8_t </span><span>opcode</span><span>) {
</span><span>    </span><span>if </span><span>((opcode &amp; </span><span>0b11</span><span>) != </span><span>0b11</span><span>) {
</span><span>        </span><span>return </span><span>2</span><span>;
</span><span>    } </span><span>else </span><span>{
</span><span>        </span><span>return </span><span>4</span><span>;
</span><span>    }
</span><span>}
</span></code></pre>
<p>Try it and see what works best for your own code.</p>
<h2 id="minimize-string-stripping-copies">Minimize string stripping copies</h2>
<p>Here's a special case of minimizing string copying. This function removes the parentheses and optionally the number 0 from a string like "0(t4)":</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>void </span><span>remove_extraneous_parentheses</span><span>(ultrastring&amp; </span><span>str</span><span>) {
</span><span>    </span><span>if </span><span>(str.</span><span>back</span><span>() == '</span><span>)</span><span>') {
</span><span>        str.</span><span>pop_back</span><span>();
</span><span>    }
</span><span>    </span><span>if </span><span>(str.</span><span>front</span><span>() == '</span><span>0</span><span>') {
</span><span>        str.</span><span>erase</span><span>(</span><span>0</span><span>, </span><span>1</span><span>);
</span><span>    }
</span><span>    </span><span>if </span><span>(str.</span><span>front</span><span>() == '</span><span>(</span><span>') {
</span><span>        str.</span><span>erase</span><span>(</span><span>0</span><span>, </span><span>1</span><span>);
</span><span>    }
</span><span>}
</span></code></pre>
<p>Why do we tackle the last character first? When you erase one or more characters from a string, C++ internally copies every individual character after setting the characters to erase to blank. In other words, it looks a little like this:</p>
<pre><code><span># Erase "foo" from "foobar"
</span><span>
</span><span>foobar
</span><span>
</span><span> oobar
</span><span>
</span><span>  obar
</span><span>
</span><span>   bar
</span><span>
</span><span>b  bar
</span><span>
</span><span>ba bar
</span><span>
</span><span>barbar
</span><span>
</span><span>barba
</span><span>
</span><span>barb
</span><span>
</span><span>bar
</span></code></pre>
<p>That's a lot of copies. So it would be great if we can avoid copying more of these characters in the future. Then, we handle the case where the input string is like "(t4)" where there is no 0 at the beginning. Finally is the removal of the front parenthesis. </p>
<p>This optimization yielded a surprising speedup (several percent overall) due to how often the case of "0(reg)" shows up in RISC-V assembly.</p>
<h2 id="call-small-lambda-functions-frequently">Call small lambda functions frequently</h2>
<p>These three lambda functions both help make parsing faster and simplify the code:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>auto</span><span> is_whitespace = [](</span><span>const char</span><span>&amp; c) {
</span><span>    </span><span>return</span><span> c == '</span><span>\t</span><span>' || c == ' ';
</span><span>};
</span><span>auto</span><span> ch = [&amp;]() {
</span><span>    </span><span>return</span><span> data[i];
</span><span>};
</span><span>auto</span><span> not_at_end = [](</span><span>const char</span><span>&amp; c) {
</span><span>    </span><span>return</span><span> c != '</span><span>\n</span><span>' &amp;&amp; c != '</span><span>#</span><span>';
</span><span>};
</span></code></pre>
<p>Why do they work? The simplification part is obvious, but maybe not for speed. One reason might be because the compiler now knows how often we do the same comparisons over and over. If it knows we do the same thing many times, it can optimize with that known fact.</p>
<p>Also note how the first and last functions violate the earlier optimization trick regarding passing fundamental types by value. That trick does not entirely apply to lambda functions, which work differently, where they could be inline and incur zero function call overhead. Passing by reference enables the zero function call overhead optimization.</p>
<h2 id="strip-out-the-compilation-junk">Strip out the compilation junk</h2>
<p>By default, C++ compilers like GCC and Clang add in a lot of junk that we can safely strip out. Here's how we do it in CMake:</p>
<pre data-lang="cmake"><code data-lang="cmake"><span>target_compile_options</span><span>(objultra </span><span>PRIVATE </span><span>-fno-rtti -fno-stack-protector -fomit-frame-pointer)
</span></code></pre>
<h3 id="fno-rtti">-fno-rtti</h3>
<p>RTTI is runtime type identification. Only some software uses this feature but it adds nonzero overhead to all. Therefore, we disable it to eliminate that overhead.</p>
<h3 id="fno-stack-protector">-fno-stack-protector</h3>
<p>The stack protector is a feature that many Programming Furus©️®️™️ peddle to improve security. However, it adds considerable overhead, and does nothing for security outside of a specific attack. Therefore, we disable it to eliminate that overhead.</p>
<h3 id="fomit-frame-pointer">-fomit-frame-pointer</h3>
<p>The frame pointer is a specific feature on some CPU platforms (like x86). However, it's not actually needed anymore for modern CPUs, and it adds overhead. Therefore, we disable it to eliminate that overhead.</p>
<h2 id="link-time-optimization">Link-time optimization</h2>
<p>Link-time optimization, or LTO, is a more intelligent way for the compiler to optimize your code than regular optimization passes. It can enable some serious speedups if your code benefits from function inlining or has code across many files. It's been supported for a while now but isn't enabled by default. Here's how to enable it in CMake:</p>
<pre data-lang="cmake"><code data-lang="cmake"><span>include</span><span>(CheckIPOSupported)
</span><span>check_ipo_supported</span><span>(</span><span>RESULT </span><span>lto_supported)
</span><span>if</span><span>(lto_supported AND NOT NO_LTO)
</span><span>  </span><span>set_property</span><span>(</span><span>TARGET </span><span>${</span><span>this_target</span><span>} </span><span>PROPERTY </span><span>INTERPROCEDURAL_OPTIMIZATION TRUE)
</span><span>  </span><span>if</span><span>(CMAKE_COMPILER_IS_GNUCXX)
</span><span>    </span><span>list</span><span>(</span><span>APPEND </span><span>CMAKE_CXX_COMPILE_OPTIONS_IPO "</span><span>-flto=auto</span><span>") </span><span># set the thread amount to what is available on the CPU
</span><span>  </span><span>endif</span><span>()
</span><span>endif</span><span>()
</span></code></pre>
<p>This has been nothing but a benefit for Ultrassembler.</p>
<h2 id="make-structs-memory-friendly">Make structs memory-friendly</h2>
<p>This struct holds variables that most of the Ultrassembler code uses:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>struct </span><span>assembly_context {
</span><span>    ultrastring inst;
</span><span>    ultrastring arg1;
</span><span>    ultrastring arg2;
</span><span>    ultrastring arg3;
</span><span>    ultrastring arg4;
</span><span>    ultrastring arg5;
</span><span>    ultrastring arg6;
</span><span>    ultrastring arg_extra;
</span><span>    ultravector&lt;uint8_t&gt; machine_code;
</span><span>    ultravector&lt;RVInstructionSet&gt; supported_sets;
</span><span>    ultravector&lt;std::pair&lt;ultrastring, </span><span>int</span><span>&gt;&gt; labels;
</span><span>    ultravector&lt;label_loc&gt; label_locs;
</span><span>    ultravector&lt;std::pair&lt;ultrastring, ultrastring&gt;&gt; constants;
</span><span>    ultravector&lt;directive_options&gt; options;
</span><span>    int32_t custom_inst = </span><span>0</span><span>;
</span><span>    uint32_t line = </span><span>1</span><span>;
</span><span>    uint32_t column = </span><span>0</span><span>;
</span><span>    uint16_t inst_offset = </span><span>0</span><span>;
</span><span>};
</span></code></pre>
<p>We order them in descending memory size, from 32 bytes for <code>ultrastring</code> to 2 for <code>uint16_t</code>. This packs the members the most efficient way possible for memory usage.</p>
<p>Also, these variables are not in the global scope or a namespace because holding them all in a struct enables multithreaded operation. It would be possible to add <code>thread_local</code> to each one to enable multithreading easily, but in testing, this added enormous overhead compared to a plain old struct.</p>
<h2 id="memory-locality">Memory locality</h2>
<p>Memory locality is the general idea that the most frequently accessed memory should be close together. Ultrassembler has many such cases, and we already help ensure memory locality through preallocated memory pools. We go further by ensuring sections of code which frequently work on one area of memory get their own space to work with.</p>
<p>Here's an example:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>void </span><span>make_inst</span><span>(assembly_context&amp; </span><span>c</span><span>) {
</span><span>    </span><span>// boilerplate
</span><span>
</span><span>    uint32_t inst = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>// code which modifies this inst variable
</span><span>
</span><span>    reinterpret_cast&lt;uint32_t&amp;&gt;(c.</span><span>machine_code</span><span>[c.</span><span>machine_code</span><span>.</span><span>size</span><span>() - bytes]) = inst;
</span><span>}
</span></code></pre>
<p>We work on the local <code>inst</code> variable to prevent far reaches across memory to the <code>c.machine_code</code> vector. When we're done, we write to <code>c.machine_code</code> once and invoke only one far memory access as a result.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Congrats if you read all the way here!</p>
<p>Hopefully you've learned something new and/or useful. Although I've crafted the optimizations here for Ultrassembler, there's nothing stopping you from applying the same underlying principles to your own code. </p>
<p>Check out Ultrassembler: <a href="https://github.com/Slackadays/Chata/tree/main/ultrassembler">https://github.com/Slackadays/Chata/tree/main/ultrassembler</a></p>



        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Use One Big Server (2022) (300 pts)]]></title>
            <link>https://specbranch.com/posts/one-big-server/</link>
            <guid>45085029</guid>
            <pubDate>Sun, 31 Aug 2025 17:29:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://specbranch.com/posts/one-big-server/">https://specbranch.com/posts/one-big-server/</a>, See on <a href="https://news.ycombinator.com/item?id=45085029">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A lot of ink is spent on the "monoliths vs. microservices" debate, but the real issue behind
this debate is about whether distributed system architecture is worth the developer time and
cost overheads.  By thinking about the real operational considerations of our systems, we can
get some insight into whether we actually need distributed systems for most things.</p>
<p>We have all gotten so familiar with virtualization and abstractions between our software
and the servers that run it.  These days, "serverless" computing is all the rage, and even
"bare metal" is a class of virtual machine.  However, every piece of software runs on a
server.  Since we now live in a world of virtualization, most of these servers are a lot
bigger and a lot cheaper than we actually think.</p>
<h2 id="meet-your-server">Meet Your Server</h2>
<figure><img src="https://www.servethehome.com/wp-content/uploads/2021/03/Microsoft-Azure-HPC-HBv3-Hosting-Node-2.jpg#center" width="70%">
</figure>

<p>This is a picture of a server used by Microsoft Azure with AMD CPUs.  Starting from the left,
the big metal fixture on the left (with the copper tubes) is a heatsink, and the metal boxes
that the copper tubes are attached to are heat exchangers on each CPU.  The CPUs are AMD's
third generation server CPU, each of which has the following specifications:</p>
<ul>
<li>64 cores</li>
<li>128 threads</li>
<li>~2-2.5 GHz clock</li>
<li>Cores capable of 4-6 instructions per clock cycle</li>
<li>256 MB of L3 cache</li>
</ul>
<p>In total, this server has 128 cores with 256 simultaneous threads.  With all of the cores working
together, this server is capable of 4 TFLOPs of peak double precision computing performance. This
server would sit at the top of the top500 supercomputer list in early 2000. It would take until
2007 for this server to leave the top500 list.  Each CPU core is substantially more powerful than a
single core from 10 years ago, and boasts a much wider computation pipeline.</p>
<p>Above and below each CPU is the memory: 16 slots of DDR4-3200 RAM per socket.  The largest
capacity "cost effective" DIMMs today are 64 GB.  Populated cost-efficiently, this server can hold
<strong>1 TB</strong> of memory.  Populated with specialized high-capacity DIMMs (which are generally slower
than the smaller DIMMs), this server supports up to <strong>8 TB</strong> of memory total.  At DDR4-3200, with
a total of 16 memory channels, this server will likely see ~200 Gbps of memory throughput across
all of its cores.</p>
<p>In terms of I/O, each CPU offers 64 PCIe gen 4 lanes.  With 128 PCIe lanes total, this server is
capable of supporting 30 NVMe SSDs plus a network card.  Typical configurations you can buy will
offer slots for around 16 SSDs or disks. The final thing I wanted to point out in this picture is
in the top right, the network card.  This server is likely equipped with a 50-100 Gbps network
connection.</p>
<h4 id="the-capabilities-of-one-server">The Capabilities of One Server</h4>
<p>One server today is capable of:</p>
<ul>
<li><a href="https://people.freebsd.org/~gallatin/talks/euro2021.pdf">Serving video files at 400 Gbps</a> (now <a href="http://nabstreamingsummit.com/wp-content/uploads/2022/05/2022-Streaming-Summit-Netflix.pdf">800 Gbps</a>)</li>
<li><a href="https://www.scylladb.com/2017/05/10/faster-and-better-what-to-expect-running-scylla-on-aws-i3-instances/">1 million IOPS on a NoSQL database</a></li>
<li><a href="https://www.enterprisedb.com/blog/pgbench-performance-benchmark-postgresql-12-and-edb-advanced-server-12">70k IOPS in PostgreSQL</a></li>
<li><a href="https://openbenchmarking.org/test/pts/nginx">500k requests per second to nginx</a></li>
<li><a href="https://openbenchmarking.org/test/pts/build-linux-kernel-1.14.0">Compiling the linux kernel in 20 seconds</a></li>
<li><a href="https://openbenchmarking.org/test/pts/x264-2.7.0">Rendering 4k video with x264 at 75 FPS</a></li>
</ul>
<p>Among other things.  There are a lot of public benchmarks these days, and if you know how your
service behaves, you can probably find a similar benchmark.</p>
<h4 id="the-cost-of-one-server">The Cost of One Server</h4>
<p>In a large hosting provider, OVHCloud, you can rent an HGR-HCI-6 server with similar specifications
to the above, with 128 physical cores (256 threads), 512 GB of memory, and 50 Gbps of bandwidth
for $1,318/month.</p>
<p>Moving to the popular budget option, Hetzner, you can rent a smaller server with 32 physical cores
and 128 GB of RAM for about €140.00/month.  This is a smaller server than the one from OVHCloud
(1/4 the size), but it gives you some idea of the price spread between hosting providers.</p>
<p>In AWS, one of the largest servers you can rent is the m6a.metal server. It offers 50 Gbps
of network bandwidth, 192 vCPUs (96 physical cores), and 768 GB of memory, and costs $8.2944/hour
in the US East region.  This comes out to $6,055/month.  The cloud premium is real!</p>
<p>A similar server, with 128 physical cores and 512 GB of memory (as well as appropriate NICs,
SSDs, and support contracts), can be purchased from the Dell website for about $40,000.  However,
if you are going to spend this much on a server, you should probably chat with a salesperson to
make sure you are getting the best deal you can.  You will also need to pay to host this server
and connect it to a network, though.</p>
<p>In comparison, buying servers takes about 8 months to break even compared to using cloud servers,
and 30 months to break even compared to renting.  Of course, buying servers has a lot of drawbacks,
and so does renting, so going forward, we will think a little bit about the "cloud premium" and
whether you should be willing to pay it (spoiler alert: the answer is "yes, but not as much as the
cloud companies want you to pay").</p>
<h2 id="thinking-about-the-cloud">Thinking about the Cloud</h2>
<p>The "cloud era" began in earnest around 2010.  At the time, the state of the art CPU was an
8-core Intel Nehalem CPU.  Hyperthreading had just begun, so that 8-core CPU offered a
whopping 16 threads.  Hardware acceleration was about to arrive for AES encryption, and
vectors were 128 bits wide. The largest CPUs had 24 MB of cache, and your server could fit a
whopping 256 GB of DDR3-1066 memory. If you wanted to store data, Seagate had just begun to
offer a 3 TB hard drive.  Each core offered 4 FLOPs per cycle, meaning that your 8-core
server running at 2.5 GHz offered a blazing fast 80 GFLOPs.</p>
<p>The boom in distributed computing rode on this wave: if you wanted to do anything that
involved retrieval of data, you needed a lot of disks to get the storage throughput you want.
If you wanted to do large computations, you generally needed a lot of CPUs. This meant that
you needed to coordinate between a lot of CPUs to get most things done.</p>
<p>Since that time began, the size of servers has increased a lot, and SSDs have increased available
IOPS by a factor of at least 100, but the size of mainstream VMs and containers hasn't increased
much, and we still use virtualized drives that perform more like hard drives than SSDs (although
this gap is closing).</p>
<h4 id="one-server-plus-a-backup-is-usually-plenty">One Server (Plus a Backup) is Usually Plenty</h4>
<p>If you are doing anything short of video streaming, and you have under 10k QPS, one server
will generally be fine for most web services.  For really simple services, one server could
even make it to a million QPS or so.  Very few web services get this much traffic - if you
have one, you know about it.  Even if you're serving video, running only one server for your
control plane is very reasonable.  A benchmark can help you determine where you are.
Alternatively, you can use common benchmarks of similar applications, or
<a href="https://specbranch.com/posts/common-perf-numbers/">tables of common performance numbers</a> to estimate how big of a
machine you might need.</p>
<h4 id="tall-is-better-than-wide">Tall is Better than Wide</h4>
<p>When you need a cluster of computers, if one server is not enough, using fewer larger servers
will often be better than using a large fleet of small machines.  There is non-zero overhead
to coordinate a cluster, and that overhead is frequently O(n) on each server.  To reduce this
overhead, you should generally prefer to use a few large servers than to use many small servers.
In the case of things like serverless computing, where you allocate tiny short-lived containers,
this overhead accounts for a large fraction of the cost of use.  On the other extreme end,
coordinating a cluster of one computer is trivial.</p>
<h4 id="big-servers-and-availability">Big Servers and Availability</h4>
<p>The big drawback of using a single big server is availability.  Your server is going to need
downtime, and it is going to break.  Running a primary and a backup server is usually enough,
keeping them in different datacenters.  A 2x2 configuration should appease the truly paranoid: two
servers in a primary datacenter (or cloud provider) and two servers in a backup datacenter will
give you a lot of redundancy.  If you want a third backup deployment, you can often make that
smaller than your primary and secondary.</p>
<p>However, you may still have to be concerned about <em>correlated</em> hardware failures.  Hard drives
(and now SSDs) have been known to occasionally have correlated failures: if you see one disk
fail, you are a lot more likely to see a second failure before getting back up if your disks
are from the same manufacturing batch.  Services like Backblaze overcome this by using many
different models of disks from multiple manufacturers.  Hacker news learned this the hard way
recently when the primary and backup server went down at the same time.</p>
<p>If you are using a hosting provider which rents pre-built servers, it is prudent to rent two
different types of servers in each of your primary and backup datacenters.  This should avoid
almost every failure mode present in modern systems.</p>
<h2 id="use-the-cloud-but-dont-be-too-cloudy">Use the Cloud, but don't be too Cloudy</h2>
<p>A combination of availability and ease of use is one of the big reasons why I (and most other
engineers) like cloud computers.  Yes, you pay a significant premium to rent the machines, but
your cloud provider has so much experience building servers that you don't even see most failures,
and for the other failures, you can get back up and running really quickly by renting a new
machine in their nearly-limitless pool of compute.  It is their job to make sure that you don't
experience downtime, and while they don't always do it perfectly, they are pretty good at it.</p>
<p>Hosting providers who are willing to rent you a server are a cheaper alternative to cloud
providers, but these providers can sometimes have poor quality and some of them don't understand
things like network provisioning and correlated hardware failures. Also, moving from one rented
server to a larger one is a lot more annoying than resizing a cloud VM. Cloud servers have a
price premium for a good reason.</p>
<p>However, when you deal with clouds, your salespeople will generally push you towards
"cloud-native" architecture.  These are things like microservices in auto-scaling VM groups with
legions of load balancers between them, and vendor-lock-in-enhancing products like serverless
computing and managed high-availability databases.  There is a good reason that cloud
salespeople are the ones pushing "cloud architecture" - it's better for them!</p>
<p>The conventional wisdom is that using cloud architecture is good because it lets you scale up
effortlessly. There are good reasons to use cloud-native architecture, but serving lots of people
is not one of them: most services can serve millions of people at a time with one server, and
will never give you a surprise five-figure bill.</p>
<h4 id="why-should-i-pay-for-peak-load">Why Should I Pay for Peak Load?</h4>
<p>One common criticism of the "one big server" approach is that you now have to pay for your peak
usage instead of paying as you go for what you use.  Thus, serverless computing or fleets of
microservice VMs more closely align your costs with your profit.</p>
<p>Unfortunately, since all of your services run on servers (whether you like it or not), someone
in that supply chain is charging you based on their peak load.  Part of the "cloud premium" for
load balancers, serverless computing, and small VMs is based on how much extra capacity your
cloud provider needs to build in order to handle <em>their</em> peak load.  You're paying for someone's
peak load anyway!</p>
<p>This means that if your workload is exceptionally bursty - like a simulation that needs
to run once and then turn off forever - you should prefer to reach for "cloudy" solutions, but if
your workload is not so bursty, you will often have a cheaper system (and an easier time building
it) if you go for few large servers.  If your cloud provider's usage is more bursty than yours,
you are going to pay that premium for no benefit.</p>
<p>This premium applies to VMs, too, not just cloud services. However, if you are running a cloud VM
24/7, you can avoid paying the "peak load premium" by using 1-year contracts or negotiating with
a salesperson if you are big enough.</p>
<p>Generally, the burstier your workload is, the more cloudy your architecture should be.</p>
<h4 id="how-much-does-it-cost-to-be-cloudy">How Much Does it Cost to be Cloudy?</h4>
<p>Being cloudy is expensive.  Generally, I would anticipate a 5-30x price premium depending on what
you buy from a cloud company, and depending on the baseline. <em>Not 5-30%, a factor of between 5 and
30.</em></p>
<p>Here is the pricing of AWS lambda: $0.20 per 1M requests + $0.0000166667 per GB-second of RAM.  I
am using pricing for an x86 CPU here to keep parity with the m6a.metal instance we saw above.
Large ARM servers and serverless ARM compute are both cheaper.</p>
<p>Assuming your server costs $8.2944/hour, and is capable of 1k QPS with 768 GB of RAM:</p>
<ul>
<li>
<p>1k QPS is 60k queries per minute, or 3.6M queries per hour</p>
</li>
<li>
<p>Each query here gets 0.768 GB-seconds of RAM (amortized)</p>
</li>
<li>
<p>Replacing this server would cost about $46/hour using serverless computing</p>
</li>
</ul>
<p>The price premium for serverless computing over the instance is a factor of 5.5.  If you can keep
that server over 20% utilization, using the server will be cheaper than using serverless computing.
This is before any form of savings plan you can apply to that server - if you can rent those big
servers from the spot market or if you compare to the price you can get with a 1-year contract,
the price premium is even higher.</p>
<p><em><strong>If you compare to the OVHCloud rental price for the same server, the price premium of buying your
compute through AWS lambda is a factor of 25</strong></em></p>
<p>If you are considering renting a server from a low-cost hosting provider or using AWS lambda, you
should prefer the hosting provider if you can keep the server operating at 5% capacity!</p>
<p>Also, note that the actual QPS number doesn't matter: if the $8.2944/hour server is capable of 100k
QPS, the query would use 100x less memory-time, meaning that you would arrive at the same 5.5x
(or 25x) premium. Of course, you should scale the size of the server to fit your application.</p>
<h2 id="common-objections-to-one-big-server">Common Objections to One Big Server</h2>
<p>If you propose using the one big server approach, you will often get pushback from people who are
more comfortable with the cloud, prefer to be fashionable, or have legitimate concerns.  Use your
judgment when you think about it, but most people vastly underestimate how much "cloud
architecture" actually costs compared to the underlying compute.  Here are some common objections.</p>
<h4 id="but-if-i-use-cloud-architecture-i-dont-have-to-hire-sysadmins">But if I use Cloud Architecture, I Don't Have to Hire Sysadmins</h4>
<p>Yes you do.  They are just now called "Cloud Ops" and are under a different manager. Also, their
ability to read the arcane documentation that comes from cloud companies and keep up  with the
corresponding torrents of updates and deprecations makes them 5x more expensive than system
administrators.</p>
<h4 id="but-if-i-use-cloud-architecture-i-dont-have-to-do-security-updates">But if I use Cloud Architecture, I Don't Have to Do Security Updates</h4>
<p>Yes you do.  You may have to do fewer of them, but the ones you don't have to do are the easy ones
to automate.  You are still going to share in the pain of auditing libraries you use, and making
sure that all of your configurations are secure.</p>
<h4 id="but-if-i-use-cloud-architecture-i-dont-have-to-worry-about-it-going-down">But if I use Cloud Architecture, I Don't Have to Worry About it Going Down</h4>
<p>The "high availability" architectures you get from using cloudy constructs and microservices just
about make up for the fragility they add due to complexity.  At this point, if you use two
different cloud regions or two cloud providers, you can generally assume that is good enough to
avoid your service going down.  However, cloud providers have often had global outages in the past,
and there is no reason to assume that cloud datacenters will be down any less often than your
individual servers.</p>
<p>Remember that we are trying to prevent <em>correlated</em> failures.  Cloud datacenters have a lot of
parts that can fail in correlated ways.  Hosting providers have many fewer of these parts.
Similarly, complex cloud services, like managed databases, have more failure modes than simple
ones (VMs).</p>
<h4 id="but-i-can-develop-more-quickly-if-i-use-cloud-architecture">But I can Develop More Quickly if I use Cloud Architecture</h4>
<p>Then do it, and just keep an eye on the bill and think about when it's worth it to switch.  This
is probably the strongest argument in favor of using cloudy constructs.  However, if you don't
think about it as you grow, you will likely end up burning a lot of money on your cloudy
architecture long past the time to switch to something more boring.</p>
<h4 id="my-workload-is-really-bursty">My Workload is Really Bursty</h4>
<p>Cloud away.  That is a great reason to use things like serverless computing.  One of the big
benefits of cloud architecture constructs is that the <em>scale down</em> really well.  If your workload
goes through long periods of idleness punctuated with large unpredictable bursts of activity, cloud
architecture probably works really well for you.</p>
<h4 id="what-about-cdns">What about CDNs?</h4>
<p>It's impossible to get the benefits of a CDN, both in latency improvements and bandwidth savings,
with one big server.  This is also true of other systems that need to be distributed, like backups.
Thankfully CDNs and backups are competitive markets, and relatively cheap. These are the kind of
thing to buy rather than build.</p>
<h2 id="a-note-on-microservices-and-monoliths">A Note On Microservices and Monoliths</h2>
<p>Thinking about "one big server" naturally lines up with thinking about monolithic architectures.
However, you don't need to use a monolith to use one server.  You can run many containers on one
big server, with one microservice per container.  However, microservice architectures in general
add a lot of overhead to a system for dubious gain when you are running on one big server.</p>
<h2 id="conclusions">Conclusions</h2>
<p>When you experience growing pains, and get close to the limits of your current servers, today's
conventional wisdom is to go for sharding and horizontal scaling, or to use a cloud architecture
that gives you horizontal scaling "for free."  It is often easier and more efficient to scale
vertically instead.  Using one big server is comparatively cheap, keeps your overheads at a
minimum, and actually has a pretty good availability story if you are careful to prevent correlated
hardware failures.  It's not glamorous and it won't help your resume, but one big server will serve
you well.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: How do you fight YouTube addiction and procrastination? I'm struggling (135 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=45085014</link>
            <guid>45085014</guid>
            <pubDate>Sun, 31 Aug 2025 17:27:17 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=45085014">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bigbox"><td><table><tbody><tr id="45085014"><td><span></span></td><td><center><a id="up_45085014" href="https://news.ycombinator.com/vote?id=45085014&amp;how=up&amp;goto=item%3Fid%3D45085014"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=45085014">Ask HN: How do you fight YouTube addiction and procrastination? I'm struggling</a></span></td></tr><tr><td colspan="2"></td><td><span><span id="score_45085014">105 points</span> by <a href="https://news.ycombinator.com/user?id=angelochecked">angelochecked</a> <span title="2025-08-31T17:27:17 1756661237"><a href="https://news.ycombinator.com/item?id=45085014">7 hours ago</a></span> <span id="unv_45085014"></span> | <a href="https://news.ycombinator.com/hide?id=45085014&amp;goto=item%3Fid%3D45085014">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20How%20do%20you%20fight%20YouTube%20addiction%20and%20procrastination%3F%20I%27m%20struggling&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=45085014&amp;auth=4ce2c00ea8f5ec1678f7c52d34f2182ba5d4a83d">favorite</a> | <a href="https://news.ycombinator.com/item?id=45085014">99&nbsp;comments</a></span></td></tr><tr><td colspan="2"></td><td><div><p>My current daily routine looks like this:</p><p>- 8:00~9:00 – Getting ready for work</p><p>- 9:00–13:00 – Work</p><p>- 13:00–14:00 – Lunch + YouTube</p><p>- 14:00–18:00 – Work</p><p>- 18:00–20:00 – Break from work + Dinner + YouTube</p><p>- 20:00~1:00 – YouTube, gaming, occasional events, personal projects, or sports. Lately, I’ve noticed my screen time during this period has increased a lot, and I’ve been feeling lazy to do anything productive—mostly just doomscrolling or watching videos</p><p>What’s your routine like? How do you manage your time, maintain social connections, avoid digital distractions, and stay on track with your goals and learning?</p></div></td></tr><tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr></tbody></table><br>
</td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When the sun will literally set on what's left of the British Empire (218 pts)]]></title>
            <link>https://oikofuge.com/sun-sets-on-british-empire/</link>
            <guid>45084913</guid>
            <pubDate>Sun, 31 Aug 2025 17:15:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oikofuge.com/sun-sets-on-british-empire/">https://oikofuge.com/sun-sets-on-british-empire/</a>, See on <a href="https://news.ycombinator.com/item?id=45084913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<div>
<figure><a href="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?ssl=1" target="_blank" rel=" noreferrer noopener"><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="474" height="241" data-attachment-id="21378" data-permalink="https://oikofuge.com/june-solstice-terminator/" data-orig-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?fit=1600%2C814&amp;ssl=1" data-orig-size="1600,814" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Without British Indian Ocean Territory, night falls on the British Empire, June" data-image-description="<p>Without British Indian Ocean Territory, night falls on the British Empire, June</p>
" data-image-caption="" data-medium-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?fit=300%2C153&amp;ssl=1" data-large-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?fit=474%2C241&amp;ssl=1" src="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?resize=474%2C241&amp;ssl=1" alt="Without British Indian Ocean Territory, night falls on the British Empire, June" srcset="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?resize=1024%2C521&amp;ssl=1 1024w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?resize=300%2C153&amp;ssl=1 300w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?resize=768%2C391&amp;ssl=1 768w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?resize=1536%2C781&amp;ssl=1 1536w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?w=1600&amp;ssl=1 1600w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?w=948&amp;ssl=1 948w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/June-solstice-terminator.png?w=1422&amp;ssl=1 1422w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption>Click to enlarge</figcaption></figure></div>


<p>A while ago I treated you to a dissertation entitled “<a href="https://oikofuge.com/sun-set-british-empire/" data-type="link" data-id="https://oikofuge.com/sun-set-british-empire/" target="_blank" rel="noreferrer noopener">Does The Sun Set On The British Empire?</a>”, and concluded that it doesn’t. The UK’s widely scattered overseas territories, sparse though they are, mean that the sun is still always shining, somewhere in the world, over British territory.</p>



<p>The most important territories in maintaining this late-empire sunlight are the Pitcairn Islands, in the Pacific, and the British Indian Ocean Territory, in the Indian Ocean. To illustrate that, I offered the sunlight chart below, showing how Pitcairn and BIOT catch the sunlight when it’s dark in the UK.</p>


<div>
<figure><a href="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset3.png?ssl=1" target="_blank" rel=" noreferrer noopener"><img data-recalc-dims="1" decoding="async" width="474" height="301" data-attachment-id="20922" data-permalink="https://oikofuge.com/britishempiresunset3/" data-orig-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset3.png?fit=1200%2C763&amp;ssl=1" data-orig-size="1200,763" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Sunrise and sunset in Greenwich, Pitcairn &amp; BIOT" data-image-description="<p>Sunrise and sunset in Greenwich, Pitcairn &amp; BIOT</p>
" data-image-caption="<p>Click to enlarge</p>
" data-medium-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset3.png?fit=300%2C191&amp;ssl=1" data-large-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset3.png?fit=474%2C301&amp;ssl=1" src="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset3.png?resize=474%2C301&amp;ssl=1" alt="Sunrise and sunset in Greenwich, Pitcairn &amp; BIOT" srcset="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset3.png?resize=1024%2C651&amp;ssl=1 1024w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset3.png?resize=300%2C191&amp;ssl=1 300w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset3.png?resize=768%2C488&amp;ssl=1 768w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset3.png?w=1200&amp;ssl=1 1200w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset3.png?w=948&amp;ssl=1 948w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption>Click to enlarge</figcaption></figure></div>


<p>In fact, as my map at the head of this post shows, BIOT is pivotal. There, I’ve plotted the distribution of light and darkness, across the globe, at 02:15 Greenwich Mean Time, during the June solstice of 2024.<strong><mark>*</mark></strong></p>



<p>And here’s the situation at the December solstice:</p>


<div>
<figure><a href="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?ssl=1" target="_blank" rel=" noreferrer noopener"><img data-recalc-dims="1" decoding="async" width="474" height="241" data-attachment-id="21377" data-permalink="https://oikofuge.com/december-solstice-terminator/" data-orig-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?fit=1600%2C814&amp;ssl=1" data-orig-size="1600,814" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Without British Indian Ocean Territory, night falls on the British Empire, December" data-image-description="<p>Without British Indian Ocean Territory, night falls on the British Empire, December</p>
" data-image-caption="<p>Click to enlarge</p>
" data-medium-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?fit=300%2C153&amp;ssl=1" data-large-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?fit=474%2C241&amp;ssl=1" src="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?resize=474%2C241&amp;ssl=1" alt="Without British Indian Ocean Territory, night falls on the British Empire, December" srcset="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?resize=1024%2C521&amp;ssl=1 1024w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?resize=300%2C153&amp;ssl=1 300w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?resize=768%2C391&amp;ssl=1 768w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?resize=1536%2C781&amp;ssl=1 1536w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?w=1600&amp;ssl=1 1600w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?w=948&amp;ssl=1 948w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/12/December-solstice-terminator.png?w=1422&amp;ssl=1 1422w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption>Click to enlarge</figcaption></figure></div>


<p>Just after the sun sets in Pitcairn, it’s dark over every British territory except BIOT.</p>



<p>I’m revisiting the situation because the UK government has announced plans to hand over sovereignty of the Chagos Archipelago, which houses BIOT, to Mauritius. The announcement was made in October 2024, but the original agreement has now been contested by a new government in Mauritius. And the situation is further complicated by the fact that BIOT houses a large US military base on the island of Diego Garcia, so the new Trump administration also has a say in the process. (Meanwhile, the unfortunate Chagossians, evicted from their homeland in 1968 to make way for the military base, have so far been given no voice in the negotiations.)</p>



<p>The current proposal suggests that the military base would be maintained under a long-term lease agreement, in which case British sovereignty would be lost, and BIOT would cease to exist. At that point, the role of easternmost British territory would fall to the Sovereign Base Areas (SBAs), in Cyprus.</p>



<p>The SBAs are worth a few paragraphs, both because they’re relatively obscure, and because their existence, as sovereign military territories, perhaps has some slight relevance to how the situation on Diego Garcia might play out, should the Trump administration raise strong objections to the current plan.</p>



<p>The SBAs came into existence when Cyprus gained its independence from the UK in 1960. Under the Treaty of Establishment, the UK retained sovereignty over about 250 square kilometres of the island, in two separate areas—the Western Sovereign Base Area of Akrotiri, and the Eastern Sovereign Base Area of Dhekelia. These have extremely complicated boundaries, designed to avoid Cypriot settlements while including British military establishments. The Eastern SBA contains three Cypriot enclaves—the towns of Ormideia and Xylotymbou, and the area surrounding the Dhekelia power station (which is crossed by a British road). It also features a long northward extension along the road to the village of Ayios Nikolaos, which now houses a signals intelligence unit.</p>



<p>And the whole border situation became even more complicated after the Turkish invasion of Cyprus in 1974, which has left the island traversed by a UN buffer zone. British territory, including the Ayios Nikolaos road, forms part of the buffer zone. Elsewhere, the Turkish-controlled town of Kokkina has its very own buffer zone. Here’s an overview map, followed by some detail of the SBAs:</p>







<p>(Interestingly, the British military settlements within the SBAs are referred to as <em>cantonments</em>, a military term which, to me at least, has something of a colonial ring to it, given its association with British rule in India.)</p>



<p>The relevance, here, to the current situation of Diego Garcia, is because the UK government made plans to hand the SBAs back to Cyprus in 1974, but were persuaded to retain sovereignty by the USA, which valued access to signals intelligence in the Eastern Mediterranean, as well as a convenient location from which to fly, among other things, <a href="https://web.archive.org/web/20181011213916/https://cyprus-mail.com/2018/10/11/anastasiades-stays-mum-on-secret-us-base/" data-type="link" data-id="https://web.archive.org/web/20181011213916/https://cyprus-mail.com/2018/10/11/anastasiades-stays-mum-on-secret-us-base/" target="_blank" rel="noreferrer noopener">U2 spy planes</a>. The difference, of course, is that the Cypriot government appears to have been compliant with that arrangement, whereas it seems unlikely, at time of writing, that the Mauritians would agree to such a deal.</p>



<p>We’ll see how it goes. Meanwhile, I’ve plotted another sunrise/sunset graph, showing how sunlight is handed off between the two key players in the absence of BIOT: </p>


<div>
<figure><a href="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset4.jpg?ssl=1" target="_blank" rel=" noreferrer noopener"><img data-recalc-dims="1" loading="lazy" decoding="async" width="474" height="303" data-attachment-id="21004" data-permalink="https://oikofuge.com/britishempiresunset4/" data-orig-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset4.jpg?fit=1200%2C766&amp;ssl=1" data-orig-size="1200,766" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Without BIOT, night falls on the British Empire, between sunset in Pitcairn and sunrise in the Sovereign Base Areas, Cyprus" data-image-description="<p>Without BIOT, night falls on the British Empire, between sunset in Pitcairn and sunrise in the Sovereign Base Areas, Cyprus</p>
" data-image-caption="<p>Click to enlarge</p>
" data-medium-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset4.jpg?fit=300%2C192&amp;ssl=1" data-large-file="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset4.jpg?fit=474%2C303&amp;ssl=1" src="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset4.jpg?resize=474%2C303&amp;ssl=1" alt="Without BIOT, night falls on the British Empire, between sunset in Pitcairn and sunrise in the Sovereign Base Areas, Cyprus" srcset="https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset4.jpg?resize=1024%2C654&amp;ssl=1 1024w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset4.jpg?resize=300%2C192&amp;ssl=1 300w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset4.jpg?resize=768%2C490&amp;ssl=1 768w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset4.jpg?w=1200&amp;ssl=1 1200w, https://i0.wp.com/oikofuge.com/wp-content/uploads/2024/10/britishempiresunset4.jpg?w=948&amp;ssl=1 948w" sizes="auto, (max-width: 474px) 100vw, 474px"></a><figcaption>Click to enlarge</figcaption></figure></div>


<p>(For my sunlight calculation, I’ve plugged in the latitude and longitude of the easternmost part of the Eastern SBA—Ayios Nikolaos.)</p>



<p>It’s close—in June there’s less than an hour when it’s dark in both Pitcairn and the SBAs. But, if BIOT goes, when the sun sets on Pitcairn, it will also set on (what’s left of) the British Empire.</p>



<hr>



<p><strong><mark>*</mark></strong> <small>I haven’t plotted British Antarctic Territory, because territorial claims in Antarctica are in abeyance under the Antarctic Treaty.</small></p>





	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Don't Have Spotify (221 pts)]]></title>
            <link>https://idonthavespotify.sjdonado.com/</link>
            <guid>45084673</guid>
            <pubDate>Sun, 31 Aug 2025 16:50:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://idonthavespotify.sjdonado.com/">https://idonthavespotify.sjdonado.com/</a>, See on <a href="https://news.ycombinator.com/item?id=45084673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Paste a link from Spotify, YouTube Music, Apple Music, Deezer or SoundCloud to start.</p></div></div>]]></description>
        </item>
    </channel>
</rss>