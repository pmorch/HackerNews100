<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 22 Sep 2023 08:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Suppressing negative thoughts may be good for mental health after all (121 pts)]]></title>
            <link>https://www.cam.ac.uk/research/news/suppressing-negative-thoughts-may-be-good-for-mental-health-after-all-study-suggests</link>
            <guid>37607203</guid>
            <pubDate>Fri, 22 Sep 2023 02:35:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cam.ac.uk/research/news/suppressing-negative-thoughts-may-be-good-for-mental-health-after-all-study-suggests">https://www.cam.ac.uk/research/news/suppressing-negative-thoughts-may-be-good-for-mental-health-after-all-study-suggests</a>, See on <a href="https://news.ycombinator.com/item?id=37607203">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Researchers at the Medical Research Council (MRC) Cognition and Brain Sciences Unit trained 120 volunteers worldwide to suppress thoughts about negative events that worried them, and found that not only did these become less vivid, but that the participants’ mental health also improved.</p>

<p>“We’re all familiar with the Freudian idea that if we suppress our feelings or thoughts, then these thoughts remain in our unconscious, influencing our behaviour and wellbeing perniciously,” said Professor Michael Anderson.</p>

<p>“The whole point of psychotherapy is to dredge up these thoughts so one can deal with them and rob them of their power. In more recent years, we’ve been told that suppressing thoughts is intrinsically ineffective and that it actually causes people to think the thought more – it’s the classic idea of ‘Don’t think about a pink elephant’."</p>

<p>These ideas have become dogma in the clinical treatment realm, said Anderson, with national guidelines talking about thought avoidance as a major maladaptive coping behaviour to be eliminated and overcome in depression, anxiety, PTSD, for example.</p>

<p>When COVID-19 appeared in 2020, like many researchers, Professor Anderson wanted to see how his own research could be used to help people through the pandemic. His interest lay in a brain mechanism known as inhibitory control – the ability to override our reflexive responses – and how it might be applied to memory retrieval, and in particular to stopping the retrieval of negative thoughts when confronted with potent reminders to them.</p>

<p>Dr Zulkayda Mamat – at the time a PhD student in Professor Anderson’s lab and at Trinity College, Cambridge – believed that inhibitory control was critical in overcoming trauma in experiences occurring to herself and many others she has encountered in life. She had wanted to investigate whether this was an innate ability or something that was learnt – and hence could be taught.</p>

<p>Dr Mamat said: “Because of the pandemic, we were seeing a need in the community to help people cope with surging anxiety. There was already a mental health crisis, a hidden epidemic of mental health problems, and this was getting worse. So with that backdrop, we decided to see if we could help people cope better.”</p>

<p>Professor Anderson and Dr Mamat recruited 120 people across 16 countries to test whether it might in fact be possible – and beneficial – for people to practice suppressing their fearful thoughts. Their findings are published today in <em>Science Advances</em>.</p>

<p>In the study, each participant was asked to think of a number of scenarios that might plausibly occur in their lives over the next two years – 20 negative ‘fears and worries’ that they were afraid might happen, 20 positive ‘hopes and dreams’, and 36 routine and mundane neutral events. The fears had to be worries of current concern to them, that have repeatedly intruded in their thoughts.</p>

<p>Each event had to be specific to them and something they had vividly imagined occurring. For each scenario, they were to provide a cue word (an obvious reminder that could be used to evoke the event during training) and a key detail (a single word expressing a central event detail). For example:</p>

<ul>
	<li>Negative – visiting one’s parents at the hospital as a result of COVID-19, with the cue ‘Hospital’ and the detail ‘Breathing’.</li>
	<li>Neutral – a visit to the opticians, with the cue ‘Optician’ and the detail ‘Cambridge’.</li>
	<li>Positive – seeing one’s sister get married, with the cue ‘Wedding’ and the detail ‘Dress’.</li>
</ul>

<p>Participants were asked to rate each event on a number of points: vividness, likelihood of occurrence, distance in the future, level of anxiety about the event (or level of joy for positive events), frequency of thought, degree of current concern, long-term impact, and emotional intensity.</p>

<p>Participants also completed questionnaires to assess their mental health, though no one was excluded, allowing the researchers to look at a broad range of participants, including many with serious depression, anxiety, and pandemic-related post-traumatic stress.</p>

<p>Then, over Zoom, Dr Mamat took each participant through the 20-minute training, which involved 12 ‘No-imagine’ and 12 ‘Imagine’ repetitions for events, each day for three days.</p>

<p>For No-imagine trials, participants were given one of their cue words, asked to first acknowledge the event in their mind.&nbsp; Then, while continuing to stare directly at the reminder cue, they were asked to stop thinking about the event – they should not try to imagine the event itself or use diversionary thoughts to distract themselves, but rather should try to block any images or thoughts that the reminder might evoke. &nbsp;For this part of the trial, one group of participants was given their negative events to suppress and the other given their neutral ones.</p>

<p>For Imagine trials, participants were given a cue word and asked to imagine the event as vividly as possible, thinking what it would be like and imagining how they would feel at the event. For ethical reasons, no participant was given a negative event to imagine, but only positive or neutral ones.</p>

<p>At the end of the third day and again three months later, participants were once again asked to rate each event on vividness, level of anxiety, emotional intensity, etc., and completed questionnaires to assess changes in depression, anxiety, worry, affect, and wellbeing, key facets of mental health.</p>

<p>Dr Mamat said: “It was very clear that those events that participants practiced suppressing were less vivid, less emotionally anxiety-inducing, than the other events and that overall, participants improved in terms of their mental health. But we saw the biggest effect among those participants who were given practice at suppressing fearful, rather than neutral, thoughts.”&nbsp;</p>

<p>Following training – both immediately and after three months – participants reported that suppressed events were less vivid and less fearful. They also found themselves thinking about these events less.</p>

<p>Suppressing thoughts even improved mental health amongst participants with likely post-traumatic stress disorder. Among participants with post-traumatic stress who suppressed negative thoughts, their negative mental health indices scores fell on average by 16% (compared to a 5% fall for similar participants suppressing neutral events), whereas positive mental health indices scores increased by almost 10% (compared to a 1% fall in the second group).</p>

<p>In general, people with worse mental health symptoms at the outset of the study improved more after suppression training, but only if they suppressed their fears. This finding directly contradicts the notion that suppression is a maladaptive coping process.</p>

<p>Suppressing negative thoughts did not lead to a ‘rebound’, where a participant recalled these events more vividly. Only one person out of 120 showed higher detail recall for suppressed items post-training, and just six of the 61 participants that suppressed fears reported increased vividness for No-Imagine items post-training, but this was in line with the baseline rate of vividness increases that occurred for events that were not suppressed at all. &nbsp;</p>

<p>“What we found runs counter to the accepted narrative,” said Professor Anderson. “Although more work will be needed to confirm the findings, it seems like it is possible and could even be potentially beneficial to actively suppress our fearful thoughts.”</p>

<p>Although participants were not asked to continue practising the technique, many of them chose to do so spontaneously. When Dr Mamat contacted the participants after three months, she found that the benefits in terms of reduced levels of depression and negative emotions, continued for all participants, but were most pronounced among those participants who continued to use the technique in their daily lives.</p>

<p>“The follow up was my favourite time of my entire PhD, because every day was just joyful,” she said. “I didn’t have a single participant who told me ‘Oh, I feel bad’ or ‘This was useless’. I didn't prompt them or ask ‘Did you find this helpful?’ They were just automatically telling me how helpful they found it.”</p>

<p>One participant was so impressed by the technique that she taught her daughter and her own mother how to do it. Another reported how she had moved home just prior to COVID-19 and so felt very isolated during the pandemic.</p>

<p>“She said this study had come exactly at the time she needed it because she was having all these negative thoughts, all these worries and anxiety about the future, and this really, really helped her,” said Dr Mamat. “My heart literally just melted, I could feel goosebumps all over me. I said to her ‘If everyone else hated this experiment, I would not care because of how much this benefited you!’.”</p>

<p>The research was funded by the Medical Research Council and the Mind Science Foundation.</p>

<p><em><strong>Reference</strong><br>
Mamat, Z, and Anderson, MC. <a href="https://doi.org/10.1126/sciadv.adh5292">Improving Mental Health by Training the Suppression of Unwanted Thoughts.</a> Sci Adv; 20 Sept 2023; DOI: 10.1126/sciadv.adh5292</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Luiz André Barroso has died (116 pts)]]></title>
            <link>https://www.wired.com/story/google-mourns-luiz-andre-barroso-veteran-engineer-invented-the-modern-data-center/</link>
            <guid>37606775</guid>
            <pubDate>Fri, 22 Sep 2023 01:29:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/google-mourns-luiz-andre-barroso-veteran-engineer-invented-the-modern-data-center/">https://www.wired.com/story/google-mourns-luiz-andre-barroso-veteran-engineer-invented-the-modern-data-center/</a>, See on <a href="https://news.ycombinator.com/item?id=37606775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>Luiz André Barroso</span> had never designed a data center before Google asked him to do it in the early 2000s. By the time he finished his first, he had overturned many conventions of the computing industry, laying the foundations for Silicon Valley’s development of cloud computing.</p><p>Barroso, a 22-year veteran of Google who unexpectedly died on September 16 at age 59, <a href="https://www.wired.com/2012/10/ff-inside-google-data-center/">built his data centers</a> with low-cost components instead of expensive specialized hardware. He reimagined how they worked together to develop the concept of “the data center as a computer,” which now underpins the web, mobile apps, and other internet services.</p><p>Jen Fitzpatrick, senior vice president of Google’s infrastructure organization, says Barroso left an indelible imprint at the company whose contributions to the industry are countless. “We lost a beloved friend, colleague and respected leader,” she writes in a statement on behalf of the company.&nbsp;</p><p>Barroso continued to lead major projects at Google, including development of <a href="https://www.wired.com/story/covid-exposure-apps-are-headed-for-a-mass-extinction-event/">its Covid exposure notifications app</a>, for which he served as a mediator across teams within the company and with outside partners. In an email Fitzpatrick sent to Google staff seen by WIRED, she wrote that it's understood Barroso died from natural causes.</p><div data-testid="GenericCallout"><figure><p><span>Luiz André Barroso</span><span>Photograph: Sebastian Kennerknecht</span></p></figure></div><p>Fitzpatrick says Barroso’s family, which includes his wife Catherine Warner, a singer for whom he sometimes <a data-offer-url="https://www.beforebossa.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.beforebossa.com/&quot;}" href="https://www.beforebossa.com/" rel="nofollow noopener" target="_blank">played guitar</a>, is seeking privacy. &nbsp;The cause of death could take weeks to determine, according to the medical examiner’s office of Santa Clara County, in Silicon Valley.</p><p>Barroso had wanted to be an electrical engineer since his childhood days in Brazil, where he got into amateur radio with his grandfather and earned bachelor’s and master’s degrees in electrical engineering from the Pontifical Catholic University of Rio de Janeiro. He came to the US for a doctorate in computer architecture from the University of Southern California and worked on chips at Compaq and Digital Equipment Corporation. But he came to Google in 2001 wanting to focus on software engineering.</p><p>Barroso wasn’t a coder for long—the then small startup’s few employees had to pitch in wherever help was needed. Three years after joining Google, <a href="https://www.wired.com/2017/06/google-copes-even-cant-afford-enough-gear/">Urs Hölzle</a>, the company’s first vice president of engineering, tasked Barroso with rebuilding the company's infrastructure. “I was the closest thing we had to a hardware person,” Barroso <a href="https://www.wired.com/2012/01/google-man/">recalled to WIRED in 2012</a>.</p><div><p>When he took on the infrastructure gig, internet businesses such as Google typically hosted their websites on servers in data centers maintained by another company. But these vendors couldn’t handle the surging search startup’s growing needs.</p><p>Barroso’s inexperience in data center design helped lead him to reinventing it, he wrote in <a data-offer-url="https://barroso.org/publications/IEEEMicro2021.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://barroso.org/publications/IEEEMicro2021.pdf&quot;}" href="https://barroso.org/publications/IEEEMicro2021.pdf" rel="nofollow noopener" target="_blank">an essay</a> and <a data-offer-url="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf&quot;}" href="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" rel="nofollow noopener" target="_blank">recalled during a podcast</a> in 2021. He found himself asking “Wait, wait, wait, but why are we doing it this way?” Barroso said on the podcast. “And it just turns out that the people who had been living in that area hadn't really thought about questioning that. And sometimes it's something that was based on a good reason three years ago, and that reason had a sell-by date, and it's time to do something else.”</p></div></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Google’s first data center consisted of 40-foot, server-filled shipping containers, which enabled advanced cooling and fewer construction headaches. It opened its own data center campus in Oregon in 2006, resembling the conventional bland, boxy, and massive buildings that now dot the world. But Barroso’s ideas made the insides exceptional.</p><p>He and his Google colleagues <a data-offer-url="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/908d5966b1fa946034e382e608999d51e70d5b22.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/908d5966b1fa946034e382e608999d51e70d5b22.pdf&quot;}" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/908d5966b1fa946034e382e608999d51e70d5b22.pdf" rel="nofollow noopener" target="_blank">turned away from the then standard approach</a> of centralizing key software in a data center on a few expensive and powerful machines. Instead they began distributing Google’s programs across thousands of cheaper, mid-grade servers. That saved money spent on pricey hardware while also saving energy and allowing software to run more nimbly.</p><p>Barroso laid out his new philosophy in <a data-offer-url="https://research.google/pubs/pub41606/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://research.google/pubs/pub41606/&quot;}" href="https://research.google/pubs/pub41606/" rel="nofollow noopener" target="_blank"><em>The Datacenter as a Computer</em></a>, a book he coauthored with Hölzle that became a seminal text on modern computing infrastructure. “We must treat the data center itself as one massive warehouse-scale computer,” the book says.</p><p>The efforts of Barroso’s “speed-up” team, as he liked to call it, paid off for Google and helped establish its reputation as not just a neat search engine but also a place that broke new ground in computing. By <a data-offer-url="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf&quot;}" href="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" rel="nofollow noopener" target="_blank">customizing nearly every inch of Google’s data centers and the hardware within them</a>, including power supplies and cooling kits, the search giant could deliver results, emails, and other services faster—even as the “slow-down” teams integrated more algorithms and features.</p><p>“It’s easy to forget just how crazy the amount of computational data is required to be able to give you a new result every 20 milliseconds or something,” he told WIRED’s Steven Levy in 2012. “We’re essentially searching our web corpus, our images corpus, you name it, every time you do a keystroke.”</p><p>Barroso’s ideas spread quickly across Silicon Valley. Meta and other internet giants adopted an approach similar to Google's for their data centers. The architecture Barroso devised became the basis for Google’s cloud computing unit, which now accounts for about 10 percent of the company’s overall revenue.</p><p>Over the past decade, Barroso helped start the team that designed <a href="https://www.wired.com/2017/04/building-ai-chip-saved-google-building-dozen-new-data-centers/">Google’s AI chips known as TPUs</a>; led engineering for Google’s “geo” services, including the infusion of augmented reality and machine learning into Maps; and founded Google’s core unit, which manages software and other tools used across the company. He held the title of Google fellow, the company’s highest rank for technical staff. In 2020, he received the Eckert Mauchly award from the Association for Computing Machinery and the Institute of Electrical and Electronics Engineers for his contributions to computer architecture.</p><p>Barroso recently joined the board of Stone, an ecommerce company in Brazil, where the engineer was born and where he successfully pushed Google to hire more. Stone wrote in a disclosure to investors this week that Barroso “made significant contributions to our technology team and overall strategy” and that “our hearts and thoughts are with [Barroso’s] family, friends, and colleagues.” A spokesperson for the company declined further comment.</p><p>Barroso was also active in environmental projects. He served on the board of Rainforest Trust, a nonprofit for whom he organized and led a weeklong trip to Brazil's Pantanal wetlands last month. He also <a data-offer-url="https://www.allaboutcircuits.com/podcast/ep-27-two-google-senior-vps-of-engineering-from-shipping-containers-to-todays-data-centers/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.allaboutcircuits.com/podcast/ep-27-two-google-senior-vps-of-engineering-from-shipping-containers-to-todays-data-centers/&quot;}" href="https://www.allaboutcircuits.com/podcast/ep-27-two-google-senior-vps-of-engineering-from-shipping-containers-to-todays-data-centers/" rel="nofollow noopener" target="_blank">expressed concern</a> about <a href="https://www.wired.com/story/bitcoin-mining-guzzles-energyand-its-carbon-footprint-just-keeps-growing/">the cryptocurrency industry’s thirst for electricity</a>. Barroso had been executive sponsor for Google’s Hispanic and Latinx employee group and a program awarding fellowships to doctoral students in Latin America.</p><p>Despite all his technical achievements, Barroso told WIRED in 2012 that mentoring interns was “probably the thing I’m best at.” Google chief scientist Jeff Dean, who brought Barroso to Google in 2001 with interviews over crème brûlée, tweeted on Monday without naming his onetime research partner, “Sometimes close friends and colleagues leave us altogether too soon.”</p><p><em>Additional reporting by Steven Levy.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nintendo 3DS Architecture (206 pts)]]></title>
            <link>https://www.copetti.org/writings/consoles/nintendo-3ds/</link>
            <guid>37606380</guid>
            <pubDate>Fri, 22 Sep 2023 00:31:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.copetti.org/writings/consoles/nintendo-3ds/">https://www.copetti.org/writings/consoles/nintendo-3ds/</a>, See on <a href="https://news.ycombinator.com/item?id=37606380">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><h2 id="imagery">Supporting imagery</h2><section><ul><li><a href="#cover-model">Model</a></li><li><a href="#cover-motherboard">Motherboard</a></li><li><a href="#cover-diagram">Diagram</a></li></ul></section><hr><h2 id="a-quick-introduction">A quick introduction</h2><p>As smartphones surge in adoption, the videogame market is experiencing an unusual growth led by discount App Stores and affordable development licenses. With this, one can only wonder when kids will prefer an iPhone 4 over a Nintendo DSi.</p><p>In the midst of finding out the answer, Nintendo conceives a thrilling successor to its triumphant portable system. In it, users will find old, present and unfamiliar technology - many of which can’t be replicated by smartphones.</p><p>And so, this new production of the Architecture of Consoles series will give you a profound description of how this new console works, both internally and externally.</p><h3 id="recommended-reading">Recommended reading</h3><p>If you are new to this <a href="https://www.copetti.org/writings/consoles/">article series</a>, I strongly suggest reading the <a href="https://www.copetti.org/writings/consoles/gamecube/">GameCube</a>, <a href="https://www.copetti.org/writings/consoles/game-boy-advance/">Game Boy Advance</a> and <a href="https://www.copetti.org/writings/consoles/nintendo-ds/">Nintendo DS</a> articles beforehand, as they will explain various terms and concepts referenced in this one.</p><hr><h2 id="models-and-variants">Models and variants</h2><p>Throughout the lifecycle (and struggles) of this console, Nintendo released numerous revisions in an attempt to correct its target audience and recover loyal customers.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/side_n3ds.305bcbf7b0cd255a64549d2027f9f6478a7b9777009c85e229f6f93b4c025cc5.webp"><picture><img alt="Image" width="1111" height="506" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_huc15dcfe92f3446e20d07697c5f45f979_25336_2089e178d0cad5b37d8d0bc91ff41544.png" loading="lazy"></picture></a><figcaption>An original Nintendo 3DS (the first generation, from 2011) next to a New Nintendo 3DS XL (the last generation, from 2015).</figcaption></figure><p>From the architectural point of view, there were a total of six different models:</p><ul><li><strong>Nintendo 3DS</strong> (2011) and <strong>Nintendo 3DS XL</strong> (2012): The debuting series featuring the original architecture. The only relevant difference between the XL and non-XL models is the screen size.</li><li><strong>Nintendo 2DS</strong> (2013): A cheaper alternative to the original Nintendo 3DS by removing the stereoscopic screen and featuring a <a href="https://www.copetti.org/writings/consoles/game-boy/">Game Boy</a>-inspired shape.</li><li><strong>New Nintendo 3DS</strong> (2014) and <strong>New Nintendo 3DS XL</strong> (2015): A re-engineering of the standard 3DS models. The ‘New’ variants exhibit an incremental hardware upgrade, an NFC reader, a larger button set and an improved stereoscopic system.</li><li><strong>New Nintendo 2DS XL</strong> (2017): The ‘New’ affordable alternative to the New Nintendo 3DS XL by omitting stereoscopic functionality.</li></ul><p>Now, for this article, the focus will be on the original Nintendo 3DS (after all, it’s the lowest common denominator for games). However, since the architectural differences of the ‘New’ series are worth studying, these will receive a dedicated section.</p><hr><h2 id="displays">Displays</h2><p>There’s only one company that keeps altering the standard structure of all my analyses, and that’s Nintendo. This time, I’ll start with the <strong>stereoscopic screens</strong> (a.k.a. ‘3D without glasses’).</p><p>First things first, the Nintendo 3DS, as a successor of the Nintendo DS, includes two LCD screens. The upper screen has a resolution of <strong>800 x 240 pixels</strong> and somehow can display images with a sense of depth. When I first read this, only questions popped into my head:</p><ul><li>What optics principles are they applying?</li><li>How is the screen designed?</li><li>How do games comply with this system?</li></ul><p>Well, here are the answers!</p><h3 id="principles">Principles</h3><p>Liked or not, the fundamentals are not so different from the <a href="https://www.copetti.org/writings/consoles/virtual-boy/">Virtual Boy</a>, which I’ve happened to analyse two years before. To recall, the Virtual Boy displays two images, one to each eye, and shows objects individually shifted from the centre. By looking at the two pictures at the same time, they are perceived as some objects are behind others (sense of depth). This is the basis of <strong>Stereoscopic Parallax</strong>.</p><figure><ul><li id="tab-1-1-left-link"><a href="#tab-1-1-left">Left</a></li><li id="tab-1-2-right-link"><a href="#tab-1-2-right">Right</a></li></ul><figure id="tab-1-1-left"><a href="https://www.copetti.org/images/consoles/virtualboy/tennis/left.5650091c8dae2d51fd18a73a5bc37e22b6d001e54d8a18a1ade37e73b8b4d14e.png"><picture><img alt="Image" width="384" height="224" src="https://www.copetti.org/images/consoles/virtualboy/tennis/left.5650091c8dae2d51fd18a73a5bc37e22b6d001e54d8a18a1ade37e73b8b4d14e.png" loading="lazy"></picture></a><figcaption>Left display.</figcaption></figure><figure id="tab-1-2-right"><a href="https://www.copetti.org/images/consoles/virtualboy/tennis/right.3df8c5ee7cf0e2b8b6acab7349b10cd3b4b33183427fd6a58e813228962928b6.png"><picture><img alt="Image" width="384" height="224" src="https://www.copetti.org/images/consoles/virtualboy/tennis/right.3df8c5ee7cf0e2b8b6acab7349b10cd3b4b33183427fd6a58e813228962928b6.png" loading="lazy"></picture></a><figcaption>Right display.</figcaption></figure><figcaption>Demonstration of how the Virtual Boy displayed stereoscopic imagery.<br>Mario’s Tennis (1995).</figcaption></figure><p>Now, the way the Virtual Boy executed this was a bit cumbersome: it required users to place their heads close to the eyepiece and then adjust the focal length and inter-pupil distance. 15 years later, Nintendo rightly said ‘No’ to all of that nuisance, and designed a new system where users could enjoy 3D-looking scenery without <em>considerable</em> intervention.</p><figure><ul><li id="tab-2-1-left-link"><a href="#tab-2-1-left">Left</a></li><li id="tab-2-2-right-link"><a href="#tab-2-2-right">Right</a></li></ul><figure id="tab-2-1-left"><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_left.296143904d4490fa7e10203667d603c4588adf55f7b2dc0b15d6426bbbb31676.png"><picture><img alt="Image" width="400" height="240" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_left.296143904d4490fa7e10203667d603c4588adf55f7b2dc0b15d6426bbbb31676.png" loading="lazy"></picture></a><figcaption>Top screen, left eye.</figcaption></figure><figure id="tab-2-2-right"><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_right.7d50fe7647f1d01ef0f864286c9e8909fee1821a91fd840fd8eda93ca934b026.png"><picture><img alt="Image" width="400" height="240" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_right.7d50fe7647f1d01ef0f864286c9e8909fee1821a91fd840fd8eda93ca934b026.png" loading="lazy"></picture></a><figcaption>Top screen, right eye.</figcaption></figure><figcaption>An example of two frames the Nintendo 3DS shows on its top screen at the same time. Looks like the fish is going to hit you. The same principle applies 15 years later.<br>Luigi’s Mansion (2018).</figcaption></figure><p>This brings us to our next question.</p><h3 id="the-special-screen">The special screen</h3><p>Take a look again at the resolution of the upper LCD screen. On paper, it says it’s <strong>800 x 240 pixels</strong> wide, which results in a ludicrous aspect ratio.</p><div><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/international.283e187450b432e5ca6f46f4bb6a495bdfc36f08cb2e1da082b020d6022d1f8e.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/_hud950cf4543f954b474f553674fc302a5_240147_da7038c582d606c66574ccd907cc6aac.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud950cf4543f954b474f553674fc302a5_240147_a8b4b2a43322fb984cdf1b2394d5a99e.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud950cf4543f954b474f553674fc302a5_240147_bc329dbcc83d74e2fc1cf5fae9dfa34e.webp 1000w"><img alt="Image" width="1000" height="907" src="https://www.copetti.org/images/consoles/nintendo3ds/international.283e187450b432e5ca6f46f4bb6a495bdfc36f08cb2e1da082b020d6022d1f8e.png" loading="lazy"></picture></a><figcaption>The Nintendo 3DS again <sup id="bibref:1"><a href="#bib:photography-amos" role="doc-biblioref">[1]</a></sup>, take a closer look at its screens.</figcaption></figure><p>In reality, the physical screen is made of <strong>half-width pixels</strong> and operates in two modes:</p><ul><li><strong>Traditional/2D mode</strong>: When the stereoscopic function is disabled, groups of two horizontal pixel pairs are treated as a single one.<ul><li>Truth be told, the screen can still display a frame of 800 x 240 px, although no commercial game ever used this.</li></ul></li><li><strong>Stereoscopic/3D mode</strong>: All pixels are treated individually, and with it, the screen displays <strong>two frames</strong> of <strong>400 x 240 pixels</strong> at the same time.</li></ul><p>Moreover, to perform stereoscopic parallax, this particular LCD houses an extra layer called <strong>Parallax Barrier</strong> <sup id="bibref:2"><a href="#bib:graphics-display_teardown" role="doc-biblioref">[2]</a></sup>. These opaque shutters deviate the backlight beamed behind the pixels of the LCD, so each eye will receive the light of a different subset of pixels <sup id="bibref:3"><a href="#bib:graphics-display_howworks" role="doc-biblioref">[3]</a></sup>. The half-width pixels will also appear to be wider, thereby giving the feeling they have the traditional aspect ratio.</p><p>All in all, this recreates the original effect of the Virtual Boy without requiring controls for adjustment.</p></div><p>The technology is not perfect, however, as there are a few caveats:</p><ul><li>The parallax barrier requires extra brightness, thereby impacting the battery life.</li><li>The user must not hold the screen in a tilted position (compared to the user’s eyes). Otherwise, the user will end up seeing a confusing mix of the two parallax frames, which can be a disorienting experience. Not to mention the eyes won’t enjoy the extra fatigue.</li><li>Combining the fact the user must maintain a fixed posture while playing, and that stereoscopic parallax can tire the eyes quicker. The 3D feature, as a whole, can be an unnecessary hassle for most.</li></ul><div><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/tilt.a90f13873f8d4fb7e34e72734a026aa48c63063ed1cfdcb7be0259533b637534.webp"><picture><img alt="Image" width="1000" height="750" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/_hu68de68eb39eeb5477ffb3e92dc8b5860_29218_11a22cc1e2532e946c972f3f59bcc0ba.png" loading="lazy"></picture></a><figcaption>My attempt to capture the tilt effect of the original 3DS. The 3D depth slider (at the right side of the screen) is all the way up, and by looking at the screen from one side, a ghosting effect appears on the top screen. This is quite eye-straining to look at in reality!</figcaption></figure><p>To remediate things, Nintendo added a slider control (called <strong>3D depth slider</strong>) to adjust the level of depth between objects. In doing so, it either increases or decreases the difference between the two frames. This was done to reduce the depth effect for people who didn’t find it enjoyable or too fatiguing.</p><p>Setting the 3D slider to the max can be disorienting at first. In my experience, my eyes eventually got focused, at which point I perceived the top LCD screen as if I were looking through a window. The main problem is that users will need to continuously shift their eyes to see the bottom screen, and the repeated action can be very straining.</p></div><p>As a side note, one can’t help but find it amusing how the graphics pipeline has gone full circle when rendering stereoscopic frames. During rendering, 3D data is projected into a 2D space, and now with the stereoscopic screen, that 2D space is displayed again as 3D. At this point, let’s just use holograms and skip the 3D projection stage altogether.</p><h4 id="a-small-update">A small update</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/new3ds.1ab0c98b61b0946592b63bc6732fcf95f0b6c312e1ce7814df84ec2cb1a8470f.webp"><picture><img alt="Image" width="900" height="158" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/_hu025c37239d25a6aa4098cc524d1fdd78_10338_753f29eb89554cf724e8ee8dd01456e8.png" loading="lazy"></picture></a><figcaption>Top part of the New 3DS XL. At its centre, there’s a front camera and an infrared LED, both used for head tracking.</figcaption></figure><p>With the advent of the ‘New 3DS’ model, Nintendo revisioned their stereoscopic screen in an effort to reach enjoyability levels. In the new model, the console incorporates a face-tracking mechanism to tackle the tilting effect, so issues don’t need to worry about keeping a good head-console posture anymore.</p><h3 id="the-special-games">The special games</h3><p>Now for this system to work, games must play along (pun intended). Just like they traditionally interact with the GPU to draw frames on the display, they must now broadcast two frames of the scenery but with objects slightly shifted.</p><p>To make life easier for developers, there are official APIs that assist in this, especially for those games with 3D sceneries. These APIs help by providing routines that construct two projection matrices, the graphics pipeline then uses them to render the two slightly-shifted frames.</p><hr><h2 id="cpu">CPU</h2><p>Now that we know how the display works, let’s look at the internals of this console. If you get a hold of the motherboard, you’ll see three big chips, one being the <strong>CPU CTR</strong>. That’s the big System-On-Chip (SoC) that houses the entire system (aside from storage and RAM).</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/cpu_photo.f1977dfae04cf532272eac2f256cafc4571f4bd76d90ba92e509f80065d69b41.webp"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_7aff8078eb7c7ec1542a270f9f966adc.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_c5342c8a4cc85707ed9853e78e1d98bf.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_7bba4f72e3e2d7084e976dc626153693.webp 1000w"><img alt="Image" width="1000" height="534" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_eac66c08da1275ec4651225b62bc7d48.png" loading="lazy"></picture></a><figcaption>CPU CTR next to some FCRAM</figcaption></figure><p>CPU CTR follows the design methods of previous portable consoles from Nintendo. That is, squash all your engineering into a single block. In doing so, it will reduce the production of counterfeits, protect sensible components and improve heat dissipation.</p><p>In terms of the actual CPU, Nintendo partnered again with their old friend, <strong>ARM</strong>, to produce their next-generation core. ARM’s licensing model happens to be favourable to Nintendo as they have always offered synthesisable designs, which allows Nintendo to mould to their needs (including, fitting them into a big SoC). In the end, ARM gave them a relatively antiquated with substantial upgrades. Their choice was the <strong>ARM11</strong> core, a successor of the ARM9 (featured with the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#cpu">Nintendo DS</a>). More specifically, the <strong>MPCore</strong> variant, ARM’s first <strong>homogenous multi-core</strong> solution.</p><p>Using ARM’s designs, Nintendo crafted an ARM11 MPCore cluster housing <strong>two</strong> ARM11 cores <sup id="bibref:4"><a href="#bib:cpu-lioncash" role="doc-biblioref">[4]</a></sup>. Three years later, with the arrival of the ‘New’ 3DS, the SoC was expanded to contain <strong>four</strong> ARM11 cores. The effects of this will be explained in due time so, before anything else, let’s analyse what the new CPU cores offered to this console.</p><h3 id="an-iconic-industry">An iconic industry</h3><p>The ARM11 series originates from 2002, as a successor of the popular ARM9 and the short-lived ARM10.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/devices.b32e1b8964b560bbeed11ec8ae0e743efb6b9cf4163964bc086ba8aa1b804a5c.webp"><picture><img alt="Image" width="1000" height="384" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hucd71f9346efedab899883028b66e7ae5_21546_4af778c28e8044da94eb7237b76f5f62.png" loading="lazy"></picture></a><figcaption>A Nokia 5230 (2009), a red 3DS (2011) and a Raspberry Pi Model B (2012), all carrying an ARM11.</figcaption></figure><p>In case you haven’t heard about them before, ARM11s are best known for powering the 2006-2008 generation of smartphones (back when many of them featured a keypad or a clamshell design). If you owned a Nokia N95, 5230 or the first iPhone, you’ve used an ARM11. This also applied to many high-end cameras, GPS or similar peripherals. If you wonder, other manufacturers like RIM and Samsung held onto Intel XScale (the continuation of <a href="https://www.copetti.org/writings/consoles/nintendo-ds/##tab-2-2-a-question-about-the-hardware-choice">StrongARM</a>, implementing the ARMv5TE instruction set) until 2009, when they made the switch to ARM11 (this is a bit ironic, considering the iPhone’s CPU was supplied by Samsung!). Last but not least, the ARM11 was the choice of the CPU for the first model of the Raspberry Pi.</p><p>Now, by the time Nintendo adopted the ARM11, its creator had already succeeded it with the Cortex-A series. This is nothing but expected, as Nintendo’s model favours cost-effectiveness over avant-garde CPUs. Look at it from another way, saving in CPU costs allows them to focus their budget on other aspects of the console, you’ll soon see.</p><h4 id="new-dialects">New dialects…</h4><p>Along with the new shiny CPUs, a new instruction set arrived, the <strong>ARMv6</strong>.</p><p>From a programmer’s perspective, the ARMv6 ISA innovates with a new set of vector instructions and multi-core support <sup id="bibref:5"><a href="#bib:cpu-thomas" role="doc-biblioref">[5]</a></sup>. The new vector set provides SIMD instructions that operate groups of <strong>four 8-bit values</strong> or <strong>two 16-bit values</strong> at the same time (using the existing 32-bit registers) <sup id="bibref:6"><a href="#bib:cpu-armcc" role="doc-biblioref">[6]</a></sup>. The new multi-core instructions consist of <code>Store</code> and <code>Load</code> opcodes with special care for synchronisation (crucial for an environment of multiple CPUs using the same memory locations) <sup id="bibref:7"><a href="#bib:cpu-sync" role="doc-biblioref">[7]</a></sup>.</p><p>All in all, this may not seem that thriving for a new chip series, but remember that ARM’s CPUs speak many ‘languages’. In the case of an ARM11-based core, you are provided with:</p><ul><li>The main 32-bit ISA, called <strong>ARMv6</strong>.</li><li>A compressed alternative called <strong>Thumb</strong>. Its instructions fit in 16-bit words instead. If you’d like to know more, I go over it in the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#whats-new">Game Boy Advance article</a>, as it weighs significant importance in that console.</li><li><strong>Jazelle</strong>, a Java bytecode interpreter, mostly forgotten and left unused. I’ve mentioned a bit of it in the <a href="https://www.copetti.org/writings/consoles/wii/#the-hidden-co-processor">Wii article</a>.</li><li>Any extension bundled into the core. For instance, the MPCore includes a <strong>Vector Floating-point Coprocessor</strong> with additional instructions to control said coprocessor <sup id="bibref:8"><a href="#bib:cpu-vfp" role="doc-biblioref">[8]</a></sup>.</li></ul><p>To make matters less confusing, ARM tends to package all of these with a single nomenclature. For instance, in the case of the ARM11 MPCore opcodes, ARM refers to them as the <strong>ARMv6k</strong> instruction set.</p><h4 id="and-a-fragmented-distribution">… and a fragmented distribution</h4><p>The adoption of extensions and alternative instruction sets eventually made things very convoluted for developers targeting generic ARM hardware, you only have to look at the uncountable ARM ports devised for Linux distributions.</p><p>Debian, one of the most popular distributions, tried to tackle the disparities by developing two ports in parallel:</p><ul><li><code>armel</code>: unoptimized, compatible with ARMv4T onwards.</li><li><code>armhf</code>: accelerated with VFP, but only compatible with ARMv7 onwards.</li></ul><p>Yet, with the arrival of the Raspberry Pi (powered by ARMv6 and accelerated with VFP), neither of them was deemed acceptable. Thus, an unofficial port called ‘Raspbian’ was developed to provide a VFP-accelerated version for ARMv6 CPUs <sup id="bibref:9"><a href="#bib:cpu-armhf" role="doc-biblioref">[9]</a></sup>. Even so, the trend continued: years later, with the arrival of ARMv8 and AArch64, Debian spawned yet-another port, <code>arm64</code>, optimised for the new 64-bits ISA.</p><p>I don’t remember seeing this labyrinth with x86, but at least things are now getting more orderly. AArch64 has unified many extensions and dropped alternative modes (<em>farewell, Thumb and Jazelle</em>).</p><h3 id="core-functionality">Core functionality</h3><p>That was a big deviation. Let’s go back to the 3DS CPU, the ARM11, and check what’s inside.</p><p>For this study, we can divide the ARM11 MPCore into two areas:</p><ul><li>The <strong>MP11 cores</strong> that make up the cluster.</li><li>The <strong>Advanced eXtensible Interface (AXI)</strong> bus, a new invention that interconnects the cores and interfaces with the outside world.</li></ul><p>Let’s start with the cores now and then we’ll check the AXI bus.</p><div><ul><li id="tab-3-1-the-original-mpcore-link"><a href="#tab-3-1-the-original-mpcore">The original MPCore</a></li><li id="tab-3-2-the-new-mpcore-link"><a href="#tab-3-2-the-new-mpcore">The ‘New’ MPCore</a></li><li id="tab-3-3-the-axi-bus-link"><a href="#tab-3-3-the-axi-bus">The AXI bus</a></li></ul><div><div id="tab-3-1-the-original-mpcore"><h4 id="tab-3-1-the-original-mpcore">The original MPCore</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_overview.593ad79da3baa7e6b91e954dab46aff1bfe8dd75567506ad3626f9412aa91533.png"><picture><img alt="Image" width="513" height="542" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_overview.593ad79da3baa7e6b91e954dab46aff1bfe8dd75567506ad3626f9412aa91533.png" loading="lazy"></picture></a><figcaption>Overview of the ARM11 MPCore CPU cluster</figcaption></figure><p>The first ARM11 MPCore variant, which debuted with the original 3DS, includes two cores. Each is called <strong>MP11</strong> and runs at <strong>268 MHz</strong> <sup id="bibref:10"><a href="#bib:cpu-lioncash" role="doc-biblioref">[10]</a></sup>.</p><p>Apart from implementing the ARMv6k instruction set, the CPU features an <strong>8-stage pipeline</strong> <sup id="bibref:11"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[11]</a></sup>. Furthermore, the core provides <strong>two levels of branch prediction</strong>, ‘dynamic’ (based on previous executions) and ‘static’ (based on the current instruction alone). Overall, both enhancements will be quickly noticed, considering the 5-stage ARM9 couldn’t predict a thing!</p><p>Additionally, since the ARM946E-S CPU, ARM has been fitting a <strong>System Control Coprocessor</strong> called <strong>CP15</strong>. This time, it provides <strong>Memory-Management</strong> (MMU functions) and registers that output information about the MPCore cluster.</p><p>Now, there’s no more <strong>Tightly-Coupled Memory</strong> (TCM). There are however <strong>16 KB of instruction cache</strong> and <strong>16 KB of data cache</strong>, this change of model resembles other systems of the same generation. If you are curious, this L1 cache is 4-way set associative.</p><p>Finally, each core houses a co-processor called <strong>Vector Floating-point Coprocessor</strong> (also known as ‘VFP11’). This accelerates arithmetic operations with floating-point numbers, both 32-bit single-precision (a.k.a. <code>float</code>) and 64-bit double-precision (a.k.a. <code>double</code>) ones <sup id="bibref:12"><a href="#bib:cpu-vfp" role="doc-biblioref">[12]</a></sup>. It’s not a big coprocessor though, as its register file is composed of 32 32-bit registers, so doubles will consume two registers. In any case, this processor implements the <strong>VFPv2 instruction set</strong> and follows the <strong>IEEE 754</strong> standard. The latter is a welcomed decision, considering the architecture of <a href="https://www.copetti.org/writings/consoles/playstation-2/#the-leader">previous generations</a>.</p></div><div id="tab-3-2-the-new-mpcore"><h4 id="tab-3-2-the-new-mpcore">The ‘New’ MPCore</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_new_overview.73042809fa9f8f34cbea6e462ef0d89128a814b522968e63bb28ae4e3a00b9e1.png"><picture><img alt="Image" width="960" height="624" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_new_overview.73042809fa9f8f34cbea6e462ef0d89128a814b522968e63bb28ae4e3a00b9e1.png" loading="lazy"></picture></a><figcaption>Overview of the ‘New’ CPU cluster</figcaption></figure><p>With the arrival of the New 3DS in 2014, a new SoC was included (<strong>CPU LGR</strong>) and with it, a luxurious CPU upgrade.</p><p>The most apparent change is that we have now <strong>four MP11 cores</strong> instead of two. The consequences of this, however, are not simple to disseminate, but we’ll see them in due time.</p><p>The second change is that the CPU incorporates <strong>2 MB of L2 cache</strong> shared between the four cores. This type of cache is 16-way associative, which anticipates four cores accessing it at the same time. If you’d like to know more, I went over associative caches with the <a href="https://www.copetti.org/writings/consoles/xbox-360/#shared-cache">Xbox 360 article</a>.</p><p>Moving on, all cores now run at <strong>804 MHz</strong> (three times the original speed, which will certainly raise a few eyebrows).</p></div><div id="tab-3-3-the-axi-bus"><h4 id="tab-3-3-the-axi-bus">The AXI bus</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/axi.7e49c2473554d83f6944120a017622217417d3b8704a631434e0b4e609048377.png"><picture><img alt="Image" width="510" height="600" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/axi.7e49c2473554d83f6944120a017622217417d3b8704a631434e0b4e609048377.png" loading="lazy"></picture></a><figcaption>Example of how the AXI protocol interconnects different types of components</figcaption></figure><p>Whether there are two or four cores, all of these are connected using a specialised bus, proudly authored by ARM, called the <strong>Advanced eXtensible Interface</strong> (AXI). This protocol is part of the AMBA3 model, a successor of the original AMBA revision that we’ve seen in the <a href="https://www.copetti.org/writings/consoles/wii/#the-hidden-co-processor">Wii</a> and <a href="https://www.copetti.org/writings/consoles/wiiu/#internal-interfaces">Wii U</a> (both housing an ARM9 CPU).</p><p>Generally speaking, the AMBA model provides a set of protocols for connecting components with distinct bandwidth requirements using a <strong>bus topology</strong>. Compare this to the token-ring model of the <a href="https://www.copetti.org/writings/consoles/playstation-3/#inside-cell-the-heart">PlayStation 3</a> or the mesh solution made for the <a href="https://www.copetti.org/writings/consoles/xbox-360/#inside-xenon-the-messenger">Xbox 360</a>. All of these consoles shared the same problem, but each came up with different solutions, neither better nor worse, just different.</p><p>Following AMBA’s methodologies for interconnecting components, there will be a master-slave hierarchy imposed to maintain order. The master components (typically, the CPU cores) will be the ones sending commands to the slaves (i.e.&nbsp;memory and I/O blocks).</p><p>Now, as part of the AMBA3 specification, ARM offered the AXI model as a critical ingredient for building <strong>System On Chips</strong> (SoC). Instead of using a single bus, AXI uses a dedicated block (called <strong>AXI interconnect</strong>) acting as a <strong>bus matrix</strong> <sup id="bibref:13"><a href="#bib:cpu-axi" role="doc-biblioref">[13]</a></sup>, this is connected to every single component using <strong>64-bit dedicated buses</strong> <sup id="bibref:14"><a href="#bib:cpu-arm11_overview" role="doc-biblioref">[14]</a></sup>. In doing so, AXI overcomes the limitations of high-bandwidth components sharing the same bus (as it happened with the <a href="https://www.copetti.org/writings/consoles/playstation-2/#cpu">PlayStation 2</a>). Moreover, multiple master devices can communicate with slave nodes using separate channels to avoid waiting for other masters to finish. Finally, traditional enhancements like <a href="https://www.copetti.org/writings/consoles/gamecube/#ibms-enhancements">burst transactions</a> are implemented, from which the MP11 cores take advantage.</p><p>In the case of the 3DS, the AXI interconnect is housed in a bigger block called <strong>Snoop Control Unit</strong> (SCU) that also takes care of automatically maintaining L1 cache coherency between the MP11 cores.</p></div></div></div><h3 id="any-other-cpus">Any other CPUs?</h3><p>Up to this moment, I’ve been talking about the MPCore as if it were the only CPU in this system, the reason being mixing up distinct CPUs for this analysis can turn it into an incomprehensible essay. That is, until now.</p><p>The truth is, Nintendo had extra requirements for this console. They wanted a proper security system, but also the possibility to turn the console into a <strong>Nintendo DSi or a GBA</strong> on-the-fly. So, for all of that, they ended up bundling <strong>three distinct CPU packages</strong> - one being the mentioned ARM11. The other two are well hidden, in the sense that games are completely unaware of them. In fact, 3DS emulators like Citra don’t care about them either <sup id="bibref:15"><a href="#bib:cpu-citra_cpu" role="doc-biblioref">[15]</a></sup>.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/side_ds.3eba2a2f3a4631917db480bd5069986d1298ec9182c0806ba209e2b55dd0eb09.webp"><picture><img alt="Image" width="1111" height="530" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hufe4dbc5506e4ce358fb18d5a09cafc51_22792_04c9dc378631b6c66caa6384f2d190a3.png" loading="lazy"></picture></a><figcaption>The Nintendo 3DS next to a predecessor (a Nintendo DS Lite), the latter has become a common denominator.</figcaption></figure><p>But we do! Here’s the complete list of CPUs this system houses:</p><ul><li>The <strong>ARM11 MPCore</strong> we’ve just seen.</li><li>An <strong>ARM946E-S</strong> from the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#cpu">Nintendo DS days</a>. It’s treated as a secret co-processor and it’s only managed by the operating system. Alternatively, it becomes the main processor whenever a DS or DSi game is executed.<ul><li>Thanks to its bundled CP15 co-processor, there’s a <a href="https://www.copetti.org/writings/consoles/playstation-portable/#focused-memory-management">Memory Protection Unit</a> (MPU) in place. This will protect the CPU from arbitrarily executing code from any location in memory.</li></ul></li><li>An <strong>ARM7TDMI</strong> from the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#cpu">Game Boy Advance days</a>. It’s a relatively ignored CPU, unless a DS or DSi game is being played, in which case it acts as a co-processor. However, on the special occasion when a Game Boy Advanced game is running, the main execution falls into this CPU.</li></ul><p>Unfortunately, or for obvious reasons, the three CPUs are never usable at the same time. Instead, the console has three modes of operation:</p><ul><li><strong>Native 3DS mode</strong>: The ARM11 MPCore executes a 3DS game while the ARM946E-S deals with I/O and security. The ARM7, on the other side, is switched off.</li><li><strong>Nintendo DSi mode</strong>: The ARM946E-S and ARM7TDMI operate in a multi-processor configuration to execute a Nintendo DS or DSi game. Just like with its predecessor, the ARM7TDMI has greater access to I/O. Meanwhile, the ARM11 MPCore will be working in the background to replicate missing and re-located DS hardware (real-time clock, power management, keypad, GBA/DS PPU display and so forth).</li><li><strong>Game Boy Advance mode</strong>: The ARM7TDMI is the only CPU executing instructions (in 99% of cases, that will come from a GBA game). The ARM11 MPCore and ARM9, both still operating within the capacities of ‘Native 3DS mode’, will be working in the background.</li></ul><p>If you stop to think about it, the Nintendo 3DS ends up housing four processors in total (Two MP11 cores + one ARM9 + one ARM7), or the absurd amount of six in the case of the New 3DS. How convoluted is that? Luckily, this system didn’t suffer the complications of the <a href="https://www.copetti.org/writings/consoles/sega-saturn/#cpu">Sega Saturn</a> and you can thank Nintendo and ARM’s engineering for that. After all, 3DS developers only had to deal with the MPCore.</p><p>Since the ARM9 and ARM7 are predominantly for I/O, security and backwards compatibility (neither of which require the developer’s awareness), I discuss them in later sections of this article. But if you’d like to know more about the design of the ARM7 and ARM9, I wrote about them in previous articles (the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#cpu">Game Boy Advance</a> and <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#cpu">Nintendo DS</a> ones, respectively).</p><h4 id="multi-core-communication">Multi-core communication</h4><p>I guess the question now is, how can CPUs and cores talk to each other? Well, the easiest way is to share RAM… but you could also try a more efficient approach, depending on the cores trying to communicate:</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/inter_core.bb00bae6a6b8bc25be2a95f6c369baac03e16a1fcb64ff5029ed70f1eb5ac0a6.png"><picture><img alt="Image" width="1281" height="288" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/inter_core.bb00bae6a6b8bc25be2a95f6c369baac03e16a1fcb64ff5029ed70f1eb5ac0a6.png" loading="lazy"></picture></a><figcaption>Representation of the communication channels each CPU is provided with.</figcaption></figure><ul><li>With <strong>inter-core ARM11 communication</strong>, a core can send interrupts to another core by writing on its <code>Software Interrupt Register</code> <sup id="bibref:16"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[16]</a></sup>.</li><li>In the case of <strong>ARM11↔︎ARM9</strong> or <strong>ARM9↔︎ARM7 communication</strong>, the same <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#interconnection">FIFO model</a> from the Nintendo DS is implemented. Plus, the ARM11↔︎ARM9 FIFO is also called ‘PXI’ <sup id="bibref:17"><a href="#bib:cpu-korth" role="doc-biblioref">[17]</a></sup>.</li></ul><h3 id="memory-available">Memory available</h3><p>Having three different CPUs also means the memory layout will not be simple, especially if you care about security.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/memory.35bd0fe628c283231e7d16616a17ceaf4fd5d51e9dbca30ab8d8d10a0bfdba65.png"><picture><img alt="Image" width="621" height="650" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/memory.35bd0fe628c283231e7d16616a17ceaf4fd5d51e9dbca30ab8d8d10a0bfdba65.png" loading="lazy"></picture></a><figcaption>Overview of memory organisation on the Nintendo 3DS.</figcaption></figure><p>To make a long story short, we’ve got the following blocks:</p><ul><li>From the developer’s perspective, the system provides <strong>128 MB of FCRAM</strong>. The New 3DS increased this to <strong>256 MB</strong>. The rest is redundant for games.</li><li>For predominantly security reasons, the ARM11 is also provided with a fast block of <strong>512 KB of SRAM</strong>. The ARM9 is also given a block of <strong>1 MB of SRAM</strong> (<strong>1.5 MB</strong> in the case of the New 3DS).</li><li>By inheriting the model of the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-1-2-arm946e-s">Nintendo DS</a>, the ARM9 also houses <strong>Tightly-Coupled Memory</strong> (TCM). Particularly, there’s <strong>32 KB for instructions</strong> and <strong>16 KB for data</strong>.</li></ul><h4 id="a-new-type-of-memory-spotted">A new type of memory spotted</h4><p>It’s all jolly that the Nintendo 3DS includes 32 times the general-purpose memory of its predecessor, but what about that ‘FCRAM’? Is it any different from the other standards?</p><p>Well, <strong>Fast Cycle DRAM</strong> (FCRAM) is yet another RAM invention, this time authored in 2002 by Fujitsu and Toshiba. Presented as an alternative to DRAM-based technology (i.e.&nbsp;<a href="https://www.copetti.org/writings/consoles/xbox/#memory-layout">SDRAM</a>, <a href="https://www.copetti.org/writings/consoles/playstation/#the-offering">EDO DRAM</a>, <a href="https://www.copetti.org/writings/consoles/nintendo-64/#memory-design">RDRAM</a>, etc.), FCRAM excels on non-continuous reads, where it exhibits a lower latency than DRAM <sup id="bibref:18"><a href="#bib:cpu-fcram1" role="doc-biblioref">[18]</a></sup>. This was done to replicate the performance offered by the more expensive SRAM.</p><p>FCRAM competes directly with DDR DRAM by offering a revamped design of the memory arrays. In place of adding more circuitry on top of it, arrays are split into smaller subblocks, which are then accessed using a 3-stage pipeline <sup id="bibref:19"><a href="#bib:cpu-fcram2" role="doc-biblioref">[19]</a></sup>. In doing so, reading and writing on random locations become faster. These changes are still designed with backwards compatibility in mind. Thus, FCRAM is compatible with DDR DRAM controllers (hence, its full name is ‘DDR FCRAM’).</p><h3 id="faster-memory-transfers">Faster memory transfers</h3><p>The inventors of the MPCore and the AMBA bus happen to also offer a brand of <a href="https://www.copetti.org/writings/consoles/playstation/#taking-over-the-cpu">DMA controllers</a> called <strong>CoreLink</strong>, with Nintendo being a loyal client. So, it’s no mystery as to why the 3DS bundles multiple blocks of <strong>CoreLink DMA-330</strong> into their SoC <sup id="bibref:20"><a href="#bib:cpu-korth" role="doc-biblioref">[20]</a></sup>.</p><p>These DMAs in particular are attached to an AXI bus and act as master devices. They can transfer data between two slaves interconnected with the AMBA protocol (either AXI or the slower APB) with the following advantages:</p><ul><li>Faster transfer rates compared to either CPU.</li><li>Support of up to eight channels (eight transfers at the same time) <sup id="bibref:21"><a href="#bib:cpu-corelink" role="doc-biblioref">[21]</a></sup>.</li></ul><p>To be precise, Nintendo fitted one CoreLink DMA next to the ARM9, this is referred to as <strong>XDMA</strong> and provides <strong>up to four channels</strong>. There’s another DMA next to the ARM11 block, this time called <strong>CDMA</strong>, which provides <strong>up to eight channels</strong>. With the arrival of the New 3DS, another CoreLink DMA-330 is fitted next to the ARM11 block (now a quad-core cluster).</p><h3 id="programming">Programming</h3><p>With all being said, how do you program a system featuring this unorthodox CPU arrangement? To be fair, unusual systems are no strangers to videogame developers. But in this case, <strong>3DS programmers only have access to the ARM11 MPCore</strong>. Furthermore, once you reach the ‘Operating System’ section, you’ll learn the abilities with this cluster are further restricted.</p><p>In any case, no matter the console revision, programmers base their algorithms on the <strong>multi-threading model</strong>: the program groups sequences of instructions using <strong>threads</strong>, these are then dispatched by the operating system to the physical cores, as the former deems fit. Once a novelty for <a href="https://www.copetti.org/writings/consoles/xbox-360/#inside-xenon-programming-styles">Xbox 360 software</a>, this standard provides a layer of abstraction that blinds the developers from writing software only compatible with a fixed number and type of CPU cores.</p><h4 id="dealing-with-the-new-hardware">Dealing with the ‘New’ hardware</h4><p>Since the New 3DS diverts considerably from the original specification, Nintendo set up a thin compatibility layer to enable old 3DS games to work with the new hardware without manual intervention.</p><p>In essence, when a game is launched from a New 3DS console, the game’s code specifies if it’s specifically targeting the new models or not <sup id="bibref:22"><a href="#bib:cpu-applet_manager" role="doc-biblioref">[22]</a></sup>. If it is, the operating system will proceed to activate all the novelties (faster clock speed, extra RAM and use of L2 cache) for that game to enjoy. If it’s not, the operating system will keep its exclusive hardware deactivated until the user exits the game, so the game can safely assume it’s running on the old hardware and will do so without issue.</p><p>To keep supporting the old 3DS, games can be packaged with two codebases (one for the ‘New’ model and the other for the ‘Old’ one). It’s up to the game studios to decide whether to support the old and new 3DS, or only the new 3DS.</p><p>You may be wondering what happens with the rest of the exclusive hardware the New 3DS houses (i.e.&nbsp;extra ARM11 cores and DMA). Well, to properly understand the rationale, I explain this once you reach the ‘Operating System’ section, but I’m afraid you won’t like the answer!</p><hr><h2 id="graphics">Graphics</h2><p>Next to a new CPU is always a modern GPU. So, what kind of <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#graphics">Picture Processing Unit</a> did Nintendo build this time? To tell the truth, none. For the first time in their portable line, <strong>they resorted to a GPU supplier</strong>.</p><p>Nevertheless, the requirements of Nintendo haven’t shifted. The company still wanted a chip with acceptable performance… and the <strong>intellectual property</strong>. This will allow them to embed the GPU into their SoC, in the same way they did with the ARM CPUs.</p><figure><ul><li id="tab-4-1-kart-link"><a href="#tab-4-1-kart">Kart</a></li><li id="tab-4-2-sonic-link"><a href="#tab-4-2-sonic">Sonic</a></li><li id="tab-4-3-mario-link"><a href="#tab-4-3-mario">Mario</a></li><li id="tab-4-4-animal-link"><a href="#tab-4-4-animal">Animal</a></li><li id="tab-4-5-zelda-link"><a href="#tab-4-5-zelda">Zelda</a></li></ul><figure id="tab-4-1-kart"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_kart.f3aee78ccef9f74a20c423ce9ea8803dd71a28438cbf4c9f0aed0ed8e3bca596.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/games/_hu5324041165f0a097655cc1fd8a33a74b_77731_31339cf3bb9271013ab1a183d3738ff6.webp 400w"><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_kart.f3aee78ccef9f74a20c423ce9ea8803dd71a28438cbf4c9f0aed0ed8e3bca596.png" loading="lazy"></picture></a><figcaption>Mario Kart 7 (2011)</figcaption></figure><figure id="tab-4-2-sonic"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/sonic.c943fd06acd604456c0cc247d4364116870b667443bf96ad3a023a50616c0a27.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/sonic.c943fd06acd604456c0cc247d4364116870b667443bf96ad3a023a50616c0a27.png" loading="lazy"></picture></a><figcaption>Sonic Generations (2011)</figcaption></figure><figure id="tab-4-3-mario"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_bros.5d75761bef28df652709a0ad18ed54be183bd72e52c93558e78c9fddb6a2cbf6.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_bros.5d75761bef28df652709a0ad18ed54be183bd72e52c93558e78c9fddb6a2cbf6.png" loading="lazy"></picture></a><figcaption>New Super Mario Bros.&nbsp;2 (2012)</figcaption></figure><figure id="tab-4-4-animal"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/animal_crossing.7aded94c435aa5cce95ad2fe5395a7c699c3ad91af694f7cae42d43ac377aee5.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/animal_crossing.7aded94c435aa5cce95ad2fe5395a7c699c3ad91af694f7cae42d43ac377aee5.png" loading="lazy"></picture></a><figcaption>Animal Crossing: New Leaf (2012)</figcaption></figure><figure id="tab-4-5-zelda"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/zelda.d59e2d38f265b4ad9aca33e5213c11cbdf3313b468768abe4bc0914de2be3d68.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/games/_huc7d8aa9f2330731c61256ecd3c86f738_72234_d95f355bd8ccd2a1150852bb8d302ac0.webp 400w"><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/zelda.d59e2d38f265b4ad9aca33e5213c11cbdf3313b468768abe4bc0914de2be3d68.png" loading="lazy"></picture></a><figcaption>The Legend of Zelda: Majora’s Mask 3D (2015)</figcaption></figure><figcaption>Example of Nintendo 3DS games. All render two frames of 400 x 240 pixels and one frame of 320 x 240 pixels.</figcaption></figure><p>Meanwhile, a potential candidate just finished unveiling their new invention at SIGGRAPH 2006 <sup id="bibref:23"><a href="#bib:graphics-dmp_insight" role="doc-biblioref">[23]</a></sup>. For some time, <strong>Digital Media Professionals Inc.</strong> (also known as ‘DMP’) have been building affordable GPUs for the embedded market and, while their chips are nothing out of the ordinary, they guarantee decent OpenGL ES support. Furthermore, their licensing framework offers <strong>synthesisable GPUs</strong>.</p><p>This seemed enough for Nintendo, who happily negotiated a license for DMP’s latest core, the <strong>PICA200</strong> and subsequently bundled it on CTR CPU (the Nintendo 3DS’ SoC). The GPU runs at <strong>268 MHz</strong>.</p><h3 id="architecture-of-the-pica200">Architecture of the PICA200</h3><p>If I had to summarise it in one sentence, the PICA200 is a budget low-power 3D processor that combines a pre-<a href="https://www.copetti.org/writings/consoles/xbox-360/#a-new-foundation-on-the-way">unified architecture</a> with a modernised API. The underlying architecture of the PICA200 is called <strong>Maestro 2G</strong> <sup id="bibref:24"><a href="#bib:graphics-siggraph" role="doc-biblioref">[24]</a></sup> and its design is compliant with <strong>OpenGL ES 1.1</strong>, but extended with elements from <strong>OpenGL ES 2.0</strong> <sup id="bibref:25"><a href="#bib:graphics-nintendo_gl" role="doc-biblioref">[25]</a></sup>. However, the PICA200’s APIs are not limited to either standard.</p><p>You see, even though the pipeline is segregated and the pixel stage is fixed-function (ala <a href="https://www.copetti.org/writings/consoles/playstation-2/#graphics">PlayStation 2</a>), DMP expanded the limited circuitry with a set of <strong>Maestro functions</strong> that provide capabilities beyond the expectations of the embedded market <sup id="bibref:26"><a href="#bib:graphics-ocp" role="doc-biblioref">[26]</a></sup>. This includes fragment lighting, multiple shadowing algorithms, polygon subdivision, bump mapping, procedural textures and many fog effects.</p><p>Additionally and in contrast to the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-5-3-result">Nintendo DS</a>, the PICA200 <strong>only works with framebuffers</strong>. That’s it. The <a href="https://www.copetti.org/writings/consoles/nes/#graphics">sprite engine</a>, a popular workaround to tackle unaffordable memory requirements, is now a thing of the past. This also includes <a href="https://www.copetti.org/writings/consoles/nes/#secrets-and-limitations">scan-line tricks</a>, as contemporary GPUs work way faster than the refresh rate of a CRT.</p><h4 id="organising-the-content">Organising the content</h4><p>Now that we know that this console can draw 3D shapes, the question now is: where does it store its materials? There are two locations, the large <strong>FCRAM</strong> block and the smaller but faster <strong>VRAM</strong>.</p><p>Nintendo only provided <strong>6 MB of VRAM</strong> exclusively for the GPU. Ideally, programmers would fit as much as they can there, but since it will fill up pretty quickly, it is expected to be used to store data that needs instant access (i.e.&nbsp;commands, buffers and recurrent textures) while placing the rest on FCRAM. The PICA200 comes with a <strong>DMA unit</strong> that can transfer data between FCRAM and VRAM. So, at the end of the day, it’s the responsibility of the programmer to come up with an efficient placement to avoid bottlenecks.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/content.6bb4820e85f8f07d78fa991cbd9cb8eb5f695a045b6ae921d87f081f5f592e84.png"><picture><img alt="Image" width="939" height="605" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/content.6bb4820e85f8f07d78fa991cbd9cb8eb5f695a045b6ae921d87f081f5f592e84.png" loading="lazy"></picture></a><figcaption>Example of how data is organised across the memory available.</figcaption></figure><p>During rendering, programmers allocate dedicated render buffers (i.e.&nbsp;frame, stencil, depth, etc.) for many operations. That’s always been the case. With the 3DS, alongside these buffers, programmers are also expected to reserve extra space for <strong>Display buffers</strong>, these are bound to the physical screens. The 3DS requires to allocate <strong>three Display buffers</strong> (two for the stereoscopic upper screen and one for the bottom one). To give you an idea, the display process works as follows:</p><ol><li>The LCD continuously displays the content of the front (active) Display buffer, as instructed by the value of the buffer index.</li><li>Meanwhile, the GPU finishes rendering geometry in a framebuffer.</li><li>The framebuffer is exported to the back (inactive) Display buffer.</li><li>The GPU swaps the index of the front Display buffer.<ul><li>For practical reasons, the index swap should happen at the end of <a href="https://www.copetti.org/writings/consoles/nes/#tab-5-5-result">Vertical Sync</a> to avoid tearing down the picture <sup id="bibref:27"><a href="#bib:graphics-opengl_swap" role="doc-biblioref">[27]</a></sup>. The official APIs provide synchronisation functions to keep all operations at the correct pace.</li></ul></li><li>The LCD will now be scanning the recently updated Display buffer from now on.</li></ol><h4 id="adopting-open-standards">Adopting open standards</h4><p>On an interesting note, just like the ARM11 MPCore adopts ARM’s AXI protocol for interconnecting its cores, DMP adopted a less-proprietary option called <strong>Open Core Protocol</strong> (OCP) <sup id="bibref:28"><a href="#bib:graphics-ocp" role="doc-biblioref">[28]</a></sup>. As its name indicates, the Open Core protocol does not impose any licensing restrictions on its users, something that vendors using the PICA200 may find advantageous. For comparison purposes, AXI was released in 2003 (along with the AMBA 3 specification) while OCP was published in 2001. It does make me wonder what kind of technology Nintendo fitted to adapt the OCP signal coming from the PICA200 into an AXI-compliant signal, so the rest of the SoC understands it. I assume that there’s a bridge between the PICA200 and the AXI bus.</p><p>Interestingly enough, the predecessor of the PICA200, the ULTRAY2000, shares many similarities with its successor. The most notable difference, however, is that the data interfaces use the PCI and DDR-SDRAM protocols instead <sup id="bibref:29"><a href="#bib:graphics-hardware" role="doc-biblioref">[29]</a></sup>.</p><h3 id="constructing-the-frame">Constructing the frame</h3><p>Naturally, the GPU is not aware of the stereoscopic or dual-screen nature of the displays, it will only be tasked with rendering three screens during gameplay:</p><ul><li><strong>Top stereoscopic-left</strong>: 400 x 240 pixels wide.</li><li><strong>Top stereoscopic-right</strong>: 400 x 240 pixels wide.</li><li><strong>Bottom</strong>: 320 x 240 pixels wide.</li></ul><p>All of them can display 8-bit RGB colours, which equates to up to 16.78 million colours.</p><p>Considering players will expect acceptable frame rates on all three screens (especially on the first two), the single PICA200 will be subject to high amounts of workload throughout its operation, an important aspect to remember when judging its performance.</p><p>That being said, here is an overview of how data travels to draw a single frame:</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline.1242abb39bcb4fa2d51bfbaf5325b31977b76e5f0542e8847a06d84a447de79e.png"><picture><img alt="Image" width="548" height="479" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline.1242abb39bcb4fa2d51bfbaf5325b31977b76e5f0542e8847a06d84a447de79e.png" loading="lazy"></picture></a><figcaption>Overview of the graphics pipeline in PICA200.</figcaption></figure><p>… and as customary in this series of articles, we’ll now take a look at what happens at each stage.</p><div><ul><li id="tab-5-1-commands-link"><a href="#tab-5-1-commands">Commands</a></li><li id="tab-5-2-vertex-link"><a href="#tab-5-2-vertex">Vertex</a></li><li id="tab-5-3-geometry-link"><a href="#tab-5-3-geometry">Geometry</a></li><li id="tab-5-4-rasteriser-link"><a href="#tab-5-4-rasteriser">Rasteriser</a></li><li id="tab-5-5-fragment-link"><a href="#tab-5-5-fragment">Fragment</a></li><li id="tab-5-6-post-processing-link"><a href="#tab-5-6-post-processing">Post-processing</a></li></ul><div><div id="tab-5-1-commands"><h4 id="tab-5-1-commands">Commands</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/command.1772409bf9d3938e254256094ca5ce70533f1645a965f019601248ced7fda6a7.png"><picture><img alt="Image" width="548" height="392" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/command.1772409bf9d3938e254256094ca5ce70533f1645a965f019601248ced7fda6a7.png" loading="lazy"></picture></a><figcaption>Overview of the command stage.</figcaption></figure><p>This is Nintendo’s first portable console to finally draw triangles in ‘the usual way’. That is, with the use of commands. But it’s not a surprising factor, as the PICA200 is expected to abide by the teachings of OpenGL ES.</p><p>In essence, the PICA200 draws polygons by reading a <a href="https://www.copetti.org/writings/consoles/xbox-360/#tab-6-1-commands">command buffer</a> <sup id="bibref:30"><a href="#bib:graphics-nintendo_gpu_reg" role="doc-biblioref">[30]</a></sup>. Furthermore, the vertex data can either be embedded within the command or stored in a separate buffer in VRAM, with the latter being the most efficient.</p></div><div id="tab-5-2-vertex"><h4 id="tab-5-2-vertex">Vertex</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/vertex.d863f782cc69f3351251f43b8bbf5f9d05d04d6e1ac080e7cebc7cf336a4424a.png"><picture><img alt="Image" width="885" height="482" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/vertex.d863f782cc69f3351251f43b8bbf5f9d05d04d6e1ac080e7cebc7cf336a4424a.png" loading="lazy"></picture></a><figcaption>Overview of the vertex stage.</figcaption></figure><p>The PICA200 provides <strong>four Vertex Processors</strong> (VP) that operate in parallel. However, if the geometry shader (the next pipeline stage) is activated, only <strong>three</strong> processors can be utilised.</p><p>Each core computes 96-bit vectors made of four 24-bit floating-point values <sup id="bibref:31"><a href="#bib:graphics-picasso" role="doc-biblioref">[31]</a></sup>, but unlike the ARM11’s VFP, they don’t comply with IEEE-754 <sup id="bibref:32"><a href="#bib:graphics-shader_isa" role="doc-biblioref">[32]</a></sup>. The vertex processors are programmed using assembly language specific to the PICA200 (reminiscent of the days of the <a href="https://www.copetti.org/writings/consoles/xbox/#graphics">Nvidia NV30</a>) and are operated as follows <sup id="bibref:33"><a href="#bib:graphics-game-vertex" role="doc-biblioref">[33]</a></sup>:</p><ol><li>Developers write the vertex shader using PICA200 assembly. For reference, the instruction set is very similar to Microsoft’s <code>vs_2_0</code> <sup id="bibref:34"><a href="#bib:graphics-vs2" role="doc-biblioref">[34]</a></sup>.</li><li>The shader is compiled using a proprietary assembler.</li><li>The 3DS program must copy the compiled binary to memory (either FCRAM or VRAM).</li><li>Then, the 3DS program issues a GPU command to load the binary and connect it with the program.</li></ol><p>Once the vertex cores finish processing, they output the results to the <strong>Sync Control</strong> block, which acts as a vertex cache and buffer. It has a capacity of <strong>384 Bytes</strong>, enabling it to hold up to 32 96-bit vectors. Finally, the next stage reads from this block.</p></div><div id="tab-5-3-geometry"><h4 id="tab-5-3-geometry">Geometry</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/geometry.0789eaa32552820dd323d1b845f38aa592644f3aa2df21051a90013cad21a0c5.png"><picture><img alt="Image" width="885" height="482" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/geometry.0789eaa32552820dd323d1b845f38aa592644f3aa2df21051a90013cad21a0c5.png" loading="lazy"></picture></a><figcaption>Overview of the geometry stage.</figcaption></figure><p>The geometry stage is a signature feature of 8th-generation consoles, allowing developers to spawn complex geometry out of simple vertex data.</p><p>In this case, the PICA200’s geometry stage is implemented by <strong>stealing one of the four Vertex Processors</strong>. Then, the ‘geometry’ vertex core is loaded with a different vertex shader. Finally, it receives the vertex data from the three other processors.</p><p>Even though the geometry shader is programmable, in practice, <strong>Nintendo doesn’t allow this</strong>. Thus, game developers can only choose from a pre-programmed set of geometry shader programs (found in the SDK). Examples of available geometry shaders include square and line generation (using point primitives), geometry subdivision, silhouette edge rendering; and random particle generation.</p></div><div id="tab-5-4-rasteriser"><h4 id="tab-5-4-rasteriser">Rasteriser</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/rasteriser.f08b4b3d24323cb39f25cf1bc0480ed1229a281438210c64066e59fb6283c992.png"><picture><img alt="Image" width="428" height="345" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/rasteriser.f08b4b3d24323cb39f25cf1bc0480ed1229a281438210c64066e59fb6283c992.png" loading="lazy"></picture></a><figcaption>Overview of the rasteriser stage.</figcaption></figure><p>At this stage, all primitives are converted into pixels.</p><p>The rasterizer unit on the PICA200 is very simple, it just generates triangles out of primitives, then applies culling and clipping to remove unseen triangles (hidden behind others and/or outside the view area, respectively). This is all very similar to OpenGL ES’ modus operandi, albeit developers have to watch out for some coordinate systems that are inverted when working with the PICA200.</p></div><div id="tab-5-5-fragment"><h4 id="tab-5-5-fragment">Fragment</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/fragment.95c8b5fb5695c54a540c32f09ae215f2e5e1274421e46761750fb3e7266fc49c.png"><picture><img alt="Image" width="705" height="482" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/fragment.95c8b5fb5695c54a540c32f09ae215f2e5e1274421e46761750fb3e7266fc49c.png" loading="lazy"></picture></a><figcaption>Overview of the fragment stage.</figcaption></figure><p>The fragment stage is made of two areas: the <strong>texture units</strong>, which can fetch textures in memory and process them. And the <strong>shading unit</strong>, which can perform extra operations on the texture data.</p><p>The PICA200 contains <strong>four</strong> texture units <sup id="bibref:35"><a href="#bib:graphics-fragment" role="doc-biblioref">[35]</a></sup>, each houses <strong>256 Bytes of L1</strong> cache and all of them share <strong>8 KB of L2</strong> cache. However, the units are not homogenous. Instead, the range of services varies between each unit <sup id="bibref:36"><a href="#bib:graphics-pica_pipeline_diagram" role="doc-biblioref">[36]</a></sup>:</p><ul><li>Only three units can process 2D textures.</li><li>Only one unit can perform shadow, cube and <a href="https://www.copetti.org/writings/consoles/playstation-portable/#tab-2-4-textures">projective texture</a> mapping.</li><li>The last unit is more of a noise generator, meaning it only outputs <strong>random textures</strong>. It uses a combination of a random number generator and a colour lookup table. This is a slender yet efficient way of implementing <a href="https://www.copetti.org/writings/consoles/playstation-2/#infinite-worlds">procedure generation</a> with textures, saving bandwidth along the way.</li></ul><p>Afterwards, it’s the job of the shading unit to creatively fiddle with the textures coming in. However - and something unexpected considering we’re talking about an 8th-generation console - is that the PICA200’s unit is <strong>not programmable with <a href="https://www.copetti.org/writings/consoles/xbox/#tab-2-3-pixel">pixel shaders</a></strong> <sup id="bibref:37"><a href="#bib:graphics-kazakov" role="doc-biblioref">[37]</a></sup>. Instead, we find six <strong>configurable colour combiners</strong>, each combiner receives three RGB or Alpha values and performs a logical operation on them. The result is passed to the next combiner and so forth. Each colour combiner can get its input from the previous combiner (except the first), a texture unit or a constant value.</p><p>All in all, a modern reflection of the <a href="https://www.copetti.org/writings/consoles/gamecube/#tab-1-3-texture">Flipper era</a> (while abiding by the OpenGL specification <sup id="bibref:38"><a href="#bib:graphics-glTexEnv" role="doc-biblioref">[38]</a></sup>), but don’t forget developers may also combine this with the aforementioned Maestro functions.</p></div><div id="tab-5-6-post-processing"><h4 id="tab-5-6-post-processing">Post-processing</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/post.23c51da1a21d56daf18466fbb6c4373d4b9b47d82f41956f636b096d7e6b8eca.png"><picture><img alt="Image" width="956" height="527" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/post.23c51da1a21d56daf18466fbb6c4373d4b9b47d82f41956f636b096d7e6b8eca.png" loading="lazy"></picture></a><figcaption>Overview of the post-processing stage.</figcaption></figure><p>After the frame is processed and ready to be written into the framebuffer (or <a href="https://www.copetti.org/writings/consoles/xbox-360/#tab-6-4-pixel-shader">render targets</a>), it goes through a sequence of final ‘corrections’. This is similar to the OpenGL ES 2.0’s pipeline.</p><p>That being said, the frame goes through <strong>alpha</strong>, <strong>stencil</strong> and <strong>depth</strong> testing. Afterwards, the result can be mixed with an existing frame (in the framebuffer) using the colour blender or logical operators (AND, XOR, etc.). Finally, the frame is written into the assigned buffer in memory either as a whole or through a stencil filter (for masking).</p><p>For additional smoothing of the edges, the PICA200 can render the framebuffer at twice the selected dimensions, and then average it with antialiasing 2x2. This is an <a href="https://www.copetti.org/writings/consoles/xbox/#tab-2-4-post-processing">old technique</a> known as <strong>supersampling</strong>.</p><p>Once the framebuffer is ready to be displayed, it must be copied into another block in memory called <strong>Display Buffer</strong> (whose format is better aligned to the scan-line procedure of the LCD screen) and then transferred to the LCD in the form of scan-lines.</p></div></div></div><h3 id="interactive-comparison">Interactive comparison</h3><p>Now that you’ve seen how the PICA200 draws its triangles on the screen, it’s time for some practical examples. Here I’ve gathered two Marios from Smash Bros games, the Wii and 3DS one. Notice how the level of detail of ‘angry Mario’ hasn’t changed that much, considering we’re comparing a 2006 home console with its 2011 portable.</p><p>It’s worth reminding again that, in practice, the PICA200 will be rendering three screens at the same time, something that the <a href="https://www.copetti.org/writings/consoles/wii/#graphics">Wii’s GPU</a> wasn’t subjected to.</p><h3 id="nostalgic-rendering">Nostalgic rendering</h3><p>After all that’s been explained, there’s one question left unanswered: How does the PICA200 render Nintendo DS and Game Boy Advanced games? You may remember that the DS and GBA’s GPU exhibit completely different modus operandi for <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#the-3d-accelerator">rendering and broadcasting</a> frames.</p><figure><a href="https://www.copetti.org/images/consoles/nintendods/mario/complete.ad64c1f4bb4e348934057c8f4809801019adc8e0ad46312f00c17fd40c24b475.png"><picture><img alt="Image" width="256" height="192" src="https://www.copetti.org/images/consoles/nintendods/mario/complete.ad64c1f4bb4e348934057c8f4809801019adc8e0ad46312f00c17fd40c24b475.png" loading="lazy"></picture></a><figcaption>A frame rendered by the Nintendo DS’ <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#graphics">Graphics Engine</a>, whose pipeline segregates between 2D and 3D data. That’s something the OpenGL-compliant PICA200 doesn’t understand.</figcaption></figure><p>The explanation is that the <strong>DS and GBA PPUs are housed in the SoC</strong> and DSi/DS/GBA games will operate them as they originally did on previous consoles. The PPUs output (scanlines) is delivered to a block called <strong>LgyFB</strong>, which may optionally upscale the frame, and then forwarded to the framebuffer, where the PICA200 will take care of displaying it. It’s the job of the ARM11 and its DMA to take care of all memory transfers during this process.</p><p>Naturally, this arrangement will add some delay (a.k.a. lag), albeit negligible to the user.</p><hr><h2 id="audio">Audio</h2><p>Overall, the SoC houses <strong>two audio blocks</strong>:</p><ul><li>A proprietary <strong>DSP</strong> exclusively programmed for sound operations. This is used by 3DS games.</li><li>A variant of the <a href="">Nintendo DS audio block</a> named <strong>CSND</strong>. 3DS, DS and GBA games use it.</li></ul><h3 id="the-3ds-only-hardware">The 3DS-only hardware</h3><p>You may know that this same DSP was previously bundled with the Nintendo DSi, but treated as an optional accelerator instead. With the 3DS, it’s become the designated audio processor, so it’s no longer a voluntary component.</p><p>The DSP is called <strong>CEVA TeakLite II</strong> <sup id="bibref:39"><a href="#bib:audio-teakra" role="doc-biblioref">[39]</a></sup> and operates at <strong>~134 MHz</strong> <sup id="bibref:40"><a href="#bib:audio-teakra_arch" role="doc-biblioref">[40]</a></sup>. It’s manufactured by ParthusCeva, a company that provides synthesisable cores for audio processing <sup id="bibref:41"><a href="#bib:audio-dsp_press" role="doc-biblioref">[41]</a></sup>, and I guess ‘synthesisable’ was the keyword Nintendo was looking for when they partnered.</p><p>Moving on, the DSP outputs stereo samples (<strong>2 channels</strong>) of up to <strong>32 kHz</strong> of sampling rate and <strong>16-bit</strong> resolution.</p><p>Next to this component, we can find <strong>512 KB of RAM</strong> that is used by the DSP as a working area. It’s double-buffered (256 KB per buffer), so both the CPU and DSP can read and write without interruption <sup id="bibref:42"><a href="#bib:audio-dsp_memory" role="doc-biblioref">[42]</a></sup>. Apart from that, the DSP comes with a dedicated DMA that can transfer data in and out of those 512 KB.</p><h4 id="operation">Operation</h4><p>For all intents and purposes, games treat this as an opaque DSP. Thus, only Nintendo knows how to program it.</p><p>3DS programs, as a consequence of being developed using the official SDK, bundle a DSP firmware (solely authored by Nintendo) which is then uploaded to the DSP chip at runtime <sup id="bibref:43"><a href="#bib:audio-dsp_binary" role="doc-biblioref">[43]</a></sup>. Afterwards, programs rely on that firmware to execute audio-related routines. Furthermore, the audio services provided by the operating system further abstract the communication between the program and the DSP’s firmware <sup id="bibref:44"><a href="#bib:audio-dsp_services" role="doc-biblioref">[44]</a></sup>.</p><p>In any case, while the DSP firmware may change over the years, some capabilities have remained the same. For instance, the DSP can mix <strong>ADPCM</strong> and <strong>PCM</strong> samples. with support of up to <strong>24 channels</strong> <sup id="bibref:45"><a href="#bib:audio-dsp_memory" role="doc-biblioref">[45]</a></sup>. There’s also functionality for filtering and sequencing, including the generation of <a href="https://www.copetti.org/writings/consoles/nes/#audio">PSG</a>-like sounds.</p><p>Interestingly enough, the steps followed for hacking the 3DS (so it can execute homebrew application) optionally involve extracting the HOME Menu’s DSP firmware, so homebrew may use it to provide audio output <sup id="bibref:46"><a href="#bib:audio-dsp_dump" role="doc-biblioref">[46]</a></sup>.</p><h3 id="the-backwards-compatible-block">The backwards-compatible block</h3><p>At the other end of the spectrum, we find the CSND block. 3DS may use it as an extension of the DSP and DS/DSi/GBA games rely on it to replicate their hardware.</p><p>In terms of functionality, the CSND features <strong>32 channels</strong> <sup id="bibref:47"><a href="#bib:audio-3ds_sound" role="doc-biblioref">[47]</a></sup>, which is twice the amount of the Nintendo DS counterpart.</p><p>Curiously enough, early homebrew defaulted to this block for providing sounds, while waiting for the DSP to be reverse-engineered.</p><h3 id="pipeline">Pipeline</h3><p>Both DSP and CSND work independently and separately output their audio to the speaker.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/audio.57ad2d480615199c857bfd09b469bde2b54de077df9f05eefab4c38ceed8b390.png"><picture><img alt="Image" width="665" height="303" src="https://www.copetti.org/images/consoles/nintendo3ds/audio.57ad2d480615199c857bfd09b469bde2b54de077df9f05eefab4c38ceed8b390.png" loading="lazy"></picture></a><figcaption>Overview of the audio pipeline.</figcaption></figure><p>As a curious note, the original Nintendo 3DS didn’t play well with the speaker’s capabilities, as Nintendo ended up providing troubleshooting guides for cases of buzzing noises and fluctuations with 3D slider <sup id="bibref:48"><a href="#bib:audio-buzzing" role="doc-biblioref">[48]</a></sup>, all caused by the design of the case.</p><hr><h2 id="io">I/O</h2><p>This section tends to be very rich in technologies considering Nintendo’s consoles favour generous I/O before state-of-the-art CPUs and GPUs. Let’s see what the Nintendo 3DS offers.</p><h3 id="external-interfaces-and-peripherals">External interfaces and peripherals</h3><p>The Nintendo DS had tons of modules built-in and the Nintendo DSi added more on top of it (after removing the GBA Slot). Now we found ourselves with a new console combining interfaces from two decades (the 2000s and 2010s).</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/diagram.4c27d5fbaf9d12fd48f1f6fd2a03bbd5b6652a15e22d418a486fc874896b126c.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/_hud261010f8fda45b6fb91d14bdc6124f7_88175_01c078a07c7bbb5949a953c8edcc6b9f.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud261010f8fda45b6fb91d14bdc6124f7_88175_2c4c532e83c7d9e10248df14d45e089e.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud261010f8fda45b6fb91d14bdc6124f7_88175_678c74a178b4bb3822d17fa443a083bc.webp 2252w"><img alt="Image" width="2252" height="971" src="https://www.copetti.org/images/consoles/nintendo3ds/diagram.4c27d5fbaf9d12fd48f1f6fd2a03bbd5b6652a15e22d418a486fc874896b126c.png" loading="lazy"></picture></a><figcaption>Main diagram of the console’s architecture. You can sense that the I/O area on the left side was a strong selling point of this console.</figcaption></figure><p>To be fair, we still don’t have a standard like USB, but that may be expendable considering the Nintendo 3DS bundles the following:</p><ul><li>A keypad composed of <strong>digital buttons</strong>, an <strong>analogue circle pad</strong>.</li><li>3D and volume <strong>sliders</strong>.</li><li>A Wi-Fi <strong>switch</strong>.</li><li>A <strong>resistive touch sensor</strong> on the bottom screen.</li><li>A <strong>gyroscope</strong> measuring the console’s rotation changes.</li><li>An <strong>accelerometer</strong> to measure the console’s motion.</li><li><strong>One front camera</strong> and <strong>two back cameras</strong>, the latter allowing to take stereoscopic pictures.</li><li>An <strong>infrared transceiver</strong>, used to transfer data between external accessories.</li><li>An <strong>SD card</strong> slot, serving as external storage.</li><li>A standard <strong>3.5 mm jack socket</strong> for headphones.</li><li>A <strong>game card reader</strong>, where 3DS, DSi and DS retail games are read from.</li></ul><h4 id="the-new-enhancements">The ‘New’ enhancements</h4><p>If that wasn’t enough, the New 3DS came with more modules on top. This includes:</p><ul><li>Two <strong>extra digital buttons</strong> and an extra analogue circle pad (called <strong>‘C-Stick’</strong>).</li><li>An <strong>NFC Reader</strong> on the bottom screen.</li><li>An <strong>infrared LED</strong>, reserved for head tracking.</li><li>The SD slot is replaced with a <strong>microSD slot</strong>.</li><li>The Wi-Fi switch has been removed, now it’s only controlled through software.</li></ul><p>Now, to prevent leaving ‘old’ users behind, Nintendo provided external accessories to enhance the old models, although most of them relied on the single infrared transceiver to connect. Thus, only one accessory could be connected at the same time.</p><p>Not all the exclusive features of the New 3DS can be replicated, however. For instance, the New 3DS’ head tracking mechanism depends on the extra ARM11 core.</p><h3 id="internal-interfaces">Internal interfaces</h3><p>Now it’s time to check how are these interfaces - and others - internally wired up.</p><p>Firstly, a large subset is interconnected with the standard <strong>Serial Peripheral Interface</strong> (SPI) protocol. There are four SPI buses and all of them are accessed by the ARM9 (which I assume also includes the ARM7). The ARM11 only has access to <em>most</em> of them <sup id="bibref:49"><a href="#bib:io-spi_registers" role="doc-biblioref">[49]</a></sup>. In any case, the SPI buses connect the following modules <sup id="bibref:50"><a href="#bib:io-spi_devices" role="doc-biblioref">[50]</a></sup>:</p><ul><li>The flash memory found inside 3DS game cards, for storing save data.</li><li>DS’ Power Management.</li><li>Parts of the Wi-Fi chip.</li><li>Touch screen.</li><li>Sound.</li><li>Microphone.</li><li>Circle Pad.</li></ul><p>Curiously enough, some peripherals are interfaced twice to replicate the old DS/DSi’s I/O layout and also provide extended capabilities for 3DS software.</p><p>Secondly, there’s a <strong>Human-interface device</strong> (HID) module connected to both ARM11 and ARM9 data buses. This is how the digital keypad is accessed. The data is read through a 16-bit register.</p><p>Moving on, we got an <strong>I²C</strong> block which uses a more sophisticated serial protocol. This is connected to the following <sup id="bibref:51"><a href="#bib:io-i2c_devices" role="doc-biblioref">[51]</a></sup>:</p><ul><li>Front camera, also works in DSi mode.</li><li>Two back cameras, the right camera is accessible in DSi mode as well.</li><li>Infrared transceiver.</li><li>The NFC interface, in the case of the New 3DS.</li><li>The ‘QTM’ module, used for head-tracking (New 3DS only).</li><li>Gyroscope.</li><li>MCU chip, a separate controller that interfaces more components (explained in the next section).</li></ul><p>Finally, there are various <strong>registers</strong> interfacing FIFO blocks which, in turn, connect to two relatively high-speed (16 MB/s) peripherals <sup id="bibref:52"><a href="#bib:io-misc" role="doc-biblioref">[52]</a></sup>:</p><ul><li>Internal eMMC memory.</li><li>SD card slot.</li></ul><p>As confusing as it may sound, there’s more hardware left to discuss. The rest is handled by a middle-man chip called <strong>Auxiliary Microcontroller</strong> (MCU) <sup id="bibref:53"><a href="#bib:graphics-hardware" role="doc-biblioref">[53]</a></sup>. This is just a microcontroller designed by NEC and manufactured by Renesas. Particularly, the <strong>model 78K0R</strong>, which bundles a proprietary (yet low-power and relatively modern) processor and a ROM <sup id="bibref:54"><a href="#bib:io-renesas" role="doc-biblioref">[54]</a></sup>. The 78K0R stores a firmware handled by the console’s operating system, both ARM9 and ARM11 can interact with it but so do other peripherals.</p><p>The MCU chip exclusively controls the following <sup id="bibref:55"><a href="#bib:graphics-hardware" role="doc-biblioref">[55]</a></sup> <sup id="bibref:56"><a href="#bib:io-i2c_mcu" role="doc-biblioref">[56]</a></sup>:</p><ul><li>Accelerometer.</li><li>LCD screens.</li><li>LED indicators.</li><li>Power Management.</li><li>Battery fuel gauge and rejection (whether to enable the charging circuitry or not).</li><li>Wi-Fi’s EEPROM.</li><li>Real-Time Clock (RTC).</li><li>3D slider and Wi-Fi switch, the latter is only found on old 3DS models.</li><li>HOME and power buttons.</li></ul><p>A subset of this group is already accessible by the main CPUs. This is because the MCU also perform monitoring tasks, thereby saving resources from the ARM11 or ARM9.</p><h3 id="ready-for-trends">Ready for trends</h3><p>With such a heavy list of I/O hardware, you can now see how Nintendo tried to compete against the smartphone market. This led to interesting services deployed throughout the console’s lifecycle:</p><ul><li>A <strong>QR Reader</strong> bundled with the camera app.</li><li><strong>AR Games</strong>: Nintendo shipped ‘AR Cards’ that could be scanned with the 3DS camera using an app called ‘AR Games’. This would make static Nintendo characters pop up in your room, like any traditional augmented-reality-based application.</li><li><strong>Face Riders</strong>: Another camera-based app, but in this case, takes a photo of the player to compose the game’s characters. The player must then use the gyroscope and microphone to battle his/her evil clones.</li><li><strong>Amiibos</strong>: Uses the NFC reader to scan figurines and unlock game bonuses, the same service was also <a href="https://www.copetti.org/writings/consoles/wiiu/#the-supplemental-interface">implemented in the Wii U</a>.</li><li><strong>SpotPass</strong>: The continuation of <a href="https://www.copetti.org/writings/consoles/wii/#games">WiiConnect24</a>, now automatically connects to unsecured Wi-Fi access points.</li><li><strong>StreetPass</strong>: Automatically exchanges data between nearby 3DS systems. Nintendo marketed it as a way of connecting random 3DS players on the street.</li><li><strong>Play Coins</strong>: Unlocks game content by doing some exercise (walking).</li></ul><hr><h2 id="operating-system">Operating System</h2><p>Having a large number of CPUs eventually impacts the overall complexity of the operating system. Not only that, but this console also stores more than one OS. This originates as a mechanism for providing large services (i.e.&nbsp;DSi/DS/GBA backwards compatible, rescue mode, etc.).</p><p>So, to avoid making this section any more confusing, let’s go by steps.</p><h3 id="architecture">Architecture</h3><p>The Nintendo 3DS, as a whole, comes with four firmware <sup id="bibref:57"><a href="#bib:operating_system-firm" role="doc-biblioref">[57]</a></sup>:</p><ul><li><strong>NATIVE_FIRM</strong>: Operates the console in ‘native’ mode (with the functionality exclusive to the Nintendo 3DS). Here, the ARM11 executes the main program.<ul><li>Curiously enough, there are two instances of NATIVE_FIRM installed (named <strong>FIRM0</strong> and <strong>FIRM1</strong>, respectively) in case the first one gets corrupted, for some reason.</li><li>This firmware is often referred to as ‘Horizon’ as well.</li></ul></li><li><strong>TWL_FIRM</strong>: It commands the Nintendo 3DS to behave like a Nintendo DSi. It does come at the expense of disabling all the exclusive features, but considering how the CPU, GPU, sound and I/O are intertwined; TWL_FIRM is truly a work of art. Consequently, the ARM9 and ARM7 are placed in the foreground (they execute the main program) <sup id="bibref:58"><a href="#bib:operating_system-gbatek_firm" role="doc-biblioref">[58]</a></sup>.<ul><li>The name ‘TWL’ comes from the codename of the Nintendo DSi.</li></ul></li><li><strong>AGB_FIRM</strong>: Similarly to TWL_FIRM but the 3DS now becomes a Game Boy Advance. Here, the ARM7 executes the main program.</li><li><strong>SAFE_FIRM</strong>: Used solely for maintenance-related tasks, such as system updates. This firmware is basically an early revision of NATIVE_FIRM (doesn’t go beyond version <code>3.0</code> on the old 3DS and version <code>8.1</code> on the New 3DS <sup id="bibref:59"><a href="#bib:operating_system-safehax" role="doc-biblioref">[59]</a></sup>).</li></ul><p>All of these firmware come with separate binaries for the ARM11, ARM9 and ARM7 CPUs. The only exception is that the ARM7 won’t be active under NATIVE_FIRM and SAFE_FIRM.</p><p>Generally speaking, the Nintendo 3DS will first launch a Boot ROM and then bootstrap NATIVE_FIRM. Afterwards, the running operating system may choose to reboot to another firmware based on the user’s actions (i.e.&nbsp;load a Nintendo DS game or run the firmware update assistant).</p><p>Let’s take a look now at how each CPU behaves in NATIVE_FIRM mode.</p><h4 id="the-security-processor">The security processor</h4><p>Once NATIVE_FIRM is bootstrapped, the ARM9 runs its own operating system made of a kernel called <strong>Kernel9</strong> and a single program called <strong>Process9</strong> <sup id="bibref:60"><a href="#bib:operating_system-overview" role="doc-biblioref">[60]</a></sup>.</p><p>Kernel9’s design follows the <strong>microkernel</strong> model, meaning it only provides essential abstraction with the hardware, including:</p><ul><li>Memory management.</li><li>Process scheduling.</li><li>Inter-Process Communication.</li></ul><p>On the other side, Process9 is a userland application that implements these services:</p><ul><li>Communication with the ARM11, called ‘PXI’.</li><li>Cryptography-related functions. This involves AES, RSA, SHA and ECDSA.</li><li>I/O management.</li><li>File System.</li><li>Title (3DS software) verification and installation.</li></ul><p>Both Kernel and Process9 reside on an ARM9-only block of 1 MB of SRAM (1.5 MB in the case of the New 3DS).</p><p>In terms of security, there’s no privilege distinction between Kernel9 and Process9, since the latter has unconditional access to a system call that runs arbitrary code in kernel mode.</p><p>In summary, combined with the exclusive I/O hardwired into the ARM9, this CPU has the role of a <strong>security processor</strong>, much like what the <a href="https://www.copetti.org/writings/consoles/wii/#the-hidden-co-processor">Wii and Wii U’s ARM9</a> also did, and unlike the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#design">co-processor architecture</a> of the Nintendo DS, where its second processor just offloaded I/O and audio tasks.</p><h4 id="the-user-processor">The user processor</h4><p>Likewise, the ARM11 runs a kernel of similar architecture to the ARM9 one (now called <strong>Kernel11</strong>). The big difference is that the ARM11 will be running multiple userland processes, and in doing so they provide services like:</p><ul><li>Communication with the ARM9, called ‘PXI’.</li><li>Multi-core processing.</li><li>Networking, HTTP and SSL.</li><li>Connection with Nintendo online infrastructure.</li><li>The graphical shell (called ‘HOME Menu’).</li><li>The ability to launch applications.</li><li>A layer of abstraction for apps called <strong>Services</strong>, which games must call to access hardware resources. Some components like the GPU are interfaced by a very thin API, nonetheless.<ul><li>Furthermore, services are implemented in a layered manner. Games only access a subset of these, and the latter in turn invokes greater privileged and specialised services.</li></ul></li></ul><p>The ARM11’s kernel resides on a dedicated block 512 KB of SRAM <sup id="bibref:61"><a href="#bib:operating_system-glossary" role="doc-biblioref">[61]</a></sup>, also called ‘AXI Work RAM’ or ‘AXI WRAM’, because it’s connected to the ARM11 using the AXI protocol.</p><h4 id="imposed-behaviour">Imposed behaviour</h4><p>Now for the bitter news, NATIVE_FIRM also enforces unusual restrictions on user programs.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/os_levels.a8d96b55f3e7dd4e555220d3bbcf9a0dff72eb90418a51175a6abac405813dd3.png"><picture><img alt="Image" width="761" height="519" src="https://www.copetti.org/images/consoles/nintendo3ds/os_levels.a8d96b55f3e7dd4e555220d3bbcf9a0dff72eb90418a51175a6abac405813dd3.png" loading="lazy"></picture></a><figcaption>Overview of the privilege levels in the Nintendo 3DS running NATIVE_FIRM, after combining both operating systems.</figcaption></figure><p>To start with, the ARM11’s scheduler is hard-coded with specific behaviour for each core (as opposed to treating every core as general-purpose units):</p><ul><li>The first core, called <strong>appcore</strong>, is for the game to use. Yet, its thread scheduling policy is FIFO, meaning that the game can deadlock itself if multi-threading is improperly used <sup id="bibref:62"><a href="#bib:operating_system-multithreading" role="doc-biblioref">[62]</a></sup>.</li><li>The second core, called <strong>syscore</strong>, gets assigned system-related tasks.</li></ul><p>Conversely, syscore can lend 30% of its execution time to user applications, which may be helpful to offload some operations, although not every routine may work under syscore (especially time-sensitive ones) <sup id="bibref:63"><a href="#bib:operating_system-dsx86" role="doc-biblioref">[63]</a></sup>.</p><p>The curtailment is further extended with the New 3DS, whose ARM11 MPCore now comes with four cores, namely:</p><ul><li><strong>The fourth MP11 is solely used for face-tracking</strong>. Instead of adding circuitry, Nintendo engineers implemented face-tracking through pure software. Thus, it reserves one MP11 for this. I’m guessing this was a cost-effective solution for Nintendo.</li><li><strong>The third MP11 core is permanently idle</strong>, as the scheduler is never instructed to dispatch threads there. Game meta-data does include a flag to enable thread scheduling on this core, albeit its usage on any commercial game is yet to be confirmed. It’s highly possible that at one point Nintendo considered it, but ultimately rendered it unfeasible for battery consumption reasons, or maybe because single-core games would suffer some compatibility issue.<ul><li>Considering the New Nintendo 2DS XL is a New Nintendo 3DS without the stereoscopic screen, that means half of its quad-core CPU is wasted!</li></ul></li><li>CDMA, the ‘New’ DMA unit, is only accessible during the console’s boot <sup id="bibref:64"><a href="#bib:operating_system-dma" role="doc-biblioref">[64]</a></sup>. After the boot process finishes, <strong>CDMA is never used again</strong>.</li></ul><p>Moving on, in terms of usable RAM for games, we know that the Nintendo 3DS and New Nintendo 3DS come with 128 MB and 256 MB of FCRAM, respectively. What you need to know now is that the available RAM for apps is only <strong>64 MB</strong> and <strong>124 MB</strong> <sup id="bibref:65"><a href="#bib:operating_system-memory" role="doc-biblioref">[65]</a></sup>, respectively. This means that the OS consumes ~50% of the console’s main memory, not a particularly pleasant quality! To alleviate this, games also have the option to set a flag in their metadata (called <code>APPMEMTYPE</code>) to claim more FCRAM from the system, up to <strong>96 MB</strong> and <strong>176 MB</strong>, respectively. Behind the scenes, that flag instructs the system to reboot the console and boot the game without launching the HOME Menu beforehand, saving memory in the way.</p><p>All things considered, you can now sense how not all extra hardware in the New 3DS will automatically imply faster software. It’s a shame, and it gives me the feeling that the New 3DS was a rushed product, from the software perspective. But to be fair, Nintendo never planned the ‘New’ 3DS to be a full successor of the original 3DS. The ‘New’ brand was a clear move to refresh the 3DS line, considering the sales number wasn’t satisfying, to say the least.</p><h3 id="storage-medium">Storage Medium</h3><p>Now that we know how the operating system is designed, let’s look at where and how data is stored in this console.</p><div><ul><li id="tab-6-1-boot-roms-link"><a href="#tab-6-1-boot-roms">Boot ROMs</a></li><li id="tab-6-2-otp-memory-link"><a href="#tab-6-2-otp-memory">OTP memory</a></li><li id="tab-6-3-emmc-nand-link"><a href="#tab-6-3-emmc-nand">eMMC NAND</a></li><li id="tab-6-4-sdmicrosd-link"><a href="#tab-6-4-sdmicrosd">SD/microSD</a></li></ul><div><div id="tab-6-1-boot-roms"><h4 id="tab-6-1-boot-roms">Boot ROMs</h4><p>Following its long ancestor, the <a href="https://www.copetti.org/writings/consoles/game-boy/#cpu">Game Boy</a>, the SoC stores a series of unencrypted ROMs containing the programs used for booting up NATIVE_FIRM <sup id="bibref:66"><a href="#bib:operating_system-bootloader" role="doc-biblioref">[66]</a></sup>. These bootstrappers are called <strong>Boot9</strong> and <strong>Boot11</strong>; and are executed by the ARM9 and ARM11, respectively. Likewise, they are physically and virtually kept hidden for security reasons. To give you an example, Boot9 stores AES decryption keys, which are not something to carelessly leave anywhere.</p><p>Interestingly enough, Boot9’s code has revealed that it’s more capable than just bootstrapping NATIVE_FIRM from eMMC NAND. However, due to certain routines having hardcoded directories and security layers added on top, the only firmware the Boot ROMs can ultimately load is NATIVE_FIRM from eMMC NAND.</p><p>Moreover, while multiple components have changed with the arrival of the New 3DS, the BootROMs have not changed a bit <sup id="bibref:67"><a href="#bib:operating_system-boot" role="doc-biblioref">[67]</a></sup>.</p></div><div id="tab-6-2-otp-memory"><h4 id="tab-6-2-otp-memory">OTP memory</h4><p>To further increase the level of security, the console stores a series of console-unique information in <strong>One-Time-Programmable</strong> (OTP) memory <sup id="bibref:68"><a href="#bib:operating_system-otp" role="doc-biblioref">[68]</a></sup>. Similarly to the <a href="https://www.copetti.org/writings/consoles/wii/#tab-7-1-shared-encryption">Wii</a> and <a href="https://www.copetti.org/writings/consoles/wiiu/#tab-7-1-dedicated-hardware">Wii U</a>, this information also includes encryption keys.</p><p>OTP is written once during manufacturing, so the keys differ between each console. Hence, one hacked console won’t necessarily be able to compromise the rest. This is a significant milestone for a portable console, considering a certain <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-10-1-encryption-system">previous implementation</a> included global keys.</p></div><div id="tab-6-3-emmc-nand"><h4 id="tab-6-3-emmc-nand">eMMC NAND</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/emmc.8c2a1c478ffded9127a1389a411ec8ff8eb6b16c39a7ebc62f693480261828e4.webp"><picture><img alt="Image" width="900" height="699" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hua5f2cd62267f437d12e49c5b8a374e56_58072_86c90e2811a715b2aa2a3980f340716e.png" loading="lazy"></picture></a><figcaption>Samsung-made eMMC chip on the original 3DS.</figcaption></figure><p>Next to the big SoC, there’s an eMMC NAND Flash chip. However, its size is slightly different depending on the manufacturer <sup id="bibref:69"><a href="#bib:operating_system-fs" role="doc-biblioref">[69]</a></sup>. For instance, Toshiba supplied <strong>943 MB</strong> and <strong>1,888 MB</strong> chips, while Samsung provided ones with <strong>954 MB</strong> and <strong>1,240 MB</strong>.</p><p>To tackle this disparity, Nintendo defined the 3DS partition table using a common size: <strong>943 MB</strong> for Old 3DS (Toshiba-size) and <strong>1,240 MB</strong> (Samsung-size) for New 3DS. So, if your console came with a larger eMMC chip, the extra space is unfortunately left unemployed.</p><p>In any case, the console relies on eMMC for storing its system data, including the multiple firmware, and user data (saves and configs within 3DS and DSi mode).</p></div><div id="tab-6-4-sdmicrosd"><h4 id="tab-6-4-sdmicrosd">SD/microSD</h4><p>Once an optional (and sometimes, symbolic) medium, SD cards now enjoy similar responsibilities to internal storage, as the 3DS is dependent on it to download software from the eShop and store user data (game saves, camera pictures and microphone recordings) <sup id="bibref:70"><a href="#bib:operating_system-sd" role="doc-biblioref">[70]</a></sup>.</p><p>Software and user data stored here are protected with AES-128-CTR encryption.</p></div></div></div><h3 id="boot-process">Boot process</h3><p>Now that we know how the operating system is structured and where data is stored, let’s see how the Nintendo 3DS goes from a glossy powered-off brick to becoming an operating console offering multiple services.</p><h4 id="multi-core-chaos">Multi-core chaos</h4><p>Considering the 3DS must be able to manage four processors (2-core ARM11 + ARM9 + ARM7) in its SoC - or six, if you look at the New 3DS - one can only wonder how these ‘central’ processors suddenly become exceptionally coordinated during the console’s startup. Well, it’s all about implementing a master-slave hierarchy.</p><p>With the ARM9 and ARM7, there’s not a lot of room for doubt, both can be powered on separately and load different binaries. So the challenge is mainly focused on disseminating the homogenous multi-core ARM11.</p><p>In the ARM11 MPCore cluster, all cores start execution at vector <code>0x00000000</code> <sup id="bibref:71"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[71]</a></sup>. However, CP15 (the System Control co-processor) provides a register called <code>CPU ID</code> which, among other things, serves to identify the core currently executing instructions. Thus, programmers can query this register to decide whether the current CPU core should give orders (master) or wait for commands (slave). ARM later improved this technique by supplying a dedicated register called <code>mpidr</code>, found in ARMv7 CPUs.</p><p>Thanks to this, Nintendo engineers were able to identify any CPU core within the 3DS cluster and implement a bootloader where all cores become coordinated, and then carry out the necessary functions to bring the console to life.</p><h4 id="boot-procedure">Boot procedure</h4><p>Time to dive into the boot process. As with any other console of its generation, security is of great importance, which will have an impact on the boot stage. To avoid making this section too dense, I’ve simplified the stages where the security system is set up, but you’ll find more information in the ‘Anti-piracy’ section.</p><p>Having said that, once the console is powered on, the following sequence of events takes place <sup id="bibref:72"><a href="#bib:operating_system-boot" role="doc-biblioref">[72]</a></sup> <sup id="bibref:73"><a href="#bib:operating_system-bootrom" role="doc-biblioref">[73]</a></sup>:</p><ol><li>The ARM9 and ARM11 power up.</li><li>The ARM9’s reset vector is at address <code>0xFFFF0000</code>, which points to Boot9 <sup id="bibref:74"><a href="#bib:operating_system-memory" role="doc-biblioref">[74]</a></sup>. The ARM11 is induced in an infinite reset until its reset pin is lifted.</li><li>Boot9 clears ARM11’s reset pin and then initialises the ARM9’s MPU.</li></ol><p>The ARM11 MPCore will now start execution of Boot11 in parallel:</p><ol><li>ARM11’s reset vector is at address <code>0x00000000</code> <sup id="bibref:75"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[75]</a></sup>, which happens to be in the same place as Boot11.</li><li>Boot11 will branch depending on which core is it being executed on. If it’s greater than core 2, it hangs indefinitely.</li><li>Wait until ARM9 is finished bootstrapping a firmware</li></ol><p>Meanwhile, the ARM9 will be busy continuig with Boot9 execution:</p><ol start="4"><li>The AES and RSA public keys are exported to the AES and RSA engines (these will be explained in the ‘Anti-piracy’ section).</li><li>Boot9 will try to boot from NAND.<ol><li>In NAND, there’s a partition at location <code>0x0</code> called ‘NCSD header’, this states that there are eight partitions, each with a firmware to boot from.</li><li>For each firmware partition listed, Boot9 will fetch its header, validate the SHA-256 hash and RSA-2048 signature (using a set of keys previously loaded from BootROM) and repeat this process until one validation succeeds. Then, it will boot from there.</li></ol></li><li>If all validations in NAND fail, Boot9 will try to boot from a Flash memory in the Wi-Fi module. If that also fails, the console will display an error screen.</li><li>The first partition validated happens to contain NATIVE_FIRM. Boot9 will proceed to copy the firmware to different memory areas based on the header’s parameters.</li><li>Disable half of Boot9 and Boot11. In doing so, FCRAM will be accessible.</li><li>Redirect ARM9 and ARM11’s execution to the firmware’s entry points.</li></ol><p>Now that NATIVE_FIRM is bootstrapped:</p><ol><li>The ARM9 will:<ol><li>Load Kernel9.</li><li>Kernel9 hides OTP memory and loads Process9.</li><li>The ARM9 CPU is now up and running Process9.</li></ol></li><li>Whilst the ARM11 does the following:<ol><li>Load Kernel11.</li><li>In the case of the New 3DS, Kernel11 will write to a new register called <code>CFG11_BOOTROM_OVERLAY_CNT</code> to overlay Boot11 code <sup id="bibref:76"><a href="#bib:operating_system-pdn" role="doc-biblioref">[76]</a></sup>. This will allow to redirect execution of the new ARM11 cores (core 3 and core 4) away from Boot11 to arbitrary functions in Kernel11, thereby taking control of them.</li><li>Kernel11 will start various system processes, including PM (Process Manager).</li><li>PM will start the ‘NS’ (Nintendo User Interface Shell) system module.</li><li>NS will either launch a game or the HOME Menu application.</li><li>The user is now in control.</li></ol></li></ol><h4 id="alternative-boot-processes">Alternative boot processes</h4><p>All of the previous explanations have been focused on booting up NATIVE_FIRM, which results in the traditional native 3DS mode. For other firmware such as TWL_FIRM, AGB_FIRM and SAFE_FIRM, it’s a bit more complicated. Turns out the previous boot process is still needed because only NATIVE_FIRM can be booted from a power cycle. But once this is running, it can bootstrap any of those firmware, and each will program the ARM9 differently. In either case, the security set-up during Boot9 will still be enforced.</p><p>TWL_FIRM and AGB_FIRM, in particular, operate a special set of registers that mould the 3DS hardware and memory layout in accordance with what DS, DSi or GBA games expect to find. FCRAM can still be accessed, allowing to boot a game ROM from those places as well (apart from the NTR card reader). However, FCRAM will be reconfigured to follow the DS and GBA bus specification (16-bit wide, instead of 32-bit).</p><p>A big difference about the backwards compatible firmware is that, at last, the ARM7 will be active (as Nintendo DSi/DS and GBA software require it).</p><p>To exit either mode, non-NATIVE firmware contain a routine that reboots the system and consequently returns it to NATIVE_FIRM. Thus, 3DS mode.</p><h3 id="interactive-shell">Interactive shell</h3><p>The 7th generation of console interfaces has landed on the Nintendo 3DS. A clear indication is that users don’t need a retail game to make the most out of their console, just navigate through the shell and you’ll find numerous apps and services bundled. This includes the special offering of this console (3D camera, stereoscopic view and augmented reality). The pressure to compete against smartphones couldn’t be clearer.</p><p>In terms of user interface design, I’m inclined to say there are many patterns borrowed from the <a href="https://www.copetti.org/writings/consoles/wii/#broadways-os">Wii System Menu</a>, yet ported to a dual-screen portable system. The <strong>HOME Menu</strong> (name of the interactive shell) uses a 1-page navigation system where every installed application is shown on a scrollable grid. Except for a few shortcuts here and there, every service is an application to be launched.</p><p>Now, being a Nintendo product, you can expect a special focus on creativity and attention to detail. Families are the target audience, nevertheless, adults are the ones paying, and Nintendo knows that.</p><h4 id="maintaining-consistency">Maintaining consistency</h4><p>The NS module is not only responsible for launching the interactive shell, it also offers 3DS software with the ability to invoke routines to handle certain interactions. One example is the ‘Back to HOME Menu’ overlay, which must be shown whenever the user presses the ‘HOME’ button.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/shell/back_home.2ce3303f63dc4e934fb2426840b63819f305e9727fd44f5ff4de45a7c927d4c4.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/shell/back_home.2ce3303f63dc4e934fb2426840b63819f305e9727fd44f5ff4de45a7c927d4c4.png" loading="lazy"></picture></a><figcaption>Users can press the ‘HOME’ button midgame, this will reinstate the HOME Menu without closing the current application. This event is handled by the running application but the routines are provided by the NS service.</figcaption></figure><p>Furthermore, 3DS software may also invoke ‘mini applications’ for attending other events (i.e.&nbsp;show the virtual keyboard), these are known as <strong>Applets</strong> <sup id="bibref:77"><a href="#bib:operating_system-ns" role="doc-biblioref">[77]</a></sup>.</p><p>Both sets are a crucial dependency for all applications, as they are responsible for properly reacting to external events consistently. Interestingly enough, since Applets and NS routines are not part of the game itself, in the case of New 3DS systems, even if a game is running in compatibility mode (that is, with all the ‘New’ hardware disabled), they will still be executed using the full extend of hardware, giving a small performance boost to unoptimised 3DS games.</p><h4 id="the-legacy-shell">The legacy shell</h4><p>Whilst the special firmware includes the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#operating-system">old BIOS routines</a> DS/GBA games will expect, there’s no DS or DSi shell in sight.</p><p>The old <strong>Wi-Fi setup screens</strong> (invoked by DSi and DS games) are the only exceptions. Interestingly enough, while the original DS Wi-Fi settings are useless (as they can only connect to WEP-protected access points), the DSi counterpart (accessed from DSi and ‘DSi enhanced’ DS games) can alter the 3DS’ Wi-Fi settings. Yet, these games bundle an old Wi-Fi driver that only worked with the real DSi (the 3DS contains an Atheros AR6014 while the DSi came with an Atheros AR6002 or AR6013). So, to tackle this, both Wi-Fi settings are automatically synced when the firmware boots up <sup id="bibref:78"><a href="#bib:operating_system-firm" role="doc-biblioref">[78]</a></sup>.</p><h4 id="updatability">Updatability</h4><p>Well, of course, an updatable system is pretty much a requirement, not only for providing new functionality but also from a security perspective.</p><p>You can update the system software either online or through a game cartridge. Confusingly enough, both contain different update packages. Game cartridges only bundle system updates without the updated user apps, whereas network updates include everything <sup id="bibref:79"><a href="#bib:operating_system-home" role="doc-biblioref">[79]</a></sup>. Consequently, version names encode a mix of the two, in case the user used both channels.</p><p>To install updates, the NS service reboots into <code>SAFE_FIRM</code>, where the <strong>System Updater</strong> takes care of this process <sup id="bibref:80"><a href="#bib:operating_system-settings" role="doc-biblioref">[80]</a></sup>.</p><hr><h2 id="games">Games</h2><p>It’s time to check how game development and distribution were carried out. Additionally, we’ll see some exclusive services Nintendo prepared for this console.</p><h3 id="development-ecosystem">Development ecosystem</h3><p>Before the Nintendo 3DS arrived, developing for embedded system involved monumental efforts and high levels of patience. Compared to desktop applications, the tooling wasn’t standardised and sometimes it didn’t converge well with each other (ActiveSync is the clearest example I remember). The range of documentation didn’t usually go beyond what the manufacturer provided, the same applied for technical support.</p><p>Enter the 2010s decade, coinciding with the influx of an ARM-based smartphone industry and more efficient compilers, development for those platforms was no longer a complicated endeavour. Consequently, game studios developing for the Nintendo 3DS were able to enjoy this evolution. Now, Nintendo was not providing a standard toolchain yet, but they were on the right track (finally reached with the Nintendo Switch).</p><p>Curiously enough (and this is an interesting contrast), back in 2011, Apple offered Clang/LLVM 1.3 and OpenGL ES 3.0 for developing iOS apps, this was considered state-of-the-art for mobile projects. Well, you’ll see throughout this section that this wasn’t the case for Nintendo. Yet, at present, if you grab an old iPhone 4s and try to install any app on the App Store (its only official medium), it will tell you your system is too old. Whereas you can still play any retail game on your 3DS. Food for thought.</p><div><ul><li id="tab-8-1-hardware-kits-link"><a href="#tab-8-1-hardware-kits">Hardware Kits</a></li><li id="tab-8-2-software-kits-link"><a href="#tab-8-2-software-kits">Software Kits</a></li></ul><div><div id="tab-8-1-hardware-kits"><h4 id="tab-8-1-hardware-kits">Hardware Kits</h4><p>Nintendo partnered with two suppliers to produce development kits <sup id="bibref:81"><a href="#bib:games-hardware" role="doc-biblioref">[81]</a></sup>. The first supplier was the well-known <strong>Intelligent Systems</strong> and the other was <strong>Kyoto MicroComputers</strong>.</p><p>Among the many options, studios could rent a general-purpose ‘CTR-BOX’. This is a metallic box housing the 3DS hardware, and connected to it is a ‘dummy’ 3DS case that serves as a controller and display. With it, developers could deploy, test and debug their code.</p><p>For more single-purpose tools, studios could get official <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-9-1-the-hardware">flashcards</a> to distribute game prototypes to external testers. These flashcards still only run on non-retail equipment, though this included cheaper options (with reduced functionality) than the fully-fledged CTR-BOX.</p><p>With the arrival of the New 3DS, IS and Partner offered the ‘SNAKE’ kits with updated hardware.</p></div><div id="tab-8-2-software-kits"><h4 id="tab-8-2-software-kits">Software Kits</h4><p>As always, licensed studios would also get an SDK package from Nintendo which included <sup id="bibref:82"><a href="#bib:games-software" role="doc-biblioref">[82]</a></sup>:</p><ul><li>A variant of <code>armcc</code> (ARM’s <strong>C</strong> and <strong>C++ compiler</strong>) modelled for the Nintendo 3DS and the ARM11 MPCore.<ul><li>This will just produce code for the ARM11 MPCore. Both ARM9 and ARM7 are out of the equation, as 3DS games only run within the ARM11 cluster.</li></ul></li><li><strong>Debuggers</strong> made for IS and Partner’s development kits.</li><li><strong>APIs</strong> to communicate with the hardware and operating system’s services.</li><li>Four <strong>graphics libraries</strong>:<ul><li><strong>GL</strong>: a simpler but slower OpenGL ES API.</li><li><strong>GD</strong>: a faster alternative to GL that generates PICA200 commands.</li><li><strong>GR</strong>: the closest-to-metal PICA200 command API, albeit with the steepest learning curve.</li><li><strong>GX</strong>: the general-purpose library used for PICA200 management.</li></ul></li><li>A 3DS <strong>app packager</strong>, so an executable can be created.</li><li>Further libraries to ease common development tasks, such as implementing network protocols, online gaming and audio/video decoding and processing.</li><li>A plugin for <strong>Visual Studio 2010</strong>, so it can be adopted as the main IDE.</li><li><strong>Assistant tools</strong> for the PICA200.</li><li><strong>Profilers</strong>, for measuring and optimising performance.</li></ul><p>If that wasn’t enough, developers also had access to NintendoWare to download code samples, libraries and further tools designed for Nintendo 3DS development. Furthermore, with the arrival of the New 3DS, game engines like Unity lent their support to this (once ignored) platform <sup id="bibref:83"><a href="#bib:games-unity" role="doc-biblioref">[83]</a></sup>.</p></div></div></div><h3 id="medium">Medium</h3><p>The Nintendo 3DS can run software from three different mediums.</p><div><ul><li id="tab-9-1-gamecards-link"><a href="#tab-9-1-gamecards">Gamecards</a></li><li id="tab-9-2-eshopsd-card-link"><a href="#tab-9-2-eshopsd-card">eShop/SD Card</a></li><li id="tab-9-3-local-wireless-link"><a href="#tab-9-3-local-wireless">Local wireless</a></li></ul><div><div id="tab-9-1-gamecards"><h4 id="tab-9-1-gamecards">Gamecards</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/gamecard.7848bd8ebfd8dcfe983d8f2883ce66afafff32cbd45fd9742c0a8161b400692c.webp"><picture><img alt="Image" width="815" height="571" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hu73e8e42e37f2e81f9775ed553ee0b728_42910_ac83099bc32b8ab4125c6c805b799620.png" loading="lazy"></picture></a><figcaption>Example of a retail game. Notice the creative touch with Luigi holding onto the 3DS banner.</figcaption></figure><p>This is the distribution channel for retail software. Internally called ‘CTR cards’, they’re just another proprietary card/cartridge designed by Nintendo. To be fair, they’re not very different from <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#games">NTR Cards</a> (used by the Nintendo DS), aside from a cosmetic notch at the top right. Inside them, there is read-only and/or writable storage.</p><p>The variants of CTR cards range drastically. Their PCB can bundle a ROM chip ranging from <strong>128 MB</strong> to <strong>4 GB</strong> in size, while also including some ‘backup memory’ to store saves, this can be either <strong>128 KB or 512 KB</strong>. In other variants, the whole CTR storage is instead filled with <strong>Flash</strong> (up to 2 GB), and it’s partitioned to store both the game and saves in the same physical chip.</p><p>Moreover, since games will be bundling the official SDK as well, the usable capacity of ROM/RAM allowed to the game depends on the revision of the SDK linked to.</p><p>Internally, the ROM chip is connected to an 8-bit data bus <sup id="bibref:84"><a href="#bib:games-gamecards" role="doc-biblioref">[84]</a></sup>, while the backup memory relies on a serial bus. Both are connected to a 16.6 MHz clock. When the card is inserted into the console, the 3DS first queries it using NTR (Nintendo DS) commands <sup id="bibref:85"><a href="#bib:games-card_registers" role="doc-biblioref">[85]</a></sup>, and then switches to ‘CTR mode’ once it detects it’s a 3DS card.</p></div><div id="tab-9-2-eshopsd-card"><h4 id="tab-9-2-eshopsd-card">eShop/SD Card</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/store/store_mario.68333a025886ba2f935c53f3da7502b46a56678e36170d0b904ac02ec8320e2c.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/store/store_mario.68333a025886ba2f935c53f3da7502b46a56678e36170d0b904ac02ec8320e2c.png" loading="lazy"></picture></a><figcaption>The Nintendo eShop store for the Nintendo 3DS.</figcaption></figure><p>Expandable storage, an icon of the 8th generation of consoles. The Nintendo 3DS now enjoys installing and launching software from the SD (or microSD) card; and with it, retail cards are no longer the only medium for games. In fact, SD storage is the result of an emerging distribution channel: The Nintendo online store, bundled into every Nintendo 3DS.</p><p>Thanks to the eShop, there were new distribution techniques: users could <strong>pre-order</strong> games and DLCs before the shipping date <sup id="bibref:86"><a href="#bib:games-preorder" role="doc-biblioref">[86]</a></sup>. These would get downloaded ahead of time, but can only be played once the release date arrived.</p><p>When it comes to storing the respective saves, Nintendo allows its downloaded software to request up to <strong>1 MB</strong> of SD card storage. However, this rule is waived if the retail counterpart already requires more space, in which case the system will allocate as much ‘backup memory’ as the respective CTR card already provides.</p></div><div id="tab-9-3-local-wireless"><h4 id="tab-9-3-local-wireless">Local wireless</h4><p><strong>Download Play</strong>, a <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#medium">debuting feature</a> of the Nintendo DS that enabled players to transfer small games between their consoles, has been pushed into the Nintendo 3DS. It now comes with a revamped protocol that relies on a thick security layer, in line with the rest of the software executed in the new console.</p><p>I’d say the big difference now is that transferred games (or ‘demos’, for it’s worth), are now installed into NAND (as any other installable package) <sup id="bibref:87"><a href="#bib:games-download_play" role="doc-biblioref">[87]</a></sup>. There’s only one slot reserved for them, so the installed program is replaced whenever a new game is transferred in.</p></div></div></div><h3 id="virtual-consoles">Virtual console(s)</h3><p>If, after all explained, users still got bored, Nintendo prepared another offering for them: <strong>Virtual Console</strong>.</p><p>Once again, thanks to the bundled eShop app, Nintendo also expanded its 3DS-only catalogue by incorporating games originally published for the following consoles:</p><ul><li>Nintendo DSi (DSiWare only).</li><li>NES/Famicom.</li><li><em>Sega</em> Game Gear.</li><li>Game Boy.</li><li>Game Boy Color.</li></ul><p>Virtual Console games behave as any other application installed into the console. Except for DSiWare software, the application package includes a ROM and emulator. The latter implements interesting capabilities, such as Download Play (for some games) and save states.</p><p>Once again, things were different for New 3DS users, as they could also access the <a href="https://www.copetti.org/writings/consoles/super-nintendo/">Super Nintendo</a> catalogue. This particularly strikes me as odd, as I remember a time when (homebrew) SNES emulators were developed for the original Nintendo DS (with its mere ARM9-ARM7 and a couple of megs of RAM).</p><p>Now, here’s another peculiarity of Virtual console games: this console can also play <a href="https://www.copetti.org/writings/consoles/game-boy-advance/">Game Boy Advance</a> games, officially. Yet, they’re not available for everyone. Only those who purchased a Nintendo 3DS before August 2011 (right before the console received an $80 price cut), became members of the ‘Ambassador Program’ <sup id="bibref:88"><a href="#bib:games-ambassador" role="doc-biblioref">[88]</a></sup>. One of the perks included access to a selection of GBA games which, for one reason or another, were kept exclusive until date.</p><p>Even more puzzling, GBA titles don’t run on the ARM11 using an emulator (albeit there’s one installed, but never been used!). Instead, they kickstart the third firmware, AGB_FIRM, to run natively on top of the ARM7. What makes it puzzling, is that these GBA games, only offered through the Ambassador Program, remained the only purpose of AGB_FIRM, as if Nintendo planned for something bigger in the future, but never materialised. This is another example of how the Nintendo 3DS possessed more hardware than the software ever took advantage of.</p><p>If you’re curious, GBA titles make use of the bundled ARM7 core instead. Thus, they don’t allow for the extra features that emulators (running on ARM11 cores) provide. Although, this happens at the exchange of running at full speed and precision. Be as it may, since the 3DS doesn’t contain a GBA cartridge slot, the GBA game is instead copied into FCRAM before the system reboots into AGB_FIRM, and then lets the ARM7 take control (while the ARM11 and ARM9 provide basic support tasks) <sup id="bibref:89"><a href="#bib:io-misc" role="doc-biblioref">[89]</a></sup>.</p><p>With all these capabilities, and on top of being a portable console, one can’t help but wonder why Nintendo didn’t distribute GBA and Nintendo DS/DSi games on the eShop as well. Most probably a marketing and licensing issue, I sense.</p><h3 id="game-updates">Game updates</h3><p>Another requirement of the 8th generation of consoles, games can now receive patches after the shipping date. That’s right, no more need to quality control the game before selling it!</p><p>Leaving irony aside, game updates are distributed through the eShop as well <sup id="bibref:90"><a href="#bib:games-updates" role="doc-biblioref">[90]</a></sup>, which applies to all types of games (except Download Play). All updates are downloaded onto the SD card and eShop games get their updates applied altogether (along with the game itself).</p><h3 id="network-service">Network service</h3><p>Out with the old (<a href="https://www.copetti.org/writings/consoles/nintendo-ds/#network-service">Nintendo Wi-Fi Connection</a>), in with the new (<a href="https://www.copetti.org/writings/consoles/wiiu/#network-service">Nintendo Network</a>)! The same service offered with the Wii U is also implemented on the Nintendo 3DS, and they’re admirably unified, so I recommend checking the <a href="https://www.copetti.org/writings/consoles/wiiu/#network-service">Wii U article</a> where it’s been explained in detail.</p><hr><h2 id="anti-piracy-and-homebrew">Anti-Piracy and Homebrew</h2><p>The history of hacking this console is a long and interesting sequence of events. At first, interests focused on cracking game card readers (in an attempt to replicate the success of the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#defeat">Nintendo DS</a>) and then shifted towards more sophisticated approaches, only involving the operating system.</p><h3 id="main-targets">Main targets</h3><p>First things first, let’s start by describing the two main targets of this system:</p><ul><li>The <strong>Game/CTR card reader</strong>: This is where physical games are loaded. Historically, the Nintendo DS implemented a <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#security-mechanisms">weak security mechanism</a> that was eventually cracked and subsequently led to an influx of <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#flashcarts">Flashcards</a>.</li><li>The <strong>Operating System</strong>: This area is responsible for verifying the authenticity and integrity of every single program before execution (aside from the Boot ROM). Disabling said mechanisms would grant the execution of Homebrew (unauthorised applications) without any restrictions, in theory.</li></ul><p>They may look like two independent fronts (similar to the <a href="https://www.copetti.org/writings/consoles/xbox-360/#main-targets">Xbox 360</a> and <a href="https://www.copetti.org/writings/consoles/wiiu/#main-targets">Wii U</a>), but in the case of this console, both are intertwined. You’ll see it in a bit.</p><h3 id="the-card-reader-front">The card reader front</h3><p>The card reader is the interface between the CPUs and the Gamecards’ memory chip. Its only job is to simplify the communication with the use of commands.</p><p>Inside the ROM/Flash of CTR Carts, the system will find a block of data in a secured format called <strong>NCSD</strong>, this will be handled by the operating system, who will be in charge of authenticating, validating and decrypting it.</p><p>In conclusion, it seems the OS is solely responsible for the communication with the card reader, so let’s move on to the next front.</p><h3 id="the-operating-system-front">The Operating System front</h3><p>Before we continue, if you’re not familiar with symmetric and asymmetric encryption systems, I recommend reading previous articles of this series. They will also explain why systems like this rely so much on asymmetric encryption systems (such as RSA and ECDSA).</p><div><ul><li id="tab-10-1-dedicated-hardware-link"><a href="#tab-10-1-dedicated-hardware">Dedicated hardware</a></li><li id="tab-10-2-chain-of-trust-link"><a href="#tab-10-2-chain-of-trust">Chain of trust</a></li><li id="tab-10-3-operating-system-functions-link"><a href="#tab-10-3-operating-system-functions">Operating system functions</a></li></ul><div><div id="tab-10-1-dedicated-hardware"><h4 id="tab-10-1-dedicated-hardware">Dedicated hardware</h4><p>You’d be right if you suspected that the ARM11 lacks the powerhouse to protect the whole system. Nintendo knew that too, so they took extra care and bundled extra components to compensate:</p><ul><li>The ARM11 cores implement the <strong>XN flag</strong> and the ARM9 bundles a <a href="https://www.copetti.org/writings/consoles/playstation-portable/#focused-memory-management">Memory Protection Unit</a> (MPU), meaning the CPUs won’t execute code from any location in memory just because the current program tells it to.</li><li>As said before, the ARM9 acts as a <strong>dedicated processor</strong> to handle all security-related tasks while the ARM11 MPCore executes the game. Additionally, the ARM9 is exclusively wired to a few hidden <strong>cryptographic accelerators</strong>:<ul><li>An <strong>AES engine</strong> that performs AES-128 encryption/decryption without consuming (and exposing) CPU resources. This was inherited from the Nintendo DSi, but can now store up to 64 keys and can operate in numerous block cipher modes, including CTR, CCM, CBC and ECB <sup id="bibref:91"><a href="#bib:anti_piracy-aes" role="doc-biblioref">[91]</a></sup>. Each key slot also features its own <strong>key-scrambler</strong>, meaning that two arbitrary keys can be used to generate the final AES key. Moreover, the key-scrambler won’t allow anyone to read the generated key, only to treat it as a blackbox to encrypt/decrypt data.</li><li>An <strong>RSA engine</strong>. By contrast, this performs RSA encryption/decryption using a given RSA public key. This time, it only contains four key slots and there’s no key-scrambler <sup id="bibref:92"><a href="#bib:anti_piracy-crypto" role="doc-biblioref">[92]</a></sup>. However, it’s still a write-only space, meaning no one will be able to read the keys stored there. You’ll soon see that this system is filled with RSA-2048 and RSA-4096 signatures, which explains why this component is as crucial as the AES engine.</li><li>A <strong>Pseudo Random Number Generator</strong> (PRNG): These are registers that return a different value every time they’re read from.</li></ul></li><li><strong>OTP</strong> (one-time programmable) memory that stores console-unique keys, information about the console and <strong>CTCert</strong> (an ECDSA private key to authenticate with Nintendo’s servers). To complicate things further, these keys will be encrypted with an AES-CBC key found in Boot9. Finally, this region includes a flag to disable its access once it’s not needed anymore.</li><li>Last but not least, the eMMC memory contains a register called <strong>CID</strong> (Card Identification) and stores unique information about the eMMC’s manufacturing, which will be fed to the AES’ key-scrambler for further obfuscation.</li></ul><p>To top it off, everything is sealed in an SoC, including the <strong>two boot ROMs</strong> (Boot9 and Boot11). These are unencrypted, but since they’re inaccessible, they don’t represent a concern.</p></div><div id="tab-10-2-chain-of-trust"><h4 id="tab-10-2-chain-of-trust">Chain of trust</h4><p>This should come as no surprise considering we’ve already introduced RSA, AES and the boot ROM as part of the security system. To give you an overview of the Nintendo 3DS’ change of trust:</p><ol><li>ARM9’s boot ROM (Boot9) bundles the public key for decrypting and validating the contents of the NAND. The AES engine will be initialised with the keys stored in Boot9. With this, the contents of OTP memory will be accessed.</li><li>The contents of eMMC are decrypted using Boot9’s AES keys combined with the eMMC CID.</li><li>NAND and CTR cards are formatted using the <strong>NCSD</strong> format <sup id="bibref:93"><a href="#bib:anti_piracy-ncsd" role="doc-biblioref">[93]</a></sup>. NCSD stores a header and a collection of up to eight partitions. The NCSD header contains a signature using RSA-2048 and SHA-258, which is quite strong. To decrypt this signature, the system finds its public RSA key in the boot ROM or ITCM memory (the latter was previously decrypted and copied from OTP). The choice depends on where the NCSD block came from (NAND or CTR card).</li><li>Once the NCSD block is validated, the system accesses each partition. These are structured using the <strong>NCCH</strong> (Nintendo Content Container Header) format. Independently whether the data was pulled from NAND, the CTR card or the SD card, the NCCH block also contains an RSA-2048 + SHA-258 signature <sup id="bibref:94"><a href="#bib:anti_piracy-ncch" role="doc-biblioref">[94]</a></sup>, and its payload is encrypted with AES-128 CTR.</li><li>Furthermore, installed software is catalogued in the form of <strong>Titles</strong> (similar to the <a href="https://www.copetti.org/writings/consoles/wii/#broadways-os">Wii System</a>). In this case, all titles are signed with either RSA-2048, RSA-4096 or ECDSA; plus SHA256 <sup id="bibref:95"><a href="#bib:anti_piracy-titles" role="doc-biblioref">[95]</a></sup>. The public keys are stored in <code>NATIVE_FIRM</code>.<ul><li>It does surprise me that some signatures are in the form of ECDSA, considering there’s no hardware accelerator installed for it.</li></ul></li><li>Once the payload is verified and decrypted, the system will find either an executable, library or asset (i.e.&nbsp;manual, icon or banner) that the ARM11 can read.</li></ol><p>Please note, this explanation focuses on the main 3DS firmware (<code>NATIVE_FIRM</code>). Yet, <code>TWL_FIRM</code> and <code>AGB_FIRM</code> will also have their share of cryptography implemented.</p><p>As time passed by and hackers got the handle on how this console was protected, Nintendo shuffled the chain of trust further to deter the decryption of NCCH data. In some ways, it achieved its purpose, but in others, Nintendo ended up revealing too much. You’ll see it in the following sections.</p></div><div id="tab-10-3-operating-system-functions"><h4 id="tab-10-3-operating-system-functions">Operating system functions</h4><p>Once <code>NATIVE_FIRM</code> is up and running, in addition to the aforementioned chain of trust, the following security mechanisms are present:</p><ul><li>User programs only access hardware functions through system calls, authorised at Kernel11’s discretion. Depending on the hardware, it will also involve Kernel9.</li><li>From an architecture perspective, ARM11 user programs are completely unaware of the ARM9 and its neighbouring components.</li><li>User applications are <strong>sandboxed</strong>, meaning they can’t access each other’s space.</li><li>Last but not least, with the increase in online services, users will require a legitimate game card and an updated firmware to access the new functions. This will deter users who may consider keeping their console on a vulnerable firmware.</li><li>Software downloaded from the eShop also comes with its quirks. In this scenario, the license of a Title is encoded in the form of a <strong>Ticket</strong> which, again, is signed with RSA-2048 and SHA-256 <sup id="bibref:96"><a href="#bib:anti_piracy-cdn" role="doc-biblioref">[96]</a></sup>. Tickets are either linked to a single console ID and the eShop’s user account; or made global for any console. Furthermore, Nintendo uses additional RSA certificates in the downloaded Title’s metadata to further enlarge the chain of trust <sup id="bibref:97"><a href="#bib:anti_piracy-tmd" role="doc-biblioref">[97]</a></sup>.</li></ul></div></div></div><h4 id="flaws">Flaws</h4><p>Even though the Nintendo 3DS enjoyed modern protection techniques, such as asymmetric cryptography and lots of hardware at its disposal, there were some fundamental flaws in its implementation. Take a look at the following findings discovered by the hacking community:</p><ul><li>While the XN flag in the ARM11 works without problems, Kernel11 sets up the page table in AXI WRAM (where Kernel11 resides) in a way that it grants Read, Write and Execute permissions to the whole memory block <sup id="bibref:98"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[98]</a></sup>, rendering the capabilities of XN a bit useless (at least for protecting Kernel11).</li><li>Before system version <code>3.0.0</code>, OTP memory was never hidden <sup id="bibref:99"><a href="#bib:operating_system-otp" role="doc-biblioref">[99]</a></sup>, meaning that with the help of any exploit, the OTP keys could be extracted without problem.</li><li>There’s no separation between Process9 and Kernel9, as Kernel9 provides a system call that allows Process9 to perform any function with Kernel9 privileges <sup id="bibref:100"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[100]</a></sup>.</li><li>There’s no <strong>ASLR</strong> (Address space layout randomization) implemented <sup id="bibref:101"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[101]</a></sup>, enabling Return-oriented programming (ROP) for exploitation purposes.</li><li>Similarly, there’s no protection against system <strong>downgrading</strong>.</li><li>Once again, this system also comes with a <strong>Web browser based on Webkit</strong>, which is under constant attack (especially if the fork is old).</li></ul><p>This will not only pave the way to the first exploitation attempts, but will also act as a constraint for Nintendo when they try to patch their system.</p><h3 id="defeat">Defeat</h3><p>The history of the Nintendo 3DS and Homebrew is a successful one. Tons of video tutorials can attest to that. Yet, the passage exposes very clever discoveries, which evolved from initially requiring proprietary and expensive equipment to just a couple of clicks on your computer.</p><h4 id="the-ds-flashcard-era-2011-2013">The DS flashcard era (2011-2013)</h4><p>Where to begin? Well, from where the Nintendo DSi left it off: <strong>Flashcards</strong>.</p><p>After the release of the Nintendo DSi in 2008, Nintendo incorporated a new element to fight against Flashcards: A <strong>whitelist file</strong> listing every single licensed card and thereby blocking the ‘unauthorised ones’ <sup id="bibref:102"><a href="#bib:anti_piracy-card_whitelist" role="doc-biblioref">[102]</a></sup>. By no means Flashcard manufacturers ceased their production, they just shipped new variants of their old Flashcards that allowed the user to re-program the cartridge header, enabling the card to identify as a different authorised game whilst Nintendo kept amending the list (through software updates).</p><p>This method encompassed the Nintendo 3DS as well, following the same process as the Nintendo DSi. On no account they would get access to the exclusive 3DS hardware, yet this is how Homebrew started in this console.</p><h4 id="the-3ds-flashcard-era-2013-2016">The 3DS flashcard era (2013-2016)</h4><p>There was much progress during the first two years of this console (a big achievement for Nintendo!). Yet, things took a turn in August 2013…</p><div><ul><li id="tab-11-1-the-first-real-3ds-flashcard-link"><a href="#tab-11-1-the-first-real-3ds-flashcard">The first real 3DS Flashcard</a></li><li id="tab-11-2-inside-the-gateway3ds-link"><a href="#tab-11-2-inside-the-gateway3ds">Inside the Gateway3DS</a></li><li id="tab-11-3-subsequent-anecdotes-link"><a href="#tab-11-3-subsequent-anecdotes">Subsequent anecdotes</a></li></ul><div><div id="tab-11-1-the-first-real-3ds-flashcard"><h5 id="tab-11-1-the-first-real-3ds-flashcard">The first real 3DS Flashcard</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/gateway3ds.628421a0ac282a6b29e0f974bc8c45a02539e71d5fa3e6bfe2ca0b3a54c003de.jpg"><picture><img alt="Image" width="650" height="650" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/gateway3ds.628421a0ac282a6b29e0f974bc8c45a02539e71d5fa3e6bfe2ca0b3a54c003de.jpg" loading="lazy"></picture></a><figcaption>The Gateway3DS package <sup id="bibref:103"><a href="#bib:anti_piracy-gateway_review" role="doc-biblioref">[103]</a></sup>.</figcaption></figure><p>Ignoring teasers of ‘3DS Flascards’ that never appeared <sup id="bibref:104"><a href="#bib:anti_piracy-crown3ds" role="doc-biblioref">[104]</a></sup>. <strong>Gateway3DS</strong> can be considered the first 3DS Flashcard to reach the stores. The instructions were not as simple as DS Flashcards, however. You can sense this by looking at the contents of the box:</p><ul><li>A whitelisted <strong>DS Flashcard</strong> (known as <em>Blue Gateway</em>) whose only purpose is to run a Nintendo DS ROM crafted by Gateway. As part of the ‘installation’ process, users were first required to run this ‘game’ and follow the instructions.</li><li>A <strong>Launcher.dat</strong> to be placed in the 3DS’ SD card.</li><li>A <strong>3DS Flashcard</strong> (known as <em>Red Gateway</em>) where the 3DS game is loaded from. Like any other Flashcard, it also features a microSD slot where the 3DS game is stored. The big difference, however, is that the 3DS game image is flashed into the microSD card, meaning that only one 3DS game can be stored at a time.<ul><li>This makes sense, as RSA signatures can’t be faked (at least, that’s computationally unfeasible). Yet, replicating an exact clone of the game (NCSD block) worked. Forget about Homebrew, for now.</li></ul></li></ul><p>After completing the installation process, users would have to follow these instructions to run any game:</p><ol><li>Insert the <em>Red</em> Gateway card. Nothing will appear, yet.</li><li>Open the 3DS settings app and navigate to the DS profile editor screen.</li><li>For some reason, the 3DS will restart and the flashed 3DS game will show up.</li><li>After finishing playing a game, returning to the HOME Menu will create a savefile in the 3DS’ SD card.</li></ol><p>And just like that, users were now able to download 3DS ROMs from the net and run them on their consoles… but how was all of this possible? How did anyone manage to extract decrypted games? What exploits did Gateway3DS employ (or even discover)?</p><p>Truth is, there’s a lot of hidden functionality within this product. Let’s analyse it step by step.</p></div><div id="tab-11-2-inside-the-gateway3ds"><h5 id="tab-11-2-inside-the-gateway3ds">Inside the Gateway3DS</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/shell/settings_ds_profile.fd6a65c58c189a525f1d8e65f40cbbc48687a3d05127ff4bf17975477cf7bae2.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/shell/settings_ds_profile.fd6a65c58c189a525f1d8e65f40cbbc48687a3d05127ff4bf17975477cf7bae2.png" loading="lazy"></picture></a><figcaption>The DS Message editor found on the 3DS settings app. The character limit rule depends solely on the graphical interface.</figcaption></figure><p>Sometime in 2012, hacker ‘ichfly’ discovered interesting behaviour in the Nintendo DS’ old profile editor, found on both <code>NATIVE_FIRM</code> and <code>TWL_FIRM</code>. In one of its text fields, you can enter a ‘Message’ value, which will then be displayed as a greeting on PictoChat rooms <sup id="bibref:105"><a href="#bib:anti_piracy-profile" role="doc-biblioref">[105]</a></sup>. The 3DS’ settings app won’t allow you to enter more characters than allowed. Yet, nothing prevents a Nintendo DS game from doing so. When that happens, opening the 3DS’ System Settings app (called <strong>MSET</strong>) will crash, and what makes it interesting is that this is caused by <strong>stack overflow</strong> <sup id="bibref:106"><a href="#bib:anti_piracy-waffle" role="doc-biblioref">[106]</a></sup>. Does this remind you of <a href="https://www.copetti.org/writings/consoles/wii/#the-dawn-of-homebrew">a certain horse name</a>?</p><p>Now, the mysterious Launcher.dat by Gateway is a configuration file that the Settings app normally reads. What happened is that Gateway crafted their own Launcher.dat to embed data used for the next stages of their exploit. Curiously enough, Launcher.dat is stored in NAND (not in the SD), so the initial exploit chain also alters where the Settings app loads this from.</p><p>If you combine this with a Process9/Kernel9 exploit, you get full execution privileges on this console and can start fiddling with system services. Some hardware like OTP and the boot ROMs will still be out of reach. Yet, this is a significant milestone.</p><p>So far so good? Let’s now connect this information with Gateway’s package:</p><ul><li>The DS/Blue flashcard is just an entry point to install the corrupted DS profile (which will trigger the MSET exploit).</li><li>The 3DS/Red flashcard houses a ProASIC3 FPGA programmed with a firmware (distributed by Gateway). The FPGA and the microSD card are combined to replicate a retail game.</li><li>Launcher.dat is the payload of the MSET exploit. It bundles a Kernel exploit and a collection of system patches. In other words, a <strong>Custom Firmware</strong> (CFW). A console running Gateway’s CFW can extract games or load a 3DS game using the red flashcard. Surprisingly, Gateway also crafted their CFW so it requires the red flashcard inserted to work (<em>a DRM mechanism in a Flashcard, have the tables turned?</em>).</li></ul></div><div id="tab-11-3-subsequent-anecdotes"><h5 id="tab-11-3-subsequent-anecdotes">Subsequent anecdotes</h5><p>All seemed jolly for Gateway until November 2013, when a stream of clones of their card landed. ‘R4i Gold 3DS Deluxe’ came for some healthy competition, albeit by using some of Gateway’s firmware code. In retaliation, Gateway3DS took drastic measures: Subsequent firmware updates of Gateway3DS corrupted the 3DS NAND if a clone was detected. <em>The irony!</em></p><p>In October 2013, hacker ‘Smealum’ published a video showing his own MSET-based implementation that instead booted a copy of <code>NATIVE_FIRM</code> stored in the 3DS’ SD <sup id="bibref:107"><a href="#bib:anti_piracy-rednand" role="doc-biblioref">[107]</a></sup>. This meant that consoles stuck on system <code>4.5.0</code> could boot newer system versions without losing the ability to run exploits. Smealum called this function <strong>redNAND</strong> (from ‘redirected NAND’) and, while it wasn’t publicly released, Gateway later incorporated this functionality (now referred to as <strong>emuNAND</strong>) with their CFW released in December 2013 <sup id="bibref:108"><a href="#bib:anti_piracy-gateway3ds" role="doc-biblioref">[108]</a></sup>. This became a strong selling point for Gateway3DS.</p><p>It’s not known what Process9/Kernel9 exploit Gateway employed. Yet, in December 2013, Fierce_Waffle, Xerpi and Megazig reversed engineered and open-sourced Gateway’s payload in the form of a tool called ‘3DS Toolkit’ <sup id="bibref:109"><a href="#bib:anti_piracy-waffle" role="doc-biblioref">[109]</a></sup> <sup id="bibref:110"><a href="#bib:anti_piracy-ramdump" role="doc-biblioref">[110]</a></sup>.</p><p>In the following years, a second generation of 3DS flashcards will appear in the market. Examples include <strong>Stargate</strong>, <strong>Sky3DS</strong> and dozens of clones. This time, they didn’t rely on an operating system exploit to work and could load multiple games from their microSD. However, their utility will be entirely based on replicating retail 3DS games (including their signatures), in other words, for solely piracy purposes.</p></div></div></div><h4 id="nintendo-acts-fast">Nintendo acts fast</h4><p>Having an updatable system software meant Nintendo didn’t have to stand there and watch how its system got cracked:</p><ul><li>In March 2013, system update <code>5.0.0-11</code> updated the settings app, provisionally fixing the MSET exploit <sup id="bibref:111"><a href="#bib:anti_piracy-neko" role="doc-biblioref">[111]</a></sup>. If you check the timeline, this was before Gateway3DS shipped their card! Hence, it was a prerequisite for users to stay on older versions.<ul><li>It won’t be until 2015 when the Gateway team released a notable firmware update. From then on, the flashcard relied on a new Web Browser exploit (called <strong>spider exploit</strong>, discovered by MathewE) as the entry point. This method lasted until the end of Gateway3DS’ lifespan.</li></ul></li><li>In December 2013, system update <code>7.0.0-13</code> fixed the kernel exploits used in combination with MSET and, most importantly, added the RSA module into the chain of trust to decrypt NCCH blocks (where the game data is found) <sup id="bibref:112"><a href="#bib:anti_piracy-70013" role="doc-biblioref">[112]</a></sup>. RSA keys are cleared once Kernel9 finishes loading, meaning existing exploits won’t be able to decrypt games that adopted the new <code>7.0.0</code> encryption system (unless a vulnerability is used before Kernel9 boots).</li><li>As Gateway3DS’ Launcher.dat file contained copyrighted code by Nintendo, the latter company sent Cease &amp; Desist letters to many forums, including GBATemp, which in turn blocked the distribution of those files.</li></ul><p>As always, this marked the start of another cat-and-mouse game. Though, to make a long story short, system update <code>9.3.0</code> (released in December 2014) finally put an end to Gateway3DS by patching their private Kernel exploit <sup id="bibref:113"><a href="#bib:anti_piracy-gateway3ds" role="doc-biblioref">[113]</a></sup>. Since then, Gateway3DS’ firmware updates only improved emuNAND support with the latest system versions (for those who didn’t update past the breaking update). In 2016, Gateway’s last update was released. Meanwhile, Sky3DS enjoyed support until system software <code>11.0</code> (released in May 2016) <sup id="bibref:114"><a href="#bib:anti_piracy-sky3ds" role="doc-biblioref">[114]</a></sup>, when Nintendo blacklisted it for good.</p><p>I think now it’s fair to say that the 3DS flashcard market ended up being too turbulent and unreliable for the average user, compare this to the ‘plug &amp; play’ experience Nintendo DS flashcard offered. Finally some good news for Nintendo, so far.</p><h4 id="the-dawn-of-homebrew-2014">The dawn of homebrew (2014)</h4><p>2014 saw an emergence of homebrew-focused solutions in a circle populated by piracy-oriented developments <sup id="bibref:115"><a href="#bib:anti_piracy-proto_homebrew" role="doc-biblioref">[115]</a></sup>. Hacking a 3DS still required an old system version, a Gateway3DS card and emuNAND - but that would slowly shift once alternative tools gained traction.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/32c3.603e3f980c96df85d410f37756afa2355a73eecf8413059c6e11241ac60e4777.jpeg"><picture><img alt="Image" width="854" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/32c3.603e3f980c96df85d410f37756afa2355a73eecf8413059c6e11241ac60e4777.jpeg" loading="lazy"></picture></a><figcaption>Plutoo, Derrek and Smealum presenting their findings at the 32nd Chaos Communication Congress (2015) <sup id="bibref:116"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[116]</a></sup>, the following paragraphs will explain most of them.</figcaption></figure><div><ul><li id="tab-12-1-open-source-sdks-link"><a href="#tab-12-1-open-source-sdks">Open-source SDKs</a></li><li id="tab-12-2-ninjhax-chain-link"><a href="#tab-12-2-ninjhax-chain">Ninjhax chain</a></li><li id="tab-12-3-gaining-kernel11-access-link"><a href="#tab-12-3-gaining-kernel11-access">Gaining Kernel11 access</a></li></ul><div><div id="tab-12-1-open-source-sdks"><h5 id="tab-12-1-open-source-sdks">Open-source SDKs</h5><p>Initial Homebrew appeared in the form of Laucher.dat files, these were produced with the help of devkitARM (a general-purpose toolchain for ARM-based CPUs) and a set of scripts. Fierce Waffle provided ‘ROP Loader’, a toolkit that included a DS program to install the MSET exploit; and a Launcher.dat that triggered a Kernel11 exploit. It’s worth pointing out that there wasn’t any tool available, yet, that helped access the 3DS’ exclusive hardware.</p><p>At the start of 2014, Smealum, with the collaboration of yellows8, ichfly, WinterMute, fincs, mtheall and plutoo, released <strong>ctrulib</strong>, an open-source C library to facilitate Homebrew development <sup id="bibref:117"><a href="#bib:anti_piracy-libctru" role="doc-biblioref">[117]</a></sup>. This is now known as <strong>libctru</strong> and maintained by the devkitPro group, who have incorporated it into their toolchain.</p><p>A year later, neobrain released <strong>nihstro</strong> <sup id="bibref:118"><a href="#bib:anti_piracy-nihstro" role="doc-biblioref">[118]</a></sup>, a PICA200 shader assembler a disassembler, making the job of programming the PICA200 a bit more enjoyable.</p><p>To run Homebrew, users had the option to flash a homebrew binary into a microSD, and then use the Gateway3DS to boot it (as their CFW already disabled signature checks) <sup id="bibref:119"><a href="#bib:anti_piracy-gateway_homebrew" role="doc-biblioref">[119]</a></sup>.</p></div><div id="tab-12-2-ninjhax-chain"><h5 id="tab-12-2-ninjhax-chain">Ninjhax chain</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/launcher.a5ef019f4fd8b8aa7dc8af5beea6e53aba3af518a507b6a1f7f174ce4d627235.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/launcher.a5ef019f4fd8b8aa7dc8af5beea6e53aba3af518a507b6a1f7f174ce4d627235.png" loading="lazy"></picture></a><figcaption>The Homebrew Launcher, inspired by the <a href="https://www.copetti.org/writings/consoles/wii/#a-permanent-state">iconic Wii counterpart</a>. Its arrival marked the sophistication of 3DS Homebrew.</figcaption></figure><p>The poisoned updates of Gateway left a bitter mark on their users. The time had come to look for nonproprietary alternatives.</p><p>Thankfully, people were working on this. During the second half of 2014, a new milestone awaited for the Homebrew community: Smealum published <strong>Ninjhax</strong>, a package composed of the following components <sup id="bibref:120"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[120]</a></sup>:</p><ol><li>A crafted <strong>QR code</strong> to be scanned by ‘Cubic Ninja’, a game that allows to share user-designed levels using QR codes. This served as a new entry point exploit.</li><li><strong>GSPWN</strong>: A userland vulnerability where the GPU’s DMA is used to write over the HOME Menu’s heap. Furthermore, the combination with ROP leads to privilege escalation. This resulted in the ability to create &amp; kill processes, SD card access, decrypt &amp; dump titles and override executable data.<ul><li><a href="https://www.copetti.org/writings/consoles/xbox-360/#graphics">Other GPUs</a> were also known for <a href="https://www.copetti.org/writings/consoles/xbox-360/#tab-20-3-king-kong-exploit">intruding</a> into the system’s RAM.</li></ul></li><li><strong>Homebrew launcher</strong>: A new service running under the HOME Menu process thanks to GSPWN. It provides a graphical user interface to load unsigned Homebrew apps (using a new portable .3dsx format) and take over processes. The launcher loads homebrew by opening an official application with enough privileges and then hijacks it with GSPWN, replaces the code with Homebrew code and finally executes it.<ul><li>With its ability to alter user data, the Homebrew launcher can also be used to install alternative entry points as they’re discovered (i.e.&nbsp;OotHax, Ironhax and so forth). Thus, reducing its dependency on Cubic Ninja. A notable aftermarket exploit was <strong>MenuHax</strong>, which exploited a vulnerability in the HOME Menu theme engine and was triggered at boot, <strong>making it a permanent solution to launch a payload</strong>.</li><li>If you are curious, the Wii U also experienced <a href="https://www.copetti.org/writings/consoles/wiiu/#fooling-iosu">similar methodologies</a> as early attempts to run Homebrew.</li></ul></li></ol><p>Notice how Gateway3DS is, for once, out of the equation. Be as it may, the Homebrew Launcher was still under the scope of userland (meaning homebrew apps could only access 64 MB of RAM and had no access to the audio DSP <sup id="bibref:121"><a href="#bib:anti_piracy-hbl_limitations" role="doc-biblioref">[121]</a></sup>).</p></div><div id="tab-12-3-gaining-kernel11-access"><h5 id="tab-12-3-gaining-kernel11-access">Gaining Kernel11 access</h5><p>Turns out that before the publication of Ninjhax, in February 2014, yellows8 made a very important discovery: An exploit leading to Kernel11 privileges.</p><p>Kernel11 keeps track of the unused memory pages in FCRAM using a structure called <code>memchunk header</code>. This data is stored as a linked list, where each header contains the address of the previous and next header. Well, it so happens <code>memchunk headers</code> <strong>are stored in FCRAM</strong>, which may be overwritten thanks to other exploits like GSPWN. Consequently, memchunk headers can be modified to grant userland access to AXI WRAM. In doing so, the attacker can eventually modify the Kernel11’s page table to grant all FCRAM access to user-space, leading to arbitrary control of Kernel11. This discovery was called <strong>memchunkhax</strong>.</p><p>Nevertheless, Nintendo patched it in December 2014 <sup id="bibref:122"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[122]</a></sup>. However, another hacker by the name of derrek found a race condition where the ‘next’ pointer of a <code>memchunk header</code> may be replaced with the location of a crafted one. So, when Kernel11 tries to access the crafted <code>memchunk header</code>, it will end up executing arbitrary code with Kernel11 privileges. Ipso facto, <strong>memchunkhax2</strong> came into existence.</p><p>Thanks to the new privilege escalation, Homebrew software gained complete control of the system up to the ARM9 area… but why stop there?</p></div></div></div><h4 id="most-wanted-tools">Most-wanted tools</h4><p>Considering the availability of Gateway3DS’ emuNAND, Ninjhax, CTRLib and the new Kernel exploits, the flood of new software was too great to ignore. To mention a few:</p><ul><li><strong>CtrBootManager</strong> by cpasjuste: An extra stage in HomeMenuHax’s chain that acts as a boot manager, enabling the selection of various payloads <sup id="bibref:123"><a href="#bib:anti_piracy-ctrbootmanager" role="doc-biblioref">[123]</a></sup>.<ul><li>Shortly after, a new implementation with extended functionality emerged: <strong>BootCtr</strong> by m45t3r <sup id="bibref:124"><a href="#bib:anti_piracy-bootctr" role="doc-biblioref">[124]</a></sup>.</li></ul></li><li><strong>RxTools</strong> by Roxas75: A Swiss knife for Gateway3DS users <sup id="bibref:125"><a href="#bib:anti_piracy-rxtools" role="doc-biblioref">[125]</a></sup>. This was offered as a replacement for Gateway3DS’ binaries. Among many things, it includes a CFW called <strong>RXMode</strong>. This alternative and open-source solution disables signature checks on 3DS binaries, provides emuNAND and removes <code>TWL_FIRM</code>’s whitelist checks, to mention a few.<ul><li>Other CFWs will soon make their appearance, like CakesFW, ReiNand and Pasta CFW <sup id="bibref:126"><a href="#bib:anti_piracy-cfw_old_list" role="doc-biblioref">[126]</a></sup>. These serve different purposes and include their own set of modifications.</li></ul></li><li><strong>Custom HomeMenu Manager</strong> (CHMM) by Rinnegatamante: Allows to install HOME Menu themes from the SD card <sup id="bibref:127"><a href="#bib:anti_piracy-chmm" role="doc-biblioref">[127]</a></sup>.</li><li><strong>AGB_FIRM Signature patcher</strong> by Riku. Loads arbitrary Game Boy Advance ROMs into AGB_FIRM <sup id="bibref:128"><a href="#bib:anti_piracy-agb_converter" role="doc-biblioref">[128]</a></sup>, finally expanding the abandoned catalogue of Nintendo Ambassador games.</li><li><strong>Ftpbrony</strong> by mtheall (later known as <strong>ftpd</strong>): A simple FTP server <sup id="bibref:129"><a href="#bib:anti_piracy-ftpbrony" role="doc-biblioref">[129]</a></sup>.</li><li><strong>DevMenu</strong>: Not exactly a homebrew app, but a <em>stolen</em> Nintendo-authored app from development units, enabling users to install app packages (in the form of ‘CIA’ files) into the system, just like the eShop did behind the scenes.<ul><li>Months later, <strong>BigBlueMenu</strong> was used instead, which also came from Nintendo’s development kit.</li><li>It wasn’t until a real open-source solution was brought forward some months after. <strong>FBI</strong> by Steveice10 became the standard dilemma-free tool for installing CIA files (notice the pun in the names) <sup id="bibref:130"><a href="#bib:anti_piracy-fbi" role="doc-biblioref">[130]</a></sup>.</li></ul></li></ul><h4 id="new-console-permanent-mods-2015">New console, permanent mods (2015)</h4><p>While homebrew developers were busy fiddling with their system, Nintendo released a <em>new</em> product to the surprise of everyone: The <strong>New 3DS</strong>.</p><p>Apart from the extra hardware (already mentioned throughout this article), a new stage was added to the boot process: <strong>arm9loader</strong>. With this, Nintendo enhanced their chain of trust by adding new keys, which must be decrypted with the help of a hash of OTP memory (therefore, using console-unique values) <sup id="bibref:131"><a href="#bib:anti_piracy-arm9loader" role="doc-biblioref">[131]</a></sup>. However, arm9loader and the new keys are still stored in NAND, meaning that the contents may be overwritten. This led to one of the most disrupting vulnerabilities of 2015, involving Plutoo, Yellows8 and Delebile.</p><h5 id="arm9loaderhax">arm9loaderhax</h5><p>The first implementation of arm9loader was flawed: the decryption key for the ARM9 system was never removed from the AES engine. So, with the help of additional exploitation, one could reconstruct part of the encryption keys <sup id="bibref:132"><a href="#bib:anti_piracy-arm9loaderhax" role="doc-biblioref">[132]</a></sup>. Consequently, Nintendo quickly tried again with <code>arm9loader v1.1</code> (found on system update <code>9.6.0</code>). As luck would have it, this led to a more powerful exploit: Plutoo discovered that the key used to decrypt the ARM9 system was never verified. Hence, arm9Loader will boot <code>NATIVE_FIRM</code> even if the decrypted data is wrong (a.k.a. garbage). Plus, if Firm0 (the first copy of <code>NATIVE_FIRM</code>) fails to boot, Boot9 will try to load Firm1 while the remains of Firm0 stay in the ARM9’s RAM.</p><p>All in all, if:</p><ul><li>NAND is modified (somehow) so the encrypted Firm0 contains extra crafted code at the end.</li><li>The ARM9 OS key is mangled in a way that the decrypted Firm1 will contain a jump instruction to Firm0’s crafted code.</li></ul><p>… you got yourself <strong>arm9loaderhax</strong>, a permanent exploit that provides <strong>arbitrary code execution</strong> with <strong>Kernel9 privileges</strong> at <strong>boot time</strong>!</p><p>Since Kernel9 access was now possible, albeit through difficult means, work was put into simplifying the process (i.e.&nbsp;developing an automated installer).</p><h5 id="the-effects-of-arm9loaderhax">The effects of arm9loaderhax</h5><p>New discoveries meant new developments. Over the following months, more advanced tools will become part of the ‘must have’ list of every homebrew user.</p><p>To start with, a new CFW to-rule-them-all shipped: <strong>Luma3DS</strong> <sup id="bibref:133"><a href="#bib:anti_piracy-luma3ds" role="doc-biblioref">[133]</a></sup>. Among many features, Luma3DS provides:</p><ul><li>The removal of signature and region checks.</li><li>A layered filesystem to redirect file operations to the SD card (enabling game modifications).</li><li>Rosalina Menu, an in-game menu overlay where many utilities can be accessed without closing any application.</li></ul><p>Initially, Luma3DS was bootstrapped with BootCtr, but that changed once arm9loaderhax became the de-facto hack for any 3DS. Thus, the arm9loaderhax + Luma3DS combination became part of any hacking tutorial.</p><p>Along it, other software appeared:</p><ul><li><strong>Godmode9</strong> by d0k3: A next-generation Swiss knife that takes advantage of the permissions granted by ARM9 exploits <sup id="bibref:134"><a href="#bib:anti_piracy-godmode" role="doc-biblioref">[134]</a></sup>, enabling the user to read and modify every corner of the console. It can be loaded by arm9loaderhax, Luma3DS or any other compatible hack. Now, the more powerful the exploit, the more functionality is provided. Examples of functionality include a file browser and NAND backup. Plus it’s further extended with scripts.</li><li><strong>Anemone3DS</strong> by astronautlevel: With a multitude of features, it soon became <em>the app</em> for managing HOME Menu themes <sup id="bibref:135"><a href="#bib:anti_piracy-anemone3ds" role="doc-biblioref">[135]</a></sup>.</li><li><strong>nds-bootstrap</strong> by Rocket Robz: As the name indicates, it loads Nintendo DS software (ROMs and homebrew) from the SD card <sup id="bibref:136"><a href="#bib:anti_piracy-nds_bootstrap" role="doc-biblioref">[136]</a></sup>. While it’s designed to support the three portable consoles (the Nintendo 3DS, DSi and DS, the latter requiring a flashcard), loading it from the 3DS will kickstart <code>TWL_FIRM</code>, meaning there’s no emulation at all. It’s most commonly used through ‘TWLMenu’ (now ‘TWiLight Menu++’), the front-end of nds-bootstrap.</li></ul><p>It’s worth mentioning that, at the time of this writing, these are the most popular utilities to install on a hacked 3DS.</p><h4 id="the-golden-age-2016-2017">The Golden Age (2016-2017)</h4><p>While a universal and powerful solution, installing arm9loaderhax was still considered a complicated and dangerous activity. Not only does this require dumping the console’s OTP memory beforehand (using other exploits), but neglecting any step could potentially turn a working Nintendo 3DS into a rock.</p><p>But fear not as new developments were in the works (a mighty effort considering Nintendo was still battling to protect their console).</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/33c3.437cce691de5706d0c70eab1e6a3bb0427a28c33f4285a3c0bbd5c8083b49c5b.jpeg"><picture><img alt="Image" width="1280" height="720" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/33c3.437cce691de5706d0c70eab1e6a3bb0427a28c33f4285a3c0bbd5c8083b49c5b.jpeg" loading="lazy"></picture></a><figcaption>naehrwert, nedwill and derrek presenting a new set of findings at the 33nd Chaos Communication Congress (December 2016) <sup id="bibref:137"><a href="#bib:anti_piracy-33c3" role="doc-biblioref">[137]</a></sup>.</figcaption></figure><p>At the 33C3 conference, derrek unveiled two major discoveries <sup id="bibref:138"><a href="#bib:anti_piracy-33c3" role="doc-biblioref">[138]</a></sup>, which led to subsequent milestones.</p><div><ul><li id="tab-13-2-sighax-link"><a href="#tab-13-2-sighax">Sighax</a></li><li id="tab-13-3-boot9strap-link"><a href="#tab-13-3-boot9strap">Boot9strap</a></li><li id="tab-13-4-ntrboot-link"><a href="#tab-13-4-ntrboot">Ntrboot</a></li></ul><div><div id="tab-13-1-extracting-boot9"><p>For some reason, the contents of ARM9’s RAM are not cleared upon reset. Thus, Derrek discovered that, with the use of external hardware, he could override the exception vectors from ARM9’s RAM (previously copied from Boot9) with arbitrary code. Then, reset the system, glitch it at very precise timing (also using external hardware) to trigger an exception and hope for the ARM9 to have executed the new code. This will have included something like ‘Copy all contents of Boot9 to X location in RAM’.</p><p>Lo and behold, this did work. With this, Derrek and others managed to analyse the contents of the Boot9 ROM, allowing new vulnerabilities to be found.</p></div><div id="tab-13-2-sighax"><h5 id="tab-13-2-sighax">Sighax</h5><p>One vulnerability from Boot9 was <strong>Sighax</strong> <sup id="bibref:139"><a href="#bib:anti_piracy-sighax" role="doc-biblioref">[139]</a></sup>, a flaw in Boot9’s RSA-2048 signature verification. RSA signatures of type ‘PKCS #1 v1.5’ (adopted by this system) contain an area called <strong>padding</strong> to prevent being reversed. Additionally, they store an SHA-256 hash encoded with a model called ‘ASN.1’, this <strong>guarantees the authenticity of the data</strong> being decrypted.</p><p>Now, the respective parser found in Boot9 <strong>lacks several protections</strong>, including bounds checking. In the end, this allowed Derrek to produce a crafted RSA signature (through brute-forcing) that will always succeed on any data. In doing so, the <strong>entirety of the chain of trust was nullified</strong>.</p><p>For the curious, I recommend reading a comprehensive post in GBATemp describing the theory more calmly <sup id="bibref:140"><a href="#bib:anti_piracy-mrjason" role="doc-biblioref">[140]</a></sup>.</p><p>With this, one would now be allowed to craft a firmware for the ARM9 core, sign it with a crafted RSA signature, install it on NAND and Boot9 will ‘just run it’. The question now is, how can the average user do this using an unmodified console?</p></div><div id="tab-13-3-boot9strap"><h5 id="tab-13-3-boot9strap">Boot9strap</h5><p>The year is 2017. Most know about the existence of Sighax but only a handful can apply it, all because the new method requires a crafted RSA signature and writing access to the NAND, none of which is easy to come by (and let’s not forget Nintendo was still clamping down hard on user-land exploits through system updates). Luckily, Sighax was in the process of being democratised.</p><p>Even though Derrek’s announcement didn’t include a suitable RSA signature or a copy of Boot9 (due to copyright reasons, I’m guessing), that didn’t stop hackers SciresM and Myria from finding alternative resources that would enable them to craft an RSA signature.</p><p>In summary, they discovered that system versions before <code>1.0.0</code> shared similar flaws to those previously exposed with Sighax <sup id="bibref:141"><a href="#bib:anti_piracy-sighax_pres" role="doc-biblioref">[141]</a></sup> and, thanks to this, they were able to begin brute-forcing RSA signatures. The result was a success, a match was eventually found with the help of plenty of Nvidia GPUs <sup id="bibref:142"><a href="#bib:anti_piracy-sighax_math" role="doc-biblioref">[142]</a></sup>.</p><p>Now that they could craft an alternative firmware that Boot9 would accept, they needed to find a way to redirect Boot9 to their payload. The challenge was to redirect execution before Boot9 hides its Boot ROM. To tackle this, the duo found a route through the ARM9’s exception handlers. The ARM9 can’t override these, but the NDMA can - and the CPU can command the NDMA to do so.</p><p>All in all, the team were able to use the NDMA to fill the exception handlers with a jump to arbitrary code, and then instruct the ARM9 to copy to <code>NULL</code>, resulting in an exception that would execute the payload with unrestricted access. In the end, this was packaged in a solution called <strong>boot9strap</strong> and served as an alternative bootloader that could either load a payload from the SD card or continue to boot normally. Consequently, Godmode9 was extended to backup OTP and the Boot ROMs, if needed.</p><p>And so, boot9strap quickly displaced arm9loaderhax as the de facto solution for loading arbitrary code with maximum privileges.</p></div><div id="tab-13-4-ntrboot"><h5 id="tab-13-4-ntrboot">Ntrboot</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/ntrboot_flashcard.70f62f8dd94963d654415db4d0f2a190b5a1c15eb60acc6ba713c26bb295d849.jpg"><picture><img alt="Image" width="947" height="900" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/ntrboot_flashcard.70f62f8dd94963d654415db4d0f2a190b5a1c15eb60acc6ba713c26bb295d849.jpg" loading="lazy"></picture></a><figcaption>Some DS Flashcards sold after the discovery of ntrboot came with a switch to enable a ‘3DS mode’ (see the top corner of the photo), this enables trigger ntrboot.</figcaption></figure><p>At this point, there was only one question left: How could users install boot9strap?</p><p>Well, the team didn’t stop there. By taking a look at their recent Boot ROM dumps, they found an interesting routine: During boot, Boot9 will query if a specific <strong>key combination is pressed</strong> and the <strong>lid is closed</strong>. If so, Boot9 will redirect execution to the inserted Nintendo DS card (with full privileges).</p><p>Thus, <strong>ntrboot</strong> came to fruition: Flash a sighax-signed payload into a Nintendo DS flashcard, use a magnet to simulate a closed shell and press the required key combination. Instant Boot9 privileges.</p><p>If this wasn’t enough, Nintendo couldn’t fix any of these vulnerabilities through software updates, as they’re hardwired into the Boot ROM. A possible solution would’ve been to ship new hardware revisions, yet, none ever appeared.</p></div></div></div><h4 id="the-remaining-years-2018-present">The remaining years (2018-present)</h4><p>Now that the homebrew community has achieved its magnum opus, the remaining years of the Nintendo 3DS will only see the streamlining of hacking methods, all of which share the same objective: Install boot9strap.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/3dshacks.93122646fef0484a0abbe6ae842c573dc81e29a734f760d158547a50a2964172.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/_hu1ed3670aa3a156a8d4deb11d9d975c6b_89755_67144305ea863bc41e118057e27ecc2f.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/homebrew/_hu1ed3670aa3a156a8d4deb11d9d975c6b_89755_15f7a7c7977b5b7d951c8f22169b0417.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/homebrew/_hu1ed3670aa3a156a8d4deb11d9d975c6b_89755_60452d1ade267dfc9302be1e561295f6.webp 985w"><img alt="Image" width="985" height="640" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/3dshacks.93122646fef0484a0abbe6ae842c573dc81e29a734f760d158547a50a2964172.png" loading="lazy"></picture></a><figcaption>As the methodologies used to hack a 3DS drastically evolve, sometimes too quickly for new users, community-maintained websites like 3ds.hacks.guide currently holds a reputation as the most reliable and updated set of tutorials.</figcaption></figure><p>By this point in time, there were many exploits in the wild: ‘SoundHax’, ‘Safehax’, ‘Browserhax’… too many to mention here. For the curious, 3DBrew provides a comprehensive list <sup id="bibref:143"><a href="#bib:anti_piracy-user_flaws" role="doc-biblioref">[143]</a></sup>.</p><p>To give you an idea of how elegant exploitation became by 2023, let me show you a common method users relied on and didn’t require extra hardware. This process was called ‘seedminer + BannerBomb3’ and combined the following vulnerabilities, the majority of them authored by zoogie:</p><ol><li><strong>seedminer</strong>: User data installed in the 3DS’ SD card is encrypted using AES-128-CTR. Its key is constructed from other keys found in a file called <code>movable.sed</code> (console-unique, stored in NAND). Well, it was discovered that this file can be re-constructed by using the console’s Friend Code, subdirectory names in the SD card (generated by the console) and short-term brute-forcing. Once extracted, the keys allowed to tamper with DSiWare data in the SD card.</li><li><strong>BannerBomb3</strong>: An exploit that overflows the stack of the Settings app while it tries to parse the banner of an installed DSiWare title <sup id="bibref:144"><a href="#bib:anti_piracy-bannerbomb" role="doc-biblioref">[144]</a></sup>. Combined with seedminer, this serves as an entry-level exploit with Kernel11 privileges.</li><li>Now, how to take advantage of BannerBomb3 (i.e.&nbsp;which payload to use) depended on the tutorial the user was following at the time. For simplicity purposes, there were two routes:</li></ol><div><ul><li id="tab-14-1-the-safe-mode-route-link"><a href="#tab-14-1-the-safe-mode-route">The Safe Mode route</a></li></ul><div><div id="tab-14-1-the-safe-mode-route"><h5 id="tab-14-1-the-safe-mode-route">The Safe Mode route</h5><p>This route consisted of exploiting <code>SAFE_FIRM</code> and was described in earlier tutorials:</p><ol><li><strong>unSAFE_MODE</strong>: Users can boot into Safe Mode by pressing a combination of buttons during the console’s boot, the alternative firmware then enables the user to perform a system update, which is useful for repairing the console. Well, zoogie discovered that the proxy settings can be overflowed <sup id="bibref:145"><a href="#bib:anti_piracy-unsafe_mode" role="doc-biblioref">[145]</a></sup>. Hence, providing user-land execution within Safe Mode.</li><li><strong>safehax</strong>: a port of ‘firmlaunch-hax’ to work under SAFE_FIRM. Nintendo originally patched it with system update <code>9.5.0</code> released in February 2015 <sup id="bibref:146"><a href="#bib:anti_piracy-9_5" role="doc-biblioref">[146]</a></sup>. Yet, SAFE_FIRM is an immutable replica of the factory firmware, and thus it features old exploits NATIVE_FIRM once <em>enjoyed</em>.<ol><li><strong>firmlaunch-hax</strong>: When the firmware is booting, the ARM9 stores the firmware’s header in FCRAM for verifying and then parsing. With the help of a race condition, execution can take control of the ARM9, so the boot9strap installer can be launched. Nintendo fixed this by keeping the header in ARM9 RAM instead, although this stayed unpatched on SAFE_FIRM.</li></ol></li></ol></div><div id="tab-14-2-the-home-menu-route"><p>Sometime later, a new route was proposed by new tutorials. This exploited the HOME Menu with a new Menuhax-style hack:</p><ol><li><strong>menuhax67</strong>: The screen brightness configuration value can be overflown <sup id="bibref:147"><a href="#bib:anti_piracy-menuhax67" role="doc-biblioref">[147]</a></sup>, leading to user-land control from the HOME Menu.</li><li><strong>nimdsphax</strong>: An modern exploit chain combining ‘ctr-httpwn’, ‘nimhax’ and ‘dsp pwn’ <sup id="bibref:148"><a href="#bib:anti_piracy-nimdsphax" role="doc-biblioref">[148]</a></sup>.<ol><li><strong>ctr-httpwn</strong> by yellows8: The HTTP service used for network connections can be controlled by overriding its heap memory (using the old GPU DMA exploit) <sup id="bibref:149"><a href="#bib:anti_piracy-httpwn" role="doc-biblioref">[149]</a></sup>.</li><li><strong>nimhax</strong> by luigoalma: Uses ctr-httpwn to escalate and take over the services that control the user file system, console configuration and application management <sup id="bibref:150"><a href="#bib:anti_piracy-nimhax" role="doc-biblioref">[150]</a></sup>.</li><li><strong>dsp pwn</strong> by luigoalma: Uses nimhax to take control of the DSP, which in turn uses the GPU’s DMA to override the Kernel9 memory space. Thus, obtaining ARM9 privileges.</li></ol></li></ol></div></div></div><h5 id="post-2023-and-conclusions">Post-2023 and conclusions</h5><p>Be as it may, at the time of this writing, Nintendo hasn’t quite surrendered to the cat-and-mouse game. In May 2023, system update <code>11.17.0</code> patched BannerBomb3, nullifying one of the last entry points that didn’t require additional materials <sup id="bibref:151"><a href="#bib:anti_piracy-11_17" role="doc-biblioref">[151]</a></sup>. This means users will now need to either obtain a legitimate 3DS game (which can then be exploited), an ntrboot-compatible DS flashcard; or wait for a WebKit exploit (there’s one only left for the New 3DS browser <sup id="bibref:152"><a href="#bib:anti_piracy-skaterhax" role="doc-biblioref">[152]</a></sup>).</p><hr><h2 id="thats-all-folks">That’s all folks</h2><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/my3dss.291f0c00616803ac9f9b570af267a17901a729074ded39c03576c16d4cd6917f.webp"><picture><img alt="Image" width="1037" height="500" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hu8ac0a8a08ff3edfea9a6be84b3a66ace_53002_245d657b9cd3a8518579bd5017c67086.png" loading="lazy"></picture></a><figcaption>My Nintendo 3DS(s). Apart from the XL one on the left, I bought two extra for this article: The red one you see on the right (originally listed as ‘broken’, turned out the power socket is just flaky) and another one (<em>correctly</em> listed ‘for parts’) to take the motherboard photos.</figcaption></figure><p>Phew, that was another one of those long articles. I’m glad you managed to keep up and reach the end!</p><p>If you are curious, this article took me almost a year to finish, mainly due to a combination of multiple factors, but the important thing is that I ultimately managed to complete it.</p><p>Looking back, it’s hard to admit this console didn’t enjoy the same degree of success as the Nintendo DS. Considering all of its offerings analysed here, I think many external factors hindered its marketing. For starters, the timing was unfortunate and the price tag wasn’t exactly tempting. From my perspective, back when it launched in 2011, the ‘08 financial crisis was hitting hard (I was living in Spain back then), so the Nintendo 3DS wasn’t exactly on adults’ (and kids’) priorities. I eventually got mine in 2018, by then living in the UK.</p><p>In any case, I want to thank the #ReSwitched and #Godmode9 for spotting lots of mistakes in my initial drafts. This has been the most intricate console I’ve written about (to this date!), nevertheless, I’m very grateful to find communities willing to help out.</p><p>Until next time!<br>Rodrigo</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Outperforming larger language models with less training data and smaller models (148 pts)]]></title>
            <link>https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html</link>
            <guid>37606352</guid>
            <pubDate>Fri, 22 Sep 2023 00:29:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html">https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html</a>, See on <a href="https://news.ycombinator.com/item?id=37606352">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-2762083676649407668">
<p><span>Posted by Cheng-Yu Hsieh, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team
</span>

</p><p>
Large language models (LLMs) have enabled a new data-efficient learning paradigm wherein they can be used to solve unseen new tasks via <a href="https://arxiv.org/abs/2005.14165">zero-shot or few-shot prompting</a>. However, LLMs are challenging to deploy for real-world applications due to their sheer size. For instance, serving a single 175 billion LLM requires at least 350GB of GPU memory using <a href="https://arxiv.org/abs/2201.12023">specialized infrastructure</a>, not to mention that today's state-of-the-art LLMs are composed of over <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">500 billion parameters</a>. Such computational requirements are inaccessible for many research teams, especially for applications that require low latency performance.
</p>

<p>
To circumvent these deployment challenges, practitioners often choose to deploy smaller specialized models instead. These smaller models are trained using one of two common paradigms: <a href="https://arxiv.org/abs/1801.06146">fine-tuning</a> or <a href="https://arxiv.org/abs/1503.02531">distillation</a>. Fine-tuning updates a pre-trained smaller model (e.g., <a href="https://arxiv.org/abs/1810.04805">BERT</a> or <a href="https://arxiv.org/abs/1910.10683">T5</a>) using downstream manually-annotated data. Distillation trains the same smaller models with labels generated by a larger LLM. Unfortunately, to achieve comparable performance to LLMs, fine-tuning methods require human-generated labels, which are expensive and tedious to obtain, while distillation requires large amounts of unlabeled data, which can also be hard to collect.
</p>

<p>
In “<a href="https://arxiv.org/abs/2305.02301">Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes</a>”, presented at <a href="https://2023.aclweb.org/">ACL2023</a>, we set out to tackle this trade-off between model size and training data collection cost. We introduce distilling step-by-step, a new simple mechanism that allows us to train smaller task-specific models with much less training data than required by standard fine-tuning or distillation approaches that outperform few-shot prompted LLMs’ performance. We demonstrate that the distilling step-by-step mechanism enables a 770M parameter T5 model to outperform the few-shot prompted 540B PaLM model using only 80% of examples in a benchmark dataset, which demonstrates a more than 700x model size reduction with much less training data required by standard approaches.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s1570/image3.png"><img data-original-height="788" data-original-width="1570" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s16000/image3.png"></a></td></tr><tr><td>While LLMs offer strong zero and few-shot performance, they are challenging to serve in practice. On the other hand, traditional ways of training small task-specific models require a large amount of training data. Distilling step-by-step provides a new paradigm that reduces both the deployed model size as well as the number of data required for training.</td></tr></tbody></table>


<br>


<h2>Distilling step-by-step</h2>


<p>
The key idea of distilling step-by-step is to extract informative <em>natural language</em> <em>rationales (i.e., </em>intermediate reasoning steps)<em> </em>from LLMs, which can in turn be used to train small models in a more data-efficient way. Specifically, natural language rationales explain the connections between the input questions and their corresponding outputs. For example, when asked, “<em>Jesse's room is 11 feet long and 15 feet wide. If she already has 16 square feet of carpet, how much more carpet does she need to cover the whole floor?</em>”, an LLM can be prompted by the few-shot <a href="https://blog.research.google/2022/05/language-models-perform-reasoning-via.html">chain-of-thought</a> (CoT) prompting technique to provide intermediate rationales, such as, “<em>Area = length * width. Jesse’s room has 11 * 15 square feet.</em>” That better explains the connection from the input to the final answer, “<em>(11 * 15 ) - 16</em>”. These rationales can contain relevant task knowledge, such as “<em>Area = length * width”</em>, that may originally require many data for small models to learn. We utilize these extracted rationales as additional, richer supervision to train small models, in addition to the standard task labels.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EUj386PgYZ1PP-Wq9vDSer0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s1999/image4.png"><img data-original-height="932" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EUj386PgYZ1PP-Wq9vDSer0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s16000/image4.png"></a></td></tr><tr><td>Overview on distilling step-by-step: First, we utilize CoT prompting to extract rationales from an LLM. We then use the generated rationales to train small task-specific models within a multi-task learning framework, where we prepend task prefixes to the input examples and train the model to output differently based on the given task prefix.</td></tr></tbody></table>


<p>
Distilling step-by-step consists of two main stages. In the first stage, we leverage few-shot CoT prompting to extract rationales from LLMs. Specifically, given a task, we prepare few-shot exemplars in the LLM input prompt where each example is composed of a triplet containing: (1) input, (2) rationale, and (3) output. Given the prompt, an LLM is able to mimic the triplet demonstration to generate the rationale for any new input. For instance, in a <a href="https://arxiv.org/abs/1811.00937">commonsense question answering task</a>, given the input question “Sammy wanted to go to where the people are. Where might he go? Answer Choices: (a) populated areas, (b) race track, (c) desert, (d) apartment, (e) roadblock”, distilling step-by-step provides the correct answer to the question, “(a) populated areas”, paired with the rationale that provides better connection from the question to the answer, “The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of people.” By providing CoT examples paired with rationales in the prompt, the <a href="https://arxiv.org/abs/2005.14165">in-context learning ability</a> allows LLMs to output corresponding rationales for future unseen inputs.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s1999/image2.png"><img data-original-height="1175" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s16000/image2.png"></a></td></tr><tr><td>We use the few-shot CoT prompting, which contains both an example rationale (<strong>highlighted in green</strong>) and a label (<strong>highlighted in blue</strong>), to elicit rationales from an LLM on new input examples. The example is from a commonsense question answering task.</td></tr></tbody></table>


<p>
After the rationales are extracted, in the second stage, we incorporate the rationales in training small models by framing the training process as a multi-task problem. Specifically, we train the small model with a novel <em>rationale generation task</em> in addition to the standard <em><a href="https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html?m=1">label prediction task</a></em>. The rationale generation task enables the model to learn to generate the intermediate reasoning steps for the prediction, and guides the model to better predict the resultant label. We prepend <a href="https://arxiv.org/abs/1910.10683">task prefixes</a> (i.e., [label] and [rationale] for label prediction and rationale generation, respectively) to the input examples for the model to differentiate the two tasks.
</p>




<h2>Experimental setup</h2>


<p>
In the experiments, we consider a <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">540B PaLM</a> model as the LLM. For task-specific downstream models, we use <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">T5 models</a>. For CoT prompting, we use the <a href="https://arxiv.org/abs/2201.11903">original CoT prompts</a> when available and curate our own examples for new datasets. We conduct the experiments on four benchmark datasets across three different NLP tasks: <a href="https://arxiv.org/abs/1812.01193">e-SNLI</a> and <a href="https://arxiv.org/abs/1910.14599">ANLI</a> for<a href="https://arxiv.org/abs/1508.05326"> natural language inference</a>; <a href="https://arxiv.org/abs/1811.00937">CQA</a> for commonsense question answering; and <a href="https://arxiv.org/abs/2103.07191">SVAMP</a> for <a href="https://aclanthology.org/N16-1136/">arithmetic math word problems</a>. We include two sets of baseline methods. For comparison to <a href="https://arxiv.org/abs/2005.14165">few-shot prompted LLMs</a>, we compare to <a href="https://arxiv.org/abs/2201.11903">few-shot CoT prompting</a> with a <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">540B PaLM</a> model. In the <a href="https://arxiv.org/abs/2305.02301">paper</a>, we also compare standard task-specific model training to both <a href="https://arxiv.org/abs/1801.06146">standard fine-tuning</a> and <a href="https://arxiv.org/abs/1503.02531">standard distillation</a>. In this blogpost, we will focus on the comparisons to standard fine-tuning for illustration purposes.
</p>



<h3>Less training data</h3>


<p>
Compared to <a href="https://arxiv.org/abs/1801.06146">standard fine-tuning</a>, the distilling step-by-step method achieves better performance using much less training data. For instance, on the e-SNLI dataset, we achieve better performance than standard fine-tuning when using only 12.5% of the full dataset (shown in the upper left quadrant below). Similarly, we achieve a dataset size reduction of 75%, 25% and 20% on ANLI, CQA, and SVAMP.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s1999/image6.png"><img data-original-height="1477" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s16000/image6.png"></a></td></tr><tr><td>Distilling step-by-step compared to standard fine-tuning using 220M T5 models on varying sizes of human-labeled datasets. On all datasets, distilling step-by-step is able to outperform standard fine-tuning, trained on the full dataset, by using much less training examples.</td></tr></tbody></table>

<br>


<h3>Smaller deployed model size</h3>


<p>
Compared to <a href="https://arxiv.org/abs/2201.11903">few-shot CoT prompted LLMs</a>, distilling step-by-step achieves better performance using much smaller model sizes. For instance, on the e-SNLI dataset, we achieve better performance than 540B PaLM by using a 220M T5 model. On ANLI, we achieve better performance than 540B PaLM by using a 770M T5 model, which is over 700X smaller. Note that on ANLI, the same 770M T5 model struggles to match PaLM’s performance using standard fine-tuning.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s1999/image5.png"><img data-original-height="1384" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s16000/image5.png"></a></td></tr><tr><td>We perform distilling step-by-step and standard fine-tuning on varying sizes of T5 models and compare their performance to LLM baselines, i.e., Few-shot CoT and PINTO Tuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, e.g., over 700× smaller models on ANLI. Standard fine-tuning fails to match LLM’s performance using the same model size.</td></tr></tbody></table>


<br>


<h3>Distilling step-by-step outperforms few-shot LLMs with smaller models using less data</h3>


<p>
Finally, we explore the smallest model sizes and the least amount of data for distilling step-by-step to outperform PaLM’s few-shot performance. For instance, on ANLI, we surpass the performance of the 540B PaLM using a 770M T5 model. This smaller model only uses 80% of the full dataset. Meanwhile, we observe that standard fine-tuning cannot catch up with PaLM’s performance even using 100% of the full dataset. This suggests that distilling step-by-step simultaneously reduces the model size as well as the amount of data required to outperform LLMs.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s1999/image1.png"><img data-original-height="1400" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s16000/image1.png"></a></td></tr><tr><td>We show the minimum size of T5 models and the least amount of human-labeled examples required for distilling step-by-step to outperform LLM’s few-shot CoT by a coarse-grained search. Distilling step-by-step is able to outperform few-shot CoT using not only much smaller models, but it also achieves so with much less training examples compared to standard fine-tuning.</td></tr></tbody></table>




<h2>Conclusion</h2>


<p>
We propose distilling step-by-step, a novel mechanism that extracts rationales from LLMs as informative supervision in training small, task-specific models. We show that distilling step-by-step reduces both the training dataset required to curate task-specific smaller models and the model size required to achieve, and even surpass, a few-shot prompted LLM’s performance. Overall, distilling step-by-step presents a resource-efficient paradigm that tackles the trade-off between model size and training data required.
</p>




<h2>Availability on Google Cloud Platform</h2>


<p>
Distilling step-by-step is available for private preview on <a href="https://cloud.google.com/vertex-ai">Vertex AI</a>. If you are interested in trying it out, please contact <a href="mailto:vertex-llm-tuning-preview@google.com">vertex-llm-tuning-preview@google.com</a> with your Google Cloud Project number and a summary of your use case.
</p>




<h2>Acknowledgements</h2>


<p>
<em>This research was conducted by Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Thanks to Xiang Zhang and Sergey Ioffe for their valuable feedback.</em>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unlimited Kagi searches for $10 per month (1142 pts)]]></title>
            <link>https://blog.kagi.com/unlimited-searches-for-10</link>
            <guid>37603905</guid>
            <pubDate>Thu, 21 Sep 2023 20:32:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.kagi.com/unlimited-searches-for-10">https://blog.kagi.com/unlimited-searches-for-10</a>, See on <a href="https://news.ycombinator.com/item?id=37603905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <p><img src="https://assets.kagi.com/v1/kagi_assets/doggo/doggo_2.png" alt=""></p>

<p>This year has been extraordinary for <a href="https://kagi.com/">Kagi</a>. We had tremendous support from our customers, and we want to start this by taking a moment to say <strong>thank you</strong>. We really appreciate it.</p>

<p>Since we launched Kagi, our users have raved about the search quality, but many have stressed about the cost of search and having to change their search habits when switching from an ad-supported service, so we’re fixing that.&nbsp;</p>

<p>We’re thrilled to announce that <strong>unlimited search is now included in our $10/month Professional plan and our Ultimate, Family, and Duo plans</strong>.</p>

<blockquote>
<p>“When I first heard of Kagi (and saw the prices) I thought: this is the stupidest thing ever, jesus it’s priced for Silicon Valley bros.</p>

<p>Well, I just turned on yearly payments seconds ago… I guess it’s just that good?”</p>
</blockquote>

<p><em>Kagi Discord user, a couple of weeks ago - good news for this member :)</em></p>

<p>When we first <a href="https://blog.kagi.com/update-kagi-search-pricing">adjusted our pricing</a>, the search landscape was undergoing seismic shifts and brimming with uncertainty. The world watched as the emergence of powerful AI began rewriting the rules of search, and giants in the tech industry started a battle for dominance. It was a time of challenges and tough decisions for a small bootstrapped company like ours. We were determined to ensure Kagi was still around next year and remains a constant, reliable companion for your search needs.</p>

<p>Today, the tides are changing. With new search sources proving more cost-efficient, the improved efficiency of our infrastructure, and the broader market embracing Kagi, we can again offer an unlimited experience to a broader group of users. We’re excited that this change will let many more people enjoy a fun, ad-free, and user-centric web search.</p>

<p>Here is how the changes look through our membership plans.</p>

<h3>Professional Plan</h3>

<p>Kagi has one goal -  creating the most delightful search experience for our members. Starting today, enjoy unlimited searches with Kagi. For a mere $10/mo, step into the expansive world of Kagi’s renowned search quality and rich features, all crafted with you in mind.</p>

<p>Oh, and by the way, this also includes unlimited use of our <a href="https://kagi.com/summarizer/index.html">Universal Summarizer</a>, which can summarize unlimited-length documents, audio, and video!</p>

<p>As a part of this update, we are also introducing the annual payment option with 10% off!</p>

<blockquote>
<p>“For me Kagi represents an incredible accomplishment: the first search engine that gives better results than Google, respects privacy, offers <a href="https://help.kagi.com/kagi/features/website-info-personalized-results.html">customization</a> and so much more.</p>

<p>Thank you.”</p>
</blockquote>

<p><em>No, thank you, for supporting us!</em></p>

<h3>Starter Plan (formerly called Standard)</h3>

<p>This plan continues as our special introductory offer, designed for those ready to step beyond the free trial but still exploring whether the Professional plan aligns with their needs or fall into the category of most internet users who search just a few times a day on average.</p>

<p>We are removing the pay-per-use component (to simplify our billing infrastructure) and making it simple: 300 searches for $5/month. If a user hits this cap consistently, we recommend upgrading to unlimited searches at $10/month. This change also allows us to add the annual payment option at 10% off, which many of you requested.</p>

<blockquote>
<p>“3 months in - I find myself annoyed when I’m on a device that isn’t mine and I have to use google.”</p>
</blockquote>

<p><em>Same here, same here</em></p>

<h3 id="ultimate-plan">Ultimate Plan</h3>

<p>We’re aware that many of you used the Ultimate Plan for its access to unlimited searches. Starting today, we will be giving Ultimate Plan users access to the closed beta of the next generation of tools that Kagi is bringing to the web, and we’re targeting access for all our current Ultimate users in the next few days.</p>

<p>While we can not reveal yet what that is in detail as we’re still iterating quickly, no competitor comes close to the value proposition of this new feature suite. We would like you to help us better shape their future and join the fun!</p>

<p>When we release these new tools publicly, Ultimate users will have exclusive use of their most advanced features - ensuring your Kagi experience remains top of the line.</p>

<p>The Ultimate plan is also still the best way to support Kagi on its mission to humanize the web, and we are incredibly grateful to all of you who have chosen to do so.</p>

<blockquote>
<p>“The belief the search was better because they have so much history of me and such a strong ML team is why I kept using Google Search despite trying to get away from other Google products.
Kagi <a href="https://help.kagi.com/kagi/features/lenses.html">lenses</a> really do show how much it’s not true, and how much Google’s collection of my data isn’t actually returning on my data and privacy investment.”</p>
</blockquote>

<p><em>Turns out people know what they want better than algorithms do</em></p>

<p><a href="https://kagifeedback.org/assets/files/2023-09-21/1695322703-234286-screenshot-2023-09-21-at-115717.png"><img src="https://kagifeedback.org/assets/files/2023-09-21/1695322703-234286-screenshot-2023-09-21-at-115717.png" \=""></a>
</p><center><em>New Kagi Search Individual plans</em></center>

<h3>Family &amp; Duo Plans</h3>

<p>Family moments are precious, and we’re here to enrich them. Now, the whole family can explore a world of information together, with unlimited searches for $20/month (Family, up to six members) and $14/mo (Duo, for a couple).</p>

<p>We are inching closer to our vision where families can trust their search engine to prioritize their well-being over third parties and advertisers and where young minds grow uninfluenced by the data-hungry algorithms profiling them and changing their behavior.</p>

<p>Discover the joy of safe and ad-free search, nurturing curiosity without compromise. Learn more about the values that shape the <a href="https://blog.kagi.com/family-plan">Kagi Family plan</a>.</p>

<blockquote>
<p>“This sort of stuff makes me really happy to be a Kagi subscriber. Not only do I get value out of Kagi, but it shows me that the money is being used to develop Kagi in a way I agree with. By comparison, REDACTED (just picking one of my subs) feels hostile to me. I pay them, but I would cancel in a heartbeat if I felt I had options.
I really appreciate Kagi’s development matching what i feel like i’m buying. Thanks Kagi Team ”</p>
</blockquote>

<p><em>Awww, thank you! We'll keep on putting your money to good use</em></p>

<p><a href="https://kagifeedback.org/assets/files/2023-09-21/1695322702-804109-screenshot-2023-09-21-at-115735.png"><img src="https://kagifeedback.org/assets/files/2023-09-21/1695322702-804109-screenshot-2023-09-21-at-115735.png" \=""></a>
</p><center><em>New Kagi Search Family plans</em></center>

<h3>Frequently Asked Questions</h3>

<p>Q. <strong>How do I sign up?</strong><br>
A. Click <a href="https://kagi.com/onboarding?p=choose_plan">here</a> to become a member.</p>

<p>Q. <strong>Do I need to do anything to get the benefits of the new plans?</strong><br>
A. All changes will be automatic and applied to your account today.</p>

<p>Q. <strong>I do not have a credit card. Can I also pay for Kagi with PayPal or crypto?</strong><br>
A. Yes. We have recently added <a href="https://blog.kagi.com/accepting-paypal-bitcoin">new payment options</a> to allow payment through credit card, PayPal, and Bitcoin/Lightning through OpenNode.</p>

<p>Q. <strong>Now that I don’t have to count my searches, how can I switch all my browsers and devices to Kagi?</strong><br>
A. We have several options available. See <a href="https://help.kagi.com/kagi/getting-started/setting-default.html">here</a> for more details.</p>

<p>Q. <strong>I have an outstanding pay-per-use balance on my current Standard or Professional plan. What happens now?</strong><br>
A. Effective today, we are waiving all pending metered usage charges for you. Yahoo!</p>

<p>Q. <strong>I noticed that the Ultimate plan changed to 10% off for annual payments. I previously had 15% off. Do I remain locked in?</strong><br>
A. Yes, as long as you do not cancel, you will have the previous discount applied to your renewals.</p>

<p>Q. <strong>When do I get the new advanced features of the Ultimate plan?</strong><br>
A. We are starting to roll out access today in batches, and we expect to onboard all Ultimate plan users to new features by the end of the next week.</p>

<p>Q. <strong>How can I get the new Ultimate plan level features in the Family plan?</strong><br>
A. We haven’t established a mechanism to incorporate Ultimate plan features into the Family plan yet. However, we are open to suggestions and would appreciate any ideas you may have. Please share your thoughts at <a href="https://kagifeedback.org/">KagiFeedback.org</a>.</p>

<p>Q. <strong>Do I need to do anything with my grandfathered Early-adopter/Legacy Professional plan?</strong><br>
A. No. Additionally, we will be simplifying this over the next few days, as this is all the same Professional plan now. If you have the Early-Adopter flag, you will still keep it - we just didn’t have anything to tie it to right now, which does not mean we won’t in the future.</p>

<p>Q. <strong>I’m keen to support Kagi but concerned about its longevity, given the history of search startups. How can I be assured Kagi is here to stay?</strong><br>
A. What sets Kagi apart from other search engines is our fundamentally different, user-centric approach that prioritizes the interests of our members and the alignment of incentives. It’s worth noting that Kagi’s growth has been purely organic, without any expenditure on marketing or customer acquisition, attesting to the inherent value and trust our user base places in us and the growing need for better search.</p>

<p>We have priced our offering so that we do not rely financially on VC funding, data sales, or anything but our users’ contributions. We have had requests from our users to invest in Kagi, and we’re happy to offer that way to support us (see below).</p>

<p>We are striving to build a sustainable business, and we invite you to review our <a href="https://kagi.com/stats">live stats</a> for a transparent view of our steady progress. Your support matters, and we appreciate it a lot.</p>

<p>Q. <strong>How can I deepen my involvement with Kagi’s journey?</strong><br>
A. We are thrilled to hear of your interest! </p>

<p>Kagi is very open to collaboration, and we like to say that 50% of our product has been built by its members. We have a variety of ways you can <a href="https://help.kagi.com/kagi/support-and-community/contribute.html">get involved and contribute</a>. </p>

<p>We are also open to <a href="https://help.kagi.com/kagi/company/hiring-kagi.html">applications for positions</a> at Kagi. </p>

<p>If you’re already a Kagi user and are interested in becoming a shareholder, we’re considering <a href="https://blog.kagi.com/safe-round">another fundraiser</a> early next year. Please <a href="https://forms.gle/1Try2v6JtXbKSjKx9">get in touch</a>, and we will let you know once we are ready. </p>

<p>For other inquiries or simply to connect, join our <a href="https://kagi.com/discord">discord</a> or email Vladimir Prelovac (Kagi Founder) at <a href="mailto:vlad@kagi.com">vlad@kagi.com</a>.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A customer stuck due to a hurricane who needed SSH (173 pts)]]></title>
            <link>https://rachelbythebay.com/w/2023/09/21/hurricane/</link>
            <guid>37603554</guid>
            <pubDate>Thu, 21 Sep 2023 20:09:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2023/09/21/hurricane/">https://rachelbythebay.com/w/2023/09/21/hurricane/</a>, See on <a href="https://news.ycombinator.com/item?id=37603554">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2023/09/21/hurricane/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Android 14 adds support for using your smartphone as a webcam (292 pts)]]></title>
            <link>https://www.esper.io/blog/android-14-adds-support-for-using-your-smartphone-as-a-webcam</link>
            <guid>37603467</guid>
            <pubDate>Thu, 21 Sep 2023 20:04:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.esper.io/blog/android-14-adds-support-for-using-your-smartphone-as-a-webcam">https://www.esper.io/blog/android-14-adds-support-for-using-your-smartphone-as-a-webcam</a>, See on <a href="https://news.ycombinator.com/item?id=37603467">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Video conferencing platforms like Zoom and Google Meet exploded in popularity during the COVID era, but the webcam market struggled to keep up. The best webcams were hard to get or were too expensive, causing most people to turn to cheaper, more readily available webcams or their laptop’s integrated webcam. The camera in most smartphones offers significantly better image quality than the one found in the vast majority of webcams, though, so many people installed third-party software on their phones to turn them into webcams. Starting in <a href="https://blog.esper.io/android-14-deep-dive/">Android 14</a>, it may not be necessary to use a third-party app to turn your smartphone into a webcam for your PC, as that functionality is getting baked into the Android OS itself — though there’s a catch.</p><p>When you plug an Android phone into a PC, you have the option to change the USB mode between file transfer/Android Auto (MTP), USB tethering (NCM), MIDI, or PTP. In Android 14, however, a new option can appear in USB Preferences: USB webcam. Selecting this option switches the USB mode to UVC (USB Video Class), provided the device supports it, turning your Android device into a standard USB webcam that other devices will recognize, including Windows, macOS, and Linux PCs, and <a href="https://source.android.com/docs/core/camera/external-usb-cameras">possibly even other Android devices</a>.</p><figure><p><img src="https://assets-global.website-files.com/602d2f7be70ffc3452f5a079/64f7545560167bedc1f93ad3_Android_14_webcam_in_usb_preferences.png" loading="lazy" alt=""></p></figure><p>Webcam support in Android 14 is not enabled out of the box, however. In order to enable it, four things are required: a Linux kernel config needs to be enabled, the UVC device needs to be configured, the USB HAL needs to be updated, and a new system app needs to be preloaded.&nbsp;</p><h3><strong>Kernel config</strong></h3><p>The Linux kernel config (<a href="https://cateee.net/lkddb/web-lkddb/USB_CONFIGFS_F_UVC.html">CONFIG_USB_CONFIGFS_F_UVC</a>) is necessary so that the Android device can be mounted as a UVC gadget. Fortunately, many devices upgrading to and nearly all devices launching with Android 14 will have a kernel with this config enabled. This is because it is enabled by default on Generic Kernel Image (GKI) versions <a href="https://cs.android.com/android/_/android/kernel/common/+/8d5dd0a5a458f951f0fdc25aba0cb8329b121d51">starting from android12-5.10 and later</a>, and devices launching with Android 12 or later on top of Linux kernel version 5.10 or higher <a href="https://source.android.com/docs/core/architecture/kernel/generic-kernel-image#gki2">are required</a> to ship the GKI kernel.</p><p>Because major kernel version upgrades are rare in the Android space and since the <a href="https://blog.esper.io/android-dessert-bites-11-grf-323579/">Google Requirements Freeze (GRF) program</a> allows for older vendor implementations to still be certifiable, some devices upgrading to Android 14 won’t have kernels that support the USB webcam function. It’s hard to put together a list of such devices, so the best way to tell if your device is capable is to check its kernel version and whether the config is enabled.</p><p>To check your device’s kernel version, run:</p><p>‍</p><p>	adb shell “cat /proc/version”</p><p>‍</p><p>However, to actually verify that the kernel config is enabled, it’s necessary to run another command to see if “CONFIG_USB_CONFIGFS_F_UVC” appears in config.gz, a compressed copy of the configuration file used to build the kernel on the device.</p><p>‍</p><p>	adb shell "zcat /proc/config.gz | grep 'CONFIG_USB_CONFIGFS_F_UVC'"</p><p>‍</p><p>For example, here is the output from a Galaxy Z Fold 5 running Android 13. Since it is using the android13-5.15 GKI and since “CONFIG_USB_CONFIGFS_F_UVC=y”, it should be capable of supporting the USB webcam feature once it’s upgraded to Android 14. That’s assuming, though, that the device meets the other prerequisites I mentioned before.</p><figure><p><img src="https://assets-global.website-files.com/602d2f7be70ffc3452f5a079/64f75477809e9cc6e3d8c7f2_Galaxy_Z_Fold_5_kernel_and_UVC_config.png" loading="lazy" alt=""></p></figure><h3><strong>Webcam Service app</strong></h3><p>Within the Android 14 QPR1 beta for select Pixel devices is a new system app called “Webcam Service” (com.android.deviceaswebcam). This app relies on a shared library named libjni_deviceAsWebcam.so. This app and library are set to be included as part of Android 14’s upcoming source code release.</p><p>The Webcam Service app implements the “DeviceAsWebcam” service that <a href="https://twitter.com/MishaalRahman/status/1621194700790054914">I previously reported</a> would “[turn] an Android device into a webcam.” The service forwards camera frames to a /dev/video node that host devices can read from. <a href="https://android-review.googlesource.com/c/platform/system/sepolicy/+/2410788">SELinux policy</a> dictates that only processes in the device_as_webcam domain, ie. only the Webcam Service system app, can access the /dev/video nodes. Thus, only the device maker and not any third-party can actually take advantage of Android 14’s native USB webcam support.</p><p>How does the Webcam Service app actually know when to start forwarding camera frames? When the new “USB webcam” option in “USB Preferences” is toggled, the system broadcasts the android.hardware.usb.action.USB_STATE intent with the “connected” and “uvc” intent extras. Webcam Service has a receiver for this intent, which starts the system service if the “uvc” intent extra is set to “true” and the framework method android.hardware.usb#isUvcSupportEnabled() returns true.</p><p>The method isUvcSupportEnabled() returns true when the system property “ro.usb.uvc.enabled” is set to true. This property must be set by the OEM at build time, and if it is not set, then “USB Preferences” won’t show the “USB webcam” option and Webcam Service won’t start. This property is <a href="https://android-review.googlesource.com/c/platform/system/sepolicy/+/2415830">only readable by</a> system apps like Settings and the Webcam Service app.</p><p>When the Webcam Service starts, a new notification is posted that lets the user configure the webcam. Tapping the notification opens a camera preview where the user can zoom in or out or change lenses. Under the hood, the Webcam Service app starts a foreground service to ensure it is kept alive by the system. It uses the Camera2 API and supports streaming at either 720p (1280x720) or 1080p (1920x1080p) resolutions. The webcam device appears on the connected host as “Android Webcam”.</p><figure><p><img src="https://assets-global.website-files.com/602d2f7be70ffc3452f5a079/650b5a0741e33b9aefbf3f8b_Android_14_webcam_service_3.png" loading="lazy" alt=""></p></figure><h3><strong>ConfigFS and USB HAL</strong></h3><p>The exact encoding method, video parameters, and name displayed to the host depends on how the device maker sets up the UVC gadget using configfs. For example, on the Tensor-powered Pixel devices, Google’s UVC gadget function configuration can be found in the init.gs[101|201].usb.rc file located in /vendor/etc/init/hw.</p><figure><p><img src="https://assets-global.website-files.com/602d2f7be70ffc3452f5a079/64f7549e858c09450c069ea4_init.gs201.usb.rc.jpeg" loading="lazy" alt=""></p></figure><p>In addition, the device needs an updated USB HAL so that Android is able to switch USB modes to UVC when the option is selected in Settings. Due to the aforementioned GRF program, however, it’s likely that many devices upgrading to Android 14 won’t receive an updated USB HAL, meaning this feature won’t work.</p><h2>Conclusion</h2><p>It’s good to see Google implement native USB webcam functionality into Android. Assuming that the Webcam Service app will be available in AOSP as we expect, that means this feature can be picked up by any device maker that wishes to implement webcam functionality. Many will call this feature a clone of Apple’s “<a href="https://support.apple.com/en-us/HT213244">Continuity Camera</a>”, but it’s worth noting that Android’s version works with multiple platforms. Any phone running Android 14 that meets the requirements mentioned in this article can be turned into a standard USB webcam that works with any PC, and that’s a big deal.</p><p>‍</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LabelContactRelationYoungerCousinMothersSiblingsDaughterOrFathersSistersDaughter (246 pts)]]></title>
            <link>https://developer.apple.com/documentation/contacts/cnlabelcontactrelationyoungercousinmotherssiblingsdaughterorfatherssistersdaughter</link>
            <guid>37603331</guid>
            <pubDate>Thu, 21 Sep 2023 19:56:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.apple.com/documentation/contacts/cnlabelcontactrelationyoungercousinmotherssiblingsdaughterorfatherssistersdaughter">https://developer.apple.com/documentation/contacts/cnlabelcontactrelationyoungercousinmotherssiblingsdaughterorfatherssistersdaughter</a>, See on <a href="https://news.ycombinator.com/item?id=37603331">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Speeding up the JavaScript ecosystem – Polyfills gone rogue (181 pts)]]></title>
            <link>https://marvinh.dev/blog/speeding-up-javascript-ecosystem-part-6/</link>
            <guid>37602923</guid>
            <pubDate>Thu, 21 Sep 2023 19:31:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marvinh.dev/blog/speeding-up-javascript-ecosystem-part-6/">https://marvinh.dev/blog/speeding-up-javascript-ecosystem-part-6/</a>, See on <a href="https://news.ycombinator.com/item?id=37602923">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>📖 tl;dr: Many popular npm packages depend on 6-8x more packages than they need to. Most of these are unnecessary polyfills and it's one of the key reasons node_modules folders are so large. The eslint ecosystem seems to be most affected by this.</p><div>
						<p>In the previous posts we looked at runtime performance and I thought it would be fun to look at node modules install time instead. A lot has been already written about various algorithmic optimizations or using more performant syscalls, but why do we even have this problem in the first place? Why is every <code>node_modules</code> folders so big? Where are all these dependencies coming from?</p>
						<p>It all started when a buddy of mine noticed something odd with the dependency tree of his project. Whenever he updated a dependency, it would pull in several new ones and with each subsequent update the total number of dependencies grew over time.</p>
						<pre><code> ├─┬ arraybuffer.prototype.slice <span>1.0</span>.2<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ function.prototype.name <span>1.1</span>.6<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ globalthis <span>1.0</span>.3<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ object.assign <span>4.1</span>.4<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ regexp.prototype.flags <span>1.5</span>.1<br> │ ├─┬ define-properties <span>1.2</span>.1<br> │ │ └── define-data-property <span>1.1</span>.0<br> │ └─┬ set-function-name <span>2.0</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ string.prototype.trim <span>1.2</span>.8<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ string.prototype.trimend <span>1.0</span>.7<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> └─┬ string.prototype.trimstart <span>1.0</span>.7<br>   └─┬ define-properties <span>1.2</span>.1<br> 	└── define-data-property <span>1.1</span>.0</code></pre>
						<p>Now to be fair, there are valid reasons why a package might depend on an additional dependencies. Here, we began to notice a pattern though: The new dependencies were all polyfills for JavaScript functions that have long been supported everywhere. The <code>Object.defineProperties</code> method for example was shipped as part of the very first public Node <code>0.10.0</code> release dating back to 2013. Heck, even Internet Explorer 9 supported that. And yet there were numerous packages in that dependend on a polyfill for it.</p>
						<p>
							Among the various packages that pulled in <code>define-properties</code> was <a href="https://github.com/jsx-eslint/eslint-plugin-react/"><code>eslint-plugin-react</code></a>. It caught my eye, because it's very popular in the React ecosystem. Why does it pull in a polyfill for <code>Object.defineProperties</code>? There is no JavaScript engine that doesn't come with it already built in.
						</p>
						<h2>Polyfills that don’t polyfill</h2>
						<p>Reading the source of the packages revealed something even more bizarre: The polyfill functions were imported and called directly, rather than patching missing functionality in the runtime environment. The whole point of a polyfill is to be transparent to the user’s code. It should check if the function or method to patch is available and only add it if it’s missing. When there is no need for the polyfill it should do nothing. The odd thing to me is that the functions were used directly like a function from a library.</p>
						<pre><code><span>// Why is the `define` function imported directly?</span><br><span>var</span> define <span>=</span> <span>require</span><span>(</span><span>"define-properties"</span><span>)</span><span>;</span><br><span>// ...</span><p><span>// and even worse, why is called directly?</span><br><span>define</span><span>(</span>polyfill<span>,</span> <span>{</span><br>	<span>getPolyfill</span><span>:</span> getPolyfill<span>,</span><br>	<span>implementation</span><span>:</span> implementation<span>,</span><br>	<span>shim</span><span>:</span> shim<span>,</span><br><span>}</span><span>)</span><span>;</span></p></code></pre>
						<p>Instead they should call <code>Object.defineProperties</code> directly. The whole point of polyfills is to patch <em>the environment</em> not be called directly. Compare this to what a polyfill for <code>Object.defineProperties</code> is supposed to look like:</p>
						<pre><code><span>// Check if the current environment already supports</span><br><span>// `Object.defineProperties`. If it does, then we do nothing.</span><br><span>if</span> <span>(</span><span>!</span>Object<span>.</span>defineProperties<span>)</span> <span>{</span><br>	<span>// Patch in Object.defineProperties here</span><br><span>}</span></code></pre>
						<p>The most common place where <code>define-properties</code> was used was ironically inside other polyfills, which loaded even more polyfills. And before you ask, the <code>define-properties</code> package relies on even more dependencies than just itself.</p>
						<pre><code><span>var</span> keys <span>=</span> <span>require</span><span>(</span><span>"object-keys"</span><span>)</span><span>;</span><br><span>// ...</span><br><span>var</span> defineDataProperty <span>=</span> <span>require</span><span>(</span><span>"define-data-property"</span><span>)</span><span>;</span><br><span>var</span> supportsDescriptors <span>=</span> <span>require</span><span>(</span><span>"has-property-descriptors"</span><span>)</span><span>(</span><span>)</span><span>;</span><p><span>var</span> <span>defineProperties</span> <span>=</span> <span>function</span> <span>(</span><span>object<span>,</span> map</span><span>)</span> <span>{</span><br>	<span>// ...</span><br><span>}</span><span>;</span></p><p>module<span>.</span>exports <span>=</span> defineProperties<span>;</span></p></code></pre>
						<p>Inside <code>eslint-plugin-react</code>, that polyfill is loaded via a polyfill for <code>Object.entries()</code> to process <a href="https://github.com/jsx-eslint/eslint-plugin-react/blob/ecadb92609998520be80d64c0bd6bc5e05934aa9/configs/all.js#L4">their configuration</a>:</p>
						<pre><code><span>const</span> fromEntries <span>=</span> <span>require</span><span>(</span><span>"object.fromentries"</span><span>)</span><span>;</span> <span>// &lt;- Why is this used directly?</span><br><span>const</span> entries <span>=</span> <span>require</span><span>(</span><span>"object.entries"</span><span>)</span><span>;</span> <span>// &lt;- Why is this used directly?</span><br><span>const</span> allRules <span>=</span> <span>require</span><span>(</span><span>"../lib/rules"</span><span>)</span><span>;</span><p><span>function</span> <span>filterRules</span><span>(</span><span>rules<span>,</span> predicate</span><span>)</span> <span>{</span><br>	<span>return</span> <span>fromEntries</span><span>(</span><span>entries</span><span>(</span>rules<span>)</span><span>.</span><span>filter</span><span>(</span><span>entry</span> <span>=&gt;</span> <span>predicate</span><span>(</span>entry<span>[</span><span>1</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span><br><span>}</span></p><p><span>const</span> activeRules <span>=</span> <span>filterRules</span><span>(</span>allRules<span>,</span> <span>rule</span> <span>=&gt;</span> <span>!</span>rule<span>.</span>meta<span>.</span>deprecated<span>)</span><span>;</span></p></code></pre>
						<h2>Doing a little bit of housekeeping</h2>
						<p>At the time of this writing installing <code>eslint-plugin-react</code> pulls in a whopping number of 97 dependencies in total. I was curious about how much of these were polyfills and began patching them out one by one locally. After all was done, this brought down the total number of dependencies down to 15. Out of the original 97 dependencies 82 of them are not needed.</p>
						<p>Coincidentally, <code>eslint-plugin-import</code> which is equally popular in various eslint presets, shows a similar problems. Installing that fills up your <code>node_modules</code> folder with 87 packages. After another local cleanup pass I was able to cut down that number to just 17.</p>
						<h2>Filling up everyone's disk space</h2>
						<p>Now you might be wondering if you’re affected or not. I did a quick search and basically every widely popular eslint plugin or preset that you can think of is affected. For some reason this whole ordeal reminds me of the <code>is-even</code>/<code>is-odd</code> incident the industry had a while back.</p>
						<p>Having so many dependencies makes it much harder to audit the dependencies of a project. It's a waste of space too. Case in point: Deleting all eslint plugins and presets in a project alone got rid of <code>220</code> packages.</p>
						<pre><code><span>pnpm</span> <span>-r</span> <span>rm</span> eslint-plugin-react eslint-plugin-import eslint-import-resolver-typescript eslint-config-next eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin eslint-plugin-prettier prettier eslint-config-prettier eslint-plugin-react-hooks<br>Scope: all <span>8</span> workspace projects<br><span>.</span>                                        <span>|</span> <span>-220</span> ----------------------</code></pre>
						<p>Maybe we don't need that many dependencies in the first place. My mind went to this fantastic quote by the creator of the Erlang programming language:</p>
						<blockquote>
							<p>You wanted a banana but what you got was a gorilla holding the banana and the entire jungle - Joe Armstrong</p>
						</blockquote>
						<p>All I wanted was some linting rules. I didn’t want a bunch of polyfills that I don't need.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open source AI will win (118 pts)]]></title>
            <link>https://varunshenoy.substack.com/p/why-open-source-ai-will-win</link>
            <guid>37602674</guid>
            <pubDate>Thu, 21 Sep 2023 19:17:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://varunshenoy.substack.com/p/why-open-source-ai-will-win">https://varunshenoy.substack.com/p/why-open-source-ai-will-win</a>, See on <a href="https://news.ycombinator.com/item?id=37602674">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h4>Discover more from Public Experiments</h4><p>Marginalia on science, engineering, and culture.</p> </div><div dir="auto"><blockquote><p>Linux is subversive. Who would have thought even five years ago (1991) that a world-class operating system could coalesce as if by magic out of part-time hacking by several thousand developers scattered all over the planet, connected only by the tenuous strands of the Internet? </p><p>Certainly not I.</p><p><em>opening remarks in The Cathedral and the Bazaar by Eric Raymond.</em></p></blockquote><p>There’s a popular floating theory on the Internet that a combination of the existing foundation model companies will be the end game for AI. </p><p>In the near future, every company will rent a “brain” from a model provider, such as OpenAI/Anthropic, and build applications that build on top of its cognitive capabilities.</p><p>In other words, AI is shaping up to be an oligopoly of sorts, with only a small set of serious large language model (LLM) providers.</p><p>I don’t think this could be farther from the truth. I truly believe that open source will have more of an impact on the future of LLMs and image models than the broad public believes.</p><p>There are a few arguments against open source that I see time and time again.</p><ol><li><p><strong>Open source AI cannot compete with the resources at industry labs.</strong><span> Building foundation models is expensive, and non-AI companies looking to build AI features will outsource their intelligence layer to a company that specializes in it. Your average company cannot scale LLMs or produce novel results the same way a well capitalized team of talented researchers can. On the image generation side, Midjourney is miles ahead of anything else.</span></p></li><li><p><strong>Open source AI is not safe.</strong><span> Mad scientists cooking up intelligence on their </span><a href="https://nonint.com/2022/05/30/my-deep-learning-rig/" rel="">cinderblock-encased GPUs</a><span> will not align their models with general human interests</span></p><span>. </span></li><li><p><strong>Open source AI is incapable of reasoning.</strong><span> Not only do open source models perform more poorly than closed models on benchmarks, but they also lack </span><a href="https://arxiv.org/abs/2206.07682" rel="">emergent capabilities</a><span>, those that would enable agentic workflows, for example.</span></p></li></ol><p>While they seem reasonable, I think these arguments hold very little water.</p><p>Outsourcing a task is fine — when the task is not business critical. </p><p><a href="https://www.baseten.co/" rel="">Infrastructure products</a><span> save users from wasting money and energy on learning Kubernetes or hiring a team of DevOps engineers. No company should have to hand-roll their own HR/bill payments software. There are categories of products that enable companies to “focus on what makes their beer taste better”</span></p><p><span>. </span></p><p>LLMs, for the most part, do not belong in this category. There are some incumbents building AI features on existing products, where querying OpenAI saves them on hiring ML engineers. For them, leveraging closed AI makes sense. </p><p>However, there’s a whole new category of AI native businesses for whom this risk is too great. Do you really want to outsource your core business, one that relies on confidential data, to OpenAI or Anthropic? Do you want to spend the next few years of your life working on a “GPT wrapper”? </p><p><strong>Obviously not.</strong></p><p>If you’re building an AI native product, your primary goal is getting off of OpenAI as soon as you possibly can. Ideally, you can bootstrap your intelligence layer using a closed source provider, build a data flywheel from engaged users, and then fine-tune your own models to perform your tasks with higher accuracy, less latency, and more control.</p><p><span>Every business needs to own their core product, and for AI native startups, their core product is a model trained on proprietary data</span></p><p><span>. Using closed source model providers for the long haul exposes an AI native company to undue risk.</span></p><p><span>There is too much pressure pent up for open source LLMs to flop. The lives of many companies are at stake. Even Google has acknowledged that </span><a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither" rel="">they have no moat</a><span> in this new world of open source AI.</span></p><p>The general capabilities of LLMs open them up to an exponential distribution of use cases. The most important tasks are fairly straightforward: summarization, explain like I’m 5, create a list (or some other structure) from a blob of text, etc. </p><p>Reasoning, the type you get from scaling these models to get larger, doesn’t matter for 85% of use cases. Researchers love sharing that their 200B param model can solve challenging math problems or build a website from a napkin sketch, but I don’t think most users (or developers) have a burning need for these capabilities.</p><p><span>The truth is that open source models are </span><em>incredibly</em><span> good at the most valuable tasks, and can be fine-tuned to cover likely up to 99% of use-cases when a product has collected enough labeled data.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png" width="578" height="328.74010079193664" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:790,&quot;width&quot;:1389,&quot;resizeWidth&quot;:578,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Llama 2 performance&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Llama 2 performance" title="Llama 2 performance" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Fine-tuned Llama 2 models vs. GPT-4 (from </span><a href="https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications" rel="">Anyscale</a><span>)</span></figcaption></figure></div><p>Reasoning, the holy grail that researchers are chasing, probably doesn’t matter nearly as much as people think.</p><p>More important than reasoning is context length and truthfulness. </p><p>Let’s start with context length. The longer the context length for a language model, the longer the prompts and chat logs you can pass in. </p><p>The original Llama has a context length of 2k tokens. Llama 2 has a context length of 4k. </p><p><span>Earlier this year, </span><a href="https://kaiokendev.github.io/til" rel="">an indie AI hacker</a><span> discovered that a single line code change to the RoPE embeddings for Llama 2 would give you up to 8K of context length </span><em>for free with no additional training.</em><span> </span></p><p><span>Just last week another indie research project was released, </span><a href="https://github.com/jquesnelle/yarn" rel="">YaRN</a><span>, that extends Llama 2’s context length to 128k tokens. </span></p><p>I still don’t have access to GPT-4 32k. This is the speed of open source.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png" width="466" height="410.95054945054943" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1284,&quot;width&quot;:1456,&quot;resizeWidth&quot;:466,&quot;bytes&quot;:574648,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>While contexts have scaled up, the hardware requirements to run massive models have also scaled down. You can now run state-of-the-art </span><a href="https://twitter.com/ggerganov/status/1699791226780975439?s=20" rel="">massive</a><span> language models from your Macbook thanks to projects like </span><a href="https://github.com/ggerganov/llama.cpp" rel="">Llama.cpp</a><span>. Being able to use these models locally is a huge plus for security and costs as well. In the limit, you can run your models on your users’ hardware. Models are continuing to scale down while retaining quality. Microsoft’s Phi-1.5 is only 1.3 billion parameters but meets Llama 2 7B </span><a href="https://x.com/Teknium1/status/1701422303643615571?s=20" rel="">on several benchmarks</a><span>. Open source LLM experimentation will continue to explode as consumer hardware and </span><a href="https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini" rel="">the GPU poor</a><span> rise to the challenge.</span></p><p><span>On truthfulness: out-of-the-box open source models are less truthful than closed source models, and I think this is actually fine. In many cases, </span><a href="https://towardsdatascience.com/llm-hallucinations-ec831dcd7786" rel="">hallucination</a><span> can be a feature, not a bug, particularly when it comes to creative tasks like storytelling. </span></p><p><span>Closed AI models have a certain filter that make them sound </span><em>artificial</em><span> and less interesting. </span><a href="https://huggingface.co/Gryphe/MythoMax-L2-13b" rel="">MythoMax-L2</a><span> tells significantly better stories than Claude 2 or ChatGPT, at only 13B parameters. When it comes to honestly, the latest open source LLMs work well with </span><a href="https://www.pinecone.io/learn/retrieval-augmented-generation/" rel="">retrieval augmented generation</a><span>, and they will only get better.</span></p><p>Let’s take a brief look at the image generation side. </p><p>I would argue that Stable Diffusion XL (SDXL), the best open source model, is nearly on-par with Midjourney. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png" width="1456" height="507" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:507,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2871704,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Stable Diffusion XL generations for the prompt “an astronaut playing a guitar on Mars with a llama”. These images were generated on the first try, no cherry-picking needed.</figcaption></figure></div><p><span>In exchange for the slightly worse ergonomics, Stable Diffusion users have access to hundreds of community crafted LoRAs</span></p><p><span>, fine-tunes, and textual embeddings. Users quickly discovered hands were a sore for SDXL, and within weeks a LoRA </span><a href="https://minimaxir.com/2023/08/stable-diffusion-xl-wrong/" rel="">that fixes hands appeared online</a><span>. </span></p><p><span>Other open source projects like </span><a href="https://huggingface.co/docs/diffusers/training/controlnet" rel="">ControlNet</a><span> give Stable Diffusion users significantly more power when it comes to structuring their outputs, where Midjourney falls flat.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png" width="512" height="208.21333333333334" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:305,&quot;width&quot;:750,&quot;resizeWidth&quot;:512,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>A flowchart of how Stable Diffusion + ControlNet works. Clipped from </span><a href="https://stable-diffusion-art.com/controlnet/" rel="">here</a><span>.</span></figcaption></figure></div><p>Moreover, Midjourney doesn’t have an API, so if you want to build a product with an image diffusion feature, you would have to use Stable Diffusion in some form. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg" width="432" height="432" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:960,&quot;width&quot;:960,&quot;resizeWidth&quot;:432,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;r/StableDiffusion - Spiral Town - different approach to qr monster&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="r/StableDiffusion - Spiral Town - different approach to qr monster" title="r/StableDiffusion - Spiral Town - different approach to qr monster" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>This image went viral on </span><a href="https://x.com/MrUgleh/status/1702041188482658758?s=20" rel="">Twitter</a><span> and </span><a href="https://www.reddit.com/r/StableDiffusion/comments/16ew9fz/spiral_town_different_approach_to_qr_monster/" rel="">Reddit</a><span> this week. It uses Stable Diffusion with ControlNet. Currently, you can’t create images like this on Midjourney.</span></figcaption></figure></div><p>There are similar controllable features and optimizations that open source LLMs enable.</p><p>An LLM’s logits, the token-wise probability mass function at each iteration, can be used to generate structured output. In other words, you can guarantee the generation of JSON without entering a potentially expensive “validate-retry” loop, which is what you would need to do if you were using OpenAI. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png" width="818" height="218" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:218,&quot;width&quot;:818,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;How to Get Better Outputs from Your Large Language Model | NVIDIA Technical  Blog&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="How to Get Better Outputs from Your Large Language Model | NVIDIA Technical  Blog" title="How to Get Better Outputs from Your Large Language Model | NVIDIA Technical  Blog" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>An example of logits from </span><a href="https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/" rel="">NVIDIA</a><span>.</span></figcaption></figure></div><p><span>Open source models are smaller and run on your own dedicated instance, leading to lower end-to-end </span><a href="https://twitter.com/abacaj/status/1699602420882378932?s=20" rel="">latencies</a><span>. You can improve throughput by batching queries and using inference servers like </span><a href="https://vllm.ai/" rel="">vLLM</a><span>. </span></p><p><span>There are many more tricks (see: </span><a href="https://arxiv.org/abs/2302.01318" rel="">speculative</a><span> </span><a href="https://twitter.com/ggerganov/status/1697262700165013689?s=20" rel="">sampling</a><span>, </span><a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html#concurrent-model-execution" rel="">concurrent model execution</a><span>, </span><a href="https://github.com/ggerganov/llama.cpp/issues/64" rel="">KV caching</a><span>) that you can apply to improve on the axes of latency and throughput. The latency you see on the OpenAI endpoint is the best you can do with closed models, rendering it useless for many latency-sensitive products and too costly for large consumer products.</span></p><p><span>On top of all this, you can also fine-tune or train your own LoRAs on top of open source models with maximal control. Frameworks like </span><a href="https://github.com/OpenAccess-AI-Collective/axolotl#axolotl" rel="">Axolotl</a><span> and </span><a href="https://huggingface.co/docs/trl/sft_trainer" rel="">TRL</a><span> have made this process simple</span></p><p><span>. While closed source model providers also have their own fine-tuning endpoints, you wouldn’t get the same level of control or visibility than if you did it yourself. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png" width="468" height="382.7168674698795" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1086,&quot;width&quot;:1328,&quot;resizeWidth&quot;:468,&quot;bytes&quot;:384308,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Falcon 180B, the largest open source model to date, was </span><a href="https://huggingface.co/blog/falcon-180b" rel="">released last week</a><span>. Within hours, Discords filled with mostly anonymous developers began exploring how they could recreate GPT-4 using this new model as a base layer.</span></figcaption></figure></div><p>Open source also provides guarantees on privacy and security.</p><p>You control the inflow and outflow of data in open models. The option to self-host is a necessity for many users, especially those working in regulated fields like healthcare. Many applications will also need to run on proprietary data, on both the training and inference side.</p><p><span>Security is best explained by </span><a href="https://en.wikipedia.org/wiki/Linus%27s_law" rel="">Linus’s Law</a><span>:</span></p><blockquote><p>Given a large enough beta-tester and co-developer base, almost every problem will be characterized quickly and the fix obvious to someone. </p><p>Or, less formally, ‘‘Given enough eyeballs, all bugs are shallow.’’</p></blockquote><p><span>Linux succeeded because it was built in the open. Users knew </span><em>exactly</em><span> what they were getting and had the opportunity to file bugs or even attempt to fix them on their own with community support. </span></p><p><span>The same is true for open source models. Even </span><a href="https://karpathy.medium.com/software-2-0-a64152b37c35" rel="">software 2.0</a><span> needs to be audited. Otherwise, things can change under the hood, leading to regressions in your application. This is unacceptable for most business use cases.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png" width="526" height="377.15934065934067" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1044,&quot;width&quot;:1456,&quot;resizeWidth&quot;:526,&quot;bytes&quot;:275898,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><a href="https://arxiv.org/pdf/2307.09009.pdf" rel="">This paper</a><span> recently showed that OpenAI’s endpoints drift over time. You cannot be confident that a prompt that works flawlessly will perform the same a month from now.</span></figcaption></figure></div><p><span>Adopting an open source approach for AI technology can create a wide-reaching network of checks and balances. Scientists and developers globally can peer-review, critique, study, and understand the underlying mechanisms, leading to improved safety, reliability, interpretability, and trust. Furthermore, widespread knowledge helps advance the technology responsibly while mitigating the risk of its misuse. </span><strong>Hugging Face is the new RedHat.</strong><span> </span></p><p>You can only trust models that you own and control. The same can’t be said for black box APIs. This is also why the AI safety argument against open source makes zero sense. History suggests, open source AI is, in fact, safer. </p><p><span>Why do people currently prefer closed source? Two reasons: </span><em>ease-of-use</em><span> and </span><em>mindshare</em><span>.</span></p><p>Open source is much harder to use than closed source models. It seems like you need to hire a team of machine learning engineers to build on top of open source as opposed to using the OpenAI API. This is ok, and will be true in the short-term. This is the cost of control and the rapid pace of innovation. People who are willing to spend time at the frontier will be treated by being able to build much better products. The ergonomics will get better.</p><p><span>The more unfortunate issue is </span><strong>mindshare</strong><span>. </span></p><p>Closed source model providers have captured the collective mindshare of this AI hype cycle. People don’t have time to mess around with open source nor do they have the awareness of what open source is capable of. But they do know about OpenAI, Pinecone, and LangChain. </p><p>Using the right tool is often conflated with using the best known tool. The current hype cycle has put closed source AI in the spotlight. As open source offerings mature and become more user-friendly and customizable, they will emerge as the superior choice for many applications. </p><p>Rather than getting swept up in the hype, forward-thinking organizations will use this period to deeply understand their needs and lay the groundwork to take full advantage of open source AI. They will build defensible and differentiated AI experiences on open technology. This measured approach enables a sustainable competitive advantage in the long run. </p><p>The future remains bright for pragmatic adopters who see past the hype and keep their eyes on the true prize: truly open AI.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA’s Webb finds carbon source on surface of Jupiter’s moon Europa (348 pts)]]></title>
            <link>https://webbtelescope.org/contents/news-releases/2023/news-2023-113</link>
            <guid>37602239</guid>
            <pubDate>Thu, 21 Sep 2023 18:52:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webbtelescope.org/contents/news-releases/2023/news-2023-113">https://webbtelescope.org/contents/news-releases/2023/news-2023-113</a>, See on <a href="https://news.ycombinator.com/item?id=37602239">Hacker News</a></p>
Couldn't get https://webbtelescope.org/contents/news-releases/2023/news-2023-113: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Most UI applications are broken real-time applications (149 pts)]]></title>
            <link>https://thelig.ht/ui-apps-are-broken/</link>
            <guid>37601064</guid>
            <pubDate>Thu, 21 Sep 2023 17:34:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thelig.ht/ui-apps-are-broken/">https://thelig.ht/ui-apps-are-broken/</a>, See on <a href="https://news.ycombinator.com/item?id=37601064">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>I’ve been programming for a long time. When I say long time, I mean
decades, with an S. Hopefully that’s long enough. In that time my
experience has primarily been programming for contemporary platforms,
e.g.&nbsp;Linux, Windows, macOS on desktop-class or server-class CPU
architectures. Recently, I embarked on building a <a href="https://www.supermidipak.com/">MIDI engine for a system with
significantly less processing power</a>.</p>
<p>Soon after I started, I ran into the issue of guaranteeing that it
was impossible for the queue of input events to build up indefinitely.
This essentially boils down to making sure that each event handler
doesn’t run longer than some maximum amount. Then it hit me, I’ve heard
this before, <em>maximum amount of time</em>: I’m building a real-time
system.</p>
<p>Once I realized that I had to additionally take real-time constraints
into account while building, it drove a lot of the engineering decisions
I made in a specific direction. In particular, the worst case time of
every sequence of code must be accounted for, the average-case time was
irrelevant for correctness. Under this discipline, algorithms which had
better worst-case time but worse average-case time are preferred,
branching usually must be to the faster path, and adding fast paths to
slow algorithms was not helpful. It was interesting work and it changed
how I thought about building systems in a profound way.</p>
<p>Armed with this new awareness, I began to notice the lack of
real-time discipline in other applications, including my own. This was a
jarring experience, how could I have never noticed this before? The
juggernaut during this period was when I realized that most mainstream
desktop UI applications were <em>fundamentally broken</em>.</p>
<p>When I click a mouse button, when I press a key on the keyboard, I
expect a response in a bounded amount of time. <em>Bounded amount of
time?</em> We’ve heard this before! UI applications are also real-time
systems. How much time is this bounded amount of time? 100ms or maybe
250ms. Well, take your pick, the key point is that the response time
should not be indefinite. I should never see a <strong>beach ball of
death</strong>. <em>Never</em>.</p>
<h2 id="library-functions-are-not-real-time">Library Functions are not
Real-time</h2>
<p>One of the fundamental problems is that many UI applications on
Windows, Linux, and macOS call functions that are not specified to run
in a bounded amount of time. Here’s a basic example: many applications
don’t think twice about doing file IO in a UI event handler. That
results in a tolerable amount of latency most of the time on standard
disk drives but what if the file is stored on a network drive? It could
take much longer than a second to service the file request. This will
result in a temporarily hung application with the user not knowing what
is happening. The network drive is operating correctly, the UI
application isn’t.</p>
<p>So all we have to do is avoid file system IO functions from the main
thread? Not a big deal. That doesn’t mean UI applications are
fundamentally broken. That’s just one broken application and it’s still
relatively easily fixable.</p>
<p>It’s not just file system IO functions. File system IO functions
belong to a class of functions called blocking functions. These are
functions that are specified not to return until some external event
happens. So correct UI applications cannot call any blocking function
from their main threads.</p>
<p>It gets worse. Literally none of the standard library functions on
contemporary systems are guaranteed to return in any amount of time. If
you want to write a correct UI application, you technically cannot call
any of them. I’m talking <code>malloc()</code>. Each call risks taking
an amount of time longer than the maximum time allotted to respond to
the event.</p>
<p>You may think I am being excessively pedantic with the previous
point. Maybe you think, “No sane implementation of any standard library
function will take more than a 500us on good data. It’s good enough to
avoid blocking functions on the main thread.” I have two words for you:
virtual memory.</p>
<h2 id="virtual-memory">Virtual Memory</h2>
<p>When it comes to Windows, Linux, and macOS, these operating systems
are virtual memory systems. When applications allocate memory, they are
not actually allocating physical memory. They are telling the operating
system that they will be using a certain memory region for a certain
purpose. This enables lots of functionality but in particular this
allows operating systems to save physical memory by transparently
storing memory pages onto a hard disk and restoring the page when the
application accesses the page again. This means that <em>a memory access
can block on a hard disk access</em>.</p>
<p>This is a transparent process that is not under control of the
application. Thus, if any given memory access can block on IO from a
disk drive, that means the system is fundamentally not real-time,
therefore UI applications on such a system are fundamentally broken.</p>
<p>This doesn’t seem like a common problem but whole system “out of
memory” conditions are not that uncommon. When the system is in this
state, it starts rapidly paging memory onto the hard disk. UI
applications will be affected and this will cause your system to hang
without warning and with no way to intervene since keypresses cannot be
processed. From a user standpoint, this is worse than a kernel panic.
This type of failure has happened to me multiple times on Linux so I
know it’s a problem there. Perhaps Windows and macOS engineers have
already considered this issue but I doubt it.</p>
<p>Is there a way to fix this? At least on Linux there is the
<code>mlock()</code> family of functions that tell the operating system
to put and keep the process’s memory pages into RAM. There are likely
similar functions available on Windows and macOS. Of course there are
still complications, e.g.&nbsp;is the application or the operating system
responsible for locking memory pages? how does the application know
which pages to lock? how does the operating system know which pages to
lock?</p>
<h2 id="real-time-scheduling">Real-time Scheduling</h2>
<p>The final fundamental issue with implementing real-time UI on top of
contemporary mainstream platforms is the lack of real-time scheduling
for the active UI application. These systems are time-sharing systems,
meaning that a process’s execution can be indefinitely paused if there
are many other processes competing for use of the CPU.</p>
<p>Imagine you have multiple background process running at 100% CPU,
then a UI event comes in to the active UI application. The operating
system may block for 100ms * N before allowing the UI application to
process the event, where N is the number of competing background
processes, potentially causing a delayed response to the user that
violates the real-time constraint (NB: 10Hz is a common timeslice for
time-sharing systems).</p>
<p>There is a solution for this as well, the window manager or
equivalent can tell the OS to give scheduling priority to whatever UI
application has active focus. This means that while the UI application
is active and needs CPU, background processes are starved. There are
complications with adapting a solution like this to existing systems as
well, e.g.&nbsp;What to do when the active UI application runs into an
infinite loop? What about multi-process UI applications?</p>
<h2 id="conclusion">Conclusion</h2>
<p>Hopefully I’ve at least convinced you that mainstream UI applications
are built on poor foundations. Do these UI applications work? Sure, most
of the time but when they fail due to bad real-time assumptions, they
fail in an annoying way. <strong>Beach ball of death</strong>. It’s
unacceptible for workstation-class interactive systems to ever fail this
way. I want to use responsive, correct applications. In the future, the
UIs on which people will depend will take real-time constraints into
account across the entire stack.</p>
<p>As far as I can tell, fixing this in a meaningful way will require
large ecosystem-level changes and broad awareness. Lots of wide-reaching
architectural decisions that ignore these issues have accumulated over
decades. I’m tempted to abandon using Windows, macOS and Linux as the
main platforms with which I interact.</p>
<p>Send any comments to <a href="https://twitter.com/cejetvole"><span data-cites="cejetvole">@cejetvole</span></a></p>
<p>Rian Hunter<br> 2023-09-21</p>
<p><em>Edit: The “Real-time Scheduling” section was added shortly after
initial publication.</em></p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The urgent need for memory safety in software products (133 pts)]]></title>
            <link>https://www.cisa.gov/news-events/news/urgent-need-memory-safety-software-products</link>
            <guid>37600937</guid>
            <pubDate>Thu, 21 Sep 2023 17:26:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cisa.gov/news-events/news/urgent-need-memory-safety-software-products">https://www.cisa.gov/news-events/news/urgent-need-memory-safety-software-products</a>, See on <a href="https://news.ycombinator.com/item?id=37600937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For over <a href="https://apps.dtic.mil/sti/citations/AD0758206" title="DTIC Bibliography">half a century</a>, software engineers have known malicious actors could exploit a class of software defect called “memory safety vulnerabilities” to compromise applications and systems. During that time, experts have repeatedly warned of the problems associated with memory safety vulnerabilities. Memory unsafe code even led to a major internet outage in 1988. Just how big a problem is memory unsafety? In a blog post, <a href="https://msrc-blog.microsoft.com/2019/07/16/a-proactive-approach-to-more-secure-code/" title="Microsoft Blog: A Proactive Approach to More Secure Code">Microsoft reported</a> that “~70% of the vulnerabilities Microsoft assigns a CVE [Common Vulnerability and Exposure] each year continue to be memory safety issues.” <a href="https://www.chromium.org/Home/chromium-security/memory-safety/" title="Chromium Security: Memory Safety">Google likewise reported</a> that “the Chromium project finds that around 70% of our serious security bugs are memory safety problems.” <a href="https://hacks.mozilla.org/2019/02/rewriting-a-browser-component-in-rust/" title="Mozilla: Rewriting a Browser Component in Rust">Mozilla reports</a> that in an analysis of security vulnerabilities, that “of the 34 critical/high bugs, 32 were memory-related.”</p><p>These vulnerabilities are not theoretical. Attackers use them in the commission of attacks against real people. For example, Google’s Project Zero team analyzed vulnerabilities that were used in the wild by attackers before they were reported to software providers (also called “zero-day vulnerabilities”). <a href="https://googleprojectzero.blogspot.com/2022/04/the-more-you-know-more-you-know-you.html" title="Google: The More You Know, The More You Know">They report</a> that “out of the 58 [such vulnerabilities] for the year, 39, or 67% were memory corruption vulnerabilities.” <a href="https://citizenlab.ca/2023/09/blastpass-nso-group-iphone-zero-click-zero-day-exploit-captured-in-the-wild/" title="Citizen Lab Blog">Citizen Lab uncovered</a> spyware used against civil society organizations that exploited memory safety vulnerabilities.</p><p>In what other industry would the market tolerate such well-understood and severe dangers for users of products for decades?</p><p>Over the years, software engineers have invented numerous clever, but ultimately insufficient mitigations for this class of vulnerability, including tools like memory randomization and sandboxing techniques that reduce impact, and tools for static and dynamic code analysis that reduce occurrence. In addition to those tools, organizations have spent significant time and money training their developers to avoid unsafe memory operations. There are also several parallel efforts to improve the memory safety of existing C/C++ code. Despite these efforts (and associated costs in complexity, time, and money), memory unsafety has been the most common type of software security defect for decades.</p><p>There are, however, a few areas that every software company should investigate. First, there are some promising memory safety mitigations in hardware. The Capability Hardware Enhanced RISC Instructions (<a href="https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/" title="CHERI Research Paper">CHERI</a>) research project uses modified processors to give memory unsafe languages like C and C++ protection against many widely exploited vulnerabilities. Another hardware assisted technology comes in the form of memory tagging extensions (MTE) that are available in some systems. While some of these hardware-based mitigations are still making the journey from research to shipping products, many observers believe they will become important parts of an overall strategy to eliminate memory safety vulnerabilities.</p><p>Second, companies should investigate memory safe programming languages. Most modern programming languages other than C/C++ are already memory safe. Memory safe programming languages manage the computer’s memory so the programmer cannot introduce memory safety vulnerabilities. Compared to other available mitigations that require constant upkeep – either in the form of developing new defenses, sifting through vulnerability scans, or human labor – no work has to be done once code is written in a memory safe programming language to keep it memory safe.</p><p>What has been lacking until a few years ago is a language with the speed of C/C++ with built-in memory safety assurances. In 2006, a software engineer at Mozilla began working on a new programming language called Rust. Rust version 1.0 was officially announced in 2015. Since then, several prominent software organizations have started to use it in their systems, including Amazon, Facebook, Google, Microsoft, Mozilla, and many others. It is also supported in the development of the Linux kernel.</p><p>Different products will require different investment strategies to mitigate memory unsafe code. The balance between C/C++ mitigations, hardware mitigations, and memory safe programming languages may even differ between products from the same company. No one approach will solve all problems for all products. The one thing software manufacturers cannot do, however, is ignore the problem. The software industry must not kick the can down the road another decade through inaction.</p><p>CISA’s <a href="https://www.cisa.gov/securebydesign" title="CISA | Secure By Design">secure by design white paper</a> outlines three core principles for software manufacturers: take ownership of customer security outcomes, embrace radical transparency, and lead security transformations from the top of the organization. Solutions to the memory unsafety problem will incorporate all three principles.</p><p>CISA urges software manufacturers to make it a top-level company goal to reduce and eventually eliminate memory safety vulnerabilities from their product lines. To demonstrate such a commitment, companies can publish a “memory safety roadmap” that includes information about how they are modifying their software development lifecycle (SDLC) to accomplish this goal. A roadmap might include details like the date after which it will build new products or components in a memory safe programming language and plans to support the memory safety initiatives of open source libraries that are part of their supply chain.</p><p>Memory unsafety has plagued the software industry for decades and will continue to be a major source of vulnerabilities and real-world harm until top business leaders from the software manufacturers make appropriate investments and take ownership of the security outcomes of their customers. As we recognize National Coding Week, we look forward to participants across the software industry working together to make software that is safer by design, and memory safety is the key to achieving that goal.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The WebP 0day (272 pts)]]></title>
            <link>https://blog.isosceles.com/the-webp-0day/</link>
            <guid>37600852</guid>
            <pubDate>Thu, 21 Sep 2023 17:21:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.isosceles.com/the-webp-0day/">https://blog.isosceles.com/the-webp-0day/</a>, See on <a href="https://news.ycombinator.com/item?id=37600852">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                    <p>Early last week, Google released a new <a href="https://chromereleases.googleblog.com/2023/09/stable-channel-update-for-desktop_11.html?ref=blog.isosceles.com">stable update</a> for Chrome. The update included a single security fix that was reported by Apple's Security Engineering and Architecture (SEAR) team. The issue, CVE-2023-4863, was a heap buffer overflow in the WebP image library, and it had a familiar warning attached: </p><p>"Google is aware that an exploit for CVE-2023-4863 exists in the wild."</p><p>This means that someone, somewhere, had been caught using an exploit for this vulnerability. But who discovered the vulnerability and how was it being used? How does the vulnerability work? Why wasn't it discovered earlier? And what sort of impact does an exploit like this have?</p><p>There are still a lot of details that are missing, but this post attempts to explain what we know about the unusual circumstances of this bug, and provides a new technical analysis and proof-of-concept trigger for CVE-2023-4863 ("the WebP 0day").</p><p>This work was made possible by major technical contributions from <a href="https://twitter.com/mistymntncop?ref=blog.isosceles.com">@mistymntncop</a> -- thank you!</p><h2 id="unraveling-the-timeline">Unraveling the Timeline</h2><p>Immediately after the Chrome security update was released, experts began to <a href="https://twitter.com/msuiche/status/1701714151091949812?ref=blog.isosceles.com">speculate</a> that there was a link between CVE-2023-4863 and an earlier CVE from Apple, CVE-2023-41064. The theory goes something like this.</p><p>Early in September (exact date unknown), Citizen Lab detected suspicious behavior on the iPhone of "an individual employed by a Washington DC-based civil society organization":</p><p><a href="https://citizenlab.ca/2023/09/blastpass-nso-group-iphone-zero-click-zero-day-exploit-captured-in-the-wild/?ref=blog.isosceles.com">BLASTPASS: NSO Group iPhone Zero-Click, Zero-Day Exploit Captured in the Wild</a></p><p>They attributed the behavior to a "zero-click" exploit for iMessage being used to deploy NSO group's Pegasus spyware, and sent their technical findings to Apple. Apple responded swiftly, and on September 7 they released a <a href="https://support.apple.com/en-us/HT213905?ref=blog.isosceles.com">security bulletin</a> that featured two new CVEs from the attack Citizen Lab identified. On each CVE they note: "<em>Apple is aware of a report that this issue may have been actively exploited.</em>"</p><p>Citizen Lab called this attack "BLASTPASS", since the attackers found a clever way to bypass the "<a href="https://googleprojectzero.blogspot.com/2021/01/a-look-at-imessage-in-ios-14.html?ref=blog.isosceles.com">BlastDoor</a>" iMessage sandbox. We don't have the full technical details, but it looks like by bundling an image exploit in a <a href="https://developer.apple.com/documentation/passkit/?ref=blog.isosceles.com">PassKit</a> attachment, the malicious image would be processed in a different, unsandboxed process. This corresponds to the first CVE that Apple released, CVE-2023-41061.</p><p>But you'd still need an image exploit to take advantage of this situation, and indeed, the second CVE that Apple released is CVE-2023-41064, a buffer overflow vulnerability in ImageIO. ImageIO is Apple's image parsing framework. It will take a sequence of bytes and attempt to match the bytes to a suitable image decoder. Several different formats are supported, and ImageIO has been an <a href="https://googleprojectzero.blogspot.com/2020/04/fuzzing-imageio.html?ref=blog.isosceles.com">active</a> <a href="https://support.apple.com/en-ng/HT213670?ref=blog.isosceles.com">area</a> of security research. We don't have any technical details about CVE-2023-41064 yet, so we don't know which image format it affects. </p><p>But we do know that ImageIO recently began to support WebP files, and we know that on September 6 (one day before the iOS/macOS security bulletin), Apple's security team reported a WebP vulnerability to Chrome that was urgently patched (just 5 days after the initial report) and marked by Google as "exploited in the wild". Based on this, it seems likely that the BLASTPASS vulnerability and CVE-2023-4863 ("the WebP 0day") are the same bug.</p><h2 id="the-webp-0daytechnical-analysis">The WebP 0day -- Technical Analysis</h2><p>By cross-referencing the bug ID from Chrome's security bulletin with recent open source commits to the libwebp library code, it's possible to find the following patch:</p><p><a href="https://chromium.googlesource.com/webm/libwebp/+/902bc9190331343b2017211debcec8d2ab87e17a?ref=blog.isosceles.com">Fix OOB write in BuildHuffmanTable</a></p><p>This patch was created on September 7 (one day after Apple's report), and corresponds to CVE-2023-4863. Based on an initial review of the patch, we learn the following:</p><ul><li>The vulnerability is in the "lossless compression" support for WebP, sometimes known as VP8L. A lossless image format can store and restore pixels with 100% accuracy, meaning that the image will be displayed with perfect accuracy. To achieve this, WebP uses an algorithm called <a href="https://en.wikipedia.org/wiki/Huffman_coding?ref=blog.isosceles.com">Huffman coding</a>.<br></li><li>Although Huffman coding is conceptually based on a tree data structure, modern implementations have been optimized to use tables instead. The patch suggests that it was possible to overflow the Huffman table when decoding an untrusted image.<br></li><li>Specifically, the vulnerable versions use memory allocations based on pre-calculated buffer sizes from a fixed table, and will then construct the Huffman tables directly into that allocation. The new version does a "first pass" construction that calculates the total size that the output table will require, but doesn't actually write the table to the buffer. If the total size is bigger than the pre-calculated buffer size, then a larger allocation is made.</li></ul><p>This is a great start, but it's non-constructive. We want to be able to construct an example file that can actually trigger the overflow, and to do that we have to understand how this code is actually working and why the pre-calculated buffer sizes weren't sufficient.</p><p>Stepping back, what is the vulnerable code actually doing? When a WebP image is compressed in a lossless way, a frequency analysis of the input pixels is performed. The basic idea is that input values that occur more frequently can be assigned to a shorter sequence of output bits, and values that occur less frequently can be assigned to longer sequences of output bits. The real trick is that the output bits are cleverly chosen so that the decoder can always work out the length of that particular sequence -- i.e. it's always possible to disambiguate between a 2-bit code and a 3-bit code, and so on, and so the decoder always knows how many bits to consume.</p><p>To achieve this, the compressed image has to include all of the statistical information about frequencies and code assignments, so that the decoder can reproduce the same mapping between codes and values. As mentioned, internally webp uses a table for this (they call it the "huffman_table")... but the tables themselves can be quite large, and including them alongside the compressed image would make the file size increase. The solution is to use Huffman coding to compress the tables as well. It's turtles all the way down.</p><p>This means that there's a non-trivial amount of mental gymnastics involved in analyzing/triggering the bug. Based on a review of the patch, we can isolate the memory allocation that is the most likely candidate for being overflowed and come up with a plan. </p><p>We're trying to overflow the huffman_tables allocation in ReadHuffmanCodes (src/dec/vp8l_dec.c), and the idea is to use the VP8LBuildHuffmanTable/BuildHuffmanTable call in ReadHuffmanCode (not the one in ReadHuffmanCodeLengths) to shift the huffman_table pointer past the pre-calculated buffer size. To add to the complexity, there's actually 5 different segments of the Huffman table, each with a different alphabet size (e.g. number of possible output symbols for that particular segment of the table) -- and we'll probably have to craft all 5 of those to get close enough to the end of the buffer to cause an overflow.</p><p>At this point I had come up with a basic theory of how to proceed and started manually crafting a file that could reach this deep into the code, and around this time I started chatting with <a href="https://twitter.com/mistymntncop?ref=blog.isosceles.com">@mistymntncop</a>. It turns out that they had also been attempting to reproduce this issue, and they had built harness code to create a well-formed WebP with arbitrary Huffman coding data ("code lengths"). I tried it out and it worked perfectly, we could pass arbitrary code_lengths array into the BuildHuffmanTable call that we were targeting. Brilliant.</p><p>Now the challenge was to find a group of code_lengths that would make BuildHuffmanTable exceed the pre-calculated buffer size. I started with some manual experimentation -- changing the code_lengths array to affect the internal histogram (essentially the count array in BuildHuffmanTable), and then watching what affect each of the 16 histogram entries had on total_size, the key variable that we needed to increase to a larger than expected value. </p><p>It quickly became clear that there was a complex interaction between the histogram's starting state, the tree statistics (num_open and num_nodes), and the "key" variable that tracks the starting location of the "ReplicateValue" operation that wrote entries into the output table that we're trying to overflow. It reminded me of watching the internal state of a cryptographic hash function, and without knowing a lot more about Huffman trees and WebP's specific implementation choices, I didn't feel confident that I'd be able to manually craft an input that would even be considered correct by BuildHuffmanTable, let alone one that makes BuildHuffmanTable return an unexpectedly large value.</p><p>My next idea was to brute-force a solution. I had noticed that the first 9 entries in the histogram (e.g. count[0] .. count[8], which are called the "root table") wouldn't have much influence on the total_size, but could influence the internal state for subsequent computations (such as by pushing the number of nodes too high). The final entries in the histogram (e.g. count[9] .. count[15], which are called the "second level tables") had a direct effect on the final total_size value. With this in mind I created a few different statistical distributions that generally kept the values of the root table low (typically summing to less than 8) and the second level table higher. This approach managed to find correct inputs, and some of them resulted in output tables that were quite large, but still less than the pre-calculated buffer sizes.</p><p>I decided I needed to understand how the pre-calculated sizes were derived. There are actually several different pre-calculated size buckets depending on the number of color cache bits that are specified. The buckets are defined in kTableSize, which includes a helpful description of the values and an invaluable tip: <em>"All values computed for 8-bit first level lookup with Mark Adler's tool: &nbsp; </em><a href="https://github.com/madler/zlib/blob/v1.2.5/examples/enough.c?ref=blog.isosceles.com"><em>https://github.com/madler/zlib/blob/v1.2.5/examples/enough.c</em></a><em>"</em></p><p>The "enough" tool emits the histogram for the largest possible Huffman tree lookup table for any given alphabet size, root table size, and maximum code length. Using Mark Adler's tool, I could replicate the pre-calculated buffer sizes, and using <a href="https://twitter.com/mistymntncop?ref=blog.isosceles.com">@mistymntncop</a>'s tool I could verify that the specific code_lengths emitted by "enough" would 100% fill up the huffman_tables allocation. That's great, but the whole idea of a heap overflow would be to fill up the allocation to 101%...</p><p>I followed a dead-end here, which is that the "enough" tool only works for color_cache sizes up to 8-bits. How did they derive the values for 9-bit, 10-bit, or 11-bit caches, all of which are considered valid? Maybe they just guessed and these values are wrong? I think Google must have modified "enough" to work on larger alphabet sizes, because I managed to replicate their numbers by making some minor changes to "enough" (things like using the 128-bit integer scalar type compiler extension to be able to count the number of trees without overflow).</p><p>At this point there was a long process of angst. The "enough" tool is clear in its documentation that it calculates the maximum value for <em>valid and complete</em> codes. There must be some configuration of this input histogram that produces a tree that WebP considers to be valid and complete, but is actually incomplete/invalid in a way that produces a larger expansion than anticipated. The patch even hints in this direction, saying: <em>"make sure that valid (but unoptimized because of unbalanced codes) streams are still decodable"</em></p><p>In the end I managed to convince myself that this wasn't possible by enumerating all of the possible valid trees in the smallest of the tables (a symbol size of 40), which also happened to be the last of the 5 tables we needed to fill. The purported maximum size for a symbol size of 40 with a root table of 8-bits and a maximum code length of 15 is 410. If you can generate anything bigger than 410, then you win. But none of the codes that BuildHuffmanTable would consider valid had a size bigger than 410 (and most of them were much smaller). It seems like the consistency check at the end of BuildHuffmanTable, e.g. checking that the number of output nodes is an expected value, was ensuring that the codes it accepted were in line with "enough" and the pre-calculated buffer sizes it gave.</p><p>But the BuildHuffmanTable function is writing values to the output table using the "ReplicateValue" operation mentioned earlier. What if we built 4 valid Huffman trees that resulted in 4 maximally sized output tables, and then supplied an invalid Huffman tree for the last table? Could we get ReplicateValue to write out-of-bounds from an invalid starting key prior to the final consistency check on the node count? The answer is: yes, we can.</p><p>Here's how to replicate the bug:<br></p><pre><code>  # checkout webp
$ git clone https://chromium.googlesource.com/webm/libwebp/ webp_test
$ cd webp_test/
  # checkout vulnerable version
$ git checkout 7ba44f80f3b94fc0138db159afea770ef06532a0
  # enable AddressSanitizer
$ sed -i 's/^EXTRA_FLAGS=.*/&amp; -fsanitize=address/' makefile.unix
  # build webp
$ make -f makefile.unix
$ cd examples/
  # fetch mistymntncop's proof-of-concept code
$ wget https://raw.githubusercontent.com/mistymntncop/CVE-2023-4863/main/craft.c
  # build and run proof-of-concept
$ gcc -o craft craft.c
$ ./craft bad.webp
  # test trigger file
$ ./dwebp bad.webp -o test.png
=================================================================
==207551==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x626000002f28 at pc 0x56196a11635a bp 0x7ffd3e5cce90 sp 0x7ffd3e5cce80
WRITE of size 1 at 0x626000002f28 thread T0
#0 0x56196a116359 in BuildHuffmanTable (/home/isosceles/source/webp/webp_test/examples/dwebp+0xb6359)
#1 0x56196a1166e7 in VP8LBuildHuffmanTable (/home/isosceles/source/webp/webp_test/examples/dwebp+0xb66e7)
#2 0x56196a0956ff in ReadHuffmanCode (/home/isosceles/source/webp/webp_test/examples/dwebp+0x356ff)
#3 0x56196a09a2b5 in DecodeImageStream (/home/isosceles/source/webp/webp_test/examples/dwebp+0x3a2b5)
#4 0x56196a09e216 in VP8LDecodeHeader (/home/isosceles/source/webp/webp_test/examples/dwebp+0x3e216)
#5 0x56196a0a011b in DecodeInto (/home/isosceles/source/webp/webp_test/examples/dwebp+0x4011b)
#6 0x56196a0a2f06 in WebPDecode (/home/isosceles/source/webp/webp_test/examples/dwebp+0x42f06)
#7 0x56196a06c026 in main (/home/isosceles/source/webp/webp_test/examples/dwebp+0xc026)
#8 0x7f7ea8a8c082 in __libc_start_main ../csu/libc-start.c:308
#9 0x56196a06e09d in _start (/home/isosceles/source/webp/webp_test/examples/dwebp+0xe09d)
0x626000002f28 is located 0 bytes to the right of 11816-byte region [0x626000000100,0x626000002f28)
allocated by thread T0 here:
#0 0x7f7ea8f2d808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144
#1 0x56196a09a0eb in DecodeImageStream (/home/isosceles/source/webp/webp_test/examples/dwebp+0x3a0eb)
SUMMARY: AddressSanitizer: heap-buffer-overflow (/home/isosceles/source/webp/webp_test/examples/dwebp+0xb6359) in BuildHuffmanTable
...</code></pre><p>In practice there are many such inputs that will overflow huffman_tables. I've found code lengths that result in writes as far as 400 bytes past the end of the huffman_tables allocation. Even with only partial control of the value being written, it definitely looks exploitable. To exploit this issue you would likely need to use the color cache bits (or num_htree_groups) to get a huffman_tables allocation that is roughly page aligned, but that shouldn't be a problem. It may be that there are other ways of causing an OOB write on the huffman_tables allocation, but this method looks like an acceptable approach. </p><p>The invalid input itself is quite unusual -- mistymntncop provided the following visualization of the Huffman tree it creates using a <a href="https://github.com/mistymntncop/CVE-2023-4863/blob/main/print_tree.c?ref=blog.isosceles.com">tool</a> they wrote to assist in this analysis:</p><figure><img src="https://blog.isosceles.com/content/images/2023/09/webp_a.jpg" alt="" loading="lazy" width="2000" height="625" srcset="https://blog.isosceles.com/content/images/size/w600/2023/09/webp_a.jpg 600w, https://blog.isosceles.com/content/images/size/w1000/2023/09/webp_a.jpg 1000w, https://blog.isosceles.com/content/images/size/w1600/2023/09/webp_a.jpg 1600w, https://blog.isosceles.com/content/images/2023/09/webp_a.jpg 2048w" sizes="(min-width: 720px) 720px"></figure><p>If you zoom in, you can see that the tree is partially unbalanced, and that a section of the unbalanced branch has a large number of internal nodes with no children in them at all. This structure results in a "key" index that a valid tree would never be able to reach. Here's what a valid tree looks like:</p><figure><img src="https://blog.isosceles.com/content/images/2023/09/webp_b.jpg" alt="" loading="lazy" width="2000" height="1518" srcset="https://blog.isosceles.com/content/images/size/w600/2023/09/webp_b.jpg 600w, https://blog.isosceles.com/content/images/size/w1000/2023/09/webp_b.jpg 1000w, https://blog.isosceles.com/content/images/size/w1600/2023/09/webp_b.jpg 1600w, https://blog.isosceles.com/content/images/2023/09/webp_b.jpg 2048w" sizes="(min-width: 720px) 720px"></figure><p>As for the patch, it seems to work almost by accident. As mentioned earlier, the patched version does a first pass with BuildHuffmanTable to calculate the total size required. In practice, this issue is patched because BuildHuffmanTable will fail (return 0) for all of the invalid inputs that would otherwise have resulted in an out-of-bounds write, and since the first pass is explicitly not writing to the table, it doesn't matter that the invalid tree is partially processed. In other words, I thought the patch was dynamically increasing the size of the buffer as needed to prevent heap overflow, but it's actually just denying the inputs that would cause a heap overflow instead. It's definitely hard to reason about, but I searched for "valid and complete" codes that would still trigger this overflow, and I couldn't find any. So it looks like the patch should be sufficient.</p><h2 id="early-discovery">Early Discovery?</h2><p>Immediately after Chrome's security update, there was some discussion about fuzzing. A binary file format implemented by a C code library is an ideal target for fuzzing -- so why hadn't this bug been found earlier? Had the library not been fuzzed enough? Or had it not been fuzzed right?</p><p>Google's OSS-Fuzz project has fuzzed hundreds of open source libraries for many years now, including libwebp and many other image decoding libraries. It's possible to look in <a href="https://storage.googleapis.com/oss-fuzz-coverage/libwebp/reports/20230901/linux/src/libwebp/src/utils/report.html?ref=blog.isosceles.com">full detail</a> at the code coverage for OSS-Fuzz projects, and it's clear that lossless support for WebP was being fuzzed extensively: </p><figure><img src="https://lh4.googleusercontent.com/QSgfxn8AQf2OzBIZuM3D6WYjT3lMFV9hFfP5zzfCTFZb6ekL1yabmrEo9QYn2qFhgnd7fBDneL6Jfei58v06VjUCSfsCL8AQW8Lj_QLVr5dz0dk2kyOU4OQQX7KqMuMRih0IhhLI_6YzrDzzY1A6dt4" alt="" loading="lazy" width="406" height="341"></figure><p>The problem, we now know, is that this format is incredibly complex and fragile, and the preconditions to trigger this issue are immense. Out of billions of possibilities, we have to construct a sequence of 4 valid Huffman tables that are maximally sized for two different alphabet sizes (280 and 256) before constructing a very specific type of invalid Huffman table for a third alphabet size (40). If a single bit is wrong at any stage, the image decoder throws an error and nothing bad happens.</p><p>In fact one of the first things that Google did after the WebP 0day was fixed was to release a new fuzzer specifically for the Huffman routines in WebP. I tried running this fuzzer for a bit (with a bit of backporting required due to API changes) and it predictably did not find CVE-2023-4863.</p><p>Perhaps I'm wrong and some of the newer techniques involving symbolic execution (like Quarkslab's <a href="https://blog.quarkslab.com/introducing-tritondse-a-framework-for-dynamic-symbolic-execution-in-python.html?ref=blog.isosceles.com">TritonDSE</a>) would be able to solve this -- but standard approaches based on bitflip mutations with a code-coverage feedback loop, and even slightly more sophisticated approaches like <a href="https://github.com/AFLplusplus/AFLplusplus/blob/stable/instrumentation/README.cmplog.md?ref=blog.isosceles.com">CmpLog</a> (input-to-state), would not be able to navigate through all of these intermediary steps to reach this extremely pessimal state. </p><p>It's interesting to contrast this bug with an earlier vulnerability, the <a href="https://googleprojectzero.github.io/0days-in-the-wild/0day-RCAs/2020/CVE-2020-15999.html?ref=blog.isosceles.com">Load_SBit_Png</a> bug in FreeType, which was also discovered "in the wild" in an advanced 0day exploit. It's similar in the sense of being a heap overflow in a common library for a binary file format (for fonts in this instance) written in C, it's similar that it affected Chrome, and it's similar in the sense that FreeType had been heavily fuzzed in the months and years leading up to this attack. The difference was that the Load_SBit_Png bug wasn't found during fuzzing due to a lack of adequate harnessing, rather than some specific constraint of the vulnerability that made it difficult to fuzz. If the fuzzing harnesses had been updated earlier to better reflect the APIs usage, the Load_SBit_Png bug would have been discovered with fuzzing.</p><p>That's not the case for the WebP 0day (CVE-2023-4863) -- unless, perhaps, you got incredibly lucky by having a file in your fuzzing corpus that was already extremely close to the bug and your fuzzer was very well calibrated in terms of its mutation rates. </p><p>In practice, I suspect this bug was discovered through manual code review. In reviewing the code, you &nbsp;would see the huffman_tables allocation being made during header parsing of a VP8L file, so naturally you would look to see how it's used. You would then try to rationalize the lack of bounds checks on the huffman_tables allocation, and if you're persistent enough, you would progressively go deeper and deeper into the problem before realizing that the code was subtly broken. I suspect that most code auditors aren't that persistent though -- this Huffman code stuff is mind bending -- so I'm impressed.</p><h2 id="whats-the-big-deal">What's The Big Deal</h2><p>There's some good news, and some bad news. </p><p>✓ The good news is that the team at Citizen Lab has, once again, done an amazing job of catching a top tier exploit being used in the wild. They have cultivated a lot of trust with the organizations and individuals that are most likely to be harmed by exploits. It's very impressive.</p><p>✗ The bad news is that exploits like this continue to have societal ramifications, and we can only guess how bad the situation really is. The truth is that nobody knows for sure, even the people with exploits.</p><p>✓ The good news is that Apple and Chrome did an amazing job at responding to this issue with the urgency that it deserves. It looks like both groups pushed out an update to their <em>billions</em> of users in just a number of days. That's an impressive feat, it takes an incredible effort and coordination across threat analysis, security engineering, software engineering, product management, and testing teams to make this even remotely possible.</p><p>✗ The bad news is that Android is still likely affected. Similar to Apple's ImageIO, Android has a facility called the <a href="https://developer.android.com/reference/android/graphics/BitmapFactory?ref=blog.isosceles.com">BitmapFactory</a> that handles image decoding, and of course libwebp is supported. As of today, Android hasn't released a security bulletin that includes a fix for CVE-2023-4863 -- although the fix has been merged into AOSP. To put this in context: if this bug does affect Android, then it could potentially be turned into a remote exploit for apps like Signal and WhatsApp. I'd expect it to be fixed in the October bulletin.</p><p>✓ The good news is that the bug seems to be patched correctly in the upstream libwebp, and that patch is making its way to everywhere it should go. </p><p>✗ The bad news is that libwebp is used in a lot of places, and it could be a while until the patch reaches saturation. Also, the code is still very difficult to reason about, and we can't rely on fuzzers to find any other bugs that are lurking here.</p><h2 id="final-thoughts">Final Thoughts</h2><p>The WebP 0day (CVE-2023-4863) is a subtle but powerful vulnerability in a widely used open source library that is highly exposed to attacker inputs. It's both very difficult to fuzz, and very difficult to manually trigger -- but the prize is an exploitable heap overflow that works on multiple browsers, operating systems, and applications. It's likely that CVE-2023-4863 is the same vulnerability used in the <a href="https://citizenlab.ca/2023/09/blastpass-nso-group-iphone-zero-click-zero-day-exploit-captured-in-the-wild/?ref=blog.isosceles.com">BLASTPASS</a> attacks.</p><p>I started this technical analysis shortly after releasing last week's <a href="https://blog.isosceles.com/phineas-fisher-hacktivism-and-magic-tricks/">blog post on Phineas Fisher</a>, which means I was several days late to the party. In practice it took about 3 full work days worth of work (with a lot of additional help from <a href="https://twitter.com/mistymntncop?ref=blog.isosceles.com">@mistymntncop</a>) to figure out the bug and build a reproducing testcase. </p><p>The lack of available technical information from the vendors here made verification challenging, and it's questionable who this really benefits. Attackers are <a href="https://googleprojectzero.blogspot.com/2023/09/analyzing-modern-in-wild-android-exploit.html?ref=blog.isosceles.com">clearly highly motivated</a> to track and exploit N-day vulnerabilities, and the lack of technical details being released won't significantly slow them down. On the other hand, very few defenders are resourced to be able to perform the type of technical analysis I've shared today. It's counter-intuitive, but withholding basic technical details about how these attacks are working in an asymmetry that mostly benefits attackers -- you quickly end up in a situation where attackers have access to insights about the vulnerability/exploit that defenders don't have.</p><p>This bug also shows that we have an over-reliance on fuzzing for security assurance of complex parser code. Fuzzing is great, but we know that there are many serious security issues that aren't easy to fuzz. For sensitive attack surfaces like image decoding (zero-click remote exploit attack surface), there needs to 1) be a bigger investment in proactive source code reviews, and 2) a renewed focus on ensuring these parsers are adequately sandboxed.</p><p>Finally, thanks again to <a href="https://twitter.com/mistymntncop?ref=blog.isosceles.com">@mistymntncop</a> for both their encouragement and huge technical contributions to this post.</p>
                    
                </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[USPS In-Person Identity Proofing (161 pts)]]></title>
            <link>https://faq.usps.com/s/article/USPS-In-Person-Identity-Proofing</link>
            <guid>37600618</guid>
            <pubDate>Thu, 21 Sep 2023 17:06:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://faq.usps.com/s/article/USPS-In-Person-Identity-Proofing">https://faq.usps.com/s/article/USPS-In-Person-Identity-Proofing</a>, See on <a href="https://news.ycombinator.com/item?id=37600618">Hacker News</a></p>
Couldn't get https://faq.usps.com/s/article/USPS-In-Person-Identity-Proofing: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Bloomberg Is Throwing $500M at Efforts to Shut Down All U.S. Coal Plants (146 pts)]]></title>
            <link>https://gizmodo.com/michael-bloomberg-500-million-shut-down-coal-plants-1850861082</link>
            <guid>37600484</guid>
            <pubDate>Thu, 21 Sep 2023 16:57:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/michael-bloomberg-500-million-shut-down-coal-plants-1850861082">https://gizmodo.com/michael-bloomberg-500-million-shut-down-coal-plants-1850861082</a>, See on <a href="https://news.ycombinator.com/item?id=37600484">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Billionaire, philanthropist, and former NYC Mayor Michael Bloomberg <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.bloomberg.org/press/michael-r-bloomberg-doubles-down-with-additional-500m-to-help-end-fossil-fuels-and-usher-in-a-new-era-of-clean-energy-in-the-united-states/#:~:text=Starting%20with%20the%20Beyond%20Coal,plants%2C%20stopping%20the%20expansion%20of&quot;,{&quot;metric25&quot;:1}]]" href="https://www.bloomberg.org/press/michael-r-bloomberg-doubles-down-with-additional-500m-to-help-end-fossil-fuels-and-usher-in-a-new-era-of-clean-energy-in-the-united-states/#:~:text=Starting%20with%20the%20Beyond%20Coal,plants%2C%20stopping%20the%20expansion%20of" target="_blank" rel="noopener noreferrer">announced this week</a></span> that he will invest $500 million into his campaign to shut down coal plants and <!-- -->halve<!-- --> gas use<!-- --> by 2030.</p><div data-video-id="193562" data-monetizable="true" data-position="sidebar" data-video-title="What Is Carbon Capture? With Gizmodo’s Molly Taft | Techmodo" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="424" data-playlist="193562,195689,195681" data-current="193562"><div><p>What Is Carbon Capture? With Gizmodo’s Molly Taft | Techmodo</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/193562/193562_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/193562/193562_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/193562/193562_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/193562/193562_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/19012.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>Through the Beyond Carbon campaign, Bloomberg has successfully helped shut down about 70% of all coal plants in the U.S. This new push is intended to shut down the remaining 150 coal plants. The Beyond Carbon initiative also aims to work with a range of local and state organizations to block the construction of new gas plants. </p><p>The financing is intended to support research, including studies and analysis to deliver accurate data to partnering organizations and officials for better decision-making<!-- -->. It will also fund local policy and advocacy, along with litigation brought against power companies. </p><p>According to a press release from Bloomberg Philanthropies, this push to shut down coal-fired<!-- --> power plants will push officials to invest in renewable energy and clean jobs. “Our climate is warming at a breakneck pace, and there’s more urgency than ever to cut emissions from fossil fuels &amp; move the U.S. faster toward a clean energy future,” Bloomberg <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://twitter.com/MikeBloomberg/status/1704629062763692184&quot;,{&quot;metric25&quot;:1}]]" href="https://twitter.com/MikeBloomberg/status/1704629062763692184" target="_blank" rel="noopener noreferrer">tweeted yesterday</a></span>, referring to the recent announcement.</p><p>Several organizations that have worked closely with the Beyond Carbon campaign have lauded Bloomberg’s efforts and financing. “Combatting the climate crisis is the most critical fight of our time,” Ben Jealous, the executive director of the Sierra Club, said in a press release. “We must transition from fossil fuels to clean energy if we want to protect our health, our environment, and our children, and we must do so in a way that empowers local communities and prioritizes environmental justice.”</p><p>When the Beyond Carbon campaign was first launched, the goal was to retire about 30% of coal plants by 2020. But with the support of environmental groups nationwide, and an early $500 million from Bloomberg, the campaign managed to shut down more than half of the nation’s coal plants by 2022, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.reuters.com/business/energy/michael-bloomberg-pumps-500-million-into-bid-close-all-us-coal-plants-2023-09-20/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.reuters.com/business/energy/michael-bloomberg-pumps-500-million-into-bid-close-all-us-coal-plants-2023-09-20/" target="_blank" rel="noopener noreferrer">Reuters reported</a></span>. </p><p>Bloomberg had long championed environmental and climate causes. When he was the mayor of New York, Bloomberg took the subway regularly—by way of SUV, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://gothamist.com/news/mayor-bloombergs-subway-commute-not-like-yours&quot;,{&quot;metric25&quot;:1}]]" href="https://gothamist.com/news/mayor-bloombergs-subway-commute-not-like-yours" target="_blank" rel="noopener noreferrer">Gothamist reported back in 2007</a></span>. He also pushed for more biking infrastructure in the city, and the Citi Bike system was launched in 2013.</p><p>Bloomberg has also financed other environmental campaigns, including one to stop new petrochemical plants that produce packaging, plastics, and fertilizers, The New York Times<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nytimes.com/2023/09/20/climate/michael-blooomberg-climate-petrochemicals.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nytimes.com/2023/09/20/climate/michael-blooomberg-climate-petrochemicals.html" target="_blank" rel="noopener noreferrer"> reported</a></span>. He launched a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.bloomberg.org/press/michael-r-bloomberg-launches-new-85-million-campaign-to-stop-rapid-rise-of-pollution-from-the-petrochemical-industry-in-the-united-states/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.bloomberg.org/press/michael-r-bloomberg-launches-new-85-million-campaign-to-stop-rapid-rise-of-pollution-from-the-petrochemical-industry-in-the-united-states/" target="_blank" rel="noopener noreferrer">$85 million campaign</a></span> last year to support the new Beyond Petrochemicals campaign with the goal to “block the expansion of more than 120 proposed petrochemical projects” in Louisiana, the Ohio River Valley, and Texas. </p><p><em>Want more climate and environment stories? Check out Earther’s guides to </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/what-s-the-best-way-to-decarbonize-your-home-1847518817&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/what-s-the-best-way-to-decarbonize-your-home-1847518817" target="_blank"><em>decarbonizing your home</em></a></span><em>, </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/how-can-i-divest-from-fossil-fuels-1847774633&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/how-can-i-divest-from-fossil-fuels-1847774633" target="_blank"><em>divesting from fossil fuels</em></a></span><em>, </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/how-to-pack-a-go-bag-climate-disasters-wildfires-floods-1849457027&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/how-to-pack-a-go-bag-climate-disasters-wildfires-floods-1849457027" target="_blank"><em>packing a disaster go bag</em></a></span><em>, and </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/how-can-you-overcome-climate-dread-1847606185&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/how-can-you-overcome-climate-dread-1847606185" target="_blank"><em>overcoming climate dread</em></a></span><em>. And don’t miss our coverage of the </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/ipcc-report-2023-climate-change-paris-agreement-un-1850242687&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/ipcc-report-2023-climate-change-paris-agreement-un-1850242687" target="_blank"><em>latest IPCC climate report</em></a></span><em>, the future of </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/future-of-carbon-dioxide-removal-frontier-project-1848782278&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/future-of-carbon-dioxide-removal-frontier-project-1848782278" target="_blank"><em>carbon dioxide removal</em></a></span><em>, and the un-greenwashed facts on </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/what-is-bioplastic-biodegradable-plant-based-plastic-1848999921&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/what-is-bioplastic-biodegradable-plant-based-plastic-1848999921" target="_blank"><em>bioplastics</em></a></span><em> and </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/least-recyclable-plastics-1848853267&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/least-recyclable-plastics-1848853267" target="_blank"><em>plastic recycling</em></a></span><em>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My Single-File Python Script I Used to Replace Splunk in My Startup (271 pts)]]></title>
            <link>https://github.com/Dicklesworthstone/automatic_log_collector_and_analyzer</link>
            <guid>37600019</guid>
            <pubDate>Thu, 21 Sep 2023 16:26:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Dicklesworthstone/automatic_log_collector_and_analyzer">https://github.com/Dicklesworthstone/automatic_log_collector_and_analyzer</a>, See on <a href="https://news.ycombinator.com/item?id=37600019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-automatically-download-and-analyze-log-files-from-remote-machines" dir="auto"><a href="#automatically-download-and-analyze-log-files-from-remote-machines">Automatically Download and Analyze Log Files from Remote Machines</a></h2>
<p dir="auto">This application is designed to collect and analyze logs from remote machines hosted on Amazon Web Services (AWS) and other cloud hosting services.</p>
<p dir="auto"><strong>Note</strong>: This application was specifically designed for use with Pastel Network's log files. However, it can be easily adapted to work with any log files by modifying the parsing functions, data models, and specifying the location and names of the log files to be downloaded. It is compatible with log files stored in a standard format, where each entry is on a separate line and contains a timestamp, a log level, and a message. The application has been tested with log files several gigabytes in size from dozens of machines and can process all of it in minutes. It is designed for Ubuntu 22.04+, but can be adapted for other Linux distributions.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Dicklesworthstone/automatic_log_collector_and_analyzer/main/demo_screenshot.png"><img src="https://raw.githubusercontent.com/Dicklesworthstone/automatic_log_collector_and_analyzer/main/demo_screenshot.png" alt="Demo Screenshot:"></a></p>
<h2 tabindex="-1" id="user-content-customization" dir="auto"><a href="#customization">Customization</a></h2>
<p dir="auto">To adapt this application for your own use case, refer to the included sample log files and compare them to the parsing functions in the code. You can also modify the data models to store log entries as desired.</p>
<h2 tabindex="-1" id="user-content-features" dir="auto"><a href="#features">Features</a></h2>
<p dir="auto">The application consists of various Python scripts that perform the following functions:</p>
<ul dir="auto">
<li><strong>Connect to Remote Machines</strong>: Using the boto3 library for AWS instances and an Ansible inventory file for non-AWS instances, the application establishes SSH connections to each remote machine.</li>
<li><strong>Download and Parse Log Files</strong>: Downloads specified log files from each remote machine and parses them. The parsed log entries are then queued for database insertion.</li>
<li><strong>Insert Log Entries into Database</strong>: Uses SQLAlchemy to insert the parsed log entries from the queue into an SQLite database.</li>
<li><strong>Process and Analyze Log Entries</strong>: Processes and analyzes log entries stored in the database, offering functions to find error entries and create views of aggregated data based on specified criteria.</li>
<li><strong>Generate Network Activity Data</strong>: Fetches and processes network activity data from each remote machine.</li>
<li><strong>Expose Database via Web App using Datasette</strong>: Once the database is generated, it can be shared over the web using Datasette.</li>
</ul>
<h2 tabindex="-1" id="user-content-compatibility" dir="auto"><a href="#compatibility">Compatibility</a></h2>
<p dir="auto">The tool is compatible with both AWS-hosted instances and any list of Linux instances stored in a standard Ansible inventory file with the following structure:</p>
<div dir="auto" data-snippet-clipboard-copy-content="all:
  vars:
    ansible_connection: ssh
    ansible_user: ubuntu
    ansible_ssh_private_key_file: /path/to/ssh/key/file.pem
  hosts:
    MyCoolMachine01:
      ansible_host: 1.2.3.41
    MyCoolMachine02:
      ansible_host: 1.2.3.41.19"><pre><span>all</span>:
  <span>vars</span>:
    <span>ansible_connection</span>: <span>ssh</span>
    <span>ansible_user</span>: <span>ubuntu</span>
    <span>ansible_ssh_private_key_file</span>: <span>/path/to/ssh/key/file.pem</span>
  <span>hosts</span>:
    <span>MyCoolMachine01</span>:
      <span>ansible_host</span>: <span>1.2.3.41</span>
    <span>MyCoolMachine02</span>:
      <span>ansible_host</span>: <span>1.2.3.41.19</span></pre></div>
<p dir="auto">(Both can be used seamlessly.)</p>
<h2 tabindex="-1" id="user-content-warning" dir="auto"><a href="#warning">Warning</a></h2>
<p dir="auto">To simplify the code, the tool is designed to delete all downloaded log files and generated databases each time it runs. Consequently, this can consume significant bandwidth depending on your log files' size. However, the design's high level of parallel processing and concurrency allows it to run quickly, even when connecting to dozens of remote machines and downloading hundreds of log files.</p>
<h2 tabindex="-1" id="user-content-usage" dir="auto"><a href="#usage">Usage</a></h2>
<p dir="auto">Designed for Ubuntu 22.04+, first install the requirements:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m venv venv
source venv/bin/activate
python3 -m pip install --upgrade pip
python3 -m pip install wheel
pip install -r requirements.txt"><pre>python3 -m venv venv
<span>source</span> venv/bin/activate
python3 -m pip install --upgrade pip
python3 -m pip install wheel
pip install -r requirements.txt</pre></div>
<p dir="auto">You will also need to install Redis:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install redis -y"><pre>sudo apt install redis -y</pre></div>
<p dir="auto">And install Datasette to expose the results as a website:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install pipx -y &amp;&amp; pipx ensurepath &amp;&amp; pipx install datasette"><pre>sudo apt install pipx -y <span>&amp;&amp;</span> pipx ensurepath <span>&amp;&amp;</span> pipx install datasette</pre></div>
<p dir="auto">To run the application every 30 minutes as a cron job, execute:</p>

<p dir="auto">And add the following line:</p>
<div dir="auto" data-snippet-clipboard-copy-content="*/15 * * * * . $HOME/.profile; /home/ubuntu/automatic_log_collector_and_analyzer/venv/bin/python /home/ubuntu/automatic_log_collector_and_analyzer/automatic_log_collector_and_analyzer.py >> /home/ubuntu/automatic_log_collector_and_analyzer/log_$(date +\%Y-\%m-\%dT\%H_\%M_\%S).log 2>&amp;1"><pre><span>*</span>/15 <span>*</span> <span>*</span> <span>*</span> <span>*</span> <span>.</span> <span>$HOME</span>/.profile<span>;</span> /home/ubuntu/automatic_log_collector_and_analyzer/venv/bin/python /home/ubuntu/automatic_log_collector_and_analyzer/automatic_log_collector_and_analyzer.py <span>&gt;&gt;</span> /home/ubuntu/automatic_log_collector_and_analyzer/log_<span><span>$(</span>date +<span>\%</span>Y-<span>\%</span>m-<span>\%</span>dT<span>\%</span>H_<span>\%</span>M_<span>\%</span>S<span>)</span></span>.log <span>2&gt;&amp;1</span></pre></div>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RAG is more than just embedding search (140 pts)]]></title>
            <link>https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/</link>
            <guid>37599873</guid>
            <pubDate>Thu, 21 Sep 2023 16:18:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/">https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/</a>, See on <a href="https://news.ycombinator.com/item?id=37599873">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
      
      <main data-md-component="main">
        <div data-md-component="content">
    
    <article>
      
        

  
  



<p>With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of <a href="https://jxnl.notion.site/Working-with-me-ec2bb36a5ac048c2a8f6bd888faea6c2?pvs=4">helping startups</a> integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.</p>
<div>
<p>What is RAG?</p>
<p>Retrival augmented generation (RAG) is a technique that uses a LLM to generate responses, but uses a search backend to augment the generation, in the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized.</p>
</div>
<figure>
<p><img alt="RAG" src="https://jxnl.github.io/instructor/blog/img/dumb_rag.png">
  </p>
<figcaption>Simple RAG that embedded the user query and makes a search.</figcaption>
</figure>
<p>So let's kick things off by examining what I like to call the 'Dumb' RAG Model—a basic setup that's more common than you'd think.</p>
<h2 id="the-dumb-rag-model">The 'Dumb' RAG Model</h2>
<p>When you ask a question like, "what is the capital of France?" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like <code>search(query: str) -&gt; List[str]</code>. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings.</p>
<h3 id="why-is-this-a-problem">Why is this a problem?</h3>
<ul>
<li>
<p><strong>Query-Document Mismatch</strong>: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation!</p>
</li>
<li>
<p><strong>Monolithic Search Backend</strong>: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more.</p>
</li>
<li>
<p><strong>Limitation of text search</strong>: Restricts complex queries to a single string (<code>{query: str}</code>), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, <code>what problems did we fix last week</code> that cannot be answered by a simple text search, since documents that contain <code>problem, last week</code> are going to be present at every week.</p>
</li>
<li>
<p><strong>Limited ability to plan</strong>: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results.</p>
</li>
</ul>
<p>Now let's dive into how we can make it smarter with query understanding. This is where things get interesting.</p>
<h2 id="improving-the-rag-model-with-query-understanding">Improving the RAG Model with Query Understanding</h2>

<p>Ultimately what you want to deploy is a <a href="https://en.wikipedia.org/wiki/Query_understanding">system that understands</a> how to take the query and rewrite it to improve precision and recall. </p>
<figure>
<p><img alt="RAG" src="https://jxnl.github.io/instructor/blog/img/query_understanding.png">
  </p>
<figcaption>Query Understanding system routes to multiple search backends.</figcaption>
</figure>
<p>Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems.</p>
<h2 id="whats-instructor">Whats instructor?</h2>
<p>Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling api..</p>
<ul>
<li><strong>Widespread Adoption</strong>: Pydantic is a popular tool among Python developers.</li>
<li><strong>Simplicity</strong>: Pydantic allows model definition in Python.</li>
<li><strong>Framework Compatibility</strong>: Many Python frameworks already use Pydantic.</li>
</ul>

<p>Take <a href="https://metaphor.systems/">Metaphor Systems</a>, which turns natural language queries into their custom search-optimized query. If you take a look web ui you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using an language model, and turn it into a fully specified metaphor systems query.</p>
<figure>
<p><img alt="Metaphor Systems" src="https://jxnl.github.io/instructor/blog/img/meta.png"></p>
<figcaption>Metaphor Systems UI</figcaption>
</figure>
<p>If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. Its actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library</p>
<div><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span>class</span> <span>DateRange</span><span>(</span><span>BaseModel</span><span>):</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span>start</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span>end</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span>class</span> <span>MetaphorQuery</span><span>(</span><span>BaseModel</span><span>):</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span>rewritten_query</span><span>:</span> <span>str</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span>published_daterange</span><span>:</span> <span>DateRange</span>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span>domains_allow_list</span><span>:</span> <span>List</span><span>[</span><span>str</span><span>]</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span>async</span> <span>def</span> <span>execute</span><span>():</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>        <span>return</span> <span>await</span> <span>metaphor</span><span>.</span><span>search</span><span>(</span><span>...</span><span>)</span>
</span></code></pre></div>
<p>Note how we model a rewritten query, range of published dates, and a list of domains to search in. This is a powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works. </p>
<div><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span>import</span> <span>instructor</span> 
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span>import</span> <span>openai</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span># Enables response_model in the openai client</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span>instructor</span><span>.</span><span>patch</span><span>()</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span>query</span> <span>=</span> <span>openai</span><span>.</span><span>ChatCompletion</span><span>.</span><span>create</span><span>(</span>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>    <span>model</span><span>=</span><span>"gpt-4"</span><span>,</span>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    <span>response_model</span><span>=</span><span>MetaphorQuery</span><span>,</span>
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>    <span>messages</span><span>=</span><span>[</span>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>        <span>{</span>
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>            <span>"role"</span><span>:</span> <span>"system"</span><span>,</span> 
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>            <span>"content"</span><span>:</span> <span>"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ..."</span>
</span><span id="__span-1-14"><a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>        <span>},</span>
</span><span id="__span-1-15"><a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>        <span>{</span>
</span><span id="__span-1-16"><a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>            <span>"role"</span><span>:</span> <span>"user"</span><span>,</span> 
</span><span id="__span-1-17"><a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>            <span>"content"</span><span>:</span> <span>"What are some recent developments in AI?"</span>
</span><span id="__span-1-18"><a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>        <span>}</span>
</span><span id="__span-1-19"><a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>    <span>],</span>
</span><span id="__span-1-20"><a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a><span>)</span>
</span></code></pre></div>
<p><strong>Example Output</strong></p>
<div><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span>{</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span>    </span><span>"rewritten_query"</span><span>:</span><span> </span><span>"novel developments advancements ai artificial intelligence machine learning"</span><span>,</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span>    </span><span>"published_daterange"</span><span>:</span><span> </span><span>{</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span>        </span><span>"start"</span><span>:</span><span> </span><span>"2023-09-17"</span><span>,</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span>        </span><span>"end"</span><span>:</span><span> </span><span>"2021-06-17"</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span>    </span><span>},</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span>    </span><span>"domains_allow_list"</span><span>:</span><span> </span><span>[</span><span>"arxiv.org"</span><span>]</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a><span>}</span>
</span></code></pre></div>
<p>This isn't just about adding some date ranges. It's about nuanced, tailored searches, that is deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features.</p>
<div><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span>class</span> <span>DateRange</span><span>(</span><span>BaseModel</span><span>):</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>    <span>start</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>    <span>end</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>    <span>chain_of_thought</span><span>:</span> <span>str</span> <span>=</span> <span>Field</span><span>(</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>        <span>None</span><span>,</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>        <span>description</span><span>=</span><span>"Think step by step to plan what is the best time range to search in"</span>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>    <span>)</span>
</span></code></pre></div>
<p>Now, let's see how this approach can help model an agent like personal assistant.</p>
<h2 id="case-study-2-personal-assistant">Case Study 2: Personal Assistant</h2>
<p>Another great example of this multiple dispatch pattern is a personal assistant. You might ask, "What do I have today?", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts.</p>
<div><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span>class</span> <span>ClientSource</span><span>(</span><span>enum</span><span>.</span><span>Enum</span><span>):</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>    <span>GMAIL</span> <span>=</span> <span>"gmail"</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>    <span>CALENDAR</span> <span>=</span> <span>"calendar"</span>
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>
</span><span id="__span-4-5"><a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a><span>class</span> <span>SearchClient</span><span>(</span><span>BaseModel</span><span>):</span>
</span><span id="__span-4-6"><a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>    <span>query</span><span>:</span> <span>str</span>
</span><span id="__span-4-7"><a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>    <span>keywords</span><span>:</span> <span>List</span><span>[</span><span>str</span><span>]</span>
</span><span id="__span-4-8"><a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a>    <span>email</span><span>:</span> <span>str</span>
</span><span id="__span-4-9"><a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a>    <span>source</span><span>:</span> <span>ClientSource</span>
</span><span id="__span-4-10"><a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a>    <span>start_date</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-4-11"><a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a>    <span>end_date</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-4-12"><a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a>
</span><span id="__span-4-13"><a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a>    <span>async</span> <span>def</span> <span>execute</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
</span><span id="__span-4-14"><a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a>        <span>if</span> <span>self</span><span>.</span><span>source</span> <span>==</span> <span>ClientSource</span><span>.</span><span>GMAIL</span><span>:</span>
</span><span id="__span-4-15"><a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a>            <span>...</span>
</span><span id="__span-4-16"><a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a>        <span>elif</span> <span>self</span><span>.</span><span>source</span> <span>==</span> <span>ClientSource</span><span>.</span><span>CALENDAR</span><span>:</span>
</span><span id="__span-4-17"><a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a>            <span>...</span>
</span><span id="__span-4-18"><a id="__codelineno-4-18" name="__codelineno-4-18" href="#__codelineno-4-18"></a>
</span><span id="__span-4-19"><a id="__codelineno-4-19" name="__codelineno-4-19" href="#__codelineno-4-19"></a><span>class</span> <span>Retrival</span><span>(</span><span>BaseModel</span><span>):</span>
</span><span id="__span-4-20"><a id="__codelineno-4-20" name="__codelineno-4-20" href="#__codelineno-4-20"></a>    <span>queries</span><span>:</span> <span>List</span><span>[</span><span>SearchClient</span><span>]</span>
</span><span id="__span-4-21"><a id="__codelineno-4-21" name="__codelineno-4-21" href="#__codelineno-4-21"></a>
</span><span id="__span-4-22"><a id="__codelineno-4-22" name="__codelineno-4-22" href="#__codelineno-4-22"></a>    <span>async</span> <span>def</span> <span>execute</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
</span><span id="__span-4-23"><a id="__codelineno-4-23" name="__codelineno-4-23" href="#__codelineno-4-23"></a>        <span>return</span> <span>await</span> <span>asyncio</span><span>.</span><span>gather</span><span>(</span><span>*</span><span>[</span><span>query</span><span>.</span><span>execute</span><span>()</span> <span>for</span> <span>query</span> <span>in</span> <span>self</span><span>.</span><span>queries</span><span>])</span>
</span></code></pre></div>
<p>Now we can call this with a simple query like "What do I have today?" and it will try to async dispatch to the correct backend. Its will important to prompt the language model well, but we'll leave that for another day.</p>
<div><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span>import</span> <span>instructor</span> 
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span>import</span> <span>openai</span>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a><span># Enables response_model in the openai client</span>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span>instructor</span><span>.</span><span>patch</span><span>()</span>
</span><span id="__span-5-6"><a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>
</span><span id="__span-5-7"><a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span>retrival</span> <span>=</span> <span>openai</span><span>.</span><span>ChatCompletion</span><span>.</span><span>create</span><span>(</span>
</span><span id="__span-5-8"><a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>    <span>model</span><span>=</span><span>"gpt-4"</span><span>,</span>
</span><span id="__span-5-9"><a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>    <span>response_model</span><span>=</span><span>Retrival</span><span>,</span>
</span><span id="__span-5-10"><a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a>    <span>messages</span><span>=</span><span>[</span>
</span><span id="__span-5-11"><a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a>        <span>{</span><span>"role"</span><span>:</span> <span>"system"</span><span>,</span> <span>"content"</span><span>:</span> <span>"You are Jason's personal assistant."</span><span>},</span>
</span><span id="__span-5-12"><a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a>        <span>{</span><span>"role"</span><span>:</span> <span>"user"</span><span>,</span> <span>"content"</span><span>:</span> <span>"What do I have today?"</span><span>}</span>
</span><span id="__span-5-13"><a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a>    <span>],</span>
</span><span id="__span-5-14"><a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a><span>)</span>
</span></code></pre></div>
<p><strong>Example Output</strong></p>
<div><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span>{</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span>    </span><span>"queries"</span><span>:</span><span> </span><span>[</span>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span>        </span><span>{</span>
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span>            </span><span>"query"</span><span>:</span><span> </span><span>No</span><span>ne</span><span>,</span>
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a><span>            </span><span>"keywords"</span><span>:</span><span> </span><span>No</span><span>ne</span><span>,</span>
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span>            </span><span>"email"</span><span>:</span><span> </span><span>"jason@example.com"</span><span>,</span>
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span>            </span><span>"source"</span><span>:</span><span> </span><span>"gmail"</span><span>,</span>
</span><span id="__span-6-8"><a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a><span>            </span><span>"start_date"</span><span>:</span><span> </span><span>"2023-09-17"</span><span>,</span>
</span><span id="__span-6-9"><a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a><span>            </span><span>"end_date"</span><span>:</span><span> </span><span>No</span><span>ne</span>
</span><span id="__span-6-10"><a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span>        </span><span>},</span>
</span><span id="__span-6-11"><a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a><span>        </span><span>{</span>
</span><span id="__span-6-12"><a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a><span>            </span><span>"query"</span><span>:</span><span> </span><span>No</span><span>ne</span><span>,</span>
</span><span id="__span-6-13"><a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a><span>            </span><span>"keywords"</span><span>:</span><span> </span><span>[</span><span>"meeting"</span><span>,</span><span> </span><span>"call"</span><span>,</span><span> </span><span>"zoom"</span><span>]]],</span>
</span><span id="__span-6-14"><a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a><span>            </span><span>"email"</span><span>:</span><span> </span><span>"jason@example.com"</span><span>,</span>
</span><span id="__span-6-15"><a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a><span>            </span><span>"source"</span><span>:</span><span> </span><span>"calendar"</span><span>,</span>
</span><span id="__span-6-16"><a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a><span>            </span><span>"start_date"</span><span>:</span><span> </span><span>"2023-09-17"</span><span>,</span>
</span><span id="__span-6-17"><a id="__codelineno-6-17" name="__codelineno-6-17" href="#__codelineno-6-17"></a><span>            </span><span>"end_date"</span><span>:</span><span> </span><span>No</span><span>ne</span>
</span><span id="__span-6-18"><a id="__codelineno-6-18" name="__codelineno-6-18" href="#__codelineno-6-18"></a>
</span><span id="__span-6-19"><a id="__codelineno-6-19" name="__codelineno-6-19" href="#__codelineno-6-19"></a><span>        </span><span>}</span>
</span><span id="__span-6-20"><a id="__codelineno-6-20" name="__codelineno-6-20" href="#__codelineno-6-20"></a><span>    </span><span>]</span>
</span><span id="__span-6-21"><a id="__codelineno-6-21" name="__codelineno-6-21" href="#__codelineno-6-21"></a><span>}</span>
</span></code></pre></div>
<p>Notice that we have a list of queries, that route to different search backends, email and calendar. We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well, perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app.</p>
<div>
<p>Can I used framework X?</p>
<p>I get this question many times, but its just code, within these dispatchs you can do whatever you want. You can use <code>input()</code> to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information, the sky is the limit.</p>
</div>
<p>Both of these examples show case how both search providors and consumers can use <code>instructor</code> to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build a LLM layer, from scratch, in front of any arbitrary backend.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This isnt about fancy embedding tricks, its just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place.</p>
<h2 id="whats-next">What's Next?</h2>
<p>Here I want to show that `instructor`` isn’t just about data extraction. It’s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning — the untapped goldmine is skilled use of tools and APIs.</p>
<p>I believe collaboration between domain experts and AI engineers the key to enable advanced tool use. I’ve been building a new tool on top of instructor that enables seamless collaboration and experimentation on LLMs with structured outputs. If you’re interested, visit <a href="https://useinstructor.com/">useinstructor.com</a> and take our survey to join the waitlist.
Together, let’s create tools that are as brilliant as the minds that use them.</p>


  


  



      
    </article>
  </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Insider trade on Splunk acquisition for 45,650% return (400 pts)]]></title>
            <link>https://twitter.com/unusual_whales/status/1704870849831125446</link>
            <guid>37599587</guid>
            <pubDate>Thu, 21 Sep 2023 15:58:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/unusual_whales/status/1704870849831125446">https://twitter.com/unusual_whales/status/1704870849831125446</a>, See on <a href="https://news.ycombinator.com/item?id=37599587">Hacker News</a></p>
Couldn't get https://twitter.com/unusual_whales/status/1704870849831125446: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Lead poisoning causes more death, IQ loss than thought: study (420 pts)]]></title>
            <link>https://medicalxpress.com/news/2023-09-poisoning-death-iq-loss-thought.html</link>
            <guid>37599542</guid>
            <pubDate>Thu, 21 Sep 2023 15:55:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2023-09-poisoning-death-iq-loss-thought.html">https://medicalxpress.com/news/2023-09-poisoning-death-iq-loss-thought.html</a>, See on <a href="https://news.ycombinator.com/item?id=37599542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2023/lead-poisoning-causes.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2023/lead-poisoning-causes.jpg" data-sub-html="Blood lead level IQ loss function from Crump and colleagues. The blood lead level is mean lifetime blood lead level in children younger than 5 years. The solid line is the central estimate and the shaded area is the 95% CI as per the study by Crump and colleagues. IQ=intelligence quotient. Credit: <i>The Lancet Planetary Health</i> (2023). DOI: 10.1016/S2542-5196(23)00166-3">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2023/lead-poisoning-causes.jpg" alt="Lead poisoning causes far more death, IQ loss than thought: study" title="Blood lead level IQ loss function from Crump and colleagues. The blood lead level is mean lifetime blood lead level in children younger than 5 years. The solid line is the central estimate and the shaded area is the 95% CI as per the study by Crump and colleagues. IQ=intelligence quotient. Credit: The Lancet Planetary Health (2023). DOI: 10.1016/S2542-5196(23)00166-3" width="800" height="355">
             <figcaption>
                Blood lead level IQ loss function from Crump and colleagues. The blood lead level is mean lifetime blood lead level in children younger than 5 years. The solid line is the central estimate and the shaded area is the 95% CI as per the study by Crump and colleagues. IQ=intelligence quotient. Credit: <i>The Lancet Planetary Health</i> (2023). DOI: 10.1016/S2542-5196(23)00166-3
            </figcaption>        </figure>
    </div>
<p>Lead poisoning has a far greater impact on global health than previously thought, potentially contributing to over five million deaths a year and posing a similar threat to air pollution, modeling research suggested Tuesday.
                                                </p>                                                                                
<p>The study, described as "a wake-up call", also estimated that exposure to the toxic metal causes <a href="https://medicalxpress.com/tags/young+children/" rel="tag">young children</a> in developing countries to lose an average of nearly six IQ points each.
</p><p>Lead pollution has been shown to cause a range of serious health problems, particularly relating to <a href="https://medicalxpress.com/tags/heart+disease/" rel="tag">heart disease</a> and the brain development of small children, resulting in <a href="https://medicalxpress.com/tags/leaded+gasoline/" rel="tag">leaded gasoline</a> being banned worldwide.
</p><p>But people can still be exposed to the <a href="https://medicalxpress.com/tags/potent+neurotoxin/" rel="tag">potent neurotoxin</a> via food, soil, cookware, fertilizers, cosmetics, lead-acid car batteries and other sources.
</p><p>The two World Bank economists who authored the study, published in the <i>Lancet Planetary Health</i> journal, said it was the first to assess the impact of lead exposure on heart disease deaths and child IQ loss in wealthy and developing nations.
</p><p>Lead author Bjorn Larsen told AFP that when the pair first saw the figure their model calculated, "we didn't even dare to whisper the number" because it was so "enormous".
</p><p>Their model estimates that 5.5 million adults died from heart disease in 2019 because of lead exposure, 90 percent of them in low- and <a href="https://medicalxpress.com/tags/middle-income+countries/" rel="tag">middle-income countries</a>.
</p><p>That is six times higher than the previous estimate, and represents around 30 percent of all deaths from <a href="https://medicalxpress.com/tags/cardiovascular+disease/" rel="tag">cardiovascular disease</a>—the leading cause of death worldwide.
</p><p>It would mean that lead exposure is a bigger cause of heart disease than smoking or cholesterol, Larsen said.
</p><h2>$6 trillion cost</h2>
<p>The research also estimated that children under five lost a cumulative 765 million IQ points due to lead poisoning globally in 2019, with 95 percent of those losses coming in developing countries.
</p><p>That number is nearly 80 percent higher than previously estimated.
</p><p>The World Bank researchers put the economic cost of lead exposure at $6 trillion in 2019, equivalent to seven percent of global gross domestic product.
</p><p>For the analysis, the researchers used estimates of blood lead levels in 183 countries taken from the landmark 2019 Global Burden of Disease study.
</p><p>Previous research had measured only lead's effect on heart disease when it came to raising <a href="https://medicalxpress.com/tags/blood+pressure/" rel="tag">blood pressure</a>. But the new study looked at numerous other ways lead affects hearts, such as the hardening of arteries that can lead to stroke, resulting in the higher numbers, Larsen said.
</p><p>Roy Harrison, an expert in <a href="https://medicalxpress.com/tags/air+pollution/" rel="tag">air pollution</a> and health at Birmingham University in the UK, who was not involved in the study, told AFP it was "interesting, but subject to many uncertainties".
</p><p>For example, the relationship between lead in blood and <a href="https://medicalxpress.com/tags/heart/" rel="tag">heart</a> disease is based on a survey in the United States, and whether those findings could be applied worldwide "is a huge jump of faith", he said.
</p><p>Harrison also pointed out that the model used estimations—not tests—of lead in blood in many developing countries.
</p><p>If the results were confirmed, "they would be of major public health significance, but at present, this is simply an interesting hypothesis", he said.
</p><h2>'Piece of the puzzle'</h2>
<p>Richard Fuller, president of the NGO Pure Earth, said that when surveys in developing countries did test for lead in blood, they mostly found higher levels than estimated in the new study.
</p><p>This means "the impact of lead might be worse than the report describes", he told AFP, calling it a "wake-up call".
</p><p>Larsen said "we're still a little in the dark" when it came to understanding how much different sources of lead contribute to blood contamination.
</p><p>Fuller said part of this "missing piece of the puzzle" was revealed in a Pure Earth report released on Tuesday, which analyzed 5,000 samples of consumer goods and food in 25 developing countries.
</p><p>It found high rates of lead contamination in metal pots and pans, ceramic cookware, paint, cosmetics and toys.
</p><p>"This is why poorer countries have so much <a href="https://medicalxpress.com/tags/lead+poisoning/" rel="tag">lead poisoning</a>," Fuller said. "It's items in the kitchen that are poisoning them."
                                                                                
                                        											</p><div>
												                                                    <p><strong>More information:</strong>
                                                    Bjorn Larsen et al, Global health burden and cost of lead exposure in children and adults: a health impact and economic modelling analysis, <i>The Lancet Planetary Health</i> (2023).  <a data-doi="1" href="https://dx.doi.org/10.1016/S2542-5196(23)00166-3" target="_blank">DOI: 10.1016/S2542-5196(23)00166-3</a>
																								
																								</p>
																							</div>
                                        											
										                                                                                    <p>
                                                © 2023 AFP
                                            </p>
                                                                                
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Lead poisoning causes far more death, IQ loss than thought: study (2023, September 12)
                                                 retrieved 21 September 2023
                                                 from https://medicalxpress.com/news/2023-09-poisoning-death-iq-loss-thought.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Matrix 2.0: The Future of Matrix (491 pts)]]></title>
            <link>https://matrix.org/blog/2023/09/matrix-2-0/</link>
            <guid>37599510</guid>
            <pubDate>Thu, 21 Sep 2023 15:53:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matrix.org/blog/2023/09/matrix-2-0/">https://matrix.org/blog/2023/09/matrix-2-0/</a>, See on <a href="https://news.ycombinator.com/item?id=37599510">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><strong><em>TL;DR: If you want to play with a shiny new Matrix 2.0 client, head over to <a href="https://element.io/blog/element-x-ignition/">Element X</a>.</em></strong></p>
<p>Matrix has been going for over 9 years now, providing an open standard for secure, decentralised communication for the open Web - and it’s been quite the journey to get to where we are today.  Right now, according to Synapse’s opt-in usage reporting, in total there are 111,873,374 matrix IDs on the public network, spanning 17,289,201 rooms, spread over 64,256 servers.  This is just scratching the surface, given we estimate that 66% of servers in the public network don’t report stats, and there are many enormous private networks of servers too.  We’ve come a long way from creating Matrix HQ as the first ever room on today’s public network, back on Aug 13th 2014 :)</p>
<p>Meanwhile, the Matrix ecosystem has continued to grow unbelievably - with huge numbers of independent clients, bots and bridges maturing into ecosystems of their own, whole new companies forming around the protocol, and organisations ranging from open source projects to governments, NGOs and Fortune 100 companies adopting Matrix as a way to run their own secure, decentralised, standards-based self-sovereign communication.</p>
<p>The world needs Matrix more than ever.  Every day the importance of decentralisation is more painfully obvious, as we concretely see the terrifying risks of centralised Internet services - whether that’s through corporate takeover, state censorship, blanket surveillance, Internet shutdowns, surveillance capitalism, or the spectre of gigantic centralised data breaches.  It’s been amazing to see the world pivot in favour of decentralisation over the time we’ve been building Matrix, and our mission has never been more important.</p>
<p>On one hand it feels we’re creeping ever closer to that goal of providing the missing communication layer for the open Web.  The European Union’s Digital Markets Act (DMA) is a huge step in that direction - regulation that mandates that if the large centralised messaging providers are to operate in the EU, they <strong>must</strong> interoperate.  We’ve been busy working away to make this a reality, including participating in the IETF for the first time as part of the MIMI working group - demonstrating <a href="https://datatracker.ietf.org/meeting/117/materials/slides-117-mimi-linearized-matrix-for-mimi-01">concretely</a> how (for instance) Android Messages could natively speak Matrix in order to interoperate with other services, while preserving end-to-end encryption.</p>
<p>On the other hand, Matrix has often got stuck in focusing on solving the Hard Problems of decentralisation, decentralised end-to-end encryption, and the logistical complexities of supporting a massive heterogeneous public communication network and its surrounding heterogeneous ecosystem.  It’s fair to say that in the early days our focus was on making something that worked at all - and then later, we shifted to focusing on something that worked and scaled correctly… but we hadn’t managed to focus on ensuring that Matrix provides the building blocks necessary to create blazingly fast, hyper-efficient communication apps which has potential to outperform the centralised mainstream messaging services…</p>
<p><strong>…until now!</strong></p>
<h2 id="matrix-2-0">Matrix 2.0</h2>
<p>Back at FOSDEM <a href="https://archive.fosdem.org/2023/schedule/event/matrix20/">we announced the idea of Matrix 2.0</a> - a series of huge step changes in terms of Matrix’s usability and performance, made up of <a href="https://github.com/matrix-org/matrix-spec-proposals/pull/3575">Sliding Sync</a> (instant login/launch/sync), <a href="https://github.com/matrix-org/matrix-spec-proposals/pull/3861">Native OIDC</a> (industry-standard authentication), <a href="https://github.com/matrix-org/matrix-spec-proposals/pull/3401">Native Group VoIP</a> (end-to-end encrypted large-scale voice &amp; video conferencing) and <a href="https://github.com/matrix-org/matrix-spec-proposals/pull/3902">Faster Joins</a> (lazy-loading room state when your server joins a room).</p>
<p>Now, we’re excited to announce that as of today everyone can start playing with these Matrix 2.0 features. There’s still some work to bring them formally into the specification, but we’re putting it out there for folks to experience right now. Developers: watch this space for updates on the spec front.</p>
<p>Practically speaking, this means there are now implementations of the four pillars of Matrix 2.0 available today which you can use to power a daily-driver Matrix 2.0 client.  The work here has been driven primarily by <a href="https://element.io/">Element</a>, using their new <a href="https://element.io/labs/element-x">Element X</a> client as the test-bed for the new Matrix 2.0 functionality and to prove that the new APIs are informed by real-world usage and can concretely demonstrably create an app which begins to outperform iMessage, WhatsApp and Telegram in terms of usability and performance… all while benefiting from being 100% built on Matrix.</p>
<h3 id="matrix-rust-sdk-and-element-x">matrix-rust-sdk and Element X</h3>
<p><a href="https://element.io/blog/element-x-ignition/"><img src="https://matrix.org/blog/img/20230921-element-x.png"></a></p>
<p>The mission of Matrix 2.0 has been to provide a huge step forwards in real-world performance, usability and stability - and that means using a real client codebase as a guinea pig to ensure the new protocol is fit for purpose. <a href="https://github.com/matrix-org/matrix-rust-sdk">matrix-rust-sdk</a> has been the main vehicle for this, with <a href="https://element.io/labs/element-x">Element X</a> as the app primarily driving the new features (although other clients built on matrix-rust-sdk such as <a href="https://gitlab.gnome.org/GNOME/fractal#beta-version">Fractal 5</a> can then automatically benefit from the work should they wish).</p>
<p>To see what all the fuss is about, your best bet is probably to head over to the <a href="https://element.io/blog/element-x-ignition/">Element X launch blog post</a> and read all about it!  But from the Matrix perspective, this is a flag day in terms of the existence of a Matrix client which empirically outperforms the mainstream clients both in terms of usability and performance: it shows that Matrix is indeed viable to power communication for billions of users, should we get the chance.</p>
<p>From a client perspective: this has meant implementing Sliding Sync (<a href="https://github.com/matrix-org/matrix-spec-proposals/blob/kegan/sync-v3/proposals/3575-sync.md">MSC3575</a>) in matrix-rust-sdk - and then creating the entirely new <a href="https://matrix-org.github.io/matrix-rust-sdk/matrix_sdk_ui/index.html">matrix-sdk-ui</a> crate in order to expose higher level APIs to help apps efficiently drive their UI, without each app having to keep reinventing the wheel and risking getting it wrong.  The new UI crate gives APIs for efficiently managing a lazy-loaded room list, lazy-loaded room timelines (including edits, reactions, aggregations, redactions etc), and even when the app should show a sync spinner or not.  As a result, the vast majority of the heavy lifting can be handled in matrix-rust-sdk, ensuring that the app layer can focus on UI rather than Matrix guts - and performance improvements (e.g. roomlist caching and timeline caching) can all be handled in one place to the benefit of all clients using the SDK.</p>
<p>This is a huge breakthrough relative to the old days of Matrix where each client would have no choice but burn significant amounts of time hand-carving its own timeline and encryption glue logic (although of course clients are still very welcome to do so if they wish!) - but for those wanting higher-level building building blocks, matrix-rust-sdk now provides an excellent basis for experimenting with Matrix 2.0 clients.  It’s worth noting that the library is still evolving <strong>fast</strong>, though, and many APIs are not long-term stable.  Both the Sliding Sync API and the UI crates are still subject to significant change, and while the crypto crate and its underlying <a href="https://github.com/matrix-org/vodozemac">vodozemac</a> E2EE implementation is pretty stable, features such as E2EE Backup are still being added to the top-level matrix-rust-sdk (and thence Element X).</p>
<p>In order to hook matrix-rust-sdk up to Element X, the Element team <a href="https://github.com/mozilla/uniffi-rs/pull/1346">ended</a> <a href="https://github.com/mozilla/uniffi-rs/pull/1292">up</a> <a href="https://github.com/mozilla/uniffi-rs/pull/1259">contributing</a> <a href="https://github.com/mozilla/uniffi-rs/pull/1684">cancellable</a> <a href="https://github.com/mozilla/uniffi-rs/pull/1409">async bindings</a> to <a href="https://mozilla.github.io/uniffi-rs/">uniffi</a>, Mozilla’s language binding generator, so you can now call matrix-rust-sdk directly from Swift, Kotlin and (in theory) other languages, complete with beautifully simple async/await non-blocking semantics.  This looks to be a pretty awesome stack for doing modern cross-platform development - so even if you have a project which isn’t natively in Rust, you should be able to lean on matrix-rust-sdk if you so desire!  We hope that other projects will follow the Rust + Swift/Kotlin pattern for their extreme performance needs :)</p>
<h3 id="sliding-sync">Sliding Sync</h3>
<p>The single biggest change in Matrix 2.0 is the proposal of an entirely new sync API called Sliding Sync (<a href="https://github.com/matrix-org/matrix-spec-proposals/blob/kegan/sync-v3/proposals/3575-sync.md">MSC3575</a>).  The goal of Sliding Sync is to ensure that the application has the option of loading the absolutely bare essential data required to render its visible user interface - ensuring that operations which have historically been horribly slow in Matrix (login and initial sync, launch and incremental sync) are instant, no matter how many rooms the user is in or how large those rooms are.</p>
<p>While matrix-rust-sdk implements both Sync v2 (the current API in Matrix 1.8) as well as Sliding Sync, Element X deliberately only implements Sliding Sync, in order to focus exclusively on getting the fastest UI possible (and generally to exercise the API).  Therefore to use Element X, you need to be running a homeserver with Sliding Sync support, which (for now) means running a <a href="https://github.com/matrix-org/sliding-sync">sliding-sync proxy</a> which bolts Sliding Sync support on to existing homeservers.  You can check out Thib’s <a href="https://www.youtube.com/watch?v=25wkV2ZCSsM">excellent tutorial</a> for how to get up and running (or <a href="https://element.io/server-registration">Element Server Suite</a> provides packages from the Element team)</p>
<p>Now, implementing Sliding Sync in matrix-rust-sdk has been a bit of a journey.  Since we <a href="https://archive.fosdem.org/2023/schedule/event/matrix_clients_as_good_as_youd_expect/">showed off</a> the very first implementation at FOSDEM, two big problems came to light.  For a bit of context: the original design of Sliding Sync was heavily inspired by Discord’s architecture - where the server calculates an ordered list of large numbers of items (your room list, in Matrix’s case); the client says which window into the list it’s currently displaying; and the server sends updates to the client as the view changes.  The user then scrolls around that list, sliding the window up and down, and the server sends the appropriate updates - hence the name Sliding Sync.</p>
<p>Sliding Sync was originally driven by our work on <a href="https://github.com/matrix-org/lb">Low Bandwidth Matrix</a> - as it makes no sense to have a fancy line protocol which can run over a 2400 baud modem… if the first thing the app tries to do is download a 100MB Sync v2 initial-sync response, or for that matter a 10MB incremental-sync response after having been offline for a few days (10MB takes 9 hours to shift over a 2400 baud modem, for those who missed out on the 80s).  Instead, you clearly only want to send the absolute essentials to the client, no matter how big their account is, and that’s what Sliding Sync does.</p>
<p>The first minor flaw in the plan, however, is that the server doesn’t necessarily have all the data it needs to order the room list.  Room ordering depends on what the most recent visible events are in a room, and if the room’s end-to-end encrypted, the server has no way of knowing which events are going to be visible for a given client or not.  It also doesn’t know which rooms have encrypted mentions inside them, and we <a href="https://github.com/matrix-org/matrix-spec-proposals/pull/3952#discussion_r1112203279">don’t want to leak mention metadata</a> to the server, or design out keyword mentions.  So, MSC3575 proposed some complicated contortions to let the client tweak the order client-side based on its superior knowledge of the ordering (given most clients would need to sync all the encrypted rooms anyway, in order to index them and search for keyword notifications etc).  Meanwhile, the order might be ‘good enough’ even without those tweaks.</p>
<p>The second minor flaw in the plan was that having implemented Sliding Sync in Element X, it turns out that the user experience on mobile of incrementally loading in room list entries from the server as the user scrolls around the list is simply not good enough, especially on bad connectivity - and the last thing we want to do is to design out support for bad connectivity in Matrix.  Users have been trained on mobile to expect to be able to swipe rapidly through infinite-scrolling lists of tens of thousands of photos in their photo gallery, or tens of thousands of emails in their mail client, without ever seeing a single placeholder, even for a frame.  So if the network roundtrip time to your server is even 100ms, and Sliding Sync is operating infinitely quickly, you’re still going to end up showing a placeholders for a few frames (6 frames, at 60fps, to be precise) if the user starts scrolling rapidly through their room list.  And empirically that doesn’t look great - the 2007-vintage <a href="https://www.amazon.co.uk/Creative-Selection-Ken-Kocienda/dp/1250194466">iOS team</a> have a lot to answer for in terms of setting user expectations!</p>
<p>So, the obvious way to solve both of these problems is simply to pull in more data in the background, to anticipate the user scrolling around.  In fact, it turns out we need to do that anyway, and indeed pull in <em>all</em> the room data so that room-search is instantly responsive; waiting 100ms or more to talk to the server whenever the user tries to search their roomlist is no fun at all, and it transpires that many users navigate their roomlist entirely by search rather than scrolling.  As a result, the sliding sync implementation in matrix-rust-sdk has ended up maintaining an ‘all rooms’ list, which starts off syncing the roomlist details for the most recent N rooms, and then in the background expands to sync all the rest.  At which point we’re not really sliding a window around any more: instead it’s more of a QoSed incremental sync.</p>
<p>So, to cut a long story short: while the current Sliding Sync implementation in matrix-rust-sdk and Element X empirically works very well, it’s ended up being a bit too complicated and we expect some pretty significant simplifications in the near future based on the best practices figured out with clients using it.  Watch this space for updates, although it’s likely that the current form of MSC3575 will prevail in some respect in order to support low-bandwidth environments where roomlist ordering and roomsearch latency is less important than preserving bandwidth.  Critically, we want to figure this out before we encourage folks to implement native server implementations - so for now, we’ll be keeping using the sliding-sync proxy as a way to rapidly experiment with the API as it evolves.</p>
<h3 id="native-matrix-group-voip">Native Matrix Group VoIP</h3>
<p>Another pillar of Matrix 2.0 is that we finally have native Matrix Group VoIP calling (<a href="https://github.com/matrix-org/matrix-spec-proposals/blob/matthew/group-voip/proposals/3401-group-voip.md">MSC3401</a>)!  Much like Sliding Sync has been developed using Element X as a testbed, <a href="https://call.element.io/">Element Call</a> has been the guinea pig for getting fully end-to-end-encrypted, scalable group voice/video calling implemented on top of Matrix, building on top of matrix-js-sdk.  And as of today, Element Call finally has it working, complete with end-to-end encryption (and integrated in Element X, for that matter)!</p>
<p><img src="https://matrix.org/blog/img/20230921-element-call.png" alt=""></p>
<p>Much like Sliding Sync, this has also been a bit of a journey.  The <a href="https://element.io/blog/introducing-native-matrix-voip-with-element-call/">original</a> implementations of Element Call strictly followed MSC3401, using full mesh conferencing to effectively have every participant place a call to every other participant - thus decentralising the conference and avoiding the need for a conferencing ‘focus’ server… but limiting the conference to 7 or 8 participants given all the duplication of the sent video required.  In Element Call <a href="https://element.io/blog/element-call-beta-2-encryption-spatial-audio-walkie-talkie-mode-and-more/">Beta 2</a>, end-to-end encryption was enabled; easy, given it’s just a set of 1:1 calls.</p>
<p>Then the real adventure began: to implement a Selective Forwarding Unit (SFU) which can be used to scale up to hundreds of users - or beyond. The unexpected first move came from Sean DuBois, project lead of the awesome <a href="https://pion.ly/">Pion</a> WebRTC stack for Golang - who wrote a proof-of-concept called sfu-to-sfu to demonstrate the viability of decentralised heterogenous cascading SFUs, as detailed in <a href="https://github.com/matrix-org/matrix-spec-proposals/blob/SimonBrandner/msc/sfu/proposals/3898-sfu.md">MSC3898</a>. This would not only let calls on a single focus scale beyond hundreds of users, but also share the conferencing out across all the participating foci, providing the world’s first heterogeneous decentralised video conferencing.  Element took the sfu-to-sfu implementation, hooked it up to Element Call on a branch, and renamed it as <a href="https://github.com/matrix-org/waterfall">waterfall</a>.</p>
<p>However, when Sean first contributed sfu-to-sfu, he mentioned to us that if Matrix is serious about SFUs, we should take a look at <a href="https://livekit.io/">LiveKit</a> - an open source startup not dissimilar to Element who were busy building best-in-class SFUs on top of Pion. And while waterfall worked well as a proof of concept, it became increasingly obvious that there’s a lot of work to be done around tuning congestion control, error correction, implementing end-to-end encryption etc which the LiveKit team had already spent years doing.  So, Element reached out to the LiveKit team, and started experimenting with what it might take to implement a Matrix-capable SFU on top of the LiveKit engine.</p>
<p>The end result was Element Call <a href="https://element.io/blog/element-call-beta-3/">Beta 3</a>, which is an interesting hybrid between MSC3401 and LiveKit’s existing signalling: the high-level signalling of the call (its existence, membership, duration etc) is advertised by Matrix - but the actual WebRTC signalling is handled by LiveKit, providing support for hundreds of users per call.</p>
<p>Finally, today marks the release of Element Call <a href="https://element.io/blog/element-x-ignition/#native-matrix-video-conferencing-with-element-call">Beta 4</a>, which adds back end-to-end encryption via the LiveKit SFU (currently by using a shared static secret, but in the near future will support full Matrix-negotiated end-to-end encryption with sender keys) - and also includes a complete visual refresh.  The next steps here include bringing back support for full mesh as well as SFU, for environments without an SFU, and updating all the MSCs to recognise the hybrid signalling model that reality has converged on when using LiveKit.  Meanwhile, head over to <a href="https://call.element.io/">https://call.element.io</a> to give it a go, or read more about it in the <a href="https://element.io/blog/element-x-ignition/">Element X Ignition blog post</a>!</p>
<h3 id="native-open-id-connect">Native Open ID Connect</h3>
<p>Finally, last but not least, we’re proud to announce that the project to replace Matrix’s venerable existing authentication APIs with industry-standard Open ID Connect in Matrix 2.0 has taken a huge leap forwards today, with <a href="https://matrix-org.github.io/matrix-authentication-service">matrix-authentication-service</a> now being available to add Native OIDC support to Synapse, as well as Element X now implementing account registration, login and management via Native OIDC (with legacy support only for login/logout).</p>
<p>This is a critical step forwards in improving the security and maintainability for Matrix’s authentication, and you can read all about it in this <a href="https://matrix.org/blog/2023/09/better-auth/">dedicated post</a>, explaining the rationale for adopting OpenID Connect for all forms of authentication throughout Matrix, and what you need to know about the transition.</p>
<h2 id="conclusion">Conclusion</h2>
<p>There has been an <strong>enormous</strong> amount of work that has gone into Matrix 2.0 so far - whether that’s implementing sliding sync in matrix-rust-sdk and sliding-sync proxy, matrix-authentication-service and all the native OIDC infrastructure on servers and clients, the entirety of Element Call and its underpinning matrix-js-sdk and SFU work, or indeed Faster Joins in Synapse, which shipped back in <a href="https://matrix.org/blog/2023/01/31/synapse-1-76-released/">Jan</a>.</p>
<p>It’s been a pretty stressful sprint to pull it all together, and huge thanks go to everyone who’s contributed - both from the team at Element, but also contributors to other projects like matrix-rust-sdk who have got caught in the crossfire :)  It’s also been amazing seeing the level of support, high quality testing and excellent feedback from the wider community as folks have got excited about the promise of Matrix 2.0.</p>
<p>On the Foundation side, we’d like to thank the <a href="https://matrix.org/blog/2023/06/membership-program/">Members</a> whose financial support has been critical in providing bandwidth to enable the progress on Matrix 2.0 - and for those who want to help accelerate Matrix, especially those commercially building on top of Matrix, please consider <a href="https://matrix.org/membership/">joining the Foundation</a> as a member!  Also, in case you missed it, we’re super excited to <a href="https://matrix.org/blog/2023/09/introducing-josh-simmons-managing-director/">welcome Josh Simmons as Managing Director</a> for the Foundation - focusing on running the Foundation membership programme and generally ensuring the growth of the Foundation funding for the benefit of the whole Matrix community. Matthew and Amandine continue to lead the overall project (alongside their day jobs at Element), with the support of the other three independent Guardians - but Josh is working full time exclusively on running the non-profit foundation and gathering funds to support Matrix.</p>
<p>Talking of funding, we should mention that we’ve had to pause work in other places due to lack of Matrix funding - especially while focusing on successfully shipping Matrix 2.0. Major next-generation projects including <a href="https://thirdroom.io/">Third Room</a>, <a href="https://arewep2pyet.com/">P2P Matrix</a>, and <a href="https://matrix.org/blog/2021/06/10/low-bandwidth-matrix-an-implementation-guide/">Low Bandwidth Matrix</a> have all been paused unless there’s a major shift in circumstances - so, if you have money and you’re interested in a world where the more experimental next-generation Matrix projects progress with folks working on them as their day job, please <a href="https://matrix.org/membership/">get in touch</a> with the Foundation.</p>
<h2 id="what-s-next">What’s next?</h2>
<p>While this is the first usable release of Matrix 2.0 implementations, there’s loads of work still to be done - obvious work on Matrix 2.0 includes:</p>
<ul>
<li>Getting Native OIDC enabled on matrix.org, and providing migration tools to Native OIDC for existing homeservers in general</li>
<li>Reworking Sliding Sync based on the lessons learned implementing it in matrix-rust-sdk</li>
<li>Actually getting the Matrix 2.0 MSCs stabilised and matured to the point they can be approved and merged into the spec</li>
<li>Adding encrypted backups to matrix-rust-sdk</li>
<li>Reintroducing full-mesh support for Native Matrix Group VoIP calling</li>
<li>Having a big Matrix 2.0 launch party once the spec lands!</li>
</ul>
<p>Outside of Matrix 2.0 work, other big items on the horizon include:</p>
<ul>
<li>Adding Rust matrix-sdk-crypto to matrix-js-sdk, at which point all the official Matrix.org client SDKs will (at last!) be using the same stable performant E2EE implementation</li>
<li>Continuing to contribute Matrix input to the MIMI working group in IETF for Digital Markets Act interoperability</li>
<li>Working on <a href="https://arewemlsyet.com/">MLS</a> for next-generation E2EE</li>
<li>Next generation moderation tooling and capabilities</li>
<li>Account Portability and Multihomed accounts</li>
<li>…and much much more.</li>
</ul>
<p>So: welcome to our brave new Matrix 2.0 world. We hope you’re excited about it as we are - and thanks to everyone for continuing to use Matrix and build on it.  Here’s to the beginning of a whole new era!</p>
<p>Matthew, Amandine and the whole Matrix team.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sunken temple and sanctuary from ancient Egypt (130 pts)]]></title>
            <link>https://www.franckgoddio.org/projects/sunken-civilizations/heracleion/</link>
            <guid>37599312</guid>
            <pubDate>Thu, 21 Sep 2023 15:40:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.franckgoddio.org/projects/sunken-civilizations/heracleion/">https://www.franckgoddio.org/projects/sunken-civilizations/heracleion/</a>, See on <a href="https://news.ycombinator.com/item?id=37599312">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									<p>With a unique survey-based approach that utilises the most sophisticated <a href="https://www.franckgoddio.org/franck-goddio/system-approach-technology/">technical equipment</a>, Franck Goddio and his team, in cooperation with the Egyptian Supreme Council of Antiquities, were able to locate, map and excavate parts of the city of Thonis-Heracleion, which lies 6.5 kilometres off today’s coastline. The city is located within an overall research area of 11 by 15 kilometres in the western part of Aboukir Bay at a depth of approx. 10 metres. Research started in 1996. It took years to map the entire area. First discoveries could be made in 2000. Franck Goddio has found important information on the ancient landmarks of Thonis-Heracleion, such as the <b>grand temple of Amun</b> and his son Khonsou (Herakles for the Greeks), the harbours that once controlled all trade into Egypt, and the daily life of its inhabitants. He has also solved a <a href="https://www.franckgoddio.org/fileadmin/pics/3_5_finds/documents/Franck_Goddio_Stele_Heracleion.pdf" title="Opens internal link in current window" target="_blank">historic enigma</a> that has puzzled Egyptologists over the years: the archaeological material has revealed that Heracleion and Thonis were in fact one and the same city with two names; Heracleion being the name of the city for the Greeks and Thonis for the Egyptians.</p>
									<h5>What the City looked like</h5>
									<p>The <a href="https://www.franckgoddio.org/finds/" title="Opens internal link in current window" target="_blank">objects</a> recovered from the excavations illustrate the cities’ beauty and glory, the magnificence of their grand temples and the abundance of historic evidence: <a href="https://www.franckgoddio.org/fileadmin/pics/3_5_finds/documents/Franck_Goddio_ColossalStatues.pdf" title="Opens internal link in current window" target="_blank">colossal statues</a>, inscriptions and architectural elements, jewellery and coins, ritual objects and ceramics - a civilization frozen in time.</p>
									<p>The quantity and quality of the archaeological material excavated from the site of Thonis-Heracleion show that this city had known a time of opulence and a peak in its occupation from the 6th to the 4th century BC. This is readily seen in the large quantity of coins and ceramics dated to this period. The port of Thonis-Heracleion had numerous large basins and functioned as a <b>hub of international trade</b>. The intense activity in the port fostered the city’s prosperity. More than <b>seven hundred discovered ancient anchors</b> of various forms and <b>79 wrecks</b> dating from the 6th to the 2nd century BC (further 40 wrecks could be detected but still require verification) are also an eloquent testimony to the intensity of maritime activity here.</p>
									<p>The city extended all around the temple and a network of canals in and around the city must have given it a lake dwelling appearance. On the islands and islets dwellings and secondary sanctuaries were located. Excavations here have revealed beautiful archaeological material such as <a href="https://www.franckgoddio.org/finds/#c2007">bronze statuettes</a>. On the north side of the temple to Herakles, a grand canal flowed through the city from east to west and connected the port basins with a lake to the west.</p>
									<div><p>Thonis-Heracleion was also the site of the <b>celebration of the Mysteries of Osiris</b>. This important ceremony was performed each year in honour of the rebirth of the god Osiris. Texts and figures in the Osirian chapels in the temple of Dendera and on the stele of the royal Decree of Canopus describe the details of the celebration of this vigil and re-awakening of the god. In his ceremonial boat Osiris was brought in procession from the city's great temple of Amun-Gereb to his shrine in <a href="https://www.franckgoddio.org/projects/sunken-civilizations/canopus/">Canopus</a>.</p><p> The underwater archaeological research in Thonis-Heracleion is ongoing until today. Franck Goddio estimates that only 5 percent of the city have yet been discovered.</p></div>
								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Article reply “Godot is not the new Unity” from Juan Linietsky (BDFL of Godot) (253 pts)]]></title>
            <link>https://gist.github.com/reduz/cb05fe96079e46785f08a79ec3b0ef21</link>
            <guid>37598985</guid>
            <pubDate>Thu, 21 Sep 2023 15:21:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/reduz/cb05fe96079e46785f08a79ec3b0ef21">https://gist.github.com/reduz/cb05fe96079e46785f08a79ec3b0ef21</a>, See on <a href="https://news.ycombinator.com/item?id=37598985">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-godot_binding_system_explained-md">
    <article itemprop="text"><p dir="auto">During the past days, this <a href="https://sampruden.github.io/posts/godot-is-not-the-new-unity/" rel="nofollow">great article</a> by Sam Pruden has been
making the rounds around the gamedev community. While the article provides an in-depth analysis, its a bit easy to miss the
point and exert the wrong conclusions from it. As such, and in many cases, users unfamiliar with Godot internals have used
it points such as following:</p>
<ul dir="auto">
<li>Godot C# support is inefficient</li>
<li>Godot API and binding system is designed around GDScript</li>
<li>Godot is not production ready</li>
</ul>
<p dir="auto">In this brief article, I will shed a bit more light about how the Godot binding system works and some detail on the Godot
architecture. This should hopefully help understand many of the technical decisions behind it.</p>
<h3 id="user-content-built-in-types" dir="auto"><a href="#built-in-types">Built-in Types</a></h3>
<p dir="auto">Compared to other game engines, Godot is designed with a relatively high level data model in mind. At the heart, it uses several
datatypes across the whole engine. These datatypes are:</p>
<ul dir="auto">
<li><strong>Nil</strong>: To indicate an empty value.</li>
<li><strong>Bool, Int64 and Float64</strong>: For scalar math.</li>
<li><strong>String</strong>: For String and Unicode handling.</li>
<li><strong>Vector2, Vector2i, Rect2, Rect2i, Transform2D</strong>: For 2D Vector math.</li>
<li><strong>Vector3, Vector4, Quaternion, AABB, Plane, Projection, Basis, Transform3D</strong>: For 3D Vector math.</li>
<li><strong>Color</strong>: For color space math.</li>
<li><strong>StringName</strong>: For fast processing of Unique IDs (internally a unique pointer).</li>
<li><strong>NodePath</strong>: For referencing paths between nodes in the Scene Tree.</li>
<li><strong>RID</strong>: Resource ID for referencing a resource inside a server.</li>
<li><strong>Object</strong>: An instance of a class.</li>
<li><strong>Callable</strong>: A generic function pointer.</li>
<li><strong>Signal</strong>: A signal (see Godot docs).</li>
<li><strong>Dictionary</strong>: A generic dictionary (can contain any of these datatypes as either key or value).</li>
<li><strong>Array</strong>: A generic array (can contain any of these datatypes).</li>
<li><strong>PackedByteArray, PackedInt32Array, PackedInt64Array, PackedFloatArray, PackedDoubleArray</strong>: Scalar packed arrays.</li>
<li><strong>PackedVector2Array, PackedVector3Array, PackedColorarray</strong>: Vector packed arrays.</li>
<li><strong>PackedStringArray</strong>: Packed string array.</li>
</ul>
<p dir="auto">Does this mean that anything you do in Godot has to use these datatypes? Absolutely not.
These datatypes have several roles in Godot:</p>
<ul dir="auto">
<li><strong>Storage</strong>: Any of these datatypes can be saved to disk and loaded back very efficiently.</li>
<li><strong>Transfer</strong>: These datatypes can be very efficiently marshalled and compressed for transfer over a network.</li>
<li><strong>Introspection</strong>: Objects in Godot can only expose their properties as any of those datatypes.</li>
<li><strong>Editing</strong>: When editing any object in Godot, it is done via any of these datatypes (of course, different editors can exist for the same datatype, depending on the context).</li>
<li><strong>Languge API</strong>: Godot exposes its API to all languages it binds via those datatypes.</li>
</ul>
<p dir="auto">Of course, if you are absolutely unfamliar to Godot, the first questions that come to mind are:</p>
<ul dir="auto">
<li>How do you expose more complex datatypes?</li>
<li>What about other datatypes such as int16?</li>
</ul>
<p dir="auto">In general, you can expose more complex datatypes via Object API, so this is not much of an issue. Additionally, modern processors all have at minimum 64 bit buses, so exposing anything other than 64 bit scalar types makes no sense.</p>
<p dir="auto">If you are unfamliar to Godot, I can totally understand the disbelief. But in truth, it works fine and it makes everything
far simpler at the time of developing the engine. This data model is one of the main reasons why Godot is such a tiny,
efficient and yet feature packed engine compared to the large mainstream mamooths. As you get more familiar with the source
code, you will start to see why.</p>
<h3 id="user-content-language-binding-system" dir="auto"><a href="#language-binding-system">Language Binding System</a></h3>
<p dir="auto">Now that we have our data model, Godot imposes a strict requirement that almost any function exposed to the engine API
must be done via those datatypes. Any function parameters, return types or properties exposed must be via them too.</p>
<p dir="auto">This makes the job of the binder much simpler. As such, Godot has what we call an universal binder. How does this binder work, then?</p>
<p dir="auto">Godot registers any C++ function to the binder like this:</p>
<div dir="auto"><pre>Vector3 <span>MyClass::my_function</span>(<span>const</span> Vector3&amp; p_argname) {
   <span><span>//</span>..//</span>
}

<span><span>//</span> Then, on a special function, Godot does:</span>

<span><span>//</span> Describe the method as having a name and the name of the argument, the pass the method pointer</span>
<span>ClassDB::bind_method</span>(D_METHOD(<span><span>"</span>my_function<span>"</span></span>,<span><span>"</span>my_argname<span>"</span></span>), &amp;MyClass::my_function);</pre></div>
<p dir="auto">Internally, <em>my_function</em> and <em>my_argument</em> are converted to a StringName (described above), so from now onwards they are
treated just as a unique pointer by the binding API. In fact, when compiling on release, the argument name is ignored by
the template and no code is generated, since it serves no purpose.</p>
<p dir="auto">So, what does <code>ClassDB::bind_method</code> do? If you want to dive into the depths of insanity and try to understand the
incredibly complex and optimized C++17 variadic templates black magic, feel free <a href="https://github.com/godotengine/godot/blob/master/core/object/method_bind.h">to go ahead</a>.</p>
<p dir="auto">But In short, it creates a static function like this, which Godot calls "ptrcall" form.:</p>
<div dir="auto"><pre><span><span>//</span> Not really done like this, but simplifying as much as possible so you get an idea:</span>

<span>static</span> <span>void</span> <span>my_function_ptrcall</span>(<span>void</span> *instance, <span>void</span> **arguments, <span>void</span> *ret_value) {
    MyClass *c = (MyClass*)instance;
    Vector3 *ret = (Vector3*)ret_value;
    *ret = c-&gt;<span>my_method</span>( *(Vector3*)arguments[<span>0</span>] );
}</pre></div>
<p dir="auto">This wrapper is basically as efficient as it can be. In fact, for critical functions, inline is forced into the class method, resulting in a C function pointer to the actual function code.</p>
<p dir="auto">Then Language API works by allowing the request of any engine function in "ptrcall" format. To call this format,
the language must:</p>
<ul dir="auto">
<li>Allocate a bit of stack (basically just adjusting the stack pointer of the CPU)</li>
<li>set a pointer to the arguments (which already exist in native form in this language 1:1, be it GodotCPP, C#, Rust, etc).</li>
<li>call.</li>
</ul>
<p dir="auto">And that's it. This is an incredibly efficient generic glue API that you can use to expose any language to Godot efficiently.</p>
<p dir="auto">So, as you can imagine, the C# API in Godot basically uses a C function pointer via unsafe API to call after assigning pointers
to native C# types. It is very, very efficient.</p>
<h3 id="user-content-godot-is-not-the-new-unity---the-anatomy-of-a-godot-api-call" dir="auto"><a href="#godot-is-not-the-new-unity---the-anatomy-of-a-godot-api-call">Godot is not the new Unity - The anatomy of a Godot API call</a></h3>
<p dir="auto">I want to insist that the article written by Sam Pruden is fantastic, but if you are not familiar with how Godot is intended to work under the hood it can be very misleading. I will proceed to explain a bit more in detail what is easy to misunderstand.</p>
<h4 id="user-content-only-a-pathological-use-case-is-shown-the-rest-of-the-api-is-fine" dir="auto"><a href="#only-a-pathological-use-case-is-shown-the-rest-of-the-api-is-fine">Only a pathological use case is shown, the rest of the API is fine.</a></h4>
<p dir="auto">The use case shown in the article, the ray_cast function, is a pathological one in the Godot API.
Cases like this are most likely less 0.01% of the API exposed by Godot. Why the author found this specific one,
I have no idea nor I will speculate, but I think it was just an unfortunate coincidence.</p>
<p dir="auto">The problem is that, at the C++ level, this function takes a struct pointer for performance. But at the language
binding API this is difficult to expose properly. This is very old code (dating to the opensourcing of Godot) and
a Dictionary was hacked-in to use temporarily until something better is found. Of course, other stuff was more prioritary and very few games need thousands of raycasts, so pretty much nobody complained. Still, there is a <a href="https://github.com/godotengine/godot-proposals/issues/7329" data-hovercard-type="issue" data-hovercard-url="/godotengine/godot-proposals/issues/7329/hovercard">recently open proposal</a> to discuss more efficient binding of these types of functions.</p>
<p dir="auto">Additionally, to add to how unfortunate this choice of function is, the Godot language binding system <em>does</em> support
struct pointers like this. GodotCPP and Rust bindings can use pointers to structs without any issue. The problem
is that C# support in Godot predates the extension system and it was not converted to it yet. Eventually, C# will be
moved to the universal extension system and this will allow the unifying of the default and .net editors, it is just
not the case yet, but its top in the list of priorities.</p>
<h4 id="user-content-the-workaround-is-even-more-pathological" dir="auto"><a href="#the-workaround-is-even-more-pathological">The workaround is even more pathological</a></h4>
<p dir="auto">Although this time, due to a limitation of C#. If you bind C++ to C#, you need to create a C# version of a C++ instance
as an adapter. This is not an unique problem to Godot, any other engine or application doing this will require the same.</p>
<p dir="auto">Why is it troublesome? because C# has a garbage collector and C++ does not. This forces the C++ instance to keep a link
to the C# instance to avoid it from being collected.</p>
<p dir="auto">Due to this, the C# binder must do extra work when calling Godot functions that take class instances. You can see
this code in Sam's article:</p>
<div dir="auto"><pre><span>public</span> <span><span>static</span></span> GodotObject <span>UnmanagedGetManaged</span><span>(</span><span>IntPtr</span> <span>unmanaged</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>unmanaged</span> <span>==</span> IntPtr<span>.</span>Zero<span>)</span> <span>return</span> <span>null</span><span>;</span>

    <span>IntPtr</span> <span>intPtr</span> <span>=</span> NativeFuncs<span>.</span><span>godotsharp_internal_unmanaged_get_script_instance_managed</span><span>(</span>unmanaged<span>,</span> <span>out</span> <span>var</span> r_has_cs_script_instance<span>)</span><span>;</span>
    <span>if</span> <span>(</span><span>intPtr</span> <span>!=</span> IntPtr<span>.</span>Zero<span>)</span> <span>return</span> <span>(</span>GodotObject<span>)</span>GCHandle<span>.</span><span>FromIntPtr</span><span>(</span>intPtr<span>)</span><span>.</span>Target<span>;</span>
    <span>if</span> <span>(</span>r_has_cs_script_instance<span>.</span><span>ToBool</span><span>(</span><span>)</span><span>)</span> <span>return</span> <span>null</span><span>;</span>

    <span>intPtr</span> <span>=</span> NativeFuncs<span>.</span><span>godotsharp_internal_unmanaged_get_instance_binding_managed</span><span>(</span>unmanaged<span>)</span><span>;</span>
    <span>object</span> <span>obj</span> <span>=</span> <span>(</span><span>(</span><span>intPtr</span> <span>!=</span> IntPtr<span>.</span>Zero<span>)</span> <span>?</span> GCHandle<span>.</span><span>FromIntPtr</span><span>(</span>intPtr<span>)</span><span>.</span>Target <span>:</span> <span>null</span><span>)</span><span>;</span>
    <span>if</span> <span>(</span><span>obj</span> <span>!=</span> <span>null</span><span>)</span> <span>return</span> <span>(</span><span>GodotObject</span><span>)</span><span>obj</span><span>;</span>

    <span>intPtr</span> <span>=</span> NativeFuncs<span>.</span><span>godotsharp_internal_unmanaged_instance_binding_create_managed</span><span>(</span>unmanaged<span>,</span> intPtr<span>)</span><span>;</span>
    <span>if</span> <span>(</span><span>!</span><span>(</span><span>intPtr</span> <span>!=</span> IntPtr<span>.</span>Zero<span>)</span><span>)</span> <span>return</span> <span>null</span><span>;</span>

    <span>return</span> <span>(</span>GodotObject<span>)</span>GCHandle<span>.</span><span>FromIntPtr</span><span>(</span>intPtr<span>)</span><span>.</span>Target<span>;</span>
<span>}</span></pre></div>
<p dir="auto">While very efficient, it's still not ideal for hot paths so the Godot API exposed is considerate and does not expose anything critical this way. The workaround used, however, is quite complex and hits this path due to not using the actual function
intended for it.</p>
<h4 id="user-content-the-question-of-cherry-picking" dir="auto"><a href="#the-question-of-cherry-picking">The question of cherry picking</a></h4>
<p dir="auto">I firmly believe the author did not cherry pick this API on purpose. In fact, he himself writes that he checked other places of
API usages and did not find anything with this level of pathology either.</p>
<p dir="auto">However, he mentions:</p>
<pre><code>Let’s also remember that Dictionary is only part of the problem. If we look a little wider for things returning 
Godot.Collections.Array&lt;T&gt; (remember: heap allocated, contents as Variant) we find lots from physics, 
mesh &amp; geometry manipulation, navigation, tilemaps, rendering, and more.
</code></pre>
<p dir="auto">From my side and contributors side, none of those usages are hot paths or pathological. Remember that, as I mentioned above,
Godot uses the Godot types mainly for serialization and API communication. While it is true that they do heap allocation,
this only happens once when the data is created.</p>
<p dir="auto">I think what may have confused Sam and a few others in this area (which is normal if you are not familiar with the Godot codebase) is that Godot containers don't work like STL containers. Because they are used mainly to pass data around, they are
allocated once and then kept via reference counting.</p>
<p dir="auto">This means, the function that reads your mesh data from disk is the only one doing the allocation, then this pointer gets
passed through many layers via reference counting until arrives Vulkan and is uploaded to the GPU. Zero copies happen along
the way.</p>
<p dir="auto">Likewise, when these containers are exposed to C# via the Godot collections, they are also reference counted internally.
If you create one of those arrays to pass the the Godot API, the allocation only happens <em>once</em>. Then no further copies happen
and the data arrives intact to the consumer.</p>
<p dir="auto">Of course, intenally, Godot uses far more optimized containers that are not directly exposed to the binder API.</p>
<h4 id="user-content-misleading-conclusion" dir="auto"><a href="#misleading-conclusion">Misleading conclusion</a></h4>
<p dir="auto">The article concludes like this:</p>
<pre><code>Godot has made a philosophical decision to be slow. The only practical way to interact with the engine is via this binding layer, and its core design prevents it from ever being fast. No amount of optimising the implementation of Dictionary or speeding up the physics engine is going to get around the fact we’re passing large heap allocated values around when we should be dealing with tiny structs. While C# and GDScript APIs remain synchronised, this will always hold the engine back.
</code></pre>
<p dir="auto">As you have read in the above points, the binding layer is absolutely not slow. What can be slow is an extremely limited amount of use cases that can be pathological. For those cases, a dedicated solution is found. This is a general <a href="https://docs.godotengine.org/en/stable/contributing/development/best_practices_for_engine_contributors.html#to-each-problem-its-own-solution" rel="nofollow">philosophy</a> behind Godot development that helps keep the codebase small, tidy, maintainable and easy to understand.</p>
<p dir="auto">In other words, this principle:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/6265307/269689859-d3bb5bec-1473-4803-a7d4-9ebc33736d48.png"><img src="https://user-images.githubusercontent.com/6265307/269689859-d3bb5bec-1473-4803-a7d4-9ebc33736d48.png" alt="image"></a></p>
<p dir="auto">The current binder serves its purpose and works well and efficiently for over 99.99% of use cases. For the exceptional ones, as mentioned before, the extension API supports structs already (which you can see here in this excerpt of the extension api dump):</p>
<pre><code>		{
			"name": "PhysicsServer2DExtensionRayResult",
			"format": "Vector2 position;Vector2 normal;RID rid;ObjectID collider_id;Object *collider;int shape"
		},
		{
			"name": "PhysicsServer2DExtensionShapeRestInfo",
			"format": "Vector2 point;Vector2 normal;RID rid;ObjectID collider_id;int shape;Vector2 linear_velocity"
		},
		{
			"name": "PhysicsServer2DExtensionShapeResult",
			"format": "RID rid;ObjectID collider_id;Object *collider;int shape"
		},
		{
			"name": "PhysicsServer3DExtensionMotionCollision",
			"format": "Vector3 position;Vector3 normal;Vector3 collider_velocity;Vector3 collider_angular_velocity;real_t depth;int local_shape;ObjectID collider_id;RID collider;int collider_shape"
		},
		{
			"name": "PhysicsServer3DExtensionMotionResult",
			"format": "Vector3 travel;Vector3 remainder;real_t collision_depth;real_t collision_safe_fraction;real_t collision_unsafe_fraction;PhysicsServer3DExtensionMotionCollision collisions[32];int collision_count"
		},
</code></pre>
<p dir="auto">So, ultimately, I believe that the conclusion that "Godot is slow by design" is a bit rushed. What is currently missing is the move of the C# language to the GDExtension system in order to be able to take advantage of these. This is currently a work in progress.</p>
<h3 id="user-content-to-sum-up" dir="auto"><a href="#to-sum-up">To sum up</a></h3>
<p dir="auto">I hope that this short article is used to dispell a few misconceptions that unintentionally arised from Sam's excellent article:</p>
<ul dir="auto">
<li><strong>Godot C# API is inefficient:</strong> This is absolutely not the case, but very few pathological cases remain to be solved and were already being in discussion before last week. In practice, very very few games may run into them and, by next year, hopefully none.</li>
<li><strong>Godot API is designed around GDScript:</strong> This is also not true. In fact, until Godot 4.1, typed GDScript did calls via "ptrcall" syntax, and the argument encoding was a bottleneck. As a result, we created a <a href="https://github.com/godotengine/godot/pull/79893" data-hovercard-type="pull_request" data-hovercard-url="/godotengine/godot/pull/79893/hovercard">special path</a> for GDScript to call more efficiently.</li>
</ul>
<p dir="auto">Thanks for reading and remember that Godot is not commercial software developed behind closed doors. All of us who make it are available online in the same communities as you. If you have any doubt, feel free to ask us directly.</p>
<p dir="auto"><strong>Bonus:</strong> As a side note, and contrary to popular belief, the Godot data model was not created for GDScript. Originaly, the engine used other languages such as Lua or Squirrel, with several published games while an in-house engine. GDScript was developed afterwards.</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We're the Researchers who looked into the privacy of 25 of the top car brands (104 pts)]]></title>
            <link>https://old.reddit.com/r/IAmA/comments/16oi17v/were_the_researchers_who_looked_into_the_privacy/</link>
            <guid>37598845</guid>
            <pubDate>Thu, 21 Sep 2023 15:13:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/IAmA/comments/16oi17v/were_the_researchers_who_looked_into_the_privacy/">https://old.reddit.com/r/IAmA/comments/16oi17v/were_the_researchers_who_looked_into_the_privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=37598845">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>UPDATE: Thank you for joining us and for your thoughtful questions! To learn more, you can visit <a href="https://www.privacynotincluded.org/">www.privacynotincluded.org</a> and read our full reviews. You can also get smarter about your online life with regular <a href="https://foundation.mozilla.org/en/newsletter/">newsletters</a> from Mozilla and remember to sign our <a href="https://foundation.mozilla.org/en/privacynotincluded/articles/car-companies-stop-your-huge-data-collection-programs-en/">petition</a> to help us demand change!</p>

<p>To learn more about the data your car might be collecting, access your free Vehicle Privacy Report from Privacy4Cars here: <a href="https://vehicleprivacyreport.com/">https://vehicleprivacyreport.com</a>.   </p>

<p>Hi, we’re Jen Caltrider, Misha Rykov and Zoe MacDonald- lead Researchers of the <a href="https://foundation.mozilla.org/en/privacynotincluded/">*Privacy Not Included Guide</a> from Mozilla! We're also joined by Andrea from Privacy4Cars,a privacy-tech company focused on solving privacy challenges posed by vehicle data, and we’re all here to answer your burning questions about our recent Cars + Privacy report.</p>

<p><a href="https://twitter.com/mozilla/status/1704602084291613032?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet">Here's our proof.</a></p>

<p>We’ve reviewed a lot of product privacy policies over the years, but the car category is the worst for privacy that we have ever reviewed. All 25 of of the brands we researched failed our review and earned our *Privacy Not Included label; a sad first.Here's a summary of what we found:</p>

<ul>
<li>They collect too much personal data (all of them) - On top of collecting information regarding your in-car app usage and connected services, they can also collect super intimate information about you -- from your medical information, your genetic information, to your “sex life”</li>
<li>Most (84%) share or sell your data, and some (56%) also say they can share your information with the government or law enforcement in response to a “request.”</li>
<li>Most (92%) give drivers little to no control over their personal data - All but two of the 25 car brands we reviewed earned our “ding” for data control</li>
<li>We couldn’t confirm whether any of them meet our Minimum Security Standards</li>
</ul>

<p>Learn more about our findings and read the full report <a href="https://foundation.mozilla.org/en/privacynotincluded/articles/its-official-cars-are-the-worst-product-category-we-have-ever-reviewed-for-privacy/">here</a>.</p>

<p>Also! Check out Privacy4Cars' Vehicle <a href="https://vehicleprivacyreport.com/">Privacy Report</a> to know about and take actions for your vehicle.</p>

<p>Ask us anything about our guide, research or anything else!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The SEC cracks down on greenwashing (160 pts)]]></title>
            <link>https://www.semafor.com/article/09/21/2023/the-sec-cracks-down-on-greenwashing</link>
            <guid>37598329</guid>
            <pubDate>Thu, 21 Sep 2023 14:40:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.semafor.com/article/09/21/2023/the-sec-cracks-down-on-greenwashing">https://www.semafor.com/article/09/21/2023/the-sec-cracks-down-on-greenwashing</a>, See on <a href="https://news.ycombinator.com/item?id=37598329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img width="30" height="30" src="https://img.semafor.com/eb687b94b93248efbff9e65b02568febf0405a45-2000x2127.png" alt="Jeronimo Gonzalez"><span>/</span></p><p><span>The U.S. Securities and Exchange Commission, the country’s top financial regulator, adopted a new rule to crack down on investment fund “greenwashing.”</span> Funds will be required to ensure that 80% of their portfolio match the asset advertised by the fund’s name. Since 2021, the SEC has prosecuted funds that bill themselves as investing exclusively in securities that rank highly in environmental, social, and governance measurements but which rather cast a much wider net with their investments. “<a href="https://www.ft.com/content/c626c311-7699-43b1-a98d-9740e06efc85" rel="no-referrer">It is truth in advertising,</a>” Gary Gensler, the SEC chair, said.</p></div><div><div><p><strong>The move was largely welcomed by activists and environmentalists, including some who had long considered the lax restrictions deceptive or even predatory of retail investors. </strong>“These rules will help cut down on greenwashing and misleading marketing so that millions of U.S. investors ensure … their money is being invested in line with their interests and their values,” a strategist at the Sierra Club, an environmental advocacy organization, said. “The SEC’s action today is a vital step,” the Environmental Defense Fund wrote.</p></div><div><figure><img width="800" height="607" src="https://img.semafor.com/aefe84b7e5a2af5f8ac552065f61a3c91ac02a03-1106x840.png?w=1600&amp;q=75&amp;auto=format" alt="" loading="lazy"></figure><p><strong>Despite a market downturn in 2022</strong> — during which traditional funds suffered billions of dollars in outflows — investors continued to flock to ESG funds, pushing their assets under management to a record $2.8 trillion last year. Demand has been driven largely by Europe<span>• <!-- -->1<!-- --> </span>, which accounted for 89% of sustainable funds’ assets. After years of out-performing traditional funds, however, sustainable funds’ returns fell below those of traditional ones last year, according to Morgan Stanley research.</p></div><div><p><strong>The EU is also cracking down on greenwashing.</strong> From 2026, products that can’t back up the accuracy of products marketed as being “climate neutral,” “eco,” or other sweeping environmental claims will be banned. The new rule, which climate NGOs have long agitated for, will make the EU the toughest region in the world in terms of green claims made to the public, the Financial Times reported. “Carbon neutral claims are greenwashing <span>• <!-- -->2<!-- --> </span>,” the head of a European consumer association said. “The truth is that these claims are scientifically incorrect and should never be used.”</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google sued for negligence after man died following map directions (128 pts)]]></title>
            <link>https://apnews.com/article/google-maps-lawsuit-north-carolina-death-f4707247ee3295bf51bbcb37bd0eb6c8</link>
            <guid>37597207</guid>
            <pubDate>Thu, 21 Sep 2023 13:19:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/google-maps-lawsuit-north-carolina-death-f4707247ee3295bf51bbcb37bd0eb6c8">https://apnews.com/article/google-maps-lawsuit-north-carolina-death-f4707247ee3295bf51bbcb37bd0eb6c8</a>, See on <a href="https://news.ycombinator.com/item?id=37597207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-module="" data-padding="none">
                    
                    
                        
                            

    <div><figure>
    

    
        <picture data-crop="medium-3x2">
    
        <source media="(min-width: 1280px)" type="image/webp" width="980" height="653" srcset="https://dims.apnews.com/dims4/default/2c99e55/2147483647/strip/true/crop/4500x2998+0+2/resize/980x653!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x" loading="lazy">

    

    
        <source media="(min-width: 1280px)" width="980" height="653" srcset="https://dims.apnews.com/dims4/default/c1575bb/2147483647/strip/true/crop/4500x2998+0+2/resize/980x653!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x" loading="lazy">

    

    
        <source media="(min-width: 1024px)" type="image/webp" width="820" height="546" srcset="https://dims.apnews.com/dims4/default/15bc497/2147483647/strip/true/crop/4500x2996+0+3/resize/820x546!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x" loading="lazy">

    

    
        <source media="(min-width: 1024px)" width="820" height="546" srcset="https://dims.apnews.com/dims4/default/2b16c66/2147483647/strip/true/crop/4500x2996+0+3/resize/820x546!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x" loading="lazy">

    

    
        <source media="(min-width: 768px)" type="image/webp" width="1024" height="683" srcset="https://dims.apnews.com/dims4/default/3b0f355/2147483647/strip/true/crop/4500x3001+0+1/resize/1024x683!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x" loading="lazy">

    

    
        <source media="(min-width: 768px)" width="1024" height="683" srcset="https://dims.apnews.com/dims4/default/fe2eefc/2147483647/strip/true/crop/4500x3001+0+1/resize/1024x683!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x" loading="lazy">

    

    
        <source media="(min-width: 600px)" type="image/webp" width="767" height="511" srcset="https://dims.apnews.com/dims4/default/be63112/2147483647/strip/true/crop/4500x2998+0+2/resize/767x511!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x,https://dims.apnews.com/dims4/default/4eb3b00/2147483647/strip/true/crop/4500x2998+0+2/resize/1534x1022!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 2x" loading="lazy">

    

    
        <source media="(min-width: 600px)" width="767" height="511" srcset="https://dims.apnews.com/dims4/default/fd70e96/2147483647/strip/true/crop/4500x2998+0+2/resize/767x511!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x,https://dims.apnews.com/dims4/default/832a8f7/2147483647/strip/true/crop/4500x2998+0+2/resize/1534x1022!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 2x" loading="lazy">

    

    
        <source media="(max-width: 599px)" type="image/webp" width="567" height="378" srcset="https://dims.apnews.com/dims4/default/4813c64/2147483647/strip/true/crop/4500x3000+0+2/resize/567x378!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x,https://dims.apnews.com/dims4/default/d7f616e/2147483647/strip/true/crop/4500x3000+0+2/resize/1134x756!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 2x" loading="lazy">

    

    
        <source media="(max-width: 599px)" width="567" height="378" srcset="https://dims.apnews.com/dims4/default/38bf924/2147483647/strip/true/crop/4500x3000+0+2/resize/567x378!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x,https://dims.apnews.com/dims4/default/a23ddc4/2147483647/strip/true/crop/4500x3000+0+2/resize/1134x756!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 2x" loading="lazy">

    

    
        <source type="image/webp" width="320" height="213" srcset="https://dims.apnews.com/dims4/default/1627617/2147483647/strip/true/crop/4500x2995+0+4/resize/320x213!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x,https://dims.apnews.com/dims4/default/4eaa70b/2147483647/strip/true/crop/4500x2995+0+4/resize/640x426!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 2x" loading="lazy">

    

    
        <source width="320" height="213" srcset="https://dims.apnews.com/dims4/default/edde91d/2147483647/strip/true/crop/4500x2995+0+4/resize/320x213!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x,https://dims.apnews.com/dims4/default/a795125/2147483647/strip/true/crop/4500x2995+0+4/resize/640x426!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 2x" loading="lazy">

    

    <img alt="FILE - The Google Maps app is seen on a smartphone, March 22, 2017, in New York. On Tuesday, Sept. 19, 2023, the family of a North Carolina man who died after driving his car off a collapsed bridge while following Google Maps directions filed a lawsuit against the technology giant for negligence, claiming it had been informed of the collapse but failed to update its navigation system. (AP Photo/Patrick Sison, File)" srcset="https://dims.apnews.com/dims4/default/edde91d/2147483647/strip/true/crop/4500x2995+0+4/resize/320x213!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 1x,https://dims.apnews.com/dims4/default/a795125/2147483647/strip/true/crop/4500x2995+0+4/resize/640x426!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616 2x" width="320" height="213" src="https://dims.apnews.com/dims4/default/edde91d/2147483647/strip/true/crop/4500x2995+0+4/resize/320x213!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F95%2F8d%2Fb3026e25edb067087ea9cca7fdaf%2Fc596b13599d44021a963fdea1fa02616" loading="lazy">
</picture>

    

    
        <div>
            <div><bsp-read-more data-more-button-text="Read More" data-less-button-text="Read Less" data-expand="ReadMore-expand" data-limit="110" data-main-class="ReadMore">
                    <figcaption><p>FILE - The Google Maps app is seen on a smartphone, March 22, 2017, in New York. On Tuesday, Sept. 19, 2023, the family of a North Carolina man who died after driving his car off a collapsed bridge while following Google Maps directions filed a lawsuit against the technology giant for negligence, claiming it had been informed of the collapse but failed to update its navigation system. (AP Photo/Patrick Sison, File)</p></figcaption>
                </bsp-read-more></div>
            <bsp-lead-superlead-ui></bsp-lead-superlead-ui>
        </div>
    
</figure>
</div>



                        
                    

                    <div>
                                        <p>RALEIGH, N.C. (AP) — The family of a North Carolina man who died after driving his car off a collapsed bridge while following Google Maps directions is suing the technology giant for negligence, claiming it had been informed of the collapse but failed to update its navigation system.</p><p>Philip Paxson, a medical device salesman and father of two, drowned Sept. 30, 2022, after his Jeep Gladiator plunged into Snow Creek in Hickory, according to a lawsuit filed Tuesday in Wake County Superior Court. Paxson was driving home from his daughter’s ninth birthday party through an unfamiliar neighborhood when Google Maps allegedly directed him to cross a bridge that had collapsed nine years prior and was never repaired.</p><p>“Our girls ask how and why their daddy died, and I’m at a loss for words they can understand because, as an adult, I still can’t understand how those responsible for the GPS directions and the bridge could have acted with so little regard for human life,” his wife, Alicia Paxson, said.</p>
    

<p>State troopers who found Paxton’s body in his overturned and partially submerged truck had said there were no barriers or warning signs along the washed-out roadway. He had driven off an unguarded edge and crashed about 20 feet below, according to the lawsuit.</p>



<p>The North Carolina State Patrol had said the bridge was not maintained by local or state officials, and the original developer’s company had dissolved. The lawsuit names several private property management companies that it claims are responsible for the bridge and the adjoining land.</p>

<p>Multiple people had notified Google Maps about the collapse in the years leading up to Paxson’s death and had urged the company to update its route information, according to the lawsuit. </p><p>The Tuesday court filing includes email records from another Hickory resident who had used the map’s “suggest an edit” feature in September 2020 to alert the company that it was directing drivers over the collapsed bridge. A November 2020 email confirmation from Google confirms the company received her report and was reviewing the suggested change, but the lawsuit claims Google took no further actions.</p><p>“We have the deepest sympathies for the Paxson family,” Google spokesperson José Castañeda told The Associated Press. “Our goal is to provide accurate routing information in Maps and we are reviewing this lawsuit.”</p><h2>___</h2><p>Hannah Schoenbaum is a corps member for the Associated Press/Report for America Statehouse News Initiative. <span><a href="https://www.reportforamerica.org/" target="_blank" rel="noopener">Report for America</a></span> is a nonprofit national service program that places journalists in local newsrooms to report on undercovered issues.</p>
                                    </div>

                    


                    
    <div>
    <div>
        <a href="https://apnews.com/author/hannah-schoenbaum">
            
                
                    <picture data-crop="small-square">
    
        <source type="image/webp" width="100" height="100" srcset="https://dims.apnews.com/dims4/default/367b1c3/2147483647/strip/true/crop/512x512+0+0/resize/100x100!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F0e%2F1c%2Fc98d3bd249588d758c40ecebad4c%2Fschoenbaum-hannah.jpeg" loading="lazy">

    

    
        <source width="100" height="100" srcset="https://dims.apnews.com/dims4/default/0f4b0d6/2147483647/strip/true/crop/512x512+0+0/resize/100x100!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F0e%2F1c%2Fc98d3bd249588d758c40ecebad4c%2Fschoenbaum-hannah.jpeg" loading="lazy">

    

    <img alt="HANNAH SCHOENBAUM" width="100" height="100" src="https://dims.apnews.com/dims4/default/0f4b0d6/2147483647/strip/true/crop/512x512+0+0/resize/100x100!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F0e%2F1c%2Fc98d3bd249588d758c40ecebad4c%2Fschoenbaum-hannah.jpeg" loading="lazy">
</picture>

                
            
        </a>
    </div>

    <div>
        
            
        

        
            <p>
                Schoenbaum covers government and politics in North Carolina.
            </p>
        

        
            
        
    </div>
</div>



                    
    



                    
    


                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Odin – the integration of LLMs with Obsidian note taking (128 pts)]]></title>
            <link>https://github.com/memgraph/odin</link>
            <guid>37597201</guid>
            <pubDate>Thu, 21 Sep 2023 13:18:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/memgraph/odin">https://github.com/memgraph/odin</a>, See on <a href="https://news.ycombinator.com/item?id=37597201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a href="https://reactjs.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/ea88da5714e4d0bb27afeb1a662a46e024ee7f1f8f8459dd4212adc75a2f5b23/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f52656163742d3631444246423f7374796c653d666f722d7468652d6261646765266c6f676f3d7265616374266c6f676f436f6c6f723d626c61636b" alt="react" data-canonical-src="https://img.shields.io/badge/React-61DBFB?style=for-the-badge&amp;logo=react&amp;logoColor=black"></a>
<a href="https://www.typescriptlang.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/9e98eab478e098342c2933b383b774088b092bff05174f33637fa6307253e8ee/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547970655363726970742d3331373843363f7374796c653d666f722d7468652d6261646765266c6f676f3d74797065736372697074266c6f676f436f6c6f723d7768697465" alt="typescript" data-canonical-src="https://img.shields.io/badge/TypeScript-3178C6?style=for-the-badge&amp;logo=typescript&amp;logoColor=white"></a>
<a href="https://styled-components.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/764d2c45635fde66c960fd6e024cd4dbd21c4dc8cf2160aa80aa0049f8632e22/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374796c65645f636f6d706f6e656e74732d4442373039333f7374796c653d666f722d7468652d6261646765266c6f676f3d7374796c6564636f6d706f6e656e7473266c6f676f436f6c6f723d7768697465" alt="styledcomponents" data-canonical-src="https://img.shields.io/badge/styled_components-DB7093?style=for-the-badge&amp;logo=styledcomponents&amp;logoColor=white"></a></p>
<p dir="auto"><a href="https://obsidian.md/" rel="nofollow"><img src="https://camo.githubusercontent.com/92d23a4981eec0df5192953800a52f3b1910e5a54c984afdaf82397cc3acf67b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f6273696469616e2d3743334145443f7374796c653d666f722d7468652d6261646765266c6f676f3d6f6273696469616e266c6f676f436f6c6f723d7768697465" alt="obsidian" data-canonical-src="https://img.shields.io/badge/obsidian-7C3AED?style=for-the-badge&amp;logo=obsidian&amp;logoColor=white"></a>
<a href="https://www.docker.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/b184cf7adbab9f5464e80c0f5dd32c85393f6248499a57d743e619f4214391c4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d3234393645443f7374796c653d666f722d7468652d6261646765266c6f676f3d646f636b6572266c6f676f436f6c6f723d7768697465" alt="docker" data-canonical-src="https://img.shields.io/badge/docker-2496ED?style=for-the-badge&amp;logo=docker&amp;logoColor=white"></a></p>
<h2 tabindex="-1" id="user-content-odin---obsidian-driven-information-network" dir="auto"><a href="#odin---obsidian-driven-information-network">ODIN - Obsidian Driven Information Network</a></h2>
<h2 tabindex="-1" id="user-content-disclaimer" dir="auto"><a href="#disclaimer">Disclaimer</a></h2>
<p dir="auto"><span>Warning</span><br>
It is recommended that you have access to GPT-4 via the OpenAI API. GPT-3.5 will probably fail to make correct knowledge graphs from your data.
Since we still don't have access to GPT-4 OpenAI API, although we made our account a month ago and generated &gt;1$ in billing a week ago,
the <code>init_repo</code>, <code>update_file</code> and <code>add_file</code> endpoints are still untested. We initialized knowledge graphs manually, through ChatGPT.
<strong>Here be dragons.</strong></p>
<h2 tabindex="-1" id="user-content-prerequisites" dir="auto"><a href="#prerequisites">Prerequisites</a></h2>
<p dir="auto">Before you begin, make sure you have the following:</p>
<ul dir="auto">
<li>Obsidian installed on your system.</li>
<li>An active Obsidian vault.</li>
</ul>
<h2 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h2>
<ol dir="auto">
<li>
<p dir="auto"><strong>Download the Plugin:</strong></p>
<ul dir="auto">
<li>Clone the repository inside the plugins folder (your_vault/.obsidian/plugins) using Git:
<div data-snippet-clipboard-copy-content="git clone https://github.com/memgraph/odin.git"><pre><code>git clone https://github.com/memgraph/odin.git
</code></pre></div>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Install Dependencies and Start the Plugin:</strong></p>
<ul dir="auto">
<li>
<p dir="auto">Open your terminal or command prompt.</p>
</li>
<li>
<p dir="auto">Navigate to the plugin's root directory:</p>

</li>
<li>
<p dir="auto">You have the option to install ODIN using Docker, which will automatically install, set up and run the Memgraph database, the backend, and frontend components, or you can manually run the project locally for a more customized setup or if you already have Memgraph up and running.</p>
<h3 tabindex="-1" id="user-content-docker-installation" dir="auto"><a href="#docker-installation">Docker installation</a></h3>
<p dir="auto">Before you start, make sure you have a running <a href="https://www.docker.com/" rel="nofollow">Docker</a> instance and <a href="https://docs.docker.com/compose/install/" rel="nofollow">Docker compose</a> installed.</p>
<ol dir="auto">
<li>You will need to create a <code>.env</code> file inside the ODIN folder with your OpenAI API key to access the app features. It should look like this:</li>
</ol>
<div data-snippet-clipboard-copy-content="OPENAI_API_KEY=YOUR_API_KEY"><pre><code>OPENAI_API_KEY=YOUR_API_KEY
</code></pre></div>
<p dir="auto">Where YOUR_API_KEY is a key you can get <a href="https://openai.com/" rel="nofollow">here</a>.</p>
<ol start="2" dir="auto">
<li>Run:</li>
</ol>

<p dir="auto">It will take up to ten minutes to download and run all dependencies. Now, that you have ODIN successfully installed, you can go to the next step.</p>
<h3 tabindex="-1" id="user-content-manual-installation" dir="auto"><a href="#manual-installation">Manual installation</a></h3>
<p dir="auto">Make sure you have <a href="https://nodejs.org/en/download/current" rel="nofollow">Node.js</a> version 14 or above and <a href="https://docs.npmjs.com/downloading-and-installing-node-js-and-npm" rel="nofollow">npm</a> installed.</p>
<ol dir="auto">
<li>Install the required Node.js dependencies:</li>
</ol>

<ol start="2" dir="auto">
<li>Start the development build:</li>
</ol>

<p dir="auto">You now have the app frontend up and running.</p>
<ol start="3" dir="auto">
<li>You will also need to run the <a href="https://memgraph.com/docs/memgraph/installation" rel="nofollow">Memgraph</a> database and the application backend by following the installation steps for <a href="https://github.com/memgraph/bor">BOR</a> - backend for Obsidian and Rune.</li>
</ol>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Disable Restricted Mode:</strong></p>
<ul dir="auto">
<li>In the Obsidian settings, navigate to "Options."</li>
<li>Click on the "Community plugins" tab.</li>
<li>Click the "Turn on community plugins" button.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Enable the Plugin:</strong></p>
<ul dir="auto">
<li>If you don't see ODIN in the list, try reloading Obsidian.</li>
<li>Navigate back to the "Community plugins" section in the Obsidian settings.</li>
<li>Find "ODIN" in the list of plugins.</li>
<li>Toggle the switch next to the plugin name to enable it.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Plugin Usage:</strong></p>
<ul dir="auto">
<li>The ODIN plugin is now installed and active. You can access its features through the Obsidian interface.</li>
</ul>
</li>
</ol>
<h2 tabindex="-1" id="user-content-features" dir="auto"><a href="#features">Features</a></h2>
<p dir="auto">Most features are accessible through the <code>Graph Prompt view</code> button in the menu opened by clicking the <code>Expand</code> button in the right upper corner of Obsidian.</p>
<ol dir="auto">
<li>Prompt Bar for LLM Queries</li>
</ol>
<ul dir="auto">
<li>ODIN integrates Large Language Models (LLMs) into Obsidian using LangChain, allowing you to ask questions about the data stored in your knowledge graph right from the prompt bar.</li>
</ul>
<ol start="2" dir="auto">
<li>Graph Visualization</li>
</ol>
<ul dir="auto">
<li><code>Vault view</code> will give you a comprehensive understanding of your notes and knowledge by visualizing your entire Obsidian vault as a dynamic knowledge graph.</li>
<li>Switch between <code>Vault view</code> and <code>File view</code> to get a detailed visualization of specific files.</li>
<li>By clicking nodes in the <code>File view</code> you will get highlighted sentences thematically connected to that node in your editor.</li>
</ul>
<ol start="3" dir="auto">
<li>Dropdown Menu Functions</li>
</ol>
<p dir="auto">Right click on the highlighted text in the editor to access the following features:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Generate questions</strong>: Extract thought-provoking questions from your markdown files, encouraging deeper contemplation and critical thinking.</p>
</li>
<li>
<p dir="auto"><strong>Link prediction</strong>: Automatically generate links to other markdown files in your vault that are thematically connected to the highlighted text, enriching your notes with relevant references.</p>
</li>
<li>
<p dir="auto"><strong>Node suggestion</strong>: Access thematically connected nodes related to the highlighted text, fostering meaningful connections and comprehensive understanding of your information.</p>
</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Call to shut down Bristol schools’ use of app to ‘monitor’ pupils and families (156 pts)]]></title>
            <link>https://www.theguardian.com/education/2023/sep/21/calls-to-shut-down-bristol-schools-use-of-think-family-education-app-pupils-and-families</link>
            <guid>37597165</guid>
            <pubDate>Thu, 21 Sep 2023 13:16:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/education/2023/sep/21/calls-to-shut-down-bristol-schools-use-of-think-family-education-app-pupils-and-families">https://www.theguardian.com/education/2023/sep/21/calls-to-shut-down-bristol-schools-use-of-think-family-education-app-pupils-and-families</a>, See on <a href="https://news.ycombinator.com/item?id=37597165">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Criminal justice and antiracist campaigners have raised concerns over an app being used by schools in <a href="https://www.theguardian.com/uk/bristol" data-link-name="in body link" data-component="auto-linked-tag">Bristol</a> to “monitor and profile” pupils and their families.</p><p>The app, which is being used by more than 100 schools, gives safeguarding leads quick, easy access to pupils’ and their families’ contacts with police, child protection and welfare services.</p><p>One of the concerns campaigners have is that the Think Family Education (TFE) app includes analysing which children could be at risk of exposure to criminality, which they argue risks leading to more discrimination against pupils from minority ethnic or working-class backgrounds.</p><p>Staff using the app have told <a href="https://www.fairtrials.org/" data-link-name="in body link">the criminal justice campaign charity Fair Trials</a> that they keep it secret from parents and carers, and admitted many would be concerned about it if they knew of it.</p><p>Bristol city council and Avon and Somerset police, who worked together on the system, insist it is in place to protect children, not criminalise them, and deny it is secret, pointing out that <a href="https://www.bristol.gov.uk/residents/social-care-and-health/children-and-families/insight-bristol" data-link-name="in body link">information about its existence is publicly available</a>.</p><p>But Fair Trials said the vast majority of parents would know nothing about the app. Griff Ferris, the charity’s senior legal and policy officer, said: “Schoolchildren should not be monitored, profiled and criminalised by secretive police databases. Surveillance is not safeguarding.</p><p>“Systems like this uphold existing discrimination against children and families from minoritised ethnic and more deprived backgrounds. This system is expanding the net of surveillance. It should be shut down.”</p><p>A spokesperson for <a href="https://www.nomoreexclusions.com/" data-link-name="in body link">the antiracist organisation No More Exclusions Bristol</a> said: “Technologies that gather and use information in the name of ‘public safety’ overwhelmingly reproduce racialised ideas of problematic behaviour.”</p><p>Liz Fekete, the director of the Institute of Race Relations, strongly criticised elements of the app, saying the approach “stigmatises whole families and leaves even primary school children vulnerable to police surveillance and intelligence gathering”.</p><p>When it was consulted, the Bristol City Youth Council, an elected group of young people, expressed reservations that if the system was not used properly it could lead to “prejudice and judgment”.</p><p>Systems to collate information about children are used in other parts of England but Bristol city council describes Think Family as “innovative” and a number of local authorities are watching how the app works.</p><p>On its website, the council says the <a href="https://www.bristol.gov.uk/residents/social-care-and-health/children-and-families/insight-bristol" data-link-name="in body link">Think Family database</a>, which the app draws on, includes information from about 50,000 families across the city collected from agencies including social care, police and the Department for Work and Pensions. It says it highlights “vulnerabilities or needs” and uses “targeted analytics” to help identify children at risk of sexual or criminal exploitation.</p><p>Critics say the reality is that this risks children from minority ethnic or poorer backgrounds being profiled as being involved in gangs or county lines operations.</p><p>Schools using the TFE app receive alerts about children’s and family members’ contact with police, antisocial behaviour and domestic violence incidents. The system also gives schools access to sensitive personal details about families’ financial situations.</p><p>School safeguarding leads told Fair Trials that they kept the system secret from children and their families. One said: “They [parents and carers] wouldn’t know about this ... parents will have no kind of sight of it at all ... They just don’t know of its existence.”</p><p>They described the system as “an early warning process” and admitted: “I think there’s a bit of a risk it getting out there that schools hold this kind of central bank of information.”</p><p>A spokesperson for Bristol city council said the Think Family database was introduced to counter the trend of agencies working in silos at a time of a “generational squeeze” on public finances.</p><p>The spokesperson said: “The introduction of the Think Family Education app means that schools … have access to appropriate information in a secure and restricted way to make decisions about how they support children. There are strict controls in place about who can access this information, how they do this and the reasons why.”</p><p>A spokesperson for Avon and Somerset police said the database gave professionals working with children joined-up information to identify and safeguard those at risk of criminal and sexual exploitation.</p><p>“The TFE app gives professionals immediate access to this information, helping them to act swiftly on any identified risks.” The spokesperson said neither the app or database assessed the likelihood of an individual to commit a crime.</p><p>The force said “robust privacy and sharing agreements” had been approved by <a href="https://ico.org.uk/" data-link-name="in body link">the Information Commissioner’s Office</a> and development of the system done in collaboration with the <a href="https://www.gov.uk/government/organisations/centre-for-data-ethics-and-innovation" data-link-name="in body link">Centre for Data Ethics and Innovation</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nippon Television has just acquired Studio Ghibli (516 pts)]]></title>
            <link>https://www.catsuka.com/breves/2023-09-21/nippon-television-rachete-le-studio-ghibli</link>
            <guid>37596788</guid>
            <pubDate>Thu, 21 Sep 2023 12:44:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.catsuka.com/breves/2023-09-21/nippon-television-rachete-le-studio-ghibli">https://www.catsuka.com/breves/2023-09-21/nippon-television-rachete-le-studio-ghibli</a>, See on <a href="https://news.ycombinator.com/item?id=37596788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>Nippon Television</strong> vient de racheter le <strong>Studio Ghibli</strong>, qui deviendra une filiale de leur soci�t�.</p><p>

Le producteur Toshio Suzuki (75 ans) cherchait un successeur, il a propos� Goro Miyazaki, mais Hayao Miyazaki (82 ans) a refus� (et Goro Miyazaki aussi).</p><p>

Nippon Television, qui d�tiendra d�sormais 42,3% des droits de vote, �tait d�j� un partenaire de longue date de Ghibli, notamment � travers la case TV "Friday Road Show", qui diffuse les films du studio sur la cha�ne depuis Nausicaa.</p><a href="https://pbs.twimg.com/media/F6iDsiSWsAAPJZY?format=jpg&amp;name=large" target="_blank"><center><img src="https://pbs.twimg.com/media/F6iDsiSWsAAPJZY?format=jpg&amp;name=small"></center></a><p>

From <a href="https://twitter.com/catsuka/status/1704757264324665345" target="_blank">Catsuka on Twitter</a> :</p><center>

<blockquote><p lang="en" dir="ltr">Nippon Television has just acquired Studio Ghibli.<br>Producer Toshio Suzuki (75) was looking for a successor, and proposed Goro Miyazaki, but Hayao Miyazaki (85) refused (and so did Goro).<br>And Nippon Television was an old Ghibli's partner (Friday Road Show).<a href="https://t.co/EX4jsKXrkP">https://t.co/EX4jsKXrkP</a> <a href="https://t.co/n0TkyTxeZS">pic.twitter.com/n0TkyTxeZS</a></p>� Catsuka 💙 (@catsuka) <a href="https://twitter.com/catsuka/status/1704757264324665345?ref_src=twsrc%5Etfw">September 21, 2023</a></blockquote>

</center><p>

Source : <a href="https://www.oricon.co.jp/news/2295679/full/" target="_blank">Oricon</a></p><div><p><img src="https://www.catsuka.com/interf/images/english_flag.gif"> <b>English audience</b> :</p><p>Nippon Television has just acquired Studio Ghibli, which will become a subsidiary of their company.<br>
Producer Toshio Suzuki (75y old) was looking for a successor, and proposed Goro Miyazaki, but Hayao Miyazaki (82y old) refused (and so did Goro).<br>
Nippon Television was already a long-standing partner of Ghibli ("Friday Road Show").</p><p>(2023/09/21)</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Kakoune (123 pts)]]></title>
            <link>https://andreyor.st/posts/2023-09-20-why-kakoune/</link>
            <guid>37596776</guid>
            <pubDate>Thu, 21 Sep 2023 12:42:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andreyor.st/posts/2023-09-20-why-kakoune/">https://andreyor.st/posts/2023-09-20-why-kakoune/</a>, See on <a href="https://news.ycombinator.com/item?id=37596776">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <p>Recently I’ve stumbled upon a video about Kakoune, a code editor: <a href="https://www.youtube.com/watch?v=5WLlLxU2EZE" target="_blank">Idiot user tries to use Kakoune (for notes? Also Helix?)</a>.
Funnily enough, I was <a href="https://www.youtube.com/watch?v=5WLlLxU2EZE&amp;t=1004s" target="_blank">mentioned</a> in this video, which was a surprise, and made me laugh for quite a while:</p>
<blockquote>
<p>Let’s go back to the official plugins page.
This guy has made a bunch of plugins.
<strong>Who is he?</strong>
<em>How is he able to make such good use of Kakoune?</em>
<strong>Oh, he’s an Emacs user!</strong>
<strong>Of course!</strong></p>
</blockquote>
<p>Yeah, I am.</p>
<p>Even though the video <strong>is</strong> about Kakoune, the author’s main focus is note-taking and the oddities that come with this process when using a <em>code editor</em> to edit <em>text</em>.
Kakoune advertises itself as a code editor for the most part, and I have to agree.
As far as I know, Maxime Coste (<a href="https://github.com/mawww" target="_blank">@mawww</a>), the creator of Kakoune, made it because of the desire for a better programming experience.
As a result, a small and contained code editor was made.
And I can appreciate a desire of the system one can fully grasp and understand.</p>
<p>Kakoune is relatively small, compared to Emacs and Vim that is.
It doesn’t feature a scripting language, instead relying on shelling out if you need any programmable features.
It’s a clever trick, and the editor exposes its internal state as a set of shell variables, so you still can do interactive things based on your workflow.
And I did a lot of this back in the day when I used Kakoune.</p>
<p>I have mentioned Kakoune in this blog previously, but it was rather sparse.
The reason for that is stated in the video pretty accurately - I use Emacs and not Kakoune, so there’s little to no reason for me to write about it besides occasional praise or comparisons.
I mention it on my about page, and in various text-editor-related posts, but that’s it.
I don’t participate in the Kakoune community anymore, and no longer actively maintain my packages, as I no longer use Kakoune.</p>
<p>But I still need to address <a href="https://youtu.be/5WLlLxU2EZE?t=1017" target="_blank">this</a> point of the video - I wasn’t an Emacs user when I started with Kakoune.
Before Kakoune I was a Vim user!
And transition from Vim to Kakoune was caused by several factors, one of which is again stated in the video:</p>
<blockquote>
<p>…and people have written so many damn Vim plugins over the years that if you have a need it’s already been addressed like three or four different ways.
So with Vim you could just piece together your ideal text editor like LEGO bricks…</p>
</blockquote>
<p>And that’s exactly my problem with Vim - too many ways to do the same thing.
At the time, I was working with SoC in C and started using the <a href="https://github.com/dense-analysis/ale" target="_blank">Ale</a> plugin for asynchronous linting of the project, as the synchronous linting was quite slow.
This was before LSP inception - just look at <a href="https://github.com/dense-analysis/ale/blob/master/supported-tools.md" target="_blank">how many tools Ale supports</a>.
As far as I remember LSP was added to Ale much later, when competing plugins showed up.</p>
<p>Competing plugins.</p>
<p>There’s nothing bad with competition on its own, and in Emacs, this is also present, with many plugins, but in Vim’s case, I feel that people created most of the plugins purely because of the NIH<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> syndrome.
I lost count of how many plugins provided autocomplete interfaces, there were tons of list-narrowing frameworks, plugin managers, and snippet managers, and everything was poorly integrated with each other.
I remember that some autocomplete plugins did not integrate well or at all with snippets, some synchronous completion providers were not supported by asynchronous completion frameworks, and so on.
Again, this was before LSP came to the scene and basically became a standard for these features.
So perhaps the situation is a bit better today, but I still have doubts.
NeoVim people are going crazy over Lua API, writing their configs in Fennel, and making new Lua-based plugins that may or may not be compatible with the rest of the ecosystem.</p>
<p>Fact is, you <em>can</em> piece your dream text editor like <em>LEGO</em> bricks, just beware that some of these bricks are actually <a href="https://en.wikipedia.org/wiki/Lego_Duplo" target="_blank">Duplo</a> blocks, some are <a href="https://en.wikipedia.org/wiki/Cobi_%28building_blocks%29" target="_blank">COBI</a> bricks, lots and lots are probably <a href="https://brickscompare.com/brands/8-lele" target="_blank">LELE</a>, and some are even freakin’ <em><a href="https://andreyor.st/2023-09-20-why-kakoune/oleg.jpg">OLEG</a></em>.
Well, at least that was the situation when I used Vim, I gave up on updating my config around 2018 and made the switch to Kakoune.
By that time I already made three plugins for Vim because I was unsatisfied with existing ones, but they were crappy too.
Integrating these plugins into different other plugins was a huge pain.
So I <a href="https://github.com/andreyorst/dotfiles/commit/d91e8ca4ccb59c89c2043d5d2c4eb8af3fbe498d" target="_blank">made the switch</a>.
You can trace the history from that point if you’re interested in my Kakoune journey (why would you be though).</p>
<p>Obviously, I missed a lot of features from Vim, and Kakoune actually has an entry on their wiki on how to migrate from Vim.
Unfortunately, though, the suggestions were either too hardcore-minimalist or uncooperative.
For example, Vim’s <code>smarttab</code> feature didn’t exist, and <code>expandtab</code> was <a href="https://github.com/mawww/kakoune/wiki/Indentation-and-Tabulation/e6756dc1a8af07add53145a9251eeb2ba0e0c5a5" target="_blank">suggested</a> to be done via hooks.
Not that it was wrong, but it was suggested when people asked about a very specific feature of Vim, and these did not provide the same feature as in Vim.
So I started writing plugins.</p>
<p>However, Kakoune didn’t have conventional plugins at all at that time.
Well, there was a section with plugins on the official page, but there was no real ecosystem.
There was nothing such as Vimplug if you will.</p>
<p>Installing plugins meant you had to manually copy files around, or load them pathogen-style, but the process wasn’t convenient or easy to automate in my opinion.
Updating plugins installed in this way was problematic too.
This motivated me to make <a href="https://github.com/andreyorst/plug.kak" target="_blank">plug.kak</a>.
And then I started experimenting more and more with other interesting plugins.</p>
<p>But, around the same time I switched to Kakoune, I briefly tried Emacs.
In reality, I tried Emacs like 4 times at that point, the earliest one dates back to around 2010.
All four times I did not succeed, but something gravitated me to it for some reason.
This last one actually was a reason why I made some plugins like <a href="https://github.com/andreyorst/langmap.kak" target="_blank">langmap.kak</a> or <a href="https://github.com/andreyorst/kaktree" target="_blank">kaktree</a>, which resemble what I saw in Emacs at that point.
Many plugins were inspired by Vim, like <a href="https://github.com/andreyorst/smarttab.kak" target="_blank">smarttab.kak</a>, <a href="https://github.com/andreyorst/powerline.kak" target="_blank">powerline.kak</a>, <a href="https://github.com/andreyorst/fzf.kak" target="_blank">fzf.kak</a>, <a href="https://github.com/andreyorst/tagbar.kak" target="_blank">tagbar.kak</a>, equivalents to which I daily used in Vim before.
And at that time, Kakoune really did everything I needed and was a very capable code editor.</p>
<p>But I still wanted something more.
So why I made the switch to Emacs - but for a bit different reasons.</p>
<p>First of all, I started enjoying writing more prose instead of just writing code.
And if you’ve watched the video I linked above, the author similarly wants a text editor, not a code editor.
There’s <a href="https://www.youtube.com/watch?v=XRpHIa-2XCE" target="_blank">another video</a> on their channel about note-taking, featuring a lot of programs made specifically for this task, and it features a text editor section at the end in which the author talks about Emacs and Vim, briefly touching Kakoune and Helix.
What they’re saying about Emacs is also very similar to what I’ve experienced, although I didn’t use an Emacs distribution, I started with vanilla<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.</p>
<p><strong>Org Mode.</strong></p>
<p>I don’t know how to explain this to a non-Emacs, non-Org person, but every time someone asks the “Why Emacs?” question, Org Mode is somewhere at the top of the answers.
And I never understood that, until I tried for myself.
And boy are they right.
But before I understood that, there was a <a href="https://github.com/andreyorst/dotfiles/commit/442941e8a230ccbb075a7bdfcf6075367c46cd73" target="_blank">long</a> period of <a href="https://github.com/andreyorst/dotfiles/commit/9088b4fd041005c6143de8de012541f0bc6f913f" target="_blank">adoption</a>.
I still used Kakoune, but more and more I was shifted towards Emacs - it slowly consumed me.</p>
<p>A big part of that was that I started writing in Lisps.
Emacs is <strong>the king</strong> when it comes to Lisp editing.
Plugins for various Schemes, that I used to do tasks from the SICP book were amazing, and Kakoune <code>:repl</code> command paled in comparison.
Though I can’t blame anyone here - Emacs is a lisp machine on its own, it’s bound to have great lisp editing experience.</p>
<p>Though lisp wasn’t my primary language back then, more like a novelty.
I used Rust and considered switching jobs from a C engineer to a Rust back-end developer.
Rust seemed both a perspective and a <em>safe</em> enough bet for the foreseeable future.
Who knew how the tables would turn?!</p>
<p>Kakoune actually was great as a Rust IDE of sorts.
The <a href="https://github.com/kak-lsp/kak-lsp" target="_blank">kak-lsp</a> plugin was on it, written in Rust it supported Rust well.
And it helped me at work with C too.
That was 2019, the year I started using Emacs for real.</p>
<p>That year, I made my first, kinda big post, in which I realized that I wanted to write more.
Ironically, it was a <a href="https://discuss.kakoune.com/t/i-have-been-using-emacs-at-work-for-whole-week/" target="_blank">post about Emacs on the Kakoune forum</a>.
It was even written partly in Emacs and partly in Kakoune - I was comparing editors at that time, much like the author of already mentioned videos.
But this day signified that I was ready to fully migrate to Emacs - my config was more or less ready for work at that point.
Emacs seemed better at writing, although I was missing cool Kakoune features, such as multiple selections, a lot.
I started writing this blog in 2020, and it was done in Emacs from the get-go.
Not so long after that, I moved to Emacs <a href="https://github.com/andreyorst/dotfiles/commit/0e31f525b069f097a99cd1876a009f03cd26299c" target="_blank">completely</a>.</p>
<p>I used Kakoune for 1.9835616438356165 years (first commit on Jul 24 2018, last commit on Jul 17 2020).</p>
<p>But this post actually is called “Why Kakoune” and not “Why I switched to Emacs”, so let’s address that!</p>
<h2 id="why-kakoune">Why Kakoune</h2>
<p>What an awfully long preamble.
If you read that, you have my thanks.
If not - fair enough.</p>
<p>I think it’s kinda weird to read reasoning on why someone should use Kakoune from someone who’s not using Kakoune right now and hasn’t for another three years already.
But, as far as I can see, not much has changed in Kakoune since!
Which, actually, is great - I can actually just check out to a commit previous to the one I deleted my Kakoune config in the <code>dotfiles</code> repository, and run it.
A fresh clone of Kakoune’s latest stable release builds in just two minutes on my machine, and loads my old configuration without too many errors:</p>
<figure><img src="https://andreyor.st/2023-09-20-why-kakoune/kakoune.png">
</figure>

<p>At this point of the post I wanted to write about stability, but as it seems, the situation isn’t that great.</p>
<ul>
<li>Some plugins simply no longer exist.</li>
<li>Some defaults were changed in 2022, making keys behave differently (can be turned back via a remap)</li>
<li>Some changes were made to how Kakscript is interpreted.</li>
<li>Most of my plugins broke (but that’s on me).</li>
<li>There’s possibly more, but I’m out of the loop.</li>
</ul>
<p>What’s hasn’t changed is that <a href="https://discuss.kakoune.com/t/ive-lost-my-syntax-highlighting/" target="_blank">people</a> <a href="https://discuss.kakoune.com/t/autoload-directory-disables-doc-command/1656" target="_blank">still</a> <a href="https://github.com/mawww/kakoune/issues/4301" target="_blank">stumble</a> on the <code>autoload</code> directory after all these <a href="https://github.com/mawww/kakoune/issues/1" target="_blank">years</a>.
Because there’s no plugin manager in Kakoune, it relies on storing scripts you want to load automatically during startup in the <code>~/.config/kak/autoload</code> directory.
This, however, for some weird reason, disables loading a system-wide Kakoune <code>autoload</code> directory, and Kakoune simply stops loading all of its inbuilt features that are shipped as <code>.kak</code> files.
I also experienced this problem, and it was one of the main reasons for making <code>plug.kak</code>.
So, if anything above seems too weird, perhaps Kakoune is not for you.</p>
<p>But, given all that, Kakoune hasn’t changed that drastically over the three years I haven’t used it, and that’s a good thing.
Even now, I can still edit files in it pretty comfortably after my brain does the switch from Emacs keybindings to a modal model.
For the most part, that is, some habits are hard.</p>
<p>But one thing, that I think can be a main reason why people should try Kakoune, in my opinion, is its POSIX integration.
Back in the day, I really liked this idea, can’t say so today<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>, <strong>but</strong>, it’s still a good reason why Kakoune is interesting.</p>
<p>I mentioned that I made plugins for Kakoune, and because of that I now know POSIX sh pretty well.
Not that I need this knowledge that often, but when I do, I’m glad Kakoune taught me well.
The same goes for other POSIX tools - Kakoune basically forces you to learn your shell stuff, because there’s no other way to be productive in Kakoune.
Everything is done via shelling out to use some tool like <code>fmt</code>, <code>grep</code>, <code>find</code>, etc.</p>
<p>And when shell tools are not enough you can always call different programming languages from the shell.
For example, some of my plugins are written in Perl of all languages.
And while I can’t say that I’m proud of that, or that I know Perl that well, I can still say that Kakoune <a href="https://github.com/andreyorst/langmap.kak/blob/fe72a9980988c97ec901df1c36129b6959468b08/perl/langmap.pl" target="_blank">made</a> me <a href="https://github.com/andreyorst/langmap.kak/blob/fe72a9980988c97ec901df1c36129b6959468b08/perl/display_layout.pl" target="_blank">learn</a> <a href="https://github.com/andreyorst/kaktree/blob/acd47e0c8549afe1634a79a5bbd6d883daa8ba0a/perl/kaktree.pl" target="_blank">Perl</a>, well, to some degree.
Also, <a href="https://github.com/andreyorst/dotfiles/blob/187ebb84f9542b76a4f3c3e08f9533cd8187faa1/.config/kak/commands.kak#L151-L192" target="_blank">Awk</a>.
And I still occasionally use both when I need to send a code snippet to my colleague so that they can send me some filtered logs instead of full logs.
Because that’s what these tools excel at, and learning how to use them from within an editor really makes it apparent how they can be useful.
So Kakoune really helps you learn your standard tools, and some extra things too.</p>
<p>Another thing I think can be said is that Kakoune really makes you learn and understand regular expressions.
When I started using Kakoune, I once told my friend that I started using an editor that is built around using regular expressions for text manipulation.
They were quite skeptical, because I didn’t know regular expressions back then, and they had some experience and said that it’s a terrible idea to use them at all.
But turns out, that regular expressions are actually easy to learn, and Kakoune really helps with that, because you’re constantly creating multiple selections, selections in selections, and filtering selections - all done with regexes.
So, if you think that regexes are hard and you’ll never learn them (and you’re a Vim user by chance), give Kakoune a try.</p>
<p>And finally, Kakoune is just fun!
Especially if you’re a seasoned Vim user, the inverted paradigm of object-verb really messes with your brain.
I think Kakoune features a really unique editing model, where it doesn’t need any separate mode for selecting text - all motions do it automatically.
When I started, I adjusted to the object verb paradigm pretty quickly, it’s very natural to how things are done - in real life, we usually don’t think upfront what we want to clean and then how many of <em>what was that</em>, ah yeah the shelves.
We think that these shelves are dusty and we need to clean them.
I should probably do it right now.</p>
<p>Anyway, a TL;DR for this could as well have been:</p>
<blockquote>
<p>Kakoune gives you:</p>
<ul>
<li>Small and understandable core.</li>
<li>Proficiency with POSIX tools,
<ul>
<li>and maybe even some programming languages other than <code>sh</code>.</li>
</ul>
</li>
<li>Structural regular expressions as a central way of text manipulation.
<ul>
<li>With multiple selections created via regular expressions, acting upon regular expressions.</li>
</ul>
</li>
<li>Fresh take on the modal editing paradigm.</li>
</ul>
</blockquote>
<p>So, yeah, Kakoune definitively deserves your attention, if you’re into experiments with your workflow.
I, certainly, am.
At least, I was, now I do everything from Emacs.</p>


  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Airlines Are Just Banks Now (302 pts)]]></title>
            <link>https://www.theatlantic.com/ideas/archive/2023/09/airlines-banks-mileage-programs/675374/</link>
            <guid>37596755</guid>
            <pubDate>Thu, 21 Sep 2023 12:40:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/ideas/archive/2023/09/airlines-banks-mileage-programs/675374/">https://www.theatlantic.com/ideas/archive/2023/09/airlines-banks-mileage-programs/675374/</a>, See on <a href="https://news.ycombinator.com/item?id=37596755">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header data-event-module="hero"><div><div><p>They make more money from mileage programs than from flying planes—and it shows.</p></div><div><figure><div><picture><img alt="Illustration of planes against a backdrop of a credit card" sizes="(min-width: 976px) 976px, 100vw" srcset="https://cdn.theatlantic.com/thumbor/i1zxOBjUfAa-LcAk39Vpb8-iF8U=/0x0:2000x1125/750x422/media/img/mt/2023/09/airlines_are_banks/original.jpg 750w, https://cdn.theatlantic.com/thumbor/wbxnD5HvcVxcZq5D4_fZVem-SAU=/0x0:2000x1125/828x466/media/img/mt/2023/09/airlines_are_banks/original.jpg 828w, https://cdn.theatlantic.com/thumbor/AY1IMnrdT1NrCRmGfjeURl9WzME=/0x0:2000x1125/960x540/media/img/mt/2023/09/airlines_are_banks/original.jpg 960w, https://cdn.theatlantic.com/thumbor/tYcp3eiVo3YWMCqFQx21qfiBwtE=/0x0:2000x1125/976x549/media/img/mt/2023/09/airlines_are_banks/original.jpg 976w, https://cdn.theatlantic.com/thumbor/_Tgm4RT8P6wSfIJSTVjg7zzGljA=/0x0:2000x1125/1952x1098/media/img/mt/2023/09/airlines_are_banks/original.jpg 1952w" src="https://cdn.theatlantic.com/thumbor/AY1IMnrdT1NrCRmGfjeURl9WzME=/0x0:2000x1125/960x540/media/img/mt/2023/09/airlines_are_banks/original.jpg" width="960" height="540"></picture></div><figcaption>Illustration by The Atlantic. Sources: Getty.</figcaption></figure></div></div><div><p><time datetime="2023-09-21T11:00:00Z">September 21, 2023, 7 AM ET</time></p></div><gpt-ad format="injector" sizes-at-0="mobile-wide" targeting-pos="injector-article-start" sizes-at-976="desktop-wide"></gpt-ad></header><section data-event-module="article body"><p>L<span>ast week</span>, Delta Air Lines announced <a data-event-element="inline link" href="https://thepointsguy.com/news/delta-skymiles-changes/">changes</a> to its SkyMiles program that will make accruing status and taking advantage of perks much harder. Instead of relying on a combination of dollars spent and miles traveled in the air, Delta will grant status based on a single metric—dollars spent—and raise the amount of spending required to get it. In short, SkyMiles is no longer a frequent-flier program; it’s a big-spender program. These changes are so drastic that one of the reporters at the preeminent travel-rewards website The Points Guy <a data-event-element="inline link" href="https://thepointsguy.com/news/why-i-wont-chase-airline-status/">declared</a> that he’s going to “stop chasing airline status.”</p><p>When even the points insiders are sick of playing the mileage game, something has clearly gone wrong. In fact, frequent-flier programs are a symptom of a much deeper rot in the American air-travel industry. And although getting mad at airlines is perfectly reasonable, the blame ultimately lies with Congress.</p><p>From the late 1930s through the ’70s, the federal government regulated airlines as a public utility. The Civil Aeronautics Board decided which airlines could fly what routes and how much they could charge. It aimed to set prices that were fair for travelers and that would provide airlines with a modest profit. Then, in 1978, Congress passed a sweeping law deregulating the airline industry and ultimately abolishing the CAB. Unleashed from regulation, airlines devised new tactics to capture the market. American Airlines was one of the most aggressive. In the lead-up to the deregulation bills, it created discount “super saver” fares to sell off the final few remaining seats on planes. That meant cheap prices for last-minute travelers and more revenue for American, because the planes were going to take off whether or not the seat was filled. But these fares upset business travelers, who tended to buy tickets further in advance for higher prices. So in <a data-event-element="inline link" href="https://www.yahoo.com/news/timeline-events-american-airlines-history-011902886.html?guccounter=1">1981</a>, American developed AAdvantage, its frequent-flier program, to give them additional benefits. Other airlines followed suit.</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/newsletters/archive/2022/06/summer-air-travel-flights-cancelled/661385/">Read: Air travel is a disaster right now. Here’s why.</a></p><p>In the early years, these programs were simple, like the punch card at a café where your 11th coffee is free. But three big changes transformed them into the systems we know today. First, in 1987, American partnered with Citibank to offer a branded credit card that offered points redeemable for flights on the airline. Second, in the ’90s, the airlines proliferated the number of fare classes, charging differential prices for tickets. With more complicated fare structures came the third change: Virgin America realized that the amount people spend on a flight, based on the fare class, is more important to their bottom line than the number of miles flown. So, in 2007, it introduced a loyalty <a data-event-element="inline link" href="https://www.usatoday.com/story/travel/roadwarriorvoices/2016/02/25/frequent-flier-miles-points-loyalty-programs/80860962/">program</a> rewarding money spent rather than mileage accrued.</p><p>These three shifts fundamentally transformed the airline industry. They turned frequent-flier systems into the sprawling points systems they are today. And they turned airlines into something more like financial institutions that happen to fly planes on the side.</p><p>Here’s how the system works now: Airlines create points out of nothing and sell them for real money to banks with co-branded credit cards. The banks award points to cardholders for spending, and both the banks and credit-card companies make money off the swipe fees from the use of the card. Cardholders can redeem points for flights, as well as other goods and services sold through the airlines’ proprietary e-commerce portals.</p><p>For the airlines, this is a great deal. They incur no costs from points until they are redeemed—or ever, if the points are forgotten. This setup has made loyalty programs highly lucrative. <a data-event-element="inline link" href="https://www.fastcompany.com/90934980/how-much-do-we-charge-to-our-delta-air-american-express-cards-its-a-lot">Consumers now</a> charge nearly 1 percent of U.S. GDP to Delta’s American Express credit cards alone. A 2020 analysis by the <i>Financial Times</i> <a data-event-element="inline link" href="https://www.ft.com/content/1bb94ed9-90de-4f15-aee0-3bf390b0f85e">found</a> that Wall Street lenders valued the major airlines’ mileage programs more highly than the airlines themselves. United’s MileagePlus program, for example, was valued at $22 billion, while the company’s market cap at the time was only $10.6 billion.</p><p>Is this a good deal for the American consumer? That’s a trickier question. Paying for a flight or a hotel room with points may feel like a free bonus, but because credit-card-swipe fees increase prices across the economy—Visa or Mastercard takes a cut of every sale—redeeming points is more like getting a little kickback. Certainly the system is bad for Americans who don’t have points-earning cards. They pay higher prices on ordinary goods and services but don’t get the points, effectively <a data-event-element="inline link" href="https://www.brookings.edu/articles/how-credit-card-companies-reward-the-rich-and-punish-the-rest-of-us/">subsidizing</a> the perks of card users, who tend to be wealthier already.</p><p>Like the federal reserve, airlines issue currency—points—out of thin air. They also get to decide how much that currency is worth and what it can be spent on. This helps explain why the points system feels so opaque and, often, unfair. Online analysts try to offer estimates of points’ cash value, but airlines can <a data-event-element="inline link" href="https://viewfromthewing.com/delta-air-lines-is-even-devaluing-your-banked-rollover-elite-qualifying-miles/">reduce</a> these <a data-event-element="inline link" href="https://viewfromthewing.com/delta-ceo-were-not-done-making-changes-to-skymiles-status-or-first-class/">values</a> after the fact and change how points can be redeemed. Airlines even <a data-event-element="inline link" href="https://www.forbes.com/advisor/credit-cards/is-buying-frequent-flyer-miles-ever-a-good-deal/">sell</a> <a data-event-element="inline link" href="https://www.nerdwallet.com/article/travel/times-it-actually-makes-sense-to-buy-miles">points</a> at above their exchange-rate valuation, meaning that people are paying for something worth less than the money they’re buying it with, in part because it’s so hard to know what the real value is.</p><p>In this context, it’s easy to see why Delta is making changes. The shift to a focus on spending, rather than mileage, has long been coming, because of the rise of multiple fare classes and the decoupling of mileage and revenue. Limiting benefits and increasing the requirements for status, meanwhile, looks like a way to spread out costs: 1 percent of GDP spending is a lot of outstanding points that could be redeemed.</p><p>Still, you might wonder how airlines can get away with angering their customers by devaluing loyalty programs. Aren’t they worried that those customers will get a little less loyal? Well, not really. The U.S. has only four major carriers, which <a data-event-element="inline link" href="https://www.npr.org/2023/03/07/1161640389/jetblue-spirit-airlines-doj-lawsuit">account</a> for more than three-quarters of the market, and they tend to move in lockstep. Indeed, American Airlines recently made a <a data-event-element="inline link" href="https://thepointsguy.com/news/american-aadvantage-vs-delta-skymiles/">similar</a> change to its mileage program. Customers don’t have many other places to go.</p><p>I<span>n this</span> and other respects, the strange evolution of airlines into quasi-banks reflects how badly deregulation has gone. Regulation carefully set the terms under which airlines could do business. It was designed to ensure that they remained a stable business and a reliable mode of transportation. Deregulation, in turn, allowed the airlines to pursue profits in whatever way they could—including getting into the financial sector.</p><p>The proponents of deregulation made a few big promises. The cost of flying would go down once airlines were free to compete on price. The industry would get less monopolistic as hundreds of new players entered the market, and it would be stable even without the government guaranteeing profitable rates. Small cities wouldn’t lose service. In the deregulators’ minds, airlines were like any other business. If they were allowed to compete freely, the magic of the market would make everything better. Whatever was good for the airlines’ bottom line would be good for consumers.</p><p>They were wrong. As I explain in my <a data-event-element="inline link" href="https://tertulia.com/book/why-flying-is-miserable-and-how-to-fix-it-ganesh-sitaraman/9798987053584?affiliate_id=atl-347">forthcoming book</a>, most of their predictions didn’t come true, because air travel isn’t a normal business. There are barriers to entry, such as the fixed supply of airport runways and gates. (And, for that matter, mileage programs, designed to keep customers from ditching an established airline for a rival.) There are network effects and economies of scale. There are high capital costs. (Airplanes aren’t cheap.) The idea that anyone could successfully start an airline and outcompete the big incumbents never made much sense.</p><p>After a relatively short period of fierce competition, the deregulated era quickly turned to consolidation and cost-cutting, as dozens of airlines either went bankrupt or were acquired. Service keeps getting worse, because the airlines, facing little competition, have nothing to fear from antagonizing passengers with cramped legroom, cancellations, and ever-multiplying fees for baggage and snacks. Worse still, without mandated service, cities and regions across the country have lost commercial air service, with <a data-event-element="inline link" href="https://washingtonmonthly.com/2012/03/01/terminal-sickness/">serious consequences</a> for their economies. And when a crisis like 9/11 or the coronavirus pandemic comes along, the airlines—which prefer to direct their profits to stock buybacks rather than rainy-day funds—need massive financial relief from the federal government.</p><p id="injected-recirculation-link-1" data-view-action="view link - injected link - item 2" data-event-element="injected link" data-event-position="2"><a href="https://www.theatlantic.com/technology/archive/2023/06/airline-customer-service-chatbot-ai/674412/">Read: Somehow, airline customer service is getting even worse</a></p><p>Deregulation even failed to deliver the one thing it is sometimes credited with: lowering prices. Airfare did get cheaper in the years after the 1978 deregulation law. But the cost of flying had already been falling before<i> </i>deregulation, and it kept falling after at about the same rate.</p><p>The old system of airline regulation wasn’t perfect. Barred from competing directly on price, the airlines got into an amenities arms race that notoriously included <a data-event-element="inline link" href="https://www.youtube.com/watch?v=KnimcgMPuXk">in-flight piano bars</a>. But the cure was worse than the disease. The industry went from being a regulated oligopoly, which had real problems, to an unregulated oligopoly, which we are now seeing is much worse.</p><p>Airlines serve a vital public need, just like railroads, the electric grid, and communication networks. They also exist within a system of special privileges from the government. The public has built and paid for a substantial federal infrastructure to coordinate flights safely. Historically, these are all standard reasons to regulate an industry. A modernized set of rules could arrest the trajectory of airlines becoming financialized e-commerce platforms—and maybe even get them to focus on making air travel less miserable.</p><div data-view-action="view - affiliate module" data-view-label="Why Flying Is Miserable - And How To Fix It"><a href="https://web.tertulia.com/book/9798987053584?affiliate=atl-347" rel="noopener noreferrer" data-label="Why Flying Is Miserable - And How To Fix It" data-action="click link - affiliate module - book cover" target="_blank"><picture><img alt="" loading="lazy" srcset="https://cdn.theatlantic.com/thumbor/Zh_7jqRmMvGK6Gg6nXJl9R3pn9E=/0x0:333x500/80x120/media/img/book_reviews/2023/09/19/51XeRgTLhgL._SL500_/original.jpg, https://cdn.theatlantic.com/thumbor/KdBuSjWrqT4SQxIcOGX5eMfm2WE=/0x0:333x500/160x240/media/img/book_reviews/2023/09/19/51XeRgTLhgL._SL500_/original.jpg 2x" src="https://cdn.theatlantic.com/thumbor/Zh_7jqRmMvGK6Gg6nXJl9R3pn9E=/0x0:333x500/80x120/media/img/book_reviews/2023/09/19/51XeRgTLhgL._SL500_/original.jpg" width="80" height="120"></picture></a></div><div><hr><p>​When you buy a book using a link on this page, we receive a commission. Thank you for supporting<!-- --> <span>The Atlantic.</span></p></div></section><gpt-ad format="injector" sizes-at-0="mobile-wide,native,house" targeting-pos="injector-most-popular" sizes-at-976="desktop-wide,native,house"></gpt-ad></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BrainTree has been down for more than 7 hours now (134 pts)]]></title>
            <link>https://www.paypal-status.com/incident/production</link>
            <guid>37596498</guid>
            <pubDate>Thu, 21 Sep 2023 12:15:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.paypal-status.com/incident/production">https://www.paypal-status.com/incident/production</a>, See on <a href="https://news.ycombinator.com/item?id=37596498">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Cisco Acquires Splunk (691 pts)]]></title>
            <link>https://www.splunk.com/en_us/blog/leadership/splunk-and-cisco-unite-to-accelerate-digital-resilience-as-one-of-the-leading-global-software-companies.html</link>
            <guid>37596497</guid>
            <pubDate>Thu, 21 Sep 2023 12:15:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.splunk.com/en_us/blog/leadership/splunk-and-cisco-unite-to-accelerate-digital-resilience-as-one-of-the-leading-global-software-companies.html">https://www.splunk.com/en_us/blog/leadership/splunk-and-cisco-unite-to-accelerate-digital-resilience-as-one-of-the-leading-global-software-companies.html</a>, See on <a href="https://news.ycombinator.com/item?id=37596497">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-emptytext="Blogs details page content">
            <p>For nearly 20 years, Splunk has been delivering on the idea that harnessing the power of data can help our customers solve the most complex problems that test the resilience of their digital systems. In recent years, the advent of AI and continued demand for digital transformation have created a world of new possibilities, along with complex challenges. Organizations now have a greater surface area to protect and infinitely more data to manage — often across fragmented, hybrid and multi-cloud environments.</p>
<p>As our customers have had to evolve to meet these challenges, Splunk has transformed to deliver exceptional value. Along the way, we have stayed true to our customer promise: to be a step ahead of their needs and continually deliver meaningful innovations that keep mission-critical systems secure and reliable.&nbsp;</p>
<p><span><b>Today, we took the next step in our journey to advance this promise and realize our vision for the future of security and observability by joining forces with Cisco.</b></span></p>
<p>Uniting with Cisco is a transformative milestone for Splunk and our customers, partners, employees and shareholders. Cisco and Splunk have had a long and successful partnership, underpinned by products and capabilities that fundamentally complement each other and enhance the value we deliver to customers. By bringing our two companies together, we will be able to build on our industry-leading solutions to deliver the most comprehensive visibility and insight in the market across security, observability and network operations. Combining our capabilities will allow us to accelerate our work to transform the industry for the benefit of all of our stakeholders.</p>
<p>Innovation, execution and the drive to deliver on our customer promise will always be at the core of Splunk’s mission. With Cisco, we will have greater resources to innovate and serve our customers, accelerating their digital resilience. Cisco’s world-class go-to-market engine and extensive global network of trusted partners can bring Splunk’s enterprise-grade AI-powered, security and observability solutions to even more customers worldwide. At the same time, we will have the opportunity to accelerate the pace of innovation and develop game-changing solutions to help businesses access, analyze and act on data faster and more securely than ever before. Simply put, our leading technologies, coupled with Cisco’s technology portfolio and powered by its extensive go-to-market capabilities and global scale, is a winning combination for our customers, our industry and our people.</p>
<p>I’m excited by what’s next for Splunk as part of Cisco. For our Splunkers around the globe, today’s announcement is a testament to their hard work, innovative vision and belief that our technology can help organizations all over the world become more resilient and ultimately realize their potential. The talent and drive that made today possible are only going to be more important in the years to come.&nbsp;</p>
<p>Over the years, we’ve created a vibrant community and ecosystem that brings together the most visionary technology minds of our generation. This announcement reinforces our unwavering commitment to helping build a safer and more resilient digital world, and I hope you’ll join us as we celebrate this achievement and continue to write Splunk’s legacy.&nbsp;&nbsp;</p>
<p>For more information, please read our <a href="https://www.splunk.com/en_us/newsroom/press-releases/2023/cisco-to-acquire-splunk-to-help-make-organizations-more-secure-and-resilient-in-an-ai-powered-world.html" target="_blank">press release</a>.</p>
<p>Gary Steele<br>
President &amp; CEO, Splunk</p>
<hr>

<p><b><i>Forward-Looking Statements</i></b></p>
<p><i>This communication contains “forward-looking statements” within the meaning of the federal securities laws, including Section 27A of the Securities Act of 1933, as amended, and Section 21E of the Securities Exchange Act of 1934, as amended. These forward-looking statements are based on Splunk’s current expectations, estimates and projections about the expected date of closing of the proposed transaction and the potential benefits thereof, its business and industry, management’s beliefs and certain assumptions made by Splunk and Cisco, all of which are subject to change. In this context, forward-looking statements often address expected future business and financial performance and financial condition, and often contain words such as “expect,” “anticipate,” “intend,” “plan,” “believe,” “could,” “seek,” “see,” “will,” “may,” “would,” “might,” “potentially,” “estimate,” “continue,” “expect,” “target,” similar expressions or the negatives of these words or other comparable terminology that convey uncertainty of future events or outcomes. All forward-looking statements by their nature address matters that involve risks and uncertainties, many of which are beyond our control, and are not guarantees of future results, such as statements about the consummation of the proposed transaction and the anticipated benefits thereof. These and other forward-looking statements, including the failure to consummate the proposed transaction or to make or take any filing or other action required to consummate the transaction on a timely matter or at all, are not guarantees of future results and are subject to risks, uncertainties and assumptions that could cause actual results to differ materially from those expressed in any forward-looking statements. Accordingly, there are or will be important factors that could cause actual results to differ materially from those indicated in such statements and, therefore, you should not place undue reliance on any such statements and caution must be exercised in relying on forward-looking statements. Important risk factors that may cause such a difference include, but are not limited to: (i) the completion of the proposed transaction on anticipated terms and timing, including obtaining shareholder and regulatory approvals, anticipated tax treatment, unforeseen liabilities, future capital expenditures, revenues, expenses, earnings, synergies, economic performance, indebtedness, financial condition, losses, future prospects, business and management strategies for the management, expansion and growth of Splunk’s business and other conditions to the completion of the transaction; (ii) the impact of the COVID-19 pandemic on Splunk’s business and general economic conditions; (iii) Splunk’s ability to implement its business strategy; (iv) significant transaction costs associated with the proposed transaction; (v) potential litigation relating to the proposed transaction; (vi) the risk that disruptions from the proposed transaction will harm Splunk’s business, including current plans and operations; (vii) the ability of Splunk to retain and hire key personnel; (viii) potential adverse reactions or changes to business relationships resulting from the announcement or completion of the proposed transaction; (ix) legislative, regulatory and economic developments affecting Splunk’s business; (x) general economic and market developments and conditions; (xi) the evolving legal, regulatory and tax regimes under which Splunk operates; (xii) potential business uncertainty, including changes to existing business relationships, during the pendency of the merger that could affect Splunk’s financial performance; (xiii) restrictions during the pendency of the proposed transaction that may impact Splunk’s ability to pursue certain business opportunities or strategic transactions; and (xiv) unpredictability and severity of catastrophic events, including, but not limited to, acts of terrorism or outbreak of war or hostilities, as well as Splunk’s response to any of the aforementioned factors. These risks, as well as other risks associated with the proposed transaction, are more fully discussed in the Proxy Statement to be filed with the U.S. Securities and Exchange Commission in connection with the proposed transaction. While the list of factors presented here is, and the list of factors presented in the Proxy Statement will be, considered representative, no such list should be considered to be a complete statement of all potential risks and uncertainties. Unlisted factors may present significant additional obstacles to the realization of forward looking statements. Consequences of material differences in results as compared with those anticipated in the forward-looking statements could include, among other things, business disruption, operational problems, financial loss, legal liability to third parties and similar risks, any of which could have a material adverse effect on Splunk’s financial condition, results of operations, or liquidity. Splunk does not assume any obligation to publicly provide revisions or updates to any forward-looking statements, whether as a result of new information, future developments or otherwise, should circumstances change, except as otherwise required by securities and other applicable laws.</i></p>
<p><b><i>Important Information and Where to Find It</i></b></p>
<p><i>In connection with the proposed transaction between Splunk Inc. (“Splunk”) and Cisco Systems, Inc. (“Cisco”), Splunk will file with the Securities and Exchange Commission (“SEC”) a proxy statement (the “Proxy Statement”), the definitive version of which will be sent or provided to Splunk stockholders. Splunk may also file other documents with the SEC regarding the proposed transaction. This document is not a substitute for the Proxy Statement or any other document which Splunk may file with the SEC. INVESTORS AND SECURITY HOLDERS ARE URGED TO READ THE PROXY STATEMENT AND ANY OTHER RELEVANT DOCUMENTS THAT ARE FILED OR WILL BE FILED WITH THE SEC, AS WELL AS ANY AMENDMENTS OR SUPPLEMENTS TO THESE DOCUMENTS, CAREFULLY AND IN THEIR ENTIRETY BECAUSE THEY CONTAIN OR WILL CONTAIN IMPORTANT INFORMATION ABOUT THE PROPOSED TRANSACTION AND RELATED MATTERS. Investors and security holders may obtain free copies of the Proxy Statement (when it is available) and other documents that are filed or will be filed with the SEC by Splunk through the website maintained by the SEC at www.sec.gov, Splunk’s investor relations website at <a href="https://investors.splunk.com/" target="_blank">https://investors.splunk.com</a> or by contacting the Splunk investor relations department at the following:</i></p>
<p><i>Splunk Inc.<br>
<a href="mailto:ir@splunk.com" target="_blank">ir@splunk.com</a>&nbsp;<br>
(415) 848-8400</i></p>
<p><b><i>Participants in the Solicitation</i></b></p>
<p><i>Splunk and certain of its directors and executive officers may be deemed to be participants in the solicitation of proxies in respect of the proposed transaction. Information regarding Splunk’s directors and executive officers, including a description of their direct interests, by security holdings or otherwise, is contained in Splunk’s proxy statement for its 2023 annual meeting of stockholders, which was filed with the SEC on May 9, 2023. Splunk stockholders may obtain additional information regarding the direct and indirect interests of the participants in the solicitation of proxies in connection with the proposed transaction, including the interests of Splunk directors and executive officers in the transaction, which may be different than those of Splunk stockholders generally, by reading the Proxy Statement and any other relevant documents that are filed or will be filed with the SEC relating to the transaction. You may obtain free copies of these documents using the sources indicated above.</i></p>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An INI Critique of TOML (2021) (129 pts)]]></title>
            <link>https://github.com/madmurphy/libconfini/wiki/An-INI-critique-of-TOML</link>
            <guid>37595766</guid>
            <pubDate>Thu, 21 Sep 2023 10:41:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/madmurphy/libconfini/wiki/An-INI-critique-of-TOML">https://github.com/madmurphy/libconfini/wiki/An-INI-critique-of-TOML</a>, See on <a href="https://news.ycombinator.com/item?id=37595766">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p><em>Be conservative in what you do, be liberal in what you accept from others.</em></p>
<p>— <a href="https://en.wikipedia.org/wiki/Robustness_principle" rel="nofollow">Postel's law</a></p>
<p>Comparing TOML and INI is not straightforward. The first is a unique standard, the second is a federation of dialects. All INI dialects however are well-defined (every INI file is parsed by some application, and by studying a parser's source code it is <a href="https://github.com/madmurphy/libconfini/wiki/INI-formats">possible to deduce its rules</a>), and, if one looks closely, the number of INI dialects actually used in the wild is not infinite. With an inclusive approach in mind, <strong>libconfini</strong> tries to acknowledge, catalog and extend many of them, so that what once was an informal standard becomes a flexible standard engraved by years of common habits. In referring to “the INI format” this document implicitly refers to that fluid format that <strong>libconfini</strong> is able to parse; and such flexibility is referred to as one of the language's intrinsic features.</p>
<p>Although it claims to be a human-friendly language, TOML constitutes a step back into something more robotic and primitive when compared to INI files and <strong>libconfini</strong>'s approach. Some of TOML's problems are shared with JSON, which is a problematic format outside the ECMAScript realm. Other are instead problems that TOML has created on its own. The reasons why this document addresses TOML and not, for example, JSON or YAML are two. The first obvious reason is that TOML's syntax is so similar to INI that it is useful to draw a line in front of other formats' problematic design and explain why INI is something else. The other reason is that, while other formats like YAML modestly define themselves as “official subsets” of JSON (which was not born as a configuration format, but rather as a serialization format), TOML claims to be a “minimal configuration file format” – despite being another, definitely not minimal, JSON preprocessor, with dates.</p>
<p>TOML's syntax is documented at <a href="https://toml.io/en/v1.0.0" rel="nofollow">https://toml.io/en/v1.0.0</a> (at the time of writing its current revision is <a href="https://github.com/toml-lang/toml/tree/8296d6ba97aaaf3151a32a22ed0513301ac650bf">r793.8296d6b</a>). The following paragraphs explain why you might want to avoid TOML for your next configuration file for a C or a C++ application.</p>
<p>Writing a harsh critique of someone else's efforts is never a pleasant task. But it might be a necessary task when these efforts go in the wrong direction. It is possible that at some point TOML designers will fix the issues listed below. If they will do so, they will finally end up re-inventing INI files.</p>
<h2 id="user-content-1-data-types"><a href="#1-data-types">1. Data types</a></h2>
<p>A TOML document syntactically <em>defines</em> data types. This means that writing <code>89</code> and writing <code>"89"</code> are two different things (the first is a number, the second is a string). And this also means that a compliant TOML parser <em>must</em> respond to type changes in a TOML document. Today you write <code>"89"</code>, your application receives a string and everything goes well, but tomorrow you write <code>89</code> and your application <em>must</em> receive a number…</p>
<p>…and crash.</p>
<p>Of course no application would let anyone do that. Any judicious application using TOML for its configuration will be either tolerant towards improper data types, or will be obstinate and refuse type changes. In the first scenario you will have a non-compliant TOML parser (basically an INI parser), in the second scenario you will have a parser slightly more stupid than an INI parser and still non-compliant (it will not allow a TOML document to define a data type). The question is then: why giving the configuration file the power to speak about data types when at the end of the day it does not have this power (the application does)? The whole thing sounds like “You decide. No, wait, I decide.”</p>
<p>In INI files everything is <em>a castable string</em>. It means that an application always receives a string, and such string is always able to produce a boolean, a number, a simple string, or <em>an array of castable strings</em>, without generating errors. But the application decides what to pick up and how to react to it, not the configuration file. If you want to tell the human about it, write it in a comment, but don't give the configuration file the illusion of a power it does not possess.</p>
<h2 id="user-content-2-quotes-in-values"><a href="#2-quotes-in-values">2. Quotes in values</a></h2>
<p>There is also a deeper issue with data types. Imagine the following configuration file:</p>
<div data-snippet-clipboard-copy-content="[server]
continent = Europe"><pre><code>[server]
continent = Europe
</code></pre></div>
<p>The value above is not required to be a string, it is required to be <em>a continent name</em>. Writing</p>
<div data-snippet-clipboard-copy-content="[server]
continent = 1009"><pre><code>[server]
continent = 1009
</code></pre></div>
<p>is not worse than writing</p>
<div data-snippet-clipboard-copy-content="[server]
continent = &quot;Vacuum cleaner&quot;"><pre><code>[server]
continent = "Vacuum cleaner"
</code></pre></div>
<p>There is no award to gain in demanding that a value avoid a syntax that for some reason is reserved for numbers when it must not be a lot of other things either. And it does not make much sense to create a “string data type” when a “continent name data type” would be required – once again: <em>the <code>continent</code> key above does not expect a string, it expects a continent name</em> (which is not a string more than the ASCII characters used to express numbers are).</p>
<p>Instead, without any valid reason, TOML's syntax forces humans to encapsulate anything that is not a number, a boolean or a date in quotes, disregarding the fact that this would incorrectly present <code>Europe</code> as a string (it is an enumeration label to be exact – in configuration files most values tend to be enumeration labels of some sort) and despite humans would not need quotes for understanding when a sequence of characters – like <code>poet</code> – is not a boolean, or a number, or a date – as for the machines, that would not be a hard task either.</p>
<div data-snippet-clipboard-copy-content="[shakespeare]
birth = 1564
death = 1616

# invalid in TOML
job = poet"><pre><code>[shakespeare]
birth = 1564
death = 1616

# invalid in TOML
job = poet
</code></pre></div>
<p>It is probably not a coincidence that one of the first things that <a href="http://hjson.org/" rel="nofollow">the Hjson project</a> did in order to create a dialect of JSON “easy for humans to read and write” was to remove the necessity of using quotes for declaring strings.</p>
<p>TOML's creator claims that unquoted strings are inherently ambiguous. We can try to imagine the following scenario,</p>

<p>where the quotes seem to suggest that also <code>"252.1"</code> (i.e. a string) would be a valid value for the <code>version</code> key. But do they? What about?</p>
<div data-snippet-clipboard-copy-content="version = &quot;252_1%2!3?4-5/6=7&quot;"><pre><code>version = "252_1%2!3?4-5/6=7"
</code></pre></div>
<p>Would that be a valid version string? What other information does quoting <code>252</code> give except that there could also be “something else” than a simple number?</p>
<p>Without a comment that explains exactly how to format the <code>version</code> key there is just no way to make it unambiguous, quotes or not.</p>
<div data-snippet-clipboard-copy-content="#INI

# Please use MAJOR(.MINOR(.REVISION)) here
version = 252.1.0"><pre><code>#INI

# Please use MAJOR(.MINOR(.REVISION)) here
version = 252.1.0
</code></pre></div>
<p>As in a language that has an extensible semantics, in INI files quotes serve the simple purpose of giving hints, expressing literalness, or removing <em>syntactic</em> (not semantic) ambiguity when there is the risk of it, exactly like a human would do. For instance, an INI file would use quotes like the following example does, for indicating that the <code>#</code> character in <code>#fff000</code> does not mark a comment.</p>

<h2 id="user-content-3-case-sensitivity"><a href="#3-case-sensitivity">3. Case sensitivity</a></h2>
<p>TOML's syntax is always case-sensitive, despite the fact that there are situations where a configuration file <em>must</em> be case-insensitive (think of configuration files that map a FAT32 filesystem or HTML tags, for example). INI formats can be either case-sensitive or case-insensitive depending on the application's choice.</p>
<h2 id="user-content-4-unicode-key-names"><a href="#4-unicode-key-names">4. Unicode key names</a></h2>
<p>TOML's syntax forbids non-ASCII key names unless these are surrounded by quotes.</p>
<div data-snippet-clipboard-copy-content="value in € = 345    # valid with libconfini but invalid in TOML"><pre><code>value in € = 345    # valid with libconfini but invalid in TOML
</code></pre></div>
<p>There is no apparent motivation behind this rule, except that of conforming TOML to JSON, and probably a personal habit in dealing with the latter. But although JSON <em>does</em> have a valid reason to do so because of the programming language it has been designed to work with (ECMAScript property names follow the same rule of identifiers), TOML's reason remains somewhat mysterious.</p>
<h2 id="user-content-5-square-brackets"><a href="#5-square-brackets">5. Square brackets</a></h2>
<p>TOML forces arrays to be encapsulated within square brackets (exactly like section paths do), although humans do not need square brackets for recognizing when something is a list.</p>
<div data-snippet-clipboard-copy-content="# not an array in TOML
wishes = apples, cars, elephants, chairs"><pre><code># not an array in TOML
wishes = apples, cars, elephants, chairs
</code></pre></div>
<p>Nested arrays are also not a valid reason for justifying square brackets, since in INI files it is already possible to nest arrays either by using different delimiters for each level,</p>
<div data-snippet-clipboard-copy-content="wishes = \
    apples : oranges : lemons, \
    cars, \
    elephants : tigers, \
    chairs"><pre><code>wishes = \
    apples : oranges : lemons, \
    cars, \
    elephants : tigers, \
    chairs
</code></pre></div>
<p>or by recursively quoting.</p>
<div data-snippet-clipboard-copy-content="wishes = \
    &quot;apples oranges lemons&quot; \
    cars \
    &quot;elephants tigers&quot; \
    chairs"><pre><code>wishes = \
    "apples oranges lemons" \
    cars \
    "elephants tigers" \
    chairs
</code></pre></div>
<p>But there is a more important reason why square brackets are a bad idea in a human-friendly configuration format: one-member arrays. There is no way to convince a human that something composed of only one member is a list (if you think differently, chances are that you are partly non-human). As friendly as they are, INI files behave accordingly, while TOML of course doesn't. Compare this (INI):</p>

<p>with this (TOML):</p>

<p>As in the C language, in INI files <em>a one-member array and a simple value are stored in the same way</em>. Of course you can declare a one-member array in INI files: just write a simple string.</p>
<p>Thanks to this, INI arrays do not constitute a syntactically distinct type and any string can be parsed as an array. If you have ever dealt with m4 macro arguments you will know the beauty of this.</p>
<h2 id="user-content-6-array-delimiters"><a href="#6-array-delimiters">6. Array delimiters</a></h2>
<p>TOML forces arrays to be always comma-separated, although a human can recognize a list even when the separator is a mushroom.</p>
<div data-snippet-clipboard-copy-content="[Super Mario]
wishes = jumping 🍄 sneaking into pipes 🍄 princess Peach 🍄 flying
coins = 39586235"><pre><code>[Super Mario]
wishes = jumping 🍄 sneaking into pipes 🍄 princess Peach 🍄 flying
coins = 39586235
</code></pre></div>
<p><strong>libconfini</strong> does not allow mushrooms either – but for practical, not philosophical reasons (and the library is not human yet) – but you are free to choose any character within the ASCII range as array delimiter and change it as often as you wish. For instance, in an INI file where normally arrays are comma-separated you might decide that an IP address is also an array, but whose members are separated by dots instead of commas – and just because that is what an IP address actually is, and that might be what your application needs.</p>
<h2 id="user-content-7-mixed-arrays"><a href="#7-mixed-arrays">7. Mixed arrays</a></h2>
<p>TOML encourages a nightmare for strongly typed languages like C and C++: mixed arrays. In short, after deciding that a configuration file must express strong types (and nevertheless still allowing <code>"Vacuum cleaner"</code> as a continent name), TOML forces applications to be able to mix them and display some kind of support for something that is natively not supported.</p>
<p>An array that mixes numbers, strings and other arrays is something a C or C++ application would escape from. Although it is possible to reach the same result also with INI, with both TOML and INI a mixed array can be just emulated, <em>never really implemented</em> from the C perspective (we have left an example under <code>examples/miscellanea/toml-like.c</code>, and we would discourage anyone from doing it). The difference between INI and TOML? INI syntax has <em>the power to express</em> mixed arrays but does not require applications to map them as such, TOML does.</p>
<h2 id="user-content-8-composite-configuration-files"><a href="#8-composite-configuration-files">8. Composite configuration files</a></h2>
<p>TOML's syntax forbids to populate sections in different steps (sections are named “tables” in TOML). The following example, understood by a human and an INI parser, would be forbidden in TOML:</p>
<div data-snippet-clipboard-copy-content="[visitors]
list = karl, lisa, Andrew Smith, rick92

[host]
foo = bar

[visitors]          # invalid in TOML
checked = true      # invalid in TOML"><pre><code>[visitors]
list = karl, lisa, Andrew Smith, rick92

[host]
foo = bar

[visitors]          # invalid in TOML
checked = true      # invalid in TOML
</code></pre></div>
<p>Although this might look like an insignificant detail, allowing to populate a configuration file in different steps can come very much in handy when dealing with the composition of several smaller configuration files.</p>
<h2 id="user-content-9-dates"><a href="#9-dates">9. Dates</a></h2>
<p>This is probably the most mysterious part of TOML language. In INI files a value can be <em>interpreted</em> as a boolean, a number, a string, an array, or whatever else you like (although in this last case <strong>libconfini</strong> will not help you). The situation is kind of similar in TOML (without the “whatever else you like” part), except that a value can also be <em>a date</em>.</p>
<p>There is something intriguing in all this. Even forgetting that an application might not need dates at all, why constraining something so particular and that can be formatted in so many different ways into a rigid primitive? Why not doing that for <em>a path</em>? Or <em>a username</em>? Or <em>an email address</em>? Or <em>a regular expression</em>? …Or <em>a continent name</em>? These have all a more constraining semantics than dates.</p>
<p>In INI files a date is either a time stamp or a human-friendly string.</p>
<div data-snippet-clipboard-copy-content="date = &quot;Thu, 30 Aug 2012 12:31:00 GMT&quot;"><pre><code>date = "Thu, 30 Aug 2012 12:31:00 GMT"
</code></pre></div>
<h2 id="user-content-10--empty-key-names"><a href="#10--empty-key-names">10.  Empty key names</a></h2>
<p>Although a human would have no idea of what it could possibly mean (and probably a machine would not do any better), TOML's syntax explicitly allows (but discourages) to assign values to empty key names.</p>
<div data-snippet-clipboard-copy-content="&quot;&quot; = &quot;whatever&quot;     # valid in TOML
'' = 'whatever'     # valid in TOML
= 'whatever'        # invalid in TOML (seriously?)"><pre><code>"" = "whatever"     # valid in TOML
'' = 'whatever'     # valid in TOML
= 'whatever'        # invalid in TOML (seriously?)
</code></pre></div>
<h2 id="user-content-11-arrays-of-tables-aka-arrays-of-sections"><a href="#11-arrays-of-tables-aka-arrays-of-sections">11. Arrays of tables (a.k.a. arrays of sections)</a></h2>
<p>Sometimes what initially appears to be a nice invention can end up being the opposite – yes, this can happen too. We are talking about arrays of tables here (a.k.a. arrays of sections).</p>
<p>Arrays of tables are declared in TOML using the double square bracket notation:</p>
<div data-snippet-clipboard-copy-content="# TOML

[[server]]
ip = &quot;214.252.11.145&quot;
country = &quot;Australia&quot;

[[server]]
ip = &quot;214.252.11.146&quot;
country = &quot;India&quot;

[[server]]
ip = &quot;214.252.11.147&quot;
country = &quot;Sweden&quot;

..."><pre><code># TOML

[[server]]
ip = "214.252.11.145"
country = "Australia"

[[server]]
ip = "214.252.11.146"
country = "India"

[[server]]
ip = "214.252.11.147"
country = "Sweden"

...
</code></pre></div>
<p>Strictly speaking, arrays of tables introduce the concept of “unnamed tables” – <code>server</code> in the example above is not the name of a table, it is the name of <em>a collection of tables, each of which does not have a name</em>. But independently of the syntactical consequences, this feature carries a major problem: it encourages using configuration files as databases.</p>
<p>The common way to deal with similar scenarios in INI files would be that of keeping a common parent section – so that the application can scroll blindly through the sibling subsections – and making the nesting explicit by giving each subsection a name (in fact, the only unnamed section in INI files can be the document's root):</p>
<div data-snippet-clipboard-copy-content="# INI

[server.main]
ip = 214.252.11.145
country = Australia

[server.secondary]
ip = 214.252.11.146
country = India

[server.broken]
ip = 214.252.11.147
country = Sweden

# You can add an infinite number of `server.*` subsections here and use
# arbitrary names, the application will retrieve all of them."><pre><code># INI

[server.main]
ip = 214.252.11.145
country = Australia

[server.secondary]
ip = 214.252.11.146
country = India

[server.broken]
ip = 214.252.11.147
country = Sweden

# You can add an infinite number of `server.*` subsections here and use
# arbitrary names, the application will retrieve all of them.
</code></pre></div>
<p>The INI way is inherently more human-readable (humans like descriptive names), and produces the nice outcome that when the entries have become too many, and naming each of them has become too cumbersome, it is the good sign that you should finally switch to a database format and keep your configuration file clean.</p>
<p>Even <a href="https://github.com/toml-lang/toml/blob/8296d6ba97aaaf3151a32a22ed0513301ac650bf/README.md#Example">TOML's featured example</a> proposes “the INI way”</p>
<div data-snippet-clipboard-copy-content="[servers.alpha]
ip = &quot;10.0.0.1&quot;
role = &quot;frontend&quot;

[servers.beta]
ip = &quot;10.0.0.2&quot;
role = &quot;backend&quot;"><pre><code>[servers.alpha]
ip = "10.0.0.1"
role = "frontend"

[servers.beta]
ip = "10.0.0.2"
role = "backend"
</code></pre></div>
<p>instead of “the TOML way”</p>
<div data-snippet-clipboard-copy-content="[[servers]]
ip = &quot;10.0.0.1&quot;
role = &quot;frontend&quot;

[[servers]]
ip = &quot;10.0.0.2&quot;
role = &quot;backend&quot;"><pre><code>[[servers]]
ip = "10.0.0.1"
role = "frontend"

[[servers]]
ip = "10.0.0.2"
role = "backend"
</code></pre></div>
<p>for presenting the language.</p>
<p>But if this does not convince you, and at the end of the day you really want to play dirty with your configuration files, INI sill offers you its quirks to reach TOML's effect, without the inconvenience of introducing anonymous sections:</p>
<div data-snippet-clipboard-copy-content="# INI

[server.&quot;214.252.11.145&quot;]
country = Australia

[server.&quot;214.252.11.146&quot;]
country = India

[server.&quot;214.252.11.147&quot;]
country = Sweden

..."><pre><code># INI

[server."214.252.11.145"]
country = Australia

[server."214.252.11.146"]
country = India

[server."214.252.11.147"]
country = Sweden

...
</code></pre></div>
<p>It goes without saying that you should not follow TOML in this. INI is not a database format; it targets primarily humans, not machines. If you want to store multiple sections of the same kind, please give them human-friendly names, and have fun.</p>
<h2 id="user-content-12-lack-of-support-for-implicit-keys"><a href="#12-lack-of-support-for-implicit-keys">12. Lack of support for implicit keys</a></h2>
<p>Serialization formats often have shortcuts for expressing a <code>true</code> boolean implicitly. A bare HTML attribute, for instance, is automatically given the <code>"true"</code> value – i.e. the <code>contenteditable</code> attribute in the following example is automatically parsed as <code>contenteditable="true"</code>.</p>
<div data-snippet-clipboard-copy-content="<div contenteditable class=&quot;my-class&quot;></div>"><pre><code>&lt;div contenteditable class="my-class"&gt;&lt;/div&gt;
</code></pre></div>
<p>Similarly, in the following INI fragment from <code>/etc/pacman.conf</code> (<strong>Arch</strong>), <code>Color</code> is an implicit key representing a <code>true</code> boolean – i.e. <code>Color = YES</code>.</p>
<div data-snippet-clipboard-copy-content="HoldPkg = pacman glibc
Architecture = auto
IgnorePkg =
Color
SigLevel = Required DatabaseOptional
LocalFileSigLevel = Optional"><pre><code>HoldPkg = pacman glibc
Architecture = auto
IgnorePkg =
Color
SigLevel = Required DatabaseOptional
LocalFileSigLevel = Optional
</code></pre></div>
<p>TOML lacks support for implicit keys, and key names not followed by an equals sign always constitute syntax errors.</p>
<h2 id="user-content-13-inline-tables-must-remain-inline"><a href="#13-inline-tables-must-remain-inline">13. Inline tables must remain… inline</a></h2>
<p>In addition to the INI way, TOML introduces a duplicate way of declaring sections: “inline tables”. The following TOML example:</p>
<div data-snippet-clipboard-copy-content="# TOML

homepage = { page_header = &quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit.&quot;, page_footer = &quot;Orci varius natoque penatibus et magnis dis parturient montes.&quot; }"><pre><code># TOML

homepage = { page_header = "Lorem ipsum dolor sit amet, consectetur adipiscing elit.", page_footer = "Orci varius natoque penatibus et magnis dis parturient montes." }
</code></pre></div>
<p>is an exact synonym of:</p>
<div data-snippet-clipboard-copy-content="# TOML

[homepage]
page_header = &quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit.&quot;
page_footer = &quot;Orci varius natoque penatibus et magnis dis parturient montes.&quot;"><pre><code># TOML

[homepage]
page_header = "Lorem ipsum dolor sit amet, consectetur adipiscing elit."
page_footer = "Orci varius natoque penatibus et magnis dis parturient montes."
</code></pre></div>
<p>Besides the visual inconvenience of presenting entire sections like keys, not much would be wrong with this feature, not even the redundancy, had the feature not come with an ugly rule attached: inline tables must remain inline.</p>
<p>Such a coercion would become tolerable after being reminded that in that language a new line is supposed to end a node, if only there had not been an exception that makes it intolerable: arrays, on the contrary, can span multiple lines.</p>
<p>Thus, you can write,</p>
<div data-snippet-clipboard-copy-content="# TOML

homepage = [
	&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit.&quot;,
	&quot;Orci varius natoque penatibus et magnis dis parturient montes.&quot;
]"><pre><code># TOML

homepage = [
	"Lorem ipsum dolor sit amet, consectetur adipiscing elit.",
	"Orci varius natoque penatibus et magnis dis parturient montes."
]
</code></pre></div>
<p>but you cannot write</p>
<div data-snippet-clipboard-copy-content="# Invalid TOML example

homepage = {
	page_header = &quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit.&quot;,
	page_footer = &quot;Orci varius natoque penatibus et magnis dis parturient montes.&quot;
}"><pre><code># Invalid TOML example

homepage = {
	page_header = "Lorem ipsum dolor sit amet, consectetur adipiscing elit.",
	page_footer = "Orci varius natoque penatibus et magnis dis parturient montes."
}
</code></pre></div>
<p>What makes things worse is the fact that this prohibition exists only for the sake of not having two ways of declaring tables that are both multi-line. It is “a moral prohibition”, not dictated by any practical reasons. In short, it exists only for telling you how to behave.</p>
<p>TOML's designers insist saying that allowing multi-line tables declared in this way would break one of TOML's pillars, which is precisely that of terminating a node when a (non-escaped) new line is found. But that is one of INI's pillars, not TOML's: TOML had already betrayed this principle after establishing its array syntax. Looking at the asymmetry above from a different perspective, one could indeed say that the original mistake lies with arrays, not with inline tables (but however one puts it, a mistake lies somewhere).</p>
<p>It is possible to argue further that inline tables bring TOML's syntax closer to JSON. That alone is a good reason to be happy that inline tables are alien in INI files.</p>
<h2 id="user-content-14-incompatibility"><a href="#14-incompatibility">14. Incompatibility</a></h2>
<p>By design TOML is explicitly incompatible with about fourty years of configuration files.</p>
<h2 id="user-content-15-immediacy"><a href="#15-immediacy">15. Immediacy</a></h2>
<p>A configuration file is meant to be edited by a human – possibly someone who has only <strong>Microsoft Notepad</strong> as text editor and has never heard of TOML or INI before – and editing it should feel like a natural and welcomed thing to do, not like hacking a program source code, especially if this does not give any expressive advantage.</p>
<p>If a person who has never heard of TOML sees the following configuration file,</p>
<div data-snippet-clipboard-copy-content="# TOML

[&quot;bank&quot;]
&quot;ip&quot; = &quot;192.168.1.1&quot;
&quot;square root&quot; = 15000

[&quot;client&quot;]
&quot;hello world&quot; = [&quot;sunny&quot;]
&quot;foo&quot; = &quot;9234&quot;"><pre><code># TOML

["bank"]
"ip" = "192.168.1.1"
"square root" = 15000

["client"]
"hello world" = ["sunny"]
"foo" = "9234"
</code></pre></div>
<p>how is the person supposed to know that <code>"ip"</code> can be written also without quotes, but that is not the case of <code>"square root"</code>, as this contains spaces and keys containing spaces must be always quoted? or that the string <code>["sunny"]</code> is not a reference to a section name but is an array instead? or what data types a particular array can contain?</p>
<p>INI, on the other hand, encourages human-friendly comments for explaining what is not immediately visible.</p>
<div data-snippet-clipboard-copy-content="# INI

[bank]
ip = 192.168.1.1
square root = 15000

[client]
hello world = sunny  # it is possible to write a comma-separated list here
foo = 9234"><pre><code># INI

[bank]
ip = 192.168.1.1
square root = 15000

[client]
hello world = sunny  # it is possible to write a comma-separated list here
foo = 9234
</code></pre></div>
<p>You can write comments in TOML as well, of course. But the risk is that they end up being lists of things to avoid that have nothing to do with the application you are configuring, rather than suggestions of what is possible to do.</p>
<div data-snippet-clipboard-copy-content="# TOML

[&quot;bank&quot;]
&quot;ip&quot; = &quot;192.168.1.1&quot;    # you can remove the quotes from `&quot;ip&quot;` if you want
&quot;square root&quot; = 15000   # do not remove the quotes from `&quot;square root&quot;`, TOML forbids it

[&quot;client&quot;]
&quot;hello world&quot; = [&quot;sunny&quot;]   # `[&quot;sunny&quot;]` is not a section name
&quot;foo&quot; = &quot;9234&quot;  # do not remove the quotes from `&quot;9234&quot;`, it is not a number (I know...)"><pre><code># TOML

["bank"]
"ip" = "192.168.1.1"    # you can remove the quotes from `"ip"` if you want
"square root" = 15000   # do not remove the quotes from `"square root"`, TOML forbids it

["client"]
"hello world" = ["sunny"]   # `["sunny"]` is not a section name
"foo" = "9234"  # do not remove the quotes from `"9234"`, it is not a number (I know...)
</code></pre></div>
<h2 id="user-content-16-genesis"><a href="#16-genesis">16. Genesis</a></h2>
<p>Configuration files are born out of necessity, and different applications can have different requirements. There are cases where a configuration file differs substantially from the INI format. It is not rare in these situations that developers have ended up abandoning a widespread and solid configuration format such as INI only after realizing that they had no other choice and not without pain.</p>
<p>In this respect, the way <strong>libconfini</strong> was born is paradigmatic. It was born for an application – an editor – and that application had a very peculiar task: read different types of INI files written in the real world for the applications typically installed on a <strong>GNU/Linux</strong> distribution. What a better scenario for creating a parser?</p>
<p>The genesis of TOML instead is quite different. Someone without a parser <a href="https://github.com/toml-lang/toml/issues/411#issuecomment-219203431">decided that unquoted strings in INI files <em>are ugly</em> and forbad them</a>. A lot of rules have then been added afterwards on paper, without really thinking of any real case usage and only keeping JSON as a reference point.</p>
<p>In the beginning it was still only a specification. Many people, enthusiastic finally to read <em>a specification of something somewhere</em>, started to create their own parser for the newborn language. And that was the moment when problems began to appear.</p>
<p>When you write a parser you might indeed begin to notice contradictions in an apparently unflawed rule, your code might start to become unnecessarily complex because of absurd edge cases, and you might realize that the language you are trying to parse is not that well-designed after all.</p>
<p>And even if you do survive the process of writing a parser that is fully compliant with TOML (<a href="https://github.com/avakar/pytoml/issues/15#issuecomment-217739462">some people don't</a>), you still have done only half of the job, that of writing a parser, without really thinking of any real case usage. It is still possible that you have completely wasted your time after all.</p>
<p>There are of course cases where TOML works just fine, and these are the cases where JSON would also work fine (although one has always to tolerate TOML's idea to introduce a syntax for dates). But where JSON works fine, also other JSON dialects more human-friendly than TOML do.</p>
<h2 id="user-content-17-against-postels-law-by-design"><a href="#17-against-postels-law-by-design">17. Against Postel's law by design</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Robustness_principle" rel="nofollow">Postel's law</a> is a good indicator of how robust a language is: the more a language is able to make sense of different types of input, the more robust the language.</p>
<p>In front of a heterogeneous landscape like that of configuration files, a parser that applies Postel's law will try to make sense of the largest possible set of habits and explore all possible solutions to avoid that errors be generated merely because of diversity.</p>
<p>TOML is a good example of a language designed against this principle. <em>The language's founding element was that of generating errors</em> when quotes were missing, and subsequent rules seem to have <em>in generating errors their only reason</em> (think of the requirement of using quotes for key names containing spaces or unicode characters, which has no justification whatsoever – if it is for aesthetical reasons, think that if you were a Chinese speaker you would rather be tempted to use quotes for the Latin characters and leave the Chinese ideograms out of quotes instead).</p>
<p>One would think that a language with such tendencies will always have only one way to express the same thing, at least. And instead no: inline tables were introduced as a duplicate of standard tables, despite being less readable and completely alien in the common practice – something vaguely similar were <a href="http://hyperrealm.github.io/libconfig/" rel="nofollow"><strong>libconfig</strong></a>'s sections, but these spanned multiple lines and constituted the only way to declare sections in that language.</p>
<p>At the end of the day TOML's main goal seems to be that of generating errors. The opposite approach, instead, would be that of taking advantage of diversity and regard it is as a strength.</p>
<h2 id="user-content-18-performance"><a href="#18-performance">18. Performance</a></h2>
<p>It is not so obvious to talk about “TOML's performance”: TOML is a language, not a particular parser. It is possible however to predict that any TOML-compliant parser will be on average <em>much slower</em> than an INI parser.</p>
<p>This entire section is being created while a TOML parser written in C (<a href="https://github.com/cktan/tomlc99"><strong>tomlc99</strong></a>) tries to parse a 50 MiB file that <strong>libconfini</strong> usually parses in half a second – and <strong>libconfini</strong>'s primary goal is not speed (yes, we gave the TOML parser an INI file to parse).</p>
<p>This is not a critique to the particular parser chosen – we can assume that <strong>tomlc99</strong> is doing its best in its hard task. The reason why a TOML parser will always be slow is the error checking fury required by the language. Where most INI parsers' approach will be that of “don't throw an error unless you really cannot make any sense of what is written in a configuration file – the application will do the rest and will do it better”, the approach of a TOML-compliant parser will be that of searching for errors even when both the application and the human would have already understood a content.</p>
<p>After 13 minutes and 20 seconds our TOML parser has finally parsed…</p>
<div data-snippet-clipboard-copy-content="54691749 bytes parsed in 800.849218 seconds.
Number of bytes parsed per second: 68292.192551

ERROR: cannot parse - line 1: extra chars after value"><pre><code>54691749 bytes parsed in 800.849218 seconds.
Number of bytes parsed per second: 68292.192551

ERROR: cannot parse - line 1: extra chars after value
</code></pre></div>
<p>Of course. Someone forgot to put quotes around the first value.</p>
<h2 id="user-content-19-human-friendly-vs-human-readable"><a href="#19-human-friendly-vs-human-readable">19. Human-friendly vs. human-readable</a></h2>
<p>“Human-friendly” and “human-readable” might sound as synonyms, but often they are not. Some texts can be very easy to read but hard to edit.</p>
<p>An inscription on the front of the Pantheon in Rome says “Marcus Agrippa, son of Lucius, made this building when consul for the third time”. This is a very human-readable text if you know a bit of Latin. But in order to edit it you would need a ladder, a chisel and the wish to ruin a millenary monument – please do not try to do it.</p>
<p>An emblematic example of this in file formats is JSON. Due to curly brackets, a systematic indentation and a strict syntax it is probably one of the most human-readable serialization formats. But exactly because of the same reasons it is not the most human-friendly one.</p>
<p>Similarly, if used with a syntax highlighter, the human-readability of TOML is comparable to that of INI. Its human-friendliness, instead, lies a few steps below.</p>
<h2 id="user-content-20-aesthetics"><a href="#20-aesthetics">20. Aesthetics</a></h2>
<p>Appearance has its importance too. TOML's specification comes with <a href="https://github.com/toml-lang/toml/blob/8296d6ba97aaaf3151a32a22ed0513301ac650bf/README.md#Example">the following example</a> for illustrating the language:</p>
<div data-snippet-clipboard-copy-content="# This is a TOML document.

title = &quot;TOML Example&quot;

[owner]
name = &quot;Tom Preston-Werner&quot;
dob = 1979-05-27T07:32:00-08:00 # First class dates

[database]
server = &quot;192.168.1.1&quot;
ports = [ 8000, 8001, 8002 ]
connection_max = 5000
enabled = true

[servers]

  # Indentation (tabs and/or spaces) is allowed but not required
  [servers.alpha]
  ip = &quot;10.0.0.1&quot;
  dc = &quot;eqdc10&quot;

  [servers.beta]
  ip = &quot;10.0.0.2&quot;
  dc = &quot;eqdc10&quot;

[clients]
data = [ [&quot;gamma&quot;, &quot;delta&quot;], [1, 2] ]

# Line breaks are OK when inside arrays
hosts = [
  &quot;alpha&quot;,
  &quot;omega&quot;
]"><pre><code># This is a TOML document.

title = "TOML Example"

[owner]
name = "Tom Preston-Werner"
dob = 1979-05-27T07:32:00-08:00 # First class dates

[database]
server = "192.168.1.1"
ports = [ 8000, 8001, 8002 ]
connection_max = 5000
enabled = true

[servers]

  # Indentation (tabs and/or spaces) is allowed but not required
  [servers.alpha]
  ip = "10.0.0.1"
  dc = "eqdc10"

  [servers.beta]
  ip = "10.0.0.2"
  dc = "eqdc10"

[clients]
data = [ ["gamma", "delta"], [1, 2] ]

# Line breaks are OK when inside arrays
hosts = [
  "alpha",
  "omega"
]
</code></pre></div>
<p>There are many ways of expressing exactly the same content using <strong>libconfini</strong>. The following is probably the most obvious one:</p>
<div data-snippet-clipboard-copy-content="# examples/ini_files/toml-like.conf

# Relax, this is an INI document.


title = INI Example

[owner]
name = madmurphy
dob = &quot;Sun, 27 May 1979 15:32:00 GMT&quot;

[database]
server = 192.168.1.1    # you can parse an IP address as an array too! :-)
ports = 8000, 8001, 8002
connection_max = 5000
enabled

  # Indentation (tabs and/or spaces) is allowed but not required
  [servers.alpha]
  ip = 10.0.0.1
  dc = eqdc10

  [servers.beta]
  ip = 10.0.0.2
  dc = eqdc10

[clients]
data = gamma : delta, 1 : 2

hosts = alpha, omega"><pre><code># examples/ini_files/toml-like.conf

# Relax, this is an INI document.


title = INI Example

[owner]
name = madmurphy
dob = "Sun, 27 May 1979 15:32:00 GMT"

[database]
server = 192.168.1.1    # you can parse an IP address as an array too! :-)
ports = 8000, 8001, 8002
connection_max = 5000
enabled

  # Indentation (tabs and/or spaces) is allowed but not required
  [servers.alpha]
  ip = 10.0.0.1
  dc = eqdc10

  [servers.beta]
  ip = 10.0.0.2
  dc = eqdc10

[clients]
data = gamma : delta, 1 : 2

hosts = alpha, omega
</code></pre></div>
<p>For a parsing example, please have a look at <code>examples/miscellanea/toml-like.c</code>.</p>
<h2 id="user-content-further-readings"><a href="#further-readings">Further readings</a></h2>
<ul>
<li><a href="https://toml.io/en/v1.0.0" rel="nofollow">TOML Specification</a></li>
<li><a href="https://hitchdev.com/strictyaml/why-not/toml/" rel="nofollow">What is wrong with TOML?</a></li>
</ul>
<h2 id="user-content-state-of-this-document"><a href="#state-of-this-document">State of this document</a></h2>
<p>Last revision: October 2021</p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ship of Fools (127 pts)]]></title>
            <link>https://successfulsoftware.net/2023/09/19/ship-of-fools/</link>
            <guid>37595322</guid>
            <pubDate>Thu, 21 Sep 2023 09:36:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://successfulsoftware.net/2023/09/19/ship-of-fools/">https://successfulsoftware.net/2023/09/19/ship-of-fools/</a>, See on <a href="https://news.ycombinator.com/item?id=37595322">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>I recently had a 3 week holiday in Florida with my family. My 17 year old son is interested in rocketry and my wife is interested in wildlife. We got to see plenty of both and had a great time. There is a lot to like about America and Americans. But the sheer waste of resources on show everywhere was pretty shocking. In Europe we absolutely aren’t doing enough to protect the environment and avert the impending climate catastrophe (I flew to Florida and drove a car there, so I am no environmental saint myself). In Florida they don’t appear to be even trying. </p>



<p>Let’s start with plastic. Everything seems to be made of plastic, wrapped in plastic or both. This is a hotel breakfast for the 3 of us. That is a serious amount of plastic. </p>



<figure><a href="https://successfulsoftware.files.wordpress.com/2023/09/img_1572-1.jpg"><img data-attachment-id="11185" data-permalink="https://successfulsoftware.net/2023/09/19/ship-of-fools/img_1572-1/" data-orig-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1572-1.jpg" data-orig-size="959,552" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 7&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;40&quot;,&quot;shutter_speed&quot;:&quot;0.0083333333333333&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1572-1" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1572-1.jpg?w=300" data-large-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1572-1.jpg?w=625" src="https://successfulsoftware.files.wordpress.com/2023/09/img_1572-1.jpg?w=959" alt="" srcset="https://successfulsoftware.files.wordpress.com/2023/09/img_1572-1.jpg 959w, https://successfulsoftware.files.wordpress.com/2023/09/img_1572-1.jpg?w=150 150w, https://successfulsoftware.files.wordpress.com/2023/09/img_1572-1.jpg?w=300 300w, https://successfulsoftware.files.wordpress.com/2023/09/img_1572-1.jpg?w=768 768w" sizes="(max-width: 959px) 100vw, 959px"></a></figure>



<p>Plastic cutlery is the order of the day. And even the plastic cutlery is individually wrapped in plastic! The very cheapest hotels in the UK give you metal cutlery.</p>



<figure><a href="https://successfulsoftware.files.wordpress.com/2023/09/img_1569.jpg"><img data-attachment-id="11187" data-permalink="https://successfulsoftware.net/2023/09/19/ship-of-fools/img_1569/" data-orig-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1569.jpg" data-orig-size="1000,736" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 7&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1691915877&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;3.99&quot;,&quot;iso&quot;:&quot;20&quot;,&quot;shutter_speed&quot;:&quot;0.0026109660574413&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1569" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1569.jpg?w=300" data-large-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1569.jpg?w=625" src="https://successfulsoftware.files.wordpress.com/2023/09/img_1569.jpg?w=1000" alt="" srcset="https://successfulsoftware.files.wordpress.com/2023/09/img_1569.jpg 1000w, https://successfulsoftware.files.wordpress.com/2023/09/img_1569.jpg?w=150 150w, https://successfulsoftware.files.wordpress.com/2023/09/img_1569.jpg?w=300 300w, https://successfulsoftware.files.wordpress.com/2023/09/img_1569.jpg?w=768 768w" sizes="(max-width: 1000px) 100vw, 1000px"></a></figure>



<figure><a href="https://successfulsoftware.files.wordpress.com/2023/09/img_1573.jpg"><img data-attachment-id="11186" data-permalink="https://successfulsoftware.net/2023/09/19/ship-of-fools/img_1573/" data-orig-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1573.jpg" data-orig-size="750,1000" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 7&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1691999126&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;3.99&quot;,&quot;iso&quot;:&quot;20&quot;,&quot;shutter_speed&quot;:&quot;0.0073529411764706&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1573" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1573.jpg?w=225" data-large-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1573.jpg?w=625" src="https://successfulsoftware.files.wordpress.com/2023/09/img_1573.jpg?w=750" alt="" srcset="https://successfulsoftware.files.wordpress.com/2023/09/img_1573.jpg 750w, https://successfulsoftware.files.wordpress.com/2023/09/img_1573.jpg?w=113 113w, https://successfulsoftware.files.wordpress.com/2023/09/img_1573.jpg?w=225 225w" sizes="(max-width: 750px) 100vw, 750px"></a></figure>



<p>Apples were individually wrapped in plastic. </p>



<figure><a href="https://successfulsoftware.files.wordpress.com/2023/09/img_1578.jpg"><img data-attachment-id="11188" data-permalink="https://successfulsoftware.net/2023/09/19/ship-of-fools/img_1578/" data-orig-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1578.jpg" data-orig-size="750,1000" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone 7&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1691999172&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;3.99&quot;,&quot;iso&quot;:&quot;25&quot;,&quot;shutter_speed&quot;:&quot;0.016949152542373&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1578" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1578.jpg?w=225" data-large-file="https://successfulsoftware.files.wordpress.com/2023/09/img_1578.jpg?w=625" src="https://successfulsoftware.files.wordpress.com/2023/09/img_1578.jpg?w=750" alt="" srcset="https://successfulsoftware.files.wordpress.com/2023/09/img_1578.jpg 750w, https://successfulsoftware.files.wordpress.com/2023/09/img_1578.jpg?w=113 113w, https://successfulsoftware.files.wordpress.com/2023/09/img_1578.jpg?w=225 225w" sizes="(max-width: 750px) 100vw, 750px"></a></figure>



<p>We even saw oranges wrapped in plastic. Nature already provided oranges with their own wrapper! I don’t remember the plastic issue being as bad when I travelled through Wyoming, Utah and Colorado in 1999. Maybe it’s a hangover from COVID?</p>



<p>And then there are the cars. We did a quick informal survey and over half the vehicles on the road were massive SUVs and even more massive pickup trucks, with macho names like ‘Raptor’ and ‘Titan’. The very low tax on petrol/gas (by European standards) makes this possible. These pickup trucks are clearly being used mostly by people from the suburbs who do not need a huge pickup truck. We hired a ‘mid-size’ (but big by European standards) SUV ourselves as, in a previous trip, we had found it quite intimidating to drive a European sized saloon car on American roads. </p>



<p>The front of these pick-up trucks is so high that a pedestrian hit by one is definitely going under, rather than over. Especially the ridiculous ‘raised’ pickup trucks, which are very common.</p>



<figure><a href="https://successfulsoftware.files.wordpress.com/2023/09/img_3534.jpg"><img data-attachment-id="11190" data-permalink="https://successfulsoftware.net/2023/09/19/ship-of-fools/img_3534/" data-orig-file="https://successfulsoftware.files.wordpress.com/2023/09/img_3534.jpg" data-orig-size="2933,2336" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone SE (2nd generation)&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;20&quot;,&quot;shutter_speed&quot;:&quot;0.0027932960893855&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_3534" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.files.wordpress.com/2023/09/img_3534.jpg?w=300" data-large-file="https://successfulsoftware.files.wordpress.com/2023/09/img_3534.jpg?w=625" src="https://successfulsoftware.files.wordpress.com/2023/09/img_3534.jpg?w=1024" alt="" srcset="https://successfulsoftware.files.wordpress.com/2023/09/img_3534.jpg?w=1024 1024w, https://successfulsoftware.files.wordpress.com/2023/09/img_3534.jpg?w=2048 2048w, https://successfulsoftware.files.wordpress.com/2023/09/img_3534.jpg?w=150 150w, https://successfulsoftware.files.wordpress.com/2023/09/img_3534.jpg?w=300 300w, https://successfulsoftware.files.wordpress.com/2023/09/img_3534.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>Not that there are many pedestrians in Florida, of course. You are expected to have a car and drive everywhere. You can even eat your breakfast in your car.</p>



<figure><a href="https://successfulsoftware.files.wordpress.com/2023/09/dsc_0639.jpg"><img data-attachment-id="11192" data-permalink="https://successfulsoftware.net/2023/09/19/ship-of-fools/dsc_0639/" data-orig-file="https://successfulsoftware.files.wordpress.com/2023/09/dsc_0639.jpg" data-orig-size="1000,526" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;5&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;NIKON D7000&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;32&quot;,&quot;iso&quot;:&quot;800&quot;,&quot;shutter_speed&quot;:&quot;0.01&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="dsc_0639" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.files.wordpress.com/2023/09/dsc_0639.jpg?w=300" data-large-file="https://successfulsoftware.files.wordpress.com/2023/09/dsc_0639.jpg?w=625" src="https://successfulsoftware.files.wordpress.com/2023/09/dsc_0639.jpg?w=1000" alt="" srcset="https://successfulsoftware.files.wordpress.com/2023/09/dsc_0639.jpg 1000w, https://successfulsoftware.files.wordpress.com/2023/09/dsc_0639.jpg?w=150 150w, https://successfulsoftware.files.wordpress.com/2023/09/dsc_0639.jpg?w=300 300w, https://successfulsoftware.files.wordpress.com/2023/09/dsc_0639.jpg?w=768 768w" sizes="(max-width: 1000px) 100vw, 1000px"></a><figcaption>The breakfast drive-thru queue at Fort Myers Dunkin Donuts.</figcaption></figure>



<p>The provision of pavements/sidewalks is decidely lacking and public transport is pretty much non-existent. If you are too poor to own a car, hard luck. There did seem to be some cycle lanes, but they ran along major roads and weren’t segregated from all the enormous vehicles. They looked utterly terrifying. No wonder no-one was using them. Perhaps cyclists had tried, but they had all been run over.</p>



<p>Everywhere has air con and it all seems to run 24×7. Often with doors left open. When you turn up to your hotel/motel room, the air con is running and it doesn’t turn off when you take your card out of the slot to leave the room. It has probably been running in every room since the hotel was built, regardless of whether the rooms are occupied or not. Heaven forbid that you should have to wait 2 minutes for the air con to cool the room down. </p>



<p>This might be ok if the air con was powered by solar. But it isn’t. We hardly saw a solar panel in our whole trip to ‘The Sunshine State’. This is hard to fathom, as there are solar panels everywhere in temperate and cloudy Britain. When we asked one of the locals why she didn’t have solar, she told us that solar power was penalised by the power company, so it wasn’t worth it. We didn’t see a single wind turbine either.</p>



<p>The irony is that Florida is one of the most vulnerable places on earth to climate change. It is already ridiculously hot in the summer. A few more degrees of extra temperature will make it unbearable outside your air conditioned room or vehicle. Higher temperatures means more air con, which means more carbon in the atmosphere, which means even higher temperatures. Florida has a <a href="https://www.statista.com/statistics/1325529/lowest-points-united-states-state/">mean elevation of just 31m/100ft</a> above sea level. The majority of Miami-Dade county is <a href="https://wusfnews.wusf.usf.edu/environment/2023-03-11/miamis-hidden-high-ground-what-sea-rise-risk-means-for-some-prime-real-estate">less than 2m/6ft above sea level</a> (possibly less, depending on when you are reading this). The only thing we saw that looked like a hill in Florida, was in fact a huge landfill. Probably mostly full of single-use plastic cutlery. The rich are already starting to move to higher ground in Miami. Maybe only the landfills will be left above sea level by the end of the century? Florida is also  regularly devastated by hurricanes. The devastation left by 2022 <a href="https://en.wikipedia.org/wiki/Hurricane_Ian">category 5 hurricane Ian</a> is still very obvious and <a href="https://en.wikipedia.org/wiki/Hurricane_Idalia">category 4 hurricane Idalia</a> hit a few days after we left. Rising sea temperatures can only lead to more devastating hurricanes. </p>



<p>And Florida isn’t even one of the worst offenders, placing 39th out of the 50 US states with around <a href="https://solarpower.guide/solar-energy-insights/states-ranked-carbon-dioxide-emissions">10.8 metric tons of CO2 per capita per year</a>. In part due to the lack of any heavy industry. The worst offending state in the USA is Wyoming with a whopping 104.5  metric tons of CO2 per capita per year. Across the country Americans average <a href="https://www.worldometers.info/co2-emissions/co2-emissions-per-capita/">15.3 tons per capita per year</a>, compared to 5.6 tons for the UK. And the USA isn’t even the worst offender. Qatar clocks in at 38.1 tons per capita per year.</p>



<p>Climate change is not some minor inconvenience where we lose a few obscure species of frogs and have to wear a bit more sunscreen. We could be talking about widescale crop failures and extreme weather events making large parts of the globe unliveable. Leading to famine and migration on a scale way beyond anything we have seen so far. Given the seriousness of the situation it is depressing to see such profligate waste. My fear is that other people will look at places like Florida and think “why am I even trying to do the right thing? Look at them!” and not even try.</p>



<p>We are in trouble. The current system of sovereign states with politicians driven by short-term goals is poorly placed to fix long-term, global problems. And the billionaires are not going to save us. They are the main beneficiaries of the current system and they are going to use their money and power to keep it that way. If we let them. Geo-engineering is hugely risky. Carbon sequestration looks unlikely to make any meaningful difference. Moving to Mars is a pipedream for 99.9999% of the population. This is the only planet in the universe we have evolved to live on. We are stuck here with the mess we have created in a slow motion <a href="https://en.wikipedia.org/wiki/Tragedy_of_the_commons">tragedy of the commons.</a> Individual choice is not going to cut it. We need deep structural change. Much higher taxes on fossil fuels and less enormous pickup trucks for a start. We need to get our act together, and soon. For ourselves and our children. But, having seen the situation in Florida, I don’t hold out much hope.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[India's biggest tech centers named as cyber crime hotspots (183 pts)]]></title>
            <link>https://www.theregister.com/2023/09/21/india_cybercrime_trends_report/</link>
            <guid>37594855</guid>
            <pubDate>Thu, 21 Sep 2023 08:39:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/09/21/india_cybercrime_trends_report/">https://www.theregister.com/2023/09/21/india_cybercrime_trends_report/</a>, See on <a href="https://news.ycombinator.com/item?id=37594855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>India is grappling with a three-and-a-half year surge in cyber crime, with analysis suggesting cities like Bengaluru and Gurgaon – centers of India's tech development – are also hubs of evil activity.</p>
<p>The report – <i>A Deep Dive into Cybercrime Trends Impacting India</i> from the non-profit Future Crime Research Foundation (FCRF) – identified cyber crime hot spots, as well as the most popular types of infosec assaults, from January 2020 until June 2023.</p>
<p>"The analysis of the top 10 cyber crime-prone districts in India reveals several common factors contributing to their vulnerability. These include geographical proximity to major urban centers, limited cyber security infrastructure, socioeconomic challenges, and low digital literacy," <a target="_blank" href="https://www.futurecrime.org/cyber-crime-research">states</a> the report.</p>

    

<p>Several of the most cyber crime-prone top geographies house tech hubs. Gurgaon and Bangalore – both <a target="_blank" href="https://hightech.cbrevancouver.com/wp-content/uploads/2019/05/Asia-Pacific-Major-Report_Programming-Asia-Pacific-Tech-Cities-as-Global-Tech-Hubs_April-2019.pdf">considered</a> [PDF] among the top five most attractive cities for the IT industry in Asia – featured for the wrong reasons.</p>

        


        

<p>The Gurgaon district, which is home to a planned IT-focused city of the same name, made number six on FCRF's list. The district accounted for 8.1 percent of reported cyber crime, despite being home to less than 0.2 percent of India's population.</p>
<p>FCRF cited the high crime rate as "likely influenced by its status as a major corporate and IT hub, making it an attractive target for cyber criminals seeking valuable data or financial gains."</p>

        

<p>Outsourcing services and call centers are prominent in the area. Globally recognised tech names including Google, Microsoft, IBM India, Accenture, Cognizant, Infosys, Wipro and more all have presence in the city.</p>
<p>And while the city is known for economic affluence – it's <a target="_blank" href="https://www.gmda.gov.in/aboutus/metropolitan-area.html?language=en">said</a> to have the third highest per capita income in India – the Foundation suggested "disparities in digital literacy and cyber security awareness" could be factors likely to drive criminal activity.</p>
<ul>

<li><a href="https://www.theregister.com/2023/03/27/indian_cybergang_busted_for_selling/">India-based cybergang busted for selling fake KFC franchises</a></li>

<li><a href="https://www.theregister.com/2023/07/06/hpe_india_server_manufacturing/">HPE prepares for spicy affair with India to churn out $1B worth of servers</a></li>

<li><a href="https://www.theregister.com/2022/11/01/india_lending_app_crackdown_ordered/">India's Home Ministry cracks down on predatory lending apps following suicides</a></li>

<li><a href="https://www.theregister.com/2023/09/05/qualys_top_20_vulnerabilities/">You patched yet? Years-old Microsoft security holes still hot targets for cyber-crooks</a></li>
</ul>
<p>Meanwhile, Bangalore – in the district of Karnataka – was named by FCRF as an emerging cyber crime hotspot. The city is known as the "Silicon Valley of India" thanks to its proliferation of IT employers – including Infosys, Wipro, Tata Consultancy Services, IBM India, Microsoft, Google, Amazon Intel, Cisco, Samsung Research Institute, Nvidia Graphics and more.</p>
<p>Topping the list was Gurgaon's neighbor Bharatpur, with 18 percent of India's overall cyber crime. FCRF cites limited employment opportunities and lack of digital literacy as reasons for the region's prevalence of crime, as well as the fact it contains major urban centers like Delhi and Jaipur.</p>
<p>Mathura – a district filled with significant religious sites – took second place with 12 percent. The FCRF cited limited cyber security infrastructure and the area's status as a tourist attraction among likely reasons it was so popular with crims.</p>

        

<p>Another finding in the report was that of all reported cyber crimes in India, almost half (47.25 percent) involved Unified Payments Interface (UPI) fraud. Debit, credit card and sim swap fraud came in a distant second place with 11.27 percent. Overall, financially motivated crime accounted for 77.41 percent of incidents. ®</p>                                
                    </div></div>]]></description>
        </item>
    </channel>
</rss>