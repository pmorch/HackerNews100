<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 17 Jun 2024 12:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How to get stuff repaired when the manufacturer don't wanna: take 'em to court (255 pts)]]></title>
            <link>https://blog.simonrumble.com/how-to-get-your-stuff-repaired-when-the-retailer-and-manufacturer-dont-wanna-take-em-to-court</link>
            <guid>40702782</guid>
            <pubDate>Mon, 17 Jun 2024 06:14:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.simonrumble.com/how-to-get-your-stuff-repaired-when-the-retailer-and-manufacturer-dont-wanna-take-em-to-court">https://blog.simonrumble.com/how-to-get-your-stuff-repaired-when-the-retailer-and-manufacturer-dont-wanna-take-em-to-court</a>, See on <a href="https://news.ycombinator.com/item?id=40702782">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post_body_2116956"><div id="posthaven_gallery[2140350]">
          <p>
          <img src="https://phaven-prod.s3.amazonaws.com/files/image_part/asset/3196658/CiUdmRx96L6fOGk-we1a_QjyMIk/medium_PXL_20240530_015115189.jpg" data-posthaven-state="processed" data-medium-src="https://phaven-prod.s3.amazonaws.com/files/image_part/asset/3196658/CiUdmRx96L6fOGk-we1a_QjyMIk/medium_PXL_20240530_015115189.jpg" data-medium-width="800" data-medium-height="450" data-large-src="https://phaven-prod.s3.amazonaws.com/files/image_part/asset/3196658/CiUdmRx96L6fOGk-we1a_QjyMIk/large_PXL_20240530_015115189.jpg" data-large-width="1200" data-large-height="675" data-thumb-src="https://phaven-prod.s3.amazonaws.com/files/image_part/asset/3196658/CiUdmRx96L6fOGk-we1a_QjyMIk/thumb_PXL_20240530_015115189.jpg" data-thumb-width="200" data-thumb-height="200" data-xlarge-src="https://phaven-prod.s3.amazonaws.com/files/image_part/asset/3196658/CiUdmRx96L6fOGk-we1a_QjyMIk/xlarge_PXL_20240530_015115189.jpg" data-xlarge-width="2400" data-xlarge-height="1350" data-orig-src="https://phaven-prod.s3.amazonaws.com/files/image_part/asset/3196658/CiUdmRx96L6fOGk-we1a_QjyMIk/PXL_20240530_015115189.jpg" data-orig-width="4032" data-orig-height="2268" data-posthaven-id="3196658">
        </p>
          
        </div>
<p>A few weeks ago I was roasting some pumpkin for a delicious soup and towards the end of the cooking time the fan on the oven started into overdrive, making a lot of noise then it started beeping and popped up this obscure error message. I phoned the manufacturer, Electrolux, on the provided number and they told me I'd need to pay at least $160 to have their engineer come out and tell me what was wrong.</p><p>You've probably had this experience with lots of stuff. "Sorry, the item is out of warranty so you'll have to pay." The problem with this is that <a href="https://consumer.gov.au/consumers-and-acl">Australian Consumer Law</a> gives an <i>automatic</i> warranty. You can expect the item to last a reasonable amount of time. Now an old fashioned light bulb shouldn't be expected to last a decade, but an oven?</p><p>Challenged on this, I went around and around in circles with the Electrolux call centre worker. "So you think an oven should only last for two years?" and eventually I asked to be escalated to a manager who could actually make a decision. After some follow up, I finally got a call from a manager who was well drilled in shutting down any idea I should expect something from them. Eventually I said okay thanks, I'll see you at the Tribunal.</p><h2>How long should an appliance last?</h2><p>The "warranty" companies talk about is actually an "express warranty" and if you read them you'll notice these days they now include mandatory text about how they aren't able to exclude guarantees that come from the Australian Consumer Law. Anything they offer in their written warranty is in addition to your base rights.</p><p>So you have a reasonable expectation that your appliance will last a reasonable amount of time. So how long is reasonable? Well if you look around on the web you'll find different lengths of time for different classes of appliance. And if you buy the cheapest Chinesium appliance, you shouldn't expect it to last as long as the exxy Miele model.</p><h2>Time to book a court date</h2><p>NSW (and I think all the other states) has a tribunal especially for consumer claims, what used to be the "small claims court" is now the <a href="https://ncat.nsw.gov.au/" title="Link: https://ncat.nsw.gov.au/">NSW Civil and Administrative Tribunal</a>, NCAT. It's specifically designed to be low cost and straightforward. You shouldn't need a lawyer and can turn up with your documents.</p><p>The important thing to know about tribunals like NCAT is you're paying mostly with your time. You'll need to front up on the booked date and make your case. There's a small filing fee: in this case it was $58, which is still a lot less than Electrolux wanted to charge just to tell me what the problem was.</p><p>Before you book your date, you need to work out who is the other side of the transaction. You don't ordinarily go after the manufacturer but the retailer. So I contacted Appliances Online to talk it through. They took the same line as Electrolux that it was out of warranty and so not their problem. Again: see you in the Tribunal.</p><p>I filled in the forms. They're annoyingly slow, but the online system mostly works. Paid my fee and bingo, out comes an email with a tribunal date and location.</p><h2>Amazing service, just add NCAT date</h2><p>And the next day I get a call from the lovely Dylan from Appliances Online, someone who's evidently empowered to make decisions that make tribunal appointments go away. He tells me he'll get Electrolux to invoice them instead, an appointment is booked and we're off to the races.</p><p>One thing worth understanding is this: if the retailer has to send someone along to the Tribunal, they lose already. Even the cost of a junior lawyer going to the tribunal is going to be more than it would cost to repair your appliance. They might do it on principle if you're taking the piss but if you have a decent case they're wasting time and money.</p><h2>A couple of engineer visits</h2><p>Don't go cancelling your NCAT date just yet though! First you need the problem resolved. Remember, this could have all been resolved by them applying Australian Consumer Law when you first asked, so keep that clock ticking, it keeps things moving.</p><p>I had a lovely engineer from Electrolux visit and take a look. He wasn't sure what the problem was: tested the fan and heater element and found no problem. In the end he replaced the light bulb (which hadn't worked for years, we hadn't bothered replacing it) and the message had gone away anyway.</p><p>Next day the message is back, and Dylan drops me a note asking me to remove the NCAT booking. I respond saying it still isn't resolved and magically another engineer appointment pops up.</p><p>The second visit does the trick. The engineer replaced the heating element and fan and the message has gone away. We've done a few baking projects since and all seems good!</p><h2>Satisfaction, but annoyed I have to assert my rights</h2><p>So my oven is fixed. Otherwise it's a <i>great</i> oven. It's annoying that I have to push to get my consumer rights though. It should just be standard! My hope is that by encouraging others to also assert their rights it'll become easier. Don't put up with this shit about appliances having a tiny warranty period!<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenBSD, the computer appliance maker's secret weapon (112 pts)]]></title>
            <link>https://hiandrewquinn.github.io/til-site/posts/openbsd-the-computer-appliance-maker-s-secret-weapon/</link>
            <guid>40702180</guid>
            <pubDate>Mon, 17 Jun 2024 03:53:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hiandrewquinn.github.io/til-site/posts/openbsd-the-computer-appliance-maker-s-secret-weapon/">https://hiandrewquinn.github.io/til-site/posts/openbsd-the-computer-appliance-maker-s-secret-weapon/</a>, See on <a href="https://news.ycombinator.com/item?id=40702180">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Between our ESP32 prokaryotic organisms and our 24/7 Internet-enabled
megafauna servers, there exists a vast and loosely-defined ecosystem of things
the
B2B world likes to call <strong>computer appliances</strong>. Picture a bespoke Pi 4 packaged
up neatly with some Python scripts, a little fancy plastic embossing, and maybe
a well-guarded <code>id_ed25519.pub</code> in case you end up in hot water during the
(long - very long, stable cash flow for generations long) maintenance contract,
and you’re in the ballpark.</p><p>This is the little slice of computing heaven I currently live within. In a lot
of ways it feels like what I imagine employed hackers of old in the 90s were up
to. You can’t feed your data into Grafana, but you <em>can</em> <code>tail -f /var/log/syslog</code>
and make
a tidy profit off of your long-gestating Bash/Perl scripting skills.
You probably can’t <code>terraform destroy &amp;&amp; terraform apply</code>, but when was the
last time you saw immutable infrastructure done right anyway? Et cetera,
et similia.</p><p>Hey, you know what’s really dangerous over the 15 to 30 year lifespan of an
average B2B computer appliance? <em>Forgetting stuff.</em> Everyone can feel their
way around Debian 12, 11, maybe even 10 – but how do you debug a service that
is running all the way back on Debian <em>4</em>? Let’s not even get into the horrors
of Windows XP-based appliances, which power more of the world than you want
to know.</p><p>If only there were a freely-usable set of Unix-like operating systems, with an
emphasis on keeping things very, very stable over releases, even more stable
than Debian does. Enter <strong>the BSDs</strong>. Free, Open, Net, take your pick. All of
them take a “don’t fix it if it ain’t broken” approach to things, which means
someone who started slinging NetBSD installs back in 2007 can probably spin up
a well-manicured VM of that 2007 install and reliably make their way around
the system, even today, in 2024.</p><p>I’ve recently
<a href="https://hiandrewquinn.github.io/til-site/posts/i-m-turning-30-so-i-m-switching-to-openbsd/">taken to learning OpenBSD</a>
for this very reason. And for reasons of security: While having the box not
physically connected to the Internet creates an activation energy to doing
something nasty with them that
99.9% of
ne’er-do-wells will ignore, the remaining 0.1% are likely to be
<em>really motivated</em> to want to do this. If they run into OpenBSD, however, their
efforts are quite likely to be thwarted just because the blowfish is so darn
spiky.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I learned Haskell in just 15 years (154 pts)]]></title>
            <link>https://duckrabbit.tech/articles/learning-haskell.html</link>
            <guid>40702146</guid>
            <pubDate>Mon, 17 Jun 2024 03:44:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://duckrabbit.tech/articles/learning-haskell.html">https://duckrabbit.tech/articles/learning-haskell.html</a>, See on <a href="https://news.ycombinator.com/item?id=40702146">Hacker News</a></p>
Couldn't get https://duckrabbit.tech/articles/learning-haskell.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[They make USB-C cables with displays now (151 pts)]]></title>
            <link>https://ounapuu.ee/posts/2024/06/05/usb-c-cables/</link>
            <guid>40701310</guid>
            <pubDate>Mon, 17 Jun 2024 00:37:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ounapuu.ee/posts/2024/06/05/usb-c-cables/">https://ounapuu.ee/posts/2024/06/05/usb-c-cables/</a>, See on <a href="https://news.ycombinator.com/item?id=40701310">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p>I’ve reached a point in my setup where most of the devices that I use are
based around the coveted USB-C port. This meant that I had a valid reason to get
a few extra because I didn’t yet have a stockpile of good USB-C cables.</p>
<p>That’s when I found out that there exist cables that have little screens on them
that show the power consumption of the connected device. This is a great little
addition
to <a href="https://ounapuu.ee/posts/2024/05/02/smartplugs/">my power consumption monitoring addiction.</a>
It’s also a simple way to understand if your device is charging at the speed
that you expect it to.</p>
<p>The cable I ordered cost 6.72 EUR. It is a bit stiff and hard to work with,
probably due to its supposed USB 4 support requiring actually good cabling and
shielding. It works well enough for an USB-C dock with a DisplayPort
connection that’s running a 3440x1440 display at 60 Hz.</p>
<p>It’s too early to give a definitive answer about the longevity of the cable.</p>
<p>You should be able to find these types of cables with a search query like “USB 4
Cable with LED Display”.</p>
<p>The paranoid side of me suspects that a cable like this one would be an ideal
place to hide a malicious chip. That’s the only downside that I can think of.</p>
<p>If you’re interested in going more in depth with measuring the power consumption
over USB, then you might want to look at other options.
<a href="https://github.com/fqueze/usb-power-profiling?tab=readme-ov-file#power-meters-known-to-work">This GitHub repository lists a few examples of these types of measuring devices.</a></p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: SQLite Database Explorer (155 pts)]]></title>
            <link>https://github.com/frectonz/sqlite-studio</link>
            <guid>40700343</guid>
            <pubDate>Sun, 16 Jun 2024 21:39:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/frectonz/sqlite-studio">https://github.com/frectonz/sqlite-studio</a>, See on <a href="https://news.ycombinator.com/item?id=40700343">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">SQLite Studio</h2><a id="user-content-sqlite-studio" aria-label="Permalink: SQLite Studio" href="#sqlite-studio"></a></p>
<p dir="auto">Single binary, single command SQLite database explorer.</p>
<div dir="auto" data-snippet-clipboard-copy-content="sqlite-studio <sqlite_db>"><pre>sqlite-studio <span>&lt;</span>sqlite_db<span>&gt;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Overview page with common metadata.</li>
<li>Tables page with each table's metadata, including the disk size being used by each table.</li>
<li>Infinite scroll rows view.</li>
<li>A custom query page that gives you more access to your db.</li>
</ul>
<p dir="auto">More features available on the <a href="https://github.com/frectonz/sqlite-studio/releases">releases page</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Home Page</h3><a id="user-content-home-page" aria-label="Permalink: Home Page" href="#home-page"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/frectonz/sqlite-studio/blob/main/screenshots/homepage.png"><img src="https://github.com/frectonz/sqlite-studio/raw/main/screenshots/homepage.png" alt="homepage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tables Page</h3><a id="user-content-tables-page" aria-label="Permalink: Tables Page" href="#tables-page"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/frectonz/sqlite-studio/blob/main/screenshots/tables.png"><img src="https://github.com/frectonz/sqlite-studio/raw/main/screenshots/tables.png" alt="tables"></a>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/53809656/339976146-b6d8f627-4a21-46c2-bef7-8dea206b3689.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTg2MDk3MDIsIm5iZiI6MTcxODYwOTQwMiwicGF0aCI6Ii81MzgwOTY1Ni8zMzk5NzYxNDYtYjZkOGY2MjctNGEyMS00NmMyLWJlZjctOGRlYTIwNmIzNjg5LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWYwOWExZjJhZDZjZTU3NDc4ZWFhOWViZmY0MjNhNzNmNjEwNzY4YzUwYjRkNGY4Njc4ZTJjNzg4ZjNlNDAxMDkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xYlazaBFqPOQj-3JXdGLwu69ErTrZaSAf73buD_vGDk"><img src="https://private-user-images.githubusercontent.com/53809656/339976146-b6d8f627-4a21-46c2-bef7-8dea206b3689.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTg2MDk3MDIsIm5iZiI6MTcxODYwOTQwMiwicGF0aCI6Ii81MzgwOTY1Ni8zMzk5NzYxNDYtYjZkOGY2MjctNGEyMS00NmMyLWJlZjctOGRlYTIwNmIzNjg5LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWYwOWExZjJhZDZjZTU3NDc4ZWFhOWViZmY0MjNhNzNmNjEwNzY4YzUwYjRkNGY4Njc4ZTJjNzg4ZjNlNDAxMDkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xYlazaBFqPOQj-3JXdGLwu69ErTrZaSAf73buD_vGDk" alt="infinite scroll" data-animated-image=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Query Page</h3><a id="user-content-query-page" aria-label="Permalink: Query Page" href="#query-page"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/frectonz/sqlite-studio/blob/main/screenshots/query.png"><img src="https://github.com/frectonz/sqlite-studio/raw/main/screenshots/query.png" alt="query"></a>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/53809656/339525291-3e47a890-ddd9-4c7f-be88-53e30cc23b15.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTg2MDk3MDIsIm5iZiI6MTcxODYwOTQwMiwicGF0aCI6Ii81MzgwOTY1Ni8zMzk1MjUyOTEtM2U0N2E4OTAtZGRkOS00YzdmLWJlODgtNTNlMzBjYzIzYjE1LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZlMGIwZTUxMWM4NmQ1NDg1MGUyYWYyMDEwMzgxNzAzMjE3MGEwNjYzOTk2N2JkZTMxNDMyYjFmYWRjZWIwZTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.PAPxhum6gYAgbbmU1X3h6VJcDFdHJpwubPo9RYOxLCs"><img src="https://private-user-images.githubusercontent.com/53809656/339525291-3e47a890-ddd9-4c7f-be88-53e30cc23b15.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTg2MDk3MDIsIm5iZiI6MTcxODYwOTQwMiwicGF0aCI6Ii81MzgwOTY1Ni8zMzk1MjUyOTEtM2U0N2E4OTAtZGRkOS00YzdmLWJlODgtNTNlMzBjYzIzYjE1LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjE3VDA3MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZlMGIwZTUxMWM4NmQ1NDg1MGUyYWYyMDEwMzgxNzAzMjE3MGEwNjYzOTk2N2JkZTMxNDMyYjFmYWRjZWIwZTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.PAPxhum6gYAgbbmU1X3h6VJcDFdHJpwubPo9RYOxLCs" alt="query gif" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How To Run It</h2><a id="user-content-how-to-run-it" aria-label="Permalink: How To Run It" href="#how-to-run-it"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pre-Built Binaries</h3><a id="user-content-pre-built-binaries" aria-label="Permalink: Pre-Built Binaries" href="#pre-built-binaries"></a></p>
<p dir="auto">You can find pre-built binaries for the following targets on the <a href="https://github.com/frectonz/sqlite-studio/releases">releases</a> page.</p>
<ul dir="auto">
<li>Linux <code>sqlite-studio_&lt;release&gt;_x86_64-unknown-linux-musl.zip</code></li>
<li>Windows <code>sqlite-studio_&lt;release&gt;_x86_64-pc-windows-gnu.zip</code></li>
<li>MacOS x86 <code>sqlite-studio_&lt;release&gt;_x86_64-apple-darwin.zip</code></li>
</ul>
<p dir="auto">After downloading the ZIP archive, you can extract it and get the binary.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Nix</h3><a id="user-content-nix" aria-label="Permalink: Nix" href="#nix"></a></p>
<p dir="auto">If you are using <a href="https://nixos.org/" rel="nofollow">Nix</a>, to build it from source.</p>
<div dir="auto" data-snippet-clipboard-copy-content="nix shell github:frectonz/sqlite-studio
sqlite-studio <sqlite_db>"><pre>nix shell github:frectonz/sqlite-studio
sqlite-studio <span>&lt;</span>sqlite_db<span>&gt;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Before executing <code>cargo run</code> you need to build the UI because the rust app statically embedded the UI files in the binary.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:frectonz/sqlite-studio.git
cd sqlite-studio
nix develop # if you use nix
cd ui
npm install
npm run build
cd ..
cargo run <sqlite_db>"><pre>git clone git@github.com:frectonz/sqlite-studio.git
<span>cd</span> sqlite-studio
nix develop <span><span>#</span> if you use nix</span>
<span>cd</span> ui
npm install
npm run build
<span>cd</span> ..
cargo run <span>&lt;</span>sqlite_db<span>&gt;</span></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Google Takeout is sooo bad (120 pts)]]></title>
            <link>https://marcin.cylke.com.pl/2024/06/16/why-google-takeout-is-sooo-bad/</link>
            <guid>40700146</guid>
            <pubDate>Sun, 16 Jun 2024 21:12:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marcin.cylke.com.pl/2024/06/16/why-google-takeout-is-sooo-bad/">https://marcin.cylke.com.pl/2024/06/16/why-google-takeout-is-sooo-bad/</a>, See on <a href="https://news.ycombinator.com/item?id=40700146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>















  
  
      
      
  <picture>
  <img src="https://marcin.cylke.com.pl/post_images/why-google-takeout-is-sooo-bad-0.png?v=583ac1f00b55e860bec2435a9f014ec7" alt="Image" loading="lazy" height="1024" width="1024">
</picture>

<em>Author: <a href="https://ideogram.ai/">IdeogramAI</a> Prompt: The image shows a typical Google Takeout interface with a large error message indicating that the service is unavailable. In the background, there are frustrated users trying to download their data with unsuccessful attempts, represented by repeated “Failed” and “Retry” notifications.</em></p>
<hr>
<p>My current data setup is centered around Google Cloud as a place where I store my data. That’s also related to how I use the whole vendor ecosystem.</p>
<p>The main issue I have with keeping the data only in Google Cloud is data safety. I just don’t want either identity theft or accidental block on my account (by Google) to cut me off from all of my previous photos. You might think it’s something close to impossible, but there are numerous examples of that happening to people - eg. look here <a href="https://www.nytimes.com/2022/08/21/technology/google-surveillance-toddler-photo.html">https://www.nytimes.com/2022/08/21/technology/google-surveillance-toddler-photo.html</a> And ok, this might be a reasonable approach from Google, but who knows what else can trigger that kind of blockage.</p>
<p>So, to be safe against anything similar or just any other data lose, it would be good to have some backup in place.</p>
<p>An ideal solution right now would be to:</p>
<ul>
<li>copy all the photos to some other cloud</li>
<li>have an offline copy of those files/photos</li>
</ul>
<p>Ideally both of those should happen automatically, according to some predefined schedule. Of course be as maintenance-free as possible.</p>
<p>I have ~200GB of data that I would like backed up on a regular basis. I’d imagine this is not the biggest set of photos a person can have stored there.</p>
<h2 id="first-attempts">First attempts</h2>
<p>My first approach was to synchronize all the files and all the photos using some syncing tools. Store them on a local hard drive, for future use. But that was less then satisfying. Mainly due to how long the process would take for my size of data.</p>
<p>There’s this open-source python tool <a href="https://github.com/gilesknap/gphotos-sync">https://github.com/gilesknap/gphotos-sync</a> - but I’m yet to try that. Although they also note on their project page issues with the backup process. Citing from there:</p>
<ul>
<li>Videos are transcoded to lower quality</li>
<li>Raw or Original photos are converted to ‘High Quality’</li>
<li>GPS info is removed from photos metadata</li>
</ul>
<p>Bad, but I could live with that, if treating this as a backup.</p>
<p>There’s also a paid tool <a href="https://photovaultone.com/">https://photovaultone.com/</a> - but it’s just another small vendor lock-in. I’d like to avoid getting deeper into other vendor to be protected from Google.</p>
<h2 id="so-theres-google-takeout">So there’s Google Takeout</h2>
<p>And yet, there’s this Google-offered option, that’s designed as a way to take all your data out of Google Cloud and potentially go to some other vendor, or host that on your own. Great idea! Of course Google, as other BigTech companies was coherced to implement it mainly due to GDPR restrictions - so that any user can take the data he/she owns and move out to other place.
Naturally, I thought I’d use this approach to backup my data.
This seemed even more amazing, as there’s an option to backup directly to other providers - the options are Dropbox, Microsoft OneDrive, Box.</p>
<p>I thought that using Microsoft OneDrive is a good alternative! Especially in terms of provider diversification. Also, the options of the backup would allow me to create a couple of bigger files - and copying those to my local backup would be much easier then doing so with hundreds of thousands of photos.</p>
<p>Choosing Microsoft OneDrive as an intermediate solution is just using another vendor. But the goal is to:</p>
<ul>
<li>have the data backed up in another cloud</li>
<li>be able to download the data in bigger chunks</li>
</ul>
<p>On the surface, it looks nice. You can create a scheduled backup process - that may happen at specified intervals, or you can do it one-time.
You can select data you want to export - be it just Photos, or maybe whole Google Drive, and Calendar, Gmail. Everything is there.</p>
<p>But when it actually needs to happen - that’s where the issues happen.</p>
<h2 id="issues">Issues</h2>
<ul>
<li>the process always fails - no matter what’s the size of a chunk I choose</li>
<li>the logs are kept only for some time, afer 7 days I’m not able to see what happened with the chunks. It’s easy to spot missing files - when you have parts from 1 to 10, but there’s a 7th missing. But what if there are 10 parts but should be 11?</li>
<li>no logs of the process</li>
<li>errors are very cryptic “Failed to backup”</li>
<li>tech support is non-existent - I’ve reached out to the tech support, but they were just passing my issue between different support divisions. Ultimately there were hints like “reinstall your operating system”, “turn it off and on”</li>
</ul>
<p>All in all I don’t see this whole process as <strong>extremely</strong> hard. Also, I totally understand those things may fail. But we, as an industry, know what to do with that. There are retries, informative logging. And here I got none of the above.</p>
<p>So for me, this broadly looks like Google just does not want people to backup their data outside of their cloud, which is super bad!</p>
<h2 id="important-considerations-that-i-may-have-missed">Important considerations that I may have missed</h2>
<p>I share my photo collection with my closest family. I suspect that Google Takeout won’t backup those shared photos. But my purpose <strong>is</strong> to do also copy those.</p>
<p>If I’ll be able to make this process reliable, I’d need to think of data retention on target.</p>
<p>Right now I backup just Google Photos - wanted to move to other parts of the ecosystem, when the backup issues are sorted out.</p>
<h2 id="follow-up-actions">Follow-up actions</h2>
<ul>
<li>try other providers - I don’t want to use Dropbox, but perhaps Box would work better then OneDrive?</li>
<li>are there tools that I’ve missed that might actually work?</li>
<li>schedule separate backup processes for other parts of the data stored in Google - they’ll be smaller and maybe that will result in successful backups</li>
<li>monitor the backup process state - based on target drive contents</li>
</ul>
<h2 id="closing-notes">Closing notes</h2>
<p>So, for know I still do the backups. I accept the issues with Google Takeout. It’s just better to have majority of my data, then to not have any of that.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MicroMac, a Macintosh for Under £5 (627 pts)]]></title>
            <link>https://axio.ms/projects/2024/06/16/MicroMac.html</link>
            <guid>40699684</guid>
            <pubDate>Sun, 16 Jun 2024 20:02:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://axio.ms/projects/2024/06/16/MicroMac.html">https://axio.ms/projects/2024/06/16/MicroMac.html</a>, See on <a href="https://news.ycombinator.com/item?id=40699684">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    
<p><a href="https://axio.ms/images/umac/umac_startup.png"> 
    <img src="https://axio.ms/images/umac/umac_startup.png" srcset="     https://axio.ms/images/umac/umac_startup.png 454w" width="100%" alt=" ">
 </a></p>

<h2 id="a-microcontroller-macintosh">A microcontroller Macintosh</h2>

<p>This all started from a conversation about the RP2040 MCU, and building a simple desktop/GUI for it.  I’d made a comment along the lines of “or, just run some old OS”, and it got me thinking about the <a href="https://en.wikipedia.org/wiki/Macintosh_128K">original Macintosh</a>.</p>

<p>The original Macintosh was released 40.5 years before this post, and is a pretty cool machine especially considering that the hardware is very simple.  <a href="https://www.stevenlevy.com/insanely-great">Insanely Great</a> and <a href="https://folklore.org/">folklore.org</a> are fun reads, and give a glimpse into the Macintosh’s development.  Memory was a squeeze; the original 128KB version was underpowered and only sold for a few months before being replaced by the <em>Macintosh 512K</em>, arguably a more appropriate amount of memory.</p>

<p>But, the 128 still runs <em>some</em> real applications and, though it pre-dates MultiFinder/actual multitasking, I found it pretty charming.  As a tourist.  In 1984 the Mac cost roughly 1/3 as much as a VW Golf and, as someone who’s into old computers and old cars, it’s hard to decide which is more frustrating to use.</p>

<p>So back to this £3.80 RPi Pico microcontroller board:  The RP2040’s 264KB of RAM gives a lot to play with after carving out the Mac’s 128KB – how cool would it be to do a quick hack, and play with a Mac on it?</p>

<p>Time passes.  A lot of time.  But I totally delivered on the janky hack front:</p>

<p><a href="https://axio.ms/images/umac/umac_whole.jpg"> 
    <img src="https://axio.ms/assets/resized/1400/umac_whole.jpg" srcset="            https://axio.ms/assets/resized/480/umac_whole.jpg 480w,            https://axio.ms/assets/resized/800/umac_whole.jpg 800w,            https://axio.ms/assets/resized/1400/umac_whole.jpg 1400w,    " width="100%" alt=" ">
 </a></p>

<p><del>You won’t believe that this quality item didn’t take that long to build.</del> So the software was obviously the <em>involved</em> part, and turned into work on 3 distinct projects.</p>

<p>This post is going to be a “development journey” story, as a kind of code/design/venting narrative.  If you’re just here for the pictures, scroll along!</p>

<h2 id="what-is-pico-mac">What is pico-mac?</h2>

<p>A Raspberry Pi RP2040 microcontroller (on a Pico board), driving monochrome VGA video and taking USB keyboard/mouse input, emulating a <em>Macintosh 128K</em> computer and disc storage.  The RP2040 has easily enough RAM to house the Mac’s memory, plus that of the emulator; it’s fast enough (with some tricks) to meet the performance of the real machine, has USB host capability, and the PIO department makes driving VGA video fairly uneventful (with some tricks).  The basic Pico board’s 2MB of flash is plenty for a disc image with OS and software.</p>

<p>Here’s the Pico MicroMac in action, ready for the paperless office of the future:</p>

<figure>
 <a href="https://axio.ms/images/umac/umac_workstation.jpg"> 
    <img src="https://axio.ms/assets/resized/1400/umac_workstation.jpg" srcset="            https://axio.ms/assets/resized/480/umac_workstation.jpg 480w,            https://axio.ms/assets/resized/800/umac_workstation.jpg 800w,            https://axio.ms/assets/resized/1400/umac_workstation.jpg 1400w,    " width="100%" alt=" The Pico MicroMac RISC CISC workstation of the future">
 </a> 
<figcaption>The Pico MicroMac RISC CISC workstation of the future</figcaption></figure>

<p>I hadn’t really used a <em>Mac 128K</em> much before; a few clicks on a museum machine once.  But I knew they ran MacDraw, and MacWrite, and MacPaint.  All three of these applications are pretty cool for a 128K machine; a largely WYSIWYG word processor with multiple fonts, and a vector drawing package.</p>

<p>A great way of playing with early Macintosh system software, and applications of these wonderful machines is via <a href="https://infinitemac.org/">https://infinitemac.org</a>, which has shrinkwrapped running the Mini vMac emulator by emscriptening it to run in the browser.  Highly recommended, lots to play with.</p>

<p>As a spoiler, MicroMac does run MacDraw, and it was great to play with it on “real fake hardware”:</p>

<p><a href="https://axio.ms/images/umac/umac_workstation2.jpg"> 
    <img src="https://axio.ms/assets/resized/1400/umac_workstation2.jpg" srcset="            https://axio.ms/assets/resized/480/umac_workstation2.jpg 480w,            https://axio.ms/assets/resized/800/umac_workstation2.jpg 800w,            https://axio.ms/assets/resized/1400/umac_workstation2.jpg 1400w,    " width="100%" alt=" ">
 </a></p>

<p>(Do you find “Pico Micro Mac” doesn’t really scan?  I didn’t think this taxonomy through, did I?)</p>

<p>GitHub links are at the bottom of this page:  the <code>pico-mac</code> repo has <a href="https://github.com/evansm7/pico-mac?tab=readme-ov-file#hardware-contruction">construction directions</a> if you want to build your own!</p>

<h2 id="the-journey">The journey</h2>

<p>Back up a bit.  I wasn’t committed to building a Pico thing, but was vaguely interested in whether it was feasible, so started tinkering with building a <em>Mac 128K emulator</em> on my normal computer first.</p>

<h3 id="the-three-rules">The three rules</h3>

<p>I had a few simple rules for this project:</p>

<ol>
  <li>It had to be fun.  It’s OK to hack stuff to get it working, it’s not as though I’m being paid for this.</li>
  <li>I like writing emulation stuff, but I really don’t want to learn 68K assembler, or much about the 68K.  There’s a lot of love for 68K out there and that’s cool, but meh I don’t adore it as a CPU.  So, right from the outset I wanted to use someone else’s 68K interpreter – I knew there were loads around.</li>
  <li>Similarly, there are a load of OSes whose innards I’d like to learn more about, but the shittiest early Mac System software isn’t high on the list.  Get in there, emulate the hardware, boot the OS as a black box, done.</li>
</ol>

<p>I ended up breaking 2 of and sometimes all 3 of these rules during this project.</p>

<h3 id="the-mac-128k">The Mac 128K</h3>

<p>The machines are generally pretty simple, and of their time.  I started with schematics and <em>Inside Macintosh</em>, PDFs of which covered various details of the original Mac hardware, memory map, mouse/keyboard, etc.</p>
<ul>
  <li><a href="https://tinkerdifferent.com/resources/macintosh-128k-512k-schematics.79/">https://tinkerdifferent.com/resources/macintosh-128k-512k-schematics.79/</a></li>
  <li><a href="https://vintageapple.org/inside_o/">https://vintageapple.org/inside_o/</a> <em>Inside Macintosh Volumes I-III</em> are particularly useful for hardware information; also <em>Guide to Macintosh Family Hardware 2nd Edition</em>.</li>
</ul>

<p>The Macintosh has:</p>
<ul>
  <li>A Motorola 68000 CPU running at <del>7.whatever MHz</del> roughly 8MHz</li>
  <li>Flat memory, decoded into regions for memory-mapped IO going to the 6522 VIA, the 8530 SCC, and the IWM floppy controller.  (Some of the address decoding is a little funky, though.)</li>
  <li>Keyboard and mouse hang off the VIA/SCC chips.</li>
  <li>No external interrupt controller: the 68K has 3 IRQ lines, and there are 3 IRQ sources (VIA, SCC, programmer switch/NMI).</li>
  <li>“No slots” or expansion cards.</li>
  <li>No DMA controller: a simple autonomous PAL state machine scans video (and audio samples) out of DRAM.  Video is fixed at 512x342 1BPP.</li>
  <li>The only storage is an internal FDD (plus an external drive), driven by the IWM chip.</li>
</ul>

<p>The first three Mac models are extremely similar:</p>
<ul>
  <li>The <em>Mac 128K</em> and <em>Mac 512K</em> are the same machine, except for RAM.</li>
  <li>The <em>Mac Plus</em> added SCSI to a convenient space in the memory map and an 800K floppy drive, which is double-sided whereas the original was a single 400K side.</li>
  <li>The <em>Mac Plus</em> ROM also supports the 128K/512K, and was an upgrade to create the <em>Macintosh 512Ke</em>.  ‘e’ for Extra ROM Goodness.</li>
</ul>

<p>The <em>Mac Plus</em> ROM supports the HD20 external hard disc, and HFS, <em>and</em> Steve Chamberlin has <a href="https://www.bigmessowires.com/rom-adapter/plus-rom-listing.asm">annotated a disassembly of it</a>.  This was the ROM to use:  I was making a <em>Macintosh 128Ke</em>.</p>

<h3 id="mac-emulator-umac">Mac emulator: umac</h3>

<p>After about 8 minutes of research, I chose the <a href="https://github.com/kstenerud/Musashi">Musashi</a> 68K interpreter.  It’s C, simple to interface to, and had a simple out-of-box example of a 68K system with RAM, ROM, and some IO.  <em>Musashi</em> is structured to be embedded in bigger projects: wire in memory read/write callbacks, a function to raise an IRQ, call execute in a loop, done.</p>

<p>I started building an emulator around it, which ultimately became the <a href="https://github.com/evansm7/umac">umac</a> project.  The first half (of, say, five halves) went pretty well:</p>

<ol>
  <li>A simple commandline app loading the ROM image, allocating RAM, providing debug messages/assertions/logging, and configuring <em>Musashi</em>.</li>
  <li>Add address decoding: CPU reads/writes are steered to RAM, or ROM.  The “overlay” register lets the ROM boot at <code>0x00000000</code> and then trampoline up to a high ROM mirror after setting up CPU exception vectors – this affects the address decoding.  This is done by poking a VIA register, so decoded just that bit of that register for now.</li>
  <li>At this point, the ROM starts running and accessing more non-existent VIA and SCC registers.  Added more decoding and a skeleton for emulating these devices elsewhere – the MMIO read/writes are just stubbed out.</li>
  <li>There are some magic addresses that the ROM accesses that “miss” documented devices: there’s a manufacturing test option that probes for a plugin (just thunk it), and then we witness the RAM size probing.  The <em>Mac Plus</em> ROM is looking for up to 4MB of RAM.  In the large region devoted to RAM, the smaller amount of actual RAM is mirrored over and over, so the probe writes a magic value at high addresses and spots where it starts to wrap around.</li>
  <li>RAM is then initialised and filled with a known pattern.  This was an exciting point to get to because I could dump the RAM, convert the region used for the video framebuffer into an image, and see the “diagonal stripe” pattern used for RAM testing!  <em>“She’s alive!”</em></li>
  <li>Not all of the device code enjoyed reading all zeroes, so there was a certain amount of referring to the disassembly and returning, uh, <code>0xffffffff</code> sometimes to push it further.  The goal was to get it as far as accessing the IWM chip, i.e. trying to load the OS.</li>
  <li>After seeing some IWM accesses there and returning random rubbish values, the first wonderful moment was getting the “Unknown Disc” icon with the question mark – real graphics!  The ROM was <em>REALLY DOING SOMETHING!</em></li>
  <li>I <em>think</em> I hadn’t implemented any IRQs at this point, and found the ROM in an infinite loop: it was counting a few Vsyncs to delay the flashing question mark.  Diversion into a better VIA, with callbacks for GPIO register read/write, and IRQ handling.  This also needed to wire into <em>Musashi</em>’s IRQ functions.</li>
</ol>

<p>This was motivating to get to – remembering rule #1 – and “graphics”, even though via a manual memory dump/ImageMagick conversion, was great.</p>

<p>I knew the <a href="https://en.wikipedia.org/wiki/Integrated_Woz_Machine">IWM</a> was an “interesting” chip, but didn’t know details.  I planned to figure it out when I got there (rule #1).</p>

<h4 id="iwm-68k-and-disc-drivers">IWM, 68K, and disc drivers</h4>

<p>My god, I’m glad I put IWM off until this point.  If I’d read the “datasheet” (vague register documentation) first, I’d’ve just gone to the pub instead of writing this shitty emulator.</p>

<p>IWM is very clever, but very very low-level.  The disc controllers in other contemporary machines, e.g. <a href="https://en.wikipedia.org/wiki/Western_Digital_FD1771">WD1770</a>, abstract the disc physics.  At one level, you can poke regs to step to track 17 and then ask the controller to grab sector 3.  Not so with IWM:  first, the discs are Constant Linear Velocity, meaning the angular rotation needs to change appropriate to whichever track you’re on, and second the IWM just gives the CPU a firehose of crap from the disc head (with minimal decoding).  I spent a while reading through the disassembly of the ROM’s IWM driver (breaking rule #2 and rule #1): there’s some kind of servo control loop where the driver twiddles PWM values sent to a DAC to control the disc motor, measured against a VIA timer reference to do some sort of dynamic rate-matching to get the correct bitrate from the disc sectors.  I think once it finds the track start it then streams the track into memory, and the driver decodes the symbols (more clever encoding) and selects the sector of interest.</p>

<p>I was sad.  Surely <em>Basilisk II</em> and <em>Mini vMac</em> etc. had solved this in some clever way – they emulated floppy discs.  I learned they do not, and do the smart engineering thing instead:  avoid the problem.</p>

<p>The other emulators do quite a lot of ROM patching: the ROM isn’t run unmodified.  You can argue that this then isn’t a perfect hardware emulation if you’re patching out inconvenient parts of the ROM, but so what.  I suspect they were also abiding by a rule #1 too.</p>

<p>I was going to do the same:  I figured out a bit of how the Mac driver interface works (gah, rule #3!) and understood how the other emulators patched this.  They use a custom <em>paravirtualised</em> 68K driver which is copied over the ROM’s IWM driver, servicing <code>.Sony</code> requests from the block layer and routing them to more convenient host-side code to manage the requests.  <em>Basilisk II</em> uses some custom 68K opcodes and a simple driver, and <em>Mini vMac</em> a complex driver with trappy accesses to a custom region of memory.  I reused the <em>Basilisk II</em> driver but converted to access a trappy region (easier to route: just emulate another device).  The driver callbacks land in the host/C side and some cut-down <em>Basilisk II</em> code interprets the requests and copies data to/from the OS-provided buffers.  Right now, all I needed was to read blocks from one disc:  I didn’t need different formats (or even write support), or multiple drives, or ejecting/changing images.</p>

<p>Getting the first block loaded from disc took waaaayyy longer than the first part.  And, I’d had to learn a bit of 68K (gah), but just in the nick of time I got a Happy Mac icon as the System software started to load.</p>

<p>This was still a simple Linux commandline application, with zero UI.  No keyboard or mouse, no video.  Time to wrap it in an SDL2 frontend (the <code>unix_main</code> test build in the <code>umac</code> project), and I could watch the screen redraw live.  I hadn’t coded the 1Hz timer interrupt into the VIA, and after adding that it booted to a desktop!</p>

<figure>
 <a href="https://axio.ms/images/umac/umac_first_desktop.png"> 
    <img src="https://axio.ms/assets/resized/480/umac_first_desktop.png" srcset="            https://axio.ms/assets/resized/480/umac_first_desktop.png 480w,    " width="100%" alt=" The first boot">
 </a> 
<figcaption>The first boot</figcaption></figure>

<p>As an aside, I try to create a dual-target build for all my embedded projects, with a native host build for rapid prototyping/debugging; libSDL instead of an LCD.  It means I don’t need to code <em>at</em> the MCU, so I can code in the garden.  :)</p>

<p>Next was mouse support.  <em>Inside Macintosh</em> and the schematics show how it’s wired, to the VIA (good) and the SCC (a beast).  The SCC is my second least-favourite chip in this machine; it’s complex and the datasheet/manual seems to be intentionally written to hide information, piss off readers, get one back at the world.  (I didn’t go near the serial side, its main purpose, just external IRQ management.  But, it’ll do all kinds of exciting 1980s line coding schemes, offloading bitty work from the CPU.  It was key for supporting things like AppleTalk.)</p>

<p>Life was almost complete at this point; with a working mouse I could build a new disc image (using <em>Mini vMac</em>, an exercise in itself) with <em>Missile Command</em>.  This game is pretty fun for under 10KB on disc.</p>

<p>So:</p>
<ul>
  <li>Video works</li>
  <li>Boots from disc</li>
  <li>Mouse works, Missile Command</li>
</ul>

<p>I had no keyboard, but it’s largely working now.  Time to start on sub-project numero due:</p>

<h3 id="hardware-and-rp2040">Hardware and RP2040</h3>

<p>Completely unrelated to <code>umac</code>, I built up a circuit and firmare with two goals:</p>

<ol>
  <li>Display 512x342x1 video to VGA with minimal components,</li>
  <li>Get the TinyUSB HID example working and integrated.</li>
</ol>

<p>This would just display a test image copied to a framebuffer, and <code>printf()</code> keyboard/mouse events, as a PoC.  The video portion was fun:  I’d done some <code>I2S</code> audio PIO work before, but here I wanted to scan out video and arbitrarily control Vsync/Hsync.</p>

<p>Well, to test I needed a circuit.  VGA wants 0.7V max on the video R,G,B signals and (mumble, some volts) on the syncs.  The R,G,B signals are 75Ω to ground:  with some maths, a 3.3V GPIO driving all three through a 100Ω resistor is roughly right.</p>

<p>The day I started soldering it together I needed a VGA connector.  I had a DB15 but wanted it for another project, and felt bad about cutting up a VGA cable.  But when I took a walk at lunchtime, no shitting you, I passed some street cables.  I had a VGA cable – the rust helps with the janky aesthetic.</p>

<figure>
 <a href="https://axio.ms/images/umac/street_wire.jpg"> 
    <img src="https://axio.ms/assets/resized/1400/street_wire.jpg" srcset="            https://axio.ms/assets/resized/480/street_wire.jpg 480w,            https://axio.ms/assets/resized/800/street_wire.jpg 800w,            https://axio.ms/assets/resized/1400/street_wire.jpg 1400w,    " width="100%" alt=" Free VGA cable">
 </a> 
<figcaption>Free VGA cable</figcaption></figure>

<p>The <a href="https://github.com/evansm7/pico-mac/blob/main/src/pio_video.pio">VGA PIO side</a> was pretty fun.  It ended up as PIO reading config info dynamically to control Hsync width, display position, and so on, and then some tricks with DMA to scan out the config info interleaved with framebuffer data.  By shifting the bits in the right direction and by using the byteswap option on the RP2040 DMA, the big-endian Mac framebuffer can be output directly without CPU-side copies or format conversion.  Cool.  This can be fairly easily re-used in other projects: see <a href="https://github.com/evansm7/pico-mac/blob/main/src/video.c">video.c</a>.</p>

<p>But.  I ended up (re)writing the video side three times in total:</p>

<p>First version had two DMA channels writing to the PIO TX FIFO.  The first would transfer the config info, then trigger the second to transfer video data, then raise an IRQ.  The IRQ handler would then have a short time (the FIFO depth!) to choose a new framebuffer address to read from, and reprogram DMA.  It worked OK, but was highly sensitive to other activity in the system.  First and most obvious fix is that any latency-sensitive IRQ handler <em>must</em> have the <code>__not_in_flash_func()</code> attribute so as to run out of RAM.  But even with that, the design didn’t give much time to reconfigure the DMA:  random glitches and blanks occurred when moving the mouse rapidly.</p>

<p>Second version did double-buffering with the goal of making the IRQ handler’s job trivial: poke in a pre-prepared DMA config quickly, then after the critical rush calculate the buffer to use for next time.  Lots better, but still some glitches under some high load.  Even weirder, it’d sometimes just blank out completely, requiring a reset.  This was puzzling for a while; I ended up printing out the PIO FIFO’s <code>FDEBUG</code> register to try to catch the bug in the act.  I saw that the <code>TXOVER</code> overflow flag was set, and this should be impossible: the FIFOs pull data from DMA on demand with DMA requests and a credited flow-contr…OH WAIT.  If credits get messed up or duplicated, too many transfers can happen, leading to an overflow at the receiver side.</p>

<p>Well, I’d missed a subtle rule in the RP2040 DMA docs:</p>

<blockquote>
  <p>Another caveat is that multiple channels should not be connected to the same DREQ.</p>
</blockquote>

<p>So the third version…… doesn’t break this rule, and is more complicated as a result:</p>
<ul>
  <li>One DMA channel transfers to the PIO TX FIFO</li>
  <li>Another channel programs the first channel to send from the config data buffer</li>
  <li>A third channel programs the first to send the video data</li>
  <li>The programming of the first triggers the corresponding “next reprogram me” channel</li>
</ul>

<p>The nice thing – aside from no lock-ups or video corruption – is that this now triggers a Hsync IRQ during the video line scan-out, greatly relaxing the deadline of reconfiguring the DMA.  I’d like to further improve this (with yet another DMA channel) to transfer without an IRQ per line, as the current IRQ overhead of about 1% of CPU time can be avoided.</p>

<p>(It would’ve been simpler to just hardwire the VGA display timing in the PIO code, but I like (for future projects) being able to dynamically-reconfigure the video mode.)</p>

<p>So now we have a platform and firmware framework to embed <code>umac</code> into, HID in and video out.  The hardware’s done, fuggitthat’lldo, let’s throw it over to the software team:</p>

<figure>
 <a href="https://axio.ms/images/umac/umac_annotated.jpg"> 
    <img src="https://axio.ms/assets/resized/2048/umac_annotated.jpg" srcset="            https://axio.ms/assets/resized/480/umac_annotated.jpg 480w,            https://axio.ms/assets/resized/800/umac_annotated.jpg 800w,            https://axio.ms/assets/resized/1400/umac_annotated.jpg 1400w,            https://axio.ms/assets/resized/2048/umac_annotated.jpg 2048w,    " width="100%" alt=" How it all works">
 </a> 
<figcaption>How it all works</figcaption></figure>

<h3 id="back-to-emulating-things">Back to emulating things</h3>

<p>A glance at the native <code>umac</code> binary showed a few things to fix before it could run on the Pico:</p>

<ul>
  <li><em>Musashi</em> constructed a <em>huge</em> opcode decode jumptable at runtime, in RAM.  It’s never built differently, and never changes at runtime.  I added a <em>Musashi</em> build-time generator so that this table could be <code>const</code> (and therefore live in flash).</li>
  <li>The disassembler was large, and not going to be used on the Pico, so another option to build without.</li>
  <li><em>Musashi</em> tries to accurately count execution cycles for each instruction, with more large lookup tables.  Maybe useful for console games, but the Mac doesn’t have the same degree of timing sensitivity.  <em>REMOVED</em>.</li>
</ul>

<p>(This work is in my <a href="https://github.com/evansm7/Musashi/commits/small-build/">small-build</a> branch.)</p>

<p><code>pico-mac</code> takes shape, with the ROM and disc image in flash, and enjoyably it now builds and runs on the Pico!  With some careful attention to not shoving stuff in RAM, the RAM use is looking pretty good.  The emulator plus HID code is using about 35-40KB on top of the Mac’s 128KB RAM area – there’s 95+KB of RAM still free.</p>

<p>This was a good time to finish off adding the keyboard support to <code>umac</code>.  The Mac keyboard is interfaced serially through the VIA ‘shift register’, a basic synchronous serial interface.  This was logically simple, but frustrating because early attempts at replying to the ROM’s “init” command just were persistently ignored.  The ROM disassembly was super-useful again: reading the keyboard init code, it looked like a race condition in interrupt acknowledgement if the response byte appears too soon after the request is sent.  Shoved in a delay to hold off a reply until a later poll, and then it was just a matter of mapping keycodes (boooooorrrriiiiing).</p>

<p>With a keyboard, the end-of-level MacWrite boss is reached:</p>

<p><a href="https://axio.ms/images/umac/umac_macwrite.jpg"> 
    <img src="https://axio.ms/assets/resized/2048/umac_macwrite.jpg" srcset="            https://axio.ms/assets/resized/480/umac_macwrite.jpg 480w,            https://axio.ms/assets/resized/800/umac_macwrite.jpg 800w,            https://axio.ms/assets/resized/1400/umac_macwrite.jpg 1400w,            https://axio.ms/assets/resized/2048/umac_macwrite.jpg 2048w,    " width="100%" alt=" ">
 </a></p>

<p>One problem though:  it totally sucked.  It was suuuuper slow.  I added a 1Hz dump of instruction count, and it was doing about 300 KIPS.</p>

<p>The 68000 isn’t an amazing CPU in terms of IPC.  Okay, there are some instructions that execute in 4 cycles.  But you want to use those extravagant addressing modes don’t you, and touching memory is spending those cycles all over the place.  Not an expert, but targeting about 1 MIPS for an about 8MHz 68000 seems right.  Only 3x improvement needed.</p>

<h3 id="performance">Performance</h3>

<p>I didn’t say I wasn’t gonna cheat:  let’s run that Pico at 250MHz instead of 125MHz.  Okay better, but not 2x better.  From memory, only about 30% better.  Damn, no free lunch today.</p>

<p><em>Musashi</em> has a lot of configurable options.  My first goal was to get its main loop (as seen from disassembly/post-compile end!) small:  the Mac doesn’t report Bus Errors, so the registers don’t need copies for unwinding.  The opcodes are always fetched from a 16b boundary, so don’t need alignment checking, and can use halfword loads (instead of two byte loads munged into a halfword!).  For the Cortex-M0+/<code>armv6m</code> ISA, reordering some of the CPU context structure fields enabled immediate-offset access and better code.  The CPU type, mysteriously, was dynamically-changeable and led to a bunch of runtime indirection.</p>

<p>Looking better, maybe 2x improvement, but not enough.  <em>Missile Command</em> was still janky and the mouse wasn’t smooth!</p>

<p>Next, some naughty/dangerous optimisations: remove address alignment checking, because unaligned accesses don’t happen <em>in this constrained environment</em>.</p>

<p>(Then, this work is in my <a href="https://github.com/evansm7/Musashi/commits/umac-hacks">umac-hacks</a> branch.)</p>

<p>But the real perf came from a different trick.  First, a diversion!</p>

<h4 id="rp2040-memory-access">RP2040 memory access</h4>

<p>The RP2040 has fast RAM, which is multi-banked so as to allow generally single-cycle access to multiple users (2 CPUs, DMA, etc.).  Out of the box, most code runs via XIP from external QSPI flash.  The QSPI usually runs at the core clock (125MHz default), but has a latency of ~20 cycles for a random word read.  The RP2040 uses a relatively simple 16KB cache in front of the flash to protect you from horrible access latency, but the more code you have the more likely you are to call a function and have to crank up QSPI.  When overclocking to 250MHz, the QSPI can’t go that fast so stays at 125MHz (I think).  Bear in mind, then, that your 20ish QSPI cycles on a miss become 40ish CPU cycles.</p>

<p>The particular rock-and-a-hard-place here is that <em>Musashi</em> build-time generates a ton of code, a function for each of its 1968 opcodes, plus that 256KB opcode jumptable.  Even if we make the inner execution loop completely free, the opcode dispatch might miss in the flash cache, and the opcode function itself too.  (If we want to get 1 MIPS out of about 200 MIPS, a few of these delays are going to really add up.)</p>

<p>The <code>__not_in_flash_func()</code> attribute can be used to copy a given function into RAM, guaranteeing fast execution.  At the very minimum, the main loop and memory accessors are decorated:  every instruction is going to access an opcode and most likely read or write RAM.</p>

<p>This improves performance a few percent.</p>

<p>Then, I tried decorating whole classes of opcodes: <code>move</code> is frequent, as are branches, so put ‘em in RAM.  This helped a lot, but the remaining free RAM was used up very quickly, and I wasn’t at my goal of much above 1 MIPS.</p>

<p>Remember that <a href="https://en.wikipedia.org/wiki/Reduced_instruction_set_computer">RISC architecture</a> is gonna change everything?</p>

<p>We want to put some of those 1968 68K opcodes into RAM to make them fast.  What are the top 10 most often-used instructions?  Top 100?  By adding a 64K table of counters to <code>umac</code>, booting the Mac and running key applications (okay, playing <em>Missile Command</em> for a bit), we get a profile of dynamic instruction counts.  It turns out that the 100 hottest opcodes (5% of the total) account for 89% of the execution.  And the top 200 account for a whopping 98% of execution.</p>

<p>Armed with this profile, the <code>umac</code> build post-processes the <em>Musashi</em> auto-generated code and decorates the top 200 functions with <code>__not_in_flash_func()</code>.  This adds only 17KB of extra RAM usage (leaving 95KB spare), and hits about 1.4 MIPS!  Party on!</p>

<p>At last, the world can enjoy <em>Missile Command</em>’s dark subject matter in performant comfort:</p>

<figure>
 <a href="https://axio.ms/images/umac/umac_missile.jpg"> 
    <img src="https://axio.ms/assets/resized/1400/umac_missile.jpg" srcset="            https://axio.ms/assets/resized/480/umac_missile.jpg 480w,            https://axio.ms/assets/resized/800/umac_missile.jpg 800w,            https://axio.ms/assets/resized/1400/umac_missile.jpg 1400w,    " width="100%" alt=" Missile Command on pico-mac">
 </a> 
<figcaption>Missile Command on pico-mac</figcaption></figure>

<h3 id="what-about-macpaint">What about MacPaint?</h3>

<p>Everyone loves MacPaint.  Maybe <em>you</em> love MacPaint, and have noticed I’ve deftly avoided mentioning it.  Okay, FINE:</p>

<p><img src="https://axio.ms/images/umac/macpaint_mem.png" alt="There is not enough memory for MacPaint!"></p>

<p>It doesn’t run on a <em>Mac 128Ke</em>, because the <em>Mac Plus</em> ROM uses more RAM than the original.  :sad-face:</p>

<p>I’d seen this thread on 68kMLA about a “Mac 256K”: <a href="https://68kmla.org/bb/index.php?threads/the-mythical-mac-256k.46149/">https://68kmla.org/bb/index.php?threads/the-mythical-mac-256k.46149/</a>  Chances are that the <em>Mac 128K</em> was really a <em>Mac 256K</em> in the lab (or maybe even intended to have 256K and cost-cut before release), as the OS functions fine with 256KB.</p>

<p>I wondered, does the Mac ROM/OS need a power-of-two amount of RAM?  If not, I have that 95K going spare.  Could I make a “Mac 200K”, and then run precious MacPaint?</p>

<p>Well, I tried a local hack that patches the ROM to update its global <code>memTop</code> variable based on a given memory size, and yes, System 3.2 is happy with non-power-of-2 sizes.  I booted with 256K, 208K, and 192K.  However, there were some additional problems to solve:  the ROM memtest craps itself without a power-of-2 size (totally fair), and NOPping that out leads to other issues.  These can be fixed, though also some parts of boot access off the end of RAM.  A power-of-2 size means a cheap address mask wraps RAM accesses to the valid buffer, and that can’t be done with 192K.</p>

<p>Unfortunately, when I then tested MacPaint it <em>still</em> wouldn’t run because it wanted to write a scratch file to the read-only boot volume.  This is totally breaking rule #1 by this point, so we are staying with 128KB for now.</p>

<p>However, a 256K MicroMac is extremely possible.  We just need an MCU with, say, 300KB of RAM…  Then we’d be cooking on gas.</p>

<h2 id="goodbye-friend">Goodbye, friend</h2>

<p>Well, dear reader, this has been a blast.  I hope there’s been something fun here for ya.  Ring off now, caller!</p>



<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/evansm7/umac">https://github.com/evansm7/umac</a></li>
  <li><a href="https://github.com/evansm7/pico-mac">https://github.com/evansm7/pico-mac</a></li>
  <li><a href="https://www.macintoshrepository.org/7038-all-macintosh-roms-68k-ppc-">https://www.macintoshrepository.org/7038-all-macintosh-roms-68k-ppc-</a></li>
  <li><a href="https://winworldpc.com/product/mac-os-0-6/system-3x">https://winworldpc.com/product/mac-os-0-6/system-3x</a></li>
  <li><a href="https://68kmla.org/bb/index.php?threads/macintosh-128k-mac-plus-roms.4006/">https://68kmla.org/bb/index.php?threads/macintosh-128k-mac-plus-roms.4006/</a></li>
  <li><a href="https://docs.google.com/spreadsheets/d/1wB2HnysPp63fezUzfgpk0JX_b7bXvmAg6-Dk7QDyKPY/edit#gid=840977089">https://docs.google.com/spreadsheets/d/1wB2HnysPp63fezUzfgpk0JX_b7bXvmAg6-Dk7QDyKPY/edit#gid=840977089</a></li>
</ul>

  </div>

</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bouba/kiki effect (212 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Bouba/kiki_effect</link>
            <guid>40699583</guid>
            <pubDate>Sun, 16 Jun 2024 19:50:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Bouba/kiki_effect">https://en.wikipedia.org/wiki/Bouba/kiki_effect</a>, See on <a href="https://news.ycombinator.com/item?id=40699583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Booba-Kiki.svg"><img alt="A spiky geometric shape (left) and a rounded geometric shape (right)" src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Booba-Kiki.svg/250px-Booba-Kiki.svg.png" decoding="async" width="250" height="128" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Booba-Kiki.svg/375px-Booba-Kiki.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Booba-Kiki.svg/500px-Booba-Kiki.svg.png 2x" data-file-width="500" data-file-height="255"></a><figcaption>This picture is used as a test to demonstrate that people may not attach sounds to shapes arbitrarily. When given the names "kiki" and "bouba", many cultural and linguistic communities worldwide robustly tend to label the shape on the left "kiki" and the one on the right "bouba".</figcaption></figure>
<p>The <b>bouba/kiki effect</b>, or <b>kiki/bouba effect</b>, is a non-arbitrary <a href="https://en.wikipedia.org/wiki/Association_(psychology)" title="Association (psychology)">mental association</a> between certain speech sounds and certain visual shapes. Most narrowly, it is the tendency for people, when presented with the <a href="https://en.wikipedia.org/wiki/Nonsense_word" title="Nonsense word">nonsense words</a> <i>bouba</i>  and <i>kiki</i> , to associate <i>bouba</i> with a rounded shape and <i>kiki</i> with a spiky shape. Its discovery dates back to the 1920s, when psychologists documented experimental participants as connecting nonsense words to shapes in consistent ways. There is a strong general tendency towards the effect worldwide; it has been robustly confirmed across a majority of cultures and languages in which it has been researched,<sup id="cite_ref-Cwiek_1-0"><a href="#cite_note-Cwiek-1">[1]</a></sup> for example including among English-speaking American university students, <a href="https://en.wikipedia.org/wiki/Tamil_language" title="Tamil language">Tamil</a> speakers in India, speakers of certain languages with no writing system, young children, infants, and (though to a much lesser degree) the <a href="https://en.wikipedia.org/wiki/Congenital_blindness" title="Congenital blindness">congenitally blind</a>.<sup id="cite_ref-Cwiek_1-1"><a href="#cite_note-Cwiek-1">[1]</a></sup> It has also been shown to occur with familiar names. The effect was investigated using <a href="https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging" title="Functional magnetic resonance imaging">fMRI</a> in 2018.<sup id="cite_ref-:0_2-0"><a href="#cite_note-:0-2">[2]</a></sup> The bouba/kiki effect is one form of <a href="https://en.wikipedia.org/wiki/Sound_symbolism" title="Sound symbolism">sound symbolism</a>.<sup id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup>
</p>
<meta property="mw:PageProp/toc">
<h2><span id="Research">Research</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Bouba/kiki_effect&amp;action=edit&amp;section=1" title="Edit section: Research"><span>edit</span></a><span>]</span></span></h2>
<h3><span id="Discovery">Discovery</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Bouba/kiki_effect&amp;action=edit&amp;section=2" title="Edit section: Discovery"><span>edit</span></a><span>]</span></span></h3>
<p>This effect was first observed by Georgian <a href="https://en.wikipedia.org/wiki/Psychologist" title="Psychologist">psychologist</a> <a href="https://en.wikipedia.org/wiki/Dimitri_Uznadze" title="Dimitri Uznadze">Dimitri Uznadze</a> in a 1924 paper.<sup id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup><sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources" title="Wikipedia:No original research"><span title="This claim needs references to reliable secondary sources. (May 2023)">non-primary source needed</span></a></i>]</sup> He conducted an experiment with 10 participants who were given a list with nonsense words, shown six drawings for five seconds each, then instructed to pick a name for the drawing from the list of given words. He describes the different "strategies" participants developed to match words to drawings and quotes their reasoning. He also describes situations where participants described very specific forms that they associated with a nonsense word, without reference to the shown drawings. He develops a theory of four factors that influence the way names for objects are decided.
</p><p>In total, there were 42 words. For one particular drawing, 45% picked the same word. For three others, the percentages were 40%. Uznadze points out that this is significantly more overlap than one could expect, given the high number of possible words. He speculates that there must therefore be certain regularities "which the human soul follows in the process of name-giving". 
</p><p><a href="https://en.wikipedia.org/wiki/German_Americans" title="German Americans">German American</a> <a href="https://en.wikipedia.org/wiki/Psychologist" title="Psychologist">psychologist</a> <a href="https://en.wikipedia.org/wiki/Wolfgang_K%C3%B6hler" title="Wolfgang Köhler">Wolfgang Köhler</a> referred to Uznadze's experiment in a 1929 book<sup id="cite_ref-Kohler1929_5-0"><a href="#cite_note-Kohler1929-5">[5]</a></sup> which showed two forms and asked readers which shape was called "takete" and which was called "maluma". Although he does not say so outright, Köhler implies that there is a strong preference to pair the jagged shape with "takete" and the rounded shape with "maluma".<sup id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup>
</p>
<h3><span id="Extension_to_other_contexts">Extension to other contexts</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Bouba/kiki_effect&amp;action=edit&amp;section=3" title="Edit section: Extension to other contexts"><span>edit</span></a><span>]</span></span></h3>
<p>In 2001, <a href="https://en.wikipedia.org/wiki/V._S._Ramachandran" title="V. S. Ramachandran">V. S. Ramachandran</a> and Edward Hubbard repeated Köhler's experiment using the words "kiki" and "bouba" and asked American college undergraduates and <a href="https://en.wikipedia.org/wiki/Tamil_language" title="Tamil language">Tamil</a> speakers in India, "Which of these shapes is bouba and which is kiki?" In both groups, 95% to 98% selected the curvy shape as "bouba" and the jagged one as "kiki", suggesting that the human brain somehow attaches abstract meanings to the shapes and sounds consistently.<sup id="cite_ref-Rama2001_7-0"><a href="#cite_note-Rama2001-7">[7]</a></sup><sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability"><span title="The material near this tag failed verification of its source citation(s). (August 2020)">failed verification</span></a> – <a href="https://en.wikipedia.org/wiki/Talk:Bouba/kiki_effect#Ramachandran_and_Hubbard_2001" title="Talk:Bouba/kiki effect">see discussion</a></i>]</sup>
</p><p><a href="https://en.wikipedia.org/wiki/Daphne_Maurer" title="Daphne Maurer">Daphne Maurer</a> and colleagues showed that even children as young as 2<span><span>1</span>⁄<span>2</span></span> years old may show this preference.<sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup> More recent work by Ozge Ozturk and colleagues in 2013 showed that even 4-month-old infants have the same sound–shape mapping biases as adults and toddlers.<sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> Infants are able to differentiate between congruent trials (pairing an angular shape with "kiki" or a curvy shape with "bubu") and incongruent trials (pairing a curvy shape with "kiki" or an angular shape with "bubu"). Infants looked longer at incongruent pairings than at congruent pairings. Infants' mapping was based on the combination of <a href="https://en.wikipedia.org/wiki/Consonant" title="Consonant">consonants</a> and <a href="https://en.wikipedia.org/wiki/Vowel" title="Vowel">vowels</a> in the words, and neither consonants nor vowels alone sufficed for mapping. These results suggest that some sound–shape mappings precede <a href="https://en.wikipedia.org/wiki/Language_acquisition" title="Language acquisition">language learning</a>, and may in fact aid in language learning by establishing a basis for matching labels to referents and narrowing the hypothesis space for young infants. Adults in this study, like infants, used a combination of consonant and vowel information to match the labels they heard with the shapes they saw. However, this was not the only strategy that was available to them. Adults, unlike infants, were also able to use consonant information alone and vowel information alone to match the labels to the shapes, albeit less frequently than the consonant–vowel combination. When vowels and consonants were put in conflict, adults used consonants more often than vowels.
</p><p>The effect has also been shown to emerge in other contexts, such as when words are paired with evaluative meanings (with "bouba" words associated with positive concepts and "kiki" words associated with negative concepts)<sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup> or when the words to be paired are existing first names, suggesting that some familiarity with the linguistic stimuli does not eliminate the effect. A study showed that individuals will pair names such as "Molly" with round silhouettes, and names such as "Kate" with sharp silhouettes. Moreover, individuals will associate different personality traits with either group of names (e.g., easygoingness with "round names"; determination with "sharp names"). This may hint at a role of abstract concepts in the effect.<sup id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup>
</p>
<h3><span id="Contexts_where_the_effect_is_smaller_or_absent">Contexts where the effect is smaller or absent</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Bouba/kiki_effect&amp;action=edit&amp;section=4" title="Edit section: Contexts where the effect is smaller or absent"><span>edit</span></a><span>]</span></span></h3>
<p>Other research suggests that this effect does not occur in all communities,<sup id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> and it appears that the effect breaks if the sounds do not make licit words in the language.<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup> The bouba/kiki effect seems to be dependent on a long <a href="https://en.wikipedia.org/wiki/Critical_period" title="Critical period">sensitive period</a>, with high visual capacities in childhood being necessary for its typical development. Although the congenitally blind have been reported to show a bouba/kiki effect, they show a much smaller one for touched shapes than sighted individuals do for visual shapes.<sup id="cite_ref-Fryer2014_14-0"><a href="#cite_note-Fryer2014-14">[14]</a></sup><sup id="cite_ref-Hamilton-Fletcher2018_15-0"><a href="#cite_note-Hamilton-Fletcher2018-15">[15]</a></sup>
</p>
<h3><span id="Languages_where_the_effect_is_smaller_or_absent">Languages where the effect is smaller or absent</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Bouba/kiki_effect&amp;action=edit&amp;section=5" title="Edit section: Languages where the effect is smaller or absent"><span>edit</span></a><span>]</span></span></h3>
<p>Studies show that speakers of certain languages have notably failed to show the effect, namely including <a href="https://en.wikipedia.org/wiki/Mandarin_Chinese" title="Mandarin Chinese">Mandarin Chinese</a>, <a href="https://en.wikipedia.org/wiki/Turkish_language" title="Turkish language">Turkish</a>, and <a href="https://en.wikipedia.org/wiki/Romanian_language" title="Romanian language">Romanian</a>.<sup id="cite_ref-Cwiek_1-2"><a href="#cite_note-Cwiek-1">[1]</a></sup>
</p>
<h3><span id="Neuroscience">Neuroscience</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Bouba/kiki_effect&amp;action=edit&amp;section=6" title="Edit section: Neuroscience"><span>edit</span></a><span>]</span></span></h3>
<p>In 2019, Nathan Peiffer-Smadja and Laurent Cohen published the first study using <a href="https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging" title="Functional magnetic resonance imaging">fMRI</a> to explore the bouba/kiki effect.<sup id="cite_ref-:0_2-1"><a href="#cite_note-:0-2">[2]</a></sup> They found that prefrontal activation is stronger to mismatching (bouba with spiky shape) than to matching (bouba with round shape) stimuli. A subsequent study by Kelly McCormick and colleagues reported a similar pattern of greater activation for mismatched word-shape stimuli, but with most activity in <a href="https://en.wikipedia.org/wiki/Parietal_region" title="Parietal region">parietal regions</a> including the <a href="https://en.wikipedia.org/wiki/Intraparietal_sulcus" title="Intraparietal sulcus">intraparietal sulcus</a> and <a href="https://en.wikipedia.org/wiki/Supramarginal_gyrus" title="Supramarginal gyrus">supramarginal gyrus</a>, regions known to play a role in sensory association and perceptual-motor processing.<sup id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup> Peiffer-Smadja and Cohen also found that sound-shape matching also influences activations in the auditory and visual cortices, suggesting an effect of matching at an early stage in <a href="https://en.wikipedia.org/wiki/Sensory_processing" title="Sensory processing">sensory processing</a>.<sup id="cite_ref-:0_2-2"><a href="#cite_note-:0-2">[2]</a></sup>
</p>
<h3><span id="Implications_for_understanding_language">Implications for understanding language</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Bouba/kiki_effect&amp;action=edit&amp;section=7" title="Edit section: Implications for understanding language"><span>edit</span></a><span>]</span></span></h3>
<p>Ramachandran and Hubbard suggest that the kiki/bouba effect has implications for the evolution of language, because it suggests that the naming of objects is not completely arbitrary.<sup id="cite_ref-Rama2001_7-1"><a href="#cite_note-Rama2001-7">[7]</a></sup><sup><span title="Page / location: 17">: 17 </span></sup> The rounded shape may most commonly be named "bouba" because the mouth makes a more rounded shape to produce that sound while a more taut, angular mouth shape is needed to make the sounds in "kiki".<sup id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup> Alternatively, the distinction may be between <a href="https://en.wikipedia.org/wiki/Coronal_consonant" title="Coronal consonant">coronal</a> or <a href="https://en.wikipedia.org/wiki/Dorsal_consonant" title="Dorsal consonant">dorsal</a> consonants like <a href="https://en.wikipedia.org/wiki/Voiceless_velar_stop" title="Voiceless velar stop">/k/</a> and <a href="https://en.wikipedia.org/wiki/Labial_consonant" title="Labial consonant">labial</a> consonants like <a href="https://en.wikipedia.org/wiki/Voiced_bilabial_stop" title="Voiced bilabial stop">/b/</a>.<sup id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> Additionally, it was shown that it is not only different consonants (e.g., voiceless versus voiced) and different vowel qualities (e.g., /a/ versus /i/) that play a role in the effect, but also vowel quantity (long versus short vowels). In one study, participants rated words containing long vowels to refer to longer objects and short vowels to short objects, at least for languages that make a <a href="https://en.wikipedia.org/wiki/Vowel_length" title="Vowel length">vowel length</a> distinction.<sup id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup>  The presence of these "<a href="https://en.wikipedia.org/wiki/Synesthesia" title="Synesthesia">synesthesia</a>-like mappings" suggest that this effect may be the neurological basis for <a href="https://en.wikipedia.org/wiki/Sound_symbolism" title="Sound symbolism">sound symbolism</a>, in which sounds are non-arbitrarily mapped to objects and events in the world.<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2016)">citation needed</span></a></i>]</sup> Research has also indicated that the effect may be a case of <a href="https://en.wikipedia.org/wiki/Ideasthesia" title="Ideasthesia">ideasthesia</a>,<sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> a phenomenon in which activations of concepts (inducers) evoke perception-like experiences (concurrents). The name comes from the Greek <i>idea</i> and <i>aisthesis</i>, meaning "sensing concepts" or "sensing ideas", and was introduced by Danko Nikolić.<sup id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup>
</p>
<h2><span id="See_also">See also</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Bouba/kiki_effect&amp;action=edit&amp;section=8" title="Edit section: See also"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Color_symbolism" title="Color symbolism">Color symbolism</a></li>
<li><a href="https://en.wikipedia.org/wiki/Japanese_sound_symbolism" title="Japanese sound symbolism">Japanese sound symbolism</a></li>
<li><a href="https://en.wikipedia.org/wiki/Origin_of_language" title="Origin of language">Origin of language</a></li>
<li><a href="https://en.wikipedia.org/wiki/Semiotics" title="Semiotics">Semiotics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Universal_language" title="Universal language">Universal language</a></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Bouba/kiki_effect&amp;action=edit&amp;section=9" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div><ol>
<li id="cite_note-Cwiek-1"><span>^ <a href="#cite_ref-Cwiek_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Cwiek_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Cwiek_1-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFĆwiekFuchsDraxlerAsu2022">Ćwiek, Aleksandra; Fuchs, Susanne; Draxler, Christoph; Asu, Eva Liina; Dediu, Dan; Hiovain, Katri; Kawahara, Shigeto; Koutalidis, Sofia; Krifka, Manfred; Lippus, Pärtel; Lupyan, Gary; Oh, Grace E.; Paul, Jing; Petrone, Caterina; Ridouane, Rachid; Reiter, Sabine; Schümchen, Nathalie; Szalontai, Ádám; Ünal-Logacev, Özlem; Zeller, Jochen; Perlman, Marcus; Winter, Bodo (2022). <a rel="nofollow" href="https://doi.org/10.1098/rstb.2020.0390">"The <i>bouba/Kiki</i> effect is robust across cultures and writing systems"</a>. <i>Philosophical Transactions of the Royal Society B: Biological Sciences</i>. <b>377</b> (1841). <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1098%2Frstb.2020.0390">10.1098/rstb.2020.0390</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8591387">8591387</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/34775818">34775818</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophical+Transactions+of+the+Royal+Society+B%3A+Biological+Sciences&amp;rft.atitle=The+bouba%2FKiki+effect+is+robust+across+cultures+and+writing+systems&amp;rft.volume=377&amp;rft.issue=1841&amp;rft.date=2022&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC8591387%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F34775818&amp;rft_id=info%3Adoi%2F10.1098%2Frstb.2020.0390&amp;rft.aulast=%C4%86wiek&amp;rft.aufirst=Aleksandra&amp;rft.au=Fuchs%2C+Susanne&amp;rft.au=Draxler%2C+Christoph&amp;rft.au=Asu%2C+Eva+Liina&amp;rft.au=Dediu%2C+Dan&amp;rft.au=Hiovain%2C+Katri&amp;rft.au=Kawahara%2C+Shigeto&amp;rft.au=Koutalidis%2C+Sofia&amp;rft.au=Krifka%2C+Manfred&amp;rft.au=Lippus%2C+P%C3%A4rtel&amp;rft.au=Lupyan%2C+Gary&amp;rft.au=Oh%2C+Grace+E.&amp;rft.au=Paul%2C+Jing&amp;rft.au=Petrone%2C+Caterina&amp;rft.au=Ridouane%2C+Rachid&amp;rft.au=Reiter%2C+Sabine&amp;rft.au=Sch%C3%BCmchen%2C+Nathalie&amp;rft.au=Szalontai%2C+%C3%81d%C3%A1m&amp;rft.au=%C3%9Cnal-Logacev%2C+%C3%96zlem&amp;rft.au=Zeller%2C+Jochen&amp;rft.au=Perlman%2C+Marcus&amp;rft.au=Winter%2C+Bodo&amp;rft_id=http%3A%2F%2Fdoi.org%2F10.1098%2Frstb.2020.0390&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-:0-2"><span>^ <a href="#cite_ref-:0_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_2-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:0_2-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFPeiffer-SmadjaCohen2019">Peiffer-Smadja, Nathan; Cohen, Laurent (2019-02-01). <a rel="nofollow" href="https://doi.org/10.1016%2Fj.neuroimage.2018.11.033">"The cerebral bases of the bouba-kiki effect"</a>. <i>NeuroImage</i>. <b>186</b>: 679–689. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1016%2Fj.neuroimage.2018.11.033">10.1016/j.neuroimage.2018.11.033</a></span>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1053-8119">1053-8119</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/30503933">30503933</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:54164828">54164828</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeuroImage&amp;rft.atitle=The+cerebral+bases+of+the+bouba-kiki+effect&amp;rft.volume=186&amp;rft.pages=679-689&amp;rft.date=2019-02-01&amp;rft.issn=1053-8119&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A54164828%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F30503933&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neuroimage.2018.11.033&amp;rft.aulast=Peiffer-Smadja&amp;rft.aufirst=Nathan&amp;rft.au=Cohen%2C+Laurent&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1016%252Fj.neuroimage.2018.11.033&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span><cite id="CITEREFMargiotoudi_Konstantina_and_Pulvermüller_Friedemann2020">Margiotoudi Konstantina and Pulvermüller Friedemann (2020). <a rel="nofollow" href="https://www.proquest.com/docview/2428279185">"Action sound–shape congruencies explain sound symbolism"</a>. <i>Scientific Reports</i>. <b>10</b> (1): 12706. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/2020NatSR..1012706M">2020NatSR..1012706M</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1038%2Fs41598-020-69528-4">10.1038/s41598-020-69528-4</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7392762">7392762</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/32728096">32728096</a>. <a href="https://en.wikipedia.org/wiki/ProQuest_(identifier)" title="ProQuest (identifier)">ProQuest</a>&nbsp;<a rel="nofollow" href="https://search.proquest.com/docview/2428279185">2428279185</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Scientific+Reports&amp;rft.atitle=Action+sound%E2%80%93shape+congruencies+explain+sound+symbolism&amp;rft.volume=10&amp;rft.issue=1&amp;rft.pages=12706&amp;rft.date=2020&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC7392762%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F32728096&amp;rft_id=info%3Adoi%2F10.1038%2Fs41598-020-69528-4&amp;rft_id=info%3Abibcode%2F2020NatSR..1012706M&amp;rft.au=Margiotoudi+Konstantina+and+Pulverm%C3%BCller+Friedemann&amp;rft_id=https%3A%2F%2Fwww.proquest.com%2Fdocview%2F2428279185&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><cite id="CITEREFDimitri_Usnadze">Dimitri Usnadze. <a rel="nofollow" href="https://core.ac.uk/download/pdf/210861794.pdf">"Ein experimenteller Beitrag zum Problem der psychologischen Grundlagen der Namengebung"</a> <span>(PDF)</span>. <i>bard.edu</i> (in German)<span>. Retrieved <span>18 April</span> 2023</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=bard.edu&amp;rft.atitle=Ein+experimenteller+Beitrag+zum+Problem+der+psychologischen+Grundlagen+der+Namengebung&amp;rft.au=Dimitri+Usnadze&amp;rft_id=https%3A%2F%2Fcore.ac.uk%2Fdownload%2Fpdf%2F210861794.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-Kohler1929-5"><span><b><a href="#cite_ref-Kohler1929_5-0">^</a></b></span> <span><cite id="CITEREFKöhler1929">Köhler, Wolfgang (1929). <span title="Free registration required"><a rel="nofollow" href="https://archive.org/details/gestaltpsycholog0000kohl"><i>Gestalt Psychology</i></a></span>. New York: Liveright.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Gestalt+Psychology.&amp;rft.place=New+York&amp;rft.pub=Liveright&amp;rft.date=1929&amp;rft.aulast=K%C3%B6hler&amp;rft.aufirst=Wolfgang&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fgestaltpsycholog0000kohl&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-6"><span><b><a href="#cite_ref-6">^</a></b></span> <span><cite id="CITEREFKöhler1947">Köhler, Wolfgang (1947). <span title="Free registration required"><a rel="nofollow" href="https://archive.org/details/gestaltpsycholog00kh"><i>Gestalt Psychology</i></a></span> (2nd&nbsp;ed.). New York: Liveright. p.&nbsp;<a rel="nofollow" href="https://archive.org/details/gestaltpsycholog00kh/page/132">133</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Gestalt+Psychology&amp;rft.place=New+York&amp;rft.pages=133&amp;rft.edition=2nd&amp;rft.pub=Liveright&amp;rft.date=1947&amp;rft.aulast=K%C3%B6hler&amp;rft.aufirst=Wolfgang&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fgestaltpsycholog00kh&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-Rama2001-7"><span>^ <a href="#cite_ref-Rama2001_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Rama2001_7-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFRamachandranHubbard2001">Ramachandran, V.S. &amp; Hubbard, E.M. (2001). <a rel="nofollow" href="https://web.archive.org/web/20110813064348/http://cbc.ucsd.edu/pdf/Synaesthesia%20-%20JCS.pdf">"Synaesthesia: A window into perception, thought and language"</a> <span>(PDF)</span>. <i>Journal of Consciousness Studies</i>. <b>8</b> (12): 3–34. Archived from <a rel="nofollow" href="http://cbc.ucsd.edu/pdf/Synaesthesia%20-%20JCS.pdf">the original</a> <span>(PDF)</span> on 2011-08-13<span>. Retrieved <span>2011-10-20</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Consciousness+Studies&amp;rft.atitle=Synaesthesia%3A+A+window+into+perception%2C+thought+and+language&amp;rft.volume=8&amp;rft.issue=12&amp;rft.pages=3-34&amp;rft.date=2001&amp;rft.aulast=Ramachandran&amp;rft.aufirst=V.S.&amp;rft.au=Hubbard%2C+E.M.&amp;rft_id=http%3A%2F%2Fcbc.ucsd.edu%2Fpdf%2FSynaesthesia%2520-%2520JCS.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite id="CITEREFMaurerPathmanMondloch2006">Maurer, Daphne; Pathman, Thanujeni &amp; Mondloch, Catherine J. (2006). <a rel="nofollow" href="https://web.archive.org/web/20110723035944/http://psych.mcmaster.ca/maurerlab/Publications/Maurer_bouba.pdf">"The shape of boubas: Sound-shape correspondences in toddlers and adults"</a> <span>(PDF)</span>. <i>Developmental Science</i>. <b>9</b> (3): 316–322. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1111%2Fj.1467-7687.2006.00495.x">10.1111/j.1467-7687.2006.00495.x</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/16669803">16669803</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:7297731">7297731</a>. Archived from <a rel="nofollow" href="http://psych.mcmaster.ca/maurerlab/Publications/Maurer_bouba.pdf">the original</a> <span>(PDF)</span> on 2011-07-23<span>. Retrieved <span>2011-06-19</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Developmental+Science&amp;rft.atitle=The+shape+of+boubas%3A+Sound-shape+correspondences+in+toddlers+and+adults&amp;rft.volume=9&amp;rft.issue=3&amp;rft.pages=316-322&amp;rft.date=2006&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A7297731%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F16669803&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1467-7687.2006.00495.x&amp;rft.aulast=Maurer&amp;rft.aufirst=Daphne&amp;rft.au=Pathman%2C+Thanujeni&amp;rft.au=Mondloch%2C+Catherine+J.&amp;rft_id=http%3A%2F%2Fpsych.mcmaster.ca%2Fmaurerlab%2FPublications%2FMaurer_bouba.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><cite id="CITEREFOzturkKrehmVouloumanos2013">Ozturk, Ozge; Krehm, Madelaine; Vouloumanos, Athena (2013). <a rel="nofollow" href="https://web.archive.org/web/20200817164233/http://www.psych.nyu.edu/niccl/publicationlinks/OzturkKrehmVouloumanos_JECP_InPress.pdf">"Sound symbolism in infancy: Evidence for sound–shape cross-modal correspondences in 4-month-olds"</a> <span>(PDF)</span>. <i>Journal of Experimental Child Psychology</i>. <b>114</b> (2): 173–186. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1016%2Fj.jecp.2012.05.004">10.1016/j.jecp.2012.05.004</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/22960203">22960203</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:7274252">7274252</a>. Archived from <a rel="nofollow" href="http://www.psych.nyu.edu/niccl/publicationlinks/OzturkKrehmVouloumanos_JECP_InPress.pdf">the original</a> <span>(PDF)</span> on 2020-08-17<span>. Retrieved <span>2019-09-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Experimental+Child+Psychology&amp;rft.atitle=Sound+symbolism+in+infancy%3A+Evidence+for+sound%E2%80%93shape+cross-modal+correspondences+in+4-month-olds&amp;rft.volume=114&amp;rft.issue=2&amp;rft.pages=173-186&amp;rft.date=2013&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A7274252%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F22960203&amp;rft_id=info%3Adoi%2F10.1016%2Fj.jecp.2012.05.004&amp;rft.aulast=Ozturk&amp;rft.aufirst=Ozge&amp;rft.au=Krehm%2C+Madelaine&amp;rft.au=Vouloumanos%2C+Athena&amp;rft_id=http%3A%2F%2Fwww.psych.nyu.edu%2Fniccl%2Fpublicationlinks%2FOzturkKrehmVouloumanos_JECP_InPress.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><cite id="CITEREFBross2018">Bross, Fabian (2018). "The Good, the Bad, the Bouba, and the Kiki. Cross-Modal Correspondences Between Evaluative Meanings, Speech-Sounds, and Object Shapes". <i>14th conference "Phonetics &amp; Phonology in the German-Speaking World"</i>. University of Vienna. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.13140%2FRG.2.2.11463.14240">10.13140/RG.2.2.11463.14240</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=The+Good%2C+the+Bad%2C+the+Bouba%2C+and+the+Kiki.+Cross-Modal+Correspondences+Between+Evaluative+Meanings%2C+Speech-Sounds%2C+and+Object+Shapes&amp;rft.btitle=14th+conference+%22Phonetics+%26+Phonology+in+the+German-Speaking+World%22&amp;rft.place=University+of+Vienna&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.13140%2FRG.2.2.11463.14240&amp;rft.aulast=Bross&amp;rft.aufirst=Fabian&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-11"><span><b><a href="#cite_ref-11">^</a></b></span> <span><cite id="CITEREFSidhuPexman2015">Sidhu, David M.; Pexman, Penny M. (2015-05-27). <a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4446333">"What's in a Name? Sound Symbolism and Gender in First Names"</a>. <i>PLOS ONE</i>. <b>10</b> (5): e0126809. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/2015PLoSO..1026809S">2015PLoSO..1026809S</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1371%2Fjournal.pone.0126809">10.1371/journal.pone.0126809</a></span>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1932-6203">1932-6203</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4446333">4446333</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/26016856">26016856</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLOS+ONE&amp;rft.atitle=What%27s+in+a+Name%3F+Sound+Symbolism+and+Gender+in+First+Names&amp;rft.volume=10&amp;rft.issue=5&amp;rft.pages=e0126809&amp;rft.date=2015-05-27&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4446333%23id-name%3DPMC&amp;rft_id=info%3Abibcode%2F2015PLoSO..1026809S&amp;rft_id=info%3Apmid%2F26016856&amp;rft_id=info%3Adoi%2F10.1371%2Fjournal.pone.0126809&amp;rft.issn=1932-6203&amp;rft.aulast=Sidhu&amp;rft.aufirst=David+M.&amp;rft.au=Pexman%2C+Penny+M.&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4446333&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-12"><span><b><a href="#cite_ref-12">^</a></b></span> <span><cite id="CITEREFRogersRoss1975">Rogers, Susan K.; Ross, Abraham S. (1975). "A cross-cultural test of the maluma–takete phenomenon". <i>Perception</i>. <b>4</b> (1): 105–106. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1068%2Fp040105">10.1068/p040105</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/1161435">1161435</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:30045028">30045028</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Perception&amp;rft.atitle=A+cross-cultural+test+of+the+maluma%E2%80%93takete+phenomenon&amp;rft.volume=4&amp;rft.issue=1&amp;rft.pages=105-106&amp;rft.date=1975&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A30045028%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F1161435&amp;rft_id=info%3Adoi%2F10.1068%2Fp040105&amp;rft.aulast=Rogers&amp;rft.aufirst=Susan+K.&amp;rft.au=Ross%2C+Abraham+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span><cite id="CITEREFStylesGawne2017">Styles, Suzy; Gawne, Lauren (2017). <a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5574486">"When Does Maluma/Takete Fail? Two Key Failures and a Meta-Analysis Suggest That Phonology and Phonotactics Matter"</a>. <i>i-Perception</i>. <b>8</b> (4): 204166951772480. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1177%2F2041669517724807">10.1177/2041669517724807</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5574486">5574486</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/28890777">28890777</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=i-Perception&amp;rft.atitle=When+Does+Maluma%2FTakete+Fail%3F+Two+Key+Failures+and+a+Meta-Analysis+Suggest+That+Phonology+and+Phonotactics+Matter&amp;rft.volume=8&amp;rft.issue=4&amp;rft.pages=204166951772480&amp;rft.date=2017&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5574486%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F28890777&amp;rft_id=info%3Adoi%2F10.1177%2F2041669517724807&amp;rft.aulast=Styles&amp;rft.aufirst=Suzy&amp;rft.au=Gawne%2C+Lauren&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5574486&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-Fryer2014-14"><span><b><a href="#cite_ref-Fryer2014_14-0">^</a></b></span> <span><cite id="CITEREFFryerFreemanPring2014">Fryer, Louise; Freeman, Jonathan &amp; Pring, Linda (2014). <a rel="nofollow" href="http://research.gold.ac.uk/10499/1/PSY-Fryer-Freeman-Pring-2014.pdf">"Touching words is not enough: How visual experience influences haptic–auditory associations in the "Bouba–Kiki" effect"</a> <span>(PDF)</span>. <i>Cognition</i>. <b>132</b> (2): 164–173. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1016%2Fj.cognition.2014.03.015">10.1016/j.cognition.2014.03.015</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/24809744">24809744</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:29605784">29605784</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognition&amp;rft.atitle=Touching+words+is+not+enough%3A+How+visual+experience+influences+haptic%E2%80%93auditory+associations+in+the+%22Bouba%E2%80%93Kiki%22+effect.&amp;rft.volume=132&amp;rft.issue=2&amp;rft.pages=164-173&amp;rft.date=2014&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A29605784%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F24809744&amp;rft_id=info%3Adoi%2F10.1016%2Fj.cognition.2014.03.015&amp;rft.aulast=Fryer&amp;rft.aufirst=Louise&amp;rft.au=Freeman%2C+Jonathan&amp;rft.au=Pring%2C+Linda&amp;rft_id=http%3A%2F%2Fresearch.gold.ac.uk%2F10499%2F1%2FPSY-Fryer-Freeman-Pring-2014.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-Hamilton-Fletcher2018-15"><span><b><a href="#cite_ref-Hamilton-Fletcher2018_15-0">^</a></b></span> <span><cite id="CITEREFHamilton-FletcherPisanskiRebyStefańczyk2018">Hamilton-Fletcher, Giles; Pisanski, Katarzyna; Reby, David; Stefańczyk, Michał; Ward, Jamie &amp; Sorokowska, Agnieszka (2018). <a rel="nofollow" href="http://sro.sussex.ac.uk/id/eprint/74197/1/__smbhome.uscs.susx.ac.uk_ellenaj_Desktop_SRO_after%20august_blnd%20correspondences%20cognition%20accepted%20version.pdf">"The role of visual experience in the emergence of cross-modal correspondences"</a> <span>(PDF)</span>. <i>Cognition</i>. <b>175</b>: 114–121. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1016%2Fj.cognition.2018.02.023">10.1016/j.cognition.2018.02.023</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/29502009">29502009</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:3688492">3688492</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognition&amp;rft.atitle=The+role+of+visual+experience+in+the+emergence+of+cross-modal+correspondences.&amp;rft.volume=175&amp;rft.pages=114-121&amp;rft.date=2018&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A3688492%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F29502009&amp;rft_id=info%3Adoi%2F10.1016%2Fj.cognition.2018.02.023&amp;rft.aulast=Hamilton-Fletcher&amp;rft.aufirst=Giles&amp;rft.au=Pisanski%2C+Katarzyna&amp;rft.au=Reby%2C+David&amp;rft.au=Stefa%C5%84czyk%2C+Micha%C5%82&amp;rft.au=Ward%2C+Jamie&amp;rft.au=Sorokowska%2C+Agnieszka&amp;rft_id=http%3A%2F%2Fsro.sussex.ac.uk%2Fid%2Feprint%2F74197%2F1%2F__smbhome.uscs.susx.ac.uk_ellenaj_Desktop_SRO_after%2520august_blnd%2520correspondences%2520cognition%2520accepted%2520version.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-16"><span><b><a href="#cite_ref-16">^</a></b></span> <span><cite id="CITEREFMcCormickLaceyStillaNygaard2021">McCormick, Kelly; Lacey, Simon; Stilla, Randall; Nygaard, Lynne C.; Sathian, K. (2021-08-11). <a rel="nofollow" href="https://brill.com/view/journals/msr/aop/article-10.1163-22134808-bja10060/article-10.1163-22134808-bja10060.xml">"Neural Basis of the Sound-Symbolic Crossmodal Correspondence Between Auditory Pseudowords and Visual Shapes"</a>. <i>Multisensory Research</i>. <b>-1</b> (aop): 29–78. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1163%2F22134808-bja10060">10.1163/22134808-bja10060</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/2213-4794">2213-4794</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9196751">9196751</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/34384048">34384048</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:236998825">236998825</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Multisensory+Research&amp;rft.atitle=Neural+Basis+of+the+Sound-Symbolic+Crossmodal+Correspondence+Between+Auditory+Pseudowords+and+Visual+Shapes&amp;rft.volume=-1&amp;rft.issue=aop&amp;rft.pages=29-78&amp;rft.date=2021-08-11&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9196751%23id-name%3DPMC&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A236998825%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1163%2F22134808-bja10060&amp;rft.issn=2213-4794&amp;rft_id=info%3Apmid%2F34384048&amp;rft.aulast=McCormick&amp;rft.aufirst=Kelly&amp;rft.au=Lacey%2C+Simon&amp;rft.au=Stilla%2C+Randall&amp;rft.au=Nygaard%2C+Lynne+C.&amp;rft.au=Sathian%2C+K.&amp;rft_id=https%3A%2F%2Fbrill.com%2Fview%2Fjournals%2Fmsr%2Faop%2Farticle-10.1163-22134808-bja10060%2Farticle-10.1163-22134808-bja10060.xml&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-17"><span><b><a href="#cite_ref-17">^</a></b></span> <span><cite id="CITEREFD'Onofrio2013">D'Onofrio, Annette (2013). "Phonetic Detail and Dimensionality in Sound-shape Correspondences: Refining the <i>Bouba-Kiki</i> Paradigm". <i>Language and Speech</i>. <b>57</b> (3): 367–393. <a href="https://en.wikipedia.org/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.1020.1352">10.1.1.1020.1352</a></span>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1177%2F0023830913507694">10.1177/0023830913507694</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:51937587">51937587</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Language+and+Speech&amp;rft.atitle=Phonetic+Detail+and+Dimensionality+in+Sound-shape+Correspondences%3A+Refining+the+Bouba-Kiki+Paradigm&amp;rft.volume=57&amp;rft.issue=3&amp;rft.pages=367-393&amp;rft.date=2013&amp;rft_id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.1020.1352%23id-name%3DCiteSeerX&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A51937587%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1177%2F0023830913507694&amp;rft.aulast=D%27Onofrio&amp;rft.aufirst=Annette&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-18"><span><b><a href="#cite_ref-18">^</a></b></span> <span><cite id="CITEREFMcCormickKimListNygaard2015">McCormick, Kelly; Kim, Jee Young; List, Sara; Nygaard, Lynne C. (2015). <a rel="nofollow" href="https://web.archive.org/web/20190209124406/https://mindmodeling.org/cogsci2015/papers/0273/paper0273.pdf">"Sound to Meaning Mappings in the Bouba-Kiki Effect"</a> <span>(PDF)</span>. <i>Proceedings of the 37th Annual Conference of the Cognitive Science Society: Mind, Technology, and Society: Pasadena, California, 23–25 July 2015</i>. Austin, TX: Cognitive Science Society. pp.&nbsp;1565–1570. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-9911967-2-2" title="Special:BookSources/978-0-9911967-2-2"><bdi>978-0-9911967-2-2</bdi></a>. Archived from <a rel="nofollow" href="https://mindmodeling.org/cogsci2015/papers/0273/paper0273.pdf">the original</a> <span>(PDF)</span> on 2019-02-09<span>. Retrieved <span>2019-02-08</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Sound+to+Meaning+Mappings+in+the+Bouba-Kiki+Effect&amp;rft.btitle=Proceedings+of+the+37th+Annual+Conference+of+the+Cognitive+Science+Society%3A+Mind%2C+Technology%2C+and+Society%3A+Pasadena%2C+California%2C+23%E2%80%9325+July+2015&amp;rft.place=Austin%2C+TX&amp;rft.pages=1565-1570&amp;rft.pub=Cognitive+Science+Society&amp;rft.date=2015&amp;rft.isbn=978-0-9911967-2-2&amp;rft.aulast=McCormick&amp;rft.aufirst=Kelly&amp;rft.au=Kim%2C+Jee+Young&amp;rft.au=List%2C+Sara&amp;rft.au=Nygaard%2C+Lynne+C.&amp;rft_id=https%3A%2F%2Fmindmodeling.org%2Fcogsci2015%2Fpapers%2F0273%2Fpaper0273.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-19"><span><b><a href="#cite_ref-19">^</a></b></span> <span><cite id="CITEREFBross2018">Bross, Fabian (2018). <a rel="nofollow" href="https://www.researchgate.net/publication/323749384">"Cognitive associations between vowel length and object size: A new feature contributing to a bouba/kiki effect"</a>. In Belz, M.; Mooshammer, C.; Fuchs, S.; Jannedy, S.; Rasskazova, O.; Zygis, M. (eds.). <i>Proceedings of the Conference on Phonetics &amp; Phonology in German-Speaking Countries</i>. Vol.&nbsp;13. Berlin: Humbold University. pp.&nbsp;17–20.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Cognitive+associations+between+vowel+length+and+object+size%3A+A+new+feature+contributing+to+a+bouba%2Fkiki+effect&amp;rft.btitle=Proceedings+of+the+Conference+on+Phonetics+%26+Phonology+in+German-Speaking+Countries&amp;rft.place=Berlin&amp;rft.pages=17-20&amp;rft.pub=Humbold+University&amp;rft.date=2018&amp;rft.aulast=Bross&amp;rft.aufirst=Fabian&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F323749384&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span><cite id="CITEREFGómez_MilánIborrade_CórdobaJuárez-Ramos2013">Gómez Milán, E.; Iborra, O.; de Córdoba, M.J.; Juárez-Ramos, V.; Rodríguez Artacho, M.A.; Rubio, J.L. (2013). <span title="Paid subscription required"><a rel="nofollow" href="https://www.ingentaconnect.com/content/imp/jcs/2013/00000020/F0020001/art00005">"The Kiki-Bouba effect: A case of personification and ideaesthesia"</a></span>. <i>Journal of Consciousness Studies</i>. <b>20</b> (1–2): 84–102.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Consciousness+Studies&amp;rft.atitle=The+Kiki-Bouba+effect%3A+A+case+of+personification+and+ideaesthesia&amp;rft.volume=20&amp;rft.issue=1%E2%80%932&amp;rft.pages=84-102&amp;rft.date=2013&amp;rft.aulast=G%C3%B3mez+Mil%C3%A1n&amp;rft.aufirst=E.&amp;rft.au=Iborra%2C+O.&amp;rft.au=de+C%C3%B3rdoba%2C+M.J.&amp;rft.au=Ju%C3%A1rez-Ramos%2C+V.&amp;rft.au=Rodr%C3%ADguez+Artacho%2C+M.A.&amp;rft.au=Rubio%2C+J.L.&amp;rft_id=https%3A%2F%2Fwww.ingentaconnect.com%2Fcontent%2Fimp%2Fjcs%2F2013%2F00000020%2FF0020001%2Fart00005&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
<li id="cite_note-21"><span><b><a href="#cite_ref-21">^</a></b></span> <span><cite id="CITEREFNikolić2009">Nikolić, Danko (2009). <a rel="nofollow" href="http://www.danko-nikolic.com/wp-content/uploads/2011/09/Synesthesia2009-Nikolic-Ideaesthesia.pdf">"Is synaesthesia actually ideaestesia? An inquiry into the nature of the phenomenon"</a> <span>(PDF)</span>. <i>Proceedings of the Third International Congress on Synaesthesia, Science &amp; Art</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Is+synaesthesia+actually+ideaestesia%3F+An+inquiry+into+the+nature+of+the+phenomenon&amp;rft.btitle=Proceedings+of+the+Third+International+Congress+on+Synaesthesia%2C+Science+%26+Art&amp;rft.date=2009&amp;rft.aulast=Nikoli%C4%87&amp;rft.aufirst=Danko&amp;rft_id=http%3A%2F%2Fwww.danko-nikolic.com%2Fwp-content%2Fuploads%2F2011%2F09%2FSynesthesia2009-Nikolic-Ideaesthesia.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABouba%2Fkiki+effect"></span></span>
</li>
</ol></div>
<!-- 
NewPP limit report
Parsed by mw‐web.codfw.main‐776d5fcf9b‐dns87
Cached time: 20240520162536
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.366 seconds
Real time usage: 0.485 seconds
Preprocessor visited node count: 2296/1000000
Post‐expand include size: 69597/2097152 bytes
Template argument size: 2604/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 95402/5000000 bytes
Lua time usage: 0.222/10.000 seconds
Lua memory usage: 7323083/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  435.253      1 -total
 53.67%  233.597      1 Template:Reflist
 37.98%  165.311     14 Template:Cite_journal
 16.52%   71.884      1 Template:Short_description
  9.17%   39.923      2 Template:Pagetype
  7.84%   34.145      2 Template:IPAc-en
  5.48%   23.836      3 Template:Fix
  5.32%   23.162      4 Template:Cite_conference
  5.02%   21.864      1 Template:Rp
  4.78%   20.819      4 Template:Main_other
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:21438003-0!canonical and timestamp 20240520162536 and revision id 1217045468. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First look at the upcoming Starlink Mini (158 pts)]]></title>
            <link>https://www.starlinkhardware.com/first-look-at-the-upcoming-starlink-mini/</link>
            <guid>40699504</guid>
            <pubDate>Sun, 16 Jun 2024 19:39:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.starlinkhardware.com/first-look-at-the-upcoming-starlink-mini/">https://www.starlinkhardware.com/first-look-at-the-upcoming-starlink-mini/</a>, See on <a href="https://news.ycombinator.com/item?id=40699504">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text"><p>The launch of the highly anticipated Starlink Mini dish is imminent. Last week, Starlink got <a href="https://fcc.report/FCC-ID/2AWHPW231/7390216.pdf" target="_blank" rel="noreferrer noopener">approval from the FCC</a> for the Mini’s Wifi router. Today, Starlink updated their app with some juicy new details. In this post, I’m going to dive into what we know so far, and show you the first images of the Mini.</p><p>Based on previous FCC documents filed by SpaceX, we know the size of Starlink Mini to be 11.4″ x 9.8″, or about the size of a laptop. Here is a graphic I made to compare the size of the Standard and the Mini:</p><div><figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/www.starlinkhardware.com\/wp-content\/uploads\/2024\/01\/Starlink-Standard-Gen-3.jpg&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-5368&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1920,&quot;targetHeight&quot;:1080,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Starlink Standard vs Starlink Mini Size Comparison&quot;,&quot;alt&quot;:&quot;Starlink Standard vs Starlink Mini Size Comparison&quot;}" data-wp-interactive="core/image"><img fetchpriority="high" decoding="async" width="1920" height="1080" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://www.starlinkhardware.com/wp-content/uploads/2024/01/Starlink-Standard-Gen-3.jpg" alt="Starlink Standard vs Starlink Mini Size Comparison" srcset="https://www.starlinkhardware.com/wp-content/uploads/2024/01/Starlink-Standard-Gen-3.jpg 1920w, https://www.starlinkhardware.com/wp-content/uploads/2024/01/Starlink-Standard-Gen-3-400x225.jpg 400w, https://www.starlinkhardware.com/wp-content/uploads/2024/01/Starlink-Standard-Gen-3-300x169.jpg 300w, https://www.starlinkhardware.com/wp-content/uploads/2024/01/Starlink-Standard-Gen-3-1024x576.jpg 1024w, https://www.starlinkhardware.com/wp-content/uploads/2024/01/Starlink-Standard-Gen-3-768x432.jpg 768w, https://www.starlinkhardware.com/wp-content/uploads/2024/01/Starlink-Standard-Gen-3-1536x864.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px"><figcaption>Dimensions of the Standard dish (left) vs Mini dish (right)</figcaption></figure></div><p>Based on the size of the dish, it looks like Starlink Mini will be aimed at portable use cases, such as camping, RV’s, vans, hiking, etc. It’s designed to be easy to store, transport, and deploy. I’m hoping Starlink listens to customer feedback and offers a DC power supply accessory, but many details are still unknown.</p><p>What I do know at this point comes to us from a recent Starlink app update. The app update shows a new shop page for the upcoming Mini dish, as well as developer mode pages. This gives us our first look at the Mini:</p><div><figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/www.starlinkhardware.com\/wp-content\/uploads\/2024\/06\/IMG_3293.jpeg&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-6199&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1179,&quot;targetHeight&quot;:704,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Starlink Mini&quot;,&quot;alt&quot;:&quot;Starlink Mini&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1179" height="704" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3293.jpeg" alt="Starlink Mini" srcset="https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3293.jpeg 1179w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3293-400x239.jpeg 400w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3293-1024x611.jpeg 1024w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3293-768x459.jpeg 768w" sizes="(max-width: 1179px) 100vw, 1179px"></figure></div><div><figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/www.starlinkhardware.com\/wp-content\/uploads\/2024\/06\/IMG_3292.jpeg&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-6200&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1179,&quot;targetHeight&quot;:693,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Starlink Mini Side View&quot;,&quot;alt&quot;:&quot;Starlink Mini Side View&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1179" height="693" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3292.jpeg" alt="Starlink Mini Side View" srcset="https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3292.jpeg 1179w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3292-400x235.jpeg 400w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3292-1024x602.jpeg 1024w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3292-768x451.jpeg 768w" sizes="(max-width: 1179px) 100vw, 1179px"></figure></div><p>As you can see, the Mini is very similar to the Standard dish, just smaller. It has a similar shape, and even a kickstand.</p><p>The Starlink app update gives us more details about the Mini. If you go into developer mode and play around with the Mini network settings, you notice something interesting. There is no separate router. Devices are connected to the dish itself:</p><div><figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/www.starlinkhardware.com\/wp-content\/uploads\/2024\/06\/IMG_3294.jpeg&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-6198&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1179,&quot;targetHeight&quot;:670,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Starlink Mini network page showing connected devices&quot;,&quot;alt&quot;:&quot;Starlink Mini network page showing connected devices&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1179" height="670" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3294.jpeg" alt="Starlink Mini network page showing connected devices" srcset="https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3294.jpeg 1179w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3294-400x227.jpeg 400w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3294-1024x582.jpeg 1024w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3294-768x436.jpeg 768w" sizes="(max-width: 1179px) 100vw, 1179px"></figure></div><p>That can only mean one thing: The Starlink Mini will feature a built-in Wifi router. I’m guessing that, in order to make the Mini as portable as possible, Starlink decided it was best to simplify the system and limit the number of components.</p><p>There are more Wifi details that have been revealed, and that is mesh compatibility. For those of you that might be interested in using the Mini at home, or for larger events where you need additional Wifi coverage, the Mini’s built-in router will be compatible with Starlink mesh. You’ll be able to wirelessly pair another Starlink router to the Mini, as seen in this Starlink app graphic:</p><div><figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/www.starlinkhardware.com\/wp-content\/uploads\/2024\/06\/IMG_3295.jpeg&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-large is-resized&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-6197&quot;,&quot;imgStyles&quot;:&quot;width:474px;height:auto&quot;,&quot;targetWidth&quot;:1179,&quot;targetHeight&quot;:1187,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Starlink Mini paired with a mesh router&quot;,&quot;alt&quot;:&quot;Starlink Mini paired with a mesh router&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1017" height="1024" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3295-1017x1024.jpeg" alt="Starlink Mini paired with a mesh router" srcset="https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3295-1017x1024.jpeg 1017w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3295-397x400.jpeg 397w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3295-150x150.jpeg 150w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3295-768x773.jpeg 768w, https://www.starlinkhardware.com/wp-content/uploads/2024/06/IMG_3295.jpeg 1179w" sizes="(max-width: 1017px) 100vw, 1017px"></figure></div><p>There you have it, we’ve now seen what the Mini will look like, and how practical it will be for camping and other activities where portability is key. I speculate that Starlink will launch the Mini within the next several weeks, based on the familiar pattern we’ve seen from other product launches. The fact that Starlink created the shop page and updated the app suggests we are very close.</p><p>I’d love to hear your thoughts on the images and details shared in this post. Let’s discuss it in the comments below!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NumPy 2.0 (279 pts)]]></title>
            <link>https://numpy.org/devdocs/release/2.0.0-notes.html</link>
            <guid>40699470</guid>
            <pubDate>Sun, 16 Jun 2024 19:32:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://numpy.org/devdocs/release/2.0.0-notes.html">https://numpy.org/devdocs/release/2.0.0-notes.html</a>, See on <a href="https://news.ycombinator.com/item?id=40699470">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
                  
  <section id="numpy-2-0-0-release-notes">

<div>
<p>Note</p>
<p>The release of 2.0 is in progress and the current release overview and
highlights are still in a draft state. However, the highlights should
already list the most significant changes detailed in the full notes below,
and those full notes should be complete (if not copy-edited well enough
yet).</p>
</div>
<p>NumPy 2.0.0 is the first major release since 2006. It is the result of X months
of development since the last feature release by Y contributors, and contains a
large amount of exciting new features as well as a large amount of changes to
both the Python and C APIs.</p>
<p>This major release includes breaking changes that could not happen in a regular
minor (feature) release - including an ABI break, changes to type promotion
rules, and API changes which may not have been emitting deprecation warnings
in 1.26.x. Key documents related to how to adapt to changes in NumPy 2.0, in
addition to these release notes, include:</p>
<ul>
<li><p>The <a href="https://numpy.org/devdocs/numpy_2_0_migration_guide.html#numpy-2-migration-guide"><span>NumPy 2.0 migration guide</span></a></p></li>
<li><p>The <a href="https://numpy.org/devdocs/dev/depending_on_numpy.html#numpy-2-abi-handling"><span>NumPy 2.0-specific advice</span></a> in
<a href="https://numpy.org/devdocs/dev/depending_on_numpy.html#for-downstream-package-authors"><span>For downstream package authors</span></a></p></li>
</ul>
<section id="highlights">
<h2>Highlights<a href="#highlights" title="Link to this heading">#</a></h2>
<p>Highlights of this release include:</p>
<ul>
<li><p>New features:</p>
<ul>
<li><p>A new variable-length string dtype, <a href="https://numpy.org/devdocs/reference/routines.dtypes.html#numpy.dtypes.StringDType" title="numpy.dtypes.StringDType"><code><span>StringDType</span></code></a> and a new
<a href="https://numpy.org/devdocs/reference/routines.strings.html#module-numpy.strings" title="numpy.strings"><code><span>numpy.strings</span></code></a> namespace with performant ufuncs for string operations,</p></li>
<li><p>Support for <code><span>float32</span></code> and <code><span>longdouble</span></code> in all <a href="https://numpy.org/devdocs/reference/routines.fft.html#module-numpy.fft" title="numpy.fft"><code><span>numpy.fft</span></code></a> functions,</p></li>
<li><p>Support for the array API standard in the main <code><span>numpy</span></code> namespace.</p></li>
</ul>
</li>
<li><p>Performance improvements:</p>
<ul>
<li><p>Sorting functions (<a href="https://numpy.org/devdocs/reference/generated/numpy.sort.html#numpy.sort" title="numpy.sort"><code><span>sort</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.argsort.html#numpy.argsort" title="numpy.argsort"><code><span>argsort</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.partition.html#numpy.partition" title="numpy.partition"><code><span>partition</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.argpartition.html#numpy.argpartition" title="numpy.argpartition"><code><span>argpartition</span></code></a>)
have been accelerated through the use of the Intel x86-simd-sort and Google
Highway libraries, and may see large (hardware-specific) speedups,</p></li>
<li><p>macOS Accelerate support and binary wheels for macOS &gt;=14, with significant
performance improvements for linear algebra operations on macOS, and wheels
that are about 3 times smaller,</p></li>
<li><p><a href="https://numpy.org/devdocs/reference/routines.char.html#module-numpy.char" title="numpy.char"><code><span>numpy.char</span></code></a> fixed-length string operations have been accelerated by
implementing ufuncs that also support <a href="https://numpy.org/devdocs/reference/routines.dtypes.html#numpy.dtypes.StringDType" title="numpy.dtypes.StringDType"><code><span>StringDType</span></code></a> in
addition to the the fixed-length string dtypes,</p></li>
<li><p>A new tracing and introspection API, <a href="https://numpy.org/devdocs/reference/generated/numpy.lib.introspect.opt_func_info.html#numpy.lib.introspect.opt_func_info" title="numpy.lib.introspect.opt_func_info"><code><span>opt_func_info</span></code></a>,
to determine which hardware-specific kernels are available and will be
dispatched to.</p></li>
</ul>
</li>
<li><p>Python API improvements:</p>
<ul>
<li><p>A clear split between public and private API, with a new
<a href="https://numpy.org/devdocs/reference/module_structure.html#module-structure"><span>module structure</span></a>, and each public function now
available in a single place,</p></li>
<li><p>Many removals of non-recommended functions and aliases. This should make
it easier to learn and use NumPy. The number of objects in the main
namespace decreased by ~10% and in <code><span>numpy.lib</span></code> by ~80%,</p></li>
<li><p><a href="https://numpy.org/devdocs/user/basics.types.html#canonical-python-and-c-types"><span>Canonical dtype names</span></a> and a new
<a href="https://numpy.org/devdocs/reference/generated/numpy.isdtype.html#numpy.isdtype" title="numpy.isdtype"><code><span>isdtype</span></code></a> introspection function,</p></li>
</ul>
</li>
<li><p>C API improvements:</p>
<ul>
<li><p>A new <a href="https://numpy.org/devdocs/reference/c-api/array.html#dtype-api"><span>public C API for creating custom dtypes</span></a>,</p></li>
<li><p>Many outdated functions and macros removed, and private internals hidden to
ease future extensibility,</p></li>
<li><p>New, easier to use, initialization functions:
<a href="https://numpy.org/devdocs/reference/c-api/array.html#c.PyArray_ImportNumPyAPI" title="PyArray_ImportNumPyAPI"><code><span>PyArray_ImportNumPyAPI</span></code></a> and <a href="https://numpy.org/devdocs/reference/c-api/ufunc.html#c.PyUFunc_ImportUFuncAPI" title="PyUFunc_ImportUFuncAPI"><code><span>PyUFunc_ImportUFuncAPI</span></code></a>.</p></li>
</ul>
</li>
<li><p>Improved behavior:</p>
<ul>
<li><p>Improvements to type promotion behavior was changed by adopting <a href="https://numpy.org/devdocs/release/NEP50">NEP
50</a>. This fixes many user surprises about promotions which
previously often depended on data values of input arrays rather than only
their dtypes.  Please see the NEP and the <a href="https://numpy.org/devdocs/numpy_2_0_migration_guide.html#numpy-2-migration-guide"><span>NumPy 2.0 migration guide</span></a>
for details as this change can lead to changes in output dtypes and lower
precision results for mixed-dtype operations.</p></li>
<li><p>The default integer type on Windows is now <code><span>int64</span></code> rather than <code><span>int32</span></code>,
matching the behavior on other platforms,</p></li>
<li><p>The maximum number of array dimensions is changed from 32 to 64</p></li>
</ul>
</li>
<li><p>Documentation:</p>
<ul>
<li><p>The reference guide navigation was signficantly improved, and there is now
documentation on NumPy’s <a href="https://numpy.org/devdocs/reference/module_structure.html#module-structure"><span>module structure</span></a>,</p></li>
<li><p>The <a href="https://numpy.org/devdocs/building/index.html#building-from-source"><span>building from source</span></a> documentation was
completely rewritten,</p></li>
</ul>
</li>
</ul>
<p>Furthermore there are many changes to NumPy internals, including continuing to
migrate code from C to C++, that will make it easier to improve and maintain
NumPy in the future.</p>
<p>The “no free lunch” theorem dictates that there is a price to pay for all these
API and behavior improvements and better future extensibility. This price is:</p>
<ol>
<li><p>Backwards compatibility. There are a significant number of breaking changes
to both the Python and C APIs. In the majority of cases, there are clear
error messages that will inform the user how to adapt their code. However,
there are also changes in behavior for which it was not possible to give
such an error message - these cases are all covered in the Deprecation and
Compatibility sections below, and in the <a href="https://numpy.org/devdocs/numpy_2_0_migration_guide.html#numpy-2-migration-guide"><span>NumPy 2.0 migration guide</span></a>.</p>
<p>Note that there is a <code><span>ruff</span></code> mode to auto-fix many things in Python code.</p>
</li>
<li><p>Breaking changes to the NumPy ABI. As a result, binaries of packages that
use the NumPy C API and were built against a NumPy 1.xx release will not
work with NumPy 2.0. On import, such packages will see an <code><span>ImportError</span></code>
with a message about binary incompatibiliy.</p>
<p>It is possible to build binaries against NumPy 2.0 that will work at runtime
with both NumPy 2.0 and 1.x. See <a href="https://numpy.org/devdocs/dev/depending_on_numpy.html#numpy-2-abi-handling"><span>NumPy 2.0-specific advice</span></a> for more details.</p>
<p><strong>All downstream packages that depend on the NumPy ABI are advised to do a
new release built against NumPy 2.0 and verify that that release works with
both 2.0 and 1.26 - ideally in the period between 2.0.0rc1 (which will be
ABI-stable) and the final 2.0.0 release to avoid problems for their users.</strong></p>
</li>
</ol>
<p>The Python versions supported by this release are 3.9-3.12.</p>
</section>
<section id="numpy-2-0-python-api-removals">
<h2>NumPy 2.0 Python API removals<a href="#numpy-2-0-python-api-removals" title="Link to this heading">#</a></h2>
<ul>
<li><p><code><span>np.geterrobj</span></code>, <code><span>np.seterrobj</span></code> and the related ufunc keyword argument
<code><span>extobj=</span></code> have been removed.  The preferred replacement for all of these
is using the context manager <code><span>with</span> <span>np.errstate():</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23922">gh-23922</a>)</p>
</li>
<li><p><code><span>np.cast</span></code> has been removed. The literal replacement for
<code><span>np.cast[dtype](arg)</span></code> is <code><span>np.asarray(arg,</span> <span>dtype=dtype)</span></code>.</p></li>
<li><p><code><span>np.source</span></code> has been removed. The preferred replacement is
<code><span>inspect.getsource</span></code>.</p></li>
<li><p><code><span>np.lookfor</span></code> has been removed.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24144">gh-24144</a>)</p>
</li>
<li><p><code><span>numpy.who</span></code> has been removed. As an alternative for the removed functionality, one
can use a variable explorer that is available in IDEs such as Spyder or Jupyter Notebook.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24321">gh-24321</a>)</p>
</li>
<li><p>Warnings and exceptions present in <a href="https://numpy.org/devdocs/reference/routines.exceptions.html#module-numpy.exceptions" title="numpy.exceptions"><code><span>numpy.exceptions</span></code></a> (e.g,
<a href="https://numpy.org/devdocs/reference/generated/numpy.exceptions.ComplexWarning.html#numpy.exceptions.ComplexWarning" title="numpy.exceptions.ComplexWarning"><code><span>ComplexWarning</span></code></a>,
<a href="https://numpy.org/devdocs/reference/generated/numpy.exceptions.VisibleDeprecationWarning.html#numpy.exceptions.VisibleDeprecationWarning" title="numpy.exceptions.VisibleDeprecationWarning"><code><span>VisibleDeprecationWarning</span></code></a>) are no longer exposed in the
main namespace.</p></li>
<li><p>Multiple niche enums, expired members and functions have been removed from
the main namespace, such as: <code><span>ERR_*</span></code>, <code><span>SHIFT_*</span></code>, <code><span>np.fastCopyAndTranspose</span></code>,
<code><span>np.kernel_version</span></code>, <code><span>np.numarray</span></code>, <code><span>np.oldnumeric</span></code> and <code><span>np.set_numeric_ops</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24316">gh-24316</a>)</p>
</li>
<li><p>Replaced <code><span>from</span> <span>...</span> <span>import</span> <span>*</span></code> in the <code><span>numpy/__init__.py</span></code> with explicit imports.
As a result, these main namespace members got removed: <code><span>np.FLOATING_POINT_SUPPORT</span></code>,
<code><span>np.FPE_*</span></code>, <code><span>np.NINF</span></code>, <code><span>np.PINF</span></code>, <code><span>np.NZERO</span></code>, <code><span>np.PZERO</span></code>, <code><span>np.CLIP</span></code>,
<code><span>np.WRAP</span></code>, <code><span>np.WRAP</span></code>, <code><span>np.RAISE</span></code>, <code><span>np.BUFSIZE</span></code>, <code><span>np.UFUNC_BUFSIZE_DEFAULT</span></code>,
<code><span>np.UFUNC_PYVALS_NAME</span></code>, <code><span>np.ALLOW_THREADS</span></code>, <code><span>np.MAXDIMS</span></code>, <code><span>np.MAY_SHARE_EXACT</span></code>,
<code><span>np.MAY_SHARE_BOUNDS</span></code>, <code><span>add_newdoc</span></code>, <code><span>np.add_docstring</span></code> and
<code><span>np.add_newdoc_ufunc</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24357">gh-24357</a>)</p>
</li>
<li><p>Alias <code><span>np.float_</span></code> has been removed. Use <code><span>np.float64</span></code> instead.</p></li>
<li><p>Alias <code><span>np.complex_</span></code> has been removed. Use <code><span>np.complex128</span></code> instead.</p></li>
<li><p>Alias <code><span>np.longfloat</span></code> has been removed. Use <code><span>np.longdouble</span></code> instead.</p></li>
<li><p>Alias <code><span>np.singlecomplex</span></code> has been removed. Use <code><span>np.complex64</span></code> instead.</p></li>
<li><p>Alias <code><span>np.cfloat</span></code> has been removed. Use <code><span>np.complex128</span></code> instead.</p></li>
<li><p>Alias <code><span>np.longcomplex</span></code> has been removed. Use <code><span>np.clongdouble</span></code> instead.</p></li>
<li><p>Alias <code><span>np.clongfloat</span></code> has been removed. Use <code><span>np.clongdouble</span></code> instead.</p></li>
<li><p>Alias <code><span>np.string_</span></code> has been removed. Use <code><span>np.bytes_</span></code> instead.</p></li>
<li><p>Alias <code><span>np.unicode_</span></code> has been removed. Use <code><span>np.str_</span></code> instead.</p></li>
<li><p>Alias <code><span>np.Inf</span></code> has been removed. Use <code><span>np.inf</span></code> instead.</p></li>
<li><p>Alias <code><span>np.Infinity</span></code> has been removed. Use <code><span>np.inf</span></code> instead.</p></li>
<li><p>Alias <code><span>np.NaN</span></code> has been removed. Use <code><span>np.nan</span></code> instead.</p></li>
<li><p>Alias <code><span>np.infty</span></code> has been removed. Use <code><span>np.inf</span></code> instead.</p></li>
<li><p>Alias <code><span>np.mat</span></code> has been removed. Use <code><span>np.asmatrix</span></code> instead.</p></li>
<li><p><code><span>np.issubclass_</span></code> has been removed. Use the <code><span>issubclass</span></code> builtin instead.</p></li>
<li><p><code><span>np.asfarray</span></code> has been removed. Use <code><span>np.asarray</span></code> with a proper dtype instead.</p></li>
<li><p><code><span>np.set_string_function</span></code> has been removed. Use <code><span>np.set_printoptions</span></code>
instead with a formatter for custom printing of NumPy objects.</p></li>
<li><p><code><span>np.tracemalloc_domain</span></code> is now only available from <code><span>np.lib</span></code>.</p></li>
<li><p><code><span>np.recfromcsv</span></code> and <code><span>recfromtxt</span></code> are now only available from <code><span>np.lib.npyio</span></code>.</p></li>
<li><p><code><span>np.issctype</span></code>, <code><span>np.maximum_sctype</span></code>, <code><span>np.obj2sctype</span></code>, <code><span>np.sctype2char</span></code>,
<code><span>np.sctypes</span></code>, <code><span>np.issubsctype</span></code> were all removed from the
main namespace without replacement, as they where niche members.</p></li>
<li><p>Deprecated <code><span>np.deprecate</span></code> and <code><span>np.deprecate_with_doc</span></code> has been removed
from the main namespace. Use <code><span>DeprecationWarning</span></code> instead.</p></li>
<li><p>Deprecated <code><span>np.safe_eval</span></code> has been removed from the main namespace.
Use <code><span>ast.literal_eval</span></code> instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24376">gh-24376</a>)</p>
</li>
<li><p><code><span>np.find_common_type</span></code> has been removed. Use <code><span>numpy.promote_types</span></code> or
<code><span>numpy.result_type</span></code> instead. To achieve semantics for the <code><span>scalar_types</span></code>
argument, use <code><span>numpy.result_type</span></code> and pass <code><span>0</span></code>, <code><span>0.0</span></code>, or <code><span>0j</span></code> as a
Python scalar instead.</p></li>
<li><p><code><span>np.round_</span></code> has been removed. Use <code><span>np.round</span></code> instead.</p></li>
<li><p><code><span>np.nbytes</span></code> has been removed. Use <code><span>np.dtype(&lt;dtype&gt;).itemsize</span></code> instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24477">gh-24477</a>)</p>
</li>
<li><p><code><span>np.compare_chararrays</span></code> has been removed from the main namespace.
Use <code><span>np.char.compare_chararrays</span></code> instead.</p></li>
<li><p>The <code><span>charrarray</span></code> in the main namespace has been deprecated. It can be imported
without a deprecation warning from <code><span>np.char.chararray</span></code> for now,
but we are planning to fully deprecate and remove <code><span>chararray</span></code> in the future.</p></li>
<li><p><code><span>np.format_parser</span></code> has been removed from the main namespace.
Use <code><span>np.rec.format_parser</span></code> instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24587">gh-24587</a>)</p>
</li>
<li><p>Support for seven data type string aliases has been removed from <code><span>np.dtype</span></code>:
<code><span>int0</span></code>, <code><span>uint0</span></code>, <code><span>void0</span></code>, <code><span>object0</span></code>, <code><span>str0</span></code>, <code><span>bytes0</span></code> and <code><span>bool8</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24807">gh-24807</a>)</p>
</li>
<li><p>The experimental <code><span>numpy.array_api</span></code> submodule has been removed. Use the main
<code><span>numpy</span></code> namespace for regular usage instead, or the separate
<code><span>array-api-strict</span></code> package for the compliance testing use case for which
<code><span>numpy.array_api</span></code> was mostly used.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25911">gh-25911</a>)</p>
</li>
</ul>
<section id="array-prepare-is-removed">
<h3><code><span>__array_prepare__</span></code> is removed<a href="#array-prepare-is-removed" title="Link to this heading">#</a></h3>
<p>UFuncs called <code><span>__array_prepare__</span></code> before running computations
for normal ufunc calls (not generalized ufuncs, reductions, etc.).
The function was also called instead of <code><span>__array_wrap__</span></code> on the
results of some linear algebra functions.</p>
<p>It is now removed. If you use it, migrate to <code><span>__array_ufunc__</span></code> or rely on
<code><span>__array_wrap__</span></code> which is called with a context in all cases, although only
after the result array is filled. In those code paths, <code><span>__array_wrap__</span></code> will
now be passed a base class, rather than a subclass array.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25105">gh-25105</a>)</p>
</section>
</section>
<section id="deprecations">
<h2>Deprecations<a href="#deprecations" title="Link to this heading">#</a></h2>
<ul>
<li><p><code><span>np.compat</span></code> has been deprecated, as Python 2 is no longer supported.</p></li>
<li><p><code><span>np.safe_eval</span></code> has been deprecated. <code><span>ast.literal_eval</span></code> should be used instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23830">gh-23830</a>)</p>
</li>
<li><p><code><span>np.recfromcsv</span></code>, <code><span>np.recfromtxt</span></code>, <code><span>np.disp</span></code>, <code><span>np.get_array_wrap</span></code>,
<code><span>np.maximum_sctype</span></code>, <code><span>np.deprecate</span></code> and <code><span>np.deprecate_with_doc</span></code>
have been deprecated.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24154">gh-24154</a>)</p>
</li>
<li><p><code><span>np.trapz</span></code> has been deprecated. Use <code><span>np.trapezoid</span></code> or a <code><span>scipy.integrate</span></code> function instead.</p></li>
<li><p><code><span>np.in1d</span></code> has been deprecated. Use <code><span>np.isin</span></code> instead.</p></li>
<li><p>Alias <code><span>np.row_stack</span></code> has been deprecated. Use <code><span>np.vstack</span></code> directly.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24445">gh-24445</a>)</p>
</li>
<li><p><code><span>__array_wrap__</span></code> is now passed <code><span>arr,</span> <span>context,</span> <span>return_scalar</span></code> and
support for implementations not accepting all three are deprecated.  Its signature
should be <code><span>__array_wrap__(self,</span> <span>arr,</span> <span>context=None,</span> <span>return_scalar=False)</span></code></p>
<p>(<a href="https://github.com/numpy/numpy/pull/25408">gh-25408</a>)</p>
</li>
<li><p>Arrays of 2-dimensional vectors for <code><span>np.cross</span></code> have been deprecated. Use
arrays of 3-dimensional vectors instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24818">gh-24818</a>)</p>
</li>
<li><p><code><span>np.dtype("a")</span></code> alias for <code><span>np.dtype(np.bytes_)</span></code> was deprecated. Use
<code><span>np.dtype("S")</span></code> alias instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24854">gh-24854</a>)</p>
</li>
<li><p>Use of keyword arguments <code><span>x</span></code> and <code><span>y</span></code> with functions
<code><span>assert_array_equal</span></code> and <code><span>assert_array_almost_equal</span></code> has been deprecated.
Pass the first two arguments as positional arguments instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24978">gh-24978</a>)</p>
</li>
</ul>
<section id="numpy-fft-deprecations-for-n-d-transforms-with-none-values-in-arguments">
<h3><a href="https://numpy.org/devdocs/reference/routines.fft.html#module-numpy.fft" title="numpy.fft"><code><span>numpy.fft</span></code></a> deprecations for n-D transforms with None values in arguments<a href="#numpy-fft-deprecations-for-n-d-transforms-with-none-values-in-arguments" title="Link to this heading">#</a></h3>
<p>Using <code><span>fftn</span></code>, <code><span>ifftn</span></code>, <code><span>rfftn</span></code>, <code><span>irfftn</span></code>, <code><span>fft2</span></code>, <code><span>ifft2</span></code>,
<code><span>rfft2</span></code> or <code><span>irfft2</span></code> with the <code><span>s</span></code> parameter set to a value that is not
<code><span>None</span></code> and the <code><span>axes</span></code> parameter set to <code><span>None</span></code> has been deprecated, in
line with the array API standard. To retain current behaviour, pass a sequence
[0, …, k-1] to <code><span>axes</span></code> for an array of dimension k.</p>
<p>Furthermore, passing an array to <code><span>s</span></code> which contains <code><span>None</span></code> values is
deprecated as the parameter is documented to accept a sequence of integers
in both the NumPy docs and the array API specification. To use the default
behaviour of the corresponding 1-D transform, pass the value matching
the default for its <code><span>n</span></code> parameter. To use the default behaviour for every
axis, the <code><span>s</span></code> argument can be omitted.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25495">gh-25495</a>)</p>
</section>
<section id="np-linalg-lstsq-now-defaults-to-a-new-rcond-value">
<h3><code><span>np.linalg.lstsq</span></code> now defaults to a new <code><span>rcond</span></code> value<a href="#np-linalg-lstsq-now-defaults-to-a-new-rcond-value" title="Link to this heading">#</a></h3>
<p><a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq" title="numpy.linalg.lstsq"><code><span>lstsq</span></code></a> now uses the new rcond value of the machine precision
times <code><span>max(M,</span> <span>N)</span></code>.  Previously, the machine precision was used but a
FutureWarning was given to notify that this change will happen eventually.
That old behavior can still be achieved by passing <code><span>rcond=-1</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25721">gh-25721</a>)</p>
</section>
</section>
<section id="expired-deprecations">
<h2>Expired deprecations<a href="#expired-deprecations" title="Link to this heading">#</a></h2>
<ul>
<li><p>The <code><span>np.core.umath_tests</span></code> submodule has been removed from the public API.
(Deprecated in NumPy 1.15)</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23809">gh-23809</a>)</p>
</li>
<li><p>The <code><span>PyDataMem_SetEventHook</span></code> deprecation has expired and it is
removed.  Use <code><span>tracemalloc</span></code> and the <code><span>np.lib.tracemalloc_domain</span></code>
domain.  (Deprecated in NumPy 1.23)</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23921">gh-23921</a>)</p>
</li>
<li><p>The deprecation of <code><span>set_numeric_ops</span></code> and the C functions
<code><span>PyArray_SetNumericOps</span></code> and <code><span>PyArray_GetNumericOps</span></code> has
been expired and the functions removed.  (Deprecated in NumPy 1.16)</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23998">gh-23998</a>)</p>
</li>
<li><p>The <code><span>fasttake</span></code>, <code><span>fastclip</span></code>, and <code><span>fastputmask</span></code>  <code><span>ArrFuncs</span></code>
deprecation is now finalized.</p></li>
<li><p>The deprecated function <code><span>fastCopyAndTranspose</span></code> and its C counterpart
are now removed.</p></li>
<li><p>The deprecation of <code><span>PyArray_ScalarFromObject</span></code> is now finalized.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24312">gh-24312</a>)</p>
</li>
<li><p><code><span>np.msort</span></code> has been removed. For a replacement, <code><span>np.sort(a,</span> <span>axis=0)</span></code>
should be used instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24494">gh-24494</a>)</p>
</li>
<li><p><code><span>np.dtype(("f8",</span> <span>1)</span></code> will now return a shape 1 subarray dtype
rather than a non-subarray one.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25761">gh-25761</a>)</p>
</li>
<li><p>Assigning to the <code><span>.data</span></code> attribute of an ndarray is disallowed and will
raise.</p></li>
<li><p><code><span>np.binary_repr(a,</span> <span>width)</span></code> will raise if width is too small.</p></li>
<li><p>Using <code><span>NPY_CHAR</span></code> in <code><span>PyArray_DescrFromType()</span></code> will raise, use
<code><span>NPY_STRING</span></code> <code><span>NPY_UNICODE</span></code>, or <code><span>NPY_VSTRING</span></code> instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25794">gh-25794</a>)</p>
</li>
</ul>
</section>
<section id="compatibility-notes">
<h2>Compatibility notes<a href="#compatibility-notes" title="Link to this heading">#</a></h2>
<section id="loadtxt-and-genfromtxt-default-encoding-changed">
<h3><code><span>loadtxt</span></code> and <code><span>genfromtxt</span></code> default encoding changed<a href="#loadtxt-and-genfromtxt-default-encoding-changed" title="Link to this heading">#</a></h3>
<p><code><span>loadtxt</span></code> and <code><span>genfromtxt</span></code> now both default to <code><span>encoding=None</span></code>
which may mainly modify how <code><span>converters</span></code> work.
These will now be passed <code><span>str</span></code> rather than <code><span>bytes</span></code>. Pass the
encoding explicitly to always get the new or old behavior.
For <code><span>genfromtxt</span></code> the change also means that returned values will now be
unicode strings rather than bytes.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25158">gh-25158</a>)</p>
</section>
<section id="f2py-compatibility-notes">
<h3><code><span>f2py</span></code> compatibility notes<a href="#f2py-compatibility-notes" title="Link to this heading">#</a></h3>
<p><code><span>f2py</span></code> will no longer accept ambiguous <code><span>-m</span></code> and <code><span>.pyf</span></code> CLI combinations.
When more than one <code><span>.pyf</span></code> file is passed, an error is raised. When both <code><span>-m</span></code>
and a <code><span>.pyf</span></code> is passed, a warning is emitted and the <code><span>-m</span></code> provided name is
ignored.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25181">gh-25181</a>)</p>
<p>The <code><span>f2py.compile()</span></code> helper has been removed because it leaked memory, has
been marked as experimental for several years now, and was implemented as a thin
<code><span>subprocess.run</span></code> wrapper. It is also one of the test bottlenecks. See
<a href="https://github.com/numpy/numpy/issues/25122">gh-25122</a> for the full
rationale. It also used several <code><span>np.distutils</span></code> features which are too fragile
to be ported to work with <code><span>meson</span></code>.</p>
<p>Users are urged to replace calls to <code><span>f2py.compile</span></code> with calls to
<code><span>subprocess.run("python",</span> <span>"-m",</span> <span>"numpy.f2py",...</span></code> instead, and to use
environment variables to interact with <code><span>meson</span></code>. <a href="https://mesonbuild.com/Machine-files.html">Native files</a> are also an option.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25193">gh-25193</a>)</p>
</section>
<section id="arange-s-start-argument-is-positional-only">
<h3><code><span>arange</span></code>’s <code><span>start</span></code> argument is positional-only<a href="#arange-s-start-argument-is-positional-only" title="Link to this heading">#</a></h3>
<p>The first argument of <code><span>arange</span></code> is now positional only. This way,
specifying a <code><span>start</span></code> argument as a keyword, e.g. <code><span>arange(start=0,</span> <span>stop=4)</span></code>,
raises a TypeError. Other behaviors, are unchanged so <code><span>arange(stop=4)</span></code>,
<code><span>arange(2,</span> <span>stop=4)</span></code> and so on, are still valid and have the same meaning as
before.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25336">gh-25336</a>)</p>
</section>
<section id="minor-changes-in-behavior-of-sorting-functions">
<h3>Minor changes in behavior of sorting functions<a href="#minor-changes-in-behavior-of-sorting-functions" title="Link to this heading">#</a></h3>
<p>Due to algorithmic changes and use of SIMD code, sorting functions with methods
that aren’t stable may return slightly different results in 2.0.0 compared to
1.26.x. This includes the default method of <a href="https://numpy.org/devdocs/reference/generated/numpy.argsort.html#numpy.argsort" title="numpy.argsort"><code><span>argsort</span></code></a> and
<a href="https://numpy.org/devdocs/reference/generated/numpy.argpartition.html#numpy.argpartition" title="numpy.argpartition"><code><span>argpartition</span></code></a>.</p>
</section>
<section id="removed-ambiguity-when-broadcasting-in-np-solve">
<h3>Removed ambiguity when broadcasting in <code><span>np.solve</span></code><a href="#removed-ambiguity-when-broadcasting-in-np-solve" title="Link to this heading">#</a></h3>
<p>The broadcasting rules for <code><span>np.solve(a,</span> <span>b)</span></code> were ambiguous when <code><span>b</span></code> had 1
fewer dimensions than <code><span>a</span></code>. This has been resolved in a backward-incompatible
way and is now compliant with the Array API. The old behaviour can be
reconstructed by using <code><span>np.solve(a,</span> <span>b[...,</span> <span>None])[...,</span> <span>0]</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25914">gh-25914</a>)</p>
</section>
<section id="modified-representation-for-polynomial">
<h3>Modified representation for <code><span>Polynomial</span></code><a href="#modified-representation-for-polynomial" title="Link to this heading">#</a></h3>
<p>The representation method for <a href="https://numpy.org/devdocs/reference/generated/numpy.polynomial.polynomial.Polynomial.html#numpy.polynomial.polynomial.Polynomial" title="numpy.polynomial.polynomial.Polynomial"><code><span>Polynomial</span></code></a> was
updated to include the domain in the representation. The plain text and latex
representations are now consistent. For example the output of
<code><span>str(np.polynomial.Polynomial([1,</span> <span>1],</span> <span>domain=[.1,</span> <span>.2]))</span></code> used to be <code><span>1.0</span> <span>+</span>
<span>1.0</span> <span>x</span></code>, but now is <code><span>1.0</span> <span>+</span> <span>1.0</span> <span>(-3.0000000000000004</span> <span>+</span> <span>20.0</span> <span>x)</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/21760">gh-21760</a>)</p>
</section>
</section>
<section id="c-api-changes">
<h2>C API changes<a href="#c-api-changes" title="Link to this heading">#</a></h2>
<ul>
<li><p>The <code><span>PyArray_CGT</span></code>, <code><span>PyArray_CLT</span></code>, <code><span>PyArray_CGE</span></code>, <code><span>PyArray_CLE</span></code>,
<code><span>PyArray_CEQ</span></code>, <code><span>PyArray_CNE</span></code> macros have been removed.</p></li>
<li><p><code><span>PyArray_MIN</span></code> and <code><span>PyArray_MAX</span></code> have been moved from <code><span>ndarraytypes.h</span></code>
to <code><span>npy_math.h</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24258">gh-24258</a>)</p>
</li>
<li><p>A C API for working with <a href="https://numpy.org/devdocs/reference/routines.dtypes.html#numpy.dtypes.StringDType" title="numpy.dtypes.StringDType"><code><span>numpy.dtypes.StringDType</span></code></a> arrays has been exposed.
This includes functions for acquiring and releasing mutexes which lock access
to the string data, as well as packing and unpacking UTF-8 bytestreams from
array entries.</p></li>
<li><p><code><span>NPY_NTYPES</span></code> has been renamed to <code><span>NPY_NTYPES_LEGACY</span></code> as it does not
include new NumPy built-in DTypes. In particular the new string DType
will likely not work correctly with code that handles legacy DTypes.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25347">gh-25347</a>)</p>
</li>
<li><p>The C-API now only exports the static inline function versions
of the array accessors (previously this depended on using “deprecated API”).
While we discourage it, the struct fields can still be used directly.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25789">gh-25789</a>)</p>
</li>
<li><p>NumPy now defines <a href="https://numpy.org/devdocs/reference/c-api/array.html#c.PyArray_Pack" title="PyArray_Pack"><code><span>PyArray_Pack</span></code></a> to set an individual memory
address.  Unlike <code><span>PyArray_SETITEM</span></code> this function is equivalent to setting
an individual array item and does not require a NumPy array input.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25954">gh-25954</a>)</p>
</li>
<li><p>The <code><span>-&gt;f</span></code> slot has been removed from <code><span>PyArray_Descr</span></code>.
If you use this slot, replace accessing it with
<code><span>PyDataType_GetArrFuncs</span></code> (see its documentation and the
<a href="https://numpy.org/devdocs/numpy_2_0_migration_guide.html#numpy-2-migration-guide"><span>NumPy 2.0 migration guide</span></a>). In some cases using other functions like
<code><span>PyArray_GETITEM</span></code> may be an alternatives.</p></li>
<li><p><code><span>PyArray_GETITEM</span></code> and <code><span>PyArray_SETITEM</span></code> now require the import of the
NumPy API table to be used and are no longer defined in <code><span>ndarraytypes.h</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25812">gh-25812</a>)</p>
</li>
<li><p>Due to runtime dependencies, the definition for functionality accessing
the dtype flags was moved from <code><span>numpy/ndarraytypes.h</span></code> and is only available
after including <code><span>numpy/ndarrayobject.h</span></code> as it requires <code><span>import_array()</span></code>.
This includes <code><span>PyDataType_FLAGCHK</span></code>, <code><span>PyDataType_REFCHK</span></code> and
<code><span>NPY_BEGIN_THREADS_DESCR</span></code>.</p></li>
<li><p>The dtype flags on <code><span>PyArray_Descr</span></code> must now be accessed through the
<code><span>PyDataType_FLAGS</span></code> inline function to be compatible with both 1.x and 2.x.
This function is defined in <code><span>npy_2_compat.h</span></code> to allow backporting.
Most or all users should use <code><span>PyDataType_FLAGCHK</span></code> which is available on
1.x and does not require backporting.
Cython users should use Cython 3.  Otherwise access will go through Python
unless they use <code><span>PyDataType_FLAGCHK</span></code> instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25816">gh-25816</a>)</p>
</li>
</ul>
<section id="datetime-functionality-exposed-in-the-c-api-and-cython-bindings">
<h3>Datetime functionality exposed in the C API and Cython bindings<a href="#datetime-functionality-exposed-in-the-c-api-and-cython-bindings" title="Link to this heading">#</a></h3>
<p>The functions <code><span>NpyDatetime_ConvertDatetime64ToDatetimeStruct</span></code>,
<code><span>NpyDatetime_ConvertDatetimeStructToDatetime64</span></code>,
<code><span>NpyDatetime_ConvertPyDateTimeToDatetimeStruct</span></code>,
<code><span>NpyDatetime_GetDatetimeISO8601StrLen</span></code>, <code><span>NpyDatetime_MakeISO8601Datetime</span></code>,
and <code><span>NpyDatetime_ParseISO8601Datetime</span></code> have been added to the C API to
facilitate converting between strings, Python datetimes, and NumPy datetimes in
external libraries.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/21199">gh-21199</a>)</p>
</section>
<section id="const-correctness-for-the-generalized-ufunc-c-api">
<h3>Const correctness for the generalized ufunc C API<a href="#const-correctness-for-the-generalized-ufunc-c-api" title="Link to this heading">#</a></h3>
<p>The NumPy C API’s functions for constructing generalized ufuncs
(<code><span>PyUFunc_FromFuncAndData</span></code>, <code><span>PyUFunc_FromFuncAndDataAndSignature</span></code>,
<code><span>PyUFunc_FromFuncAndDataAndSignatureAndIdentity</span></code>) take <code><span>types</span></code> and <code><span>data</span></code>
arguments that are not modified by NumPy’s internals. Like the <code><span>name</span></code> and
<code><span>doc</span></code> arguments, third-party Python extension modules are likely to supply
these arguments from static constants. The <code><span>types</span></code> and <code><span>data</span></code> arguments are
now const-correct: they are declared as <code><span>const</span> <span>char</span> <span>*types</span></code> and
<code><span>void</span> <span>*const</span> <span>*data</span></code>, respectively. C code should not be affected, but C++
code may be.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23847">gh-23847</a>)</p>
</section>
<section id="larger-npy-maxdims-and-npy-maxargs-npy-ravel-axis-introduced">
<h3>Larger <code><span>NPY_MAXDIMS</span></code> and <code><span>NPY_MAXARGS</span></code>, <code><span>NPY_RAVEL_AXIS</span></code> introduced<a href="#larger-npy-maxdims-and-npy-maxargs-npy-ravel-axis-introduced" title="Link to this heading">#</a></h3>
<p><code><span>NPY_MAXDIMS</span></code> is now 64, you may want to review its use.  This is usually
used in a stack allocation, where the increase should be safe.
However, we do encourage generally to remove any use of <code><span>NPY_MAXDIMS</span></code> and
<code><span>NPY_MAXARGS</span></code> to eventually allow removing the constraint completely.
For the conversion helper and C-API functions mirroring Python ones such as
<code><span>take</span></code>, <code><span>NPY_MAXDIMS</span></code> was used to mean <code><span>axis=None</span></code>. Such usage must be
replaced with <code><span>NPY_RAVEL_AXIS</span></code>. See also <a href="https://numpy.org/devdocs/numpy_2_0_migration_guide.html#migration-maxdims"><span>Increased maximum number of dimensions</span></a>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25149">gh-25149</a>)</p>
</section>
<section id="npy-maxargs-not-constant-and-pyarraymultiiterobject-size-change">
<h3><code><span>NPY_MAXARGS</span></code> not constant and <code><span>PyArrayMultiIterObject</span></code> size change<a href="#npy-maxargs-not-constant-and-pyarraymultiiterobject-size-change" title="Link to this heading">#</a></h3>
<p>Since <code><span>NPY_MAXARGS</span></code> was increased, it is now a runtime constant and not
compile-time constant anymore.
We expect almost no users to notice this.  But if used for stack allocations
it now must be replaced with a custom constant using <code><span>NPY_MAXARGS</span></code> as an
additional runtime check.</p>
<p>The <code><span>sizeof(PyArrayMultiIterObject)</span></code> no longer includes the full size
of the object.  We expect nobody to notice this change.  It was necessary
to avoid issues with Cython.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25271">gh-25271</a>)</p>
</section>
<section id="required-changes-for-custom-legacy-user-dtypes">
<h3>Required changes for custom legacy user dtypes<a href="#required-changes-for-custom-legacy-user-dtypes" title="Link to this heading">#</a></h3>
<p>In order to improve our DTypes it is unfortunately necessary
to break the ABI, which requires some changes for dtypes registered
with <code><span>PyArray_RegisterDataType</span></code>.
Please see the documentation of <code><span>PyArray_RegisterDataType</span></code> for how
to adapt your code and achieve compatibility with both 1.x and 2.x.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25792">gh-25792</a>)</p>
</section>
<section id="new-public-dtype-api">
<h3>New Public DType API<a href="#new-public-dtype-api" title="Link to this heading">#</a></h3>
<p>The C implementation of the NEP 42 DType API is now public. While the DType API
has shipped in NumPy for a few versions, it was only usable in sessions with a
special environment variable set. It is now possible to write custom DTypes
outside of NumPy using the new DType API and the normal <code><span>import_array()</span></code>
mechanism for importing the numpy C API.</p>
<p>See <a href="https://numpy.org/devdocs/reference/c-api/array.html#dtype-api"><span>Custom Data Types</span></a> for more details about the API. As always with a new
feature, please report any bugs you run into implementing or using a new
DType. It is likely that downstream C code that works with dtypes will need to
be updated to work correctly with new DTypes.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25754">gh-25754</a>)</p>
</section>
<section id="new-c-api-import-functions">
<h3>New C-API import functions<a href="#new-c-api-import-functions" title="Link to this heading">#</a></h3>
<p>We have now added <code><span>PyArray_ImportNumPyAPI</span></code> and <code><span>PyUFunc_ImportUFuncAPI</span></code>
as static inline functions to import the NumPy C-API tables.
The new functions have two advantages over <code><span>import_array</span></code> and
<code><span>import_ufunc</span></code>:</p>
<ul>
<li><p>They check whether the import was already performed and are light-weight
if not, allowing to add them judiciously (although this is not preferable
in most cases).</p></li>
<li><p>The old mechanisms were macros rather than functions which included a
<code><span>return</span></code> statement.</p></li>
</ul>
<p>The <code><span>PyArray_ImportNumPyAPI()</span></code> function is included in <code><span>npy_2_compat.h</span></code>
for simpler backporting.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25866">gh-25866</a>)</p>
</section>
<section id="structured-dtype-information-access-through-functions">
<h3>Structured dtype information access through functions<a href="#structured-dtype-information-access-through-functions" title="Link to this heading">#</a></h3>
<p>The dtype structures fields <code><span>c_metadata</span></code>, <code><span>names</span></code>,
<code><span>fields</span></code>, and <code><span>subarray</span></code> must now be accessed through new
functions following the same names, such as <code><span>PyDataType_NAMES</span></code>.
Direct access of the fields is not valid as they do not exist for
all <code><span>PyArray_Descr</span></code> instances.
The <code><span>metadata</span></code> field is kept, but the macro version should also be preferred.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25802">gh-25802</a>)</p>
</section>
<section id="descriptor-elsize-and-alignment-access">
<h3>Descriptor <code><span>elsize</span></code> and <code><span>alignment</span></code> access<a href="#descriptor-elsize-and-alignment-access" title="Link to this heading">#</a></h3>
<p>Unless compiling only with NumPy 2 support, the <code><span>elsize</span></code> and <code><span>aligment</span></code>
fields must now be accessed via <code><span>PyDataType_ELSIZE</span></code>,
<code><span>PyDataType_SET_ELSIZE</span></code>, and <code><span>PyDataType_ALIGNMENT</span></code>.
In cases where the descriptor is attached to an array, we advise
using <code><span>PyArray_ITEMSIZE</span></code> as it exists on all NumPy versions.
Please see <a href="https://numpy.org/devdocs/numpy_2_0_migration_guide.html#migration-c-descr"><span>The PyArray_Descr struct has been changed</span></a> for more information.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25943">gh-25943</a>)</p>
</section>
</section>
<section id="numpy-2-0-c-api-removals">
<h2>NumPy 2.0 C API removals<a href="#numpy-2-0-c-api-removals" title="Link to this heading">#</a></h2>
<ul>
<li><p><code><span>npy_interrupt.h</span></code> and the corresponding macros like <code><span>NPY_SIGINT_ON</span></code>
have been removed.  We recommend querying <code><span>PyErr_CheckSignals()</span></code> or
<code><span>PyOS_InterruptOccurred()</span></code> periodically (these do currently require
holding the GIL though).</p></li>
<li><p>The <code><span>noprefix.h</span></code> header has been removed. Replace missing symbols with
their prefixed counterparts (usually an added <code><span>NPY_</span></code> or <code><span>npy_</span></code>).</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23919">gh-23919</a>)</p>
</li>
<li><p><code><span>PyUFunc_GetPyVals</span></code>, <code><span>PyUFunc_handlefperr</span></code>, and <code><span>PyUFunc_checkfperr</span></code>
have been removed.
If needed, a new backwards compatible function to raise floating point errors
could be restored. Reason for removal: there are no known users and the
functions would have made <code><span>with</span> <span>np.errstate()</span></code> fixes much more difficult).</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23922">gh-23922</a>)</p>
</li>
<li><p>The <code><span>numpy/old_defines.h</span></code> which was part of the API deprecated since NumPy 1.7
has been removed.  This removes macros of the form <code><span>PyArray_CONSTANT</span></code>.
The <a href="https://github.com/numpy/numpy/blob/main/tools/replace_old_macros.sed">replace_old_macros.sed</a>
script may be useful to convert them to the <code><span>NPY_CONSTANT</span></code> version.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24011">gh-24011</a>)</p>
</li>
<li><p>The <code><span>legacy_inner_loop_selector</span></code> member of the ufunc struct is removed
to simplify improvements to the dispatching system.
There are no known users overriding or directly accessing this member.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24271">gh-24271</a>)</p>
</li>
<li><p><code><span>NPY_INTPLTR</span></code> has been removed to avoid confusion (see <code><span>intp</span></code>
redefinition).</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24888">gh-24888</a>)</p>
</li>
<li><p>The advanced indexing <code><span>MapIter</span></code> and related API has been removed.
The (truly) public part of it was not well tested and had only one
known user (Theano).  Making it private will simplify improvements
to speed up <code><span>ufunc.at</span></code>, make advanced indexing more maintainable,
and was important for increasing the maximum number of dimensions of arrays
to 64. Please let us know if this API is important to you so we can find a
solution together.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25138">gh-25138</a>)</p>
</li>
<li><p>The <code><span>NPY_MAX_ELSIZE</span></code> macro has been removed, as it only ever reflected
builtin numeric types and served no internal purpose.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25149">gh-25149</a>)</p>
</li>
<li><p><code><span>PyArray_REFCNT</span></code> and <code><span>NPY_REFCOUNT</span></code> are removed. Use <code><span>Py_REFCNT</span></code> instead.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25156">gh-25156</a>)</p>
</li>
<li><p><code><span>PyArrayFlags_Type</span></code> and <code><span>PyArray_NewFlagsObject</span></code> as well as
<code><span>PyArrayFlagsObject</span></code> are private now.
There is no known use-case; use the Python API if needed.</p></li>
<li><p><code><span>PyArray_MoveInto</span></code>, <code><span>PyArray_CastTo</span></code>, <code><span>PyArray_CastAnyTo</span></code> are removed
use <code><span>PyArray_CopyInto</span></code> and if absolutely needed <code><span>PyArray_CopyAnyInto</span></code>
(the latter does a flat copy).</p></li>
<li><p><code><span>PyArray_FillObjectArray</span></code> is removed, its only true use is for
implementing <code><span>np.empty</span></code>.  Create a new empty array or use
<code><span>PyArray_FillWithScalar()</span></code> (decrefs existing objects).</p></li>
<li><p><code><span>PyArray_CompareUCS4</span></code> and <code><span>PyArray_CompareString</span></code> are removed.
Use the standard C string comparison functions.</p></li>
<li><p><code><span>PyArray_ISPYTHON</span></code> is removed as it is misleading, has no known
use-cases, and is easy to replace.</p></li>
<li><p><code><span>PyArray_FieldNames</span></code> is removed, as it is unclear what it would
be useful for.  It also has incorrect semantics in some possible
use-cases.</p></li>
<li><p><code><span>PyArray_TypestrConvert</span></code> is removed, since it seems a misnomer and unlikely
to be used by anyone.  If you know the size or are limited to few types, just
use it explicitly, otherwise go via Python strings.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25292">gh-25292</a>)</p>
</li>
<li><p><code><span>PyDataType_GetDatetimeMetaData</span></code> has been removed, it did not actually
do anything since at least NumPy 1.7.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25802">gh-25802</a>)</p>
</li>
</ul>
<section id="pyarray-getcastfunc-was-removed">
<h3><code><span>PyArray_GetCastFunc</span></code> was removed<a href="#pyarray-getcastfunc-was-removed" title="Link to this heading">#</a></h3>
<p>Note that custom legacy user dtypes can still provide a castfunc
as their implementation, but any access to them is now removed.
The reason for this is that NumPy never used these internally
for many years.
If you use simple numeric types, please just use C casts directly.
In case you require an alternative, please let us know so we can
create new API such as <code><span>PyArray_CastBuffer()</span></code> which could
use old or new cast functions depending on the NumPy version.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25161">gh-25161</a>)</p>
</section>
</section>
<section id="new-features">
<h2>New Features<a href="#new-features" title="Link to this heading">#</a></h2>
<ul>
<li><p><code><span>np.add</span></code> was extended to work with <code><span>unicode</span></code> and <code><span>bytes</span></code> dtypes.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24858">gh-24858</a>)</p>
</li>
</ul>
<section id="a-new-bitwise-count-function">
<h3>A new <code><span>bitwise_count</span></code> function<a href="#a-new-bitwise-count-function" title="Link to this heading">#</a></h3>
<p>This new function counts the number of 1-bits in a number.
<a href="https://numpy.org/devdocs/reference/generated/numpy.bitwise_count.html#numpy.bitwise_count" title="numpy.bitwise_count"><code><span>bitwise_count</span></code></a> works on all the numpy integer types and
integer-like objects.</p>
<div><pre><span></span><span>&gt;&gt;&gt; </span><span>a</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>2</span><span>**</span><span>i</span> <span>-</span> <span>1</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>16</span><span>)])</span>
<span>&gt;&gt;&gt; </span><span>np</span><span>.</span><span>bitwise_count</span><span>(</span><span>a</span><span>)</span>
<span>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],</span>
<span>      dtype=uint8)</span>
</pre></div>
<p>(<a href="https://github.com/numpy/numpy/pull/19355">gh-19355</a>)</p>
</section>
<section id="macos-accelerate-support-including-the-ilp64">
<h3>macOS Accelerate support, including the ILP64<a href="#macos-accelerate-support-including-the-ilp64" title="Link to this heading">#</a></h3>
<p>Support for the updated Accelerate BLAS/LAPACK library, including ILP64 (64-bit
integer) support, in macOS 13.3 has been added. This brings arm64 support, and
significant performance improvements of up to 10x for commonly used linear
algebra operations. When Accelerate is selected at build time, or if no
explicit BLAS library selection is done, the 13.3+ version will automatically
be used if available.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24053">gh-24053</a>)</p>
<p>Binary wheels are also available. On macOS &gt;=14.0, users who install NumPy from
PyPI will get wheels built against Accelerate rather than OpenBLAS.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25255">gh-25255</a>)</p>
</section>
<section id="option-to-use-weights-for-quantile-and-percentile-functions">
<h3>Option to use weights for quantile and percentile functions<a href="#option-to-use-weights-for-quantile-and-percentile-functions" title="Link to this heading">#</a></h3>
<p>A <code><span>weights</span></code> keyword is now available for <a href="https://numpy.org/devdocs/reference/generated/numpy.quantile.html#numpy.quantile" title="numpy.quantile"><code><span>quantile</span></code></a>,
<a href="https://numpy.org/devdocs/reference/generated/numpy.percentile.html#numpy.percentile" title="numpy.percentile"><code><span>percentile</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.nanquantile.html#numpy.nanquantile" title="numpy.nanquantile"><code><span>nanquantile</span></code></a> and <a href="https://numpy.org/devdocs/reference/generated/numpy.nanpercentile.html#numpy.nanpercentile" title="numpy.nanpercentile"><code><span>nanpercentile</span></code></a>. Only
<code><span>method="inverted_cdf"</span></code> supports weights.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24254">gh-24254</a>)</p>
</section>
<section id="improved-cpu-optimization-tracking">
<h3>Improved CPU optimization tracking<a href="#improved-cpu-optimization-tracking" title="Link to this heading">#</a></h3>
<p>A new tracer mechanism is available which enables tracking of the enabled
targets for each optimized function (i.e., that uses hardware-specific SIMD
instructions) in the NumPy library. With this enhancement, it becomes possible
to precisely monitor the enabled CPU dispatch targets for the dispatched
functions.</p>
<p>A new function named <code><span>opt_func_info</span></code> has been added to the new namespace
<a href="https://numpy.org/devdocs/reference/generated/numpy.lib.introspect.html#module-numpy.lib.introspect" title="numpy.lib.introspect"><code><span>numpy.lib.introspect</span></code></a>, offering this tracing capability. This function allows
you to retrieve information about the enabled targets based on function names
and data type signatures.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24420">gh-24420</a>)</p>
</section>
<section id="a-new-meson-backend-for-f2py">
<h3>A new Meson backend for <code><span>f2py</span></code><a href="#a-new-meson-backend-for-f2py" title="Link to this heading">#</a></h3>
<p><code><span>f2py</span></code> in compile mode (i.e. <code><span>f2py</span> <span>-c</span></code>) now accepts the <code><span>--backend</span> <span>meson</span></code>
option. This is the default option for Python &gt;=3.12. For older Python versions,
<code><span>f2py</span></code> will still default to <code><span>--backend</span> <span>distutils</span></code>.</p>
<p>To support this in realistic use-cases, in compile mode <code><span>f2py</span></code> takes a
<code><span>--dep</span></code> flag one or many times which maps to <code><span>dependency()</span></code> calls in the
<code><span>meson</span></code> backend, and does nothing in the <code><span>distutils</span></code> backend.</p>
<p>There are no changes for users of <code><span>f2py</span></code> only as a code generator, i.e. without <code><span>-c</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24532">gh-24532</a>)</p>
</section>
<section id="bind-c-support-for-f2py">
<h3><code><span>bind(c)</span></code> support for <code><span>f2py</span></code><a href="#bind-c-support-for-f2py" title="Link to this heading">#</a></h3>
<p>Both functions and subroutines can be annotated with <code><span>bind(c)</span></code>. <code><span>f2py</span></code> will
handle both the correct type mapping, and preserve the unique label for other
C interfaces.</p>
<p><strong>Note:</strong> <code><span>bind(c,</span> <span>name</span> <span>=</span> <span>'routine_name_other_than_fortran_routine')</span></code> is not
honored by the <code><span>f2py</span></code> bindings by design, since <code><span>bind(c)</span></code> with the <code><span>name</span></code>
is meant to guarantee only the same name in C and Fortran, not in Python and
Fortran.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24555">gh-24555</a>)</p>
</section>
<section id="a-new-strict-option-for-several-testing-functions">
<h3>A new <code><span>strict</span></code> option for several testing functions<a href="#a-new-strict-option-for-several-testing-functions" title="Link to this heading">#</a></h3>
<p>The <code><span>strict</span></code> keyword is now available for <a href="https://numpy.org/devdocs/reference/generated/numpy.testing.assert_allclose.html#numpy.testing.assert_allclose" title="numpy.testing.assert_allclose"><code><span>assert_allclose</span></code></a>,
<a href="https://numpy.org/devdocs/reference/generated/numpy.testing.assert_equal.html#numpy.testing.assert_equal" title="numpy.testing.assert_equal"><code><span>assert_equal</span></code></a>, and <a href="https://numpy.org/devdocs/reference/generated/numpy.testing.assert_array_less.html#numpy.testing.assert_array_less" title="numpy.testing.assert_array_less"><code><span>assert_array_less</span></code></a>.
Setting <code><span>strict=True</span></code> will disable the broadcasting behaviour for scalars
and ensure that input arrays have the same data type.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24680">gh-24680</a>,
<a href="https://github.com/numpy/numpy/pull/24770">gh-24770</a>,
<a href="https://github.com/numpy/numpy/pull/24775">gh-24775</a>)</p>
</section>
<section id="add-np-core-umath-find-and-np-core-umath-rfind-ufuncs">
<h3>Add <code><span>np.core.umath.find</span></code> and <code><span>np.core.umath.rfind</span></code> UFuncs<a href="#add-np-core-umath-find-and-np-core-umath-rfind-ufuncs" title="Link to this heading">#</a></h3>
<p>Add two <code><span>find</span></code> and <code><span>rfind</span></code> UFuncs that operate on unicode or byte strings
and are used in <code><span>np.char</span></code>. They operate similar to <code><span>str.find</span></code> and
<code><span>str.rfind</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24868">gh-24868</a>)</p>
</section>
<section id="diagonal-and-trace-for-numpy-linalg">
<h3><code><span>diagonal</span></code> and <code><span>trace</span></code> for <a href="https://numpy.org/devdocs/reference/routines.linalg.html#module-numpy.linalg" title="numpy.linalg"><code><span>numpy.linalg</span></code></a><a href="#diagonal-and-trace-for-numpy-linalg" title="Link to this heading">#</a></h3>
<p><a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.diagonal.html#numpy.linalg.diagonal" title="numpy.linalg.diagonal"><code><span>numpy.linalg.diagonal</span></code></a> and <a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.trace.html#numpy.linalg.trace" title="numpy.linalg.trace"><code><span>numpy.linalg.trace</span></code></a> have been
added, which are array API standard-compatible variants of <a href="https://numpy.org/devdocs/reference/generated/numpy.diagonal.html#numpy.diagonal" title="numpy.diagonal"><code><span>numpy.diagonal</span></code></a> and
<a href="https://numpy.org/devdocs/reference/generated/numpy.trace.html#numpy.trace" title="numpy.trace"><code><span>numpy.trace</span></code></a>. They differ in the default axis selection which define 2-D
sub-arrays.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24887">gh-24887</a>)</p>
</section>
<section id="new-long-and-ulong-dtypes">
<h3>New <code><span>long</span></code> and <code><span>ulong</span></code> dtypes<a href="#new-long-and-ulong-dtypes" title="Link to this heading">#</a></h3>
<p><a href="https://numpy.org/devdocs/reference/arrays.scalars.html#numpy.long" title="numpy.long"><code><span>numpy.long</span></code></a> and <a href="https://numpy.org/devdocs/reference/arrays.scalars.html#numpy.ulong" title="numpy.ulong"><code><span>numpy.ulong</span></code></a> have been added as NumPy integers mapping
to C’s <code><span>long</span></code> and <code><span>unsigned</span> <span>long</span></code>. Prior to NumPy 1.24, <code><span>numpy.long</span></code> was
an alias to Python’s <code><span>int</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24922">gh-24922</a>)</p>
</section>
<section id="svdvals-for-numpy-linalg">
<h3><code><span>svdvals</span></code> for <a href="https://numpy.org/devdocs/reference/routines.linalg.html#module-numpy.linalg" title="numpy.linalg"><code><span>numpy.linalg</span></code></a><a href="#svdvals-for-numpy-linalg" title="Link to this heading">#</a></h3>
<p><a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.svdvals.html#numpy.linalg.svdvals" title="numpy.linalg.svdvals"><code><span>numpy.linalg.svdvals</span></code></a> has been added. It computes singular values for
(a stack of) matrices. Executing <code><span>np.svdvals(x)</span></code> is the same as calling
<code><span>np.svd(x,</span> <span>compute_uv=False,</span> <span>hermitian=False)</span></code>.
This function is compatible with the array API standard.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24940">gh-24940</a>)</p>
</section>
<section id="a-new-isdtype-function">
<h3>A new <code><span>isdtype</span></code> function<a href="#a-new-isdtype-function" title="Link to this heading">#</a></h3>
<p><a href="https://numpy.org/devdocs/reference/generated/numpy.isdtype.html#numpy.isdtype" title="numpy.isdtype"><code><span>numpy.isdtype</span></code></a> was added to provide a canonical way to classify NumPy’s dtypes
in compliance with the array API standard.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25054">gh-25054</a>)</p>
</section>
<section id="a-new-astype-function">
<h3>A new <code><span>astype</span></code> function<a href="#a-new-astype-function" title="Link to this heading">#</a></h3>
<p><a href="https://numpy.org/devdocs/reference/generated/numpy.astype.html#numpy.astype" title="numpy.astype"><code><span>numpy.astype</span></code></a> was added to provide an array API standard-compatible
alternative to the <a href="https://numpy.org/devdocs/reference/generated/numpy.ndarray.astype.html#numpy.ndarray.astype" title="numpy.ndarray.astype"><code><span>numpy.ndarray.astype</span></code></a> method.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25079">gh-25079</a>)</p>
</section>
<section id="array-api-compatible-functions-aliases">
<h3>Array API compatible functions’ aliases<a href="#array-api-compatible-functions-aliases" title="Link to this heading">#</a></h3>
<p>13 aliases for existing functions were added to improve compatibility with the array API standard:</p>
<ul>
<li><p>Trigonometry: <code><span>acos</span></code>, <code><span>acosh</span></code>, <code><span>asin</span></code>, <code><span>asinh</span></code>, <code><span>atan</span></code>, <code><span>atanh</span></code>, <code><span>atan2</span></code>.</p></li>
<li><p>Bitwise: <code><span>bitwise_left_shift</span></code>, <code><span>bitwise_invert</span></code>, <code><span>bitwise_right_shift</span></code>.</p></li>
<li><p>Misc: <code><span>concat</span></code>, <code><span>permute_dims</span></code>, <code><span>pow</span></code>.</p></li>
<li><p>In <code><span>numpy.linalg</span></code>: <code><span>tensordot</span></code>, <code><span>matmul</span></code>.</p></li>
</ul>
<p>(<a href="https://github.com/numpy/numpy/pull/25086">gh-25086</a>)</p>
</section>
<section id="new-unique-functions">
<h3>New <code><span>unique_*</span></code> functions<a href="#new-unique-functions" title="Link to this heading">#</a></h3>
<p>The <a href="https://numpy.org/devdocs/reference/generated/numpy.unique_all.html#numpy.unique_all" title="numpy.unique_all"><code><span>unique_all</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.unique_counts.html#numpy.unique_counts" title="numpy.unique_counts"><code><span>unique_counts</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.unique_inverse.html#numpy.unique_inverse" title="numpy.unique_inverse"><code><span>unique_inverse</span></code></a>,
and <a href="https://numpy.org/devdocs/reference/generated/numpy.unique_values.html#numpy.unique_values" title="numpy.unique_values"><code><span>unique_values</span></code></a> functions have been added. They provide
functionality of <a href="https://numpy.org/devdocs/reference/generated/numpy.unique.html#numpy.unique" title="numpy.unique"><code><span>unique</span></code></a> with different sets of flags. They are array API
standard-compatible, and because the number of arrays they return does not
depend on the values of input arguments, they are easier to target for JIT
compilation.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25088">gh-25088</a>)</p>
</section>
<section id="matrix-transpose-support-for-ndarrays">
<h3>Matrix transpose support for ndarrays<a href="#matrix-transpose-support-for-ndarrays" title="Link to this heading">#</a></h3>
<p>NumPy now offers support for calculating the matrix transpose of an array (or
stack of arrays). The matrix transpose is equivalent to swapping the last two
axes of an array. Both <code><span>np.ndarray</span></code> and <code><span>np.ma.MaskedArray</span></code> now expose a
<code><span>.mT</span></code> attribute, and there is a matching new <a href="https://numpy.org/devdocs/reference/generated/numpy.matrix_transpose.html#numpy.matrix_transpose" title="numpy.matrix_transpose"><code><span>numpy.matrix_transpose</span></code></a>
function.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23762">gh-23762</a>)</p>
</section>
<section id="array-api-compatible-functions-for-numpy-linalg">
<h3>Array API compatible functions for <code><span>numpy.linalg</span></code><a href="#array-api-compatible-functions-for-numpy-linalg" title="Link to this heading">#</a></h3>
<p>Six new functions and two aliases were added to improve compatibility with
the Array API standard for <a href="https://numpy.org/devdocs/reference/routines.linalg.html#module-numpy.linalg" title="numpy.linalg"><code><span>numpy.linalg</span></code></a>:</p>
<ul>
<li><p><a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.matrix_norm.html#numpy.linalg.matrix_norm" title="numpy.linalg.matrix_norm"><code><span>numpy.linalg.matrix_norm</span></code></a> - Computes the matrix norm of a matrix (or a stack of matrices).</p></li>
<li><p><a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.vector_norm.html#numpy.linalg.vector_norm" title="numpy.linalg.vector_norm"><code><span>numpy.linalg.vector_norm</span></code></a> - Computes the vector norm of a vector (or batch of vectors).</p></li>
<li><p><a href="https://numpy.org/devdocs/reference/generated/numpy.vecdot.html#numpy.vecdot" title="numpy.vecdot"><code><span>numpy.vecdot</span></code></a> - Computes the (vector) dot product of two arrays.</p></li>
<li><p><a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.vecdot.html#numpy.linalg.vecdot" title="numpy.linalg.vecdot"><code><span>numpy.linalg.vecdot</span></code></a> - An alias for <a href="https://numpy.org/devdocs/reference/generated/numpy.vecdot.html#numpy.vecdot" title="numpy.vecdot"><code><span>numpy.vecdot</span></code></a>.</p></li>
<li><p><a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.matrix_transpose.html#numpy.linalg.matrix_transpose" title="numpy.linalg.matrix_transpose"><code><span>numpy.linalg.matrix_transpose</span></code></a> - An alias for <a href="https://numpy.org/devdocs/reference/generated/numpy.matrix_transpose.html#numpy.matrix_transpose" title="numpy.matrix_transpose"><code><span>numpy.matrix_transpose</span></code></a>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25155">gh-25155</a>)</p>
</li>
<li><p><a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.outer.html#numpy.linalg.outer" title="numpy.linalg.outer"><code><span>numpy.linalg.outer</span></code></a> has been added. It computes the outer product of two
vectors. It differs from <a href="https://numpy.org/devdocs/reference/generated/numpy.outer.html#numpy.outer" title="numpy.outer"><code><span>numpy.outer</span></code></a> by accepting one-dimensional arrays
only. This function is compatible with the array API standard.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25101">gh-25101</a>)</p>
</li>
<li><p><a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.cross.html#numpy.linalg.cross" title="numpy.linalg.cross"><code><span>numpy.linalg.cross</span></code></a> has been added. It computes the cross product of two
(arrays of) 3-dimensional vectors. It differs from <a href="https://numpy.org/devdocs/reference/generated/numpy.cross.html#numpy.cross" title="numpy.cross"><code><span>numpy.cross</span></code></a> by accepting
three-dimensional vectors only. This function is compatible with the array
API standard.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25145">gh-25145</a>)</p>
</li>
</ul>
</section>
<section id="a-correction-argument-for-var-and-std">
<h3>A <code><span>correction</span></code> argument for <code><span>var</span></code> and <code><span>std</span></code><a href="#a-correction-argument-for-var-and-std" title="Link to this heading">#</a></h3>
<p>A <code><span>correction</span></code> argument was added to <a href="https://numpy.org/devdocs/reference/generated/numpy.var.html#numpy.var" title="numpy.var"><code><span>var</span></code></a> and <a href="https://numpy.org/devdocs/reference/generated/numpy.std.html#numpy.std" title="numpy.std"><code><span>std</span></code></a>, which is
an array API standard compatible alternative to <code><span>ddof</span></code>. As both arguments
serve a similar purpose, only one of them can be provided at the same time.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25169">gh-25169</a>)</p>
</section>
<section id="ndarray-device-and-ndarray-to-device">
<h3><code><span>ndarray.device</span></code> and <code><span>ndarray.to_device</span></code><a href="#ndarray-device-and-ndarray-to-device" title="Link to this heading">#</a></h3>
<p>An <code><span>ndarray.device</span></code> attribute and <code><span>ndarray.to_device</span></code> method were
added to <code><span>numpy.ndarray</span></code> for array API standard compatibility.</p>
<p>Additionally, <code><span>device</span></code> keyword-only arguments were added to:
<a href="https://numpy.org/devdocs/reference/generated/numpy.asarray.html#numpy.asarray" title="numpy.asarray"><code><span>asarray</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><code><span>arange</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.empty.html#numpy.empty" title="numpy.empty"><code><span>empty</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.empty_like.html#numpy.empty_like" title="numpy.empty_like"><code><span>empty_like</span></code></a>,
<a href="https://numpy.org/devdocs/reference/generated/numpy.eye.html#numpy.eye" title="numpy.eye"><code><span>eye</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.full.html#numpy.full" title="numpy.full"><code><span>full</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.full_like.html#numpy.full_like" title="numpy.full_like"><code><span>full_like</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace"><code><span>linspace</span></code></a>,
<a href="https://numpy.org/devdocs/reference/generated/numpy.ones.html#numpy.ones" title="numpy.ones"><code><span>ones</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.ones_like.html#numpy.ones_like" title="numpy.ones_like"><code><span>ones_like</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.zeros.html#numpy.zeros" title="numpy.zeros"><code><span>zeros</span></code></a>, and <a href="https://numpy.org/devdocs/reference/generated/numpy.zeros_like.html#numpy.zeros_like" title="numpy.zeros_like"><code><span>zeros_like</span></code></a>.</p>
<p>For all these new arguments, only <code><span>device="cpu"</span></code> is supported.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25233">gh-25233</a>)</p>
</section>
<section id="stringdtype-has-been-added-to-numpy">
<h3>StringDType has been added to NumPy<a href="#stringdtype-has-been-added-to-numpy" title="Link to this heading">#</a></h3>
<p>We have added a new variable-width UTF-8 encoded string data type, implementing
a “NumPy array of Python strings”, including support for a user-provided missing
data sentinel. It is intended as a drop-in replacement for arrays of Python
strings and missing data sentinels using the object dtype. See <a href="https://numpy.org/neps/nep-0055-string_dtype.html">NEP 55</a> and <a href="https://numpy.org/devdocs/user/basics.strings.html#stringdtype"><span>the
documentation</span></a> for more details.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25347">gh-25347</a>)</p>
</section>
<section id="new-keywords-for-cholesky-and-pinv">
<h3>New keywords for <code><span>cholesky</span></code> and <code><span>pinv</span></code><a href="#new-keywords-for-cholesky-and-pinv" title="Link to this heading">#</a></h3>
<p>The <code><span>upper</span></code> and <code><span>rtol</span></code> keywords were added to <a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.cholesky.html#numpy.linalg.cholesky" title="numpy.linalg.cholesky"><code><span>numpy.linalg.cholesky</span></code></a> and
<a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.pinv.html#numpy.linalg.pinv" title="numpy.linalg.pinv"><code><span>numpy.linalg.pinv</span></code></a>, respectively, to improve array API standard compatibility.</p>
<p>For <a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.pinv.html#numpy.linalg.pinv" title="numpy.linalg.pinv"><code><span>pinv</span></code></a>, if neither <code><span>rcond</span></code> nor <code><span>rtol</span></code> is specified,
the <code><span>rcond</span></code>’s default is used. We plan to deprecate and remove <code><span>rcond</span></code> in
the future.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25388">gh-25388</a>)</p>
</section>
<section id="new-keywords-for-sort-argsort-and-linalg-matrix-rank">
<h3>New keywords for <code><span>sort</span></code>, <code><span>argsort</span></code> and <code><span>linalg.matrix_rank</span></code><a href="#new-keywords-for-sort-argsort-and-linalg-matrix-rank" title="Link to this heading">#</a></h3>
<p>New keyword parameters were added to improve array API standard compatibility:</p>
<ul>
<li><p><code><span>rtol</span></code> was added to <a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.matrix_rank.html#numpy.linalg.matrix_rank" title="numpy.linalg.matrix_rank"><code><span>matrix_rank</span></code></a>.</p></li>
<li><p><code><span>stable</span></code> was added to <a href="https://numpy.org/devdocs/reference/generated/numpy.sort.html#numpy.sort" title="numpy.sort"><code><span>sort</span></code></a> and <a href="https://numpy.org/devdocs/reference/generated/numpy.argsort.html#numpy.argsort" title="numpy.argsort"><code><span>argsort</span></code></a>.</p></li>
</ul>
<p>(<a href="https://github.com/numpy/numpy/pull/25437">gh-25437</a>)</p>
</section>
<section id="new-numpy-strings-namespace-for-string-ufuncs">
<h3>New <code><span>numpy.strings</span></code> namespace for string ufuncs<a href="#new-numpy-strings-namespace-for-string-ufuncs" title="Link to this heading">#</a></h3>
<p>NumPy now implements some string operations as ufuncs. The old <code><span>np.char</span></code>
namespace is still available, and where possible the string manipulation
functions in that namespace have been updated to use the new ufuncs,
substantially improving their performance.</p>
<p>Where possible, we suggest updating code to use functions in <code><span>np.strings</span></code>
instead of <code><span>np.char</span></code>. In the future we may deprecate <code><span>np.char</span></code> in favor of
<code><span>np.strings</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25463">gh-25463</a>)</p>
</section>
<section id="numpy-fft-support-for-different-precisions-and-in-place-calculations">
<h3><a href="https://numpy.org/devdocs/reference/routines.fft.html#module-numpy.fft" title="numpy.fft"><code><span>numpy.fft</span></code></a> support for different precisions and in-place calculations<a href="#numpy-fft-support-for-different-precisions-and-in-place-calculations" title="Link to this heading">#</a></h3>
<p>The various FFT routines in <a href="https://numpy.org/devdocs/reference/routines.fft.html#module-numpy.fft" title="numpy.fft"><code><span>numpy.fft</span></code></a> now do their calculations natively in
float, double, or long double precision, depending on the input precision,
instead of always calculating in double precision. Hence, the calculation will
now be less precise for single and more precise for long double precision.
The data type of the output array will now be adjusted accordingly.</p>
<p>Furthermore, all FFT routines have gained an <code><span>out</span></code> argument that can be used
for in-place calculations.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25536">gh-25536</a>)</p>
</section>
<section id="configtool-and-pkg-config-support">
<h3>configtool and pkg-config support<a href="#configtool-and-pkg-config-support" title="Link to this heading">#</a></h3>
<p>A new <code><span>numpy-config</span></code> CLI script is available that can be queried for the
NumPy version and for compile flags needed to use the NumPy C API. This will
allow build systems to better support the use of NumPy as a dependency.
Also, a <code><span>numpy.pc</span></code> pkg-config file is now included with Numpy. In order to
find its location for use with <code><span>PKG_CONFIG_PATH</span></code>, use
<code><span>numpy-config</span> <span>--pkgconfigdir</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25730">gh-25730</a>)</p>
</section>
<section id="array-api-standard-support-in-the-main-namespace">
<h3>Array API standard support in the main namespace<a href="#array-api-standard-support-in-the-main-namespace" title="Link to this heading">#</a></h3>
<p>The main <code><span>numpy</span></code> namespace now supports the array API standard. See
<a href="https://numpy.org/devdocs/reference/array_api.html#array-api-standard-compatibility"><span>Array API standard compatibility</span></a> for details.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25911">gh-25911</a>)</p>
</section>
</section>
<section id="improvements">
<h2>Improvements<a href="#improvements" title="Link to this heading">#</a></h2>
<ul>
<li><p>Strings are now supported by <code><span>any</span></code>, <code><span>all</span></code>, and the logical ufuncs.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25651">gh-25651</a>)</p>
</li>
</ul>
<section id="integer-sequences-as-the-shape-argument-for-memmap">
<h3>Integer sequences as the shape argument for <code><span>memmap</span></code><a href="#integer-sequences-as-the-shape-argument-for-memmap" title="Link to this heading">#</a></h3>
<p><a href="https://numpy.org/devdocs/reference/generated/numpy.memmap.html#numpy.memmap" title="numpy.memmap"><code><span>numpy.memmap</span></code></a> can now be created with any integer sequence as the <code><span>shape</span></code>
argument, such as a list or numpy array of integers. Previously, only the
types of tuple and int could be used without raising an error.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23729">gh-23729</a>)</p>
</section>
<section id="errstate-is-now-faster-and-context-safe">
<h3><code><span>errstate</span></code> is now faster and context safe<a href="#errstate-is-now-faster-and-context-safe" title="Link to this heading">#</a></h3>
<p>The <a href="https://numpy.org/devdocs/reference/generated/numpy.errstate.html#numpy.errstate" title="numpy.errstate"><code><span>numpy.errstate</span></code></a> context manager/decorator is now faster and
safer.  Previously, it was not context safe and had (rare)
issues with thread-safety.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23936">gh-23936</a>)</p>
</section>
<section id="aarch64-quicksort-speed-improved-by-using-highway-s-vqsort">
<h3>AArch64 quicksort speed improved by using Highway’s VQSort<a href="#aarch64-quicksort-speed-improved-by-using-highway-s-vqsort" title="Link to this heading">#</a></h3>
<p>The first introduction of the Google Highway library, using VQSort on AArch64.
Execution time is improved by up to 16x in some cases, see the PR for benchmark
results. Extensions to other platforms will be done in the future.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24018">gh-24018</a>)</p>
</section>
<section id="complex-types-underlying-c-type-changes">
<h3>Complex types - underlying C type changes<a href="#complex-types-underlying-c-type-changes" title="Link to this heading">#</a></h3>
<ul>
<li><p>The underlying C types for all of NumPy’s complex types have been changed to
use C99 complex types.</p></li>
<li><p>While this change does not affect the memory layout of complex types, it
changes the API to be used to directly retrieve or write the real or
complex part of the complex number, since direct field access (as in <code><span>c.real</span></code>
or <code><span>c.imag</span></code>) is no longer an option. You can now use utilities provided in
<code><span>numpy/npy_math.h</span></code> to do these operations, like this:</p>
<div><pre><span></span><span>npy_cdouble</span><span> </span><span>c</span><span>;</span>
<span>npy_csetreal</span><span>(</span><span>&amp;</span><span>c</span><span>,</span><span> </span><span>1.0</span><span>);</span>
<span>npy_csetimag</span><span>(</span><span>&amp;</span><span>c</span><span>,</span><span> </span><span>0.0</span><span>);</span>
<span>printf</span><span>(</span><span>"%d + %di</span><span>\n</span><span>"</span><span>,</span><span> </span><span>npy_creal</span><span>(</span><span>c</span><span>),</span><span> </span><span>npy_cimag</span><span>(</span><span>c</span><span>));</span>
</pre></div>
</li>
<li><p>To ease cross-version compatibility, equivalent macros and a compatibility
layer have been added which can be used by downstream packages to continue
to support both NumPy 1.x and 2.x. See <a href="https://numpy.org/devdocs/reference/c-api/coremath.html#complex-numbers"><span>Support for complex numbers</span></a> for more info.</p></li>
<li><p><code><span>numpy/npy_common.h</span></code> now includes <code><span>complex.h</span></code>, which means that <code><span>complex</span></code>
is now a reserved keyword.</p></li>
</ul>
<p>(<a href="https://github.com/numpy/numpy/pull/24085">gh-24085</a>)</p>
</section>
<section id="iso-c-binding-support-and-improved-common-blocks-for-f2py">
<h3><code><span>iso_c_binding</span></code> support and improved common blocks for <code><span>f2py</span></code><a href="#iso-c-binding-support-and-improved-common-blocks-for-f2py" title="Link to this heading">#</a></h3>
<p>Previously, users would have to define their own custom <code><span>f2cmap</span></code> file to use
type mappings defined by the Fortran2003 <code><span>iso_c_binding</span></code> intrinsic module.
These type maps are now natively supported by <code><span>f2py</span></code></p>
<p>(<a href="https://github.com/numpy/numpy/pull/24555">gh-24555</a>)</p>
<p><code><span>f2py</span></code> now handles <code><span>common</span></code> blocks which have <code><span>kind</span></code> specifications from
modules. This further expands the usability of intrinsics like
<code><span>iso_fortran_env</span></code> and <code><span>iso_c_binding</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25186">gh-25186</a>)</p>
</section>
<section id="call-str-automatically-on-third-argument-to-functions-like-assert-equal">
<h3>Call <code><span>str</span></code> automatically on third argument to functions like <code><span>assert_equal</span></code><a href="#call-str-automatically-on-third-argument-to-functions-like-assert-equal" title="Link to this heading">#</a></h3>
<p>The third argument to functions like <a href="https://numpy.org/devdocs/reference/generated/numpy.testing.assert_equal.html#numpy.testing.assert_equal" title="numpy.testing.assert_equal"><code><span>assert_equal</span></code></a> now has
<code><span>str</span></code> called on it automatically. This way it mimics the built-in <code><span>assert</span></code>
statement, where <code><span>assert_equal(a,</span> <span>b,</span> <span>obj)</span></code> works like <code><span>assert</span> <span>a</span> <span>==</span> <span>b,</span> <span>obj</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24877">gh-24877</a>)</p>
</section>
<section id="support-for-array-like-atol-rtol-in-isclose-allclose">
<h3>Support for array-like <code><span>atol</span></code>/<code><span>rtol</span></code> in <code><span>isclose</span></code>, <code><span>allclose</span></code><a href="#support-for-array-like-atol-rtol-in-isclose-allclose" title="Link to this heading">#</a></h3>
<p>The keywords <code><span>atol</span></code> and <code><span>rtol</span></code> in <a href="https://numpy.org/devdocs/reference/generated/numpy.isclose.html#numpy.isclose" title="numpy.isclose"><code><span>isclose</span></code></a> and <a href="https://numpy.org/devdocs/reference/generated/numpy.allclose.html#numpy.allclose" title="numpy.allclose"><code><span>allclose</span></code></a>
now accept both scalars and arrays. An array, if given, must broadcast
to the shapes of the first two array arguments.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24878">gh-24878</a>)</p>
</section>
<section id="consistent-failure-messages-in-test-functions">
<h3>Consistent failure messages in test functions<a href="#consistent-failure-messages-in-test-functions" title="Link to this heading">#</a></h3>
<p>Previously, some <a href="https://numpy.org/devdocs/reference/routines.testing.html#module-numpy.testing" title="numpy.testing"><code><span>numpy.testing</span></code></a> assertions printed messages that
referred to the actual and desired results as <code><span>x</span></code> and <code><span>y</span></code>.
Now, these values are consistently referred to as <code><span>ACTUAL</span></code> and
<code><span>DESIRED</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24931">gh-24931</a>)</p>
</section>
<section id="n-d-fft-transforms-allow-s-i-1">
<h3>n-D FFT transforms allow <code><span>s[i]</span> <span>==</span> <span>-1</span></code><a href="#n-d-fft-transforms-allow-s-i-1" title="Link to this heading">#</a></h3>
<p>The <a href="https://numpy.org/devdocs/reference/generated/numpy.fft.fftn.html#numpy.fft.fftn" title="numpy.fft.fftn"><code><span>fftn</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.fft.ifftn.html#numpy.fft.ifftn" title="numpy.fft.ifftn"><code><span>ifftn</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.fft.rfftn.html#numpy.fft.rfftn" title="numpy.fft.rfftn"><code><span>rfftn</span></code></a>,
<a href="https://numpy.org/devdocs/reference/generated/numpy.fft.irfftn.html#numpy.fft.irfftn" title="numpy.fft.irfftn"><code><span>irfftn</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.fft.fft2.html#numpy.fft.fft2" title="numpy.fft.fft2"><code><span>fft2</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.fft.ifft2.html#numpy.fft.ifft2" title="numpy.fft.ifft2"><code><span>ifft2</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.fft.rfft2.html#numpy.fft.rfft2" title="numpy.fft.rfft2"><code><span>rfft2</span></code></a>
and <a href="https://numpy.org/devdocs/reference/generated/numpy.fft.irfft2.html#numpy.fft.irfft2" title="numpy.fft.irfft2"><code><span>irfft2</span></code></a> functions now use the whole input array along the axis
<code><span>i</span></code> if <code><span>s[i]</span> <span>==</span> <span>-1</span></code>, in line with the array API standard.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25495">gh-25495</a>)</p>
</section>
<section id="guard-pyarrayscalar-val-and-pyunicodescalarobject-for-the-limited-api">
<h3>Guard PyArrayScalar_VAL and PyUnicodeScalarObject for the limited API<a href="#guard-pyarrayscalar-val-and-pyunicodescalarobject-for-the-limited-api" title="Link to this heading">#</a></h3>
<p><code><span>PyUnicodeScalarObject</span></code> holds a <code><span>PyUnicodeObject</span></code>, which is not available
when using <code><span>Py_LIMITED_API</span></code>. Add guards to hide it and consequently also make
the <code><span>PyArrayScalar_VAL</span></code> macro hidden.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25531">gh-25531</a>)</p>
</section>
</section>
<section id="changes">
<h2>Changes<a href="#changes" title="Link to this heading">#</a></h2>
<ul>
<li><p><code><span>np.gradient()</span></code> now returns a tuple rather than a list making the
return value immutable.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23861">gh-23861</a>)</p>
</li>
<li><p>Being fully context and thread-safe, <code><span>np.errstate</span></code> can only
be entered once now.</p></li>
<li><p><code><span>np.setbufsize</span></code> is now tied to <code><span>np.errstate()</span></code>: leaving an
<code><span>np.errstate</span></code> context will also reset the <code><span>bufsize</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23936">gh-23936</a>)</p>
</li>
<li><p>A new public <code><span>np.lib.array_utils</span></code> submodule has been introduced and it
currently contains three functions: <code><span>byte_bounds</span></code> (moved from
<code><span>np.lib.utils</span></code>), <code><span>normalize_axis_tuple</span></code> and <code><span>normalize_axis_index</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24540">gh-24540</a>)</p>
</li>
<li><p>Introduce <a href="https://numpy.org/devdocs/reference/arrays.scalars.html#numpy.bool" title="numpy.bool"><code><span>numpy.bool</span></code></a> as the new canonical name for NumPy’s boolean dtype,
and make <a href="https://numpy.org/devdocs/reference/arrays.scalars.html#numpy.bool_" title="numpy.bool_"><code><span>numpy.bool_</span></code></a> an alias to it. Note that until NumPy 1.24,
<code><span>np.bool</span></code> was an alias to Python’s builtin <code><span>bool</span></code>. The new name helps
with array API standard compatibility and is a more intuitive name.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25080">gh-25080</a>)</p>
</li>
<li><p>The <code><span>dtype.flags</span></code> value was previously stored as a signed integer.
This means that the aligned dtype struct flag lead to negative flags being
set (-128 rather than 128). This flag is now stored unsigned (positive). Code
which checks flags manually may need to adapt.  This may include code
compiled with Cython 0.29.x.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25816">gh-25816</a>)</p>
</li>
</ul>
<section id="representation-of-numpy-scalars-changed">
<h3>Representation of NumPy scalars changed<a href="#representation-of-numpy-scalars-changed" title="Link to this heading">#</a></h3>
<p>As per <a href="https://numpy.org/neps/nep-0051-scalar-representation.html#nep51" title="(in NumPy Enhancement Proposals)"><span>NEP 51</span></a>, the scalar representation has been
updated to include the type information to avoid confusion with
Python scalars.</p>
<p>Scalars are now printed as <code><span>np.float64(3.0)</span></code> rather than just <code><span>3.0</span></code>.
This may disrupt workflows that store representations of numbers
(e.g., to files) making it harder to read them. They should be stored as
explicit strings, for example by using <code><span>str()</span></code> or <code><span>f"{scalar!s}"</span></code>.
For the time being, affected users can use <code><span>np.set_printoptions(legacy="1.25")</span></code>
to get the old behavior (with possibly a few exceptions).
Documentation of downstream projects may require larger updates,
if code snippets are tested.  We are working on tooling for
<a href="https://github.com/scientific-python/pytest-doctestplus/issues/107">doctest-plus</a>
to facilitate updates.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/22449">gh-22449</a>)</p>
</section>
<section id="truthiness-of-numpy-strings-changed">
<h3>Truthiness of NumPy strings changed<a href="#truthiness-of-numpy-strings-changed" title="Link to this heading">#</a></h3>
<p>NumPy strings previously were inconsistent about how they defined
if the string is <code><span>True</span></code> or <code><span>False</span></code> and the definition did not
match the one used by Python.
Strings are now considered <code><span>True</span></code> when they are non-empty and
<code><span>False</span></code> when they are empty.
This changes the following distinct cases:</p>
<ul>
<li><p>Casts from string to boolean were previously roughly equivalent
to <code><span>string_array.astype(np.int64).astype(bool)</span></code>, meaning that only
valid integers could be cast.
Now a string of <code><span>"0"</span></code> will be considered <code><span>True</span></code> since it is not empty.
If you need the old behavior, you may use the above step (casting
to integer first) or <code><span>string_array</span> <span>==</span> <span>"0"</span></code> (if the input is only ever <code><span>0</span></code> or <code><span>1</span></code>).
To get the new result on old NumPy versions use <code><span>string_array</span> <span>!=</span> <span>""</span></code>.</p></li>
<li><p><code><span>np.nonzero(string_array)</span></code> previously ignored whitespace so that
a string only containing whitespace was considered <code><span>False</span></code>.
Whitespace is now considered <code><span>True</span></code>.</p></li>
</ul>
<p>This change does not affect <code><span>np.loadtxt</span></code>, <code><span>np.fromstring</span></code>, or <code><span>np.genfromtxt</span></code>.
The first two still use the integer definition, while <code><span>genfromtxt</span></code> continues to
match for <code><span>"true"</span></code> (ignoring case).
However, if <code><span>np.bool_</span></code> is used as a converter the result will change.</p>
<p>The change does affect <code><span>np.fromregex</span></code> as it uses direct assignments.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/23871">gh-23871</a>)</p>
</section>
<section id="a-mean-keyword-was-added-to-var-and-std-function">
<h3>A <code><span>mean</span></code> keyword was added to var and std function<a href="#a-mean-keyword-was-added-to-var-and-std-function" title="Link to this heading">#</a></h3>
<p>Often when the standard deviation is needed the mean is also needed. The same
holds for the variance and the mean. Until now the mean is then calculated twice,
the change introduced here for the <a href="https://numpy.org/devdocs/reference/generated/numpy.var.html#numpy.var" title="numpy.var"><code><span>var</span></code></a> and <a href="https://numpy.org/devdocs/reference/generated/numpy.std.html#numpy.std" title="numpy.std"><code><span>std</span></code></a> functions
allows for passing in a precalculated mean as an keyword argument. See the
docstrings for details and an example illustrating the speed-up.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24126">gh-24126</a>)</p>
</section>
<section id="remove-datetime64-deprecation-warning-when-constructing-with-timezone">
<h3>Remove datetime64 deprecation warning when constructing with timezone<a href="#remove-datetime64-deprecation-warning-when-constructing-with-timezone" title="Link to this heading">#</a></h3>
<p>The <a href="https://numpy.org/devdocs/reference/arrays.scalars.html#numpy.datetime64" title="numpy.datetime64"><code><span>numpy.datetime64</span></code></a> method now issues a UserWarning rather than a
DeprecationWarning whenever a timezone is included in the datetime
string that is provided.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24193">gh-24193</a>)</p>
</section>
<section id="default-integer-dtype-is-now-64-bit-on-64-bit-windows">
<h3>Default integer dtype is now 64-bit on 64-bit Windows<a href="#default-integer-dtype-is-now-64-bit-on-64-bit-windows" title="Link to this heading">#</a></h3>
<p>The default NumPy integer is now 64-bit on all 64-bit systems as the historic
32-bit default on Windows was a common source of issues. Most users should not
notice this. The main issues may occur with code interfacing with libraries
written in a compiled language like C.  For more information see
<a href="https://numpy.org/devdocs/numpy_2_0_migration_guide.html#migration-windows-int64"><span>Windows default integer</span></a>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24224">gh-24224</a>)</p>
</section>
<section id="renamed-numpy-core-to-numpy-core">
<h3>Renamed <code><span>numpy.core</span></code> to <code><span>numpy._core</span></code><a href="#renamed-numpy-core-to-numpy-core" title="Link to this heading">#</a></h3>
<p>Accessing <code><span>numpy.core</span></code> now emits a DeprecationWarning. In practice
we have found that most downstream usage of <code><span>numpy.core</span></code> was to access
functionality that is available in the main <code><span>numpy</span></code> namespace.
If for some reason you are using functionality in <code><span>numpy.core</span></code> that
is not available in the main <code><span>numpy</span></code> namespace, this means you are likely
using private NumPy internals. You can still access these internals via
<code><span>numpy._core</span></code> without a deprecation warning but we do not provide any
backward compatibility guarantees for NumPy internals. Please open an issue
if you think a mistake was made and something needs to be made public.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24634">gh-24634</a>)</p>
<p>The “relaxed strides” debug build option, which was previously enabled through
the <code><span>NPY_RELAXED_STRIDES_DEBUG</span></code> environment variable or the
<code><span>-Drelaxed-strides-debug</span></code> config-settings flag has been removed.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24717">gh-24717</a>)</p>
</section>
<section id="redefinition-of-np-intp-np-uintp-almost-never-a-change">
<h3>Redefinition of <code><span>np.intp</span></code>/<code><span>np.uintp</span></code> (almost never a change)<a href="#redefinition-of-np-intp-np-uintp-almost-never-a-change" title="Link to this heading">#</a></h3>
<p>Due to the actual use of these types almost always matching the use of
<code><span>size_t</span></code>/<code><span>Py_ssize_t</span></code> this is now the definition in C.
Previously, it matched <code><span>intptr_t</span></code> and <code><span>uintptr_t</span></code> which would often
have been subtly incorrect.
This has no effect on the vast majority of machines since the size
of these types only differ on extremely niche platforms.</p>
<p>However, it means that:</p>
<ul>
<li><p>Pointers may not necessarily fit into an <code><span>intp</span></code> typed array anymore.
The <code><span>p</span></code> and <code><span>P</span></code> character codes can still be used, however.</p></li>
<li><p>Creating <code><span>intptr_t</span></code> or <code><span>uintptr_t</span></code> typed arrays in C remains possible
in a cross-platform way via <code><span>PyArray_DescrFromType('p')</span></code>.</p></li>
<li><p>The new character codes <code><span>nN</span></code> were introduced.</p></li>
<li><p>It is now correct to use the Python C-API functions when parsing
to <code><span>npy_intp</span></code> typed arguments.</p></li>
</ul>
<p>(<a href="https://github.com/numpy/numpy/pull/24888">gh-24888</a>)</p>
</section>
<section id="numpy-fft-helper-made-private">
<h3><code><span>numpy.fft.helper</span></code> made private<a href="#numpy-fft-helper-made-private" title="Link to this heading">#</a></h3>
<p><code><span>numpy.fft.helper</span></code> was renamed to <code><span>numpy.fft._helper</span></code> to indicate
that it is a private submodule. All public functions exported by it
should be accessed from <a href="https://numpy.org/devdocs/reference/routines.fft.html#module-numpy.fft" title="numpy.fft"><code><span>numpy.fft</span></code></a>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24945">gh-24945</a>)</p>
</section>
<section id="numpy-linalg-linalg-made-private">
<h3><code><span>numpy.linalg.linalg</span></code> made private<a href="#numpy-linalg-linalg-made-private" title="Link to this heading">#</a></h3>
<p><code><span>numpy.linalg.linalg</span></code> was renamed to <code><span>numpy.linalg._linalg</span></code>
to indicate that it is a private submodule. All public functions
exported by it should be accessed from <a href="https://numpy.org/devdocs/reference/routines.linalg.html#module-numpy.linalg" title="numpy.linalg"><code><span>numpy.linalg</span></code></a>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/24946">gh-24946</a>)</p>
</section>
<section id="out-of-bound-axis-not-the-same-as-axis-none">
<h3>Out-of-bound axis not the same as <code><span>axis=None</span></code><a href="#out-of-bound-axis-not-the-same-as-axis-none" title="Link to this heading">#</a></h3>
<p>In some cases <code><span>axis=32</span></code> or for concatenate any large value
was the same as <code><span>axis=None</span></code>.
Except for <code><span>concatenate</span></code> this was deprecate.
Any out of bound axis value will now error, make sure to use
<code><span>axis=None</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25149">gh-25149</a>)</p>
</section>
<section id="new-copy-keyword-meaning-for-array-and-asarray-constructors">
<span id="copy-keyword-changes-2-0"></span><h3>New <code><span>copy</span></code> keyword meaning for <code><span>array</span></code> and <code><span>asarray</span></code> constructors<a href="#new-copy-keyword-meaning-for-array-and-asarray-constructors" title="Link to this heading">#</a></h3>
<p>Now <a href="https://numpy.org/devdocs/reference/generated/numpy.array.html#numpy.array" title="numpy.array"><code><span>numpy.array</span></code></a> and <a href="https://numpy.org/devdocs/reference/generated/numpy.asarray.html#numpy.asarray" title="numpy.asarray"><code><span>numpy.asarray</span></code></a> support three values for <code><span>copy</span></code> parameter:</p>
<ul>
<li><p><code><span>None</span></code> - A copy will only be made if it is necessary.</p></li>
<li><p><code><span>True</span></code> - Always make a copy.</p></li>
<li><p><code><span>False</span></code> - Never make a copy. If a copy is required a <code><span>ValueError</span></code> is raised.</p></li>
</ul>
<p>The meaning of <code><span>False</span></code> changed as it now raises an exception if a copy is needed.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25168">gh-25168</a>)</p>
</section>
<section id="the-array-special-method-now-takes-a-copy-keyword-argument">
<h3>The <code><span>__array__</span></code> special method now takes a <code><span>copy</span></code> keyword argument.<a href="#the-array-special-method-now-takes-a-copy-keyword-argument" title="Link to this heading">#</a></h3>
<p>NumPy will pass <code><span>copy</span></code> to the <code><span>__array__</span></code> special method in situations where
it would be set to a non-default value (e.g. in a call to
<code><span>np.asarray(some_object,</span> <span>copy=False)</span></code>). Currently, if an
unexpected keyword argument error is raised after this, NumPy will print a
warning and re-try without the <code><span>copy</span></code> keyword argument. Implementations of
objects implementing the <code><span>__array__</span></code> protocol should accept a <code><span>copy</span></code> keyword
argument with the same meaning as when passed to <a href="https://numpy.org/devdocs/reference/generated/numpy.array.html#numpy.array" title="numpy.array"><code><span>numpy.array</span></code></a> or
<a href="https://numpy.org/devdocs/reference/generated/numpy.asarray.html#numpy.asarray" title="numpy.asarray"><code><span>numpy.asarray</span></code></a>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25168">gh-25168</a>)</p>
</section>
<section id="cleanup-of-initialization-of-numpy-dtype-with-strings-with-commas">
<h3>Cleanup of initialization of <code><span>numpy.dtype</span></code> with strings with commas<a href="#cleanup-of-initialization-of-numpy-dtype-with-strings-with-commas" title="Link to this heading">#</a></h3>
<p>The interpretation of strings with commas is changed slightly, in that a
trailing comma will now always create a structured dtype.  E.g., where
previously <code><span>np.dtype("i")</span></code> and <code><span>np.dtype("i,")</span></code> were treated as identical,
now <code><span>np.dtype("i,")</span></code> will create a structured dtype, with a single
field. This is analogous to <code><span>np.dtype("i,i")</span></code> creating a structured dtype
with two fields, and makes the behaviour consistent with that expected of
tuples.</p>
<p>At the same time, the use of single number surrounded by parenthesis to
indicate a sub-array shape, like in <code><span>np.dtype("(2)i,")</span></code>, is deprecated.
Instead; one should use <code><span>np.dtype("(2,)i")</span></code> or <code><span>np.dtype("2i")</span></code>.
Eventually, using a number in parentheses will raise an exception, like is the
case for initializations without a comma, like <code><span>np.dtype("(2)i")</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25434">gh-25434</a>)</p>
</section>
<section id="change-in-how-complex-sign-is-calculated">
<h3>Change in how complex sign is calculated<a href="#change-in-how-complex-sign-is-calculated" title="Link to this heading">#</a></h3>
<p>Following the array API standard, the complex sign is now calculated as
<code><span>z</span> <span>/</span> <span>|z|</span></code> (instead of the rather less logical case where the sign of
the real part was taken, unless the real part was zero, in which case
the sign of the imaginary part was returned).  Like for real numbers,
zero is returned if <code><span>z==0</span></code>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25441">gh-25441</a>)</p>
</section>
<section id="return-types-of-functions-that-returned-a-list-of-arrays">
<h3>Return types of functions that returned a list of arrays<a href="#return-types-of-functions-that-returned-a-list-of-arrays" title="Link to this heading">#</a></h3>
<p>Functions that returned a list of ndarrays have been changed to return a tuple
of ndarrays instead. Returning tuples consistently whenever a sequence of
arrays is returned makes it easier for JIT compilers like Numba, as well as for
static type checkers in some cases, to support these functions. Changed
functions are: <a href="https://numpy.org/devdocs/reference/generated/numpy.atleast_1d.html#numpy.atleast_1d" title="numpy.atleast_1d"><code><span>atleast_1d</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.atleast_2d.html#numpy.atleast_2d" title="numpy.atleast_2d"><code><span>atleast_2d</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.atleast_3d.html#numpy.atleast_3d" title="numpy.atleast_3d"><code><span>atleast_3d</span></code></a>,
<a href="https://numpy.org/devdocs/reference/generated/numpy.broadcast_arrays.html#numpy.broadcast_arrays" title="numpy.broadcast_arrays"><code><span>broadcast_arrays</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.meshgrid.html#numpy.meshgrid" title="numpy.meshgrid"><code><span>meshgrid</span></code></a>, <a href="https://numpy.org/devdocs/reference/generated/numpy.ogrid.html#numpy.ogrid" title="numpy.ogrid"><code><span>ogrid</span></code></a>,
<a href="https://numpy.org/devdocs/reference/generated/numpy.histogramdd.html#numpy.histogramdd" title="numpy.histogramdd"><code><span>histogramdd</span></code></a>.</p>
</section>
<section id="np-unique-return-inverse-shape-for-multi-dimensional-inputs">
<h3><code><span>np.unique</span></code> <code><span>return_inverse</span></code> shape for multi-dimensional inputs<a href="#np-unique-return-inverse-shape-for-multi-dimensional-inputs" title="Link to this heading">#</a></h3>
<p>When multi-dimensional inputs are passed to <code><span>np.unique</span></code> with <code><span>return_inverse=True</span></code>,
the <code><span>unique_inverse</span></code> output is now shaped such that the input can be reconstructed
directly using <code><span>np.take(unique,</span> <span>unique_inverse)</span></code> when <code><span>axis=None</span></code>, and
<code><span>np.take_along_axis(unique,</span> <span>unique_inverse,</span> <span>axis=axis)</span></code> otherwise.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25553">gh-25553</a>,
<a href="https://github.com/numpy/numpy/pull/25570">gh-25570</a>)</p>
</section>
<section id="any-and-all-return-booleans-for-object-arrays">
<h3><code><span>any</span></code> and <code><span>all</span></code> return booleans for object arrays<a href="#any-and-all-return-booleans-for-object-arrays" title="Link to this heading">#</a></h3>
<p>The <code><span>any</span></code> and <code><span>all</span></code> functions and methods now return
booleans also for object arrays.  Previously, they did
a reduction which behaved like the Python <code><span>or</span></code> and
<code><span>and</span></code> operators which evaluates to one of the arguments.
You can use <code><span>np.logical_or.reduce</span></code> and <code><span>np.logical_and.reduce</span></code>
to achieve the previous behavior.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/25712">gh-25712</a>)</p>
</section>
<section id="np-can-cast-cannot-be-called-on-python-int-float-or-complex">
<h3><code><span>np.can_cast</span></code> cannot be called on Python int, float, or complex<a href="#np-can-cast-cannot-be-called-on-python-int-float-or-complex" title="Link to this heading">#</a></h3>
<p><code><span>np.can_cast</span></code> cannot be called with Python int, float, or complex instances
anymore.  This is because NEP 50 means that the result of <code><span>can_cast</span></code> must
not depend on the value passed in.
Unfortunately, for Python scalars whether a cast should be considered
<code><span>"same_kind"</span></code> or <code><span>"safe"</span></code> may depend on the context and value so that
this is currently not implemented.
In some cases, this means you may have to add a specific path for:
<code><span>if</span> <span>type(obj)</span> <span>in</span> <span>(int,</span> <span>float,</span> <span>complex):</span> <span>...</span></code>.</p>
<p><strong>Content from release note snippets in doc/release/upcoming_changes:</strong></p>
</section>
</section>
<section id="id1">
<h2>Deprecations<a href="#id1" title="Link to this heading">#</a></h2>
<blockquote>
<div><ul>
<li><p>The <em>fix_imports</em> keyword argument in <a href="https://numpy.org/devdocs/reference/generated/numpy.save.html#numpy.save" title="numpy.save"><code><span>numpy.save</span></code></a> is deprecated. Since
NumPy 1.17, <a href="https://numpy.org/devdocs/reference/generated/numpy.save.html#numpy.save" title="numpy.save"><code><span>numpy.save</span></code></a> uses a pickle protocol that no longer supports
Python 2, and ignored <em>fix_imports</em> keyword. This keyword is kept only
for backward compatibility. It is now deprecated.</p></li>
</ul>
</div></blockquote>
<p>(<a href="https://github.com/numpy/numpy/pull/26452">gh-26452</a>)</p>
</section>
<section id="id2">
<h2>Expired deprecations<a href="#id2" title="Link to this heading">#</a></h2>
<ul>
<li><p>Scalars and 0D arrays are disallowed for <a href="https://numpy.org/devdocs/reference/generated/numpy.nonzero.html#numpy.nonzero" title="numpy.nonzero"><code><span>numpy.nonzero</span></code></a> and <a href="https://numpy.org/devdocs/reference/generated/numpy.ndarray.nonzero.html#numpy.ndarray.nonzero" title="numpy.ndarray.nonzero"><code><span>numpy.ndarray.nonzero</span></code></a>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/26268">gh-26268</a>)</p>
</li>
</ul>
</section>
<section id="id3">
<h2>C API changes<a href="#id3" title="Link to this heading">#</a></h2>
<section id="api-symbols-now-hidden-but-customizable">
<h3>API symbols now hidden but customizable<a href="#api-symbols-now-hidden-but-customizable" title="Link to this heading">#</a></h3>
<p>NumPy now defaults to hide the API symbols it adds to allow all NumPy API
usage.
This means that by default you cannot dynamically fetch the NumPy API from
another library (this was never possible on windows).</p>
<p>If you are experiencing linking errors related to <code><span>PyArray_API</span></code> or
<code><span>PyArray_RUNTIME_VERSION</span></code>, you can define the
<a href="https://numpy.org/devdocs/reference/c-api/array.html#c.NPY_API_SYMBOL_ATTRIBUTE" title="NPY_API_SYMBOL_ATTRIBUTE"><code><span>NPY_API_SYMBOL_ATTRIBUTE</span></code></a> to opt-out of this change.</p>
<p>If you are experiencing problems due to an upstream header including NumPy,
the solution is to make sure you <code><span>#include</span> <span>"numpy/ndarrayobject.h"</span></code> before
their header and import NumPy yourself based on  <a href="https://numpy.org/devdocs/reference/c-api/array.html#including-the-c-api"><span>Including and importing the C API</span></a>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/26103">gh-26103</a>)</p>
</section>
</section>
<section id="id4">
<h2>New Features<a href="#id4" title="Link to this heading">#</a></h2>
<ul>
<li><p><a href="https://numpy.org/devdocs/reference/generated/numpy.reshape.html#numpy.reshape" title="numpy.reshape"><code><span>numpy.reshape</span></code></a> and <a href="https://numpy.org/devdocs/reference/generated/numpy.ndarray.reshape.html#numpy.ndarray.reshape" title="numpy.ndarray.reshape"><code><span>numpy.ndarray.reshape</span></code></a> now support <code><span>shape</span></code> and <code><span>copy</span></code> arguments.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/26292">gh-26292</a>)</p>
</li>
<li><p>NumPy now supports DLPack v1, support for older versions will
be deprecated in the future.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/26501">gh-26501</a>)</p>
</li>
<li><p><a href="https://numpy.org/devdocs/reference/generated/numpy.asanyarray.html#numpy.asanyarray" title="numpy.asanyarray"><code><span>numpy.asanyarray</span></code></a> now supports <code><span>copy</span></code> and <code><span>device</span></code> arguments, matching <a href="https://numpy.org/devdocs/reference/generated/numpy.asarray.html#numpy.asarray" title="numpy.asarray"><code><span>numpy.asarray</span></code></a>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/26580">gh-26580</a>)</p>
</li>
</ul>
</section>
<section id="id5">
<h2>Improvements<a href="#id5" title="Link to this heading">#</a></h2>
<section id="histogram-auto-binning-now-returns-bin-sizes-1-for-integer-input-data">
<h3><code><span>histogram</span></code> auto-binning now returns bin sizes &gt;=1 for integer input data<a href="#histogram-auto-binning-now-returns-bin-sizes-1-for-integer-input-data" title="Link to this heading">#</a></h3>
<p>For integer input data, bin sizes smaller than 1 result in spurious empty
bins.  This is now avoided when the number of bins is computed using one of the
algorithms provided by <a href="https://numpy.org/devdocs/reference/generated/numpy.histogram_bin_edges.html#numpy.histogram_bin_edges" title="numpy.histogram_bin_edges"><code><span>histogram_bin_edges</span></code></a>.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/12150">gh-12150</a>)</p>
</section>
</section>
<section id="performance-improvements-and-changes">
<h2>Performance improvements and changes<a href="#performance-improvements-and-changes" title="Link to this heading">#</a></h2>
<section id="ma-cov-and-ma-corrcoef-are-now-significantly-faster">
<h3><code><span>ma.cov</span></code> and <code><span>ma.corrcoef</span></code> are now significantly faster<a href="#ma-cov-and-ma-corrcoef-are-now-significantly-faster" title="Link to this heading">#</a></h3>
<p>The private function has been refactored along with <a href="https://numpy.org/devdocs/reference/generated/numpy.ma.cov.html#numpy.ma.cov" title="numpy.ma.cov"><code><span>ma.cov</span></code></a> and
<a href="https://numpy.org/devdocs/reference/generated/numpy.ma.corrcoef.html#numpy.ma.corrcoef" title="numpy.ma.corrcoef"><code><span>ma.corrcoef</span></code></a>. They are now significantly faster, particularly on large,
masked arrays.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/26285">gh-26285</a>)</p>
<blockquote>
<div><ul>
<li><p><a href="https://numpy.org/devdocs/reference/generated/numpy.save.html#numpy.save" title="numpy.save"><code><span>numpy.save</span></code></a> now uses pickle protocol version 4 for saving arrays with
object dtype, which allows for pickle objects larger than 4GB and improves
saving speed by about 5% for large arrays.</p></li>
</ul>
</div></blockquote>
<p>(<a href="https://github.com/numpy/numpy/pull/26388">gh-26388</a>)</p>
</section>
</section>
<section id="id6">
<h2>Changes<a href="#id6" title="Link to this heading">#</a></h2>
<ul>
<li><p>As <a href="https://numpy.org/devdocs/reference/generated/numpy.vecdot.html#numpy.vecdot" title="numpy.vecdot"><code><span>numpy.vecdot</span></code></a> is now a ufunc it has a less precise signature.
This is due to the limitations of ufunc’s typing stub.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/26313">gh-26313</a>)</p>
</li>
</ul>
<section id="ma-corrcoef-may-return-a-slightly-different-result">
<h3><code><span>ma.corrcoef</span></code> may return a slightly different result<a href="#ma-corrcoef-may-return-a-slightly-different-result" title="Link to this heading">#</a></h3>
<p>A pairwise observation approach is currently used in <a href="https://numpy.org/devdocs/reference/generated/numpy.ma.corrcoef.html#numpy.ma.corrcoef" title="numpy.ma.corrcoef"><code><span>ma.corrcoef</span></code></a> to
calculate the standard deviations for each pair of variables. This has been
changed as it is being used to normalise the covariance, estimated using
<a href="https://numpy.org/devdocs/reference/generated/numpy.ma.cov.html#numpy.ma.cov" title="numpy.ma.cov"><code><span>ma.cov</span></code></a>, which does not consider the observations for each variable in a
pairwise manner, rendering it unnecessary. The normalisation has been
replaced by the more appropriate standard deviation for each variable,
which significantly reduces the wall time, but will return slightly different
estimates of the correlation coefficients in cases where the observations
between a pair of variables are not aligned. However, it will return the same
estimates in all other cases, including returning the same correlation matrix
as <a href="https://numpy.org/devdocs/reference/generated/numpy.corrcoef.html#numpy.corrcoef" title="numpy.corrcoef"><code><span>corrcoef</span></code></a> when using a masked array with no masked values.</p>
<p>(<a href="https://github.com/numpy/numpy/pull/26285">gh-26285</a>)</p>
</section>
</section>
</section>


                </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quake 1 potential original font (155 pts)]]></title>
            <link>https://cohost.org/bekoha/post/2859948-quake-1-potential-or</link>
            <guid>40699459</guid>
            <pubDate>Sun, 16 Jun 2024 19:30:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cohost.org/bekoha/post/2859948-quake-1-potential-or">https://cohost.org/bekoha/post/2859948-quake-1-potential-or</a>, See on <a href="https://news.ycombinator.com/item?id=40699459">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Some time last year while watching a video I got jumpscared by a logo that used a font that is immediately recognizable as <!-- --><em>the Quake font<!-- --></em>, and they used it at the very least 20+ years before Quake 1 even came out. This made me realize that <!-- --><em>the Quake font<!-- --></em> wasn't drawn completely from scratch and that it's probably possible to find it, most likely in a book or a digitization by someone.<!-- --></p><div><hr>
<!-- --><p><img src="https://staging.cohostcdn.org/attachment/e3875a72-5d4a-455a-87c6-f2c1f58f05a1/chrome_2023-09-16_02-39-25.jpg" alt=""><br>
This has been nagging me for some time and I've made several unsuccessful attempts at trying to find it before, but recently I've come across Dan X. Solo's "The Solotype Catalog of 4,147 Display Typefaces", 1992, and under "Stencil fonts" on the page 119 surprisingly actually found a sample for a font named "Visa", as pictured above. Checking Dan X. Solo's other books lead me to an earlier book named "Stencil Alphabets: 100 Complete Fonts", published in 1988, and turns out there's an entire page dedicated to this font showing the entire alphabet and numbers and other characters.<!-- --></p>
<!-- --><p><img src="https://staging.cohostcdn.org/attachment/cb178dcc-fea9-403a-bcba-bebb404b8d7d/e9780486168883_i0098.jpg" alt=""><br>
While differences between this sample and both the Schott logo and final result in Quake are present, similarities between most of letterforms and some really specific ones like X and Y make me believe that Visa (or, more likely, its progenitor or a derivative because legally it's a typography free-for-all out there with everyone ripping off each other) was in fact used as a base for the Quake font.<!-- --></p>
<!-- --><p>edit: According to <!-- --><a href="http://luc.devroye.org/fonts-44242.html" target="_blank" rel="nofollow noopener">Luc Devroye's "On Snot and Fonts"<!-- --></a>, original Visa font's designer is Raphael Boguslav.<!-- --></p>
<!-- --><blockquote>
<!-- --><p>His typeface Avia (VGC) was an expansion of a logofont he did for Abex Corporation, almost like a stencil. It is now at Font Bureau, where Jill Pichotta has added the Light and Bold in 2000. His typeface Visa (1966, VGC) won the Second Prize in the 1966 VGC National Type Face Design Competition. Others (thanks, Alexander Tochilovsky) confirm what I thought---that Visa and Avia are the same thing.<!-- --></p>
<!-- --></blockquote>
<!-- --><p>Excerpt from the official promotional PDF for FontBureau's Avia<!-- --><br>
<!-- --><img src="https://staging.cohostcdn.org/attachment/5814a628-676a-4f76-95bb-105d02757125/Avia.png" alt=""><br>
<!-- --><a href="https://fontsinuse.com/typefaces/2173/avia" target="_blank" rel="nofollow noopener">https://fontsinuse.com/typefaces/2173/avia<!-- --></a><br>
<!-- --><a href="http://web.archive.org/web/20130617135330/http://www.fontbureau.com/fonts/avia/" target="_blank" rel="nofollow noopener">http://web.archive.org/web/20130617135330/http://www.fontbureau.com/fonts/avia/<!-- --></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Experts vs. Imitators (292 pts)]]></title>
            <link>https://fs.blog/experts-vs-imitators/</link>
            <guid>40699079</guid>
            <pubDate>Sun, 16 Jun 2024 18:33:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fs.blog/experts-vs-imitators/">https://fs.blog/experts-vs-imitators/</a>, See on <a href="https://news.ycombinator.com/item?id=40699079">Hacker News</a></p>
Couldn't get https://fs.blog/experts-vs-imitators/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Japanese words and names sound African (2022) (106 pts)]]></title>
            <link>https://www.farooqkperogi.com/2022/10/japanese-words-and-names-sound-african.html</link>
            <guid>40699007</guid>
            <pubDate>Sun, 16 Jun 2024 18:20:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.farooqkperogi.com/2022/10/japanese-words-and-names-sound-african.html">https://www.farooqkperogi.com/2022/10/japanese-words-and-names-sound-african.html</a>, See on <a href="https://news.ycombinator.com/item?id=40699007">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post23291839112526094550"><p><b>By Farooq Kperogi</b></p><p><b>Twitter: <a href="https://twitter.com/farooqkperogi" target="_blank">@farooqkperogi</a></b></p><p>Most Japanese words and names sound like—and actually appear in—most languages in west, central, east, and even southern African. The reverse is also true: Japanese people find a curious phonetic correspondence between their language and most African languages.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKGPHxg2vWy_cisaKH6ry9hPwULEtZsaqTseBZKfXidmXOCmczY4FhHYhBB4f3SsDiS_W8EM8IQH2qg_toA94ez_WmAjh69UqBLReN2PKz6nh2y75l3YLZZ0VueHnz0YURpn5NaREe4v-8ol88tML1L1ob6Xk-d025Ja5ImTrpasCFXpq5eODfrRfA/s960/Japanese%20and%20Africans.jpg" imageanchor="1"><img data-original-height="720" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKGPHxg2vWy_cisaKH6ry9hPwULEtZsaqTseBZKfXidmXOCmczY4FhHYhBB4f3SsDiS_W8EM8IQH2qg_toA94ez_WmAjh69UqBLReN2PKz6nh2y75l3YLZZ0VueHnz0YURpn5NaREe4v-8ol88tML1L1ob6Xk-d025Ja5ImTrpasCFXpq5eODfrRfA/s16000/Japanese%20and%20Africans.jpg"></a></p><p>The most famous example of this is the name Obama, a Luo name from Kenya, which is also the name of a town in Japan. When Barack Obama was elected US president in 2008, the town shot to international prominence.&nbsp;</p><p>We later learned that Obama means a “small beach” in the Japanese language. (There’s also a town called Obama in Nigeria’s River State).</p><p>Until relatively recently, I used to think Ajinomoto, whose seasoning was omnipresent in Nigerian kitchens when I lived there, was a Nigerian company that derived its name from a Nigerian language.</p><p>In fact, just yesterday, I asked my wife, who is half Nigerian (Igbo) and half American, to guess what language Ajinomoto came from. She said with cocksure certainty that it had to be Yoruba! (She spent three years in Ogbomosho and speaks some Yoruba). She was shocked to realize that Ajinomoto is Japanese.</p><p>Japanese brands like Suzuki, Yamaha, Kawasaki, etc. resonated with us when I was growing up in Nigeria because they sounded every bit African, even Nigerian.</p><p>But it gets even more interesting. Japanese capital Tokyo used to be called Edo, the name of a state and people in midwestern Nigeria.&nbsp;</p><p>The Japanese also bear person names like Chika, an Igbo name; Aina, a Yoruba name; Fumi, a Yoruba name; Ikimi, an Edo name; Yaru, a Borgu name; Sambo, a Fulani name; Maitama, a Hausa name; Adachi, a Nupe/Igala name; and so on.</p><p>Tutor-World also identified several more Japanese personal names that are either also personal names, names of common things, or names of towns in Nigeria, including: Azuka, Baba, Duro, Eijiro, Emiko, Femi, Fuji, Gobe, Goro, Haruna, Imoko, Iyamu, Kano, Kwashi, Mabuchi, Maduka, Mai, Obi, Oba, Ogi, Okada, Osahon, Sada, Ta-Daura (even Buhari’s adopted hometown made the list!), Waka, and Zoro.</p><p>Many people have pointed out that most words and names in a Black African language has a phonetic, and sometimes semantic, match in a Japanese word or name—and vice versa.&nbsp;</p><p>Take, for an example, the word Yoruba, the name of a major ethnic group in Nigeria. It also occurs in Japanese and can be translated as a “night horse,” according to the Sundaland Research Society, which says “yoru” means night and “ba” means horse in Japanese.</p><p>&nbsp;Several random words in Japanese can also have meanings, often unrelated to the original, in several African languages.</p><p>In spite of the uncanny congruence between the speech sounds of Black African languages and Japanese, there is no evidence that it’s anything more than a pleasantly giant phono-semantic accident.</p><p>It’s similar to the similarities in sound and rhythm between Plateau State languages and the Sino-Tibetan languages of China and its neighbors.&nbsp;</p><p>Many Plateau State names can pass for Mandarin names. A former Plateau journalist I used to be fond of (because of the exotic musicality of his name) is called Shok Jok. That name could pass for a Mandarin or Cantonese name.&nbsp;</p><p>There’s also a Professor Pam Dung Sha who teaches political science at the University of Jos. People who pay attention to politics are probably familiar with the late Senator Gyang Nyam Shom Pwajok. Names can’t get more Chinese than that!&nbsp;</p><p>How about Plateau names like Chuwang and Vongkong that Chinese people actually bear? The popular Plateau name Gyang sounds similar to a place in Tibet called Qungdo’gyang.</p><p>Similarly, many Plateau toponyms such as Qua'an Pan, Pankshin, Shendam, etc. sound distinctly Chinese. (This is also true of the toponyms and sometimes personal names in southern Kaduna and southern Adamawa.)</p><p>The first time I visited Jos in the late 1990s, I stayed at a hotel called Kufang Chindi International Hotel. I honestly thought it was a Chinese-owned hotel, more so that it had “international” in its name, but its name is actually native to Plateau!</p><p>I read about a Japanese linguist who was intrigued by the sounds of Plateau names and decided to map the glottochronology of the languages.&nbsp;</p><p>His lexicostatistical analysis found that less than 30 percent of the similar-sounding words between Plateau State languages and China’s Sino-Tibetan languages share similar meanings. Linguists call these kinds of similarities "accidental evidence."&nbsp;</p><p>In other words, the researcher found that there was no evidence of even a remote common origin for Chinese and the Plateau languages. It was merely a case of accidental similarities in sound.</p><p>Interestingly, Chinese-sounding Plateau languages and the Japanese language don’t belong to the major language families of their locations. They are both what linguists call language isolates.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Structure and Interpretation of Computer Programs Matters (241 pts)]]></title>
            <link>https://people.eecs.berkeley.edu/~bh/sicp.html</link>
            <guid>40698906</guid>
            <pubDate>Sun, 16 Jun 2024 18:02:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://people.eecs.berkeley.edu/~bh/sicp.html">https://people.eecs.berkeley.edu/~bh/sicp.html</a>, See on <a href="https://news.ycombinator.com/item?id=40698906">Hacker News</a></p>
<div id="readability-page-1" class="page">

<cite>Brian Harvey<br>University of California, Berkeley</cite>

<p>In 2011, to celebrate the 150th anniversary of MIT, the <i>Boston Globe</i>
made a list of the most important innovations developed there.  They asked me
to explain the importance of <i>SICP,</i> and this is what I sent them:

</p><p>SICP was revolutionary in many different ways.  Most importantly, it
dramatically raised the bar for the intellectual content of introductory
computer science.  Before SICP, the first CS course was almost always
entirely filled with learning the details of some programming language.
SICP is about standing back from the details to learn big-picture ways
to think about the programming process.  It focused attention on the
central idea of <i>abstraction</i> -- finding general patterns from specific
problems and building software tools that embody each pattern.  It made
heavy use of the idea of <i>functions as data</i>, an idea that's hard to learn
initially, but immensely powerful once learned.  (This is the same idea,
in a different form, that makes freshman calculus so notoriously hard even
for students who've done well in earlier math classes.)  It fit into the
first CS course three different <i>programming paradigms</i> (functional,
object oriented, and declarative), when most other courses didn't even
really discuss even one paradigm.

</p><p>Another revolution was the choice of Scheme as the programming language.
To this day, most introductions to computer science use whatever is the
"hot" language of the moment: from Pascal to C to C++ to Java to Python.
Scheme has never been widely used in industry, but it's the perfect
language for an introduction to CS.  For one thing, it has a very simple,
uniform notation for everything.  Other languages have one notation for
variable assignment, another notation for conditional execution, two or
three more for looping, and yet another for function calls.  Courses that
teach those languages spend at least half their time just on learning the
notation.  In my SICP-based course at Berkeley, we spend the first hour
on notation and that's all we need; for the rest of the semester we're
learning ideas, not syntax.  Also, despite (or because of) its simplicity,
Scheme is a very versatile language, making it possible for us to examine
those three programming paradigms and, in particular, letting us see how
object oriented programming is implemented, so OOP languages don't seem
like magic to our students.  Scheme is a dialect of Lisp, so it's great
at handling functions as data, but it's a stripped-down version compared
to the ones more commonly used for professional programming, with a
minimum of bells and whistles.  It was very brave of Abelson and Sussman
to teach their introductory course in the best possible language <i>for
teaching</i>, paying no attention to complaints that all the jobs were in
some other language.  Once you learned the big ideas, they thought, and
this is my experience also, learning another programming language isn't
a big deal; it's a chore for a weekend.  I tell my students, "the language
in which you'll spend most of your working life hasn't been invented yet,
so we can't teach it to you.  Instead we have to give you the skills you
need to learn new languages as they appear."

</p><p>Finally, SICP was firmly optimistic about what a college freshman can be
expected to accomplish.  SICP students write interpreters for programming
languages, ordinarily considered more appropriate for juniors or seniors.
The text itself isn't easy reading; it has none of the sidebars and colored
boxes and interesting pictures that typify the modern textbook aimed at
students with low attention spans.  There are no redundant exercises; each
exercise teaches an important new idea.  It uses big words.  But it repays a
close reading; every sentence matters.

</p><p>Statistically, SICP-based courses have been a small minority.  But the book
has had an influence beyond that minority.  It inspired a number of later
textbooks whose authors consciously tried to live up to SICP's standard.  The
use of Scheme as a language for learners has been extended by others over a
range from middle school to graduate school.  Even the more mainstream courses
have become sensitive to the idea of programming paradigms, although most of
them concentrate on object oriented programming.  The idea that computer
science should be about ideas, not entirely about programming practice, has
since widened to include non-technical ideas about the context and social
implications of computing.

</p><p>SICP itself has had a longevity that's very unusual for introductory CS
textbooks.  Usually, a book lasts only as long as the language fad to which it
is attached.  SICP has been going strong for over 25 years and shows no sign
of going out of print.  Computing has changed enormously over that time, from
giant mainframe computers to personal computers to the Internet on cell
phones.  And yet the big ideas behind these changes remain the same, and they
are well captured by SICP.

</p><p>I've been teaching a SICP-based course since 1987.  The course has changed
incrementally over that time; we've added sections on parallelism, concurrency
control, user interface design, and the client/server paradigm.  But it's
still essentially the same course.  Every five years or so, someone on the
faculty suggests that our first course should use language X instead; each
time, I say "when someone writes the best computer science book in the world
using language X, that'll be fine" and so far the faculty have always voted to
stay with the SICP course.  We'll find out pretty soon whether the course can
survive my own retirement.

</p><ul><li>(Footnote: Nope.  Berkeley's new first course for majors uses Python,
with lecture notes that try to keep the ideas (and some of the text) of SICP.)
</li></ul>

<p>The discussion has been sharper recently because MIT underwent a major
redesign of their lower division EECS curriculum.  People outside MIT tend to
summarize that redesign as "MIT decided to switch to Python," but that's not a
perceptive description.  What MIT decided was to move from a curriculum
organized around topics (programming paradigms, then circuits, then signal
processing, then architecture) to a curriculum organized around applications
(let's build and program a robot; let's build and program a cell phone).
<i>Everything</i> about their courses had to be reorganized; the choice of
programming language was the least of those decisions.  Their new approach is
harder to teach; for one thing, each course requires a partnership of
Electrical Engineering faculty and Computer Science faculty.  Perhaps in time
the applications-first approach will spark a revolution as profound as the one
that followed SICP, but it hasn't happened yet.

</p><p>In my experience, relatively few students appreciate how much they're
learning in my course while they're in it.  But in surveys of all our
CS students, it turns out to be among the most popular courses in
retrospect, and I regularly get visits and emails from long-gone students
to tell me about how they're using in their work ideas that they thought
were impractical ivory-tower notions as students.  The invention of the
MapReduce software for data parallelism at Google, based on functional
programming ideas, has helped eliminate that ivory-tower reputation.

</p><address>
<a href="https://people.eecs.berkeley.edu/~bh/index.html"><code>www.cs.berkeley.edu/~bh</code></a>
</address>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[I've compared nearly all Rust crates.io crates to contents of their Git repos (143 pts)]]></title>
            <link>https://mastodon.social/@kornel/112626463128422583</link>
            <guid>40698536</guid>
            <pubDate>Sun, 16 Jun 2024 17:07:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.social/@kornel/112626463128422583">https://mastodon.social/@kornel/112626463128422583</a>, See on <a href="https://news.ycombinator.com/item?id=40698536">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Building SimCity: How to put the world in a machine (358 pts)]]></title>
            <link>https://mitpress.mit.edu/9780262547482/building-simcity/</link>
            <guid>40698442</guid>
            <pubDate>Sun, 16 Jun 2024 16:55:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitpress.mit.edu/9780262547482/building-simcity/">https://mitpress.mit.edu/9780262547482/building-simcity/</a>, See on <a href="https://news.ycombinator.com/item?id=40698442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
        <p><a href="#content">Skip to content</a></p><header role="banner" id="masthead">
                <div>
            
        <p><a href="https://mitpress.mit.edu/" rel="home"><img width="386" height="610" src="https://dhjhkxawhe8q4.cloudfront.net/mit-press/wp-content/uploads/2022/05/20143118/logo.png" alt="MIT Press" decoding="async" fetchpriority="high" srcset="https://dhjhkxawhe8q4.cloudfront.net/mit-press/wp-content/uploads/2022/05/20143118/logo.png 386w, https://dhjhkxawhe8q4.cloudfront.net/mit-press/wp-content/uploads/2022/05/20143118/logo-190x300.png 190w, https://dhjhkxawhe8q4.cloudfront.net/mit-press/wp-content/uploads/2022/05/20143118/logo-309x488.png 309w, https://dhjhkxawhe8q4.cloudfront.net/mit-press/wp-content/uploads/2022/05/20143118/logo-127x200.png 127w" sizes="(max-width: 386px) 100vw, 386px"></a>
        </p>

        
        <div>

                
        <nav role="navigation" aria-label="main menu: press escape to close the menu">
            <div><ul id="primary-menu"><li><a href="https://mitpress.mit.edu/" role="link"><img src="https://dhjhkxawhe8q4.cloudfront.net/mit-press/wp-content/uploads/2021/11/26163520/mitp-colophon-white-black-bkg.gif" alt="MIT Press"></a></li><li id="menu-item-10853"><a href="#" aria-haspopup="true" aria-expanded="false">Books</a>
<ul>
	<li id="menu-item-77"><a href="#" aria-haspopup="true" aria-expanded="false">Column</a>
	<ul>
		<li id="menu-item-121"><a href="https://mitpress.mit.edu/books/subjects/">View all subjects</a></li>
		<li id="menu-item-11089"><a href="https://mitpress.mit.edu/new-releases/">New releases</a></li>
		<li id="menu-item-85"><a href="https://mitpress.mit.edu/catalogs/">Catalogs</a></li>
		<li id="menu-item-9600"><a href="https://mitpress.mit.edu/textbooks/">Textbooks</a></li>
		<li id="menu-item-118"><a href="https://mitpress.mit.edu/books/series/">Series</a></li>
		<li id="menu-item-13156"><a href="https://mitpress.mit.edu/awards/">Awards</a></li>
	</ul>
</li>
	<li id="menu-item-117"><a href="#" aria-haspopup="true" aria-expanded="false">Column</a>
	<ul>
		<li id="menu-item-76"><a href="https://mitpress.mit.edu/authors/">Authors</a></li>
		<li id="menu-item-9677"><a href="https://mitpress.mit.edu/publishers/">Distributed presses</a></li>
		<li id="menu-item-3490"><a target="_blank" rel="noopener" href="https://thereader.mitpress.mit.edu/">The MIT Press Reader</a></li>
		<li id="menu-item-3492"><a target="_blank" rel="noopener" href="https://newbooksnetwork.com/category/up-partners/mit-press-podcast">Podcasts</a></li>
		<li id="menu-item-12667"><a href="https://mitpress.mit.edu/collections/">Collections</a></li>
	</ul>
</li>
	<li id="menu-item-124"><a href="#" aria-haspopup="true" aria-expanded="false">Column</a>
	<ul>
		<li id="menu-item-3434"><a target="_blank" rel="noopener" href="https://direct.mit.edu/" aria-haspopup="true" aria-expanded="false">MIT Press Direct</a><p>MIT Press Direct is a distinctive collection of influential MIT Press books curated for scholars and libraries worldwide.</p>
		<ul>
			<li id="menu-item-3435"><a href="https://direct.mit.edu/">Learn more</a></li>
		</ul>
</li>
	</ul>
</li>
</ul>
</li>
<li id="menu-item-3494"><a href="#" aria-haspopup="true" aria-expanded="false">Journals</a>
<ul>
	<li id="menu-item-3495"><a href="#" aria-haspopup="true" aria-expanded="false">column</a>
	<ul>
		<li id="menu-item-3497"><a target="_blank" rel="noopener" href="https://direct.mit.edu/journals/pages/browse_by_topic">Journals all topics</a></li>
		<li id="menu-item-3498"><a target="_blank" rel="noopener" href="https://direct.mit.edu/journals/pages/browse_by_topic#econ">Economics</a></li>
		<li id="menu-item-3499"><a target="_blank" rel="noopener" href="https://direct.mit.edu/journals/pages/browse_by_topic#interpol">International Affairs, History, &amp; Political Science</a></li>
	</ul>
</li>
	<li id="menu-item-3496"><a href="#" aria-haspopup="true" aria-expanded="false">column</a>
	<ul>
		<li id="menu-item-3500"><a target="_blank" rel="noopener" href="https://direct.mit.edu/journals/pages/browse_by_topic#artshuman">Arts &amp; Humanities</a></li>
		<li id="menu-item-3501"><a target="_blank" rel="noopener" href="https://direct.mit.edu/journals/pages/browse_by_topic#scitech">Science &amp; Technology</a></li>
		<li id="menu-item-11045"><a target="_blank" rel="noopener" href="https://direct.mit.edu/journals/pages/open-access">Open access</a></li>
	</ul>
</li>
	<li id="menu-item-3503"><a href="#" aria-haspopup="true" aria-expanded="false">column</a>
	<ul>
		<li id="menu-item-3502"><a target="_blank" rel="noopener" href="https://direct.mit.edu/journals" aria-haspopup="true" aria-expanded="false">MIT Press journals</a><p>MIT Press began publishing journals in 1970 with the first volumes of <em>Linguistic Inquiry</em> and the <em>Journal of Interdisciplinary History</em>. Today we publish over 30 titles in the arts and humanities, social sciences, and science and technology.</p>
		<ul>
			<li id="menu-item-3504"><a target="_blank" rel="noopener" href="https://direct.mit.edu/journals">Learn more</a></li>
		</ul>
</li>
	</ul>
</li>
</ul>
</li>
<li id="menu-item-3430"><a aria-haspopup="true" aria-expanded="false">Open Access</a>
<ul>
	<li id="menu-item-3464"><a href="#" aria-haspopup="true" aria-expanded="false">column</a>
	<ul>
		<li id="menu-item-18875"><a href="https://mitpress.mit.edu/open-access-at-mit-press/">Open access at the MIT Press</a></li>
		<li id="menu-item-18876"><a href="https://mitpress.mit.edu/open-access-at-mit-press/initiatives/">Open access initiatives</a></li>
		<li id="menu-item-3467"><a target="_blank" rel="noopener" href="https://direct.mit.edu/books/pages/direct-to-open">Direct to Open</a></li>
		<li id="menu-item-18847"><a href="https://mitpress.mit.edu/open-access-at-mit-press/mit-open-publishing-services/">MIT Open Publishing Services</a></li>
	</ul>
</li>
	<li id="menu-item-3468"><a href="#" aria-haspopup="true" aria-expanded="false">column</a>
	<ul>
		<li id="menu-item-18878"><a href="https://mitpress.mit.edu/open-access-at-mit-press/books/">Open access books</a></li>
		<li id="menu-item-18877"><a href="https://mitpress.mit.edu/open-access-at-mit-press/journals/">Open access journals</a></li>
		<li id="menu-item-11873"><a href="https://mitpressonpubpub.mitpress.mit.edu/">MIT Press Open Access @ PubPub</a></li>
	</ul>
</li>
	<li id="menu-item-3470"><a href="#" aria-haspopup="true" aria-expanded="false">Column</a>
	<ul>
		<li id="menu-item-10527"><a href="https://mitpress.mit.edu/?page_id=9671" aria-haspopup="true" aria-expanded="false">Open access</a><p>The MIT Press has been a leader in open access book publishing for over two decades, beginning in 1995 with the publication of William Mitchell’s City of Bits, which appeared simultaneously in print and in a dynamic, open web edition.</p>
		<ul>
			<li id="menu-item-10445"><a href="https://mitpress.mit.edu/about-our-oa-program/">Learn more</a></li>
		</ul>
</li>
	</ul>
</li>
</ul>
</li>
<li id="menu-item-3431"><a href="#" aria-haspopup="true" aria-expanded="false">Info for</a>
<ul>
	<li id="menu-item-3474"><a href="#" aria-haspopup="true" aria-expanded="false">column</a>
	<ul>
		<li id="menu-item-3482"><a href="https://mitpress.mit.edu/for-authors/">Current authors</a></li>
		<li id="menu-item-3478"><a href="https://mitpress.mit.edu/prospective-authors/">Prospective authors</a></li>
		<li id="menu-item-9771"><a href="https://mitpress.mit.edu/instructors/">Instructors</a></li>
	</ul>
</li>
	<li id="menu-item-3476"><a href="#" aria-haspopup="true" aria-expanded="false">column</a>
	<ul>
		<li id="menu-item-9665"><a href="https://mitpress.mit.edu/media-inquiries/">Media inquiries</a></li>
		<li id="menu-item-9664"><a href="https://mitpress.mit.edu/booksellers/">Booksellers</a></li>
		<li id="menu-item-3609"><a href="https://mitpress.mit.edu/rights-permissions/">Rights and permissions</a></li>
	</ul>
</li>
	<li id="menu-item-3475"><a href="#" aria-haspopup="true" aria-expanded="false">column</a>
	<ul>
		<li id="menu-item-3485"><a href="#" aria-haspopup="true" aria-expanded="false">Resources</a><p>Collaborating with authors, instructors, booksellers, librarians, and the media is at the heart of what we do as a scholarly publisher. If you can’t find the resource you need here, visit our contact page to get in touch.</p>
		<ul>
			<li id="menu-item-10443"><a href="https://mitpress.mit.edu/for-authors/">Learn more</a></li>
		</ul>
</li>
	</ul>
</li>
</ul>
</li>
<li id="menu-item-19570"><a href="https://mitpress.mit.edu/give-to-the-mit-press/">Give</a></li>
<li id="menu-item-10854"><a href="#" aria-haspopup="true" aria-expanded="false">About</a>
<ul>
	<li id="menu-item-10441"><a href="#" aria-haspopup="true" aria-expanded="false">Column</a>
	<ul>
		<li id="menu-item-11348"><a href="https://mitpress.mit.edu/about/">About</a></li>
		<li id="menu-item-10327"><a href="https://mitpress.mit.edu/jobs/">Jobs</a></li>
		<li id="menu-item-10328"><a href="https://mitpress.mit.edu/internships-paid-internships-books-journals/">Internships</a></li>
		<li id="menu-item-10324"><a href="https://mitpress.mit.edu/mit-press-editorial-board/">MIT Press Editorial Board</a></li>
		<li id="menu-item-10325"><a href="https://mitpress.mit.edu/mit-press-management-board/">MIT Press Management Board</a></li>
		<li id="menu-item-14905"><a href="https://mitpress.mit.edu/collections/our-mit-story/">Our MIT story</a></li>
	</ul>
</li>
	<li id="menu-item-10329"><a href="#" aria-haspopup="true" aria-expanded="false">Column</a>
	<ul>
		<li id="menu-item-10330"><a href="https://mitpress.mit.edu/catalogs/">Catalogs</a></li>
		<li id="menu-item-10331"><a href="https://mitpress.mit.edu/?page_id=105">News</a></li>
		<li id="menu-item-9516"><a href="https://mitpress.mit.edu/events/">Events</a></li>
		<li id="menu-item-10332"><a href="https://mitpress.mit.edu/conferences/">Conferences</a></li>
		<li id="menu-item-10333"><a href="http://mitpressbookstore.mit.edu/">Bookstore</a></li>
	</ul>
</li>
	<li id="menu-item-10440"><a href="#" aria-haspopup="true" aria-expanded="false">Column</a>
	<ul>
		<li id="menu-item-10334"><a href="#" aria-haspopup="true" aria-expanded="false">The MIT Press</a><p>Established in 1962, the MIT Press is one of the largest and most distinguished university presses in the world and a leading publisher of books and journals at the intersection of science, technology, art, social science, and design.</p>
		<ul>
			<li id="menu-item-10442"><a href="https://mitpress.mit.edu/about">Learn more</a></li>
		</ul>
</li>
	</ul>
</li>
</ul>
</li>
<li id="menu-item-11708"><a href="https://mitpress.mit.edu/contact-us/">Contact Us</a></li>
</ul></div>        </nav>

                        
        
                </div>

                </div>
        
        

        
                    
        
            </header>
    
    <div id="content">
        
    <main id="main">

        <div id="product-details-32" data-widget-params="{&quot;include_price&quot;:1}" data-ajax-url="https://mitpress.mit.edu/wp-admin/admin-ajax.php">
                <ul>
                                                                        <li>
                                <a href="#tab-1">
                                    Description                                </a>
                            </li>
                                                                                                                                            <li>
                                <a href="#tab-3">Author(s)</a>
                            </li>
                                                                                                <li>
                                <a href="#tab-4">Praise</a>
                            </li>
                                                                                                                                </ul>
                                                            
                                                                                                                    
                                                                                
                                                                                    </div>

    </main><!-- #main -->


</div><!-- #content -->



    

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Edinburgh, Scotland makes it illegal to advertise SUVs (119 pts)]]></title>
            <link>https://www.washingtonpost.com/climate-solutions/2024/06/15/fossil-fuel-advertising-bans-edinburgh/</link>
            <guid>40698412</guid>
            <pubDate>Sun, 16 Jun 2024 16:50:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/climate-solutions/2024/06/15/fossil-fuel-advertising-bans-edinburgh/">https://www.washingtonpost.com/climate-solutions/2024/06/15/fossil-fuel-advertising-bans-edinburgh/</a>, See on <a href="https://news.ycombinator.com/item?id=40698412">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="C2FKBZ64ANAXDEZ4Q4FRVJSIOU" data-el="text" dir="null">EDINBURGH — Last month this Scottish city — filled with medieval spires and shadowed by the looming castle on the hill said to have inspired the Harry Potter books — made a startlingly modern decision. Edinburgh’s city council voted to <a href="https://democracy.edinburgh.gov.uk/documents/s70730/9.1%20Policy%20on%20Advertising%20and%20Sponsorship%20-%20Proposed%20Amendments.pdf" target="_blank">ban fossil fuel advertisements</a> on city property, undermining the ability of not only <a href="https://www.washingtonpost.com/politics/2022/06/13/house-democrats-probe-pr-industry-role-advertising-big-oil/?itid=lk_inline_manual_2" target="_blank">oil companies</a>, but also car manufacturers, <a href="https://www.washingtonpost.com/climate-solutions/2024/02/02/sustainable-aviation-fuel-future/?itid=lk_inline_manual_2" target="_blank">airlines</a> and <a href="https://www.washingtonpost.com/travel/2022/09/28/green-cruises-environment/?itid=lk_inline_manual_2" target="_blank">cruise ships</a>, to promote their products. The ban targeted arms manufacturers as well.</p></div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="OQM4JNVRWFAPJD2JQWJTA3RBW4" data-el="text" dir="null">Edinburgh is not alone. <a href="https://www.euronews.com/green/2021/05/20/amsterdam-becomes-first-city-in-the-world-to-ban-this-type-of-advert" target="_blank">Amsterdam</a> and <a href="https://www.adnews.com.au/news/city-of-sydney-votes-to-end-fossil-fuel-advertising" target="_blank">Sydney</a> have cracked down on advertisements for fossil fuels and high-emissions products. France also limited the promotion of coal, gas and hydrogen made from fossil fuels. Even the United Nations Secretary General, António Guterres, has joined in, endorsing a ban on fossil fuel ads this month in a speech in New York this month: “Stop the Mad Men from fueling the madness.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="LJIYXFY3EVCDLPP4BNACCOPQCM" data-el="text" dir="null">“There’s a moment happening here,” said Ben Parker, the Edinburgh city councilor who spearheaded the ban and a member of the Scottish Green Party. “It’s a way of saying fossil fuel companies and arms manufacturers are not welcome in our city.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="BIJKKZVQFNAXTECEJMO4TRTVCM" data-el="text" dir="null">A local ban on fossil fuel advertisements might seem minor at a time when carbon emissions — and <a href="https://www.washingtonpost.com/weather/2024/06/05/global-temperatures-1-5-celsius-record-year/?itid=lk_inline_manual_8" target="_blank">temperatures</a> — continue to march upward. But there is evidence that sweeping advertising bans, such as those targeting tobacco products in many countries, can change how consumers view and purchase certain products. The question is whether the new fossil fuel advertising bans are substantial enough to have an impact.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="N5OGPPP4DRC57AOZICGCHWXQUI" data-el="text" dir="null">“A lot of these bans that are being put forward are at the municipal and city level,” said Timothy Dewhirst, professor of marketing and consumer studies at the University of Guelph. “And partial bans have proven to be ineffective.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="2MX33FQJRBF67LDBQQQPBOT5HY" data-el="text" dir="null">Fossil fuel producers counter that they are focused on addressing climate change. “Our industry is focused on continuing to produce affordable, reliable energy while tackling the climate challenge, and any allegations to the contrary are false,” Scott Lauermann, a spokesperson for the American Petroleum Institute, said in an email.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="GMM7ZG63PZHHRL5A5OG2W25KTA" data-el="text" dir="null">Proponents of advertising bans seek to accomplish<b> </b>two goals: convince people not to use<b> </b>the product, and lower the reputation of an industry or company. Given how embedded fossil fuels are in modern society, some experts see the latter goal as more achievable.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="Z2MSLJH52FDKRETQAJDF5FXAP4" data-el="text" dir="null">At the individual level, seeing fewer advertisements for gas-guzzling cars or international trips could make people less likely to opt for those products. “What you are doing is reducing the amount of consumption that is coming from those advertisements,” said Andrew Simms, co-director of the New Weather Institute and a campaigner for fossil fuel ad bans.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="PR5KYC5F2VHPBMJ5IO65D4J2JI" data-el="text" dir="null">There’s <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167629608000155?via%3Dihub" target="_blank">evidence</a> that this works. Starting in the 1970s, the constant drumbeat of new findings on the health effects of cigarettes triggered a lengthy process where nations restricted advertisements for cigarettes on TV, the radio and in public spaces. In the United States, bans began with cigarette advertising on television, and grew to covering the sponsorship of events, public transit ads and more.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="7BNZVWICBFARJLY3XWD3MTUDNM" data-el="text" dir="null">Today, dozens of countries — including the <a href="https://www.fda.gov/tobacco-products/products-guidance-regulations/advertising-and-promotion" target="_blank">United States</a>,<b> </b><a href="https://www.tobaccocontrollaws.org/legislation/china" target="_blank">China</a> and the <a href="https://health.ec.europa.eu/tobacco/ban-cross-border-tobacco-advertising-and-sponsorship_en" target="_blank">European Union</a><b> — </b>have bans, restrictions or other limitations on selling tobacco products. There is even an <a href="https://fctc.who.int/who-fctc/overview" target="_blank">international treaty</a> under the World Health Organization, adopted in 2003,<b> </b>that urges all countries to enact bans that target all forms of tobacco advertising. There are currently 168 signatories on the treaty; the<b> </b>United States has signed on the treaty but not ratified it.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="JOY3PX4PRZGEHEGXVR4YQMOPKI" data-el="text" dir="null">Research shows that those bans that block TV, radio, print and in-store advertising — as well as sponsorship of events — are most effective at stopping smoking, particularly among young people who have yet to start smoking in the first place. As of 2017, full bans were implemented in less than 20 percent of countries worldwide.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="DVWP7M35TJHP3AL725OTGVECA4" data-el="text" dir="null">But bans that are partial, such as those that only target TV commercials, are less effective. Companies may just reallocate their advertising budgets to other media, or shift to sponsoring sports teams and similar.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="4QV5CMPSZRBGFGXAYE5DTBPUXI" data-el="text" dir="null">“It’s like a tube of toothpaste,” said David Hammond, a professor of public health at the University of Waterloo. “If you press in just one spot, it just squeezes to another part of the tube.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="I25TBOYU2VB4XGZL6BF3C7COGQ" data-el="text" dir="null">Fossil fuels also present a particular challenge: While an individual can choose not to smoke, it is almost impossible to disconnect from an electricity grid that runs partly on fossil fuels. Ad bans can target some discretionary spending, like cruise ships and air travel, but oil, gas and coal are deeply embedded in everyday life.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="II7OMBGFZ5H6TMYA5PJZHESDRQ" data-el="text" dir="null">Parker, the city councilor, says that there are still reasons to target sources of global warming. “We protect people from things like gambling, alcohol, and tobacco,” he said. “Climate change is a different type of harm, but it’s still a harm.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="4NBEDPX22FCCTLYGNBNSA2553U" data-el="text" dir="null">Meanwhile, some advocates and scholars emphasize that advertising allows companies to shape their public image, which can protect them from stricter regulation.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="EI4ESXJ4LFAT5K36RPXITCKRTI" data-el="text" dir="null">Researchers say that fossil fuel companies use ads to maintain their “social license to operate” — a shorthand for a corporation’s ability to be seen as acceptable by society and policymakers. By showing ads connecting their operations to clean energy, jobs, or energy security — and sponsoring popular events — fossil fuel companies can bolster their reputations in the public sphere.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="2V2LR5FUEZCLXK5QJ6MRRR2UHU" data-el="text" dir="null">“Political scientists refer to fossil fuel advertising as a form of ‘outside’ lobbying,” said Geoffrey Supran, associate professor of environmental science and policy at the University of Miami. “It complements lobbying inside the Hill and in state governments and so on.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="R7TNUGIUYRA5VN4CEXOHFHXQXM" data-el="text" dir="null">Earlier this month, for example, the fossil fuel company Chevron sponsored the annual Congressional Baseball Game — which was interrupted by <a href="https://www.washingtonpost.com/dc-md-va/2024/06/12/congressional-baseball-game-arrests-climate-activists/?itid=lk_inline_manual_32" target="_blank">climate protesters</a>.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="ZYTXPN4XDBBKRBK7GWJBLHBSLI" data-el="text" dir="null">Robert Brulle, a visiting professor of environment and society at Brown University, says that advertising allows fossil fuel companies to help define the solutions to climate change — such as things like carbon capture from oil and gas plants. “They’re saying ‘We need to be part of the solution, we have the technical know-how,’” Brulle explained.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="3ETU2DQF3FCQXDVG64OPJTE5ZM" data-el="text" dir="null">In one <a href="https://link.springer.com/article/10.1007/s10584-019-02582-8" target="_blank">study</a> by Brulle and his co-authors, the researchers found that fossil fuel companies increased their advertising spending in response to congressional attention to and media coverage of climate change.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="LF53Y2HKRVA7BHQEM4GUHLQHLY" data-el="text" dir="null">Even if there were substantial advertising bans instituted for fossil fuels, it would be difficult to measure how such bans affect a company’s reputation. But some experts believe that it could make a difference. “It would be monumental,” said Supran. “It could loosen the stranglehold of the industry in a way that would politically and financially open the door to lower carbon technologies.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="YQFY2ZL6GFDE5BR6LZC7SGS4BE" data-el="text" dir="null">For now, ad bans are still only instituted in a small number of cities and nations worldwide — in a manner not so different from how tobacco advertising bans began. “It happened incrementally,” said Hammond. “It was a multi-decade process.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Packaging Swift Apps for Alpine Linux (108 pts)]]></title>
            <link>https://mko.re/blog/swift-alpine-packaging/</link>
            <guid>40697992</guid>
            <pubDate>Sun, 16 Jun 2024 15:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mko.re/blog/swift-alpine-packaging/">https://mko.re/blog/swift-alpine-packaging/</a>, See on <a href="https://news.ycombinator.com/item?id=40697992">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><header><img src="https://mko.re/blog/swift-alpine-packaging/hero.jpg"><a href="https://mko.re/blog/">BLOG</a></header><section><p>While trying to build my <a href="https://github.com/remko/age-plugin-se">Age Apple Secure Enclave
plugin</a>, a small Swift CLI app, on
<a href="https://www.alpinelinux.org/">Alpine Linux</a>, I realized that the Swift
toolchain doesn’t run on Alpine Linux. The upcoming Swift 6 will support
(cross-)compilation to a static (musl-based) Linux target, but I suspect an
Alpine Linux version of Swift itself isn’t going to land soon. So, I
explored some alternatives for getting my Swift app on Alpine.</p><p>You can find all the scripts used in this post in <a href="https://github.com/remko/age-plugin-se/tree/main/Scripts/alpine">the <code>age-plugin-se</code> repository</a>.</p><h2 id="option-1-running-a-pre-built-binary">Option 1: Running a pre-built binary</h2><p>A first option is to use a pre-built dynamically linked binary compiled by
Swift on e.g. Debian, and get it to run on Alpine.</p><p>Installing <a href="https://pkgs.alpinelinux.org/packages?name=gcompat"><code>gcompat</code></a> is
usually the easiest way of getting glibc binaries to run
Alpine. Unfortunately, this doesn’t seem to cut it: the binary crashes at load time (most likely due to incompatibilities of libraries such as libstdc++):</p><pre><code>Error relocating age-plugin-se: fts_open: symbol not found
Error relocating age-plugin-se: fts_read: symbol not found
Error relocating age-plugin-se: fts_close: symbol not found
Error relocating age-plugin-se: fts_set: symbol not found
</code></pre><p>The second recommended workaround is installing a glibc-based distribution in a
chroot, and set up the loader using symlinks, as described <a href="https://wiki.alpinelinux.org/wiki/Running_glibc_programs">in this
article</a>. After
setting up a Debian chroot this way, the pre-built binaries generated by Swift
on Debian work as expected. However, having to go through this setup on every
installation just to get a simple app working is still a nuisance.</p><h2 id="option-2-packaging-a-binary-with-loader--libraries">Option 2: Packaging a binary with loader &amp; libraries</h2><p>A second option is to create a package containing the
(glibc-based) Swift-compiled binary, bundled with all its dynamic library
dependencies, including the glibc <code>ld</code> dynamic linker used to load the binary.</p><p>For <code>age-plugin-se</code>, I run the entire procedure from <a href="https://github.com/remko/age-plugin-se/blob/main/Scripts/alpine/chroot-build.sh">a shell script</a> running on Alpine. The script has following
steps:</p><ol><li><p>Create a Debian chroot (using <a href="https://wiki.debian.org/Debootstrap">debootstrap</a>)</p></li><li><p>Install <a href="https://www.swift.org/download/">the Swift compiler</a> (and all its
dependencies) in the chroot</p></li><li><p>Copy the sources of the Swift app into the chroot</p></li><li><p>Use Swift inside the chroot to compile the sources into a (glibc-based) executable.</p><p>This executable is loaded using the glibc <code>ld</code> loader, which will be bundled at a
package-specific private location (<code>/usr/lib/my-package/ld-....</code>).
The loader is always hard-coded as an absolute path into an
executable, so you need to tell the Swift compiler the full path where
the custom loader will be located after installation. This is done using the
<code>--dynamic-linker</code> linker flag.</p><p>All the dynamic library dependencies will also be included in the package.
To make the linker find these (and not pick up the system ones if they exist),
you also have to set the run-time search path
of the executable to a relative path where these libraries will be shipped.
This is done using the <code>-rpath</code> linker flag. Since the executable will be installed in
<code>/usr/bin</code>, the relative path where the dynamic libraries will end up will
be <code>../lib/my-package</code>.</p><p>The full Swift compiler invocation looks like:</p><pre><code>swift build -c release --static-swift-stdlib \
  -Xlinker --dynamic-linker=/usr/lib/my-package/ld-linux-x86-64.so.2 \
  -Xlinker -rpath='$ORIGIN'/../lib/my-package
</code></pre></li><li><p>Copy all files from the chroot dir to the system (or package) dir:
the resulting executable is copied to <code>/usr/bin</code>, and the dynamic linker
(<code>ld-linux-x86-64.so.2</code> for <code>x86-64</code> platforms, <code>ld-linux-aarch64.so.1</code> for
<code>aarch64</code> platforms) and all dynamic libraries (<code>libc.so.6</code>,
<code>libstdc++.so.6</code>, <code>libgcc_s.so.1</code>, <code>libm.so.6</code>) to <code>/usr/lib/my-package</code>.</p></li></ol><p>Note that, because the script runs commands in a chroot, it has to
be run with root privileges.</p><p>The <a href="https://github.com/remko/age-plugin-se/blob/main/Scripts/alpine/chroot-build.sh">chroot build script</a> can finally be integrated into <a href="https://github.com/remko/age-plugin-se/blob/main/Scripts/alpine/APKBUILD">an <code>APKBUILD</code> script</a> to create a self-contained Alpine package:</p><pre><code>$ abuild -r
&gt;&gt;&gt; age-plugin-se: Building main/age-plugin-se 0.1.3-r0 (using abuild 3.13.0-r3) 
...
&gt;&gt;&gt; age-plugin-se: Build complete

$ ls ~/packages/main/aarch64/*.apk
age-plugin-se-0.1.3-r0.apk
age-plugin-se-doc-0.1.3-r0.apk
</code></pre><p>The resulting package can then be installed on a clean Alpine system using <code>apk add</code>, without any other steps or requirements:</p><pre><code>$ doas apk add ./age-plugin-se-0.1.3-r0.aarch64.apk 
(1/1) Installing age-plugin-se (0.1.3-r0)
OK: 237 MiB in 72 packages

$ age-plugin-se --version
v0.1.3
</code></pre><h2 id="option-3-cross-compiling-a-static-linux-binary">Option 3: (Cross-)Compiling a static Linux binary</h2><p>Swift 6 will support compiling a fully static Linux binary, using
<a href="https://www.swift.org/documentation/articles/static-linux-getting-started.html">musl</a>
as its standard C library. The complete instructions for creating a static
Linux build of your app can be found <a href="https://www.swift.org/documentation/articles/static-linux-getting-started.html">on the Swift.org
page</a>.</p><p>Since you can even create Linux binaries using the macOS toolchain, and since
there currently only is a Swift 6 build for macOS, I adapted the <code>age-plugin-se</code>
packaging procedure to create a static Linux binary on macOS for all architectures.</p><p>Although the resulting static Linux binaries run fine on Alpine Linux, I wanted
to have a cleaner way of installing the package instead of just extracting the
package tarball somewhere. Since I’m building the binaries on macOS, and
Alpine’s <a href="https://wiki.alpinelinux.org/wiki/Abuild_and_Helpers"><code>abuild</code></a>
package build system doesn’t run on macOS, I created <a href="https://github.com/remko/age-plugin-se/blob/main/Scripts/alpine/dir2apk.go">a Go script to convert
the binary release tarball into an <code>.apk</code>
file</a>.
This script implements the <a href="https://wiki.alpinelinux.org/wiki/Apk_spec">APK package
specification</a> in pure Go, and
creates and signs an <code>.apk</code> without relying on external tools (such as abuild).
This script is integrated into the GitHub workflow to package a binary release
of <code>age-plugin-se</code>.</p><h2 id="package-size">Package size</h2><p>Here’s a comparison of the package sizes of the above approaches:</p><table><thead><tr><th>Package</th><th>OS</th><th>Size</th></tr></thead><tbody><tr><td>Dynamic</td><td>macOS</td><td>252 KiB</td></tr><tr><td>Dynamic</td><td>Linux</td><td>1.2 MiB</td></tr><tr><td>Dynamic + static Swift stdlib</td><td>Linux</td><td>43.4 MiB</td></tr><tr><td>Dynamic + static Swift stdlib + dylibs</td><td>Linux</td><td>48.1 MiB</td></tr><tr><td>Static</td><td>Linux</td><td>100.4 MiB</td></tr></tbody></table><p>The minimal baseline package is the dynamically linked binary on macOS, which
comes in at 252 KiB. This binary links dynamically against the Swift standard
library and all required system libraries.</p><p>Linking dynamically on Linux yields a bigger binary (1.2 MiB). This is because
on Linux, <code>age-plugin-se</code> compiles against <a href="https://github.com/apple/swift-crypto">Swift Crypto</a>, a drop-in replacement for the <a href="https://developer.apple.com/documentation/cryptokit">CryptoKit</a> framework on macOS. Swift Crypto uses <a href="https://boringssl.googlesource.com/boringssl/">BoringSSL</a>, which is linked statically into the binary (contrary to CryptoKit on macOS, which is a dynamic system library). This causes the binary to be
bigger, although the size increase is still limited in absolute numbers.</p><p>Using dynamic linking on Linux would require the Swift standard library to be
present on the target system. Since Swift isn’t always available on
Linux distributions (including Alpine), this isn’t a practical solution.
By statically linking the Swift standard library into the binary (using
<code>--static-swift-stdlib</code>), the dependency on Swift can be removed. Doing this
makes the binary a lot larger, though: 43.4 MiB.</p><p>The binary with static Swift standard library still depends on system libraries
(libc, libstdc++, libm, …). As explained above, these aren’t available on
Alpine Linux, so we package these together with the binary. Doing this adds a few
megabytes to the package, resulting in a total of 48.1 MiB.</p><p>Finally, creating a statically linked Swift binary, which avoids any dependency
on system libraries, results in more than double the size of the packaged
dynamic binary with all its deendencies: 100.4 MiB. I’m not sure why the package
is so much larger (maybe some link-time optimizations and dead-code elimination that
isn’t done), and I don’t think there’s a good reason in theory for it to be
this way. I hope this is something that will be improved in later releases.</p></section></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Raspberry Pi 5 Is No Match for a Tini-Mini-Micro PC (405 pts)]]></title>
            <link>https://louwrentius.com/the-raspberry-pi-5-is-no-match-for-a-tini-mini-micro-pc.html</link>
            <guid>40697831</guid>
            <pubDate>Sun, 16 Jun 2024 15:38:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://louwrentius.com/the-raspberry-pi-5-is-no-match-for-a-tini-mini-micro-pc.html">https://louwrentius.com/the-raspberry-pi-5-is-no-match-for-a-tini-mini-micro-pc.html</a>, See on <a href="https://news.ycombinator.com/item?id=40697831">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              <p>I've always been fond of the idea of the Raspberry Pi. An energy efficient, small, cheap but capable computer. An ideal home server. Until the Pi 4, the Pi was not that capable, and only with the relatively recent Pi 5 (fall 2023) do I feel the Pi is OK performance wise, although still hampered by SD card performance<sup id="fnref:good"><a href="#fn:good">1</a></sup>. And the Pi isn't that cheap either.</p>
<p>The Pi 5 can be fitted with an NVME SSD, but for me it's too little, too late.
Because I feel there is a type of computer on the market, that is much more compelling than the Pi. </p>
<p>I'm talking about the <a href="https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/">tinyminimicro</a> home lab 'revolution' started by
<a href="https://www.servethehome.com/">servethehome.com</a> about four years ago (2020).</p>
<p><img alt="mini pc" src="https://louwrentius.com/static/images/tmm/tmm01.webp"></p>
<p><em>A 1L mini PC (Elitedesk 705 G4) with a Raspberry Pi 5 on top</em></p>
<p>During the pandemic, the Raspberry Pi was in short supply and people started looking for alternatives. The people at servethehome realised that these small enterprise desktop PCs could be a good option. Dell (micro), Lenovo (tiny) and HP (mini) all make these small desktop PCs, which are also known as 1L (one liter) PCs.</p>
<p>These Mini PC are not cheap<sup id="fnref:cheap"><a href="#fn:cheap">2</a></sup> when bought new, but older models are sold at a very steep discount as enterprises offload old models by the thousands on the second hand market (through intermediates).</p>
<p>Although these computers are often several years old, they are still much faster than a Raspberry Pi (including the Pi 5) and can hold more RAM.</p>
<p>I decided to buy two HP Elitedesk Mini PCs to try them out, one based on AMD and the other based on AMD.</p>
<h3>The Hardware</h3>
<table>
<thead>
<tr>
<th></th>
<th><a href="https://support.hp.com/th-en/document/c05371240">Elitedesk Mini G3 800</a></th>
<th><a href="https://support.hp.com/us-en/document/c06101574">Elitedesk Mini G4 705</a></th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>Intel i5-6500 (65W)</td>
<td>AMD Ryzen 3 PRO 2200GE (35W)</td>
</tr>
<tr>
<td>RAM</td>
<td>16 GB (max 32 GB)</td>
<td>16 GB (max 32 GB)</td>
</tr>
<tr>
<td>HDD</td>
<td>250 GB (SSD)</td>
<td>250 GB (NVME)</td>
</tr>
<tr>
<td>Network</td>
<td>1Gb (Intel)</td>
<td>1Gb (Realtek)</td>
</tr>
<tr>
<td>WiFi</td>
<td>Not installed</td>
<td>Not installed</td>
</tr>
<tr>
<td>Display</td>
<td>2 x DP, 1 x VGA</td>
<td>3 x DP</td>
</tr>
<tr>
<td>Remote management</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Idle power</td>
<td>4 W</td>
<td>10 W</td>
</tr>
<tr>
<td>Price</td>
<td>€160</td>
<td>€115</td>
</tr>
</tbody>
</table>
<p>The AMD-based system is cheaper, but you 'pay' in higher idle power usage. In absolute terms 10 watt is still decent, but the Intel model directly competes with the Pi 5 on idle power consumption. </p>
<p><a href="https://louwrentius.com/static/images/tmm/tmm03.webp"><img alt="inside the mini pic" src="https://louwrentius.com/static/images/tmm/tmm03_small.webp"></a></p>
<p><em>Elitedesk 705 left, Elitedesk 800 right (click to enlarge)</em></p>
<p>Regarding display output, these devices have two fixed displayport outputs, but there is one port that is configurable. It can be displayport, VGA or HDMI. Depending on the supplier you may be able to configure this option, or you can buy them separately for €15-€25 online.</p>
<p><a href="https://louwrentius.com/static/images/tmm/HPEliteDesk800G3DesktopMini.pdf"><img alt="back800" src="https://louwrentius.com/static/images/tmm/tmm06.webp"></a>
<a href="https://louwrentius.com/static/images/tmm/HPEliteDesk705G4DesktopMini.pdf"><img alt="back705" src="https://louwrentius.com/static/images/tmm/tmm05.webp"></a>
<em>Click on image for official specs in PDF format</em></p>
<p>Both models seem to be equipped with socketed CPUs. Although options for this formfactor are limited, it's possible to upgrade.</p>
<h3>Comparing cost with the Pi 5</h3>
<p>The Raspberry Pi 5 with (max) 8 GB of RAM costs ~91 Euro, almost exactly the same price as the AMD-based mini PC<sup id="fnref:psu"><a href="#fn:psu">3</a></sup> in its base configuration (8GB RAM). Yet, with the Pi, you still need:</p>
<ol>
<li>power supply (€13)</li>
<li>case (€11)</li>
<li>SD card or NVME SSD (€10-€45)</li>
<li>NVME hat (€15) (optional but would be more comparable)</li>
</ol>
<p>It's true that I'm comparing a new computer to a second hand device, and you can decide if that matters in this case. With a complete Pi 5 at around €160 including taxes and shipping, the AMD-based 1L PC is clearly the cheaper and still more capable option.</p>
<h3>Comparing performance with the Pi 5</h3>
<p>The first two rows in this table show the Geekbench 6 score of the Intel and AMD mini PCs I've bought for evaluation. I've added the benchmark results of some other computers I've access to, just to provide some context.</p>
<table>
<thead>
<tr>
<th>CPU</th>
<th>Single-core</th>
<th>Multi-core</th>
</tr>
</thead>
<tbody>
<tr>
<td>AMD Ryzen 3 PRO 2200GE (32W)</td>
<td>1148</td>
<td>3343</td>
</tr>
<tr>
<td>Intel i5-6500 (65W)</td>
<td>1307</td>
<td>3702</td>
</tr>
<tr>
<td>Mac Mini M2</td>
<td>2677</td>
<td>9984</td>
</tr>
<tr>
<td>Mac Mini i3-8100B</td>
<td>1250</td>
<td>3824</td>
</tr>
<tr>
<td>HP Microserver Gen8 Xeon E3-1200v2</td>
<td>744</td>
<td>2595</td>
</tr>
<tr>
<td>Raspberry Pi 5</td>
<td>806</td>
<td>1861</td>
</tr>
<tr>
<td>Intel i9-13900k</td>
<td>2938</td>
<td>21413</td>
</tr>
<tr>
<td>Intel E5-2680 v2</td>
<td>558</td>
<td>5859</td>
</tr>
</tbody>
</table>
<p>Sure, these mini PCs won't come close to modern hardware like the Apple M2 or the intel i9. But if we look at the performance of the mini PCs we can observe that:</p>
<ol>
<li>The Intel i5-6500T CPU is 13% faster in single-core than the AMD Ryzen 3 PRO</li>
<li>Both the Intel and AMD processors are 42% - 62% faster than the Pi 5 regarding single-core performance.</li>
</ol>
<h3>Storage (performance)</h3>
<p>If there's one thing that really holds the Pi back, it's the SD card storage.
If you buy a decent SD card (A1/A2) that doesn't have terrible random IOPs performance, you realise that you can get a SATA or NVME SSD for almost the same price that has more capacity and much better (random) IO performance.</p>
<p>With the Pi 5, NVME SSD storage isn't standard and requires an extra hat. I feel that the missing integrated NVME storage option for the Pi 5 is a missed opportunity that - in my view - hurts the Pi 5.</p>
<p>Now in contrast, the Intel-based mini PC came with a SATA SSD in a special mounting bracket. That bracket also contained a small fan(1) to keep the underlying NVME storage (not present) cooled. </p>
<p><a href="https://louwrentius.com/static/images/tmm/tmm04.webp"><img alt="inside the mini pic" src="https://louwrentius.com/static/images/tmm/tmm04_small.webp"></a></p>
<p><em>There is a fan under the SATA SSD (click to enlarge)</em></p>
<p>The AMD-based mini PC was equipped with an NVME SSD and was not equipped with the SSD mounting bracket. The low price must come from somewhere...</p>
<p>However, both systems have support for SATA SSD storage, an 80mm NVME SSD and a small 2230 slot for a WiFi card. There seems no room on the 705 G4 to put in a small SSD, but there are adapters available that convert the WiFi slot to a slot usable for an extra NVME SSD, which might be an option for the 800 G3.</p>
<h3>Noice levels (subjective)</h3>
<p>Both systems are barely audible at idle, but you will notice them (if you sensitive to that sort of thing). The AMD system seems to become quite loud under full load. The Intel system also became loud under full load, but much more like a Mac Mini: the noise is less loud and more tolerable in my view.</p>
<h2>Idle power consumption</h2>
<h3>Elitedesk 800 (Intel)</h3>
<p>I can get the Intel-based Elitedesk 800 G3 to 3.5 watt at idle. Let that sink in for a moment. That's about the same power draw as the Raspberry Pi 5 at idle!</p>
<p>Just installing Debian 12 instead of Windows 10 makes the idle power consumption drop from 10-11 watt to around 7 watt. </p>
<p>Then on Debian, you:</p>
<ol>
<li>run <code>apt install powertop</code></li>
<li>run <code>powertop --auto-tune</code> (saves ~2 Watt)</li>
<li>Unplug the monitor (run headless) (saves ~1 Watt)</li>
</ol>
<p>You have to put the <code>powertop --auto-tune</code> command in /etc/rc.local:</p>
<div><pre><span></span><code><span>#!/usr/bin/env bash</span>
powertop<span> </span>--auto-tune
<span>exit</span><span> </span><span>0</span>
</code></pre></div>

<p>Then apply <code>chmod +x /etc/rc.local</code></p>
<p>So, for about the same idle power draw you get so much more performance, and go beyond the max 8GB RAM of the Pi 5.</p>
<h3>Elitedesk 705 (AMD)</h3>
<p>I managed to get this system to 10-11 watt at idle, but it was a pain to get there. </p>
<p>I measured around 11 Watts idle power consumption running a preinstalled Windows 11 (with monitor connected). After installing Debian 12 the system used 18 Watts at idle and so began a journey of many hours trying to solve this problem. </p>
<p>The culprit is the integrated Radeon Vega GPU. To solve the problem you have to:</p>
<ol>
<li>Configure the 'bios' to only use UEFI</li>
<li>Reinstall Debian 12 using UEFI</li>
<li>install the appropriate firmware with <code>apt install firmware-amd-graphics</code></li>
</ol>
<p>If you boot the computer using legacy 'bios' mode, the AMD Radeon firmware won't load no matter what you try. You can see this by issuing the commands:</p>
<div><pre><span></span><code>rmmod<span> </span>amdgpu
modprobe<span> </span>amdgpu
</code></pre></div>

<p>You may notice errors on the physical console or in the logs that the GPU driver isn't loaded because it's missing firmware (a lie).</p>
<p>This whole process got me to around 12 Watt at idle. To get to <strong>~10 Watts idle</strong> you need to do also run <code>powertop --auto-tune</code> and disconnect the monitor, as stated in the 'Intel' section earlier.</p>
<p>Given the whole picture, 10-11 Watt at idle is perfectly okay for a home server, and if you just want the cheapest option possible, this is still a fine system.</p>
<h2>KVM Virtualisation</h2>
<p>I'm running vanilla KVM (Debian 12) on these Mini PCs and it works totally fine. I've created multiple virtual machines without issue and performance seemed perfectly adequate. </p>
<h2>Boot performance</h2>
<p>From the moment I pressed the power button to SSH connecting, it took 17 seconds for the Elitedesk 800.</p>
<p>The Elitedesk 705 took 33 seconds until I got an SSH shell.</p>
<p>These boot times include the 5 second boot delay within the GRUB bootloader screen that is default for Debian 12.</p>
<h2>Remote management support</h2>
<p>Some of you may be familiar with <a href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface">IPMI</a> (ILO, DRAC, and so on) which is standard on most servers. But there is also similar technology for (enterprise) desktops.</p>
<p>Intel <a href="https://en.wikipedia.org/wiki/Intel_Management_Engine">AMT/ME</a> is a technology used for remote out-of-band management of computers. It can be an interesting feature in a homelab environment but I have no need for it. If you want to try it, you can follow <a href="https://en.wikipedia.org/wiki/Intel_Management_Engine">this guide</a>.</p>
<p>For most people, it may be best to disable the AMT/ME feature as it has a history of security vulnerabilities. This may not be a huge issue within a trusted home network, but you have been warned.</p>
<p>The AMD-based Elitedesk 705 didn't came with equivalent remote management capabilities as far as I can tell.</p>
<h2>Alternatives</h2>
<p>The models discussed here are older models that are selected for a particular price point. Newer models from Lenovo, HP and Dell, equip more modern processors which are faster and have more cores. They are often also priced significantly higher. </p>
<p>If you are looking for low-power small formfactor PCs with more potent or customisable hardware, you may want to look at second-hand NUC formfactor PCs.</p>
<h2>Stacking multiple mini PCs</h2>
<p>The AMD-based Elitedesk 705 G4 is closed at the top and it's possible to stack other mini PCs on top. </p>
<p>The Intel-based Elitedesk 800 G3 has a perforated top enclosure, and putting another mini pc on top might suffocate the CPU fan.</p>
<p><img alt="topbottom" src="https://louwrentius.com/static/images/tmm/tmm07_small.webp"></p>
<p>As you can see, the bottom/foot of the mini PC doubles as a VESA mount and has four screw holes. By putting some screws in those holes, you may effectively create standoffs that gives the machine below enough space to breathe (maybe you can use actual standoffs).</p>
<h2>Evaluation and conclusion</h2>
<p>I think these second-hand 1L tinyminimicro PCs are better suited to play the role of home (lab) server than the Raspberry Pi (5). </p>
<p>The increased CPU performance, the built-in SSD/NVME support, the option to go beyond 8 GB of RAM (up to 32GB) and the price point on the second-hand market really makes a difference. </p>
<p>I love the Raspberry Pi and I still have a ton of Pi 4s. This <a href="https://louwrentius.com/i-made-my-blog-solar-powered-then-things-escalated.html">solar-powered</a> blog is hosted on a Pi 4 because of the low power consumption and the availability of GPIO pins for the solar status display. </p>
<p>That said, unless the Raspberry Pi becomes a lot cheaper (and more potent), I'm not so sure it's such a compelling home server.</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Interview with AMD CEO Lisa Su About Solving Hard Problems (108 pts)]]></title>
            <link>https://stratechery.com/2024/an-interview-with-amd-ceo-lisa-su-about-solving-hard-problems/</link>
            <guid>40697341</guid>
            <pubDate>Sun, 16 Jun 2024 14:29:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stratechery.com/2024/an-interview-with-amd-ceo-lisa-su-about-solving-hard-problems/">https://stratechery.com/2024/an-interview-with-amd-ceo-lisa-su-about-solving-hard-problems/</a>, See on <a href="https://news.ycombinator.com/item?id=40697341">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-13089">
	<!-- .entry-header -->

	<div>
		<p>Good morning,</p>
<p>This week’s Stratechery Interview is with AMD CEO <a href="https://x.com/lisasu">Lisa Su</a>. Su began her career at Texas Instruments, after earning her PhD in electrical engineering at MIT, where she played a significant role in developing silicon-on-insulator transistor technology. Su then spent 12 years at IBM, where she led the development of copper interconnects for semiconductors, served as technical assistant to CEO Lou Gerstner, and led the team that created the Cell microprocessor used in the PlayStation 3. After a stint as the CTO of Freescale Semiconductor, Su joined AMD in 2012, before ascending to the CEO role in 2014.</p>
<p>Su has led a remarkable run of success for AMD over the last decade. After decades of being an also-ran to Intel, AMD has developed the best x86 chips in the world, and continues to take significant share from Intel in datacenters in particular. AMD has also been a major player in console gaming, in addition to its traditional PC business and graphics chip business. That GPU business is now increasingly at center stage, as AMD takes on Nvidia in the market for datacenter GPUs.</p>
<p>In this interview, conducted a day after <a href="https://www.amd.com/en/corporate/events/computex.html">Su’s Computex keynote</a>, we talk about Su’s career path, including lessons she learned at her various stops to the top, before discussing why AMD has been able to achieve so much during her tenure. We discuss how the “ChatGPT” moment changed the industry, how AMD has responded, and why Su believes the long-run structure of the industry will ultimately work in the company’s favor.</p>
<p>As a reminder, all Stratechery content, including interviews, is available as a podcast; click the link at the top of this email to add Stratechery to your podcast player.</p>
<p>On to the Interview:</p>
<h3>An Interview with AMD CEO Lisa Su About Solving Hard Problems</h3>
<p><em>This interview is lightly edited for clarity.</em></p>
<p><strong>Topics:</strong><br>
<a href="#ibm">Lessons From IBM</a> | <a href="#amd">The AMD Turnaround</a> | <a href="#ai">AI</a> | <a href="#competition">Competing With Nvidia</a></p>

<h4>Lessons From IBM</h4>
<p><strong>Lisa Su, welcome to Stratechery.</strong></p>
<p><strong>Dr. Lisa Su:</strong> Thank you. It’s great to be here.</p>
<p><strong>I’m truly honored to have you here. I’ve had <a href="https://stratechery.com/2024/an-interview-with-arm-ceo-rene-haas/">the opportunity</a> to <a href="https://stratechery.com/2023/an-interview-with-intel-ceo-pat-gelsinger-about-intels-progress-towards-process-leadership/">talk to</a> a lot of <a href="https://stratechery.com/2023/an-interview-with-qualcomm-ceo-cristiano-amon/">your peers</a> in the <a href="https://stratechery.com/2023/an-interview-with-nvidia-ceo-jensen-huang-about-ais-iphone-moment/">semiconductor space</a>, and without fail one of the bits of feedback I get from my subscribers is, “When are you going to talk to Lisa Su?”. I want to thank you for helping me now avoid those emails going forward.</strong></p>
<p><strong>LS:</strong> Thank you, I appreciate that. I’m very happy to have this opportunity to chat.</p>
<p><strong>I know you don’t want to talk about yourself too much, but I need some fact verification here. We were just talking before we started recording, you were born in Taiwan, emigrated to the US when you were very young, eventually ended up at MIT, where as legend has it, you were deciding between computer science and electrical engineering, and chose electrical engineering because it was harder. Is this true?</strong></p>
<p><strong>LS:</strong> It is actually true. I was always around math and science, my parents were always saying, “You have to do these hard things”. When I went to MIT, at that time it was a decision between electrical engineering and computer science. Computer science, you could just write software programs, whereas electrical engineering, you had to build things. I wanted to build things.</p>
<p><strong>They had to actually work, right?</strong></p>
<p><strong>LS:</strong> Yes, that’s right.</p>
<p><strong>Your PhD was focused on silicon, on insulator technology, and then you went to IBM — you pioneered using <a href="https://www.ibm.com/history/copper-interconnects">copper interconnects</a> on chips. I have three questions about your IBM experience and what lessons you might’ve learned. Number one, when it comes to the copper interconnect, you said something, I think was to <a href="https://www.technologyreview.com/2006/05/10/229145/found-in-translation/">the MIT Review</a>, where you were ready to do something new, but your boss made you stay, and you felt like the actual learnings you accumulated in that time when you thought you were done were some of the most impactful. What were those learnings?</strong></p>
<p><strong>LS:</strong> I really learned so much when I was at IBM, it was the early part of my career. When you go to school and when you get a PhD, you think the sexy thing is the research that you do and the papers that you write, which we all write papers and things like that.</p>
<p>When you actually join a company and join a project, these projects are usually several years to complete something. But the “sexy stuff” is at the beginning when you’re coming up with the new ideas.</p>
<p>What I learned actually was one of the very first products that I worked on was a microprocessor with copper interconnects, and it turns out that the last 5% of what it takes to get a product out is probably the hardest, where most of the secret sauce is. And if you learn how to do that, then frankly, it’s—</p>
<p><strong>All the software engineers are saying, “Hey, it’s the same thing for us. Don’t you know?”</strong></p>
<p><strong>LS:</strong> (laughing) That might be true, that might be true. But we all have this view of what “secret sauce” is. It’s things like yields, reliability, when something goes wrong. When you’re trying to produce millions versus producing five of something, you learn a lot, I learned a ton.</p>
<p>Yes, as a young researcher you think, “Hey, I’m ready to move on to my next thing”, and you realize that it’s so rewarding to see your product actually ship and go on the shelf and you can walk into Best Buy and buy it. Those are the types of things that I learned.</p>
<p><strong>How much, even today, do you feel your time and attention ends up being balanced between what you’re building going forward versus actually executing and getting what you’ve promised out the door?</strong></p>
<p><strong>LS:</strong> Certainly today, I personally spend a lot of time looking forward in terms of roadmaps, forward in terms of technologies.</p>
<p><strong>You just have more patience for the rest of the company that’s trying to get it shipped.</strong></p>
<p><strong>LS:</strong> That’s right. Frankly, a lot of time on customers and markets, and where’s the market going and where should we be investing.</p>
<p><strong>Just out of curiosity, how deeply do you need to be involved in things like, not you specifically, but AMD generally, <a href="https://www.anandtech.com/show/2635">now that you’re fabless</a>, in thinking about that actual final mile? What’s the degree of interaction with, say, a TSMC or your packaging partners or whatever it might be, and actually getting the yields up and out the door?</strong></p>
<p><strong>LS:</strong> It’s definitely true for us as a fabless company or a design company. We are actually doing end-to-end development, so you can imagine from the day one of concept of a product — actually even before that, we’re thinking about what technologies are going to be ready, what are the next big things that we should bet on? That goes all the way through. Sometimes it could be a five-year cycle or even longer before the technology actually comes to fruition. We’re right there at the end as well, ensuring that it ships with high quality, at the right yields, right cost structure, in high volume production.</p>
<p>So, it’s really end-to-end and the difference is it’s not all in one company, which you would see in a more traditional integrated manufacturing model, it’s through partnership. We found that actually it works extremely well, because you have experts on all sides working together.</p>
<p><strong>The second IBM lesson I’m curious about is you worked on the <a href="https://forbes.com/forbes/2006/0130/076.html">Cell processor</a> that shipped in the PlayStation 3. That chip was a technological marvel, but the PlayStation 3 is viewed as the least successful PlayStation, which drove a real shift in Sony strategy in the long run, <a href="https://stratechery.com/2022/consoles-and-competition/">away from hardware differentiation towards exclusives</a>. I guess this is a two-parter, but number one: What did you take away from that experience? Number two ties into this: How much impact did that have on your later gaming experience? The gaming experience question is obvious. I’m more curious, was there a management takeaway from all the work you put into the Cell processor and the reality of how that manifested in the market?</strong></p>
<p><strong>LS:</strong> Yeah, it’s interesting that you mentioned that. I’ve been working on PlayStation for a long time, if you think about it, PlayStation 3, 4, 5…</p>
<p><strong>It’s like the common thread through your career.</strong></p>
<p><strong>LS:</strong> Across multiple companies, yes. What I would say honestly is, these are decisions that are made that are more around architectural decisions. From that standpoint, whether you talk about any of the PlayStation consoles or some of the other work that we’ve done in partnership — we being AMD, but it was similar during that time at IBM — it really is a close collaboration of what the customer or partner is trying to achieve.</p>
<p>The Cell processor was extremely ambitious at that time, thinking about the type of parallelism that it was trying to get out there. Again, I would say from a business standpoint, it was certainly successful. As you rank things, I think history will tell you that there may be different rankings.</p>
<p><strong>My perspective is, the console era has gone through phases, and that phase in PlayStation 1 and PlayStation 2, they made smart hardware decisions, and that differentiated their approach from Nintendo in particular. But once you went to HD, you had tremendous increase in cost of asset creation, you had developers heavily motivated to support multiple processors, you had game engines coming along. Suddenly, no one wanted to go to the burden of differentiating on the Cell, they just wanted to run on the Cell.</strong></p>
<p><strong>LS:</strong> Perhaps one could say, if you look in hindsight, programmability is so important.</p>
<p><strong>Right.</strong></p>
<p><strong>LS:</strong> Being able to have real business success on day one of anything, we have to think about both hardware and software. As we’ve seen, one of the things that I’m very proud of of the work that we’re doing or have done at AMD over the last 10-plus years was PlayStation 4 and PlayStation 5 is we’ve always had new leaps in hardware.</p>
<p><strong>Much easier.</strong></p>
<p><strong>LS:</strong> And they come with compatibility to the previous generations, which is super helpful.</p>
<p><strong>Number three: You were <a href="https://www.ibm.com/history/louis-gerstner">Lou Gerstner’s</a> technical assistant for a year. What did you learn from him? <a href="https://stratechery.com/2018/ibms-old-playbook/">This is a pro-Lou Gerstner podcast</a>, for the record.</strong></p>
<p><strong>LS:</strong> (laughing) You’ve done your homework, haven’t you? The year that I spent with Lou was one of the most educational experiences of my career. IBM was a fantastic company at talent development, so they identified people earlier in their career and they said, “Hey, what types of experiences would you like?”.</p>
<p>In my case they asked me, do you want to go on the technical track or do you want to be more on — let’s call it — the management track, the terminology would be an IBM Fellow or an IBM Vice President. Honestly, I didn’t think I was going to be smart enough to be an IBM Fellow. There were people like <a href="https://www.ibm.com/history/bob-dennard">Bob Dennard</a>, who are—</p>
<p><strong>I feel like there’s people that would dispute that characterization.</strong></p>
<p><strong>LS:</strong> There were amazing people there, and so I was like, “Okay, let me try this thing at management and business”. They gave me an opportunity to spend a year with Lou, he’s just an amazing person. If you think about someone out of school five years who has really only done, let’s call it, pure engineering, and then getting to sit for—</p>
<p><strong>It’s basically the best MBA in the world.</strong></p>
<p><strong>LS:</strong> Yes! Yes, it absolutely was, and what was most interesting to me is really understanding where he spent his time. The time was always trying to learn, was very externally focused, and understanding what’s going on in the market, what’s going on with customers.</p>
<p><strong>That ties into what you were saying what you do today.</strong></p>
<p><strong>LS:</strong> Exactly. How does that change your strategy and how does that change how you guide the leadership team? The fun thing that I got to do is, I got to teach him about technology. I would say, “Hey Lou, there’s this interesting new thing, Napster, people are downloading it” — I don’t know if you remember that.</p>
<p><strong>I do, I introduced my college dorm floor to Napster. That was my claim to fame in college.</strong></p>
<p><strong>LS:</strong> I got to introduce Lou to Napster. We were really thinking about what digital rights management meant at that point in time, those were just the types of things that we got to think about.</p>
<p><strong>The other thing that I always appreciate about Lou Gerstner is, to your point, it wasn’t just looking outside and understanding the market, what’s going on, but really understanding what was IBM, what was IBM intrinsically capable of and uniquely differentiated at. Basically my perspective is, IBM was big, and what does that actually mean? What can you actually bring to bear in a way? The whole middleware revolution, and look, we can solve this Internet problem for companies that are even older and bigger than us, and that’s going to be a differentiated thing. But then, obviously, it all fell apart. IBM should have done the cloud, Lou actually wrote that <a href="https://www.harperacademic.com/book/9780061756085/who-says-elephants-cant-dance/">in his book</a>, I don’t know how much looking backwards that was. If you had taken over after him, could you have led IBM to greater heights?</strong></p>
<p><strong>LS:</strong> I don’t know that I would’ve been on that path. I was a semiconductor person, I am a semiconductor person. If I really think about, IBM was such a wonderful career for me, but if I wanted to stay a semiconductor person, I had to go to a semiconductor company.</p>
<p><strong>You went to Freescale [Semiconductor] after that, much more in a business role.</strong></p>
<p><strong>LS:</strong> Yes.</p>
<p><strong>Was there a personal admission of, “Okay, I’m a business person now”, or was that in the choice when you went that direction?</strong></p>
<p><strong>LS:</strong> I have always straddled the technology/business line there. At Freescale I actually started as CTO. I joined as CTO, and then over a couple of years I ended up running the networking and a multimedia business, that was definitely a choice and the choice is, at the end of the day, I want to drive outcomes, and to drive outcomes requires, yes, the technology is great, but you need to have the right business strategy.</p>
<p><strong>Is that the limiter for a lot of technical people, is they under-appreciate all the drivers of outcomes that have nothing to do with technology?</strong></p>
<p><strong>LS:</strong> I think that that is something that technologists have to learn. And by the way, there are phenomenal CTOs who truly understand that. My CTO right now, <a href="https://www.amd.com/en/corporate/leadership/mark-papermaster.html">Mark Papermaster</a>, he was my partner in crime at IBM, we grew up together, and then we’ve been partners here at AMD, he truly understands that technology is great, but you also need to drive business outcomes. That’s what I love about what I get to do, because yes, I get to put together great tech with a phenomenal team, but there’s also an opportunity to drive very significant business outcomes.</p>

<h4>The AMD Turnaround</h4>
<p><strong>Let’s talk about AMD. I mentioned the console strategy earlier, that was a big shift in focus when you came on board. Was that just a view like, “Look, this is an easy win, high volume, we can get back in the game”? What was the thinking there?</strong></p>
<p><strong>LS:</strong> Well, I would never say anything is an easy win.</p>
<p><strong>Yeah, fair enough.</strong></p>
<p><strong>LS:</strong> Let me start with this notion of when I first joined AMD, we were probably 90 plus percent in the PC market and by the way, I really like the PC market. We’re going to talk more about it I’m sure.</p>
<p><strong>Absolutely. You spent the first 45 minutes talking about the PC market <a href="https://www.youtube.com/watch?v=MCi8jgALPYA">yesterday</a>.</strong></p>
<p><strong>LS:</strong> Yes, so I’ll caveat with that. But the PC market goes in cycles and the cycles can be quite dramatic.</p>
<p><strong>Very painful.</strong></p>
<p><strong>LS:</strong> They can be — I would use the word quite dramatic. So when you see from business strategy standpoint, it was really important for us early in the AMD days to diversify and get to a strategy where the underlying principle is around high-performance computing. We are a computing company, we are great at building computing capability, and now what are the markets that can really utilize that? Gaming is one of those markets and we are very fortunate to have both Sony and Microsoft as leading console manufacturers to choose us.</p>
<p><strong>Who was driving that shift to x86 in consoles? How much was this Sony learning the lesson of Cell? Was that you going to them and saying, “Look, this is the way to go”? How did this commonality of architecture develop?</strong></p>
<p><strong>LS:</strong> Yeah, look, I think it was a set of choices, so it was a choice between x86 and other architectures, and if you think about just the developer ecosystem around x86 when you’re thinking about software development, I think that was a very key piece, but I don’t know that the architecture itself was enough. I think the incredible graphics capability and the fact that graphics, especially if you want to customize graphics, there are very few companies who can do that, AMD was one of them.</p>
<p><strong>Even then, to what extent was there integration between the CPUs you’re delivering and the GPUs? <a href="https://www.nytimes.com/2006/07/24/technology/24cnd-semi.html">AMD had acquired ATI in 2006</a>, so before your time there, but was there any other company that could have actually delivered what you did for the consoles?</strong></p>
<p><strong>LS:</strong> I would say we were unique in what we were capable of doing for two reasons. One is we had the fundamental IP, so the combination of let’s call it the CPU or the microprocessor cores with the graphics IP capability, and we were willing to customize. Frankly, we have huge teams that were put on these projects to customize.</p>
<p><strong>Do you see this as a pattern where initially, it’s all about the cutting-edge, getting the best possible performance, but as it, I don’t want to say slows down, but capabilities commoditize, the customization is more important. I mean, <a href="https://stratechery.com/2020/amd-acquires-xilinx-microsofts-earnings-teams-wins/">you’ve acquired</a>…I can never pronounce it correctly.</strong></p>
<p><strong>LS:</strong> <a href="https://www.amd.com/en/corporate/xilinx-acquisition.html">Xilinx</a>.</p>
<p><strong>Xilinx, I’ve never been able to pronounce that word for two years. It sounds like this customization approach, there seems to be a common thread there. That’s something you want to leverage.</strong></p>
<p><strong>LS:</strong> The best way I say is there are a couple of principles. First, the fact is the world needs more semiconductors. Semiconductors, chips are now foundational to so much of what we do and much of what we do are, let’s call it standard products that fit a broad set of use cases. But you do find those high volume applications like game consoles, like some of the work that’s being done in the cloud right now, like some of the AI work I believe will be customized, and in these cases, because the volume is so high, it makes sense to customize. That’s something that I’ve always believed. It’s part of our strategy, it’s part of our strategy of deep partnerships. So if you have the right building blocks, then you can work with the broad set of customers to really figure out what they need to accomplish their vision.</p>
<p><strong>Is there a bit though, where as design costs as we move down the process curve are just becoming so astronomically high that there’s a bottom limit to customization, where only AMD has sufficient scale to customize, paradoxically?</strong></p>
<p><strong>LS:</strong> I think the important thing is looking at which markets really lend themselves to let’s call it significant customization, and it’s not everything. Probably your IoT device, you’re not going to want to do that because it just won’t return on investment. But for large computing capabilities, I think the combination of the right IP plus the ability to work deeply with partners. By the way, I should say, it doesn’t all have to be hardware customization, there’s a lot that we can do in software work as well, I think it is one of the important trends going forward.</p>
<p><strong>So I have to ask, you came to AMD, you were there for a couple of years, then you took over the CEO role. Was that another example of choosing the hard problem?</strong></p>
<p><strong>LS:</strong> I think it was. I’ll say it this way, when I joined AMD, it was really this idea that I’ve worked on high performance processors all my life, that was my background, there are very few companies in the United States that you could do that at. I always respected AMD very much as a company that mattered, but I thought I could make a difference, and so joining the company, I realized that, “Boy, there was a lot I had to learn”. Those first couple of years, I did learn a lot about just the market dynamics in this world, but it was also a phenomenal opportunity to make a difference.</p>
<p><strong>Where could you make the difference? We can see the difference — I mean, just look at <a href="https://www.google.com/finance/quote/AMD:NASDAQ?window=10Y">the stock chart</a>, and we can look at the performance of your chips. So in that context, it’s maybe hard to get in your exact state of mind 10 years ago, but what was your plan? What did you say, “Look, I can do this, there’s something, there’s a path here, I see it”? What was the path that you saw?</strong></p>
<p><strong>LS:</strong> What I saw that was very clear is that we had the building blocks of what you needed to build an incredible roadmap. We were very differentiated in those building blocks.</p>
<p><strong>What were those building blocks? Was that IP or was that customer relationships?</strong></p>
<p><strong>LS:</strong> It was high performance CPUs and high performance GPUs and if you think about it, those are pretty incredible building blocks. Now, what we didn’t have is a very clear strategy of what did we want to be when we grew up, and then an execution machine that could make that happen.</p>
<p>So from a strategy standpoint, I think we had some choices to make. If you remember back, this is 2014, the exciting thing then was mobile phones, like apps processors. So we would have these conversations like, “Should we go into phones?”, and we were like, “No, we shouldn’t because we’re not a phone company. There are others who are much better at that, we are a high performance company, so we have to build a roadmap that leverages our strengths and that requires us to revamp the way we do architecture and design and manufacturing”. I know how to do that, it’ll take time, you can’t do that in 12 months, I think I felt it would take five years. It would take five years, but it was very clear that we had the pieces, we just had to really methodically build that execution engine.</p>
<p><strong>Well, you mentioned manufacturing. <a href="https://www.wired.com/2012/03/amd-global-foundries/">AMD had spun out GlobalFoundries</a> before you took over, I want to use the technical term here, how much of a pain in the ass was the <a href="https://ir.amd.com/news-events/press-releases/detail/98/amd-amends-wafer-supply-agreement-with-globalfoundries">eternally amended wafer agreement</a> you had with GlobalFoundries? Was that just something you just had to deal with on a constant basis as you’re trying to execute the strategy?</strong></p>
<p><strong>LS:</strong> Well, to be fair, AMD and GlobalFoundries were one company at one time, yeah.</p>
<p><strong>It was there for a reason, it was understandable.</strong></p>
<p><strong>LS:</strong> Exactly. So that wafer supply agreement was something that was before my time, but I think it was one of the larger — if I think about the couple of big strategy things that we had to do, it was if you want to build high performance processors, you need the best technology partner, the best manufacturing partner and GlobalFoundries is a great company and they’re still a great partner. It’s just you need scale to be able to build at the bleeding-edge, and the scale didn’t exist.</p>
<p><strong>Was it almost a blessing in disguise when they internalized that and, “<a href="https://www.anandtech.com/show/13277/globalfoundries-stops-all-7nm-development">We’re not going to 7nm</a>“?</strong></p>
<p><strong>LS:</strong> It was a very good decision for both of us and financially, AMD had to —</p>
<p><strong>Yeah, you had to give all the money back that you had gotten originally.</strong></p>
<p><strong>LS:</strong> There was a business arrangement, but from a technology standpoint, it was absolutely the right thing to do, and like I said, GlobalFoundries is a great partner for us. I have tremendous respect for [GlobalFoundries CEO] <a href="https://gf.com/leadership/dr-thomas-caulfield/">Tom Caulfield</a> as a partner, and I think both companies were better off by focusing on what we were going to be good at.</p>
<p><strong>Well, you were the first high performance chip maker to move to chiplets, and everyone’s headed there now, so you’re definitely in the lead in that regard. Is there a bit where you were actually forced there because of the wafer agreement so that you could do some volume with GlobalFoundries, some with TSMC and still deliver your chips?</strong></p>
<p><strong>LS:</strong> Not at all. Actually, I think that was clearly one of the best decisions that we made, it wasn’t that clear at the time.</p>
<p><strong>Yeah, for sure.</strong></p>
<p><strong>LS:</strong> But what we were looking at is where was Moore’s Law going, and how were we going to differentiate ourselves? Frankly, our thought process was we needed to bring something to the processor market that was different, so building these big humongous chips that didn’t yield and were very expensive wasn’t going to be the answer.</p>
<p>So I remember very well spending time with Mark and our architects and trying to decide, “Is this the time that we go to chiplets? Is this the time we’re going to bet the company on going to chiplets?” And we said, “Yes, it is because we’re going to get to much higher performance, many more cores, as well as a much better cost point”, and it gave us tremendous flexibility, and we learned a lot along the way.</p>
<p>The first generation <a href="https://en.wikipedia.org/wiki/Zen_(first_generation)">Zen 1</a> chiplets were okay, but we had some programming model issues that we had to deal with and that got better with <a href="https://en.wikipedia.org/wiki/Zen_2">Zen 2</a> and really, really hit our stride with <a href="https://en.wikipedia.org/wiki/Zen_3">Zen 3</a>.</p>
<p><strong>When you were took over in 2014 and you felt like you could make a difference, I see a couple of big shifts here. So you have the shift to chiplets on your side, that’s around the time TSMC is beginning or transitioning to EUV. To what extent did you see those secular shifts in the market and that informed your decision that, “Look, there’s something I can do here”?</strong></p>
<p><strong>LS:</strong> Yeah, we definitely looked very much at the technology roadmaps and what TSMC was doing, as well as just where the packaging technology was at the time and we decided that this was the time to make the bet. I like to say that the world that we live in is we have to make bets that sometimes take three to five years to come to fruition.</p>
<p><strong>Yeah. I don’t mind asking you about 2014 decisions because that’s often when decisions that matter today were being made.</strong></p>
<p><strong>LS:</strong> That’s exactly right, and there was risk associated with that in terms of, “Would we actually get the performance that we thought we were going to get by going to chiplets?”, but we learned a ton, and I think history would say we made the right bet, but at the time, <a href="https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/">some of our competitors were calling it glue</a>, they were gluing chips together. It’s like, “We’re not gluing chips together”.</p>
<p><strong><a href="https://www.intel.com/content/www/us/en/foundry/chiplets.html">And now they’re doing the exact same thing</a>. What’s the balance of credit when you look back over the 10 years and AMD actually taking the performance lead in a really meaningful way in x86? Where do you balance the credit between your design decisions and being on the leading-edge process from TSMC and how that paid off?</strong></p>
<p><strong>LS:</strong> I really believe that they are inextricably linked.</p>
<p><strong>Yeah, the decisions went together.</strong></p>
<p><strong>LS:</strong> Absolutely, and it’s one of those things what we found was so helpful, and TSMC is a phenomenal partner in this realm. It’s when you take a lot of design risk, you want to know that your technology is rock-solid so that you know where to spend time and effort.</p>
<p><strong>That’s what TSMC and ASML did, like going to 300mm and then going to EUV, that co-partnership, they had proved out that could be done and then you were able to do it with you at the same time, following on from that.</strong></p>
<p><strong>LS:</strong> That’s right, I think it’s been a very synergistic partnership.</p>
<p><strong>AMD’s most consequential moment before your tenure was actually, we were talking about this earlier, is when they went from <a href="https://en.wikipedia.org/wiki/X86-64">x86 to 64 bit</a> and dragged Intel kicking and screaming in that regard, and it’s kind of a hardware and a software story. That was before your time, but it strikes me that one of the ongoing critiques of AMD is the software needs to be better. Where is the software piece of this? You can’t just be a hardware cowboy. When you joined in, was there a sense of, “Look, we had this opportunity, we could have built on this over time”. What is the reticence to software at AMD and how have you worked to change that?</strong></p>
<p><strong>LS:</strong> Well, let me be clear, there’s no reticence at all.</p>
<p><strong>Is that a change though?</strong></p>
<p><strong>LS:</strong> No, not at all. I think we’ve always believed in the importance of the hardware-software linkage and really, the key thing about software is, we’re supposed to make it easy for customers to use all of the incredible capability that we’re putting in these chips, there is complete clarity on that.</p>
<p>I think what you will see is that we’ve actually been on several arcs of technology development. So, the CPU arc and everything that we’ve done to build the Zen product portfolio. Now, we just <a href="https://www.amd.com/en/newsroom/press-releases/2024-6-2-amd-unveils-next-gen-zen-5-ryzen-processors-to-p.html">previewed Zen 5 here at Computex</a>, in the data center, and then launched it in the client products. That particular arc was one arc and now we’re in sort of the next arc, which is around—</p>
<p><strong>The GPU arc.</strong></p>
<p><strong>LS:</strong> Yeah, AI and GPU.</p>
<p><strong>I do want to ask you one other thing. As far as this trend, we talked about the chiplet trend, we talked about the EUV thing. How important was the rise of hyperscalers to your success? Because what I see from that is, they’re buying at such scale, they will actually do LTV calculations to say that, “Look, yes, these AMD processors are worth it in the long run”. And number two, to the extent there is a software hole, they will do the work to fill that in, because they can see the long-term benefit. Did that impact when you were thinking about what we can actually win here? Was that a driver?</strong></p>
<p><strong>LS:</strong> Yeah, it’s a great point. When you think about high-performance computing and just how things have changed, the fact is, the hyperscalers are such a significant piece of the overall market that we have spent a lot of time there, and the point that you make is absolutely true, which is — you’d like to think in every market, the product always wins but that’s not necessarily true. In the hyperscaler market, the best product wins.</p>
<p><strong>Yes.</strong></p>
<p><strong>LS:</strong> And we were able to show that. Frankly, the key thing in this market is, it’s not enough to win once and it’s not enough to win temporally.</p>
<p><strong>You have to win the roadmap.</strong></p>
<p><strong>LS:</strong> You have to win the roadmap and that was very much what we did in that particular point in time.</p>
<p><strong>And so when you come in 2014, you’re like, “Look, I can see a roadmap where we can actually win”.</strong></p>
<p><strong>LS:</strong> That’s right.</p>
<p><strong>And there’s customers coming along that actually will buy on the roadmap.</strong></p>
<p><strong>LS:</strong> That’s right and by the way, they’ll ask you to prove it. In Zen 1, they were like, “Okay, that’s pretty good”, Zen 2 was better, Zen 3 was much, much better. That roadmap execution has put us in the spot where now we are very much deep partners with all the hyperscalers, which we really appreciate and as you think about, again, the AI journey, it is a similar journey.</p>
<p><strong>Yes. Well, one more question on x86. How do you think about the consumer space in conjunction with all this? You think about, say like an Intel, they have to keep the fabs full, so they need to maximize their chips for everything. The point with fabs is, Intel wants to be integrated and there’s a bit where AMD is in a different position so they can meet the hyperscalers where they are better and just make great chips. But is there a volume consideration just because you want to get leverage on your design costs, on your IP investments? I’m just curious how those calculations work in a world where it’s not your fabs on the line, it’s not your billions of CapEx. I’m curious how you think about that differently from an integrated player.</strong></p>
<p><strong>LS:</strong> The way we think about it is, it is about scale. When we were back in 2014-15, we were a $4 billion company and in that case, you can spend a certain amount of R&amp;D. Last year, we were like, a whatever, $22 plus billion company, you can spend a lot more on R&amp;D.</p>
<p><strong>Yes, so basically, it’s still the same calculation by and large.</strong></p>
<p><strong>LS:</strong> It is the same calculation of how do we leverage.</p>
<p><strong>But maybe less risk of going bankrupt if you spend way too much on fabs.</strong></p>
<p><strong>LS:</strong> Well, I think the key thing is leveraging of the IP. It’s sort of the engines, the compute engines that we have. That’s our absolutely number one priority, is to get those compute engines on a very aggressive roadmap and then, we build products out of that.</p>

<h4>AI</h4>
<p><strong>What was your response in November 2022 when ChatGPT shows up?</strong></p>
<p><strong>LS:</strong> Well, it was really the crystallization of what AI is all about.</p>
<p><strong>Obviously you’ve been in the graphics game for a long time, you’ve been thinking about high-performance computing, so the idea that GPUs would be important was not foreign to you. But were you surprised the extent to which it changed the perception of everyone else around you and what happened after that?</strong></p>
<p><strong>LS:</strong> We were very much on this path of GPUs for high-performance computing and AI. Actually, it was probably a very significant arc that we started, let’s call it back in the 2017 plus timeframe. We’ve always been in GPUs, but really focusing on-</p>
<p><strong>What was it in 2017 that made you realize that, “Wait, we have these, we thought we bought ATI for gaming, suddenly, there’s this completely different application”?</strong></p>
<p><strong>LS:</strong> It was the next big opportunity, we knew it was the next big opportunity. It was something that Mark and I discussed, which was, by putting CPUs and GPUs together in systems and designing them together, we’re going to get a better answer and the first near-term applications were around super-computing. We were very focused on these large machines that would reside at national laboratories and deep research facilities and we knew that we could build these massively parallel GPU machines to do that. The AI portion, we always also thought about it as clearly a HPC plus AI play.</p>
<p><strong>You said before that <a href="https://www.theverge.com/23894647/amd-ceo-lisa-su-ai-chips-nvidia-supply-chain-interview-decoder">AI is the killer application for HPC</a>.</strong></p>
<p><strong>LS:</strong> Yes.</p>
<p><strong>But you will talk to people in HPC, they’re like, “Well, it’s a little bit different”, to what extent is that the same category versus adjacent categories?</strong></p>
<p><strong>LS:</strong> It’s adjacent but highly-related categories, and it all depends on the accuracy that you want in your calculations, whether you’re using the full accuracy or you want to use some of these other data formats. But I think the real key though, and the thing that really we had good foresight on is, because of our chiplet strategy, we could build a highly modular system that could be, let’s call it, an integrated CPU and GPU, or it could be just incredible GPU capability that people needed.</p>
<p>And so, the ChatGPT moment for me was the clarity around, now everybody knew what AI was for. Before, it was only the scientists and the engineers who thought about AI, now everybody could use AI. These models are not perfect, but they’re amazingly good, and with that, I think the clarity around how do we get more AI compute in people’s hands as soon as possible was clear. Because of the way we had built our design system, we could really have two flavors. We had HPC-only flavor, which is what we would call our MI300A and we had AI only flavor, which was the MI300X.</p>
<p><strong>Was that kind of an uncomfortable shift? Like, “Actually, no, we want less precision because the scalability is so important”.</strong></p>
<p><strong>LS:</strong> It wasn’t uncomfortable. It was strikingly fast.</p>
<p><strong>It happened so fast. AMD has done very well, you hit an all-time high a couple of months ago. But by and large, obviously <a href="https://stratechery.com/2023/nvidia-on-the-mountaintop/">Nvidia captured the gestalt</a> as it were in a lot of the momentum and upside. What did they have, from your perspective, in that period that AMD had to catch up on?</strong></p>
<p><strong>LS:</strong> I think the way to think about it is just, where was the focus and relatively speaking — look, I give [Nvidia CEO] Jensen [Huang] and Nvidia a lot of credit. They were investing in this space for a long time before it was absolutely clear where things were going. We were also investing, although I would say we had a couple of arcs. We had our CPU arc, and then we have our GPU arc.</p>
<p><strong>Hey, your hands are full crushing Intel, so I get it.</strong></p>
<p><strong>LS:</strong> I would say it a different way, we are at the beginning of what AI is all about. One of the things that I find curious is when people think about technology in short spurts. Technology is not a short spurt kind of sport, this is like a 10-year arc we’re on, we’re through maybe the first 18 months. From that standpoint, I think we’re very clear on where we need to go and what the roadmap needs to look like. One of the things that you mentioned earlier on software, very, very clear on how do we make that transition super easy for developers, and one of the great things about our acquisition of Xilinx is we acquired a phenomenal team of 5,000 people that included a tremendous software talent that is right now working on making AMD AI as easy to use as possible.</p>
<p><strong>One of the things that does strike me about the contrast is, and one of Nvidia’s really brilliant moves was the <a href="https://nvidianews.nvidia.com/news/nvidia-to-acquire-mellanox-for-6-9-billion">acquisition of Mellanox</a> and their portfolio in networking, and to the extent it matters to tie all these chips together, particularly for training.</strong></p>
<p><strong>In your Computex keynote, you talked about the new <a href="https://www.afp.com/en/news/1315/amd-broadcom-cisco-google-hewlett-packard-enterprise-intel-meta-and-microsoft-form-ultra-accelerator-link-ualink-promoter-group-drive-data-center-ai-connectivity-202405306536021">Ultra Accelerator Link</a> and Ultra Ethernet Link standards, and this idea of bringing lots of companies together, kind of calling back to the Open Compute Project back in the day as far as data centers. Makes perfect sense, particularly given Nvidia’s proprietary solutions have the same high margins, we all know and love, as the rest of their products.</strong></p>
<p><strong>But I guess this is my question about your long-term run — do you think it’s fair to say that, from a <a href="https://stratechery.com/2024/ai-integration-and-modularization/">theoretical Clayton Christensen perspective</a>, because we’re early in AI, maybe it’s not a surprise, the more proprietary integrated solution is the belle of the ball in many respects? There’s a bit where, yes, being open and modular all makes sense, but maybe that’s not going to be good enough for a while.</strong></p>
<p><strong>LS:</strong> I would say it this way. When you look at what the market will look like five years from now, what I see is a world where you have multiple solutions. I’m not a believer in one-size-fits-all, and from that standpoint, the beauty of open and modular is that you are able to, I don’t want to use the word customize here because they may not all be custom, but you are able to tailor.</p>
<p><strong>Customize in the broad sense.</strong></p>
<p><strong>LS:</strong> That’s right.</p>
<p><strong>Tailor is a good word.</strong></p>
<p><strong>LS:</strong> Tailor is the right word — you are able to tailor the solutions for different workloads, and my belief is that there’s no one company who’s going to come up with every possible solution for every possible workload. So, I think we’re going to get there in different ways.</p>
<p>By the way, I am a big believer that these big GPUs that we’re going to build are going to continue to be the center of the universe for a while, and yes, you’re going to need the entire network system and reference system together. The point of what we’re doing is, all of those pieces are going to be in reference architectures going forward, so I think architecturally that’s going to be very important.</p>
<p>My only point is, there is no one size that’s going to fit all and so the modularity and the openness will allow the ecosystem to innovate in the places that they want to innovate. The solution that you want for hyperscaler 1 may not be the same as a solution you want for hyperscaler 2, or 3.</p>
<p><strong>Where do you think the balance is going to be then, between there being a standard approach versus, “This is the Microsoft approach”, “This is the Meta approach”? There’s some commonality there, but it is actually fairly customized to their use cases and needs. Again, not next year, but in the long run.</strong></p>
<p><strong>LS:</strong> I think as you get out three, four or five years, I think you’re going to see more tailoring for different workloads, and what happens is, the algorithms are going to — right now, we’re going through a period of time where the algorithms are just changing so, so quickly. At some point, you’re going to get to the place where, “Hey, it’s a bit more stable, it’s a little bit more clear”, and at the types of volumes that we’re talking about, there is significant benefit you can get not just from a cost standpoint, but from a power standpoint. People talk about chip efficiency, system efficiency now being as important if not more important than performance, and for all of those reasons, I think you’re going to see multiple solutions.</p>
<p><strong>Is this an underrated tailwind for your x86 business? You talked in your keynote about the fact that the majority of CPUs in the cloud are more than five years old, and you said something like, “One of our CPUs can replace five or six of these old ones”. Do you see that being actually — because right now I think there’s a lot of trepidation around your business and Intel’s business that all the spend is going to AI, no one’s even buying CPUs anymore, is this sort of power wall where if we can take out a bunch of CPUs from our data center, we can save power by putting other ones?</strong></p>
<p><strong>LS:</strong> I think both things are true. I think the modernization of data centers absolutely has to happen. It will happen, and then the other point is—</p>
<p><strong>It might not happen right now.</strong></p>
<p><strong>LS:</strong> Well, no. I think we’re seeing the investments come back into the areas for modernization, but the other thing that’s really important is, again, as much as we love GPUs, that’s a huge growth driver for us going forward, not every workload’s going to go to a GPU. You are going to have traditional workloads, you’re going to have mixed workloads, and I think that’s the key point of the story is there’s a lot of things that you have to do in large enterprises, and our goal is to make sure that we have the right solution across from all of those capabilities.</p>
<p><strong>How much inference do you see actually going back to the CPU?</strong></p>
<p><strong>LS:</strong> I think a good amount of inference will be done on the CPU, and even as you think about what we’re talking about is the very large models obviously need to be on GPUs, but how many companies can really afford to be on the largest of models? And so, you can see now already that for smaller models, they’re more fine-tuning for those kinds of things, the CPU is quite capable of it, and especially if you go to the edge.</p>

<h4>Competing With Nvidia</h4>
<p><strong>Right. You noted on the <a href="https://stratechery.com/2024/nvidia-earnings-amd-earnings-nvidias-goldilocks-pitch/">last earnings call</a> that the MI300, it’s been supply-constrained, your fastest ramp ever, but is maybe from the expectations of some investors, a little disappointing in the projections for the end of the year. How much do you feel that shift to being demand-constrained is about <a href="https://ir.amd.com/news-events/press-releases/detail/1201/amd-accelerates-pace-of-data-center-ai-innovation-and">the 325 coming along</a>, which you talked about this week, versus the fact that just generally Nvidia supply has gone up, as everyone’s trying to figure this stuff out? Yes, your long-term opportunity is being this sort of customized supplier — tailored supplier, sorry, is the word that we’re going for — versus, “Look, I don’t want to say picking up but just we need GPUs, we’ll buy them from anyone”. Where do you feel your demand curves are relative to the competition and the rapid progression of the space?</strong></p>
<p><strong>LS:</strong> Again, let me take a step back and make sure we frame the conversation. The demand for AI compute has been off the charts, I think nobody would have predicted this type of demand, and so when I say that there is tightness in the supply chain, that’s to be expected, because nobody expected that you would need this many GPUs in this timeframe. The fact is the semiconductor industry is really good at building capacity, and so that is really what we’ve seen. As we’ve started to forecast-</p>
<p><strong>And so you feel it’s more a function of there’s just so much supply coming online?</strong></p>
<p><strong>LS:</strong> Absolutely, and that’s our job. Our job is to make it to a place where you’re not constrained by manufacturing capacity.</p>
<p>Really, for us, it is about ensuring that customers are really ramping their workloads and that is a lot of deep work, deep partnerships that we’re doing with our customers. So honestly, I feel really good about the opportunities here. We’ve been through this before where it’s very similar to what we saw when we did the initial data center server CPU ramps, which is our customers work very closely with us, they get their software optimized, and then they add new workloads, and add more volumes, and that’s what I would expect to happen here, too.</p>
<p>The difference in AI is that I think customers are willing to take more risk, because there’s a desire to get as much, as fast as possible.</p>
<p><strong>Is there a challenge for you, because that desire to take more risks means they’re more accepting of say, high margins to get the leading GPUs or whatever it might be, or the GPU with the largest ecosystem, developer ecosystem?</strong></p>
<p><strong>LS:</strong> What I will say is I’m super happy with the progress we’ve made on software.</p>
<p><strong>Fair enough.</strong></p>
<p><strong>LS:</strong> What we’re seeing is excellent out-of-box performance. The fact is things just run, the fact is that much of the developer ecosystem wants to move up the abstraction layer, because everybody wants choice.</p>
<p><strong>And you feel you’re going to get to a stage where that move up the abstraction layer is a common layer across companies, as opposed to getting one company internally moves up the abstraction layer, and so they can buy any CPU, but that doesn’t necessarily benefit you going into another company, or do you feel that’s going to be-</strong></p>
<p><strong>LS:</strong> I absolutely believe that it’ll be across the industry. Things <a href="https://en.wikipedia.org/wiki/PyTorch">like PyTorch</a>, I think PyTorch is extremely widely adopted, <a href="https://openai.com/index/triton/">OpenAI Triton</a>, similar. These are larger industry things where frankly, part of the desire is it takes a long time to program down to the hardware. Everyone wants to innovate quickly, and so the abstraction layer is good from the standpoint of just rapid innovation.</p>
<p><strong>You’ve been traditionally a second wave adopter of TSMC’s new nodes, maybe a year, year-and-a-half behind. Do you feel pressure to move up to the top tier? Obviously, you’re a relatively small company to some of the players in this world, $22 billion is impressive, but you still have to think about your costs in that regard. Or is it just a pressing need to be on the absolute cutting-edge?</strong></p>
<p><strong>LS:</strong> Well, I think you would say that we’re one of the top five for sure in terms of just overall our volumes from a fabless standpoint, and absolutely bleeding-edge is helpful. It’s not something that we think about in terms of should we or shouldn’t we, I think what we think about is from a roadmap standpoint, like for example, we talked about a one-year cadence in terms of GPUs coming out.</p>
<p><strong>Unfortunately, for you kind of on the opposite tick tock from Nvidia a little bit, is that a little frustrating?</strong></p>
<p><strong>LS:</strong> No, not at all. Look, again, one of the things that’s important for me is our roadmap is based on what we believe is possible, and what we believe our customers want and need.</p>
<p><strong>Everyone like me wants to talk about the short term head-to-head, so annoying.</strong></p>
<p><strong>LS:</strong> No, it’s not so annoying, it’s just context, everything requires context.</p>
<p><strong>Is there ever a world where AMD fabs with Intel?</strong></p>
<p><strong>LS:</strong> I would say that we’re very happy with our manufacturing relationships right now.</p>
<p><strong>It does occur to me, Intel, AMD — it’s one of the greatest rivalries in the history of technology from basically the very beginning. Is there a bit though where when you step back, you want to step back in these conversations, there is a bit where you are in it together, because the real enemy is Arm?</strong></p>
<p><strong>LS:</strong> You make it sound like Arm is an enemy, I don’t consider ARM an enemy, so let me start with that. We use Arm all over our product portfolio. I consider the fact that we think x86 is a phenomenal architecture, and the capabilities are there, but please don’t think of AMD as an x86 company, we are a computing company, and we will use the right compute engine for the right workload.</p>
<p>As it relates to how I think about — if you look at the semiconductor industry today, there are places where we compete, and then there are places where we partner. So on your Intel point, we do compete in certain areas, but we also partner in certain areas. Intel is part of the UALink Consortium, they’re part of the Ultra Ethernet Consortium.</p>
<p><strong>They’re very interested in this sort of modularization and standardization as well.</strong></p>
<p><strong>LS:</strong> We agree with this idea of having a link that can go across different accelerators is actually a good thing. So, I think that’s true across the industry. We’re at a place where there are places we compete, but there are also places where we can partner.</p>
<p><strong>You have had an amazing 10-year run with the x86 results you’ve done in the server space, the data center, speaks for itself. Now, it’s like a new champion appears, are you girded up and ready to go for another round?</strong></p>
<p><strong>LS:</strong> This is the next arc. I can tell you that the thing that’s so amazing about where we are today in high-performance computing is, who would imagine? It’s like a new world. It’s an incredibly exciting.</p>
<p><strong>You’re feeling re-energized, you’re ready to go?</strong></p>
<p><strong>LS:</strong> Absolutely ready to go. More than ready.</p>
<p><strong>Lisa, thank you very much.</strong></p>
<p><strong>LS:</strong> Thank you.</p>
<hr>
<p>This Daily Update Interview is also available as a podcast. To receive it in your podcast player, <a href="https://stratechery.passport.online/member">visit Stratechery</a>.</p>
<p>The Daily Update is intended for a single recipient, but occasional forwarding is totally fine! If you would like to order multiple subscriptions for your team with a group discount (minimum 5), please contact me directly.</p>
<p>Thanks for being a supporter, and have a great day!</p>

	</div><!-- .entry-content -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NLRB judge declares non-compete clause is an unfair labor practice (356 pts)]]></title>
            <link>https://www.nlrbedge.com/p/in-first-case-of-its-kind-nlrb-judge</link>
            <guid>40696992</guid>
            <pubDate>Sun, 16 Jun 2024 13:40:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nlrbedge.com/p/in-first-case-of-its-kind-nlrb-judge">https://www.nlrbedge.com/p/in-first-case-of-its-kind-nlrb-judge</a>, See on <a href="https://news.ycombinator.com/item?id=40696992">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg" width="1456" height="1820" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1820,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:544238,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc49767-e9ba-4305-8c65-9f4243361e92_1638x2048.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Administrative Law Judge (ALJ) Sarah Karpinen issued her decision in </span><em><strong><a href="https://nlrbresearch.com/pdfs/09031d4583d765f7.pdf" rel="">J.O. Mory, Inc.</a></strong></em><span> yesterday. The case mostly revolves around an employer firing a union organizer that became employed at the company with the goal of organizing his coworkers (also known as “salting”). The union salt in this case lied about his employment history to get hired, declared he was a union organizer after being hired, and then was fired. Salting is protected activity, lying about your employment history to salt is also protected activity, and firing someone for salting is an unfair labor practice. Thus, Judge Karpinen ordered that the employer rehire the salt with backpay.</span></p><p><span>As part of litigating this case, the General Counsel (GC) of the National Labor Relations Board (NLRB) also alleged that the employer’s non-compete clause and coworker non-solicitation clause were illegal work rules under the </span><em><a href="https://nlrbresearch.com/pdfs/09031d4583af43bd.pdf" rel="">Stericycle</a></em><span> standard. The GC has been pursuing this </span><a href="https://nlrbresearch.com/pdfs/09031d4583a87168.pdf" rel="">particular legal theory</a><span> since early last year, but this is the first time the theory has been put in front of an ALJ and also the first time an ALJ has ruled that these kinds of clauses are unfair labor practices that violate the National Labor Relations Act (NLRA).</span></p><p>The non-compete clause in question states that:</p><blockquote><p>(A) For a period of twelve (12) months following termination or separation of employment for any reason, Employee will not directly or indirectly, on Employee's behalf or on behalf of others:  … (iii) Engage in, be employed by, or become interested in, in any manner or capacity, as a principal, agent, partner, officer, director, employee, consultant, independent contractor, advisor or in any other capacity, with any insurance agency, insurance business or in any other business similar or competitive with Employer’s business as the same may exist at any time during the term of this Agreement, this covenant restricting Employee’s employment being limited to Employer’s service area which is defined as the county of the office where the Employee is located and to all contiguous counties thereto. If, during Employee’s employment, Employee is employed in any other of Employer’s locations, then these restrictions shall also apply to the county in which such office is located, and to all contiguous counties to that location. The parties expressly agree that the restrictions above set forth are fair and reasonable with regard to scope, time periods, geographic area and in all other respects.</p></blockquote><p>The ALJ determined that this clause was illegal with the following reasoning:</p><blockquote><p><span>The non-compete provision in Provision 2(A) is overly broad in scope and would deter a reasonable employee from engaging in protected activity by barring employees from directly or indirectly, and in any capacity, engaging in, being employed by, or becoming interested in any enterprise that is “similar or competitive” to the employer’s business. Not only is this provision ridiculously broad in scope (could an employee indirectly engage with a competitor by sending a family member to buy something from its store?), but </span><strong>it would also cause a reasonable employee to refrain from engaging in protected activities that come with a risk of retaliation. If an employee knows they are barred from being involved in any capacity with any company that operates a similar business to Respondent, they will logically be more fearful of being fired and less willing to rock the boat because they face the prospect of being unable to find any work in their geographic area if they are fired or forced to leave their job.</strong></p></blockquote><p>The coworker non-solicitation clause in question states that:</p><blockquote><p>(C) During the term of this Agreement and for a period of 24 months after termination of employment for any reason, Employee will not, either directly or indirectly for himself or on behalf of others, solicit, encourage, or attempt to persuade any other employee of Employer to leave the employ of Employer. This is intended to prevent “pirating” of Employer employees.</p></blockquote><p>The ALJ determined that this clause was illegal with the following reasoning:</p><blockquote><p><span>The prohibition in Provision 1(C) on soliciting employees to leave Respondent’s employ would </span><strong>dissuade a reasonable employee from engaging in protected activity like telling their coworkers about the wages and benefits offered by the Union out of a reasonable fear that Respondent might accuse them of inducing other employees to quit</strong><span>. See </span><strong><a href="https://nlrbresearch.com/pdfs/09031d45800b7ca9.pdf" rel="">M.J. Mechanical Services, 325 NLRB 1098, 1106 (1998)</a></strong><span> (telling employees about union benefits, encouraging them to engage in salting activities, and referring them to union hall protected even when it resulted in one employee going to work for a union contractor). </span><strong>It may also deter employees from asking their coworkers to make a concerted threat to quit unless their working conditions improve.</strong><span> See </span><strong><a href="https://nlrbresearch.com/pdfs/09031d4583883f78.pdf" rel="">Morgan Corp., 371 NLRB No. 142, slip op. at 5 (2022)</a></strong><span> (employee who told supervisor that he and his co-workers would quit over demand for higher wages was “indisputably” engaged in protected concerted activity).</span></p></blockquote><p>Despite all the discussion about the FTC banning non-competes, there still seems to be little recognition that non-competes for non-supervisory workers are effectively impossible to enforce at the moment due to the policies of the NLRB GC.</p></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>